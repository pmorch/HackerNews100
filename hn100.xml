<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 17 Dec 2025 03:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[No AI* Here – A Response to Mozilla's Next Chapter (144 pts)]]></title>
            <link>https://www.waterfox.com/blog/no-ai-here-response-to-mozilla/</link>
            <guid>46295268</guid>
            <pubDate>Tue, 16 Dec 2025 22:07:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.waterfox.com/blog/no-ai-here-response-to-mozilla/">https://www.waterfox.com/blog/no-ai-here-response-to-mozilla/</a>, See on <a href="https://news.ycombinator.com/item?id=46295268">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p>Mozilla’s new CEO recently announced their <a href="https://blog.mozilla.org/en/mozilla/leadership/mozillas-next-chapter-anthony-enzor-demeo-new-ceo/">vision for the future</a>: positioning Mozilla as “the world’s most trusted software company” with AI at its centre. As someone who has spent nearly 15 years building and maintaining Waterfox, I understand the existential pressure Mozilla faces. Their lunch is being eaten by AI browsers. Alphabet themselves reportedly see the writing on the wall, developing what appears to be a new browser separate from Chrome. The threat is real, and I have genuine sympathy for their position.</p>
<p>But I believe Mozilla is making a fundamental mistake.</p>
<h2 id="the-asterisk-matters">The Asterisk Matters</h2>
<p>Let’s be clear about what we’re talking about. “AI” has become a catch-all term that to me, obscures more than it reveals. Machine learning technologies like the <a href="https://github.com/browsermt/bergamot-translator">Bergamot translation project</a> offer real, tangible utility. Bergamot is transparent in what it does (translate text locally, period), auditable (you can inspect the model and its behavior), and has clear, limited scope, even if the internal neural network logic isn’t strictly deterministic.</p>
<p>Large language models are something else entirely˟. They are black boxes. You cannot audit them. You cannot truly understand what they do with your data. You cannot verify their behaviour. And Mozilla wants to put them at the heart of the browser and that doesn’t sit well.</p>
<p>But it’s important to note I do find LLMs have utility, measurably so. But here I am talking in the context of a web browser and the fundamental scepticism I have toward it in that context.</p>
<h2 id="what-is-a-browser-for">What Is a Browser For?</h2>
<p>A browser is meant to be a user agent, more specifically, <em>your</em> agent on the web. It represents you, acts on your behalf, and executes your instructions. It’s called a user agent for a reason.</p>
<p>When you introduce a potential LLM layer between the user and the web, you create something different: “a user agent user agent” of sorts. The AI becomes the new user agent, mediating and interpreting between you and the browser. It reorganises your tabs. It rewrites your history. It makes decisions about what you see and how you see it, based on logic you cannot examine or understand.</p>
<p>Mozilla promises that “AI should always be a choice - something people can easily turn off.” That’s fine. But how do you keep track of what a black box actually does when it’s turned on? How do you audit its behaviour? How do you know it’s not quietly reshaping your browsing experience in ways you haven’t noticed?</p>
<p>Even if you can disable individual AI features, the cognitive load of monitoring an opaque system that’s supposedly working on your behalf would be overwhelming. Now, I truly believe and trust that Mozilla will do what they think is best for the user; but I’m not convinced it will be.</p>
<p>This isn’t paranoia, because after all, “It will evolve into a modern AI browser and support a portfolio of new and trusted software additions.” It’s a reasonable response to fundamentally untrustworthy technology being positioned as the future of web browsing.</p>
<h2 id="mozillas-dilemma">Mozilla’s Dilemma?</h2>
<p>I get it. Mozilla is facing an existential crisis. AI browsers are proliferating. The market is shifting. Revenue diversification from search is urgent. Firefox’s market share continues to decline. The pressure to “do something” must be immense, and I understand that.</p>
<p>But there’s a profound irony in their response. Mozilla speaks about trust, transparency, and user agency while simultaneously embracing technology that undermines all three principles. They promise AI will be optional, but that promise acknowledges they’re building AI so deeply into Firefox that an opt-out mechanism becomes necessary in the first place.</p>
<p>Mozilla’s strength has always come from the technical community - developers, power users, privacy advocates. These are the people who understand what browsers should be and what they’re for. Yet Mozilla seems convinced they need to chase the average user, the mainstream market that Chrome already dominates.</p>
<p>That chase has been failing for over a decade. Firefox’s market share has declined steadily as Mozilla added features their core community explicitly didn’t want. Now they’re doubling down on that strategy, going after “average Joe” users while potentially alienating the technical community that has been their foundation.</p>
<h2 id="what-waterfox-offers-instead">What Waterfox Offers Instead</h2>
<p>Waterfox exists because some users want a browser that simply works well at being a browser. The UI is mature - arguably, it has been a solved for problem for years. The customisation features are available and apparent. The focus is on performance and web standards.</p>
<p>In many ways, browsers are operating systems of their own, and a browser’s job is to be a good steward of that environment. AI, in its current form and in my opinion does not match that responsibility.</p>
<p>And yes, yes - disabling features is all well and good, but at the end of the day, if these AI features are black boxes, how are we to keep track of what they actually do? The core browsing experience should be one that fully puts the user in control, not one where you’re constantly monitoring an inscrutable system that claims to be helping you.</p>
<p>Waterfox will not include LLMs. Full stop. At least and most definitely not in their current form or for the foreseeable future.</p>
<h3 id="a-note-on-other-forks-and-governance">A Note on other Forks and Governance</h3>
<p>The Firefox fork ecosystem includes several projects that tout their independence from Mozilla. Some strip out more features than Waterfox does, some make bolder design choices.</p>
<p>But here’s what often gets overlooked - many of these projects operate without any formal governance structure, privacy policies, or terms of service. There’s no legal entity, no accountability mechanism, no recourse if promises are broken. Open source gives developers the freedom to fork code and make claims, but it doesn’t automatically make those claims trustworthy.</p>
<p>When it comes to something as critical as a web browser - software that mediates your most sensitive online interactions - the existence of a responsible organisation with clear policies becomes crucial. Waterfox maintains formal policies and a legal entity, not because it’s bureaucratic overhead, but because it creates accountability that many browser projects simply don’t have.</p>
<p>You deserve to know who is responsible for the software you rely on daily and how decisions about your privacy are made. The existence of formal policies, even imperfect ones, represents a commitment that your interests matter and that there’s someone to hold accountable.</p>
<p>You may think, so what? And fair enough, I can’t change your mind on that, but Waterfox’s governance has allowed it to do something no other fork has (and likely will not do) - trust from other large, imporant third parties which in turn has given Waterfox users access to protected streaming services via Widevine. It’s a small thing, but to me it showcases the power of said governance.</p>
<h2 id="on-inevitability">On Inevitability</h2>
<p>Some will argue that AI browsers are inevitable, that we’re fighting against the tide of history. Perhaps. AI browsers may eat the world.
But the web, despite having core centralised properties, is fundamentally decentralised. There will always be alternatives. If AI browsers dominate and then falter, if users discover they want something simpler and more trustworthy, Waterfox will still be here, marching patiently along.
We’ve been here before. When Firefox abandoned XUL extensions, Waterfox Classic preserved them. When Mozilla started adding telemetry and Pocket and sponsored content, Waterfox stripped it out. When the technical community asked for a browser that simply respected them, Waterfox delivered.</p>
<p>I’ll keep doing that. Not because it’s the most profitable path or because it’s trendy, but because it’s what users who value independence and transparency actually need.</p>
<p>The browser’s job is to serve you, not to think for you. That core Waterfox principle hasn’t changed, and it won’t.</p>
<hr>
<p>* The asterisk acknowledges that “AI” has become a catch-all term. Machine learning tools like local translation engines (Bergamot) are valuable and transparent. Large language models, in their current black-box form, are neither.</p>
<p>˟ As is my understanding, but please feel free to correct me if that isn’t correct.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MIT professor shot at his Massachusetts home dies (209 pts)]]></title>
            <link>https://www.bbc.com/news/articles/cly08y25688o</link>
            <guid>46295071</guid>
            <pubDate>Tue, 16 Dec 2025 21:52:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/cly08y25688o">https://www.bbc.com/news/articles/cly08y25688o</a>, See on <a href="https://news.ycombinator.com/item?id=46295071">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="text-block"><p>A Massachusetts university professor who was shot at his home has died, campus officials say. </p><p>Nuno F Gomes Loureiro, 47, a nuclear science and engineering professor from Portugal, was shot "multiple times" on Monday and died on Tuesday morning in hospital, according to Brookline police and Massachusetts Institute of Technology (MIT) officials.</p><p>Police said officers responded to a call for gunshots at an apartment at about 8:30pm local time. Loureiro was taken by ambulance to a Boston hospital, where he died on Tuesday morning. </p><p>No one is in custody and police are treating the incident as "an active and ongoing homicide investigation",  the Norfolk County District Attorney's Office said.</p></div><div data-component="text-block"><p>CBS News, the BBC's US media partner, reported that a neighbour said he heard "three loud bangs" Monday evening and thought somebody in the apartment building was kicking in a door. </p><p>Long-time resident Anne Greenwald told CBS that the professor had a young family and went to school nearby.</p><p>Loureiro majored in Physics at Instituto Superior Técnico in Lisbon in 2000 and obtained a Phd in physics at Imperial College London in 2005, according to his faculty web page.</p></div><div data-component="text-block"><p>The theoretical physicist and fusion scientist was known for his award-winning research in magnetised plasma dynamics. </p><p>Magnetised plasma dynamics is the study of the state of matter in which the motion of charged particles is influenced by the presence of an external magnetic field, according to Nature.</p><p>Loureiro joined MIT's faculty in 2016 and was named director of MIT's Plasma Science and Fusion Center in 2024.</p><p>His research addressed "complex problems lurking at the center of fusion vacuum chambers and at the edges of the universe", according to the university's obituary. </p><p>He also studied how to harness clean "fusion power" to combat climate change, CBS said.</p><p>"Our deepest sympathies are with his family, students, colleagues, and all those who are grieving," an MIT spokesperson said in a statement provided to the BBC.</p><p>The university added that "focused outreach and conversations" are taking place within the MIT community to offer care and support for those who knew the professor.</p><p>The centre's preceding director, Dennis Whyte, described Loureiro as both a brilliant scientist and a brilliant person.</p><p>"He shone a bright light as a mentor, friend, teacher, colleague and leader, and was universally admired for his articulate, compassionate manner," Mr Whyte told MIT News. </p><p>Deepto Chakrabarty, the head of MIT's department of physics, echoed those sentiments and said that Loureiro was a champion of plasma physics and that his recent research was "a particularly exciting new scientific direction".</p><p><i id="correction-16-december:-an-earlier-version-of-this-story-incorrectly-defined-the-kind-of-plasma-that-professor-loureiro-researched.">Correction 16 December: An earlier version of this story incorrectly defined the kind of plasma that Professor Loureiro researched.</i></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI will make formal verification go mainstream (374 pts)]]></title>
            <link>https://martin.kleppmann.com/2025/12/08/ai-formal-verification.html</link>
            <guid>46294574</guid>
            <pubDate>Tue, 16 Dec 2025 21:14:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://martin.kleppmann.com/2025/12/08/ai-formal-verification.html">https://martin.kleppmann.com/2025/12/08/ai-formal-verification.html</a>, See on <a href="https://news.ycombinator.com/item?id=46294574">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
                

                
                <p>Published by Martin Kleppmann on 08 Dec 2025.</p>
                

                <p>Much has been said about the effects that AI will have on software development, but there is an
angle I haven’t seen talked about: I believe that AI will bring formal verification, which for
decades has been a bit of a fringe pursuit, into the software engineering mainstream.</p>

<p>Proof assistants and proof-oriented programming languages such as <a href="https://rocq-prover.org/">Rocq</a>,
<a href="https://isabelle.in.tum.de/">Isabelle</a>, <a href="https://lean-lang.org/">Lean</a>,
<a href="https://fstar-lang.org/">F*</a>, and <a href="https://agda.readthedocs.io/">Agda</a> have been around for a long
time. They make it possible to write a formal specification that some piece of code is supposed to
satisfy, and then mathematically prove that the code <em>always</em> satisfies that spec (even on weird
edge cases that you didn’t think of testing). These tools have been used to develop some large
formally verified software systems, such as an <a href="https://sel4.systems/">operating system kernel</a>,
a <a href="https://compcert.org/">C compiler</a>, and a
<a href="https://project-everest.github.io/">cryptographic protocol stack</a>.</p>

<p>At present, formal verification is mostly used by research projects, and it is
<a href="https://hillelwayne.com/post/why-dont-people-use-formal-methods/">uncommon</a> for industrial software
engineers to use formal methods (even those working on classic high-assurance software such as
medical devices and aircraft). The reason is that writing those proofs is both very difficult
(requiring PhD-level training) and very laborious.</p>

<p>For example, as of 2009, the formally verified seL4 microkernel consisted of 8,700 lines of C code,
but proving it correct required 20 person-years and
<a href="https://www.sigops.org/s/conferences/sosp/2009/papers/klein-sosp09.pdf">200,000 lines</a> of Isabelle
code – or 23 lines of proof and half a person-day for every single line of implementation. Moreover,
there are maybe a few hundred people in the world (wild guess) who know how to write such proofs,
since it requires a lot of arcane knowledge about the proof system.</p>

<p>To put it in simple economic terms: for most systems, the expected cost of bugs is lower than the
expected cost of using the proof techniques that would eliminate those bugs. Part of the reason is
perhaps that bugs are a negative externality: it’s not the software developer who bears the cost of
the bugs, but the users. But even if the software developer were to bear the cost, formal
verification is simply very hard and expensive.</p>

<p>At least, that was the case until recently. Now, LLM-based coding assistants are getting pretty good
not only at writing implementation code, but also at
<a href="https://www.nature.com/articles/s41586-025-09833-y">writing</a>
<a href="https://www.galois.com/articles/claude-can-sometimes-prove-it">proof scripts</a> in
<a href="https://arxiv.org/pdf/2503.14183v1">various languages</a>. At present, a human with specialist
expertise still has to guide the process, but it’s not hard to extrapolate and imagine that process
becoming fully automated in the next few years. And when that happens, it will totally change the
economics of formal verification.</p>

<p>If formal verification becomes vastly cheaper, then we can afford to verify much more software. But
on top of that, AI also creates a <em>need</em> to formally verify more software: rather than having humans
review AI-generated code, I’d much rather have the AI prove to me that the code it has generated is
correct. If it can do that, I’ll take AI-generated code over handcrafted code (with all its
artisanal bugs) any day!</p>

<p>In fact, I would argue that writing proof scripts is one of the best applications for LLMs. It
doesn’t matter if they hallucinate nonsense, because the proof checker will reject any invalid proof
and force the AI agent to retry. The proof checker is a small amount of code that is itself
verified, making it virtually impossible to sneak an invalid proof past the checker.</p>

<p>That doesn’t mean software will suddenly be bug-free. As the verification process itself becomes
automated, the challenge will move to correctly defining the specification: that is, how do you know
that the properties that were proved are actually the properties that you cared about? Reading and
writing such formal specifications still requires expertise and careful thought. But writing the
spec is vastly easier and quicker than writing the proof by hand, so this is progress.</p>

<p>I could also imagine AI agents helping with the process of writing the specifications, translating
between formal language and natural language. Here there is the potential for subtleties to be lost
in translation, but this seems like a manageable risk.</p>

<p>I find it exciting to think that we could just specify in a high-level, declarative way the
properties that we want some piece of code to have, and then to vibe code the implementation along
with a proof that it satisfies the specification. That would totally change the nature of software
development: we wouldn’t even need to bother looking at the AI-generated code any more, just like we
don’t bother looking at the machine code generated by a compiler.</p>

<p>In summary: 1. formal verification is about to become vastly cheaper; 2. AI-generated code needs
formal verification so that we can skip human review and still be sure that it works; 3. the
precision of formal verification counteracts the imprecise and probabilistic nature of LLMs. These
three things taken together mean formal verification is likely to go mainstream in the foreseeable
future. I suspect that soon the limiting factor will not be the technology, but the culture change
required for people to realise that formal methods have become viable in practice.</p>


                <div>
                    <p>If you found this post useful, please
                    <a href="https://www.patreon.com/martinkl">support me on Patreon</a>
                    so that I can write more like it!</p>
                    <p>
                    To get notified when I write something new,
                    <a href="https://bsky.app/profile/martin.kleppmann.com">follow me on Bluesky</a> or
                    <a href="https://nondeterministic.computer/@martin">Mastodon</a>,
                    or enter your email address:
                    </p>

                    

                    <p>
                    I won't give your address to anyone else, won't send you any spam, and you can unsubscribe at any time.
                    </p>
                </div>

                
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ty: A fast Python type checker and LSP (369 pts)]]></title>
            <link>https://astral.sh/blog/ty</link>
            <guid>46294289</guid>
            <pubDate>Tue, 16 Dec 2025 20:52:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://astral.sh/blog/ty">https://astral.sh/blog/ty</a>, See on <a href="https://news.ycombinator.com/item?id=46294289">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p><strong>TL;DR:</strong> <a href="https://github.com/astral-sh/ty">ty</a> is an <strong>extremely fast Python type checker and
language server</strong>, written in Rust, and designed as an alternative to tools like mypy, Pyright, and
Pylance.</p>
<p>Today, we're announcing the Beta release of <a href="https://github.com/astral-sh/ty">ty</a>. We now use ty
exclusively in our own projects and are ready to recommend it to motivated users for production use.</p>
<hr>
<p>At Astral, we build high-performance developer tools for the Python ecosystem. We're best known for
<a href="https://github.com/astral-sh/uv">uv</a>, our Python package manager, and
<a href="https://github.com/astral-sh/ruff">Ruff</a>, our linter and formatter.</p>
<p>Today, we're announcing the Beta release of the next tool in the Astral toolchain: <strong>ty, an
extremely fast Python type checker and language server</strong>, written in Rust.</p>
<div><p><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 430 117"><g aria-roledescription="group mark container" fill="none" stroke-miterlimit="10"><g aria-roledescription="axis" aria-label="X-axis for a linear scale with values from 0 to 50"><g pointer-events="none"><path stroke="rgba(127,127,127,0.25)" d="M341.5 100.5"></path></g><g pointer-events="none"><text text-anchor="middle" transform="translate(61.5 115.5)" font-family="Roboto Mono,monospace" font-size="12">0s</text><text text-anchor="middle" transform="translate(201.5 115.5)" font-family="Roboto Mono,monospace" font-size="12">20s</text><text text-anchor="middle" transform="translate(341.5 115.5)" font-family="Roboto Mono,monospace" font-size="12">40s</text></g></g><g aria-roledescription="axis" aria-label="Y-axis for a discrete scale with 4 values: ty, Pyrefly, Pyright, mypy"><g pointer-events="none"><text text-anchor="end" transform="translate(51.5 16.5)" font-family="Roboto Mono,monospace" font-size="12" font-weight="bold">ty</text><text text-anchor="end" transform="translate(51.5 41.5)" font-family="Roboto Mono,monospace" font-size="12">Pyrefly</text><text text-anchor="end" transform="translate(51.5 66.5)" font-family="Roboto Mono,monospace" font-size="12">Pyright</text><text text-anchor="end" transform="translate(51.5 91.5)" font-family="Roboto Mono,monospace" font-size="12">mypy</text></g></g><g aria-roledescription="rect mark container"><path aria-label="Sum of time: 2.186; tool: ty" aria-roledescription="bar" d="M61 6h15.302v13H61Z"></path><path aria-label="Sum of time: 5.32; tool: Pyrefly" aria-roledescription="bar" d="M61 31h37.24v13H61Z"></path><path aria-label="Sum of time: 19.623; tool: Pyright" aria-roledescription="bar" d="M61 56h137.361v13H61Z"></path><path aria-label="Sum of time: 45.662; tool: mypy" aria-roledescription="bar" d="M61 81h319.634v13H61Z"></path></g><g aria-roledescription="text mark container"><text aria-label="Sum of time: 5.32; tool: Pyrefly; timeFormat: 5.32s" aria-roledescription="text mark" transform="translate(104.24 41.5)" font-family="Roboto Mono,monospace" font-size="12">5.32s</text><text aria-label="Sum of time: 19.623; tool: Pyright; timeFormat: 19.62s" aria-roledescription="text mark" transform="translate(204.361 66.5)" font-family="Roboto Mono,monospace" font-size="12">19.62s</text><text aria-label="Sum of time: 45.662; tool: mypy; timeFormat: 45.66s" aria-roledescription="text mark" transform="translate(386.634 91.5)" font-family="Roboto Mono,monospace" font-size="12">45.66s</text></g><g aria-roledescription="text mark container"><text aria-label="Sum of time: 2.186; tool: ty; timeFormat: 2.19s" aria-roledescription="text mark" transform="translate(82.302 16.5)" font-family="Roboto Mono,monospace" font-size="12" font-weight="bold">2.19s</text></g></g></svg></p><p><span>Type checking the<!-- --> <a target="_blank" rel="noreferrer" href="https://github.com/home-assistant/core">home-assistant</a> <!-- -->project on the command-line, without caching (<a href="https://github.com/astral-sh/ruff/blob/7f7485d608d2da19a0632a1238f2d4be551f612f/scripts/ty_benchmark/README.md" target="_blank" rel="noreferrer">M4</a>).</span></p></div>
<p>ty was designed from the ground up to power a language server. The entire ty architecture is built
around "incrementality", enabling us to selectively re-run only the necessary computations when a
user (e.g.) edits a file or modifies an individual function. This makes live updates extremely fast
in the context of an editor or long-lived process.</p>
<div><p><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 429 92"><g aria-roledescription="group mark container" fill="none" stroke-miterlimit="10"><g aria-roledescription="axis" aria-label="X-axis for a linear scale with values from 0.0 to 2.8"><g pointer-events="none"><path stroke="rgba(127,127,127,0.25)" d="M311.5 75.5"></path></g><g pointer-events="none"><text text-anchor="middle" transform="translate(61.5 90.5)" font-family="Roboto Mono,monospace" font-size="12">0s</text><text text-anchor="middle" transform="translate(186.5 90.5)" font-family="Roboto Mono,monospace" font-size="12">1s</text><text text-anchor="middle" transform="translate(311.5 90.5)" font-family="Roboto Mono,monospace" font-size="12">2s</text></g></g><g aria-roledescription="axis" aria-label="Y-axis for a discrete scale with 3 values: ty, Pyright, Pyrefly"><g pointer-events="none"><text text-anchor="end" transform="translate(51.5 16.5)" font-family="Roboto Mono,monospace" font-size="12" font-weight="bold">ty</text><text text-anchor="end" transform="translate(51.5 41.5)" font-family="Roboto Mono,monospace" font-size="12">Pyright</text><text text-anchor="end" transform="translate(51.5 66.5)" font-family="Roboto Mono,monospace" font-size="12">Pyrefly</text></g></g><g aria-roledescription="rect mark container"><path aria-label="Sum of time: 0.0044956; tool: ty" aria-roledescription="bar" d="M61 6h.562v13H61Z"></path><path aria-label="Sum of time: 0.3704936; tool: Pyright" aria-roledescription="bar" d="M61 31h46.312v13H61Z"></path><path aria-label="Sum of time: 2.6047441; tool: Pyrefly" aria-roledescription="bar" d="M61 56h325.593v13H61Z"></path></g><g aria-roledescription="text mark container"><text aria-label="Sum of time: 0.3704936; tool: Pyright; timeFormat: 370.5ms" aria-roledescription="text mark" transform="translate(113.312 41.5)" font-family="Roboto Mono,monospace" font-size="12">370.5ms</text><text aria-label="Sum of time: 2.6047441; tool: Pyrefly; timeFormat: 2.60s" aria-roledescription="text mark" transform="translate(392.593 66.5)" font-family="Roboto Mono,monospace" font-size="12">2.60s</text></g><g aria-roledescription="text mark container"><text aria-label="Sum of time: 0.0044956; tool: ty; timeFormat: 4.5ms" aria-roledescription="text mark" transform="translate(67.562 16.5)" font-family="Roboto Mono,monospace" font-size="12" font-weight="bold">4.5ms</text></g></g></svg></p><p><span>Re-computing diagnostics in the language server after editing a file in the<!-- --> <a target="_blank" rel="noreferrer" href="https://github.com/pytorch/pytorch">PyTorch</a> <!-- -->project (<a href="https://github.com/astral-sh/ruff/blob/7f7485d608d2da19a0632a1238f2d4be551f612f/scripts/ty_benchmark/README.md" target="_blank" rel="noreferrer">M4</a>).</span></p></div>
<p>You can install ty today with <code>uv tool install ty@latest</code>, or via our
<a href="https://marketplace.visualstudio.com/items?itemName=astral-sh.ty">VS Code extension</a>.</p>
<p>Like Ruff and uv, ty's implementation was grounded in some of our core product principles:</p>
<ol>
<li>
<p><strong>An obsessive focus on performance.</strong> Without caching, ty is consistently between 10x and 60x
faster than mypy and Pyright. When run in an editor, the gap is even more dramatic. As an
example, after editing a load-bearing file in the PyTorch repository, ty recomputes diagnostics
in 4.7ms: 80x faster than Pyright (386ms) and 500x faster than Pyrefly (2.38 seconds). ty is very
fast!</p>
</li>
<li>
<p><strong>Correct, pragmatic, and ergonomic.</strong> With features like
<a href="https://docs.astral.sh/ty/features/type-system/#intersection-types">first-class intersection types</a>,
<a href="https://docs.astral.sh/ty/features/type-system/#top-and-bottom-materializations">advanced type narrowing</a>,
and
<a href="https://docs.astral.sh/ty/features/type-system/#reachability-based-on-types">sophisticated reachability analysis</a>,
ty pushes forward the state of the art in Python type checking, providing more accurate feedback
and <a href="https://docs.astral.sh/ty/features/type-system/#gradual-guarantee">avoiding assumptions</a>
about user intent that often lead to false positives. Our goal with ty is not only to build a
faster type checker; we want to build a better type checker, and one that balances correctness
with a deep focus on the end-user experience.</p>
</li>
<li>
<p><strong>Built in the open.</strong> ty was built by our core team alongside dozens of active contributors
under the MIT license, and the same goes for our
<a href="https://marketplace.visualstudio.com/items?itemName=astral-sh.ty">editor extensions</a>. You can
run ty anywhere that you write Python (including in the <a href="https://play.ty.dev/">browser</a>).</p>
</li>
</ol>
<p>Even compared to other Rust-based language servers like Pyrefly, ty can run orders of magnitude
faster when performing incremental updates on large projects.</p>
<div><p><video width="100%" preload="none" autoplay="" loop="" muted="" controls="" playsinline=""><source src="https://astral.sh/static/MP4/TyPyTorch.mp4" type="video/mp4">Your browser does not support the video tag.</video></p><p><span>Editing a central file in the<!-- --> <a href="https://github.com/pytorch/pytorch" target="_blank" rel="noreferrer">PyTorch</a> <!-- -->repository with<!-- --> <a href="https://github.com/astral-sh/ty" target="_blank" rel="noreferrer">ty</a> <!-- -->(left) and<!-- --> <a href="https://github.com/facebook/pyrefly" target="_blank" rel="noreferrer">Pyrefly</a> <!-- -->(right). ty's incremental architecture is designed to make live updates extremely fast.</span></p></div>
<p>ty also includes a
<a href="https://docs.astral.sh/ty/features/diagnostics/">best-in-class diagnostic system</a>, inspired by the
Rust compiler's own world-class error messages. A single ty diagnostic can pull in context from
multiple files at once to explain not only what's wrong, but why (and, often, how to fix it).</p>
<div><div><p><span><span></span><img alt="ty diagnostic showing an invalid assignment error to a TypedDict key with reference to the item declaration" srcset="https://astral.sh/_next/image?url=%2Fstatic%2FPNG%2FTyDiagnostic3Light.png&amp;w=3840&amp;q=75 1x" src="https://astral.sh/_next/image?url=%2Fstatic%2FPNG%2FTyDiagnostic3Light.png&amp;w=3840&amp;q=75" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p><p><span><span></span><img alt="ty diagnostic showing an invalid assignment error to a TypedDict key with reference to the item declaration" srcset="https://astral.sh/_next/image?url=%2Fstatic%2FPNG%2FTyDiagnostic3Dark.png&amp;w=3840&amp;q=75 1x" src="https://astral.sh/_next/image?url=%2Fstatic%2FPNG%2FTyDiagnostic3Dark.png&amp;w=3840&amp;q=75" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div><p><span>When assigning an invalid value to a dictionary key,<!-- --> <a href="https://github.com/astral-sh/ty" target="_blank" rel="noreferrer">ty</a> <!-- -->surfaces both the type mismatch at the assignment site and the corresponding item declaration.</span></p></div>
<p>Diagnostic output is the primary user interface for a type checker; we prioritized our diagnostic
system from the start (with both humans and agents in mind) and view it as a first-class feature in
ty.</p>
<div><div><p><span><span></span><img alt="ty diagnostic showing an unresolved import error for tomllib module with reference to Python version configuration" srcset="https://astral.sh/_next/image?url=%2Fstatic%2FPNG%2FTyDiagnostic2Light.png&amp;w=3840&amp;q=75 1x" src="https://astral.sh/_next/image?url=%2Fstatic%2FPNG%2FTyDiagnostic2Light.png&amp;w=3840&amp;q=75" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p><p><span><span></span><img alt="ty diagnostic showing an unresolved import error for tomllib module with reference to Python version configuration" srcset="https://astral.sh/_next/image?url=%2Fstatic%2FPNG%2FTyDiagnostic2Dark.png&amp;w=3840&amp;q=75 1x" src="https://astral.sh/_next/image?url=%2Fstatic%2FPNG%2FTyDiagnostic2Dark.png&amp;w=3840&amp;q=75" decoding="async" data-nimg="intrinsic" loading="lazy" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div><p><span>When importing an unresolved module,<!-- --> <a href="https://github.com/astral-sh/ty" target="_blank" rel="noreferrer">ty</a> <!-- -->surfaces both the unresolved import at the import site and the corresponding Python version configuration.</span></p></div>
<p>If you use VS Code, Cursor, or a similar editor, we recommend installing the
<a href="https://marketplace.visualstudio.com/items?itemName=astral-sh.ty">ty VS Code extension</a>. The ty
language server supports <a href="https://docs.astral.sh/ty/features/language-server/">all the capabilities</a>
that you'd expect for a modern language server (Go to Definition, Symbol Rename, Auto-Complete,
Auto-Import, Semantic Syntax Highlighting, Inlay Hints, etc.), and runs in any editor that
implements the <a href="https://microsoft.github.io/language-server-protocol/">Language Server Protocol</a>.</p>
<p>Following the Beta release, our immediate priority is supporting early adopters. From there, we're
working towards a Stable release next year, with the gap between the
<a href="https://github.com/astral-sh/ty/milestone/2">Beta</a> and
<a href="https://github.com/astral-sh/ty/milestone/4">Stable</a> milestones largely focusing on: (1) stability
and bug fixes, (2) completing the long tail of features in the
<a href="https://github.com/astral-sh/ty/issues/1889">Python typing specification</a>, and (3) first-class
support for popular third-party libraries like <a href="https://pypi.org/project/pydantic/">Pydantic</a> and
<a href="https://pypi.org/project/Django/">Django</a>.</p>
<p>On a longer time horizon, though, ty will power semantic capabilities across the Astral toolchain:
dead code elimination, unused dependency detection, SemVer-compatible upgrade enforcement, CVE
reachability analysis, type-aware linting, and more (including some that are too ambitious to say
out loud just yet).</p>
<p>We want to make Python the most productive programming ecosystem on Earth. Just as with
<a href="https://github.com/astral-sh/ruff">Ruff</a> and <a href="https://github.com/astral-sh/uv">uv</a>, our commitment
from here is that ty will get significantly better every week by working closely with our users.
Thank you for building with us.</p>
<h3><span id="acknowledgements"></span>Acknowledgements<!-- --> <a href="#acknowledgements">#</a></h3>
<p>ty is the most sophisticated product we've built, and its design and implementation have surfaced
some of the hardest technical problems we've seen at Astral. Working on ty requires a deep
understanding of type theory, Python runtime semantics, and how the Python ecosystem actually uses
Python.</p>
<p>I'd like to thank all those that contributed directly to the development of ty, including:
<a href="https://github.com/dcreager">Douglas Creager</a>, <a href="https://github.com/AlexWaygood">Alex Waygood</a>,
<a href="https://github.com/sharkdp">David Peter</a>, <a href="https://github.com/MichaReiser">Micha Reiser</a>,
<a href="https://github.com/BurntSushi">Andrew Gallant</a>, <a href="https://github.com/Gankra">Aria Desires</a>,
<a href="https://github.com/carljm">Carl Meyer</a>, <a href="https://github.com/zanieb">Zanie Blue</a>,
<a href="https://github.com/ibraheemdev">Ibraheem Ahmed</a>,
<a href="https://github.com/dhruvmanila">Dhruv Manilawala</a>, <a href="https://github.com/oconnor663">Jack O'Connor</a>,
<a href="https://github.com/zsol">Zsolt Dollenstein</a>, <a href="https://github.com/mtshiba">Shunsuke Shibayama</a>,
<a href="https://github.com/MatthewMckee4">Matthew Mckee</a>, <a href="https://github.com/ntBre">Brent Westbrook</a>,
<a href="https://github.com/UnboundVariable">UnboundVariable</a>,
<a href="https://github.com/Glyphack">Shaygan Hooshyari</a>, <a href="https://github.com/thejchap">Justin Chapman</a>,
<a href="https://github.com/InSyncWithFoo">InSync</a>, <a href="https://github.com/Bhuminjay-Soni">Bhuminjay Soni</a>,
<a href="https://github.com/abhijeetbodas2001">Abhijeet Prasad Bodas</a>,
<a href="https://github.com/RasmusNygren">Rasmus Nygren</a>, <a href="https://github.com/lipefree">lipefree</a>,
<a href="https://github.com/ericmarkmartin">Eric Mark Martin</a>, <a href="https://github.com/TomerBin">Tomer Bin</a>,
<a href="https://github.com/lucach">Luca Chiodini</a>, <a href="https://github.com/brandtbucher">Brandt Bucher</a>,
<a href="https://github.com/dylwil3">Dylan Wilson</a>, <a href="https://github.com/tyralla">Eric Jolibois</a>,
<a href="https://github.com/felixscherz">Felix Scherz</a>, <a href="https://github.com/leandrobbraga">Leandro Braga</a>,
<a href="https://github.com/nickkuang">Renkai Ge</a>, <a href="https://github.com/brainwane">Sumana Harihareswara</a>,
<a href="https://github.com/TaKO8Ki">Takayuki Maeda</a>, <a href="https://github.com/maxmynter">Max Mynter</a>,
<a href="https://github.com/med1844">med1844</a>, <a href="https://github.com/woodruffw">William Woodruff</a>,
<a href="https://github.com/kiran-4444">Chandra Kiran G</a>, <a href="https://github.com/DetachHead">DetachHead</a>,
<a href="https://github.com/esadek">Emil Sadek</a>, <a href="https://github.com/j178">Jo</a>,
<a href="https://github.com/jorenham">Joren Hammudoglu</a>, <a href="https://github.com/mahmoud">Mahmoud Saada</a>,
<a href="https://github.com/mmlb">Manuel Mendez</a>, <a href="https://github.com/markzding">Mark Z. Ding</a>,
<a href="https://github.com/silamon">Simon Lamon</a>, <a href="https://github.com/suneettipirneni">Suneet Tipirneni</a>,
<a href="https://github.com/fgiacome">Francesco Giacometti</a>,
<a href="https://github.com/adamaaronson">Adam Aaronson</a>, <a href="https://github.com/alpaylan">Alperen Keleş</a>,
<a href="https://github.com/charliecloudberry">charliecloudberry</a>,
<a href="https://github.com/danparizher">Dan Parizher</a>, <a href="https://github.com/danielhollas">Daniel Hollas</a>,
<a href="https://github.com/dsherret">David Sherret</a>, <a href="https://github.com/mdqst">Dmitry</a>,
<a href="https://github.com/ercbot">Eric Botti</a>, <a href="https://github.com/eruditmorina">Erudit Morina</a>,
<a href="https://github.com/frgfm">François-Guillaume Fernandez</a>,
<a href="https://github.com/fabridamicelli">Fabrizio Damicelli</a>,
<a href="https://github.com/Guillaume-Fgt">Guillaume-Fgt</a>, <a href="https://github.com/hugovk">Hugo van Kemenade</a>,
<a href="https://github.com/JosiahKane">Josiah Kane</a>, <a href="https://github.com/LoicRiegel">Loïc Riegel</a>,
<a href="https://github.com/Mathemmagician">Ramil Aleskerov</a>, <a href="https://github.com/s-rigaud">Samuel Rigaud</a>,
<a href="https://github.com/soof-golan">Soof Golan</a>, <a href="https://github.com/Usul-Dev">Usul-Dev</a>,
<a href="https://github.com/decorator-factory">decorator-factory</a>, <a href="https://github.com/omahs">omahs</a>, and
<a href="https://github.com/fatelei">wangxiaolei</a>.</p>
<p>We'd also like to thank the <a href="https://github.com/salsa-rs/salsa">Salsa</a> team (especially
<a href="https://github.com/nikomatsakis">Niko Matsakis</a>, <a href="https://github.com/davidbarsky">David Barsky</a>,
and <a href="https://github.com/veykril">Lukas Wirth</a>) for their support and collaboration; the
<a href="https://github.com/elixir-lang/elixir">Elixir</a> team (especially
<a href="https://github.com/josevalim">José Valim</a>, <a href="https://www.irif.fr/~gc/">Giuseppe Castagna</a>, and
<a href="https://gldubc.github.io/">Guillaume Duboc</a>), whose work strongly influenced our approach to
gradual types and intersections; and a few members of the broader Python typing community:
<a href="https://github.com/erictraut">Eric Traut</a>, <a href="https://github.com/JelleZijlstra">Jelle Zijlstra</a>,
<a href="https://github.com/grievejia">Jia Chen</a>, <a href="https://github.com/samwgoldman">Sam Goldman</a>,
<a href="https://github.com/hauntsaninja">Shantanu Jain</a>, and <a href="https://github.com/stroxler">Steven Troxler</a>.</p>
<p>Finally, on a personal level, I'd like to highlight the core team
(<a href="https://github.com/AlexWaygood">Alex</a>, <a href="https://github.com/BurntSushi">Andrew</a>,
<a href="https://github.com/Gankra">Aria</a>, <a href="https://github.com/carljm">Carl</a>,
<a href="https://github.com/sharkdp">David</a>, <a href="https://github.com/dhruvmanila">Dhruv</a>,
<a href="https://github.com/dcreager">Doug</a>, <a href="https://github.com/ibraheemdev">Ibraheem</a>,
<a href="https://github.com/oconnor663">Jack</a>, and <a href="https://github.com/MichaReiser">Micha</a>), who created ty
from nothing and pushed it to be great from Day 1.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[No Graphics API (452 pts)]]></title>
            <link>https://www.sebastianaaltonen.com/blog/no-graphics-api</link>
            <guid>46293062</guid>
            <pubDate>Tue, 16 Dec 2025 19:20:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sebastianaaltonen.com/blog/no-graphics-api">https://www.sebastianaaltonen.com/blog/no-graphics-api</a>, See on <a href="https://news.ycombinator.com/item?id=46293062">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page" role="main">
        
          
            
<article id="sections" data-page-sections="6741ad819bc40a3797cb7348">
  
  
    
    


  
  





<div data-content-field="main-content" data-item-id="" data-test="page-section" data-section-theme="" data-section-id="6741ad819bc40a3797cb734a" data-controller="SectionWrapperController" data-current-styles="{
&quot;imageOverlayOpacity&quot;: 0.15,
&quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
&quot;sectionHeight&quot;: &quot;section-height--medium&quot;,
&quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
&quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
&quot;contentWidth&quot;: &quot;content-width--wide&quot;,
&quot;sectionAnimation&quot;: &quot;none&quot;,
&quot;backgroundMode&quot;: &quot;image&quot;
}" data-current-context="{
&quot;video&quot;: {
&quot;playbackSpeed&quot;: 0.5,
&quot;filter&quot;: 1,
&quot;filterStrength&quot;: 0,
&quot;zoom&quot;: 0,
&quot;videoSourceProvider&quot;: &quot;none&quot;
},
&quot;backgroundImageId&quot;: null,
&quot;backgroundMediaEffect&quot;: null,
&quot;divider&quot;: null,
&quot;typeName&quot;: &quot;blog-basic-grid&quot;
}" data-animation="none">
  <article id="article-">
  
    
    
    
    <div data-layout-label="Post Body" data-type="item" id="item-6741ad819bc40a3797cb7340"><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1735829877825_13469">
  <h2>Introduction</h2><p>My name is Sebastian Aaltonen. I have been writing graphics code for 30 years. I shipped my first 3d accelerated game in 1999. Since then I have been working with almost every gaming console generation (Nokia N-Gage, Nintendo DS/Switch, Sony Playstation/Portable, Microsoft Xbox) and every PC graphics API (DirectX, OpenGL, Vulkan). For the last 4 years I have been building a new renderer for HypeHype targeting WebGPU, Metal (Mac &amp; iOS) and Vulkan (Android). During my career I have been building several Ubisoft internal engines, optimizing Unreal Engine 4 and leading the Unity DOTS graphics team. I am a member of the Vulkan Advisory Panel and an Arm Ambassador.</p><p>This blog post includes lots of low level hardware details. When writing this post I used “GPT5 Thinking” AI model to cross reference public Linux open source drivers to confirm my knowledge and to ensure no NDA information is present in this blog post. Sources: AMD RDNA ISA documents and GPUOpen, Nvidia PTX ISA documents, Intel PRM, Linux open source GPU drivers (Mesa, Freedreno, Turnip, Asahi) and vendor optimization guides/presentations. The blog post has been screened by several industry insiders before the public release.</p><h2>Low-level graphics APIs change the industry</h2><p>Ten years ago, a significant shift occurred in real-time computer graphics with the introduction of new low-level PC graphics APIs. AMD had won both Xbox One (2013) and Playstation 4 (2013) contracts. Their new Graphics Core Next (GCN) architecture became the de-facto lead development platform for AAA games. PC graphics APIs at that time, DirectX 11 and OpenGL 4.5, had heavy driver overhead and were designed for single threaded rendering. AAA developers demanded higher performance APIs for PC. DICE joined with AMD to create a low level AMD GCN specific API for the PC called Mantle. As a response, Microsoft, Khronos and Apple started developing their own low-level APIs: DirectX 12, Vulkan and Metal were born.</p><p>The initial reception of these new low-level APIs was mixed. Synthetic benchmarks and demos showed substantial performance increases, but performance gains couldn’t be seen in major game engines such as Unreal and Unity. At Ubisoft, our teams noticed that porting existing DirectX 11 renderers to DirectX 12 often resulted in performance regression. Something wasn’t right.</p><p>Existing high-level APIs featured minimal persistent state, with fine-grained state setters and individual data inputs bound to the shader just prior to draw call submission. New low-level APIs aimed to make draw calls cheaper by ahead-of-time bundling shader pipeline state and bindings into persistent objects. GPU architectures were highly heterogeneous back in the day. Doing the data remapping, validation, and uploading ahead of time was a big gain. However, the rendering hardware interfaces (RHI) of existing game engines were designed for fine grained immediate mode rendering, while the new low-level APIs required bundling data in persistent objects.</p><p>To address this incompatibility, a new low-level graphics remapping layer grew beneath the RHI. This layer assumed the complexity previously handled by the OpenGL and DirectX 11 graphics drivers, tracking resources and managing mappings between the fine-grained dynamic user-land state and the persistent low-level GPU state. Graphics programmers started specializing into two distinct roles: low-level graphics programmers, who focused on the new low-level “driver” layer and the RHI, and high-level graphics programmers, who built visual graphics algorithms on top of the RHI. Visual programming was also getting more complex due to physically based lighting models, compute shaders and later ray-tracing.&nbsp;</p><h2>Modern APIs?</h2><p>DirectX 12, Vulkan, and Metal are often referred to as “modern APIs”. These APIs are now 10 years old. They were initially designed to support GPUs that are now 13 years old, an incredibly long time in GPU history. Older GPU architectures were optimized for traditional vertex and pixel shader tasks rather than the compute-intensive generic workloads prevalent today. They had vendor specific binding models and data paths. Hardware differences had to be wrapped under the same API. Ahead-of-time created persistent objects were crucial in offloading the mapping, uploading, validation and binding costs.</p><p>In contrast, the console APIs and Mantle were exclusively designed for AMD's GCN architecture, a forward-thinking design for its time. GCN boasted a comprehensive read/write cache hierarchy and scalar registers capable of storing texture and buffer descriptors, effectively treating everything as memory. No complex API for remapping the data was required, and significantly less ahead-of-time work was needed. The console APIs and Mantle had much less API complexity due to targeting a single modern GPU architecture.</p><p>A decade has passed, and GPUs have undergone a significant evolution. All modern GPU architectures now feature complete cache hierarchies with coherent last-level caches. CPUs can write directly to GPU memory using PCIe REBAR or UMA and 64-bit GPU pointers are directly supported in shaders. Texture samplers are bindless, eliminating the need for a CPU driver to configure the descriptor bindings. Texture descriptors can be directly stored in arrays within the GPU memory (often called descriptor heaps). If we were to design an API tailored for modern GPUs today, it wouldn’t need most of these persistent “retained mode” objects. The compromises that DirectX 12.0, Metal 1 and Vulkan 1.0 had to make are not needed anymore. We could simplify the API drastically.</p><p>The past decade has revealed the weaknesses of the modern APIs. The PSO permutation explosion is the biggest problem we need to solve. Vendors (Valve, Nvidia, etc) have massive cloud servers storing terabytes of PSOs for each different architecture/driver combination. User's local PSO cache size can exceed 100GB. No wonder the gamers are complaining that loading takes ages and stutter is all over the place.</p><h2>The history of GPUs and APIs</h2><p>Before we talk about stripping the API surface, we need to understand why graphics APIs were historically designed this way. OpenGL wasn't intentionally slow, nor was Vulkan intentionally complex. 10-20 years ago GPU hardware was highly diverse and undergoing rapid evolution. Designing a cross-platform API for such a diverse set of hardware required compromises.</p><p>Let’s start with a classic: The 3dFX Voodoo 2 12MB (1998) was a three chip design: A single rasterizer chip connected to a 4MB framebuffer memory and two texture sampling chips, each connected to their own 4MB texture memory. There was no geometry pipeline and no programmable shaders. CPU sent pre-transformed triangle vertices to the rasterizer. The rasterizer had a configurable blending equation to control how the vertex colors and the two texture sampler results were combined together. Texture samplers could not read each-other’s memory or the framebuffer. Thus there was no support for multiple render passes. Since the hardware was incapable of window composition, it had a loopback cable to connect your dedicated 2d video card. 3d rendering only worked in exclusive fullscreen mode. A 3d graphics card was a highly specialized piece of hardware, with little in common with the current GPUs and their massive programmable SIMD arrays. Hardware of this era had a massive impact on DirectX (1995) and OpenGL (1992) design. Backwards compatibility played a huge role. APIs improved iteratively. These 30 year old API designs still impact the way we write software today.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-aspect-ratio="57.3346116970278" data-block-type="5" id="block-yui_3_17_2_1_1732357558950_5657">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/7411bc9f-22bc-4ca8-b664-78c2edaa6684/voodoo2.jpg" data-image="https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/7411bc9f-22bc-4ca8-b664-78c2edaa6684/voodoo2.jpg" data-image-dimensions="1200x688" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/7411bc9f-22bc-4ca8-b664-78c2edaa6684/voodoo2.jpg" width="1200" height="688" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/7411bc9f-22bc-4ca8-b664-78c2edaa6684/voodoo2.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/7411bc9f-22bc-4ca8-b664-78c2edaa6684/voodoo2.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/7411bc9f-22bc-4ca8-b664-78c2edaa6684/voodoo2.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/7411bc9f-22bc-4ca8-b664-78c2edaa6684/voodoo2.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/7411bc9f-22bc-4ca8-b664-78c2edaa6684/voodoo2.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/7411bc9f-22bc-4ca8-b664-78c2edaa6684/voodoo2.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/7411bc9f-22bc-4ca8-b664-78c2edaa6684/voodoo2.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p data-rte-preserve-empty="true">3dFX Voodoo 2 12MB (1998): Individual processors and traces between them and their own memory chips (four 1MB chips for each processor) are clearly visible. Image © TechPowerUp.</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765887159238_91097">
  <p>Nvidia’s Geforce 256 coined the term GPU. It had a geometry processor in addition to the rasterizer. The geometry processor, rasterizer and texture sampling units were all integrated in the same die and shared memory. DirectX 7 introduced two new concepts: render target textures and uniform constants. Multipass rendering meant that texture samplers could read the rasterizer output, invalidating the 3dFX Voodoo 2 separate memory design.</p><p>The geometry processor API featured uniform data inputs for transform matrices (float4x4), light positions, and colors (float4). GPU implementations varied among manufacturers, many opting to embed a small constant memory block within the geometry engine. But this wasn’t the only way to do it. In the OpenGL API each shader had its own persistent uniforms. This design enabled the driver to embed constants directly in the shader's instruction stream, an API peculiarity that still persists in OpenGL 4.6 and ES 3.2 today.</p><p>GPUs back then didn’t have generic read &amp; write caches. Rasterizer had screen local cache for blending and depth buffering and texture samplers leaned on linearly interpolated vertex UVs for data prefetching. When shaders were introduced in DirectX 8 shader model 1.0 (SM 1.0), the pixel shader stage didn’t support calculating texture UVs. UVs were calculated at vertex granularity, interpolated by the hardware and passed directly to the texture samplers.&nbsp;</p><p>DirectX 9 brought a substantial increase in shader instruction limits, but shader model 2.0 didn’t expose any new data paths. Both vertex and pixel shaders still operated as 1:1 input:output machines, allowing users to only customize the transform math of the vertex position and attributes and the pixel color. Programmable load and store were not supported. The fixed-function input blocks persisted: vertex fetch, uniform (constant) memory and texture sampler. Vertex shader was a separate execution unit. It gained new features like the ability to index constants (limited to float4 arrays) but still lacked texture sampling support.</p><p>DirectX 9 shader model 3.0 increased the instruction limit to 65536 making it difficult for humans to write and maintain shader assembly anymore. Higher level shading languages were born: HLSL (2002) and GLSL (2002-2004). These languages adapted the 1:1 elementwise transform design. Each shader invocation operated on a single data element: vertex or pixel. Framework-style shader design heavily affected the graphics API design in the following years. It was a nice way to abstract hardware differences back in the day, but is showing scaling pains today.&nbsp;</p><p>DirectX 11 was a significant shift in the data model, introducing support for compute shaders, generic read-write buffers and indirect drawing. The GPU could now fully feed itself. The inclusion of generic buffers enabled shader programs to access and modify programmable memory locations, which forced hardware vendors to implement generic cache hierarchies. Shaders evolved beyond simple 1:1 data transformations, marking the end of specialized, hardcoded data paths. GPU hardware started to shift towards a generic SIMD design. SIMD units were now executing all the different shader types: vertex, pixel, geometry, hull, domain and compute. Today the framework has 16 different shader entry points. This adds a lot of API surface and makes composition difficult. As a result GLSL and HLSL still don’t have a flourishing library ecosystem.</p><p>DirectX 11 featured a whole zoo of buffer types, each designed to accommodate specific hardware data path peculiarities: typed SRV &amp; UAV, byte address SRV &amp; UAV, structured SRV &amp; UAV, append &amp; consume (with counter), constant, vertex, and index buffers. Like textures, buffers in DirectX utilize an opaque descriptor. Descriptors are hardware specific (commonly 128-256 bit) data blobs encoding the size, format, properties and data address of the resource in GPU memory. DirectX 11 GPUs leveraged their texture samplers for buffer load (gather) operations. This was natural since the sampler already had a type conversion hardware and a small read-only data cache. Typed buffers supported the same formats as textures, and DirectX used the same SRV (shader resource view) abstraction for both textures and buffers.</p><p>The use of opaque buffer descriptors meant that the buffer format was not known at shader compile time. This was fine for read-only buffers as they were handled by the texture sampler. Read-write buffer (UAV in DirectX) was initially limited to 32-bit and 128-bit (vec4) types. Subsequent API and hardware revisions gradually addressed typed UAV load limitations, but the core issues persisted: a descriptor requires an indirection (contains a pointer), compiler optimizations are limited (data type is known only at runtime), format conversion hardware introduces latency (vs raw L1$&nbsp;load), expand at load reserves registers for longer time (vs expand at use), descriptor management adds CPU driver complexity, and the API is complex (ten different buffer types).</p><p>In DirectX 11 the structured buffers were the only buffer type allowing an user defined struct type. All other buffer types represented a homogeneous array of simple scalar/vector elements. Unfortunately, structured buffers were not layout compatible with other buffer types. Users were not allowed to have structured buffer views to typed buffers, byte address buffers, or vertex/index buffers. The reason was that structured buffers had special AoSoA swizzle optimization under the hood, which was important for older vec4 architectures. This hardware specific optimization limited the structured buffer usability.</p><p>DirectX 12 made all buffers linear in memory, making them compatible with each other. SM 6.2 also added load&lt;T&gt; syntactic sugar for the byte address buffer, allowing clean struct loading syntax from arbitrary offset. All the old buffer types are still supported for backwards compatibility reasons and all the buffers still use opaque descriptors. HLSL is still missing support for 64-bit GPU pointers. In contrast, the Nvidia CUDA computing platform (2007) fully leaned on 64-bit pointers, but its popularity was initially limited to academic use. Today it is the leading AI platform and is heavily affecting the hardware design.</p><p>Support for 16-bit registers and 16-bit math was disorganized when DirectX 12 launched. Microsoft initially made a questionable decision to not backport DirectX 12 to Windows 7. Shader binaries targeting Windows 8 supported 16-bit types, but most gamers continued using Windows 7. Developers didn’t want to ship two sets of shaders. OpenGL lowp/mediump specification was also messy. Bit depths were not properly standardized. Mediump was a popular optimization in mobile games, but most PC drivers ignored it, making game developer’s life miserable. AAA games mostly ignored 16-bit math until PS4 Pro launched in 2016 with double rate fp16 support.</p><p>With the rise of AI, ray-tracing, and GPU-driven rendering, GPU vendors started focusing on optimizing their raw data load paths and providing larger and faster generic caches. Routing loads though the texture sampler (type conversion) added too much latency, as dependent load chains are common in modern shaders. Hardware got native support for narrow 8-bit, 16-bit, and 64-bit types and pointers.</p><p>Most vendors ditched their fixed function vertex fetch hardware, emitting standard raw load instructions in the vertex shader instead. Fully programmable vertex fetch allowed developers to write new algorithms such as clustered GPU-driven rendering. Fixed function hardware transistor budget could be used elsewhere.</p><p>Mesh shaders represent the culmination of rasterizer evolution, eliminating the need for index deduplication hardware and post-transform caches. In this paradigm, all inputs are treated as raw memory. The user is responsible for dividing the mesh into self-contained meshlets that internally share vertices. This process is often done offline. The GPU no longer needs to do parallel index deduplication for each draw call, saving power and transistors. Given that gaming accounts for only 10% of Nvidia's revenue today, while AI represents 90% and ray-tracing continues to grow, it is likely only a matter of time before the fixed function geometry hardware is stripped to bare minimum and drivers automatically convert vertex shaders to mesh shaders.</p><p>Mobile GPUs are tile-based renderers. Tilers bin the individual triangles to small tiles (commonly between 16x16 to 64x64 pixels) . Mesh shaders are too coarse grained for this purpose. Binning meshlets to tiny tiles would cause significant geometry overshading. There’s no clear convergence path. We still need to support the vertex shader path.</p><p>10 years ago when DirectX 12.0, Vulkan 1.0 and Metal 1.0 arrived, the existing GPU hardware didn’t widely support bindless resources. APIs adapted complex binding models to abstract the hardware differences. DirectX allowed indexing up to 128 resources per stage, Vulkan and Metal didn’t initially support descriptor indexing at all. Developers had to continue using traditional workarounds to reduce the bindings change overhead, such as packing textures into atlases and merging meshes together. The GPU hardware has evolved significantly during the past decade and converged to generic bindless SIMD design.</p><p>Let’s investigate how much simpler the graphics API and the shader language would become if we designed them solely for modern bindless hardware.</p><h2>Modern GPU memory management</h2><p>Let’s start our journey discussing memory management. Legacy graphics APIs abstracted the GPU memory management completely. Abstraction was necessary, as old GPUs had split memories and/or special data paths with various cache coherency concerns. When DirectX 12 and Vulkan arrived 10 years ago, the GPU hardware had matured enough to expose placement heaps to the user. Consoles had already exposed memory for a few generations and developers requested similar flexibility for PC and mobile. Apple introduced placement heaps 4 years after Vulkan and DirectX 12 in Metal 2.</p><p>Modern APIs require the user to enumerate the heap types to find out what kind of memory the GPU driver has to offer. It’s a good practice to preallocate memory in big chunks and suballocate it using a user-land allocator. However, there’s a design flaw in Vulkan: You have to create your texture/buffer object first. Then you can ask which heap types are compatible with the new resource. This forces the user into a lazy allocation pattern, which can cause performance hitches and memory spikes at runtime. This also makes it difficult to wrap a GPU memory allocation into a cross-platform library. AMD VMA, for example, creates both the Vulkan-specific buffer/texture object in addition to allocating memory. We want to fully separate these concerns.</p><p>Today the CPU has full visibility into the GPU memory. Integrated GPUs have UMA, and modern discrete GPUs have PCIe Resizable BAR. The whole GPU heap can be mapped. Vulkan heap API naturally supports CPU mapped GPU heaps. DirectX 12 got support in 2023 (HEAP_TYPE_GPU_UPLOAD).</p><p>CUDA has a simple design for GPU memory allocation: The GPU malloc API takes the size as input and returns a mapped CPU pointer. The GPU free API frees the memory. CUDA doesn’t support CPU mapped GPU memory. The GPU reads the CPU memory though the PCIe bus. CUDA also supports GPU memory allocations, but they can’t be directly written by the CPU.</p><p>We combine CUDA malloc design with CPU mapped GPU memory (UMA/ReBAR). It's the best of both worlds: The data is fast for the CPU to write and fast for the GPU to read, yet we maintain the clean, easy to use design.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765889087118_55741"><pre><code>// Allocate GPU memory for array of 1024 uint32
uint32* numbers = gpuMalloc(1024 * sizeof(uint32));

// Directly initialize (CPU mapped GPU pointer)
for (int i = 0; i &lt; 1024; i++) numbers[i] = random();

gpuFree(numbers);</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765889087118_55803">
  <p>Default gpuMalloc alignment is 16 bytes (vec4 alignment). If you need wider alignment use gpuMalloc(size, alignment) overload. My example code uses gpuMalloc&lt;T&gt; wrapper, doing gpuMalloc(elements * sizeof(T), alignof(T)). </p><p>Writing data directly into GPU memory is optimal for small data like draw arguments, uniforms and descriptors. For large persistent data, we still want to perform a copy operation. GPUs store textures in a swizzled layout similar to Morton-order to improve cache locality. DirectX 11.3 and 12 tried to standardize the swizzle layout, but couldn’t get all GPU manufacturers onboard. The common way to perform texture swizzling is to use a driver provided copy command. The copy command reads linear texture data from a CPU mapped “upload” heap and writes to a swizzled layout in a private GPU heap. Every modern GPU also has lossless delta color compression (DCC). Modern GPUs copy engines are capable of DCC compression and decompression. DCC and Morton swizzle are the main reasons we want to copy textures into a private GPU heap. Recently, GPUs have also added generic lossless memory compression for buffer data. If the memory heap is CPU mapped, the GPU can’t enable vendor specific lossless compression, as the CPU wouldn’t know how to read or write it. A copy command must be used to compress the data.</p><p>We need a memory type parameter in the GPU malloc function to add support for private GPU memory. The standard memory type should be CPU mapped GPU memory (write combined CPU access). It is fast for the GPU to read, and the CPU can directly write to it just like it was a CPU memory pointer. GPU-only memory is used for textures and big GPU-only buffers. The CPU can’t directly write to these GPU pointers. The user writes the data to CPU mapped GPU memory first and then issues a copy command, which transforms the data to optimal compressed format. Modern texture samplers and display engines can read compressed GPU data directly, so there’s no need for subsequent data layout transforms (see chapter: Modern barriers). The uploaded data is ready to use immediately.</p><p>We have two types of GPU pointers, a CPU mapped virtual address and a GPU virtual address. The GPU can only dereference GPU addresses. All pointers in GPU data structures must use GPU addresses. CPU mapped addresses are only used for CPU writes. CUDA has an API to transform a CPU mapped address to a GPU address (cudaHostGetDevicePointer). Metal 4 buffer object has two getters: .contents (CPU mapped address) and .gpuAddress (GPU address). Since the gpuMalloc API returns a pointer, not a managed object handle (like Metal), we choose the CUDA approach (gpuHostToDevicePointer). This API call is not free. The driver likely implements it using a hash map (if other than base addresses need to be translated, we need a tree). Preferably we call the address translation once per allocation and cache in a user land struct (void *cpu, void *gpu). This is the approach my userland GPUBumpAllocator uses (see appendix for full implementation).</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765889087118_119073"><pre><code>// Load a mesh using a 3rd party library
auto mesh = createMesh("mesh.obj");
auto upload = uploadBumpAllocator.allocate(mesh.byteSize); // Custom bump allocator (wraps a gpuMalloc ptr)
mesh.load(upload.cpu);

// Allocate GPU-only memory and copy into it
void* meshGpu = gpuMalloc(mesh.byteSize, MEMORY_GPU);
gpuMemCpy(commandBuffer, meshGpu, upload.gpu);</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765889087118_120697">
  <p>Vulkan recently got a new extension called VK_EXT_host_image_copy. The driver implements a direct CPU to GPU image copy operation, performing the hardware specific texture swizzle on CPU. This extension is currently only available on UMA architectures, but there’s no technical reason why it’s not available on PCIe ReBAR as well. Unfortunately this API doesn’t support DCC. It would be too expensive to perform DCC compression on the CPU. The extension is mainly useful for block compressed textures, as they don’t require DCC. It can’t universally replace hardware copy to GPU private memory.</p><p>There’s also a need for a third memory type, CPU-cached, for readback purposes. This memory type is slower for the GPU to write due to cache coherency with the CPU. Games only use readback seldomly. Common use cases are screenshots and virtual texturing readback. GPGPU algorithms such as AI training and inference lean on efficient communication between the CPU and the GPU.</p><p>When we mix the simplicity of CUDA malloc with CPU-mapped GPU memory we get a flexible and fast GPU memory allocation system with minimal API surface. This is an excellent starting point for a minimalistic modern graphics API.</p><h2>Modern data</h2><p>CUDA, Metal and OpenCL leverage C/C++ shader languages featuring 64-bit pointer semantics. These languages support loading and storing of structs from/to any appropriately aligned GPU memory location. The compiler handles behind-the-scenes optimizations, including wide loads (combine), register mappings, and bit extractions. Many modern GPUs offer free instruction modifiers for extracting 8/16-bit portions of a register, allowing the compiler to pack 8-bit and 16-bit values into a single register. This keeps the shader code clean and efficient.</p><p>If you load a struct of eight 32-bit values, the compiler will most likely emit two 128-bit wide loads (each filling 4 registers), a 4x reduction in load instruction count. Wide loads are significantly faster, especially if the struct contains narrow 8 and 16-bit fields. GPUs are ALU dense and have big register files, but compared to CPUs their memory paths are relatively slow. A CPU often has two load ports each doing a load per cycle. On a modern GPU we can achieve one SIMD load per 4 cycles. Wide load + unpack in the shader is often the most efficient way to handle data.&nbsp;</p><p>Compact 8-16 bit data has been traditionally stored in texel buffers (Buffer&lt;T&gt;) in DirectX games. Modern GPUs are optimized for compute workloads. Raw buffer load instructions nowadays have up to 2x higher throughput and up to 3x lower latency than texel buffers. Texel buffers are no longer the optimal choice on modern GPUs. Texel buffers do not support structured data, the user is forced to split their data into SoA layout in multiple texel buffers. Each texel buffer has its own descriptor, which must be loaded before the data can be accessed. This consumes resources (SGPRs, descriptor cache slots) and adds startup latency compared to using a single 64-bit raw pointer. SoA data layout also results in significantly more cache misses for non-linear index lookups (examples: material, texture, triangle, instance, bone id). Texel buffers offer free conversion of normalized ([0,1] and [-1,1]) types to floating point registers. It’s true that there’s no ALU cost, but you lose wide load support (combine loads) and the instruction goes through the slow texture sampler hardware path. Narrow texel buffer loads also add register bloat. RGBA8_UNORM load to vec4 allocates four vector registers immediately. The sampler hardware will eventually write the value to these registers. Compilers try to maximize the distance of load→use by moving load instructions in the beginning of the shader. This hides the load latency by ALU and allows overlapping multiple loads. If we instead use wide raw loads, our uint8x4 data consumes just a single 32-bit register. We unpack the 8-bit channels on use. The register life time is much shorter. Modern GPUs can directly access 16-bit low/high halves of registers without unpack, and some can even do 8-bit (AMD SDWA modifier). Packed double rate math makes 2x16 bit conversion instructions faster. Some GPU architectures (Nvidia, AMD) can also do 64-bit pointer raw loads directly from VRAM into groupshared memory, further reducing the register bloat needed for latency hiding. By using 64-bit pointers, game engines benefit from AI hardware optimizations.</p><p>Pointer based systems make memory alignment explicit. When you are allocating a buffer object in DirectX or Vulkan, you need to query the API for alignment. Buffer bind offsets must also be properly aligned. Vulkan has an API for querying the bind offset alignment and DirectX has fixed alignment rules. Alignment contract allows the low level shader compiler to emit optimal code  (such as aligned 4x32-byte wide loads). The DirectX ByteAddressBuffer abstraction has a design flaw: load2, load3 and load4 instructions only require 4-byte alignment. The new SM 6.2 load&lt;T&gt; also only requires elementwise alignment (half4 = 2, float4 = 4). Some GPU vendors (like Nvidia) have to split ByteAddressBuffer.load4 into four individual load instructions. The buffer abstraction can’t always shield the user from bad codegen. It makes bad codegen hard to fix. C/C++ based languages (CUDA, Metal) allow the user to explicitly declare struct alignment with the alignas attribute. We use alignas(16) in all our example code root structs.</p><p>By default, GPU writes are only visible to the threads inside the same thread group (= inside a compute unit). This allows non-coherent L1$ design. Visibility is commonly provided by barriers. If the user needs memory visibility between the groups in a single dispatch, they decorate the buffer binding with the [globallycoherent] attribute. The shader compiler emits coherent load/store instructions for accesses of that buffer. Since we use 64-bit pointers instead of buffer objects, we offer explicit coherent load/store instructions. The syntax is similar to atomic load/store. Similarly we can provide non-temporal load/store instructions that bypass the whole cache hierarchy.</p><p>Vulkan supports 64-bit pointers using the (2019) VK_KHR_buffer_device_address extension (<a href="https://docs.vulkan.org/samples/latest/samples/extensions/buffer_device_address/README.html"><span>https://docs.vulkan.org/samples/latest/samples/extensions/buffer_device_address/README.html</span></a>). Buffer device address extension is widely supported by all GPU vendors (including mobile), but is not a part of core Vulkan 1.4. The main issue with BDA is lack of pointer support in the GLSL and the HLSL shader languages. The user has to use raw 64-bit integers instead. A 64-bit integer can be cast to a struct. Structs are defined with custom BDA syntax. Array indexing requires declaring an extra BDA struct type with an array in it, if the user wants the compiler to generate the index addressing math. Debugging support is currently limited. Usability matters a lot and BDA will remain a niche until HLSL and GLSL support pointers natively. This is a stark contrast to CUDA, OpenCL and Metal, where native pointer support is a language core pillar and debugging works flawlessly.&nbsp;</p><p>DirectX 12 has no support for pointers in shaders. As a consequence, HLSL doesn’t allow passing arrays as function parameters. Simple things like having a material array inside UBO/SSBO requires hacking around with macros. It’s impossible to make reusable functions for reductions (prefix sum, sort, etc), since groupshared memory arrays can’t be passed between functions. You could of course declare a separate global array for each utility header/library, but the compiler will allocate groupshared memory for each of them separately, reducing occupancy. There’s no easy way to alias groupshared memory. GLSL has identical issues. Pointer based languages like CUDA and Metal MSL don’t have such issues with arrays. CUDA has a vast ecosystem of 3rd party libraries, and this ecosystem makes Nvidia the most valued company on the planet. Graphics shading languages need to evolve to meet modern standards. We need a library ecosystem too.</p><p>I will be using a C/C++ style shading language similar to CUDA and Metal MSL in my examples, with some HLSL-style system value (SV) semantics mixed in for the graphics specific bits and pieces.</p><h2>Root arguments</h2><p>Operating system threading APIs commonly provide a single 64-bit void pointer to the thread function. The operating system doesn’t care about the user’s data input layout. Let’s apply the same ideology to the GPU kernel data inputs. The shader kernel receives a single 64-bit pointer, which we cast to our desired struct (by the kernel function signature). Developers can use the same shared C/C++ header in both CPU and GPU side.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765889087118_282276"><pre><code>// Common header...
struct alignas(16) Data
{
    // Uniform data
    float16x4 color; // 16-bit float vector
    uint16x2 offset; // 16-bit integer vector
    const uint8* lut; // pointer to 8-bit data array

    // Pointers to in/out data arrays
    const uint32* input;
    uint32* output;
};

// CPU code...
gpuSetPipeline(commandBuffer, computePipeline);

auto data = myBumpAllocator.allocate&lt;Data&gt;(); // Custom bump allocator (wraps gpuMalloc ptr, see appendix)
data.cpu-&gt;color = {1.0f, 0.0f, 0.0f, 1.0f};
data.cpu-&gt;offset = {16, 0};
data.cpu-&gt;lut = luts.gpu + 64; // GPU pointers support pointer math (no need for offset API)
data.cpu-&gt;input = input.gpu;
data.cpu-&gt;output = output.gpu;

gpuDispatch(commandBuffer, data.gpu, uvec3(128, 1, 1));

// GPU kernel...
[groupsize = (64, 1, 1)]
void main(uint32x3 threadId : SV_ThreadID, const Data* data)
{
    uint32 value = data-&gt;input[threadId.x]; 
    // TODO: Code using color, offset, lut, etc...
    data-&gt;output[threadId.x] = value;
}</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765889087118_283855">
  <p>In the example code we use a simple linear bump allocator (myBumpAllocator) for allocating GPU arguments (see appendix for implementation). It returns a struct {void* cpu, void *gpu}. The CPU pointer is used for writing directly to persistently mapped GPU memory and the GPU pointer can be stored to GPU data structures or passed as dispatch command argument.&nbsp;</p><p>Most GPUs preload root uniforms (including 64-bit pointers) into constant or scalar registers just before launching a wave. This optimization remains viable: the draw/dispatch command carries the base data pointer. All the input uniforms (including pointers to other data) are found at small fixed offsets from the base pointer. Since shaders are pre-compiled and further optimized into device-specific microcode during the PSO creation, drivers have ample opportunity to set up register preloading and similar root data optimizations. Users should put the most important data in the beginning of the root struct as root data size is limited in some architectures. Our root struct has no hard size limit. The shader compiler will emit standard (scalar/uniform) memory loads for the remaining fields. The root data pointer provided to the shader is const. Shader can’t modify the root input data, as it might be still used by the command processor for preloading data to new waves. Output is done through non-const pointers (see Data::output in above example). By forcing the root data to be const, we also allow GPU drivers to perform their special uniform data path optimizations. </p><p>Do we need a special uniform buffer type? Modern shader compilers perform automatic uniformity analysis. If all inputs to an instruction are uniform, the output is also uniform. Uniformity propagates over the shader. All modern architectures have scalar registers/loads or a similar construct (SIMD1 on Intel). Uniformity analysis is used to convert vector loads into scalar loads, which saves registers and reduces latency. Uniformity analysis doesn’t care about the buffer type (UBO vs SSBO). The resource must be readonly (this is why you should always decorate SSBO with readonly attribute in GLSL or prefer SRV over UAV in DirectX 12). The compiler also needs to be able to prove that the pointer is not aliased. The C/C++ const keyword means that data can’t be modified though this pointer, it doesn’t guarantee that other read-write pointers might alias the same memory region. C99 added the restrict keyword for this purpose and CUDA kernels use it frequently. Root pointers in Metal are no-alias (restrict) by default, and so are buffer objects in Vulkan and DirectX 12. We should adopt the same convention to give the compiler more freedom to do optimizations. </p><p>The shader compiler is not always able to prove address uniformity at compile time. Modern GPUs opportunistically optimize dynamic uniform address loads. If the memory controller detects that all lanes of a vector load instruction have a uniform address, it emits a single lane load instead of a SIMD wide gather. The result is replicated to all lanes. This optimization is transparent, and doesn’t affect shader code generation or register allocation. Dynamically uniform data is a much smaller performance hit than it used to be in the past, especially when combined with the new fast raw load paths.</p><p>Some GPU vendors (ARM Mali and Qualcomm Adreno) take the uniformity analysis a step further. The shader compiler extracts uniform loads and uniform math. A scalar preamble runs before the shader. Uniform memory loads and math is executed once for the whole draw/dispatch and the results are stored in special hardware constant registers (the same registers used by root constants).</p><p>All of the above optimizations together provide a better way of handling uniform data than the classic 16KB/64KB uniform/constant buffer abstraction. Many GPUs still have special uniform registers for root constants, system values and the preamble (see above paragraph).</p><h2>Texture bindings</h2><p>Ideally, texture descriptors would behave like any other data in GPU memory, allowing them to be freely mixed in structs with other data. However, this level of flexibility isn't universally supported by all modern GPUs. Fortunately bindless texture sampler designs have converged over the last decade, with only two primary methods remaining: 256-bit raw descriptors and the indexed descriptor heap.</p><p>AMDs raw descriptor method loads 256-bit descriptors directly from GPU memory into the compute unit’s scalar registers. Eight subsequent 32-bit scalar registers contain a single descriptor. During the SIMD texture sample instruction, the shader core sends a 256-bit texture descriptor and per-lane UVs to the sampler unit. This provides the sampler all the data it needs to address and load texels without any indirections. The drawback is that the 256-bit descriptor takes a lot of register space and needs to be resent to the sampler for each sample instruction.</p><p>The indexed descriptor heap approach uses 32-bit indices (20 bits for old Intel iGPUs). 32-bit indices are trivial to store in structs, load into standard SIMD registers and efficient to pass around. During a SIMD sample instruction, the shader core sends the texture index and the per-lane UVs to the sampler unit. The sampler fetches the descriptor from the descriptor heap: heap base address + texture index * stride (256-bits in modern GPUs). The texture heap base address is either abstracted by the driver (Vulkan and Metal) or provided by the user (SetDescriptorHeaps in DirectX 12). Changing the texture heap base address may result in an internal pipeline barrier (on older hardware). On modern GPUs the texture heap 64-bit base address is often part of each sample instruction data, allowing sampling from multiple heaps seamlessly (64-bit base + 32-bit offset per lane). The sampler unit has a tiny internal descriptor cache to avoid indirect reads after the first access. Descriptor caches must be invalidated whenever the descriptor heap is modified.</p><p>A few years ago it looked like AMDs scalar register based texture descriptors were the winning formula in the long run. Scalar registers are more flexible than a descriptor heap, allowing descriptors to be embedded inside GPU data structures directly. But there’s a downside. Modern GPU workloads such as ray-tracing and deferred texturing (Nanite) lean on non-uniform texture indices. The texture heap index is not uniform over a SIMD wave. A 32-bit heap index is just 4 bytes, we can send it per lane. In contrast, a 256-bit descriptor is 32 bytes. It is not feasible to fetch and send a full 256-bit descriptor per lane. Modern Nvidia, Apple and Qualcomm GPUs support per-lane descriptor index mode in their sample instructions, making the non-uniform case more efficient. The sampler unit performs an internal loop if required. Inputs/outputs to/from sampler units are sent once, regardless of the heap index coherence. AMDs scalar register based descriptor architecture requires the shader compiler to generate a scalarization loop around the texture sample instruction. This costs extra ALU cycles and requires sending and receiving (partially masked) sampler data multiple times. It’s one of the reasons why Nvidia is faster in ray-tracing than AMD. ARM and Intel use 32-bit heap indices too (like Nvidia, Qualcomm and Apple), but their latest architectures don’t yet have a per-lane heap index mode. They emit a similar scalarization loop as AMD for the non-uniform index case.</p><p>All of these differences can be wrapped under an unified texture descriptor heap abstraction. The de-facto texture descriptor size is 256 bits (192 bits on Apple for a separate texture descriptor, sampler is the remaining 32 bits). The texture heap can be presented as a homogeneous array of 256-bit descriptor blobs. Indexing is trivial. DirectX 12 shader model 6.6 provides a texture heap abstraction like this, but doesn’t allow direct CPU or compute shader write access to the descriptor heap memory. A set of APIs are used for creating descriptors and copying descriptors from the CPU to the GPU. The GPU is not allowed to write the descriptors. Today, we can remove this API abstraction completely by allowing direct CPU and GPU write to the descriptor heap. All we need is a simple (user-land) driver helper function for creating a 256-bit (uint64[4]) hardware specific descriptor blob. Modern GPUs have UMA or PCIe ReBAR. The CPU can directly write descriptor blobs into GPU memory. Users can also use compute shaders to copy or generate descriptors. The shader language has a descriptor creation intrinsic too. It returns a hardware specific uint64x4 descriptor blob (analogous to the CPU API). This approach cuts the API complexity drastically and is both faster and more flexible than the DirectX 12 descriptor update model. Vulkan’s VK_EXT_descriptor_buffer (<a href="https://www.khronos.org/blog/vk-ext-descriptor-buffer"><span>https://www.khronos.org/blog/vk-ext-descriptor-buffer</span></a>) extension (2022) is similar to my proposal, allowing direct CPU and GPU write. It is supported by most vendors, but unfortunately is not part of the Vulkan 1.4 core spec.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765889087118_380784"><pre><code>// App startup: Allocate a texture descriptor heap (for example 65536 descriptors)
GpuTextureDescriptor *textureHeap = gpuMalloc&lt;GpuTextureDescriptor&gt;(65536);

// Load an image using a 3rd party library
auto pngImage = pngLoad("cat.png");
auto uploadMemory = uploadBumpAllocator.allocate(pngImage.byteSize); // Custom bump allocator (wraps gpuMalloc ptr)
pngImage.load(uploadMemory.cpu);

// Allocate GPU memory for our texture (optimal layout with metadata)
GpuTextureDesc textureDesc { .dimensions = pngImage.dimensions, .format = FORMAT_RGBA8_UNORM, .usage = SAMPLED };
GpuTextureSizeAlign textureSizeAlign = gpuTextureSizeAlign(textureDesc);
void *texturePtr = gpuMalloc(textureSizeAlign.size, textureSizeAlign.align, MEMORY_GPU);
GpuTexture texture = gpuCreateTexture(textureDesc, texturePtr);

// Create a 256-bit texture view descriptor and store it
textureHeap[0] = gpuTextureViewDescriptor(texture, { .format = FORMAT_RGBA8_UNORM });

// Batched upload: begin
GpuCommandBuffer uploadCommandBuffer = gpuStartCommandRecording(queue);

// Copy all textures here!
gpuCopyToTexture(uploadCommandBuffer, texturePtr, uploadMemory.gpu, texture);
// TODO other textures...

// Batched upload: end
gpuBarrier(uploadCommandBuffer, STAGE_TRANSFER, STAGE_ALL, HAZARD_DESCRIPTORS);
gpuSubmit(queue, { uploadCommandBuffer });

// Later during rendering...
gpuSetActiveTextureHeapPtr(commandBuffer, gpuHostToDevicePointer(textureHeap));</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765889087118_380844">
  <p>It is almost possible to get rid of the CPU side texture object (GpuTexture) completely. Unfortunately the triangle rasterizer units of all modern GPUs are not yet bindless. The CPU driver needs to prepare command packets to bind render targets, depth-stencil buffers, clear and resolve. These APIs don’t use the 256-bit GPU texture descriptor. We need driver specific extra CPU data (stored in the GpuTexture object).</p><p>The simplest way to reference a texture in a shader is to use a 32-bit index. A single index can also represent the starting offset of a range of descriptors. This offers a straightforward way to implement the DirectX 12 descriptor table abstraction and the Vulkan descriptor set abstraction without an API. We also get an elegant solution to the fast material switch use case: All we need is a single 64-bit GPU pointer, pointing to a material data struct (containing material properties + 32-bit texture heap start index). Vulkan vkCmdBindDescriptorSets and DirectX 12 SetGraphicsRootDescriptorTable are relatively fast API calls, but they are nowhere as fast as writing a single 64-bit pointer to persistently mapped GPU memory. A lot of complexity is removed by not needing to create, update and delete resource binding API objects. CPU time is also saved as the user no longer needs to maintain a hash map of descriptor sets, a common approach to solve the immediate vs retained mode discrepancy in game engines.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765889087118_412508"><pre><code>// Common header...
struct alignas(16) Data
{
    uint32 srcTextureBase;
    uint32 dstTexture;
    float32x2 invDimensions;
};

// GPU kernel...
const Texture textureHeap[];

[groupsize = (8, 8, 1)]
void main(uint32x3 threadId : SV_ThreadID, const Data* data)
{
    Texture textureColor = textureHeap[data-&gt;srcTextureBase + 0];
    Texture textureNormal = textureHeap[data-&gt;srcTextureBase + 1];
    Texture texturePBR = textureHeap[data-&gt;srcTextureBase + 2];

    Sampler sampler = {.minFilter = LINEAR, .magFilter = LINEAR}; // Embedded sampler (Metal-style)

    float32x2 uv = float32x2(threadId.xy) * data-&gt;invDimensions;

    float32x4 color = sample(textureColor, sampler, uv);
    float32x4 normal = sample(textureNormal, sampler, uv);
    float32x4 pbr = sample(texturePBR, sampler, uv);

    float32x4 lit = calculateLighting(color, normal, pbr);

    TextureRW dstTexture = TextureRW(textureHeap[data-&gt;dstTexture]);
    dstTexture[threadId.xy] = lit;
}</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765889087118_412569">
  <p>Metal 4 manages the texture descriptor heap automatically. Texture objects have .gpuResourceID, which is a 64-bit heap index (Xcode GPU debugger reveals small values such as 0x3). You can directly write texture IDs into GPU structs, as you would use texture indices in DirectX SM 6.6 and Vulkan (descriptor buffer extension). As the heap management in Metal is automatic, users can’t allocate texture descriptors in contiguous ranges. It’s a common practice to store a 32-bit index to the first texture in the range and calculate the indices for other textures in the set (see above code example). Metal doesn’t support this. The user has to write a 64-bit texture handle for each texture separately. To address a set of 5 textures, you need 40 bytes in Metal (5 * 64-bit). Vulkan and DirectX 12 only need 4 bytes (1 * 32-bit). Apple GPU hardware is able to implement SM 6.6 texture heaps. The limitation is the Metal API (software).</p><p>Texel buffers can be still supported for backwards compatibility. DirectX 12 stores texel buffer descriptors in the same heap with texture descriptors. A texel buffer functions similarly to a 1d texture (unfiltered tfetch path). Since texel buffers would be mainly used for backwards compatibility, driver vendors wouldn’t need to jump over the hoops to replace them with faster code paths such as raw memory loads behind the scenes. I am not a big fan of driver background threads and shader replacements.</p><p>Non-uniform texture index needs to use NonUniformResourceIndex notation similar to GLSL and HLSL. This tells the low level GPU shader compiler to emit a special texture instruction with per-lane heap index, or a scalarization loop for GPUs that only support uniform descriptors. Since buffers are not descriptors, we never need NonUniformResourceIndex for buffers. We simply pass a 64-bit pointer per lane. It works on all modern GPUs. No scalarization loop, no mess. Additionally, the language should natively support ptr[index] notation for memory loads, where the index is 32-bits. Some GPUs support raw memory load instructions with 32-bit per lane offset. It reduces the register pressure. Feedback to GPU vendors: Please add the missing 64-bit shared base + 32-bit per lane offset raw load instruction and 16-bit uv(w) texture load instructions, if your architecture is still missing them.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765889087118_449566"><pre><code>const Texture textureHeap[];

[groupsize = (8, 8, 1)]
void main(uint32x3 threadId : SV_ThreadID, const Data* data)
{
    // Non-uniform "buffer data" is not an issue with pointer semantics! 
    Material* material = data-&gt;materialMap[threadId.xy];

    // Non-uniform texture heap index
    uint32 textureBase = NonUniformResourceIndex(material.textureBase);

    Texture textureColor = textureHeap[textureBase + 0];
    Texture textureNormal = textureHeap[textureBase + 1];
    Texture texturePBR = textureHeap[textureBase + 2];

    Sampler sampler = {.minFilter = LINEAR, .magFilter = LINEAR};

    float32x2 uv = float32x2(threadId.xy) * data-&gt;invDimensions;

    float32x4 color = sample(textureColor, sampler, uv);
    float32x4 normal = sample(textureNormal, sampler, uv);
    float32x4 pbr = sample(texturePBR, sampler, uv);
    
    color *= material.color;
    pbr *= material.pbr;

    // Rest of the shader
}</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765889087118_449626">
  <p>Modern bindless texturing lets us remove all texture binding APIs. A global indexable texture heap makes all textures visible to all shaders. Texture data still needs to be loaded into GPU memory by copy commands (to enable DCC and Morton swizzle). Texture descriptor creation still needs a thin GPU specific user land API. The texture heap can be exposed directly to both the CPU and the GPU as a raw GPU memory array, removing most of the texture heap API complexity compared to DirectX 12 SM 6.6.</p><h2>Shader pipelines</h2><p>Since our shader root data is just a single 64-bit pointer and our textures are just 32-bit indices, the shader pipeline creation becomes dead simple. There’s no need to define texture bindings, buffer bindings, bind groups (descriptor sets, argument buffers) or the root signature.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765889087118_464986"><pre><code>auto shaderIR = loadFile("computeShader.ir");
GpuPipeline computePipeline = gpuCreateComputePipeline(shaderIR);</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765889087118_465047">
  <p>DirectX 12 and Vulkan utilize complex APIs to bind and set up root signatures, push descriptors, push constants, and descriptor sets. A modern GPU driver essentially constructs a single struct into GPU memory and passes its pointer to the command processor. We have shown that such API complexity is unnecessary. The user simply writes the root struct into persistently mapped GPU memory and passes a 64-bit GPU pointer directly to the draw/dispatch function. Users can also include 64-bit pointers and 32-bit texture heap indices inside their structs to build any indirect data layout that fits their needs. Root bindings APIs and the whole DX12 buffer zoo can be replaced efficiently with 64-bit pointers.​​ This simplifies the shader pipeline creation drastically. We don’t need to define the data layout at all. We successfully removed a massive chunk of API complexity while providing more flexibility to the user.</p><h2>Static constants</h2><p>Vulkan, Metal and WebGPU have a concept of static (specialization) constants, locked in at shader pipeline creation. The driver's internal shader compiler applies these constants as literals in the input shader IR and does constant propagation and dead code elimination pass afterward. This can be used to create multiple permutations of the same shader at pipeline creation, reducing the time and storage required for offline compiling all the shader permutations.</p><p>Vulkan and Metal have a set of APIs and a special shader syntax for describing the shader specialization constants and their values. It would be nicer to simply provide a C struct that matches the constant struct defined in the shader side. That would require minimal API surface and would bring important improvements.</p><p>Vulkan’s specialization constants have a design flaw. Specialization constants can’t modify the descriptor set layouts. Data inputs and outputs are fixed. The user could hack around the limitation by implementing an uber-layout containing all potential inputs/outputs and skip updating unused descriptors, but this is cumbersome and sub-optimal. Our proposed design doesn’t have the same problem. One can simply branch by a constant (the other side is dead code eliminated) and reinterpret the shader data input pointer as a different struct. One could also mimic the C++ inheritance data layout. Use a common layout for the beginning of the input struct and put specialized data at the end. Static polymorphism can be achieved cleanly. Runtime performance is identical to hand optimized shader. The specialization struct can also include GPU pointers, allowing the user to hardcode runtime memory locations, avoiding indirections. This has never been possible in a shader language before. Instead, the GPU vendors had to use background threads to analyze the shaders to do similar shader replacement optimizations at runtime, increasing the CPU cost and the driver complexity significantly.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765889087118_493029"><pre><code>// Common header...
struct alignas(16) Constants
{
    int32 qualityLevel;
    uint8* blueNoiseLUT;
};

// CPU code...
Constants constants { .qualityLevel = 2, blueNoiseLUT = blueNoiseLUT.gpu };

auto shaderIR = loadFile("computeShader.ir");
GpuPipeline computePipeline = gpuCreateComputePipeline(shaderIR, &amp;constants);

// GPU kernel...
[groupsize = (8, 8, 1)]
void main(uint32x3 threadId : SV_ThreadID, const Data* data, const Constants constants)
{
    if (constants.qualityLevel == 3)
    {
        // Dead code eliminated
    }
}</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765889087118_493089">
  <p>The shader permutation hell is one of the biggest issues in modern graphics today. Gamers are complaining about stutter, devs are complaining about offline shader compilation taking hours. This new design gives the user added flexibility. They can toggle between static and dynamic behavior inside the shader, making it easy to have a generic fallback and specialization on demand. This design reduces the number of shader permutations and the runtime stalls caused by pipeline creation.</p><h2>Barriers and fences</h2><p>The most hated feature in modern graphics APIs must be the barriers. Barriers serve two purposes: enforce producer-to-consumer execution dependencies and transition textures between layouts.</p><p>Many graphics programmers have an incorrect mental model about the GPU synchronization. A common belief is that GPU synchronization is based on fine-grained texture and buffer dependencies. In reality, modern GPU hardware doesn’t really care about individual resources. We spend lots of CPU cycles in userland preparing a list of individual resources and how their layouts change, but modern GPU drivers practically throw that list away. The abstraction doesn’t match reality.</p><p>Modern bindless architecture gives the GPU a lot of freedom. A shader can write to any 64-bit pointer or any texture in the global descriptor heap. The CPU doesn't know what decisions the GPU is going to make. How is it supposed to emit transition barriers for each affected resource? This is a clear mismatch between bindless architecture and classic CPU-driven rendering APIs today. Let’s investigate why the APIs were designed like this 10 years ago.&nbsp;</p><p>AMD GCN had a big influence on modern graphics API design. GCN was ahead of its time with async compute and bindless texturing (using scalar registers to store descriptors), but it also had crucial limitations in its delta color compression (DCC) and cache design. These limitations are a great example why the barrier model we have today is so complex. GCN didn’t have a coherent last-level cache. ROPs (raster operations = pixel shader outputs) had special non-coherent caches directly connected to the VRAM. The driver had to first flush the ROP caches to memory and then invalidate the L2$ to make pixel shader writes visible to shaders and samplers. The command processor also wasn’t a client of the L2$. Indirect arguments written in compute shaders weren’t visible to the command processor without invalidating the whole L2$ and flushing all dirty lines into VRAM. GCN 3 introduced delta color compression (DCC) for ROPs, but AMD’s texture samplers were not able to directly read DCC compressed textures or compressed depth buffers. The driver had to perform an internal decompress compute shader to eliminate the compression. The display engine could not read DCC compressed textures either. The common case of sampling a render target required two internal barriers and flushing all caches (wait for ROPs, flush ROP cache and L2$, run decompress compute shader, wait for compute).</p><p>AMD’s new RDNA architecture has several crucial improvements: It has a coherent L2$ covering all memory operations. ROPs and the command processor are clients of the L2$. The only non-coherent caches are the tiny L0$ and K$ (scalar cache) inside the compute units. A barrier now requires only flushing the outstanding writes in the tiny caches into the higher level cache. The driver no longer has to flush the last-level (L2) cache into the VRAM, making barriers significantly faster. RDNA’s improved display engine is capable of reading DCC compressed textures and a (de)compressor sits between the L2$ and the L0$ texture cache. There’s no need to decompress textures into VRAM before sampling, removing the need for texture layout transitions (compressed / uncompressed). All desktop and mobile GPU vendors have reached similar conclusions: Bandwidth is the bottleneck today. We should never waste bandwidth decoding resources into VRAM. Layout transitions are no longer needed.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1765891714485_30775">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/985cf28c-1e47-4d47-b246-47da92a2ceb7/Screenshot+2025-12-16+at+15.38.28.png" data-image="https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/985cf28c-1e47-4d47-b246-47da92a2ceb7/Screenshot+2025-12-16+at+15.38.28.png" data-image-dimensions="3448x1936" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/985cf28c-1e47-4d47-b246-47da92a2ceb7/Screenshot+2025-12-16+at+15.38.28.png" width="3448" height="1936" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/985cf28c-1e47-4d47-b246-47da92a2ceb7/Screenshot+2025-12-16+at+15.38.28.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/985cf28c-1e47-4d47-b246-47da92a2ceb7/Screenshot+2025-12-16+at+15.38.28.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/985cf28c-1e47-4d47-b246-47da92a2ceb7/Screenshot+2025-12-16+at+15.38.28.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/985cf28c-1e47-4d47-b246-47da92a2ceb7/Screenshot+2025-12-16+at+15.38.28.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/985cf28c-1e47-4d47-b246-47da92a2ceb7/Screenshot+2025-12-16+at+15.38.28.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/985cf28c-1e47-4d47-b246-47da92a2ceb7/Screenshot+2025-12-16+at+15.38.28.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/985cf28c-1e47-4d47-b246-47da92a2ceb7/Screenshot+2025-12-16+at+15.38.28.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p data-rte-preserve-empty="true">AMD RDNA (2019): Improved cache hierarchy, DCC and display engine in the RDNA architecture. L2$ contains DCC compressed data. (De)compressor sits between L2$ and lower levels. L0$ (texture) is decompressed. Image © AMD.</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765891714485_31092">
  <p>Resource lists are the most annoying aspect of barriers in DirectX 12 and Vulkan. Users are expected to track the state of each resource individually, and tell the graphics API their previous and next state for each barrier. This was necessary on 10 year old GPUs as vendors hid various decompress commands under the barrier API. The barrier command functioned as the decompress command, so it had to know which resources required decompression. Today’s hardware doesn’t need texture layouts or decompress steps. Vulkan just got a new VK_KHR_unified_image_layouts (<a href="https://www.khronos.org/blog/so-long-image-layouts-simplifying-vulkan-synchronisation"><span>https://www.khronos.org/blog/so-long-image-layouts-simplifying-vulkan-synchronisation</span></a>) extension (2025), removing the image layout transitions from the barrier command. But it still requires the user to list individual textures and buffers. Why is this?</p><p>The main reason is legacy API and tooling compatibility. People are used to thinking about resource dependencies and the existing Vulkan and DirectX 12 validation layers are designed that way. However, the barrier command executed by the GPU contains no information about textures or buffers at all. The resource list is consumed solely by the driver.</p><p>Our modern driver loops through your resource list and populates a set of flags. Drivers no longer need to worry about resource layouts or last level cache coherency, but there still exists tiny non-coherent caches that need flushing in special cases. Modern GPUs flush the majority of the non-coherent caches automatically in every barrier. For example the AMD L0$ and K$ (scalar cache) are always flushed, since every pass writes some outputs and these outputs live in some of these caches. Fine grained tracking of all write addresses would be too expensive. Tiny non-coherent caches tend to be inclusive. Modified lines get flushed to the next cache level. This is fast and doesn’t produce VRAM traffic. Some architectures have special caches that are not automatically flushed. Examples: descriptor caches in the texture samplers (see above chapter), rasterizer ROP caches and HiZ caches. The command processor commonly runs ahead to reduce the wave spawn latency. If we write indirect arguments in a shader, we need to inform the GPU to stall the command processor prefetcher to avoid a race. The GPU doesn’t actually know whether your compute shader was writing into an indirect argument buffer or not. In DirectX 12 the buffer is transitioned to D3D12_RESOURCE_STATE_INDIRECT_ARGUMENT and in Vulkan the consumer dependency has a special stage VK_PIPELINE_STAGE_DRAW_INDIRECT_BIT. When a barrier has a resource transition like this or a stage dependency like this, the driver will include command processor prefetcher stall flag into the barrier.</p><p>A modern barrier design replaces the resource list with a single bitfield describing what happens to these special non-coherent caches. Special cases include: Invalidate texture descriptors, invalidate draw arguments and invalidate depth caches. These flags are needed when we generate draw arguments, write to the descriptor heap or write to a depth buffer with a compute shader. Most barriers don’t need special cache invalidation flags.</p><p>Some GPUs still need to decompress data in special cases. For example during a copy or a clear command (fast clear eliminate if clear color has changed). Copy and clear commands take the affected resource as a parameter. The driver can take necessary steps to decode the data if needed. We don’t need a resource list in our barrier for these special cases. Not all formats and usage flags support compression. The driver will keep the data uncompressed in these cases, instead of transitioning it back and forth, wasting bandwidth.&nbsp;</p><p>A standard UAV barrier (compute → compute) is trivial.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765889087118_569070"><pre><code>gpuBarrier(commandBuffer, STAGE_COMPUTE, STAGE_COMPUTE);</code></pre>

</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765889087118_569131">

<p>If you write to the texture descriptor heap (uncommon), you need to add a special flag.</p>




















  
  



</div><div data-block-type="44" id="block-yui_3_17_2_1_1765889087118_588768"><pre><code>gpuBarrier(commandBuffer, STAGE_COMPUTE, STAGE_COMPUTE, HAZARD_DESCRIPTORS);</code></pre>

</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765889087118_588829">

<p>A barrier between rasterizer output and pixel shader is a common case for offscreen render target → sampling. Our example has dependency stages set up in a way that the barrier doesn’t block vertex shaders, allowing vertex shading (and tile binning on mobile GPUs) to overlap with previous passes. A barrier with raster output stage (or later) as the producer automatically flushes non-coherent ROP caches if the GPU architecture needs that. We don’t need an explicit flag for it.</p>




















  
  



</div><div data-block-type="44" id="block-yui_3_17_2_1_1765889087118_608158"><pre><code>gpuBarrier(commandBuffer, STAGE_RASTER_COLOR_OUT | STAGE_RASTER_DEPTH_OUT, STAGE_PIXEL_SHADER);</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765889087118_608218">
  <p>Users only describe the queue execution dependencies: producer and consumer stage masks. There’s no need to track the individual texture and buffer resource states, removing a lot of complexity and saving a significant amount of CPU time versus the current DirectX 12 and Vulkan designs. Metal 2 has a modern barrier design already: it doesn’t use resource lists.</p><p>Many GPUs have custom scratchpads memories: Groupshared memory inside each compute unit, tile memory, large shared scratchpads like the Qualcomm GMEM. These memories are managed automatically by the driver. Temporary scratchpads like groupshared memory are never stored to memory. Tile memories are stored automatically by the tile rasterizer (store op == store). Uniform registers are read-only and pre-populated before each draw call. Scratchpads and uniform registers don’t have cache coherency protocols and don’t interact with the barriers directly.</p><p>Modern GPUs support a synchronization command that writes a value to memory when a shader stage is finished, and a command that waits for a value to appear in memory location before a shader stage is allowed to begin (wait includes optional cache flush semantics). This is equivalent to splitting the barrier into two: the producer and the consumer. DirectX 12 split barriers and Vulkan event→wait are examples of this design. Splitting the barrier into consumer→producer allows putting independent work between them, avoiding draining the GPU.</p><p>Vulkan event→wait (and DX12 split barriers) see barely any use. The main reason is that normal barriers are already highly complicated, and developers want to avoid extra complexity. Driver support for split barriers also hasn’t been perfect in the past. Removing the resource lists simplifies the split barriers significantly. We can also make split barriers semantically similar to timeline semaphores: Signal command writes to a monotonically increasing 64-bit value (atomic max) and wait command waits for the value to be &gt;= N (greater equal). The counter is just a GPU memory pointer, no persistent API object is required. This provides us with a significantly simpler event→wait API.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765889087118_644705"><pre><code>gpuSignalAfter(commandBuffer, STAGE_RASTER_COLOR_OUT, gpuPtr, counter, SIGNAL_ATOMIC_MAX);
// Put independent work here
gpuWaitBefore(commandBuffer, STAGE_PIXEL_SHADER, gpuPtr, counter++, OP_GREATER_EQUAL);</code></pre>

</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765889087118_644766">

<p>This API is much simpler than the existing VkEvent API, yet offers improved flexibility. In the above example we implemented the timeline semaphore semantics, but we can implement other patterns too, such as waiting multiple producers using a bitmask: mark bits with SIGNAL_ATOMIC_OR and wait for all bits in a mask to be set (mask is an optional parameter in the gpuWaitBefore command).</p>




















  
  



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1765891714485_29270">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/f044fea3-4dfc-4d7a-9400-2eb58f7ca5a3/Screenshot+2025-12-16+at+15.37.11.png" data-image="https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/f044fea3-4dfc-4d7a-9400-2eb58f7ca5a3/Screenshot+2025-12-16+at+15.37.11.png" data-image-dimensions="3260x1730" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/f044fea3-4dfc-4d7a-9400-2eb58f7ca5a3/Screenshot+2025-12-16+at+15.37.11.png" width="3260" height="1730" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/f044fea3-4dfc-4d7a-9400-2eb58f7ca5a3/Screenshot+2025-12-16+at+15.37.11.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/f044fea3-4dfc-4d7a-9400-2eb58f7ca5a3/Screenshot+2025-12-16+at+15.37.11.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/f044fea3-4dfc-4d7a-9400-2eb58f7ca5a3/Screenshot+2025-12-16+at+15.37.11.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/f044fea3-4dfc-4d7a-9400-2eb58f7ca5a3/Screenshot+2025-12-16+at+15.37.11.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/f044fea3-4dfc-4d7a-9400-2eb58f7ca5a3/Screenshot+2025-12-16+at+15.37.11.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/f044fea3-4dfc-4d7a-9400-2eb58f7ca5a3/Screenshot+2025-12-16+at+15.37.11.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/f044fea3-4dfc-4d7a-9400-2eb58f7ca5a3/Screenshot+2025-12-16+at+15.37.11.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p data-rte-preserve-empty="true">Cascading signal→wait with independent work between producer→consumer avoids GPU stalls. Image © Timothy Lottes.</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765891714485_29587">

<p>GPU→CPU synchronization was initially messy in Vulkan and Metal. Users needed a separate fence object for each submit. N buffering was a common technique for reusing the objects. This is a similar usability issue as discussed above regarding VkEvent. DirectX 12 was the first API to solve the GPU→CPU synchronization cleanly with timeline semaphores. Vulkan 1.2 and Metal 2 adapted the same design later. A timeline semaphore needs only a single 64-bit monotonically increasing counter. This reduces complexity over the older Vulkan and Metal fence APIs, which many engines still use today.</p>




















  
  



</div><div data-block-type="44" id="block-yui_3_17_2_1_1765889087118_719605"><pre><code>#define FRAMES_IN_FLIGHT 2

GpuSemaphore frameSemaphore = gpuCreateSemaphore(0);
uint64 nextFrame = 1;

while (running)
{
    if (nextFrame &gt; FRAMES_IN_FLIGHT) 
    {
        gpuWaitSemaphore(frameSemaphore, nextFrame - FRAMES_IN_FLIGHT);
    }
    
    // Render the frame here

    gpuSubmit(queue, {commandBuffer}, frameSemaphore, nextFrame++);
}

gpuDestroySemaphore(frameSemaphore);</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765889087118_719666">
  <p>Our proposed barrier design is a massive improvement over DirectX 12 and Vulkan. It reduces the API complexity significantly. Users no longer need to track individual resources. Our simple hazard tracking has queue + stage granularity. This matches what GPU hardware does today. Game engine graphics backends can be simplified and CPU cycles are saved.</p><h2>Command buffers</h2><p>Vulkan and DirectX 12 were designed to promote the pre-creation and reuse of resources. Early Vulkan examples recorded a single command buffer at startup, replaying it every frame. Developers quickly discovered that command buffer reuse was impractical. Real game environments are dynamic and the camera is in constant motion. The visible object set changes frequently.</p><p>Game engines ignored prerecorded command buffers entirely. Metal and WebGPU feature transient command buffers, which are created just before recording and disappear after GPU has finished rendering. This eliminates the need for command buffer management and prevents multiple submissions of the same commands. GPU vendors recommend one shot command buffers (a resettable command pool per frame in flight) in Vulkan too, as it simplifies the driver’s internal memory management (bump allocator vs heap allocator). The best practices match Metal and WebGPU design. Persistent command buffer objects can be removed. That API complexity didn’t provide anything worth using.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765889087118_737461"><pre><code>while (running)
`
    GpuCommandBuffer commandBuffer = gpuStartCommandRecording(queue);
    // Render frame here
}</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765889087118_737524">
  <h2>Graphics shaders</h2><p>Let’s start with a burning question: Do we need graphics shaders anymore? UE5 Nanite uses compute shaders to plot pixels using 64-bit atomics. High bits contain the pixel depth and low bits contain the payload. Atomic-min ensures that the closest surface remains. This technique was first presented at SIGGRAPH 2015 by Media Molecule Dreams (Alex Evans). Hardware rasterizer still has some advantages, like hierarchical/early depth-stencil tests. Nanite has to lean solely on coarse cluster culling, which results in extra overdraw with kitbashed content. Ubisoft (me and Ulrich Haar) presented this two-pass cluster culling algorithm at SIGGRAPH 2015. Ubisoft used cluster culling in combination with the hardware rasterizer for more fine grained culling. Today’s GPUs are bindless and much better suited for GPU-driven workloads like this. 10 years ago Ubisoft had to lean on virtual texturing (all textures in the same atlas) instead of bindless texturing. Despite many compute-only rasterizers today (Nanite, SDF sphere tracing,  DDA voxel tracing) the hardware rasterizer still remains the most used technique for rendering triangles in games today. It’s definitely worth discussing how to make the rasterization pipeline more flexible and easier to use.</p><p>The modern shader framework has grown to 16 shader entry points. We have eight entry points for rasterization (pixel, vertex, geometry, hull, domain, patch constant, mesh and amplification), and six for ray-tracing (ray generation, miss, closest hit, any hit, intersection and callable). In comparison, CUDA has a single entry point: kernel. This makes CUDA composable. CUDA has a healthy ecosystem of 3rd party libraries. New GPU hardware blocks such as the tensor cores (AI) are exposed as intrinsic functions. This is how it all started in the graphics land as well: texture sampling was our first intrinsic function. Today, texture sampling is fully bindless and doesn’t even require driver setup. This is the design developers prefer. Simple, easy to compose, and extend.</p><p>We recently got more intrinsics: inline raytracing and cooperative matrix (wave matrix in DirectX 12, subgroup matrix in Metal). I am hoping that this is the new direction. We should start tearing down the massive 16 shader framework and replacing it with intrinsics that can be composed in a flexible way.</p><p>Solving the shader framework complexity is a massive topic. To keep the scope of this blog post in check, I will today only discuss compute shaders and raster pipelines. I am going to be writing a followup about simplifying the shader framework, including modern topics such as ray-tracing, shader execution reordering (SER), dynamic register allocation extensions and Apple’s new L1$ backed register file (called dynamic caching).</p><h2>Raster pipelines</h2><p>There are two relevant raster pipelines today: Vertex+pixel and mesh+pixel. Mobile GPUs employing tile based deferred rendering (TBDR) perform per-triangle binning. Tile size is commonly between 16x16 to 64x64 pixels, making meshlets too coarse grained primitive for binning. Meshlet has no clear 1:1 lane to vertex mapping, there’s no straightforward way to run a partial mesh shader wave for selected triangles. This is the main reason mobile GPU vendors haven’t been keen to adapt the desktop centric mesh shader API designed by Nvidia and AMD. Vertex shaders are still important for mobile.</p><p>I will not be discussing geometry, hull, domain, and patch constant (tessellation) shaders. The graphics community widely considers these shader types as failed experiments. They all have crucial performance issues in their design. In all relevant use cases, you can run a compute prepass generating an index buffer to outperform these stages. Additionally, mesh shaders allow generating a compact 8-bit index buffer into on-chip memory, further increasing the performance gap over these legacy shader stages.</p><p>Our goal is to build a modern PSO abstraction with a minimal amount of baked state. One of the main critiques of Vulkan and DirectX 12 has been the pipeline permutation explosion. The less state we have inside the PSO, the less pipeline permutations we get. There are two main areas to improve: graphics shader data bindings and the rasterizer state.</p><h2>Graphics shader bindings&nbsp;</h2><p>Vertex+pixel shader pipeline needs several additional inputs compared to a compute kernel: vertex buffers, index buffer, rasterizer state, render target views and a depth-stencil view. Let’s start by discussing the shader visible data bindings.</p><p>Vertex buffer bindings are easy to solve: We simply remove them. Modern GPUs have fast raw load paths. Most GPU vendors have been emulating vertex fetch hardware already for several generations. Their low level shader compiler reads the user defined vertex layout and emits appropriate raw load instructions in the beginning of the vertex shader.&nbsp;</p><p>The vertex bindings declaration is another example of a special C/C++ API for defining a struct memory layout. It adds complexity and forces compiling multiple PSO permutations for different layouts. We simply replace the vertex buffers with standard C/C++ structs. No API is required.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765890239746_32538"><pre><code>// Common header...
struct Vertex
{
    float32x4 position;
    uint8x4 normal;
    uint8x4 tangent;
    uint16x2 uv;
};

struct alignas(16) Data
{
    float32x4x4 matrixMVP;
    const Vertex *vertices;
};

// CPU code...
gpuSetPipeline(commandBuffer, graphicsPipeline);

auto data = myBumpAllocator.allocate&lt;Data&gt;();
data.cpu-&gt;matrixMVP = camera.viewProjection * modelMatrix;
data.cpu-&gt;vertices = mesh.vertices;

gpuDrawIndexed(commandBuffer, data.gpu, mesh.indices, mesh.indexCount);

// Vertex shader...
struct VertexOut 
{
    float32x4 position : SV_Position;
    float16x4 normal;
    float32x2 uv;
};

VertexOut main(uint32 vertexIndex : SV_VertexID, const Data* data)
{
    Vertex vertex = data-&gt;vertices[vertexIndex];
    float32x4 position = data-&gt;matrixMVP * vertex.position;
    // TODO: Normal transform here
    return { .position = position, .normal = normal, .uv = vertex.uv };
}</code></pre>

</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765890239746_33528">

<p>The same is true for per-instance data and multiple vertex streams. We can implement them efficiently with raw memory loads. When we use raw load instructions, we can dynamically adjust the vertex stride, branch over secondary vertex buffer loads and calculate our vertex indices using custom formulas to implement clustered GPU-driven rendering, particle quad expansion, higher order surfaces, efficient terrain rendering and many other algorithms. Additional shader entry points and binding APIs are not needed. We can use our new static constant system to dead code eliminate vertex streams at pipeline creation or provide a static vertex stride if we so prefer. All the old optimization strategies still exist, but we can now mix and match techniques freely to match our renderer’s needs.&nbsp;</p>




















  
  



</div><div data-block-type="44" id="block-yui_3_17_2_1_1765890239746_47086"><pre><code>// Common header...
struct VertexPosition
{
    float32x4 position;
};

struct VertexAttributes
{
    uint8x4 normal;
    uint8x4 tangent;
    uint16x2 uv;
};

struct alignas(16) Instance
{
    float32x4x4 matrixModel;
}

struct alignas(16) Data
{
    float32x4x4 matrixViewProjection;
    const VertexPosition *vertexPositions;
    const VertexAttributes *vertexAttribues;
    const Instance *instances;
};

// CPU code...
gpuSetPipeline(commandBuffer, graphicsPipeline);

auto data = myBumpAllocator.allocate&lt;Data&gt;();
data.cpu-&gt;matrixViewProjection = camera.viewProjection;
data.cpu-&gt;vertexPositions = mesh.positions;
data.cpu-&gt;vertexAttributes = mesh.attributes;
data.cpu-&gt;instances = batcher.instancePool + instanceOffset; // pointer arithmetic is convenient

gpuDrawIndexedInstanced(commandBuffer, data.gpu, mesh.indices, mesh.indexCount, instanceCount);

// Vertex shader...
struct VertexOut 
{
    float32x4 position : SV_Position; // SV values are not real struct fields (doesn't affect the layout)
    float16x4 normal;
    float32x2 uv;
};

VertexOut main(uint32 vertexIndex : SV_VertexID, uint32 instanceIndex : SV_InstanceID, const Data* data)
{
    Instance instance = data-&gt;instances[SV_InstanceIndex];

    // NOTE: Splitting positions/attributes benefits TBDR GPUs (vertex shader is split in two parts)
    VertexPosition vertexPosition = data-&gt;vertexPositions[SV_VertexIndex];
    VertexAttributes vertexAttributes = data-&gt;vertexAttributes[SV_VertexIndex];

    float32x4x4 matrix = data-&gt;matrixViewProjection * instance.matrixModel;
    float32x4 position = matrix * vertexPosition.position;

    // TODO: Normal transform here

    return { .position = position, .normal = normal, .uv = vertexAttributes.uv };
}</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765890239746_47147">
  <p>The index buffer binding is still special. GPUs have index deduplication hardware. We don’t want to run the vertex shader twice for the same vertex. The index deduplication hardware packs the vertex waves eliminating duplicate vertices. Index buffering is still a crucial optimization today. Non-indexed geometry executes 3 vertex shader invocations (lanes) per triangle. A perfect grid has two triangles per cell, thus it only needs one vertex shader invocation per two triangles (ignoring the last row/column). Modern offline vertex cache optimizers output meshes with around 0.7 vertices per triangle efficiency. We can achieve around 4x to 6x reduction in vertex shading cost with index buffer in real world scenarios.</p><p>The index buffer hardware nowadays connects to the same cache hierarchy as all the other GPU units. Index buffer is simply an extra GPU pointer in the drawIndexed call. That’s the sole API surface we need for index buffering.</p><p>Mesh shaders lean on offline vertex deduplication. A common implementation shades one vertex per lane, and outputs it into on-chip memory. An 8-bit local index buffer tells the rasterizer which 3 vertices are used by each triangle. Since all the meshlet outputs are available at once and are already transformed in on-chip storage, there’s no need to deduplicate or pack vertices after triangle setup. This is why mesh shaders don’t need the index deduplication hardware or the post transform cache. All mesh shader inputs are raw data. No extra API surface is needed beyond the gpuDrawMeshlets command.</p><p>My example mesh shader uses 128 lane thread groups. Nvidia supports up to 126 vertices and 64 triangles per output meshlet. AMD supports 256 vertices and 128 triangles. The shader masks out excess lanes. Since there’s never more than 64 triangles, you might also opt for a 64 lane thread group for optimal triangle lane utilization and do a two iteration loop for vertex shading. My triangle fetch logic is just a single memory load instruction, wasting half of the lanes there isn’t a problem. I chose the extra parallelism for vertex shading instead. Optimal choices depend on your workload and the target hardware.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765890239746_68808"><pre><code>// Common header...
struct Vertex
{
    float32x4 position;
    uint8x4 normal;
    uint8x4 tangent;
    uint16x2 uv;
};

struct alignas(16) Meshlet
{
    uint32 vertexOffset;
    uint32 triangleOffset;
    uint32 vertexCount;
    uint32 triangleCount;
};

struct alignas(16) Data
{
    float32x4x4 matrixMVP;
    const Meshlet *meshlets;
    const Vertex *vertices;
    const uint8x4 *triangles;
};

// CPU code...
gpuSetPipeline(commandBuffer, graphicsMeshPipeline);

auto data = myBumpAllocator.allocate&lt;Data&gt;();
data.cpu-&gt;matrixMVP = camera.viewProjection * modelMatrix;
data.cpu-&gt;meshlets = mesh.meshlets;
data.cpu-&gt;vertices = mesh.vertices;
data.cpu-&gt;triangles = mesh.triangles;

gpuDrawMeshlets(commandBuffer, data.gpu, uvec3(mesh.meshletCount, 1, 1));

// Mesh shader...
struct VertexOut 
{
    float32x4 position : SV_Position;
    float16x4 normal;
    float32x2 uv;
};

[groupsize = (128, 1, 1)]
void main(uint32x3 groupThreadId : SV_GroupThreadID, uint32x3 groupId : SV_GroupID, const Data* data)
{
    Meshlet meshlet = data-&gt;meshlets[groupId.x];

    // Meshlet output allocation intrinsics
    VertexOut* outVertices = allocateMeshVertices&lt;VertexOut&gt;(meshlet.vertexCount);
    uint8x3* outIndices = allocateMeshIndices(meshlet.triangleCount);

    // Triangle indices (3x 8 bit)
    if (groupThreadId.x &lt; meshlet.triangleCount)
    {
        outIndices[groupThreadId.x] = triangles[meshlet.triangleOffset + groupThreadId.x].xyz;
    }

    // Vertices
    if (groupThreadId.x &lt; meshlet.vertexCount)
    {
        Vertex vertex = data-&gt;vertices[meshlet.vertexOffset + groupThreadId.x];
        float32x4 position = data-&gt;matrixMVP * vertex.position;
        // TODO: Normal transform here
        outVertices[groupThreadId.x] = { .position = position, .normal = normal, .uv = vertex.uv };
    }
}</code></pre>

</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765890239746_68869">

<p>Both vertex shaders and mesh shaders use pixel shaders. Rasterizer spawns pixel shader work based on triangle pixel coverage, HiZ, and early depth/stencil test results. Hardware can pack multiple triangles and multiple instances in the same pixel shader wave. Pixel shaders itself aren’t that special. Nowadays pixel shaders run on the same SIMD cores as all the other shader types. There are some special inputs available: interpolated vertex outputs, screen location, sample index and coverage mask, triangle id, triangle facing, etc. Special inputs are declared as kernel function parameters using the system value (: SV) semantics, similar to existing APIs.</p>




















  
  



</div><div data-block-type="44" id="block-yui_3_17_2_1_1765890239746_77331"><pre><code>// Pixel shader...
const Texture textureHeap[];

struct VertexIn // Matching vertex shader output struct layout
{
    float16x4 normal;
    float32x2 uv;
};

struct PixelOut 
{
    float16x4 color : SV_Color0;
};

PixelOut main(const VertexIn &amp;vertex, const DataPixel* data)
{
    Texture texture = textureHeap[data-&gt;textureIndex];
    Sampler sampler = {.minFilter = LINEAR, .magFilter = LINEAR};

    float32x4 color = sample(texture, sampler, vertex.uv);
    return { .color = color };
}</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765890239746_77392">
  <p>The removal of data bindings makes vertex and pixel shaders simpler to use. All of the complex data bindings APIs are replaced by a 64-bit GPU pointer. Users are able to write flexible vertex fetch code to avoid creating a PSO permutation per vertex layout.</p><h2>Rasterizer state</h2><p>Legacy APIs (OpenGL and DirectX 9) had fine grained commands for setting all the shader inputs and rasterizer states. The driver had to build shader pipelines on demand. Hardware specific rasterizer, blender, and input assembler command packets were constructed from the shadow state that combined all individual fine grained states. Vulkan 1.0 and DirectX 12 chose the fully opposite design. All state is baked in the PSO ahead of time. Only a select few states, such as viewport rect, scissor rect, and stencil values, can be changed dynamically. This resulted in a massive explosion of PSO permutations.</p><p>PSO creation is expensive, as it requires calling the GPU driver’s low-level shader compiler. PSO permutations consume a significant amount of storage and RAM. Changing the PSO is the most expensive state change. The small performance advantages that some vendors achieved by embedding render state directly into the shader microcode were overshadowed by the performance issues caused by significantly amplified pipeline creation, binding and data management costs everywhere. The pendulum swung too far to the opposite side.</p><p>Modern GPUs are ALU dense. Nvidia and AMD recently doubled their ALU rate with additional pipelines. Apple also doubled their fp32 pipelines in their M-series chips. Simple states resulting only in a constant replacement in the shader should not require pipeline duplication, even if it adds an extra ALU instruction or wastes an uniform register. Most shaders today are not ALU bound. The cost is often not measurable, but the benefits of having less permutations are significant. Vulkan 1.3 is a big step in the right direction. A lot of baked PSO states can now be set dynamically.</p><p>If we investigate deeper, we notice that all GPUs use command packets for configuring their rasterizer and depth-stencil units. These command packets are not directly tied to the shader microcode. We don’t need to modify the shader microcode to change the rasterizer and depth-stencil state. Metal has a separate depth-stencil state object and a separate command for applying it. A separate state object reduces the PSO permutations and reduces the expensive shader binding calls. Vulkan 1.3 dynamic state achieves similar PSO permutation reduction, but is more fine grained. Metal’s design is a better match for the actual hardware command packets. Bigger packets reduce the API bloat and overhead. DirectX 12 unfortunately still bundles most of the depth-stencil state inside the PSO (stencil ref and depth bias are the only dynamic state). In our design the depth-stencil state is a separate object.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765890239746_92642"><pre><code>GpuDepthStencilDesc depthStencilDesc = 
{
    .depthMode = DEPTH_READ | DEPTH_WRITE,
    .depthTest = OP_LESS_EQUAL,
    .depthBias = 0.0f,
    .depthBiasSlopeFactor = 0.0f,
    .depthBiasClamp = 0.0f,
    .stencilReadMask = 0xff,
    .stencilWriteMask = 0xff,
    .stencilFront =
    {
        .test = OP_ALWAYS,
        .failOp = OP_KEEP,
        .passOp = OP_KEEP,
        .depthFailOp = OP_KEEP,
        .reference = 0
    },
    .stencilBack =
    {
        .test = COMPARE_ALWAYS,
        .failOp = OP_KEEP,
        .passOp = OP_KEEP,
        .depthFailOp = OP_KEEP,
        .reference = 0
    }
};

// A minimal way to descibe the above (using C++ API struct default values):
GpuDepthStencilDesc depthStencilDesc = 
{
    .depthMode = DEPTH_READ | DEPTH_WRITE,
    .depthTest = OP_LESS_EQUAL,
};

GpuDepthStencilState depthStencilState = gpuCreateDepthStencilState(depthStencilDesc);</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765890239746_92703">
  <p>Immediate mode (desktop) GPUs have similar command packets for configuring the alpha blender unit. If we were designing DirectX 13, we would simply split the blend state object out of the PSO and call it a day. But we are designing a cross platform API, and blending works completely differently on mobile GPUs.</p><p>Mobile GPUs (TBDR) have always supported programmable blending. A render tile fits into a scratchpad memory close to the compute unit (such as the groupshared memory), allowing pixel shaders a direct low latency read+write access to previously rasterized pixels. Most mobile GPUs don’t have any fixed function blending hardware. When a traditional graphics API is used, the driver's low level shader compiler adds blending instructions at the end of the shader. This is analogous to the vertex fetch code generation (described above). If we were designing an API solely for mobile GPUs, we would get rid of the blend state APIs completely, just like we got rid of the vertex buffers. Mobile centric APIs expose a framebuffer fetch intrinsic to efficiently obtain current pixel’s previous color. The user can write any blending formula they want, including complex formulas to implement advanced algorithms such as order independent transparency. The user can also write a generic parametrised formula to eliminate the PSO permutation explosion. As we can see, both the desktop and the mobile GPUs have their own ways to reduce the pipeline permutations regarding blending, the limitation is the current APIs.</p><p>Vulkan subpasses were designed to wrap framebuffer fetch into a cross platform API. This was another misstep in Vulkan design. Vulkan inherited a simple low level design from Mantle, but Vulkan was designed as an OpenGL replacement, so it had to target all mobile and desktop GPU architectures. Wrapping two entirely different architecture types under the same API isn’t easy. Subpasses ended up being a high level concept inside a low level API. A subpass could define an entire chain of render passes but pretend that it’s just a single render pass. Subpasses increased driver complexity and made the shader and renderpass APIs needlessly complex. Users were forced to create complex persistent multi-render pass objects ahead of time and pass those objects into shader pipeline creation. Shader pipelines became multi-pipelines under the hood (one pipeline per sub-pass). Vulkan added all of this complexity simply to avoid exposing the framebuffer fetch intrinsic to the shader language. To add insult to injury, the subpasses weren’t even good enough to solve the programmable blending. Pixel ordering is only preserved at pass boundaries. Subpasses were only useful for narrow 1:1 multi-pass use cases. Vulkan 1.3 scrapped the subpasses and introduced “dynamic rendering”. Users no longer need to create persistent render pass objects, just like in Metal, DirectX 12 and WebGPU. This is a great example of how a complex framework can be tempting for API designers, but developers prefer simple shader intrinsics. Game engines already support building different shaders to different platforms. Apple’s Metal examples do the same: Framebuffer fetch is used on iOS and a traditional multipass algorithm on Mac.</p><p>It’s apparent that we can’t abstract hardware differences regarding blending and framebuffer fetch. Vulkan 1.0 tried that and failed miserably. The correct solution is to provide the user a choice. They can choose to embed the blend state into the PSO. This works on all platforms and is the perfect approach for shaders that don’t suffer from blend state related pipeline permutation issues. Mobile GPU’s internal driver shader compiler adds the blending instructions in the end of the pixel shader as usual. On immediate mode (desktop) GPUs (and some mobile GPUs), the user can choose to use separate blend state objects. This reduces the amount of PSO permutations and makes it faster to change the blend state at runtime, as a full pipeline change is not needed (only a blend state configuration packet is sent).&nbsp;</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765890239746_105493"><pre><code>GpuBlendDesc blendDesc = 
{
    .colorOp = OP_ONE,
    .srcColorFactor = FACTOR_SRC_ALPHA,
    .dstColorFactor = FACTOR_ONE_MINUS_SRC_ALPHA,
    .alphaOp = OP_ONE,
    .srcAlphaFactor = FACTOR_SRC_ALPHA,
    .dstAlphaFactor = FACTOR_ONE_MINUS_SRC_ALPHA,
    .colorWriteMask = 0xf
};

// Create blend state object (needs feature flag)
GpuBlendState blendState = gpuCreateBlendState(blendDesc);

// Set dynamic blend state (needs feature flag)
gpuSetBlendState(commandBuffer, blendState);</code></pre>

</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765890239746_105554">

<p>On mobile GPUs the user can embed the blend state into the PSO as usual, or choose to use framebuffer fetch to write a custom blending formula. If the mobile developer wants to avoid compiling multiple PSO permutations for different alpha blending modes, they can write a general formula parametrized with dynamic draw struct inputs.</p>




















  
  



</div><div data-block-type="44" id="block-yui_3_17_2_1_1765890239746_110757"><pre><code>// Standard percentage blend formula (added automatically by internal shader compiler)
dst.rgb = src.rgb * src.a + dst.rgb * (1.0 - src.a);
dst.a = src.a * src.a + dst.a * (1.0 - src.a);

// Custom formula supporting all blend modes used by HypeHype
const BlendParameters&amp; p = data-&gt;blendParameters;
vec4 fs = src.a * vec4(p.sc_sa.xxx + p.sc_one.xxx, p.sa_sa + p.sa_one) + dst.rgba * vec4(p.sc_dc.xxx, sa_da);
vec4 fd = (1.0 - src.a) * vec4(p.dc_1msa.xxx, p.da_1msa) + vec4(p.dc_one.xxx, p.da_one);
dst.rgb = src.rgb * fs.rgb + dst.rgb * fd.rgb;
dst.a  = src.a * fs.a + dst.a * fd.a;</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765890239746_110818">
  <p>As the blend state is separated from the PSO, it’s possible we lose some automatic dead code optimizations. If the user wants to disable the color output, they traditionally use the colorWriteMask in the blend state. Since the blend state is burned in the PSO, the compiler can do dead code elimination based on it. To allow similar dead code optimizations, we have writeMask for each color target in the PSO.</p><p>Dual source blending is a special blend mode requiring two color outputs from the pixel shader. Dual source blending only supports a single render target. Since our blend state can be separate, we need to have a supportDualSourceBlending field in our PSO desc. When enabled, the shader compiler knows the second output is for dual source blending. The validation layer would complain if the output is not present. A pixel shader exporting two colors can be used without dual source blending (the second color is ignored), but there’s a small cost for exporting two colors.</p><p>The remaining rendering state in the PSO is minimal: primitive topology, render target and depth-stencil target formats, MSAA sample count and alpha to coverage. All of this state affects the generated shader microcode so it needs to stay in the PSO. We never want to rebuild the shader PSO microcode due to a state change. If an embedded blend state is used, it's also burned in the PSO. This leaves us with a simple raster state struct for PSO creation.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765890239746_122451"><pre><code>GpuRasterDesc rasterDesc = 
{
    .topology = TOPOLOGY_TRIANGLE_LIST,
    .cull = CULL_CCW,
    .alphaToCoverage = false,
    .supportDualSourceBlending = false,
    .sampleCount = 1`
    .depthFormat = FORMAT_D32_FLOAT,
    .stencilFormat = FORMAT_NONE,
    .colorTargets = 
    {
        { .format = FORMAT_RG11B10_FLOAT	},		// G-buffer with 3 render targets
        { .format = FORMAT_RGB10_A2_UNORM },
        { .format = FORMAT_RGBA8_UNORM }
    },
    .blendstate = GpuBlendDesc { ... }			// optional (embedded blend state, otherwise dynamic)
};

// A minimal way to descibe the above (using C++ API struct default values):
GpuRasterDesc rasterDesc = 
{
    .depthFormat = FORMAT_D32_FLOAT,
    .colorTargets = 
    {
        { .format = FORMAT_RG11B10_FLOAT	},
        { .format = FORMAT_RGB10_A2_UNORM },
        { .format = FORMAT_RGBA8_UNORM }
    },
};

// Pixel + vertex shader
auto vertexIR = loadFile("vertexShader.ir");
auto pixelIR = loadFile("pixelShader.ir");
GpuPipeline graphicsPipeline = gpuCreateGraphicsPipeline(vertexIR, pixelIR, rasterDesc);

// Mesh shader
auto meshletIR = loadFile("meshShader.ir");
auto pixelIR = loadFile("pixelShader.ir");
GpuPipeline graphicsMeshletPipeline = gpuCreateGraphicsMeshletPipeline(meshletIR, pixelIR, rasterDesc);</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765890239746_122833">
  <p>HypeHype’s Vulkan shader PSO initialization backend code is 400 lines, and I would consider it compact compared to other engines I have been developing. Here, we managed to initialize a pixel + vertex shader with just 18 lines of code. It’s easy to read and understand. Yet, there’s no compromise on performance.&nbsp;</p><p>Rendering with the raster pipeline is similar to rendering with a compute pipeline. Instead of providing one data pointer, we provide two since there’s two kernel entry points: One for vertex shader and one for pixel shader. Metal has a separate set of data binding slots for vertex and pixel shaders. DirectX, Vulkan and WebGPU use a visibility mask (vertex, pixel, compute, etc) for each individual binding. Many engines choose to bind the same data to both vertex and pixel shader. This is a fine practice on DirectX, Vulkan and WebGPU as you can combine the mask bits, but doubles the binding calls on Metal. Our proposed approach using two data pointers is the best of both worlds. You can simply pass the same pointer twice if you want to use the same data in both the vertex and the pixel shader. Or you can provide independent data pointers, if you prefer full separation between the shader stages. The shader compiler does dead code elimination and constant/scalar preload optimizations separately for pixel and vertex shader. Neither data sharing nor data duplication results in bad performance. The user can choose whatever fits their design.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765890239746_128385"><pre><code>// Common header...
struct Vertex
{
    float32x4 position;
    uint16x2 uv;
};

struct alignas(16) DataVertex
{
    float32x4x4 matrixMVP;
    const Vertex *vertices;
};

struct alignas(16) DataPixel
{
    float32x4 color;
    uint32 textureIndex;
};

// CPU code...
gpuSetDepthStencilState(commandBuffer, depthStencilState);
gpuSetPipeline(commandBuffer, graphicsPipeline);

auto dataVertex = myBumpAllocator.allocate&lt;DataVertex&gt;();
dataVertex.cpu-&gt;matrixMVP = camera.viewProjection * modelMatrix;
dataVertex.cpu-&gt;vertices = mesh.vertices;

auto dataPixel = myBumpAllocator.allocate&lt;DataPixel&gt;();
dataPixel.cpu-&gt;color = material.color;
dataPixel.cpu-&gt;textureIndex = material.textureIndex;

gpuDrawIndexed(commandBuffer, dataVertex.gpu, dataPixel.gpu, mesh.indices, mesh.indexCount);

// Vertex shader...
struct VertexOut 
{
    float32x4 position : SV_Position; // SV values are not real struct fields (doesn't affect the layout)
    float32x2 uv;
};

VertexOut main(uint32 vertexIndex : SV_VertexID, const DataVertex* data)
{
    Vertex vertex = data.vertices[vertexIndex];
    float32x4 position = data-&gt;matrixMVP * vertex.position;
    return { .position = position, .uv = vertex.uv };
}

// Pixel shader...
const Texture textureHeap[];

struct VertexIn // Matching vertex shader output struct layout
{
    float32x2 uv;
};

PixelOut main(const VertexIn &amp;vertex, const DataPixel* data)
{
    Texture texture = textureHeap[data-&gt;textureIndex];
    Sampler sampler = {.minFilter = LINEAR, .magFilter = LINEAR};

    float32x4 color = sample(texture, sampler, vertex.uv);
    return { .color = color };
}</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765890239746_128446">
  <p>Our goal was to reduce the PSO permutation explosion by minimizing the remaining state inside the PSO. The depth-stencil can be separated on all architectures. The blend state separation is possible on desktop hardware, but most mobile hardware burns the blend equation at the end of the pixel shader microcode. Exposing the framebuffer fetch intrinsics directly to the user is a much better idea than Vulkan’s failed subpass approach. Users can write their own blend formulas unlocking new rendering algorithms, or they can author general parametrized blending formulas to reduce the PSO count.</p><h2>Indirect drawing</h2><p>Standard draw/dispatch commands utilize C/C++ function parameters to provide the arguments: thread group dimensions, index count, instance count, etc. Indirect draw calls allow the user to provide a GPU buffer + offset pair instead as the draw argument source, a crucial addition enabling GPU-driven rendering. Our version uses a single GPU pointer instead of the usual buffer object + offset pair, simplifying the API slightly.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765890239746_131320"><pre><code>gpuDispatchIndirect(commandBuffer, data.gpu, arguments.gpu);
gpuDrawIndexedInstancedIndirect(commandBuffer, dataVertex.gpu, dataPixel.gpu, arguments.gpu);</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765890239746_131381">
  <p>All of our arguments are GPU pointers. Both the data and the arguments are indirect. This is a great improvement over existing APIs. DirectX 12, Vulkan and Metal don’t support indirect root arguments. The CPU has to provide them.</p><p>Indirect multidraw (MDI) should also be supported. The draw count comes from a GPU address. MDI parameters are: an array of root data (GPU pointer, for both vertex and pixel), an array of draw arguments (GPU pointer), and stride for the root data array (for both vertex and pixel). Stride = 0 naturally means that the same root data is replicated for each draw.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765890239746_145649"><pre><code>gpuDrawIndexedInstancedIndirectMulti(commandBuffer, dataVertex.gpu, sizeof(DataVertex), dataPixel.gpu, sizeof(DataPixel), arguments.gpu, drawCount.gpu);</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765890239746_145709">
  <p>Vulkan’s multidraw doesn’t allow changing bindings per draw call. You can use gl_DrawID to index into a buffer which contains your draw data structs. This adds an indirection in the shader. You need to use either descriptor indexing or the new descriptor buffer extension to fetch textures. DirectX 12 ExecuteIndirect has a configurable command signature, allowing the user to manually setup a root constant per draw, but this doesn’t hit the fast path on all GPU command processors. ExecuteIndirect tier 1.1 (2024) added a new optional counter increment feature: D3D12_INDIRECT_ARGUMENT_TYPE_INCREMENTING_CONSTANT. This can be used to implement draw ID. SM6.8 (2024) finally added support for SV_StartInstanceLocation, allowing the user to directly embed a constant in the indirect draw arguments. Unlike SV_InstanceID, the new SV_StartInstanceLocation is uniform across the whole draw call, providing optimal codegen for indexed loads (uniform/scalar path). The data fetch still requires an indirection. GPU-generated root data is not supported.&nbsp;</p><p>If we generate draw arguments or root data on the GPU, we need to ensure that the command processor waits for the dispatch to finish. Modern command processors prefetch commands and their arguments to hide the latency. We have a flag in our barrier to prevent this. The best practice is to batch update all your draw arguments and root data to avoid fine-grained barriers.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765890239746_148980"><pre><code>gpuBarrier(commandBuffer, STAGE_COMPUTE, STAGE_COMPUTE, HAZARD_DRAW_ARGUMENTS);</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765890239746_149113">
  <p>The lack of indirect shader selection is a significant limitation in the current PC and mobile graphics APIs. Indirect shader selection can be implemented using multi-pipelines similar to ray-tracing. Metal also supports indirect command creation (Nvidia has a similar Vulkan extension). An efficient way to skip over draw calls is a valuable subset. DirectX work graphs and CUDA dynamic parallelism allow a shader to spawn more waves on demand. Unfortunately, the APIs to access these hardware improvements is still highly platform specific and scattered in multiple shader entry points. There’s no clear standardization. My followup post will discuss the shader framework and cover this topic in depth.</p><p>Our proposed design makes indirect drawing extremely powerful. Both the shader root data and the draw parameters can be indirectly provided by the GPU. These advantages power up multi-draw, allowing clean and efficient per-draw data bindings with no hacks. The future of indirect drawing and the shader framework will be discussed in a followup post.</p><h2>Render passes</h2><p>The rasterizer hardware needs to be prepared for rendering before we can start drawing. Common operations include binding render target and depth-stencil views and clearing color and depth. Clear might trigger a fast clear elimination if the clear color is changed. This is transparently handled by the clear command. On mobile GPUs, tiles are stored from on-chip storage to VRAM during rendering. Vulkan, Metal and WebGPU use render pass abstraction for clearing, loading and storing the render target. DirectX 12 added render pass support in the 2018 update in order to optimize rendering on the latest Intel (Gen11) and the Qualcomm (Adreno 630) GPUs. The render pass abstraction doesn’t add notable API complexity, so it's an easy choice for a modern cross platform API.</p><p>DirectX12 has render target views and depth-stencil views, and separate descriptor heaps to store them. This is just an API abstraction. These heaps are simply CPU memory allocated by the driver. Render target and depth-stencil views are not GPU descriptors. The rasterizer API is not bindless. The CPU driver sets up the rasterizer using command packets. In Vulkan and Metal you pass the existing texture/view objects to the beginRenderPass directly. The driver gets the required information from the texture object behind the scenes. Our proposed GpuTexture object fits this job. Rasterization output is the main reason we still need a CPU-side texture object. We write texture descriptors directly into GPU memory. The CPU side driver can’t access those.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765890239746_152053"><pre><code>GpuRenderPassDesc renderPassDesc =
{
    .depthTarget = {.texture = deptStencilTexture, .loadOp = CLEAR, .storeOp = DONT_CARE, .clearValue = 1.0f},
    .stencilTarget = {.texture = deptStencilTexture, .loadOp = CLEAR, .storeOp = DONT_CARE, .clearValue = 0},
    .colorTargets =
    {
        {.texture = gBufferColor, .loadOp = LOAD, .storeOp = STORE, .clearColor = {0,0,0,0}},
        {.texture = gBufferNormal, .loadOp = LOAD, .storeOp = STORE, .clearColor = {0,0,0,0}},
        {.texture = gBufferPBR, .loadOp = LOAD, .storeOp = STORE, .clearColor = {0,0,0,0}}
    }
};

gpuBeginRenderPass(commandBuffer, renderPassDesc);
// Add draw calls here!
gpuEndRenderPass(commandBuffer);</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765890239746_152114">
  <p>It would be nice to have bindless render passes, bindless/indirect (multi-)clear commands, indirect scissor/viewport rectangles (array), etc. Unfortunately many GPUs today still need the CPU driver to set up their rasterizer.</p><p>A note about barriers: Render pass begin/end commands don’t automatically emit barriers. The user can render multiple render passes&nbsp;simultaneously, if they write to disjoint render targets. A barrier between the raster output stage (or later) and the consumer stage will flush the tiny ROP caches if the GPU architecture needs it. Not having an automatic barrier between the render passes is also crucial for efficient depth prepass implementations (ROP caches are not flushed needlessly).</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1765891714485_24990">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/00e4b579-4b5b-4d09-854d-b89f04df7556/Screenshot+2025-12-16+at+15.33.03.png" data-image="https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/00e4b579-4b5b-4d09-854d-b89f04df7556/Screenshot+2025-12-16+at+15.33.03.png" data-image-dimensions="3524x1996" data-image-focal-point="0.5,0.5" alt="" data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/00e4b579-4b5b-4d09-854d-b89f04df7556/Screenshot+2025-12-16+at+15.33.03.png" width="3524" height="1996" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/00e4b579-4b5b-4d09-854d-b89f04df7556/Screenshot+2025-12-16+at+15.33.03.png?format=100w 100w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/00e4b579-4b5b-4d09-854d-b89f04df7556/Screenshot+2025-12-16+at+15.33.03.png?format=300w 300w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/00e4b579-4b5b-4d09-854d-b89f04df7556/Screenshot+2025-12-16+at+15.33.03.png?format=500w 500w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/00e4b579-4b5b-4d09-854d-b89f04df7556/Screenshot+2025-12-16+at+15.33.03.png?format=750w 750w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/00e4b579-4b5b-4d09-854d-b89f04df7556/Screenshot+2025-12-16+at+15.33.03.png?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/00e4b579-4b5b-4d09-854d-b89f04df7556/Screenshot+2025-12-16+at+15.33.03.png?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/6741ac845a0db27835868e3c/00e4b579-4b5b-4d09-854d-b89f04df7556/Screenshot+2025-12-16+at+15.33.03.png?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p data-rte-preserve-empty="true">GPU-based clay simulation and ray-tracing tech in Claybook (Sebastian Aaltonen, GDC 2018): I optimized the Unreal Engine 4 console barrier implementations (Xbox One, PS4) to allow render target overlap. The barrier stall is avoided.</p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765891714485_25314">
  <h2>Prototype API</h2><p>My prototype API fits in one screen: 150 lines of code. The blog post is titled “No Graphics API”. That’s obviously an impossible goal today, but we got close enough. WebGPU has a smaller feature set and features a ~2700 line API (Emscripten C header). Vulkan header is ~20,000 lines, but it supports ray-tracing, and many other features our minimalistic API doesn’t yet support. We didn’t have to trade off performance to achieve the reduction in API complexity. For this feature set, our API offers more flexibility than existing APIs. A fully extended summer 2025 Vulkan 1.4 can do all the same things in practice, but is significantly more complex to use and has more API overhead.</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765888768382_375054"><pre><code>// Opaque handles
struct GpuPipeline;
struct GpuTexture;
struct GpuDepthStencilState;
struct GpuBlendState;
struct GpuQueue;
struct GpuCommandBuffer;
struct GpuSemaphore;

// Enums
enum MEMORY { MEMORY_DEFAULT, MEMORY_GPU, MEMORY_READBACK };
enum CULL { CULL_CCW, CULL_CW, CULL_ALL, CULL_NONE };
enum DEPTH_FLAGS { DEPTH_READ = 0x1, DEPTH_WRITE = 0x2 };
enum OP { OP_NEVER, OP_LESS, OP_EQUAL, OP_LESS_EQUAL, OP_GREATER, OP_NOT_EQUAL, OP_GREATER_EQUAL, OP_ALWAYS }; 
enum BLEND { BLEND_ADD, BLEND_SUBTRACT, BLEND_REV_SUBTRACT, BLEND_MIN, BLEND_MAX };
enum FACTOR { FACTOR_ZERO, FACTOR_ONE, FACTOR_SRC_COLOR, FACTOR_DST_COLOR, FACTOR_SRC_ALPHA, ... };
enum TOPOLOGY { TOPOLOGY_TRIANGLE_LIST, TOPOLOGY_TRIANGLE_STRIP, TOPOLOGY_TRIANGLE_FAN };
enum TEXTURE { TEXTURE_1D, TEXTURE_2D, TEXTURE_3D, TEXTURE_CUBE, TEXTURE_2D_ARRAY, TEXTURE_CUBE_ARRAY };
enum FORMAT { FORMAT_NONE, FORMAT_RGBA8_UNORM, FORMAT_D32_FLOAT, FORMAT_RG11B10_FLOAT, FORMAT_RGB10_A2_UNORM, ... };
enum USAGE_FLAGS { USAGE_SAMPLED, USAGE_STORAGE, USAGE_COLOR_ATTACHMENT, USAGE_DEPTH_STENCIL_ATTACHMENT, ... };
enum STAGE { STAGE_TRANSFER, STAGE_COMPUTE, STAGE_RASTER_COLOR_OUT, STAGE_PIXEL_SHADER, STAGE_VERTEX_SHADER, ... };
enum HAZARD_FLAGS { HAZARD_DRAW_ARGUMENTS = 0x1, HAZARD_DESCRIPTORS = 0x2, , HAZARD_DEPTH_STENCIL = 0x4 };
enum SIGNAL { SIGNAL_ATOMIC_SET, SIGNAL_ATOMIC_MAX, SIGNAL_ATOMIC_OR, ... };

// Structs
struct Stencil 
{
    OP test = OP_ALWAYS,
    OP failOp = OP_KEEP;
    OP passOp = OP_KEEP;
    OP depthFailOp = OP_KEEP;
    uint8 reference = 0;
};

struct GpuDepthStencilDesc 
{
    DEPTH_FLAGS depthMode = 0;
    OP depthTest = OP_ALWAYS;
    float depthBias = 0.0f;
    float depthBiasSlopeFactor = 0.0f;
    float depthBiasClamp = 0.0f;
    uint8 stencilReadMask = 0xff;
    uint8 stencilWriteMask = 0xff;
    Stencil stencilFront;
    Stencil stencilBack;
};

struct GpuBlendDesc
{
    BLEND colorOp = BLEND_ADD,
    FACTOR srcColorFactor = FACTOR_ONE;
    FACTOR dstColorFactor = FACTOR_ZERO;
    BLEND alphaOp = BLEND_ADD;
    FACTOR srcAlphaFactor = FACTOR_ONE;
    FACTOR dstAlphaFactor = FACTOR_ZERO;
    uint8 colorWriteMask = 0xf;
};

struct ColorTarget {
    FORMAT format = FORMAT_NONE;
    uint8 writeMask = 0xf;
};

struct GpuRasterDesc
{
    TOPOLOGY topology = TOPOLOGY_TRIANGLE_LIST;
    CULL cull = CULL_NONE;
    bool alphaToCoverage = false;
    bool supportDualSourceBlending = false;
    uint8 sampleCount = 1;
    FORMAT depthFormat = FORMAT_NONE;
    FORMAT stencilFormat = FORMAT_NONE;
    Span&lt;ColorTarget&gt; colorTargets = {};
    GpuBlendDesc* blendstate = nullptr; // optional embedded blend state
};

struct GpuTextureDesc
{ 
    TEXTURE type = TEXTURE_2D;
    uint32x3 dimensions;
    uint32 mipCount = 1;
    uint32 layerCount = 1;
    uint32 sampleCount = 1;
    FORMAT format = FORMAT_NONE; 
    USAGE_FLAGS usage = 0;
};

struct GpuViewDesc 
{
    FORMAT format = FORMAT_NONE;
    uint8 baseMip = 0;
    uint8 mipCount = ALL_MIPS;
    uint16 baseLayer = 0;
    uint16 layerCount = ALL_LAYERS;
};

struct GpuTextureSizeAlign { size_t size; size_t align; };
struct GpuTextureDescriptor { uint64[4] data; };

// Memory
void* gpuMalloc(size_t bytes, MEMORY memory = MEMORY_DEFAULT);
void* gpuMalloc(size_t bytes, size_t align, MEMORY memory = MEMORY_DEFAULT);
void gpuFree(void *ptr);
void* gpuHostToDevicePointer(void *ptr);

// Textures
GpuTextureSizeAlign gpuTextureSizeAlign(GpuTextureDesc desc);
GpuTexture gpuCreateTexture(GpuTextureDesc desc, void* ptrGpu);
GpuTextureDescriptor gpuTextureViewDescriptor(GpuTexture texture, GpuViewDesc desc);
GpuTextureDescriptor gpuRWTextureViewDescriptor(GpuTexture texture, GpuViewDesc desc);

// Pipelines
GpuPipeline gpuCreateComputePipeline(ByteSpan computeIR);
GpuPipeline gpuCreateGraphicsPipeline(ByteSpan vertexIR, ByteSpan pixelIR, GpuRasterDesc desc);
GpuPipeline gpuCreateGraphicsMeshletPipeline(ByteSpan meshletIR, ByteSpan pixelIR, GpuRasterDesc desc);
void gpuFreePipeline(GpuPipeline pipeline);

// State objects
GpuDepthStencilState gpuCreateDepthStencilState(GpuDepthStencilDesc desc);
GpuBlendState gpuCreateBlendState(GpuBlendDesc desc);
void gpuFreeDepthStencilState(GpuDepthStencilState state);
void gpuFreeBlendState(GpuBlendState state);

// Queue
GpuQueue gpuCreateQueue(/* DEVICE &amp; QUEUE CREATION DETAILS OMITTED */);
GpuCommandBuffer gpuStartCommandRecording(GpuQueue queue);
void gpuSubmit(GpuQueue queue, Span&lt;GpuCommandBuffer&gt; commandBuffers);

// Semaphores
GpuSemaphore gpuCreateSemaphore(uint64 initValue);
void gpuWaitSemaphore(GpuSemaphore sema, uint64 value);
void gpuDestroySemaphore(GpuSemaphore sema);

// Commands
void gpuMemCpy(GpuCommandBuffer cb, void* destGpu, void* srcGpu,);
void gpuCopyToTexture(GpuCommandBuffer cb, void* destGpu, void* srcGpu, GpuTexture texture);
void gpuCopyFromTexture(GpuCommandBuffer cb, void* destGpu, void* srcGpu, GpuTexture texture);

void gpuSetActiveTextureHeapPtr(GpuCommandBuffer cb, void *ptrGpu);

void gpuBarrier(GpuCommandBuffer cb, STAGE before, STAGE after, HAZARD_FLAGS hazards = 0);
void gpuSignalAfter(GpuCommandBuffer cb, STAGE before, void *ptrGpu, uint64 value, SIGNAL signal);
void gpuWaitBefore(GpuCommandBuffer cb, STAGE after, void *ptrGpu, uint64 value, OP op, HAZARD_FLAGS hazards = 0, uint64 mask = ~0);

void gpuSetPipeline(GpuCommandBuffer cb, GpuPipeline pipeline);
void gpuSetDepthStencilState(GpuCommandBuffer cb, GpuDepthStencilState state);
void gpuSetBlendState(GpuCommandBuffer cb, GpuBlendState state); 

void gpuDispatch(GpuCommandBuffer cb, void* dataGpu, uvec3 gridDimensions);
void gpuDispatchIndirect(GpuCommandBuffer cb, void* dataGpu, void* gridDimensionsGpu);

void gpuBeginRenderPass(GpuCommandBuffer cb, GpuRenderPassDesc desc);
void gpuEndRenderPass(GpuCommandBuffer cb);

void gpuDrawIndexedInstanced(GpuCommandBuffer cb, void* vertexDataGpu, void* pixelDataGpu, void* indicesGpu, uint32 indexCount, uint32 instanceCount);
void gpuDrawIndexedInstancedIndirect(GpuCommandBuffer cb, void* vertexDataGpu, void* pixelDataGpu, void* indicesGpu, void* argsGpu);
void gpuDrawIndexedInstancedIndirectMulti(GpuCommandBuffer cb, void* dataVxGpu, uint32 vxStride, void* dataPxGpu, uint32 pxStride, void* argsGpu, void* drawCountGpu);

void gpuDrawMeshlets(GpuCommandBuffer cb, void* meshletDataGpu, void* pixelDataGpu, uvec3 dim);
void gpuDrawMeshletsIndirect(GpuCommandBuffer cb, void* meshletDataGpu, void* pixelDataGpu, void *dimGpu);</code></pre>

</div><div data-sqsp-text-block-content="" data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" data-sqsp-block="text" id="block-yui_3_17_2_1_1765888768382_375116">
  <h2>Tooling</h2><p>How can we debug code that doesn’t bind buffer and texture objects and doesn’t call an API to describe the memory layout explicitly? C/C++ debuggers have been doing that for decades. There’s no special operating system APIs for describing your software’s memory layout. The debugger is able to follow 64-bit pointer chains and use the debug symbol data provided by your compiler. This includes the memory layouts of your structs and classes. CUDA and Metal use C/C++ based shading languages with full 64-bit pointer semantics. Both have robust debuggers that traverse pointer chains without issues. The texture descriptor heap is just GPU memory. The debugger can index it, load a texture descriptor, show the descriptor data and visualize the texels. All of this works already in the Xcode Metal debugger. Click on a texture or a sampler handle in any struct in any GPU address. Debugger will visualize it.</p><p>Modern GPUs virtualize memory. Each process has their own page table. The GPU capture has a separate replayer process with its own virtual address space. If the replayer would naively replay all the allocations, it would get a different GPU virtual address for each memory allocation. This was fine for legacy APIs as it wasn’t possible to directly store GPU addresses in your data structures. A modern API needs special replay memory allocation APIs that force the replayer to mirror the exact GPU virtual memory layout. DX12 and Vulkan BDA have public APIs for this: RecreateAt and VkMemoryOpaqueCaptureAddressAllocateInfo. Metal and CUDA debuggers do the same using internal undocumented APIs. A public API is preferable as it allows open source tools like RenderDoc to function.</p><p>Don’t raw pointers bring security concerns? Can’t you just read/write other apps' memory? This is not possible due to virtual memory. You can only access your own memory pages. If you accidentally use a stale pointer or overflow, you will get a page fault. Page faults are possible with existing buffer based APIs. DirectX 12 and Vulkan don’t clamp your storage (byteaddress/structured) buffer addresses. OOB causes a page fault. Users can also accidentally free a memory heap and continue using stale buffer or texture descriptors to get a page fault. Nothing really changes. An access to an unmapped region is a page fault and the application crashes. This is familiar to C/C++ programmers. If you want robustness, you can use ptr + size pairs. That’s exactly how WebGPU is implemented. The WebGPU shader compiler (Tint or Naga) emits an extra clamp instruction for each buffer access, including vertex accesses (index buffer value out of bounds). WebGL didn’t allow shading index buffer data with other data. WebGL scanned through the indices on the CPU side (making index buffer update very slow). Back then custom vertex fetch was not possible. The hardware page faulted before the shader even ran.</p><h2>Translation layers</h2><p>Being able to run existing software is crucial. Translation layers such as ANGLE, Proton and MoltenVK play a crucial role in the portability and deprecation process of legacy APIs. Let’s talk about translating DirectX 12, Vulkan and Metal to our new API.</p><p>MoltenVK (Vulkan to Metal translation layer) proves that Vulkan’s buffer centric API can be translated to Metal’s 64-bit pointer based ecosystem. MoltenVK translates Vulkan’s descriptor sets into Metal’s argument buffers. The generated argument buffers are standard GPU structs containing a 64-bit GPU pointer per buffer binding and a 64-bit texture ID per texture binding. We can do better by allocating a contiguous range of texture descriptors in our texture heap for each descriptor set, and storing a single 32-bit base index instead of a 64-bit texture ID for each texture binding. This is possible since our API has a user managed texture heap unlike Metal.</p><p>MoltenVK maps descriptor sets to Metal API root bind slots. We generate a root struct with up to eight 64-bit pointer fields, each pointing to a descriptor set struct (see above). Root constants are translated into value fields and root descriptors (root buffers) are translated into 64-bit pointers. The efficiency should be identical, assuming the GPU driver preloads our root struct fields into uniform/scalar registers (as discussed in the root arguments chapter).</p><p>Our API uses 64-bit pointer semantics like Metal. We can use the same techniques employed by MoltenVK to translate the buffer load/store instructions in the shader. MoltenVK also supports translating Vulkan’s new buffer device address extension.</p><p>Proton (DX12 to Vulkan translation layer) proves that DirectX 12 SM 6.6 descriptor heap can be translated to Vulkan’s new descriptor buffer extension. Proton also translates other DirectX 12 features to Vulkan. We have already shown that Vulkan to Metal translation is possible with MoltenVK, transitively proving that translation from DirectX 12 to Metal should be possible. The biggest missing feature in MoltenVK is the SM 6.6 style descriptor heap (Vulkan’s descriptor buffer extension). Metal doesn’t expose the descriptor heap directly to the user. Our new proposed API has no such limitation. Our descriptor heap semantics are a superset to SM 6.6 descriptor heap and a close match to Vulkan’s descriptor buffer extension. Translation is straightforward. Vulkan’s extension also adds a special flag for descriptor invalidate, matching our HAZARD_DESCRIPTORS. DirectX 12 descriptor heap API is easy to translate, as it’s just a thin wrapper over the raw descriptor array in GPU memory.</p><p>To support Metal 4.0, we need to implement Metal’s driver managed texture descriptor heap. This can be implemented using a simple freelist over our texture heap. Metal uses 64-bit texture handles which are implemented as direct heap indices on modern Apple Silicon devices. Metal allows using the texture handles in shaders directly as textures. This is syntactic sugar for textureHeap[uint64(handle)]. A Metal texture handle is translated into uint64 by our shader translator, maintaining identical GPU memory layout.</p><p>Our API doesn’t support vertex buffers. WebGPU doesn’t use hardware vertex buffers either, yet it implements the classic vertex buffer abstraction. WGSL shader translator (Tint or Naga) adds one storage buffer binding per vertex stream and emits vertex load instructions in the beginning of the vertex shader. Custom vertex fetch allows emitting clamp instructions to avoid OOB behavior. A misbehaving website can’t crash the web browser. Our own shader translator adds a 64-bit pointer to the root struct for each vertex stream, generates a struct matching its layout and emits vertex struct load instructions in the beginning of the vertex shader.</p><p>We have shown that it’s possible to write translation layers to run DirectX 12, Vulkan and Metal applications on top of our new API. Since WebGPU is implemented on top of these APIs by browsers, we can run WebGPU applications too.</p><h2>Min spec hardware</h2><p><strong>Nvidia Turing</strong> (RTX 2000 series, 2018) introduced ray-tracing, tensor cores, mesh shaders, low latency raw memory paths, bigger &amp; faster caches, scalar unit, secondary integer pipeline and many other future looking features. Officially PCIe ReBAR support launched with RTX 3000 series, but there exists hacked Turing drivers that support it too, indicating that the hardware is capable of it. This 7 year old GPU supports everything we need. Nvidia just ended GTX 1000 series driver support in fall 2025. All currently supported Nvidia GPUs could be supported by our new API.</p><p><strong>AMD RDNA2</strong> (RX 6000 series, 2020) matched Nvidia’s feature set with ray-tracing and mesh shaders. One year earlier, RDNA 1 introduced coherent L2$, new L1$ level, fast L0$, generic DCC read/write paths, fastpath unfiltered loads and a modern SIMD32 architecture. PCIe ReBAR is officially supported (brand name “Smart Access Memory”). This 5 year old GPU supports everything we need. AMD ended GCN driver support already in 2021. Today RDNA 1 &amp; RDNA 2 only receive bug fixes and security updates, RDNA 3 is the oldest GPU receiving game optimizations. All the currently supported AMD GPUs could be supported by our API.</p><p><strong>Intel Alchemist / Xe1</strong> (2022) were the first Intel chips with SM 6.6 global indexable heap support. These chips also support ray-tracing, mesh shaders, PCIe ReBAR (discrete) and UMA (integrated). These 3 year old Intel GPUs support everything we need.</p><p><strong>Apple M1 / A14</strong> (MacBook M1, iPhone 12, 2020) support Metal 4.0. Metal 4.0 guarantees GPU memory visibility to CPU (UMA on both phones and computers), and allows the user to write 64-bit pointers and 64-bit texture handles directly into GPU memory. Metal 4.0 has a new residency set API, solving a crucial usability issue with bindless resource management in the old useResource/useHeap APIs. iOS 26 still supports iPhone 11. Developers are not allowed to ship apps that require Metal 4.0 just yet. iOS 27 likely deprecates iPhone 11 support next year. On Mac, if you drop Intel Mac support, you have guaranteed Metal 4.0 support. M1-M5 = 5 generations = 5 years.</p><p><strong>ARM Mali-G710</strong> (2021) is ARMs first modern architecture. It introduced their new command stream frontend (CSF), reducing the CPU dependency of draw call building and adding crucial features like multi-draw indirect and compute queues. Non-uniform index texture sampling is significantly faster and the AFBC lossless compressor now supports 16-bit floating point targets. G710 supports Vulkan BDA and descriptor buffer extensions and is capable of supporting the new 2025 unified image layout extension with future drivers. The Mali-G715 (2022) introduced support for ray-tracing.</p><p><strong>Qualcomm Adreno 650 </strong>(2019) supports Vulkan BDA, descriptor buffer and unified image layout extensions, 16-bit storage/math, dynamic rendering and extended dynamic state with the latest Turnip open source drivers. Adreno 740 (2022) introduced support for ray-tracing.</p><p><strong>PowerVR DXT</strong> (Pixel 10, 2025) is PowerVRs first architecture that supports Vulkan descriptor buffer and buffer device address extensions. It also supports 64-bit atomics, 8-bit and 16-bit storage/math, dynamic rendering, extended dynamic state and all the other features we require.</p><h2>Conclusion</h2><p>Modern graphics API have improved gradually in the past 10 years. Six years after DirectX 12 launch, SM 6.6 (2021) introduced the modern global texture heap, allowing fully bindless renderer design. Metal 4.0 (2025) and CUDA have a clean 64-bit pointer based shader architecture with minimal binding API surface. Vulkan has the most restrictive standard, but extensions such as buffer device access (2020), descriptor buffer (2022) and unified image layouts (2025) add support for modern bindless infrastructure, but tools are still lagging behind. As of today, there’s no single API that meets all our requirements, but if we combine their best bits together, we can build the perfect API for modern hardware.</p><p>10 years ago, modern APIs were designed for CPU-driven binding models. New bindless features were presented as optional features and extensions. A clean break would improve the usability and reduce the API bloat and driver complexity significantly. It’s extremely difficult to get the whole industry behind a brand new API. I am hoping that vendors are willing to drop backwards compatibility in their new major API versions (Vulkan 2.0, DirectX 13) to embrace the fully bindless GPU architecture we have today. A new bindless API design would solve the mismatch between the API and the game engine RHI, allowing us to get rid of the hash maps and fine grained resource tracking. Metal 4.0 is close to this goal, but it is still missing the global indexable texture heap. A 64-bit texture handle can’t represent a range of textures.</p><p>HLSL and GLSL shading languages were designed over 20 years ago as a framework of 1:1 elementwise transform functions (vertex, pixel, geometry, hull, domain, etc). Memory access is abstracted and array handling is cumbersome as there’s no support for pointers. Despite 20 years of existence, HLSL and GLSL have failed to accumulate a library ecosystem. CUDA in contrast is a composable language exposing memory directly and new features (such as AI tensor cores) though intrinsics. CUDA has a broad library ecosystem, which has propelled Nvidia into $4T valuation. We should learn from it.</p><p>WebGPU note: WebGPU design is based on 10 year old core Vulkan 1.0 with extra restrictions. WebGPU doesn’t support bindless resources, 64-bit GPU pointers or persistently mapped GPU memory. It feels like a mix between DirectX 11 and Vulkan 1.0. It is a great improvement for web graphics, but doesn’t meet modern bindless API standards. I will discuss WebGPU in a separate blog post.&nbsp;</p><p>My prototype API shows what is achievable with modern GPU architectures today, if we mix the best bits from all the latest APIs. It is possible to build an API that is simpler to use than DirectX 11 and Metal 1.0, yet it offers better performance and flexibility than DirectX 12 and Vulkan. We should embrace the modern bindless hardware.&nbsp;</p><h2>Appendix</h2><p>A simple user land GPU bump allocator used in all example code. We call gpuHostToDevicePointer once in the temp allocator constructor. We can perform standard pointer arithmetic (such as offset) on GPU pointers. Traditional Vulkan/DX12 buffer APIs require a separate offset. This simplifies the API and user code (ptr vs handle+offset pair). A production ready temp allocator would implement overflow handing (grow, flush, etc).&nbsp;</p>
</div><div data-block-type="44" id="block-yui_3_17_2_1_1765887159238_683986"><pre><code>template&lt;typename T&gt;
struct GPUTempAllocation&lt;T&gt;
{
    T* cpu;
    T* gpu;
}

struct GPUBumpAllocator
{
    uint8 *cpu;
    uint8 *gpu;
    uint32 offset = 0;
    uint32 size;

    TempBumpAllocator(uint32 size) : size(size)
    {
        cpu = gpuMalloc(size);
        gpu = gpuHostToDevicePointer(cpu);
    }

    TempAllocation&lt;uint8&gt; alloc(int bytes, int align = 16)
    {
        offset = alignRoundUp(offset, align);
        if (offset + bytes &gt; size) offset = 0; // Simple ring wrap (no overflow detection)
        TempAllocation&lt;uint8&gt; alloc = { .cpu = cpu + offset, . gpu = gpu + offset };
        offset += bytes;
        return alloc;
    }

    template&lt;typename T&gt;&nbsp;
    T* alloc(int count = 1)
    {
        TempAllocation&lt;uint8&gt; mem = alloc(sizeof(T) * count, alignof(T));
        return TempAllocation&lt;T&gt; { .cpu = (T*)mem.cpu, . gpu = (T*)mem.gpu };
    }
};</code></pre>

</div></div>
  
</article>

</div>

  
</article>

          

          

          
            
              

            
          
        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vibe coding creates fatigue? (143 pts)]]></title>
            <link>https://www.tabulamag.com/p/too-fast-to-think-the-hidden-fatigue</link>
            <guid>46292365</guid>
            <pubDate>Tue, 16 Dec 2025 18:32:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tabulamag.com/p/too-fast-to-think-the-hidden-fatigue">https://www.tabulamag.com/p/too-fast-to-think-the-hidden-fatigue</a>, See on <a href="https://news.ycombinator.com/item?id=46292365">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span>After vibe coding for some time, I feel fatigue. I’m coding </span><a href="https://www.marvai.dev/" rel="">Marvai - a package manager for prompts</a><span> on my own, with a combination of Claude Code and Cursor.</span></p><div><li><p><span>I use Claude Code for code generation,</span><a href="https://www.tabulamag.com/p/how-to-use-ai-as-a-developer-today" rel=""> bug fixing</a><span>, test writing, test fixing, security checks etc.</span></p></li><li><p>Claude Code is especially useful to fix linting errors from nilaway or staticcheck - for a developer those are boring and tedious.</p></li><li><p>I use Cursor for augmented coding, generate functions, adapt copy&amp;pasted code, for refactoring, fix error handling, and many other tedious tasks.</p></li><li><p>I have been a coder for 40 years and I have seen many tools for developers that haven’t worked, like Model Driven Development MDD/MDA and executable UML.</p></li><li><p><span>With AI I’m now much faster at generating code and building features than I ever was before. </span><strong>The combination of Claude Code and Cursor is a real speedup.</strong></p></li><li><p><em><strong>I encountered a new phenomenon.</strong></em></p></li><li><p><span>Again and again I feel fatigue. I finish a feature, and another feature, concetrate on  reviewing the code the AI generated, and fix a bug and finish a feature with such velocity and I feel fatigue after some hours - sometimes as soon as one hour. AI working at such speed, finishing things to accept or review, feels too much for my brain to process or keep up with. </span><em><strong>I need to  pause for some time before I can start again.</strong></em></p></li><li><p>I haven’t felt this before as a developer. </p></li><li><p><span>I first encountered the concept of cognitive load in the book</span><a href="https://teamtopologies.com/book" rel=""> Team Toplogies</a><span>. The idea there is to structure teams in a way that the cognitive load for developers is not too small and not to big. The more responsibilities a team and it’s members get, the bigger the cognitive load. And if you put many different topics on a team, the cognitive load for the team becomes too big for the team to work.</span></p></li><li><p><span>As a young adult I was working in a plastic factory, sitting at a machine. The machine produced vacuum cleaner cases, had it’s rhythm, it finished vacuum cleaner cases on it’s own schedule, made a “PING” when finished, opened the door and I had to grab the casing and put it down (and package it in fine paper etc. which took some time). The machine would close while I was finishing the casing. And for some time it was stressful, working to the rhythm of the machine. Then I got accustomed to the speed and rhythm, until my boss increased the frequency. </span><em><strong>Living by machine time is what I sometimes feel with Vibe Coding and Cursor generating code or Claude fixing a bug</strong></em><span>.</span></p></li><li><p><span>I had </span><a href="https://xkcd.com/303/" rel="">waiting times with compiling</a><span> and waited for the machine to finish, but it feels differently, with vibe coding it feels like the machine is in control not me.</span></p></li><li><p>With traditional coding, the speed of your output matches the complexity of the task and the speed of your coding. This gives your brain time to process what is happening. With vibe coding the coding happens so fast, that my brain can’t process what is happening in real time, and thoughts are getting clogged up. Complex tasks are compressed into seconds or minutes.</p></li><li><p>My brain does not get the baking time to mentally process architecture, decisions and edge cases the AI creates - not enough time to put the AI output into one coherent picture. I’m running a marathon at the pace of a sprint - speeds don’t match.</p></li><li><p>One reason developers are developers is the dopamine loop.</p></li><li><p><span>You write code, it doesn’t work, you fix it, it works, great! Dopamine rush. Several dozens or a hundred times a day. Now this loop speeds up. Faster and faster we run around that loop, and get washed with Dopamine - and your brain gets overwhelmed. And stress hormones at the same time! </span><em><strong>You get fatigue - instead of the usually happiness and satisfaction of coding.</strong></em></p></li><li><p><span>With coding there is a limit to context switching. A context switch in coding is like dumping the cache of your brain and reloading the cache of your brain with a new context. This takes time and energy. You need to build a </span><a href="https://leanpub.com/ideaflow/" rel="">mental model of the code</a><span>, to decide what to change and how to change it and then writing the changes out into source code, the essence of coding.</span></p></li><li><p><span>With vibe coding the frequency of content switches speeds up tremendously - with the AI fixing and creating many different things in different packages or modules with one go. Even when I </span><em>&lt;tab&gt;, &lt;tab&gt;, &lt;tab&gt;</em><span> in Cursor, each change is a micro-content switch from function to function.</span></p></li><li><p>Each context switch takes energy, every context switch is heavy lifting for your brain. Normally this materializes as the fact that it takes time to context switch. With AI in the driver seat, context switches are forced on you faster and faster. </p></li><li><p>When each context switch takes energy, fast energy switches from feature delivered to feature delivered, vibe coding drains your energy - fatigue!</p></li><li><p>With AI it seems we all become managers, we all become reviewers. The core of the role is changing from turning requirements into code to managing AI output - more like a team lead manages the output of a team, but on a much deeper level. Your responsibility grows to manage a team of five with vibe coding, while still being a developer being responsible for the code. It’s like being a traffic officer in the middle of a busy intersection - which is a stressful job on it’s own - while also overseeing five intersections in parallel.</p></li><li><p>Reviewing, directing and guiding an AI puts more stress on you than writing code, where your writing matches your thinking and doesn’t jump ahead, with you in a rush to catch up.</p></li><li><p>What can be said:</p><ul><li><p>Me and many more I assume feel fatigue from vibe coding</p></li><li><p>We need deliberate pacing when working with AI tools</p></li><li><p>We need AI-aware retrospectives to understand what is going on - perhaps having a daily retrospective to get the mind and the code in sync again</p></li><li><p>We need to be aware of new mental health challenges for AI coders</p></li><li><p><span>We might need to </span><a href="https://www.tabulamag.com/p/introduction-to-theory-of-control" rel="">let go of micro managing AIs</a><span> and trust the AI more - stop trying to bridge the gap between managing an AI and controlling it’s output</span></p></li></ul></li><li><p>I don’t think we’ve figured out yet where this is going. AI has made us faster than we’ve ever been, but our brains haven’t caught up to the pace. We’re like early pilots flying with autopilot—capable, but drained. We need new rhythms, new boundaries, and new ways of thinking about what it means to “code.”</p></li><li><p><em>Maybe the future of coding isn’t just faster. Maybe it’s also slower in a way, on purpose.</em></p></li></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT Image 1.5 (338 pts)]]></title>
            <link>https://openai.com/index/new-chatgpt-images-is-here/</link>
            <guid>46291941</guid>
            <pubDate>Tue, 16 Dec 2025 18:07:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/new-chatgpt-images-is-here/">https://openai.com/index/new-chatgpt-images-is-here/</a>, See on <a href="https://news.ycombinator.com/item?id=46291941">Hacker News</a></p>
Couldn't get https://openai.com/index/new-chatgpt-images-is-here/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[AI is wiping out entry-level tech jobs, leaving graduates stranded (114 pts)]]></title>
            <link>https://restofworld.org/2025/engineering-graduates-ai-job-losses/</link>
            <guid>46291504</guid>
            <pubDate>Tue, 16 Dec 2025 17:37:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://restofworld.org/2025/engineering-graduates-ai-job-losses/">https://restofworld.org/2025/engineering-graduates-ai-job-losses/</a>, See on <a href="https://news.ycombinator.com/item?id=46291504">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<!-- Article Start -->
				
<p>In 2022, Rishabh Mishra joined a high-ranking engineering college in India’s Jabalpur with the most predictable dream in global tech: study computer science, write code, and one day make it to Silicon Valley.</p>



<p>Three years later, Mishra faces a sobering reality.</p>



<p>Artificial intelligence has gutted entry-level roles in the tech industry that Mishra and his classmates were counting on. Among his 400 classmates at the Indian Institute of Information Technology, Design and Manufacturing, fewer than 25% have secured job offers. His course ends in May 2026, and there’s a sense of panic on the campus.</p>



<figure><audio controls="" src="https://restofworld.org/wp-content/uploads/2025/12/Rishab-final.mp3"></audio><figcaption><strong>Listen:</strong>&nbsp;Rishabh Mishra describes how the reality of tech jobs is very different from what he was told</figcaption></figure>



<p>“It is really bad out there,” Mishra told <em>Rest of World. </em>“Everyone is so panicked — even our juniors. As the degree end nears, the anxiety is heightened among all of us.” Some of his classmates are exploring the option of pursuing higher studies before entering the job market. “But after one year, if you return to the job market, your degree is even more irrelevant,” he said.</p>



<p>Students at engineering colleges in India, China, Dubai, and Kenya are facing a “jobpocalypse” as artificial intelligence replaces humans in entry-level roles. Tasks once assigned to fresh graduates, such as debugging, testing, and routine software maintenance, are now increasingly automated.</p>



<p>Over the last three years, the <a href="https://www.signalfire.com/blog/signalfire-state-of-talent-report-2025">number of fresh graduates</a> hired by big tech companies globally has declined by more than 50%, according to a report published by SignalFire, a San Francisco-based venture capital firm. Even though hiring rebounded slightly in 2024, only 7% of new hires were recent graduates. As many as 37% of managers said they’d rather use AI than hire a Gen Z employee.</p>



<p>“As demand for junior roles declines, even highly credentialed engineering graduates are struggling to break into tech, especially at the Big Tech companies,” the report said.&nbsp;</p>


<figure><blockquote><p>Even highly credentialed engineering graduates are struggling to break into tech.”</p></blockquote></figure>


<p>Indian IT services companies have reduced entry-level roles by 20%–25% thanks to automation and AI, consulting firm <a href="https://www.moneycontrol.com/artificial-intelligence/entry-level-tech-demand-falls-20-25-as-india-shifts-to-a-diamond-shaped-workforce-ey-article-13679052.html">EY said in a report last month</a>. Job platforms like LinkedIn, Indeed, and Eures noted a <a href="https://talentup.io/blog/why-entry-level-jobs-in-europe-are-becoming-harder-to-find-in-2025/">35% decline in junior tech positions</a> across major EU countries during 2024.</p>



<p>The World Economic Forum’s <a href="https://www.weforum.org/publications/the-future-of-jobs-report-2025/">Future of Jobs Report 2025</a> warned that 40% of employers expect to reduce staff where AI can automate tasks.</p>



<p>“Five years ago, there was a real war for [coders and developers]. There was bidding to hire,” and 90% of the hires were for off-the-shelf technical roles, or positions that utilize ready-made technology products rather than requiring in-house development, said Vahid Haghzare, director at IT hiring firm Silicon Valley Associates Recruitment in Dubai<em>.</em></p>



<p>Since the rise of AI, “it has dropped dramatically,” he said. “I don’t even think it’s touching 5%. It’s almost completely vanished.” The company headhunts workers from multiple countries including China, Singapore, and the U.K.</p>



<p>While high-paying jobs at coveted brands like Apple, Microsoft, Amazon, and Meta rarely cross his desk these days, companies that hire recent engineering graduates expect them to execute “additional responsibilities,” like managing a project or leading sales. “They have to face the customer and have customer communications and maybe even do some selling,” Haghzare said.</p>



<p>Some engineering students have realigned their ambitions to meet such demands from employers. Nishant Kaushik, who studied computer science at another well-ranked college in eastern India, has decided to look for roles in sales or marketing.&nbsp;</p>



<p>The rise of AI has also rendered engineering degrees less relevant: Workplace demands now differ from what is taught in colleges.</p>



<p>When Rita Sande Lukale enrolled in an electronics engineering program at the Technical University of Kenya in 2021, she hoped to land a role in the system architecture sector after graduation. Over the past few years, however, she has seen such roles disappear.</p>



<figure><audio controls="" src="https://restofworld.org/wp-content/uploads/2025/12/Rita-final.m4a"></audio><figcaption><strong>Listen:</strong> Rita Sande Lukale describes how AI has replaced humans in simple repetitive tasks.</figcaption></figure>



<p>Entry-level jobs such as handling data logging, system diagnostics, or code writing have been replaced by AI, Lukale told <em>Rest of World</em>. Now, fresh graduates “must possess higher-level skills, necessary to understand algorithms and use the engineering judgement to troubleshoot the complex and automated systems,” she said.</p>



<p>While she doesn’t consider AI to be a “job destroyer,” it has fundamentally changed the type of engineers that companies need to hire, Lukale said. She feels she needs to adapt and learn more to land a job.</p>



<p>Not only are fresh graduates expected to understand and use the latest tools efficiently, “they are asked to up their output by 70% because ‘they are using AI,’” Liam Fallon, who heads the product division at GoodSpace AI, an AI-powered recruiting company, told <em>Rest of World</em>. As a result, students face a rapidly changing industry that requires them to upskill outside the curriculum on their own. Experts believe universities are unable to align their academic practices fast enough to meet&nbsp;AI-driven industry demands.</p>



<p>The current system, where a student commits three to five years to learn computer science and then looks for a job, is “not sustainable,” Haghzare said. Students are “falling down a hole, and they don’t know how to get out of it.”</p>
				<!-- Article End -->
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The GitHub Actions control plane is no longer free (214 pts)]]></title>
            <link>https://www.blacksmith.sh/blog/actions-pricing</link>
            <guid>46291500</guid>
            <pubDate>Tue, 16 Dec 2025 17:37:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.blacksmith.sh/blog/actions-pricing">https://www.blacksmith.sh/blog/actions-pricing</a>, See on <a href="https://news.ycombinator.com/item?id=46291500">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="w-node-c021e89c-c247-0617-1dae-6dd36d1352b4-250b2c58"><p>TL;DR</p><p>GitHub is adding a $0.002-per-minute fee on all GitHub Actions usage, so the control plane is no longer free.</p></div><div id="w-node-_9e21598a-bd92-e71f-267e-33e11a0f4df0-250b2c58"><h2><strong>What's happening?</strong></h2><p>GitHub just <a href="https://resources.github.com/actions/2026-pricing-changes-for-github-actions/">announced changes</a> to Actions pricing. Previously, GitHub Actions had a free control plane. That meant if you used GitHub Actions but ran jobs outside of GitHub-hosted runners, whether that’s on Blacksmith, on your own machines, or in your own AWS account, you paid nothing to GitHub for those minutes; you only paid for the compute.</p><p>With this change, GitHub is introducing a $0.002 per-minute platform fee for <em>all</em> GitHub Actions usage.</p><p>In practice, this means CI costs now have two components:</p><ul role="list"><li>Compute costs (whoever runs your runners)</li><li>A flat GitHub platform fee, charged per minute of Actions usage</li></ul><p>These changes go into effect on March 1st, 2026.</p><h2>Our perspective on why they’re making these changes</h2><p>GitHub Actions has long had a graduation churn problem. As companies grow, their CI workloads become larger, more complex, and more expensive. At a certain scale, GitHub-hosted runners become both slow and costly, pushing teams to self-host or move to third-party runners like Blacksmith.</p><p>Until now, that shift had an important side effect: companies could continue using the GitHub Actions control plane while paying GitHub nothing for CI execution. GitHub provided scheduling, orchestration, and workflow automation, but captured no revenue from some of its largest and fastest-growing customers.</p><p>The new per-minute platform fee changes that. It directly monetizes the Actions control plane and establishes a floor on what GitHub earns from CI, regardless of where jobs run. In effect, self-hosting is no longer free.</p><p>At the same time, GitHub reduced the price of GitHub-hosted runners. This isn’t accidental. Lower hosted runner prices make GitHub-hosted runners more attractive, while the platform fee introduces a new, unavoidable cost for self-hosting.</p><p>From GitHub’s perspective, this is a rational move. Most Actions usage is concentrated on smaller runners, so the hosted runner price cuts likely don’t materially impact revenue. More importantly, GitHub is trading lower-margin compute revenue for higher-margin platform revenue.</p><p>Hosted runners are fundamentally a compute business. The platform fee, by contrast, monetizes software without scaling infrastructure costs linearly. As CI usage grows, that revenue scales with significantly better unit economics.</p><p>In the past, our customers have asked us how GitHub views third-party runners long-term. The platform fee largely answers that: GitHub now monetizes Actions usage regardless of where jobs run, aligning third-party runners like Blacksmith as ecosystem partners rather than workarounds.</p><h2><strong>Per-minute CI pricing and its impact on self-hosting</strong></h2><p>Before this change, self-hosting was a way to avoid paying GitHub entirely. That’s no longer true. Now, self-hosting retains the operational burden of running CI infrastructure while still incurring per-minute charges on GitHub.</p><p>At that point, the primary variable you can still control is how many minutes your CI jobs consume. One approach is to run CI on infrastructure designed to minimize wall-clock time and eliminate redundant work. That’s the problem Blacksmith focuses on. In practice, this shows up in a few areas:</p><ul role="list"><li>Faster machines. Blacksmith runs CI jobs on CPUs that have 50%+ higher single-core performance than GitHub-hosted runners. As part of ongoing fleet upgrades, we’re deploying even newer instances that add an additional 15–25% improvement, further reducing runtime for CPU-bound workloads.</li><li>Reusing work across runs. <a href="https://docs.blacksmith.sh/blacksmith-caching/docker-builds">Docker layer caches</a> are persisted across CI runs, allowing unchanged layers to be reused rather than rebuilt, cutting Docker build times from tens of minutes to seconds for many customers.</li><li>Container caching (beta). Service containers can be pre-hydrated on runners, removing repeated <a href="https://docs.blacksmith.sh/blacksmith-caching/docker-container-caching">image pulls and extraction</a> from the job startup path.</li></ul><p>With a per-minute platform fee, CI performance and cost are tightly coupled. The remaining lever is reducing CI time and total Actions.</p><p>‍</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GitHub will begin charging for self-hosted action runners on March 2026 (447 pts)]]></title>
            <link>https://github.blog/changelog/2025-12-16-coming-soon-simpler-pricing-and-a-better-experience-for-github-actions/</link>
            <guid>46291414</guid>
            <pubDate>Tue, 16 Dec 2025 17:32:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.blog/changelog/2025-12-16-coming-soon-simpler-pricing-and-a-better-experience-for-github-actions/">https://github.blog/changelog/2025-12-16-coming-soon-simpler-pricing-and-a-better-experience-for-github-actions/</a>, See on <a href="https://news.ycombinator.com/item?id=46291414">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
<p>On <strong>January 1, 2026</strong>, GitHub will <strong>reduce the price of GitHub-hosted runners by up to 39%</strong> <a href="https://docs.github.com/billing/reference/actions-runner-pricing">depending on the machine type used</a>. The free usage minute quotas will remain the same.</p>
<p>On <strong>March 1, 2026,</strong> GitHub will introduce a new <strong>$0.002 per minute GitHub Actions cloud platform charge</strong> that will apply to self-hosted runner usage. Any usage subject to this charge will count toward the minutes included in your plan, as explained in <a href="https://docs.github.com/billing/concepts/product-billing/github-actions#free-use-of-github-actions">our GitHub Actions billing documentation</a>.</p>
<p>Runner usage in public repositories will <strong>remain free.</strong> There will be no changes in price structure for GitHub Enterprise Server customers.</p>
<h3 id="deeper-investment-in-the-actions-self-hosted-experience"><a href="#deeper-investment-in-the-actions-self-hosted-experience">Deeper investment in the Actions self-hosted experience</a></h3>
<p>We are increasing our investment into our self-hosted experience to ensure that we can provide autoscaling for scenarios beyond just Linux containers. This will include new approaches to scaling, new platform support, Windows support, and more as we move through the next 12 months.</p>
<p>For more details about the product investments we’re making in Actions, please visit our <a href="https://resources.github.com/actions/2026-pricing-changes-for-github-actions">Executive Insights page</a>.</p>
<h3 id="recommended-resources"><a href="#recommended-resources">Recommended resources</a></h3>
<ul>
<li>For answers to common questions about this change, see the FAQ in our <a href="https://resources.github.com/actions/2026-pricing-changes-for-github-actions">post on GitHub’s Executive Insights page</a>.</li>
<li>See the <a href="https://docs.github.com/billing/reference/actions-runner-pricing">GitHub Actions runner pricing documentation</a> for the new GitHub-hosted runner rates effective January 1, 2026.  </li>
<li>For more details on upcoming GitHub Actions releases, see the <a href="https://github.com/orgs/github/projects/4247">GitHub public roadmap</a>.  </li>
<li>For help estimating your expected Actions usage cost, use the newly updated <a href="https://github.com/pricing/calculator#actions">Actions pricing calculator</a>.  </li>
<li>If you are interested in moving existing self-hosted runner usage to GitHub-hosted runners, see the <a href="https://docs.github.com/actions/tutorials/migrate-to-github-runners">SHR to GHR migration guide</a> in our documentation.</li>
</ul>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FVWM-95 (113 pts)]]></title>
            <link>https://fvwm95.sourceforge.net/</link>
            <guid>46291172</guid>
            <pubDate>Tue, 16 Dec 2025 17:13:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fvwm95.sourceforge.net/">https://fvwm95.sourceforge.net/</a>, See on <a href="https://news.ycombinator.com/item?id=46291172">Hacker News</a></p>
<div id="readability-page-1" class="page">
<center></center>

<h2>

<hr><i>What is FVWM-95?</i></h2>
Fvwm95 is a hack based on <a href="http://www.fvwm.org/">fvwm2</a>.
It tries to emulate the good features of a well known product without bloating
the regular fvwm code.
<p>The main aspects are:
</p><ul>
<li>
A familiar look and feel. It can be interesting for users moving from the
MS-World to Unix, or for those who have to switch regularly between the
two. Or for those that simply would like to have the same MS look and feel
in Unix, or for those that want to have just another window manager.
But before all, it's meant to be useful, simple and efficient.</li>

<li>
Same flexible and easy configuration as the native fvwm.</li>

<li>
Functionality extensible via loadable modules.</li>

<li>
A taskbar: find quickly an application window and don't take space with
icon windows.</li>
</ul>
Shortly after releasing the first version of fvwm95, we started working
on a desktop-folder metaphor for the window manager in order to have a more complete
emulation of the Windows-95 desktop. The development evolved towards the
creation of a <a href="http://xclass.sourceforge.net/index.html">C++ class
library</a>, providing an easily extensible set of Win95-looking widgets,
which eased the development of new applications.
<p>Here are some fvwm95 screenshots:
</p><ul>
<li>
<a href="https://fvwm95.sourceforge.net/screenshot-full.gif">a full screen-shot of one of the latest versions</a></li>

<li>
<a href="https://fvwm95.sourceforge.net/screenshot-decor.gif">a window with decorations</a></li>

<li>
<a href="https://fvwm95.sourceforge.net/screenshot-start.gif">the taskbar showing the Start menu</a></li>
</ul>

<h2>

<hr><i>Downloads</i></h2>
The main distribution site has moved from <a href="ftp://mitac11.uia.ac.be/pub">mitac11.uia.ac.be</a>
to sourceforge. Go to the <a href="https://sourceforge.net/project/showfiles.php?group_id=40864">releases</a> directory.
<h2>

<hr><a name="Mailing list"></a><i>Mailing list</i></h2>
The old mailing list at physik.uni-muenchen.de has been moved to lists.sourceforge.net.
The address of the new list is:
<ul><a href="mailto:fvwm95-users@lists.sourceforge.net">fvwm95-users@lists.sourceforge.net</a></ul>
To subscribe or unsubscribe a message should be sent to:
<ul><a href="mailto:fvwm95-request@lists.sourceforge.net">fvwm95-request@lists.sourceforge.net</a></ul>
with the word "subscribe" or "unsubscribe" in the message body. The messages
will be archived and could be requested from:
<ul><a href="mailto:majordomo@lists.sourceforge.net">majordomo@lists.sourceforge.net</a></ul>
A weekly (or when 40kByte of size is reached) digest is also available:
<ul><a href="mailto:fvwm95-digest-request@lists.sourceforge.net">fvwm95-digest-request@lists.sourceforge.net</a></ul>

<h2>

<hr><i>Some fvwm and fvwm95-related links</i></h2>

<ul>
<li>
<a href="http://xclass.sourceforge.net/index.html">The Xclass home page</a>.
You'll find there the explorer, the desktop manager and an enhanced taskbar.
</li>
<li>
<a href="http://www.fvwm.org/">Official fvwm home page</a>.</li>

<li>
<a href="http://www.plig.net/xwinman/">Window Managers for X</a>.</li>

<li>
<a href="http://www.katsurajima.seya.yokohama.jp/Gadgets/Xaw95/">Xaw95
Toolkit</a>: Xaw95 is a hacked version of the Athena widget set which
gives Win95 look to old X applications.</li>
</ul>


<hr><span size="-1">Page last updated: Nov 26, 2001.</span>
<p>Access count since last update:&nbsp;


</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pricing Changes for GitHub Actions (514 pts)]]></title>
            <link>https://resources.github.com/actions/2026-pricing-changes-for-github-actions/</link>
            <guid>46291156</guid>
            <pubDate>Tue, 16 Dec 2025 17:12:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://resources.github.com/actions/2026-pricing-changes-for-github-actions/">https://resources.github.com/actions/2026-pricing-changes-for-github-actions/</a>, See on <a href="https://news.ycombinator.com/item?id=46291156">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Today we’re announcing updates to our pricing and product models for GitHub Actions. </p><div><p>Why? </p>
<p>When we shipped Actions in 2018, we had no idea how popular it would become. By early 2024, the platform was running about 23 million jobs per day and our existing architecture couldn’t reliably support our growth curve. In order to increase feature velocity, we first needed to improve reliability and modernize the legacy frameworks that supported GitHub Actions. </p>
<p><a href="https://github.blog/news-insights/product-news/lets-talk-about-github-actions/">Our solution was to re-architect the core backend services powering GitHub Actions jobs</a> and runners with the goals of improving uptime and resilience against infrastructure issues, enhancing performance, reducing internal throttles, and leveraging GitHub’s broader platform investments and developer experience improvements. This work is paying off by helping us handle our current scale, even as we work through the last pieces of stabilizing our new platform.</p>
<p>Since August, all GitHub Actions jobs have run on our new architecture, which handles 71 million jobs per day (over 3x from where we started). Individual enterprises are able to start 7x more jobs per minute than our previous architecture could support. </p>
<p>As with any product, our goal at GitHub has been to meet customer needs while providing enterprises with flexibility and transparency.</p>
<p>This change better supports a world where CI/CD must be faster and more reliable, better caching, more workflow flexibility, rock-solid reliability, and strengthens the core experience while positioning GitHub Actions to power GitHub’s open, secure platform for agentic workload. </p>
<h2>What’s changing?</h2>
<h3>Lower prices for GitHub-hosted runners</h3>
<p>Starting today, we’re charging fairly for Actions across the board which reduces the price of GitHub Hosted Runners and the price the average GitHub customer pays. And we’re <strong>reducing the net cost of GitHub-hosted runners by up to 39%</strong>, depending on which machine type is used.</p>
<p>This reduction is driven by a ~40% price reduction across all runner sizes, paired with the addition of a new $0.002 per-minute GitHub Actions cloud platform charge. For GitHub-hosted runners, the new Actions cloud platform charge is <strong>already included into the reduced meter price.</strong> </p>
<p>Standard GitHub-hosted or self-hosted runner usage on <strong>public repositories will remain free</strong>. <strong>GitHub Enterprise Server pricing is not impacted</strong> by this change.</p>
<p>The price reduction you will see in your account depends on the types of machines that you use most frequently – smaller runners will have a smaller relative price reduction, larger runners will see a larger relative reduction.</p>
<p>This price reduction makes high-performance compute more accessible for both high-volume CI workloads and the agent jobs that rely on fast, secure execution environments.</p>
<p>For full pricing update details, see the updated Actions runner prices <a href="https://docs.github.com/billing/reference/actions-runner-pricing">in our documentation</a>.</p>
<p>This price change will <strong>go into effect on January 1, 2026</strong>.</p>
<h3>Introduction of the GitHub Actions cloud platform charge</h3>
<p>We are introducing a <strong>$0.002 per-minute Actions cloud platform</strong> <strong>charge</strong> for all Actions workflows across GitHub-hosted and self-hosted runners. The new listed GitHub-runner rates include this charge. This will not impact Actions usage in public repositories or GitHub Enterprise Server customers.  </p>
<p>This aligns pricing to match consumption patterns and ensures consistent service quality as usage grows across both hosting modalities.</p>
<p>This will impact <strong>self-hosted runner pricing starting March 1, 2026.</strong></p>
<h3>Deepened investment in the Actions self-hosted experience</h3>
<p>We are increasing our investment into our self-hosted experience to ensure that we can provide autoscaling for scenarios beyond just Linux containers. This will include new approaches to scaling, new platform support, Windows support, and more as we move through the next 12 months. Here’s a preview of what to expect in the new year:</p>
<h4>GitHub Scale Set Client</h4>
<p>This <a href="https://github.com/github/roadmap/issues/1192">new client</a> provides enterprises with a lightweight Go SDK to build custom autoscaling solutions without the complexity of Kubernetes or reliance on ARC. It integrates seamlessly with existing infrastructure—containers, virtual machines, cloud instances, or bare metal—while managing job queuing, secure configuration, and intelligent scaling logic. Customers gain a supported path to implement flexible autoscaling, reduce setup friction, and extend GitHub Actions beyond workflows to scenarios such as self-hosted Dependabot and Copilot Coding Agent.</p>
<h4>Multi-label support</h4>
<p>We are reintroducing <a href="https://github.com/github/roadmap/issues/1195">multi-label functionality</a> for both GitHub-hosted larger runners and self-hosted runners, including those managed by Actions Runner Controller (ARC) and the new Scale Set Client.</p>
<h4>Actions Runner Controller 0.14.0</h4>
<p><a href="https://github.com/github/roadmap/issues/1194">This upcoming release</a> introduces major quality-of-life improvements, including refined Helm charts for easier Docker configuration, enhanced logging, updated metrics, and formalized versioning requirements. It also announces the deprecation of legacy ARC, providing a clear migration path to a more reliable and maintainable architecture. Customers benefit from simplified setup, improved observability, and confidence in long-term support, reducing operational friction and improving scalability.</p>
<h4>Actions Data Stream</h4>
<p>The <a href="https://github.com/github/roadmap/issues/1193">Actions Data Stream</a> will deliver a near real-time, authoritative feed of GitHub Actions workflow and job event data, including metadata such as the version of the action that was executed on any given workflow run. This capability enhances observability and troubleshooting by enabling organizations to integrate event data into monitoring and analytics systems for compliance and operational insights. By providing structured, high-fidelity data at scale, it eliminates reliance on manual log parsing and empowers teams to proactively manage reliability and performance.</p>
<h2>Why this matters</h2>
<p>Agents are expanding what teams can automate—but CI/CD remains the heartbeat of modern software delivery. These updates enable both a faster, more reliable CI/CD experience for every developer, and a scalable, flexible, secure execution layer to power GitHub’s agentic platform.</p>
<p>Our goal is to ensure GitHub Actions continues to meet the needs of the largest enterprises and of individual developers alike, with clear pricing, stronger performance, and a product direction built for the next decade of software development.</p>
<h2>FAQ</h2>
<p><strong>What are the new GitHub-hosted runner rates?</strong><br>See the GitHub Actions <a href="https://docs.github.com/billing/reference/actions-runner-pricing">runner pricing reference</a> for the updated rates that will go into effect on January 1, 2026. These listed rates include the new $0.002 per-minute Actions cloud platform charge.</p>
<p><strong>Which job execution scenarios for GitHub Actions are affected by this pricing change?</strong></p>
<ul>
<li>Jobs that run in private repositories and use standard GitHub-hosted or self-hosted runners  </li>
<li>Any jobs running on larger GitHub-hosted runners</li>
</ul>
<p>Standard GitHub-hosted or self-hosted runner usage on public repositories will remain free. GitHub Enterprise Server pricing is not impacted by this change.</p>
<p><strong>When will this pricing change take effect?</strong></p>
<p>The price decrease for GitHub-hosted runners will take effect on January 1, 2026. The new charge for self-hosted runners will apply beginning on March 1, 2026. The price changes will impact all customers on these dates.</p>
<p><strong>Will the free usage quota available in my plan change?</strong><br>Beginning March 1, 2026, self-hosted runners will be included within your free usage quota, and will consume available usage based on list price the same way that Linux, Windows, and MacOS standard runners work today.</p>
<p><strong>Will self-hosted runner usage consume from my free usage minutes?</strong><br>Yes, billable self-hosted runner usage will be able to consume minutes from the free quota associated with your plan.</p>
<p><strong>How does this pricing change affect customers on GitHub Enterprise Server?</strong></p>
<p>This pricing change does not affect customers using GitHub Enterprise Server. Customers running Actions jobs on self-hosted runners on GitHub Enterprise Server may continue to host, manage, troubleshoot and use Actions on and in conjunction with their implementation free of charge. </p>
<p><strong>Can I bill my self-hosted runner usage on private repositories through Azure?</strong></p>
<p>Yes, as long as you have an active Azure subscription ID associated with your GitHub Enterprise or Organization(s).</p>
<p><strong>What is the overall impact of this change to GitHub customers?</strong></p>
<p>Of Actions users impacted by this change, 85% will see their Actions bill decrease. Of the 15% who are impacted across all cohorts the median increase is $13. </p>
<p><strong>Did GitHub consider how this impacts individual developers, not just Enterprise scale customers of GitHub?</strong><br>From our individual users (free &amp; Pro plans) of those who used GitHub Actions in the last month in private repos only 0.09% would end up with a price increase, with a median increase of under $2 a month. Note that this impact is after these users have made use of their included minutes in their plans today, entitling them to over 33 hours of included GitHub compute, and this has no impact on their free use of public repos. A further 2.8% of this total user base will see a decrease in their monthly cost as a result of these changes. The rest are unimpacted by this change.</p>
<p><strong>How can I figure out what my new monthly cost for Actions looks like?</strong></p>
<p>GitHub Actions provides <a href="https://docs.github.com/billing/how-tos/products/view-productlicense-use">detailed usage reports for the current and prior year</a>. You can use this prior usage alongside the <a href="https://docs.github.com/billing/reference/actions-runner-pricing">rate changes</a> that will be introduced in January and March to estimate cost under the new pricing structure. We have created a <a href="https://docs.github.com/billing/tutorials/estimate-actions-costs">Python script</a> to help you leverage <a href="https://docs.github.com/billing/how-tos/products/view-productlicense-use#downloading-usage-reports">full usage reports</a> to calculate your expected cost after the price updates.</p>
<p>We have also updated our <a href="https://github.com/pricing/calculator#actions">Actions pricing calculator</a>, making it easier to estimate your future costs, particularly if your historical usage is limited or not representative of expected future usage.</p>
<h2>Additional resources</h2>
<ul>
<li>See the <a href="https://docs.github.com/billing/reference/actions-runner-pricing">GitHub Actions runner pricing documentation</a> for the new GitHub-hosted runner rates effective January 1, 2026.  </li>
<li>For more details on upcoming GitHub Actions releases, see the <a href="https://github.com/orgs/github/projects/4247">GitHub public roadmap</a>.  </li>
<li>For help estimating your expected Actions usage cost, use the newly updated <a href="https://github.com/pricing/calculator#actions">Actions pricing calculator</a>.  </li>
<li>To see your current or historical Actions usage, see our documentation for <a href="https://docs.github.com/billing/how-tos/products/view-productlicense-use">viewing and downloading detailed usage reports</a>.  </li>
<li>If you are interested in moving existing self-hosted runner usage to GitHub-hosted runners, see the <a href="http://docs.github.com/actions/tutorials/migrate-to-github-runners">SHR to GHR migration guide</a> in our documentation.</li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Track Surveillance (Flock Cameras) Tech in Local Government Meetings (653 pts)]]></title>
            <link>https://alpr.watch/</link>
            <guid>46290916</guid>
            <pubDate>Tue, 16 Dec 2025 16:54:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alpr.watch/">https://alpr.watch/</a>, See on <a href="https://news.ycombinator.com/item?id=46290916">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
        ✓ Link copied! Share this meeting with others.
    </p><div>
        <div>
            <div>
                
                <p>Your local government might be discussing surveillance tech like Flock cameras, facial recognition, or automated license plate readers right now. This map helps you find those meetings and take action.</p>
            </div>
            
            <p><span>Beautiful</span>
                <span>Unethical</span>
                <span>Dangerous</span>
            </p>
            
        </div>

        <div>
            <p><strong>Why this matters:</strong> &nbsp;Municipalities across the US are quietly adopting surveillance technologies in rapidly growing numbers with over 80,000 cameras already out on the streets. These systems track residents' movements, collect biometric data, and build massive databases of our daily lives.</p>
            <p>alpr.watch scans meeting agendas for keywords like "flock," "license plate reader," "alpr," and more. Each pin on the map shows where these conversations are happening so that you can make a difference.</p>
        </div>

        <div>
            <div>
                    <p><label>
                            
                            <span></span>
                        </label>
                        <span>Show only upcoming meetings</span>
                    </p>
                </div>

            <div>
                    <p><span>Get Email Alerts for Your Area</span>
                        <span>▼</span>
                    </p>
                    <div id="notifContent">
                            <p>Enter your email below and we'll send you a login link. After logging in, you can set your notification preferences.</p>
                            <p><label for="email">Your Email</label>
                                
                            </p>
                            
                            <p>
                                ✓ Check your email for a login link!
                            </p>
                            <p>
                                ✗ Please enter a valid email address.
                            </p>
                        </div>
                </div>
        </div>

        <div>
            <div id="loadingOverlay">
                
                <p>Loading map data</p>
                <p>Fetching meetings...</p>
            </div>
            
            <p>
                🔍 Zoom in to see ALPR surveillance cameras
            </p>
        </div>
        
        <p>
            Data before mid-December may be unverified. All future submissions are 100% moderator approved.
        </p>

        <div>
                <h3>Statistics</h3>
                <div>
                    <p><span id="stat-municipalities">--</span>
                        <span>Local Councils Monitored</span>
                    </p>
                    <p><span id="stat-meetings">--</span>
                        <span>Meetings Indexed</span>
                    </p>
                    <p><span id="stat-population">--</span>
                        <span>Cameras Mapped</span>
                    </p>
                </div>
            </div>

        <div>
            <h2>Understanding Mass Surveillance</h2>
            
            <div>
                <div>
                    <h3>What is ALPR?</h3>
                    <p><strong>Automated License Plate Recognition (ALPR)</strong> systems use cameras and artificial intelligence to capture, read, and store license plate data from every passing vehicle.</p>
                    <p>These systems work 24/7 creating a massive database of where vehicles, and by extension, people, travel. Every trip to the grocery store, doctor's office, or place of worship gets recorded and stored.</p>
                </div>

                <div>
                    <h3>What is Flock Safety?</h3>
                    <p><strong>Flock Safety</strong> is one of the largest manufacturers of ALPR cameras in the United States, marketing their systems to neighborhoods and law enforcement.</p>
                    <p>Flock cameras capture license plates, vehicle make/model, color, and other identifying features. This data is shared across a massive network of agencies and jurisdictions, creating a surveillance web that tracks millions of Americans.</p>
                </div>


                <div>
                    <h3>The Slippery Slope</h3>
                    <p>History shows that surveillance systems expand beyond their original scope:</p>
                    <ul>
                        <li>Systems marketed for "solving crimes" get used for immigration enforcement</li>
                        <li>Temporary programs become permanent infrastructure</li>
                        <li>Data sharing agreements grow to include more agencies</li>
                        <li>Technology advances enable new invasive uses</li>
                        <li>Regulations and oversight consistently lag behind deployment</li>
                    </ul>
                </div>
            </div>
        </div>

        <div>
                <h3>Organizations Fighting for Your Privacy</h3>
                <p>These groups and individuals are leading the fight against mass surveillance. Consider supporting their work or getting involved locally.</p>
                
                <div>
                    <div>
                        <p><strong>Electronic Frontier Foundation (EFF)</strong></p><p>Leading nonprofit defending digital privacy and civil liberties. <a href="https://www.eff.org/" target="_blank">eff.org</a></p>
                    </div>
                    
                    <div>
                        <p><strong>ACLU</strong></p><p>Fighting surveillance overreach through litigation and advocacy nationwide. <a href="https://www.aclu.org/" target="_blank">aclu.org</a></p>
                    </div>
                    
                    <div>
                        <p><strong>Fight for the Future</strong></p><p>Digital rights organization mobilizing grassroots opposition to surveillance. <a href="https://www.fightforthefuture.org/" target="_blank">fightforthefuture.org</a></p>
                    </div>
                    
                    <div>
                        <p><strong>Surveillance Technology Oversight Project (STOP)</strong></p><p>Litigating against invasive surveillance in New York and beyond. <a href="https://www.stopspying.org/" target="_blank">stopspying.org</a></p>
                    </div>
                    
                    <div>
                        <p><strong>Institute for Justice</strong></p><p>This civil liberties law firm has filed lawsuits challenging the constitutionality of Flock's mass, warrantless surveillance <a href="https://ij.org/press-release/public-interest-law-firm-responds-to-flock-safety-pausing-federal-access-to-license-plate-reader-cameras/" target="_blank">ij.org</a></p>
                    </div>
                    
                    <div>
                        <p><strong>Local Community Groups</strong></p><p>Check for privacy advocacy organizations in your area fighting surveillance at the local level.</p>
                    </div>
                </div>
            </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mozilla appoints new CEO Anthony Enzor-Demeo (435 pts)]]></title>
            <link>https://blog.mozilla.org/en/mozilla/leadership/mozillas-next-chapter-anthony-enzor-demeo-new-ceo/</link>
            <guid>46288491</guid>
            <pubDate>Tue, 16 Dec 2025 13:53:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.mozilla.org/en/mozilla/leadership/mozillas-next-chapter-anthony-enzor-demeo-new-ceo/">https://blog.mozilla.org/en/mozilla/leadership/mozillas-next-chapter-anthony-enzor-demeo-new-ceo/</a>, See on <a href="https://news.ycombinator.com/item?id=46288491">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
  <main id="main">

    
<article id="post-84084">
  

  <div>
    
<p>Today, I step into the role of CEO of <a href="https://www.mozilla.org/en-US/">Mozilla Corporation</a>. It is a privilege to lead an organization with a long history of standing up for people and building technology that puts them first. The internet is changing fast, and so are the expectations people bring to the products they use every day. Mozilla has a critical role to play at this moment.</p>



<p>I want to thank Laura Chambers for her exceptional leadership. As interim CEO, Laura led Mozilla through a defining moment in the web’s history — navigating AI’s arrival, a major antitrust case, double-digit mobile growth in Firefox, and the early success of our revenue diversification strategy. She brought clarity, stability, and focus to the organization, and I’m grateful for her leadership through this transition and am glad she’ll continue to be part of Mozilla, returning to her role on the Mozilla board of directors.</p>



<p>When I joined Mozilla, it was clear that trust was going to become the defining issue in technology and the browser would be where this battle would play out. AI was already reshaping how people search, shop, and make decisions in ways that were hard to see and even harder to understand. I saw how easily people could lose their footing in experiences that feel personal but operate in ways that are anything but clear. And I knew this would become a defining issue, especially in the browser, where so many decisions about privacy, data, and transparency now originate.&nbsp;</p>



<figure><img decoding="async" width="1024" height="576" src="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/12/Distilled_Quote_Anthony-1024x576.png" alt="" srcset="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/12/Distilled_Quote_Anthony-1024x576.png 1024w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/12/Distilled_Quote_Anthony-300x169.png 300w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/12/Distilled_Quote_Anthony-768x432.png 768w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/12/Distilled_Quote_Anthony-1536x864.png 1536w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/12/Distilled_Quote_Anthony-2048x1152.png 2048w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/12/Distilled_Quote_Anthony-1000x563.png 1000w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/12/Distilled_Quote_Anthony-1280x720.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>People want software that is fast, modern, but also honest about what it does. They want to understand what’s happening and to have real choices.</p>



<p>Mozilla and Firefox can be that choice.</p>



<p>Few companies share our strengths. People trust our brand. Firefox brings us global reach. Our teams know how to build reliable, independent software at scale, and our business model puts the user first.</p>



<p>As Mozilla moves forward, we will focus on becoming the trusted software company. This is not a slogan. It is a direction that guides how we build and how we grow. It means three things.</p>



<ul>
<li>First: Every product we build must give people agency in how it works. Privacy, data use, and AI must be clear and understandable. Controls must be simple. AI should always be a choice — something people can easily turn off. People should know why a feature works the way it does and what value they get from it.</li>



<li>Second: our business model must align with trust. We will grow through transparent monetization that people recognize and value.&nbsp;</li>



<li>Third: Firefox will grow from a browser into a broader ecosystem of trusted software. Firefox will remain our anchor. It will evolve into a modern AI browser and support a portfolio of new and trusted software additions.</li>
</ul>



<p>We will measure our progress against a <a href="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2025/11/Mozilla-Summary-Portfolio-Strategy.pdf">double bottom line</a>. Our work must advance our mission and succeed in the market. In the next three years, that means investing in AI that reflects the Mozilla Manifesto. It means diversifying revenue beyond search.&nbsp;</p>



<p>Success means Firefox grows across generations. Mozilla builds new revenue engines. Our principles become a differentiator.</p>



<p>We will move with urgency. AI is changing software. Browsers are becoming the control point for digital life. Regulation is shifting defaults. These shifts play to Mozilla’s strengths.</p>



<p>If we stay focused, Mozilla will grow in relevance and resilience. Firefox will reach new audiences. Our portfolio will strengthen our independence. Our approach to building trusted software will set a high standard for the industry.</p>



<p>Mozilla is ready for this moment. I am excited for the work ahead and grateful for the trust placed in me.</p>
  </div>

</article><!-- #post-84084 -->

  </main><!-- #main -->
  

<div id="related-articles">
    <h2>Related Articles</h2>
    
  </div>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[40 percent of fMRI signals do not correspond to actual brain activity (401 pts)]]></title>
            <link>https://www.tum.de/en/news-and-events/all-news/press-releases/details/40-percent-of-mri-signals-do-not-correspond-to-actual-brain-activity</link>
            <guid>46288415</guid>
            <pubDate>Tue, 16 Dec 2025 13:46:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tum.de/en/news-and-events/all-news/press-releases/details/40-percent-of-mri-signals-do-not-correspond-to-actual-brain-activity">https://www.tum.de/en/news-and-events/all-news/press-releases/details/40-percent-of-mri-signals-do-not-correspond-to-actual-brain-activity</a>, See on <a href="https://news.ycombinator.com/item?id=46288415">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	
	
		<p>For almost three decades, functional magnetic resonance imaging (fMRI) has been one of the main tools in brain research. Yet a new study published in the renowned journal Nature Neuroscience fundamentally challenges the way fMRI data have so far been interpreted with regard to neuronal activity. According to the findings, there is no generally valid coupling between the oxygen content measured by MRI and neuronal activity.</p>
	

	
		<div>
			
<figure>
	<div>
		
				
						



	





<picture>
	
			<source media="(min-width: 43.75em)" srcset="https://www.tum.de/fileadmin/user_upload_87/_processed_/7/c/csm_Foto_Riedl_Epp_Gabriel_Castrillon_-_quer_e915521e41.webp" type="image/webp">
			<source media="(min-width: 31.25em)" srcset="https://www.tum.de/fileadmin/user_upload_87/_processed_/7/c/csm_Foto_Riedl_Epp_Gabriel_Castrillon_-_quer_963ad61082.webp" type="image/webp">
			<source srcset="https://www.tum.de/fileadmin/user_upload_87/_processed_/7/c/csm_Foto_Riedl_Epp_Gabriel_Castrillon_-_quer_17bce250c1.webp" type="image/webp">
		

	
			<img loading="lazy" src="https://www.tum.de/fileadmin/user_upload_87/_processed_/7/c/csm_Foto_Riedl_Epp_Gabriel_Castrillon_-_quer_89b836aefe.jpg" width="1920" height="1219" alt="">
		
</picture>

	<p><small>Gabriel Castrillon</small>



					
			

		
	</p></div>

	
		<figcaption>
			Dr. Samira Epp and Prof. Dr. Valentin Riedl
		</figcaption>
	
</figure>


		</div>
	


	<p>Researchers at the Technical University of Munich (TUM) and the Friedrich-Alexander-University Erlangen-Nuremberg (FAU) found that an increased fMRI signal is associated with reduced brain activity in around 40 percent of cases. At the same time, they observed decreased fMRI signals in regions with elevated activity. First author Dr. Samira Epp emphasizes: “This contradicts the long-standing assumption that increased brain activity is always accompanied by an increased blood flow to meet higher oxygen demand. Since tens of thousands of fMRI studies worldwide are based on this assumption, our results could lead to opposite interpretations in many of them.”</p>
</div><div>
			<p><span>
					Publications
				</span>
			</p>
			<p>Samira M. Epp, Gabriel Castrillón, Beijia Yuan, Jessica Andrews-Hanna, Christine Preibisch, Valentin Riedl: BOLD signal changes can oppose oxygen metabolism across the human cortex, published in Nature Neuroscience, December 12, 2025, <a href="https://doi.org/10.1038/s41593-025-02132-9" target="_blank" rel="noreferrer">https://doi.org/10.1038/s41593-025-02132-9</a></p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A brief history of Times New Roman (101 pts)]]></title>
            <link>https://typographyforlawyers.com/a-brief-history-of-times-new-roman.html</link>
            <guid>46288414</guid>
            <pubDate>Tue, 16 Dec 2025 13:46:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://typographyforlawyers.com/a-brief-history-of-times-new-roman.html">https://typographyforlawyers.com/a-brief-history-of-times-new-roman.html</a>, See on <a href="https://news.ycombinator.com/item?id=46288414">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="doc"><topic>A brief history of Times&nbsp;New&nbsp;Roman</topic><p><img src="https://typographyforlawyers.com/images/tally-of-types.jpg"></p><p>Times New Ro­man gets its name from the <em>Times</em> of Lon­don, the British news­pa­per. In 1929, the <em>Times</em> hired ty­pog­ra­pher Stan­ley Mori­son to cre­ate a new text font. Mori­son led the project, su­per­vis­ing Vic­tor Lar­dent, an ad­ver­tis­ing artist for the <em>Times</em>, who drew the <span>letterforms.</span></p><p>Even when new, Times New Ro­man had its crit­ics. In his ty­po­graphic mem­oir, <book>A Tally of Types</book>, Mori­son good-na­turedly imag­ined what William Mor­ris (re­spon­si­ble for the open­ing il­lus­tra­tion in <a href="https://typographyforlawyers.com/page-layout.html">page lay­out</a>) might have said about it:<dquo-open-push></dquo-open-push><dquo-open-pull> “As</dquo-open-pull> a new face it should, by the grace of God and the art of man, have been broad and open, gen­er­ous and am­ple; in­stead, by the vice of Mam­mon and the mis­ery of the ma­chine, it is big­oted and nar­row, mean and <span>puritan.”</span></p><p>Be­cause it was used in a daily news­pa­per, the new font quickly be­came pop­u­lar among print­ers of the day. In the decades since, type­set­ting de­vices have evolved, but Times New Ro­man has al­ways been one of the first fonts avail­able for each new de­vice (in­clud­ing per­sonal com­put­ers). This, in turn, has only in­creased its <span>reach.</span></p><p>Ob­jec­tively, there’s noth­ing wrong with Times New Ro­man. It was de­signed for a news­pa­per, so it’s a bit nar­rower than most text fonts—<strong>es­pe­cially the bold style</strong>. (News­pa­pers pre­fer nar­row fonts be­cause they fit more text per line.) <em>The italic is mediocre.</em> But those aren’t fa­tal flaws. Times New Ro­man is a work­horse font that’s been suc­cess­ful for a <span>reason.</span></p><p>Yet it’s an open ques­tion whether its longevity is at­trib­ut­able to its qual­ity or merely its ubiq­uity. Hel­vetica still in­spires enough af­fec­tion to have been the sub­ject of a 2007 doc­u­men­tary fea­ture. Times New Ro­man, mean­while, has not at­tracted sim­i­lar acts of <span>homage.</span></p><p>Why not? Fame has a dark side. When Times New Ro­man ap­pears in a book, doc­u­ment, or ad­ver­tise­ment, it con­notes ap­a­thy. It says,<dquo-open-push></dquo-open-push><dquo-open-pull> “I</dquo-open-pull> sub­mit­ted to the font of least re­sis­tance.” Times New Ro­man is not a font choice so much as the ab­sence of a font choice, like the black­ness of deep space is not a color. To look at Times New Ro­man is to gaze into the&nbsp;<span>void.</span></p><p>This is how Times New Ro­man ac­crued its rep­u­ta­tion as the de­fault font of the le­gal pro­fes­sion—it’s the de­fault font of every­thing. As a re­sult, many law­yers er­ro­neously as­sume that courts de­mand 12-point Times New Ro­man. In fact, I’ve never found one that does. (But there is one no­table court that for­bids it—see <a href="https://typographyforlawyers.com/court-opinions.html">court opin­ions</a>.) In gen­eral, law­yers keep us­ing it not be­cause they must, but be­cause it’s fa­mil­iar and en­trenched—much like those ob­so­lete <a href="https://typographyforlawyers.com/typewriter-habits.html">type­writer habits</a>.</p><p>If you have a choice about us­ing Times New Ro­man, <strong>please stop</strong>. You have plenty of bet­ter al­ter­na­tives—whether it’s a dif­fer­ent <a href="https://typographyforlawyers.com/system-fonts.html">sys­tem font</a> or one of the many pro­fes­sional fonts shown in this <span>chapter.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[This is not the future (670 pts)]]></title>
            <link>https://blog.mathieui.net/this-is-not-the-future.html</link>
            <guid>46288371</guid>
            <pubDate>Tue, 16 Dec 2025 13:42:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.mathieui.net/this-is-not-the-future.html">https://blog.mathieui.net/this-is-not-the-future.html</a>, See on <a href="https://news.ycombinator.com/item?id=46288371">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<ul>

<li title="2025-11-08T19:15:02+01:00">
          on&nbsp;Sat 08 November 2025
        </li>
</ul>
</div><!-- /.post-info --> <p>I thought about this when reading a mastodon post which commented on a news
where a project adopted a "use Generative AI but disclose it" policy, because
it is "the future" and "people are going to use it anyway".</p>
<p>I find the "this is the future, like it or not" framing particularly disgusting,
and it is somewhat common in tech circles to accept it for most "new"
technologies as if it was backed by evidence.</p>
<p>This post is to underline that <strong>Nothing is inevitable.</strong></p>
<div id="modern-technology-is-abusive">
<h2>Modern technology is abusive.</h2>
<p>A small contingent of power users using niche OSes (like myself) survive by avoiding as much of the tech oligarchs’ world as I can, sure, but overall everything is disgusting, and using FOSS is certainly no silver bullet.</p>
<p>Tech enthusiasts who do not apply critical thinking are even worse, because they get beat up everyday by the things they buy at a premium <em>and they like it</em> because they have this twisted idea of what constitutes progress. This is slowly infusing into the general population, which is also a problem.</p>
<p>People have been trained to be abused by software and by hardware, to ignore their needs, to accept any change as inevitable.
I speak of abuse because people have been trained to <em>expect</em> and <em>accept</em> change at the same time, with no agency whatsoever.</p>
<p>Most old people in particular (sorry mom) have given up and resigned themselves to drift wherever their computing devices take them, because under the guise of convenience, everything is so hostile that there is no point trying to learn things, and dark patterns are everywhere.
Not being in control of course makes people endlessy frustrated, but at the same time trying to wrestle control from the parasites is an uphill battle that they expect to lose, with more frustration as a result.</p>
<p>I want to emphasize here that there are good products (both software and hardware) on the market even though the list gets shorter every year,
some products even manage to solve real problems (!!).
That does not change the fact that consent, hype and projected consumer needs are manufactured by years and years of abuse and marketing campaigns.</p>
<div id="those-things-were-or-are-not-inevitable">
<h3>Those things were or are not inevitable</h3>
<ul>
<li>Internet-connected beds are not inevitable.</li>
<li>AI browsers are not inevitable.</li>
<li>Talking to chatbots instead of public servants is not inevitable.</li>
<li>Requiring a smartphone to exist in society is not inevitable.</li>
<li>Unrepairable devices are not inevitable.</li>
<li>"AI-enhanced" vacation pictures are not inevitable.</li>
<li>NFTs were not inevitable.</li>
<li>The Metaverse was not inevitable.</li>
<li>Your computer changing where things are on every update is not inevitable.</li>
<li>Websites that require your ID are not inevitable.</li>
<li>Garbage companies using refurbished plane engines to power their data centers is not inevitable.</li>
<li>Juicero was not inevitable.</li>
<li>Ads are not inevitable.</li>
<li>Being on a platform owned by Meta is not inevitable.</li>
<li>The Apple Vision pro was not inevitable.</li>
<li>"Copilot PCs" are not inevitable.</li>
<li>Tiktok is not inevitable.</li>
<li>Your computer sending screenshots to microsoft so they can train AIs on it is not inevitable.</li>
</ul>
<p>I could spend years filling this list up, because the tech grifters always find new ways to make us more miserable.</p>
<p>Nothing is inevitable, nothing sold by powerful grifters is "the future" no matter how much they wish that were true.
Sure, some things can keep on existing, even for a very long time, even more if they have an untold number of billions - that they wormed they way into having by selling and exploiting personal data and attention -, but nobody has to be complicit. Some things might even end up existing because they are useful.</p>
<p>But what is important to me is to keep the perspective of what consitutes a <strong>desirable future</strong>, and which actions get us closer or further from that.</p>
<p>Every choice is <em>both</em> a <strong>political statement</strong> and a <strong>tradeoff</strong> based on the energy we can spend on the consequences of that choice.</p>
</div>
</div>
</div><p>
            If you have remarks or suggestions concerning this article, please by all means <a href="https://blog.mathieui.net/pages/about.html" title="contact">contact me</a>.
        </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rust GCC back end: Why and how (170 pts)]]></title>
            <link>https://blog.guillaume-gomez.fr/articles/2025-12-15+Rust+GCC+backend%3A+Why+and+how</link>
            <guid>46288291</guid>
            <pubDate>Tue, 16 Dec 2025 13:33:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.guillaume-gomez.fr/articles/2025-12-15+Rust+GCC+backend%3A+Why+and+how">https://blog.guillaume-gomez.fr/articles/2025-12-15+Rust+GCC+backend%3A+Why+and+how</a>, See on <a href="https://news.ycombinator.com/item?id=46288291">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Whenever you compile using Rust, the compiler goes through different passes and in the end, generated binary code for the target processor. By default, it uses LLVM as backend to generate the binary code, but more backends exist like cranelift and GCC. This post is about how it's possible for one compiler to use different backend to generate binaries, in particular GCC.</p>
<h2><a href="#Passes"><span></span></a>Passes</h2>
<p>Before going into details, we need to describe how compilers actually work. They read source code and convert it internally into a format they can manipulate, commonly called Abstract Syntax Tree (shortened "AST").</p>
<p>However, compilers go through multiple passes, and often each pass has their own AST. Let's take a short and very incomplete example with the Rust compiler passes. We have 4 steps (again, this is simplified!):</p>
<ol>
<li>AST: checks that the syntax is valid</li>
<li>HIR: checks if types are valid</li>
<li>MIR: checks lifetimes and runs borrow-checker</li>
<li>codegen: generate binary code (which also has multiple steps, but not detailed here)</li>
</ol>
<p>Each step generates a new AST with new information if no error was encountered and provides it to the next pass.</p>
<p>Little side-note: If enough people are interested by this topic, I can write a (much) longer explanation of these passes.</p>
<h2><a href="#Backend-vs-front-end"><span></span></a>Backend vs front-end</h2>
<p>So now that we have a high-level idea of Rust compiler passes, what is the difference between "front-end" and "back-end" exactly?</p>
<p>We consider the front-end to be the part handling (high-level non-exhaustive list) code parsing, linting, type-checking and borrow-checking (steps 1 to 3). When all this is done, it means the code is valid and needs to be translated to the target processor instructions set. To do so, we call LLVM/GCC which will translate the Rust compiler AST into assembly code (step 4).</p>
<p>The Rust compiler backends are the bridge between the Rust compiler AST and the actual code generator. They receive the AST and call the LLVM/GCC/... API which will in turn run their passes, optimize and finally generate the assembly code.</p>
<h2><a href="#Why-having-a-GCC-backend"><span></span></a>Why having a GCC backend</h2>
<p>LLVM being much more recent than GCC (2003 vs 1987), a lot of older processors are not supported and will never be. So if you want to write a Rust program on an old platform like Dreamcast, you have no choice to either write your own backend or use the GCC backend (or the <code>gccrs</code> front-end once ready).</p>
<p>For the readers interested in doing so, there is a guide explaining how to build Rust programs for Dreamcast <a href="https://www.dreamcast.rs/">here</a>.</p>
<h2><a href="#gccrs-vs-GCC-backend"><span></span></a>gccrs vs GCC backend</h2>
<p>The GCC backend is different than <a href="https://rust-gcc.github.io/">gccrs</a> which is a front-end for GCC written in C++, which doesn't reuse the front-end of <code>rustc</code>, meaning they need to reimplement parsing, type-checking, linting, borrow-checking, compilation errors, etc.</p>
<p>On the other hand, the GCC backend (the crate name is <code>rustc_codegen_gcc</code>) is just "yet another backend codegen" of the Rust compiler, like LLVM or Cranelift, only meant to generate the binary from the Rust compiler input. It's a bridge between Rust compiler's AST and the codegen API.</p>
<p>On that note: GCC doesn't provide a nice library to give access to its internals (unlike LLVM). So we have to use <code>libgccjit</code> which, unlike the "jit" ("just in time", meaning compiling sub-parts of the code on the fly, only when needed for performance reasons and often used in script languages like Javascript) part in its name implies, can be used as "aot" ("ahead of time", meaning you compile everything at once, allowing you to spend more time on optimization). To do so we use bindings, which are split in two parts:</p>
<ul>
<li><a href="https://crates.io/crates/gccjit_sys"><code>gccjit-sys</code></a> which redeclares the C items we need.</li>
<li><a href="https://crates.io/crates/gccjit"><code>gccjit</code></a> which provides a nice API over <code>gccjit-sys</code>.</li>
</ul>
<p>If you want to write your own compiler and use GCC as codegen, you can do it thanks to <code>libgccjit</code>. And if you write it in Rust, you can even use the Rust bindings.</p>
<h2><a href="#Implementing-a-Rust-backend"><span></span></a>Implementing a Rust backend</h2>
<p>Rustc has a crate named <code>rustc_codegen_ssa</code> which provides an abstract interface that a backend needs to implement through traits like:</p>
<ul>
<li><a href="https://doc.rust-lang.org/nightly/nightly-rustc/rustc_codegen_ssa/traits/trait.CodegenBackend.html">CodegenBackend</a></li>
<li><a href="https://doc.rust-lang.org/nightly/nightly-rustc/rustc_codegen_ssa/traits/trait.ExtraBackendMethods.html">ExtraBackendMethods</a></li>
<li><a href="https://doc.rust-lang.org/nightly/nightly-rustc/rustc_codegen_ssa/traits/trait.WriteBackendMethods.html">WriteBackendMethods</a></li>
</ul>
<p>The full list is available <a href="https://doc.rust-lang.org/nightly/nightly-rustc/rustc_codegen_ssa/traits/index.html">here</a>.</p>
<p>One last thing you need to write in your backend:</p>
<pre><a href="https://play.rust-lang.org/?code=fn+main%28%29+%7B%0A++++%23%5Bno_mangle%5D%0A++++pub+fn+_rustc_codegen_backend%28%29+-%3E+Box%3Cdyn+CodegenBackend%3E+%7B%0A++++++++%2F%2F+This+is+the+entrypoint.%0A++++%7D%0A%7D%0A">Run</a><code>#[no_mangle]
pub fn _rustc_codegen_backend() -&gt; Box&lt;dyn CodegenBackend&gt; {
    // This is the entrypoint.
}</code></pre>
<p>This is the function that will be called by rustc to run your backend.</p>
<h2><a href="#Codegen-implementation-example"><span></span></a>Codegen implementation example</h2>
<p>Let's take an example: how the GCC backend creates a constant string. I picked this one because it's small enough to showcase how things work while not being too much information to digest at once.</p>
<p>In the <a href="https://doc.rust-lang.org/nightly/nightly-rustc/rustc_codegen_ssa/traits/trait.ConstCodegenMethods.html">ConstCodegenMethods</a> trait, there is a <a href="https://doc.rust-lang.org/nightly/nightly-rustc/rustc_codegen_ssa/traits/trait.ConstCodegenMethods.html#tymethod.const_str">const_str</a> method. This is the method we will implement to declare a constant string.</p>
<p>So the method implementation so far looks like this:</p>
<pre><a href="https://play.rust-lang.org/?code=fn+main%28%29+%7B%0A++++impl%3C%27gcc%2C+%27tcx%3E+ConstCodegenMethods+for+CodegenCx%3C%27gcc%2C+%27tcx%3E+%7B%0A++++++++%2F%2F%2F+Returns+the+pointer+to+the+string+and+its+length.%0A++++++++fn+const_str%28%26self%2C+s%3A+%26str%29+-%3E+%28RValue%3C%27gcc%3E%2C+RValue%3C%27gcc%3E%29+%7B%0A++++++++++++%2F%2F+Call+GCC+API+to+declare+this+string.%0A++++++++%7D%0A++++%7D%0A%7D%0A">Run</a><code>impl&lt;'gcc, 'tcx&gt; ConstCodegenMethods for CodegenCx&lt;'gcc, 'tcx&gt; {
    /// Returns the pointer to the string and its length.
    fn const_str(&amp;self, s: &amp;str) -&gt; (RValue&lt;'gcc&gt;, RValue&lt;'gcc&gt;) {
        // Call GCC API to declare this string.
    }
}</code></pre>
<p>We need to pause here to give some extra explanations: <code>CodegenCx</code> is the type on which most <code>rustc_codegen_ssa</code> traits will be implemented. It is created in each <a href="https://doc.rust-lang.org/nightly/nightly-rustc/rustc_codegen_ssa/traits/trait.ExtraBackendMethods.html#tymethod.compile_codegen_unit">ExtraBackendMethods::compile_codegen_unit</a> call and passed down from there to generate the code for this module. You can consider it the same as a cache. It keeps the list of items declared, like functions, types, globals, etc. But also information such as "boolean type", "i8 type" and equivalents so we don't need to recompute them every time we need them.</p>
<p>Ok so now let's actually implement it. We have a few things to do:</p>
<ol>
<li>To avoid adding the same constant string multiple times, we will need to cache them in our context.</li>
<li>We need to cast the Rust str type (<code>*const u8</code>) into the C type (<code>*const char</code>).</li>
<li>Get the pointer to this constant string and return it.</li>
</ol>
<p>Let's translate it into code with a lot of comments to help understanding what's going on:</p>
<pre><a href="https://play.rust-lang.org/?code=fn+main%28%29+%7B%0A++++fn+const_str%28%26self%2C+s%3A+%26str%29+-%3E+%28RValue%3C%27gcc%3E%2C+RValue%3C%27gcc%3E%29+%7B%0A++++++++%2F%2F+We+get+the+const+string+cache.%0A++++++++let+mut+const_str_cache+%3D+self.const_str_cache.borrow_mut%28%29%3B%0A++++++++%2F%2F+We+get+the+address+of+the+stored+string+and+we+add+it+to+the+cache+and%0A++++++++%2F%2F+return+its+address.%0A++++++++let+str_global+%3D+const_str_cache.get%28s%29.copied%28%29.unwrap_or_else%28%7C%7C+%7B%0A++++++++++++%2F%2F+We+call+the+%60GCC%60+API+to+create+a+new+const+string.%0A++++++++++++let+string+%3D+self.context.new_string_literal%28s%29%3B%0A++++++++++++%2F%2F+We+name+the+const.%0A++++++++++++let+sym+%3D+self.generate_local_symbol_name%28%22str%22%29%3B%0A++++++++++++%2F%2F+We+declare+it.%0A++++++++++++let+global+%3D+self.declare_private_global%28%26sym%2C+self.val_ty%28string%29%29%3B%0A++++++++++++%2F%2F+All+done%2C+we+can+add+it+to+the+cache+and+return+it.%0A++++++++++++const_str_cache.insert%28s.to_owned%28%29%2C+global%29%3B%0A++++++++++++global%0A++++++++%7D%29%3B%0A++++++++let+len+%3D+s.len%28%29%3B%0A++++++++%2F%2F+We+cast+the+pointer+to+the+target+architecture+string+pointer+type.%0A++++++++let+cs+%3D+self.const_ptrcast%28%0A++++++++++++str_global.get_address%28None%29%2C%0A++++++++++++self.type_ptr_to%28self.layout_of%28self.tcx.types.str_%29.gcc_type%28self%29%29%2C%0A++++++++%29%3B%0A++++++++%2F%2F+And+we+return+the+pointer+and+its+length.%0A++++++++%28cs%2C+self.const_usize%28len+as+_%29%29%0A++++%7D%0A%7D%0A">Run</a><code>fn const_str(&amp;self, s: &amp;str) -&gt; (RValue&lt;'gcc&gt;, RValue&lt;'gcc&gt;) {
    // We get the const string cache.
    let mut const_str_cache = self.const_str_cache.borrow_mut();
    // We get the address of the stored string and we add it to the cache and
    // return its address.
    let str_global = const_str_cache.get(s).copied().unwrap_or_else(|| {
        // We call the `GCC` API to create a new const string.
        let string = self.context.new_string_literal(s);
        // We name the const.
        let sym = self.generate_local_symbol_name("str");
        // We declare it.
        let global = self.declare_private_global(&amp;sym, self.val_ty(string));
        // All done, we can add it to the cache and return it.
        const_str_cache.insert(s.to_owned(), global);
        global
    });
    let len = s.len();
    // We cast the pointer to the target architecture string pointer type.
    let cs = self.const_ptrcast(
        str_global.get_address(None),
        self.type_ptr_to(self.layout_of(self.tcx.types.str_).gcc_type(self)),
    );
    // And we return the pointer and its length.
    (cs, self.const_usize(len as _))
}</code></pre>
<p>But the codegen backends can also add more information to the underlying binary code generator. For example, in Rust, we use references a lot. A reference is basically a pointer that cannot be <code>NULL</code>. We need to give this information as well!</p>
<p>In both GCC and LLVM, you can add attributes to a lot of items, like arguments of functions. So every time we see an argument behind a reference, we add the <code>nonnnull()</code> attribute.</p>
<p>Let's show an example with this Rust function:</p>
<pre><a href="https://play.rust-lang.org/?code=fn+main%28%29+%7B%0A++++fn+t%28a%3A+%26i32%29+-%3E+i32+%7B%0A++++++++%2Aa%0A++++%7D%0A%7D%0A">Run</a><code>fn t(a: &amp;i32) -&gt; i32 {
    *a
}</code></pre>
<p>The C equivalent looks like this:</p>
<pre><code>int t(int *a) {
  if (!a) {
    return -1;
  }
  return *a;
}</code></pre>
<p>Compiled with the <code>-O3</code> option, it generates this assembly:</p>
<pre><code>t:
        test    rdi, rdi              ; Check if `a` is 0
        je      .L5                   ; If `a` is 0, we jump to `.L1`
        mov     eax, DWORD PTR [rdi]  ; We store `*a` value into `eax` registry
        ret                           ; We exit the function
.L5:
        mov     eax, -1               ; We store `-1` into `eax` registry
        ret                           ; We exit</code></pre>
<p>However, the Rust compiler knows that <code>a</code> can never be <code>NULL</code>, so the codegen adds <code>_attribute_((nonnull(1)))</code> on the function:</p>
<pre><code>_attribute_((nonnull(1)))
int t(int *a) {
  if (!a) {
    return -1;
  }
  return *a;
}</code></pre>
<p>Which generates this assembly:</p>
<pre><code>t:
        mov     eax, DWORD PTR [rdi]
        ret</code></pre>
<p>Since the codegen knows that the <code>if (!a)</code> condition will never be true, why keeping it around?</p>
<p>And it's just one example of extra information/optimization we do in the Rust backends. And that doesn't even cover in the slighest the monstruous amount of optimizations the codegen themselves do. If you want to have more examples of such optimizations, I strongly recommend reading the <a href="https://xania.org/AoCO2025">"Advent of Compiler Optimizations"</a> blog posts written by Matt Godbolt (the developer of <a href="https://godbolt.org/">godbolt.org</a>, another priceless tool).</p>
<h2><a href="#Words-of-the-end"><span></span></a>Words of the end</h2>
<p>So now you know what a Rust backend is, and why GCC backend is also an interesting thing to have while also learning about some optimizations we do behind developers back. :)</p>
<p>This blog post was made thanks to my cat hanging to it.</p>
<p><img id="1" onclick="click_img(1)" src="https://blog.guillaume-gomez.fr/blog/images/cat-in-tree.jpg" alt="my cat in a tree, hanging on a branch"></p><p><a href="https://blog.guillaume-gomez.fr/rss" data-tooltip="RSS feed"><img height="35" src="https://blog.guillaume-gomez.fr/blog/feed.svg" alt="RSS feed"></a><a href="https://blog.guillaume-gomez.fr/atom" data-tooltip="Atom feed"><img height="35" src="https://blog.guillaume-gomez.fr/blog/atom-feed.svg" alt="RSS feed"></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sega Channel: VGHF Recovers over 100 Sega Channel ROMs (and More) (228 pts)]]></title>
            <link>https://gamehistory.org/segachannel/</link>
            <guid>46288024</guid>
            <pubDate>Tue, 16 Dec 2025 13:07:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gamehistory.org/segachannel/">https://gamehistory.org/segachannel/</a>, See on <a href="https://news.ycombinator.com/item?id=46288024">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-content">

	
<article id="post-29279">
	
	<!-- .cover-header -->

	<div id="post-inner">

		
<p>Sega broke ground in the late 90s with one of the first digital game distribution systems for consoles. Sega Channel offered access to a rotating library of Sega Genesis titles, along with game tips, demos, and even a few exclusive games that never came out in the United States in any other format. In an era of dial-up internet, Sega Channel delivered game data over television cable — a novel approach that gave the service its name.</p>



<p>In the years since, Sega Channel has been shrouded in a bit of mystery. The service was discontinued in 1998, and the lack of retrievable game data and documentation around Sega Channel has led to decades of speculation about it. We’ve mostly been left with magazine articles and second-hand accounts. Once in a while, one or two Sega Channel ROMs will show up online. How do you <em>preserve</em> a service like Sega Channel?</p>



<p>For the last two years, we’ve been working on a large-scale project to preserve the history of Sega Channel. Today, we unveiled our findings in a new YouTube video.</p>



<figure><p>
<iframe title="Don't Just Watch TV: The Secrets of Sega Channel" width="800" height="450" src="https://www.youtube.com/embed/CWCUmTTVjMY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>



<p>We’ll cut to the chase: In collaboration with multiple parties, we have recovered <strong>over 100 new Sega Channel ROMs</strong>, including system data, exclusive games, and even prototypes that were never published. We’ve also digitized internal paperwork and correspondence that reveals how Sega Channel operated, how it was marketed, and what would’ve come next for the service.</p>



<hr>


<div>
<figure><img fetchpriority="high" decoding="async" width="169" height="300" src="https://gamehistory.org/wp-content/uploads/2025/12/PXL_20240320_191933296-169x300.jpg" alt="" srcset="https://gamehistory.org/wp-content/uploads/2025/12/PXL_20240320_191933296-169x300.jpg 169w, https://gamehistory.org/wp-content/uploads/2025/12/PXL_20240320_191933296-576x1024.jpg 576w, https://gamehistory.org/wp-content/uploads/2025/12/PXL_20240320_191933296-740x1316.jpg 740w, https://gamehistory.org/wp-content/uploads/2025/12/PXL_20240320_191933296-768x1365.jpg 768w, https://gamehistory.org/wp-content/uploads/2025/12/PXL_20240320_191933296-50x89.jpg 50w, https://gamehistory.org/wp-content/uploads/2025/12/PXL_20240320_191933296-864x1536.jpg 864w, https://gamehistory.org/wp-content/uploads/2025/12/PXL_20240320_191933296-1152x2048.jpg 1152w, https://gamehistory.org/wp-content/uploads/2025/12/PXL_20240320_191933296-1200x2133.jpg 1200w, https://gamehistory.org/wp-content/uploads/2025/12/PXL_20240320_191933296.jpg 1440w" sizes="(max-width: 169px) 100vw, 169px"><figcaption>Michael Shorrock was surprisingly easygoing about finding a picture of himself in a museum exhibit.</figcaption></figure>
</div>


<p>This project kicked off in 2024, when we met former Sega Channel vice president of programming Michael Shorrock at the Game Developers Expo. <a href="https://www.youtube.com/watch?v=dGl14yIeOsc" target="_blank" rel="noreferrer noopener">Our booth that year</a> highlighted interesting games from outside the traditional game industry, including <em><a href="https://gamehistory.org/where-in-north-dakota-is-carmen-sandiego/" target="_blank" rel="noreferrer noopener">Where in North Dakota is Carmen Sandiego?</a></em>, which our director Frank Cifaldi recovered back in 2016.</p>



<p>By complete coincidence, one of the items we put out was a promotional brochure for Broderbund Software… featuring Michael Shorrock on the cover! We got talking with Michael about our work, and we realized we both wanted to preserve and celebrate the history of Sega Channel.</p>



<p>At the same time this was happening, we were contacted by a community member named Ray (going by the pseudonym Sega Channel Guy). He had been contacting former Sega Channel staff to see if they still had any old swag or had saved things from the company. In the process, he came into possession of a collection of tape backups containing an unquantifiable amount of internal data from Sega Channel… including a significant number of game and system ROMs.</p>



<p>We realized we could put these two threads together! With Michael’s own collection and Ray’s data backups, we could tell a cohesive, wide-ranging story about what Sega Channel <em>was</em> and that was actually distributed through this service.</p>



<p>There are two end products from this process. The first is <a href="https://archive.gamehistory.org/folder/4cdb4e35-216f-46e8-9215-04ab8b6b49c1" target="_blank" rel="noreferrer noopener">the Michael Shorrock collection</a>, a new collection in our digital library. You can view the correspondence, notes, and presentations from Michael Shorrock’s personal collection, which shed light on the formation of Sega Channel and their audience. From these papers, you can also learn about Express Games: an unannounced successor that would have brought Sega’s cable data delivery service to computers and replaced Sega Channel entirely.</p>



<hr>



<p>The other output here is the collection of Sega Channel ROM data. We’ve donated the data from the <strong>144* new ROMs</strong> we recovered to <a href="https://www.gamingalexandria.com/wp/2025/12/sega-channel-prototype-sega-genesis-roms/" target="_blank" rel="noreferrer noopener">the team at Gaming Alexandria</a>, which will be sharing access to the files.</p>



<p>* Our video states that we recovered 142 unique ROMs. However, after uploading the video, we realized we miscounted! There are two additional Sega Channel variant ROM in this collection. The actual total is 144. This does not include the two outliers mentioned in the video, which were <a href="https://www.infochunk.com/schannel/index.html" target="_blank" rel="noreferrer noopener">previously recovered by users on Sonic Retro in November 2024</a> but went mostly unreported.</p>



<p>This collection includes nearly 100 unique system ROMs, covering almost every version of the system that was distributed to consumers from 1994 to mid-1997. This batch also includes system ROM prototypes and some truly unusual experiments, like a <em>Sega Genesis web browser</em> that would’ve delivered compressed, static websites over television cable.</p>







<p>Of great interest to fans, this collection of ROMs also has dozens of previously undumped game variants and Sega Channel exclusives. This includes <em>Garfield: Caught in the Act – The Lost Levels</em> and <em>The Flintstones</em>, two games that were previously believed to be permanently lost and unrecoverable. These are both interesting from a development standpoint; both games appear to have their roots as abandoned projects that were repurposed as Sega Channel-exclusive content.</p>







<p>Also included are the previously unpreserved limited editions of Sega Genesis games. These versions have been cut down to fit within Sega Channel’s filesize limit, sometimes omitting content or splitting the game into multiple parts. We’re not sure anyone is especially eager to play a version of <em>Super Street Fighter II</em> missing half the characters, but we’re glad to have it documented.</p>







<p>With a few exceptions, this recovery project has accounted for <strong>almost all outstanding Sega Channel games</strong>. We believe this also means there are now digital backup copies of <strong>every unique Sega Genesis game released in the United States</strong>.</p>



<hr>



<p>This has been a years-long project that wouldn’t have been possible without support from the broader gaming community. Besides Michael Shorrock and Ray, we want to give special thanks to:</p>



<ul>
<li>Sega Retro, The Cutting Room Floor, and Hidden Palace for documenting everything we’ve known about Sega Channel up to this point.</li>



<li><a href="https://forums.sonicretro.org/threads/more-sega-channel-prototypes-dumped.25935/page-17#post-1084673" target="_blank" rel="noreferrer noopener">RisingFromRuins</a> and Nathan Misner (<a href="https://www.infochunk.com/schannel/index.html" target="_blank" rel="noreferrer noopener">infochunk</a>) for putting all the pieces together to crack the Sega Channel data formats.</li>



<li>Dustin Hubbard (Hubz) from Gaming Alexandria for working with us to share this ROM data.</li>



<li>Rob Curl from the Museum of Art and Digital Entertainment, who flagged us down at GDC to let us know that Michael Shorrock had seen a picture of himself at our booth and brought him over to say hello.</li>
</ul>



<p>We also want to give a special thanks to Chuck Guzis, a long-time expert on data tapes, who digitized Ray’s Sega Channel backups for us in 2024. Chuck’s business <a href="https://web.archive.org/web/20250330034327/https://www.sydex.com/" target="_blank" rel="noreferrer noopener">Sydex</a> was, for a long time, the go-to vendor for working with data tapes, and we’ve used his services in the past.</p>



<p>Shortly before launching this project, we learned that Chuck passed away over the summer. His death leaves a hole in our community and our collective expertise. We know that the gaming community (and specifically the Sega community) will be excited by all this new documentation and data; we hope that their excitement is a testament to what Chuck’s work meant to the digital preservation community.</p>







<hr>



<h3>Complete list of recovered titles</h3>



<p>This is a list of all Sega Channel-specific game data recovered from this project and shared with Gaming Alexandria. This does not include the 97 unique pieces of menu data ROMs and system software that were also recovered.</p>



<details><summary>Game list</summary>
<p><span>Unique Sega Channel exclusive games:</span></p>



<ul>
<li>The Berenstain Bears’ A School Day</li>



<li>BreakThru</li>



<li>The Flintstones</li>



<li>Garfield: Caught in the Act – The Lost Levels</li>



<li>Iron Hammer</li>



<li>Waterworld</li>
</ul>



<p><span>Sega Channel variants:</span></p>



<ul>
<li>The Adventures of Batman and Robin, Test Drive version</li>



<li>Comix Zone, Test Drive version (1)</li>



<li>Comix Zone, Test Drive version (2)</li>



<li>Earthworm Jim, Test Drive version</li>



<li>Earthworm Jim VideoHints (1)</li>



<li>Earthworm Jim VideoHints (2)</li>



<li>The Great Earthworm Jim Race</li>



<li>The Lost World: Jurassic Park, Part A</li>



<li>The Lost World: Jurassic Park, Part B</li>



<li>The Lost World: Jurassic Park, Test Drive version</li>



<li>NCAA Final Four Basketball: Special Edition (1)</li>



<li>NCAA Final Four Basketball: Special Edition (2)</li>



<li>Mortal Kombat 3, Part A</li>



<li>Mortal Kombat 3, Part B</li>



<li>Scholastic’s The Magic School Bus: Space Exploration Game, Test Drive version</li>



<li>Sonic 3D Blast, Part A</li>



<li>Sonic 3D Blast, Part B</li>



<li>Super Street Fighter II: Limited Edition</li>



<li>Triple Play Baseball 96: Special Edition</li>



<li>Virtua Fighter 2, Part A</li>



<li>Virtua Fighter 2, Part B</li>



<li><em>World Series Baseball ’96: Limited Edition*</em></li>



<li>X-Men 2: Clone Wars, Test Drive version</li>
</ul>



<p><span>Prototypes received by Sega Channel:</span></p>



<ul>
<li>Al Unser Jr.’s Road to the Top</li>



<li>Dan Marino Football</li>



<li>Light Crusader</li>



<li>Nick Faldo’s Championship Golf</li>



<li>Popeye in High Seas High-Jinks</li>



<li>Shadows of the Wind</li>



<li>WildSnake</li>



<li>Wrath of the Demon</li>



<li>Yogi Bear [Yogi Bear’s Cartoon Capers]</li>
</ul>



<p><span>Data differences:</span></p>



<ul>
<li>Body Count (US revision)</li>



<li>Maui Mallard in Cold Shadow</li>



<li>Primal Rage</li>



<li>Pulseman</li>



<li><em>Richard Scarry’s Busytown*</em></li>



<li>Shining Force II</li>
</ul>



<p><span>Header differences only:</span></p>



<ul>
<li>Battle Frenzy (US header)</li>



<li>Power Drive (US header)</li>



<li>QuackShot</li>



<li>Super Hang-On</li>



<li>Wacky Worlds Creativity Studio</li>



<li>X-Men 2: Clone Wars</li>
</ul>



<p>* These games were previously found <a href="https://www.infochunk.com/schannel/index.html" target="_blank" rel="noreferrer noopener">on a CD obtained by a user on the Sonic Retro forums in November 2024</a>. However, these ROMs were overshadowed by the recovery of the Sega Channel exclusive games <em>The Chessmaster</em> and <em>Klondike</em> from the same CD. Although our copies of these ROMs are not unique, we included them on this list to make sure their existence doesn’t get lost.</p>
</details>



<h3>A footnote for hardcore Sega fans</h3>



<p>We believe this recovery project accounts for all unique Sega Channel exclusive games. But the most hardcore fans might be wondering: What about <em>Ozone Kid</em>? In a feature article on Sega Channel from <a href="https://archive.gamehistory.org/item/374968d3-ec6d-4e61-a4cb-28758897b930" target="_blank" rel="noreferrer noopener">the June 1995 issue of <em>Electronic Gaming Monthly</em></a> (p.29), <em>Ozone Kid</em> was identified as the first Sega Channel exclusive.</p>



<p>We can confirm that this game was never actually distributed through Sega Channel. According to data recovered by Ray, <em>The Environmental Detective</em> (as it was titled prior to cancellation) was slated for release alongside the Sega Channel test markets, but it was pulled from their programming plans in July 1994.</p>



<p>Reading the between the lines in Sega Channel’s internal project tracking, the game appears to have suffered from a variety of problems over several months. When the game was finally shelved, Sega issued a “partial test report based on items found at the time code was pulled,” suggesting there were still major issues when it was removed from their plans.</p>

		</div><!-- .post-inner -->

	
</article><!-- .post -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ArkhamMirror: Airgapped investigation platform with CIA-style hypothesis testing (134 pts)]]></title>
            <link>https://github.com/mantisfury/ArkhamMirror</link>
            <guid>46286666</guid>
            <pubDate>Tue, 16 Dec 2025 09:51:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mantisfury/ArkhamMirror">https://github.com/mantisfury/ArkhamMirror</a>, See on <a href="https://news.ycombinator.com/item?id=46286666">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mantisfury/ArkhamMirror/blob/main/docs/assets/logo.png"><img src="https://github.com/mantisfury/ArkhamMirror/raw/main/docs/assets/logo.png" width="40" height="40" alt="ArkhamMirror Logo"></a> ArkhamMirror</h2><a id="user-content--arkhammirror" aria-label="Permalink:  ArkhamMirror" href="#-arkhammirror"></a></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mantisfury/ArkhamMirror/blob/main/docs/assets/banner.png"><img src="https://github.com/mantisfury/ArkhamMirror/raw/main/docs/assets/banner.png" alt="ArkhamMirror Banner"></a></p>
<blockquote>
<p dir="auto"><strong>Connect the dots without connecting to the cloud.</strong></p>
</blockquote>
<p dir="auto">ArkhamMirror is an air-gapped, AI-powered investigation platform for journalists and researchers. It runs 100% locally on your machine, turning chaos into order using advanced NLP, Vision AI, and Knowledge Graphs.</p>
<p dir="auto"><a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/fdf2982b9f5d7489dcf44570e714e3a15fce6253e0cc6b5aa61a075aac2ff71b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a>
<a href="https://www.python.org/downloads/" rel="nofollow"><img src="https://camo.githubusercontent.com/93a33cfc2339ec3fa9be792576576fbaafc42b0c7031285662b02f3aca1e1c59/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e31302b2d626c75652e737667" alt="Python 3.10+" data-canonical-src="https://img.shields.io/badge/python-3.10+-blue.svg"></a>
<a href="https://ko-fi.com/arkhammirror" rel="nofollow"><img src="https://camo.githubusercontent.com/06224b100168bb3a0ba553256ca368308546dd9cadfbde24cbfeac553eb70269/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53706f6e736f722d4b6f2d2d66692d726564" alt="Sponsor" data-canonical-src="https://img.shields.io/badge/Sponsor-Ko--fi-red"></a></p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">⚡ Key Features at a Glance</h2><a id="user-content--key-features-at-a-glance" aria-label="Permalink: ⚡ Key Features at a Glance" href="#-key-features-at-a-glance"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Feature</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>🕵️ Local AI</strong></td>
<td>Chat with your data using <strong>Offline RAG</strong> (Retrieval-Augmented Generation).</td>
</tr>
<tr>
<td><strong>🔍 Semantic Search</strong></td>
<td>Find documents by <em>concept</em>, not just exact keywords.</td>
</tr>
<tr>
<td><strong>🕸️ Knowledge Graph</strong></td>
<td>Visualize hidden connections between People, Orgs, and Places.</td>
</tr>
<tr>
<td><strong>⏳ Auto-Timeline</strong></td>
<td>Extract dates and events to reconstruct what happened when.</td>
</tr>
<tr>
<td><strong>📊 Visual Table Extraction</strong></td>
<td>Recover complex financial tables from PDFs/Images using Vision models.</td>
</tr>
<tr>
<td><strong><g-emoji alias="warning">⚠️</g-emoji> Contradiction Detection</strong></td>
<td>Automatically flag conflicting statements across documents.</td>
</tr>
<tr>
<td><strong>🔒 Absolute Privacy</strong></td>
<td>Zero cloud dependencies. Your data never leaves your specialized "Data Silo".</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Getting Started</h2><a id="user-content--getting-started" aria-label="Permalink: 🚀 Getting Started" href="#-getting-started"></a></p>
<p dir="auto">ArkhamMirror includes a <strong>Smart Installer</strong> that sets up Python, Docker, and Database dependencies for you.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows (One-Click)</h3><a id="user-content-windows-one-click" aria-label="Permalink: Windows (One-Click)" href="#windows-one-click"></a></p>
<p dir="auto">Double-click <code>setup.bat</code> and follow the <strong>AI Setup Wizard</strong>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mac / Linux</h3><a id="user-content-mac--linux" aria-label="Permalink: Mac / Linux" href="#mac--linux"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="chmod +x setup.sh
./setup.sh"><pre>chmod +x setup.sh
./setup.sh</pre></div>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">📚 Documentation</h2><a id="user-content--documentation" aria-label="Permalink: 📚 Documentation" href="#-documentation"></a></p>
<p dir="auto">Detailed guides for features and workflows:</p>
<ul dir="auto">
<li><strong><a href="https://github.com/mantisfury/ArkhamMirror/blob/main/docs/index.md#user-guide">User Guide</a></strong>: Full walkthrough of features.</li>
<li><strong><a href="https://github.com/mantisfury/ArkhamMirror/blob/main/docs/user_guide/01-getting-started.md">Installation</a></strong>: Detailed setup instructions.</li>
<li><strong><a href="https://github.com/mantisfury/ArkhamMirror/blob/main/docs/index.md#developer-guide">Developer Guide</a></strong>: Architecture and contributing.</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">🖼️ Gallery</h2><a id="user-content-️-gallery" aria-label="Permalink: 🖼️ Gallery" href="#️-gallery"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mantisfury/ArkhamMirror/blob/main/docs/assets/images/dashboard.png"><img src="https://github.com/mantisfury/ArkhamMirror/raw/main/docs/assets/images/dashboard.png" alt="Dashboard"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Advanced Analysis</h3><a id="user-content-advanced-analysis" aria-label="Permalink: Advanced Analysis" href="#advanced-analysis"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Narrative Reconstruction</th>
<th>Gap Finding</th>
<th>Contradiction Chain</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/mantisfury/ArkhamMirror/blob/main/docs/assets/images/motive.png"><img src="https://github.com/mantisfury/ArkhamMirror/raw/main/docs/assets/images/motive.png" alt="Motive"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/mantisfury/ArkhamMirror/blob/main/docs/assets/images/gaps.png"><img src="https://github.com/mantisfury/ArkhamMirror/raw/main/docs/assets/images/gaps.png" alt="Gaps"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/mantisfury/ArkhamMirror/blob/main/docs/assets/images/weboflies.png"><img src="https://github.com/mantisfury/ArkhamMirror/raw/main/docs/assets/images/weboflies.png" alt="Web of Lies"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Forensics</h3><a id="user-content-forensics" aria-label="Permalink: Forensics" href="#forensics"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Entity Graph</th>
<th>Author Unmasking</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/mantisfury/ArkhamMirror/blob/main/docs/assets/images/graph.png"><img src="https://github.com/mantisfury/ArkhamMirror/raw/main/docs/assets/images/graph.png" alt="Graph"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/mantisfury/ArkhamMirror/blob/main/docs/assets/images/authorunmask.png"><img src="https://github.com/mantisfury/ArkhamMirror/raw/main/docs/assets/images/authorunmask.png" alt="Author Unmask"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">💖 Support the Project</h2><a id="user-content--support-the-project" aria-label="Permalink: 💖 Support the Project" href="#-support-the-project"></a></p>
<p dir="auto">This tool was born from a desire to give journalists powerful forensics without the monthly subscription costs or privacy risks of cloud platforms.</p>
<p dir="auto">If it helps you uncover the truth, consider buying me a coffee!</p>
<p dir="auto"><a href="https://ko-fi.com/arkhammirror" rel="nofollow"><img src="https://camo.githubusercontent.com/201ef269611db7eb6b5d08e9f756ab8980df3014b64492770bdf13a6ed924641/68747470733a2f2f6b6f2d66692e636f6d2f696d672f676974687562627574746f6e5f736d2e737667" alt="Support on Ko-fi" data-canonical-src="https://ko-fi.com/img/githubbutton_sm.svg"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'm a Tech Lead, and nobody listens to me. What should I do? (150 pts)]]></title>
            <link>https://world.hey.com/joaoqalves/i-m-a-tech-lead-and-nobody-listens-to-me-what-should-i-do-e16e454d</link>
            <guid>46286559</guid>
            <pubDate>Tue, 16 Dec 2025 09:38:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://world.hey.com/joaoqalves/i-m-a-tech-lead-and-nobody-listens-to-me-what-should-i-do-e16e454d">https://world.hey.com/joaoqalves/i-m-a-tech-lead-and-nobody-listens-to-me-what-should-i-do-e16e454d</a>, See on <a href="https://news.ycombinator.com/item?id=46286559">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page" id="main-content">
        

<header>
  <a aria-label="All posts from João Alves" href="https://world.hey.com/joaoqalves">
  <img src="https://world.hey.com/joaoqalves/avatar-2f194f659bc641b1c66c0ed5979077ec9ba0e960" size="50x50">

  <p>
    João Alves
  </p>
</a>
</header>

<p>
  December 16, 2025
</p>



  <section>
    <article>
      <div>
  <div><p>In June 2018, I joined mytaxi (<a href="https://www.free-now.com/uk/">FREE NOW</a>), a competitor of Uber in the ride-hailing space, as Backend Chapter Lead. I was looking for an opportunity to grow in technical leadership. Honestly, I did not even fully understand what “Chapter Lead” meant. After some research, I learned it was part of <a href="https://agile-frameworks.com/_spotify/spotify.html">Spotify</a>’s squad (team) and chapter (horizontal domain, such as iOS, Android, Backend, Data, etc.) model, as well as tribes (groups of squads organized around vertical domains, for example, everything related to drivers).</p><p><em>Note</em>: this article is a translation from the original “<a href="https://enespanol.joaoqalves.net/p/soy-tech-lead-y-no-me-hacen-caso">Soy Tech Lead y no me hacen caso. ¿Qué hago?</a>”, in Spanish.</p><figure>
      <a download="Screenshot 2025-12-10 at 20.10.55.png" title="Download Screenshot 2025-12-10 at 20.10.55.png" data-click-proxy-target="lightbox_link_blob_2380165026" href="https://world.hey.com/joaoqalves/e16e454d/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjM4MDE2NTAyNiwicHVyIjoiYmxvYl9pZCJ9fQ--faeb376287e9805a250b9f66538ddbad3f2a57406a2c5e8bd67ac6e9263ff351/Screenshot%202025-12-10%20at%2020.10.55.png?disposition=attachment">
        <img src="https://world.hey.com/joaoqalves/e16e454d/representations/eyJfcmFpbHMiOnsiZGF0YSI6MjM4MDE2NTAyNiwicHVyIjoiYmxvYl9pZCJ9fQ--faeb376287e9805a250b9f66538ddbad3f2a57406a2c5e8bd67ac6e9263ff351/eyJfcmFpbHMiOnsiZGF0YSI6eyJmb3JtYXQiOiJwbmciLCJyZXNpemVfdG9fbGltaXQiOlszODQwLDI1NjBdLCJxdWFsaXR5Ijo2MCwibG9hZGVyIjp7InBhZ2UiOm51bGx9LCJjb2FsZXNjZSI6dHJ1ZX0sInB1ciI6InZhcmlhdGlvbiJ9fQ--7edc7b21f6fad97fa22412618822c4d19725431f296c7ce47dc174b61535d27c/Screenshot%202025-12-10%20at%2020.10.55.png" alt="Screenshot 2025-12-10 at 20.10.55.png" decoding="async" loading="lazy">
</a>
  </figure></div><div><p><em>The squads, tribes, chapters, and guilds model popularized by Spotify. </em><a href="https://agile-frameworks.com/_spotify/spotify.html"><em>Source</em></a><em>.</em></p></div><p>That role had two main components:</p><ul><li><strong>Backend technical leadership</strong> (TL), driven by best practices and with a strong emphasis on continuous improvement. At the time, mytaxi was experiencing major traffic growth. Some services — for example, the one used to incentivize drivers to complete more rides — experienced significant traffic spikes and required improvements, re-architecting, and similar work. On top of that, there were a bit over 200 services to manage.</li><li><strong>People management</strong> in a horizontal setup. The idea was that all backend engineers would report to either <a href="https://www.linkedin.com/in/ariel-cardieri-6971261/">Ariel</a>, the other Chapter Lead, or me, regardless of their team. This was not a very orthodox setup, but at the time, around 3–5 backend engineers were joining every month. There was a strong need to make people productive as quickly as possible, align on architecture, and keep the product moving.</li></ul><div><p>I came in with no prior formal experience as a Tech Lead. I think I never read as much in my life as during the month between announcing I was leaving my previous job and joining mytaxi. Not only that. I had never really had a proper Tech Lead to learn from. What could go wrong?</p><p>---</p></div><div><p>The first day was very entertaining. I log into Slack and introduce myself to the infrastructure and platform manager. He says:</p></div><blockquote>By the way, you have an incident in service X. Could you take a look?</blockquote><div><p>Just like that. It is nine in the morning on a Monday, and we already have an incident. I do not even know where the logs are, Henning. After the initial shock, I managed to find the responsible team. They identified the issue, fixed it, and everything got resolved.</p></div><p>This first interaction made several things very clear to me:</p><ul><li>Nobody really knows how to manage incidents, document what happened, or communicate progress. Great start.</li><li>There is no culture of doing incident reviews or extracting actions to prevent similar incidents in the future.</li><li>There is some coupling between domains. In this case, the Value Added Tax (VAT) concept was applied to two completely different use cases. A change in one of them caused the incident in the other.</li><li>Many people do not know how to debug. They look at me as if the logs were talking to me, or as if I were Harry Potter.</li></ul><div><p>The good part was that there was clearly a lot to fix. I had a pretty clear idea of how engineering culture could be improved. On top of that, I had the Tech Lead title. This was going to be easy. Or maybe not.</p><p>---</p></div><div><p>👋 Hi, João here. This is the opening post of a series designed for Tech Leads and Engineering Managers who want to lead with greater clarity and intention.</p></div><ul><li>“<a href="https://world.hey.com/joaoqalves/traits-of-a-good-tech-lead-b5cac0ae">Traits of a good Tech Lead</a>”</li><li>“I’m a Tech Lead, and nobody listens to me. What should I do?” ← This article</li><li>“KPIs, SLOs, and operational excellence”. Coming soon. Subscribe so you do not miss it.</li><li>To be continued…</li></ul><div><p>I’m currently writing “The Tech Lead Handbook”, scheduled for release in H1 2026. The ideas in this series will form its core.</p></div><h2>Trust</h2><div><p>After that incident, I created an incident review document and suggested a small review of the tasks that should be prioritized to prevent it from happening again. I got carried away and created an initial presentation for the other backend Chapter Leads with a backend strategy. I do not remember it perfectly, but it included hexagonal architecture, a testing pyramid with contract tests to avoid breaking APIs used by mobile apps, and more. Days go by, and I start thinking:</p></div><blockquote>Damn, nobody is listening to me. I put a lot of work into those slides and that strategy.</blockquote><div><p>Today, the reason seems obvious to me. Titles do not grant influence. To influence, you need to build trust. And I had not earned enough of it yet to propose something so fundamental. Through my own experience and through coaching sessions, I have seen this exact mistake repeated several times throughout my career.</p></div><p><em>The trust equation. Generated with Gemini 3 / NanoBanana.</em></p><div><p>The first time I read it, my mind was blown because it described exactly what was happening to me. Let’s break it down:</p></div><ul><li><strong>Credibility</strong>: knowing what you are talking about and having technical judgment. When you say something, people feel it is well-founded. In 2018, I might have had some of this credibility, but it had not yet been proven in that context, with those people, with those systems. I was coming from the outside. Imported credibility is always worth less — unless you come from a FAANG or have built a strong personal brand — than credibility earned on the ground.</li><li><strong>Reliability</strong>: doing what you say you will do. Being consistent and showing up when needed. In a high-paced environment like mytaxi, this matters a lot. In those first days, I was still learning where the logs lived. It is hard to demonstrate reliability if you do not even control the map.</li><li><strong>Intimacy</strong>: people feel they can talk to you, that you will not leave them exposed, and that you understand their fears and doubts. For a TL, this is more important than it seems. Without this, any technical proposal feels like a judgment. And when people get defensive, everything slows down.</li><li>And then there is the denominator: <strong>self-orientation</strong>. When your proposals seem to serve your own agenda more than the team’s needs, trust collapses. That was my mistake. I arrived with a strategy too early, without listening, without seeing what they actually needed, without having earned the moral right to propose it.</li></ul><p>In other words, even if my ideas were good, the equation still did not work out. I had some credibility, a bit of reliability still to build, intimacy yet to be created, and too much self-orientation. The result was obvious. Low trust.</p><p><strong><br>Two key moments<br></strong><br></p><p>Over time, I realized that trust is not built through big speeches, but through concrete actions that solve real, everyday problems. Looking back, two obvious moments accelerated the team’s shift in how they perceived me.</p><p><strong><br>Regulatory complexity<br></strong><br></p><div><p>Because mytaxi competed with Uber in a highly regulated taxi market, with very local regulations across Europe, the application needed to support multiple variants of the same flow. This led to the proliferation of dozens of configuration flags across all services. The result was chaos. Nobody knew for sure what was enabled in each city, what affected iOS, what affected Android, or where each option was actually defined. To make matters worse, the configuration was spread across roughly 200 services.</p></div><div><p>One day, <a href="https://www.linkedin.com/in/mariachec/">Maria</a> — an Agile Coach — talked to me very directly about this pain. I did what I knew best at the time. I built something. I put together a portal — a bit rough, to be honest — that queried the configuration APIs of all services and aggregated that information by functionality, country, or city. Features could be browsed by city or by name. The website was very simple, generated from an HTML template by a Python service.</p><figure>
      <a download="ff-tool-mytaxi-en.png" title="Download ff-tool-mytaxi-en.png" data-click-proxy-target="lightbox_link_blob_2381720738" href="https://world.hey.com/joaoqalves/e16e454d/blobs/eyJfcmFpbHMiOnsiZGF0YSI6MjM4MTcyMDczOCwicHVyIjoiYmxvYl9pZCJ9fQ--0761f5b0a191c12c02353aad23d02bd91dcf2d69af8cd01efea104da80e6abd5/ff-tool-mytaxi-en.png?disposition=attachment">
        <img src="https://world.hey.com/joaoqalves/e16e454d/representations/eyJfcmFpbHMiOnsiZGF0YSI6MjM4MTcyMDczOCwicHVyIjoiYmxvYl9pZCJ9fQ--0761f5b0a191c12c02353aad23d02bd91dcf2d69af8cd01efea104da80e6abd5/eyJfcmFpbHMiOnsiZGF0YSI6eyJmb3JtYXQiOiJwbmciLCJyZXNpemVfdG9fbGltaXQiOlszODQwLDI1NjBdLCJxdWFsaXR5Ijo2MCwibG9hZGVyIjp7InBhZ2UiOm51bGx9LCJjb2FsZXNjZSI6dHJ1ZX0sInB1ciI6InZhcmlhdGlvbiJ9fQ--7edc7b21f6fad97fa22412618822c4d19725431f296c7ce47dc174b61535d27c/ff-tool-mytaxi-en.png" alt="ff-tool-mytaxi-en.png" decoding="async" loading="lazy">
</a>
  </figure><p><em><br>The features could be queried by city or by name. The website was very simple, with HTML generated by a Python service.</em></p></div><div><p>Suddenly, at a glance, anyone could see what was enabled and where. It was not pretty, but it solved a problem. More importantly, it showed that <strong>I was there to help them work better</strong> (credibility), not to impose an abstract technical agenda. Soon after, other teams started using the portal, including Product Owners, QA, and even Operations. Without intending to, it became an organizational alignment tool. And it led to something even more interesting. Other engineers started contributing.</p></div><div><p>Once they saw the value it created, several colleagues proposed improvements, fixed minor bugs, and added features I had never even considered. One of them built a small website to visualize city zones, which solved a long-standing pain for teams working with geofencing or driver-passenger assignment. Another automated part of the flag update process. Someone else added metrics to detect inconsistent configurations across platforms.</p></div><div><p>What started as a quick hack turned into a small ecosystem of internal tools that reduced uncertainty, sped up decisions, and made the team’s life a little easier every week.</p></div><div><p>That domino effect taught me something important. When you solve a real problem and make it visible, people join in. Trust is also built that way, by inviting others to improve what you started and celebrating when they do it better than you.</p></div><p><strong><br>Debugging<br></strong><br></p><div><p>The second moment concerned something much more human. Helping people debug. I have never considered myself especially smart, but I have always been very systematic when connecting error messages, code, hypotheses, and system behavior. To my surprise, many people saw this as almost magical. It was not magic. It was a mix of experience, fundamentals, intuition, knowing where to look, and not being afraid to dive into third-party library code.</p></div><div><p>I started pairing with colleagues during incident resolution (intimacy), teaching them to formulate and discard hypotheses, read logs with intent, and distinguish symptoms from root causes. I proposed incident-review practices that improved the quality of our responses and helped us learn collectively.</p></div><p>Without realizing it, these two contributions did more for my reputation than any presentation or strategy deck. <strong>Building helpful things and standing by people when the system is on fire creates more trust than any title</strong>. That was when my ideas finally started gaining traction. Interestingly, these two actions reduced my self-orientation to zero. I stopped thinking about “my strategy” and started thinking about “our work”.</p><h2>What would you tell your 2018 self?</h2><p>Looking back, one idea stands out. No, TLs don’t earn influence just because “it is their role”. It is earned every day, not through speeches, but by solving painful problems and being present when people need real support.</p><p>If you are in a similar situation, here is some advice I wish I had received in 2018:</p><ul><li>Before proposing a strategy, first understand what actually hurts your team.</li><li>Pick one or two actions that deliver immediate value and execute them.</li><li>Talk less about architecture and more about how your proposal reduces toil, risk, or uncertainty.</li><li>Do not try to prove you are the smartest person in the room. Try to help others do their job better.</li><li>Feedback cycles, unlike code, are slower. They are measured in weeks or months. Be patient.</li><li>And above all, remember that trust is cumulative. It is earned in every interaction.</li></ul><div><p>Technical influence does not start with a title. It begins with the <strong>visible impact</strong> you create. Because when a TL feels unheard, the solution is not to speak louder.</p><p>It is to change the conversation. And to start from the only place you truly control: <strong>your own behavior</strong>.</p><p>---</p><p>🎁 <strong>Want to put this into practice with your team tomorrow? Subscriber-only gift</strong></p></div><div><p>Many Tech Leads feel unheard because EMs, TLs, and the rest of the team operate with different expectations that no one has made explicit. That friction is not resolved with more meetings or more processes. It is determined with clarity. To help you close that gap, I have prepared a FREE alignment toolkit with three practical tools:</p></div><ul><li>For <strong>Tech Leads</strong>: a self-assessment traffic light to fight impostor syndrome and clearly understand where you are creating value and where you are burning out.</li><li>For <strong>Engineering Managers</strong>: an evaluation traffic light to give objective feedback based on behaviors, not gut feelings. Help your Tech Leads have a<strong> </strong>real impact.</li><li>For the <strong>team</strong>: an operational principles template to stop debating the same decisions every week and create shared criteria.</li></ul><div><p>In addition, to complement this article,<strong> I will include a concrete plan for your first 90 days as a Tech Lead</strong>: what to observe, what to prioritize, what to avoid, and how to build trust through small, visible steps. It is the plan I wish I had had during my first week at mytaxi.</p></div><div><p>If you have already downloaded the toolkit, you do not need to do anything. You already have the updated version and will automatically receive the 90-day plan.</p></div><div><p><span>If you are not yet subscribed, </span><strong>subscribe, </strong><a href="https://forms.gle/UBZtSnBMXTgqZYt46"><strong>complete this form</strong></a><strong>, and I’ll send you the kit </strong><span>so you can move from intention to action.</span> It is FREE!</p></div>
</div>

    </article>
  </section>

  



      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A2UI: A Protocol for Agent-Driven Interfaces (142 pts)]]></title>
            <link>https://a2ui.org/</link>
            <guid>46286407</guid>
            <pubDate>Tue, 16 Dec 2025 09:16:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://a2ui.org/">https://a2ui.org/</a>, See on <a href="https://news.ycombinator.com/item?id=46286407">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
          
        
      
      <main data-md-component="main">
        <div data-md-component="content">
              
                

  



  


              
              <article>
                
                  
  



  
  
    
      
    
    <a href="https://github.com/google/A2UI/raw/master/docs/index.md" title="View source of this page">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"></path></svg>
    </a>
  


<!-- markdownlint-disable MD041 -->
<!-- markdownlint-disable MD033 -->
<div>
<!-- Logo for Light Mode (shows dark logo on light background) -->
<p><img src="https://a2ui.org/assets/A2UI_dark.svg" alt="A2UI Logo" width="120"></p>
<!-- Logo for Dark Mode (shows light logo on dark background) -->
<p><img src="https://a2ui.org/assets/A2UI_light.svg" alt="A2UI Logo" width="120"></p>

<p>
A2UI enables AI agents to generate rich, interactive user interfaces that render natively across web, mobile, and desktop—without executing arbitrary code.
</p>
</div>
<div>
<p>️Status: Early Stage Public Preview</p>
<p>A2UI is currently in <strong>v0.8 (Public Preview)</strong>. The specification and
implementations are functional but are still evolving. We are opening the project to
foster collaboration, gather feedback, and solicit contributions (e.g., on client renderers).
Expect changes.</p>
</div>
<h2 id="at-a-glance">At a Glance<a href="#at-a-glance" title="Permanent link">¶</a></h2>
<p>A2UI is currently <a href="https://a2ui.org/specification/v0.8-a2ui/">v0.8</a>,
Apache 2.0 licensed,
created by Google with contributions from CopilotKit and the open source community,
and is in active development <a href="https://github.com/google/A2UI">on GitHub</a>.</p>
<p>The problem A2UI solves is: <strong>how can AI agents safely send rich UIs across trust boundaries?</strong></p>
<p>Instead of text-only responses or risky code execution, A2UI lets agents send <strong>declarative component descriptions</strong> that clients render using their own native widgets. It's like having agents speak a universal UI language.</p>
<p>In this repo you will find
<a href="https://a2ui.org/specification/v0.8-a2ui/">A2UI specifications</a>
and implementations for
<a href="https://a2ui.org/renderers/">renderers</a> (eg: Angular, Flutter, etc.) on the client side,
and <a href="https://a2ui.org/transports.md">transports</a> (eg: A2A, etc.) which communicate A2UI messages between agents and clients.</p>
<div>
<ul>
<li>
<p><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m10 17-4-4 1.41-1.41L10 14.17l6.59-6.59L18 9m-6-8L3 5v6c0 5.55 3.84 10.74 9 12 5.16-1.26 9-6.45 9-12V5z"></path></svg></span> <strong>Secure by Design</strong></p>
<hr>
<p>Declarative data format, not executable code. Agents can only use pre-approved components from your catalog—no UI injection attacks.</p>
</li>
<li>
<p><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m13.13 22.19-1.63-3.83c1.57-.58 3.04-1.36 4.4-2.27zM5.64 12.5l-3.83-1.63 6.1-2.77C7 9.46 6.22 10.93 5.64 12.5M21.61 2.39S16.66.269 11 5.93c-2.19 2.19-3.5 4.6-4.35 6.71-.28.75-.09 1.57.46 2.13l2.13 2.12c.55.56 1.37.74 2.12.46A19.1 19.1 0 0 0 18.07 13c5.66-5.66 3.54-10.61 3.54-10.61m-7.07 7.07c-.78-.78-.78-2.05 0-2.83s2.05-.78 2.83 0c.77.78.78 2.05 0 2.83s-2.05.78-2.83 0m-5.66 7.07-1.41-1.41zM6.24 22l3.64-3.64c-.34-.09-.67-.24-.97-.45L4.83 22zM2 22h1.41l4.77-4.76-1.42-1.41L2 20.59zm0-2.83 4.09-4.08c-.21-.3-.36-.62-.45-.97L2 17.76z"></path></svg></span> <strong>LLM-Friendly</strong></p>
<hr>
<p>Flat, streaming JSON structure designed for easy generation. LLMs can build UIs incrementally without perfect JSON in one shot.</p>
</li>
<li>
<p><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18V4H3c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h4v-2H3zm10 6H9v1.78c-.61.55-1 1.33-1 2.22s.39 1.67 1 2.22V20h4v-1.78c.61-.55 1-1.34 1-2.22s-.39-1.67-1-2.22zm-2 5.5c-.83 0-1.5-.67-1.5-1.5s.67-1.5 1.5-1.5 1.5.67 1.5 1.5-.67 1.5-1.5 1.5M22 8h-6c-.5 0-1 .5-1 1v10c0 .5.5 1 1 1h6c.5 0 1-.5 1-1V9c0-.5-.5-1-1-1m-1 10h-4v-8h4z"></path></svg></span> <strong>Framework-Agnostic</strong></p>
<hr>
<p>One agent response works everywhere. Render the same UI on Angular, Flutter, React, or native mobile with your own styled components.</p>
</li>
<li>
<p><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M2 2h2v18h18v2H2zm5 8h10v3H7zm4 5h10v3H11zM6 4h16v4h-2V6H8v2H6z"></path></svg></span> <strong>Progressive Rendering</strong></p>
<hr>
<p>Stream UI updates as they're generated. Users see the interface building in real-time instead of waiting for complete responses.</p>
</li>
</ul>
</div>
<h2 id="get-started-in-5-minutes">Get Started in 5 Minutes<a href="#get-started-in-5-minutes" title="Permanent link">¶</a></h2>
<div>
<ul>
<li>
<p><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M15 4a8 8 0 0 1 8 8 8 8 0 0 1-8 8 8 8 0 0 1-8-8 8 8 0 0 1 8-8m0 2a6 6 0 0 0-6 6 6 6 0 0 0 6 6 6 6 0 0 0 6-6 6 6 0 0 0-6-6m-1 2h1.5v3.78l2.33 2.33-1.06 1.06L14 12.4zM2 18a1 1 0 0 1-1-1 1 1 0 0 1 1-1h3.83c.31.71.71 1.38 1.17 2zm1-5a1 1 0 0 1-1-1 1 1 0 0 1 1-1h2.05L5 12l.05 1zm1-5a1 1 0 0 1-1-1 1 1 0 0 1 1-1h3c-.46.62-.86 1.29-1.17 2z"></path></svg></span> <strong><a href="https://a2ui.org/quickstart/">Quickstart Guide</a></strong></p>
<hr>
<p>Run the restaurant finder demo and see A2UI in action with Gemini-powered agents.</p>
<p><a href="https://a2ui.org/quickstart/"><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13.22 19.03a.75.75 0 0 1 0-1.06L18.19 13H3.75a.75.75 0 0 1 0-1.5h14.44l-4.97-4.97a.749.749 0 0 1 .326-1.275.75.75 0 0 1 .734.215l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0"></path></svg></span> Get started</a></p>
</li>
<li>
<p><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 21.5c-1.35-.85-3.8-1.5-5.5-1.5-1.65 0-3.35.3-4.75 1.05-.1.05-.15.05-.25.05-.25 0-.5-.25-.5-.5V6c.6-.45 1.25-.75 2-1 1.11-.35 2.33-.5 3.5-.5 1.95 0 4.05.4 5.5 1.5 1.45-1.1 3.55-1.5 5.5-1.5 1.17 0 2.39.15 3.5.5.75.25 1.4.55 2 1v14.6c0 .25-.25.5-.5.5-.1 0-.15 0-.25-.05-1.4-.75-3.1-1.05-4.75-1.05-1.7 0-4.15.65-5.5 1.5M12 8v11.5c1.35-.85 3.8-1.5 5.5-1.5 1.2 0 2.4.15 3.5.5V7c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5m1 3.5c1.11-.68 2.6-1 4.5-1 .91 0 1.76.09 2.5.28V9.23c-.87-.15-1.71-.23-2.5-.23q-2.655 0-4.5.84zm4.5.17c-1.71 0-3.21.26-4.5.79v1.69c1.11-.65 2.6-.99 4.5-.99 1.04 0 1.88.08 2.5.24v-1.5c-.87-.16-1.71-.23-2.5-.23m2.5 2.9c-.87-.16-1.71-.24-2.5-.24-1.83 0-3.33.27-4.5.8v1.69c1.11-.66 2.6-.99 4.5-.99 1.04 0 1.88.08 2.5.24z"></path></svg></span> <strong><a href="https://a2ui.org/concepts/overview/">Core Concepts</a></strong></p>
<hr>
<p>Understand surfaces, components, data binding, and the adjacency list model.</p>
<p><a href="https://a2ui.org/concepts/overview/"><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13.22 19.03a.75.75 0 0 1 0-1.06L18.19 13H3.75a.75.75 0 0 1 0-1.5h14.44l-4.97-4.97a.749.749 0 0 1 .326-1.275.75.75 0 0 1 .734.215l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0"></path></svg></span> Learn concepts</a></p>
</li>
<li>
<p><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8 3a2 2 0 0 0-2 2v4a2 2 0 0 1-2 2H3v2h1a2 2 0 0 1 2 2v4a2 2 0 0 0 2 2h2v-2H8v-5a2 2 0 0 0-2-2 2 2 0 0 0 2-2V5h2V3m6 0a2 2 0 0 1 2 2v4a2 2 0 0 0 2 2h1v2h-1a2 2 0 0 0-2 2v4a2 2 0 0 1-2 2h-2v-2h2v-5a2 2 0 0 1 2-2 2 2 0 0 1-2-2V5h-2V3z"></path></svg></span> <strong><a href="https://a2ui.org/guides/client-setup/">Developer Guides</a></strong></p>
<hr>
<p>Integrate A2UI renderers into your app or build agents that generate UIs.</p>
<p><a href="https://a2ui.org/guides/client-setup/"><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13.22 19.03a.75.75 0 0 1 0-1.06L18.19 13H3.75a.75.75 0 0 1 0-1.5h14.44l-4.97-4.97a.749.749 0 0 1 .326-1.275.75.75 0 0 1 .734.215l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0"></path></svg></span> Start building</a></p>
</li>
<li>
<p><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 9h5.5L13 3.5zM6 2h8l6 6v12a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V4c0-1.11.89-2 2-2m9 16v-2H6v2zm3-4v-2H6v2z"></path></svg></span> <strong><a href="https://a2ui.org/specification/v0.8-a2ui/">Protocol Reference</a></strong></p>
<hr>
<p>Dive into the complete technical specification and message types.</p>
<p><a href="https://a2ui.org/specification/v0.8-a2ui/"><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13.22 19.03a.75.75 0 0 1 0-1.06L18.19 13H3.75a.75.75 0 0 1 0-1.5h14.44l-4.97-4.97a.749.749 0 0 1 .326-1.275.75.75 0 0 1 .734.215l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0"></path></svg></span> Read the spec</a></p>
</li>
</ul>
</div>
<h2 id="how-it-works">How It Works<a href="#how-it-works" title="Permanent link">¶</a></h2>
<ol>
<li><strong>User sends a message</strong> to an AI agent</li>
<li><strong>Agent generates A2UI messages</strong> describing the UI (structure + data)</li>
<li><strong>Messages stream</strong> to the client application</li>
<li><strong>Client renders</strong> using native components (Angular, Flutter, React, etc.)</li>
<li><strong>User interacts</strong> with the UI, sending actions back to the agent</li>
<li><strong>Agent responds</strong> with updated A2UI messages</li>
</ol>
<p><img alt="End-to-End Data Flow" src="https://a2ui.org/assets/end-to-end-data-flow.png"></p>
<h2 id="a2ui-in-action">A2UI in Action<a href="#a2ui-in-action" title="Permanent link">¶</a></h2>
<h3 id="landscape-architect-demo">Landscape Architect Demo<a href="#landscape-architect-demo" title="Permanent link">¶</a></h3>
<div>
  <p>
    <video width="100%" height="auto" controls="" playsinline="">
      <source src="https://a2ui.org/assets/landscape-architect-demo.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  </p>
  <p>
    Watch an agent generate all of the interfaces for a landscape architect application. The user uploads a photo; the agent uses Gemini to understand it and generate a custom form for landscaping needs.
  </p>
</div>

<h3 id="custom-components-interactive-charts-maps">Custom Components: Interactive Charts &amp; Maps<a href="#custom-components-interactive-charts-maps" title="Permanent link">¶</a></h3>
<div>
  <p>
    <video width="100%" height="auto" controls="" playsinline="">
      <source src="https://a2ui.org/assets/a2ui-custom-compnent.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  </p>
  <p>
    Watch an agent chose to respond with a chart component to answer a numberical summary quesiton.  Then the agent chooses a Google Map component to answer a location question.  Both are custom components offered by the client.
  </p>
</div>

<h3 id="a2ui-composer">A2UI Composer<a href="#a2ui-composer" title="Permanent link">¶</a></h3>
<p>CopilotKit has a public <a href="https://go.copilotkit.ai/A2UI-widget-builder">A2UI Widget Builder</a> to try out as well.</p>
<p><a href="https://go.copilotkit.ai/A2UI-widget-builder"><img alt="A2UI Composer" src="https://a2ui.org/assets/A2UI-widget-builder.png"></a></p>







  
  



  


  



                
              </article>
            </div>
        
          
        
      </main>
      
        
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VS Code deactivates IntelliCode in favor of the paid Copilot (204 pts)]]></title>
            <link>https://www.heise.de/en/news/VS-Code-deactivates-IntelliCode-in-favor-of-the-paid-Copilot-11115783.html</link>
            <guid>46286383</guid>
            <pubDate>Tue, 16 Dec 2025 09:12:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.heise.de/en/news/VS-Code-deactivates-IntelliCode-in-favor-of-the-paid-Copilot-11115783.html">https://www.heise.de/en/news/VS-Code-deactivates-IntelliCode-in-favor-of-the-paid-Copilot-11115783.html</a>, See on <a href="https://news.ycombinator.com/item?id=46286383">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

        

        <p>With the release of VS Code 1.107, it became known that Microsoft has deactivated the popular IntelliCode extension, which had over 60 million downloads: The extension is now deprecated and the gray inline suggestions no longer work.</p>
<!-- RSPEAK_STOP -->




  


<!-- RSPEAK_START -->

<p><a href="https://github.com/microsoft/vscode-dotnettools/issues/2537" rel="external noopener" target="_blank">Microsoft refers in the well-hidden announcement</a> from mid-November to the AI extension of Copilot in VS Code, which, however, only offers a free volume of 2,000 suggestions – a limit that developers quickly reach, as Copilot makes a suggestion with every input. From then on, users will need a paid license. The use of IntelliCode required a local model, but was therefore unlimited and free.</p>
<p>The classic IntelliSense with language server for the used language is still free – but without AI support. The following extensions are affected by the shutdown:</p>
<ul><li>IntelliCode</li><li>IntelliCode Completions</li><li>IntelliCode for C# Dev Kit</li><li>IntelliCode API Usage Examples</li></ul>
<!-- RSPEAK_STOP -->

  




<!-- RSPEAK_START -->

<h3 id="nav_typescript_7__0"><strong>TypeScript 7 and more agents for VS Code</strong></h3>
<p>Nothing about IntelliCode can be found in the announcement for VS Code 1.107. However, new is the experimental support <a href="https://github.com/microsoft/typescript-go" rel="external noopener" target="_blank">for TypeScript 7 with the new compiler written in Go</a>. This can be updated with:</p>
<p><code>npm install @typescript/native-preview</code></p>
<!-- RSPEAK_STOP -->


  



  




<!-- RSPEAK_START -->

<p>It is called with</p>
<p><code>npx tsgo</code></p>
<p>instead of <code>tsc</code>. Configuration in VS Code is done with</p>
<p><code>{ "typescript.experimental.useTsgo": true }</code></p>
<p>Further innovations in the editor concern agents, which can now be controlled via the chat. They continue to run even if the user has closed the chat. Developers can also move agents to other environments, enrich them with context, or classify them as sub-agents. <a href="https://code.visualstudio.com/updates/v1_107#_agents" rel="external noopener" target="_blank">The blog speaks militarily</a> of an Agent Head Quarter (HQ).</p>
<!-- RSPEAK_STOP -->
<div data-component="RecommendationBox"><header><h3>Read also</h3></header><a-collapse sneak-peek-elements="3" sneak-peek-elements-selector="article"></a-collapse></div>
<!-- RSPEAK_START -->

<p>

<!-- RSPEAK_STOP -->
<span>(<a href="mailto:who@heise.de" title="Wolf Hosbach">who</a>)</span>
<!-- RSPEAK_START -->
</p>
<div>
    <p>
      Don't miss any news – follow us on
      <a href="https://www.facebook.com/heiseonlineEnglish">Facebook</a>,
      <a href="https://www.linkedin.com/company/104691972">LinkedIn</a> or
      <a href="https://social.heise.de/@heiseonlineenglish">Mastodon</a>.
    </p>
    <p>
      <em>This article was originally published in
      
        <a href="https://www.heise.de/news/VS-Code-deaktiviert-IntelliCode-zugunsten-des-kostenpflichtigen-Copilot-11115668.html">German</a>.
      
      It was translated with technical assistance and editorially reviewed before publication.</em>
    </p>
  </div>



        

        
        <!-- RSPEAK_STOP -->
        

<a-gift has-access="">
    
</a-gift>


        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A linear-time alternative for Dimensionality Reduction and fast visualisation (109 pts)]]></title>
            <link>https://medium.com/@roman.f/a-linear-time-alternative-to-t-sne-for-dimensionality-reduction-and-fast-visualisation-5cd1a7219d6f</link>
            <guid>46285535</guid>
            <pubDate>Tue, 16 Dec 2025 06:47:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/@roman.f/a-linear-time-alternative-to-t-sne-for-dimensionality-reduction-and-fast-visualisation-5cd1a7219d6f">https://medium.com/@roman.f/a-linear-time-alternative-to-t-sne-for-dimensionality-reduction-and-fast-visualisation-5cd1a7219d6f</a>, See on <a href="https://news.ycombinator.com/item?id=46285535">Hacker News</a></p>
Couldn't get https://medium.com/@roman.f/a-linear-time-alternative-to-t-sne-for-dimensionality-reduction-and-fast-visualisation-5cd1a7219d6f: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Children with cancer scammed out of millions fundraised for their treatment (513 pts)]]></title>
            <link>https://www.bbc.com/news/articles/ckgz318y8elo</link>
            <guid>46285376</guid>
            <pubDate>Tue, 16 Dec 2025 06:17:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/ckgz318y8elo">https://www.bbc.com/news/articles/ckgz318y8elo</a>, See on <a href="https://news.ycombinator.com/item?id=46285376">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div data-testid="byline" data-component="byline-block"><p><span data-testid="byline-contributors"><p><span>Simi Jolaoso<!-- -->,</span></p><p><span>Jack Goodman</span><span>and</span></p><p><span>Sarah Buckley<!-- -->,</span><span data-testid="byline-contributors-contributor-2-role-location">BBC Eye Investigations</span></p></span></p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251210-151139-37baf2dfdd-web-2.36.1-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/d92e/live/e946f5f0-d4e0-11f0-a892-01d657345866.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/d92e/live/e946f5f0-d4e0-11f0-a892-01d657345866.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/d92e/live/e946f5f0-d4e0-11f0-a892-01d657345866.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/d92e/live/e946f5f0-d4e0-11f0-a892-01d657345866.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/d92e/live/e946f5f0-d4e0-11f0-a892-01d657345866.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/d92e/live/e946f5f0-d4e0-11f0-a892-01d657345866.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/d92e/live/e946f5f0-d4e0-11f0-a892-01d657345866.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/d92e/live/e946f5f0-d4e0-11f0-a892-01d657345866.jpg.webp" loading="eager" alt="Chance Letikva Khalil, a little Filipino boy, is wearing a green and blue striped t-shirt, has a shaved head, and has a small microphone clipped to his top. There is a white hospital background. He faces the camera, mid-speech."><span>Chance Letikva</span></p></div></figure><p><i id="warning:-disturbing-content"><b id="warning:-disturbing-content">Warning: Disturbing content</b></i></p><div data-component="text-block"><p>A little boy faces the camera. He is pale and has no hair.</p><p>"I am seven years old and I have cancer," he says. "Please save my life and help me."</p><p>Khalil - who is pictured above in a still from the film - didn't want to record this, says his mother Aljin. She had been asked to shave his head, and then a film crew hooked him up to a fake drip, and asked his family to pretend it was his birthday. They had given him a script to learn and recite in English.</p><p>And he didn't like it, says Aljin, when chopped onions were placed next to him, and menthol put under his eyes, to make him cry.</p><p>Aljin agreed to it because, although the set-up was fake, Khalil really did have cancer. She was told this video would help crowdfund money for better treatment. And it did raise funds - $27,000 (£20,204), according to a campaign we found in Khalil's name.</p><p>But Aljin was told the campaign had failed, and says she received none of this money - just a $700 (£524) filming fee on the day. One year later, Khalil died.</p><p>Across the world, desperate parents of sick or dying children are being exploited by online scam campaigns, the BBC World Service has discovered. The public have given money to the campaigns, which claim to be fundraising for life-saving treatment. We have identified 15 families who say they got little to nothing of the funds raised and often had no idea the campaigns had even been published, despite undergoing harrowing filming.</p><p>Nine families we spoke to - whose campaigns appear to be products of the same scam network - say they never received anything at all of the $4m (£2.9m) apparently raised in their names.</p><p>A whistleblower from this network told us they had looked for "beautiful children" who "had to be three to nine years old… without hair".</p><p>We have identified a key player in the scam as an Israeli man living in Canada called Erez Hadari. </p></div><p data-component="caption-block"><figcaption>Watch how three children, including Ana from Colombia, appeared in campaign videos</figcaption></p><div data-component="text-block"><p>Our investigation began in October 2023, when a distressing YouTube advert caught our attention. "I don't want to die," a girl called Alexandra from Ghana sobbed. "My treatments cost a lot."</p><p>A crowdfunding campaign for her appeared to have raised nearly $700,000 (£523,797).</p><p>We saw more videos of sick children from around the world on YouTube, all strikingly similar - slickly produced, and seemingly having raised huge amounts of money. They all conveyed a sense of urgency, using emotive language.</p><p>We decided to investigate further.</p><p>The campaigns with the biggest apparent international reach were under the name of an organisation called Chance Letikva (Chance for Hope, in English) - registered in Israel and the US.</p><p>Identifying the children featured was difficult. We used geolocation, social media and facial recognition software to find their families, based as far apart as Colombia and the Philippines.</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251210-151139-37baf2dfdd-web-2.36.1-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/56fb/live/707275a0-d4f9-11f0-8c06-f5d460985095.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/56fb/live/707275a0-d4f9-11f0-8c06-f5d460985095.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/56fb/live/707275a0-d4f9-11f0-8c06-f5d460985095.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/56fb/live/707275a0-d4f9-11f0-8c06-f5d460985095.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/56fb/live/707275a0-d4f9-11f0-8c06-f5d460985095.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/56fb/live/707275a0-d4f9-11f0-8c06-f5d460985095.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/56fb/live/707275a0-d4f9-11f0-8c06-f5d460985095.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/56fb/live/707275a0-d4f9-11f0-8c06-f5d460985095.jpg.webp" loading="lazy" alt="Chance Letikva A fundraising campaign page for Ana - it shows her crying, wearing a nasal tube, and the caption at the top of the page reads &quot;Two months to live&quot; with a heart emoji "><span>Chance Letikva</span></p></div><p data-component="caption-block"><figcaption>A Chance Letikva campaign for Ana in Colombia - falsely claiming she had two months to live </figcaption></p></figure><div data-component="text-block"><p>While it was difficult to know for sure if the campaign websites' cash totals were genuine, we donated small amounts to two of them and saw the totals increase by those amounts.</p><p>We also spoke to someone who says she gave $180 (£135) to Alexandra's campaign and was then inundated with requests for more, all written as if sent by Alexandra and her father.</p><p>In the Philippines, Aljin Tabasa told us her son Khalil had fallen ill just after his seventh birthday.</p><p>"When we found out it was cancer it felt like my whole world shattered," she says.</p><p>Aljin says treatment at their local hospital in the city of Cebu was slow, and she had messaged everyone she could think of for help. One person put her in touch with a local businessman called Rhoie Yncierto - who asked for a video of Khalil which, looking back, Aljin realises was essentially an audition. </p><p>Another man then arrived from Canada in December 2022, introducing himself as "Erez". He paid her the filming fee up front, she says, promising a further $1,500 (£1,122) a month if the film generated lots of donations.</p><p>Erez directed Khalil's film at a local hospital, asking for retake after retake - the shoot taking 12 hours, Aljin says.</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251210-151139-37baf2dfdd-web-2.36.1-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/05a1/live/b8bb88f0-d4e6-11f0-8c06-f5d460985095.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/05a1/live/b8bb88f0-d4e6-11f0-8c06-f5d460985095.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/05a1/live/b8bb88f0-d4e6-11f0-8c06-f5d460985095.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/05a1/live/b8bb88f0-d4e6-11f0-8c06-f5d460985095.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/05a1/live/b8bb88f0-d4e6-11f0-8c06-f5d460985095.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/05a1/live/b8bb88f0-d4e6-11f0-8c06-f5d460985095.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/05a1/live/b8bb88f0-d4e6-11f0-8c06-f5d460985095.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/05a1/live/b8bb88f0-d4e6-11f0-8c06-f5d460985095.jpg.webp" loading="lazy" alt="A graphic explaining how the campaign video for Khalil was staged shows: 1) His mother and sister clapping as ticker tape rains down with balloons in the background, 2) Khalil crying, 3) Khalil reciting lines from a script, wearing a nasal tube."></p></div></figure><div data-component="text-block"><p>Months later, the family say they had still not heard how the video had performed. Aljin messaged Erez, who told her the video "wasn't successful".</p><p>"So as I understood it, the video just didn't make any money," she says.</p><p>But we told her the campaign had apparently collected $27,000 (£20,204) as of November 2024, and was still online.</p><p>"If I had known the money we had raised, I can't help but think that maybe Khalil would still be here," Aljin says. "I don't understand how they could do this to us."</p><p>When asked about his role in the filming, Rhoie Yncierto denied telling families to shave their children's heads for filming and said he had received no payment for recruiting families.</p><p>He said he had "no control" over what happened with the funds and had no contact with the families after the day of filming. When we told him they had not received any of the campaigns' donations he said he was "puzzled" and was "very sorry for the families".</p><p>Nobody named Erez appears on registration documents for Chance Letikva. But two of its campaigns we investigated had also been promoted by another organisation called Walls of Hope, registered in Israel and Canada. Documents list the director in Canada as Erez Hadari.</p><p>Photos of him online show him at Jewish religious events in the Philippines, New York and Miami. We showed Aljin, and she said it was the same person she had met.</p></div><div data-component="text-block"><ul><li><i id="outside-the-uk,-watch-the-film-on"><b id="outside-the-uk,-watch-the-film-on">Outside the UK, watch the film on </b></i><a target="_blank" href="https://youtu.be/EGj89n7SxOo"><i id="bbc-world-service-youtube"><b id="bbc-world-service-youtube">BBC World Service YouTube</b></i></a></li></ul></div><div data-component="text-block"><p>We asked Mr Hadari about his involvement in a campaign in the Philippines. He did not respond.</p><p>We visited further families whose campaigns were either organised by, or linked to, Mr Hadari - one in a remote indigenous community in Colombia, and another in Ukraine.</p><p>As with Khalil's case, local fixers had got in touch to offer help. The children were filmed and made to cry or fake tears for a nominal fee, but never received any further money.</p><p>In Sucre, north-west Colombia, Sergio Care says he initially refused this help. He had been approached by someone called Isabel, he says, who offered financial assistance after his eight-year-old daughter, Ana, was diagnosed with a malignant brain tumour.</p><p>But Isabel came looking for him at the hospital treating Ana, he says, accompanied by a man who said he worked for an international NGO.</p><p>The description Sergio gave of the man matched that of Erez Hadari - he then recognised him in a photo we showed him.</p><p>"He gave me hope... I didn't have any money for the future."</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251210-151139-37baf2dfdd-web-2.36.1-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/44c1/live/e6eca8e0-d4f9-11f0-8c06-f5d460985095.png.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/44c1/live/e6eca8e0-d4f9-11f0-8c06-f5d460985095.png.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/44c1/live/e6eca8e0-d4f9-11f0-8c06-f5d460985095.png.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/44c1/live/e6eca8e0-d4f9-11f0-8c06-f5d460985095.png.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/44c1/live/e6eca8e0-d4f9-11f0-8c06-f5d460985095.png.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/44c1/live/e6eca8e0-d4f9-11f0-8c06-f5d460985095.png.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/44c1/live/e6eca8e0-d4f9-11f0-8c06-f5d460985095.png.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/44c1/live/e6eca8e0-d4f9-11f0-8c06-f5d460985095.png.webp" loading="lazy" alt="An excerpt from a script given to Ana to learn - it shows stage directions, directing her and her dad on what to wear and how to behave, including tears from Ana. Her dad is given lines telling her that she will get better."></p></div></figure><div data-component="text-block"><p>Demands on the family did not end with the filming.</p><p>Isabel kept ringing, Sergio says, demanding more photos of Ana in hospital. When Sergio didn't reply, Isabel started messaging Ana herself - voice notes we have heard.</p><p>Ana told Isabel she had no more photos to send. Isabel replied: "This is very bad Ana, very bad indeed."</p><p>In January this year, Ana - now fully recovered - tried to find out what happened to the money promised.</p><p>"That foundation disappeared," Isabel told her in a voice note. "Your video was never uploaded. Never. Nothing was done with it, you hear?"</p><p>But we could see the video had been uploaded and, by April 2024, appeared to have raised nearly $250,000 (£187,070).</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251210-151139-37baf2dfdd-web-2.36.1-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/1e77/live/307c0060-d4fe-11f0-8c06-f5d460985095.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/1e77/live/307c0060-d4fe-11f0-8c06-f5d460985095.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/1e77/live/307c0060-d4fe-11f0-8c06-f5d460985095.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/1e77/live/307c0060-d4fe-11f0-8c06-f5d460985095.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/1e77/live/307c0060-d4fe-11f0-8c06-f5d460985095.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/1e77/live/307c0060-d4fe-11f0-8c06-f5d460985095.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/1e77/live/307c0060-d4fe-11f0-8c06-f5d460985095.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/1e77/live/307c0060-d4fe-11f0-8c06-f5d460985095.jpg.webp" loading="lazy" alt="Ana's dad is smiling as he and Ana ride a donkey/horse - white with a straw saddle. Ana is wearing navy joggers and a black Adidas t-shirt, and her dad is wearing a dark shirt and yellow trousers   "></p></div><p data-component="caption-block"><figcaption>Ana and her dad live in a remote indigenous community in Colombia </figcaption></p></figure><div data-component="text-block"><p>In October, we persuaded Isabel Hernandez to speak to us over video link.</p><p>A friend from Israel, she explained, had introduced her to someone offering work for "a foundation" looking to help children with cancer. She refused to name who she worked for.</p><p>She was told only one of the campaigns she helped organise was published, she says, and that it had not been successful.</p><p>We showed Isabel that two campaigns had in fact been uploaded - one of them apparently raising more than $700,000 (£523,797).</p><p>"I need to apologise to [the families]," she said. "If I'd known what was going on, I would not have been able to do something like this."</p><p>In Ukraine, we discovered that the person who approached the mother of a sick child was actually employed in the place where the campaign video was filmed.</p><p>Tetiana Khaliavka organised a shoot with five-year-old Viktoriia, who has brain cancer, at Angelholm Clinic in Chernivtsi.</p><p>One Facebook post linked to Chance Letikva's campaign shows Viktoriia and her mother Olena Firsova, sitting on a bed. "I see your efforts to save my daughter, and it deeply moves us all. It's a race against time to raise the amount needed for Viktoriia's treatments," reads the caption.</p><p>Olena says she never wrote or even said these words and had no idea the campaign had been uploaded.</p><p>It appears to have raised more than €280,000 (£244,000).</p><p>Tetiana, we were told, was in charge of advertising and communications at Angelholm.</p><p>The clinic recently told the BBC it didn't approve filming on its premises - adding: "The clinic has never participated in, nor supported, any fundraising initiatives organised by any organisation."</p><p>Angelholm says it has terminated Tetiana Khaliavka's employment.</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251210-151139-37baf2dfdd-web-2.36.1-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/6bc4/live/9f537080-d4ff-11f0-9fb5-5f3a3703a365.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/6bc4/live/9f537080-d4ff-11f0-9fb5-5f3a3703a365.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/6bc4/live/9f537080-d4ff-11f0-9fb5-5f3a3703a365.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/6bc4/live/9f537080-d4ff-11f0-9fb5-5f3a3703a365.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/6bc4/live/9f537080-d4ff-11f0-9fb5-5f3a3703a365.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/6bc4/live/9f537080-d4ff-11f0-9fb5-5f3a3703a365.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/6bc4/live/9f537080-d4ff-11f0-9fb5-5f3a3703a365.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/6bc4/live/9f537080-d4ff-11f0-9fb5-5f3a3703a365.jpg.webp" loading="lazy" alt="Olena has dyed red hair, tied back, and is wearing a grey top. She is cuddling Viktoriia, who is wearing a turquoise coat and has closely cropped hair. They are outside, with a housing block behind them."></p></div><p data-component="caption-block"><figcaption>Olena with her daughter Viktoriia, who has recently been diagnosed with another brain tumour</figcaption></p></figure><div data-component="text-block"><p>Olena showed us the contract she had been asked to sign.</p><p>In addition to the family's $1,500 (£1,122) filming fee on the day, it states they would get $8,000 (£5,986) once the fundraising goal was met. The amount for the goal, however, has been left blank.</p><p>The contract showed an address in New York for Chance Letikva. On the organisation's website, there is another - in Beit Shemesh, about an hour from Jerusalem. We travelled to both, but found no sign of it.</p><p>And we discovered Chance Letikva seems to be one of many such organisations.</p><p>The man who filmed Viktoriia's campaign told our producer - who was posing as a friend of a sick child - that he works for other similar organisations.</p><p>"Each time, it's a different one," the man - who had introduced himself as "Oleh" - told her. "I hate to put it this way, but they work kind of like a conveyor belt."</p><p>"About a dozen similar companies" requested "material", he said, naming two of them - Saint Teresa and Little Angels, both registered in the US.</p><p>When we checked their registration documents, we once again found Erez Hadari's name.</p><p>What is not clear is where the money raised for the children has gone.</p><p>More than a year after Viktoriia's filming, her mother Olena rang Oleh, who seems to go by Alex Kohen online, to find out. Shortly afterwards, someone from Chance Letikva called to say the donations had paid for advertising, she says.</p><p>This is also what Mr Hadari told Aljin, Khalil's mother, when she confronted him over the phone.</p><p>"There is cost of advertising. So the company lost money," Mr Hadari told her, without giving any evidence to support this.</p><p>Charity experts told us advertising should not amount to more than 20% of the total raised by campaigns.</p><p>Someone previously employed to recruit children for Chance Letikva campaigns told us how those featured had been chosen.</p><p>They had been asked to visit oncology clinics, they said - speaking on condition of anonymity.</p><p>"They were always looking for beautiful children with white skin. The child had to be three to nine years old. They had to know how to speak well. They had to be without hair," they told us.</p><p>"They asked me for photos, to see if the child is right, and I would send it to Erez."</p><p>The whistleblower told us Mr Hadari would then send the photo on to someone else, in Israel, whose name they were never told.</p><p>As for Mr Hadari himself, we tried to reach him at two addresses in Canada but could not find him. He replied to one voice note we had sent him - asking about the money he had been apparently crowdfunding - by saying the organisation "has never been active", without specifying which one. He did not respond to a further voice note and letter laying out all our questions and allegations.</p></div><figure><div data-component="image-block"><p><img src="https://static.files.bbci.co.uk/bbcdotcom/web/20251210-151139-37baf2dfdd-web-2.36.1-1/grey-placeholder.png" aria-label="image unavailable"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/b676/live/3589b330-d51d-11f0-9fb5-5f3a3703a365.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/b676/live/3589b330-d51d-11f0-9fb5-5f3a3703a365.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/b676/live/3589b330-d51d-11f0-9fb5-5f3a3703a365.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/b676/live/3589b330-d51d-11f0-9fb5-5f3a3703a365.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/b676/live/3589b330-d51d-11f0-9fb5-5f3a3703a365.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/b676/live/3589b330-d51d-11f0-9fb5-5f3a3703a365.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/b676/live/3589b330-d51d-11f0-9fb5-5f3a3703a365.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/b676/live/3589b330-d51d-11f0-9fb5-5f3a3703a365.jpg.webp" loading="lazy" alt="Erez Hadari Erez Hadari is shown sitting in a plane - in what looks like first or business class - with a blue top and grey trousers, and is smiling, holding headphones"><span>Erez Hadari</span></p></div><p data-component="caption-block"><figcaption>Erez Hadari sent this photo of himself to Khalil's mum, Aljin </figcaption></p></figure><div data-component="text-block"><p>Campaigns set up by Chance Letikva for two children who died - Khalil and a Mexican boy called Hector - still appear to be accepting money.</p><p>Chance Letikva's US branch appears to be linked to a new organisation called Saint Raphael, which has produced more campaigns - at least two of which seem to have been filmed in Angelholm clinic in Ukraine, as the clinic's distinctive wood panelling and staff uniforms can be seen.</p><p>Olena, Viktoriia's mother, says her daughter has been diagnosed with another brain tumour. She says she is sickened by the findings of our investigation.</p><p>"When your child is… hanging on the edge of life, and someone's out there, making money off that. Well, it's filthy. It's blood money."</p><p>The BBC contacted Tetiana Khaliavka and Alex Kohen, and the organisations Chance Letikva, Walls of Hope, Saint Raphael, Little Angels and Saint Teresa - inviting them to respond to the allegations made against them. None of them replied.</p><p>The Israeli Corporations Authority, which oversees the country's non-profit organisations, told us that if it has evidence founders are using entities as "a cover for illegal activity", then registration inside Israel may be denied and the founder could be barred from working in the sector.</p><p>UK regulator, the Charity Commission, advises those wishing to donate to charities to check that those associations are registered, and that the appropriate fundraising regulator should be contacted if in doubt.</p><p><i id="additional-reporting-by:-ned-davies,-tracks-saflor,-jose-antonio-lucio,-almudena-garcia-parrado,-vitaliya-kozmenko,-shakked-auerbach,-tom-tzur-wisfelder,-katya-malofieieva,-anastasia-kucher,-alan-pulido-and-neil-mccarthy"><b id="additional-reporting-by:-ned-davies,-tracks-saflor,-jose-antonio-lucio,-almudena-garcia-parrado,-vitaliya-kozmenko,-shakked-auerbach,-tom-tzur-wisfelder,-katya-malofieieva,-anastasia-kucher,-alan-pulido-and-neil-mccarthy">Additional reporting by: Ned Davies, Tracks Saflor, Jose Antonio Lucio, Almudena Garcia-parrado, Vitaliya Kozmenko, Shakked Auerbach, Tom Tzur Wisfelder, Katya Malofieieva, Anastasia Kucher, Alan Pulido and Neil McCarthy</b></i></p><ul><li><i id="if-you-have-any-information-to-add-to-this-investigation-please-contact"><b id="if-you-have-any-information-to-add-to-this-investigation-please-contact">If you have any information to add to this investigation please contact </b></i><a target="_self" href="mailto:simi@bbc.co.uk"><i id="simi@bbc.co.uk"><b id="simi@bbc.co.uk">simi@bbc.co.uk</b></i></a></li></ul></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bonsai: A Voxel Engine, from scratch (208 pts)]]></title>
            <link>https://github.com/scallyw4g/bonsai</link>
            <guid>46285319</guid>
            <pubDate>Tue, 16 Dec 2025 06:06:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/scallyw4g/bonsai">https://github.com/scallyw4g/bonsai</a>, See on <a href="https://news.ycombinator.com/item?id=46285319">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/logo_256.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/logo_256.png"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Welcome to Bonsai!</h2><a id="user-content-welcome-to-bonsai" aria-label="Permalink: Welcome to Bonsai!" href="#welcome-to-bonsai"></a></p>
<p dir="auto">Bonsai is a voxel engine in a pot.  It's been tended to with love and
care over the years.  It started out as a learning excercise, and has taught me
the value of simplicity.</p>
<p dir="auto">Bonsai supports massive worlds.  The current version supports a maximum world
size of ~1 billion blocks, cubed.  At one block per meter, that's the distance
from earth to the moon, 2600 times, in every direction.  The view distance is
the entire world, all the time.  Yes, you read that right.  In Bonsai, you can
see in a straight line from Jupiter to the sun.</p>
<p dir="auto">Bonsai terrain generation is fully procedural, and user configurable.  Terrain
is generated on the GPU using regular glsl shaders.  Anything you can do in a
shader, you can do in a Bonsai terrain generator.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">2.0.0-prealpha Note</h3><a id="user-content-200-prealpha-note" aria-label="Permalink: 2.0.0-prealpha Note" href="#200-prealpha-note"></a></p>
<p dir="auto">The current version is 2.0.0-prealpha-rc0, which can be found by joining the
<a href="https://discord.gg/kmRpgXBh75" rel="nofollow">Discord</a>.  This version is a large rewrite of
several core systems, including the world generation, editor and parts of the
renderer.</p>
<p dir="auto">In its current state, the engine is effectively a terrain generator and editor.
For details on remaing work, see <a href="https://github.com/scallyw4g/bonsai/issues/82" data-hovercard-type="issue" data-hovercard-url="/scallyw4g/bonsai/issues/82/hovercard">Roadmap to v2.0.0</a>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/two_doors.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/two_doors.png" alt="banner"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">Bonsai, and nearly all it's dependencies, are written completely from scratch.
One external dependency is the C runtime library for program startup. There is
a back-burner task to remove the CRT entirely, athough it's unclear when/if
anyone will ever get around to it.</p>
<p dir="auto">The only external requirements to build Bonsai are clang++ (&gt;= version 18.1)
and a few appropriate system headers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quickstart</h2><a id="user-content-quickstart" aria-label="Permalink: Quickstart" href="#quickstart"></a></p>
<p dir="auto">Grab pre-built binaries &amp; assets from the <a href="https://github.com/scallyw4g/bonsai/releases/latest">Latest Releases</a>
for your platform of your choice (as long as your platform of choice is Windows or Linux) ;)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/scallyw4g/bonsai/blob/master/docs/00_getting_started.md">Getting Started</a></h3><a id="user-content-getting-started-1" aria-label="Permalink: Getting Started" href="#getting-started-1"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/scallyw4g/bonsai/blob/master/docs/01_build_process.md">Build From Source</a></h3><a id="user-content-build-from-source" aria-label="Permalink: Build From Source" href="#build-from-source"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/scallyw4g/bonsai/blob/master/docs/controls.md">Controls</a></h3><a id="user-content-controls" aria-label="Permalink: Controls" href="#controls"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://discord.gg/kmRpgXBh75" rel="nofollow">Discord Server</a></h3><a id="user-content-discord-server" aria-label="Permalink: Discord Server" href="#discord-server"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/orks.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/orks.png" alt="banner"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Feature Sets</h2><a id="user-content-feature-sets" aria-label="Permalink: Feature Sets" href="#feature-sets"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Renderer</h2><a id="user-content-renderer" aria-label="Permalink: Renderer" href="#renderer"></a></p>
<ul dir="auto">
<li>Deferred Shading</li>
<li>HDR Lighting</li>
<li>Order-independant Transparency</li>
<li>Lighting Bloom</li>
<li>Shadow Mapping</li>
<li>Screen Space Ambient Occlusion</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/abandoned_workshop.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/abandoned_workshop.png" alt="banner"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Engine</h2><a id="user-content-engine" aria-label="Permalink: Engine" href="#engine"></a></p>
<ul dir="auto">
<li>Hot Shader &amp; Game-code Reloading</li>
<li>Async Job System</li>
<li>Entities</li>
<li>Collision</li>
<li>Transparent &amp; Emissive Particles</li>
<li>UI Framework</li>
<li>Asset Loaders</li>
<li>Primitive Physics</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/mountain.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/mountain.png" alt="banner"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Terrain Generation</h2><a id="user-content-terrain-generation" aria-label="Permalink: Terrain Generation" href="#terrain-generation"></a></p>
<ul dir="auto">
<li>Fully programmable GPU-based terrain generation</li>
<li>Batteries-included library of pre-built terrain shaders</li>
<li>1D, 2D and 3D noise library</li>
<li>Terrain derivitives available in second-stage terrain "decoration"</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/keyhole.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/keyhole.png" alt="banner"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Editing</h2><a id="user-content-editing" aria-label="Permalink: Editing" href="#editing"></a></p>
<ul dir="auto">
<li>CSG-like SDF world editing</li>
<li>Library of primitive shapes (rect, sphere, line, cylinder .. etc)</li>
<li>SDF brush-based texturing of primitives</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/ork_aerial.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/ork_aerial.png" alt="banner"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">SDF Brushes</h2><a id="user-content-sdf-brushes" aria-label="Permalink: SDF Brushes" href="#sdf-brushes"></a></p>
<ul dir="auto">
<li>Layer-based brush GUI</li>
<li>(coming soon) glsl brush shaders</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/brush.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/brush.png" alt="banner"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance Profiler</h2><a id="user-content-performance-profiler" aria-label="Permalink: Performance Profiler" href="#performance-profiler"></a></p>
<ul dir="auto">
<li>Manual Instrumentation</li>
<li>Memory allocation tracking</li>
<li>Multithreaded callgraph tracing</li>
<li>Context Switches (windows only)</li>
<li>Physical Core  (windows only)</li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/profiler.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/profiler.png" alt="banner"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Gallery</h2><a id="user-content-gallery" aria-label="Permalink: Gallery" href="#gallery"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/3_skele.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/3_skele.png" alt="banner"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/ridgeline.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/ridgeline.png" alt="banner"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/grass.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/grass.png" alt="banner"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/8_skele.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/8_skele.png" alt="banner"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Wishlist</h2><a id="user-content-wishlist" aria-label="Permalink: Wishlist" href="#wishlist"></a></p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Renderer</h2><a id="user-content-renderer-1" aria-label="Permalink: Renderer" href="#renderer-1"></a></p>
<p dir="auto">[ ] HRC : <a href="https://github.com/entropylost/amitabha">https://github.com/entropylost/amitabha</a></p>
<p dir="auto">[ ] SSR : <a href="https://lettier.github.io/3d-game-shaders-for-beginners/screen-space-reflection.html" rel="nofollow">https://lettier.github.io/3d-game-shaders-for-beginners/screen-space-reflection.html</a></p>
<p dir="auto">[ ] Screen-space lines : <a href="https://mattdesl.svbtle.com/drawing-lines-is-hard" rel="nofollow">https://mattdesl.svbtle.com/drawing-lines-is-hard</a></p>
<p dir="auto">[ ] Better shadows : <a href="https://developer.nvidia.com/gpugems/gpugems3/part-ii-light-and-shadows/chapter-8-summed-area-variance-shadow-maps" rel="nofollow">https://developer.nvidia.com/gpugems/gpugems3/part-ii-light-and-shadows/chapter-8-summed-area-variance-shadow-maps</a></p>
<p dir="auto">[ ] Screen Space Shadows : <a href="https://panoskarabelas.com/posts/screen_space_shadows/" rel="nofollow">https://panoskarabelas.com/posts/screen_space_shadows/</a></p>
<p dir="auto">[ ] Motion Blur : <a href="https://developer.nvidia.com/gpugems/gpugems3/part-iv-image-effects/chapter-27-motion-blur-post-processing-effect" rel="nofollow">https://developer.nvidia.com/gpugems/gpugems3/part-iv-image-effects/chapter-27-motion-blur-post-processing-effect</a></p>
<p dir="auto">[ ] TAA?</p>
<p dir="auto">[ ] FXAA : <a href="http://blog.simonrodriguez.fr/articles/2016/07/implementing_fxaa.html" rel="nofollow">http://blog.simonrodriguez.fr/articles/2016/07/implementing_fxaa.html</a></p>
<p dir="auto">[ ] Water : <a href="https://www.youtube.com/watch?v=5yhDb9dzJ58" rel="nofollow">https://www.youtube.com/watch?v=5yhDb9dzJ58</a></p>
<p dir="auto">[ ] Fluids : <a href="https://andrewkchan.dev/posts/fire.html" rel="nofollow">https://andrewkchan.dev/posts/fire.html</a></p>
<p dir="auto">[ ] Remove meshing entirely? <a href="https://www.youtube.com/watch?v=4xs66m1Of4A" rel="nofollow">https://www.youtube.com/watch?v=4xs66m1Of4A</a></p>
<p dir="auto">[ ] Lumen-style GI screen-space radiance caching : <a href="https://www.youtube.com/watch?v=2GYXuM10riw" rel="nofollow">https://www.youtube.com/watch?v=2GYXuM10riw</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/platapus.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/platapus.png" alt="banner"></a></p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Terrain</h2><a id="user-content-terrain" aria-label="Permalink: Terrain" href="#terrain"></a></p>
<p dir="auto">[ ] Erosion simulation</p>
<ul dir="auto">
<li><a href="https://inria.hal.science/hal-01262376/document" rel="nofollow">https://inria.hal.science/hal-01262376/document</a></li>
<li><a href="https://xing-mei.github.io/files/erosion.pdf" rel="nofollow">https://xing-mei.github.io/files/erosion.pdf</a></li>
<li><a href="https://nickmcd.me/2020/04/15/procedural-hydrology/" rel="nofollow">https://nickmcd.me/2020/04/15/procedural-hydrology/</a></li>
</ul>
<p dir="auto">[ ] Biomes</p>
<ul dir="auto">
<li><a href="https://en.wikipedia.org/wiki/Holdridge_life_zones" rel="nofollow">https://en.wikipedia.org/wiki/Holdridge_life_zones</a></li>
</ul>
<p dir="auto">[ ] Meshing</p>
<ul dir="auto">
<li>Isotropic surface meshing</li>
<li><a href="https://graphics.stanford.edu/courses/cs164-10-spring/Handouts/isotropic.pdf" rel="nofollow">https://graphics.stanford.edu/courses/cs164-10-spring/Handouts/isotropic.pdf</a></li>
<li><a href="https://inria.hal.science/inria-00071612/document" rel="nofollow">https://inria.hal.science/inria-00071612/document</a></li>
</ul>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/pillar.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/pillar.png" alt="banner"></a></p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Assets</h2><a id="user-content-assets" aria-label="Permalink: Assets" href="#assets"></a></p>
<p dir="auto">[ ] MCA importer</p>
<ul dir="auto">
<li><a href="https://github.com/GabeRundlett/gvox/blob/old/src/formats/minecraft.cpp">https://github.com/GabeRundlett/gvox/blob/old/src/formats/minecraft.cpp</a></li>
</ul>
<p dir="auto">[ ] Sound : mp3, ogg, ..? decompresser</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/5_skele.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/5_skele.png" alt="banner"></a></p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Datastructures</h2><a id="user-content-datastructures" aria-label="Permalink: Datastructures" href="#datastructures"></a></p>
<p dir="auto">[ ] Better low-discrepency sequences : <a href="https://blog.demofox.org/2017/05/29/when-random-numbers-are-too-random-low-discrepancy-sequences/" rel="nofollow">https://blog.demofox.org/2017/05/29/when-random-numbers-are-too-random-low-discrepancy-sequences/</a></p>
<p dir="auto">[ ] Better disk/sphere sampling patterns : <a href="https://extremelearning.com.au/how-to-generate-uniformly-random-points-on-n-spheres-and-n-balls/" rel="nofollow">https://extremelearning.com.au/how-to-generate-uniformly-random-points-on-n-spheres-and-n-balls/</a></p>
<p dir="auto">[ ] Better hash function! : <a href="https://nullprogram.com/blog/2018/07/31/" rel="nofollow">https://nullprogram.com/blog/2018/07/31/</a></p>
<p dir="auto">[ ] Better GPU hashing! : <a href="https://arugl.medium.com/hash-noise-in-gpu-shaders-210188ac3a3e" rel="nofollow">https://arugl.medium.com/hash-noise-in-gpu-shaders-210188ac3a3e</a></p>
<p dir="auto">[ ] Hash-trie as alternative to a table : <a href="https://nullprogram.com/blog/2023/09/30/" rel="nofollow">https://nullprogram.com/blog/2023/09/30/</a></p>
<p dir="auto">[ ] Octree ? <a href="https://graphics.tudelft.nl/Publications-new/2020/CBE20/ModifyingCompressedVoxels-main.pdf" rel="nofollow">https://graphics.tudelft.nl/Publications-new/2020/CBE20/ModifyingCompressedVoxels-main.pdf</a></p>
<p dir="auto">[ ] Better floating-point rng : <a href="https://www.corsix.org/content/higher-quality-random-floats" rel="nofollow">https://www.corsix.org/content/higher-quality-random-floats</a></p>
<p dir="auto">[ ] Better greedy meshing? <a href="https://www.youtube.com/watch?v=4xs66m1Of4A" rel="nofollow">https://www.youtube.com/watch?v=4xs66m1Of4A</a></p>
<p dir="auto">[ ] More interpolation goodies : <a href="https://paulbourke.net/miscellaneous/interpolation/" rel="nofollow">https://paulbourke.net/miscellaneous/interpolation/</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/scallyw4g/bonsai/blob/master/screenshots/dusk_defence.png"><img src="https://github.com/scallyw4g/bonsai/raw/master/screenshots/dusk_defence.png" alt="banner"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Goodies</h2><a id="user-content-goodies" aria-label="Permalink: Goodies" href="#goodies"></a></p>
<p dir="auto">[ ] Better (faster) Sin/Cos ? <a href="https://www.shadertoy.com/view/432yWW" rel="nofollow">https://www.shadertoy.com/view/432yWW</a></p>
<p dir="auto">[ ] Look into using this Intel tooling for dual CPU/GPU world-gen?
<a href="https://www.intel.com/content/dam/develop/external/us/en/documents/spir-vtointe-ispcgpu-compute-on-the-cpu.pdf" rel="nofollow">https://www.intel.com/content/dam/develop/external/us/en/documents/spir-vtointe-ispcgpu-compute-on-the-cpu.pdf</a>
<a href="https://ispc.github.io/" rel="nofollow">https://ispc.github.io/</a></p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Profiler</h2><a id="user-content-profiler" aria-label="Permalink: Profiler" href="#profiler"></a></p>
<p dir="auto">[ ] Improve the ETW layer : <a href="https://github.com/bombomby/optick/blob/master/src/optick_core.win.h">https://github.com/bombomby/optick/blob/master/src/optick_core.win.h</a></p>
<p dir="auto">[ ] GPU Profiling : <a href="https://www.khronos.org/opengl/wiki/Query_Object" rel="nofollow">https://www.khronos.org/opengl/wiki/Query_Object</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Erdős Problem #1026 (153 pts)]]></title>
            <link>https://terrytao.wordpress.com/2025/12/08/the-story-of-erdos-problem-126/</link>
            <guid>46284897</guid>
            <pubDate>Tue, 16 Dec 2025 04:49:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://terrytao.wordpress.com/2025/12/08/the-story-of-erdos-problem-126/">https://terrytao.wordpress.com/2025/12/08/the-story-of-erdos-problem-126/</a>, See on <a href="https://news.ycombinator.com/item?id=46284897">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>
 <a href="https://www.erdosproblems.com/1026">Problem 1026 on the Erdős problem web site</a> recently got solved through an interesting combination of existing literature, online collaboration, and AI tools. The purpose of this blog post is to try to tell the story of this collaboration, and also to supply a complete proof.
</p><p>
The original problem of Erdős, <a href="https://users.renyi.hu/~p_erdos/1971-25.pdf">posed in 1975</a>, is rather ambiguous. Erdős starts by <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Szekeres_theorem">recalling his famous theorem with Szekeres</a> that says that given a sequence of <img src="https://s0.wp.com/latex.php?latex=%7Bk%5E2%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%5E2%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%5E2%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k^2+1}"> distinct real numbers, one can find a subsequence of length <img src="https://s0.wp.com/latex.php?latex=%7Bk%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k+1}"> which is either increasing or decreasing; and that one cannot improve the <img src="https://s0.wp.com/latex.php?latex=%7Bk%5E2%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%5E2%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%5E2%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k^2+1}"> to <img src="https://s0.wp.com/latex.php?latex=%7Bk%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k^2}">, by considering for instance a sequence of <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k}"> blocks of length <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k}">, with the numbers in each block decreasing, but the blocks themselves increasing. He also noted a <a href="https://zbmath.org/0090.01302">result of Hanani</a> that every sequence of length <img src="https://s0.wp.com/latex.php?latex=%7Bk%28k%2B3%29%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%28k%2B3%29%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%28k%2B3%29%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k(k+3)/2}"> can be decomposed into the union of <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k}"> monotone sequences. He then wrote “As far as I know the following question is not yet settled. Let <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_1,\dots,x_n}"> be a sequence of distinct numbers, determine </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%28x_1%2C%5Cdots%2Cx_n%29+%3D+%5Cmax+%5Csum_r+x_%7Bi_r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%28x_1%2C%5Cdots%2Cx_n%29+%3D+%5Cmax+%5Csum_r+x_%7Bi_r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%28x_1%2C%5Cdots%2Cx_n%29+%3D+%5Cmax+%5Csum_r+x_%7Bi_r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  S(x_1,\dots,x_n) = \max \sum_r x_{i_r}"></p><p>
 where the maximum is to be taken over all monotonic sequences <img src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bi_1%7D%2C%5Cdots%2Cx_%7Bi_m%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_%7Bi_1%7D%2C%5Cdots%2Cx_%7Bi_m%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_%7Bi_1%7D%2C%5Cdots%2Cx_%7Bi_m%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_{i_1},\dots,x_{i_m}}">“.
</p><p>
This problem was added to the Erdős problem site on September 12, 2025, with a note that the problem was rather ambiguous. For any fixed <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{n}">, this is an explicit piecewise linear function of the variables <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_1,\dots,x_n}"> that could be computed by a simple brute force algorithm, but Erdős was presumably seeking optimal bounds for this quantity under some natural constraint on the <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_i}">. The day the problem was posted, Desmond Weisenberg proposed studying the quantity <img src="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(n)}">, defined as the largest constant such that </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%28x_1%2C%5Cdots%2Cx_n%29+%5Cgeq+c%28n%29+%5Csum_%7Bi%3D1%7D%5En+x_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%28x_1%2C%5Cdots%2Cx_n%29+%5Cgeq+c%28n%29+%5Csum_%7Bi%3D1%7D%5En+x_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%28x_1%2C%5Cdots%2Cx_n%29+%5Cgeq+c%28n%29+%5Csum_%7Bi%3D1%7D%5En+x_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  S(x_1,\dots,x_n) \geq c(n) \sum_{i=1}^n x_i"></p><p>
 for all choices of (distinct) real numbers <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_1,\dots,x_n}">. Desmond noted that for this formulation one could assume without loss of generality that the <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_i}"> were positive, since deleting negative or vanishing <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_i}"> does not decrease the left-hand side and does not increase the right-hand side. By a limiting argument one could also allow collisions between the <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_i}">, so long as one interpreted monotonicity in the weak sense.
</p><p>
Though not stated on the web site, one can formulate this problem in game theoretic terms. Suppose that Alice has a stack of <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{N}"> coins for some large <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{N}">. She divides the coins into <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{n}"> piles of consisting of <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_1,\dots,x_n}"> coins each, so that <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_i+%3D+N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_i+%3D+N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_i+%3D+N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\sum_{i=1}^n x_i = N}">. She then passes the piles to Bob, who is allowed to select a monotone subsequence of the piles (in the weak sense) and keep all the coins in those piles. What is the largest fraction <img src="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(n)}"> of the coins that Bob can guarantee to keep, regardless of how Alice divides up the coins? (One can work with either a discrete version of this problem where the <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_i}"> are integers, or a continuous one where the coins can be split fractionally, but in the limit <img src="https://s0.wp.com/latex.php?latex=%7BN+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BN+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BN+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{N \rightarrow \infty}"> the problems can easily be seen to be equivalent.)
</p>
<p>
AI-generated images continue to be problematic for a number of reasons, but here is one such image that somewhat manages at least to convey the idea of the game:
</p>
<p><img src="https://terrytao.wordpress.com/wp-content/uploads/2025/12/game.jpg" width="500"></p><p>
For small <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{n}">, one can work out <img src="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(n)}"> by hand. For <img src="https://s0.wp.com/latex.php?latex=%7Bn%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bn%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bn%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{n=1}">, clearly <img src="https://s0.wp.com/latex.php?latex=%7Bc%281%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%281%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%281%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(1)=1}">: Alice has to put all the coins into one pile, which Bob simply takes. Similarly <img src="https://s0.wp.com/latex.php?latex=%7Bc%282%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%282%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%282%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(2)=1}">: regardless of how Alice divides the coins into two piles, the piles will either be increasing or decreasing, so in either case Bob can take both. The first interesting case is <img src="https://s0.wp.com/latex.php?latex=%7Bn%3D3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bn%3D3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bn%3D3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{n=3}">. Bob can again always take the two largest piles, guaranteeing himself <img src="https://s0.wp.com/latex.php?latex=%7B2%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B2%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B2%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{2/3}"> of the coins. On the other hand, if Alice <em>almost</em> divides the coins evenly, for instance into piles <img src="https://s0.wp.com/latex.php?latex=%7B%28%281%2F3+%2B+%5Cvarepsilon%29N%2C+%281%2F3-2%5Cvarepsilon%29+N%2C+%281%2F3%2B%5Cvarepsilon%29N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%28%281%2F3+%2B+%5Cvarepsilon%29N%2C+%281%2F3-2%5Cvarepsilon%29+N%2C+%281%2F3%2B%5Cvarepsilon%29N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%28%281%2F3+%2B+%5Cvarepsilon%29N%2C+%281%2F3-2%5Cvarepsilon%29+N%2C+%281%2F3%2B%5Cvarepsilon%29N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{((1/3 + \varepsilon)N, (1/3-2\varepsilon) N, (1/3+\varepsilon)N)}"> for some small <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cvarepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\varepsilon>0}">, then Bob cannot take all three piles as they are non-monotone, and so can only take two of them, allowing Alice to limit the payout fraction to be arbitrarily close to <img src="https://s0.wp.com/latex.php?latex=%7B2%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B2%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B2%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{2/3}">. So we conclude that <img src="https://s0.wp.com/latex.php?latex=%7Bc%283%29%3D2%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%283%29%3D2%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%283%29%3D2%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(3)=2/3}">.
</p><p>
An hour after Desmond’s comment, <a href="https://www.erdosproblems.com/forum/thread/1026#post-466">Stijn Cambie noted</a> (though not in the language I used above) that a similar construction to the one above, in which Alice divides the coins into <img src="https://s0.wp.com/latex.php?latex=%7Bk%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k^2}"> pairs that are almost even, in such a way that the longest monotone sequence is of length <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k}">, gives the upper bound <img src="https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%29+%5Cleq+1%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%29+%5Cleq+1%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%29+%5Cleq+1%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(k^2) \leq 1/k}">. It is also easy to see that <img src="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(n)}"> is a non-increasing function of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{n}">, so this gives a general bound <img src="https://s0.wp.com/latex.php?latex=%7Bc%28n%29+%5Cleq+%281%2Bo%281%29%29%2F%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28n%29+%5Cleq+%281%2Bo%281%29%29%2F%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28n%29+%5Cleq+%281%2Bo%281%29%29%2F%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(n) \leq (1+o(1))/\sqrt{n}}">. Less than an hour after that, <a href="https://www.erdosproblems.com/forum/thread/1026#post-467">Wouter van Doorn noted</a> that the Hanani result mentioned above gives the lower bound <img src="https://s0.wp.com/latex.php?latex=%7Bc%28n%29+%5Cgeq+%28%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D-o%281%29%29%2F%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28n%29+%5Cgeq+%28%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D-o%281%29%29%2F%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28n%29+%5Cgeq+%28%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D-o%281%29%29%2F%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(n) \geq (\frac{1}{\sqrt{2}}-o(1))/\sqrt{n}}">, and posed the problem of determining the asymptotic limit of <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D+c%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D+c%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D+c%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\sqrt{n} c(n)}"> as <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{n \rightarrow \infty}">, given that this was now known to range between <img src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Csqrt%7B2%7D-o%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B1%2F%5Csqrt%7B2%7D-o%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B1%2F%5Csqrt%7B2%7D-o%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{1/\sqrt{2}-o(1)}"> and <img src="https://s0.wp.com/latex.php?latex=%7B1%2Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B1%2Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B1%2Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{1+o(1)}">. This version was <a href="https://www.erdosproblems.com/forum/thread/1026#post-473">accepted by Thomas Bloom</a>, the moderator of the Erdős problem site, as a valid interpretation of the original problem.
</p><p>
The next day, <a href="https://www.erdosproblems.com/forum/thread/1026#post-478">Stijn computed</a> the first few values of <img src="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(n)}"> exactly: </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2C+1%2C+2%2F3%2C+1%2F2%2C+1%2F2%2C+3%2F7%2C+2%2F5%2C+3%2F8%2C+1%2F3.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2C+1%2C+2%2F3%2C+1%2F2%2C+1%2F2%2C+3%2F7%2C+2%2F5%2C+3%2F8%2C+1%2F3.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2C+1%2C+2%2F3%2C+1%2F2%2C+1%2F2%2C+3%2F7%2C+2%2F5%2C+3%2F8%2C+1%2F3.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  1, 1, 2/3, 1/2, 1/2, 3/7, 2/5, 3/8, 1/3."></p><p>
 While the general pattern was not yet clear, this was enough data for Stijn to conjecture that <img src="https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%29%3D1%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%29%3D1%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%29%3D1%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(k^2)=1/k}">, which would also imply that <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D+c%28n%29+%5Crightarrow+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D+c%28n%29+%5Crightarrow+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D+c%28n%29+%5Crightarrow+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\sqrt{n} c(n) \rightarrow 1}"> as <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{n \rightarrow \infty}">. (EDIT: as later located by an AI deep research tool, this conjecture was also made in Section 12 of <a href="https://zbmath.org/0832.60012">this 1980 article of Steele</a>.) Stijn also described the extremizing sequences for this range of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{n}">, but did not continue the calculation further (a naive computation would take runtime exponential in <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{n}">, due to the large number of possible subsequences to consider).  
</p><p>
The problem then lay dormant for almost two months, until December 7, 2025, in which Boris Alexeev, as part of a systematic sweep of the Erdős problems using the AI tool <a href="https://arxiv.org/abs/2510.01346">Aristotle</a>, was able to get this tool to <a href="https://github.com/plby/lean-proofs/blob/main/src/v4.24.0/ErdosProblems/Erdos1026.lean">autonomously solve</a> this conjecture <img src="https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%29%3D1%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%29%3D1%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%29%3D1%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(k^2)=1/k}"> in the proof assistant language Lean. The proof converted the problem to a rectangle-packing problem.
</p><p>
This was one further addition to a recent sequence of examples where an Erdős problem had been automatically solved in one fashion or another by an AI tool. Like the previous cases, the proof turned out to not be particularly novel. Within an hour, Koishi Chan gave an alternate proof deriving the required bound <img src="https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%29+%5Cgeq+1%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%29+%5Cgeq+1%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%29+%5Cgeq+1%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(k^2) \geq 1/k}"> from the original Erdős-Szekeres theorem by a standard “blow-up” argument which we can give here in the Alice-Bob formulation. Take a large <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{M}">, and replace each pile of <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_i}"> coins with <img src="https://s0.wp.com/latex.php?latex=%7B%281%2Bo%281%29%29+M%5E2+x_i%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%281%2Bo%281%29%29+M%5E2+x_i%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%281%2Bo%281%29%29+M%5E2+x_i%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{(1+o(1)) M^2 x_i^2}"> new piles, each of size <img src="https://s0.wp.com/latex.php?latex=%7B%281%2Bo%281%29%29+x_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%281%2Bo%281%29%29+x_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%281%2Bo%281%29%29+x_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{(1+o(1)) x_i}">, chosen so that the longest monotone subsequence in this collection is <img src="https://s0.wp.com/latex.php?latex=%7B%281%2Bo%281%29%29+M+x_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%281%2Bo%281%29%29+M+x_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%281%2Bo%281%29%29+M+x_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{(1+o(1)) M x_i}">. Among all the new piles, the longest monotone subsequence has length <img src="https://s0.wp.com/latex.php?latex=%7B%281%2Bo%281%29%29+M+S%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%281%2Bo%281%29%29+M+S%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%281%2Bo%281%29%29+M+S%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{(1+o(1)) M S(x_1,\dots,x_n)}">. Applying Erdős-Szekeres, one concludes the bound </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++M+S%28x_1%2C%5Cdots%2Cx_n%29+%5Cgeq+%281-o%281%29%29+%28%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%7D+M%5E2+x_i%5E2%29%5E%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++M+S%28x_1%2C%5Cdots%2Cx_n%29+%5Cgeq+%281-o%281%29%29+%28%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%7D+M%5E2+x_i%5E2%29%5E%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++M+S%28x_1%2C%5Cdots%2Cx_n%29+%5Cgeq+%281-o%281%29%29+%28%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%7D+M%5E2+x_i%5E2%29%5E%7B1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  M S(x_1,\dots,x_n) \geq (1-o(1)) (\sum_{i=1}^{k^2} M^2 x_i^2)^{1/2}"></p><p>
 and on canceling the <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{M}">‘s, sending <img src="https://s0.wp.com/latex.php?latex=%7BM+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BM+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BM+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{M \rightarrow \infty}">, and applying Cauchy-Schwarz, one obtains <img src="https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%29+%5Cgeq+1%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%29+%5Cgeq+1%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%29+%5Cgeq+1%2Fk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(k^2) \geq 1/k}"> (in fact the argument gives <img src="https://s0.wp.com/latex.php?latex=%7Bc%28n%29+%5Cgeq+1%2F%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28n%29+%5Cgeq+1%2F%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28n%29+%5Cgeq+1%2F%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(n) \geq 1/\sqrt{n}}"> for all <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{n}">).
</p><p>
Once this proof was found, it was natural to try to see if it had already appeared in the literature. AI deep research tools have successfully located such prior literature in the past, but in this case they did not succeed, and a more “old-fashioned” Google Scholar job turned up some relevant references: a <a href="https://arxiv.org/abs/1608.04153">2016 paper by Tidor, Wang and Yang</a> contained this precise result, citing an <a href="https://arxiv.org/abs/1612.00471">earlier paper of Wagner</a> as inspiration for applying “blowup” to the Erdős-Szekeres theorem.
</p><p>
But the story does not end there! Upon reading the above story the next day, I realized that the problem of estimating <img src="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(n)}"> was a suitable task for <a href="https://arxiv.org/abs/2506.13131">AlphaEvolve</a>, which I have used recently as mentioned in <a href="https://terrytao.wordpress.com/2025/11/05/mathematical-exploration-and-discovery-at-scale/">this previous post</a>. Specifically, one could task to obtain upper bounds on <img src="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(n)}"> by directing it to produce real numbers (or integers) <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_1,\dots,x_n}"> summing up to a fixed sum (I chose <img src="https://s0.wp.com/latex.php?latex=%7B10%5E6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B10%5E6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B10%5E6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{10^6}">) with a small a value of <img src="https://s0.wp.com/latex.php?latex=%7BS%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BS%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BS%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{S(x_1,\dots,x_n)}"> as possible. After an hour of run time, AlphaEvolve produced the following upper bounds on <img src="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(n)}"> for <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Cleq+n+%5Cleq+16%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B1+%5Cleq+n+%5Cleq+16%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B1+%5Cleq+n+%5Cleq+16%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{1 \leq n \leq 16}">, with some intriguingly structured potential extremizing solutions: </p>
<p><img src="https://terrytao.wordpress.com/wp-content/uploads/2025/12/download.png" width="500"></p><p>
The numerical scores (divided by <img src="https://s0.wp.com/latex.php?latex=%7B10%5E6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B10%5E6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B10%5E6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{10^6}">) were pretty obviously trying to approximate simple rational numbers. There were a variety of ways (including modern AI) to extract the actual rational numbers they were close to, but I searched for a dedicated tool and found this useful <a href="https://www.johndcook.com/rational_approximation.html">little web page of John Cook</a> that did the job: </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2C+1%2C+2%2F3%2C+1%2F2%2C+1%2F2%2C+3%2F7%2C+2%2F5%2C+3%2F8%2C+1%2F3%2C+1%2F4.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2C+1%2C+2%2F3%2C+1%2F2%2C+1%2F2%2C+3%2F7%2C+2%2F5%2C+3%2F8%2C+1%2F3%2C+1%2F4.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2C+1%2C+2%2F3%2C+1%2F2%2C+1%2F2%2C+3%2F7%2C+2%2F5%2C+3%2F8%2C+1%2F3%2C+1%2F4.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  1, 1, 2/3, 1/2, 1/2, 3/7, 2/5, 3/8, 1/3, 1/4."></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2F3%2C+4%2F13%2C+3%2F10%2C+4%2F14%2C+3%2F11%2C+4%2F15%2C+1%2F4.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2F3%2C+4%2F13%2C+3%2F10%2C+4%2F14%2C+3%2F11%2C+4%2F15%2C+1%2F4.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2F3%2C+4%2F13%2C+3%2F10%2C+4%2F14%2C+3%2F11%2C+4%2F15%2C+1%2F4.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  1/3, 4/13, 3/10, 4/14, 3/11, 4/15, 1/4."></p><p>
 I could not immediately see the pattern here, but after some trial and error in which I tried to align numerators and denominators, I eventually organized this sequence into a more suggestive form: </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  1,"></p>
 <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2F1%2C+%5Cmathbf%7B2%2F3%7D%2C+1%2F2%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2F1%2C+%5Cmathbf%7B2%2F3%7D%2C+1%2F2%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2F1%2C+%5Cmathbf%7B2%2F3%7D%2C+1%2F2%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  1/1, \mathbf{2/3}, 1/2,"></p>
 <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%2F4%2C+%5Cmathbf%7B3%2F7%7D%2C+2%2F5%2C+%5Cmathbf%7B3%2F8%7D%2C+2%2F6%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%2F4%2C+%5Cmathbf%7B3%2F7%7D%2C+2%2F5%2C+%5Cmathbf%7B3%2F8%7D%2C+2%2F6%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%2F4%2C+%5Cmathbf%7B3%2F7%7D%2C+2%2F5%2C+%5Cmathbf%7B3%2F8%7D%2C+2%2F6%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  2/4, \mathbf{3/7}, 2/5, \mathbf{3/8}, 2/6,"></p>
 <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++3%2F9%2C+%5Cmathbf%7B4%2F13%7D%2C+3%2F10%2C+%5Cmathbf%7B4%2F14%7D%2C+3%2F11%2C+%5Cmathbf%7B4%2F15%7D%2C+3%2F12.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++3%2F9%2C+%5Cmathbf%7B4%2F13%7D%2C+3%2F10%2C+%5Cmathbf%7B4%2F14%7D%2C+3%2F11%2C+%5Cmathbf%7B4%2F15%7D%2C+3%2F12.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++3%2F9%2C+%5Cmathbf%7B4%2F13%7D%2C+3%2F10%2C+%5Cmathbf%7B4%2F14%7D%2C+3%2F11%2C+%5Cmathbf%7B4%2F15%7D%2C+3%2F12.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  3/9, \mathbf{4/13}, 3/10, \mathbf{4/14}, 3/11, \mathbf{4/15}, 3/12."></p><p>
This gave a somewhat complicated but predictable conjecture for the values of the sequence <img src="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(n)}">. On posting this, Boris found a clean formulation of the conjecture, namely that </p><a name="conj"><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++c%28k%5E2+%2B+2a+%2B+1%29+%3D+%5Cfrac%7Bk%7D%7Bk%5E2%2Ba%7D+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++c%28k%5E2+%2B+2a+%2B+1%29+%3D+%5Cfrac%7Bk%7D%7Bk%5E2%2Ba%7D+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++c%28k%5E2+%2B+2a+%2B+1%29+%3D+%5Cfrac%7Bk%7D%7Bk%5E2%2Ba%7D+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  c(k^2 + 2a + 1) = \frac{k}{k^2+a} \ \ \ \ \ (1)"></p>
</a><p> whenever <img src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k \geq 1}"> and <img src="https://s0.wp.com/latex.php?latex=%7B-k+%5Cleq+a+%5Cleq+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B-k+%5Cleq+a+%5Cleq+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B-k+%5Cleq+a+%5Cleq+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{-k \leq a \leq k}">. After a bit of effort, he also produced an explicit upper bound construction:
</p><blockquote><b>Proposition 1</b>  If <img src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k \geq 1}"> and <img src="https://s0.wp.com/latex.php?latex=%7B-k+%5Cleq+a+%5Cleq+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B-k+%5Cleq+a+%5Cleq+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B-k+%5Cleq+a+%5Cleq+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{-k \leq a \leq k}">, then <img src="https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%2B2a%2B1%29+%5Cleq+%5Cfrac%7Bk%7D%7Bk%5E2%2Ba%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%2B2a%2B1%29+%5Cleq+%5Cfrac%7Bk%7D%7Bk%5E2%2Ba%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28k%5E2%2B2a%2B1%29+%5Cleq+%5Cfrac%7Bk%7D%7Bk%5E2%2Ba%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(k^2+2a+1) \leq \frac{k}{k^2+a}}">. </blockquote>

<p>
<em>Proof:</em>  Consider a sequence <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_%7Bk%5E2%2B2a%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_%7Bk%5E2%2B2a%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_%7Bk%5E2%2B2a%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_1,\dots,x_{k^2+2a+1}}"> of numbers clustered around the “red number” <img src="https://s0.wp.com/latex.php?latex=%7B%7Ca%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7Ca%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7Ca%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{|a|}"> and “blue number” <img src="https://s0.wp.com/latex.php?latex=%7B%7Ca%2B1%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7Ca%2B1%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7Ca%2B1%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{|a+1|}">, consisting of <img src="https://s0.wp.com/latex.php?latex=%7B%7Ca%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7Ca%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7Ca%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{|a|}"> blocks of <img src="https://s0.wp.com/latex.php?latex=%7Bk-%7Ca%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk-%7Ca%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk-%7Ca%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k-|a|}"> “blue” numbers, followed by <img src="https://s0.wp.com/latex.php?latex=%7B%7Ca%2B1%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7Ca%2B1%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7Ca%2B1%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{|a+1|}"> blocks of <img src="https://s0.wp.com/latex.php?latex=%7B%7Ca%2B1%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7Ca%2B1%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7Ca%2B1%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{|a+1|}"> “red” numbers, and then <img src="https://s0.wp.com/latex.php?latex=%7Bk-%7Ca%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk-%7Ca%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk-%7Ca%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k-|a|}"> further blocks of <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k}"> “blue” numbers.  When <img src="https://s0.wp.com/latex.php?latex=%7Ba+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Ba+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Ba+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{a \geq 0}">, one should take all blocks to be slightly decreasing within each block, but the blue blocks should be are increasing between each other, and the red blocks should also be increasing between each other. When <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3C+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Ba+%3C+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Ba+%3C+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{a < 0}">, all of these orderings should be reversed. The total number of elements is indeed </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7Ca%7C+%5Ctimes+%28k-%7Ca%7C%29+%2B+%7Ca%2B1%7C+%5Ctimes+%7Ca%2B1%7C+%2B+%28k-%7Ca%7C%29+%5Ctimes+k+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7Ca%7C+%5Ctimes+%28k-%7Ca%7C%29+%2B+%7Ca%2B1%7C+%5Ctimes+%7Ca%2B1%7C+%2B+%28k-%7Ca%7C%29+%5Ctimes+k+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7Ca%7C+%5Ctimes+%28k-%7Ca%7C%29+%2B+%7Ca%2B1%7C+%5Ctimes+%7Ca%2B1%7C+%2B+%28k-%7Ca%7C%29+%5Ctimes+k+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  |a| \times (k-|a|) + |a+1| \times |a+1| + (k-|a|) \times k "></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+k%5E2+%2B+2a+%2B+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+k%5E2+%2B+2a+%2B+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+k%5E2+%2B+2a+%2B+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  = k^2 + 2a + 1"></p><p>
 and the total sum is close to </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%7Ca%7C+%5Ctimes+%28k-%7Ca%7C%29+%5Ctimes+%7Ca%2B1%7C+%2B+%7Ca%2B1%7C+%5Ctimes+%7Ca%2B1%7C+%5Ctimes+%7Ca%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%7Ca%7C+%5Ctimes+%28k-%7Ca%7C%29+%5Ctimes+%7Ca%2B1%7C+%2B+%7Ca%2B1%7C+%5Ctimes+%7Ca%2B1%7C+%5Ctimes+%7Ca%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%7Ca%7C+%5Ctimes+%28k-%7Ca%7C%29+%5Ctimes+%7Ca%2B1%7C+%2B+%7Ca%2B1%7C+%5Ctimes+%7Ca%2B1%7C+%5Ctimes+%7Ca%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle |a| \times (k-|a|) \times |a+1| + |a+1| \times |a+1| \times |a| "></p>
<p><img src="https://s0.wp.com/latex.php?latex=%2B+%28k-%7Ca%7C%29+%5Ctimes+k+%5Ctimes+%7Ca%2B1%7C+%3D+%28k%5E2+%2B+a%29+%7Ca%2B1%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%2B+%28k-%7Ca%7C%29+%5Ctimes+k+%5Ctimes+%7Ca%2B1%7C+%3D+%28k%5E2+%2B+a%29+%7Ca%2B1%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%2B+%28k-%7Ca%7C%29+%5Ctimes+k+%5Ctimes+%7Ca%2B1%7C+%3D+%28k%5E2+%2B+a%29+%7Ca%2B1%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="+ (k-|a|) \times k \times |a+1| = (k^2 + a) |a+1|."></p><p>
 With this setup, one can check that any monotone sequence consists either of at most <img src="https://s0.wp.com/latex.php?latex=%7B%7Ca%2B1%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%7Ca%2B1%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%7Ca%2B1%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{|a+1|}"> red elements and at most <img src="https://s0.wp.com/latex.php?latex=%7Bk-%7Ca%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk-%7Ca%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk-%7Ca%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k-|a|}"> blue elements, or no red elements and at most <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k}"> blue elements, in either case giving a monotone sum that is bounded by either </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7Ca%2B1%7C+%5Ctimes+%7Ca%7C+%2B+%28k-%7Ca%7C%29+%5Ctimes+%7Ca%2B1%7C+%3D+k+%7Ca%2B1%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7Ca%2B1%7C+%5Ctimes+%7Ca%7C+%2B+%28k-%7Ca%7C%29+%5Ctimes+%7Ca%2B1%7C+%3D+k+%7Ca%2B1%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7Ca%2B1%7C+%5Ctimes+%7Ca%7C+%2B+%28k-%7Ca%7C%29+%5Ctimes+%7Ca%2B1%7C+%3D+k+%7Ca%2B1%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  |a+1| \times |a| + (k-|a|) \times |a+1| = k |a+1|"></p><p>
 or </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0+%2B+k+%5Ctimes+%7Ca%2B1%7C+%3D+k+%7Ca%2B1%7C%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0+%2B+k+%5Ctimes+%7Ca%2B1%7C+%3D+k+%7Ca%2B1%7C%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0+%2B+k+%5Ctimes+%7Ca%2B1%7C+%3D+k+%7Ca%2B1%7C%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  0 + k \times |a+1| = k |a+1|,"></p><p>
 giving the claim. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\Box"></p><p>
Here is a figure illustrating the above construction in the <img src="https://s0.wp.com/latex.php?latex=%7Ba+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Ba+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Ba+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{a \geq 0}"> case (obtained after starting with a ChatGPT-provided file and then manually fixing a number of placement issues):
</p>
<p><img src="https://terrytao.wordpress.com/wp-content/uploads/2025/12/screenshot-2025-12-10-092536.png" width="500"></p><p>
Here is a plot of <img src="https://s0.wp.com/latex.php?latex=1%2Fc%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=1%2Fc%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=1%2Fc%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="1/c(n)"> (produced by ChatGPT Pro), showing that it is basically a piecewise linear approximation to the square root function:
</p>
<p>
<img src="https://terrytao.wordpress.com/wp-content/uploads/2025/12/plot.png" width="500">
</p>


<p>
Shortly afterwards, Lawrence Wu clarified the connection between this problem and a square packing problem, which was also <a href="https://www.erdosproblems.com/106">due to Erdős (Problem 106)</a>. Let <img src="https://s0.wp.com/latex.php?latex=%7Bf%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bf%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bf%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{f(n)}"> be the least number such that, whenever one packs <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{n}"> squares of sidelength <img src="https://s0.wp.com/latex.php?latex=%7Bd_1%2C%5Cdots%2Cd_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bd_1%2C%5Cdots%2Cd_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bd_1%2C%5Cdots%2Cd_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{d_1,\dots,d_n}"> into a square of sidelength <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{D}">, with all sides parallel to the coordinate axes, one has </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5En+d_i+%5Cleq+f%28n%29+D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5En+d_i+%5Cleq+f%28n%29+D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5En+d_i+%5Cleq+f%28n%29+D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \sum_{i=1}^n d_i \leq f(n) D."></p>

<blockquote><b>Proposition 2</b>  For any <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{n}">, one has <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++c%28n%29+%5Cgeq+%5Cfrac%7B1%7D%7Bf%28n%29%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++c%28n%29+%5Cgeq+%5Cfrac%7B1%7D%7Bf%28n%29%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++c%28n%29+%5Cgeq+%5Cfrac%7B1%7D%7Bf%28n%29%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  c(n) \geq \frac{1}{f(n)}."></p>
 </blockquote>

<p>
<em>Proof:</em>  Given <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_1,\dots,x_n}"> and <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Cleq+i+%5Cleq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B1+%5Cleq+i+%5Cleq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B1+%5Cleq+i+%5Cleq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{1 \leq i \leq n}">, let <img src="https://s0.wp.com/latex.php?latex=%7BS_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BS_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BS_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{S_i}"> be the maximal sum over all increasing subsequences ending in <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_i}">, and <img src="https://s0.wp.com/latex.php?latex=%7BT_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BT_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BT_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{T_i}"> be the maximal sum over all decreasing subsequences ending in <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_i}">. For <img src="https://s0.wp.com/latex.php?latex=%7Bi+%3C+j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bi+%3C+j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bi+%3C+j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{i < j}">, we have either <img src="https://s0.wp.com/latex.php?latex=%7BS_j+%5Cgeq+S_i+%2B+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BS_j+%5Cgeq+S_i+%2B+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BS_j+%5Cgeq+S_i+%2B+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{S_j \geq S_i + x_j}"> (if <img src="https://s0.wp.com/latex.php?latex=%7Bx_j+%5Cgeq+x_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_j+%5Cgeq+x_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_j+%5Cgeq+x_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_j \geq x_i}">) or <img src="https://s0.wp.com/latex.php?latex=%7BT_j+%5Cgeq+T_i+%2B+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BT_j+%5Cgeq+T_i+%2B+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BT_j+%5Cgeq+T_i+%2B+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{T_j \geq T_i + x_j}"> (if <img src="https://s0.wp.com/latex.php?latex=%7Bx_j+%5Cleq+x_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_j+%5Cleq+x_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_j+%5Cleq+x_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_j \leq x_i}">). In particular, the squares <img src="https://s0.wp.com/latex.php?latex=%7B%28S_i-x_i%2C+T_i-x_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%28S_i-x_i%2C+T_i-x_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%28S_i-x_i%2C+T_i-x_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{(S_i-x_i, T_i-x_i)}"> and <img src="https://s0.wp.com/latex.php?latex=%7B%28S_j-x_j%2C+T_j-x_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%28S_j-x_j%2C+T_j-x_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%28S_j-x_j%2C+T_j-x_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{(S_j-x_j, T_j-x_j)}"> are disjoint. These squares pack into the square <img src="https://s0.wp.com/latex.php?latex=%7B%5B0%2C+S%28x_1%2C%5Cdots%2Cx_n%29%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5B0%2C+S%28x_1%2C%5Cdots%2Cx_n%29%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5B0%2C+S%28x_1%2C%5Cdots%2Cx_n%29%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{[0, S(x_1,\dots,x_n)]^2}">, so by definition of <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{f}">, we have </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cleq+f%28n%29+S%28x_1%2C%5Cdots%2Cx_n%29%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cleq+f%28n%29+S%28x_1%2C%5Cdots%2Cx_n%29%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5En+x_i+%5Cleq+f%28n%29+S%28x_1%2C%5Cdots%2Cx_n%29%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \sum_{i=1}^n x_i \leq f(n) S(x_1,\dots,x_n),"></p><p>
 and the claim follows. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\Box"></p><p>
This idea of using packing to prove Erdős-Szekeres type results goes back to a 1959 paper of <a href="https://zbmath.org/0085.15003">Seidenberg</a>, although it was a discrete rectangle-packing argument that was not phrased in such an elegantly geometric form.  It is possible that Aristotle was “aware” of the Seidenberg argument via its training data, as it had incorporated a version of this argument in its proof.
</p>
<p>
Here is an illustration of the above argument using the AlphaEvolve-provided example </p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%5B99998%2C+99997%2C+116305%2C+117032%2C+116304%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%5B99998%2C+99997%2C+116305%2C+117032%2C+116304%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%5B99998%2C+99997%2C+116305%2C+117032%2C+116304%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle[99998, 99997, 116305, 117032, 116304,">
</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+58370%2C+83179%2C+117030%2C+92705%2C+99080%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+58370%2C+83179%2C+117030%2C+92705%2C+99080%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+58370%2C+83179%2C+117030%2C+92705%2C+99080%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle 58370, 83179, 117030, 92705, 99080]">
</p>
<p> for <img src="https://s0.wp.com/latex.php?latex=n%3D10&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=n%3D10&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=n%3D10&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="n=10"> to convert it to a square packing (image produced by ChatGPT Pro):
</p>
<p>
<img src="https://terrytao.wordpress.com/wp-content/uploads/2025/12/packing_n10_seidenberg.png" width="500">
</p>
<p>
At this point, Lawrence performed another AI deep research search, this time <a href="https://www.erdosproblems.com/forum/thread/1026#post-2108">successfully locating a paper</a> from just last year by <a href="https://arxiv.org/abs/2411.07274">Baek, Koizumi, and Ueoro</a>, where they show that
</p><blockquote><b>Theorem 3</b> <a name="fn-1"></a> For any <img src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k \geq 1}">, one has <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28k%5E2%2B1%29+%5Cleq+k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28k%5E2%2B1%29+%5Cleq+k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28k%5E2%2B1%29+%5Cleq+k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  f(k^2+1) \leq k"></p>
 </blockquote>

<p>
which, when combined with a <a href="https://zbmath.org/1178.52014">previous argument of Praton</a>, implies
</p><blockquote><b>Theorem 4</b> <a name="fn-2"></a> For any <img src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k \geq 1}"> and <img src="https://s0.wp.com/latex.php?latex=%7Bc+%5Cin+%7B%5Cbf+Z%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc+%5Cin+%7B%5Cbf+Z%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc+%5Cin+%7B%5Cbf+Z%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c \in {\bf Z}}"> with <img src="https://s0.wp.com/latex.php?latex=%7Bk%5E2%2B2c%2B1+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%5E2%2B2c%2B1+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%5E2%2B2c%2B1+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k^2+2c+1 \geq 1}">, one has <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28k%5E2%2B2c%2B1%29+%5Cleq+k+%2B+%5Cfrac%7Bc%7D%7Bk%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28k%5E2%2B2c%2B1%29+%5Cleq+k+%2B+%5Cfrac%7Bc%7D%7Bk%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28k%5E2%2B2c%2B1%29+%5Cleq+k+%2B+%5Cfrac%7Bc%7D%7Bk%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  f(k^2+2c+1) \leq k + \frac{c}{k}."></p>
 </blockquote>

<p>
This proves the conjecture!
</p><p>
There just remained the issue of putting everything together. I did feed all of the above information into a large language model, which was able to <a href="https://chatgpt.com/share/69373cfb-fa70-800e-af98-ade8926ca5e1">produce a coherent proof</a> of <a href="#conj">(1)</a> assuming the results of Baek-Koizumi-Ueoro and Praton. Of course, LLM outputs are prone to hallucination, so it would be preferable to formalize that argument in Lean, but this looks quite doable with current tools, and I expect this to be accomplished shortly. But I was also able to reproduce the arguments of Baek-Koizumi-Ueoro and Praton, which I include below for completeness.
</p><p>
<em>Proof:</em>  (Proof of Theorem <a href="#fn-1">3</a>, adapted from Baek-Koizumi-Ueoro) We can normalize <img src="https://s0.wp.com/latex.php?latex=%7BD%3Dk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BD%3Dk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BD%3Dk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{D=k}">. It then suffices to show that if we pack the length <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k}"> torus <img src="https://s0.wp.com/latex.php?latex=%7B%28%7B%5Cbf+Z%7D%2Fk%7B%5Cbf+Z%7D%29%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%28%7B%5Cbf+Z%7D%2Fk%7B%5Cbf+Z%7D%29%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%28%7B%5Cbf+Z%7D%2Fk%7B%5Cbf+Z%7D%29%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{({\bf Z}/k{\bf Z})^2}"> by <img src="https://s0.wp.com/latex.php?latex=%7Bk%5E2%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%5E2%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%5E2%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k^2+1}"> axis-parallel squares of sidelength <img src="https://s0.wp.com/latex.php?latex=%7Bd_1%2C%5Cdots%2Cd_%7Bk%5E2%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bd_1%2C%5Cdots%2Cd_%7Bk%5E2%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bd_1%2C%5Cdots%2Cd_%7Bk%5E2%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{d_1,\dots,d_{k^2+1}}">, then </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+d_i+%5Cleq+k%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+d_i+%5Cleq+k%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+d_i+%5Cleq+k%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \sum_{i=1}^{k^2+1} d_i \leq k^2."></p>

<p>
Pick <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%2C+y_0+%5Cin+%7B%5Cbf+R%7D%2Fk%7B%5Cbf+Z%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_0%2C+y_0+%5Cin+%7B%5Cbf+R%7D%2Fk%7B%5Cbf+Z%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_0%2C+y_0+%5Cin+%7B%5Cbf+R%7D%2Fk%7B%5Cbf+Z%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_0, y_0 \in {\bf R}/k{\bf Z}}">. Then we have a <img src="https://s0.wp.com/latex.php?latex=%7Bk+%5Ctimes+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk+%5Ctimes+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk+%5Ctimes+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k \times k}"> grid </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x_0+%2B+%7B%5Cbf+Z%7D%29+%5Ctimes+%28y_0+%2B+%7B%5Cbf+Z%7D%29+%5Cpmod+%7Bk%7B%5Cbf+Z%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x_0+%2B+%7B%5Cbf+Z%7D%29+%5Ctimes+%28y_0+%2B+%7B%5Cbf+Z%7D%29+%5Cpmod+%7Bk%7B%5Cbf+Z%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x_0+%2B+%7B%5Cbf+Z%7D%29+%5Ctimes+%28y_0+%2B+%7B%5Cbf+Z%7D%29+%5Cpmod+%7Bk%7B%5Cbf+Z%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  (x_0 + {\bf Z}) \times (y_0 + {\bf Z}) \pmod {k{\bf Z}^2}"></p><p>
 inside the torus. The <img src="https://s0.wp.com/latex.php?latex=%7Bi%5E%7Bth%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bi%5E%7Bth%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bi%5E%7Bth%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{i^{th}}"> square, when restricted to this grid, becomes a discrete rectangle <img src="https://s0.wp.com/latex.php?latex=%7BA_%7Bi%2Cx_0%7D+%5Ctimes+B_%7Bi%2Cy_0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BA_%7Bi%2Cx_0%7D+%5Ctimes+B_%7Bi%2Cy_0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BA_%7Bi%2Cx_0%7D+%5Ctimes+B_%7Bi%2Cy_0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{A_{i,x_0} \times B_{i,y_0}}"> for some finite sets <img src="https://s0.wp.com/latex.php?latex=%7BA_%7Bi%2Cx_0%7D%2C+B_%7Bi%2Cy_0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BA_%7Bi%2Cx_0%7D%2C+B_%7Bi%2Cy_0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BA_%7Bi%2Cx_0%7D%2C+B_%7Bi%2Cy_0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{A_{i,x_0}, B_{i,y_0}}"> with </p><a name="ao"><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%5C%23+A_%7Bi%2Cx_0%7D+-%5C%23+B_%7Bi%2Cy_0%7D%7C+%5Cleq+1.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%5C%23+A_%7Bi%2Cx_0%7D+-%5C%23+B_%7Bi%2Cy_0%7D%7C+%5Cleq+1.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%5C%23+A_%7Bi%2Cx_0%7D+-%5C%23+B_%7Bi%2Cy_0%7D%7C+%5Cleq+1.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  |\# A_{i,x_0} -\# B_{i,y_0}| \leq 1. \ \ \ \ \ (2)"></p>
</a><p> By the packing condition, we have </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+A_%7Bi%2Cx_0%7D+%5C%23+B_%7Bi%2Cy_0%7D+%5Cleq+k%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+A_%7Bi%2Cx_0%7D+%5C%23+B_%7Bi%2Cy_0%7D+%5Cleq+k%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+A_%7Bi%2Cx_0%7D+%5C%23+B_%7Bi%2Cy_0%7D+%5Cleq+k%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \sum_{i=1}^{k^2+1} \# A_{i,x_0} \# B_{i,y_0} \leq k^2."></p><p>
 From <a href="#ao">(2)</a> we have </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28%5C%23+A_%7Bi%2Cx_0%7D+-+1%29+%28%5C%23+B_%7Bi%2Cy_0%7D+-+1%29+%5Cgeq+0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28%5C%23+A_%7Bi%2Cx_0%7D+-+1%29+%28%5C%23+B_%7Bi%2Cy_0%7D+-+1%29+%5Cgeq+0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28%5C%23+A_%7Bi%2Cx_0%7D+-+1%29+%28%5C%23+B_%7Bi%2Cy_0%7D+-+1%29+%5Cgeq+0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  (\# A_{i,x_0} - 1) (\# B_{i,y_0} - 1) \geq 0"></p><p>
 hence </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5C%23+A_%7Bi%2Cx_0%7D+%5C%23+B_%7Bi%2Cy_0%7D+%5Cgeq+%5C%23+A_%7Bi%2Cx_0%7D+%2B+%5C%23+B_%7Bi%2Cy_0%7D+-+1.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5C%23+A_%7Bi%2Cx_0%7D+%5C%23+B_%7Bi%2Cy_0%7D+%5Cgeq+%5C%23+A_%7Bi%2Cx_0%7D+%2B+%5C%23+B_%7Bi%2Cy_0%7D+-+1.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5C%23+A_%7Bi%2Cx_0%7D+%5C%23+B_%7Bi%2Cy_0%7D+%5Cgeq+%5C%23+A_%7Bi%2Cx_0%7D+%2B+%5C%23+B_%7Bi%2Cy_0%7D+-+1.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \# A_{i,x_0} \# B_{i,y_0} \geq \# A_{i,x_0} + \# B_{i,y_0} - 1."></p><p>
 Inserting this bound and rearranging, we conclude that </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+A_%7Bi%2Cx_0%7D+%2B+%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+B_%7Bi%2Cy_0%7D+%5Cleq+2k%5E2+%2B+1.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+A_%7Bi%2Cx_0%7D+%2B+%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+B_%7Bi%2Cy_0%7D+%5Cleq+2k%5E2+%2B+1.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+A_%7Bi%2Cx_0%7D+%2B+%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+B_%7Bi%2Cy_0%7D+%5Cleq+2k%5E2+%2B+1.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \sum_{i=1}^{k^2+1} \# A_{i,x_0} + \sum_{i=1}^{k^2+1} \# B_{i,y_0} \leq 2k^2 + 1."></p><p>
 Taking the supremum over <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%2Cy_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bx_0%2Cy_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bx_0%2Cy_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{x_0,y_0}"> we conclude that </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx_0%7D+%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+A_%7Bi%2Cx_0%7D+%2B+%5Csup_%7By_0%7D+%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+B_%7Bi%2Cy_0%7D+%5Cleq+2k%5E2+%2B+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx_0%7D+%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+A_%7Bi%2Cx_0%7D+%2B+%5Csup_%7By_0%7D+%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+B_%7Bi%2Cy_0%7D+%5Cleq+2k%5E2+%2B+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx_0%7D+%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+A_%7Bi%2Cx_0%7D+%2B+%5Csup_%7By_0%7D+%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+B_%7Bi%2Cy_0%7D+%5Cleq+2k%5E2+%2B+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \sup_{x_0} \sum_{i=1}^{k^2+1} \# A_{i,x_0} + \sup_{y_0} \sum_{i=1}^{k^2+1} \# B_{i,y_0} \leq 2k^2 + 1"></p><p>
 so by the pigeonhole principle one of the summands is at most <img src="https://s0.wp.com/latex.php?latex=%7Bk%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k^2}">. Let’s say it is the former, thus </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx_0%7D+%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+A_%7Bi%2Cx_0%7D+%5Cleq+k%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx_0%7D+%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+A_%7Bi%2Cx_0%7D+%5Cleq+k%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx_0%7D+%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+A_%7Bi%2Cx_0%7D+%5Cleq+k%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \sup_{x_0} \sum_{i=1}^{k^2+1} \# A_{i,x_0} \leq k^2."></p><p>
 In particular, the average value of <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+A_%7Bi%2Cx_0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+A_%7Bi%2Cx_0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+%5C%23+A_%7Bi%2Cx_0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\sum_{i=1}^{k^2+1} \# A_{i,x_0}}"> is at most <img src="https://s0.wp.com/latex.php?latex=%7Bk%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k^2}">. But this can be computed to be <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+d_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+d_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B1%7D+d_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\sum_{i=1}^{k^2+1} d_i}">, giving the claim. Similarly if it is the other sum. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\Box"></p><p>
UPDATE: Actually, the above argument also proves Theorem 4 with only minor modifications.  Nevertheless, we give the original derivation of Theorem 4 using the embedding argument of Praton below for sake of completeness.
</p>
<p>
<em>Proof:</em>  (Proof of Theorem <a href="#fn-2">4</a>, adapted from Praton) We write <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+%5Cepsilon+%7Cc%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc+%3D+%5Cepsilon+%7Cc%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc+%3D+%5Cepsilon+%7Cc%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c = \epsilon |c|}"> with <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+%5Cpm+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+%5Cpm+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+%5Cpm+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\epsilon = \pm 1}">. We can rescale so that the square one is packing into is <img src="https://s0.wp.com/latex.php?latex=%7B%5B0%2Ck%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5B0%2Ck%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5B0%2Ck%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{[0,k]^2}">. Thus, we pack <img src="https://s0.wp.com/latex.php?latex=%7Bk%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k^2+2\varepsilon |c|+1}"> squares of sidelength <img src="https://s0.wp.com/latex.php?latex=%7Bd_1%2C%5Cdots%2Cd_%7Bk%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bd_1%2C%5Cdots%2Cd_%7Bk%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bd_1%2C%5Cdots%2Cd_%7Bk%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{d_1,\dots,d_{k^2+2\varepsilon |c|+1}}"> into <img src="https://s0.wp.com/latex.php?latex=%7B%5B0%2Ck%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5B0%2Ck%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5B0%2Ck%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{[0,k]^2}">, and our task is to show that </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B2%5Cvarepsilon%7Cc%7C%2B1%7D+d_i+%5Cleq+k%5E2+%2B+%5Cvarepsilon+%7Cc%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B2%5Cvarepsilon%7Cc%7C%2B1%7D+d_i+%5Cleq+k%5E2+%2B+%5Cvarepsilon+%7Cc%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B2%5Cvarepsilon%7Cc%7C%2B1%7D+d_i+%5Cleq+k%5E2+%2B+%5Cvarepsilon+%7Cc%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  \sum_{i=1}^{k^2+2\varepsilon|c|+1} d_i \leq k^2 + \varepsilon |c|."></p><p>
 We pick a large natural number <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{N}"> (in particular, larger than <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{k}">), and consider the three nested squares </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5B0%2Ck%5D%5E2+%5Csubset+%5B0%2CN%5D%5E2+%5Csubset+%5B0%2CN+%2B+%7Cc%7C+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D%5D%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5B0%2Ck%5D%5E2+%5Csubset+%5B0%2CN%5D%5E2+%5Csubset+%5B0%2CN+%2B+%7Cc%7C+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D%5D%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5B0%2Ck%5D%5E2+%5Csubset+%5B0%2CN%5D%5E2+%5Csubset+%5B0%2CN+%2B+%7Cc%7C+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D%5D%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  [0,k]^2 \subset [0,N]^2 \subset [0,N + |c| \frac{N}{N-\varepsilon}]^2."></p><p>
 We can pack <img src="https://s0.wp.com/latex.php?latex=%7B%5B0%2CN%5D%5E2+%5Cbackslash+%5B0%2Ck%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5B0%2CN%5D%5E2+%5Cbackslash+%5B0%2Ck%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5B0%2CN%5D%5E2+%5Cbackslash+%5B0%2Ck%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{[0,N]^2 \backslash [0,k]^2}"> by <img src="https://s0.wp.com/latex.php?latex=%7BN%5E2-k%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BN%5E2-k%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BN%5E2-k%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{N^2-k^2}"> unit squares. We can similarly pack </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5B0%2CN+%2B+%7Cc%7C+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D%5D%5E2+%5Cbackslash+%5B0%2CN%5D%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5B0%2CN+%2B+%7Cc%7C+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D%5D%5E2+%5Cbackslash+%5B0%2CN%5D%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5B0%2CN+%2B+%7Cc%7C+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D%5D%5E2+%5Cbackslash+%5B0%2CN%5D%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  [0,N + |c| \frac{N}{N-\varepsilon}]^2 \backslash [0,N]^2"></p>
 <p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D%5B0%2C+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D+%28N%2B%7Cc%7C-%5Cvarepsilon%29%5D%5E2+%5Cbackslash+%5B0%2C+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D+%28N-%5Cvarepsilon%29%5D%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D%5B0%2C+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D+%28N%2B%7Cc%7C-%5Cvarepsilon%29%5D%5E2+%5Cbackslash+%5B0%2C+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D+%28N-%5Cvarepsilon%29%5D%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D%5B0%2C+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D+%28N%2B%7Cc%7C-%5Cvarepsilon%29%5D%5E2+%5Cbackslash+%5B0%2C+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D+%28N-%5Cvarepsilon%29%5D%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  =[0, \frac{N}{N-\varepsilon} (N+|c|-\varepsilon)]^2 \backslash [0, \frac{N}{N-\varepsilon} (N-\varepsilon)]^2"></p><p>
 into <img src="https://s0.wp.com/latex.php?latex=%7B%28N%2B%7Cc%7C-%5Cvarepsilon%29%5E2+-+%28N-%5Cvarepsilon%29%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%28N%2B%7Cc%7C-%5Cvarepsilon%29%5E2+-+%28N-%5Cvarepsilon%29%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%28N%2B%7Cc%7C-%5Cvarepsilon%29%5E2+-+%28N-%5Cvarepsilon%29%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{(N+|c|-\varepsilon)^2 - (N-\varepsilon)^2}"> squares of sidelength <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{\frac{N}{N-\varepsilon}}">. All in all, this produces </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++k%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1+%2B+N%5E2-k%5E2+%2B+%28N%2B%7Cc%7C-%5Cvarepsilon%29%5E2+-+%28N-%5Cvarepsilon%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++k%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1+%2B+N%5E2-k%5E2+%2B+%28N%2B%7Cc%7C-%5Cvarepsilon%29%5E2+-+%28N-%5Cvarepsilon%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++k%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1+%2B+N%5E2-k%5E2+%2B+%28N%2B%7Cc%7C-%5Cvarepsilon%29%5E2+-+%28N-%5Cvarepsilon%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  k^2+2\varepsilon |c|+1 + N^2-k^2 + (N+|c|-\varepsilon)^2 - (N-\varepsilon)^2"></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%3D+%28N%2B%7Cc%7C%29%5E2+%2B+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%3D+%28N%2B%7Cc%7C%29%5E2+%2B+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%3D+%28N%2B%7Cc%7C%29%5E2+%2B+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle   = (N+|c|)^2 + 1"></p><p>
 squares, of total length </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1%7D+d_i%29+%2B%28N%5E2-k%5E2%29+%2B+%28%28N%2B%7Cc%7C-%5Cvarepsilon%29%5E2+-+%28N-%5Cvarepsilon%29%5E2%29+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1%7D+d_i%29+%2B%28N%5E2-k%5E2%29+%2B+%28%28N%2B%7Cc%7C-%5Cvarepsilon%29%5E2+-+%28N-%5Cvarepsilon%29%5E2%29+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1%7D+d_i%29+%2B%28N%5E2-k%5E2%29+%2B+%28%28N%2B%7Cc%7C-%5Cvarepsilon%29%5E2+-+%28N-%5Cvarepsilon%29%5E2%29+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle (\sum_{i=1}^{k^2+2\varepsilon |c|+1} d_i) +(N^2-k^2) + ((N+|c|-\varepsilon)^2 - (N-\varepsilon)^2) \frac{N}{N-\varepsilon}."></p><p>
 Applying Theorem <a href="#fn-1">3</a>, we conclude that </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1%7D+d_i%29+%2B%28N%5E2-k%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1%7D+d_i%29+%2B%28N%5E2-k%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1%7D+d_i%29+%2B%28N%5E2-k%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle (\sum_{i=1}^{k^2+2\varepsilon |c|+1} d_i) +(N^2-k^2)"></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%2B+%28%28N%2B%7Cc%7C-%5Cvarepsilon%29%5E2+-+%28N-%5Cvarepsilon%29%5E2%29+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D+%5Cleq+%28N%2B%7Cc%7C%29+%28N+%2B+%7Cc%7C+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%2B+%28%28N%2B%7Cc%7C-%5Cvarepsilon%29%5E2+-+%28N-%5Cvarepsilon%29%5E2%29+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D+%5Cleq+%28N%2B%7Cc%7C%29+%28N+%2B+%7Cc%7C+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%2B+%28%28N%2B%7Cc%7C-%5Cvarepsilon%29%5E2+-+%28N-%5Cvarepsilon%29%5E2%29+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D+%5Cleq+%28N%2B%7Cc%7C%29+%28N+%2B+%7Cc%7C+%5Cfrac%7BN%7D%7BN-%5Cvarepsilon%7D%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  + ((N+|c|-\varepsilon)^2 - (N-\varepsilon)^2) \frac{N}{N-\varepsilon} \leq (N+|c|) (N + |c| \frac{N}{N-\varepsilon})."></p><p>
 The right-hand side is </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++N%5E2+%2B+2%7Cc%7C+N+%2B+%7Cc%7C%5E2+%2B+%5Cvarepsilon+%7Cc%7C+%2B+O%281%2FN%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++N%5E2+%2B+2%7Cc%7C+N+%2B+%7Cc%7C%5E2+%2B+%5Cvarepsilon+%7Cc%7C+%2B+O%281%2FN%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++N%5E2+%2B+2%7Cc%7C+N+%2B+%7Cc%7C%5E2+%2B+%5Cvarepsilon+%7Cc%7C+%2B+O%281%2FN%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle  N^2 + 2|c| N + |c|^2 + \varepsilon |c| + O(1/N)"></p><p>
 and the left-hand side similarly evaluates to </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B2c%2B1%7D+d_i%29+%2B+N%5E2+-k%5E2+%2B+2%7Cc%7C+N+%2B+%7Cc%7C%5E2+%2B+O%281%2FN%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B2c%2B1%7D+d_i%29+%2B+N%5E2+-k%5E2+%2B+2%7Cc%7C+N+%2B+%7Cc%7C%5E2+%2B+O%281%2FN%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%28%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B2c%2B1%7D+d_i%29+%2B+N%5E2+-k%5E2+%2B+2%7Cc%7C+N+%2B+%7Cc%7C%5E2+%2B+O%281%2FN%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle (\sum_{i=1}^{k^2+2c+1} d_i) + N^2 -k^2 + 2|c| N + |c|^2 + O(1/N)"></p><p>
 and so we simplify to </p><p><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1%7D+d_i+%5Cleq+k%5E2+%2B+%5Cvarepsilon+%7Cc%7C+%2B+O%281%2FN%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1%7D+d_i+%5Cleq+k%5E2+%2B+%5Cvarepsilon+%7Cc%7C+%2B+O%281%2FN%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csum_%7Bi%3D1%7D%5E%7Bk%5E2%2B2%5Cvarepsilon+%7Cc%7C%2B1%7D+d_i+%5Cleq+k%5E2+%2B+%5Cvarepsilon+%7Cc%7C+%2B+O%281%2FN%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\displaystyle \sum_{i=1}^{k^2+2\varepsilon |c|+1} d_i \leq k^2 + \varepsilon |c| + O(1/N)."></p><p>
 Sending <img src="https://s0.wp.com/latex.php?latex=%7BN+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7BN+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7BN+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{N \rightarrow \infty}">, we obtain the claim. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="\Box"></p><p>
One striking feature of this story for me is how important it was to have a diverse set of people, literature, and tools to attack this problem.  To be able to state and prove the precise formula for <img src="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%7Bc%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="{c(n)}"> required multiple observations, including some version of the following:
</p><ul>
<li> The sequence can be numerically computed as a sequence of rational numbers.
</li><li> When appropriately normalized and arranged, visible patterns in this sequence appear that allow one to conjecture the form of the sequence.
</li><li> This problem is a weighted version of the Erdős-Szekeres theorem.
</li><li> Among the many proofs of the Erdős-Szekeres theorem is the proof of Seidenberg in 1959, which can be interpreted as a discrete rectangle packing argument.
</li><li> This problem can be reinterpreted as a continuous square packing problem, and in fact is closely related to (a generalized axis-parallel form of) Erdős problem 106, which concerns such packings.
</li><li> The axis-parallel form of Erdős problem 106 was recently solved by Baek-Koizumi-Ueoro.
</li><li> The paper of Praton shows that Erdős Problem 106 implies the generalized version needed for this problem.  This implication specializes to the axis-parallel case.
</li></ul><p>
It was only through the combined efforts of all the contributors and their tools that all these key inputs were able to be assembled within 48 hours.  It seems plausible that a more traditional effort involving just one or two mathematicians and simpler programming and literature search tools may eventually have been able to put all these pieces together, but I believe this process would have taken much longer (on the order of weeks or even months).
</p><p>
Another key ingredient was the <a href="https://www.erdosproblems.com/forum/">balanced AI policy</a> on the Erdős problem website, which encourages disclosed AI usage while strongly discouraging undisclosed use.  To quote from that policy: “Comments prepared with the assistance of AI are permitted, provided (a) this is disclosed, (b) the contents (including mathematics, code, numerical data, and the existence of relevant sources) have been carefully checked and verified by the user themselves without the assistance of AI, and (c) the comment is not unreasonably long.”</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sharp (481 pts)]]></title>
            <link>https://apple.github.io/ml-sharp/</link>
            <guid>46284658</guid>
            <pubDate>Tue, 16 Dec 2025 04:06:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apple.github.io/ml-sharp/">https://apple.github.io/ml-sharp/</a>, See on <a href="https://news.ycombinator.com/item?id=46284658">Hacker News</a></p>
<div id="readability-page-1" class="page"><div rel="icon" type="image/x-icon" href="thumbnails/favicon.ico">
    

    <!-- jQuery and TwentyTwenty -->
    
    
    

    <!-- Bootstrap JS -->
    


    

    

    
    


    


    <!-- Fixed Top Navbar -->
    <nav>
        
    </nav>

    <!-- Header Section -->
    <div>
        

        <p>
            Lars Mescheder, Wei Dong, Shiwei Li, Xuyang Bai, Marcel Santos, Peiyun Hu, Bruno Lecouat, Mingmin Zhen,
        </p>
        <p>
            Amaël Delaunoy, Tian Fang, Yanghai Tsin, Stephan R. Richter, Vladlen Koltun
        </p>
        <p>Apple</p>

        
    </div>

    <!-- Abstract Section -->
    <div id="abstract">
        <h2>Abstract</h2>
        <p>We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25–34% and DISTS by 21–43% versus the best prior model, while lowering the synthesis time by three orders of magnitude.</p>

        <div>
            
            <div>
                <p>Views synthesized by SHARP</p>
            </div>
            <p><img src="https://apple.github.io/ml-sharp/teaser.jpg" alt="SHARP">
            </p>
        </div>
        <p><small>SHARP synthesizes a photorealistic 3D representation from a single photograph in less
than a second. The synthesized representation supports high-resolution rendering of nearby views,
with sharp details and fine structures, at more than 100 frames per second on a standard GPU. We
illustrate on photographs from <a href="https://unsplash.com/" target="_blank">Unsplash</a>.</small></p>
    </div>

    <!-- Comparison Section -->
    <div id="videos">
        <h2>Video Comparisons</h2>

        <!-- Dataset Navigation Bar -->
        

        <!-- Dataset Content Areas -->
        <div id="content-Unsplash">
                    <p>Select a video to compare</p>
                </div>
        <div id="content-ETH3D">
                    <p>Select a video to compare</p>
                </div>
        <div id="content-Middlebury">
                    <p>Select a video to compare</p>
                </div>
        <div id="content-ScanNetPP">
                    <p>Select a video to compare</p>
                </div>
        <div id="content-TanksAndTemples">
                    <p>Select a video to compare</p>
                </div>
        <div id="content-Booster">
                    <p>Select a video to compare</p>
                </div>
        <div id="content-WildRGBD">
                    <p>Select a video to compare</p>
                </div>

    </div>

    <!-- Citation -->
    <div id="citation">
        <h2>Citation</h2>
        <div>
                <pre><code>@inproceedings{Sharp2025:arxiv,
  title      = {Sharp Monocular View Synthesis in Less Than a Second},
  author     = {Lars Mescheder and Wei Dong and Shiwei Li and Xuyang Bai and Marcel Santos and Peiyun Hu and Bruno Lecouat and Mingmin Zhen and Ama\"{e}l Delaunoyand Tian Fang and Yanghai Tsin and Stephan R. Richter and Vladlen Koltun},
  journal    = {arXiv preprint arXiv:2512.10685},
  year       = {2025},
  url        = {https://arxiv.org/abs/2512.10685},
}</code></pre>
            </div>
    </div>

    <!-- Footer -->
    



</div></div>]]></description>
        </item>
    </channel>
</rss>