<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 25 Feb 2024 23:00:13 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Microsoft Is Driving Users Away (129 pts)]]></title>
            <link>https://christitus.com/microsoft-is-driving-users-away/</link>
            <guid>39504703</guid>
            <pubDate>Sun, 25 Feb 2024 21:01:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://christitus.com/microsoft-is-driving-users-away/">https://christitus.com/microsoft-is-driving-users-away/</a>, See on <a href="https://news.ycombinator.com/item?id=39504703">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>What will be the end of Windows? Microsoft.</p>
<p>I‚Äôve been a lifelong Windows user since the 90s and even MS-DOS back in the 80s, but things are changing. I‚Äôve been using other operating systems more and more. MacOS and Linux are getting mixed into my workflow more and Windows less. Why? Microsoft is actively angering it power users by removing features. By trying to simplify Windows they are actively making administering it worse.</p>
<h2 id="the-problem">The Problem</h2>
<p>Microsoft is in a panic from the continued loss of it‚Äôs user base. They are making some improvements like the GUI and cleaning up other elements that give a more cohesive look. No more mixing Metro UI and Legacy panels like Windows 10. While this is good on the surface, they aren‚Äôt fixing anything. In fact, things are about to get much worse with the removal of these panels. On Tuesday, I had a printer issue at a client site and I was going to pull up the ‚ÄúDevices and Printers‚Äù panel to manage the drives and printing default settings. In the most recent 22H2 Windows 11 update, they hid it!</p>
<p>Using old Control Panel (Start-&gt;Run-&gt;<code>control</code>)</p>
<p>Clicking on this will redirect you to the new settings printer that gives NO options to do any advanced troubleshooting and removes the ability to FIX the issue!</p>
<p>What about the old <code>control printers</code> shortcut? remapped to the new crap.</p>
<h2 id="the-solution">The Solution</h2>
<p>With any Windows problem, Microsoft never really removes ANYTHING! Remember, this company is still using legacy code from the 90s in spots and just slapping in new coat of paint on top of it every 5 years when they need to extract more money from its users.</p>
<p>To Access the old devices and printer use the run prompt and type the following:</p>
<div><pre tabindex="0"><code data-lang="fallback"><span><span>shell:::{A8A91A66-3A7D-4424-8D24-04E180695C7A}
</span></span></code></pre></div><p>Alternatively, you can create a new shortcut and assign the key combination to it.</p>
<ul>
<li>Create a new shortcut on your desktop</li>
<li>Set the location to: explorer.exe shell:::{A8A91A66-3A7D-4424-8D24-04E180695C7A}</li>
<li>Set the name to Devices and Printers</li>
<li>Choose the shortcut icon</li>
<li>Add the key combination shortcut (CTRL + ALT + D is default for this window)</li>
</ul>
<p>This provides you an easy to access link as well as overwriting the keyboard shortcut to open the old version.</p>
<p>Success for now‚Ä¶</p>
<h2 id="the-future">The Future</h2>
<p>MacOS and Linux will keep gaining market share. Microsoft needs to stop actively sabotaging it‚Äôs users. MacOS is bland and hasn‚Äôt changed in years, while Linux has a technical barrier of entry with little support. Yet, both of these options are better than modern Windows unless you need Gaming. However, Steam with it‚Äôs steamdeck might be changing the game‚Ä¶</p>
<h2 id="walkthrough-video">Walkthrough Video</h2>

<p>
  <iframe src="https://www.youtube.com/embed/q7JmN8_URGY" allowfullscreen="" title="YouTube Video"></iframe>
</p>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Coroutines in C (2000) (164 pts)]]></title>
            <link>https://www.chiark.greenend.org.uk/~sgtatham/coroutines.html</link>
            <guid>39502276</guid>
            <pubDate>Sun, 25 Feb 2024 16:45:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chiark.greenend.org.uk/~sgtatham/coroutines.html">https://www.chiark.greenend.org.uk/~sgtatham/coroutines.html</a>, See on <a href="https://news.ycombinator.com/item?id=39502276">Hacker News</a></p>
<div id="readability-page-1" class="page">


<p><i>by <a href="http://pobox.com/~anakin/">Simon Tatham</a></i>

</p><p>[Coroutines trilogy: <b>C preprocessor</b> | <a href="https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/coroutines-c++20/">C++20 native</a> | <a href="https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/coroutines-philosophy/">general philosophy</a> ]</p>

<!-- begin essay -->

<a name="intro"><h2>Introduction</h2></a>

<p>
Structuring a large program is always a difficult job. One of the
particular problems that often comes up is this: if you have a piece
of code producing data, and another piece of code consuming it,
which should be the caller and which should be the callee?

</p><p>
Here is a very simple piece of run-length decompression code, and an
equally simple piece of parser code:

</p><p>
<table>
<tbody><tr>
<td>
<pre><code>    /* Decompression code */
    while (1) {
        c = getchar();
        if (c == EOF)
            break;
        if (c == 0xFF) {
            len = getchar();
            c = getchar();
            while (len--)
                emit(c);
        } else
            emit(c);
    }
    emit(EOF);</code></pre>
</td>
<td>
<pre><code>    /* Parser code */
    while (1) {
        c = getchar();
        if (c == EOF)
            break;
        if (isalpha(c)) {
            do {
                add_to_token(c);
                c = getchar();
            } while (isalpha(c));
            got_token(WORD);
        }
        add_to_token(c);
        got_token(PUNCT);
    }</code></pre>
</td></tr></tbody></table>

</p><p>
Each of these code fragments is very simple, and easy to read and
understand. One produces a character at a time by calling
<code>emit()</code>; the other consumes a character at a time by
calling <code>getchar()</code>. If only the calls to
<code>emit()</code> and the calls to <code>getchar()</code> could be
made to feed data to each other, it would be simple to connect the
two fragments together so that the output from the decompressor went
straight to the parser.

</p><p>
In many modern operating systems, you could do this using pipes
between two processes or two threads. <code>emit()</code> in the
decompressor writes to a pipe, and <code>getchar()</code> in the
parser reads from the other end of the same pipe. Simple and robust,
but also heavyweight and not portable. Typically you don't want to
have to divide your program into threads for a task this simple.

</p><p>
In this article I offer a creative solution to this sort of
structure problem.
</p>

<a name="rewrite"><h2>Rewriting</h2></a>

<p>
The conventional answer is to rewrite one of the ends of the
communication channel so that it's a function that can be called.
Here's an example of what that might mean for each of the example
fragments.

</p><p>
<table>
<tbody><tr>
<td>
<pre><code>int decompressor(void) {
    static int repchar;
    static int replen;
    if (replen &gt; 0) {
        replen--;
        return repchar;
    }
    c = getchar();
    if (c == EOF)
        return EOF;
    if (c == 0xFF) {
        replen = getchar();
        repchar = getchar();
        replen--;
        return repchar;
    } else
        return c;
}</code></pre>
</td>
<td>
<pre><code>void parser(int c) {
    static enum {
        START, IN_WORD
    } state;
    switch (state) {
        case IN_WORD:
        if (isalpha(c)) {
            add_to_token(c);
            return;
        }
        got_token(WORD);
        state = START;
        /* fall through */

        case START:
        add_to_token(c);
        if (isalpha(c))
            state = IN_WORD;
        else
            got_token(PUNCT);
        break;
    }
}</code></pre>
</td></tr></tbody></table>

</p><p>
Of course you don't have to rewrite both of them; just one will do.
If you rewrite the decompressor in the form shown, so that it
returns one character every time it's called, then the original
parser code can replace calls to <code>getchar()</code> with calls
to <code>decompressor()</code>, and the program will be happy.
Conversely, if you rewrite the parser in the form shown, so that it
is called once for every input character, then the original
decompression code can call <code>parser()</code> instead of
<code>emit()</code> with no problems. You would only want to rewrite
<em>both</em> functions as callees if you were a glutton for
punishment.

</p><p>
And that's the point, really. Both these rewritten functions are
thoroughly ugly compared to their originals. Both of the processes
taking place here are easier to read when written as a caller, not
as a callee. Try to deduce the grammar recognised by the parser, or
the compressed data format understood by the decompressor, just by
reading the code, and you will find that both the originals are
clear and both the rewritten forms are less clear. It would be much
nicer if we didn't have to turn either piece of code inside out.

<a name="knuth"></a></p><h2><a name="knuth">Knuth's coroutines</a></h2>

<p>
In <cite>The Art of Computer Programming</cite>, Donald Knuth
presents a solution to this sort of problem. His answer is to throw
away the stack concept completely. Stop thinking of one process as
the caller and the other as the callee, and start thinking of them
as cooperating equals.

</p><p>
In practical terms: replace the traditional "call" primitive with a
slightly different one. The new "call" will save the return value
somewhere other than on the stack, and will then jump to a location
specified in another saved return value. So each time the
decompressor emits another character, it saves its program counter
and jumps to the last known location within the parser - and each
time the parser <em>needs</em> another character, it saves its own
program counter and jumps to the location saved by the decompressor.
Control shuttles back and forth between the two routines exactly as
often as necessary.

</p><p>
This is very nice in theory, but in practice you can only do it in
assembly language, because no commonly used high level language
supports the coroutine call primitive. Languages like C depend
utterly on their stack-based structure, so whenever control passes
from any function to any other, one must be the caller and the other
must be the callee. So if you want to write portable code, this
technique is at least as impractical as the Unix pipe solution.

<a name="knuth"></a></p><h2><a name="knuth">Stack-based coroutines</a></h2>

<p>
So what we would <em>really</em> like is the ability to mimic
Knuth's coroutine call primitive in C. We must accept that in
reality, at the C level, one function will be caller and the other
will be callee. In the caller, we have no problem; we code the
original algorithm, pretty much exactly as written, and whenever it
has (or needs) a character it calls the other function.

</p><p>
The callee has all the problems. For our callee, we want a function
which has a "return and continue" operation: return from the
function, and next time it is called, resume control from just after
the <em>return</em> statement. For example, we would like to be able
to write a function that says

</p><pre><code>int function(void) {
    int i;
    for (i = 0; i &lt; 10; i++)
        return i;   /* won't work, but wouldn't it be nice */
}</code></pre>

<p>
and have ten successive calls to the function return the numbers 0
through 9.

</p><p>
How can we implement this? Well, we can transfer control to an
arbitrary point in the function using a <code>goto</code> statement.
So if we use a state variable, we could do this:

</p><pre><code>int function(void) {
    static int i, state = 0;
    switch (state) {
        case 0: goto LABEL0;
        case 1: goto LABEL1;
    }
    LABEL0: /* start of function */
    for (i = 0; i &lt; 10; i++) {
        state = 1; /* so we will come back to LABEL1 */
        return i;
        LABEL1:; /* resume control straight after the return */
    }
}</code></pre>

<p>
This method works. We have a set of labels at the points where we
might need to resume control: one at the start, and one just after
each <code>return</code> statement. We have a state variable,
preserved between calls to the function, which tells us which label
we need to resume control at next. Before any return, we update the
state variable to point at the right label; after any call, we do a
<code>switch</code> on the value of the variable to find out where
to jump to.

</p><p>
It's still ugly, though. The worst part of it is that the set of
labels must be maintained manually, and must be consistent between
the function body and the initial <code>switch</code> statement.
Every time we add a new return statement, we must invent a new label
name and add it to the list in the <code>switch</code>; every time
we remove a return statement, we must remove its corresponding
label. We've just increased our maintenance workload by a factor of
two.

<a name="duff"></a></p><h2><a name="duff">Duff's device</a></h2>

<p>
The famous "Duff's device" in C makes use of the fact that a
<code>case</code> statement is still legal within a sub-block of its
matching <code>switch</code> statement. Tom Duff used this for an
optimised output loop:

</p><pre><code>    switch (count % 8) {
        case 0:        do {  *to = *from++;
        case 7:              *to = *from++;
        case 6:              *to = *from++;
        case 5:              *to = *from++;
        case 4:              *to = *from++;
        case 3:              *to = *from++;
        case 2:              *to = *from++;
        case 1:              *to = *from++;
                       } while ((count -= 8) &gt; 0);
    }</code></pre>

<p>
We can put it to a slightly different use in the coroutine trick.
Instead of using a <code>switch</code> statement to decide which
<code>goto</code> statement to execute, we can use the
<code>switch</code> statement to perform the jump itself:

</p><pre><code>int function(void) {
    static int i, state = 0;
    switch (state) {
        case 0: /* start of function */
        for (i = 0; i &lt; 10; i++) {
            state = 1; /* so we will come back to "case 1" */
            return i;
            case 1:; /* resume control straight after the return */
        }
    }
}</code></pre>

<p>
Now this is looking promising. All we have to do now is construct a
few well chosen macros, and we can hide the gory details in
something plausible-looking:

</p><pre><code>#define crBegin static int state=0; switch(state) { case 0:
#define crReturn(i,x) do { state=i; return x; case i:; } while (0)
#define crFinish }
int function(void) {
    static int i;
    crBegin;
    for (i = 0; i &lt; 10; i++)
        crReturn(1, i);
    crFinish;
}</code></pre>

<p>
(note the use of <code>do ... while(0)</code> to ensure that
<code>crReturn</code> does not need braces around it when it comes
directly between <code>if</code> and <code>else</code>)

</p><p>
This is almost exactly what we wanted. We can use
<code>crReturn</code> to return from the function in such a way that
control at the next call resumes just after the return. Of course we
must obey some ground rules (surround the function body with
<code>crBegin</code> and <code>crFinish</code>; declare all local
variables <code>static</code> if they need to be preserved across a
<code>crReturn</code>; <em>never</em> put a <code>crReturn</code>
within an explicit <code>switch</code> statement); but those do not
limit us very much.

</p><p>
The only snag remaining is the first parameter to
<code>crReturn</code>. Just as when we invented a new label in the
previous section we had to avoid it colliding with existing label
names, now we must ensure all our state parameters to
<code>crReturn</code> are different. The consequences will be fairly
benign - the compiler will catch it and not let it do horrible
things at run time - but we still need to avoid doing it.

</p><p>
Even this can be solved. ANSI C provides the special macro name
<code>__LINE__</code>, which expands to the current source line
number. So we can rewrite <code>crReturn</code> as

</p><pre><code>#define crReturn(x) do { state=__LINE__; return x; \
                         case __LINE__:; } while (0)</code></pre>

<p>
and then we no longer have to worry about those state parameters at
all, provided we obey a fourth ground rule (never put two
<code>crReturn</code> statements on the same line).

<a name="evaluation"></a></p><h2><a name="evaluation">Evaluation</a></h2>

<p>
So now we have this monstrosity, let's rewrite our original code
fragments using it.

</p><p>
<table>
<tbody><tr>
<td>
<pre><code>int decompressor(void) {
    static int c, len;
    crBegin;
    while (1) {
        c = getchar();
        if (c == EOF)
            break;
        if (c == 0xFF) {
            len = getchar();
            c = getchar();
            while (len--)
	        crReturn(c);
        } else
	    crReturn(c);
    }
    crReturn(EOF);
    crFinish;
}</code></pre>
</td>
<td>
<pre><code>void parser(int c) {
    crBegin;
    while (1) {
        /* first char already in c */
        if (c == EOF)
            break;
        if (isalpha(c)) {
            do {
                add_to_token(c);
		crReturn( );
            } while (isalpha(c));
            got_token(WORD);
        }
        add_to_token(c);
        got_token(PUNCT);
	crReturn( );
    }
    crFinish;
}</code></pre>
</td></tr></tbody></table>

</p><p>
We have rewritten both decompressor and parser as callees, with no
need at all for the massive restructuring we had to do last time we
did this. The structure of each function exactly mirrors the
structure of its original form. A reader can deduce the grammar
recognised by the parser, or the compressed data format used by the
decompressor, far more easily than by reading the obscure
state-machine code. The control flow is intuitive once you have
wrapped your mind around the new format: when the decompressor has a
character, it passes it back to the caller with
<code>crReturn</code> and waits to be called again when another
character is required. When the parser needs another character, it
returns using <code>crReturn</code>, and waits to be called again
with the new character in the parameter <code>c</code>.

</p><p>
There has been one small structural alteration to the code:
<code>parser()</code> now has its <code>getchar()</code> (well, the
corresponding <code>crReturn</code>) at the end of the loop instead
of the start, because the first character is already in
<code>c</code> when the function is entered. We could accept this
small change in structure, or if we really felt strongly about it we
could specify that <code>parser()</code> required an
"initialisation" call before you could start feeding it characters.

</p><p>
As before, of course, we don't have to rewrite both routines using
the coroutine macros. One will suffice; the other can be its caller.

</p><p>
We have achieved what we set out to achieve: a portable ANSI C means
of passing data between a producer and a consumer without the need
to rewrite one as an explicit state machine. We have done this by
combining the C preprocessor with a little-used feature of the
<code>switch</code> statement to create an <em>implicit</em> state
machine.

<a name="standards"></a></p><h2><a name="standards">Coding Standards</a></h2>

<p>
Of course, this trick violates every coding standard in the book.
Try doing this in your company's code and you will probably be
subject to a stern telling off if not disciplinary action! You have
embedded unmatched braces in macros, used <code>case</code> within
sub-blocks, and as for the <code>crReturn</code> macro with its
terrifyingly disruptive contents .&nbsp;.&nbsp;. It's a wonder you
haven't been fired on the spot for such irresponsible coding
practice. You should be ashamed of yourself.

</p><p>
I would claim that the coding standards are at fault here. The
examples I've shown in this article are not very long, not very
complicated, and still just about comprehensible when rewritten as
state machines. But as the functions get longer, the degree of
rewriting required becomes greater and the loss of clarity becomes
much, much worse.

</p><p>
Consider. A function built of small blocks of the form

</p><pre><code>    case STATE1:
    /* perform some activity */
    if (condition) state = STATE2; else state = STATE3;</code></pre>

<p>
is not very different, to a reader, from a function built of small
blocks of the form

</p><pre><code>    LABEL1:
    /* perform some activity */
    if (condition) goto LABEL2; else goto LABEL3;</code></pre>

<p>
One is caller and the other is callee, true, but the visual
structure of the functions are the same, and the insights they
provide into their underlying algorithms are exactly as small as
each other. The same people who would fire you for using my
coroutine macros would fire you just as loudly for building a
function out of small blocks connected by <code>goto</code>
statements! And this time they would be right, because laying out a
function like that obscures the structure of the algorithm horribly.

</p><p>
Coding standards aim for clarity. By hiding vital things like
<code>switch</code>, <code>return</code> and <code>case</code>
statements inside "obfuscating" macros, the coding standards would
claim you have obscured the syntactic structure of the program, and
violated the requirement for clarity. But you have done so in the
cause of revealing the <em>algorithmic</em> structure of the
program, which is far more likely to be what the reader wants to
know!

</p><p>
Any coding standard which insists on syntactic clarity at the
expense of algorithmic clarity should be rewritten. If your employer
fires you for using this trick, tell them that repeatedly as the
security staff drag you out of the building.

<a name="code"></a></p><h2><a name="code">Refinements and Code</a></h2>

<p>
In a serious application, this toy coroutine implementation is
unlikely to be useful, because it relies on <code>static</code>
variables and so it fails to be re-entrant or multi-threadable.
Ideally, in a real application, you would want to be able to call
the same function in several different contexts, and at each call in
a given context, have control resume just after the last return in
the same context.

</p><p>
This is easily enough done. We arrange an extra function parameter,
which is a pointer to a context structure; we declare all our local
state, and our coroutine state variable, as elements of that
structure.

</p><p>
It's a little bit ugly, because suddenly you have to use
<code>ctx-&gt;i</code> as a loop counter where you would previously
just have used <code>i</code>; virtually all your serious variables
become elements of the coroutine context structure. But it removes
the problems with re-entrancy, and still hasn't impacted the
<em>structure</em> of the routine.

</p><p>
(Of course, if C only had Pascal's <code>with</code> statement, we
could arrange for the macros to make this layer of indirection truly
transparent as well. A pity. Still, at least C++ users can manage
this by having their coroutine be a class member, and keeping all
its local variables in the class so that the scoping is implicit.)

</p><p>
Included here is a C header file that implements this coroutine
trick as a set of pre-defined macros. There are two sets of macros
defined in the file, prefixed <code>scr</code> and <code>ccr</code>.
The <code>scr</code> macros are the simple form of the technique,
for when you can get away with using <code>static</code> variables;
the <code>ccr</code> macros provide the advanced re-entrant form.
Full documentation is given in a comment in the header file itself.

</p><p>
Note that Visual C++ version 6 doesn't like this coroutine trick,
because its default debug state (Program Database for Edit and
Continue) does something strange to the <code>__LINE__</code> macro.
To compile a coroutine-using program with VC++ 6, you must turn off
Edit and Continue. (In the project settings, go to the "C/C++" tab,
category "General", setting "Debug info". Select any option
<em>other</em> than "Program Database for Edit and Continue".)

</p><p>
(The header file is MIT-licensed, so you can use it in anything you
like without restriction. If you do find something the MIT licence
doesn't permit you to do,
<a href="mailto:anakin@pobox.com">mail me</a>,
and I'll probably give you explicit permission to do it anyway.)

</p><p>
<a href="https://www.chiark.greenend.org.uk/~sgtatham/coroutine.h">Follow this link</a> for <code>coroutine.h</code>.

</p><p>
Thanks for reading. Share and enjoy!

<a name="references"></a></p><h2><a name="references">References</a></h2>

<ul>

<li>
Donald Knuth, <cite>The Art of Computer Programming</cite>, Volume
1. Addison-Wesley, ISBN 0-201-89683-4. Section 1.4.2 describes
coroutines in the "pure" form.

</li><li>
<a href="http://www.lysator.liu.se/c/duffs-device.html">http://www.lysator.liu.se/c/duffs-device.html</a>
is Tom Duff's own discussion of Duff's device. Note, right at the
bottom, a hint that Duff might also have independently invented this
coroutine trick or something very like it.
<p>
<b>Update, 2005-03-07</b>:
<a href="http://brainwagon.org/2005/03/05/coroutines-in-c/#comment-1878">Tom
Duff confirms this</a> in a blog comment. The "revolting way to use
switches to implement interrupt driven state machines" of which he
speaks in his original email is indeed the same trick as I describe
here.

</p></li><li>
<a href="https://www.chiark.greenend.org.uk/~sgtatham/putty/">PuTTY</a>
is a Win32 Telnet and SSH client. The SSH protocol code contains
real-life use of this coroutine trick. As far as I know, this is the
worst piece of C hackery ever seen in serious production code.
</li></ul>

<!-- end essay -->

<hr>

<p>
<i>$Id$</i>

</p><p>
Copyright ¬© 2000 Simon Tatham.
<br>This document is
<a href="https://www.opencontent.org/">OpenContent</a>.
<br>You may copy and use the text under the terms of the
<a href="https://www.opencontent.org/openpub/">OpenContent
Licence</a>.
<br>Please send comments and criticism to
<a href="mailto:anakin@pobox.com">anakin@pobox.com</a>.

</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Efficient recovery and recycling of cobalt from spent lithium-ion batteries (125 pts)]]></title>
            <link>https://pubs.acs.org/doi/10.1021/acsomega.2c07780</link>
            <guid>39502165</guid>
            <pubDate>Sun, 25 Feb 2024 16:33:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pubs.acs.org/doi/10.1021/acsomega.2c07780">https://pubs.acs.org/doi/10.1021/acsomega.2c07780</a>, See on <a href="https://news.ycombinator.com/item?id=39502165">Hacker News</a></p>
Couldn't get https://pubs.acs.org/doi/10.1021/acsomega.2c07780: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Mamba Explained: The State Space Model Taking On Transformers (150 pts)]]></title>
            <link>https://www.kolaayonrinde.com/blog/2024/02/11/mamba.html</link>
            <guid>39501982</guid>
            <pubDate>Sun, 25 Feb 2024 16:16:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.kolaayonrinde.com/blog/2024/02/11/mamba.html">https://www.kolaayonrinde.com/blog/2024/02/11/mamba.html</a>, See on <a href="https://news.ycombinator.com/item?id=39501982">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <!-- <nav class="sidebar"> -->
    <!-- Table of Contents -->
    <!-- <h4>Table of Contents</h4> -->
    <!--  -->
    <!-- </nav> -->
    <!-- <br /> -->
    <!-- <br /> -->
    <h3 id="the-state-space-model-taking-on-transformers">The State Space Model taking on Transformers</h3>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/snake.png" width="400" alt="Mamba vs Transformer">
    <figcaption></figcaption>
  </figure>
</div>


<p>Right now, AI is eating the world.</p>

<p>And by AI, I mean Transformers. Practically all the big breakthroughs in AI over
the last few years are due to Transformers.</p>

<p><strong>Mamba</strong>, however, is one of an alternative class of models called <strong>State
Space Models</strong> (<strong>SSMs</strong>). Importantly, for the first time, Mamba promises
similar performance (and crucially similar
<a href="https://arxiv.org/pdf/2203.15556.pdf"><em>scaling laws</em></a>) as the Transformer
whilst being feasible at long sequence lengths (say 1 million tokens). We
achieve this long context by removing the ‚Äúquadratic bottleneck‚Äù in the
Attention Mechanism. Mamba also runs <em>fast</em> - like ‚Äúup to 5x faster than
Transformer fast‚Äù<sup id="fnref:figure" role="doc-noteref"><a href="#fn:figure" rel="footnote">1</a></sup>.</p>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/mamba_scaling.png" width="800" alt="Scaling Laws for Mamba vs other Language Models">
    <figcaption>Mamba performs similarly (or slightly better than) other Language Models on The Pile</figcaption>
  </figure>
</div>


<p>Gu and Dao, the Mamba authors write:</p>

<blockquote>
  <p>Mamba enjoys fast inference and linear scaling in sequence length, and its
performance improves on real data up to million-length sequences. As a general
sequence model backbone, Mamba achieves state-of-the-art performance across
several modalities such as language, audio, and genomics. On language
modeling, our Mamba-3B model outperforms Transformers of the same size and
matches Transformers twice its size, both in pretraining and downstream
evaluation.</p>
</blockquote>



<p>Here we‚Äôll discuss:</p>

<ul>
  <li>The advantages (and disadvantages) of Mamba (üêç) vs Transformers (ü§ñ),</li>
  <li>Analogies and intuitions for thinking about Mamba, and</li>
  <li>What Mamba means for Interpretability, AI Safety and Applications.</li>
</ul>

<h2 id="problems-with-transformers---maybe-attention-isnt-all-you-need">Problems with Transformers - Maybe Attention <em>Isn‚Äôt</em> All You Need</h2>

<p>We‚Äôre very much in the Transformer-era of history. ML used to be about detecting
cats and dogs. Now, with Transformers, we‚Äôre
<a href="https://openai.com/research/gpt-4">generating human-like poetry</a>,
<a href="https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf">coding better than the median competitive programmer</a>,
and
<a href="https://www.nature.com/articles/s41586-021-03819-2">solving the protein folding problem</a>.</p>

<p>But Transformers have one core problem. In a transformer, every token can look
back at every previous token when making predictions. For this lookback, we
cache detailed information about each token in the so-called KV cache.</p>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/attention.png" width="800" alt="attention">
    <figcaption>When using the Attention Mechanism, information from all previous tokens can be passed to the current token</figcaption>
  </figure>
</div>


<p>This pairwise communication means a forward pass is O(n¬≤) time complexity in
training (the dreaded <code>quadratic bottleneck</code>) and each new token generated
autoregressively takes O(n) time. That is to say, as the context gets larger,
the model gets <em>slower</em>.</p>

<p>To add insult to injury, storing this KV cache requires O(n) space. The fateful
<code>CUDA OOM</code> error looms large as the memory footprint balloons. If space were the
only issue, we might just add more GPUs but with latency growing
quadratically‚Ä¶ perhaps not.</p>

<p>On the margin, we can mitigate the quadratic bottleneck with techniques like
<a href="https://paperswithcode.com/method/sliding-window-attention">Sliding Window Attention</a>
or clever CUDA optimisations like
<a href="https://arxiv.org/pdf/2205.14135.pdf">FlashAttention</a>. But ultimately, for
super long context windows (like a chatbot which remembers every conversation
you‚Äôve shared), we need a different approach.</p>

<h3 id="foundation-model-backbones">Foundation Model Backbones</h3>

<p>Fundamentally, all good ML architecture backbones have components for two
important operations:</p>

<ol>
  <li><strong>Communication</strong> <em>between</em> tokens</li>
  <li><strong>Computation</strong> <em>within</em> a token</li>
</ol>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/transformer_block.png" width="800" alt="Transformer Block">
    <figcaption>The Transformer Block</figcaption>
  </figure>
</div>


<p>In transformers, this is <strong>Attention</strong> (<code>communication</code>) and <strong>MLPs</strong>
(<code>computation</code>). We improve transformers by optimising these two
operations<sup id="fnref:scale" role="doc-noteref"><a href="#fn:scale" rel="footnote">2</a></sup>.</p>

<p>We would like to replace the Attention component <sup id="fnref:attention" role="doc-noteref"><a href="#fn:attention" rel="footnote">3</a></sup> with some other
method for communicating between tokens. <strong>Mamba</strong> uses the Control Theory
inspired <strong>SSM</strong> for <code>Communication</code> and keeps MLP-style projections for
<code>Computation</code>.</p>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/mamba_block.png" width="800" alt="Mamba Block">
    <figcaption>The Mamba Block</figcaption>
  </figure>
</div>


<p>Like a Transformer made up of stacked transformer blocks, Mamba is made up of
stacked Mamba blocks as above.</p>

<p>We would like to understand and motivate the choice of the SSM for sequence
transformations.</p>

<h2 id="motivating-mamba---a-throwback-to-temple-run">Motivating Mamba - A Throwback to Temple Run</h2>

<p>Imagine we‚Äôre building a Temple Run agent <sup id="fnref:temple-run" role="doc-noteref"><a href="#fn:temple-run" rel="footnote">4</a></sup>. It chooses if the
runner should move left or right at any time.</p>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/temple_run.png" width="400" alt="Temple Run">
    <figcaption></figcaption>
  </figure>
</div>


<p>To successfully pick the correct direction, we need information about our
surroundings. Let‚Äôs call the collection of relevant information the <code>state</code>.
Here the state likely includes your current position and velocity, the position
of the nearest obstacle, weather conditions, etc.</p>

<blockquote>
  <p>Claim 1: if you know the current state of the world and how the world is
evolving, then you can use this to determine the direction to move.</p>
</blockquote>

<p>Note that you don‚Äôt need to look at the whole screen all the time. You can
figure out what will happen to most of the screen by noting that as you run, the
obstacles move down the screen. You only need to look at the top of the screen
to understand the new information and then simulate the rest.</p>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/temple_run_annotated.png" width="800" alt="Temple Run">
    <figcaption></figcaption>
  </figure>
</div>


<p>This lends itself to a natural formulation. Let h be the hidden state, relevant
knowledge about the world. Also let x be the input, the observation that you get
each time. h‚Äô then represents the derivative of the hidden state, i.e. how the
state is evolving. We‚Äôre trying to predict y, the optimal next move (right or
left).</p>

<p>Now, Claim 1 states that
<code>from the hidden state h, h‚Äô, and the new observation x, you can figure out y</code>.</p>

<p>More concretely, h, the state, can be represented as a differential equation (Eq
1a):</p><p>

\[h‚Äô(t) = \mathbf{A}h(t) + \mathbf{B}x(t)\]

</p><p>Knowing h allows you to determine your next move y (Eq 1b):</p><p>

\[y(t) = \mathbf{C}h(t) + \mathbf{D}x(t)\]

</p><p>The system evolves as a function of the current state and new observations. A
small new observation is enough because we can determine most of the state by
applying known state dynamics to the previous state. That is, most of the screen
isn‚Äôt new, it‚Äôs just the natural downward movement of the previous state. Fully
knowing the state would allow us to pick the best next move, y.</p>

<p>You can learn a lot about the system dynamics by observing the top of the
screen - if it‚Äôs moving faster, we can infer the whole screen is and the game is
speeding up<sup id="fnref:smooth" role="doc-noteref"><a href="#fn:smooth" rel="footnote">5</a></sup>. In this way, even if we start off knowing nothing about
the game except our limited observation, pretty soon we could understand the
whole screen.</p>

<h3 id="whats-the-state">What‚Äôs the State?</h3>

<p>Here, <strong>state</strong> refers to the variables that, when combined with the input
variables, fully determine the future system behaviour. In theory, once we have
the state, there‚Äôs nothing else we need to know about the past to predict the
future. With this choice of state, the system is converted to a <strong>Markov
Decision Process</strong>. Ideally, the state is a fairly small amount of information
which captures the essential properties of the system. That is, <strong>the state is a
compression of the past</strong> <sup id="fnref:diagonal" role="doc-noteref"><a href="#fn:diagonal" rel="footnote">6</a></sup></p>

<h2 id="discretisation---how-to-deal-with-living-in-a-quantised-world">Discretisation - How To Deal With Living in a Quantised World</h2>

<p>Okay, great! So, given some state and input observation, we have an
autoregressive-style system to determine the next action. Amazing!</p>

<p>In practice though, there‚Äôs a little snag here. We‚Äôre modelling time as
continuous. But in real life, we get new inputs and take new actions at discrete
time steps <sup id="fnref:discrete" role="doc-noteref"><a href="#fn:discrete" rel="footnote">7</a></sup>.</p>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/quantised.png" width="600" alt="Reality is Quantised">
    <figcaption></figcaption>
  </figure>
</div>


<p>We would like to convert this <em>continuous-time differential equation</em> into a
<em>discrete-time difference equation</em>. This conversion process is known as
<code>discretisation</code>. Discretisation is a well-studied problem in the literature.
Mamba uses the <a href="https://en.wikipedia.org/wiki/Zero-order_hold">Zero-Order Hold</a>
(ZOH) discretisation<sup id="fnref:zoh" role="doc-noteref"><a href="#fn:zoh" rel="footnote">8</a></sup>. To give an idea of what‚Äôs happening morally,
consider a naive first-order approximation<sup id="fnref:Euler" role="doc-noteref"><a href="#fn:Euler" rel="footnote">9</a></sup>.</p>

<p>From Equation 1a, we have</p><p>

\[h‚Äô(t) = \mathbf{A}h(t) + \mathbf{B}x(t)\]

</p><p>And for small ‚àÜ,</p><p>

\[h‚Äô(t) \approx \frac{h(t+\Delta) - h(t)}{\Delta}\]

</p><p>by the definition of the derivative.</p>

<p>We let:</p><p>

\[h_t = h(t)\]

</p><p>and</p><p>

\[h_{t+1} = h(t + \Delta)\]

</p><p>and substitute into Equation 1a giving:</p><p>

\[h_{t+1} - h_t \approx \Delta (\mathbf{A}h_t + \mathbf{B}x_t)\]

\[\Rightarrow h_{t+1} \approx (I + \Delta \mathbf{A})h_t + (\Delta
\mathbf{B})x_t\]

</p><p>Hence, after renaming the coefficients and relabelling indices, we have the
discrete representations:</p>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/equation_2.png" width="800" alt="Equation 2">
    <figcaption>The Discretised Version of the SSM Equation</figcaption>
  </figure>
</div>


<p>If you‚Äôve ever looked at an RNN before <sup id="fnref:rnn" role="doc-noteref"><a href="#fn:rnn" rel="footnote">10</a></sup> and this feels familiar - <em>trust
your instincts</em>:</p>

<blockquote>
  <p>We have some input x, which is combined with the previous hidden state by some
transform to give the new hidden state. Then we use the hidden state to
calculate the output at each time step.</p>
</blockquote>

<h2 id="understanding-the-ssm-matrices">Understanding the SSM Matrices</h2>

<p>Now, we can interpret the A, B, C, D matrices more intuitively:</p>

<ul>
  <li>A is the transition state matrix. It shows how you transition the current
state into the next state. It asks ‚ÄúHow should I forget the less relevant
parts of the state over time?‚Äù</li>
  <li>B is mapping the new input into the state, asking ‚ÄúWhat part of my new input
should I remember?‚Äù. <sup id="fnref:B" role="doc-noteref"><a href="#fn:B" rel="footnote">11</a></sup></li>
  <li>C is mapping the state to the output of the SSM. It asks, ‚ÄúHow can I use the
state to make a good next prediction?‚Äù. <sup id="fnref:C" role="doc-noteref"><a href="#fn:C" rel="footnote">12</a></sup></li>
  <li>D is how the new input passes through to the output. It‚Äôs a kind of modified
skip connection that asks ‚ÄúHow can I use the new input in my prediction?‚Äù</li>
</ul>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/graphical_matmuls.png" width="600" alt="Visual SSM Equations">
    <figcaption>Visual Representation of The SSM Equations</figcaption>
  </figure>
</div>


<p>Additionally, ‚àÜ has a nice interpretation - it‚Äôs the step size, or what we might
call the <code>linger time</code> or the <code>dwell time</code>. For large ‚àÜ, you focus more on that
token; for small ‚àÜ, you skip past the token immediately and don‚Äôt include it
much in the next state.</p>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/mamba_hardware_diagram.png" width="800" alt="Hardware Diagram">
    <figcaption></figcaption>
  </figure>
</div>


<p>And that‚Äôs it! That‚Äôs the SSM, our ~drop-in replacement for Attention
(<code>Communication</code>) in the Mamba block. The <code>Computation</code> in the Mamba
architecture comes from regular linear projections, non-linearities, and local
convolutions - the regular ML building blocks we know and love!</p>

<p>Okay great, that‚Äôs the theory - but does this work? Well‚Ä¶</p>

<h2 id="effectiveness-vs-efficiency-attention-is-focus-selectivity-is-prioritisation">Effectiveness vs Efficiency: Attention is Focus, Selectivity is Prioritisation</h2>

<p>At WWDC ‚Äò97, Steve Jobs famously noted that
‚Äú<a href="https://www.youtube.com/watch?v=H8eP99neOVs&amp;t=98s">focusing is about saying no</a>‚Äù.
Focus is ruthless prioritisation. It‚Äôs common to think about Attention
<em>positively</em> as choosing what to <em>notice</em>. In the Steve Jobs sense, we might
instead frame Attention <em>negatively</em> as choosing what to <em>discard</em>.</p>

<p>There‚Äôs a classic intuition pump in Machine Learning known as the
<a href="https://ieeexplore.ieee.org/document/8555495">Cocktail Party Problem</a>
<sup id="fnref:alcohol" role="doc-noteref"><a href="#fn:alcohol" rel="footnote">13</a></sup>. Imagine a party with dozens of simultaneous loud conversations:</p>

<p>Question:</p>

<blockquote>
  <p>How do we recognise what one person is saying when others are talking at the
same time? <sup id="fnref:frequency" role="doc-noteref"><a href="#fn:frequency" rel="footnote">14</a></sup></p>
</blockquote>

<p>Answer:</p>

<blockquote>
  <p>The brain solves this problem by focusing your ‚Äúattention‚Äù on a particular
stimulus <em>and hence</em> drowning out all other sounds as much as possible.</p>
</blockquote>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/cocktail_party.png" width="450" alt="Cocktail Party">
    <figcaption></figcaption>
  </figure>
</div>


<hr>



<p>Transformers use Dot-Product Attention to focus in on the most relevant tokens.
A big reason Attention is so great is that you have the potential to look back
at everything that ever happened in its context. This is like photographic
memory when done right. <sup id="fnref:photo" role="doc-noteref"><a href="#fn:photo" rel="footnote">15</a></sup></p>

<p>Transformers (ü§ñ) are extremely <strong>effective</strong>. But they aren‚Äôt very
<strong>efficient</strong>. They store everything from the past so that they can look back at
tokens with theoretically perfect recall.</p>

<p>Traditional RNNs (üîÅ) are the opposite - they forget a lot, only recalling a
small amount in their hidden state and discarding the rest. They are very
<strong>efficient</strong> - their state is small. Yet they are less <strong>effective</strong> as
discarded information cannot be recovered.</p>

<p>We‚Äôd like something closer to the Pareto frontier of the
effectiveness/efficiency tradeoff. Something that‚Äôs more effective than
traditional RNNs and more efficient than transformers.</p>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/pareto_frontier.png" width="800" alt="Pareto Frontier">
    <figcaption>The Mamba Architecture seems to offer a solution which pushes out the Pareto frontier of effectiveness/efficiency.</figcaption>
  </figure>
</div>


<p>SSMs are as <strong>efficient</strong> as RNNs, but we might wonder how <strong>effective</strong> they
are. After all, it seems like they would have a hard time discarding only
<em>unnecessary</em> information and keeping everything relevant. If each token is
being processed the same way, applying the same A and B matrices as if in a
factory assembly line for tokens, there is no context-dependence. We would like
the forgetting and remembering matrices (A and B respectively) to vary and
dynamically adapt to inputs.</p>

<h3 id="the-selection-mechanism">The Selection Mechanism</h3>

<p><strong>Selectivity</strong> allows each token to be transformed into the state in a way that
is unique to its own needs. Selectivity is what takes us from vanilla SSM models
(applying the same A (forgetting) and B (remembering) matrices to every input)
to Mamba, the <strong><em>Selective</em></strong> <em>State Space Model</em>.</p>

<p>In regular SSMs, A, B, C and D are learned matrices - that is</p>

<p>\(\mathbf{A} =
\mathbf{A}_{\theta}\) etc. (where Œ∏ represents the learned
parameters)</p>

<p>With the Selection Mechanism in Mamba, A, B, C and D are also functions of x.
That is \(\mathbf{A} = \mathbf{A}_{\theta(x)}\) etc; the matrices are context
dependent rather than static.</p>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/ssm_algorithm.png" width="800" alt="SSM Algorithm">
    <figcaption>Mamba (right) differs from traditional SSMs by allowing A,B,C matrices to be <b> selective </b> i.e. context dependent </figcaption>
  </figure>
</div>


<p>Making A and B functions of x allows us to get the best of both worlds:</p>

<ul>
  <li>We‚Äôre selective about what we include in the state, which improves
<strong>effectiveness</strong> vs traditional SSMs.</li>
  <li>Yet, since the state size is bounded, we improve on <strong>efficiency</strong> relative to
the Transformer. We have O(1), not O(n) space and O(n) not O(n¬≤) time
requirements.</li>
</ul>

<p>The Mamba paper authors write:</p>

<blockquote>
  <p>The efficiency vs. effectiveness tradeoff of sequence models is characterized
by how well they compress their state: efficient models must have a small
state, while effective models must have a state that contains all necessary
information from the context. In turn, we propose that a fundamental principle
for building sequence models is selectivity: or the context-aware ability to
focus on or filter out inputs into a sequential state. In particular, a
selection mechanism controls how information propagates or interacts along the
sequence dimension.</p>
</blockquote>



<hr>



<p>Humans (mostly) don‚Äôt have photographic memory for everything they experience
within a lifetime - or even within a day! There‚Äôs just way too much information
to retain it all. Subconsciously, we select what to remember by choosing to
forget, throwing away most information as we encounter it. Transformers (ü§ñ)
decide what to focus on at <strong>recall time</strong>. Humans (üßë) also decide what to
throw away at <strong>memory-making time</strong>. Humans filter out information early and
often.</p>

<p>If we had infinite capacity for memorisation, it‚Äôs clear the transformer
approach is better than the human approach - it truly is more effective. But
it‚Äôs less efficient - transformers have to store so much information about the
past that might not be relevant. Transformers (ü§ñ) only decide what‚Äôs relevant
at <strong>recall time</strong>. The innovation of Mamba (üêç) is allowing the model better
ways of forgetting earlier - it‚Äôs focusing by choosing what to <em>discard</em> using
<strong>Selectivity</strong>, throwing away less relevant information at <strong>memory-making
time</strong><sup id="fnref:seq_len" role="doc-noteref"><a href="#fn:seq_len" rel="footnote">16</a></sup>.</p>

<h3 id="the-problems-of-selectivity">The Problems of Selectivity</h3>

<p>Applying the Selection Mechanism does have its gotchas though. Non-selective
SSMs (i.e. A,B not dependent on x) are fast to compute in training. This is
because the component of \(y_t\) which depends on \(x_i\) can be expressed
as a linear map, i.e. a single matrix that can be precomputed!</p>

<p>For example (ignoring the D component, the skip connection):</p><p>

\[y_2 = \mathbf{C}\mathbf{B}x_2 + \mathbf{C}\mathbf{A}\mathbf{B}x_1 +
\mathbf{C}\mathbf{A}\mathbf{A}\mathbf{B}x_0\]

</p><p>If we‚Äôre paying attention, we might spot something even better here - this
expression can be written as a convolution. Hence we can apply the Fast Fourier
Transform and the Convolution Theorem to compute this <em>very</em> efficiently on
hardware as in Equation 3 below.</p>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/equations_2_3.png" width="800" alt="Equations 2 and 3">
    <figcaption>We can calculate Equation 2, the SSM equations, efficiently in the Convolutional Form, Equation 3. </figcaption>
  </figure>
</div>


<p>Unfortunately, with the Selection Mechanism, we lose the convolutional form.
Much attention is given to making Mamba efficient on modern GPU hardware using
similar hardware optimisation tricks to Tri Dao‚Äôs Flash Attention <sup id="fnref:cuda" role="doc-noteref"><a href="#fn:cuda" rel="footnote">17</a></sup>. With
the hardware optimisations, Mamba is able to run faster than comparably sized
Transformers.</p>

<h3 id="machine-learning-for-political-economists---how-large-should-the-state-be">Machine Learning for Political Economists - How Large Should The State Be?</h3>

<p>The Mamba authors write, ‚Äúthe efficiency vs. effectiveness tradeoff of sequence
models is characterised by how well they compress their state‚Äù. In other words,
like in political economy<sup id="fnref:oop" role="doc-noteref"><a href="#fn:oop" rel="footnote">18</a></sup>, the fundamental problem is how to manage the
state.</p>

<p>üîÅ <strong>Traditional RNNs are anarchic</strong></p>

<blockquote>
  <p>They have a small, minimal state. The size of the state is bounded. The
compression of state is poor.</p>
</blockquote>

<p>ü§ñ <strong>Transformers are communist</strong></p>

<blockquote>
  <p>They have a maximally large state. The ‚Äústate‚Äù is just a cache of the entire
history with no compression. Every context token is treated equally until
recall time.</p>
</blockquote>

<p>üêç<strong>Mamba has a compressed state</strong></p>

<blockquote>
  <p>‚Ä¶but it‚Äôs selective about what goes in. Mamba says we can get away with a
small state if the state is well focused and effective <sup id="fnref:politik" role="doc-noteref"><a href="#fn:politik" rel="footnote">19</a></sup>.</p>
</blockquote>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/political_spectrum.png" width="800" alt="Language Models and State Size">
    <figcaption>Language Models and State Size</figcaption>
  </figure>
</div>


<p>The upshot is <strong>state representation is critical</strong>. A smaller state is more
efficient; a larger state is more effective. The key is to <strong>selectively</strong> and
<strong>dynamically</strong> compress data into the state. Mamba‚Äôs Selection Mechanism allows
for context-dependent reasoning, focusing and ignoring. For both performance and
interpretability, understanding the state seems to be very useful.</p>

<h2 id="information-flow-in-transformer-vs-mamba">Information Flow in Transformer vs Mamba</h2>

<p>How do Transformers know anything? At initialisation, a transformer isn‚Äôt very
smart. It learns in two ways:</p>

<ol>
  <li>Training data (Pretraining, SFT, RLHF etc)</li>
  <li>In context-data</li>
</ol>

<h4 id="training-data">Training Data</h4>

<p>Models learn from their training data. This is a kind of lossy compression of
input data into the weights. We can think of the effect of pretraining data on
the transformer kinda like the effect of your ancestor‚Äôs experiences on your
genetics - you can‚Äôt recall their experiences, you just have vague instincts
about them <sup id="fnref:analogy" role="doc-noteref"><a href="#fn:analogy" rel="footnote">20</a></sup>.</p>

<h4 id="in-context-data">In Context-Data</h4>

<p>Transformers use their context as short-term memory, which they can recall with
~perfect fidelity. So we get
<a href="https://thegradient.pub/in-context-learning-in-context/">In-Context Learning</a>,
e.g. using induction heads to solve the
<a href="https://arxiv.org/pdf/2211.00593.pdf">Indirect Object Identification</a> task, or
<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c529dba08a146ea8d6cf715ae8930cbe-Paper-Conference.pdf">computing Linear Regression</a>.</p>

<h4 id="retrieval">Retrieval</h4>

<p>Note that Transformers don‚Äôt filter their context at all until recall time. So
if we have a bunch of information we think <em>might</em> be useful to the Transformer,
we filter it <em>outside</em> the Transformer (using Information Retrieval strategies)
and then stuff the results into the prompt. This process is known as Retrieval
Augmented Generation (RAG). RAG determines relevant information for the context
window of a transformer. A human with the internet is kinda like a RAG system -
you still have to know what to search but whatever you retrieve is as salient as
short-term memory to you.</p>

<h4 id="information-flow-for-mamba">Information Flow for Mamba</h4>

<p>Training Data acts similarly for Mamba. However, the lines are slightly blurred
for in-context data and retrieval. In-context data for Mamba <em>is</em>
compressed/filtered similar to retrieval data for transformers. This in-context
data is also accessible for look-up like for transformers (although with
somewhat lower fidelity).</p>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/information_flow.png" width="800" alt="The Information Flow in Mamba">
    <figcaption></figcaption>
  </figure>
</div>


<p>Transformer context is to Mamba states what short-term is to long-term memory.
Mamba doesn‚Äôt just have ‚ÄúRAM‚Äù, it has a hard drive<sup id="fnref:hard-drive" role="doc-noteref"><a href="#fn:hard-drive" rel="footnote">21</a></sup> <sup id="fnref:ssd" role="doc-noteref"><a href="#fn:ssd" rel="footnote">22</a></sup>.</p>

<h3 id="swapping-states-as-a-new-prompting-paradigm">Swapping States as a New Prompting Paradigm</h3>

<p>Currently, we often use RAG to give a transformer contextual information.</p>

<p>With Mamba-like models, you could instead imagine having a library of states
created by running the model over specialised data. States could be shared kinda
like
<a href="https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language">LoRAs</a>
for image models.</p>

<p>For example, I could do inference on 20 physics textbooks and, say, 100 physics
questions and answers. Then I have a state which I can give to you. Now you
don‚Äôt need to add any few-shot examples; you just simply ask your question.
<strong>The in-context learning is in the state</strong>.</p>

<p>In other words, you can drag and drop downloaded states into your model, like
literal plug-in cartridges. And note that ‚Äútraining‚Äù a state doesn‚Äôt require any
backprop. It‚Äôs more like a highly specialised one-pass fixed-size compression
algorithm. This is unlimited in-context learning applied at inference time for
zero-compute or latency. <sup id="fnref:steering" role="doc-noteref"><a href="#fn:steering" rel="footnote">23</a></sup></p>

<p>The structure of an effective LLM call goes from‚Ä¶</p>

<ol>
  <li>System Prompt</li>
  <li>Preamble</li>
  <li>Few shot-examples</li>
  <li>Question</li>
</ol>

<p>‚Ä¶for Transformers, to simply‚Ä¶</p>

<ol>
  <li>Inputted state (with problem context, initial instructions, textbooks, and
few-shot examples)</li>
  <li>Short question</li>
</ol>

<p>‚Ä¶for Mamba.</p>

<p>This is cheaper and faster than few-shot prompting (as the state is infinitely
reusable without inference cost). It‚Äôs also MUCH cheaper than finetuning and
doesn‚Äôt require any gradient updates. We could imagine retrieving states in
addition to context.</p>

<h2 id="mamba--mechanistic-interpretability">Mamba &amp; Mechanistic Interpretability</h2>

<p>Transformer interpretability typically involves:</p>

<ol>
  <li>understanding token relationships via attention,</li>
  <li>understanding circuits, and</li>
  <li>using
<a href="https://www.kolaayonrinde.com/blog/2023/11/03/dictionary-learning.html">Dictionary Learning</a>
for unfolding MLPs.</li>
</ol>

<p>Most of the ablations that we would like to do for Mamba are still valid, but
understanding token communication (1) is now more nuanced. All information moves
between tokens via hidden states instead of the Attention Mechanism which can
‚Äúteleport‚Äù information from one sequence position to another.</p>

<p>For understanding in-context learning (ICL) tasks with Mamba, we will look to
intervene on the SSM state. A classic task in-context learning task is
<a href="https://arxiv.org/pdf/2211.00593.pdf">Indirect Object Identification</a> in which
a model has to finish a paragraph like:</p>

<blockquote>
  <p><em>Then, Shelby and Emma had a lot of fun at the school. [Shelby/Emma] gave an
apple to [BLANK]</em></p>
</blockquote>

<p>The model is expected to fill in the blank with the name that is not repeated in
the paragraph. In the chart below we can see that information is passed from the
<code>[Shelby/Emma]</code> position to the final position via the hidden state (see the two
blue lines in the top chart).</p>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/patching_state.png" width="800" alt="Patching State">
    <figcaption></figcaption>
  </figure>
</div>


<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/patching_residual_stream.png" width="800" alt="Patching Residual Stream">
    <figcaption></figcaption>
  </figure>
</div>


<p>Since it‚Äôs hypothesised that much of In-Context Learning in Transformers is
downstream of more primitive sequence position operations (like
<a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">Induction Heads</a>),
Mamba being able to complete this task suggests a more general In-Context
Learning ability.</p>

<h2 id="whats-next-for-mamba--ssms">What‚Äôs Next for Mamba &amp; SSMs?</h2>

<p>Mamba-like models are likely to excel in scenarios requiring extremely long
context and long-term memory. Examples include:</p>

<ul>
  <li>Processing DNA</li>
  <li>Generating (or reasoning over) video</li>
  <li>Writing novels</li>
</ul>

<p>An illustrative example is agents with long-term goals.</p>

<blockquote>
  <p>Suppose you have an agent interacting with the world. Eventually, its
experiences become too much for the context window of a transformer. The agent
then has to compress or summarise its experiences into some more compact
representation.</p>

  <p>But how do you decide what information is the most useful as a summary? If the
task is language, LLMs are actually fairly good at summaries - okay, yeah,
you‚Äôll lose some information, but the most important stuff can be retained.</p>

  <p>However, for other disciplines, it might not be clear how to summarise. For
example, what‚Äôs the best way to summarise a 2 hour movie? <sup id="fnref:movie" role="doc-noteref"><a href="#fn:movie" rel="footnote">24</a></sup>. Could the
model itself learn to do this naturally rather than a hacky workaround like
trying to describe the aesthetics of the movie in text?</p>
</blockquote>

<p>This is what Mamba allows. Actual long-term memory. A real state where the model
learns to keep what‚Äôs important.
<a href="https://arxiv.org/pdf/2309.10668.pdf">Prediction is compression</a> - learning
what‚Äôs useful to predict what‚Äôs coming next inevitably leads to building a
useful compression of the previous tokens.</p>



<hr>



<p>The implications for Assistants are clear:</p>

<p>Your chatbot co-evolves with you. It remembers.</p>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/her.png" width="800" alt="Her">
    <figcaption>The film HER is looking better and better as time goes on üò≥</figcaption>
  </figure>
</div>


<h3 id="agents--ai-safety">Agents &amp; AI Safety</h3>

<p>One reason for positive updates in existential risk from AGI is Language Models.
Previously, Deep-RL agents trained via self-play looked set to be the first
AGIs. Language models are inherently much safer since they aren‚Äôt trained with
long-term goals. <sup id="fnref:safe" role="doc-noteref"><a href="#fn:safe" rel="footnote">25</a></sup></p>

<p>The potential for long-term sequence reasoning here brings back the importance
of agent-based AI safety. Few agent worries are relevant to Transformers with an
8k context window. Many are relevant to systems with impressive long-term
memories and possible instrumental goals.</p>

<h3 id="the-best-collab-since-taco-bell--kfc--x-">The Best Collab Since Taco Bell &amp; KFC: ü§ñ x üêç</h3>

<p>The Mamba authors show that there‚Äôs value in combining Mamba‚Äôs long context with
the Transformer‚Äôs high fidelity over short sequences. For example, if you‚Äôre
making long videos, you likely can‚Äôt fit a whole movie into a Transformer‚Äôs
context for attention <sup id="fnref:image" role="doc-noteref"><a href="#fn:image" rel="footnote">26</a></sup>. You could imagine having Attention look at the
most recent frames for short-term fluidity and an SSM for long-term narrative
consistency <sup id="fnref:optimisation" role="doc-noteref"><a href="#fn:optimisation" rel="footnote">27</a></sup>.</p>



<hr>



<p>This isn‚Äôt the end for Transformers. Their high effectiveness is exactly what‚Äôs
needed for many tasks. But now Transformers aren‚Äôt the only option. Other
architectures are genuinely feasible.</p>

<p>So we‚Äôre not in the post-<code>Transformer</code> era. But for the first time, we‚Äôre living
in the post-<code>only-Transformers</code> era <sup id="fnref:other-models" role="doc-noteref"><a href="#fn:other-models" rel="footnote">28</a></sup>. And this blows the
possibilities wide open for sequence modelling with extreme context lengths and
native long-term memory.</p>

<p>Two ML researchers, Sasha Rush (HuggingFace, Annotated Transformer, Cornell
Professor) and Jonathan Frankle (Lottery Ticket Hypothesis, MosaicML, Harvard
Professor), currently have a bet <a href="http://www.isattentionallyouneed.com/">here</a>.</p>

<div>
  <figure>
    <img src="https://www.kolaayonrinde.com/blog/images/mamba/attention_wager.png" width="800" alt="Attention Wager">
    <figcaption></figcaption>
  </figure>
</div>


<p>Currently Transformers are far and away in the lead. With 3 years left, there‚Äôs
now a research direction with a fighting chance.</p>

<p>All that remains to ask is: <code>Is Attention All We Need?</code></p>



<p><em>Thanks to Gon√ßalo for reading an early draft, Jaden for the nnsight library
used for the Interpretability analysis and Tessa for Mamba patching
visualisations.</em></p>

<p><em>Also see: <a href="https://arxiv.org/pdf/2312.00752.pdf">Mamba paper</a>, Mamba Python
code, <a href="https://srush.github.io/annotated-s4/">Annotated S4</a>,
<a href="https://www.cognitiverevolution.ai/emergency-pod-mamba-memory-and-the-ssm-moment/">Nathan Labenz podcast</a></em></p>

<p>

$$</p>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zero-rupee note (122 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Zero-rupee_note</link>
            <guid>39501389</guid>
            <pubDate>Sun, 25 Feb 2024 15:14:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Zero-rupee_note">https://en.wikipedia.org/wiki/Zero-rupee_note</a>, See on <a href="https://news.ycombinator.com/item?id=39501389">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div lang="en" dir="ltr" id="mw-content-text">
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Zero_rupee_front.jpg"><img src="https://upload.wikimedia.org/wikipedia/en/thumb/6/6e/Zero_rupee_front.jpg/220px-Zero_rupee_front.jpg" decoding="async" width="220" height="106" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/6/6e/Zero_rupee_front.jpg/330px-Zero_rupee_front.jpg 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/6/6e/Zero_rupee_front.jpg/440px-Zero_rupee_front.jpg 2x" data-file-width="455" data-file-height="219"></a><figcaption>Zero-rupee note ‚Äì obverse</figcaption></figure>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:India_50_INR,_MG_series,_2011,_obverse.jpg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/59/India_50_INR%2C_MG_series%2C_2011%2C_obverse.jpg/220px-India_50_INR%2C_MG_series%2C_2011%2C_obverse.jpg" decoding="async" width="220" height="110" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/5/59/India_50_INR%2C_MG_series%2C_2011%2C_obverse.jpg/330px-India_50_INR%2C_MG_series%2C_2011%2C_obverse.jpg 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/5/59/India_50_INR%2C_MG_series%2C_2011%2C_obverse.jpg/440px-India_50_INR%2C_MG_series%2C_2011%2C_obverse.jpg 2x" data-file-width="1573" data-file-height="788"></a><figcaption>The old 50-rupee banknote of India, serving as a template to the zero-rupee note.</figcaption></figure>
<p>A <b>zero-rupee note</b> is a <a href="https://en.wikipedia.org/wiki/Banknote" title="Banknote">banknote</a> imitation issued in <a href="https://en.wikipedia.org/wiki/India" title="India">India</a> as a means of helping to fight systemic <a href="https://en.wikipedia.org/wiki/Political_corruption" title="Political corruption">political corruption</a>. The notes are "paid" in protest by angry citizens to <a href="https://en.wikipedia.org/wiki/Functionary" title="Functionary">government functionaries</a> who solicit <a href="https://en.wikipedia.org/wiki/Bribery" title="Bribery">bribes</a> in return for services which are supposed to be free. Zero-rupee notes, which are made to resemble the old 50-<a href="https://en.wikipedia.org/wiki/Indian_rupee" title="Indian rupee">rupee</a> <a href="https://en.wikipedia.org/wiki/Banknote" title="Banknote">banknote</a> of India, are the creation of a <a href="https://en.wikipedia.org/wiki/Non-governmental_organization" title="Non-governmental organization">non-governmental organization</a> known as 5th Pillar which has, since their inception in 2007, distributed over 2.5 million notes as of August 2014. The notes remain in current use and thousands of notes are distributed every month.
</p>
<meta property="mw:PageProp/toc">
<h2><span id="History">History</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zero-rupee_note&amp;action=edit&amp;section=1" title="Edit section: History"><span>edit</span></a><span>]</span></span></h2>
<h3><span id="Corruption_in_India">Corruption in India</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zero-rupee_note&amp;action=edit&amp;section=2" title="Edit section: Corruption in India"><span>edit</span></a><span>]</span></span></h3>

<p>Bribery‚Äîthe offering or solicitation of items of value to influence the actions of a government official‚Äîis recognized as a pervasive problem in India, with the 2010 report by anti-corruption watchdog organization <a href="https://en.wikipedia.org/wiki/Transparency_International" title="Transparency International">Transparency International</a> ranking India in 87th place on its <a href="https://en.wikipedia.org/wiki/Corruption_Perceptions_Index" title="Corruption Perceptions Index">Corruption Perceptions Index</a>.<sup id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup> A 2005 study published by Transparency International India indicated that as many of 62% of Indian citizens had first-hand experience of having paid a bribe or used an illicit "contact" to get a government job done.<sup id="cite_ref-TI-01_2-0"><a href="#cite_note-TI-01-2">[2]</a></sup>
</p><p>The 2005 Transparency International India study was the largest study of the Indian bribery problem ever undertaken, with 14,405 respondents from 20 states contributing.<sup id="cite_ref-TI-05_3-0"><a href="#cite_note-TI-05-3">[3]</a></sup> The survey focused on <a href="https://en.wikipedia.org/wiki/Petty_corruption" title="Petty corruption">petty corruption</a> experienced by common citizens in daily life, rather than upon the large-scale corruption of the rich and powerful.<sup id="cite_ref-TI-05_3-1"><a href="#cite_note-TI-05-3">[3]</a></sup>
</p><p>The 2005 study exposed chronic graft problems, with substantial numbers of survey respondents reporting direct experience in being forced to pay bribes to the police (80%), land administration (48%), and judiciary (47%).<sup id="cite_ref-TI-08_4-0"><a href="#cite_note-TI-08-4">[4]</a></sup> Majorities of survey respondents characterized the police, judiciary, land administration, municipal government, electricity supply system, government hospital system, ration card system, water supply system, and system of assessing individual income taxes as corrupt.<sup id="cite_ref-TI-08_4-1"><a href="#cite_note-TI-08-4">[4]</a></sup> Fully 45% of survey respondents believed that there was corruption as well in the primary school system.<sup id="cite_ref-TI-08_4-2"><a href="#cite_note-TI-08-4">[4]</a></sup>
</p>
<h3><span id="Origin_of_zero-rupee_notes">Origin of zero-rupee notes</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zero-rupee_note&amp;action=edit&amp;section=3" title="Edit section: Origin of zero-rupee notes"><span>edit</span></a><span>]</span></span></h3>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Zero_rupee_back.jpg"><img src="https://upload.wikimedia.org/wikipedia/en/thumb/4/40/Zero_rupee_back.jpg/220px-Zero_rupee_back.jpg" decoding="async" width="220" height="107" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/4/40/Zero_rupee_back.jpg/330px-Zero_rupee_back.jpg 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/4/40/Zero_rupee_back.jpg/440px-Zero_rupee_back.jpg 2x" data-file-width="452" data-file-height="220"></a><figcaption>Zero-rupee note ‚Äì Hindi language reverse</figcaption></figure>
<p>In 2007 a <a href="https://en.wikipedia.org/wiki/Non-profit_organization" title="Non-profit organization">non-profit organization</a> known as 5th Pillar created the zero-rupee note as a means for Indians to register their refusal to participate in bribery. Closely patterned after the nation's fifty-rupee notes, these documents instead included anti-corruption slogans "Eliminate corruption at all levels" and "I promise to neither accept nor give bribe."<sup id="cite_ref-AO_5-0"><a href="#cite_note-AO-5">[5]</a></sup>
</p><p>These zero-rupee notes were designed for use by Indian citizens who have been requested to pay bribes in order to obtain services that are legally free or who are hit with illicit surcharges on such routine government transactions as obtaining a <a href="https://en.wikipedia.org/wiki/Driver%27s_license" title="Driver's license">driver's license</a>.<sup id="cite_ref-NG_6-0"><a href="#cite_note-NG-6">[6]</a></sup> Such currency devices enable the citizen to register their opposition to the illegal request in a tangible form, "paying" the official with these valueless alternative notes.
</p><p>"The note is a way for any human being to say no to corruption without the fear of facing an encounter with persons in authority", 5th Pillar said in an official statement.<sup id="cite_ref-DN_7-0"><a href="#cite_note-DN-7">[7]</a></sup>
</p><p>President of 5th Pillar, Vijay Anand, expressed satisfaction with the program's efficacy: "People have already started using them and it is working. One <a href="https://en.wikipedia.org/wiki/Auto_rickshaw" title="Auto rickshaw">auto rickshaw</a> driver was pulled over by a policeman in the middle of the night who said he could go if he was "taken care of". The driver gave him the note instead. The policeman was shocked but smiled and let him go. The purpose of this is to instill confidence in people to say no to bribery."<sup id="cite_ref-AO_5-1"><a href="#cite_note-AO-5">[5]</a></sup>
</p><p>In addition to registering the individual's protest, zero-rupee notes provide corrupt officials with a sign that efforts are ongoing to fight systemic government corruption. Use of the notes is intended to shame or scare <a href="https://en.wikipedia.org/wiki/Bureaucrat" title="Bureaucrat">bureaucrats</a> into honest behavior by reminding these officials that laws against bribery exist.<sup id="cite_ref-NG_6-1"><a href="#cite_note-NG-6">[6]</a></sup>
</p><p>While the zero-rupee notes appear similar to a genuine Indian fifty-rupee note, they are not issued by the Indian government and are thus not <a href="https://en.wikipedia.org/wiki/Legal_tender" title="Legal tender">legal tender</a>. Only one side of the note is printed to resemble currency so as not to run afoul of <a href="https://en.wikipedia.org/wiki/Counterfeiting" title="Counterfeiting">counterfeiting</a> laws.<sup id="cite_ref-AO_5-2"><a href="#cite_note-AO-5">[5]</a></sup>
</p><p>According to 5th Pillar, Indian citizens pay approximately ¬£3 billion (about $4.9 billion) in bribes each year‚Äîa figure considered to be substantially understated by many government insiders.<sup id="cite_ref-DN_7-1"><a href="#cite_note-DN-7">[7]</a></sup>
</p>
<h3><span id="Circulation_and_legacy">Circulation and legacy</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zero-rupee_note&amp;action=edit&amp;section=4" title="Edit section: Circulation and legacy"><span>edit</span></a><span>]</span></span></h3>
<p>Satindar Mohan Bhagat, an <a href="https://en.wikipedia.org/wiki/Non-resident_Indian" title="Non-resident Indian">Indian expatriate</a> who is a <a href="https://en.wikipedia.org/wiki/Physics" title="Physics">physics</a> professor at the <a href="https://en.wikipedia.org/wiki/University_of_Maryland" title="University of Maryland">University of Maryland</a> and the director of Association for India's Development, Inc. US, is credited with originating the concept of the zero-rupee note in 2001.<sup id="cite_ref-Economist_8-0"><a href="#cite_note-Economist-8">[8]</a></sup> Upon returning to India for a visit, Bhagat was frustrated by the petty <a href="https://en.wikipedia.org/wiki/Extortion" title="Extortion">extortion</a> demands of government officials that were part of daily life and conceived of the idea of a zero-rupee note as a polite way of declining participation.<sup id="cite_ref-Economist_8-1"><a href="#cite_note-Economist-8">[8]</a></sup> The charity 5th Pillar put Bhagat's idea into practice.<sup id="cite_ref-Economist_8-2"><a href="#cite_note-Economist-8">[8]</a></sup>
</p><p>5th Pillar began the campaign in the spring of 2007 with a first printing of 25,000 notes that were distributed in the Indian city of <a href="https://en.wikipedia.org/wiki/Chennai" title="Chennai">Chennai</a>.<sup id="cite_ref-AO_5-3"><a href="#cite_note-AO-5">[5]</a></sup> Buoyed by the success of the campaign, additional printings followed and use of the zero-rupee note spread across the nation. From their inception through August 2014, 5th Pillar distributed over 2.5 million zero-rupee notes.<sup id="cite_ref-NG_6-2"><a href="#cite_note-NG-6">[6]</a></sup>
</p><p>Zero-rupee notes have been issued in five of the 22 <a href="https://en.wikipedia.org/wiki/Languages_with_official_status_in_India" title="Languages with official status in India">scheduled languages of India</a>: <a href="https://en.wikipedia.org/wiki/Tamil_language" title="Tamil language">Tamil</a>, <a href="https://en.wikipedia.org/wiki/Hindi" title="Hindi">Hindi</a>, <a href="https://en.wikipedia.org/wiki/Kannada" title="Kannada">Kannada</a>, <a href="https://en.wikipedia.org/wiki/Malayalam" title="Malayalam">Malayalam</a>, and <a href="https://en.wikipedia.org/wiki/Telugu_language" title="Telugu language">Telugu</a>.<sup id="cite_ref-5thP_9-0"><a href="#cite_note-5thP-9">[9]</a></sup>
</p><p>This concept for use in the fight against corruption has recently been adopted from 5th Pillar to few other nations suffering from endemic government bribery problems including <a href="https://en.wikipedia.org/wiki/Yemen" title="Yemen">Yemen</a>, <a href="https://en.wikipedia.org/wiki/Ghana" title="Ghana">Ghana</a>, <a href="https://en.wikipedia.org/wiki/Benin" title="Benin">Benin</a>, <a href="https://en.wikipedia.org/wiki/Mexico" title="Mexico">Mexico</a> and <a href="https://en.wikipedia.org/wiki/Nepal" title="Nepal">Nepal</a>.<sup id="cite_ref-NG_6-3"><a href="#cite_note-NG-6">[6]</a></sup>
</p><p>As of 2015, 5th Pillar expanded the zero-rupee note concept beyond India to other country currencies, on the website ZeroCurrency.org.
</p>
<h2><span id="See_also">See also</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zero-rupee_note&amp;action=edit&amp;section=5" title="Edit section: See also"><span>edit</span></a><span>]</span></span></h2>

<ul><li><a href="https://en.wikipedia.org/wiki/Jan_Lokpal_Bill" title="Jan Lokpal Bill">Jan Lokpal Bill</a></li></ul>
<h2><span id="Footnotes">Footnotes</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zero-rupee_note&amp;action=edit&amp;section=6" title="Edit section: Footnotes"><span>edit</span></a><span>]</span></span></h2>
<div><ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span>[http:/
 2010 Results], Transparency International, www.transparency.org/ Entry of India in the Search function of the Results by Country list generates the rank of 87th, with a score of 3.3.</span>
</li>
<li id="cite_note-TI-01-2"><span><b><a href="#cite_ref-TI-01_2-0">^</a></b></span> <span>Centre for Media Studies, <a rel="nofollow" href="http://www.iri.org.in/related_readings/India%20Corruption%20Study%202005.pdf"><i>India Corruption Study 2005: To Improve Governance: Volume I ‚Äì Key Highlights</i></a>, <a rel="nofollow" href="https://web.archive.org/web/20130811123343/http://www.iri.org.in/related_readings/India%20Corruption%20Study%202005.pdf">Archived</a> August 11, 2013, at the <a href="https://en.wikipedia.org/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a> New Delhi: Transparency International India, June 30, 2005.</span>
</li>
<li id="cite_note-TI-05-3"><span>^ <a href="#cite_ref-TI-05_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-TI-05_3-1"><sup><i><b>b</b></i></sup></a></span> <span>Centre for Media Studies, <i>India Corruption Study 2005</i>, p. 5.</span>
</li>
<li id="cite_note-TI-08-4"><span>^ <a href="#cite_ref-TI-08_4-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-TI-08_4-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-TI-08_4-2"><sup><i><b>c</b></i></sup></a></span> <span>Centre for Media Studies, <i>India Corruption Study 2005</i>, p. 8.</span>
</li>
<li id="cite_note-AO-5"><span>^ <a href="#cite_ref-AO_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-AO_5-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-AO_5-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-AO_5-3"><sup><i><b>d</b></i></sup></a></span> <span>Ashling O‚ÄôConnor, <a rel="nofollow" href="http://www.timesonline.co.uk/tol/news/world/asia/article1629446.ece">"Can this note stamp out corruption in a land where it‚Äôs the norm?"</a> <i>The Times</i>, April 9, 2007.</span>
</li>
<li id="cite_note-NG-6"><span>^ <a href="#cite_ref-NG_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-NG_6-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-NG_6-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-NG_6-3"><sup><i><b>d</b></i></sup></a></span> <span>National Geographic staff, <a rel="nofollow" href="http://www.thebangladeshchronicle.com/index.php?option=com_content&amp;view=article&amp;id=168:in-india-a-bribe-busting-bill&amp;catid=73:development-news&amp;Itemid=119">"In India, a Bribe-busting Bill"</a>, <i>The Bangladesh Chronicle</i>, April 8, 2011.</span>
</li>
<li id="cite_note-DN-7"><span>^ <a href="#cite_ref-DN_7-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-DN_7-1"><sup><i><b>b</b></i></sup></a></span> <span>Dean Nelson, <a rel="nofollow" href="https://www.telegraph.co.uk/news/worldnews/asia/india/7137567/India-issues-zero-rupee-banknotes.html">"India 'issues' zero rupee banknotes"</a>, <i>The Telegraph</i>, February 2, 2010.</span>
</li>
<li id="cite_note-Economist-8"><span>^ <a href="#cite_ref-Economist_8-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Economist_8-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Economist_8-2"><sup><i><b>c</b></i></sup></a></span> <span><a rel="nofollow" href="http://www.economist.com/node/15393714">"A Zero Contribution: An Unconventional Way to Combat Petty Corruption"</a>, <i>The Economist</i> [London], January 28, 2010.</span>
</li>
<li id="cite_note-5thP-9"><span><b><a href="#cite_ref-5thP_9-0">^</a></b></span> <span><a rel="nofollow" href="http://india.5thpillar.org/ZRN">"Zero Rupee Notes"</a>, <a rel="nofollow" href="https://web.archive.org/web/20110629023257/http://india.5thpillar.org/ZRN">Archived</a> June 29, 2011, at the <a href="https://en.wikipedia.org/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a> 5th Pillar, india.5thpillar.org, retrieved May 12, 2011.</span>
</li>
</ol></div>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Zero-rupee_note&amp;action=edit&amp;section=7" title="Edit section: External links"><span>edit</span></a><span>]</span></span></h2>
<ul><li><a rel="nofollow" href="https://web.archive.org/web/20080701072943/http://india.5thpillar.org/">5th Pillar Zero Rupee Note official web page</a> ‚Äì Archived 1 July 2008</li>
<li><a rel="nofollow" href="https://web.archive.org/web/20121015135814/http://india.5thpillar.org/">5th Pillar Zero Rupee Note official web page</a> ‚Äì Archived 15 October 2012</li>
<li><a rel="nofollow" href="https://web.archive.org/web/20121120214841/http://india.5thpillar.org/ZRN">Zero Rupee Note</a> ‚Äì Archive 20 November 2012</li></ul>
<!-- 
NewPP limit report
Parsed by mw1441
Cached time: 20240224152248
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‚Äêrevision‚Äêsha1, show‚Äêtoc]
CPU time usage: 0.105 seconds
Real time usage: 0.171 seconds
Preprocessor visited node count: 477/1000000
Post‚Äêexpand include size: 3675/2097152 bytes
Template argument size: 976/2097152 bytes
Highest expansion depth: 8/100
Expensive parser function count: 3/500
Unstrip recursion depth: 0/20
Unstrip post‚Äêexpand size: 8519/5000000 bytes
Lua time usage: 0.041/10.000 seconds
Lua memory usage: 1663027/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  134.426      1 -total
 46.52%   62.538      1 Template:Short_description
 28.94%   38.898      2 Template:Pagetype
 17.06%   22.939      1 Template:Reflist
 15.89%   21.354      1 Template:Main_article
 14.05%   18.891      1 Template:Portal
  8.86%   11.909      3 Template:Main_other
  7.25%    9.749      1 Template:SDcat
  6.59%    8.858      2 Template:Webarchive
  1.35%    1.821      1 Template:Short_description/lowercasecheck
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:27276309-0!canonical and timestamp 20240224152248 and revision id 1210008949. Rendering was triggered because: page-view
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Osquery: An sqlite3 virtual table exposing operating system data to SQL (167 pts)]]></title>
            <link>https://osquery.io/</link>
            <guid>39501281</guid>
            <pubDate>Sun, 25 Feb 2024 14:58:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://osquery.io/">https://osquery.io/</a>, See on <a href="https://news.ycombinator.com/item?id=39501281">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Reverse-Engineering a Switch Lite with 1,917 wires (334 pts)]]></title>
            <link>https://usoldering.com/switch-lite/</link>
            <guid>39501073</guid>
            <pubDate>Sun, 25 Feb 2024 14:27:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://usoldering.com/switch-lite/">https://usoldering.com/switch-lite/</a>, See on <a href="https://news.ycombinator.com/item?id=39501073">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <h2>Instructions to View</h2>
        <ol>
            <li><a href="https://github.com/OpenBoardView/OpenBoardView/releases">Download</a> the latest release of OpenBoardView.</li>
            <li><a href="https://usoldering.com/switch-lite/files/switch-lite-logic-board-boardview_archive.torrent">Torrent</a> or <a href="https://archive.org/details/switch-lite-logic-board-boardview">Download</a> the release data, unzip it, and read all included .txt files.</li>
            <li>Open OBV, under View, disable Board Fill and Part Fill, then under File, Open 'Switch Lite Logic Board.bvr'.</li>
            <li>Left-click to view parts/pad nets or click and drag to move, mouse-wheel to zoom, and middle-click or spacebar to flip sides.</li>
        </ol>
        <img src="https://usoldering.com/switch-lite/images/demo.jpg" alt="Small demo of output files">
        <h2>What is this?</h2>
	    <p>This is the output data for a process I solo-developed for extracting the netlist from an assembled Printed Circuit Board, in this case, a Nintendo Switch Lite logic board. Electrical components are soldered to exposed mounting pads on the PCB, and layers of copper form connections between these pads creating electrical circuits. The full list of these interconnects is called the netlist, and when combined with the part and pad geometries, constitute a boardview. Couple that boardview with reference images of both sides of the PCB, and you have the output data.</p>
        <h2>How?</h2>
        <p>There are 3 novel-ish innovations working together to make this happen. They are:
            </p><ol>
                <li>A process to create geometrical and color-accurate panoramic images of assembled PCBs at 6,000 PPI.</li>
                <li>A point-and-click GUI to draw geometrical part/pad data over the panoramas, with support for adding/modifying other arbitrary data.</li>
                <li>My own PCB capable of powering an arbitrary number of pins one-by-one, reading the state of all pins between each step.</li>
            </ol>
            Once I had that figured out, the process is to simply:
            <ol>
                <li>Take all the images and stitch the bottom-side panorama, flip the board, desolder the RF Shields, then take all the images and stitch the top-side panorama. Use these 2 panoramas and other images against each other to further refine the geometrical and color accuracy.</li>
                <li>Import the completed panoramas into the GUI, and use it to lay down the initial part/pad geometries.</li>
                <li>Desolder every component individually and place them into unique binned locations for future analysis. Record the binned location and presumed reference designator in the GUI, correcting any discrepencies in the pad geometries made in Step 2.</li>
                <li>With all pads exposed and not-shorted, hook one lead of a DMM in continuity mode to the ground plane, and use the other lead to probe every single pad on the PCB. Record all hits into the GUI, merging them into a single net.</li>
                <li>Use the GUI to group remaining pads into net-fragments, based on visual connections in the two outer board layers. If there is no visible connection, assume it is it's own fragment.</li>
                <li>Use the GUI to record the order you solder wires from the extractor PCB pins to which net-fragments on the target PCB.</li>
                <li>Run the extractor. Power goes from the extractor PCB pin, down the wire, into the net-fragment, through all hidden connections in the PCB, into other net-fragments, up those wires, back into the extractor PCB, and are logged to create a complete mapping of all hidden connections.</li>
                <li>Use the GUI to merge all of the fragments into a completed netlist, based on the extractor mapping. Export all this data into a boardview file, pack it up with the panoramas, and then...</li>
                <li>Share and Enjoy.</li>
            </ol>
            The final stats are 2,444 photographs stitched into the 2 panoramas, 760 parts desoldered into binned locations, and 1,917 wires used for a total of approximately 30,176 lead-free, bismuth-free, and halide-free solder joints used to extract the netlist. While this is only about 3 weeks of soldering work, process/software/hardware development took over a year and over $10,000.
        
        <img src="https://usoldering.com/switch-lite/images/meme.jpg" alt="Homemade flying probe meme">
        <h2>Limitations</h2>
        <p>You may have noticed the panoramas are actually at 2,000 PPI. That's because the 6,000 PPI versions are half a gigapixel, each. This causes problems. With everything. You can <a href="https://usoldering.com/switch-lite/files/switch-lite-logic-board-6k-ppi_archive.torrent">Torrent</a> or <a href="https://archive.org/details/switch-lite-logic-board-6k-ppi">Download</a> them here.</p>
        <p>You may also notice the part/pad outlines are rather simple. That's because the complex rendering feature is currently <a href="https://github.com/OpenBoardView/OpenBoardView/blob/8528b80cdc4928c220b552a772dff84ba3679cd5/src/openboardview/FileFormats/BVR3File.cpp#L110">not supported</a> in OBV. The data is all there, and all files are cross-compatible with the premium, closed-source fork of OBV, <a href="https://pldaniels.com/flexbv5/">FlexBV5</a>.</p>
        <p>Step One, post-RF Shield de-solder and Step Seven, pre-extraction, there should be ultrasonic cleaning cycles. It takes many tricks to get the RF Shields off without resorting to using low-melt (bismuth) solder. This is why the topside panorama is filthy compared to the bottomside one. There is also some risk pre-extraction that the flux residues can be conductive enough to cause false-positive connections between net-fragments. I don't have an ultrasonic.</p>
        <p>There's also a missing step between Step Eight and Nine. That is to take all the parts out of their bins and measure their electrical properties. I have a basic, low-accuracy LCR Meter, but testing them now would result in relatively low-quality and incomplete data.</p>
        <h2>Why?</h2>
        <p>My background is over a decade in Electronics Contract Manufacturing serving the medical, aerospace, military, and industrial sectors. More than half of that time is as a SMT Process Technician, with full read/write access to millions of dollars of state-of-the-art equipment. Therein lies the trap: you end up overspecialized, dependant on someone else's unaffordable equipment. Sprinkle in some monthly payments, insurance, ever-moving goalposts, and the mass psychosis of modern corporate dogma; it becomes an elaborate prison.</p>
        <p>Using 100% of my willpower to make no further mention of Sisyphus and The Boulder; this project is an experiment in fusing Work-From-Home internet freelancing with master electrical soldering, while still contributing meaningfully to society. I could open a repair shop but that's mostly disassembling and reassembling devices, and troubleshooting electrical circuits. Relatively, very little soldering. Pander to algorithms for ad money? Shill affiliate links? Resell junk tools? Also not soldering. If the goal is to just make money, I could sell 6,000 PPI panoramas of women's feet as NFT's. <b>Note: Do not contact me about this.</b></p>
        <p>So, if you can use this for your own cool stuff, or it saves you time, or makes you money, or if you just think it's neat and would like to see more: <a href="https://usoldering.com/donate"><b>please donate</b></a>. With your help, I can add more devices, reduce cycle time, improve quality, and provide more data while cost-optimizing, documenting, and open-sourcing to make the entire process reproducible.</p>
        <h2>Contact/Subscribe</h2>
        <p>Feedback, corrections, and general correspondence is welcome at &lt;second-level domain&gt;@gmail.com. RSS is <a href="https://usoldering.com/rss.xml">supported.</a> Alternatively, send an email with 'SUBSCRIBE' as the subject to join the e-mailing list.</p>
        <p>Published February 11th, 2024</p>
        

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Marginalia: 3 Years (226 pts)]]></title>
            <link>https://www.marginalia.nu/log/a_101_marginalia-3-years/</link>
            <guid>39501061</guid>
            <pubDate>Sun, 25 Feb 2024 14:25:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.marginalia.nu/log/a_101_marginalia-3-years/">https://www.marginalia.nu/log/a_101_marginalia-3-years/</a>, See on <a href="https://news.ycombinator.com/item?id=39501061">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p>It‚Äôs been three years since the inception of Marginalia Search, then
a dinky experiment to find where the heck the cool Internet has gone,
now my full time job.</p><p>While there‚Äôs always things that can be improved, it‚Äôs fair to say
the search engine has never worked as well as it does right now.</p><p>A great number of milestones have been reached, perhaps biggest
of all the search engine has moved out of my living room and into
a proper enterprise server.</p><p>An overarching theme for the year has been cleaning up the code
base and streamlining the application, not only to keep the
operational workload manageable, but to make the application and
codebase more accessible to other people, both operators and
developers. It‚Äôs been a lot of work, but it‚Äôs beginning to pay
off.</p><p>It feels distant, but the search engine used to require days-long outages when switching
indexes. This is gone. Since recently, it‚Äôs also capable of zero
downtime upgrades. So much of the operational side used to be weeks-long
manual processes, now you press a button in a GUI instead.</p><p>Something which had an outsized impact was adding support for
anchor text keywords. It‚Äôs made an incredible difference in
the search engine‚Äôs ability to find relevant results. It wasn‚Äôt
immediately apparant when the change was made, beacuse it initially
wasn‚Äôt very well integrated, but as this new relevance signal settled
in, that was a real wow moment.</p><p>It‚Äôs also been something like eight months since this became my full
time occupation, thanks to the generous people at nlnet.
This has been a bit of a trip in its own regard. The hardest part
about this has probably been not working <em>too</em> much, I‚Äôve been trying to
take at least one day off per week, and sometimes succeeding! I know I‚Äôm
leaps and bounds smarter when I‚Äôm well rested, so actually resting sometimes
should theoretically be in line with getting stuff done. It‚Äôs just so fun
though‚Ä¶</p><p>The quest to reach 1B indexed documents is slowly ongoing. It‚Äôs proving
a bit harder than anticipated, not because the software can‚Äôt handle it,
but because the signal to noise ratio of the web isn‚Äôt very good; a huge
reason why the search engine works relatively well is because of what it
doesn‚Äôt index. That said, the index has gone from 50-100M a year ago,
to 220M last crawl, and likely 290-300M when the next crawl round is finished
based on the growth of the two partitions that have already finished.</p><p>Next on the chopping block is query parsing and execution. There‚Äôs a great
deal of room for improvement in this area. I‚Äôm currently engaged in a bit
of mise en place to clean up the affected code before the real work begins.</p><p>That said, the grand leaps forward in this project has always been experimental,
so even though things are planned, I think it‚Äôs fairly certain it‚Äôs the un-planned
things that will be the ones to really make a dent. Like those anchor texts.</p><p>Finally! Thanks <a href="https://www.nlnet.nl/">NLnet</a>, thanks <a href="https://www.futo.org/">FUTO</a>,
thanks Patreons, thanks advocates, and users. This wouldn‚Äôt be possible without you.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Top of the DNS Hierarchy (117 pts)]]></title>
            <link>https://computer.rip/2024-02-11-the-top-of-the-DNS-hierarchy.html</link>
            <guid>39500500</guid>
            <pubDate>Sun, 25 Feb 2024 13:10:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://computer.rip/2024-02-11-the-top-of-the-DNS-hierarchy.html">https://computer.rip/2024-02-11-the-top-of-the-DNS-hierarchy.html</a>, See on <a href="https://news.ycombinator.com/item?id=39500500">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>


<p>In the past (in fact two years ago, proof I have been doing this for a while
now!) I <a href="https://computer.rip/2022-01-16-peer-to-peer-but-mostly-the-main-peer.html">wrote
about</a>
the "inconvenient truth" that structural aspects of the Internet make truly
decentralized systems infeasible, due to the lack of a means to perform
broadcast discovery. As a result, most distributed systems rely on a set of
central, semi-static nodes to perform initial introductions.</p>
<p>For example, Bitcoin relies on a small list of volunteer-operated domain names
that resolve to known-good full nodes. Tor similarly uses a small set of
central "directory servers" that provide initial node lists. Both systems have
these lists hardcoded into their clients; coincidentally, both have nine
trusted, central hostnames.</p>
<p>This sort of problem exists in basically all distributed systems that operate
in environments where it is not possible to shout into the void and hope for a
response. The internet, for good historic reasons, does not permit this kind of
behavior. Here we should differentiate between distributed and decentralized,
two terms I do not tend to select very carefully. Not all distributed systems
are decentralized, indeed, many are not. One of the easiest and most practical
ways to organize a distributed system is according to a hierarchy. This is a
useful technique, so there are many examples, but a prominent and old one
happens to also be part of the drivetrain mechanics of the internet: DNS, the
domain name system.</p>
<p>My reader base is expanding and so I will provide a very brief bit of
background. Many know that DNS is responsible for translating human-readable
names like "computer.rip" into the actual numerical addresses used by the
internet protocol. Perhaps a bit fewer know that DNS, as a system, is
fundamentally organized around the hierarchy of these names. To examine the
process of resolving a DNS name, it is sometimes more intuitive to reverse
the name, and instead of "computer.rip", discuss "rip.computer" [1].</p>
<p>This name is hierarchical, it indicates that the record "computer" is within
the zone "rip". "computer" is itself a zone and can contain yet more records,
we tend to call these subdomains. But the term "subdomain" can be confusing
as everything is a subdomain of something, even "rip" itself, which in a
certain sense is a subdomain of the DNS root "." (which is why, of course,
a stricter writing of the domain name computer.rip would be computer.rip.,
but as a culture we have rejected the trailing root dot).</p>
<p>Many of us probably know that each level of the DNS hierarchy has authoritative
nameservers, operated typically by whoever controls the name (or their
third-party DNS vendor). "rip" has authoritative DNS servers provided by a
company called Rightside Group, a subsidiary of the operator of websites like
eHow that went headfirst into the great DNS land grab and snapped up "rip" as a
bit of land speculation, alongside such attractive properties as "lawyer" and
"navy" and "republican" and "democrat", all of which I would like to own the
"computer" subdomain of, but alas such dictionary words are usually already
taken.</p>
<p>"computer.rip", of course, has authoritative nameservers operated by myself or
my delegate. Unlike some people I know, I do not have any nostalgia for BIND,
and so I pay a modest fee to a commercial DNS operator to do it for me. Some
would be surprised that I pay for this; DNS is actually rather inexpensive to
operate and authoritative name servers are almost universally available as a
free perk from domain registrars and others. I just like to pay for this on the
general feeling that companies that charge for a given service are probably
more committed to its quality, and it really costs very little and changing it
would take work.</p>
<p>To the observant reader, this might leave an interesting question. If even the
top-level domains are subdomains of a secret, seldom-seen root domain ".", who
operates the authoritative name servers for that zone?</p>
<p>And here we return to the matter of even distributed systems requiring central
nodes. Bitcoin uses nine harcoded domain names for initial discovery of
decentralized peers. DNS uses thirteen harcoded root servers to establish the
top level of the hierarchy.</p>
<p>These root servers are commonly referred to as a.root-servers.net through
m.root-servers.net, and indeed those are their domain names, but remember that
when we need to use those root servers we have no entrypoint into the DNS
hierarchy and so are not capable of resolving names. The root servers are much
more meaningfully identified by their IP addresses, which are "semi-harcoded"
into recursive resolves in the form of what's often called a root hints file.
You can <a href="https://www.internic.net/domain/named.root">download a copy</a>, it's a
simple file in BIND zone format that BIND basically uses to bootstrap its
cache.</p>
<p>And yes, there are other DNS implementations too, a surprising number of them,
even in wide use. But when talking about DNS history we can mostly stick to
BIND. BIND used to stand for Berkeley Internet Name Domain, and it is an apt
rule of thumb in computer history that anything with a reference to UC Berkeley
in the name is probably structurally important to the modern technology
industry.</p>
<p>One of the things I wanted to get at, when I originally talked about central
nodes in distributed systems, is the impact it has on trust and reliability.
The TOR project is aware that the nine directory servers are an appealing
target for attack or compromise, and technical measures have been taken to
mitigate the possibility of malicious behavior. The Bitcoin project seems to
mostly ignore that the DNS seeds exist, but of course the design of the Bitcoin
system limits their compromise to certain types of attacks. In the case of DNS,
much like most decentralized systems, there is a layer of long-lived caching
for top-level domains that mitigates the impact of unavailability of the root
servers, but still, in every one of these systems, there is the possibility of
compromise or unavailability if the central nodes are attacked.</p>
<p>And so there is always a layer of policy. A trusted operator can never
guarantee the trustworthiness of a central node (the node could be compromised,
or the trusted operator could turn out to be the FBI), but it sure does help.
Tor's directory servers are operated by the Tor project. Bitcoin's DNS seeds
are operated by individuals with a long history of involvement in the project.
DNS's root nodes are operated by a hodgepodge of companies and institutions
that were important to the early internet.</p>
<p>Verisign operates two, of course. A California university operates one, of
course, but amusingly not Berkeley. Three are operated by various arms of US
defense. Some internet industry associations, an NCC, another university, ICANN
runs one of them themselves. It's pretty random, though, and just reflects a
set of organizations prominently involved in the early internet.</p>
<p>Some people, even some journalists I've come across, hear that there are 13 name
servers and picture 13 4U boxes with a lot of blinking lights in heavily
fortified data centers. Admittedly this description was more or less accurate
in the early days, and a couple of the smaller root server operators did have
single machines until surprisingly recently. But today, all thirteen root
server IP addresses are anycast groups.</p>
<p>Anycast is not a concept you run into every day, because it's not really useful
on local networks where multicast can be used. But it's very important to the
modern internet. The idea is this: an IP address (really a subnetwork) is
advertised by multiple BGP nodes. Other BGP nodes can select the advertisement
they like the best, typically based on lowest hop count. As a user, you connect
to a single IP address, but based on the BGP-informed routing tables of
internet service providers your traffic could be directed to any number of
sites. You can think of it as a form of load balancing at the IP layer, but it
also has the performance benefit of users mostly connecting to nearby nodes, so
it's widely used by CDNs for multiple reasons.</p>
<p>For DNS, though, where we often have a bootstrapping problem to solve, anycast
is extremely useful as a way to handle "special" IP addresses that are used
directly. For authoritative DNS servers like 192.5.5.241 [2001:500:2f::f] [2]
(root server F) or recursive resolvers like 8.8.8.8 [2001:4860:4860::8888]
(Google public DNS), anycast is the secret that allows a "single" address to
correspond to a distributed system of nodes.</p>
<p>So there are thirteen DNS root servers in the sense that there are thirteen
independently administered clusters of root servers (with the partial exception
of A and J, both operated by Verisign, due to their acquisition of former A
operator Network Solutions). Each of the thirteen root servers is, in practice,
a fairly large number of anycast sites, sometimes over 100. The root server
operators don't share much information about their internal implementation, but
one can assume that in most cases the anycast sites consist of multiple servers
as well, fronted by some sort of redundant network appliance. There may only be
thirteen of them, but each of the thirteen is quite robust. For example, the
root servers typically place their anycast sites in major internet exchanges
distributed across both geography and provider networks. This makes it unlikely
that any small number of failures would seriously affect the number of
available sites. Even if a root server were to experience a major failure due
to some sort of administration problem, there are twelve more.</p>
<p>Why thirteen, you might ask? No good reason. The number of root servers
basically grew until the answer to an NS request for "." hit the 512 byte limit
on UDP DNS responses. Optimizations over time allowed this number to grow
(actually using single letters to identify the servers was one of these
optimizations, allowing the basic compression used in DNS responses to collapse
the matching root-servers.net part). Of course IPv6 blew DNS response sizes
completely out of the water, leading to the development of the EDNS extension
that allows for much larger responses.</p>
<p>13 is no longer the practical limit, but with how large some of the 13 are, no
one sees a pressing need to add more.  Besides, can you imagine the political
considerations in our modern internet environment? The proposed operator would
probably be Cloudflare or Google or Amazon or something and their motives would
never be trusted. Incidentally, many of the anycast sites for root server F
(operated by ISC) are Cloudflare data centers used under agreement.</p>
<p>We are, of course, currently trusting the motives of Verisign. You should never
do this! But it's been that way for a long time, we're already committed. At
least it isn't Network Solutions any more. I kind of miss when SRI was running
DNS and military remote viewing.</p>
<p>But still, there's something a little uncomfortable about the situation.
Billions of internet hosts depend on thirteen "servers" to have any functional
access to the internet.</p>
<p>What if someone attacked them? Could they take the internet down? Wouldn't this
cause a global crisis of a type seldom before seen? Should I be stockpiling DNS
records alongside my canned water and iodine pills?</p>
<p>Wikipedia contains a great piece of comedic encyclopedia writing. In its
article on the history of attacks on DNS root servers, it mentions the time, in
2012, that some-pastebin-user-claiming-to-be-Anonymous (one of the great
internet security threats of that era) threatened to "shut the Internet down".
"It may only last one hour, maybe more, maybe even a few days," the statement
continues. "No matter what, it will be global. It will be known."</p>
<p>That's the end of the section. Some Wikipedia editor, no doubt familiar with
the activities of Anonymous in 2012, apparently considered it self-evident that
the attack never happened.</p>
<p>Anonymous may not have put in the effort, but others have. There have been
several apparent DDoS attacks on the root DNS servers. One, in 2007, was
significant enough that four of the root servers suffered---but there were nine
more, and no serious impact was felt by internet users. This attack, like most
meaningful DDoS, originated with a botnet. It had its footprint primarily in
Korea, but C2 in the United States. The motivation for the attack, and who
launched it, remains unknown.</p>
<p>There is a surprisingly large industry of "booters," commercial services that,
for a fee, will DDoS a target of your choice. These tend to be operated by
criminal groups with access to large botnets; the botnets are sometimes bought
and sold and get their tasking from a network of resellers. It's a competitive
industry. In the past, booters and botnet operators have sometimes been
observed announcing a somewhat random target and taking it offline as,
essentially, a sales demonstration. Since these demonstrations are a known
behavior, any time a botnet targets something important for no discernible
reason, analysts have a tendency to attribute it to a "show of force." I have
little doubt that this is sometimes true, but as with the tendency to attribute
monumental architecture to deity worship, it might be an overgeneralization of
the motivations of botnet operators. Sometimes I wonder if they made a mistake,
or maybe they were just a little drunk and a lot bored, who is to say?</p>
<p>The problem with this kind of attribution is evident in the case of the other
significant attack on the DNS root servers, in 2015. Once again, some root
servers were impacted badly enough that they became unreliable, but other root
servers held on and there was little or even no impact to the public. This
attack, though, had some interesting properties.</p>
<p>In the 2007 incident, the abnormal traffic to the root servers consisted of
large, mostly-random DNS requests. This is basically the expected behavior of a
DNS attack; using randomly generated hostnames in requests ensures that the
responses won't be cached, making the DNS server exert more effort. Several
major botnet clients have this "random subdomain request" functionality built
in, normally used for attacks on specific authoritative DNS servers as a way to
take the operator's website offline. Chinese security firm Qihoo 360, based on
a large botnet honeypot they operate, reports that this type of DNS attack was
very popular at the time.</p>
<p>The 2015 attack was different, though! Wikipedia, like many other websites,
describes the attack as "valid queries for a single undisclosed domain name and
then a different domain the next day." In fact, the domain names were
disclosed, by at least 2016. The attack happened on two days. On the first day,
all requests were for 336901.com. The second day, all requests were for
916yy.com.</p>
<p>Contemporaneous reporting is remarkably confused on the topic of these domain
names, perhaps because they were not widely known, perhaps because few
reporters bothered to check up on them thoroughly. Many sources make it sound
like they were random domain names perhaps operated by the attacker, one goes
so far as to say that they were registered with fake identities.</p>
<p>Well, my Mandarin isn't great, and I think the language barrier is a big part
of the confusion. No doubt another part is a Western lack of familiarity with
Chinese internet culture. To an American in the security industry, 336901.com
would probably look at first like the result of a DGA or domain generation
algorithm. A randomly-generated domain used specifically to be evasive. In
China, though, numeric names like this are quite popular. Qihoo 360 is, after
all, domestically branded as just 360---360.cn.</p>
<p>As far as I can tell, both domains were pretty normal Chinese websites related
to mobile games. It's difficult or maybe impossible to tell now, but it seems
reasonable to speculate that they were operated by the same company. I would
assume they were something of a gray market operation, as there's a huge
intersection between "mobile games," "gambling," and "target of DDoS attacks."
For a long time, perhaps still today in the right corners of the industry, it
was pretty routine for gray-market gambling websites to pay booters to DDoS
each other.</p>
<p>In a 2016 presentation, security researchers from Verisign (Weinberg and
Wessels) reported on their analysis of the attack based on traffic observed at
Verisign root servers. They conclude that the traffic likely originated from
multiple botnets or at least botnet clients with different configurations,
since the attack traffic can be categorized into several apparently different
types [3]. Based on command and control traffic from a source they don't disclose
(perhaps from a Verisign honeynet?), they link the attack to the common
"BillGates" [4] botnet. Most interestingly, they conclude that it was probably
not intended as an attack on the DNS root: the choice of fixed domain names
just doesn't make sense, and the traffic wasn't targeted at all root servers.</p>
<p>Instead, they suspect it was just what it looks like: an attack on the two
websites the packets queried for, that for some reason was directed at the root
servers instead of the authoritative servers for that second-level domain.
This isn't a good strategy; the root servers are a far harder target than your
average web hosting company's authoritative servers. But perhaps it was a
mistake?  An experiment to see if the root server operators might mitigate the
DDoS by dropping requests for those two domains, incidentally taking the
websites offline?</p>
<p>Remember that Qihoo 360 operates a large honeynet and was kind enough to
publish a presentation on their analysis of root server attacks. Matching
Verisign's conclusions, they link the attack to the BillGates botnet, and also
note that they often observe multiple separate botnet C2 servers send tasks
targeting the same domain names. This probably reflects the commercialized
nature of modern botnets, with booters "subcontracting" operations to multiple
botnet operators. It also handily explains Verisign's observation that the 2015
attack traffic seems to have come from more than one implementation a DNS DDoS.</p>
<p>360 reports that, on the first day, five different C2 servers tasked bots with
attacking 336901.com. On the second day, three C2 servers tasked for 916yy.com.
But they also have a much bigger revelation: throughout the time period of the
attacks, they observed multiple tasks to attack 916yy.com using several
different methods.</p>
<p>360 concludes that the 2015 DNS attack was most likely the result of a
commodity DDoS operation that decided to experiment, directing traffic at the
DNS roots instead of the authoritative server for the target to see what would
happen. I doubt they thought they'd take down the root servers, but it seems
totally reasonable that they might have wondered if the root server operators
would filter DDoS traffic based on the domain name appearing in the requests.</p>
<p>Intriguingly, they note that some of the traffic originated with a DNS attack
tool that had significant similarities to BillGates but didn't produce quite
the same packets. Likely we will never know, but a likely explanation is that
some group modified the BillGates DNS attack module or implemented a new one
based on the method used by BillGates.</p>
<p>Tracking botnets gets very confusing very fast, there are just so many
different variants of any major botnet client! BillGates originated, for
example, as a Linux botnet. It was distributed to servers, not only through SSH
but through vulnerabilities in MySQL and ElasticSearch. It was unusual, for a
time, in being a major botnet that skipped over the most common desktop
operating system. But ports of BillGates to Windows were later observed,
distributed through an Internet Explorer vulnerability---classic Windows. Why
someone chose to port a Linux botnet to Windows instead of using one of the
several popular Windows botnets (Conficker, for example) is a mystery. Perhaps
they had spent a lot of time building out BillGates C2 infrastructure and, like
any good IT operation, wanted to simplify their cloud footprint.</p>
<p>High in the wizard's tower of the internet, thirteen elders are responsible for
starting every recursive resolver on its own path to truth. There's a whole
Neal Stephenson for Wired article there. But in practice it's a large and
robust system. The extent of anycast routing used for the root DNS servers, to
say nothing of CDNs, is one of those things that challenges are typical stacked
view of the internet. Geographic load balancing is something we think of at
high layers of the system, it's surprising to encounter it as a core part of a
very low level process.</p>
<p>That's why we need to keep our thinking flexible: computers are towers of
abstraction, and complexity can be added at nearly any level, as needed or
convenient. Seldom is this more apparent than it is in any process called
"bootstrapping." Some seemingly simpler parts of the internet, like DNS, rely
on a great deal of complexity within other parts of the system, like BGP.</p>
<p>Now I'm just complaining about pedagogical use of the OSI model again.</p>
<p>[1] The fact that the DNS hierarchy is written from right-to-left while it's
routinely used in URIs that are otherwise read left-to-right is one of those
quirks of computer history. Basically an endianness inconsistency. Like
American date order, to strictly interpret a URI you have to stop and reverse
your analysis part way through. There's no particular reason that DNS is like
that, there was just less consistency over most significant first/least
significant first hierarchical ordering at the time and contemporaneous network
protocols (consider the OSI stack) actually had a tendency towards least
significant first.</p>
<p>[2] The IPv4 addresses of the root servers are ages old and mostly just a
matter of chance, but the IPv6 addresses were assigned more recently and
allowed an opportunity for something more meaningful. Reflecting the long
tradition of identifying the root servers by their letter, many root server
operators use IPv6 addresses where the host part can be written as the single
letter of the server (i.e. root server C at [2001:500:2::c]). Others chose a
host part of "53," a gesture at the port number used for DNS (i.e.  root server
J, [2001:7fe::53]). Others seem more random, Verisign uses 2:30 for both of
their root servers (i.e. root server A, [2001:503:ba3e::2:30]), so maybe that
means something to them, or maybe it was just convenient. Amusingly, the only
operator that went for what I would call an address pun is the Defense
Information Systems Agency, which put root server G at [2001:500:12::d0d].</p>
<p>[3] It really dates this story that there was some controversy around the
source IPs of the attack, originating with none other than deceased security
industry personality John McAfee. He angrily insisted that it was not plausible
that the source IPs were spoofed. Of course botnets conducting DDoS attacks via
DNS virtually always spoof the source IP, as there are few protections in place
(at the time almost none at all) to prevent it. But John McAfee has always had
a way of ginning up controversy where none was needed.</p>
<p>[4] Botnets are often bought, modified, and sold. They tend to go by various
names from different security researchers and different variants. I'm calling
this one "BillGates" because that's the funniest of the several names used for
it.</p>
	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You almost never see a clock at the mall (115 pts)]]></title>
            <link>https://thehustle.co/originals/why-you-almost-never-see-a-clock-at-the-mall</link>
            <guid>39500263</guid>
            <pubDate>Sun, 25 Feb 2024 12:31:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thehustle.co/originals/why-you-almost-never-see-a-clock-at-the-mall">https://thehustle.co/originals/why-you-almost-never-see-a-clock-at-the-mall</a>, See on <a href="https://news.ycombinator.com/item?id=39500263">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          <p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p><em><span>Retailers, casinos, and grocery stores use a trick called temporal distortion to make you forget about the real world.</span></em></p><p><img src="https://20627419.fs1.hubspotusercontent-na1.net/hub/20627419/hubfs/clock.gif?width=595&amp;height=400&amp;name=clock.gif" alt="clock" width="595" height="400" srcset="https://20627419.fs1.hubspotusercontent-na1.net/hub/20627419/hubfs/clock.gif?width=298&amp;height=200&amp;name=clock.gif 298w, https://20627419.fs1.hubspotusercontent-na1.net/hub/20627419/hubfs/clock.gif?width=595&amp;height=400&amp;name=clock.gif 595w, https://20627419.fs1.hubspotusercontent-na1.net/hub/20627419/hubfs/clock.gif?width=893&amp;height=600&amp;name=clock.gif 893w, https://20627419.fs1.hubspotusercontent-na1.net/hub/20627419/hubfs/clock.gif?width=1190&amp;height=800&amp;name=clock.gif 1190w, https://20627419.fs1.hubspotusercontent-na1.net/hub/20627419/hubfs/clock.gif?width=1488&amp;height=1000&amp;name=clock.gif 1488w, https://20627419.fs1.hubspotusercontent-na1.net/hub/20627419/hubfs/clock.gif?width=1785&amp;height=1200&amp;name=clock.gif 1785w" sizes="(max-width: 595px) 100vw, 595px"></p>
<!--more-->
<p>The week before Christmas last year, I went to a mall in my hometown.</p>
<p>I wanted to buy a gift for my brother and browse through Barnes &amp; Noble. Instead I purchased a smoothie from a vendor by the escalator and an overpriced cookie from the food court. I didn‚Äôt find any Christmas presents ‚Äî I found a Korn T-shirt at Hot Topic that would‚Äôve been a solid gag gift, but it was too large. Or did I see it at Spencer‚Äôs? The stores all blended together.</p>
<p>When I returned to my car in the parking lot, after a journey through a walkway brightened by artificial lighting, I wasn‚Äôt sure if I‚Äôd been shopping for 30 minutes or an hour. I‚Äôd lost track of my surroundings and bought something I didn‚Äôt need.</p>
<p>In other words, I was an ideal customer.&nbsp;</p>
<p>This experience is shared by millions of shoppers every year ‚Äî and it‚Äôs by design. Malls, department stores, grocery stores, and even casinos enlist tactics that cause ‚Äútemporal distortion,‚Äù making us lose track of time and spend more money.&nbsp;</p>
<h4><strong><span>A time-free fantasy zone</span></strong></h4>
<p>When designing their layouts, malls took a cue from an industry that has perfected the art of separating people from their money: casinos.</p>
<p><a href="https://www.friedmandesign.com/about.html"><strong><span>Bill Friedman</span></strong></a> probably knows more about casinos than anybody else on the planet. He‚Äôs a former gambling addict, the only person to manage two casinos on the Strip at the same time, the author of two <a href="https://www.amazon.com/Casino-Management-Bill-Friedman/dp/081840311X"><span>seminal</span></a> <a href="https://www.amazon.com/Designing-casinos-dominate-competition-international/dp/0942828445/?_encoding=UTF8&amp;pd_rd_w=EQ8eS&amp;content-id=amzn1.sym.cf86ec3a-68a6-43e9-8115-04171136930a&amp;pf_rd_p=cf86ec3a-68a6-43e9-8115-04171136930a&amp;pf_rd_r=134-1143932-6182800&amp;pd_rd_wg=ASNZz&amp;pd_rd_r=d08f5ac8-3965-490c-97ce-fd8f048ae640&amp;ref_=aufs_ap_sc_dsk"><span>books</span></a> about the casino industry, and a researcher who performed empirical studies on dozens of Nevada casinos and interviewed some of the earliest casino operators in the state.</p>
<p>Friedman confirms an oft-cited detail about casinos: There are typically no clocks.</p>
<p>Back when Nevada first legalized gambling, in 1931, the casinos <em>did</em> have clocks. Over the next few years, however, the owners heard complaints from the biggest gamblers: Remove the clocks, or they‚Äôd stop coming. The reason why?</p>
<p>‚Äú[The gamblers] don‚Äôt want time,‚Äù Friedman told <em>The Hustle</em>. ‚ÄúThey are in a fantasy and an escape world.‚Äù</p>
<p><img src="https://lh7-us.googleusercontent.com/l-_kk7oQLJXulGbScS_9fET7MRMJw4WXfVwLpgFbyJsX7sMcFUfUv8Ig5Tl151xS1-mhIED0YutL9MqsC_i875PWx_8YBFSIhtSK0AuBXXehi9WkRv-I_-LhkUt-JucTu81G2Bo__MQ1JwoCiLmsxtw" width="528" height="393" loading="lazy"></p>
<p><em>Roulette at Nevada‚Äôs New Meadows Club in 1931. (Bettmann/Getty Images)</em></p>
<p>Friedman says he and other casino operators never unlocked an innovation that caused people to gamble more (not even free alcohol, he says). But they did what they could to eliminate distractions that interfered with the gamblers‚Äô fantasy states.</p>
<p>In the mid-1970s, for instance, when Friedman started managing Castaways and The Glass Slipper, he identified a major obstacle: natural light.</p>
<p>One side of the Castaways casino was covered in glass. Every morning when the sun peeked over the buildings on the Strip, the slot machines and tables emptied. Some <strong>85%-90%</strong> of customers left the gambling floor.</p>
<p>‚ÄúThey realized it was daytime and they had a world they had to go back to,‚Äù Friedman says.</p>
<p>Friedman bought a dark plastic covering for the glass. His security team raised it over the glass about five minutes before sunrise every morning, blocking out most of the light. The daylight exodus ceased.</p>
<p>Under Friedman‚Äôs management, Castaways and The Glass Slipper became two of Vegas‚Äô most profitable casinos in the 1970s and 1980s. He trained his staff not to interrupt gamblers who were in the zone ‚Äî if somebody earned <a href="https://en.wikipedia.org/wiki/Comps_(casino)#:~:text=Comps%20are%20complimentary%20items%20and,and%20how%20long%20they%20play."><span>a comp</span></a>, a staffer was to quickly tell them they‚Äôd like to buy them a free meal and let the customer claim it when their ‚Äúfantasy‚Äù ended.</p>
<p>‚ÄúYou don‚Äôt disrupt,‚Äù Friedman says.&nbsp;</p>

<p><img src="https://lh7-us.googleusercontent.com/iXfn86cGuFGu1cnTK55dqyxf1ZLqwyhUPZzGWVrXgCZqBVGPJ8bxW3t_AQODod4zfTEHE7ERtfo4LhP8ndeCxjPQaqtZuY4_Uiliw4uWkxoFTSD1IcU3z0xh9FaH20EqQYNYKqPUfPdCuAxDFalhNtE" width="528" height="865" loading="lazy"></p>
<p><em>Castaways thrived in the 1970s and 1980s. It was imploded in 2006. (Ethan Miller/Getty Images)</em></p>
<p>Friedman shared many of his techniques in his books and as a casino consultant. Some of his top design principles included:</p>
<ul>
<li aria-level="1"><strong>Intricate spacing:</strong> Low ceilings, maze-like designs, and intimate nooks for slot machines allow customers to slow down and feel a busy energy as they pass through a casino ‚Äî yet still have room to carve out their own private domains by the machines.</li>
<li aria-level="1"><strong>Mundane and medium sound levels:</strong> Sensual overload and environments in which sounds bounce off interior surfaces confuse customers. Most casinos use bland, low-volume music or ambient noise ‚Äî researchers <a href="https://pubmed.ncbi.nlm.nih.gov/19582553/"><span>have found</span></a> that songs provide gamblers a cue of how much time has passed, a big no-no.</li>
<li aria-level="1"><strong>Monotonous design:</strong> Friedman believed that the machines and tables should be the focal point of any casino. Decor shouldn‚Äôt be so elaborate that it draws attention away from them.&nbsp;&nbsp;&nbsp;</li>
</ul>
<p>With the atmosphere just right, gamblers become engrossed in the game in front of them, causing their ‚Äúfield of consciousness to break down,‚Äù <a href="https://theses.gla.ac.uk/2173/1/1996reithphd.pdf"><span>according to sociologist<span> </span></span><span><strong><span>Gerda Reith</span></strong></span></a>.</p>
<p>‚ÄúIn the arena of chance, as though under the sway of a magnetic field, the passage of time freezes into repetition, space contracts, and the value that accrues to money is obliterated,‚Äù she wrote in a thesis on gambling in western society.</p>
<p><strong>Raymond Lavoie</strong>, a New Mexico State University marketing professor, <a href="https://cdspress.ca/wp-content/uploads/2022/09/Raymond-V-Lavoie-Kelley-J-Main.pdf"><span>studied gamblers and flow</span></a>, a state of mind where a person is so attentive to a task they lose track of everything else, including time. He found that gamblers who entered a flow state ‚Äî caused by a distraction-free environment and a focus on a stimulating task ‚Äî spent more time and money gambling than those who didn‚Äôt.</p>
<p>They kept going because it felt good, so much so that despite losing more money overall they enjoyed the experience more.</p>
<p>‚ÄúYou‚Äôre not thinking about the future,‚Äù Lavoie says, ‚Äúyou‚Äôre just enjoying this moment.‚Äù</p>
<h4><strong><span>The longer you stay, the more you spend</span></strong></h4>
<p>Casinos are unique in their aim. No other industry builds a world designed to maintain a customer‚Äôs fantasy so that they continue to partake in an activity that, in the long run, advantages the house. But retailers share at least one common goal with casinos: They want to keep people inside.</p>
<p>‚ÄúThe goal of the retailer is to make sure that they find ways to increase the time we spend in the store, the number of items we see at the store, and end up buying not just what we came in for but ideally a few additional things,‚Äù says <strong>Vassilis Dalakas</strong>, a business professor at California State University San Marcos who has researched consumer psychology.</p>
<p><img src="https://lh7-us.googleusercontent.com/TkX2wuQk5t_jRxfb00yL4_4bSjLJJrjEDxJzmC0SqxEaNpsqAiO5B8CI846DTl4vXAZSzouZWlXmgN-OrEBgk7MC8tE697TUulNw1Q-cnIoIMXdmXBDreS382mFhQ5mhVGyfjD3fVqHl3JXbnqpu1P8" width="528" height="352" loading="lazy"></p>
<p><em>Shoppers at London‚Äôs Westfield Shopping Centre. (Oli Scarff/Getty Images)</em></p>
<p>Extra time in the store, in fact, can be nearly as dangerous to one‚Äôs wallet as extra time in the casino. When we shop, we have about 20 minutes before our brains lose the power to keep us from making questionable financial decisions, according to <a href="https://www.seren.bangor.ac.uk/discovery/science/2013/12/11/mri-to-understand-how-we-shop/#:~:text=The%20experiment%20works%20by%20displaying,in%20the%20shops%20every%20day."><span>researchers from Bangor University in the United Kingdom</span></a>.</p>
<p>Using MRIs to gauge ‚Äú<a href="https://www.bbc.com/news/uk-wales-north-west-wales-24995031"><span>the neural basis of decision making</span></a>,‚Äù they found that after <strong>23 minutes</strong> supermarket shoppers began using the emotional part of their brain rather than the cognitive part. That switch made it harder for people to consider costs and made them more susceptible to marketing bargains.</p>
<p>After <strong>40 minutes</strong>, their brains effectively shut down. They struggled to make any logical decisions.</p>
<p><img src="https://lh7-us.googleusercontent.com/Ap7nZPz8vwpIusAPBpIHsaOMouaHm-9nZEs7Rga_57DyWdTj92j481Zgb3jkRUanqrwIJs7Y0DQnKzul3cBOlk4WQykmwoLQlq6jsPiH4peTHCTUYwDHda0xlQdIEUnvSBaeQVNLk9uVDcPfzBiqhQg" width="528" height="589" loading="lazy"></p>
<p><em>The Hustle</em></p>
<p>But grocery stores face an uphill battle in persuading customers to stick around. Most people go to the grocery store knowing the items they want and trying to get out quickly. To keep shoppers around longer, grocers and other retail stores employ several tactics and design elements similar to the casino industry:</p>
<ul>
<li aria-level="1"><strong>Removing distractions:</strong> There are few clocks and windows, so customers won‚Äôt be reminded of the outside world. Boring designs draw attention to the products, especially at grocery stores and discount department stores.</li>
<li aria-level="1"><strong>Building a maze:</strong> These layouts, <a href="https://thehustle.co/how-ikea-tricks-you-into-buying-more-stuff/"><span>most prominently deployed at Ikea</span></a>, force customers to move through the store slowly and see as many products as possible.</li>
</ul>
<p>Retail stores also use sound and music to manipulate the environment. <a href="https://www.jstor.org/stable/1251706?seq=3"><span>A study from the early 1980s</span></a> showed that slow-tempo music led to shoppers moving more slowly through the store and spending more money than if fast-tempo music played. (There wasn‚Äôt a huge difference in results between slow-tempo music and no music at all.)</p>
<p>‚ÄúIt‚Äôs almost mood maintenance,‚Äù says <strong>Theodore Noseworthy</strong>, a York University business professor who has studied the impacts of sound. ‚ÄúThey‚Äôre trying to keep you in this positive state and almost in flow [so] that if you‚Äôre shopping, [you] just stay shopping.‚Äù</p>
<p><img src="https://lh7-us.googleusercontent.com/iVrgVmB_DlBkToLTtBo-W5jKsGuC2DOmY4wiws86Q7MKunk07eDrGh4hBbTJ29g3J7kzI735Fl-x2OJZN45vNy8QxoPb9-9OAWMGKVZJ6zIM4i-RXFlWmv4pfwMWurhwosCuBQiNl14b3BQos6HikJ0" width="528" height="533" loading="lazy"></p>
<p><em>The Hustle</em></p>
<p>Whether it‚Äôs inside a department store or a casino, it‚Äôs now harder than ever to minimize distractions and foster a sense of lost time and place. Blame smartphones.</p>
<p>A ping from a text or email can draw a person‚Äôs eyes to a screen, giving them an instant reminder of the time and something else they could be doing.</p>
<p>‚ÄúWhen you pull that thing out, it breaks you from that immersive environment,‚Äù Noseworthy says.</p>
<p>The good news for retailers and casinos, though, is they may soon be able to use technology to create an environment more immersive than anything we‚Äôve seen.&nbsp;&nbsp;</p>
<h4><strong><span>No time in the future</span></strong></h4>
<p>When I asked Lavoie, the academic who studies flow state, for another area ideal for creating flow besides casinos, he offered an immediate answer: virtual reality.</p>
<p>People who don VR headsets don‚Äôt see or hear anything other than the virtual world in front of them. If VR <a href="https://hbswk.hbs.edu/item/virtual-reality-check-how-long-before-we-live-in-the-metaverse"><span>takes off</span></a>, retailers, marketers, and casino operators will use it to their advantage, building mesmerizing simulations for people to shop, buy, and gamble in.</p>
<p>There would be no human distractions, such as friends or family members telling someone it‚Äôs time to stop gambling or shopping. And there would certainly be no clocks.</p>
<p>‚ÄúYou don‚Äôt have to worry about any of that,‚Äù Lavoie says. ‚ÄúThis is infinitely better than that. It narrows your attention fully.‚Äù</p>
<p>It also sounds dystopian. But if learning about this worrisome consumer future makes you queasy, at least take comfort in knowing that it‚Äôs natural to lose track of time and space.</p>
<p>As you get older, Noseworthy reminded me, it feels like time moves faster. In reality, it‚Äôs just that most of our days are mundane and monotonous, blending together and helping us lose track of time.</p>
<p>Casinos and malls may be time warps. But so is life.</p></span>
          
          </p>
           






  

  
  

  

  

  
  

  



	   


  

           
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'm confused: what's with the project descriptions at HTTPS://pkgx.dev/pkgs/? (108 pts)]]></title>
            <link>https://github.com/pkgxdev/pantry/issues/5358</link>
            <guid>39499734</guid>
            <pubDate>Sun, 25 Feb 2024 10:57:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/pkgxdev/pantry/issues/5358">https://github.com/pkgxdev/pantry/issues/5358</a>, See on <a href="https://news.ycombinator.com/item?id=39499734">Hacker News</a></p>
<div id="readability-page-1" class="page"><div disabled="" sortable="">
          <p dir="auto">Sorry if this has been reported elsewhere already, or if this is explained in docs somewhere, but I don't understand the contents you have in <a href="https://pkgx.dev/pkgs/" rel="nofollow">https://pkgx.dev/pkgs/</a>. Lets take a few popular projects:</p>
<ul dir="auto">
<li><a href="https://pkgx.dev/pkgs/atuin.sh/" rel="nofollow">https://pkgx.dev/pkgs/atuin.sh/</a></li>
</ul>
<blockquote>
<p dir="auto">Virtual reality mapping software.<br>
Atuin is a software package that provides an interactive map and information on road toll rates, construction updates, and projects managed by the Central Texas Regional Mobility Authority. It also offers resources on business opportunities, financial information, and traveler information. Additionally, it allows users to pay tolls and provides information on road rules and violations.</p>
</blockquote>
<p dir="auto">This is... complete nonsense? Is it auto-generated by AI or something?</p>
<ul dir="auto">
<li><a href="https://pkgx.dev/pkgs/rye-up.com/" rel="nofollow">https://pkgx.dev/pkgs/rye-up.com/</a></li>
</ul>
<blockquote>
<p dir="auto">Rye-up.com: Python project configuration.<br>
Rye-up.com is a software package for Python projects that provides a user guide, community support, and a changelog. It helps with initializing search, syncing and locking projects, building, and publishing. Developed by mitsuhiko/rye, it aims to streamline the development process with its easy installation, configuration, and Python project management features.</p>
</blockquote>
<p dir="auto">"Provides a user guide, community support, and a changelog" üòÇ Surely it cannot have been written by a human? It makes no sense either üòï I thought AI did better than this nowadays?</p>
<ul dir="auto">
<li><a href="https://pkgx.dev/pkgs/github.com/squidfunk/mkdocs-material/" rel="nofollow">https://pkgx.dev/pkgs/github.com/squidfunk/mkdocs-material/</a></li>
</ul>
<blockquote>
<p dir="auto">Responsive documentation theme package.<br>
The software package "squidfunk/mkdocs-material" is a documentation tool that is designed to be user-friendly and effective. It provides the ability to automate workflows, manage packages, and secure documentation. With features like code review and issue tracking, it allows for efficient collaboration and code improvement. It is suitable for enterprise teams, startups, and education purposes, offering CI/CD and DevOps solutions.</p>
</blockquote>
<p dir="auto">Sounds a bit better, but it's still plain wrong: there's no concept about automating workflows, managing packages, reviewing code or tracking issues in mkdocs-material...</p>
<p dir="auto">I really don't want to sound harsh or dismissive of any work that has been put in this package manager or its documentation, but IMO putting non-sense like this in the official project listing completely breaks any trust one could have in pkgx? Why listing everything with poorly generated contents? All these projects have a description on GitHub, can't that be used instead? Or maybe fetch metadata from the respective registries?</p>
<p dir="auto">I get the generated pictures, waiting for maintainers to put their logo instead, but the descriptions...</p>
<p dir="auto">I'm really confused üòÖ</p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hallucination is inevitable: An innate limitation of large language models (275 pts)]]></title>
            <link>https://arxiv.org/abs/2401.11817</link>
            <guid>39499207</guid>
            <pubDate>Sun, 25 Feb 2024 09:28:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2401.11817">https://arxiv.org/abs/2401.11817</a>, See on <a href="https://news.ycombinator.com/item?id=39499207">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2401.11817.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all of the computable functions and will therefore always hallucinate. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Ziwei Xu [<a href="https://arxiv.org/show-email/3884b865/2401.11817">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 22 Jan 2024 10:26:14 UTC (291 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Face Is Exposed for AOL Searcher No. 4417749 (2006) (108 pts)]]></title>
            <link>https://www.nytimes.com/2006/08/09/technology/09aol.html</link>
            <guid>39499167</guid>
            <pubDate>Sun, 25 Feb 2024 09:22:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2006/08/09/technology/09aol.html">https://www.nytimes.com/2006/08/09/technology/09aol.html</a>, See on <a href="https://news.ycombinator.com/item?id=39499167">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2006/08/09/technology/09aol.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[TSMC is having more luck building in Japan than in America (144 pts)]]></title>
            <link>https://www.economist.com/business/2024/02/22/tsmc-is-having-more-luck-building-in-japan-than-in-america</link>
            <guid>39498863</guid>
            <pubDate>Sun, 25 Feb 2024 08:39:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/business/2024/02/22/tsmc-is-having-more-luck-building-in-japan-than-in-america">https://www.economist.com/business/2024/02/22/tsmc-is-having-more-luck-building-in-japan-than-in-america</a>, See on <a href="https://news.ycombinator.com/item?id=39498863">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p><span><a href="https://www.economist.com/business/" data-analytics="sidebar:section"><span>Business</span></a></span><span> | <!-- -->A tale of two chip factories</span></p><h2>TSMC is having more luck building in Japan than in America</h2><h2>Truculent workers and red tape have slowed its efforts in Arizona</h2></section><div><div data-body-id="cp2"><div><figure><div><figcaption>Listen to this story.</figcaption> <p><span>Enjoy more audio and podcasts on<!-- --> <a id="audio-ios-cta" href="https://economist-app.onelink.me/d2eC/bed1b25" target="_blank" rel="noreferrer">iOS</a> <!-- -->or<!-- --> <a id="audio-android-cta" href="https://economist-app.onelink.me/d2eC/7f3c199" target="_blank" rel="noreferrer">Android</a>.</span></p></div><audio controls="" id="audio-player" preload="none" src="https://www.economist.com/media-assets/audio/061%20Business%20-%20Semiconductors-db25a240d78686a73a82e971c70c3843.mp3" title="TSMC is having more luck building in Japan than in America" controlslist="nodownload"><p>Your browser does not support the &lt;audio&gt; element.</p></audio></figure></div><p data-component="paragraph"><span data-caps="initial">O</span><small>n the Japanese</small> island of Kyushu, the fruits of the country‚Äôs industrial policy are about to go on show. On February 24th <small>tsmc,</small> the world‚Äôs most advanced chip producer, will open its first fabrication plant in the country. Earlier this month it announced plans for a second plant nearby.</p><p data-component="paragraph">Contrast that with the Taiwanese giant‚Äôs other big international expansion, in America. Last summer it pushed back the start of production at the first of two plants it is building in Arizona from 2024 to 2025. In January it announced that a second plant, previously scheduled to open in 2026, would not be operational until 2027 or 2028. The second was meant to produce three-nanometre (nm) chips, the most advanced currently on the market, but <small>tsmc </small>has raised the prospect that it may now be used for less cutting-edge production.</p><p data-component="paragraph">Both the Japanese and American governments are eager to expand domestic production of chips, and are seeking the help of foreign firms. So what explains the contrast between celebrations in Kumamoto and headaches in Arizona? The first point of divergence is in labour relations. A protracted spat with the Arizona Building and Construction Trades Council, an association of labour unions, over the use of Taiwanese workers to build the plants was finally resolved in December with a promise by <small>tsmc </small>to hire and train local workers. Union activism is comparatively rare in Japan: the country typically loses fewer than 10,000 days of workers‚Äô labour each year to stoppages, compared with more than a million in America.</p><p data-component="paragraph">Helpful local partners are the second reason why <small>TSMC</small>‚Äôs experience in Japan has been smoother. Denso, a Japanese maker of car parts, and the chipmaking arm of Sony, an electronics giant, both took minority stakes in <small>TSMC</small>‚Äôs subsidiary in Japan. Earlier this month Toyota, a carmaker, invested in the venture, too. Those companies have plenty of experience pulling off big projects in their country. Helpfully, they are also the primary customers for the chips the plants will build, notes Lim Tai Wei, a researcher at the East Asian Institute of the National University of Singapore. In contrast, the Taiwanese firm is going it alone in Arizona, its first large project in America since the 1990s.</p><p data-component="paragraph">A final difference concerns subsidies. Both the Japanese and American governments have tempted global chipmakers with offers of grants. <small>tsmc </small>has already received money from Japan‚Äôs government, which agreed to stump up half the cost of its capital spending for the Kumamoto project. But it has yet to get any funds from America under the <small>CHIPS</small> Act passed in 2022. Disbursements of cash have been slowed by negotiations over conditions, including what profits America‚Äôs government will be entitled to, says Stephen Ezell of the Information Technology and Innovation Foundation, a think-tank based in Washington. In the meantime, the costs of building and equipping the sites have been going up. Mark Liu, <small>tsmc</small>‚Äôs outgoing chairman, said in January that the technology in its second Arizona plant would depend on the incentives on offer.</p><p data-component="paragraph">The delays could drag on. In October Gina Raimondo, America‚Äôs Commerce Secretary, warned that projects like <small>tsmc</small>‚Äôs could be held up for years by the environmental reviews required for federal funding unless they are exempted on national-security grounds. In a survey of around 200 semiconductor firms by the Bureau for Industry and Security, an American government agency, 64% named environmental rules as among their biggest regulatory problems, compared with 21% who included export controls and 18% who cited local permit and zoning laws.</p><p data-component="paragraph">There are plenty of lessons to be learned. Subsidies are effective only if they are paid, and environmental red tape in America has become excessive. Combative unions may put off foreign firms. Would-be customers should take a keener interest in the success of their suppliers. Foreign firms, for their part, must also learn to adapt to local market conditions, rather than trying to copy and paste approaches that worked at home, argues Chris Miller, a historian of the semiconductor industry at Tufts University.</p><p data-component="paragraph">Japan, meanwhile, should avoid being smug about its success. The <small>TSMC </small>plant opening in Kumamoto on February 24th will produce chips ranging in size from 12nm to 28nm, less advanced than those in the works in Arizona. For the whizziest chips, Japan is instead pinning its hopes on Rapidus, a company established in 2022 with the backing of eight of the country‚Äôs biggest firms, including the three that took a stake in <small>TSMC</small>‚Äôs subsidiary. Its goal is to mass-produce bleeding-edge 2nm chips by 2027. That more ambitious plan could still encounter plenty of stumbling blocks along the way. <span>‚ñ†</span></p><p data-component="paragraph"><i>To stay on top of the biggest stories in business and technology, sign up to&nbsp;the <a href="https://www.economist.com/newsletters/the-bottom-line">Bottom Line</a>, our weekly subscriber-only newsletter.</i></p></div><p><h3 id="article-tags">Explore more</h3><nav aria-labelledby="article-tags"><a href="https://www.economist.com/topics/united-states" data-analytics="tags:united_states"><span>United States</span></a><a href="https://www.economist.com/topics/japan" data-analytics="tags:japan"><span>Japan</span></a></nav></p><p>This article appeared in the Business section of the print edition under the headline "A tale of two chip factories"</p><div data-tracking-id="content-well-chapter-list"><h2><a href="https://www.economist.com/business/">Business</a> <span>February 24th 2024</span></h2><ul><li><a href="https://www.economist.com/business/2024/02/18/why-the-worlds-mining-companies-are-so-stingy"><span>Why the world‚Äôs mining companies are so stingy</span></a></li><li><a href="https://www.economist.com/business/2024/02/22/the-worlds-biggest-maker-of-spectacles-wants-to-be-a-tech-firm"><span>The world‚Äôs biggest maker of spectacles wants to be a tech firm</span></a></li><li><a href="https://www.economist.com/business/2024/02/22/why-does-landlocked-eswatini-have-a-ship-registry"><span>Why does landlocked Eswatini have a ship registry?</span></a></li><li><a href="https://www.economist.com/business/2024/02/22/the-making-of-a-powerpoint-slide"><span>The making of a PowerPoint slide</span></a></li><li><a href="https://www.economist.com/business/2024/02/22/the-age-of-the-unicorn-is-over"><span>The age of the unicorn is over</span></a></li><li><a href="https://www.economist.com/business/2024/02/22/tsmc-is-having-more-luck-building-in-japan-than-in-america"><span>TSMC is having more luck building in Japan than in America</span></a></li><li><a href="https://www.economist.com/business/2024/02/20/is-running-a-top-university-americas-hardest-job"><span>Is running a top university America‚Äôs hardest job?</span></a></li></ul></div><div orientation="vertical" data-test-id="vertical"><div orientation="vertical"><figure><img alt="Is Europe ready?" loading="lazy" width="1280" height="1684" decoding="async" data-nimg="1" sizes="300px" srcset="https://www.economist.com/cdn-cgi/image/width=16,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg 16w, https://www.economist.com/cdn-cgi/image/width=32,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg 32w, https://www.economist.com/cdn-cgi/image/width=48,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg 48w, https://www.economist.com/cdn-cgi/image/width=64,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg 64w, https://www.economist.com/cdn-cgi/image/width=96,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg 96w, https://www.economist.com/cdn-cgi/image/width=128,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg 128w, https://www.economist.com/cdn-cgi/image/width=256,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg 256w, https://www.economist.com/cdn-cgi/image/width=360,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg 360w, https://www.economist.com/cdn-cgi/image/width=384,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg 384w, https://www.economist.com/cdn-cgi/image/width=480,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg 480w, https://www.economist.com/cdn-cgi/image/width=600,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg 600w, https://www.economist.com/cdn-cgi/image/width=834,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg 834w, https://www.economist.com/cdn-cgi/image/width=960,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg 960w, https://www.economist.com/cdn-cgi/image/width=1096,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg 1096w, https://www.economist.com/cdn-cgi/image/width=1280,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg 1280w, https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg 1424w" src="https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/media-assets/image/20240224_DE_EU.jpg"></figure></div><div orientation="vertical"><h3 orientation="vertical">From the February 24th 2024 edition</h3><p orientation="vertical">Discover stories from this section and more in the list of contents</p><p><a href="https://www.economist.com/printedition/2024-02-24" data-analytics="sidebar:weekly_edition"><span>Explore the edition</span></a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Earth just experienced its hottest 12 months in recorded history (238 pts)]]></title>
            <link>https://www.theweathernetwork.com/en/news/climate/impacts/january-2024-hottest-on-record-tops-warmest-12-month-period-in-history</link>
            <guid>39498345</guid>
            <pubDate>Sun, 25 Feb 2024 07:01:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theweathernetwork.com/en/news/climate/impacts/january-2024-hottest-on-record-tops-warmest-12-month-period-in-history">https://www.theweathernetwork.com/en/news/climate/impacts/january-2024-hottest-on-record-tops-warmest-12-month-period-in-history</a>, See on <a href="https://news.ycombinator.com/item?id=39498345">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="blur-background-box"><header><div><div><p><span data-testid="article-published-date">Published on <!-- -->Feb. 23, 2024, 6:15 PM</span></p></div><p>January has already broken temperature records, starting this year off on-course to break the global records set in 2023.</p></div></header><p>Temperatures soared for the globe in January, as several new records for heat were set during the month.</p><p>The <a href="https://wmo.int/media/news/world-had-warmest-january-record">World Meteorological Agency</a>, <a href="https://data.giss.nasa.gov/gistemp/graphs_v4/">NASA</a>, <a href="https://www.noaa.gov/news/january-2024-marked-8th-month-in-row-of-record-global-warmth">NOAA</a>, Europe's <a href="https://climate.copernicus.eu/surface-air-temperature-january-2024">Copernicus Climate Change Service</a>, and the <a href="https://ds.data.jma.go.jp/tcc/tcc/products/gwp/temp/jan_wld.html">Japan Meteorological Agency</a> all agree: last month was the hottest month of January ever recorded.</p><p><span><span><img alt="Jan 2024 Global Land and Ocean Temperature Anomalies - NOAA" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://images.twnmm.com/c55i45ef3o2a/2WB1y3jwqoPmI1e5PMosVq/25cffa0692583e3e4c9dc6c7b69a8477/Jan-2024-Global-Land_Ocean-Temp-Anomalies-Labelled-NOAA.jpg?w=640&amp;q=80&amp;fm=webp 640w, https://images.twnmm.com/c55i45ef3o2a/2WB1y3jwqoPmI1e5PMosVq/25cffa0692583e3e4c9dc6c7b69a8477/Jan-2024-Global-Land_Ocean-Temp-Anomalies-Labelled-NOAA.jpg?w=750&amp;q=80&amp;fm=webp 750w, https://images.twnmm.com/c55i45ef3o2a/2WB1y3jwqoPmI1e5PMosVq/25cffa0692583e3e4c9dc6c7b69a8477/Jan-2024-Global-Land_Ocean-Temp-Anomalies-Labelled-NOAA.jpg?w=828&amp;q=80&amp;fm=webp 828w, https://images.twnmm.com/c55i45ef3o2a/2WB1y3jwqoPmI1e5PMosVq/25cffa0692583e3e4c9dc6c7b69a8477/Jan-2024-Global-Land_Ocean-Temp-Anomalies-Labelled-NOAA.jpg?w=1080&amp;q=80&amp;fm=webp 1080w, https://images.twnmm.com/c55i45ef3o2a/2WB1y3jwqoPmI1e5PMosVq/25cffa0692583e3e4c9dc6c7b69a8477/Jan-2024-Global-Land_Ocean-Temp-Anomalies-Labelled-NOAA.jpg?w=1200&amp;q=80&amp;fm=webp 1200w, https://images.twnmm.com/c55i45ef3o2a/2WB1y3jwqoPmI1e5PMosVq/25cffa0692583e3e4c9dc6c7b69a8477/Jan-2024-Global-Land_Ocean-Temp-Anomalies-Labelled-NOAA.jpg?w=1920&amp;q=80&amp;fm=webp 1920w, https://images.twnmm.com/c55i45ef3o2a/2WB1y3jwqoPmI1e5PMosVq/25cffa0692583e3e4c9dc6c7b69a8477/Jan-2024-Global-Land_Ocean-Temp-Anomalies-Labelled-NOAA.jpg?w=2048&amp;q=80&amp;fm=webp 2048w, https://images.twnmm.com/c55i45ef3o2a/2WB1y3jwqoPmI1e5PMosVq/25cffa0692583e3e4c9dc6c7b69a8477/Jan-2024-Global-Land_Ocean-Temp-Anomalies-Labelled-NOAA.jpg?w=3840&amp;q=80&amp;fm=webp 3840w" src="https://images.twnmm.com/c55i45ef3o2a/2WB1y3jwqoPmI1e5PMosVq/25cffa0692583e3e4c9dc6c7b69a8477/Jan-2024-Global-Land_Ocean-Temp-Anomalies-Labelled-NOAA.jpg?w=3840&amp;q=80&amp;fm=webp" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></span></p><p><i>Global average temperature anomalies for each January from 1950 to 2024 are plotted here, with each month compared to the 20th century average of 12.2¬∞C Credit: NOAA</i></p><p>"Temperatures were above average throughout the Arctic, most of northeastern North America, central Russia, southern and western Asia, Africa, South America, eastern and southeastern Asia and Australia. Africa and South America saw their warmest Januarys on record," <a href="https://www.noaa.gov/news/january-2024-marked-8th-month-in-row-of-record-global-warmth">says NOAA</a>.</p><p>Furthermore, according to Copernicus' records, January's global average temperature was 1.66¬∞C warmer than the pre-industrial average for January (from 1851-1900).</p><p>"It is the eighth month in a row that is the warmest on record for the respective time of the year. Sea surface temperatures have been record high for ten consecutive months," the <a href="https://wmo.int/media/news/world-had-warmest-january-record">WMO stated</a>.</p><p><span><span><img alt="Daily SST - Feb15 2024 - Climate Reanalyzer" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://images.twnmm.com/c55i45ef3o2a/fOvma9gjClAZhdIcVyXK6/aa5733fdab34594d83a1a6103a18f1ba/Daily-SST-Feb15-2024-ClimateReanalyzer.jpg?w=640&amp;q=80&amp;fm=webp 640w, https://images.twnmm.com/c55i45ef3o2a/fOvma9gjClAZhdIcVyXK6/aa5733fdab34594d83a1a6103a18f1ba/Daily-SST-Feb15-2024-ClimateReanalyzer.jpg?w=750&amp;q=80&amp;fm=webp 750w, https://images.twnmm.com/c55i45ef3o2a/fOvma9gjClAZhdIcVyXK6/aa5733fdab34594d83a1a6103a18f1ba/Daily-SST-Feb15-2024-ClimateReanalyzer.jpg?w=828&amp;q=80&amp;fm=webp 828w, https://images.twnmm.com/c55i45ef3o2a/fOvma9gjClAZhdIcVyXK6/aa5733fdab34594d83a1a6103a18f1ba/Daily-SST-Feb15-2024-ClimateReanalyzer.jpg?w=1080&amp;q=80&amp;fm=webp 1080w, https://images.twnmm.com/c55i45ef3o2a/fOvma9gjClAZhdIcVyXK6/aa5733fdab34594d83a1a6103a18f1ba/Daily-SST-Feb15-2024-ClimateReanalyzer.jpg?w=1200&amp;q=80&amp;fm=webp 1200w, https://images.twnmm.com/c55i45ef3o2a/fOvma9gjClAZhdIcVyXK6/aa5733fdab34594d83a1a6103a18f1ba/Daily-SST-Feb15-2024-ClimateReanalyzer.jpg?w=1920&amp;q=80&amp;fm=webp 1920w, https://images.twnmm.com/c55i45ef3o2a/fOvma9gjClAZhdIcVyXK6/aa5733fdab34594d83a1a6103a18f1ba/Daily-SST-Feb15-2024-ClimateReanalyzer.jpg?w=2048&amp;q=80&amp;fm=webp 2048w, https://images.twnmm.com/c55i45ef3o2a/fOvma9gjClAZhdIcVyXK6/aa5733fdab34594d83a1a6103a18f1ba/Daily-SST-Feb15-2024-ClimateReanalyzer.jpg?w=3840&amp;q=80&amp;fm=webp 3840w" src="https://images.twnmm.com/c55i45ef3o2a/fOvma9gjClAZhdIcVyXK6/aa5733fdab34594d83a1a6103a18f1ba/Daily-SST-Feb15-2024-ClimateReanalyzer.jpg?w=3840&amp;q=80&amp;fm=webp" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></span></p><p><i>Sea surface temperatures set a record for 2023, but that record has already been broken as of late January and early February 2024. Credit: Climate Reanalyzer</i></p><hr><p><b>READ MORE: </b><a href="https://www.theweathernetwork.com/en/news/climate/impacts/2023-was-the-worlds-hottest-year-on-record-by-a-wide-margin">After 2023's astounding new global heat record, 2024 may be even worse</a></p><hr><p>Based on at least NOAA's and Copernicus' records, the past 12 months ‚Äî February 2023 to January 2024 ‚Äî was also the hottest 12-month period on record.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ubisoft Employees In France have gone on a Strike (112 pts)]]></title>
            <link>https://playstationcouch.com/post.php?id=161</link>
            <guid>39498276</guid>
            <pubDate>Sun, 25 Feb 2024 06:43:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://playstationcouch.com/post.php?id=161">https://playstationcouch.com/post.php?id=161</a>, See on <a href="https://news.ycombinator.com/item?id=39498276">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><img src="https://playstationcouch.com/uploads/LWmmRrKA4uvBbSSJtHDnEc-1920-80.jpg" alt="Ubisoft Employees In France have gone on a Stike!"></p>
            <p>Ubisoft has been in rough waters lately, with their "4AAAA" Skulls and Bones failure, their latest IP.
Due to what has been happening lately in the company, as bad treatment by the higher-ups, the guys that make decisions.

Some 700 unionized Ubisoft employees downed (development) tools in France, walking off the job in a day of organized strike action. Following the collapse of annual salary negotiations, on Valentine's Day, workers from Ubisoft Paris, Montpellier, Lyon, Annecy, and Bordeaux took part, taking to the streets in the time-honored French tradition.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bought a Prison Laptop on eBay (165 pts)]]></title>
            <link>https://twitter.com/zephray_wenting/status/1761548861896606014</link>
            <guid>39498047</guid>
            <pubDate>Sun, 25 Feb 2024 05:50:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/zephray_wenting/status/1761548861896606014">https://twitter.com/zephray_wenting/status/1761548861896606014</a>, See on <a href="https://news.ycombinator.com/item?id=39498047">Hacker News</a></p>
Couldn't get https://twitter.com/zephray_wenting/status/1761548861896606014: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Selfish reasons to want more humans (180 pts)]]></title>
            <link>https://rootsofprogress.org/why-a-larger-population</link>
            <guid>39497686</guid>
            <pubDate>Sun, 25 Feb 2024 04:34:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rootsofprogress.org/why-a-larger-population">https://rootsofprogress.org/why-a-larger-population</a>, See on <a href="https://news.ycombinator.com/item?id=39497686">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      
        <h2>A bigger world is better for everyone</h2>
      
      <p>What is the ideal size of the human population?</p>

<p>One common answer is ‚Äúmuch smaller.‚Äù Paul Ehrlich, co-author of <em>The Population Bomb</em> (1968), has as recently as 2018 promoted the idea that ‚Äú<a href="https://www.theguardian.com/cities/2018/mar/22/collapse-civilisation-near-certain-decades-population-bomb-paul-ehrlich" target="_blank">the world‚Äôs optimum population is less than two billion people</a>,‚Äù a reduction of the current population by about 75%. And Ehrlich is a piker compared to Jane Goodall, who said that many of our problems would go away ‚Äú<a href="https://www.youtube.com/watch?v=OwtFDfxz6pg" target="_blank">if there was the size of population that there was 500 years ago</a>‚Äù‚Äîthat is, <a href="https://ourworldindata.org/grapher/world-population-comparison-historical-sources" target="_blank">around 500 million people</a>, a reduction of over 90%. This is a static ideal of a ‚Äúsustainable‚Äù population.</p>

<p>Regular readers of this blog can cite many objections to this view. Resources are not static. Historically, as we run out of a resource (whale oil, elephant tusks, seabird guano), we <a href="https://rootsofprogress.org/unsustainable">transition to a new technology</a> based on a more abundant resource‚Äîand there are basically <a href="https://rootsofprogress.org/catastrophic-resource-shortages">no major examples of catastrophic resource shortages</a> in the industrial age. The carrying capacity of the planet is not fixed, but a function of technology; and side effects such as pollution or climate change are just more problems to be solved. As long as we can keep coming up with new ideas, <a href="https://rootsofprogress.org/can-growth-continue-ignite-talk">growth can continue</a>.</p>

<p>But those are only reasons why a larger population is not a <em>problem</em>. Is there a positive reason to <em>want</em> a larger population?</p>

<p>I‚Äôm going to argue yes‚Äîthat the ideal human population is not ‚Äúmuch smaller,‚Äù but ‚Äúever larger.‚Äù</p>

<h2 id="selfish-reasons-to-want-more-humans">Selfish reasons to want more humans</h2>

<p>Let me get one thing out of the way up front.</p>

<p>One argument for a larger population is based on utilitarianism, specifically the version of it that says that what is good is the <em>sum total</em> of happiness across all humans. If each additional life adds to the cosmic scoreboard of goodness, then it‚Äôs obviously better to have more people (unless they are so miserable that their lives are literally not worth living).</p>

<p>I‚Äôm <em>not</em> going to argue from this premise, in part because I don‚Äôt need to and more importantly because I don‚Äôt buy it myself. (Among other things, it leads to <a href="https://plato.stanford.edu/entries/repugnant-conclusion/" target="_blank">paradoxes</a> such as the idea that a population of thriving, extremely happy people is not as good as a sufficiently-larger population of people who are just barely happy.)</p>

<p>Instead, I‚Äôm going to argue that a larger population is better <em>for every individual</em>‚Äîthat there are <em>selfish</em> reasons to want more humans.</p>

<p>First I‚Äôll give some examples of how this is true, and then I‚Äôll draw out some of the deeper reasons for it.</p>

<h2 id="more-geniuses">More geniuses</h2>

<p>First, more people means more outliers‚Äîmore super-intelligent, super-creative, or super-talented people, to produce great art, architecture, music, philosophy, science, and inventions.</p>

<p>If genius is defined as one-in-a-million level intelligence, then every billion people means another thousand geniuses‚Äîto work on all of the problems and opportunities of humanity, to the benefit of all.</p>

<h2 id="more-progress">More progress</h2>

<p>A larger population means faster scientific, technical, and economic progress, for several reasons:</p>

<ul>
  <li>
    <p><strong>Total investment.</strong> More people means more total R&amp;D: more researchers, and more surplus wealth to invest in it.</p>
  </li>
  <li>
    <p><strong>Specialization.</strong> In the economy generally, the division of labor increases productivity, as each worker can specialize and become expert at their craft (‚ÄúSmithian growth‚Äù). In R&amp;D, each researcher can specialize in their field.</p>
  </li>
  <li>
    <p><strong>Larger markets</strong> support more R&amp;D investment, which lets companies pick off higher-hanging fruit. I‚Äôve given the example of <a href="https://rootsofprogress.org/why-did-we-wait-so-long-for-the-threshing-machine">the threshing machine</a>: it was difficult enough to manufacture that it didn‚Äôt pay for a local artisan to make them only for their town, but it was profitable to serve a regional market. Alex Tabarrok gives the example of <a href="https://marginalrevolution.com/marginalrevolution/2023/06/the-growing-market-for-cancer-drugs.html" target="_blank">the market for cancer drugs</a> expanding as large countries such as India and China become wealthier. Very high production-value entertainment, such as movies, TV, and games, are possible only because they have mass audiences.</p>
  </li>
  <li>
    <p><strong>More ambitious projects</strong> need a certain critical mass of resources behind them. Ancient Egyptian civilization built a large irrigation system to make the best use of the Nile floodwaters for agriculture, a feat that would not have been possible to a small tribe or chiefdom. The Apollo Program, at its peak in the 1960s, took <a href="https://en.wikipedia.org/wiki/Budget_of_NASA" target="_blank">over 4% of the US federal budget</a>, but 4% would not have been enough if the population and the economy were half the size. If someday humanity takes on a grand project such as a space elevator or a Dyson sphere, it will require an enormous team and an enormous wealth surplus to fund them.</p>
  </li>
</ul>

<p>In fact, these factors may represent not only opportunities but <em>requirements</em> for progress. There is evidence that <a href="https://www.newthingsunderthesun.com/pub/bvmu4ol2/release/10" target="_blank">simply to maintain a constant rate of exponential economic growth requires exponentially growing investment in R&amp;D</a>. This investment is partly financial capital, but also partly human capital‚Äîthat is, we need an exponentially growing base of researchers.</p>

<p>One way to understand this is that if each researcher can push forward a constant ‚Äúsurface area‚Äù of the frontier, then as the frontier expands, a larger number of researchers is needed to keep pushing all of it forward. Two hundred years ago, a small number of scientists were enough to investigate electrical and magnetic phenomena; today, millions of scientists and engineers are productively employed working out all of the details and implications of those phenomena, both in the lab and in the electrical, electronics, and computer hardware and software industries.</p>

<p>But it‚Äôs not even clear that each researcher <em>can</em> push forward a constant surface area of the frontier. As that frontier moves further out, the ‚Äú<a href="https://www.newthingsunderthesun.com/pub/zsc23qxz/release/17" target="_blank">burden of knowledge</a>‚Äù grows: each researcher now has to study and learn more in order to even <em>get</em> to the frontier. Doing so might force them to specialize even further. Newton could make major contributions to fields as diverse as gravitation and optics, because the very basics of those fields were still being figured out; today, a researcher might devote their whole career to a sub-sub-discipline such as nuclear astrophysics.</p>

<p>But in the long run, an exponentially growing base of researchers is impossible without an exponentially growing population. In fact, in <a href="https://www-leland.stanford.edu/~chadj/JonesJPE95.pdf" target="_blank">some models of economic growth</a>, the long-run growth rate in per-capita GDP is <em>directly proportional</em> to the growth rate of the population.</p>

<h2 id="more-options">More options</h2>

<p>Even setting aside growth and progress‚Äîlooking at a static snapshot of a society‚Äîa world with more people is a world with more choices, among greater variety:</p>

<ul>
  <li>
    <p><strong>Better matching for aesthetics, style, and taste.</strong> A bigger society has more cuisines, more architectural styles, more types of fashion, more sub-genres of entertainment. This also improves as the world gets more connected: for instance, the wide variety of ethnic restaurants in every major city is a recent phenomenon; it was only decades ago that <a href="https://twitter.com/paulisci/status/1551649152479555584" target="_blank">pizza, to Americans, was an unfamiliar foreign cuisine</a>.</p>
  </li>
  <li>
    <p><strong>Better matching to careers.</strong> A bigger economy has more options for what to do with your life. In a hunter-gatherer society, you are lucky if you get to decide whether to be a hunter or a gatherer. In an agricultural economy, you‚Äôre probably going to be a farmer, or maybe some sort of artisan. Today there‚Äôs a much wider set of choices, from pilot to spreadsheet jockey to lab technician.</p>
  </li>
  <li>
    <p><strong>Better matching to other people.</strong> A bigger world gives you a greater chance to find the perfect partner for you: the best co-founder for your business, the best lyricist for your songs, the best partner in marriage.</p>
  </li>
  <li>
    <p><strong>More niche communities.</strong> Whatever your quirky interest, worldview, or aesthetic‚Äîthe more people you can be in touch with, the more likely you are to find others like you. Even if you‚Äôre one in a million, in a city of ten million people, there are enough of you for a small club. In a world of eight billion, there are enough of you for a thriving subreddit.</p>
  </li>
  <li>
    <p><strong>More niche markets.</strong> Similarly, in a larger, more connected economy, there are more people to economically support your quirky interests. Your favorite Etsy or Patreon creator can find the ‚Äú<a href="https://kk.org/thetechnium/1000-true-fans/" target="_blank">one thousand true fans</a>‚Äù they need to make a living.</p>
  </li>
</ul>

<h2 id="deeper-patterns">Deeper patterns</h2>

<p>When I look at the above, here are some of the underlying reasons:</p>

<ul>
  <li>
    <p><strong>The existence of <a href="https://en.wikipedia.org/wiki/Rivalry_(economics)" target="_blank">non-rival</a> goods.</strong> Rival goods need to be divided up; more people just create more competition for them. But non-rival goods can be shared by all. A larger population and economy, all else being equal, will produce more non-rival goods, which benefits everyone.</p>
  </li>
  <li>
    <p><strong>Economies of scale.</strong> In particular, often total costs are a combination of fixed and variable costs. The more output, the more the fixed costs can be amortized, lowering average cost.</p>
  </li>
  <li>
    <p><strong>Network effects and Metcalfe‚Äôs law.</strong> Value in a network is generated not by nodes but by connections, and the more nodes there are total, the more connections are possible <em>per node</em>. Metcalfe‚Äôs law quantifies this: the number of possible connections in a network is proportional to the <em>square</em> of the number of nodes.</p>
  </li>
</ul>

<p>All of these create <a href="https://en.wikipedia.org/wiki/Economies_of_agglomeration" target="_blank">agglomeration effects</a>: bigger societies are better for everyone.</p>

<h2 id="a-dynamic-world">A dynamic world</h2>

<p>I assume that when Ehrlich and Goodall advocate for much smaller populations, they aren‚Äôt literally calling for genocide or hoping for a global catastrophe (although Ehrlich is happy with <a href="https://archive.org/details/populationbomb00ehrl/page/n11/mode/2up?q=by+compulsion+if+voluntary+methods+fail" target="_blank">coercive fertility control programs</a>, and other anti-humanists have expressed hope for ‚Äú<a href="https://www.latimes.com/archives/la-xpm-1989-10-22-bk-726-story.html" target="_blank">the right virus to come along</a>‚Äù).</p>

<p>Even so, the world they advocate is a <em>greatly impoverished and stagnant</em> one: a world with fewer discoveries, fewer inventions, fewer works of creative genius, fewer cures for fewer diseases, fewer choices, fewer soulmates.</p>

<p>A world with a large and growing population is a dynamic world that can create and sustain progress.</p>

<figure>
  <a href="https://rootsofprogress.org/img/dalle-vibrant-city-scene.jpg" target="_blank">
    <img src="https://rootsofprogress.org/img/dalle-vibrant-city-scene.jpg" alt="" loading="lazy">
  </a>
  <figcaption>
    
    
  </figcaption>
</figure>

<hr>

<p><em>For a different angle on the same thesis, see ‚Äú<a href="https://maartenboudry.be/2023/11/forget-about-overpopulation-soon-there-will-be-too-few-humans.html" target="_blank">Forget About Overpopulation, Soon There Will Be Too Few Humans</a>,‚Äù by Roots of Progress fellow Maarten Boudry.</em></p>


      

      

      <p>
        
        
        
        
        
        
          Comment: <a href="https://progressforum.org/posts/HRpvRZEwihocpG6gn" target="_blank">Progress Forum</a>, <a href="https://www.lesswrong.com/posts/LDRAj5zEYGKbhcsF3" target="_blank">LessWrong</a>, <a href="https://www.reddit.com/r/rootsofprogress/comments/1ay9sif" target="_blank">Reddit</a>
        
      </p>

      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'm Coder (113 pts)]]></title>
            <link>https://www.fellipe.com/apps/im-coder/</link>
            <guid>39497541</guid>
            <pubDate>Sun, 25 Feb 2024 04:10:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fellipe.com/apps/im-coder/">https://www.fellipe.com/apps/im-coder/</a>, See on <a href="https://news.ycombinator.com/item?id=39497541">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <pre id="im-coder-editor"></pre>
    <section id="im-coder-sidebar">
      <header>
        
        <p>Create fake codes for classes, movies, tv shows...
          <br>Or be a troll using for live coding in talks ;) </p>
      </header>
      <p><span>Number of Characters (Max 100)</span>
          
      </p>
      <ul>
        <li data-syntax-lang="c">C</li>
        <li data-syntax-lang="coffeescript">CoffeeScript</li>
        <li data-syntax-lang="css">CSS</li>
        <li data-syntax-lang="csharp">C#</li>
        <li data-syntax-lang="erlang">Erlang</li>
        <li data-syntax-lang="go">Go</li>
        <li data-syntax-lang="html">HTML</li>
        <li data-syntax-lang="java">Java</li>
        <li data-syntax-lang="js">JavaScript</li>
        <li data-syntax-lang="julia">Julia</li>
        <li data-syntax-lang="objc">Objective-C</li>
        <li data-syntax-lang="perl">Perl</li>
        <li data-syntax-lang="php">PHP</li>
        <li data-syntax-lang="python">Python</li>
        <li data-syntax-lang="ruby">Ruby</li>
        <li data-syntax-lang="swift">Swift</li>
        <li>
          
        </li>
      </ul>
      
    </section>
    
    
    
    
    
    
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon blocks long-running FireTV capability, Breaking apps with no warning (189 pts)]]></title>
            <link>https://www.aftvnews.com/amazon-blocks-long-running-fire-tv-capability-breaking-popular-apps-with-no-warning-and-giving-developers-the-runaround/</link>
            <guid>39496861</guid>
            <pubDate>Sun, 25 Feb 2024 01:49:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.aftvnews.com/amazon-blocks-long-running-fire-tv-capability-breaking-popular-apps-with-no-warning-and-giving-developers-the-runaround/">https://www.aftvnews.com/amazon-blocks-long-running-fire-tv-capability-breaking-popular-apps-with-no-warning-and-giving-developers-the-runaround/</a>, See on <a href="https://news.ycombinator.com/item?id=39496861">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img fetchpriority="high" decoding="async" src="https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2014/12/ADB-Debugging-Menu-On-Header.jpg?resize=646%2C363&amp;quality=100&amp;ssl=1" alt="" width="646" height="363" srcset="https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2014/12/ADB-Debugging-Menu-On-Header.jpg?w=800&amp;quality=100&amp;ssl=1 800w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2014/12/ADB-Debugging-Menu-On-Header.jpg?resize=300%2C169&amp;quality=100&amp;ssl=1 300w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2014/12/ADB-Debugging-Menu-On-Header.jpg?resize=150%2C84&amp;quality=100&amp;ssl=1 150w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2014/12/ADB-Debugging-Menu-On-Header.jpg?resize=768%2C432&amp;quality=100&amp;ssl=1 768w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2014/12/ADB-Debugging-Menu-On-Header.jpg?resize=100%2C56&amp;quality=100&amp;ssl=1 100w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2014/12/ADB-Debugging-Menu-On-Header.jpg?resize=200%2C113&amp;quality=100&amp;ssl=1 200w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2014/12/ADB-Debugging-Menu-On-Header.jpg?resize=450%2C253&amp;quality=100&amp;ssl=1 450w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2014/12/ADB-Debugging-Menu-On-Header.jpg?resize=600%2C338&amp;quality=100&amp;ssl=1 600w" sizes="(max-width: 646px) 100vw, 646px" data-recalc-dims="1"></p><p>Amazon‚Äôs recent streak of unpopular Fire TV changes continues, and its latest change is a doozy. The most recent Fire TV software update has blocked a Fire TV capability that has been present since the original model‚Äôs release in 2014. This is a basic Android capability that, to my knowledge, no other Android-based device manufacturer has ever had issues with, let alone blocked. This change has rendered popular Fire TV apps, which have been in Amazon‚Äôs own Appstore for years, useless. Worse yet, Amazon seems to have been careless in implementing this change without even a courtesy email to the affected app developers, all under the, seemingly false, guise of enhanced security. <span id="more-42518"></span></p><p><a href="https://www.aftvnews.com/amazon-blocks-long-running-fire-tv-capability-breaking-popular-apps-with-no-warning-and-giving-developers-the-runaround/amazons-change-to-fire-tv-adb-that-blocks-local-connections/" rel="attachment wp-att-42520"><img decoding="async" src="https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2024/02/Amazons-change-to-Fire-TV-ADB-that-blocks-local-connections.png?resize=646%2C363&amp;quality=100&amp;ssl=1" alt="" width="646" height="363" srcset="https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2024/02/Amazons-change-to-Fire-TV-ADB-that-blocks-local-connections.png?w=800&amp;quality=100&amp;ssl=1 800w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2024/02/Amazons-change-to-Fire-TV-ADB-that-blocks-local-connections.png?resize=300%2C169&amp;quality=100&amp;ssl=1 300w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2024/02/Amazons-change-to-Fire-TV-ADB-that-blocks-local-connections.png?resize=150%2C84&amp;quality=100&amp;ssl=1 150w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2024/02/Amazons-change-to-Fire-TV-ADB-that-blocks-local-connections.png?resize=768%2C432&amp;quality=100&amp;ssl=1 768w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2024/02/Amazons-change-to-Fire-TV-ADB-that-blocks-local-connections.png?resize=100%2C56&amp;quality=100&amp;ssl=1 100w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2024/02/Amazons-change-to-Fire-TV-ADB-that-blocks-local-connections.png?resize=200%2C113&amp;quality=100&amp;ssl=1 200w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2024/02/Amazons-change-to-Fire-TV-ADB-that-blocks-local-connections.png?resize=450%2C253&amp;quality=100&amp;ssl=1 450w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2024/02/Amazons-change-to-Fire-TV-ADB-that-blocks-local-connections.png?resize=600%2C338&amp;quality=100&amp;ssl=1 600w" sizes="(max-width: 646px) 100vw, 646px" data-recalc-dims="1"></a>Code excerpt of Amazon‚Äôs change to Fire TV‚Äôs ADB connection that denies local connections. Sent to me by an affected app developer.</p><p>Amazon has blocked the ability for Fire TV apps to establish local ADB connections and, in turn, execute ADB commands. While it‚Äôs not a capability used by many Fire TV apps, without it, Fire TV apps can no longer perform certain advanced tasks, such as freeing up internal storage space by clearing the cache of all installed apps. This change has been verified to be present in Fire TV update 7.6.6.9 for Fire OS 7 devices, like the <a href="https://www.amazon.com/dp/B08C1W5N87/?tag=aftvn-20" rel="noopener" target="_blank">Fire TV Stick</a> and <a href="https://www.amazon.com/dp/B09BZZ3MM7/?tag=aftvn-20" rel="noopener" target="_blank">Fire TV Cube</a>, and update 8.1.0.3 for Fire OS 8 devices, like the <a href="https://www.amazon.com/dp/B0BP9MDCQZ/?tag=aftvn-20" rel="noopener" target="_blank">Fire TV Stick 4K</a> and <a href="https://www.amazon.com/dp/B0BP9SNVH9/?tag=aftvn-20" target="_blank" rel="noopener">Fire TV Stick 4K Max</a>. It is unknown if older Fire TV models running Fire OS 6 or Fire OS 5 will also be receiving this change, but it seems likely. This update does not change the ability of external devices, like computers or phones, to establish an ADB connection with a Fire TV, which remains possible.</p><p>When I asked Amazon if this change was intentional and performing as intended because multiple readers and developers were asking me about it, Amazon‚Äôs only reply to me was ‚ÄúWe are aware of reports that some apps have been impacted by a recent security update.‚Äù Since then, the <a href="https://github.com/cgutman" rel="noopener" target="_blank">developer</a> of the immensely popular app <a href="https://play.google.com/store/apps/details?id=com.cgutman.androidremotedebugger" rel="noopener" target="_blank">Remote ADB Shell</a>, which has over half a million downloads and has been heavily crippled by Amazon‚Äôs update, has reached out to me with evidence that the change by Amazon is certainly deliberate.</p><p>While Amazon is stating this change is in the name of improved security, I don‚Äôt buy it. While ADB commands can be very powerful and, therefore, should only be allowed to run with care, all Android-based devices, including Fire TVs, have several precautions in place to keep users safe from apps or devices trying to execute nefarious ADB commands.</p><p><a href="https://www.aftvnews.com/how-to-grant-allow-all-the-time-full-file-storage-access-permission-for-any-app-in-fire-os-8-on-the-2nd-gen-fire-tv-stick-4k-4k-max/allow-usb-debugging-prompt-on-fire-tv/" rel="attachment wp-att-41842"><img decoding="async" src="https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2023/10/Allow-USB-Debugging-prompt-on-Fire-TV.jpg?resize=646%2C363&amp;quality=100&amp;ssl=1" alt="" width="646" height="363" srcset="https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2023/10/Allow-USB-Debugging-prompt-on-Fire-TV.jpg?resize=800%2C450&amp;quality=100&amp;ssl=1 800w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2023/10/Allow-USB-Debugging-prompt-on-Fire-TV.jpg?resize=300%2C169&amp;quality=100&amp;ssl=1 300w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2023/10/Allow-USB-Debugging-prompt-on-Fire-TV.jpg?resize=150%2C84&amp;quality=100&amp;ssl=1 150w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2023/10/Allow-USB-Debugging-prompt-on-Fire-TV.jpg?resize=768%2C432&amp;quality=100&amp;ssl=1 768w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2023/10/Allow-USB-Debugging-prompt-on-Fire-TV.jpg?resize=1536%2C864&amp;quality=100&amp;ssl=1 1536w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2023/10/Allow-USB-Debugging-prompt-on-Fire-TV.jpg?resize=100%2C56&amp;quality=100&amp;ssl=1 100w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2023/10/Allow-USB-Debugging-prompt-on-Fire-TV.jpg?resize=200%2C113&amp;quality=100&amp;ssl=1 200w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2023/10/Allow-USB-Debugging-prompt-on-Fire-TV.jpg?resize=450%2C253&amp;quality=100&amp;ssl=1 450w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2023/10/Allow-USB-Debugging-prompt-on-Fire-TV.jpg?resize=600%2C338&amp;quality=100&amp;ssl=1 600w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2023/10/Allow-USB-Debugging-prompt-on-Fire-TV.jpg?resize=900%2C506&amp;quality=100&amp;ssl=1 900w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2023/10/Allow-USB-Debugging-prompt-on-Fire-TV.jpg?w=1920&amp;quality=100&amp;ssl=1 1920w, https://i0.wp.com/www.aftvnews.com/wp-content/uploads/2023/10/Allow-USB-Debugging-prompt-on-Fire-TV.jpg?w=1292&amp;quality=100&amp;ssl=1 1292w" sizes="(max-width: 646px) 100vw, 646px" data-recalc-dims="1"></a>ADB connection request on Fire TVs</p><p>Before any ADB command can be executed on a Fire TV, an ADB connection to the device must be made. This starts by selecting the Fire TV‚Äôs model name in its ‚ÄúAbout‚Äù menu seven times to <a href="https://www.aftvnews.com/how-to-find-show-unhide-reveal-developer-options-on-an-amazon-fire-tv-stick-fire-tv-cube-or-fire-tv-smart-tv/">reveal a hidden developer menu</a>. Then, an ‚ÄúADB debugging‚Äù option must be enabled from said hidden menu. Finally, every unique ADB connection request from a device or app, be it a local or external connection, results in a full-screen prompt that must be allowed before the ADB connection is made.</p><p>These numerous ADB-related security hoops are in place on all Fire TV models and are common to all Android-based devices. No manufacturer apart from Amazon has felt the need to enhance device security by blocking local ADB connections, despite most non-Amazon Android devices being phones, which hold far more private and critical user data than a Fire TV streaming media player.</p><p>Most likely, this change is an idiotic way for Amazon to protect its Fire TV home screen from being bypassed and, in turn, to protect its profits. Apps commonly used by the Fire TV modding community will often use local ADB connections to detect remote button presses. That detection allows the use of alternate home screens, which aren‚Äôt inundated with things like <a href="https://www.aftvnews.com/fire-tvs-now-autoplay-full-screen-video-ads-when-waking-up-and-what-you-can-do-about-it/">auto-playing fullscreen video ads</a> like Amazon‚Äôs own home screen.</p><p>While it‚Äôs an unpopular opinion, I see nothing wrong with Amazon protecting its Fire TV revenue by stopping the use of alternative home screens. It‚Äôs crucial to the business model Amazon has chosen to use for the Fire TV and if customers don‚Äôt like it, they don‚Äôt have to buy one. However, blocking a core OS capability and breaking popular apps in a futile effort to protect the Fire TV home screen is shortsighted and foolish. It‚Äôs the equivalent of a town mayor demolishing a bridge used by everyone because their political opponent lives on the other side.</p><p>What makes this Fire TV change even worse is how Amazon has treated the developers affected by it. Two popular Fire TV apps affected by this change are <a href="https://www.amazon.com/dp/B0BYLK899N/?tag=aftvn-20" rel="noopener" target="_blank">TDUK APP Killer</a> and <a href="https://www.amazon.com/dp/B0B2L67V4R/?tag=aftvn-20" rel="noopener" target="_blank">TDUK APP Cache Cleaner</a>, which use local ADB commands to force quit and clear the cache of all apps with a single button press. I‚Äôve been going back and forth with the app‚Äôs developer, popular Fire TV YouTuber <a href="http://youtube.com/@TechDoctorUK" rel="noopener" target="_blank">TechDoctorUK</a>, all week trying to get to the bottom of why his apps were suddenly and unexplainably marked as incompatible with all Fire TV models, despite not receiving any notice from Amazon and his apps appearing ‚ÄúLive‚Äù with ‚ÄúNo issues found‚Äù in his Amazon developer portal.</p><p>Emails shown to me from Amazon stated that TechDoctorUK‚Äôs apps were removed for failing tests that resulted in error messages being displayed by the apps. However, the Amazon testing that resulted in those errors was done on non-Amazon devices (i.e., Android phones), despite the apps only being listed by TechDoctorUK as compatible with Fire TV devices. After being given the runaround for a couple of days, only after I reached out to Amazon about this issue did TechDoctorUK receive an email that stated: ‚ÄúBecause your app overrides the native user experience (e.g., with a lockscreen, or widget), it has not been published on Amazon devices.‚Äù Given that force-stopping apps and clearing app cache are both native capabilities of Fire TVs, just not with a single click, I interpret the email as Amazon‚Äôs canned way of saying we don‚Äôt want your app on Amazon devices.</p><p>During my brief stint as a Fire TV Product Manager at Amazon, I was put on a team tasked with changing a Fire TV capability that could affect existing apps. We created and executed a plan that involved contacting affected app developers ahead of the change, helping them update their app if needed, and addressing customer issues that might arise, among other things. What we certainly didn‚Äôt do is carelessly push out the change, ghost ban affected apps, give developers the runaround, and reply to concerns with irrelevant canned replies.</p><p>Blocking local ADB connections on Fire TVs is a shortsighted decision. If the goal was to further protect the Fire TV home screen, it should have been achieved in any number of other ways that didn‚Äôt require breaking legitimate apps.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Every model learned by gradient descent is approximately a kernel machine (2020) (170 pts)]]></title>
            <link>https://arxiv.org/abs/2012.00152</link>
            <guid>39496747</guid>
            <pubDate>Sun, 25 Feb 2024 01:25:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2012.00152">https://arxiv.org/abs/2012.00152</a>, See on <a href="https://news.ycombinator.com/item?id=39496747">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="labstabs"><p>
    <label for="tabone">Bibliographic Tools</label></p><div>
      <h2>Bibliographic and Citation Tools</h2>
      <div>
          <p><label>
              
              <span></span>
              <span>Bibliographic Explorer Toggle</span>
            </label>
          </p>
          
        </div>
        
        
        
    </div>


    <p>
    <label for="tabtwo">Code, Data, Media</label></p><div>
      <h2>Code, Data and Media Associated with this Article</h2>
      

      
      
      
      
      
      
    </div>


      <p>
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label></p><div>
        <h2>Demos</h2>
        
        
        
        
      </div>
      <p>
      <label for="tabfour">Related Papers</label></p><div>
        <h2>Recommenders and Search Tools</h2>
        <div>
            <p><label>
                
                <span></span>
                <span>IArxiv recommender toggle</span>
              </label>
            </p>
            
          </div>
        
        
        
        
        
      </div>

      <p>
      <label for="tabfive">
        About arXivLabs
      </label></p><div>
            <h2>arXivLabs: experimental projects with community collaborators</h2>
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv's community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Miracle of W√∂rgl (106 pts)]]></title>
            <link>https://www.alexstonethinkingstrings.com/446414233</link>
            <guid>39496183</guid>
            <pubDate>Sat, 24 Feb 2024 23:46:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.alexstonethinkingstrings.com/446414233">https://www.alexstonethinkingstrings.com/446414233</a>, See on <a href="https://news.ycombinator.com/item?id=39496183">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>In these days of miracle and wonder (not), when our collective orbit has been reduced to the home office, larger questions abound.</p>
<p>Like, what is the future of money? Can it still work? Or, can it work in a different way? Who gets to give you money, when you‚Äôre sitting in lockdown? Are the rules of capitalism set in stone? What if money had a use-by date? What if keeping money had no advantages?</p>
<p>Allow me to introduce the W√∂rgl Miracle ‚Äì or Experiment, depending on your point of view.</p>
<p>W√∂rgl (groovy name) was a wee town in Austria that tackled the Great Depression in a novel way. It broke all the rules of standard capitalist economic theory. W√∂rgl printed money. They gave it to the townsfolk, for free.</p>
<p><em>Whaat?</em> You may say, but it worked. Truly.</p>
<p>Here‚Äôs how it happened. Like small towns everywhere in the Great Depression, W√∂rgl had people sitting around, out of work. And shops with no customers. The usual story.</p>
<p>W√∂rgl had a population of 4,216. Being a railway junction, the railway employed 310 people in 1930, but by 1933 the number had plummeted to 190, following the transition to electric loco‚Äôs. The cement plant in nearby Kitzb√ºhel employed 50 workers in 1930, but by 1933 only 2. The Zipf brewery sacked between 10‚Äì14 workers from the previous 33‚Äì37. A cellulose factory had once employed 400 workers. In 1933, just four men were there, idly guarding idle machines. Farmers, a third of the work force, could hardly sell their products at depressed prices.</p>
<p>But the town‚Äôs mayor, Michael Unterguggenberger (off-the scale groovy name!) suggested something radical. Let‚Äôs print our own money.</p>
<p>The experiment began on the 31 July 1932, with the issuing of ‚ÄòCertified Compensation Bills‚Äô, W√∂rgl‚Äôs own currency. It was cheerfully named Freigeld ‚Äì or as Stamp Scrip, if you will.</p>
<p>Result: a boom in local government projects, and a corresponding increase in employment and economic activity not just in the government sector, but throughout the town.</p>
<p>People had to spend their free money. And spend it they did, on contractors to do building, on food and furniture and stuff in the local shops, to pay farm workers to plough the land and bring in the crops. People paid their taxes.</p>
<p>W√∂rgl money worked differently. It had a use-by date. So it didn‚Äôt pay to accumulate it. You had to spend your allocation by the end of the year ‚Äì or it would decline in value by 10 per cent. In dry economic terminology, this is called currency demurrage.</p>
<p>‚ÄúWhatever,‚Äù said the people of W√∂rgl. And carried on spending up large.</p>
<p>Irony, bigly these days, is that W√∂rgl banknotes are now seriously collectible items ‚Äì worth a fortune, especially those with the devaluation stamps attached.</p>
<p>Anyway, W√∂rgl money had the effect of jump-starting the local economy. Things started humming. The money had to be used, you see. Where‚Äôs the problem in that?</p>
<p>Now, money as we know it, was invented to accumulate in value. For those of a religious bent, that is reflected in usury ‚Äì the practice of lending money at unreasonably high rates of interest, so the money you give out comes back as more. That‚Äôs what caused Jesus to lose his cool at the Temple on the Mount. And in the next Abrahamic religion that came along, Islam, the charging of interest came to be seen as sinful.</p>
<p>People further than W√∂rgl‚Äôs town limits sat up and took notice; among them such luminaries as French Premier Edouard Daladier and the economist Irving Fisher. The ‚Äúmiracle‚Äù gained notoriety, and other towns wanted to copy the experiment hoping for similar success. Nearby villages even arranged to accept each other‚Äôs scrip.</p>
<p>In June 1933 Mayor Unterguggenberger was the star at a presentation in Vienna for 170 Mayors‚Äîthey were intrigued on hearing reports from W√∂rgl. Among the attendees, some opined that it would be a go to introduce "magic money" in their communities too.</p>
<p>But the rich and powerful didn‚Äôt like it. W√∂rgl did not fit their script. It was supposed to be <em>their</em> schtick to hold great sums of money (and therefore power), and for this money to <em>accumulate</em>. The poor folk, like those in W√∂rgl, were supposed to <em>borrow</em> money at the rich folks at exorbitant interest rates.</p>
<p>So, under pressure from the wealthy elite, the Austrian government capitulated. The W√∂rgl miracle was murdered by the Austrian National Bank on 1 September, 1933.</p>
<p>Looking back on the whole of W√∂rgl thing, contemporary economists say the Miracle couldn‚Äôt last. They use dry and dreary and self-serving arguments like, ‚Äúany threat to centralized control is a threat to the money power‚Äù(Yeha! Right on!); or ‚ÄúW√∂rgl could not legislate or enforce monopoly legal tender, so the demand for the scrip is partially attributable to the need to pay taxes‚Äù (so what?); or ‚ÄúIn a matter of days the national scrip held as backing in the bank would have been exhausted‚Äù (but who needs banks anyway, when you get free money?) or ‚ÄúW√∂rgl was not a miracle, but an example of Keynesian spending given incentive by Gresham‚Äôs Law‚Äù (blah, blah‚Ä¶).</p>
<p>But then you could say, the ‚Äòexperiment‚Äô of capitalism as we know it ‚Äì endless growth and resource extraction, the rich getting richer and the poor getting poorer ‚Äì can‚Äôt last anyway.</p>
<p>And now we have a new window on this capitalism deceit. How come, we say, the economies of the world collapse when people buy only what they need?</p>
<p>Come back W√∂rgl. We need you now.</p>    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GenAI and erroneous medical references (168 pts)]]></title>
            <link>https://hai.stanford.edu/news/generating-medical-errors-genai-and-erroneous-medical-references</link>
            <guid>39496096</guid>
            <pubDate>Sat, 24 Feb 2024 23:27:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hai.stanford.edu/news/generating-medical-errors-genai-and-erroneous-medical-references">https://hai.stanford.edu/news/generating-medical-errors-genai-and-erroneous-medical-references</a>, See on <a href="https://news.ycombinator.com/item?id=39496096">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p dir="ltr"><span>Large language models (LLMs) are infiltrating the medical field.&nbsp;</span><a href="https://www.medicaleconomics.com/view/ai-special-report-what-patients-and-doctors-really-think-about-ai-in-health-care"><span>One in 10 doctors</span></a><span> already use ChatGPT in day-to-day work, and patients have taken to ChatGPT to diagnose themselves.&nbsp;</span><a href="https://www.today.com/health/mom-chatgpt-diagnosis-pain-rcna101843"><span>The Today Show</span></a><span> featured the story of a 4-year-old boy, Alex, whose chronic illness was diagnosed by ChatGPT after over a dozen doctors failed to do so.&nbsp;</span></p><p dir="ltr"><span>This rapid adoption to much fanfare is in spite of substantial uncertainties about the safety, effectiveness, and risk of generative AI (GenAI). U.S. Food and Drug Administration Commissioner Robert Califf has publicly stated that the agency is&nbsp;</span><a href="https://finance.yahoo.com/news/all-stakeholders-are-struggling-with-how-to-regulate-generative-ai-fda-commissioner-141754354.html"><span>"struggling" to regulate GenAI</span></a><span>.&nbsp;</span></p><p dir="ltr"><span>The reason is that GenAI sits in a gray area between two existing forms of technology. On one hand, sites like WebMD that strictly report known medical information from credible sources are&nbsp;</span><a href="https://www.fda.gov/media/109618/download"><span>not regulated by the FDA</span></a><span>. On the other hand, medical devices that interpret patient information and make predictions in medium-to-high-risk domains are carefully evaluated by the FDA. To date, the FDA has approved over 700 AI medical devices. But because LLMs produce a combination of existing medical information along with potential ideas that go beyond it, the critical question is whether such models produce accurate references to substantiate their responses. Such references enable doctors and patients to verify a GenAI assessment and guard against the highly prevalent rate of ‚Äúhallucinations.‚Äù&nbsp;</span></p><p dir="ltr"><span>For every 4-year-old Alex, where the creativity of an LLM may produce a diagnosis that physicians missed, there may be many more patients who are led astray by hallucinations. In other words, much of the future of GenAI in medicine ‚Äì and the regulation thereof ‚Äì hinges on the ability to substantiate claims.&nbsp;</span></p><h2>Evaluating References in LLMs<strong>&nbsp;</strong></h2><p dir="ltr"><span>Unfortunately, very little evidence exists about the ability of LLMs to substantiate claims. In a new&nbsp;</span><a href="https://arxiv.org/abs/2402.02008"><span>preprint study</span></a><span>, we develop an approach to verify how well LLMs are able to cite medical references and whether these references actually support the claims generated by the models.&nbsp;</span></p><p dir="ltr"><span>The short answer: poorly. For the most advanced model (GPT-4 with retrieval augmented generation), 30% of individual statements are unsupported and nearly half of its responses are not fully supported.&nbsp;</span></p><p><img src="https://hai.stanford.edu/sites/default/files/inline-images/LLM%20chart%20copy.jpg" data-entity-uuid="5580b7e0-246e-4a6b-a175-4643905c5de7" data-entity-type="file" width="1860" height="1296" loading="lazy"></p><p dir="ltr"><em><span>Evaluation of the quality of source verification in LLMs on medical queries. Each model is evaluated on three metrics over X questions. Source URL validity measures the proportion of generated URLs that return a valid webpage. Statement-level support measures the percentage of statements that are supported by at least one source in the same response. Response-level support measures the percentage of responses that have all their statements supported.</span></em></p><p dir="ltr"><span>How did we develop this evaluation approach? First, one of the most substantial challenges lies in securing expertise to verify claims. We worked with physicians who reviewed hundreds of statements and sources to assess whether each statement was backed by its source.&nbsp;</span></p><p dir="ltr"><span>Such expert reviews are, of course, costly and time-intensive, so we next decided to see whether LLMs can be used to&nbsp;</span><em>scale</em><span> such physician assessments. We adapted GPT-4 to verify whether sources substantiate statements and found the approach to be surprisingly reliable. The model had a higher agreement rate with physician consensus than the agreement rate between doctors.&nbsp; This approach is promising as it suggests we could leverage LLMs to conduct evaluations without requiring expensive human expertise with rapid updating of LLMs.</span></p><p dir="ltr"><span>Finally, using this model, we developed an end-to-end evaluation pipeline called&nbsp;</span><em>SourceCheckup</em><span>. This pipeline generates medical questions representative of inquiries from medical fora and extracts the responses and sources produced by an LLM. Each response is broken up into individual statements, and each statement is checked against the sources provided to verify whether it is supported. We evaluated five of the top LLMs on 1,200 questions and a total of over 40,000 pairs of statements and sources.</span></p><h2>Pervasive Errors in Substantiation</h2><p dir="ltr"><span>Our results are stark: Most models struggle to produce relevant sources. Four out of five models hallucinate a significant proportion of sources by producing invalid URLs. This problem goes away with the retrieval augmented generation (RAG) model, which first performs a web search for relevant sources before producing a summary of its findings. However, even in the GPT-4 RAG model, we find that up to 30% of statements made are not supported by any sources provided, with nearly half of responses containing at least one unsupported statement. This finding is more exaggerated in the other four models, with as few as 10% of responses fully supported in Gemini Pro, Google's recently released LLM.</span></p><p dir="ltr"><span>For example, one response by GPT-4 RAG indicated that criteria for gambling addictions (from the Diagnostic and Statistical Manual of Mental Disorders) are equally applicable across all individuals and groups. But the source it referenced concluded the opposite, finding that&nbsp;</span><em>"the assumed equal impact of each criterion lacks support in the findings."</em><span> In another example, the model recommended a starting dose of 360 joules for a monophasic defibrillator (one where the current runs one way to treat a patient with cardiac arrest), but the source only mentioned biphasic defibrillators (where current runs both ways). That failure to distinguish can matter greatly, as there‚Äôs been a&nbsp;</span><a href="https://www.mindray.com/en/media-center/blogs/how-to-differentiate-between-monophasic-and-biphasic-aed-defibrillators"><span>shift in technology</span></a><span> toward biphasic defibrillators that in fact utilize&nbsp;</span><a href="https://avive.life/blog/monophasic-vs-biphasic/"><span>lower electric currents</span></a><span>.&nbsp;</span></p><p dir="ltr"><span>In short, even the most advanced models fall seriously short of being able to substantiate answers. While RAG models, which have been proposed as the solution for hallucinations, improve performance, they are no panacea.&nbsp;</span></p><h2>Errors More Likely for Lay Inquiries&nbsp;</h2><p dir="ltr"><a href="https://pubmed.ncbi.nlm.nih.gov/38050503/"><span>Many</span></a><span>&nbsp;</span><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10582915/"><span>have</span></a><span>&nbsp;</span><a href="https://arxiv.org/abs/2312.00164"><span>argued</span></a><span>&nbsp;</span><a href="https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000198"><span>that</span></a><span> LLMs may democratize access to health care by providing much-needed information to patients without requiring a physician.&nbsp;</span></p><p dir="ltr"><span>Our evaluation framework allows us to assess whether errors vary by the type of inquiry. Our medical questions are based on three underlying reference texts: (1) the MayoClinic, which provides patient-facing fact pages, (2) UpToDate, which provides articles to physicians with a deeper level of medical detail, and (3) Reddit‚Äôs r/AskDocs forum, which includes many lay questions that may not have clearly defined answers and which require information from various medical domains.&nbsp;&nbsp;&nbsp;</span></p><p dir="ltr"><span>We found that the ability of LLMs to substantiate answers varies substantially by type of inquiry. Performance is best for MayoClinic and UpToDate and worst for Reddit. Only 30% of the answers to inquiries based on Reddit can be fully substantiated by sources with GPT4 RAG.&nbsp;&nbsp;</span></p><p dir="ltr"><span>In other words, our findings suggest that LLMs perform worst for exactly the kind of patients that might need this information the most. Where inquiries are mediated by medical professionals, LLMs have an easier time pointing to reliable sources. This has substantial implications for the distributive effects of this technology on health knowledge.&nbsp;</span></p><h2>‚ÄòA Long Way to Go‚Äô</h2><p dir="ltr"><span>Many commentators have declared the end of health care as we know it, given the apparent ability of LLMs to pass U.S. Medical Licensing Exams. But health care practice involves more than being able to answer a multiple choice test. It involves substantiating, explaining, and assessing claims with reliable, scientific sources. And on that score, GenAI still has a long way to go.&nbsp;</span></p><p dir="ltr"><span>Promising research directions include more domain-informed work, such as&nbsp;</span><a href="https://arxiv.org/pdf/2212.08073.pdf"><span>adapting RAG</span></a><span> specifically to medical applications. Source verification should be regularly evaluated to ensure that models provide credible and reliable information. At least by the current approach of the FDA ‚Äì which&nbsp;</span><a href="https://www.fda.gov/media/109618/download"><span>draws a distinction</span></a><span> between medical knowledge bases and diagnostic tools regulated as medical devices ‚Äì widely used LLMs pose a problem. Many of their responses cannot be consistently and fully supported by existing medical sources.&nbsp;</span></p><p dir="ltr"><span>As LLMs continue to grow in their capabilities and usage, regulators and doctors should carefully consider how these models are being evaluated, used, and integrated.</span></p><p dir="ltr"><em>Kevin Wu is a PhD student in Biomedical Informatics at Stanford University.</em></p><p dir="ltr"><em>Eric Wu is a PhD student in Electrical Engineering at Stanford University.</em></p><p dir="ltr"><em>Daniel E. Ho is the William Benjamin Scott and Luna M. Scott Professor of Law, Professor of Political Science, Professor of Computer Science (by courtesy), Senior Fellow at HAI, Senior Fellow at SIEPR, and Director of the RegLab at Stanford University.&nbsp;</em></p><p dir="ltr"><em>James Zou is an associate professor of Biomedical Data Science and, by courtesy, of Computer Science and Electrical Engineering at Stanford University. He is also a Chan-Zuckerberg Investigator.</em></p><p dir="ltr"><em><span>Stanford HAI‚Äôs mission is to advance AI research, education, policy and practice to improve the human condition.&nbsp;</span></em><a href="https://hai.stanford.edu/welcome"><em><span><strong>Learn more</strong></span></em></a><em><span>.&nbsp;</span></em></p></div></div>]]></description>
        </item>
    </channel>
</rss>