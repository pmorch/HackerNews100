(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 01 Aug 2024 17:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Stable Fast 3D: Rapid 3D Asset Generation from Single Images (143 pts)]]></title>
            <link>https://stability.ai/news/introducing-stable-fast-3d</link>
            <guid>41130042</guid>
            <pubDate>Thu, 01 Aug 2024 15:16:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stability.ai/news/introducing-stable-fast-3d">https://stability.ai/news/introducing-stable-fast-3d</a>, See on <a href="https://news.ycombinator.com/item?id=41130042">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1722509248724_3630">
  <p>We are excited to introduce Stable Fast 3D, Stability AI’s latest breakthrough in 3D asset generation technology. This innovative model transforms a single input image into a detailed 3D asset in just 0.5 seconds, setting a new standard for speed and quality in the field of 3D reconstruction.</p><p><strong>How It Works</strong></p><p>Users start by uploading a single image of an object. Stable Fast 3D then rapidly generates a complete 3D asset, including:</p><ul data-rte-list="default"><li><p>UV unwrapped mesh</p></li><li><p>Material parameters</p></li><li><p>Albedo colors with reduced illumination bake-in</p></li><li><p>Optional quad or triangle remeshing (adding only 100-200ms to processing time)</p></li></ul><p>Here is a <a href="https://www.youtube.com/watch?v=uT96UCBSBko" target="_blank"><span><span>video</span></span></a> describing how the model works and the improvements vs previous models.</p><p>Stable Fast 3D's unprecedented speed and quality make it an invaluable tool for rapid prototyping in 3D work, catering to both enterprises and indie developers in gaming and virtual reality, as well retail, architecture and design.</p><p>You can also use the model easily on<a href="https://platform.stability.ai/" target="_blank"> </a><a href="https://platform.stability.ai/docs/api-reference#tag/3D" target="_blank"><span><span>Stability AI API</span></span></a> and on<a href="https://stability.ai/stable-assistant"> <span><span>Stable Assistant</span></span></a> chatbot where you can share your 3D creations in a 3D viewer and play with them in AR (on Augmented Reality (WebXR) compatible devices).</p><p><strong>Use Cases</strong></p><p>Stable Fast 3D has several use cases in gaming and movie production.&nbsp;</p><ul data-rte-list="default"><li><p>Use the fast inference time during pre-production, where experimentation is key</p></li><li><p>Static assets for games (Background objects, clutter, furniture)</p></li><li><p>3D models for E-commerce</p></li><li><p>Fast model creation for AR/VR</p></li></ul><p><strong>Speed Meets Quality</strong></p><p>Stable Fast 3D outperforms competitors in several key areas:</p><ul data-rte-list="default"><li><p>Unmatched speed: 0.5 seconds per 3D asset generation on a GPU with 7GB VRAM or in close to a second on<a href="https://platform.stability.ai/" target="_blank"> </a><a href="https://platform.stability.ai/docs/api-reference#tag/3D" target="_blank"><span><span>Stability AI API</span></span></a></p></li><li><p>High-quality UV unwrapped mesh and material parameters</p></li><li><p>Reduced illumination entanglement in textures</p></li><li><p>Ability to generate additional material parameters and normal maps</p></li></ul><p>Compared to our previous SV3D model, Stable Fast 3D offers dramatically reduced inference times - 0.5 seconds versus 10 minutes - while maintaining high-quality output.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I recreated Shazam's algorithm with Go (190 pts)]]></title>
            <link>https://github.com/cgzirim/not-shazam</link>
            <guid>41127726</guid>
            <pubDate>Thu, 01 Aug 2024 10:29:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/cgzirim/not-shazam">https://github.com/cgzirim/not-shazam</a>, See on <a href="https://news.ycombinator.com/item?id=41127726">Hacker News</a></p>
<div id="readability-page-1" class="page"><p dir="auto">NotShazam is an implementation of Shazam's song recognition algorithm based on insights from these <a href="#resources--card_file_box">resources</a>. It integrates Spotify and YouTube APIs to find and download songs.</p><div data-snippet-clipboard-copy-content="git clone https://github.com/cgzirim/not-shazam.git"><pre><code>git clone https://github.com/cgzirim/not-shazam.git
</code></pre></div><div data-snippet-clipboard-copy-content="cd not-shazam
go get ./..."><pre><code>cd not-shazam
go get ./...
</code></pre></div><div data-snippet-clipboard-copy-content="cd not-shazam/client
npm install"><pre><code>cd not-shazam/client
npm install
</code></pre></div><div data-snippet-clipboard-copy-content="go run main.go serve [-proto <http|https>] [-port <port number>]"><pre><code>go run main.go serve [-proto &lt;http|https&gt;] [-port &lt;port number&gt;]
</code></pre></div><p dir="auto">Download a Song<br>
Note: A link from Spotify's mobile app won't work. You can copy the link from either the desktop or web app.</p><div data-snippet-clipboard-copy-content="go run main.go download <https://open.spotify.com/.../...>"><pre><code>go run main.go download &lt;https://open.spotify.com/.../...&gt;
</code></pre></div><div data-snippet-clipboard-copy-content="go run main.go find <path-to-wav-file>"><pre><code>go run main.go find &lt;path-to-wav-file&gt;
</code></pre></div><div data-snippet-clipboard-copy-content="$ go run main.go download https://open.spotify.com/track/4pqwGuGu34g8KtfN8LDGZm?si=b3180b3d61084018
Getting track info...
Now, downloading track...
Fingerprints saved in MongoDB successfully
'Voilà' by 'André Rieu' was downloaded
Total tracks downloaded: 1"><pre><code>$ go run main.go download https://open.spotify.com/track/4pqwGuGu34g8KtfN8LDGZm?si=b3180b3d61084018
Getting track info...
Now, downloading track...
Fingerprints saved in MongoDB successfully
'Voilà' by 'André Rieu' was downloaded
Total tracks downloaded: 1
</code></pre></div><div data-snippet-clipboard-copy-content="$ go run main.go find songs/Voilà\ -\ André\ Rieu.wav
Top 20 matches:
        - Voilà by André Rieu, score: 5390686.00
        - I Am a Child of God by One Voice Children's Choir, score: 2539.00
        - I Have A Dream by ABBA, score: 2428.00
        - SOS by ABBA, score: 2327.00
        - Sweet Dreams (Are Made of This) - Remastered by Eurythmics, score: 2213.00
        - The Winner Takes It All by ABBA, score: 2094.00
        - Sleigh Ride by One Voice Children's Choir, score: 2091.00
        - Believe by Cher, score: 2089.00
        - Knowing Me, Knowing You by ABBA, score: 1958.00
        - Gimme! Gimme! Gimme! (A Man After Midnight) by ABBA, score: 1941.00
        - Take A Chance On Me by ABBA, score: 1932.00
        - Don't Stop Me Now - Remastered 2011 by Queen, score: 1892.00
        - I Do, I Do, I Do, I Do, I Do by ABBA, score: 1853.00
        - Everywhere - 2017 Remaster by Fleetwood Mac, score: 1779.00
        - You Will Be Found by One Voice Children's Choir, score: 1664.00
        - J'Imagine by One Voice Children's Choir, score: 1658.00
        - When You Believe by One Voice Children's Choir, score: 1629.00
        - When Love Was Born by One Voice Children's Choir, score: 1484.00
        - Don't Stop Believin' (2022 Remaster) by Journey, score: 1465.00
        - Lay All Your Love On Me by ABBA, score: 1436.00

Search took: 856.386557ms

Final prediction: Voilà by André Rieu , score: 5390686.00"><pre><code>$ go run main.go find songs/Voilà\ -\ André\ Rieu.wav
Top 20 matches:
        - Voilà by André Rieu, score: 5390686.00
        - I Am a Child of God by One Voice Children's Choir, score: 2539.00
        - I Have A Dream by ABBA, score: 2428.00
        - SOS by ABBA, score: 2327.00
        - Sweet Dreams (Are Made of This) - Remastered by Eurythmics, score: 2213.00
        - The Winner Takes It All by ABBA, score: 2094.00
        - Sleigh Ride by One Voice Children's Choir, score: 2091.00
        - Believe by Cher, score: 2089.00
        - Knowing Me, Knowing You by ABBA, score: 1958.00
        - Gimme! Gimme! Gimme! (A Man After Midnight) by ABBA, score: 1941.00
        - Take A Chance On Me by ABBA, score: 1932.00
        - Don't Stop Me Now - Remastered 2011 by Queen, score: 1892.00
        - I Do, I Do, I Do, I Do, I Do by ABBA, score: 1853.00
        - Everywhere - 2017 Remaster by Fleetwood Mac, score: 1779.00
        - You Will Be Found by One Voice Children's Choir, score: 1664.00
        - J'Imagine by One Voice Children's Choir, score: 1658.00
        - When You Believe by One Voice Children's Choir, score: 1629.00
        - When Love Was Born by One Voice Children's Choir, score: 1484.00
        - Don't Stop Believin' (2022 Remaster) by Journey, score: 1465.00
        - Lay All Your Love On Me by ABBA, score: 1436.00

Search took: 856.386557ms

Final prediction: Voilà by André Rieu , score: 5390686.00
</code></pre></div><p dir="auto">This project is licensed under the MIT License - see the <a href="https://github.com/cgzirim/not-shazam/blob/main/LICENSE">LICENSE</a> file for details.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How I Got My Laser Eye Injury (408 pts)]]></title>
            <link>https://www.funraniumlabs.com/2024/07/how-i-got-my-laser-eye-injury/</link>
            <guid>41127706</guid>
            <pubDate>Thu, 01 Aug 2024 10:25:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.funraniumlabs.com/2024/07/how-i-got-my-laser-eye-injury/">https://www.funraniumlabs.com/2024/07/how-i-got-my-laser-eye-injury/</a>, See on <a href="https://news.ycombinator.com/item?id=41127706">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-5453">

<div>
<p><span>It has been brought to my attention that I have never actually written this story down before, merely told it in person to many students for valuable lessons and also for laughs over cocktails. It is a litany of bad ideas from several people that all came together at once to reach out and zap me.</span></p>
<p><span><strong>DISCLAIMER FOR THE SQUEAMISH:</strong> My eyes and vision are fine. There was some slight retinal bleaching in the peripheral vision of my right eye. If I hold up a large plane of white paper in front of me, like when helping a friend make posters for Ren Faire, there’s a small patch of yellowish tint in the lower right corner. Not a big deal, but damage is damage.</span></p>
<p><span><em>[Scene – A very overcast morning in the spring of 1999. Exterior driveway between Building 6 &amp; 15 of $LASER_COMPANY, roughly 10am]</em></span></p>
<p><span>It was a day much like any other in my four years, five months and eighteen days of working there, not that I was counting or anything. After checking on a couple laser labs and talking to people, I needed to go across the way to visit the optics coating facility. So, I walked out the side door of Building 6 to cross the driveway, go down the stairs and over to the loading dock of Building 15. As I was walking across the driveway, I heard a weird noise. Something was rhythmically clicking away a bit faster than once a second. My thought process went a bit like this.</span></p>
<blockquote><p><span>Me: What is that noise?</span><br>
<span>Me [a few seconds later]: Ohhhhh, I know that noise. That’s the flashlamps of a Quanta-Ray system going off.</span><br>
<span>Me [immediately after, spinning on my heels to head toward the source of the sound]: Why am I hearing this outside?</span></p></blockquote>
<p><span>It was at some point while walking that way and waving my hands and yelling “SHUT IT DOWN!!!” that I took my laser hit from a scattered, fractional beam from the shenanigans going on (I didn’t notice the damage from the hit until several months later). I am going to try to describe what I saw in enough detail that hopefully you can reconstruct the scene in your head, because I don’t want to use my non-existent art skill in Paint to draw this.</span></p>
<figure id="attachment_5454" aria-describedby="caption-attachment-5454"><a href="https://www.funraniumlabs.com/wp-content/uploads/2024/07/Qaunta-Ray.jpg"><img fetchpriority="high" decoding="async" src="https://www.funraniumlabs.com/wp-content/uploads/2024/07/Qaunta-Ray.jpg" alt="" width="226" height="300"></a><figcaption id="caption-attachment-5454"><span>Quanta Ray PRO350 with frequency doubling, emitting a 532nm beam – Sales brochure image from Quanta Ray, unknown date</span></figcaption></figure>
<p><span>The back of&nbsp; Building 6 had their shipping and receiving area and the rear parking lot. In the parking spaces closest to the shipping &amp; receiving area, several spaces had been taken up by a Quanta-Ray’s power supply, a Rubbermaid cart with a large Quanta-Ray laser balanced on it, and it was connected to some Caltrans utility trailer looking contraption downstream and in line with the output aperture of the laser. Beyond the contraption trailer was a VP of Sales’ brand new cherry red Jeep Grand Cherokee. There were umbilicals for chilled water and power running across the parking lot back into the loading dock. Three men are standing behind the laser with another rolling cart being used as moveable workbench, fiddling with the controls for the laser and the contraption it was connected to on the trailer. Two of them were wearing laser safety eyewear. The third, one of our sales engineers who is named Bob, was not wearing any.</span></p>
<p><span>After making sure everything was shut down, I assessed the scene and realized something had gone wrong beyond simply “this entire situation”. This was a sales demo for prospective customers gone horribly awry. I identified myself as the Laser Safety Officer and that I had some questions. The customers looked very much like they wanted to be anywhere but here.</span></p>
<blockquote><p><span>Me: May I see your glasses?</span><br>
<span>[Customers 1 &amp; 2 hand me them]</span><br>
<span>Me: These are argon filters. Are these your glasses at you brought with you?</span><br>
<span>Customer 1: Yes.</span><br>
<span>Me: Shame you’re working with a Nd:YAG laser, not an argon one.</span><br>
<span>Customer 2: Better than nothing, right?</span><br>
<span>Me: These are utterly useless at 1064nm. You both should go make appointments with your ophthalmologists. But at least you understood that you need gogs. Bob, where are yours?</span><br>
<span>Bob: In the lab.</span><br>
<span>Me: Would that be the lab that this laser was in before you wheeled it outside?</span><br>
<span>Bob: Yes.</span><br>
<span>Me: Bob, why is this laser outside? What are you even doing here?</span><br>
<span>Customer 1: You see, we had an idea…</span></p></blockquote>
<p><span>I want to say that, on first blush, their idea was admirable. They were trying to come up with a less destructive way to remove striping from roadways. You have to grind that stuff off, which damages the road surface, leading to increased wear &amp; tear and thus potholes. Their solution was to do it with a laser instead.</span></p>
<blockquote><p><span>Me: Let me see if I understand this right. You want to mount a high power laser on a cart, towed by a Caltrans or contractor truck, to burn the striping off roads?</span><br>
<span>Customer 1: Yes, ingenious isn’t it?</span><br>
<span>Me: The striping with <strong>REFLECTIVE</strong> paint?</span><br>
<span>Customer 2 [looks with concern at Customer 1]: Umm.</span><br>
<span>Me: I’m sure you can find a way with enough power.</span><br>
<span>Customer 1: But look, it worked!</span></p></blockquote>
<p><span>The customer motioned for me to look at the parking lot space stripe that a whole bunch of of round spots on it which had, indeed, burnt the paint off the asphalt.</span></p>
<blockquote><p><span>Me: Bob, $FACILITIES_GUY is going to kill you. He&nbsp;<em>just</em> repaved and striped this parking lot a couple of weeks ago.</span><br>
<span>Bob: [looks morose, as he’s starting to get an inkling of how bad this looks]</span><br>
<span>Me: But you’ve been having some trouble, haven’t you?</span><br>
<span>Customer 1 [surprised]: Yes! We can’t get beam no matter what we do.</span><br>
<span>Me: That’s because you’ve blown the coating on one of your steering optics.</span><br>
<span>Customer 1 &amp; 2: How do you know?</span><br>
<span>Me: Because your beam is not being steered to raster the stripe on the ground. Instead, it’s been firing a flat beam forward and doing a raster scan of [gestures] that Grand Cherokee.</span></p></blockquote>
<p><span>Bob and Customer 1 &amp; 2 looked up to see the stripe of exposed metal on the door of the VP of Sales’ car where the paint had been burnt away. On closer inspection, we later leaned that the Quanta-Ray had burnt through the wheel well and cut the brake line. At this point, I decided I want to really rub in what a terrible idea all this was to them. How they had failed on so many levels.</span></p>
<blockquote><p><span>Me: That’s $VP_of_Sales’ car, isn’t it Bob?</span><br>
<span>Bob: [groans] Yes.</span><br>
<span>Me: Pretty sure that’s your boss, Bob.</span><br>
<span>Bob: Yeah.</span><br>
<span>Me: When did you start doing this?</span><br>
<span>Customer 1: Around 8am.</span><br>
<span>Me: And when did you start having problems?</span><br>
<span>Customer 1: 8:30ish, maybe?</span><br>
<span>Me: Ah, so you were lasing through break time. Bob, what’s behind the car?</span><br>
<span>Bob: Building 15.</span><br>
<span>Me: $VP_of_Sales doesn’t normally get here until after 9am and the roach coach always pulls up at the loading dock of Building 15 at 8:45. So, hopefully you were aiming above eye level for all the employees on break. Also, that exposed brushed steel on the Cherokee is a mirror for near-infrared, so you’ve been shining that beam right back at yourselves. You <em>definitely</em> should call your ophthalmologists. But what’s behind you, Bob?</span><br>
<span>Bob: The fence.</span><br>
<span>Me: What kind of fence is it?</span><br>
<span>Bob: Chain-link.</span><br>
<span>Me: So, not a solid fence then. What’s on the other side of the fence, Bob?</span><br>
<span>Bob: [now staring at the ground in shame] The elementary school.</span><br>
<span>Me: If you’re very lucky, recess happened before your optic failed but we’re still gonna have to send a letter to the school about a potential exposure. Of course, that brushed steel mirror isn’t flat, which means your reflections went all over. Bob, what’s above us?</span><br>
<span>Bob: [picking up where I was going] Planes.</span><br>
<span>Me: How many airports worth of airspace travels over us?</span><br>
<span>Bob: SFO, Oakland, San Jose, the tiny municipal ones.</span><br>
<span>Me: You forgot a really important one. Our neighbor, Moffett fucking Field. Firing a laser into military airspace is an act of war. Are you declaring war on the United States, Bob?</span><br>
<span>Bob: [stands silently]</span><br>
<span>Me: Bob, what else is above us?</span><br>
<span>Bob: [looks up] Clouds.</span></p></blockquote>
<p><span>Because timing is the essence of comedy, that would be when it started to rain on the quarter million dollar laser system, destroying it. Bob no longer worked at $LASER_COMPANY two weeks later.</span></p>
<p><strong>MORAL: </strong>Of all the bystanders you could injure, DO NOT HURT THE SAFETY PERSON.</p>
<p>~fin~</p>
</div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Coinbase awarded a $500k bug bounty (179 pts)]]></title>
            <link>https://hackerone.com/coinbase/hacktivity</link>
            <guid>41127446</guid>
            <pubDate>Thu, 01 Aug 2024 09:40:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hackerone.com/coinbase/hacktivity">https://hackerone.com/coinbase/hacktivity</a>, See on <a href="https://news.ycombinator.com/item?id=41127446">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Breakthrough a step toward revealing hidden structure of prime numbers (263 pts)]]></title>
            <link>https://www.science.org/content/article/sensational-breakthrough-marks-step-toward-revealing-hidden-structure-prime-numbers</link>
            <guid>41126944</guid>
            <pubDate>Thu, 01 Aug 2024 07:34:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/article/sensational-breakthrough-marks-step-toward-revealing-hidden-structure-prime-numbers">https://www.science.org/content/article/sensational-breakthrough-marks-step-toward-revealing-hidden-structure-prime-numbers</a>, See on <a href="https://news.ycombinator.com/item?id=41126944">Hacker News</a></p>
Couldn't get https://www.science.org/content/article/sensational-breakthrough-marks-step-toward-revealing-hidden-structure-prime-numbers: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Stop Killing Games – European Citizens' Initiative (256 pts)]]></title>
            <link>https://www.stopkillinggames.com/eci</link>
            <guid>41126782</guid>
            <pubDate>Thu, 01 Aug 2024 06:59:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.stopkillinggames.com/eci">https://www.stopkillinggames.com/eci</a>, See on <a href="https://news.ycombinator.com/item?id=41126782">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Study: Consumers Actively Turned Off by AI (158 pts)]]></title>
            <link>https://futurism.com/the-byte/study-consumers-turned-off-products-ai</link>
            <guid>41126685</guid>
            <pubDate>Thu, 01 Aug 2024 06:38:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://futurism.com/the-byte/study-consumers-turned-off-products-ai">https://futurism.com/the-byte/study-consumers-turned-off-products-ai</a>, See on <a href="https://news.ycombinator.com/item?id=41126685">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="incArticle"><h2>"When AI is mentioned, it tends to lower emotional trust, which in turn decreases purchase intentions."</h2><h2>Trough Luck</h2><p>Researchers have found that including the words "artificial intelligence" in product marketing is a major turn-off for consumers, suggesting a growing backlash and disillusionment with the tech — and that startups trying to cram "AI" into their product are actually making a grave error.</p><p>As detailed in a <a href="https://www.tandfonline.com/doi/full/10.1080/19368623.2024.2368040">new study</a> published in the <em>Journal of Hospitality Marketing &amp; Management</em>, researchers presented 1,000 respondents with questions and descriptions of products. Surprisingly — or <a href="https://futurism.com/google-ai-ad-sad">perhaps not</a>, depending on <a href="https://futurism.com/the-byte/ai-adding-work-study">your perspective</a> — they found that products described as using AI were consistently less popular.</p><p>"When AI is mentioned, it tends to lower emotional trust, which in turn decreases purchase intentions," said lead author and Washington State University clinical assistant profess of marketing Mesut Cicek in a <a href="https://news.wsu.edu/press-release/2024/07/30/using-the-term-artificial-intelligence-in-product-descriptions-reduces-purchase-intentions/">statement</a>. "We found emotional trust plays a critical role in how consumers perceive AI-powered products."</p><h2>Strong Pass</h2><p>In an experiment, the researchers found that a group of participants were far less likely to purchase a smart television when its description included the words "artificial intelligence." A separate group was far <em>more</em> likely to buy it when the words were omitted from an otherwise identical description.</p><p>For "high-risk" purchases such as expensive electronics or medical devices, the effect was even more pronounced, with Cicek suggesting that consumers are more wary of monetary loss or danger to physical safety.</p><p>"We tested the effect across eight different product and service categories, and the results were all the same: it’s a disadvantage to include those kinds of terms in the product descriptions," Cicek said.</p><p>That kind of growing mistrust is symptomatic of a much larger trend. Earlier this year, technology research and consulting firm Gartner found that the hype surrounding generative AI had passed the "peak of inflated expectations," which is marked by "overenthusiasm and unrealistic projections."</p><p>Companies are feverishly trying to stuff what they claim to be AI into every product, from <a href="https://futurism.com/bumble-founder-future-ai-dating-other-ais">dating apps</a> to <a href="https://futurism.com/the-byte/ai-bot-disabled-dpd">automated car salesmen</a> — despite <a href="https://futurism.com/meta-ai-trump-wasnt-shot">glaring shortcomings</a> that have yet to be solved and <a href="https://futurism.com/investors-concerned-ai-making-money">mounting, astronomical costs</a>.</p><p>And consumers are getting tired of their desperate attempts to capitalize on all the hype.</p><p>"Marketers should carefully consider how they present AI in their product descriptions or develop strategies to increase emotional trust," said Cicek. "Emphasizing AI may not always be beneficial, particularly for high-risk products. Focus on describing the features or benefits and avoid the AI buzzwords."</p><p><strong>More on AI:</strong> <em><a href="https://futurism.com/meta-ai-trump-wasnt-shot">Meta's AI Says Trump Wasn't Shot</a></em></p><br></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[This month in Servo: console logging, parallel tables, OpenXR, and more (130 pts)]]></title>
            <link>https://servo.org/blog/2024/07/31/this-month-in-servo/</link>
            <guid>41126130</guid>
            <pubDate>Thu, 01 Aug 2024 04:21:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://servo.org/blog/2024/07/31/this-month-in-servo/">https://servo.org/blog/2024/07/31/this-month-in-servo/</a>, See on <a href="https://news.ycombinator.com/item?id=41126130">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span>2024-07-31</span> Servo’s unique parallel layout engine just got even better.</p><div>
  <figure><a href="https://servo.org/img/blog/quest-3-passthrough.png"><img src="https://servo.org/img/blog/quest-3-passthrough.png" alt="Servo displaying WebXR content on a Quest 3 in Quest Link mode"></a>
<figcaption>Figure 1: Servo can now render to XR headsets via OpenXR. Image: Daniel Adams (<a href="https://twitter.com/msub2official/status/1818533316477251669">Twitter</a>)</figcaption></figure>
<p><span></span>
Servo has had several new features land in our nightly builds over the last month:</p>
<ul>
<li>as of 2024-06-27, <strong>document.fonts.ready</strong> (<a href="https://github.com/mukilan">@mukilan</a>, <a href="https://github.com/mrobinson">@mrobinson</a>, <a href="https://github.com/servo/servo/pull/32576">#32576</a>)</li>
<li>as of 2024-07-03, the <strong>getCompilationInfo() method on GPUShaderModule</strong> (<a href="https://github.com/sagudev">@sagudev</a>, <a href="https://github.com/servo/servo/pull/32642">#32642</a>)</li>
<li>as of 2024-07-08, window.<strong>customElements.getName</strong> (<a href="https://github.com/keithamus">@keithamus</a>, <a href="https://github.com/servo/servo/pull/32715">#32715</a>)</li>
<li>as of 2024-07-09, <strong>&lt;caption&gt; in tables</strong> (<a href="https://github.com/mrobinson">@mrobinson</a>, <a href="https://github.com/Loirooriol">@Loirooriol</a>, <a href="https://github.com/mukilan">@mukilan</a>, <a href="https://github.com/servo/servo/pull/32657">#32657</a>, <a href="https://github.com/servo/servo/pull/32695">#32695</a>)</li>
<li>as of 2024-07-13, <strong>document.visibilityState</strong> and <strong>document.hidden</strong> (<a href="https://github.com/wusyong">@wusyong</a>, <a href="https://github.com/servo/servo/pull/32635">#32635</a>)</li>
<li>as of 2024-07-18, the <strong>measureText() method on CanvasRenderingContext2D</strong> (<a href="https://github.com/chocolate-pie">@chocolate-pie</a>, <a href="https://github.com/servo/servo/pull/32704">#32704</a>)</li>
<li>as of 2024-07-23, <strong>URL.parse()</strong> (<a href="https://github.com/shanehandley">@shanehandley</a>, <a href="https://github.com/servo/servo/pull/32819">#32819</a>)</li>
</ul>
<p>We’ve also landed an experimental <strong>OpenXR backend</strong> (<a href="https://github.com/msub2">@msub2</a>, <a href="https://github.com/servo/servo/pull/32817">#32817</a>), allowing Servo to display WebXR content on actual headsets like the <strong>Quest 3</strong> in Quest Link mode.
You can enable it with <code>--pref dom.webxr.openxr.enabled</code>, though the backend currently only works on Windows.</p>
<figure><a href="https://servo.org/img/blog/july-2024.png"><img src="https://servo.org/img/blog/july-2024.png" alt="Servo nightly showing a table with a caption, containing demos of several other new features"></a>
<figcaption>Figure 2: a table with a caption, containing demos of several other new features.</figcaption></figure>
<h2 id="rendering-changes" tabindex="-1"><span></span>Rendering changes <a href="#rendering-changes">
        <span><i></i></span>
      </a></h2>
<p><strong>Parallel table layout</strong> is now enabled (<a href="https://github.com/mrobinson">@mrobinson</a>, <a href="https://github.com/servo/servo/pull/32477">#32477</a>), spreading the work for laying out rows and their columns over all available CPU cores.
This change is a great example of the strengths of <a href="https://crates.io/crates/rayon">Rayon</a> and the opportunistic parallelism in Servo’s layout engine.</p>
<p>We‘ve also made progress on our new <strong>flexbox layout engine</strong> (<code>--pref layout.flexbox.enabled</code>), landing support for <strong>‘min-height’</strong> and <strong>‘max-height’</strong> on row containers (<a href="https://github.com/delan">@delan</a>, <a href="https://github.com/mrobinson">@mrobinson</a>, <a href="https://github.com/mukilan">@mukilan</a>, <a href="https://github.com/servo/servo/pull/32785">#32785</a>), as well as <strong>baseline alignment of row containers</strong> with their siblings (<a href="https://github.com/mrobinson">@mrobinson</a>, <a href="https://github.com/mukilan">@mukilan</a>, <a href="https://github.com/delan">@delan</a>, <a href="https://github.com/servo/servo/pull/32841">#32841</a>, <a href="https://github.com/servo/servo/pull/32810">#32810</a>) and for their items by setting <strong>‘align-items’</strong> or <strong>‘align-self’</strong> to <strong>‘baseline’</strong>, <strong>‘first baseline’</strong>, or <strong>‘last baseline’</strong> (<a href="https://github.com/delan">@delan</a>, <a href="https://github.com/mrobinson">@mrobinson</a>, <a href="https://github.com/mukilan">@mukilan</a>, <a href="https://github.com/nicoburns">@nicoburns</a>, <a href="https://github.com/servo/servo/pull/32787">#32787</a>, <a href="https://github.com/servo/servo/pull/32790">#32790</a>).</p>

<p>We’ve landed support for <strong>generic font families</strong> like ‘sans-serif’ and ‘monospace’ (<a href="https://github.com/mrobinson">@mrobinson</a>, <a href="https://github.com/mukilan">@mukilan</a>, <a href="https://github.com/servo/servo/pull/32673">#32673</a>), as well as <strong>commas in &lt;font face&gt;</strong> (<a href="https://github.com/mrobinson">@mrobinson</a>, <a href="https://github.com/servo/servo/pull/32622">#32622</a>) and fixes for font matching on Android and OpenHarmony (<a href="https://github.com/jschwe">@jschwe</a>, <a href="https://github.com/servo/servo/pull/32725">#32725</a>, <a href="https://github.com/servo/servo/pull/32731">#32731</a>).</p>
<p>For <strong>replaced elements</strong> like &lt;img&gt; and &lt;canvas&gt;, the <strong>‘min-width’</strong>, <strong>‘max-width’</strong>, <strong>‘min-height’</strong>, and <strong>‘max-height’</strong> properties now <strong>respect the aspect ratio</strong> of the element (<a href="https://github.com/valadaptive">@valadaptive</a>, <a href="https://github.com/servo/servo/pull/32777">#32777</a>), and you can now change that aspect ratio with the <strong>‘aspect-ratio’ property</strong> (<a href="https://github.com/valadaptive">@valadaptive</a>, <a href="https://github.com/servo/servo/pull/32800">#32800</a>, <a href="https://github.com/servo/servo/pull/32803">#32803</a>).</p>
<figure><a href="https://servo.org/img/blog/devtools-july-2024.png"><img src="https://servo.org/img/blog/devtools-july-2024.png" alt="Firefox devtools connected to Servo, showing several console errors"></a>
<figcaption>Figure 3: console logging is now supported when using the Firefox devtools.</figcaption></figure>

<p>When debugging in Servo <a href="https://book.servo.org/running-servoshell.html">with the <strong>Firefox devtools</strong></a>, you can now see <strong>console messages</strong> from the page (<a href="https://github.com/eerii">@eerii</a>, <a href="https://github.com/servo/servo/pull/32727">#32727</a>), as shown in <em>Figure 3</em>, and you can even debug the devtools connection itself with our new <strong>devtools protocol analyzer</strong> (<a href="https://github.com/eerii">@eerii</a>, <a href="https://github.com/servo/servo/pull/32684">#32684</a>).</p>
<p>servoshell now has experimental <strong>OpenHarmony support</strong> (<a href="https://github.com/jschwe">@jschwe</a>, <a href="https://github.com/servo/servo/pull/32594">#32594</a>), in addition to our experimental Android support and nightly releases for Windows, macOS, and Linux.
We’ve also landed <strong>directory listings</strong> for local files (<a href="https://github.com/Bobulous">@Bobulous</a>, <a href="https://github.com/mrobinson">@mrobinson</a>, <a href="https://github.com/servo/servo/pull/32580">#32580</a>), made the location bar behave more consistently on Android (<a href="https://github.com/jschwe">@jschwe</a>, <a href="https://github.com/servo/servo/pull/32586">#32586</a>), and servoshell no longer quits when you press Escape (<a href="https://github.com/mrego">@mrego</a>, <a href="https://github.com/servo/servo/pull/32603">#32603</a>).</p>
<figure><div>
<table>
<thead>
<tr>
<th>Version and build config</th>
<th><code>servo</code> binary size</th>
</tr>
</thead>
<tbody>
<tr>
<td>Before <a href="https://github.com/servo/servo/pull/32651">#32651</a></td>
<td>126364k</td>
</tr>
<tr>
<td>With <a href="https://github.com/servo/servo/pull/32651">#32651</a></td>
<td>110111k (−12.8%)</td>
</tr>
<tr>
<td>With <a href="https://github.com/servo/servo/pull/32651">#32651</a><br>• Without debug symbols</td>
<td>102878k (−18.5%)</td>
</tr>
<tr>
<td>With <a href="https://github.com/servo/servo/pull/32759">#32759</a><br>• Without <code>layout_2013</code></td>
<td>107652k (−14.8%)</td>
</tr>
<tr>
<td>With <a href="https://github.com/servo/servo/pull/32759">#32759</a><br>• Without debug symbols<br>• Without <code>layout_2013</code></td>
<td>100886k (−20.1%)</td>
</tr>
</tbody>
</table>
</div>
<figcaption>Figure 4: servoshell binary size improvements on Linux (amd64).</figcaption></figure>
<p><span></span>
To reduce servoshell’s binary size, we now build our nightly releases with <strong>ThinLTO</strong> (<a href="https://github.com/jschwe">@jschwe</a>, <a href="https://github.com/servo/servo/pull/32651">#32651</a>), and you can go even further by building Servo <strong>without debug symbols</strong> (<a href="https://github.com/jschwe">@jschwe</a>, <a href="https://github.com/servo/servo/pull/32651">#32651</a>) or <strong>without the legacy layout engine</strong> (<a href="https://github.com/jschwe">@jschwe</a>, <a href="https://github.com/servo/servo/pull/32759">#32759</a>).
Note that these builds use the <code>production</code> profile in Cargo, not the <code>release</code> profile.</p>
<h2 id="changes-for-servo-developers" tabindex="-1">Changes for Servo developers <a href="#changes-for-servo-developers">
        <span><i></i></span>
      </a></h2>
<p><a href="https://book.servo.org/"><strong>The Servo book</strong></a> is now the place to go for Servo’s documentation (<a href="https://github.com/delan">@delan</a>, <a href="https://github.com/servo/servo/pull/32743">#32743</a>).
It includes our architecture and design docs, a link to our API docs, as well as docs on building, running, testing, debugging, and profiling Servo.</p>
<p>Servo now builds without the <code>crown</code> linter by default (<a href="https://github.com/jschwe">@jschwe</a>, <a href="https://github.com/servo/servo/pull/32494">#32494</a>), simplifying the build process in some cases.
If you’re working on DOM code, you can enable it again with <code>./mach build --use-crown</code>.</p>
<figure><a href="https://servo.org/img/blog/dco-check.png"><img src="https://servo.org/img/blog/dco-check.png" alt="GitHub checks popup showing the “DCO” check failing and a link to “Details”"></a>
<figcaption>Figure 5: the DCO check will now fail unless you sign off your commits.</figcaption></figure>
<p><span></span>
When contributing to Servo, <strong>your commits must now be <a href="https://developercertificate.org/">“signed off”</a></strong>, which is essentially a promise that you own (or are allowed to contribute) the code in your patch.
If the DCO check fails, click Details for help on signing off your commits (<em>Figure 5</em>).</p>

<h2 id="donations" tabindex="-1">Donations <a href="#donations">
        <span><i></i></span>
      </a></h2>
<p>Thanks again for your generous support!
We are now receiving <strong>2955 USD/month</strong> (+32.6% over June) in recurring donations.</p>
<p>Servo is now on <a href="https://thanks.dev/">thanks.dev</a>, and already <strong>three GitHub orgs</strong> that depend on Servo are sponsoring us there.
If you use Servo libraries like <a href="https://crates.io/crates/url/reverse_dependencies">url</a>, <a href="https://crates.io/crates/html5ever/reverse_dependencies">html5ever</a>, <a href="https://crates.io/crates/selectors/reverse_dependencies">selectors</a>, or <a href="https://crates.io/crates/cssparser/reverse_dependencies">cssparser</a>, signing up for <a href="https://thanks.dev/">thanks.dev</a> could be a good way for you (or your employer) to give back to the community.</p>
<p>We are still receiving donations from <strong>14 people</strong> on LFX, but we will stop accepting donations there soon — <strong>please move your recurring donations to <a href="https://github.com/sponsors/servo">GitHub</a> or <a href="https://opencollective.com/servo">Open Collective</a></strong>.
In the meantime, we’ve transferred <strong>2723 USD</strong> of donations from LFX to our Open Collective account.</p>
<figure><div>
        <p><strong>2955</strong> USD/month</p>
        
        
        <p><strong>10000</strong></p>
    </div></figure>
<p>As always, use of these funds will be decided transparently in the Technical Steering Committee.
Our updated proposal for a <a href="https://github.com/servo/project/issues/94#issuecomment-2252262955">dedicated server for CI runners</a> (<a href="https://github.com/delan">@delan</a>, <a href="https://github.com/sagudev">@sagudev</a>, <a href="https://github.com/nicoburns">@nicoburns</a>) was accepted, which should reduce build times significantly, and this is just the start!</p>
<p>For more details, head to our <a href="https://servo.org/sponsorship/">Sponsorship page</a>.</p>
<h2 id="conferences-and-blogs" tabindex="-1">Conferences and blogs <a href="#conferences-and-blogs">
        <span><i></i></span>
      </a></h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=SamA5Oz-G5w"><strong>Servo: A web rendering engine for the future</strong></a> (<a href="https://servo.org/slides/2024-07-02-global-software-technology-summit/">slides</a>) — Manuel Rego spoke at the <a href="https://huawei-events.de/en/gsts24.htm">Global Software Technology Summit 2024</a> about the status and long-term vision of the Servo project</li>
<li><a href="https://wusyong.github.io/posts/verso-0-1/"><strong>Verso: A new browser based on Servo</strong></a> — Wu Yu Wei wrote about their plans to build a more polished Servo-based browser while improving Servo’s architecture</li>
<li><a href="https://wusyong.github.io/posts/verso-compositor-part1/"><strong>Verso: Writing its own compositor part 1</strong></a> — Wu Yu Wei uses Verso as a sandbox to explore how we might rework Servo’s compositor to support multiple windows</li>
<li><a href="https://conflor.es/blog/03_halfway_point"><strong>Halfway point</strong></a> — Eri wrote about repairing Servo’s devtools support, from planning to selecting tabs to finding resources and now the console, as part of their Outreachy internship</li>
</ul>
<h2 id="alan-jeffrey-(1967%E2%80%932024)" tabindex="-1">Alan Jeffrey (1967–2024) <a href="#alan-jeffrey-(1967%E2%80%932024)">
        <span><i></i></span>
      </a></h2>
<p><a href="https://web.archive.org/web/20240714161830/https://asaj.org/">Alan Jeffrey</a>, an early member of the Servo team and a key part of helping the Servo project find a new life outside of Mozilla, passed away on 4 July.</p>
<p>His research has furthered a wide range of fields, including concurrent and distributed systems, programming languages, formal verification, software semantics, typesetting, protocol security, and circuit design.</p>
<p>Alan’s family have also written about his kindness, curiosity, and persistence <a href="https://www.linkedin.com/feed/update/activity:7215033040614436865/">on his LinkedIn page</a>.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PyTorch – Torchchat: Chat with LLMs Everywhere (190 pts)]]></title>
            <link>https://github.com/pytorch/torchchat</link>
            <guid>41125980</guid>
            <pubDate>Thu, 01 Aug 2024 03:48:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/pytorch/torchchat">https://github.com/pytorch/torchchat</a>, See on <a href="https://news.ycombinator.com/item?id=41125980">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Chat with LLMs Everywhere</h2><a id="user-content-chat-with-llms-everywhere" aria-label="Permalink: Chat with LLMs Everywhere" href="#chat-with-llms-everywhere"></a></p>
<p dir="auto">torchchat is a small codebase showcasing the ability to run large language models (LLMs) seamlessly. With torchchat, you can run LLMs using Python, within your own (C/C++) application (desktop or server) and on iOS and Android.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What can you do with torchchat?</h2><a id="user-content-what-can-you-do-with-torchchat" aria-label="Permalink: What can you do with torchchat?" href="#what-can-you-do-with-torchchat"></a></p>
<ul dir="auto">
<li><a href="#running-via-pytorch--python">Run models via PyTorch / Python</a>
<ul dir="auto">
<li><a href="#chat">Chat</a></li>
<li><a href="#generate">Generate</a></li>
<li><a href="#browser">Run chat in the Browser</a></li>
</ul>
</li>
<li><a href="#desktopserver-execution">Run models on desktop/server without python</a>
<ul dir="auto">
<li><a href="#aoti-aot-inductor">Use AOT Inductor for faster execution</a></li>
<li><a href="#running-native-using-our-c-runner">Running in c++ using the runner</a></li>
</ul>
</li>
<li><a href="#mobile-execution">Run models on mobile</a>
<ul dir="auto">
<li><a href="#deploy-and-run-on-ios">Deploy and run on iOS</a></li>
<li><a href="#deploy-and-run-on-android">Deploy and run on Android</a></li>
</ul>
</li>
<li><a href="#eval">Evaluate a model</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Highlights</h2><a id="user-content-highlights" aria-label="Permalink: Highlights" href="#highlights"></a></p>
<ul dir="auto">
<li>Command line interaction with popular LLMs such as Llama 3, Llama 2, Stories, Mistral and more</li>
<li>PyTorch-native execution with performance</li>
<li>Supports popular hardware and OS
<ul dir="auto">
<li>Linux (x86)</li>
<li>Mac OS (M1/M2/M3)</li>
<li>Android (Devices that support XNNPACK)</li>
<li>iOS 17+ (iPhone 13 Pro+)</li>
</ul>
</li>
<li>Multiple data types including: float32, float16, bfloat16</li>
<li>Multiple quantization schemes</li>
<li>Multiple execution modes including: Python (Eager, Compile) or Native (AOT Inductor (AOTI), ExecuTorch)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">The following steps require that you have <a href="https://www.python.org/downloads/release/python-3100/" rel="nofollow">Python 3.10</a> installed.</p>
<div dir="auto" data-snippet-clipboard-copy-content="# get the code
git clone https://github.com/pytorch/torchchat.git
cd torchchat

# set up a virtual environment
python3 -m venv .venv
source .venv/bin/activate

# install dependencies
./install_requirements.sh"><pre><span><span>#</span> get the code</span>
git clone https://github.com/pytorch/torchchat.git
<span>cd</span> torchchat

<span><span>#</span> set up a virtual environment</span>
python3 -m venv .venv
<span>source</span> .venv/bin/activate

<span><span>#</span> install dependencies</span>
./install_requirements.sh</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Commands</h2><a id="user-content-commands" aria-label="Permalink: Commands" href="#commands"></a></p>
<p dir="auto">The interfaces of torchchat are leveraged through <strong>Python Commands</strong> and <strong>Native Runners</strong>. While the Python Commands are enumerable in the --help menu, the latter are explored in their respective sections.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 torchchat.py --help"><pre>python3 torchchat.py --help</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# Output
usage: torchchat [-h] {chat,browser,generate,export,eval,download,list,remove,where,server} ...

positional arguments:
  {chat,browser,generate,export,eval,download,list,remove,where,server}
                        The specific command to run
    chat                Chat interactively with a model via the CLI
    generate            Generate responses from a model given a prompt
    browser             Chat interactively with a model in a locally hosted browser
    export              Export a model artifact to AOT Inductor or ExecuTorch
    download            Download model artifacts
    list                List all supported models
    remove              Remove downloaded model artifacts
    where               Return directory containing downloaded model artifacts
    server              [WIP] Starts a locally hosted REST server for model interaction
    eval                Evaluate a model via lm-eval

options:
  -h, --help            show this help message and exit"><pre><span><span>#</span> Output</span>
usage: torchchat [-h] {chat,browser,generate,export,eval,download,list,remove,where,server} ...

positional arguments:
  {chat,browser,generate,export,eval,download,list,remove,where,server}
                        The specific <span>command</span> to run
    chat                Chat interactively with a model via the CLI
    generate            Generate responses from a model given a prompt
    browser             Chat interactively with a model <span>in</span> a locally hosted browser
    <span>export</span>              Export a model artifact to AOT Inductor or ExecuTorch
    download            Download model artifacts
    list                List all supported models
    remove              Remove downloaded model artifacts
    where               Return directory containing downloaded model artifacts
    server              [WIP] Starts a locally hosted REST server <span>for</span> model interaction
    <span>eval</span>                Evaluate a model via lm-eval

options:
  -h, --help            show this <span>help</span> message and <span>exit</span></pre></div>
<p dir="auto"><strong>Python Inference</strong> (chat, generate, browser, server)</p>
<ul dir="auto">
<li>These commands represent different flavors of performing model inference in a Python enviroment.</li>
<li>Models are constructed either from CLI args or from loading exported artifacts.</li>
</ul>
<p dir="auto"><strong>Exporting</strong> (export)</p>
<ul dir="auto">
<li>This command generates model artifacts that are consumed by Python Inference or Native Runners.</li>
<li>More information is provided in the <a href="https://github.com/pytorch/torchchat?tab=readme-ov-file#aoti-aot-inductor">AOT Inductor</a> and <a href="https://github.com/pytorch/torchchat?tab=readme-ov-file#export-for-mobile">ExecuTorch</a> sections.</li>
</ul>
<p dir="auto"><strong>Inventory Management</strong> (download, list, remove, where)</p>
<ul dir="auto">
<li>These commands are used to manage and download models.</li>
<li>More information is provided in the <a href="https://github.com/pytorch/torchchat?tab=readme-ov-file#download-weights">Download Weights</a> section.</li>
</ul>
<p dir="auto"><strong>Evaluation</strong> (eval)</p>
<ul dir="auto">
<li>This command test model fidelity via EleutherAI's <a href="https://github.com/EleutherAI/lm-evaluation-harness">lm_evaluation_harness</a>.</li>
<li>More information is provided in the <a href="https://github.com/pytorch/torchchat?tab=readme-ov-file#eval">Evaluation</a> section.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Download Weights</h2><a id="user-content-download-weights" aria-label="Permalink: Download Weights" href="#download-weights"></a></p>
<p dir="auto">Most models use Hugging Face as the distribution channel, so you will need to create a Hugging Face account.
Create a Hugging Face user access token <a href="https://huggingface.co/docs/hub/en/security-tokens" rel="nofollow">as documented here</a> with the <code>write</code> role.</p>
<p dir="auto">Log into Hugging Face:</p>

<p dir="auto">Once this is done, torchchat will be able to download model artifacts from
Hugging Face.</p>
<div data-snippet-clipboard-copy-content="python3 torchchat.py download llama3.1"><pre><code>python3 torchchat.py download llama3.1
</code></pre></div>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">This command may prompt you to request access to Llama 3 via
Hugging Face, if you do not already have access. Simply follow the
prompts and re-run the command when access is granted.*</p>
</div>
<details>
<summary>Additional Model Inventory Management Commands</summary>
<p dir="auto"><h3 tabindex="-1" dir="auto">List</h3><a id="user-content-list" aria-label="Permalink: List" href="#list"></a></p>
<p dir="auto">This subcommands shows the available models</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 torchchat.py list"><pre>python3 torchchat.py list</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Where</h3><a id="user-content-where" aria-label="Permalink: Where" href="#where"></a></p>
<p dir="auto">This subcommands shows location of a particular model.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 torchchat.py list"><pre>python3 torchchat.py list</pre></div>
<p dir="auto">This is useful in scripts when you do not want to hard-code paths</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Remove</h3><a id="user-content-remove" aria-label="Permalink: Remove" href="#remove"></a></p>
<p dir="auto">This subcommands removes the specified model</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 torchchat.py remove llama3.1"><pre>python3 torchchat.py remove llama3.1</pre></div>
<p dir="auto">More information about these commands can be found by adding the <code>--help</code> option.</p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running via PyTorch / Python</h2><a id="user-content-running-via-pytorch--python" aria-label="Permalink: Running via PyTorch / Python" href="#running-via-pytorch--python"></a></p>
<p dir="auto">The simplest way to run a model in PyTorch is via <a href="https://pytorch.org/blog/optimizing-production-pytorch-performance-with-graph-transformations/" rel="nofollow">eager execution</a>.
This is the default execution mode for both PyTorch and torchchat. It performs inference
without creating exporting artifacts or using a separate runner.</p>
<p dir="auto">The model used for inference can also be configured and tailored to specific needs
(compilation, quantization, etc.). See the <a href="https://github.com/pytorch/torchchat/blob/main/docs/model_customization.md">customization guide</a> for the options supported by torchchat.</p>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">For more information about these commands, please refer to the <code>--help</code> menu.</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Chat</h3><a id="user-content-chat" aria-label="Permalink: Chat" href="#chat"></a></p>
<p dir="auto">This mode allows you to chat with an LLM in an interactive fashion.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 torchchat.py chat llama3.1"><pre>python3 torchchat.py chat llama3.1</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Generate</h3><a id="user-content-generate" aria-label="Permalink: Generate" href="#generate"></a></p>
<p dir="auto">This mode generates text based on an input prompt.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 torchchat.py generate llama3.1 --prompt &quot;write me a story about a boy and his bear&quot;"><pre>python3 torchchat.py generate llama3.1 --prompt <span><span>"</span>write me a story about a boy and his bear<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Browser</h3><a id="user-content-browser" aria-label="Permalink: Browser" href="#browser"></a></p>
<p dir="auto">This mode allows you to chat with the model using a UI in your browser
Running the command automatically open a tab in your browser.</p>
<div data-snippet-clipboard-copy-content="streamlit run torchchat.py -- browser llama3.1"><pre><code>streamlit run torchchat.py -- browser llama3.1
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Server</h3><a id="user-content-server" aria-label="Permalink: Server" href="#server"></a></p>
<p dir="auto"><strong>Note: This feature is still a work in progress and not all endpoints are working</strong></p>
<details>
<summary>This mode gives a REST API that matches the OpenAI API spec for interacting with a model</summary>
<p dir="auto">To test out the REST API, <strong>you'll need 2 terminals</strong>: one to host the server, and one to send the request.</p>
<p dir="auto">In one terminal, start the server</p>
<div dir="auto" data-snippet-clipboard-copy-content="python3 torchchat.py server llama3.1"><pre>python3 torchchat.py server llama3.1</pre></div>
<p dir="auto">In another terminal, query the server using <code>curl</code>. Depending on the model configuration, this query might take a few minutes to respond.</p>
<p dir="auto">Setting <code>stream</code> to "true" in the request emits a response in chunks. Currently, this response
is plaintext and will not be formatted to the OpenAI API specification. If <code>stream</code> is unset or not "true", then the client will await the full response from the server.</p>
<p dir="auto"><strong>Example Input + Output</strong></p>
<div data-snippet-clipboard-copy-content="curl http://127.0.0.1:5000/chat \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;model&quot;: &quot;llama3.1&quot;,
    &quot;stream&quot;: &quot;true&quot;,
    &quot;messages&quot;: [
      {
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: &quot;You are a helpful assistant.&quot;
      },
      {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;Hello!&quot;
      }
    ]
  }'"><pre><code>curl http://127.0.0.1:5000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.1",
    "stream": "true",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "Hello!"
      }
    ]
  }'
</code></pre></div>
<div data-snippet-clipboard-copy-content="{&quot;response&quot;:&quot; I'm a software developer with a passion for building innovative and user-friendly applications. I have experience in developing web and mobile applications using various technologies such as Java, Python, and JavaScript. I'm always looking for new challenges and opportunities to learn and grow as a developer.\n\nIn my free time, I enjoy reading books on computer science and programming, as well as experimenting with new technologies and techniques. I'm also interested in machine learning and artificial intelligence, and I'm always looking for ways to apply these concepts to real-world problems.\n\nI'm excited to be a part of the developer community and to have the opportunity to share my knowledge and experience with others. I'm always happy to help with any questions or problems you may have, and I'm looking forward to learning from you as well.\n\nThank you for visiting my profile! I hope you find my information helpful and interesting. If you have any questions or would like to discuss any topics, please feel free to reach out to me. I&quot;}"><pre><code>{"response":" I'm a software developer with a passion for building innovative and user-friendly applications. I have experience in developing web and mobile applications using various technologies such as Java, Python, and JavaScript. I'm always looking for new challenges and opportunities to learn and grow as a developer.\n\nIn my free time, I enjoy reading books on computer science and programming, as well as experimenting with new technologies and techniques. I'm also interested in machine learning and artificial intelligence, and I'm always looking for ways to apply these concepts to real-world problems.\n\nI'm excited to be a part of the developer community and to have the opportunity to share my knowledge and experience with others. I'm always happy to help with any questions or problems you may have, and I'm looking forward to learning from you as well.\n\nThank you for visiting my profile! I hope you find my information helpful and interesting. If you have any questions or would like to discuss any topics, please feel free to reach out to me. I"}
</code></pre></div>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Desktop/Server Execution</h2><a id="user-content-desktopserver-execution" aria-label="Permalink: Desktop/Server Execution" href="#desktopserver-execution"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">AOTI (AOT Inductor)</h3><a id="user-content-aoti-aot-inductor" aria-label="Permalink: AOTI (AOT Inductor)" href="#aoti-aot-inductor"></a></p>
<p dir="auto"><a href="https://pytorch.org/blog/pytorch2-2/" rel="nofollow">AOTI</a> compiles models before execution for faster inference. The process creates a <a href="https://en.wikipedia.org/wiki/Shared_library" rel="nofollow">DSO</a> model (represented by a file with extension <code>.so</code>)
that is then loaded for inference. This can be done with both Python and C++ enviroments.</p>
<p dir="auto">The following example exports and executes the Llama3.1 8B Instruct
model.  The first command compiles and performs the actual export.</p>
<div data-snippet-clipboard-copy-content="python3 torchchat.py export llama3.1 --output-dso-path exportedModels/llama3.1.so"><pre><code>python3 torchchat.py export llama3.1 --output-dso-path exportedModels/llama3.1.so
</code></pre></div>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">If your machine has cuda add this flag for performance
<code>--quantize config/data/cuda.json</code> when exporting.</p>
</div>
<p dir="auto">For more details on quantization and what settings to use for your use
case visit our <a href="https://github.com/pytorch/torchchat/blob/main/docs/model_customization.md">customization guide</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Run in a Python Enviroment</h3><a id="user-content-run-in-a-python-enviroment" aria-label="Permalink: Run in a Python Enviroment" href="#run-in-a-python-enviroment"></a></p>
<p dir="auto">To run in a python enviroment, use the generate subcommand like before, but include the dso file.</p>
<div data-snippet-clipboard-copy-content="python3 torchchat.py generate llama3.1 --dso-path exportedModels/llama3.1.so --prompt &quot;Hello my name is&quot;"><pre><code>python3 torchchat.py generate llama3.1 --dso-path exportedModels/llama3.1.so --prompt "Hello my name is"
</code></pre></div>
<p dir="auto"><strong>Note:</strong> Depending on which accelerator is used to generate the .dso file, the command may need the device specified: <code>--device (cuda | cpu)</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Run using our C++ Runner</h3><a id="user-content-run-using-our-c-runner" aria-label="Permalink: Run using our C++ Runner" href="#run-using-our-c-runner"></a></p>
<p dir="auto">To run in a C++ enviroment, we need to build the runner binary.</p>
<div dir="auto" data-snippet-clipboard-copy-content="scripts/build_native.sh aoti"><pre>scripts/build_native.sh aoti</pre></div>
<p dir="auto">Then run the compiled executable, with the exported DSO from earlier.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cmake-out/aoti_run exportedModels/llama3.1.so -z `python3 torchchat.py where llama3.1`/tokenizer.model -l 3 -i &quot;Once upon a time&quot;"><pre>cmake-out/aoti_run exportedModels/llama3.1.so -z <span><span>`</span>python3 torchchat.py where llama3.1<span>`</span></span>/tokenizer.model -l 3 -i <span><span>"</span>Once upon a time<span>"</span></span></pre></div>
<p dir="auto"><strong>Note:</strong> Depending on which accelerator is used to generate the .dso file, the runner may need the device specified: <code>-d (CUDA | CPU)</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Mobile Execution</h2><a id="user-content-mobile-execution" aria-label="Permalink: Mobile Execution" href="#mobile-execution"></a></p>
<p dir="auto"><a href="https://github.com/pytorch/executorch">ExecuTorch</a> enables you to optimize your model for execution on a
mobile or embedded device.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Set Up ExecuTorch</h3><a id="user-content-set-up-executorch" aria-label="Permalink: Set Up ExecuTorch" href="#set-up-executorch"></a></p>
<p dir="auto">Before running any commands in torchchat that require ExecuTorch, you
must first install ExecuTorch.</p>
<p dir="auto">To install ExecuTorch, run the following commands.  This will download the
ExecuTorch repo to ./et-build/src and install various ExecuTorch libraries to
./et-build/install.</p>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">The following commands should be run from the torchchat root directory.</p>
</div>
<div data-snippet-clipboard-copy-content="export TORCHCHAT_ROOT=${PWD}
./scripts/install_et.sh"><pre><code>export TORCHCHAT_ROOT=${PWD}
./scripts/install_et.sh
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Export for mobile</h3><a id="user-content-export-for-mobile" aria-label="Permalink: Export for mobile" href="#export-for-mobile"></a></p>
<p dir="auto">Similar to AOTI, to deploy onto device, we first export the PTE artifact, then we load the artifact for inference.</p>
<p dir="auto">The following example uses the Llama3.1 8B Instruct model.</p>
<div data-snippet-clipboard-copy-content="# Export
python3 torchchat.py export llama3.1 --quantize config/data/mobile.json --output-pte-path llama3.1.pte"><pre><code># Export
python3 torchchat.py export llama3.1 --quantize config/data/mobile.json --output-pte-path llama3.1.pte
</code></pre></div>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">We use <code>--quantize config/data/mobile.json</code> to quantize the
llama3.1 model to reduce model size and improve performance for
on-device use cases.</p>
</div>
<p dir="auto">For more details on quantization and what settings to use for your use
case visit our <a href="https://github.com/pytorch/torchchat/blob/main/docs/model_customization.md">customization guide</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deploy and run on Desktop</h3><a id="user-content-deploy-and-run-on-desktop" aria-label="Permalink: Deploy and run on Desktop" href="#deploy-and-run-on-desktop"></a></p>
<p dir="auto">While ExecuTorch does not focus on desktop inference, it is capable
of doing so. This is handy for testing out PTE
models without sending them to a physical device.</p>
<p dir="auto">Specifically there are 2 ways of doing so: Pure Python and via a Runner</p>
<details>
<summary>Deploying via Python</summary>
<div data-snippet-clipboard-copy-content="# Execute
python3 torchchat.py generate llama3.1 --device cpu --pte-path llama3.1.pte --prompt &quot;Hello my name is&quot;"><pre><code># Execute
python3 torchchat.py generate llama3.1 --device cpu --pte-path llama3.1.pte --prompt "Hello my name is"
</code></pre></div>
</details>
<details>
<summary>Deploying via a Runner</summary>
<p dir="auto">Build the runner</p>
<div dir="auto" data-snippet-clipboard-copy-content="scripts/build_native.sh et"><pre>scripts/build_native.sh et</pre></div>
<p dir="auto">Execute using the runner</p>
<div dir="auto" data-snippet-clipboard-copy-content="cmake-out/et_run llama3.1.pte -z `python3 torchchat.py where llama3.1`/tokenizer.model -l 3 -i &quot;Once upon a time&quot;"><pre>cmake-out/et_run llama3.1.pte -z <span><span>`</span>python3 torchchat.py where llama3.1<span>`</span></span>/tokenizer.model -l 3 -i <span><span>"</span>Once upon a time<span>"</span></span></pre></div>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deploy and run on iOS</h3><a id="user-content-deploy-and-run-on-ios" aria-label="Permalink: Deploy and run on iOS" href="#deploy-and-run-on-ios"></a></p>
<p dir="auto">The following assumes you've completed the steps for <a href="#set-up-executorch">Setting up ExecuTorch</a>.</p>
<details>
<summary>Deploying with Xcode</summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Requirements</h4><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li><a href="https://apps.apple.com/us/app/xcode/id497799835?mt=12/" rel="nofollow">Xcode</a> 15.0 or later</li>
<li><a href="https://cmake.org/download/" rel="nofollow">Cmake</a> 3.19 or later
<ul dir="auto">
<li>Download and open the macOS <code>.dmg</code> installer and move the Cmake app to <code>/Applications</code> folder.</li>
<li>Install Cmake command line tools: <code>sudo /Applications/CMake.app/Contents/bin/cmake-gui --install</code></li>
</ul>
</li>
<li>A development provisioning profile with the <a href="https://developer.apple.com/documentation/bundleresources/entitlements/com_apple_developer_kernel_increased-memory-limit" rel="nofollow"><code>increased-memory-limit</code></a> entitlement.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Steps</h4><a id="user-content-steps" aria-label="Permalink: Steps" href="#steps"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Open the Xcode project:</p>
<div dir="auto" data-snippet-clipboard-copy-content="open et-build/src/executorch/examples/demo-apps/apple_ios/LLaMA/LLaMA.xcodeproj"><pre>open et-build/src/executorch/examples/demo-apps/apple_ios/LLaMA/LLaMA.xcodeproj</pre></div>
<blockquote>
<p dir="auto">Note: If you're running into any issues related to package dependencies, close Xcode, clean some of the caches and/or the build products, and open the Xcode project again:</p>
<div dir="auto" data-snippet-clipboard-copy-content="rm -rf \
  ~/Library/org.swift.swiftpm \
  ~/Library/Caches/org.swift.swiftpm \
  ~/Library/Caches/com.apple.dt.Xcode \
  ~/Library/Developer/Xcode/DerivedData"><pre>rm -rf \
  <span>~</span>/Library/org.swift.swiftpm \
  <span>~</span>/Library/Caches/org.swift.swiftpm \
  <span>~</span>/Library/Caches/com.apple.dt.Xcode \
  <span>~</span>/Library/Developer/Xcode/DerivedData</pre></div>
</blockquote>
</li>
<li>
<p dir="auto">Click the Play button to launch the app in the Simulator.</p>
</li>
<li>
<p dir="auto">To run on a device, ensure you have it set up for development and a provisioning profile with the <code>increased-memory-limit</code> entitlement. Update the app's bundle identifier to match your provisioning profile with the required capability.</p>
</li>
<li>
<p dir="auto">After successfully launching the app, copy the exported ExecuTorch model (<code>.pte</code>) and tokenizer (<code>.model</code>) files to the iLLaMA folder. You can find the model file called <code>llama3.1.pte</code> in the current <code>torchchat</code> directory and the tokenizer file at <code>$(python3 torchchat.py where llama3.1)/tokenizer.model</code> path.</p>
<ul dir="auto">
<li><strong>For the Simulator:</strong> Drag and drop both files onto the Simulator window and save them in the <code>On My iPhone &gt; iLLaMA</code> folder.</li>
<li><strong>For a device:</strong> Open a separate Finder window, navigate to the Files tab, drag and drop both files into the iLLaMA folder, and wait for the copying to finish.</li>
</ul>
</li>
<li>
<p dir="auto">Follow the app's UI guidelines to select the model and tokenizer files from the local filesystem and issue a prompt.</p>
</li>
</ol>
<p dir="auto"><em>Click the image below to see it in action!</em></p>
<p dir="auto">
<a href="https://pytorch.org/executorch/main/_static/img/llama_ios_app.mp4" rel="nofollow">
  <img src="https://camo.githubusercontent.com/e6975bf38cc4ad360ada871f6d7efc66c2d2f485c33317107c061ea70199d500/68747470733a2f2f7079746f7263682e6f72672f6578656375746f7263682f6d61696e2f5f7374617469632f696d672f6c6c616d615f696f735f6170702e706e67" width="600" alt="iOS app running a LlaMA model" data-canonical-src="https://pytorch.org/executorch/main/_static/img/llama_ios_app.png">
</a>
</p>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deploy and run on Android</h3><a id="user-content-deploy-and-run-on-android" aria-label="Permalink: Deploy and run on Android" href="#deploy-and-run-on-android"></a></p>
<p dir="auto">The following assumes you've completed the steps for <a href="#set-up-executorch">Setting up ExecuTorch</a>.</p>
<details>
<summary>Approach 1 (Recommended): Android Studio</summary>
<p dir="auto"><h4 tabindex="-1" dir="auto">Requirements</h4><a id="user-content-requirements-1" aria-label="Permalink: Requirements" href="#requirements-1"></a></p>
<ul dir="auto">
<li>Android Studio</li>
<li><a href="https://developer.android.com/build/jdks" rel="nofollow">Java 17</a></li>
<li><a href="https://developer.android.com/about/versions/14/setup-sdk" rel="nofollow">Android SDK 34</a></li>
<li><a href="https://developer.android.com/tools/adb" rel="nofollow">adb</a></li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Steps</h4><a id="user-content-steps-1" aria-label="Permalink: Steps" href="#steps-1"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Download the AAR file, which contains the Java library and corresponding JNI library, to build and run the app.</p>
<ul dir="auto">
<li><a href="https://ossci-android.s3.amazonaws.com/executorch/main/executorch-llama-tiktoken-rc3-0719.aar" rel="nofollow">executorch-llama-tiktoken-rc3-0719.aar</a> (SHASUM: c3e5d2a97708f033c2b1839a89f12f737e3bbbef)</li>
</ul>
</li>
<li>
<p dir="auto">Rename the downloaded AAR file to <code>executorch.aar</code> and move the file to <code>android/torchchat/app/libs/</code>. You may need to create directory <code>android/torchchat/app/libs/</code> if it does not exist.</p>
</li>
<li>
<p dir="auto">Push the model and tokenizer file to your device. You can find the model file called <code>llama3.1.pte</code> in the current <code>torchchat</code> directory and the tokenizer file at <code>$(python3 torchchat.py where llama3.1)/tokenizer.model</code> path.</p>
<div data-snippet-clipboard-copy-content="adb shell mkdir -p /data/local/tmp/llama
adb push <model.pte> /data/local/tmp/llama
adb push <tokenizer.model or tokenizer.bin> /data/local/tmp/llama"><pre><code>adb shell mkdir -p /data/local/tmp/llama
adb push &lt;model.pte&gt; /data/local/tmp/llama
adb push &lt;tokenizer.model or tokenizer.bin&gt; /data/local/tmp/llama
</code></pre></div>
</li>
<li>
<p dir="auto">Use Android Studio to open the torchchat app skeleton, located at <code>android/torchchat</code>.</p>
</li>
<li>
<p dir="auto">Click the Play button (^R) to launch it to emulator/device.</p>
<ul dir="auto">
<li>We recommend using a device with at least 12GB RAM and 20GB storage.</li>
<li>If using an emulated device, refer to <a href="https://stackoverflow.com/questions/45517553/cant-change-the-ram-size-in-avd-manager-android-studio" rel="nofollow">this post</a> on how to set the RAM.</li>
</ul>
</li>
<li>
<p dir="auto">Follow the app's UI guidelines to pick the model and tokenizer files from the local filesystem. Then issue a prompt.</p>
</li>
</ol>
<p dir="auto"><strong>Note:</strong> The AAR file listed in Step 1 has the tiktoken tokenizer, which is used for Llama 3. To tweak or use a custom tokenizer and runtime, modify the ExecuTorch code
and use <a href="https://github.com/pytorch/executorch/blob/main/build/build_android_llm_demo.sh">this script</a> to build the AAR library.</p>
<p dir="auto">
    <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8cf1cc62d4896d94efd214dfee81d28fc4065286236c10dd2b4cf4b0ca548581/68747470733a2f2f7079746f7263682e6f72672f6578656375746f7263682f6d61696e2f5f7374617469632f696d672f616e64726f69645f6c6c616d615f6170702e706e67"><img src="https://camo.githubusercontent.com/8cf1cc62d4896d94efd214dfee81d28fc4065286236c10dd2b4cf4b0ca548581/68747470733a2f2f7079746f7263682e6f72672f6578656375746f7263682f6d61696e2f5f7374617469632f696d672f616e64726f69645f6c6c616d615f6170702e706e67" width="600" alt="Android app running a LlaMA model" data-canonical-src="https://pytorch.org/executorch/main/_static/img/android_llama_app.png"></a>
</p>
</details>
<details>
<summary>Approach 2: E2E Script</summary>
<p dir="auto">Alternatively, you can run <code>scripts/android_example.sh</code> which sets up Java, Android SDK Manager, Android SDK, Android emulator (if no physical device is found), builds the app, and launches it for you. It can be used if you don't have a GUI.</p>
<div data-snippet-clipboard-copy-content="export TORCHCHAT_ROOT=$(pwd)
export USE_TIKTOKEN=ON # Set this only for tiktoken tokenizer
sh scripts/android_example.sh"><pre><code>export TORCHCHAT_ROOT=$(pwd)
export USE_TIKTOKEN=ON # Set this only for tiktoken tokenizer
sh scripts/android_example.sh
</code></pre></div>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Eval</h2><a id="user-content-eval" aria-label="Permalink: Eval" href="#eval"></a></p>
<p dir="auto"><strong>Note: This feature is still a work in progress and not all features are working</strong></p>
<p dir="auto">Uses the lm_eval library to evaluate model accuracy on a variety of
tasks. Defaults to wikitext and can be manually controlled using the
tasks and limit args. See <a href="https://github.com/pytorch/torchchat/blob/main/docs/evaluation.md">Evaluation</a></p>
<p dir="auto"><strong>Examples</strong></p>
<p dir="auto">Eager mode:</p>
<div data-snippet-clipboard-copy-content="python3 torchchat.py eval llama3.1 --dtype fp32 --limit 5"><pre><code>python3 torchchat.py eval llama3.1 --dtype fp32 --limit 5
</code></pre></div>
<p dir="auto">To test the perplexity for a lowered or quantized model, pass it in
the same way you would to generate:</p>
<div data-snippet-clipboard-copy-content="python3 torchchat.py eval llama3.1 --pte-path llama3.1.pte --limit 5"><pre><code>python3 torchchat.py eval llama3.1 --pte-path llama3.1.pte --limit 5
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Models</h2><a id="user-content-models" aria-label="Permalink: Models" href="#models"></a></p>
<p dir="auto">The following models are supported by torchchat and have associated
aliases.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model</th>
<th>Mobile Friendly</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct" rel="nofollow">meta-llama/Meta-Llama-3.1-8B-Instruct</a></td>
<td>✅</td>
<td>Tuned for <code>chat</code> . Alias to <code>llama3.1</code>.</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B" rel="nofollow">meta-llama/Meta-Llama-3.1-8B</a></td>
<td>✅</td>
<td>Best for <code>generate</code>. Alias to <code>llama3.1-base</code>.</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" rel="nofollow">meta-llama/Meta-Llama-3-8B-Instruct</a></td>
<td>✅</td>
<td>Tuned for <code>chat</code> . Alias to <code>llama3</code>.</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B" rel="nofollow">meta-llama/Meta-Llama-3-8B</a></td>
<td>✅</td>
<td>Best for <code>generate</code>. Alias to <code>llama3-base</code>.</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf" rel="nofollow">meta-llama/Llama-2-7b-chat-hf</a></td>
<td>✅</td>
<td>Tuned for <code>chat</code>. Alias to <code>llama2</code>.</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf" rel="nofollow">meta-llama/Llama-2-13b-chat-hf</a></td>
<td></td>
<td>Tuned for <code>chat</code>. Alias to <code>llama2-13b-chat</code>.</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama/Llama-2-70b-chat-hf" rel="nofollow">meta-llama/Llama-2-70b-chat-hf</a></td>
<td></td>
<td>Tuned for <code>chat</code>. Alias to <code>llama2-70b-chat</code>.</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama/Llama-2-7b-hf" rel="nofollow">meta-llama/Llama-2-7b-hf</a></td>
<td>✅</td>
<td>Best for <code>generate</code>. Alias to <code>llama2-base</code>.</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama/CodeLlama-7b-Python-hf" rel="nofollow">meta-llama/CodeLlama-7b-Python-hf</a></td>
<td>✅</td>
<td>Tuned for Python and <code>generate</code>. Alias to <code>codellama</code>.</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama/CodeLlama-34b-Python-hf" rel="nofollow">meta-llama/CodeLlama-34b-Python-hf</a></td>
<td>✅</td>
<td>Tuned for Python and <code>generate</code>. Alias to <code>codellama-34b</code>.</td>
</tr>
<tr>
<td><a href="https://huggingface.co/mistralai/Mistral-7B-v0.1" rel="nofollow">mistralai/Mistral-7B-v0.1</a></td>
<td>✅</td>
<td>Best for <code>generate</code>. Alias to <code>mistral-7b-v01-base</code>.</td>
</tr>
<tr>
<td><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1" rel="nofollow">mistralai/Mistral-7B-Instruct-v0.1</a></td>
<td>✅</td>
<td>Tuned for <code>chat</code>. Alias to <code>mistral-7b-v01-instruct</code>.</td>
</tr>
<tr>
<td><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2" rel="nofollow">mistralai/Mistral-7B-Instruct-v0.2</a></td>
<td>✅</td>
<td>Tuned for <code>chat</code>. Alias to <code>mistral</code>.</td>
</tr>
<tr>
<td><a href="https://huggingface.co/karpathy/tinyllamas/tree/main" rel="nofollow">tinyllamas/stories15M</a></td>
<td>✅</td>
<td>Toy model for <code>generate</code>. Alias to <code>stories15M</code>.</td>
</tr>
<tr>
<td><a href="https://huggingface.co/karpathy/tinyllamas/tree/main" rel="nofollow">tinyllamas/stories42M</a></td>
<td>✅</td>
<td>Toy model for <code>generate</code>. Alias to <code>stories42M</code>.</td>
</tr>
<tr>
<td><a href="https://huggingface.co/karpathy/tinyllamas/tree/main" rel="nofollow">tinyllamas/stories110M</a></td>
<td>✅</td>
<td>Toy model for <code>generate</code>. Alias to <code>stories110M</code>.</td>
</tr>
<tr>
<td><a href="https://huggingface.co/openlm-research/open_llama_7b" rel="nofollow">openlm-research/open_llama_7b</a></td>
<td>✅</td>
<td>Best for <code>generate</code>. Alias to <code>open-llama</code>.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">While we describe how to use torchchat using the popular llama3 model,
you can perform the example commands with any of these models.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Design Principles</h2><a id="user-content-design-principles" aria-label="Permalink: Design Principles" href="#design-principles"></a></p>
<p dir="auto">torchchat embodies PyTorch’s design philosophy <a href="https://pytorch.org/docs/stable/community/design.html" rel="nofollow">details</a>, especially "usability over everything else".</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Native PyTorch</h3><a id="user-content-native-pytorch" aria-label="Permalink: Native PyTorch" href="#native-pytorch"></a></p>
<p dir="auto">torchchat is a native-PyTorch library. While we provide integrations with the surrounding ecosystem (eg: Hugging Face models, etc), all of the core functionality is written in PyTorch.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Simplicity and Extensibility</h3><a id="user-content-simplicity-and-extensibility" aria-label="Permalink: Simplicity and Extensibility" href="#simplicity-and-extensibility"></a></p>
<p dir="auto">torchchat is designed to be easy to understand, use and extend.</p>
<ul dir="auto">
<li>Composition over implementation inheritance - layers of inheritance for code re-use makes the code hard to read and extend</li>
<li>No training frameworks - explicitly outlining the training logic makes it easy to extend for custom use cases</li>
<li>Code duplication is preferred over unnecessary abstractions</li>
<li>Modular building blocks over monolithic components</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Correctness</h3><a id="user-content-correctness" aria-label="Permalink: Correctness" href="#correctness"></a></p>
<p dir="auto">torchchat provides well-tested components with a high-bar on correctness.
We provide</p>
<ul dir="auto">
<li>Extensive unit-tests to ensure things operate as they should</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Community Contributions</h2><a id="user-content-community-contributions" aria-label="Permalink: Community Contributions" href="#community-contributions"></a></p>
<p dir="auto">We really value our community and the contributions made by our wonderful users. We'll use this section to call out some of these contributions! If you'd like to help out as well, please see the <a href="https://github.com/pytorch/torchchat/blob/main/CONTRIBUTING.md">CONTRIBUTING</a> guide.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto"><strong>CERTIFICATE_VERIFY_FAILED</strong>
Run <code>pip install --upgrade certifi</code>.</p>
<p dir="auto"><strong>Access to model is restricted and you are not in the authorized list</strong>
Some models require an additional step to access. Follow the
link provided in the error to get access.</p>
<p dir="auto"><strong>Installing ET Fails</strong>
If <code>./scripts/install_et.sh</code> fails with an error like <code>Building wheel for executorch (pyproject.toml) did not run successfully</code> It's possible that it's linking to an older version of pytorch installed some other way like via homebrew. You can break the link by uninstalling other versions such as <code>brew uninstall pytorch</code> Note: You may break something that depends on this, so be aware.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Filing Issues</h2><a id="user-content-filing-issues" aria-label="Permalink: Filing Issues" href="#filing-issues"></a></p>
<p dir="auto">Please include the exact command you ran and the output of that command.
Also, run this script and include the output saved to <code>system_info.txt</code> so that we can better debug your issue.</p>
<div data-snippet-clipboard-copy-content="(echo &quot;Operating System Information&quot;; uname -a; echo &quot;&quot;; cat /etc/os-release; echo &quot;&quot;; echo &quot;Python Version&quot;; python --version || python3 --version; echo &quot;&quot;; echo &quot;PIP Version&quot;; pip --version || pip3 --version; echo &quot;&quot;; echo &quot;Installed Packages&quot;; pip freeze || pip3 freeze; echo &quot;&quot;; echo &quot;PyTorch Version&quot;; python -c &quot;import torch; print(torch.__version__)&quot; || python3 -c &quot;import torch; print(torch.__version__)&quot;; echo &quot;&quot;; echo &quot;Collection Complete&quot;) > system_info.txt"><pre><code>(echo "Operating System Information"; uname -a; echo ""; cat /etc/os-release; echo ""; echo "Python Version"; python --version || python3 --version; echo ""; echo "PIP Version"; pip --version || pip3 --version; echo ""; echo "Installed Packages"; pip freeze || pip3 freeze; echo ""; echo "PyTorch Version"; python -c "import torch; print(torch.__version__)" || python3 -c "import torch; print(torch.__version__)"; echo ""; echo "Collection Complete") &gt; system_info.txt
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Disclaimer</h2><a id="user-content-disclaimer" aria-label="Permalink: Disclaimer" href="#disclaimer"></a></p>
<p dir="auto">The torchchat Repository Content is provided without any guarantees
about performance or compatibility. In particular, torchchat makes
available model architectures written in Python for PyTorch that may
not perform in the same manner or meet the same standards as the
original versions of those models. When using the torchchat Repository
Content, including any model architectures, you are solely responsible
for determining the appropriateness of using or redistributing the
torchchat Repository Content and assume any risks associated with your
use of the torchchat Repository Content or any models, outputs, or
results, both alone and in combination with any other
technologies. Additionally, you may have other legal obligations that
govern your use of other content, such as the terms of service for
third-party models, weights, data, or other technologies, and you are
solely responsible for complying with all such obligations.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<p dir="auto">Thank you to the community for all the
awesome libraries and tools you've built around local LLM inference.</p>
<ul dir="auto">
<li>
<p dir="auto">Georgi Gerganov and his <a href="https://github.com/ggerganov/ggml">GGML</a>
project shining a spotlight on community-based enablement and
inspiring so many other projects.</p>
</li>
<li>
<p dir="auto">Andrej Karpathy and his
<a href="https://github.com/karpathy/llama2.c">llama2.c</a> project.  So many
great (and simple!) ideas in llama2.c that we have directly adopted
(both ideas and code) from his repo.  You can never go wrong by
following Andrej's work.</p>
</li>
<li>
<p dir="auto">Michael Gschwind, Bert Maher, Scott Wolchok, Bin Bao, Chen Yang,
Huamin Li and Mu-Chu Li who built the first version of nanogpt (<code>DSOGPT</code>)
with AOT Inductor proving that AOTI can be used to build efficient
LLMs, and DSOs are a viable distribution format for models.
<a href="https://github.com/karpathy/nanoGPT">nanoGPT</a>.</p>
</li>
<li>
<p dir="auto">Bert Maher and his
<a href="https://github.com/bertmaher/llama2.so">llama2.so</a>, which built on
Andrej's llama2.c and on DSOGPT to close the loop on Llama models
with AOTInductor.</p>
</li>
<li>
<p dir="auto">Christian Puhrsch, Horace He, Joe Isaacson and many more for their
many contributions in Accelerating GenAI models in the <em>"Anything,
Fast!"</em> pytorch.org blogs, and, in particular, Horace He for <a href="https://github.com/pytorch-labs/gpt-fast">GPT,
Fast!</a>, which we have
directly adopted (both ideas and code) from his repo.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">torchchat is released under the <a href="https://github.com/pytorch/torchchat/blob/main/LICENSE">BSD 3 license</a>. (Additional
code in this distribution is covered by the MIT and Apache Open Source
licenses.) However you may have other legal obligations that govern
your use of content, such as the terms of service for third-party
models.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Differences in cancer rates among adults born between 1920 and 1990 (108 pts)]]></title>
            <link>https://www.thelancet.com/journals/lanpub/article/PIIS2468-2667(24)00156-7/fulltext</link>
            <guid>41125954</guid>
            <pubDate>Thu, 01 Aug 2024 03:41:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thelancet.com/journals/lanpub/article/PIIS2468-2667(24)00156-7/fulltext">https://www.thelancet.com/journals/lanpub/article/PIIS2468-2667(24)00156-7/fulltext</a>, See on <a href="https://news.ycombinator.com/item?id=41125954">Hacker News</a></p>
Couldn't get https://www.thelancet.com/journals/lanpub/article/PIIS2468-2667(24)00156-7/fulltext: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Building a Local Perplexity Alternative with Perplexica, Ollama, and SearXNG (128 pts)]]></title>
            <link>https://jointerminus.medium.com/building-a-local-perplexity-alternative-with-perplexica-ollama-and-searxng-71602523e256</link>
            <guid>41125919</guid>
            <pubDate>Thu, 01 Aug 2024 03:36:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jointerminus.medium.com/building-a-local-perplexity-alternative-with-perplexica-ollama-and-searxng-71602523e256">https://jointerminus.medium.com/building-a-local-perplexity-alternative-with-perplexica-ollama-and-searxng-71602523e256</a>, See on <a href="https://news.ycombinator.com/item?id=41125919">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h2 id="8ab3" data-testid="storyTitle">Building a Local Perplexity Alternative with Perplexica, Ollama, and SearXNG</h2><div><a rel="noopener follow" href="https://jointerminus.medium.com/?source=post_page-----71602523e256--------------------------------"><div aria-hidden="false"><p><img alt="Terminus" src="https://miro.medium.com/v2/resize:fill:88:88/1*TxKrA9t3wlfWX9755MK2Jg.png" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div></div><figure></figure><p id="547d">You’ve probably heard of Perplexity, the AI search engine that’s been making waves. Compared to traditional searching, it provides concise, synthesized answers to queries rather than just a list of links to boost efficiency and productivity. However, Perplexity’s unique capability comes with a price tag — $20 a month for the Pro Search, with free users limited to just 5 pro searches a day.</p><p id="846a">But what if you could build something similar on your own hardware, without monthly fees or usage limits? In this guide, we’ll walk you through the process of creating your local Perplexity alternative using <a href="https://github.com/beclab/terminus" rel="noopener ugc nofollow" target="_blank">Terminus</a>, an open-source, self-hosted operating system based on Kubernetes, along with other powerful open-source AI tools.</p><figure></figure><h2 id="da84">Recreating Perplexity’s workflow</h2><p id="ad47">Before we dive into our self-hosted solution, let’s briefly explore how Perplexity works. Note that this is a simplified version, the actual workflow is much more sophisticated.</p><figure><figcaption>Perplexity workflow</figcaption></figure><ol><li id="68bb">User sends a question to Perplexity.</li><li id="b419">Perplexity understands the question and sends it to search engines like Google</li><li id="5f79">Google searches the web in real-time.</li><li id="0ba0">Google returns sorted results to Perplexity.</li><li id="5ed2">Perplexity creates a prompt using the question and results and sends it to an AI model like OpenAI’s GPT-4.</li><li id="bf13">AI compiles everything into a coherent answer.</li></ol><p id="e5a6">This workflow boils down to three key components:</p><ul><li id="db6c">Perplexity as the AI search platform and interface.</li><li id="b422">Google as the search engine.</li><li id="4377">OpenAI as the AI model provider.</li></ul><p id="c23c">To recreate this, we will replace them with open source tools:</p><figure><figcaption>Local AI search engine workflow</figcaption></figure><ul><li id="5c98"><a href="https://github.com/ollama/ollama" rel="noopener ugc nofollow" target="_blank"><strong>Ollama</strong></a>: A popular open-source project that enables users to host large language models like Gemma2.</li><li id="142a"><a href="https://github.com/searxng/searxng" rel="noopener ugc nofollow" target="_blank"><strong>SearXNG</strong></a>: An open-source, privacy-respecting internet metasearch engine, fetching real-time results from the web.</li><li id="edff"><a href="https://github.com/ItzCrazyKns/Perplexica" rel="noopener ugc nofollow" target="_blank"><strong>Perplexica</strong></a>: An AI-powered search engine that ties everything together.</li></ul><h2 id="ef1a">Why deploy on Terminus?</h2><p id="0fe1">Compared to regular installation methods, Terminus provides a more streamlined, user-friendly experience for users looking to self-host applications:</p><ul><li id="6033"><strong>Simple and fast deployment<br></strong>Terminus offers one-click installations for apps like Ollama and Perplexica through its marketplace. This allows for rapid deployment and testing of new projects.</li><li id="0f66"><strong>Flexible app and service combination<br></strong>Applications and services in Terminus can be deployed once and flexibly assembled as needed. This allows for easy experimentation with different configurations and service combinations without complex redeployments.</li><li id="98ad"><strong>Anywhere access with a unique domain name.<br></strong>Terminus provides a dedicated domain name for each application and service out-of-the-box, enabling access from any device with a browser.</li></ul><h2 id="b6b5">Prerequisites</h2><ul><li id="1b28">A Windows (Windows 10 or 11) or Linux PC, with a Nvidia GPU</li><li id="f856">Terminus installed. Please refer to the <a href="https://docs.jointerminus.com/overview/introduction/getting-started/windows.html" rel="noopener ugc nofollow" target="_blank">Quick Start</a> guides to install Terminus.</li></ul><h2 id="9905">Step-by-step guide</h2><p id="0603">Take the steps below to build your local AI engine on Terminus.</p><h2 id="5a40">Step1: Launch Terminus</h2><p id="108a">Launch Terminus from your browser, and open Terminus Market from the desktop.</p><blockquote><p id="c59d">N<strong><em>ote</em></strong><em>: You can access Terminus using your personal domain name in any browser.</em></p></blockquote><figure><figcaption>Terminus Market</figcaption></figure><h2 id="5290">Step 2: Install the apps</h2><ol><li id="c318">In <strong>Market</strong>, find Ollama, SearXNG, and Perplexica.</li><li id="e9bd">Click <strong>Get</strong> under each app to install them. Wait until installation finishes.</li></ol><h2 id="2592">Step3: Configure Ollama</h2><p id="d409">Configure Ollama with the AI model. For our setup here, we will use Gemma2, Google’s latest open-source language model.</p><figure><figcaption>Configure Ollama</figcaption></figure><ol><li id="216a">Open Ollama from launcher.</li><li id="248e">Navigate to <strong>Admin Panel</strong> &gt; <strong>Models</strong> &gt; <strong>Pull a model from </strong><a href="http://ollama.com/" rel="noopener ugc nofollow" target="_blank"><strong>Ollama.com</strong></a>, and select <strong>gemma2:27b</strong> as the target model to pull.</li><li id="0daf">Click the download button on the right to start pulling the model. Exit the app when the pulling completes.</li></ol><h2 id="b758">Step 4: Configure Perplexica</h2><p id="339c">We are almost ready. Let’s configure Perplexica, the glue that ties everything together.</p><ol><li id="7900">Open Perplexica from launcher.</li><li id="b76f">In the <strong>Settings</strong> window, specify as shown in the screenshot.</li></ol><figure><figcaption>Configure Perplexica</figcaption></figure><p id="de3f">Since we have already installed Ollama with Gemma2, they will appear as default options here. Congratulations, you just get your self-hosted Perplexity alternative.</p><h2 id="e67f">Step 5: Test your local AI search engine</h2><p id="9bc7">Let’s try by asking the same question on Perplexica and Perplexity and make an apple-to-apple comparison.</p><figure><figcaption>Answer by Perplexica</figcaption></figure><figure><figcaption>Answer by Perplexity</figcaption></figure><p id="d603">As you can see, we asked the same question “why selfhsoting”. While the processing time, info length and details vary due to different models and tech stacks, the answer by Perplexica is acceptable considering the benefits of self-hosting.</p><blockquote><p id="1db6">T<strong><em>ip</em></strong><em>: you can access your self-hosted Perplexica from your phone anywhere, thanks to the dedicated domain provided by Terminus.</em></p></blockquote><h2 id="92b8">Conclusion</h2><p id="8275">By leveraging Terminus and open-source tools like Ollama, SearXNG, and Perplexica, you can create a powerful, self-hosted AI search engine similar to commercial offerings like Perplexity. This setup not only saves you money but also gives you control over your data and search experience.</p><p id="c2c7">We encourage you to experiment with your setup, try different language models, and customize your AI search engine to fit your unique needs.</p><p id="84a6">In our future sessions, we’ll explore setting up other exciting AI projects like Dify and Open WebUI on Terminus. In the meantime, you can keep an eye on the <a href="https://github.com/beclab/terminus" rel="noopener ugc nofollow" target="_blank">Terminus project</a> on GitHub for updates and new features if you like what we are doing.</p><p id="a7c2">Happy self-hosting!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Genomic Code: The genome instantiates a generative model of the organism (135 pts)]]></title>
            <link>https://arxiv.org/abs/2407.15908</link>
            <guid>41125648</guid>
            <pubDate>Thu, 01 Aug 2024 02:25:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2407.15908">https://arxiv.org/abs/2407.15908</a>, See on <a href="https://news.ycombinator.com/item?id=41125648">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2407.15908">View PDF</a></p><blockquote>
            <span>Abstract:</span>How does the genome encode the form of the organism? What is the nature of this genomic code? Common metaphors, such as a blueprint or program, fail to capture the complex, indirect, and evolutionarily dynamic relationship between the genome and organismal form, or the constructive, interactive processes that produce it. Such metaphors are also not readily formalised, either to treat empirical data or to simulate genomic encoding of form in silico. Here, we propose a new analogy, inspired by recent work in machine learning and neuroscience: that the genome encodes a generative model of the organism. In this scheme, by analogy with variational autoencoders, the genome does not encode either organismal form or developmental processes directly, but comprises a compressed space of latent variables. These latent variables are the DNA sequences that specify the biochemical properties of encoded proteins and the relative affinities between trans-acting regulatory factors and their target sequence elements. Collectively, these comprise a connectionist network, with weights that get encoded by the learning algorithm of evolution and decoded through the processes of development. The latent variables collectively shape an energy landscape that constrains the self-organising processes of development so as to reliably produce a new individual of a certain type, providing a direct analogy to Waddingtons famous epigenetic landscape. The generative model analogy accounts for the complex, distributed genetic architecture of most traits and the emergent robustness and evolvability of developmental processes. It also provides a new way to explain the independent selectability of specific traits, drawing on the idea of multiplexed disentangled representations observed in artificial and neural systems and lends itself to formalisation.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Kevin Mitchell [<a href="https://arxiv.org/show-email/20ca0445/2407.15908">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 22 Jul 2024 16:41:25 UTC (3,278 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Does OpenAI Survive (117 pts)]]></title>
            <link>https://www.wheresyoured.at/to-serve-altman/</link>
            <guid>41125630</guid>
            <pubDate>Thu, 01 Aug 2024 02:18:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wheresyoured.at/to-serve-altman/">https://www.wheresyoured.at/to-serve-altman/</a>, See on <a href="https://news.ycombinator.com/item?id=41125630">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      <p>Throughout the last year I’ve written in detail about the rot in tech — <a href="https://www.wheresyoured.at/sam-altman-is-full-of-shit/"><u>the spuriousness of charlatans looking to accumulate money and power</u></a>, <a href="https://www.wheresyoured.at/rotcombubble/"><u>the desperation of the most powerful executives to maintain control and rapacious growth</u></a>, <a href="https://www.wheresyoured.at/pop-culture/"><u>and the speciousness of the latest hype cycle</u></a> — but at the end of the day, these are just companies, which leads to a very simple question: can the largest, most prominent company in tech’s latest hype cycle actually survive?&nbsp;</p><p>I am, of course, talking about OpenAI. Regulars to this newsletter will know that I’m highly skeptical of OpenAI’s product, its business model, and its sustainability. While I don’t want to rehash the arguments made in previous newsletters and podcasts, here’s the crux of the matter: generative AI is a product with no mass-market utility - at least on the scale of truly revolutionary movements like the original cloud computing and smartphone booms - and it’s one that costs an eye-watering amount to build and run.&nbsp;</p><p>Those two factors raise genuine questions about OpenAI’s ability to exist on a medium-to-long term, especially if — or, if I may be so bold to say, when — the sluice of investment money and cloud computing credits dries up.</p><p>I don't have all the answers. I don't know every part of every deal that informs every part of every aspect of generative AI. I am neither an engineer nor an economist, nor do I have privileged information. However, I do have the ability to read publicly-available data, as well as evaluate the independent reporting of respected journalists and the opinions of well-informed experts and academics, and come to conclusions as a result.</p><p>I am hypothesizing that for OpenAI to survive for longer than two years, it will have to (in no particular order):</p><ul><li>Successfully navigate a convoluted and onerous relationship with Microsoft, one that exists both as a lifeline and a direct source of competition.</li><li>Raise more money than any startup has ever raised in history, and continue to do so at a pace totally unseen in the history of financing.</li><li>Have a significant technological breakthrough such that it reduces the costs of building and operating GPT — or whatever model that succeeds it — by a factor of thousands of percent.</li><li>Have such a significant technological breakthrough that GPT is able to take on entirely unseen new use cases, ones that are not currently possible or hypothesized as possible by any artificial intelligence researchers.</li><li>Have these use cases be ones that are capable of both creating new jobs and entirely automating existing ones in such a way that it will validate the massive capital expenditures and infrastructural investment necessary to continue.</li></ul><p>I ultimately believe that OpenAI in its current form is untenable. There is no path to profitability, the burn rate is too high, and generative AI as a technology requires too much energy for the power grid to sustain it, and training these models is equally untenable, both as a result of ongoing legal issues (as a result of theft) and the amount of training data necessary to develop them.</p><p>And, quite simply, any technology requiring hundreds of billions of dollars to prove itself is built upon bad architecture. There is no historical precedent for anything that OpenAI needs to happen. Nobody has ever raised the amount of money it will need, nor has a piece of technology required such an incredible financial and systemic force — such as rebuilding the American power grid — to <em>survive,</em> let alone <em>prove itself as a technology worthy of such investment.</em></p><p>To be clear, this piece is focused on OpenAI rather than Generative AI as a technology — though I believe OpenAI's continued existence is necessary to keep companies interested/invested in the industry at all. OpenAI has raised the most money of any generative AI company (<a href="https://www.crunchbase.com/organization/openai?ref=wheresyoured.at"><u>$11.3 billion</u></a>), has arguably the most attention in the press, and both popularized and created the Large Language Mode (LLM)l business model that allowed the current generation of "AI-powered" startups to exist.</p><p>What I am <em>not</em> saying is that OpenAI will for sure collapse, or that generative AI will definitively fail.<a href="https://www.wheresyoured.at/pop-culture/"><u> I have exhaustively discussed the problems with this industry</u></a> in the past, and I won't reiterate those points other than to illustrate what I believe is a deep instability in the tech ecosystem, and my point here is to coldly explain why OpenAI, in its current form, cannot survive longer than a few more years without a stunning confluence of technological breakthroughs and financial wizardry, some of which is possible, much of which has no historic precedence.</p><p>Let's have a look, shall we?</p><h2 id="the-microsoft-and-valuation-problem"><strong>The Microsoft (and Valuation) Problem</strong></h2><p>Before we continue, I should note that OpenAI and Microsoft have not publicly-disclosed the terms of their deals, and there may be things I do not know that change this picture.</p><p>Regardless, OpenAI's relationship with Microsoft is deeply strange,<a href="https://openai.com/index/microsoft-invests-in-and-partners-with-openai/?ref=wheresyoured.at"><u> starting with a $1 billion cash infusion in 2019</u></a>, framed as a "<a href="https://techcrunch.com/2019/07/22/microsoft-invests-1-billion-in-openai-in-new-multiyear-partnership/?ref=wheresyoured.at"><u>multiyear exclusive computing partnership</u></a>" that "port[ed OpenAi's] existing services to work on Azure" (Microsoft's cloud computing product) and made Microsoft "OpenAI's preferred partner." OpenAI CTO Greg<a href="https://news.ycombinator.com/item?id=20498087&amp;ref=wheresyoured.at"><u> Brockman added at the time</u></a> that this was a "cash investment," but noted that OpenAI would "plan to be a big Azure customer."&nbsp;</p><p>One particular thing to note is that Brockman stated that Microsoft would<a href="https://news.ycombinator.com/item?id=20498087&amp;ref=wheresyoured.at#:~:text=pre%2DAGI%20technologies"><u> get access to sell OpenAI's pre-AGI products</u></a> based off of [OpenAI's research] to Microsoft's customers, and in the accompanying blog post added that Microsoft and OpenAI were "<a href="https://openai.com/index/microsoft-invests-in-and-partners-with-openai/?ref=wheresyoured.at#:~:text=develop%20new%20Azure%20AI%20supercomputing%20technologies"><u>jointly developing new Azure AI supercomputing technologies</u></a>."&nbsp;</p><p>Pre-AGI in this case refers to anything OpenAI has ever developed, as it has yet to develop AGI and has<a href="https://www.wheresyoured.at/put-up-or-shut-up/#:~:text=According%20to%20Bloomberg%2C%20these%20five%20levels%20go%20from%20Level%201%20(%22Chatbots%2C%20AI%20with%20conversational%20language%22)%20to"><u> yet to get past the initial "chatbot" stage of its own 5-level system of evaluating artificial intelligence</u></a>.</p><p>In essence, the terms of this funding round involved OpenAI handing over the research it had made to Microsoft, along with the ability for Redmond to sell OpenAI's technology as its own under the Azure banner. Furthermore, Microsoft has access to OpenAI's "pre-AGI product research," meaning that it is able to see exactly how it works, which would allow it to both sell the technology and directly compete with it. This is something that Microsoft is already working on, with<a href="https://www.theinformation.com/articles/meet-mai-1-microsoft-readies-new-ai-model-to-compete-with-google-openai?rc=kz8jh3&amp;ref=wheresyoured.at"><u> The Information reporting in May that Microsoft was readying its own "MAI-1" generative model</u></a>, run by Mustafa Suleyman, the co-founder of Deepmind (<a href="https://techcrunch.com/2014/01/26/google-deepmind/?ref=wheresyoured.at"><u>acquired by Google in 2014</u></a>),<a href="https://www.cnbc.com/2022/01/28/mustafa-suleyman-deepmind-co-founder-quits-google-ai-role-to-be-vc.html?ref=wheresyoured.at"><u> which he left in 2022</u></a> to become a VC. Suleyman later went on to found Inflection ,<a href="https://www.fastcompany.com/91069182/microsoft-inflection-ai-exclusive?ref=wheresyoured.at"><u> a transformer-based chatbot company that was sort-of acquired by Microsoft in March</u></a>, and <a href="https://www.reuters.com/technology/inflection-ai-raises-13-bln-funding-microsoft-others-2023-06-29/?ref=wheresyoured.at"><u>which Microsoft backed</u></a> prior to its acquisition.</p><p>The Information also noted that the OpenAI partnership "<a href="https://www.theinformation.com/articles/microsoft-openai-inside-techs-hottest-romance?rc=kz8jh3&amp;ref=wheresyoured.at"><u>helped Microsoft get ahead of their rivals</u></a>." I am, again, hypothesizing, and the terms of this deal alone are extremely concerning for OpenAI in general.</p><p>Things now get a little confusing.</p><p>Apparently, in 2021 Microsoft made some sort of investment in OpenAI (<a href="https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/?ref=wheresyoured.at"><u>cited here in a 2023 blog post about <em>another </em>funding round I'll get to in a minute</u></a>). Confusingly, I cannot find many details on this round.<a href="https://www.crunchbase.com/funding_round/openai-secondary-market--ac1e85f3?ref=wheresyoured.at"><u> Crunchbase references a 2021 secondary market</u></a> offering of an undisclosed amount (meaning that then-current stockholders at OpenAI were able to sell liquidate their stock), but cites that the money came from Tiger Global Management, Sequoia Capital, Bedrock and Andreessen Horowitz, valuing the company at $14 billion.&nbsp;</p><p>It's unclear whether this was the same round, or if Microsoft otherwise infused capital.<a href="https://www.theinformation.com/briefings/microsoft-openai-new-multi-billion-dollar-investment?rc=kz8jh3&amp;ref=wheresyoured.at"><u> The Information mentions in an article from early 2023</u></a>, but somehow <em>nobody</em> covered it at the time. If I'm wrong, I'd love to see how it did so.</p><p>Regardless,<a href="https://www.cnbc.com/2023/01/10/microsoft-to-invest-10-billion-in-chatgpt-creator-openai-report-says.html?ref=wheresyoured.at"><u> in early 2023 Microsoft invested $10 billion in OpenAI</u></a> — but the most important parts of the deal are both its <em>terms</em> and its <em>distribution. </em>Though the terms of the deal aren't public, <a href="https://www.theverge.com/2023/1/23/23567448/microsoft-openai-partnership-extension-ai?ref=wheresyoured.at"><u>reports state that Microsoft will may receive 75% of OpenAI's profits until it secures "its investment return [a clunky way of saying "makes back the $10 billion it invested],"</u></a> along with a 49% stake in the company,<a href="https://www.wheresyoured.at/sam-altman-freed/"><u> though OpenAI's convoluted non-profit-for-profit structure is strange in and of itself</u></a>.&nbsp;</p><p><a href="https://www.semafor.com/article/11/18/2023/openai-has-received-just-a-fraction-of-microsofts-10-billion-investment?ref=wheresyoured.at"><u>Semafor reports that</u></a>, at least by November 2023, OpenAI had received "a fraction" of the $10 billion investment, which was (is?) delivered in tranches (stages), and that a "significant portion" of that money was in cloud compute credits, meaning that Microsoft's investment was predominantly in the supposed value of a currency that can only be used on its own services. For those who don’t fully understand how weird this is, it’s like an airline investing in a company but, instead of providing cash, it hands over air miles. You can still travel, but you are locked into A) one airline and B) their interpretation of what one “mile” is actually worth.</p><p>Furthermore, it’s extremely bizarre that said “investment” also may have - again, we don’t know the terms of the deal - allowed OpenAI to raise its valuation in the process.&nbsp;</p><p>Semafor also adds (vaguely) that Microsoft has "certain rights to OpenAI's intellectual property," and that it "would still be able to run OpenAI's current models on [its] servers" even if the relationship were to break down.</p><p>What's confusing is that <a href="https://www.cnbc.com/2023/04/08/microsofts-complex-bet-on-openai-brings-potential-and-uncertainty.html?ref=wheresyoured.at"><u>multiple</u></a><a href="https://www.bloomberg.com/news/articles/2024-06-28/microsoft-s-13-billion-openai-pact-faces-extra-eu-scrutiny?ref=wheresyoured.at"><u> reporters</u></a> have said that Microsoft had invested "$13 billion" in OpenAI, yet I can't find the two billion dollars anywhere. Was it in 2021? Was the amount $2 billion? Was it cash, or credits? This number has been reported so regularly for so long, and it's extremely strange that so many have just assumed this happened.</p><p>The reason I think this is concerning is that two billion dollars is a great deal of money by any startup's standards.</p><p>For example, Snowflake, a wildly-successful enterprise computing company,<a href="https://www.crunchbase.com/organization/snowflake-computing?ref=wheresyoured.at"><u> raised a total of $2 billion</u></a>, mostly before going public (though it sold $621.5 million in stock post-IPO in 2022). Also, Snowflake lost $316 million last quarter.</p><p>On top of that, this deal was totally unannounced and unreported, happening in the same year (2021) that Microsoft and OpenAI announced their Azure OpenAI services (<a href="https://azure.microsoft.com/en-us/blog/general-availability-of-azure-openai-service-expands-access-to-large-advanced-ai-models-with-added-enterprise-benefits/?ref=wheresyoured.at"><u>which took until 2023 to launch publicly</u></a>). While I am guessing here — if I'm wrong, please email me — it seems that Microsoft gave another $2 billion to OpenAI in 2021 at the same time that OpenAI raised an undisclosed amount from other sources. I hypothesize this deal is likely to have been a mixture of cash and cloud credits, though ChatGPT didn't reach the public until November 2022.&nbsp;</p><p>It could also be that those reporting that Microsoft had put "$13 billion" into OpenAI could simply be wrong, but it would be a very commonplace error and one that would likely quickly be refuted by either Microsoft or OpenAI.</p><p>The reason that I'm raising these issues is that Microsoft, at this point, effectively <em>owns</em> OpenAI.<a href="https://www.wsj.com/tech/ai/sam-altman-openai-protected-by-silicon-valley-friends-f3efcf68?ref=wheresyoured.at"><u> Microsoft CEO Satya Nadella was instrumental in Altman's return after he was fired in November 2023</u></a>, and<a href="https://www.techtarget.com/searchenterpriseai/news/366560318/Fallout-after-Microsoft-hires-former-OpenAI-CEO-Sam-Altman?ref=wheresyoured.at"><u> had already planned to poach him if he hadn't returned</u></a>. In many ways, this didn't really matter. Due to the nature of Microsoft's deal with OpenAI, it effectively owns — or at least has access to — all of the intellectual property behind OpenAI's products, along with all of the research, as well as the ability to license it at will.</p><p>OpenAI is inextricably tied to Microsoft. It is bound to use Azure, Microsoft's cloud compute platform, both by its agreements and the fact that the majority of its funding is in credits that can only be used on Microsoft Azure. Microsoft sells access to GPT through Azure, while also directly competing with it with its own upcoming model, and takes three-quarters of any of the (theoretical) profit that would come out of OpenAI's services.</p><p>Microsoft has also not had to really sacrifice anything to do these deals. Even if we assume that the <em>entirety</em> of the previous rounds of funding — a theoretical $3 billion — was in cash, and that, say, 25% of the 2023 deal was in cash versus credits, that's only $5.5 billion, with the latter half delivered in tranches on an indeterminate timeline. Assuming (again, I do not have the exact terms) that this was really $5.5 billion in cash, that's <em>nothing</em> for Microsoft, a company that had over $21 billion in <em>profits </em>in its most recent financial quarter.&nbsp;</p><p>As part of this deal, Microsoft has effectively purchased the rights to OpenAI's "pre-AGI" technology, and licensed all of its technology in a way that extends past any partnership or, I imagine, future deals. Microsoft also "invested" in cloud credits at an indeterminate valuation, both in how OpenAI was valued <em>and the credits themselves.&nbsp;</em></p><p>Ask yourself, what is a dollar of "cloud compute credits," and what do they gain you access to?<a href="https://azure.microsoft.com/en-us/pricing?ref=wheresyoured.at#Estimate-and-budget"><u> Microsoft's Azure cloud has many, many products,</u></a> and it's unclear if OpenAI would receive preferential pricing on them, what products they'd be using, and the terms under which OpenAI receives them.Microsoft effectively created its own currency to invest in OpenAI, which OpenAI would then pay Microsoft in, which Microsoft would, in turn, receive as revenue.</p><p>In many ways, OpenAI's continual existence is as an R&amp;D facility for Microsoft's generative AI business unit, one with the dice rigged in Microsoft's favor. In the event of OpenAI's collapse, OpenAI's technology would still run on Microsoft's servers, and Microsoft would still have access to both OpenAI's intellectual property and products, and in turn be able to sell them. In the event that OpenAI thrives and future generations of GPT become remarkably profitable and successful, Microsoft harvests billions of dollars of profits while still retaining access and license to any research or products used to get there. Even Microsoft's $100 billion supercomputer project is reportedly tied to Altman and OpenAI "meaningfully improving" the capabilities of its AI,<a href="https://www.theinformation.com/articles/microsoft-and-openai-plot-100-billion-stargate-ai-supercomputer?rc=kz8jh3&amp;ref=wheresyoured.at"><u> according to sources talking to The Information</u></a>.</p><p>I am obviously not certain, and have no way of confirming this, but do you not think “pre-AGI” technology and research includes <a href="https://www.theverge.com/2024/7/25/24205701/openai-searchgpt-ai-search-engine-google-perplexity-rival?ref=wheresyoured.at"><u>SearchGPT, OpenAI’s recently-announced competitor to Google Search</u></a>? Is it worth considering, for a second, that Microsoft could benefit whether OpenAI lives or dies?&nbsp;</p><p>It's a devil's deal, one that you would only make if you were burning so much cash that it was necessary to find a benefactor with deep pockets, one that could bail you out repeatedly as you chewed through billions of dollars every year.</p><p>Sadly, that may be the truth for OpenAI.</p><h2 id="the-funding-problem"><strong>The Funding Problem</strong></h2><p>Last week, <a href="https://www.theinformation.com/articles/why-openai-could-lose-5-billion-this-year?rc=kz8jh3&amp;ref=wheresyoured.at"><u>The Information reported that OpenAI could burn as much as $5 billion dollars in 2024</u></a> based on "previously undisclosed internal financial data and people involved in the business."</p><p>The piece makes several informed estimates (and I encourage you to pay for The Information for this article alone) that I am going to draw upon. While it’s <em>possible</em> that these estimates may be wrong, or that the data they were based on was misleading or incorrect, I trust the Information’s analysis and the rigor of its reporting:</p><ul><li>"OpenAI as of March [2024] was on track to spend nearly $4 billion this year on renting Microsoft's servers to power ChatGPT and its underlying LLMs," sourced to a "person with direct knowledge of the spending."</li><li>"OpenAI's training costs — including paying for the data — could balloon to as much as $3 billion this year."<ul><li>As a note, training costs are not simply getting the data, but cleaning and preparing it — a laborious task — and then using massive amounts of cloud compute to train the model using it.</li></ul></li><li>The Information "guesstimates" that OpenAI's 1500-person (and growing) workforce could cost around $1.5 billion a year. While that sounds a little high — especially considering that figure works out to $1m per person — it’s actually quite plausible. Top AI talent is extremely, extremely expensive, and <a href="https://www.wsj.com/tech/ai/the-fight-for-ai-talent-pay-million-dollar-packages-and-buy-whole-teams-c370de2b?ref=wheresyoured.at#:~:text=Some%20of%20these%20hard%2Dto,million%20a%20year%20or%20more.&amp;text=Salespeople%20in%20AI%20are%20also,set%20and%20depth%20of%20knowledge."><u>seven-figure salaries are far from unusual</u></a>. You then have to factor in things like office space, payroll taxes, equipment, and the other operational costs</li><li>The Information also estimates that OpenAI has<a href="https://www.theinformation.com/articles/openais-annualized-revenue-doubles-to-3-4-billion-since-late-2023?rc=kz8jh3&amp;ref=wheresyoured.at"><u> somewhere between $3.5 billion and $4.5 billion in revenue</u></a>, combining both ChatGPT and charging developers to access OpenAI's APIs to integrate generative functions.</li></ul><p>The Information surmises that OpenAI thus has an operating loss of <strong>$5 billion a year.</strong> That also assumes that OpenAI's revenue is on the higher-end, and could balloon to <strong>$6 billion or more.</strong></p><p>Though we don't have direct knowledge, OpenAI's operating costs have continued to rapidly increase over time.<a href="https://www.businessinsider.com/how-much-chatgpt-costs-openai-to-run-estimate-report-2023-4?ref=wheresyoured.at"><u> An estimate from early 2023 suggested that it cost $700,000 a day to run ChatGPT</u></a> at a time when it was <em>popular</em> but not <em>as popular as it is today, </em>and would put ChatGPT's costs alone at around $235.3 million a year. I would also hypothesize that they were much larger based on OpenAI having raised over $13 billion in the last five years, with the majority of the capital (and credit) raises happening between 2021 and 2023.</p><p>OpenAI has historically, based on reporting, failed to make its models more efficient,<a href="https://www.businessinsider.com/openai-model-arrakis-dystopian-desert-world-dune-2023-10?ref=wheresyoured.at"><u> failing to deliver their more efficient "Arrakis" model to Microsoft in late 2023</u></a>. While the recent launch of its GPT-4o mini model has been hailed as an "efficiency" play,<a href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/?ref=wheresyoured.at#:~:text=developers%20to%20build%20and%20scale%20powerful%20AI%20applications%20more%20efficiently%20and%20affordably"><u> it appears to only be more efficient and cost-effective for those developing using OpenAI's tools</u></a>, and while one could posit that this suggests that OpenAI found a more efficient/cost-effective model and thus made it cheaper, it has yet to volunteer this information to confirm.</p><p>Assuming everything exists in a vacuum, <strong>OpenAI needs at least $5 billion in new capital a year to survive. </strong>This would require it to raise more money than has ever been raised by any startup in history, possibly in perpetuity, which would in turn require it to access capital at a scale that I can find no comparable company to in business history.</p><p>WeWork — the decrepit failson of Silicon Valley —<a href="https://www.crunchbase.com/organization/wework/investor_financials?ref=wheresyoured.at"><u> raised a total of $22.2 billion</u></a>, with nearly half of it (over $10 billion) raised in debt financing (loans with varying terms) from Goldman Sachs and SoftBank's Vision Fund,<a href="https://www.nytimes.com/2023/03/17/business/wework-softbank-debt-deal.html?ref=wheresyoured.at"><u> some of which it had to restructure as the company collapsed</u></a>. Much of WeWork's capital was raised during a time with lower interest rates and thus much more available money, and heavily relied on SoftBank's continual willingness to dump cash into a fire. WeWork also had a far more reasonable — though extremely stupid — business model, one that a lot of financiers could get their head around, and thus was able to appeal to a much wider investment market.</p><p>A more reasonable comparable would be CoreWeave, a company that gives other companies access to (and build-outs of) the massive Graphics Processing Unit clusters to power AI applications. CoreWeave has raised a total of<a href="https://www.crunchbase.com/organization/coreweave?ref=wheresyoured.at"><u> $12.1 billion dollars</u></a> with — you guessed it — the majority of it raised in a "<a href="https://www.blackstone.com/news/press/coreweave-secures-7-5-billion-debt-financing-facility-led-by-blackstone-and-magnetar/?ref=wheresyoured.at"><u>debt financing facility</u></a>" offered by asset management firms Blackstone and Magnetar that allows it to draw upon cash at an undisclosed (but likely lower) interest rate.<a href="https://www.wsj.com/tech/ai/ai-cloud-computing-startup-coreweave-valued-at-19-billion-in-new-funding-round-dfdb47cd?ref=wheresyoured.at"><u> CoreWeave has raised at a $19 billion valuation</u></a>, and unlike OpenAI, its services are relatively straightforward. If you need a bunch of compute, CoreWeave will either build it for you, or lease it to you.</p><p>Historically, according to data provided by Crunchbase, the largest funding round of the last decade was $14 billion,<a href="https://techcrunch.com/2018/06/07/ant-financial-raises-14-billion/?ref=wheresyoured.at"><u> raised by Ant Group in 2018</u></a>, followed by Juul, also in 2018,<a href="https://investor.altria.com/press-releases/news-details/2018/Altria-Makes-128-Billion-Minority-Investment-in-JUUL-to-Accelerate-Harm-Reduction-and-Drive-Growth/default.aspx?ref=wheresyoured.at"><u> when it raised $12.8 billion</u></a>. Otherwise, OpenAI dominates.</p><blockquote>As another aside, Uber, <a href="https://www.theverge.com/2024/2/8/24065999/uber-earnings-profitable-year-net-income?ref=wheresyoured.at"><u>a company famed for burning $25 billion dollars to achieve profitability</u></a>, raised a total of...well, <a href="https://www.crunchbase.com/organization/uber?ref=wheresyoured.at"><u>$25 billion</u></a>, which included <u>four different funding rounds in the year 2018 alone.</u><p>And even then it was more profitable than OpenAI, other than in 2020, when it lost $6.7 billion — around $1.7 billion more than OpenAI might lose this year — because <em>people weren’t going anywhere.</em></p><p>On top of that, — and yes, I am directly responding to the Information’s “</p><a href="https://www.theinformation.com/articles/is-openai-a-good-business?utm_source=ti_app&amp;rc=kz8jh3"><u>is OpenAI a good business</u></a>?,” the central argument of which is that OpenAI also needs to burn a bunch of money — Uber immediately had a use case people understood, and immediately generated revenue by devouring the taxi monopolies, subsidized heavily by venture capital. <p>Conversely, OpenAI has devoured no monopoly, and the product category it created — the only one it is really part of — is one entirely subsidized by venture capital and Microsoft. What OpenAI is offering is entirely hype-driven, hard to explain to the layman, a movement driven by a lack of hypergrowth markets.&nbsp;</p><p>Uber was priced to replace a monopoly, and one that most people hated. Taxis were expensive, inconvenient, artificially scarce (especially in cities like New York, which limited the supply of taxis through its medallion system), and seldom accepted credit cards. Worse, discrimination against those of a minority background by drivers was rife and unchallenged (</p><a href="https://www.nytimes.com/2019/05/19/nyregion/nyc-taxis-medallions-suicides.html?ref=wheresyoured.at"><u>and involved exploitative loan schemes</u></a>). While we can dislike Uber as a company and criticize its business practices, you can’t deny it had an objective appeal from the outset.&nbsp;<p>By contrast, OpenAI created an industry-wide FOMO psychosis, and has profited heavily from it, but explaining what ChatGPT is to a layman is possible yet convoluted, which Uber was not. </p><a href="https://www.promarket.org/2019/11/27/false-claims-and-propaganda-why-ubers-narratives-are-wrong-but-successful/?ref=wheresyoured.at"><u>I should also add that the media was used by Uber as a means of laundering Uber’s reputation</u></a>, used specifically to (<a href="https://thebaffler.com/latest/the-miseducation-of-kara-swisher-ongweso?ref=wheresyoured.at"><u>and I quote Edward Ongweso Jr</u></a>.) convince people to “view its growth as progressive, not parasitic.” It’s important to bring attention to OpenAI and Sam Altman’s attempts to create a narrative promising things that its company has no way of doing, and even more so to not find ways to explain away how unsustainable OpenAI is. It is fundamentally not the media’s job to convince the world that OpenAI is a stable company with great things ahead — that’s OpenAI’s job. <p>While I am not sure it’s appropriate to say that I’m a “member of the media,” of the two of us, I think that the PR guy with a part-time newsletter and a podcast should never be the one who’s more willing to be critical.</p></blockquote><p>As of its last round of funding — a secondary market (meaning insiders can sell their stocks to VCs) offering from February of this year —<u> </u><a href="https://www.nytimes.com/2024/02/16/technology/openai-artificial-intelligence-deal-valuation.html?ref=wheresyoured.at"><u>OpenAI is "valued" at $80 billion</u></a>. I say "valued," because Microsoft's investment (which likely increased the valuation of the company, though I can't confirm) was predominantly in funny-money, cloud credits, rather than any actual "investment" that would in turn give something a "valuation."</p><p>Furthermore, debt financing is usually a little harder to get, with onerous cash-heavy terms that can eat a company alive in a bad month.</p><p>Comparables at this scale are few and far between.<a href="https://www.cbinsights.com/research-unicorn-companies?ref=wheresyoured.at"><u> Based on data from CBInsights</u></a>, the only private companies that compete are TikTok developer ByteDance ($225 billion,<a href="https://www.crunchbase.com/organization/bytedance?ref=wheresyoured.at"><u> raised $9.5bn</u></a>) and SpaceX ($150 billion,<a href="https://www.crunchbase.com/organization/space-exploration-technologies?ref=wheresyoured.at"><u> raised $9.8bn</u></a>), with Stripe ($70 billion (though I've seen $65 billion),<a href="https://www.crunchbase.com/organization/stripe?ref=wheresyoured.at"><u> raised $9.4bn</u></a>), Shein ($66 billion,<a href="https://www.crunchbase.com/organization/shein-b79e?ref=wheresyoured.at"><u> raised $4.1bn</u></a>) and Databricks ($43 billion,<a href="https://www.crunchbase.com/organization/databricks?ref=wheresyoured.at"><u> raised $4bn</u></a>) just behind.</p><p>In all of these cases, the companies in question make real money and have real business models. ByteDance (which owns TikTok, as well as several other companies in China)<a href="https://www.ft.com/content/275bd036-8bc2-4308-a5c9-d288325b91a9?ref=wheresyoured.at"><u> made $120 billion in revenue in 2023</u></a>, and its services are used by hundreds of millions of people. SpaceX, while unprofitable, is (for better or worse) effectively tied to the US government as a necessary contractor, and has<a href="https://www.wsj.com/tech/behind-the-curtain-of-elon-musks-secretive-spacex-revenue-growth-and-rising-costs-2c828e2b?ref=wheresyoured.at"><u> succeeded making billions of dollars in reducing its operating costs</u></a>. Stripe is one of the most well-respected payments companies in the world, makes billions of dollars, has extremely useful services, and<a href="https://techcrunch.com/2024/03/13/stripes-growth-payment-1t-fintech/?ref=wheresyoured.at"><u> is "robustly cash flow positive."</u></a> Shein, while a horrible company built on exploitation, makes over<a href="https://www.cnbc.com/2024/01/08/sheins-revenue-is-a-lot-more-than-30-billion-annually-exec.html?ref=wheresyoured.at"><u> $30 billion a year</u></a> and has<a href="https://www.ft.com/content/702223df-2e52-4e62-8f7c-93695a100d9b?ref=wheresyoured.at"><u> $2 billion in profit</u></a> <em>selling stuff. </em>Databricks, a boring-yet-useful data intelligence company,<a href="https://www.databricks.com/company/newsroom/press-releases/databricks-announces-over-70-annualized-growth-emea-fueled?ref=wheresyoured.at#:~:text=Globally%2C%20Databricks%20reached%20over%20%241.6,driven%20by%20rapid%20product%20innovation."><u> reported in April that it had reached $1.6 billion in revenue in a quarter</u></a>, and<a href="https://www.thestack.technology/databricks-just-raised-a-fresh-500-million-why/?ref=wheresyoured.at#:~:text=Databricks%20has%20yet%20to%20achieve,and%20coin%20landing%20in%20coffers."><u> is yet to achieve profitability</u></a> — making it the odd man out, and possibly the closest comparable to OpenAI.</p><p>However, one consistent difference with these companies is that they've <em>proven market viability.</em> While Databricks may be an indeterminate level of unprofitable, it has been raising for longer (since their Series A in 2013), and while it’s raised <em>a lot</em>, OpenAI has had to raise more, in a shorter period of time, and will have to raise again soon.&nbsp;</p><p>SpaceX,<u> </u><a href="https://www.space.com/spacex-starship-second-flight-explosion-cause?ref=wheresyoured.at"><u>which makes rockets that sometimes explode</u></a>, still makes rockets and satellites that provide people with internet access (<a href="https://www.expressnews.com/business/article/spacex-s-starlink-poised-record-revenue-2024-19450589.php?ref=wheresyoured.at"><u>and Starlink makes billions in revenue</u></a>) — which isn't an endorsement of Musk so much as it is a differentiator. Stripe has raised in large chunks, but over a longer timeline and <em>also provides a very obvious and useful product.</em> And I don't have to explain why TikTok or Douyin are important considering they're both some of the largest social networks in the world.</p><p>And in all cases, <strong><em>they've raised less money than OpenAI has to date,</em> </strong>though with the caveat that OpenAI's $10 billion round was mostly in cloud credits.</p><p>Yet that actually raises a much thornier question: is OpenAI <em>capable</em> of raising this much money?<a href="https://techcrunch.com/2023/03/15/stripe-now-valued-at-50b-following-6-5b-raise/?ref=wheresyoured.at"><u> When Stripe raised $6.5 billion in 2023, it dropped its valuation to $50 billion</u></a>,<a href="https://www.reuters.com/business/finance/fintech-giant-stripe-raises-nearly-694-mln-funds-2024-04-12/?ref=wheresyoured.at"><u> which it got back to $65 billion in 2024 when it raised a tender offer of $694 million</u></a>. Again, Stripe is bordering on an essential service, used across wide swaths of the internet to help people buy stuff. <a href="https://www.crunchbase.com/organization/anthropic/company_financials?ref=wheresyoured.at"><u>OpenAI competitor Anthropic has also had to raise over $7 billion since 2021</u></a> (<a href="https://techmonitor.ai/technology/ai-and-automation/aws-anthropic-ai-public-cloud?ref=wheresyoured.at"><u>including billions from Amazon and Google that could also be in cloud credits</u></a>)- and<a href="https://www.theinformation.com/briefings/anthropic-projected-to-burn-more-than-2-7-billion-in-cash-this-year?rc=kz8jh3&amp;ref=wheresyoured.at"><u> The Information reports it could burn $2.7 billion in 2024</u></a> on $800 million in revenue <em>that it has to share with Amazon.</em></p><p>What I'm trying to establish is that OpenAI would have to, at its current pace:</p><ul><li>Raise more money than anyone ever has before — likely at least $3 billion, but more like $10 billion, and do so soon, likely within the next six months.</li><li>Raise either multiple rounds, or the largest funding round ever raised by any company, and then have to keep doing so in perpetuity.</li><li>Raise at either a massive down-round — by taking on more money at a reduced valuation — or raise at a valuation higher than any privately-held company ever has.</li></ul><p>In all of these cases, OpenAI would have to show investors both how it intends to grow revenue and reduce costs, and do so in such a way that it would reassure investors that OpenAI would not simply return asking for more capital in a few months. It would also likely have to amend their corporate structure,<a href="https://www.reuters.com/technology/artificial-intelligence/openai-ceo-says-company-could-become-benefit-corporation-information-2024-06-15/?ref=wheresyoured.at#:~:text=June%2014%20(Reuters)%20%2D%20OpenAI,The%20Information%20reported%20on%20Friday."><u> as Sam Altman has suggested it might</u></a>.</p><p>This isn't <em>impossible</em>, but it feels extremely unlikely that OpenAI would be able to do this for more than a few years.<a href="https://www.wheresyoured.at/put-up-or-shut-up/"><u> Reports suggest that OpenAI is nowhere near Artificial General Intelligence</u></a>, and while Altman could potentially raise another round from venture capitalists desperate to get aboard the company, doing so would risk exposing how bad the burn rate truly is.</p><p>If we assume that<a href="https://www.nytimes.com/2024/02/16/technology/openai-artificial-intelligence-deal-valuation.html?ref=wheresyoured.at"><u> OpenAI's secondary market round from February 2024</u></a> was the best-case scenario — $5 billion in <em>cash</em>, and I'd guess it was less, we truly have no idea — the company still needs another lifeline within the next 12 months.</p><p>In a more realistic case, I believe OpenAI has to raise within the next 3-6 months, which will mean that doing so will involve them raising funds after at least one quarter of reported earnings from Microsoft, with the next coming up on July 30.</p><p>Assuming a burn rate of even $3 billion a year — which would require a remarkable reduction in costs — OpenAI would still have to raise more capital than anyone has ever raised for as long as it takes to either increase revenues or reduce costs. It's extremely concerning, and equally unsustainable.</p><h2 id="the-revenue-cost-and-market-fit-problem"><strong>The Revenue, Cost and Market-Fit Problem</strong></h2><p>As I've written repeatedly,<a href="https://www.wheresyoured.at/pop-culture/"><u> generative AI is deeply unprofitable</u></a>, and<a href="https://www.theinformation.com/articles/why-openai-could-lose-5-billion-this-year?rc=kz8jh3&amp;ref=wheresyoured.at"><u> based on the Information's estimates</u></a>, the cost of goods sold is unsustainable.</p><p>OpenAI's costs have only increased over time, and the cost of making these models "better" are only increasing, and have yet to,<a href="https://www.goldmansachs.com/intelligence/pages/gs-research/gen-ai-too-much-spend-too-little-benefit/report.pdf?ref=wheresyoured.at"><u> to paraphrase Goldman Sachs' Jim Covello</u></a>, solve the kind of complex problems that would justify their cost. "Better" is also somewhat of a misnomer — a "better" version of ChatGPT may be faster, give answers that are more accurate or that are generated more-quickly, but it is not able to do significantly <em>more</em>. Since November 2022, ChatGPT has grown more sophisticated, faster at generations, capable of ingesting more data, but has yet to generate a true "killer app," an iPhone-esque moment.</p><p>Furthermore, transformer-based models have become heavily-commoditized, with competition from independent(ish) companies like Anthropic's Claude and Meta's LLama, all trained on the same massive data-sets, to the point that ChatGPT's biggest advantage is in its brand. As a result, we're already seeing a race to the bottom, with<a href="https://x.com/minimaxir/status/1813985834728919249?ref=wheresyoured.at"><u> GPT-4o Mini (OpenAI's "cheaper" model) already beaten in price by Anthropic's Claude Haiku model</u></a>, and I am confident somebody is already working on a similarly-powerful model that they'll sell <em>for even cheaper.</em></p><p>As a result, OpenAI's revenue might <em>climb</em>, but it's likely going to climb by reducing the cost of its services rather than its own operating costs. OpenAI appears to be operating in the standard valley monopoly model — get as many customers as possible and then work out how to get profitable — but is doing so using a technology that is uniquely expensive to both operate and iterate upon.</p><p>As discussed previously, OpenAI — like every single transformer-based model developer — requires masses of training data to make its models "better," and the<a href="https://www.wsj.com/tech/ai/ai-training-data-synthetic-openai-anthropic-9230f8d8?ref=wheresyoured.at"><u> next generation of GPT will require four to five times the amount it needed for GPT-4</u></a><a href="https://www.nytimes.com/2024/07/19/technology/ai-data-restrictions.html?ref=wheresyoured.at"><u> at a time when publishers and the wider internet have found numerous ways to block them from taking it</u></a>.</p><p>Doing so is also likely going to lead to perpetual legal action, especially as 404 Media reports that Runway, a generative video company,<a href="https://www.404media.co/runway-ai-image-generator-training-data-youtube/?ref=wheresyoured.at"><u> likely trained its model on thousands of hours of videos taken from YouTube and pirated sources</u></a>. OpenAI has<a href="https://www.wheresyoured.at/peakai/"><u> been incredibly evasive when asked if it trained their "Sora" model on YouTube videos</u></a>, and if I had to guess, it absolutely has. If it hasn't, it likely will be required to buy tens of thousands — if not millions — of hours of footage, which<a href="https://www.theverge.com/2024/5/22/24162782/openai-licensing-deal-wall-street-journal-news-corp?ref=wheresyoured.at"><u> will be multitudes more expensive than the $250 million it paid to News Corp to train on its articles</u></a>.</p><p>And, to be abundantly clear, <em>I am not sure there is enough training data in existence</em> to get these models past the next generation. Even if generative AI companies were able to legally and freely download every single piece of text and visual media from the internet, it doesn't appear to be enough to train these models, with some model developers <a href="https://www.wheresyoured.at/bubble-trouble/"><u>potentially turning to model-generated "synthetic" data</u></a> — a process that could introduce "<a href="https://www.zdnet.com/article/beware-of-ai-model-collapse-how-training-on-synthetic-data-pollutes-the-next-generation/?ref=wheresyoured.at"><u>model collapse</u></a>," a form of inbreeding that Jathan Sadowski called "Habsburg AI" that destroys the models over time.</p><p>Even if they were successful in somehow acquiring this much training data, and doing so in a way that was legally sound (which, to be clear, I do not think is possible), they would then have the increasing costs of training these models. Anthropic CEO recently said on a podcast that "<a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/ai-models-that-cost-dollar1-billion-to-train-are-in-development-dollar100-billion-models-coming-soon-largest-current-models-take-only-dollar100-million-to-train-anthropic-ceo?ref=wheresyoured.at"><u>AI models that cost $1 billion to train are underway</u></a>," and that there are ones in the future that will cost $100 billion. As models become more complex and require more training data, so too does the cost of ingesting the larger (and likely more complex) training data.</p><p>And then there's the very big, annoying problem — that generative AI doesn't have a product-market fit at the scale necessary to support its existence.</p><p>To be clear, I am not saying generative AI is completely useless, or that it hasn't got <em>any</em> product-market fit. It's useful for digging through massive data-sets, quick summaries of articles (for better or worse), and generating images. These models have utility,<u> </u><a href="https://www.theverge.com/2024/5/15/24154808/ai-chatgpt-google-gemini-microsoft-copilot-hallucination-wrong?ref=wheresyoured.at"><u>despite the propensity of these models to "hallucinate" (authoritatively state something that isn't true, or generate hands with too many fingers)</u></a>,<a href="https://www.ft.com/content/c35ce925-d7b3-4920-a431-c4ca1aa33503?ref=wheresyoured.at"><u> and people are finding useful things for them to do, particularly in finance</u></a>.</p><p>But what they are not, at this time, is <em>essential.&nbsp;</em></p><p>Generative AI has yet to come up with a reason that you<em> absolutely must integrate it,</em> other than the sense that your company is "behind" if you don't use AI. This wouldn't be a problem if generative AI's operating costs were a minuscule fraction — tens or hundreds of thousands of percent — of what they are today, but as things stand, OpenAI is effectively subsidizing the generative AI movement, all while dealing the problem that while <em>cool</em> and <em>useful</em>, GPT is only changing the world as much as the markets allow it to.</p><p>While complex, generative AI is a technology that probabilistically generates answers, and has no "intelligence." It is inherently limited by its architecture, and in turn can only get "better" in a linear fashion. I see no signs that the transformer-based architecture can do significantly more than it currently does.</p><p>For OpenAI to continue growing, it will either have to significantly increase functionality — something it hasyet to do, but is theoretically possible — or vastly reduce pricing, which will only increase its operating costs.</p><p>And OpenAI <em>must grow,</em> because $3.5 billion to $4.5 billion a year in revenue is simply not enough to keep this company going. This isn't about any personal beliefs I have about generative AI. It's about the fact that this company costs more money to run than any other privately-held startup, and its technology does not — based on OpenAI’s sales — do enough to make up for the fact that it costs so much money.</p><p>Outside of reducing prices, which would increase revenues <em>and </em>operating costs, OpenAI could, theoretically, find new functionality in GPT — though I'm not sure how — or create something entirely different, something it has yet to show any sign of doing. While you could point to tools like Sora (which doesn’t seem particularly useful, and is still far from commercialization), or searchGPT (which would have the same hallucinatory issues that dogged Google search’s own pivot to AI, while also competing against the GPT-enabled Bing), it’s tough to make the case that these products will fill the burning shortfall in OpenAI’s balance sheet, and would likely only add to its operational costs</p><p><a href="https://www.reuters.com/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-12/?ref=wheresyoured.at"><u>In July, Reuters reported</u></a> that OpenAI was "working on a new technology called Strawberry" that would be "capable of delivering advanced reasoning capabilities." However, on a deeper read of the article, OpenAI is <em>thinking about this </em>and <em>trying to</em> do it, and has <em>yet to do it.</em></p><p>If it succeeds — which I add would require potentially entirely new branches of psychology and mathematics to do so, as we humans barely understand our brains — that would be a huge technological achievement that still <em>wouldn't</em> turn things around. OpenAI would own a completely new kind of technology, which would be immensely valuable, and could potentially raise in perpetuity off of that, but it would be heavily-dependent on the level of reasoning and the accompanying tasks it could achieve.</p><p>And, to be clear, it is very, very unlikely this happens. It could — and there is always stuff I might not know about OpenAI's research and development — but we have seen little sign of OpenAI innovating, and far more signs that it’s only capable at this time of iterating on GPT.</p><p>OpenAI also has a problem with its marketing.<a href="https://www.wheresyoured.at/sam-altman-is-full-of-shit/"><u> Sam Altman has repeatedly misled the media about what "AI might do,"</u></a> conflating generative AI — which does not "know" things and is not "intelligence" — with the purely-theoretical concept of an autonomous, sentient artificial intelligence. As a result, expectations are higher of what future generations of GPT might do, making it inevitable that the company will disappoint investors and customers.</p><p>While there may be ways to reduce the costs of transformer-based models, the level of cost-reduction would be unprecedented, and likely require entirely new chips, cooling solutions and physical server architecture, none of which OpenAI develops.&nbsp;</p><p>While theoretically Nvidia could produce a much-more-efficient chip, doing so would likely take longer than OpenAI has.<a href="https://techcrunch.com/2024/06/25/etched-is-building-an-ai-chip-that-only-runs-transformer-models/?ref=wheresyoured.at"><u> Though there are companies like Etched that <em>claim</em> they are working on specialized chips</u></a>, they are years from delivering any working silicon at the scale that OpenAI would need, and said chips are focused on singular models, making them iterative rather than innovative concepts.</p><p>One thing I am not heavily-discussing is the fact that there doesn't seem to be general-purpose adoption of generative AI. These numbers are hard to establish, but what I have previously established for certain —<a href="https://www.wheresyoured.at/pop-culture/"><u> in particular based on Goldman Sachs' reporting</u></a> — is that <em>actual meaningful revenue</em> is yet to materialize. This is a bigger existential threat than a lack of adoption. It means people are using it and not getting enough out of it, which could potentially lead to a significant loss of revenue as the hype cycle dies down.</p><p>To summarize:</p><ul><li>OpenAI's only real options are to reduce costs or the price of its offerings. It has not succeeded in reducing costs so far, and reducing prices would only increase costs.</li><li>To progress to the next models of GPT, OpenAI's core product, the company would have to find new functionality.</li><li>OpenAI is inherently limited by GPT's transformer-based architecture, which does not actually automate things, and as a result may only be able to do "more" and "faster," which does not significantly change the product, at least not in such a way that would make it as valuable as it needs to be.</li><li>OpenAI's only other option is to invent an entirely new kind of technology, and be able to productize and monetize said technology, something that the company has not yet been able to do.</li></ul><hr><p><strong>A Note On Energy: </strong>Most of the problems I've listed are existential threats to the future of OpenAI, ones that I can see no quick or easy way out of, but another stands in the way — energy.&nbsp;</p><p>For OpenAI to scale, it would require a massive capital expenditure on multiple levels, chief of them the American power grid (<a href="https://www.goldmansachs.com/intelligence/pages/gs-research/gen-ai-too-much-spend-too-little-benefit/report.pdf?ref=wheresyoured.at"><u>see page 15 of this Goldman Sachs report for a conversation with Microsoft's former VP of energy</u></a>), which will likely require extensive expansion the likes of which hasn't happened in decades at a time when America is far less apt at infrastructural development.&nbsp;</p><p>While the US steadily added new electricity generation capacity in the second half of the 1900s, <a href="https://www.statista.com/statistics/188521/total-us-electricity-net-generation/?ref=wheresyoured.at"><u>things started to plateau in the 2010s</u></a>. This is a combination of a bunch of things. Electricity consumption has remained flat or <a href="https://www.sciencedirect.com/science/article/abs/pii/S0301421521005188?ref=wheresyoured.at"><u>decreased slightly across both households and businesses</u></a>. While the US has added capacity, particularly when it comes to renewables and natural gas, that isn’t increasing the amount of electricity generation available, but rather <a href="https://headwaterseconomics.org/economic-development/evolution-electricity-generation/?ref=wheresyoured.at"><u>offsetting the decommissioning of coal-fired power plants</u></a>.&nbsp;</p><p>Scaling AI would require an investment in power generation that would be equivalent in ambition to the New Deal, or Eisenhower’s Interstate Highway System, and it would need to happen quickly. That’s something that doesn’t happen in the power-generation world. For context, in 2021 it took an average of 2.8 years for <a href="https://thundersaidenergy.com/downloads/renewables-how-much-time-to-connect-to-the-grid/?ref=wheresyoured.at#:~:text=The%20median%20time%20to%20receive,2021%2C%20which%20is%202.8%20years."><u>a new solar farm to be connected to the electrical grid</u></a>. Two years later, that <a href="https://www.nytimes.com/2023/02/23/climate/renewable-energy-us-electrical-grid.html?ref=wheresyoured.at"><u>time rose to four years</u></a>. Small modular reactors — a promising approach designed to reduce the cost and build times of nuclear power generations — are still far from mass-commercialization, and even if they weren’t, they’d still have to tend with the bureaucracy of the sector.&nbsp;&nbsp;</p><p>Even if changing this were possible — and it'd be good for society if it was — artificial intelligence (driven by generative AI) is already massively increasing global emissions, particularly from companies like Google,<a href="https://www.bbc.com/news/articles/c51yvz51k2xo?ref=wheresyoured.at"><u> which saw its emissions increase by 48% in the last five years thanks to AI</u></a>.</p><p>For OpenAI to continue scaling, it is reliant on a dramatic expansion of the power grid, at a time when (according to Brian Janous of Cloverleaf Infrastructure) the wait times are <a href="https://www.goldmansachs.com/intelligence/pages/gs-research/gen-ai-too-much-spend-too-little-benefit/report.pdf?ref=wheresyoured.at"><u>ranging from 40-70 months to spin up new power projects</u></a>. And OpenAI isn't the company doing the scaling — much like it’s dependent on Nvidia to continue to produce GPUs for generative AI's cloud compute, so too is it dependent on companies like Microsoft, Google, and Oracle working with power companies to expand the grid.</p><h2 id="a-grim-situation"><strong>A Grim Situation</strong></h2><p>Absolutely nothing about what I've written here is based on personal grievance, or a dislike of generative AI, or really anything other than a frank evaluation of a company that I believe may be teetering on the brink of collapse.</p><p>For OpenAI to continue operating, things have to change dramatically.</p><ul><li>In the event that OpenAI is making the highest-end of their reported revenue range — $4.5 billion — and it has that much in the bank as we speak, it will have to raise at least $2 billion, or as much as $10 billion, within the next 12 months.</li><li>In the event that it hasless than a billion in the bank, OpenAI will likely have to raise $5 billion, and do so in the next three to six months.</li><li>Otherwise, OpenAI will either have to <em>at least </em>halve their operating costs, <em>all while maintaining the current pace of revenue,</em> or find a way to literally double its revenue while keeping costs at the same rate. Even then, these numbers are extremely concerning — though I add there are always things that I don't know as OpenAI is a private company, and thus isn’t subject to the same disclosure rules as publicly-traded companies.</li></ul><p>Survival would require OpenAI to raise more money than anybody ever has. This is technically possible, but would require venture capitalists and investment banks to effectively provide a lifeline to the company in perpetuity, unless the company is capable of either heavily reducing costs or finding billions of dollars more in revenue. Even if it succeeds, if the cost of revenue increases along with sales, all growth would be for nought, and create further problems and dependencies on venture capital — or on companies like Microsoft.</p><p>It would require a way of expanding what OpenAI can sell to address the entirety of corporate America, which would require use cases that I do not believe generative AI is capable of meeting, like automating chunks of the economy rather than<a href="https://www.wsj.com/tech/ai/ai-replace-freelance-jobs-51807bc7?ref=wheresyoured.at"><u> bankrupting freelance designers and copy-editors</u></a>.&nbsp;</p><p>To be clear, I am not advocating for workers being replaced by AI. I am simply saying that for OpenAI to grow to the $10+ billion revenue a year it needs to survive, it would need to replace entire chunks of the labor force. And as a reminder, generative AI is not automation.</p><p>Even if these problems are surmountable, there is simply not enough training data, and even if there were, the cost of processing it will likely vastly outweigh whatever revenue OpenAI makes. Even if this were resolved — which would likely require $30 billion in, at best, cloud credits — doing so would not necessarily make transformer-based models capable of doing what it takes to sell $10 billion of software a year.</p><p>If I had to just choose a number, I hypothesize that OpenAI needs to raise $20 billion in the next two years to even stay in the game, and to get any further — something that isn't guaranteed — will cost them another $20 billion. For context, according to Crunchbase, the aggregate of all funding in 2023 was $299.2 billion, with $147 billion raised so far in 2024.&nbsp;</p><p>OpenAI would have to regularly make up 5-10% <em>of all startup funding, forever</em>, <em>or at least until it works out how to lose less or make more money.</em>&nbsp;</p><blockquote>The only other company that has done so is Uber — and as I’ve discussed above, their situation is very, very different. Comparing the two is ahistorical on the funding climate alone, with Uber existing at a time with lower interest rates. Along with $3.5 billion from Saudi Arabia’s Public Investment fund and over $8 billion from Softbank, the latter a secondary market sale, it also raised (equity and debt financing respectively) from Goldman Sachs and Morgan Stanley — two parties I do not believe are going to be willing to subsidize generative AI. <p>Even if I’m wrong — which I could be, stranger things have happened — the willingness and ease of getting people to hand over hundreds of millions or billions of dollars became markedly different between Uber’s last funding round (September 14 2020) and 2024. The majority of its funding was raised between 2016 and 2019, too, when interest rates were low and, thus, VC coffers were overflowing. </p><p>And, crucially, investors had an incredibly clear path to liquidity — an IPO. Uber was always a dog of a company, but they always knew it’d chug along as a growth-at-all-costs monster on the market. How would OpenAI IPO? Its most recent round was a secondary market raise, selling insider stock to new investors. </p><p>If we humor this idea as the ultimate goal of pumping this company full of money, what are its plans to go public? </p><a href="https://www.reuters.com/technology/openai-ceo-has-no-ipo-plan-due-strange-company-structure-2023-06-06/?ref=wheresyoured.at"><u>Altman said as recently as June 2023 that its company structure would prohibit an IPO</u></a>, but do you think that OpenAI wants to subject itself to the scrutiny of the public markets, especially given their approach to copyrighted material? <p>Microsoft has a built-in profit share, but what of Sequoia? How does it get paid, other than finding another investor who wants the stock?</p><p>Perhaps there’s something I’m missing, but if investors have no path to an IPO, this feels like a game of hot potato, except the loser is left with a big, useless stock.Unless, of course, that investor is Microsoft, who will end up being able to use any leftover tech, and will benefit if OpenAI becomes profitable.</p></blockquote><p>Furthermore, I hypothesize a race to the bottom in generative AI will significantly hamper OpenAI's ability to expand revenue, compounded by the fact that we're approaching the limits of transformer-based architecture.</p><p>And because OpenAI (and the competition) are so deep in the hole with transformer-based models, I believe they will continue to drive billions into them, burning money on training them using data that may or may not have been legally acquired, and any lawsuit that goes in favor of the plaintiff would have potentially apocalyptic consequences for these models, requiring them to be retrained from scratch with an entirely new dataset, costing further hundreds of millions or <em>billions</em>.</p><p>And, quite frankly, I am just not sure how OpenAI will make this all work.</p><p>While OpenAI could — and I believe will — raise another huge, industry-defining round, doing so will require pulling in Canadian pension funds, Saudi sovereign wealth funds, or massive investment funds that would require swaths of equity. Doing so multiple times is possible, but very, very unlikely, in the same way that continuing to increase revenue at hundreds of percent each year is <em>possible,</em> but very, very unlikely. It could come up with something new, or find a way to make generative AI much cheaper, but again, that is so very, very unlikely.</p><p>OpenAI could be the most important tech company of all time, in that it will have to devour the very Gods of Silicon Valley to continue its rapacious growth. In doing so, it will break records in funding and software revenue, and do so while warding off competition from startups — as well as its own investor Microsoft.</p><p>I don't know how OpenAI does it. Writing this piece took me hours, and in doing so, I genuinely tried to work out how OpenAI survives, and every single corner I turned ended with "it can do it if it does something that has never happened."</p><p>I have written this with a dispassionate tone because I need people to take me seriously when I say that to survive, generative AI must do so much more than it currently does, and much cheaper than it's currently doing so. To survive, OpenAI must break every startup record known to man, and to thrive, it must either reinvent transformer-based architecture to reduce its compute requirements, and then invent an entirely new kind of artificial intelligence to do the things that people want AI to do.</p><p>Perhaps I'm wrong. Perhaps there are things that I don't know — about OpenAI, about the things it’s working on in secret, about some sort of energy or chip breakthrough that will approach so suddenly that I will eat crow. Perhaps it has more money in the bank than I thought, or perhaps their costs are currently inflated in a way that I — and the entirety of the tech media — are unaware of.</p><p>But if there were, I believe we would know, or at least have some sort of sign.</p><p>I don't know what happens next, but I do know things have to change. I fear OpenAI will compete on price, sending its costs upward, or charge what it needs to to approach break-even, which I don't think it’s willing to do. I fear for Anthropic, which has less money, less revenue, and equally gruesome burn. I fear for those founders relying on the current pricing for GPT and other models.</p><p>Without OpenAI, the bottom drops out of the entire generative AI market, and will more than likely brutalize any public stock associated with the generative AI boom.</p><p>I recognize reading this you might dismiss me as a cynic, or a pessimist, or as someone rooting for the end, and I have taken great pains to explain my hypotheses here in detail without much opinion or editorializing. If you disagree with me, tell me how I’m wrong — explain to me what I’ve missed, show me the holes in my logic or my math.&nbsp;</p><p>I caution you not to dismiss me, even if what I’ve written deeply upsets you. What I am describing here is a deeply unsustainable company that many have pinned their hopes on, and even if I’m wrong, this kind of analysis is necessary when there is a company burning billions of dollars that will likely attempt to absorb billions more dollars of capital to survive.&nbsp;</p><p>This discussion is equal parts necessary and troubling, uncomfortable in its implications and possibilities, and unsettling in its potential outcomes.&nbsp;</p><p>I hope I'm wrong. I really do.&nbsp;</p>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don't Let Your Domain Name Become a "Sitting Duck" (168 pts)]]></title>
            <link>https://krebsonsecurity.com/2024/07/dont-let-your-domain-name-become-a-sitting-duck/</link>
            <guid>41125544</guid>
            <pubDate>Thu, 01 Aug 2024 02:00:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://krebsonsecurity.com/2024/07/dont-let-your-domain-name-become-a-sitting-duck/">https://krebsonsecurity.com/2024/07/dont-let-your-domain-name-become-a-sitting-duck/</a>, See on <a href="https://news.ycombinator.com/item?id=41125544">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
												<p>More than a million domain names — including many registered by Fortune 100 firms and brand protection companies — are vulnerable to takeover by cybercriminals thanks to authentication weaknesses at a number of large web hosting providers and domain registrars, new research finds.</p>
<div id="attachment_68225"><p><img aria-describedby="caption-attachment-68225" decoding="async" src="https://krebsonsecurity.com/wp-content/uploads/2024/07/duckstarget.png" alt="" width="750" height="433"></p><p id="caption-attachment-68225">Image: Shutterstock.</p></div>
<p>Your Web browser knows how to find a site like example.com thanks to the global <a href="https://en.wikipedia.org/wiki/Domain_Name_System" target="_blank" rel="noopener">Domain Name System</a> (DNS), which serves as a kind of phone book for the Internet by translating human-friendly website names (example.com) into numeric Internet addresses.</p>
<p>When someone registers a domain name, the registrar will typically provide two sets of DNS records that the customer then needs to assign to their domain. Those records are crucial because they allow Web browsers to find the Internet address of the hosting provider that is serving that domain.</p>
<p>But potential problems can arise when a domain’s DNS records are “lame,” meaning the authoritative name server does not have enough information about the domain and can’t resolve queries to find it. A domain can become lame in a variety of ways, such as when it is not assigned an Internet address, or because the name servers in the domain’s authoritative record are misconfigured or missing.</p>
<p>The reason lame domains are problematic is that a number of Web hosting and DNS providers allow users to claim control over a domain <em>without accessing the true owner’s account at their DNS provider or registrar</em>.</p>
<p>If this threat sounds familiar, that’s because it is hardly new. Back in 2019, KrebsOnSecurity wrote about thieves employing this method to seize control over thousands of domains registered at GoDaddy, and using those <a href="https://krebsonsecurity.com/2019/01/bomb-threat-sextortion-spammers-abused-weakness-at-godaddy-com/" target="_blank" rel="noopener">to send bomb threats and sextortion emails</a> (GoDaddy says they fixed that weakness in their systems not long after that 2019 story).</p>
<p>In the 2019 campaign, the spammers created accounts on GoDaddy and were able to take over vulnerable domains simply by registering a free account at GoDaddy and being assigned the same DNS servers as the hijacked domain.</p>
<p>Three years before that, the same pervasive weakness was described in a blog post by security researcher <strong>Matthew Bryant</strong>, who showed how one could <a href="https://thehackerblog.com/the-orphaned-internet-taking-over-120k-domains-via-a-dns-vulnerability-in-aws-google-cloud-rackspace-and-digital-ocean/" target="_blank" rel="noopener">commandeer at least 120,000 domains</a> via DNS weaknesses at some of the world’s largest hosting providers.</p>
<p>Incredibly, new research jointly released today by security experts at <strong>Infoblox</strong> and <strong>Eclypsium</strong> finds this same authentication weakness is still present at a number of large hosting and DNS providers.</p>
<p>“It’s easy to exploit, very hard to detect, and it’s entirely preventable,” said <strong>Dave Mitchell</strong>, principal threat researcher at Infoblox. “Free services make it easier [to exploit] at scale. And the bulk of these are at a handful of DNS providers.”</p>
<h2>SITTING DUCKS</h2>
<p><a href="https://blogs.infoblox.com/threat-intelligence/who-knew-domain-hijacking-is-so-easy/" target="_blank" rel="noopener">Infoblox’s report</a> found there are multiple cybercriminal groups abusing these stolen domains as a globally dispersed “traffic distribution system,” which can be used to mask the true source or destination of web traffic and to funnel Web users to malicious or phishous websites.</p>
<p>Commandeering domains this way also can allow thieves to impersonate trusted brands and abuse their positive or at least neutral reputation when sending email from those domains, as we saw in 2019 with the GoDaddy attacks.</p>
<p>“Hijacked domains have been used directly in phishing attacks and scams, as well as large spam systems,” reads the Infoblox report, which refers to lame domains as “<strong>Sitting Ducks</strong>.” “There is evidence that some domains were used for Cobalt Strike and other malware command and control (C2). Other attacks have used hijacked domains in targeted phishing attacks by creating lookalike subdomains. A few actors have stockpiled hijacked domains for an unknown purpose.”</p>
<p>Eclypsium researchers <a href="https://eclypsium.com/blog/ducks-now-sitting-dns-internet-infrastructure-insecurity/" target="_blank" rel="noopener">estimate</a> there are currently about one million Sitting Duck domains, and that at least 30,000 of them have been hijacked for malicious use since 2019.</p>
<p>“As of the time of writing, numerous DNS providers enable this through weak or nonexistent verification of domain ownership for a given account,” Eclypsium wrote.</p>
<p>The security firms said they found a number of compromised Sitting Duck domains were originally registered by brand protection companies that specialize in defensive domain registrations (reserving look-alike domains for top brands before those names can be grabbed by scammers) and combating trademark infringement.</p>
<p>For example, Infoblox found cybercriminal groups using a Sitting Duck domain called <strong>clickermediacorp[.]com</strong>, which was a CBS Interactive Inc. domain initially registered in 2009 at GoDaddy. However, in 2010 the DNS was updated to DNSMadeEasy.com servers, and in 2012 the domain was transferred to <strong>MarkMonitor</strong>.</p>
<p>Another hijacked Sitting Duck domain — <strong>anti-phishing[.]org</strong> — was registered in 2003 by the <strong>Anti-Phishing Working Group</strong> (APWG), a cybersecurity not-for-profit organization that closely tracks phishing attacks.</p>
<p>In many cases, the researchers discovered Sitting Duck domains that appear to have been configured to auto-renew at the registrar, but the authoritative DNS or hosting services were not renewed.</p>
<p>The researchers say Sitting Duck domains all possess three attributes that makes them vulnerable to takeover:</p>
<p>1) the domain uses or delegates authoritative DNS services to a different provider than the domain registrar;<br>
2) the authoritative name server(s) for the domain does not have information about the Internet address the domain should point to;<br>
3) the authoritative DNS provider is “exploitable,” i.e. an attacker can claim the domain at the provider and set up DNS records without access to the valid domain owner’s account at the domain registrar.</p>
<div id="attachment_68226"><p><img aria-describedby="caption-attachment-68226" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/07/sittingduckattack.png" alt="" width="751" height="337" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/07/sittingduckattack.png 784w, https://krebsonsecurity.com/wp-content/uploads/2024/07/sittingduckattack-768x345.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/07/sittingduckattack-782x351.png 782w" sizes="(max-width: 751px) 100vw, 751px"></p><p id="caption-attachment-68226">Image: Infoblox.</p></div>
<p>How does one know whether a DNS provider is exploitable? There is a frequently updated list published on <strong>GitHub</strong> called “<a href="https://github.com/indianajson/can-i-take-over-dns?tab=readme-ov-file" target="_blank" rel="noopener">Can I take over DNS</a>,” which has been documenting exploitability by DNS provider over the past several years. The list includes examples for each of the named DNS providers.<span id="more-68214"></span></p>
<p>In the case of the aforementioned Sitting Duck domain clickermediacorp[.]com, the domain was originally registered by , but it appears to have been hijacked by scammers by claiming it at the web hosting firm <strong>DNSMadeEasy</strong>, which is owned by <strong>Digicert</strong>, one of the industry’s largest issuers of digital certificates (SSL/TLS certificates).</p>
<p>In an interview with KrebsOnSecurity, DNSMadeEasy founder and senior vice president <strong>Steve Job</strong> said the problem isn’t really his company’s to solve, noting that DNS providers who are also not domain registrars have no real way of validating whether a given customer legitimately owns the domain being claimed.</p>
<p>“We do shut down abusive accounts when we find them,” Job said. “But it’s my belief that the onus needs to be on the [domain registrants] themselves. If you’re going to buy something and point it somewhere you have no control over, we can’t prevent that.”</p>
<p>Infoblox, Eclypsium, and the DNS wiki listing at Github all say that web hosting giant <strong>Digital Ocean</strong> is among the vulnerable hosting firms. In response to questions, Digital Ocean said it was exploring options for mitigating such activity.</p>
<p>“The DigitalOcean DNS service is not authoritative, and we are not a domain registrar,” Digital Ocean wrote in an emailed response. “Where a domain owner has delegated authority to our DNS infrastructure with their registrar, and they have allowed their ownership of that DNS record in our infrastructure to lapse, that becomes a ‘lame delegation’ under this hijack model. We believe the root cause, ultimately, is poor management of domain name configuration by the owner, akin to leaving your keys in your unlocked car, but we acknowledge the opportunity to adjust our non-authoritative DNS service guardrails in an effort to help minimize the impact of a lapse in hygiene at the authoritative DNS level. We’re connected with the research teams to explore additional mitigation options.”</p>
<p>In a statement provided to KrebsOnSecurity, the hosting provider and registrar <strong>Hostinger</strong> said they were working to implement a solution to prevent lame duck attacks in the “upcoming weeks.”</p>
<p>“We are working on implementing an SOA-based domain verification system,” Hostinger wrote. “Custom nameservers with a Start of Authority (SOA) record will be used to verify whether the domain truly belongs to the customer. We aim to launch this user-friendly solution by the end of August. The final step is to deprecate preview domains, a functionality sometimes used by customers with malicious intents. Preview domains will be deprecated by the end of September. Legitimate users will be able to use randomly generated temporary subdomains instead.”</p>
<p>What did DNS providers that have struggled with this issue in the past do to address these authentication challenges? The security firms said that to claim a domain name, the best practice providers gave the account holder random name servers that required a change at the registrar before the domains could go live. They also found the best practice providers used various mechanisms to ensure that the newly assigned name server hosts did not match previous name server assignments.</p>
<p>[Side note: Infoblox observed that many of the hijacked domains were being hosted at <strong>Stark Industries Solutions</strong>, a sprawling hosting provider that appeared two weeks before Russia invaded Ukraine and has become <a href="https://krebsonsecurity.com/2024/05/stark-industries-solutions-an-iron-hammer-in-the-cloud/" target="_blank" rel="noopener">the epicenter of countless cyberattacks against enemies of Russia</a>].</p>
<p>Both Infoblox and Eclypsium said that without more cooperation and less finger-pointing by all stakeholders in the global DNS, attacks on sitting duck domains will continue to rise, with domain registrants and regular Internet users caught in the middle.</p>
<p>“Government organizations, regulators, and standards bodies should consider long-term solutions to vulnerabilities in the DNS management attack surface,” the Infoblox report concludes.</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Just Disconnect the Internet (309 pts)]]></title>
            <link>https://computer.rip/2024-07-31-just-disconnect-the-internet.html</link>
            <guid>41125490</guid>
            <pubDate>Thu, 01 Aug 2024 01:52:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://computer.rip/2024-07-31-just-disconnect-the-internet.html">https://computer.rip/2024-07-31-just-disconnect-the-internet.html</a>, See on <a href="https://news.ycombinator.com/item?id=41125490">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>


<p>So, let's say that a security vendor, we'll call them ClownStrike, accidentally
takes down most of their Windows install base with a poorly tested content
update. Rough day at the office, huh? There are lots of things you could say
about this, lots of reasons it happens this way, lots of people to blame and
not to blame, etc., etc., but nearly every time a major security incident like
this hits the news, you see a lot of people repeating an old refrain:</p>
<blockquote>
<p>these systems shouldn't be connected to the internet.</p>
</blockquote>
<p>Every time, I get a little twitch.</p>
<p>The idea that computer systems just "shouldn't be connected to the internet,"
for security or reliability purposes, is a really common one. It's got a lot of
appeal to it! But there's not really that many environments where it's done. In
this unusually applied and present-era article, I want to talk a little about
the real considerations around "just not connecting it to the internet," and
why I wish people wouldn't bring it up if they aren't ready for some serious
considerations.</p>
<h2>We Live in a Society</h2>
<p>In the abstract, computers can perform valuable work by doing, well,
computation. In practice, the computation is rarely that important. In
industry, there is a lot more "information technology" than there is
"computation." Information technology inherently needs to ingest and produce
information, and while that was once facilitated by a department of Operators
loading tapes, we have found the whole Operator thing to be costly and slow
compared to real-time communications.</p>
<p>In other words, the modern business computer is almost primarily a
communications device.</p>
<p>There are not that many practical line-of-business computer systems that
produce value without interconnection with other line-of-business computer
systems. These interconnections often cross organizational and geographical
boundaries.</p>
<p>I am thinking, for example, of the case of airline reservation and scheduling
systems disabled by the CrowdStrike, er, sorry, whatever I called them
incident. These are fundamentally communications systems, and have their
origins as replacements for the telephone and telegraph. It is not possible
to simply not internetwork them, because networking is inherent to their
function.</p>
<h2>Networking is important to maintenance and operations</h2>
<p>But let's consider systems that don't actually require real-time communications
to perform their business purpose. Network connectivity still tends to be really
valuable for these.</p>
<p>For one, consider maintenance: how does a system obtain software updates if you
have no internet connection? How is that system monitored?</p>
<p>And even if you think you can avoid those requirements by declaring a system
"complete" and without the need for any updates or real-time monitoring or
intervention, business requirements have the frustrating habit of changing over
time, and network connectivity reduces the cost of handling those changes
tremendously.</p>
<h2>What does it mean for a system to not be connected to the internet?</h2>
<p>First, we need to consider the fact that there are as many forms of "not
connected to the internet" as there are ways of being connected to the
internet. For this reason alone, proposing that a system shouldn't be
internet-connected is usually too nonspecific to really discuss. Let's consider
a menu of possibilities:</p>
<p>List 1:</p>
<ol>
<li>A single device with no network connection at all.</li>
<li>A system of devices that is "air-gapped" in the strictest sense, with no
connection to any network other than its private local-area one, where data
never crosses the security boundary.</li>
<li>That same system, but someone carries DVD-Rs across the security boundary to
introduce new data to the private network.</li>
<li>That same system, but a cross-domain solution or "data diode" allows
movement of data from a wider (or lower-security) network into the private (or
higher-security) network.</li>
<li>That same system, where the cross-domain solution does <em>not</em> have a costly
and difficult to obtain NSA certification.</li>
</ol>
<p>List 2:</p>
<ol>
<li>A system of devices which interconnect over a private wide-area network
using fully independent physical infrastructure with physical precautions
against tampering.</li>
<li>That same system, but the independent physical infrastructure is run through
commodity shared ducts.</li>
<li>That same system, but the infrastructure is leased dark fiber.</li>
<li>That same system, but the infrastructure is wavelengths on lit fiber.</li>
<li>That same system, but the infrastructure is "virtual private ethernet"
implemented by the provider using, let's say, MPLS.</li>
<li>That same system, but the infrastructure is "virtual private ethernet"
implemented by the provider using a tunneling solution with encryption and
authentication.</li>
</ol>
<p>List 3:</p>
<ol>
<li>A system of devices which interconnect over a common-carrier network (such
as, we might even dare say, the internet), where private network traffic is
tunneled through encryption and authentication performed by hardware devices.</li>
<li>That same system, but the hardware devices do not have a costly and difficult
to obtain NSA certification.</li>
<li>That same system, but the tunneling is performed by a software solution
that is well-designed such that it configures the operating system network
stack, at a low level, to prevent any traffic bypassing the tunnel, and this
has been validated by someone much smarter than me.</li>
<li>That same system, but not so well designed and validated by someone like me.</li>
<li>That same system, but the "software solution" is like Wireguard and an
iptables script that has been "thoroughly tested" by someone on Reddit.</li>
</ol>
<p>List 4:</p>
<ol>
<li>A system of devices which interconnect on a private network that has
interconnection to the internet that is strictly limited by policy-based
routing or other reliable methods, such that only very narrowly defined
traffic flows are possible.</li>
<li>That same system, but the permissible network flows are documented in
some old Jira tickets and some of them were, you know, just thrown in to
make it work.</li>
<li>That same system, but it's basically protected by a firewall that's
pretty liberal about outbound flows (maybe with IPS or something), and
pretty restrictive about inbound flows.</li>
</ol>
<p>List 5:</p>
<ol>
<li>An AWS private VPC without any routing elsewhere.</li>
<li>An AWS private VPC with PrivateLinks and other AWS networking baubles
that allow it communicate with other private VPCs.</li>
<li>That same system, but some of the interconnected VPCs can route traffic
to/from the internet.</li>
<li>An AWS private VPC with NAT GW and IGW but the security groups are set up
pretty tight in both directions.</li>
</ol>
<p>These are all things that I have seen described as non-internet-connected.
Take a moment to work through each list and mark the point at which you think
that is no longer a reasonable claim. It's okay, I'll wait.</p>
<p>I'm not going to provide threat modeling for all of these scenarios because it
would go on for pages, but you can probably see that pretty much every option
is at least slightly different in terms of attack surface and risk.</p>
<p>This might seem like an annoying or pedantic argument, but this is actually the
biggest reason I get irritated when people say that something should never be
connected to the internet. What do they <em>mean</em> by that?  When someone says that
an airline reservation system shouldn't be internet-connected, they clearly
don't actually mean the strictest form of that contention (no network
connection at all) unless their name is Adama and they liked when airline
reservation centers had big turntables of paper cards they spun around to check
off your seat. They must mean one of the midpoints presented above, which are
pretty much all coherent positions, but all positions with different
practical considerations.</p>
<p>This ambiguity makes it hard to actually, seriously consider the merits of
dropping internet connectivity.</p>
<h2>Non-internet connected systems are so very, very annoying</h2>
<p>In my day job, I work with a wide variety of clients with a wide variety of
cultures, IT architectures, and so on. Some of them are in highly regulated
industries or defense or whatever, and so they actually conduct software
operations in networks with either no internet connectivity or tightly
restricted internet connectivity.</p>
<p>When I discover this to be the case, I mentally multiply all of the
schedule/cost estimates by a factor of, I would say, 3 to 10, depending on
where they fall on the above lists (usually 3x to 5x for list 5 and 10x to a
bajillion times forever for list 1, just rule of thumb).</p>
<p>Here's the thing: virtually the entire software landscape has been designed
with the assumption of internet connectivity. Your operating system wants to
obtain its updates from online servers. If you are paying for expensive
licenses for your operating system, the vendor probably offers additional
expensive licenses for infrastructure to perform updates within your private
network. If you are getting your operating system for free-as-in-beer, there's
a good bet you can figure it out yourself, but if you're using anything to new
and cutting-edge it might be a massive hassle.</p>
<p>But that just, you know, scratches the surface. You probably develop and deploy
software using a half dozen different package managers with varying degrees of
accommodation for operating against private, internal repositories. Some of them
make this easy, some of them don't, but the worst part is that you will have to
figure it out about fifty times because of the combinatorial complexity of
multiple package managers, multiple ways of invoking them, and multiple
environments in which they are invoked.</p>
<p>If you are operating a private network, your internal services probably don't
have TLS certificates signed by a popular CA that is in root programs. You will
spend many valuable hours of your life trying to remember the default password
for the JRE's special private trust store and discovering all of the other
things that have special private trust stores, even though your operating
system provides a perfectly reasonable trust store that is relatively easy to
manage, because of Reasons. You will discover that in some tech stacks this is
consistent but in others it depends on what libraries you use.</p>
<p>A bunch of the software you use will want to perform cloud licensing and get
irritated when it cannot phone home for entitlements. You will have to go back
and forth with your vendors to figure out a workaround somewhere between "add
these ninety seven /16s to your firewall exceptions" and "wait six months while
we figure out the internal process to issue you a bespoke licensing scheme."</p>
<p>All of your stuff that requires updates or content updates will have some
different process you have to follow to obtain those updates and then provide
them internally. Here's a not at all made up example, but a real one I have
personally lived through: you will find that a particular (and particularly
hated) enterprise software vendor provides content updates for offline use only
through a customer support portal that is held over from three acquisitions
ago, and that it is only possible to get an account in that customer support
portal by getting an entitlement manually added in a different customer support
portal held over from two acquisitions ago. It will take over three months of
support tickets and escalations through your named account executive to get
accounts opened in successively older customer support portals until you can
finally get into the right one, which incidentally has an invalid TLS cert you
are reassured is not something to worry about. Once you download your offline
content update, you will find that the documented process to apply it no longer
works, and it will take a long email chain with one of the engineers to get the
right instructions. You paid a five-figure sum for a 1-year license to this
software and it has now nearly elapsed while you figured out how to use it. You
will of course get an extension on that license pro bono, because this is
enterprise software sales and what is a quarter worth of my salary between
friends, but they won't manage to issue the extension license until after
your original one has already expired, causing a painful interruption in CI
pipelines and a violent revolution by the developers.</p>
<p>I am sorry, you are not my therapist, I will try to stop remembering that dark
time in my career. Don't worry, the software in question seems to have fallen
out of favor and cannot hurt you.</p>
<p>So, like, that's an over-the-top example (but seriously, a real one!), but you
get the point. It's not really that any individual part of operating in an
offline environment is hard---I mean some of them are, but most of them aren't.
It's a death by a thousand cuts. Every single thing you ever do is harder when
you do not have internet connectivity, and you will pay for it in money and
time.</p>
<p>The largest problem by far is that almost everyone who develops software
assumes that their product will not need to operate in an offline environment,
and if they find out that it does they will fix that with duct tape and shell
scripts because it only matters for a small portion of their customers. You,
the person with the offline environment, will become the proud owner of their
technical debt.</p>
<p>None of this really <em>needs</em> to be that way, it's just how it is! There are not
really that many offline environments, and they tend to be found in big
institutions that have adapted to the fact that they make everything cost more
and take longer, and are surprisingly tolerant of vendors who perform a three
stooges routine every time you say "air-gap," because that's what pretty much
every vendor does. Except for like Red Hat, I genuinely think Red Hat is pretty
good about this, but you betcha what you save in time you are paying in cash.</p>
<h2>Not many people do this</h2>
<p>That's kind of the point, right? The problem with non-internet-connected
environments is that they are rare. The stronger versions, things from List 1
and List 2, are mostly only seen in defense and intelligence, although I have
also seen some banks with pretty impressive practices. You will note that
defense and intelligence, and even banks, are also famously industries where
everything costs way too much and takes way too long. These correlations are
probably not coincidences.</p>
<p>Even the weaker forms tend to be limited to highly-regulated industries
(finance and healthcare are the big ones), although you see the occasional
random software company that just takes security really seriously and keeps
things locked down. Occasionally.</p>
<h2>Okay, let's stop just complaining</h2>
<p>Here's the thing: I genuinely do not think that "fewer systems should be
connected to the internet" is a bad idea. I really wish that things were
different, and that every part of the software industry was more prepared and
more comfortable operating in environments with no or limited internet
connectivity. But that is not the world that we currently live in! So let's
get optimistic, what should we be doing right now?</p>
<ol>
<li>
<p>Apply restrictive network policy on as much of your stuff as possible.
Cloud providers generally make this easier than it has ever been before, it's
not all that easy but it's also not all that hard to operate a practical
non-internet-routed environment in AWS. If you stay within the lanes of all
the AWS managed services, it's mostly pain-free. You will pay for this, but,
you know, AWS always gets their check anyway.</p>
</li>
<li>
<p>Build software with offline environments in mind. Any time that you need
to phone home to get something, provide a way to disable it (if practical)
or a way to override the endpoint that will be used. If the latter, keep in
mind that you will also need to come up with a way for a customer to feasibly
host their own endpoint. If you keep to simple static files, that's really
easy, just nginx and a directory or whatever. If it's an API or something,
well, you're probably going to have to ship your internal implementation.
Brace yourself for the maintenance overhead.</p>
</li>
<li>
<p>Try to think about the little assumptions that go into connecting to other
services that become more complex in an offline environment. Please, for the
love of God, do not assume you can reach LetsEncrypt. But that's not the only
TLS problem, offline environments virtually always imply internal certificate
authorities. <em>Use the system trust store.</em> Please. I am begging you.</p>
</li>
<li>
<p>Avoid fetching any kind of requirements or dependencies at deploy time. One
of the advantages Docker supposedly brought us was making all of the
requirements of a given package self-contained, but then I still run into
Docker containers that can't start if they can't reach the npm repos or
something. And now I have yet another place to fix configuration and trust
store and etc., in your stupid Docker container. It has made things more
difficult instead of less.</p>
</li>
</ol>
<p>Have I mentioned that Docker, paradoxically, actually makes offline
environments <em>more</em> difficult to manage? Yeah, because virtually every
third-party Docker container has <em>at least</em> a TLS trust store you'll have to
modify. Docker is, itself, a profound example of how the modern software
industry simply assumes that everything is running On The Internet.</p>
<h2>Anyway</h2>
<p>I wrote this out in a bit of a huff because I have seen "why were they
connected to the internet at all?" like four times in response to the
CrowdStrike incident. I know, I am committing the cardinal sin of taking things
that people on the internet say seriously, but I feel obligated to point out:
internet connectivity is pretty much completely orthogonal to what happened.
CrowdStrike content updates are the kind of thing that, in a perfect world,
you would promptly make available in your offline environment. In practice, an
internal CrowdStrike update mirror would probably lag days, weeks, months, or
years behind, because that's what usually ends up happening in "hard" offline
environments, but that's a case of two wrongs making a right.</p>
<p>Which they do, more often than you would think, in the world of information
technology.</p>
<p>Don't worry, I'll be back next time with something more carefully written and
less relevant to the world we live in. I just got in a mood, you know? I just
spent like half the day copying Docker images into an offline environment and
then fixing them all. I have to find something to occupy the time while a
certain endpoint security agent pegs the CPU and makes every "docker save" take
ten minutes.</p>
	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SnowflakeOS: Beginner friendly and GUI focused NixOS variant (195 pts)]]></title>
            <link>https://snowflakeos.org/</link>
            <guid>41124472</guid>
            <pubDate>Wed, 31 Jul 2024 23:06:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://snowflakeos.org/">https://snowflakeos.org/</a>, See on <a href="https://news.ycombinator.com/item?id=41124472">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <div>
                        <h2>Simple, Immutable, Reproducible</h2>
                        <p>SnowflakeOS is a <a href="https://nixos.org/">NixOS</a> based Linux distribution focused on
                            beginner friendliness and ease of use.</p>
                    </div>
                <div>
                    <figure>
                        <img alt="SnowflakeOS desktop" src="https://snowflakeos.org/assets/snowflakeos.png">
                    </figure>
                    
                    <p>Not yet ready for daily use!</p>
                </div>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Suspicious data pattern in recent Venezuelan election (824 pts)]]></title>
            <link>https://statmodeling.stat.columbia.edu/2024/07/31/suspicious-data-pattern-in-recent-venezuelan-election/</link>
            <guid>41123155</guid>
            <pubDate>Wed, 31 Jul 2024 20:34:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://statmodeling.stat.columbia.edu/2024/07/31/suspicious-data-pattern-in-recent-venezuelan-election/">https://statmodeling.stat.columbia.edu/2024/07/31/suspicious-data-pattern-in-recent-venezuelan-election/</a>, See on <a href="https://news.ycombinator.com/item?id=41123155">Hacker News</a></p>
Couldn't get https://statmodeling.stat.columbia.edu/2024/07/31/suspicious-data-pattern-in-recent-venezuelan-election/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[An affordable, portable and focused device for music, writing and coding (107 pts)]]></title>
            <link>https://tulip.computer/</link>
            <guid>41122986</guid>
            <pubDate>Wed, 31 Jul 2024 20:15:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tulip.computer/">https://tulip.computer/</a>, See on <a href="https://news.ycombinator.com/item?id=41122986">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <div data-aos="fade-right" id="top">
          <h2><span>The <br>Tulip<br>Creative<br> Computer</span></h2>
          <p><span>An affordable, portable and focused device for music, writing and coding.</span></p>
          <p><a href="#get">Get a Tulip for only US$59</a>
        </p></div>



<div id="more">
    <div>
        <p data-aos="fade-down"><h2>Tulip is an all in one portable computer running the Python programming language, with a touchscreen and music synthesizer.</h2></p>
    </div>
    <p><img src="https://tulip.computer/img/tulip_hero.jpg">
    </p>

    <div>

      <div data-aos="fade-up" data-aos-delay="200">
        <p><span>Simple, focused and fun</span></p><p>Tulip's processor is a low-power real time microcontroller. It boots right into a Python prompt. We provide a code editor and we ship music programs and other examples. It only does what you ask it to. It has no web browser or social media, other than our fun Tulip-only BBS <strong>Tulip WORLD</strong> for sharing files. Its constraints and single focus should help you make amazing creative work.</p>
      </div>

      <div data-aos="fade-up" data-aos-delay="200">
        <p><span>Make your art in code</span></p><p>Tulip ships with the <a href="https://github.com/shorepine/amy">AMY synthesizer</a>, a fully featured additive, subtractive and FM synth (think DX-7 and Juno-6) and you can control every parameter of every oscillator in code. Our graphics API is fully programmable as well, with hardware sprites and scrolling backgrounds. Write your own games or synth UIs in <a href="https://lvgl.io/">LVGL</a> on the Tulip touchscreen and control your creations over MIDI, I2C, or even Wi-Fi.</p>
      </div>

      <div data-aos="fade-up" data-aos-delay="200">
        <p><span>Completely open source and cheap as possible</span></p><p>You're paying the cost of the parts and manufacturing for your US$59 Tulip, with a tiny fee added on to support future development. Tulip is <a href="https://github.com/shorepine/tulipcc">completely open source</a>, from the hardware to the OS to the synthesizer DSP code. It's made possible by excited volunteers and <a href="#involved">we'd love your help!</a> Tulip is just as fun to work on as to make things on.</p>
      </div>
    </div>
  </div>



<div id="more">
    <div>
        <p data-aos="fade-down"><h2>Tulip is packed with features, connectivity and APIs for you to make or build anything you can imagine.</h2></p>
    </div>
    <p><img src="https://tulip.computer/img/tulipcc-voices.jpeg">
    </p>

    <div data-aos="fade-up" data-aos-delay="200">
        <p><span>Synthesizer</span></p><p>Tulip's underlying synthesizer, <a href="https://github.com/shorepine/amy">AMY</a>, supports up to 120 oscillators, stereo sound, filters, reverb, chorus, FM, PCM samples (baked in or loaded from a file), and comes with a Python library for managing voices and patches for polyphony and multitimbral operation. <a href="https://github.com/shorepine/tulipcc/blob/main/docs/music.md">You can write all your patches, music or interactions with other synths in pure Python.</a></p>
        <p><a href="https://github.com/shorepine/tulipcc/blob/main/docs/music.md">Make music in code on Tulip</a></p>


      </div>
  </div>

<div>
      <p>
    <h2>Write your music in Python. <a href="https://github.com/shorepine/tulipcc/blob/main/docs/music.md">See more</a></h2>
  </p>
    <div>
      <pre><code> 
# Play a random note of an F minor 7 chord. 
# Beat syncs with other running music programs
import tulip, midi, music, random

chord = music.Chord("F:min7").midinotes()
synth = midi.Synth(1) # single note polyphony
synth.program_change(143) # DX7 BASS 2 patch

def note(t):
    synth.note_on(random.choice(chord), 0.6, time=t)

# Call note() every 24 ticks (twice a quarter note)
slot = tulip.seq_add_callback(note, 24)
      </code></pre>
    </div>
  </div>

<div data-aos="fade" data-aos-delay="200">
  <p>
    <h2>How does it sound? Check it out:</h2>
  </p>
  <p>


        <iframe width="700" height="540" src="https://www.youtube.com/embed/1lYFjQp7Xrw?si=FaSXGabzvbs0BGeF" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

  </p>
</div>



<div id="more">
    <div>
        <p data-aos="fade-down"><h2>Make multichannel sound installations with an optional Alles speaker, or 30</h2></p>
    </div>
    <p><img src="https://tulip.computer/img/nicoboard-alles.jpg">
    </p>

    <div data-aos="fade-up" data-aos-delay="200">
        <p><span>Alles</span></p><p>With an optional <a href="https://github.com/shorepine/alles">Alles speaker</a> (or up to a few dozen!) you can perform multichannel audio in Tulip. The underlying synthesizer can address multiple Alles speakers over a Wi-Fi mesh. Each speaker has the same synthesizer capabilities as in Tulip. You can address them individually or in groups, and send all the same commands you can send on the built-in synth in Tulip. Make stunning whole-room installations of dozens of channels all powered from one Tulip.</p>
        <p><a href="https://notes.variogram.com/2022/09/23/alles-amy/">Read more about Alles</a></p>


      </div>
  </div>



<div id="get">
        <p>
          <h2>Buy a Tulip, DAC or Alles. Ships anywhere in the world.</h2>
        </p>
    <div>

        <div>
          <p><a href="https://www.makerfabs.com/the-tulip-creative-computer.html"><img src="https://tulip.computer/img/tulip-shop-3.jpg" alt="Tulip Creative Computer"></a></p>
        </div>


         <div>
          <p><a href="https://www.makerfabs.com/mabee-dac-gp8413.html"><img src="https://tulip.computer/img/mabee-dac.jpg" alt="Mabee DAC"></a></p><div>
            <h5>Two-channel DAC</h5>
            <p>Plugs right into a Tulip to give you two channels of CV control over modular synths with standard 3.5mm patch cables. With some light modification, you can have up to 4 running at once for 8 channels.</p>
            <p><a href="https://www.makerfabs.com/mabee-dac-gp8413.html">US$5.80 on Makerfabs</a>
          </p></div>
        </div>

         <div>
          <p><a href="https://shop.blinkinlabs.com/products/alles-pcb"><img src="https://tulip.computer/img/alles.jpg" alt="Alles"></a></p>
      </div>

  </div>
</div>




<div id="involved">
  <div>
    <p>
      <h3>Get involved</h3>
    </p>

    
    <div>

      <div data-aos="fade-up">
          <p><a href="https://discord.com/invite/TzBFkUb8pG"><img src="https://tulip.computer/img/discord-mark-black.svg" width="42" height="42"></a>
          </p>
          <div>
            <a href="https://discord.com/invite/TzBFkUb8pG"><h3>Discord</h3></a>
            <p>Join the <strong>shore pine sound systems</strong> Discord to chat about Tulip, AMY and Alles. A fun small community of people experimenting with Tulip.</p>
          </div>
        </div>

      <div data-aos="fade-up" data-aos-delay="200">
          <p><a href="https://github.com/shorepine/tulipcc"><img src="https://tulip.computer/img/github-mark.svg" width="42" height="42/"></a>
          </p>
          <div>
             <a href="https://github.com/shorepine/tulipcc"><h3>Github</h3></a>
            <p>Check out the Tulip Github page for issues, discussions and the code.</p>
          </div>
        </div>


    </div>
  </div>

<div data-aos="fade" data-aos-delay="200">
      <h3>Join our email list</h3>
      <p>We'll send you <strong>very rare</strong> updates about Tulip, Alles, AMY and other projects we're working on.</p>
    </div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Foobar2000 (254 pts)]]></title>
            <link>https://www.foobar2000.org/</link>
            <guid>41122920</guid>
            <pubDate>Wed, 31 Jul 2024 20:05:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.foobar2000.org/">https://www.foobar2000.org/</a>, See on <a href="https://news.ycombinator.com/item?id=41122920">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p><a href="https://www.foobar2000.org/screenshots" title="View more screenshots"><img id="pagepic" alt="View more screenshots" src="https://www.foobar2000.org/tinyfoobar.png"></a>


  foobar2000 is an advanced freeware audio player for the Windows platform.
 </p><h2>Latest news</h2>
  <div>
	<h3>2024-05-21</h3>
<p>
    New releases of old versions!
</p>
<p>
    Some of latest bug fixes have been backported to foobar2000 v1.5 and v1.6 series. <br>
    Versions 1.6.18 and 1.5.12 can be downloaded from <a href="https://www.foobar2000.org/old">old versions page</a>. <br>
    Additionally, version 1.5.12 was properly tested on old hardware; unintended SSE CPU requirement present in previous releases has been removed.
</p><h3>2024-05-20</h3>
<p>
    foobar2000 mobile v1.5 has been released. <br>
    This version introduces <a href="https://www.foobar2000.org/mobile/skinformat">new skin file format</a> which can be edited using commonly available tools. <br>
    <a href="https://www.foobar2000.org/apk">Download Android APK...</a>
</p><h3>2023-12-18</h3>
<p>
    foobar2000 v2.1 final has been released. <br>
    <a href="https://www.foobar2000.org/download">Download...</a>
</p>
<p>
    foobar2000 for Mac v2.6 final has also been released. <br>
    <a href="https://www.foobar2000.org/mac">Download foobar2000 for Mac...</a>
</p><h3> <a href="https://www.foobar2000.org/?page=News">View all news</a> </h3>
</div>
      <table>
            <caption>
          advertisement
        </caption>
      <tbody><tr> 
          <td>

          <div onclick="document.location.href='https://www.dbpoweramp.com/dmc.htm?fb=1';">
            <p><span>dBpoweramp mp3 Converter</span>
            <br>
              <span> music conversion perfected<br>&nbsp;</span>
              <br>
                <img alt="dmc" src="https://www.dbpoweramp.com/images/dmc/dmc.png" width="160" height="122"></p><p>&nbsp;
                      <span>Trusted by 30 million people, easy conversion between audio formats</span></p></div>

        </td>

            <td>

              <div onclick="document.location.href='https://www.dbpoweramp.com/perfecttunes.htm?fb=1';">
            <p><span>PerfectTUNES</span>
            <br>
              <span>
                a helping hand for your audio collection<br>&nbsp;</span>
              <br>
                <img alt="" height="122" src="https://www.dbpoweramp.com/images/pt-art-main.png" width="106"></p><p>
                    &nbsp;
                      <span>Add or upgrade Album Art, De-Dup and check for ripping errors</span></p></div>

        </td>

            <td>

              <div onclick="document.location.href='https://www.dbpoweramp.com/cd-ripper.htm?fb=1';">
            <p><span>dBpoweramp CD Ripper</span>
            <br>
              <span>
                CD ripping taken seriously<br>&nbsp;</span>
              <br>
                <img alt="cdripper" height="122" src="https://www.dbpoweramp.com/images/cd-ripper-secure.png" width="80"></p><p>
                    &nbsp;
                      <span>Secure Ripping from the inventors of AccurateRip, fast &amp; bit-perfect CD ripping</span></p></div>

        </td>

      </tr>
  </tbody></table>
  
    <h2>Main features</h2>
  <ul>
    <li>Supported audio formats: MP3, MP4, AAC, CD Audio, WMA, Vorbis, Opus, FLAC, WavPack, WAV, AIFF, Musepack, Speex, AU, SND... and more with additional <a href="https://www.foobar2000.org/?page=Download#components">components</a>.</li>
    <li><a href="https://wiki.hydrogenaudio.org/index.php?title=Gapless" rel="nofollow">Gapless</a> playback.</li>
    <li>Easily <a href="https://wiki.hydrogenaudio.org/index.php?title=Foobar2000:Layout_Editing_Mode" rel="nofollow">customizable user interface layout</a>.</li>
    <li>Advanced <a href="https://wiki.hydrogenaudio.org/index.php?title=Foobar2000redirect:C16D48EE-39BE-4C91-9A35-441BEFA286D2" rel="nofollow">tagging capabilities</a>.</li>
    <li>Support for ripping Audio CDs as well as transcoding all supported audio formats using the <a href="https://www.foobar2000.org/FAQ#converting_audio_files_to_different_file_formats">Converter component</a>.</li>
    <li>Full <a href="https://wiki.hydrogenaudio.org/index.php?title=Replaygain" rel="nofollow">ReplayGain</a> support.</li>
    <li>Customizable keyboard shortcuts.</li>
    <li>Open component architecture allowing third-party developers to extend functionality of the player.</li>
  </ul>
 </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cardie – An open source business card designer and sharing platform (147 pts)]]></title>
            <link>https://github.com/nfoert/cardie</link>
            <guid>41122793</guid>
            <pubDate>Wed, 31 Jul 2024 19:50:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/nfoert/cardie">https://github.com/nfoert/cardie</a>, See on <a href="https://news.ycombinator.com/item?id=41122793">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/nfoert/cardie/blob/main/repo/images/logo_light.png#gh-dark-mode-only"><img src="https://github.com/nfoert/cardie/raw/main/repo/images/logo_light.png#gh-dark-mode-only" alt="Cardie Logo"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/nfoert/cardie/blob/main/repo/images/logo_dark.png#gh-light-mode-only"><img src="https://github.com/nfoert/cardie/raw/main/repo/images/logo_dark.png#gh-light-mode-only" alt="Cardie Logo"></a></p>


<p dir="auto">Design a unlimited number of business or information cards about yourself, share a link or QR code to them, print it out, and save other people's cards to your virtual wallet for later. Once you've created a card you can get analytics data on how your cards are getting visited, you can edit your cards as things change, and you can keep cards private so only people with a link to your card can see it.</p>
<p><a href="https://skillicons.dev/" rel="nofollow">
    <img src="https://camo.githubusercontent.com/bd08d39a13b6f75dda24a81596f3aaad051fd93dd2dea9aa285300ab1300633c/68747470733a2f2f736b696c6c69636f6e732e6465762f69636f6e733f693d646a616e676f2c707974686f6e2c68746d6c2c6373732c6a732c6769746875622c6769742c616c70696e656a73" data-canonical-src="https://skillicons.dev/icons?i=django,python,html,css,js,github,git,alpinejs">
  </a>
</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/nfoert/cardie/blob/main/repo/images/screenshot1.png"><img src="https://github.com/nfoert/cardie/raw/main/repo/images/screenshot1.png"></a>
</p>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">Cardie is currently in an open alpha. Things will be rapidly changing and bugs are to be expected.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">First, clone this repository using the following command</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/nfoert/cardie"><pre><code>git clone https://github.com/nfoert/cardie
</code></pre></div>
<p dir="auto">Then, navigate to that directory and create a new python virtual environment</p>
<div data-snippet-clipboard-copy-content="cd cardie
python3 -m venv .venv"><pre><code>cd cardie
python3 -m venv .venv
</code></pre></div>
<p dir="auto">Activate the virtual environment using the command for your system (Linux is used here) and install the required dependencies</p>
<div data-snippet-clipboard-copy-content="source ./.venv/bin/activate
pip install -r requirements.txt"><pre><code>source ./.venv/bin/activate
pip install -r requirements.txt
</code></pre></div>
<p dir="auto">Next, create a django superuser and make and migrate the models</p>
<div data-snippet-clipboard-copy-content="cd cardie
python manage.py createsuperuser
python manage.py makemigrations
python manage.py migrate"><pre><code>cd cardie
python manage.py createsuperuser
python manage.py makemigrations
python manage.py migrate
</code></pre></div>
<p dir="auto">Now just run the server using the following command, or run the <code>Start server</code> task in your Visual Studio Code</p>
<div data-snippet-clipboard-copy-content="python manage.py runserver"><pre><code>python manage.py runserver
</code></pre></div>
<p dir="auto">Finally, navigate to <code>http://127.0.0.1:8000/admin</code> and log in using your new administrator account. Create a new <code>Server</code> object and be sure to configure the <code>ip</code> to be <code>http://127.0.0.1:8000</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Additional steps for Production installation</h3><a id="user-content-additional-steps-for-production-installation" aria-label="Permalink: Additional steps for Production installation" href="#additional-steps-for-production-installation"></a></p>
<p dir="auto">This depends on what server hosting provider you're using. However, there's a couple environment variables you need to set and there's a run command.</p>
<p dir="auto">Set the following global environment variables:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>DJANGO_ALLOWED_HOSTS</code> -&gt; <code>${APP_DOMAIN}</code> (This works on DigitalOcean, this may not work on every hosting provider)</p>
</li>
<li>
<p dir="auto"><code>DJANGO_SETTINGS_MODULE</code> -&gt; <code>cardie.settings_production</code></p>
</li>
<li>
<p dir="auto"><code>DJANGO_LOG_LEVEL</code> -&gt; <code>WARNING</code></p>
</li>
<li>
<p dir="auto"><code>STATIC_URL</code> -&gt; <code>/static/main</code></p>
</li>
<li>
<p dir="auto"><code>SECRET_KEY</code> -&gt; <code>&lt;your new secret key&gt;</code> (Generate this using <code>django.core.management.utils.get_random_secret_key()</code>. If possible you should encrypt this value in your hosting provider.)</p>
</li>
<li>
<p dir="auto"><code>DEBUG</code> -&gt; <code>False</code></p>
</li>
<li>
<p dir="auto"><code>DATABASE_URL</code> -&gt; <code>${db.DATABASE_URL}</code> (This works on DigitalOcean, this may not work on every hosting provider)</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">To Do</h2><a id="user-content-to-do" aria-label="Permalink: To Do" href="#to-do"></a></p>
<p dir="auto">There's lots of things that need implemented or changed in this project. Please see <a href="https://github.com/nfoert/cardie/blob/main/TODO.md">TODO.md</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">I'd love to see contributions to this project! Please check out the <a href="https://github.com/nfoert/cardie/issues">issues</a> page to see what things currently need fixed or added.</p>
<p dir="auto">Additionally, check <a href="https://github.com/nfoert/cardie/blob/main/TODO.md">TODO.md</a> for a rough todo list of things that need implemented, and the <a href="https://github.com/nfoert/cardie/wiki">wiki</a> for some information on how to work with some of the existing systems.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[After years of leniency, ULA cracks down on hobbyist photographers (125 pts)]]></title>
            <link>https://arstechnica.com/space/2024/07/ula-to-amateur-launch-photographers-work-for-me-but-not-for-thee/</link>
            <guid>41121689</guid>
            <pubDate>Wed, 31 Jul 2024 18:13:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/space/2024/07/ula-to-amateur-launch-photographers-work-for-me-but-not-for-thee/">https://arstechnica.com/space/2024/07/ula-to-amateur-launch-photographers-work-for-me-but-not-for-thee/</a>, See on <a href="https://news.ycombinator.com/item?id=41121689">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/07/53891838413_2c9f565c76_k-1-800x533.jpg" alt="This image of an Atlas V launch on Tuesday morning has been intentionally blurred.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/07/53891838413_2c9f565c76_k-1.jpg" data-height="1365" data-width="2048">Enlarge</a> <span>/</span> This image of an Atlas V launch on Tuesday morning has been intentionally blurred.</p><p>United Launch Alliance</p></figcaption>  </figure>

  




<!-- cache hit 82:single/related:08b9e0df1f67cb48c06d7d6cec8d2f86 --><!-- empty -->
<p>The emails from United Launch Alliance started popping into the inboxes of photographers a few days after the Fourth of July holiday. Although that day is meant to celebrate freedom and the red glare of rockets, the communication threatened to strip both from some of the company's most ardent devotees.</p>
<p>The message from the launch company announced the implementation of a new "annual agreement" between ULA and all people who place remote cameras at Space Launch Complex-41, the company's active launch site at Cape Canaveral Space Force Station. Anyone interested in setting remotes for future launch dates had 11 days to review and sign the agreement.</p>
<p>The language was clear: Photographers were welcome to set up remote shots at ULA launches if they worked for the media or wanted to post their work on social media. However, photographers could not sell this work independently, including as prints for fellow enthusiasts or for use in annual calendars.</p>
<p>"ULA will periodically confirm editorial publication for media participating in remote camera placement," the email stated. "If publication does not occur, or photos are sold outside of editorial purposes, privileges to place remote cameras may be revoked."</p>
<p>To the photographers who spend many hours preparing their equipment, waiting to set up and remove cameras, and persevering through scrubs and more, it seemed like a harsh judgment.</p>
<p>And nobody knew why it happened.</p>
<h2>No comment</h2>
<p>United Launch Alliance has offered no public comment about the new policy. The company did not respond to questions from Ars Technica about the agreement. And the company's chief executive, Tory Bruno, a frequent tweeter who regularly interacts with fans on the social media site X, has ignored dozens of questions about the policy change. Since the first questions were raised a few days ago, Bruno has not replied to anyone on X.</p>                                                                        
                                                                                
<p>The photographers themselves felt blindsided by the decision.</p>
<p>"I cannot sit by while myself and my colleagues are actively being forbidden from trying to support ourselves to be able to do what we do," said David Diebold, a photographer for Space Scout, <a href="https://x.com/DavidJDPhotos/status/1817268386939666519">on X</a>. "Being forced to sign an agreement that is a net negative for all of the media is the last thing I'll do. If this is the end of the line for covering ULA missions up close, then so be it."</p>
<p>Other photographers shared similar sentiments privately, but they did not want to be seen publicly calling out ULA, the second-most important launch provider in the United States.</p>
<p>The new rules went into effect on Tuesday with <a href="https://arstechnica.com/space/2024/07/with-a-landmark-launch-the-pentagon-is-finally-free-of-russian-rocket-engines/">the final launch of an Atlas V rocket for a national security mission</a>. A ULA representative told participating photographers that the intent of the new rules was to prohibit the sale of images to any commercial entities, including prints to individuals, except for news publications.</p>
<p>There was no explanation given for why.</p>
<h2>How did we get here?</h2>
<p>For a long time, the rules for accessing the press site at Kennedy Space Center in Florida and setting up remote cameras for launches there and at the military launch pads were clear. You had to be working press or have a letter from a publication that you were on assignment. But a decade and a half ago, several things occurred that began to change this.</p>
<p>As the Space Shuttle program wound down, NASA sought to induce positive publicity by inviting social media participants to launches and other events. The events were initially called "<a href="https://en.wikipedia.org/wiki/NASA_Social">Tweetups</a>" at their inception in 2009, and later "NASA Socials." The space agency provided access to non-media spaceflight enthusiasts, who then shared their experiences on Twitter and other social media outlets. The space agency further blurred the lines between traditional reporters and social media enthusiasts by inviting these participants to some news conferences.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stop Destroying Videogames – European Citizens' Initiative (113 pts)]]></title>
            <link>https://citizens-initiative.europa.eu/initiatives/details/2024/000007_en</link>
            <guid>41121570</guid>
            <pubDate>Wed, 31 Jul 2024 18:02:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://citizens-initiative.europa.eu/initiatives/details/2024/000007_en">https://citizens-initiative.europa.eu/initiatives/details/2024/000007_en</a>, See on <a href="https://news.ycombinator.com/item?id=41121570">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                  <p>European Citizens' Initiative</p>
                      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta Has Run Hundreds of Ads for Cocaine, Opioids and Other Drugs (164 pts)]]></title>
            <link>https://www.wsj.com/tech/meta-cocaine-opioids-ads-dea8e0fc</link>
            <guid>41121237</guid>
            <pubDate>Wed, 31 Jul 2024 17:30:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/tech/meta-cocaine-opioids-ads-dea8e0fc">https://www.wsj.com/tech/meta-cocaine-opioids-ads-dea8e0fc</a>, See on <a href="https://news.ycombinator.com/item?id=41121237">Hacker News</a></p>
Couldn't get https://www.wsj.com/tech/meta-cocaine-opioids-ads-dea8e0fc: Error: Request failed with status code 401]]></description>
        </item>
    </channel>
</rss>