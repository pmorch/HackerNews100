<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 17 Jul 2024 05:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[After 12 years of reviewing restaurants, I'm leaving the table (103 pts)]]></title>
            <link>https://www.nytimes.com/2024/07/16/dining/pete-wells-steps-down-food-critic.html</link>
            <guid>40979539</guid>
            <pubDate>Tue, 16 Jul 2024 19:33:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/07/16/dining/pete-wells-steps-down-food-critic.html">https://www.nytimes.com/2024/07/16/dining/pete-wells-steps-down-food-critic.html</a>, See on <a href="https://news.ycombinator.com/item?id=40979539">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/07/16/dining/pete-wells-steps-down-food-critic.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[DevRel at HuggingFace (142 pts)]]></title>
            <link>https://dx.tips/huggingface</link>
            <guid>40979221</guid>
            <pubDate>Tue, 16 Jul 2024 18:56:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dx.tips/huggingface">https://dx.tips/huggingface</a>, See on <a href="https://news.ycombinator.com/item?id=40979221">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content-parent"><blockquote>
<p>Following the <a target="_blank" href="https://dx.tips/zirp">ZIRP DevRel</a> post, the community has had many great discussions on where devrel needs to go next. DXTips exists to share this tacit niche industry knowledge. <strong>DX@X</strong> is our new async interview series we are starting with DevRel leaders to get more perspectives on the state of the art in DX and DevRel. We're excited to kick it off with <strong><a target="_blank" href="https://x.com/osanseviero">Omar Sanseviero</a>, Chief Llama Officer at HuggingFace</strong>!</p>
<p>HuggingFace is well known for being incredible stewards of the open source ML community, building critical infrastructure at hypergrowth (growing from 780k to 2.3m repos on the HF Hub in the past year) and doing so <a target="_blank" href="https://analyticsindiamag.com/ai-news-updates/hugging-face-announces-profitability-with-free-and-open-source-models/">profitably</a>. They are also a rare startup whose large online community also translates to <a target="_blank" href="https://x.com/search?q=huggingface%20woodstock&amp;src=recent_search_click&amp;f=top"><em>massive</em> multi-thousand people meetups</a> all <a target="_blank" href="https://x.com/search?q=huggingface%20station%20f&amp;src=typed_query&amp;f=top">over the world</a>.</p>
<p><strong>Request for Suggestions: Who else would you like to hear from? Let us know on <a target="_blank" href="https://x.com/dxtipshq">X/Twitter</a> and join our <a target="_blank" href="https://dx.tips/newsletter">Newsletter</a> to get the next issue!</strong></p>
</blockquote>
<h2 id="heading-introduction-to-omar-and-huggingface">Introduction to Omar and HuggingFace</h2>
<blockquote>
<p><strong>Intro:</strong> <em>Hey Omar! Let’s assume people know the surface level of HuggingFace - it’s the largest AI community which collaborates on open source models, datasets, and applications, with paid compute and enterprise solutions. What does a Chief Llama Officer do at HF?</em></p>
</blockquote>
<p><a target="_blank" href="https://x.com/osanseviero/media">Memes</a>! More seriously, my title might translate to “<strong>Head of Platform and Community</strong>” at another company, although the scope of what I do is quite broad. There are two aspects to my role:</p>
<ul>
<li><strong>Leadership</strong>: Within HF, my role involves horizontal and vertical leadership. Vertically, I direct a family of teams (Dev Advocacy Engineering, On-device ML, Moonshot Factory, Argilla - our most recent acquisition). Horizontally, our team sits at the intersection of Open Source, Product, and the external community (+ sometimes research). In my day-to-day, I aim to identify high-impact potential areas, connect dots across teams at HF and the community, and unblock people to succeed.</li>
<li><strong>IC</strong>: HF has a very bottom-up leadership culture. This, combined with a meeting-less async culture (<a target="_blank" href="https://x.com/mervenoyann/status/1692111147783143751">example</a>, <a target="_blank" href="https://x.com/SashaMTL/status/1773344913502929014">example</a>, <a target="_blank" href="https://x.com/osanseviero/status/1573055162070999061">example</a>, <a target="_blank" href="https://www.hbs.edu/faculty/Pages/item.aspx?num=63185">old HBS case study</a>), allows folks in leadership positions to dedicate significant time to technical and meaningful contributions to different projects. A significant part of my role involves collaborating with partners to release new models, such as the latest Llama and Gemma models. Each release is unique, intense, and fun, and I quite enjoy being deeply involved in the entire process.</li>
</ul>
<blockquote>
<p><strong>Followup</strong>: <em>What do you think is under-appreciated about HF’s open source work?</em></p>
</blockquote>
<p>Hugging Face is a community-centric company. It's hard to exaggerate how community-centric we are. Some examples:</p>
<ul>
<li>We prioritize giving the spotlight to community members and collaborators as much as possible.</li>
<li>Provide compute and no-strings-attached cash grants (including but not limited to <a target="_blank" href="https://www.theverge.com/2024/5/16/24156755/hugging-face-celement-delangue-free-shared-gpus-ai">the $10m ZeroGPU program</a>) to community members/communities (for example, in the past, Eleuther, Boris from Dall-e Mini, and lucidrains have been sponsored by HF, allowing them to keep doing their cool work without financial constraints).</li>
<li>Help maintain open-source libraries (eg <a target="_blank" href="https://github.com/UKPLab/sentence-transformers">sentence transformers</a> and <a target="_blank" href="https://github.com/bitsandbytes-foundation/bitsandbytes">bitsandbytes</a>) from other groups and closely collaborate with other tools (eg <a target="_blank" href="https://github.com/EleutherAI/lm-evaluation-harness/tree/main">LM Eval Harness</a> and <a target="_blank" href="https://github.com/mlfoundations/open_clip">OpenCLIP</a>)</li>
</ul>
<p>Our approach to working with other groups and open-source platforms and libraries is always collaborative. We view ourselves as "the Switzerland" of the ML community, actively contributing to and supporting the ML ecosystem. We want the community to be successful and grow the pie.</p>
<p>So, one aspect of HF that I think is underappreciated is the extent of the support and collaboration with the community. Many see the outputs—like models and libraries—but might not realize the significant behind-the-scenes effort that the team puts into fostering the thriving ecosystem.</p>
<h2 id="heading-devrel-at-huggingface-metrics-and-velocity">DevRel at HuggingFace: Metrics and Velocity</h2>
<blockquote>
<p><strong>Credibility/Success:</strong> <em>What are some “real” metrics that you track that point to HF’s devrel success?</em></p>
</blockquote>
<p>DevRel comes in all kinds of flavors in the industry. Some DevRel teams are part of a marketing function, and some are within a monetization-team function. At Hugging Face, DevRel sits between the open-source and product organizations and is primarily an engineering function.</p>
<p>This means <strong>HF DevRel's goal is to see increased usage of the Hub platform and open-source tools</strong> rather than focusing on revenue as our primary goal. Two of our north stars are the <strong>number of repositories on HF and logged-in usage of the Hub</strong> platform. For example, the Hub has 2.3 million repositories today, compared to 780k repositories a year ago. (Of course, we can look at everything with more granularity, e.g., the number of Spaces, which grew from 187k in June last year to 650k this year).</p>
<p><img loading="lazy" src="https://gist.github.com/user-attachments/assets/ca97fe47-127d-4bc6-9d05-b4f2c0fdf863" alt="image"></p>
<p>Each team member works on different topics (e.g., Computer Vision, Audio ML, ML for 3D CV, etc.), so we jointly define some metrics that we would like to see move based on our efforts. <strong>We prioritize usage-based</strong> (number of repos, downloads, installs) <strong>over visibility-driven</strong> (GitHub stars, Twitter likes, views), which are also valuable but not the main motivation of our work.</p>
<p>That said, I'm skeptical of cultures that overemphasize metrics (of course, this is nuanced and depends on a lot of context). From my experience at Google and looking at other startups, I've seen the downsides of measuring too much too early. Metrics are an imperfect proxy for impact and are game-able. <strong>Cultures prioritizing metrics above all risk losing sight of user needs and making wrong decisions</strong> (e.g., to improve metrics for their performance review rather than genuine user benefit). Some DevRel activities might not have immediate metric changes but have long-term impact.</p>
<p>One of the most rewarding moments was after two years of building connections with Spanish-speaking folks, we <a target="_blank" href="https://platzi.com/blog/ayuda-a-mejorar-los-llm-en-espanol-en-7-sencillos-pasos/">initiated</a> an exciting Alpaca translation effort involving Argilla, Platzi (a Colombian edtech), and many community super-users. This 'Avengers assemble' moment is becoming more frequent as we foster stronger relationships with practitioners, communities, and organizations. Examples of these are <a target="_blank" href="https://x.com/_lewtun/status/1778429536264188214">Zephyr ORPO</a> (KAIST + HF + Argilla), <a target="_blank" href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">QLoRA</a> (University of Washington), and the very recent <a target="_blank" href="https://huggingface.co/blog/winning-aimo-progress-prize">AI Math Olympiad winner</a> (NuminaM + HF).</p>
<blockquote>
<p><strong>DevRel @ HF:</strong> <em>What was your reaction to <a target="_blank" href="https://dx.tips/zirp">the ZIRP DevRel article</a>? What’s different at HF?</em></p>
</blockquote>
<p>As mentioned before, DevRel comes in all kinds of flavors. There were things that I could relate to. Specially two points:</p>
<ul>
<li>I'm a bit skeptical of the impact of traveling to conferences. While conferences can be impactful if approached strategically, the highest impact usually doesn't come from giving a talk. Instead, it's often the connections made and behind-the-scenes collaborations which require a different mindset. We support conference travel (and people do that a lot), but we encourage team members to attend with an impact-driven mindset, ready to achieve some concrete things beyond attending an event.</li>
<li>Lack of OKRs. <strong>We do not have OKRs</strong>. The ML ecosystem moves incredibly fast, so we need to be nimble and action-driven. Gemini Nano was added to Chrome? Let's figure out how to run it and <a target="_blank" href="https://x.com/xenovacom/status/1810356703826977183">release some docs</a>. Model 504B is coming out next month; let's make sure it's usable by the community. Although exciting, this comes with cons: priorities can and will change, planning becomes challenging, and maintaining focus can be difficult in the chaos of the current ML space.</li>
</ul>
<p>That said, I think HF DevRel has been successful overall for a couple of reasons:</p>
<ul>
<li>It's an <strong>engineering-centric</strong> function. Day-to-day activities might involve fine-tuning models to get a training script right, collaborating on a research project, or finding out why a model is not quantizing well to 4 bits. Our users are engineers and researchers, so it's essential that we are in their world to understand them.</li>
<li>It's a <strong>decentralized</strong> function. Although we have a dedicated DevRel team, <strong>everyone at HF is expected to do activities usually associated with DevRel</strong>. Although we have a DevRel team, everyone at HF, from research to success engineers, is involved in doing DevRel-like activities themselves, so you'll see everyone engaging in social media, creating content (youtube, blog posts, etc), giving presentations, etc. If you build a feature/tool, you're responsible of its visibility and growth (of course, with support/guidance from others). Marketing your own work could involve writing a blog post or a technical deep dive, crafting some beautiful notebooks, and yes, sometimes making memes. If you visit <a target="_blank" href="https://huggingface.co/blog">HF blog</a>, you'll see content from all across the company. Rather than "outsourcing" these responsibilities to a third team (either a marketing or a DevRel team in many companies), HF members are encouraged and expected to own their work, end to end. <a target="_blank" href="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1">FineWeb</a> is an amazing example of how this can be successful.</li>
</ul>
<p>The two points combined lead to very genuine and scalable relationships. Rather than a competitive culture, we've fostered a culture in which people are excited to collaborate both internally and externally and ready to amplify the amazing work being done by the community.</p>
<blockquote>
<p><strong>Followup question on DevRel Velocity:</strong> <em>I notice that OKRs very rarely prioritize moving fast. What has worked/not worked in encouraging your team members to move fast (other than the obvious intrinsic motivation)?</em></p>
</blockquote>
<ul>
<li><strong>What works well</strong>: Beyond intrinsic motivation, which is indeed a strong factor, collective momentum plays a big role. Being surrounded by a group of smart, driven individuals working on the latest ML advancements creates an environment where progress is both expected and contagious. This collaborative atmosphere builds some sense of urgency and encourages everyone to push forward together.</li>
<li><strong>What does not work well</strong>: On the flip side, a lack of structured planning and clear OKRs can affect some people. While flexibility is desired a lot in the industry, it can lead to ambiguity and confusion about expectations, making it harder for new team members to get up to speed quickly. This can result in onboarding challenges and potential mismatches in cultural fit. Each team is a bit different, but there's a balance between agility and more structured goal-setting that can help everyone thrive.</li>
</ul>

<blockquote>
<p><strong>Open Source Engineering and Community:</strong> <em>HF maintains a -lot- of open source work, and only (~200?) employees. How do you organize the different projects you work on, and how does the community engagement work?</em></p>
</blockquote>
<p>Yes, we're a relatively small team (215 persons), and maintain a large number of libraries ourselves: demos (Gradio), data (datasets, Argilla, distilabel), modeling (transformers, diffusers, timm, peft, Candle, tokenizers, accelerate, parler TTS, transformers.js), production (TGI and TEI), and research related (lerobot, alignment handbook), plus support community libraries (bitsandbytes and sentence-transformers and others as mentioned above).</p>
<p>There are some key strategies that have worked well for us</p>
<ul>
<li><strong>Strong async culture</strong>. We mostly communicate through Slack and GitHub, enabling collaboration across different projects. This fosters transparency, allowing everyone to gain visibility into other projects.</li>
<li><strong>Flexible organizational and role boundaries.</strong> The organizational structure is flexible, allowing people from different teams to contribute where needed. For example, when we were preparing for Llama 2 release, people from all kinds of teams contributed to make sure the model was in good shape and usable by the community. It's quite powerful to see different teams working organically to make things happen without having to go through bureaucracy or process management.</li>
<li>(other points mentioned above, such as being collaboration and community centric)</li>
<li><strong>Pragmatic</strong>. Let me dive into this one more in the next point :)</li>
</ul>
<blockquote>
<p><strong>Prioritization:</strong> <em>There’s a lot of interesting directions in ML and only so much time/resources. How do you decide -what- to invest in? And what to cut? Because you're decentralized - what do managers decide vs leave to ICs?</em></p>
</blockquote>
<p>That's a great question and likely one of our biggest challenges. As you said, there are many interesting directions, and the ecosystem is changing quickly. We see new players, from new libraries and startups to new organizations releasing models.</p>
<p>In general, I like to apply the concept of exploration/exploitation from Reinforcement Learning. This involves two main stages:</p>
<ol>
<li><strong>Exploration</strong>: We do small projects or comms to gauge their potential impact and community interest. This allows us to experiment without having too many people working on it or committing lots of time.</li>
<li><strong>Exploitation</strong>: Based on the knowledge gained from the exploration stage, we focus our efforts on things we are more confident will have a significant impact. This involves scaling up successful projects and allocating more resources to areas with proven value.</li>
</ol>
<p>Of course, it's never as simple as that (the ϵ is variable), and it's usually cyclical (exploration -&gt; exploitation -&gt; exploration), but it's a good mental framework to have. Some projects are heavy in exploration by nature (for example, exploring a very niche domain or community), and others might require a larger time investment (which tends to happen in research-oriented projects).</p>
<p>The second point, related to the above, is pragmatism. That means being willing to pause or stop projects if they aren't having the expected impact. For example, investing days to make a YouTube video with a few hundred views may not be a worthwhile investment unless it leads to some very valuable or targeted outcome. It can be sad to spend some weeks building an open-source library and then see no engagement or adoption. What is worse, however, is to keep pushing and pushing for a tool that might lack product-market fit.</p>
<p>Failure is a part of the process for all of us. The key is to learn from it, understand what went well and what didn't, and know when to pivot or stop. This pragmatic approach helps us stay focused on what truly matters.</p>
<blockquote>
<p><strong>Followup Question:</strong> <em>Let’s apply Explore-Exploit. Just to pick on a specific, visible example that has caught my (swyx’s) eye recently, <a target="_blank" href="https://x.com/reach_vb/">VB (Vaibhav)</a> has staked out a very notable position as “the audio guy” on AI Twitter. Always the first to have great takes on anything in audio, shipping insanely-fast-whisper and the TTS Arena, and goodness knows what else I don’t even know about. He of course also does <a target="_blank" href="https://x.com/reach_vb/status/1806343018640781675">other open source AI work eg on LLMs</a>. Was there a top down decision to focus on Audio? It must have been… But I’m also equally sure that audio doesn’t drive nearly as much revenue for HF as, say, LLMs or Diffusion models (Apolinario). So… great hire, but how did you decide to invest in audio in the first place? Is there any calculation driven by the GTM/Product/Sales side of HF?</em></p>
</blockquote>
<p>It might sound surprising, but audio (with VB) was a very validated area we wanted to invest in, while diffusion/art (<a target="_blank" href="https://x.com/multimodalart">Poli</a>) was a very experimental area.</p>
<p><img loading="lazy" src="https://gist.github.com/user-attachments/assets/56f995f4-8f48-467f-9463-2d4582e1f730" alt="image"></p>
<p><strong>For audio</strong>, back in 2022, we saw a significant wave of OS libraries (SpeechBrain, ESPNet, Asteroid, etc) and interesting research (Whisper, XLS-R by Meta, etc). We were actively organizing community sprints with free GPUs to help people fine-tune speech recognition models in their languages. There was a lot going on that led to the decision to hire a DA for the role (apart from the MLE in the open source team already working on the topic). VB was working in audio in his masters and had already engaged with us through different efforts. Despite being somewhat junior in the open ML space, his <strong>very</strong> strong alignment with the open ML culture and mindset allowed him to scale his impact. Since joining, VB has expanded beyond audio, leading different collaborations and integrations, including recent work with Georgi on llama.cpp. Now, VB is even <a target="_blank" href="https://x.com/reach_vb/status/1810977320275943852">hiring an intern</a> to support the ML ecosystem for audio!</p>
<p><img loading="lazy" src="https://gist.github.com/user-attachments/assets/030763e7-ce88-4cab-9371-4120525f1461" alt="image"></p>
<p><strong>For diffusion/art</strong>, Poli was our first Moonshot MLE hired to make "ML for art as accessible and open source as possible." This was before the hype around Stable Diffusion. We hired him because of his strong cultural alignment, his contributions to early HF Spaces and him being a Gradio super-user. At that time, while more experimental, the impact on Spaces and the potential of diffusion models (like latent diffusion by CompVis) showed promising signs. As a power (and somewhat early) user for Spaces, he also brought lots of product ideas on making Spaces more successful.</p>
<p>In summary, our decision to invest in audio was based on clear community and research validation as well as growth potential. In contrast, MLxArt was a more experimental exploration that showed early impacts and ended up being a very high impact area.</p>
<p>Sometimes both intercept! Talking about AI x music <a target="_blank" href="https://x.com/iamwill/status/1696546638863749154">with will.i.am</a> is definitely a highlight of last year.</p>
<p><img loading="lazy" src="https://gist.github.com/user-attachments/assets/655f586e-61f1-471c-a4d2-20c44ec621ec" alt="image"></p>

<blockquote>
<p><strong>Open Questions:</strong> <em>What are you looking for help with? What questions do you want answered that would help you get to your “next level” (whatever that means to you)?</em></p>
</blockquote>
<p>Hugging Face's core audience has traditionally been people with ML experience, but we've seen more and more <strong>developers without an ML background who want to incorporate ML into their projects or learn about ML</strong>. These developers often feel overwhelmed by the complexity of ML and the speed of the ecosystem. While the community has introduced new tools and APIs to simplify things, and we have exciting features coming soon, there's still much to be done to lower the entry barriers further. I'm looking for <strong>insights and suggestions on how we can make our tools even more accessible to non-ML developers</strong>. (<em>Editor: some might call these <a target="_blank" href="https://www.latent.space/p/ai-engineer">AI Engineers</a>?</em>)</p>
<p>Additionally, <strong>we're expanding our team and are looking for individuals with strong developer empathy and technical skills based in the Bay Area</strong>. If you're interested or know someone who might be, <a target="_blank" href="https://x.com/osanseviero">please reach out</a>!</p>
<blockquote>
<p><strong>Request for Startups/Tools:</strong> <em>You <a target="_blank" href="https://argilla.io/blog/argilla-joins-hugggingface/">recently acquired Argilla</a> for collaborating on high quality datasets — what else do you wish people worked on? (that would be useful to the ecosystem from your POV)</em></p>
</blockquote>
<p>Some topics I'm interested in (not necessarily for a startup):</p>
<p>In <strong>Research</strong>: </p>
<ul>
<li>more distillation experiments and OS tooling</li>
<li>densification of sparse (MoE) models</li>
<li>quantization (sub-1-bit for MoEs, &lt;8-bit fine-tuning), tooling on speculative decoding strategies, more people trying the KTO alignment algorithm (which removes the need of preference data for RLHF/PPO/DPO)</li>
<li>true multimodality (2+ input modalities and 2+ output modalities, e.g., text+image+video to text+image in a unified model)</li>
</ul>
<p>There are trends in all of this already.</p>
<p>More generally: We want <strong>more developer-friendly ML tooling</strong> (i.e. making it super easy for any developer to use ML, not just LLMs).  If you come from a background in which you can speak both the language of a discipline (biochemistry, chemistry, material sciences, health) and ML, and communicate and work well with both audiences, you're a unicorn and can do very impactful things, not just in the ML domain, but in other industries.</p>
<blockquote>
<p><em>Thank you for your time, Omar!</em></p>
</blockquote>
<hr>
<p><strong>CTA from DXTips: Who else would you like to hear from? Let us know on <a target="_blank" href="https://x.com/dxtipshq">X/Twitter</a> and join our <a target="_blank" href="https://dx.tips/newsletter">Newsletter</a> to get the next issue!</strong></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I am starting an AI+Education company (580 pts)]]></title>
            <link>https://twitter.com/karpathy/status/1813263734707790301</link>
            <guid>40978731</guid>
            <pubDate>Tue, 16 Jul 2024 17:57:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/karpathy/status/1813263734707790301">https://twitter.com/karpathy/status/1813263734707790301</a>, See on <a href="https://news.ycombinator.com/item?id=40978731">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[XLSTMTime: Long-Term Time Series Forecasting with xLSTM (164 pts)]]></title>
            <link>https://arxiv.org/abs/2407.10240</link>
            <guid>40978372</guid>
            <pubDate>Tue, 16 Jul 2024 17:14:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2407.10240">https://arxiv.org/abs/2407.10240</a>, See on <a href="https://news.ycombinator.com/item?id=40978372">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2407.10240">View PDF</a></p><blockquote>
            <span>Abstract:</span>In recent years, transformer-based models have gained prominence in multivariate long-term time series forecasting (LTSF), demonstrating significant advancements despite facing challenges such as high computational demands, difficulty in capturing temporal dynamics, and managing long-term dependencies. The emergence of LTSF-Linear, with its straightforward linear architecture, has notably outperformed transformer-based counterparts, prompting a reevaluation of the transformer's utility in time series forecasting. In response, this paper presents an adaptation of a recent architecture termed extended LSTM (xLSTM) for LTSF. xLSTM incorporates exponential gating and a revised memory structure with higher capacity that has good potential for LTSF. Our adopted architecture for LTSF termed as xLSTMTime surpasses current approaches. We compare xLSTMTime's performance against various state-of-the-art models across multiple real-world da-tasets, demonstrating superior forecasting capabilities. Our findings suggest that refined recurrent architectures can offer competitive alternatives to transformer-based models in LTSF tasks, po-tentially redefining the landscape of time series forecasting.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Musleh Alharthi [<a href="https://arxiv.org/show-email/df681557/2407.10240">view email</a>]      <br>    <strong>[v1]</strong>
        Sun, 14 Jul 2024 15:15:00 UTC (848 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stop Microsoft users sending 'reactions' to email by adding a postfix header (155 pts)]]></title>
            <link>https://neilzone.co.uk/2024/07/attempting-to-stop-microsoft-users-sending-reactions-to-email-from-me-by-adding-a-postfix-header/</link>
            <guid>40978073</guid>
            <pubDate>Tue, 16 Jul 2024 16:38:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neilzone.co.uk/2024/07/attempting-to-stop-microsoft-users-sending-reactions-to-email-from-me-by-adding-a-postfix-header/">https://neilzone.co.uk/2024/07/attempting-to-stop-microsoft-users-sending-reactions-to-email-from-me-by-adding-a-postfix-header/</a>, See on <a href="https://news.ycombinator.com/item?id=40978073">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<article>
    <p>Over the past few months, I’ve noticed that an increasing number of replies to email that I’ve sent are “reactions”.</p>
<p>I imagine that, to the sender, or to someone in the Microsoft ecosystem, they are handled a bit liked a “thumbs-up” or “heart” reaction to a Signal message.</p>
<p>To me - as someone not in the Microsoft ecosystem - for each reaction, I get an email:</p>
<blockquote>
<p>like 	[person] reacted to your message:</p>
</blockquote>
<p>The “like” is alt-text. Because I don’t allow loading of remote content, I don’t see an image here.</p>
<p>I don’t want this.</p>

<p>Microsoft <a href="https://techcommunity.microsoft.com/t5/outlook-blog/reactions-in-outlook-public-usability-update-september-2023/ba-p/3928103">has a specific header</a> which one can add to outgoing to email:</p>
<pre tabindex="0"><code>x-ms-reactions: disallow
</code></pre><p>My understanding is that, if that header is set, Microsoft suppresses the ability in its clients to respond with a reaction.</p>
<p>It is annoying that I need to set a specific header for this.</p>
<p>If every feature required a specific header to signal that it is unwanted, that would get irritating rather rapidly.</p>
<p>But at least there <em>is</em> this header.</p>
<p>If your mail client / MUA allows you to add headers, you could do it that way.</p>
<p>If you did, you wouldn’t need to tinker with your mailserver’s config.</p>
<p>But you’d need to do it for each client you use.</p>
<p>I want it to apply to all email I send, from whatever account, and from whatever device, so I added it to my postfix configuration.</p>
<p>In <code>/etc/postfix/main.cf</code>, I have a setting:</p>
<pre tabindex="0"><code>header_checks = pcre:/etc/postfix/header_checks
</code></pre><p>So I added my new header to <code>/etc/postfix/header_checks</code>.</p>
<p>I added</p>
<pre tabindex="0"><code># add header to deal with unwanted Microsoft reactions (2024-07-16)
/^Content-Type:/i PREPEND x-ms-reactions: disallow 
</code></pre><p>(Edit: originally, I put this line before the <code>Content-Transfer-Encoding</code> header. But my mutt configuration does send that header, so I’ve switched it to go before the <code>Content-Type</code> header instead, as all my MUAs send that.)</p>
<p>and restarted postfix (<code>sudo service postfix restart</code>).</p>
<p>Then I tested it with various clients, and looked at the message headers to check that the header was being added correctly.</p>
<p>It was, so, success!</p>
<p>Now, will I get any more unwanted “reactions” email…? I’ll have to wait and see.</p>
<h2 id="update-testing-shows-some-success">Update: Testing shows some success</h2>
<p>I’ve tested this with a couple of people.</p>
<p>In one case, the header enrichment happened, but the Microsoft-using recipient still had the options to send a reaction.</p>
<p>They did, but the reaction did not reach me - it never hit my mailserver.</p>
<p>Microsoft <a href="https://techcommunity.microsoft.com/t5/outlook-blog/reactions-in-outlook-public-usability-update-september-2023/ba-p/3928103">sort-of foreshadows this</a> when it says:</p>
<blockquote>
<p>Since Disallow Reactions will roll out to different Outlook clients at different cadences and not all Outlook clients will have the gray-out update immediately, we also have a second layer of protection. When an email has reactions disallowed, attempts to react to it will fail at the server side.</p>
</blockquote>
<p>My reading of this is that, even if a Microsoft client shows that reactions are available, and even if someone clicks it to send a reaction, Microsoft may still drop it silently.</p>
<p>That doesn’t feel like an ideal user experience (even if, for me, it achieves the goal of not getting a reaction email).</p>
<p>For the other Microsoft-using person, the reaction symbol was greyed out, and a hover text said “Reactions are disallowed on this message”.</p>
<p>Perhaps it does vary by Microsoft system, which again seems rather sub-optimal.</p>



  <div>
   
	<h2>You may also like:</h2>
	
    
</div>


</article>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Private Browsing 2.0 (169 pts)]]></title>
            <link>https://webkit.org/blog/15697/private-browsing-2-0/</link>
            <guid>40977945</guid>
            <pubDate>Tue, 16 Jul 2024 16:23:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://webkit.org/blog/15697/private-browsing-2-0/">https://webkit.org/blog/15697/private-browsing-2-0/</a>, See on <a href="https://news.ycombinator.com/item?id=40977945">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                
                <p>When <a href="https://en.wikipedia.org/wiki/Private_browsing#History">we invented</a> Private Browsing back in 2005, our aim was to provide users with an easy way to keep their browsing private from anyone who shared the same device. We created a mode where users do not leave any local, persistent traces of their browsing. Eventually all other browsers shipped the same feature. At times, this is called “ephemeral browsing.”</p>
<p>We baked in cross-site tracking prevention in all Safari browsing through our cookie policy, starting with Safari 1.0 in 2003. And we’ve increased privacy protections incrementally over the last 20 years. (Learn more by reading <a href="https://webkit.org/tracking-prevention/">Tracking Prevention in Webkit</a>.) Other popular browsers have not been as quick to follow our lead in tracking prevention but there is progress.</p>
<p>Apple believes that users should not be tracked across the web without their knowledge or their consent. Entering Private Browsing is a strong signal that the user wants the best possible protection against privacy invasions, while still being able to enjoy and utilize the web. Staying with the 2005 definition of private mode as only being ephemeral, such as <a href="https://support.google.com/chrome/answer/9845881?hl=en#zippy=%2Chow-incognito-mode-works%2Chow-incognito-mode-protects-your-privacy">Chrome’s Incognito Mode</a>, simply doesn’t cut it anymore. Users expect and deserve more.</p>
<p>So, we decided to take Private Browsing further and add even more protection beyond the normal Safari experience. Last September, we added a whole new level of privacy protections to Private Browsing in Safari 17.0. And we enhanced it even further in Safari 17.2 and Safari 17.5. Plus, when a user enables them, all of the new safeguards are available in regular Safari browsing too.</p>
<p>With this work we’ve enhanced web privacy immensely and hope to set a new industry standard for what Private Browsing should be.</p>
<h2>Enhanced Private Browsing in a Nutshell</h2>
<p>These are the protections and defenses added to Private Browsing in Safari 17.0:</p>
<ul>
<li>Link Tracking Protection</li>
<li>Blocking network loads of known trackers, including CNAME-cloaked known trackers</li>
<li>Advanced Fingerprinting Protection</li>
<li>Extensions with website or history access are off by default</li>
</ul>
<p>In addition, we added these protections and defenses in all browsing modes:</p>
<ul>
<li>Capped lifetime of cookies set in responses from cloaked third-party IP addresses</li>
<li>Partitioned SessionStorage</li>
<li>Partitioned blob URLs (starting in Safari 17.2)</li>
</ul>
<p>We also expanded Web AdAttributionKit (formerly Private Click Measurement) as a replacement for tracking parameters in URL to help developers understand the performance of their marketing campaigns even under Private Browsing.</p>
<figure><picture><source srcset="https://www.webkit.org/wp-content/uploads/safari-private-browsing-dark.png" media="(prefers-color-scheme: dark)"><img fetchpriority="high" decoding="async" src="https://www.webkit.org/wp-content/uploads/safari-private-browsing-light.png" alt="Screenshot of Private Browsing in Safari" width="2704" height="1628" srcset="https://webkit.org/wp-content/uploads/safari-private-browsing-light.png 2704w, https://webkit.org/wp-content/uploads/safari-private-browsing-light-300x181.png 300w, https://webkit.org/wp-content/uploads/safari-private-browsing-light-1024x617.png 1024w, https://webkit.org/wp-content/uploads/safari-private-browsing-light-768x462.png 768w, https://webkit.org/wp-content/uploads/safari-private-browsing-light-1536x925.png 1536w, https://webkit.org/wp-content/uploads/safari-private-browsing-light-2048x1233.png 2048w" sizes="(max-width: 2704px) 100vw, 2704px"></picture><figcaption>Private Browsing in Safari</figcaption></figure>
<p>However, before we dive into these new and enhanced privacy protections, let’s first consider an important aspect of these changes: website compatibility risk.</p>
<h2>The Risk of Breaking Websites and How We Mitigate It</h2>
<p>There are many ideas for how to protect privacy on the web, but unfortunately many of them may break the user’s experience. Like security protections in real life, a balance must be struck. The new Private Browsing goes right up to the line, attempting to never break websites. But of course there is a risk that some parts of some sites won’t work. To solve this, we give users affordances to reduce privacy protections on a per-site basis. Such a change in privacy protections is only remembered while browsing within a site. This option is a last resort when a web page is not usable due to the privacy protections.</p>
<figure><picture><source srcset="https://www.webkit.org/wp-content/uploads/webkit-tracking-prevention-dark.png" media="(prefers-color-scheme: dark)"><img decoding="async" src="https://www.webkit.org/wp-content/uploads/webkit-tracking-prevention-light.png" alt="Reload menu with Reload Reducing Privacy Protections selected" width="2564" height="1544" srcset="https://webkit.org/wp-content/uploads/webkit-tracking-prevention-light.png 2564w, https://webkit.org/wp-content/uploads/webkit-tracking-prevention-light-300x181.png 300w, https://webkit.org/wp-content/uploads/webkit-tracking-prevention-light-1024x617.png 1024w, https://webkit.org/wp-content/uploads/webkit-tracking-prevention-light-768x462.png 768w, https://webkit.org/wp-content/uploads/webkit-tracking-prevention-light-1536x925.png 1536w, https://webkit.org/wp-content/uploads/webkit-tracking-prevention-light-2048x1233.png 2048w" sizes="(max-width: 2564px) 100vw, 2564px"></picture><figcaption>Reload Reducing Privacy Protections</figcaption></figure>
<p>All of the new privacy protections in Private Browsing are also available in regular browsing. On iOS, iPadOS and visionOS go to Settings &gt; Apps &gt; Safari &gt; Advanced &gt; Advanced Tracking and Fingerprinting Protection and enable “All Browsing”. On macOS go to Safari &gt; Settings &gt; Advanced and enable  “Use advanced tracking and fingerprinting protection”:</p>
<figure><picture><source srcset="https://www.webkit.org/wp-content/uploads/safari-advanced-tracking-protection-dark.png" media="(prefers-color-scheme: dark)"><img decoding="async" width="1700" height="1155" src="https://www.webkit.org/wp-content/uploads/safari-advanced-tracking-protection-light.png" alt="Safari Advanced Settings with &quot;Use advanced tracking and fingerprinting protection in all browsing&quot; selected" srcset="https://webkit.org/wp-content/uploads/safari-advanced-tracking-protection-light.png 1700w, https://webkit.org/wp-content/uploads/safari-advanced-tracking-protection-light-300x204.png 300w, https://webkit.org/wp-content/uploads/safari-advanced-tracking-protection-light-1024x696.png 1024w, https://webkit.org/wp-content/uploads/safari-advanced-tracking-protection-light-768x522.png 768w, https://webkit.org/wp-content/uploads/safari-advanced-tracking-protection-light-1536x1044.png 1536w" sizes="(max-width: 1700px) 100vw, 1700px"></picture><figcaption>Use advanced tracking and fingerprinting protection in all browsing from Safari Advanced Settings</figcaption></figure>
<p>Let’s now walk through how these enhancements work.</p>
<h2>Link Tracking Protection</h2>
<p>Safari’s Private Browsing implements two new protections against tracking information in the destination URL when the user navigates between different websites. The specific parts of the URL covered are query parameters and the fragment. The goal of these protections is to make it more difficult for third-party scripts running on the destination site to correlate user activity across websites by reading the URL.</p>
<p>Let’s consider an example where the user clicks a link on <code>clickSource.example</code>, which takes them to <code>clickDestination.example.</code> The URL looks like this:</p>
<pre><code>https://clickDestination.example/article?known_tracking_param=123&amp;campaign=abc&amp;click_val=456
</code></pre>
<p>Safari removes a subset of query parameters that have been identified as being used for pervasive cross-site tracking granular to users or clicks. This is done <em>prior to</em> navigation, such that these values are never propagated over the network. If <code>known_tracking_param</code> above represents such a query parameter, the URL that’s used for navigation will be:</p>
<pre><code>https://clickDestination.example/article?campaign=abc&amp;click_val=456
</code></pre>
<p>As its name suggests, the <code>campaign</code> above represents a parameter that’s only used for campaign attribution, as opposed to click or user-level tracking. Safari allows such parameters to pass through.</p>
<p>Finally, on the destination site after a cross-site navigation, all third-party scripts that attempt to read the full URL (e.g. using <code>location.search</code>, <code>location.href</code>, or <code>document.URL</code>) will get a version of the URL that has no query parameters or fragment. In our example, this script-exposed value is simply:</p>
<pre><code>https://clickDestination.example/article
</code></pre>
<p>In a similar vein, Safari also hides cross-site any <code>document.referrer</code> from script access in Private Browsing.</p>
<h2>Web AdAttributionKit in Private Browsing</h2>
<p>Web AdAttributionKit (formerly Private Click Measurement) is a way for advertisers, websites, and apps to implement ad attribution and click measurement in a privacy-preserving way. You can <a href="https://webkit.org/blog/11529/introducing-private-click-measurement-pcm/">read more about it here</a>. Alongside the new suite of enhanced privacy protections in Private Browsing, Safari also brings a version of Web AdAttributionKit to Private Browsing. This allows click measurement and attribution to continue working in a privacy-preserving manner.</p>
<p>Web AdAttributionKit in Private Browsing works the same way as it does in normal browsing, but with some limits:</p>
<ul>
<li>Attribution is scoped to individual Private Browsing tabs, and transfers attribution across new tabs opened when clicking on links. However, attribution is not preserved through other indirect means of navigation: for instance, copying a link and pasting in a new tab. In effect, this behaves similarly to how Web AdAttributionKit works for <a href="https://webkit.org/blog/12042/pcm-for-in-app-direct-response-advertising/">Direct Response Advertising</a>.</li>
<li>Since Private Browsing doesn’t persist any data, pending attribution requests are discarded when the tab is closed.</li>
</ul>
<h2>Blocking Network Loads of Known Trackers</h2>
<p>Safari 17.0 also comes with an automatically enabled content blocker in Private Browsing, which blocks network loads to known trackers. While Intelligent Tracking Prevention has long blocked all third party cookies, blocking trackers’ network requests from leaving the user’s device in the first place ensures that no personal information or tracking parameters are exfiltrated through the URL itself.</p>
<p>This automatically enabled content blocker is compiled using data from DuckDuckGo and from the EasyPrivacy filtering rules from EasyList. The requests flagged by this content blocker are only entries that are flagged as trackers by <em>both</em> DuckDuckGo and EasyPrivacy. In doing so, Safari intentionally allows most ads to continue loading even in Private Browsing.</p>
<p>Private Browsing also blocks cloaked network requests to known tracking domains. They otherwise have the ability to save third party cookies in a first-party context. This protection requires macOS Sonoma or iOS 17. By cloaked we mean subdomains mapped to a third-party server via CNAME cloaking or third-party IP address cloaking. See also the “Defending Against Cloaked First Party IP Addresses” section below.</p>
<p>When Safari blocks a network request to a known tracker, a console message of this form is logged, and can be viewed using Web Inspector:</p>
<pre><code>`<span>Blocked</span> <span>connection</span> <span>to</span> <span>known</span> <span>tracker</span><span>:</span> <span>tracker</span>.<span>example</span>` 
</code></pre>
<h2>Network Privacy Enhancements</h2>
<p>Safari 15.0 started hiding IP addresses from known trackers by default. Private Browsing in Safari 17.0 adds the following protections for all users:</p>
<ul>
<li><strong>Encrypted DNS</strong>. DNS queries are used to resolve server hostnames into IP addresses, which is a necessary function of accessing the internet. However, DNS is traditionally unencrypted, and allows network operators to track user activity or redirect users to other servers. Private Browsing uses <a href="https://en.wikipedia.org/wiki/DNS_over_HTTPS#Oblivious_DNS_over_HTTPS">Oblivious DNS over HTTPS</a> by default, which encrypts and proxies DNS queries to protect the privacy and integrity of these lookups.</li>
<li><strong>Proxying unencrypted HTTP</strong>. Any unencrypted HTTP resources loaded in Private Browsing will use the same multi-hop proxy network used to hide IP addresses from trackers. This ensures that attackers in the local network cannot see or modify the content of Private Browsing traffic.</li>
</ul>
<p>Additionally, for iCloud+ subscribers who have iCloud Private Relay turned on, Private Browsing takes privacy to the next level with these enhancements:</p>
<ul>
<li><strong>Separate sessions per tab</strong>. Every tab that the user opens in Private Browsing now uses a separate session to the iCloud Private Relay proxies. This means that web servers won’t be able to tell if two tabs originated on the same device. Each session is assigned egress IP addresses independently. Note that this doesn’t apply to parent-child windows that need a programmatic relationship, such as popups and their openers.</li>
<li><strong>Geolocation privacy by default</strong>. Private Browsing uses an IP location based on your country and time zone, not a more specific location.</li>
<li><strong>Warnings before revealing IP address</strong>. When accessing a server that is not accessible on the public internet, such as a local network server or an internal corporate server, Safari cannot use iCloud Private Relay. In Private Browsing, Safari now displays a warning requesting that the user consents to revealing their IP address to the server before loading the page.</li>
</ul>
<h2>Extensions in Private Browsing</h2>
<p>Safari 17.0 also boosts the privacy of Extensions in Private Browsing. Extensions that can access website data and browsing history are now off by default in Private Browsing. Users can still choose to allow an extension to run in Private Browsing and gain all of the extension’s utility. Extensions that don’t access webpage contents or browsing history, like Content Blockers, are turned on by default in Private Browsing when turned on in Safari.</p>
<h2>Advanced Fingerprinting Protection</h2>
<p>With Safari and subsequently <a href="https://wiki.mozilla.org/Security/Anti_tracking_policy">other</a> <a href="https://www.chromium.org/Home/chromium-privacy/privacy-sandbox/#turning-down-third-party-cookies">browsers</a> restricting stateful tracking (e.g. cross-site cookies), many trackers have turned to stateless tracking, often referred to as <em>fingerprinting</em>.</p>
<h3>Types of Fingerprinting</h3>
<p>We distinguish these types of fingerprinting:</p>
<ul>
<li><strong>Device fingerprinting</strong>. This is about building a fingerprint based on device characteristics, including hardware and the current operating system and browser. It can also include connected peripherals if they are allowed to be detected. Such a fingerprint cannot be changed by the user through settings or web extensions.</li>
<li><strong>Network and geographic position fingerprinting</strong>. This is about building a fingerprint based on how the device connects to the Internet and any means of detecting its geographic position. It could be done by measuring roundtrip speeds of network requests or simply using the IP address as an identifier.</li>
<li><strong>User settings fingerprinting</strong>. This is about reading the state of user settings such as dark/light mode, locale, font size adjustments, and window size on platforms where the user can change it. It also includes detecting web extensions and accessibility tools. We find this kind of fingerprinting to be extra hurtful since it exploits how users customize their web experience to fit their needs.</li>
<li><strong>User behavior fingerprinting</strong>. This is about detecting recurring patterns in how the user behaves. It could be how the mouse pointer is used, how quickly they type in form fields, or how they scroll.</li>
<li><strong>User traits fingerprinting</strong>. This is about figuring out things about the user, such as their interests, age, health status, financial status, and educational background. Those gleaned traits can contribute to a unique ID but also can be used directly to target them with certain content, adjust prices, or tailor messages.</li>
</ul>
<h3>Fingerprint Stability</h3>
<p>A challenge for any tracker trying to create a fingerprint is how stable the fingerprint will be over time. Software version fingerprinting changes with software updates, web extension fingerprinting changes with extension updates and enablement/disablement, user settings change when the user wants, multiple users of the same device means behavior fingerprints change, and roaming devices may change network and geographic position a lot.</p>
<h3>Fingerprinting Privacy Problem 1: Cross-Site Tracking</h3>
<p>Fingerprints can be used to track the user across websites. If successful, it defeats tracking preventions such as storage partitioning and link decoration filtering.</p>
<p>There are two types of solutions to this problem:</p>
<ol>
<li>Make the fingerprint be shared among many users, so called herd immunity.</li>
<li>Make the fingerprint unique per website, typically achieved via randomized noise injection.</li>
</ol>
<h3>Fingerprinting Privacy Problem 2: Per-Site User Recall</h3>
<p>Less talked about is the fingerprinting problem of per-site user recall. Web browsers offer at least two ways for the user to reset their relationship with a website: Clear website data or use Private Browsing. Both make a subsequent navigation to a website start out fresh.</p>
<p>But fingerprinting defeats this and allows a website to remember the user even though they’ve opted to clear website data or use Private Browsing.</p>
<p>There are two types of solutions to this problem:</p>
<ol>
<li>Make the fingerprint be shared among many users, so called herd immunity.</li>
<li>Make the fingerprint unique per website, and generate a new unique fingerprint for every fresh start.</li>
</ol>
<h3>Fingerprinting Privacy Problem 3: Per-Site Visitor Uniqueness</h3>
<p>The ultimate anti fingerprinting challenge in our view is to address a specific user’s uniqueness when visiting a specific website. Here’s a simple example:</p>
<p>Having the locale setting to US/EN for American English may provide ample herd immunity in many cases. But what happens when a user with that setting visits an Icelandic government website or a Korean reading club website? They may find themselves in a very small “herd” on that particular website and combined with just a few more fingerprinting touch points they can be uniquely identified.</p>
<p>Addressing per-site visitor uniqueness is not possible in general by a browser unless it knows what the spread of visitors looks like for individual websites.</p>
<h3>Fingerprinting Protections at a High Level</h3>
<p>We view cross-site tracking and per-site user recall as privacy problems to be addressed by browsers.</p>
<p><strong>Our approach</strong>:<br>
Make the fingerprint unique per website, and generate a new unique fingerprint for every fresh start such as at website data removal.</p>
<p><strong>Our tools</strong>:</p>
<ul>
<li>Use multi-hop proxies to hide IP addresses and defend against network and geographic position fingerprinting.</li>
<li>Limit the number of fingerprintable web APIs whenever possible. This could mean altering the APIs, gating them behind user permissions, or not implementing them.</li>
<li>Inject small amounts of noise in return values of fingerprintable web APIs.</li>
</ul>
<h3>Fingerprinting Protection Details</h3>
<p>Safari’s new advanced fingerprinting protections make it difficult for scripts to <strong>reliably</strong> extract <strong>high-entropy</strong> data through the use of several web APIs:</p>
<ol>
<li>To make it more difficult to <strong>reliably</strong> extract details about the user’s configuration, Safari injects noise into various APIs: namely, during 2D canvas and WebGL readback, and when reading <code>AudioBuffer</code> samples using WebAudio.</li>
<li>To reduce the overall <strong>entropy</strong> exposed through other APIs, Safari also overrides the results of certain web APIs related to window or screen metrics to fixed values, such that fingerprinting scripts that call into these APIs for users with different screen or window configurations will get the same results, even if the users’ underlying configurations are different.</li>
</ol>
<h4>2D Canvas and WebGL</h4>
<p>Many modern web browsers use a computer’s graphics processing unit (GPU) to accelerate rendering graphics. The Web’s Canvas API (2D Canvas) and WebGL API give a web page the tools it needs for rendering arbitrary images and complex scenes using the GPU, and analyzing the result. These APIs are valuable for the web platform, but they allow the web page to learn unique details about the underlying hardware without asking for consent. With Safari’s advanced fingerprinting protections enabled, Safari applies tiny amounts of noise to pixels on the canvas that have been painted using drawing commands. These modifications reduce the value of a fingerprint when using these APIs without significantly impacting the rendered graphics.</p>
<p>It’s important to emphasize that:</p>
<ol>
<li>This noise injection only happens in regions of the canvas where drawing occurs.</li>
<li>The amount of noise injected is extremely small, and (mostly) should not result in observable differences or artifacts.</li>
</ol>
<p>This strategy helps mitigate many of the compatibility issues that arise from this kind of noise injection, while still maintaining robust fingerprinting mitigations.</p>
<p>In Safari 17.5, we’ve bolstered these protections by additionally injecting noise when reading back data from offscreen canvas in both service workers and shared workers.</p>
<h4>Web Audio</h4>
<p>Similarly, when reading samples using the WebAudio API — via <code>AudioBuffer.getChannelData()</code> — a tiny amount of noise is applied to each sample to make it very difficult to reliably measure OS differences. In practice, these differences are already extremely minor. Typically due to slight differences in the order of operations when applying FFT or IFFT. As such, a relatively low amount of noise can make it substantially more difficult to obtain a stable fingerprint.</p>
<p>In Safari 17.5, we made audio noise injection more robust in the following ways:</p>
<ul>
<li>The injected noise now applies consistently to the same values in a given audio buffer — this means a looping  <code>AudioSourceNode</code> that contains a single high-entropy sample can’t be used to average out the injected noise and obtain the original value quickly.</li>
<li>Instead of using a uniform distribution for the injected noise, we now use normally-distributed noise. The mean of this distribution converges much more slowly on the original value, when compared to the average of the minimum and maximum value in the case of uniformly-distributed noise.</li>
<li>Rather than using a low, fixed amount of noise (0.1%), we’ve refactored the noise injection mechanism to support arbitrary levels of noise injection. This allows us to easily fine-tune noise injection, such that the magnitude of noise increases when using audio nodes that are known to reveal subtle OS or hardware differences through minute differences in sample values.</li>
</ul>
<p>This noise injection also activates when using Audio Worklets (e.g. <code>AudioWorkletNode</code>) to read back audio samples.</p>
<h4>Screen/Window Metrics</h4>
<p>Lastly, for various web APIs that currently directly expose window and screen-related metrics, Safari takes a different approach: instead of the noise-injection-based mitigations described above, entropy is reduced by fixing the results to either hard-coded values, or values that match other APIs.</p>
<ul>
<li><code>screen.width</code> / <code>screen.height</code>: The screen size is fixed to the values of <code>innerWidth</code> and <code>innerHeight</code>.</li>
<li><code>screenX</code> / <code>screenY</code>: The screen position is fixed to <code>(0, 0)</code>.</li>
<li><code>outerWidth</code> / <code>outerHeight</code>: Like screen size, these values are fixed to <code>innerWidth</code> and <code>innerHeight</code>.</li>
</ul>
<p>These mitigations also apply when using media queries to indirectly observe the screen size.</p>
<h2>Don’t Add Fingerprintable APIs to the Web, Like The Topics API</h2>
<p>We have worked for many years with the standards community on improving user privacy of the web platform. There are existing web APIs that are fingerprintable, such as Canvas, and reining in their fingerprintability is a long journey. Especially since we want to ensure existing websites can continue to work well.</p>
<p>It is key for the future privacy of the web to not compound the fingerprinting problem with new, fingerprintable APIs. There are cases where the tradeoff tells us that a rich web experience or enhanced accessibility motivates some level of fingerprintability. But in general, our position is that we should progress the web without increasing fingerprintability.</p>
<p>A recent example where we opposed a new proposal is the Topics API which is now <a href="https://developers.google.com/privacy-sandbox/relevance/topics">shipping in the Chrome browser</a>. We provided <a href="https://github.com/WebKit/standards-positions/issues/111">extensive critical feedback</a> as part of the standards process and we’d like to highlight a few pieces here.</p>
<h3>The Topics API in a Nutshell</h3>
<p>From the <a href="https://github.com/patcg-individual-drafts/topics">proposal</a>:</p>
<pre><code><span>// document.browsingTopics() returns an array of up to three topic objects in random order.
</span><span>const</span> <span>topics</span> <span>=</span> <span>await</span> <span>document</span>.<span>browsingTopics</span>();
</code></pre>
<p>Any JavaScript can call this function on a webpage. Yes, that includes tracker scripts, advertising scripts, and data broker scripts.</p>
<p>The topics come from a predefined list of hundreds of topics. It’s not the user who picks from these topics, but instead Chrome will record the user’s browsing history over time and deduce interests from it. The user doesn’t get told upfront which topics Chrome has tagged them with or which topics it exposes to which parties. It all happens in the background and by default.</p>
<p>The intent of the API is to help advertisers target users with ads based on each user’s interests even though the current website does not necessarily imply that they have those interests.</p>
<h3>The Fingerprinting Problem With the Topics API</h3>
<p>A new <a href="https://arxiv.org/html/2403.19577v1">research paper</a> by Yohan Beugin and Patrick McDaniel from University of Wisconsin-Madison goes into detail on Chrome’s actual implementation of the Topics API.</p>
<p>The authors use large scale real user browsing data (voluntarily donated) to show both how the 5% noise supposed to provide plausible deniability for users can be defeated, and how the Topics API can be used to fingerprint and re-identify users.</p>
<blockquote><p>
  “We conclude that an important part of the users from this real dataset are re-identified across websites through only the observations of their topics of interest in our experiment. Thus, the real users from our dataset can be fingerprinted through the Topics API. Moreover, as can be seen, the information leakage and so, privacy violation worsen over time as more users are uniquely re-identified.” —Beugin and McDaniel, University of Wisconsin-Madison
</p></blockquote>
<p>The paper was published at the <a href="https://ieeexplore.ieee.org/document/10579537">2024 IEEE Security and Privacy Workshops (SPW)</a> in May.</p>
<h3>Further Privacy Problems With the Topics API</h3>
<p>Re-identifying and tracking users is not the only privacy problem with the Topics API. There is also the profiling of users’ cross-site activity. Here’s an example using topics on <a href="https://github.com/patcg-individual-drafts/topics/blob/main/taxonomy_v2.md">Chrome’s predefined list</a>.</p>
<p>Imagine in May 2024 you go to <code>news.example</code> where you are a subscriber and have provided your email address. Embedded on the website, <code>dataBroker.example</code>. The data broker has gleaned your email address from the login form and calls the Topics API to learn that you currently have these interests:</p>
<ul>
<li>Flowers</li>
<li>Event &amp; Studio Photography</li>
<li>Luxury Travel</li>
</ul>
<p>In May 2026 you go to <code>news.example</code> where <code>dataBroker.example</code> calls the Topics API and is told that you now have these interests:</p>
<ul>
<li>Children’s Clothing</li>
<li>Family Travel</li>
<li>Toys</li>
</ul>
<p>Finally, in May 2029 you go to <code>news.example</code> where <code>dataBroker.example</code> calls the Topics API and is told that you have these interests:</p>
<ul>
<li>Legal Services</li>
<li>Furnished Rentals</li>
<li>Child Care</li>
</ul>
<p>You haven’t told any website with access to your email address anything that’s been going on in your family life. But the data broker has been able to read your shifting interests and store them in their permanent profile of you — while you were reading the news.</p>
<p>Now imagine what advanced machine learning and artificial intelligence can deduce about you based on various combinations of interest signals. What patterns will emerge when data brokers and trackers can compare and contrast across large portions of the population? Remember that they can combine the output of the Topics API with any other data points they have available, and it’s the analysis of all of it together that feeds the algorithms that try to draw conclusions about you.</p>
<p>We think the web should not expose such information across websites and we don’t think the browser, i.e. <em>the user agent</em>, should facilitate any such data collection or use.</p>
<h2>Privacy Enhancements in Both Browsing Modes</h2>
<p>Our defenses against cloaked third-party IP addresses and our partitioning of SessionStorage and blob URLs are enabled by default in both regular browsing and Private Browsing. Here’s how those protections work.</p>
<h3>Defending Against Cloaked First Party IP Addresses</h3>
<p>In 2020, Intelligent Tracking Prevention (ITP) gained the ability to <a href="https://webkit.org/blog/11338/cname-cloaking-and-bounce-tracking-defense">cap the expiry of cookies set in third-party CNAME-cloaked HTTP responses to 7 days</a>.</p>
<p>This defense did not mitigate cases where IP aliasing is used to cloak third party requests under first party subdomains. ITP now also applies a 7-day cap to the expiry of cookies in responses from cloaked third-party IP addresses. Detection of third-party IP addresses is heuristic, and may change in the future. Currently, two IP addresses are considered different parties if any of the following criteria are met:</p>
<ol>
<li>One IP address is IPv4, while the other is IPv6.</li>
<li>If both addresses are IPv4, the length of the common subnet mask is less than 16 bits (half of the full address length).</li>
<li>If both addresses are IPv6, the length of the common subnet mask is less than 64 bits (also half of the full address length).</li>
</ol>
<h3>Partitioned SessionStorage and Blob URLs</h3>
<p>Websites have many options for how they store information over longer time periods. <a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage">Session Storage</a> is a storage area in Safari that is scoped to the current tab. When a tab in Safari is closed, all of the session storage associated with it is destroyed. Beginning in Safari 16.1 cross-site Session Storage is partitioned by first-party web site.</p>
<p>Similarly, <a href="https://developer.mozilla.org/en-US/docs/Web/API/Blob">Blobs</a> are a storage type that allow websites to store raw, file-like data in the browser. A blob can hold almost anything, from simple text to something larger and more complex like a video file. A unique URL can be <a href="https://developer.mozilla.org/en-US/docs/Web/API/URL/createObjectURL_static">created for a blob</a>, and that URL can be used to gain access to the associated blob, as long as the blob still exists. These URLs are often referred to as Blob URLs, and a Blob URL’s lifetime is scoped to the document that creates it. Beginning in Safari 17.2, cross-site Blob URLs are partitioned by first-party web site, and first-party Blob URLs are not usable by third parties.</p>
<h2>Setting a New Industry Standard</h2>
<p>The additional privacy protections of Private Browsing in Safari 17.0, Safari 17.2 and Safari 17.5 set a new bar for user protection. We’re excited for all Safari users and the web itself to benefit from this work!</p>
<h2>Feedback</h2>
<p>We love hearing from you! To share your thoughts on Private Browsing 2.0, find John Wilander on Mastodon at <a href="https://mastodon.social/@wilander">@wilander@mastodon.social</a> or send a reply on X to <a href="https://x.com/webkit">@webkit</a>. You can also <a href="https://www.linkedin.com/in/apple-webkit/">follow WebKit on LinkedIn</a>. If you run into any issues, we welcome your <a href="https://feedbackassistant.apple.com/">feedback</a> on Safari UI (learn more about <a href="https://developer.apple.com/bug-reporting/">filing Feedback</a>), or your <a href="https://bugs.webkit.org/">WebKit bug report</a> about web technologies or Web Inspector.</p>

                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deconstructing the Role-Playing Video Game (103 pts)]]></title>
            <link>https://olano.dev/blog/deconstructing-the-role-playing-videogame/</link>
            <guid>40977834</guid>
            <pubDate>Tue, 16 Jul 2024 16:09:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://olano.dev/blog/deconstructing-the-role-playing-videogame/">https://olano.dev/blog/deconstructing-the-role-playing-videogame/</a>, See on <a href="https://news.ycombinator.com/item?id=40977834">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en">
      <header>
          
          
          
          

      </header>

      
      <p>
<em>Following up on my <a href="https://olano.dev/blog/a-computing-magazine-anthology">previous post</a> on archived project ideas, today I want to write about an <a href="https://github.com/facundoolano/rpg-cli">rpg-cli</a> spin-off. rpg-cli is one of my fondest personal projects and I never properly documented its development, so I’ll start this post by doing just that.</em></p>
<h2>Contents</h2>
<nav>
<ul>
<li><a href="#the-simplest-thing-that-could-possibly-work">The simplest thing that could possibly work</a>
</li>
<li><a href="#your-file-system-as-a-dungeon">Your file system as a dungeon</a>
</li>
<li><a href="#research">Research</a>
</li>
<li><a href="#pseudo-historical-digression">Pseudo-historical digression</a>
</li>
<li><a href="#design">Design</a>
</li>
<li><a href="#development">Development</a>
</li>
<li><a href="#postscript-a-text-interface-for-rpg-cli">Postscript: A text interface for rpg-cli</a>
</li>
<li><a href="#notes">Notes</a>
</li>
</ul>
</nav>
<h2 id="the-simplest-thing-that-could-possibly-work">
The simplest thing that could possibly work
</h2>
<p>
This was back in 2021. I was going through one of those periods where I didn’t get much intellectual satisfaction from my daily job, so I thought I could use a programming side project. I had an itch to work on a video game, something I hadn’t done in a while.
In the past few years I had finished many classic Japanese RPGs—<em>Final Fantasy VI</em>, <em>A Link to the Past</em>, <em>Chrono Trigger</em>, <em>Suikoden 2</em>, <em>Final Fantasy Tactics—</em> so I felt compelled to try something with that genre. But I like personal projects to be short-lived and yield something usable, somewhat finished, after a few months; I wasn’t about to embark on a full game development project in my spare time.</p>
<p>
I’ve played JRPGs long enough that I don’t pay much attention to the characters or the plot anymore; I like the pretty pixels, yes, but most importantly I’m drawn to its underlying systems. Explore the map, visit cities, clear dungeons; kill monsters, level up, buy equipment; character stats, turn-based combat, leveling system. To me, a classic JRPG is pure mechanism, a kind of puzzle. Was there some way of getting the fun out of building such a mechanism—of solving that puzzle—, wrapping it with the minimal amount of functionality, the simplest thing that could possibly pass as a video game?</p>
<p>
In other words: how much could I have peeled off, and still gotten an RPG? One answer was obvious: no dialogues, no plot, no story<sup><a id="footnote-reference-1" href="#footnote-1">1</a></sup>. But that wouldn’t be enough: the big  blocker was the graphics, I needed to work around them in all their rabbit-hole, yak-shaving glory. I had just seen how the <em>Final Fantasy Tactics</em> designers had fit most of the standard RPG elements, save the battle sequences, into menu screens. Perhaps I could have tried something like that. If it were today, I’d consider building a mini-game with PICO-8 or an ASCII roguelike; at the time, I went with a trick that <a href="https://github.com/facundoolano/advenjure">had worked for me</a> before: using text instead of graphics.</p>
<p>
Except, you know what’s narrower than a text user interface?</p>
<p>
A command-line interface.</p>
<p>
Was there any way I could make a role-playing game fit in the shell?</p>
<h2 id="your-file-system-as-a-dungeon">
Your file system as a dungeon
</h2>
<p>This was one of those cases where formal constraints foster creativity. I derived many design decisions from restricting myself to a command-line interface. The CLI also gave me a good excuse to try Rust, something I had been looking for.</p>
<p>
The shell is the <em>environment</em> of command-line programs; the file system gives a sense of <em>place</em>. At some point, I made that association and decided that the hero of my game would <em>inhabit</em> the file system, with the working directory as its current location. Changing directories would be like moving between dungeon levels, enemies popping up along the way: the more nested the directory, the tougher the enemies. As in many early CRPGs, the game’s goal would be just to crawl down the dungeon, as deep as possible. Going back <code>~</code> (home) would restore the hero’s health and give the player a chance to buy equipment and supplies.</p>
<p>
This idea finally clicked when I imagined the program as a <code>cd</code> replacement, where players would randomly engage in combats as a side effect of doing their daily work in the terminal:</p>
<p><img src="https://olano.dev/assets/img/rpgcli.png">
</p>
<p>
A pure command-line interface also meant that the gameplay would have to be non-interactive. This was a problem for the traditional turn-based combat I had in mind. The solution was inspired by <em>Suikoden 2</em>, a PlayStation game I had recently finished.</p>
<p>
In <em>Suikoden 2,</em> you manage a huge list of playable characters (over 100), in parties of up to six members. When enemies pop up in dungeons, having to issue six commands on each turn, with characters you haven’t been using for long, can get tedious. The developers had the good sense to introduce an <em>auto-battle</em> button that just repeats the basic attack of each party member until the enemy is killed.</p>
<p>
So I decided to make this auto-battle feature the default for rpg-cli. This <em>felt right</em> to me because I’m a very dull player when it comes to combat. I don’t particularly enjoy strategizing, I just default to punch with warriors and spell with wizards, with the occasional healing potion in between, until enemies become tough enough that they force me to stop and think. So I would bake that pattern right into rpg-cli’s battle logic: default to attack unless HP is low and a potion is available. (I later extended this to account for magical classes that attack with spells and occasionally need to restore their magical points).</p>
<p>
This would obviously remove some player agency (and fun) from the combat; the opportunity to make choices and strategize would need to happen between battles: deciding whether to go further down the dungeon or back home to recover, when to use items, how to spend the gold, etc.</p>
<h2 id="research">
Research
</h2>
<p>
I felt that the radical simplicity I started from had unexpectedly led me to an interesting concept for the game, so I decided to double down on “the simplest thing that could possibly work” as my design mantra, applying it to the entire project, not just the interface.</p>
<p>
I had a concept, an implementation language, a scope, and a rough outline of the interface for my program. But, before I could start coding its basic building blocks, I needed to design the RPG model: a stat system to know what attributes to give to the characters, a leveling system to know how to raise them, and a combat routine that would put them to use.</p>
<p>
My experience of the genre was almost exclusively through JRPGs, so it felt appropriate to do some research, to see if I could get ideas from western video games and tabletop RPGs: is there a canonical set of enemy classes? has someone else already figured out the minimum set of stats to make an RPG work? Would I benefit from learning the <em>Dungeon &amp; Dragons</em> rules?</p>
<p>
I started by looking around for tabletop RPGs designed for minimalism or genericity:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/GURPS">GURPS</a>, the Generic Universal role-playing System.</li>
<li><a href="https://en.wikipedia.org/wiki/TWERPS">TWERPS</a>, the World’s Easiest role-playing System.</li>
<li><a href="https://en.wikipedia.org/wiki/Dinky_Dungeons">Dinky Dungeons</a>, the smallest RPG ever produced<sup><a id="footnote-reference-2" href="#footnote-2">2</a></sup>.</li>
<li><a href="http://www.campaignmastery.com/blog/introducing-the-sixes-system/">The Sixes System</a>, a Minimalist Universal RPG.</li>
<li><a href="https://www.perilplanet.com/freeform-universal/">FU</a>, the Freeform Universal RPG.</li>
<li><a href="https://www.stargazergames.eu/warrior-rogue-mage/">Warrior, Rogue &amp; Mage</a>, a simple, lightweight RPG.</li>
</ul>
<p><img src="https://olano.dev/assets/img/dinky.jpg">
</p>
<p>
Fun and educational as that excursion was, it left me more confused than when I started. I concluded that tabletop rulesets would contribute complexity rather than simplicity to my project, so I went back to using video games as my reference. In addition to the ones I was already familiar with, I spent some time reading about <em>Rogue</em> and its descendants since, from the little I knew about them, it sounded like they could teach me some things about minimalist design:</p>
<ul>
<li><a href="https://web.archive.org/web/20050206091120/http://www.wichman.org/roguehistory.html">A Brief History of “Rogue”</a>.</li>
<li><a href="https://insight.ieeeusa.org/articles/going-rogue-a-brief-history-of-the-computerized-dungeon-crawl/">Going Rogue: A Brief History of the Computerized Dungeon Crawl</a>.</li>
<li><a href="http://crpgaddict.blogspot.com/2010/02/rogue-most-difficult-crpg-ive-played.html">Rogue: the most difficult CRPG I’ve played</a>.</li>
<li><a href="http://crpgaddict.blogspot.com/2010/02/rogue-story-and-gameplay.html">Rogue: Story and Gameplay</a>.</li>
<li><a href="https://gamedevelopment.tutsplus.com/articles/the-key-design-elements-of-roguelikes--cms-23510">The Key Design Elements of Roguelikes</a>.</li>
</ul>
<p>Finally, I looked at some RPG design resources. The most useful was the <a href="https://howtomakeanrpg.com/">How To Make an RPG</a> series, particularly the entries on <a href="http://howtomakeanrpg.com/a/how-to-make-an-rpg-stats.html">stats</a> and <a href="http://howtomakeanrpg.com/a/how-to-make-an-rpg-levels.html">levels</a>.</p>
<h2 id="pseudo-historical-digression">
Pseudo-historical digression
</h2>
<p>I didn’t know it back then, but there is an illustrious tradition of deconstructing the role-playing game. RPG video games came from tabletop RPGs, that came from war games, that came from the <a href="https://en.wikipedia.org/wiki/Kriegsspiel">Kriegsspiel</a>, a simulation game that the Prussian army trained with during the 19th century<sup><a id="footnote-reference-3" href="#footnote-3">3</a></sup>. Like its war gaming ancestors, <em>Dungeons &amp; Dragons</em> was full of complexity: sophisticated rules for character building, catalogs of monsters and spells and armor, and battle outcomes decided by probability calculations. This was arguably part of the fun, at least for some of the players—for others, a complicated system is an invitation to simplify and abstract.</p>
<p>
It’s no secret that there was some overlap between early RPG players and computer programmers; crucially, a significant portion of the privileged few people with computer access in the late '70s were <em>Dungeons &amp; Dragons</em> players. It didn’t require much of a mental leap to try to combine the two; at first to offload number crunching to the computer, eventually to create the solo playing experiences that were the first computerized RPGs.
This process culminated in <em>Wizardry</em> and <em>Ultima</em>, the two franchises that dominated computer gaming in the '80s.</p>
<p>
Over in Japan, the Enix designers combined the dungeon crawling from <em>Wizardry</em> and the over-world exploration of <em>Ultima</em>, adjusting them to the limitations of the Famicom/NES console—and to the tastes of the local public.
With a linear story, streamlined systems focused on battles, and a more forgiving difficulty level, <em>Dragon Quest</em>
became the blueprint of what would become the Japanese RPG genre<sup><a id="footnote-reference-4" href="#footnote-4">4</a></sup>. Shigeru Miyamoto offered his own interpretation in <em>The Legend of Zelda</em>, with a shift towards arcade action and a leveling system reified as a heart count. A decade later, the Blizzard North team would reinvent role-playing on the PC by removing most of its ceremony. Drawing heavily from the roguelikes, <em>Diablo</em> simplifies character setup and stats and generally removes anything that could stand in the way of slashing monsters and grabbing loot<sup><a id="footnote-reference-5" href="#footnote-5">5</a></sup>.</p>
<p>
In retrospect, looking at tabletop RPGs felt backward because, by using the video games I already knew as models instead, I was benefiting from decades of RPG system simplifications—half the job had already been done.</p>
<h2 id="design">
Design
</h2>
<p>
I wanted the least amount of stats that could make battles work non-deterministically enough to be fun.
Inspired by <em>TWERPS</em>, I briefly considered having a single stat to determine both inflicted damage and available hit points, but that resulted in unbalanced battles, so I went instead with the classic <code>hp</code> and <code>strength</code> stats. Later, when outlining the battle routine, it became apparent that I would also need a <code>speed</code> stat to mimic the turn-based style of <em>Final Fantasy</em>; that is, rather than having each character attack in a round-robin fashion, the fastest characters would get turns more frequently. These choices <a href="https://github.com/facundoolano/rpg-cli/blob/d4c90252db34a04e9abb7e96623c62d6fe47edfe/src/character.rs#L15-L27">resulted</a> in the following struct:</p>
<div>
<pre><code><span><span><span>pub</span><span> </span><span>struct</span> <span>Character</span><span> </span>{<span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>name: <span>String</span>,<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>level: <span>i32</span>,<span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>xp: <span>i32</span>,<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>max_hp: <span>i32</span>,<span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>current_hp: <span>i32</span>,<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>strength: <span>i32</span>,<span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>speed: <span>i32</span>,<span>
</span></span></span><span><span><span></span>}</span></span></code></pre>
</div>
<p>
Item and equipment management was another feature that I found could be automated. Items would be bought at the home directory, with an <code>rpg-cli shop</code> subcommand, or found in chests, by inspecting directories with <code>rpg-cli ls</code>. Equipment would be generic and level-based; instead of a Wooden Sword, a Bronze Blade, or a Steel Saber, players would have a <code>sword[1]</code> and a <code>shield[1]</code> available at the shop from the start, a <code>sword[5]</code> and a <code>shield[5]</code> unlocked when the hero reached level 5, and so on. Stronger equipment would automatically replace its weaker equivalent when bought or found, removing the sell-old-buy-new toil of traditional JRPGs. Healing items would be similarly level-based.</p>
<p>
When I eventually imported the permadeath feature from roguelikes, I decided to drop a tombstone to recover gold, items, and equipment from the directory where the character died, giving the player some sense of progress and making it more feasible to unlock end-game features.</p>
<h2 id="development">
Development
</h2>
<p>As soon as I started prototyping, I learned that I couldn’t control the shell working directory from my program (something obvious if you think about it, but that I hadn’t considered before). The solution was for the program state to track its own “path to current hero location”, and use a shell function to sync with it:</p>
<div>
<pre><code><span><span>rpg <span>()</span> <span>{</span>
</span></span><span><span>    rpg-cli <span>"</span><span>$@</span><span>"</span>         <span># forward arguments to rpg-cli</span>
</span></span><span><span>    <span>cd</span> <span>"</span><span>$(</span>rpg-cli <span>pwd</span><span>)</span><span>"</span>  <span># move shell to the hero's location</span>
</span></span><span><span><span>}</span></span></span></code></pre>
</div>
<p>
The hardcore version would be to overwrite the built-in <code>cd</code> function so that enemies would pop up as the user changed directories:</p>
<div>
<pre><code><span><span><span>cd</span> <span>()</span> <span>{</span>
</span></span><span><span>    rpg-cli <span>cd</span> <span>"</span><span>$@</span><span>"</span>
</span></span><span><span>    <span>builtin</span> <span>cd</span> <span>"</span><span>$(</span>rpg-cli <span>pwd</span><span>)</span><span>"</span>
</span></span><span><span><span>}</span></span></span></code></pre>
</div>
<p>
Other commands like <code>rm</code>, <code>mkdir</code>, or <code>touch</code>, could be similarly aliased to integrate with the game. These usage patterns paved the way for <a href="https://github.com/facundoolano/rpg-cli/blob/da433ff186ba32e86c386e049b3f68e0b6c7de80/shell/README.md">further options and flags</a>, to show the game state at the shell prompt, write scripts, and build custom gameplay flows.</p>
<p>∗ ∗ ∗</p>
<p>
Once I got the core of the game working, I used it as a canvas, loosening up on minimalism to port features I liked from other games: character classes, status ailments, a quest to-do list, hidden enemies, easter eggs, and a final boss. This is what the <a href="https://github.com/facundoolano/rpg-cli/blob/da433ff186ba32e86c386e049b3f68e0b6c7de80/src/character/mod.rs#L16-L36">character struct</a> looked like after these extensions:</p>
<div>
<pre><code><span><span><span>pub</span><span> </span><span>struct</span> <span>Character</span><span> </span>{<span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>class: <span>Class</span>,<span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>level: <span>i32</span>,<span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>xp: <span>i32</span>,<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span>max_hp: <span>i32</span>,<span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>current_hp: <span>i32</span>,<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span>max_mp: <span>i32</span>,<span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>current_mp: <span>i32</span>,<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span>strength: <span>i32</span>,<span>
</span></span></span><span><span><span>    </span>speed: <span>i32</span>,<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>sword: <span>Option</span><span>&lt;</span>Equipment<span>&gt;</span>,<span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>shield: <span>Option</span><span>&lt;</span>Equipment<span>&gt;</span>,<span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>left_ring: <span>Option</span><span>&lt;</span>Ring<span>&gt;</span>,<span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>right_ring: <span>Option</span><span>&lt;</span>Ring<span>&gt;</span>,<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span>status_effect: <span>Option</span><span>&lt;</span>StatusEffect<span>&gt;</span>,<span>
</span></span></span><span><span><span></span>}</span></span></code></pre>
</div>
<p>
The character classes are defined in a <a href="https://github.com/facundoolano/rpg-cli/blob/f2d37631628461ee192864e464e2088415e3866c/src/character/classes.yaml">yaml file</a> that can be overridden by the user to customize the game. Here’s an excerpt:</p>
<div>
<pre><code><span><span>- <span>name</span>:<span> </span>warrior<span>
</span></span></span><span><span><span>  </span><span>hp</span>:<span> </span>[<span>50</span>,<span> </span><span>10</span>]<span>
</span></span></span><span><span><span>  </span><span>strength</span>:<span> </span>[<span>12</span>,<span> </span><span>3</span>]<span>
</span></span></span><span><span><span>  </span><span>speed</span>:<span> </span>[<span>11</span>,<span> </span><span>2</span>]<span>
</span></span></span><span><span><span>  </span><span>category</span>:<span> </span>player<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>mage<span>
</span></span></span><span><span><span>  </span><span>hp</span>:<span> </span>[<span>30</span>,<span> </span><span>6</span>]<span>
</span></span></span><span><span><span>  </span><span>mp</span>:<span> </span>[<span>10</span>,<span> </span><span>4</span>]<span>
</span></span></span><span><span><span>  </span><span>strength</span>:<span> </span>[<span>10</span>,<span> </span><span>3</span>]<span>
</span></span></span><span><span><span>  </span><span>speed</span>:<span> </span>[<span>10</span>,<span> </span><span>2</span>]<span>
</span></span></span><span><span><span>  </span><span>category</span>:<span> </span>player<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>rat<span>
</span></span></span><span><span><span>  </span><span>hp</span>:<span> </span>[<span>15</span>,<span> </span><span>5</span>]<span>
</span></span></span><span><span><span>  </span><span>strength</span>:<span> </span>[<span>5</span>,<span> </span><span>2</span>]<span>
</span></span></span><span><span><span>  </span><span>speed</span>:<span> </span>[<span>16</span>,<span> </span><span>2</span>]<span>
</span></span></span><span><span><span>  </span><span>category</span>:<span> </span>common<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>dragon<span>
</span></span></span><span><span><span>  </span><span>hp</span>:<span> </span>[<span>110</span>,<span> </span><span>5</span>]<span>
</span></span></span><span><span><span>  </span><span>strength</span>:<span> </span>[<span>25</span>,<span> </span><span>2</span>]<span>
</span></span></span><span><span><span>  </span><span>speed</span>:<span> </span>[<span>8</span>,<span> </span><span>2</span>]<span>
</span></span></span><span><span><span>  </span><span>inflicts</span>:<span> </span>[burn, 2]<span>
</span></span></span><span><span><span>  </span><span>category</span>:<span> </span>rare<span>
</span></span></span><span><span><span></span>- <span>name</span>:<span> </span>basilisk<span>
</span></span></span><span><span><span>  </span><span>hp</span>:<span> </span>[<span>180</span>,<span> </span><span>3</span>]<span>
</span></span></span><span><span><span>  </span><span>strength</span>:<span> </span>[<span>100</span>,<span> </span><span>2</span>]<span>
</span></span></span><span><span><span>  </span><span>speed</span>:<span> </span>[<span>18</span>,<span> </span><span>2</span>]<span>
</span></span></span><span><span><span>  </span><span>inflicts</span>:<span> </span>[poison, 2]<span>
</span></span></span><span><span><span>  </span><span>category</span>:<span> </span>legendary</span></span></code></pre>
</div>
<p>
The <a href="https://github.com/facundoolano/rpg-cli/blob/da433ff186ba32e86c386e049b3f68e0b6c7de80/src/game.rs#L86-L106"><code>Game::go_to</code></a> function shows how directory traversal is mapped to player movement and enemy spawning:</p>
<div>
<pre><code><span><span><span>/// Move the hero's location towards the given destination, one directory
</span></span></span><span><span><span>/// at a time, with some chance of enemies appearing on each one.
</span></span></span><span><span><span></span><span>pub</span><span> </span><span>fn</span> <span>go_to</span>(<span>
</span></span></span><span><span><span>    </span><span>&amp;</span><span>mut</span><span> </span><span>self</span>,<span>
</span></span></span><span><span><span>    </span>dest: <span>&amp;</span><span>Location</span>,<span>
</span></span></span><span><span><span>    </span>run: <span>bool</span>,<span>
</span></span></span><span><span><span>    </span>bribe: <span>bool</span>,<span>
</span></span></span><span><span><span></span>)<span> </span>-&gt; <span>Result</span><span>&lt;</span>(),<span> </span>character::Dead<span>&gt;</span><span> </span>{<span>
</span></span></span><span><span><span>    </span><span>while</span><span> </span><span>self</span>.location<span> </span><span>!=</span><span> </span><span>*</span>dest<span> </span>{<span>
</span></span></span><span><span><span>        </span><span>// set the hero's location to the one given
</span></span></span><span><span><span></span><span>        </span><span>// and apply related side effects.
</span></span></span><span><span><span></span><span>        </span><span>self</span>.visit(<span>self</span>.location.go_to(dest))<span>?</span>;<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>        </span><span>if</span><span> </span><span>!</span><span>self</span>.location.is_home()<span> </span>{<span>
</span></span></span><span><span><span>            </span><span>if</span><span> </span><span>let</span><span> </span><span>Some</span>(<span>mut</span><span> </span>enemy)<span> </span><span>=</span><span> </span>enemy::spawn(<span>&amp;</span><span>self</span>.location,<span> </span><span>&amp;</span><span>self</span>.player)<span> </span>{<span>
</span></span></span><span><span><span>                </span><span>// Attempt to bribe or run away according to the given options,
</span></span></span><span><span><span></span><span>                </span><span>// and start a battle if that fails.
</span></span></span><span><span><span></span><span>                </span><span>if</span><span> </span><span>self</span>.battle(<span>&amp;</span><span>mut</span><span> </span>enemy,<span> </span>run,<span> </span>bribe)<span>?</span><span> </span>{<span>
</span></span></span><span><span><span>                    </span><span>return</span><span> </span><span>Ok</span>(());<span>
</span></span></span><span><span><span>                </span>}<span>
</span></span></span><span><span><span>            </span>}<span>
</span></span></span><span><span><span>        </span>}<span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>    </span><span>Ok</span>(())<span>
</span></span></span><span><span><span></span>}</span></span></code></pre>
</div>
<p>
As a wrap-up, see below the full definition of <a href="https://github.com/facundoolano/rpg-cli/blob/f2d37631628461ee192864e464e2088415e3866c/src/game.rs#L266-L316"><code>Game::run_battle</code></a>, the auto-battle routine at the core of the game. In a sense, the rest of the code exists as support for this function:</p>
<div>
<pre><code><span><span><span>/// Runs a turn-based combat between the game's player and the given enemy.
</span></span></span><span><span><span>/// The frequency of the turns is determined by the speed stat of each
</span></span></span><span><span><span>/// character.
</span></span></span><span><span><span>///
</span></span></span><span><span><span>/// Some special abilities are enabled by the player's equipped rings:
</span></span></span><span><span><span>/// Double-beat, counter-attack and revive.
</span></span></span><span><span><span>///
</span></span></span><span><span><span>/// Returns Ok(xp gained) if the player wins, or Err(()) if it loses.
</span></span></span><span><span><span></span><span>fn</span> <span>run_battle</span>(<span>&amp;</span><span>mut</span><span> </span><span>self</span>,<span> </span>enemy: <span>&amp;</span><span>mut</span><span> </span>Character)<span> </span>-&gt; <span>Result</span><span>&lt;</span><span>i32</span>,<span> </span>character::Dead<span>&gt;</span><span> </span>{<span>
</span></span></span><span><span><span>    </span><span>// Player's using the revive ring can come back to life at most once per battle
</span></span></span><span><span><span></span><span>    </span><span>let</span><span> </span><span>mut</span><span> </span>already_revived<span> </span><span>=</span><span> </span><span>false</span>;<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>// These accumulators get increased based on the character's speed:
</span></span></span><span><span><span></span><span>    </span><span>// the faster will get more frequent turns.
</span></span></span><span><span><span></span><span>    </span><span>let</span><span> </span>(<span>mut</span><span> </span>pl_accum,<span> </span><span>mut</span><span> </span>en_accum)<span> </span><span>=</span><span> </span>(<span>0</span>,<span> </span><span>0</span>);<span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>mut</span><span> </span>xp<span> </span><span>=</span><span> </span><span>0</span>;<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>while</span><span> </span>enemy.current_hp<span> </span><span>&gt;</span><span> </span><span>0</span><span> </span>{<span>
</span></span></span><span><span><span>        </span>pl_accum<span> </span><span>+=</span><span> </span><span>self</span>.player.speed();<span>
</span></span></span><span><span><span>        </span>en_accum<span> </span><span>+=</span><span> </span>enemy.speed();<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>        </span><span>if</span><span> </span>pl_accum<span> </span><span>&gt;=</span><span> </span>en_accum<span> </span>{<span>
</span></span></span><span><span><span>            </span><span>// In some urgent circumstances, it's preferable to use the turn to
</span></span></span><span><span><span></span><span>            </span><span>// recover mp or hp than attacking
</span></span></span><span><span><span></span><span>            </span><span>if</span><span> </span><span>!</span><span>self</span>.autopotion(enemy)<span> </span><span>&amp;&amp;</span><span> </span><span>!</span><span>self</span>.autoether(enemy)<span> </span>{<span>
</span></span></span><span><span><span>                </span><span>let</span><span> </span>(new_xp,<span> </span>_)<span> </span><span>=</span><span> </span><span>self</span>.player.attack(enemy);<span>
</span></span></span><span><span><span>                </span>xp<span> </span><span>+=</span><span> </span>new_xp;<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>                </span><span>self</span>.player.maybe_double_beat(enemy);<span>
</span></span></span><span><span><span>            </span>}<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>            </span><span>// Status effects are applied after each turn. The player may die
</span></span></span><span><span><span></span><span>            </span><span>// during its own turn because of status ailment damage
</span></span></span><span><span><span></span><span>            </span><span>let</span><span> </span>died<span> </span><span>=</span><span> </span><span>self</span>.player.apply_status_effects();<span>
</span></span></span><span><span><span>            </span>already_revived<span> </span><span>=</span><span> </span><span>self</span>.player.maybe_revive(died,<span> </span>already_revived)<span>?</span>;<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>            </span>pl_accum<span> </span><span>=</span><span> </span><span>-</span><span>1</span>;<span>
</span></span></span><span><span><span>        </span>}<span> </span><span>else</span><span> </span>{<span>
</span></span></span><span><span><span>            </span><span>let</span><span> </span>(_,<span> </span>died)<span> </span><span>=</span><span> </span>enemy.attack(<span>&amp;</span><span>mut</span><span> </span><span>self</span>.player);<span>
</span></span></span><span><span><span>            </span>already_revived<span> </span><span>=</span><span> </span><span>self</span>.player.maybe_revive(died,<span> </span>already_revived)<span>?</span>;<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>            </span><span>self</span>.player.maybe_counter_attack(enemy);<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>            </span>enemy.apply_status_effects().unwrap_or_default();<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>            </span>en_accum<span> </span><span>=</span><span> </span><span>-</span><span>1</span>;<span>
</span></span></span><span><span><span>        </span>}<span>
</span></span></span><span><span><span>    </span>}<span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>Ok</span>(xp)<span>
</span></span></span><span><span><span></span>}</span></span></code></pre>
</div>
<p>
I like that, after a few years, I still find it reasonably self-explanatory.</p>
<h2 id="postscript-a-text-interface-for-rpg-cli">
Postscript: A text interface for rpg-cli
</h2>
<p>
Having to rely on preexisting directories to make progress in the game gets tedious after a while. I resorted to <a href="https://github.com/facundoolano/rpg-cli/tree/da433ff186ba32e86c386e049b3f68e0b6c7de80/shell#arbitrary-dungeon-levels">a function</a> that creates directories on the fly; other players wrote scripts to skip level grinding. The file system integration turned rpg-cli into a curiosity, but it had been more of an afterthought, the result of making the game fit into a command-line interface. Internally, the code converted paths into an abstract <code>Location</code> and only cared about its “distance from home” to determine things like enemy level and frequency.</p>
<p>
Since the shell wasn’t essential to it, as soon as my RPG model felt complete, I started toying with the idea of switching to a different interface. The obvious choice was a rogue-like text interface, displaying symbolic ASCII characters in the terminal.
To make that work, the main adjustments would be turning this “distance from home” into a dungeon floor level, and spawning enemies as the player moved around the floor. I was curious to experiment with procedural level generation while preserving most of the other rpg-cli design choices (basic classes, generic items, and random automatic battles).</p>
<p>
I started playing <a href="https://github.com/tmewett/BrogueCE">Brogue</a> and picked up a <a href="https://www.routledge.com/Procedural-Generation-in-Game-Design/Short-Adams/p/book/9781498799195">book on procedural generation</a> for inspiration. I scoped the project and did <a href="https://github.com/facundoolano/rpg-tui">some prototyping</a> but eventually dropped the idea, in part because I wasn’t as interested in Rust programming anymore, but mostly because I had been trying to document the development process (of both rpg-cli and this new rpg-tui project) to write a kind of book or long tutorial, which turned out to be too distracting—I was more interested in the writing than in revisiting an old project.
Some of that work made it into <a href="https://olano.dev/blog/de-von-bismarck-a-tolkien">a couple</a> <a href="https://olano.dev/blog/del-videojuego-como-puzzle">of posts</a> last year. I cannibalized the rest to write this.</p>
<h2 id="notes">
Notes
</h2>


      

      
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I quit my job and made an automatic time tracker (120 pts)]]></title>
            <link>https://taimapp.io</link>
            <guid>40977453</guid>
            <pubDate>Tue, 16 Jul 2024 15:28:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://taimapp.io">https://taimapp.io</a>, See on <a href="https://news.ycombinator.com/item?id=40977453">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>     <main><header></header>    <section><div><p><img src="https://taimapp.io/tray.png" alt="Taim logo"></p><h2 data-svelte-h="svelte-1s4m87e">Automated time tracking software<br>to save you time</h2> <p data-svelte-h="svelte-ppsrs9">Forgetting to start a timer is an issue of the past. You do your job, we keep track of it.</p> <div data-svelte-h="svelte-r1ro5t"><p><a href="#pricing">Pre-order now</a></p><p>macOS Ventura 13.1+, Win 10+* is recommended</p></div></div> <div><p><img src="https://taimapp.io/taimapp-preview.png" alt="Automatic time tracking software interface"></p>  </div> </section> <div><div id="section-one"><h2>Struggling with Tracking<br> Your Work Hours?</h2> <p>Freelancers often face the hassle of manually starting and stopping timers, leading to inaccurate time tracking and billing. 
        Missed hours and overcharges can harm your reputation and client trust.</p></div> <div> <div data-svelte-h="svelte-mv45p8"> <p>Documenting
        <span>11:30 - 13.00</span></p></div>      <div><div data-svelte-h="svelte-e0k7ia"> <p>My Project 1
        <span>09:00 - 10.00</span></p></div>  <div data-svelte-h="svelte-1v8rkjr"> <p>Mega Corp Inc.
        <span>11.30 - 13.00</span></p></div></div></div> <div id="product"><h2>Why Choose Taim?</h2> <p>You have full control over your sessions. You can either record sessions manually or automatically.
      Need to change the duration or date of a session? No problem. You can easily edit sessions to reflect the correct information.</p> </div> <div id="features"><h2>Control your sessions</h2> <p>It doesn't matter if you work on a personal project or on a client's project, you can adjust settings accordingly.</p> <div id="cards"><div><p><img src="https://taimapp.io/features/toggle-billing.png" alt="Toggle Billing"></p><div><h3>Toggle Billing</h3> <p>Select whether a session is billable &amp; paid or not, to make your invoicing process easier.</p> </div></div><div><p><img src="https://taimapp.io/features/application-flow.png" alt="Application Flow"></p><div><h3>Application Flow</h3> <p>Easily see an overview of your activity, and choose what to log.</p> </div></div><div><p><img src="https://taimapp.io/features/time-modifying.png" alt="Modify time &amp; sessions"></p><div><h3>Modify time &amp; sessions</h3> <p>Edit your session data by adding or removing time &amp; data whenever needed.</p> </div></div><div><p><img src="https://taimapp.io/features/notes.png" alt="Notes"></p><div><h3>Notes</h3> <p>Add shareable notes to your sessions &amp; projects to keep track of important details.</p> </div></div><div><p><img src="https://taimapp.io/features/ai-time-tracker.png" alt="Learns from you"></p><div><h3>Learns from you</h3> <p>More work equals bigger brains. Over time Taim can start to log sessions automatically.</p> </div></div><div><p><img src="https://taimapp.io/features/filtering.png" alt="Advanced Filtering"></p><div><h3>Advanced Filtering</h3> <p>Filter time your tracked time by date, statuses, project, tags, and more.</p> </div></div></div> </div></div> <div><h2>Resource efficient</h2> <p>Designed to consume low power, storage. Just like any native application.</p> <div data-svelte-h="svelte-1gpxnfw"><p><span>CPU USAGE: 5-15%</span>
        Other apps</p>  <svg viewBox="0 0 1653 512" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_2222_2)"><path d="M0 332.453L12.5227 333.066C25.0455 333.679 50.0909 334.904 75.1364 338.357C100.182 341.809 125.227 347.488 150.273 326.69C175.318 305.892 200.364 258.617 225.409 258.952C250.455 259.287 275.5 307.231 300.545 344.666C325.591 382.101 350.636 409.026 375.682 424.843C400.727 440.659 425.773 445.367 450.818 452.882C475.864 460.398 500.909 470.721 525.955 468.756C551 466.791 576.045 452.538 601.091 384.897C626.136 317.256 651.182 196.226 676.227 186.558C701.273 176.889 726.318 278.582 751.364 270.579C776.409 262.576 801.455 144.879 826.5 132.177C851.545 119.475 876.591 211.769 901.636 207.823C926.682 203.877 951.727 103.691 976.773 78.7498C1001.82 53.8091 1026.86 104.114 1051.91 122.007C1076.95 139.901 1102 125.383 1127.05 141.174C1152.09 156.964 1177.14 203.064 1202.18 197.931C1227.23 192.797 1252.27 136.431 1277.32 148.65C1302.36 160.868 1327.41 241.672 1352.45 285.831C1377.5 329.99 1402.55 337.503 1427.59 316.721C1452.64 295.94 1477.68 246.863 1502.73 186.553C1527.77 126.243 1552.82 54.6998 1577.86 27.2078C1602.91 -0.284178 1627.95 16.2749 1640.48 24.5544L1653 32.8339" stroke="url(#paint0_linear_2222_2)" stroke-width="3"></path><path d="M0 452.057L13.3492 452.457C26.6984 452.856 53.3968 453.656 80.0952 454.901C106.794 456.147 133.492 457.838 160.19 454.227C186.889 450.615 213.587 441.7 240.286 442.322C266.984 442.943 293.683 453.1 320.381 461.152C347.079 469.204 373.778 475.151 400.476 478.873C427.175 482.595 453.873 484.092 480.571 486.151C507.27 488.21 533.968 490.832 560.667 490.992C587.365 491.153 614.063 488.852 640.762 475.858C667.46 462.864 694.159 439.177 720.857 437.794C747.556 436.412 774.254 457.334 800.952 456.285C827.651 455.236 854.349 432.216 881.048 430.226C907.746 428.236 934.444 447.276 961.143 447.04C987.841 446.804 1014.54 427.291 1041.24 422.85C1067.94 418.408 1094.63 429.038 1121.33 433.176C1148.03 437.314 1174.73 434.96 1201.43 438.677C1228.13 442.394 1254.83 452.181 1281.52 451.707C1308.22 451.233 1334.92 440.497 1361.62 443.499C1388.32 446.5 1415.02 463.239 1441.71 472.637C1468.41 482.036 1495.11 484.095 1521.81 480.487C1548.51 476.878 1575.21 467.602 1601.9 456.077C1628.6 444.551 1655.3 430.776 1668.65 423.888L1682 417" stroke="url(#paint1_linear_2222_2)" stroke-width="3"></path></g><defs><linearGradient id="paint0_linear_2222_2" x1="0" y1="240.5" x2="1653" y2="240.5" gradientUnits="userSpaceOnUse"><stop stop-color="#A91576" stop-opacity="0"></stop><stop offset="0.255" stop-color="#A91576"></stop><stop offset="0.765" stop-color="#870505"></stop><stop offset="1" stop-color="#870505" stop-opacity="0"></stop></linearGradient><linearGradient id="paint1_linear_2222_2" x1="0" y1="454" x2="1682" y2="454" gradientUnits="userSpaceOnUse"><stop stop-color="#18A0FB" stop-opacity="0"></stop><stop offset="0.205" stop-color="#18A0FB"></stop><stop offset="0.775" stop-color="#53023C"></stop><stop offset="1" stop-color="#53023C" stop-opacity="0"></stop></linearGradient><clipPath id="clip0_2222_2"><rect width="1653" height="512" fill="white"></rect></clipPath></defs></svg></div></div> <div id="fullcontrol"><h2>Time tracking software with full control</h2> <p>Taim is a time tracking software that works just like you want it to. Clicks and calculations that would otherwise take a lot of time are made instantly for you.</p> <div><div><p><span><span>CSV</span> Jun 9 - Jun 10 sessions</span></p></div><div><p><span><span>PDF</span> Paid sessions this week</span></p></div><div><p><span><span>XLS</span> Billable sessions last month</span></p></div></div></div> <div id="section-three"><h2>Work Smarter, Not Harder</h2> <p>Customize Taim to your needs, if you want to track time manually, you can do that. 
        If you want to track time automatically, you can do that as well.
         It is up to you to decide how you want to track your time.</p> </div>  <div><div id="pricing"><h2>Pay once, use forever</h2> <p>Taim is a one-time purchase. You get all the features in every plan.</p> <div data-svelte-h="svelte-k74s6c"><p><span>PRE-SALE IS LIVE!</span> <span>Currently Taim is available to purchase as a presale. We are planning to launch Taim to the public in early autumn (Sep-Oct), be sure to checkout our <a href="https://taimapp.io/roadmap">roadmap.</a></span></p></div> <div> <div><div><h3>Individual</h3> <p>Early-bird 56% off</p></div> <p>One-time payment for a single user.</p>   <ul role="list"><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>Pay once, use forever</span> </li><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>1 macOS device</span> </li><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>All features</span> </li><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>Free updates for 12 months</span> </li><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>Local storage</span> </li></ul>  </div><div><p><h3>Teams</h3> </p> <p>Great for multi-devices setups &amp; teams.</p>   <ul role="list"><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>Pay per seat for your team.</span> </li><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>Unlimited devices</span> </li><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>All features</span> </li><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>App updates during the subscription</span> </li><li> <svg fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z" clip-rule="evenodd"></path></svg> <span>Cloud storage</span> </li></ul>  </div></div></div> <div><h2>Questions &amp; Answers</h2> <p>Get answers to your questions. For additional questions, please get in touch.</p> </div></div></main>  
			
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Devs need system design tools, not diagramming tools (159 pts)]]></title>
            <link>https://thenewstack.io/devs-need-system-design-tools-not-diagramming-tools/</link>
            <guid>40977308</guid>
            <pubDate>Tue, 16 Jul 2024 15:09:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thenewstack.io/devs-need-system-design-tools-not-diagramming-tools/">https://thenewstack.io/devs-need-system-design-tools-not-diagramming-tools/</a>, See on <a href="https://news.ycombinator.com/item?id=40977308">Hacker News</a></p>
Couldn't get https://thenewstack.io/devs-need-system-design-tools-not-diagramming-tools/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Codestral Mamba (393 pts)]]></title>
            <link>https://mistral.ai/news/codestral-mamba/</link>
            <guid>40977103</guid>
            <pubDate>Tue, 16 Jul 2024 14:44:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/codestral-mamba/">https://mistral.ai/news/codestral-mamba/</a>, See on <a href="https://news.ycombinator.com/item?id=40977103">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Following the publishing of the Mixtral family, Codestral Mamba is another step in our effort to study and provide new architectures. It is available for free use, modification, and distribution, and we hope it will open new perspectives in architecture research. Codestral Mamba was designed with help from Albert Gu and Tri Dao.</p><p>Unlike Transformer models, <a href="https://arxiv.org/abs/2312.00752">Mamba models</a> offer the advantage of linear time inference and the theoretical ability to model sequences of infinite length. It allows users to engage with the model extensively with quick responses, irrespective of the input length. This efficiency is especially relevant for code productivity use cases—this is why we trained this model with advanced code and reasoning capabilities, enabling it to perform on par with SOTA transformer-based models.</p><p><img src="https://mistral.ai/images/news/codestral-mamba/codestral-mamba-benchmarks.png" alt="Detailed Codestral Mamba benchmarks" width="100%"></p><p>We have tested Codestral Mamba on in-context retrieval capabilities up to 256k tokens. We expect it to be a great local code assistant!</p><p>You can deploy Codestral Mamba using the <a href="https://github.com/mistralai/mistral-inference">mistral-inference</a> SDK, which relies on the reference implementations from Mamba’s GitHub repository. The model can also be deployed through <a href="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/mamba">TensorRT-LLM</a>. For local inference, keep an eye out for support in llama.cpp. You may download the raw weights from <a href="https://huggingface.co/mistralai/mamba-codestral-7B-v0.1">HuggingFace</a>.</p><p>For easy testing, we made Codestral Mamba available on <a href="https://console.mistral.ai/">la Plateforme</a> (<code>codestral-mamba-2407</code>), alongside its big sister, Codestral 22B. While Codestral Mamba is available under the Apache 2.0 license, Codestral 22B is available under a <a href="https://mistral.ai/contact/">commercial license</a> for self-deployment or a community license for testing purposes.</p><p><strong>Important:</strong> This is an instructed model, with 7,285,403,648 parameters.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Godotcaml for Godot 4.2 (121 pts)]]></title>
            <link>https://fizzixnerd.com/blog/2024-06-24-announcing-godotcaml/</link>
            <guid>40975509</guid>
            <pubDate>Tue, 16 Jul 2024 11:25:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fizzixnerd.com/blog/2024-06-24-announcing-godotcaml/">https://fizzixnerd.com/blog/2024-06-24-announcing-godotcaml/</a>, See on <a href="https://news.ycombinator.com/item?id=40975509">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Hello!  Today I’m releasing a project on which I’ve been working, that is in an early stage of development, into the open source world.  It is integration and bindings to Godot (currently just 4.2) from a new language: OCaml.  It is called Godotcaml.  Details below!</p>
<h2 id="why-godot">Why Godot?</h2>
<p>There are many reasons to choose Godot, but the reason I’ll focus on is that it provides a full game-development IDE from which you can develop production quality 2D and 3D games.  It’s suitable for small-to-medium-sized teams, quite mature, and very fun to use; I think most devs have a secret inclination to “one day” make a video game.  If there is one piece of advice I could give to those devs, it’s choose a good engine that already exists, unless you want to be stuck in Vulkan hell for 6-12 months.  Godot is a good engine, and happily, already exists, and is free and open source — so is a good first choice, even if you abandon it for something different later.</p>
<h2 id="why-ocaml">Why OCaml?</h2>
<p>While my greatest loved language is Haskell, there are some specific reasons that it is somewhat unsuitable for game development.  Instead of listing those, however, I will instead take a more positive approach and talk about why OCaml is an excellent language for game development.</p>
<ul>
<li>
<p><strong>Garbage Collected by Default:</strong> This may shock game devs used to the C++ lyfe, but will come as no surprise to people who’ve used Godot and/or Unity.  Programmers are just more productive when there is a garbage collector, and programmer time is what is usually most valuable — not machine time.  Now, you can write some pretty cool allocation-free OCaml code too — but that’s an optimization that you should measure your need for before you commit to it.</p>
</li>
<li>
<p><strong>Functional by Default:</strong> I love functional programming, and I love the kind of code you can write in an ML-like curried functional language (such as OCaml or Haskell).  So <em>if</em> I were to bind Godot to a new language, it would have to be a functional one.  However, excellent bindings (<code>gdext</code>) for Rust already exist, and it can be used as a workable functional language.  That being said, Rust has the borrowchecker and an aversion to garbage collection, and put simply, I don’t think it makes a particularly good game scripting language, even though it is a wonderful systems programming language.  I think OCaml can one day prove to be a more efficient vehicle for experienced functional programmers to create a game in.  (No hate intended!  This is just how I <em>feel</em>.)</p>
</li>
<li>
<p><strong>Eager by Default:</strong> I absolutely adore lazy APIs.  I might actually be in the minority now in the Haskell community that I think that laziness was not a mistake, but was an excellent choice because of the ergonomics it provides.  However, it’s definitely true that it makes it somewhat less straight-forward to reason about the runtime performance of your code, and indeed to debug it — at least without specialized knowledge that is, in my experience, rather rare to have.  OCaml is eager by default, and I think that’s probably better for a soft-realtime system, unless you’re using some specialized framework (e.g. one that I don’t know whether exists or not!).</p>
</li>
<li>
<p><strong>Side-effects for When You Need Them:</strong> If the world was all written in one language, Haskell would make a pretty good choice — not perfect by any stretch, but pretty good.  However, we live in the real world, where a C FFI is the glue holding this pot of spaghetti we call an operating system together.  Because of that, when doing FFI heavy code, a beautiful language that makes it slightly more tedious to work with side-effects is less preferable in my experience than a language that simply encourages you to think before you use them, but still easily allows you to do things like have global mutable references at the top level.</p>
</li>
<li>
<p><strong>PPXes For CodeGen Help:</strong> I’m not totally sold on the pervasive use of PPXes for OCaml code, but one thing they are definitely useful for is code gen, and you would need a <em>lot</em> of tedious hand-written code if you wanted to interact with Godot directly by hand.  PPXes lie somewhere between Rust macros and TemplateHaskell in their power, but most problems with codegen were able to be solved to my satisfaction using the wonderful PpxLib and context-free extenders.  For example, here is the definition of a simple Godot class that inherits from the stock <code>Node</code> class, and provides a successor function for Godot <code>int</code>s:</p>
</li>
</ul>
<pre is:raw="" tabindex="0"><code><span><span>module</span><span>%</span><span>gclass </span><span>MyClass</span><span> </span><span>=</span><span> struct</span></span>
<span><span>  </span><span>[</span><span>%%</span><span>ginherits </span><span>Node]</span></span>
<span></span>
<span><span>  </span><span>let</span><span>%</span><span>gfunc </span><span>succ</span><span> </span><span>=</span><span> </span></span>
<span><span>    </span><span>[|</span><span> </span><span>ClassMethodFlags</span><span>.</span><span>default </span><span>|]</span></span>
<span><span>      (module </span><span>BuiltinClass0</span><span>.</span><span>Int</span><span>)</span></span>
<span><span>      (module </span><span>Class</span><span>.</span><span>Node</span><span>)</span></span>
<span><span>      (module </span><span>BuiltinClass0</span><span>.</span><span>Int</span><span>) </span></span>
<span><span>      (</span><span>fun</span><span> </span><span>i</span><span> </span><span>_self</span><span> </span><span>-&gt;</span><span> </span><span>Int64</span><span>.</span><span>(i </span><span>+</span><span> </span><span>1</span><span>L))</span></span>
<span><span>end</span></span></code></pre>
<p>Whether or not you like the use of PPXes in general, it is tough to argue that this code isn’t at least <em>short</em>, especially if I were to show you the amount of work you’d have to do without those <code>%</code>s!</p>
<ul>
<li><strong>More:</strong> If you’re reading this post, you probably already like OCaml already, so I’ll leave it there at “it’s a really nice pragmatic functional language, and I thought it would be a good candidate”!</li>
</ul>
<h2 id="what-can-it-do">What Can It Do?</h2>
<p>This is an extremely early stage of development, but basically at this point it is possible to:</p>
<ol>
<li>Call any builtin Godot utility function or method (static, virtual, or otherwise) from OCaml easily, and with documentation comments for the original function intact an available through your favourite OCaml LSP implementation.</li>
<li>Use Godot (binary) operators in a natural way from OCaml. (Unary operators are currently broken, which I will be investigating!)</li>
<li>Construct Godot values from OCaml easily, and from OCaml analogues if they exist (e.g. I incur a dependency on <code>Gg</code> for low-dimensional vector math)</li>
<li>Marshalling in and out of all these functions to/from the OCaml analogues.  That is, a method that is in Godot on an object of type <code>ClassyClass</code> taking an <code>int</code> parameter and returning an <code>int</code> will appear in Godotcaml as <code>int64 -&gt; ClassyClass.t structure ptr -&gt; int64</code>, where the <code>ClassyClass.t structure ptr</code> is the “pointer to the Godot object”, commonly called <code>self</code>.  (Note that this is always the <em>last</em> argument, to facilitate pipeline-style programming when GDScript programmers have a method-chaining interface.)</li>
<li>Naturally define a new Godot class in OCaml that inherits from an existing Godot-registered class.  (Currently NOT tested with classes defined in GDScript and/or externally.)</li>
<li>Most of the code-gen for custom engines that define new stock/builtin types and classes, etc.</li>
<li>Simulated inheritence for stock (and easily extendable to user-defined) classes using module inclusion:  That is if <code>Derived</code> inherits from <code>Base</code>, then simply include <code>Base</code> in the module representing <code>Derived</code>, and you get access to all the methods from <code>Base</code> without explicit casting (or in the case of Rust’s <code>gdext</code>, object composition).</li>
<li>Naturally define a new Godot method in OCaml and have it called from GDScript or another Godot-bound language. (ergonomics still WIP).</li>
</ol>
<h2 id="todo-or-what-cant-it-do">TODO (Or, What Can’t It Do):</h2>
<ol>
<li><strong>Signals:</strong> I’m still cooking ideas for how best to do this, but user-defined signals are not currently nicely supported, and even built in ones are not nice to call or interact with right now.  I’d also like them to be type-safe, so there’s that.  Very WIP, but fixable with some thought and work.</li>
<li><strong>Garbage Collection:</strong> Right now, if a OCaml reference is stored in a, say, GDScript variable, and contains no references in the OCaml world, then it might be collected out from under you.  This is fixable because of Godot’s wonderful reference-counting hooks and OCaml finalisers, I just haven’t gotten around to it yet.</li>
<li><strong>Nice Interface to Various Kinds of Methods:</strong> As of the writing of this blog post, it is only possible to define methods of arity <code>1 + Self</code> from OCaml.  No static methods, no virtual methods  This will be fixed soon, but I wanted to get the stuff in the hands of interested enthusiasts as soon as I was able to call OCaml functions from Godot.</li>
<li><strong>Real First Class Modules for Method Definitions:</strong> Taking again the example above, we can write</li>
</ol>
<pre is:raw="" tabindex="0"><code><span><span>let</span><span>%</span><span>gfunc </span><span>f</span><span> </span><span>=</span><span> </span></span>
<span><span>  </span><span>[|</span><span> </span><span>ClassMethodFlags</span><span>.</span><span>default </span><span>|]</span></span>
<span><span>  (module </span><span>ArgumentGodotTypeModule</span><span>)</span></span>
<span><span>  (module </span><span>SelfGodotTypeModule</span><span>)</span></span>
<span><span>  (module </span><span>ReturnValueGodotTypeModule</span><span>)</span></span>
<span><span>  (</span><span>fun</span><span> </span><span>x</span><span> </span><span>self</span><span> </span><span>-&gt;</span><span> </span><span>(* implementation here ...*)</span><span> </span><span>()</span><span>)</span></span></code></pre>
<p>making it seem like you could write, say</p>
<pre is:raw="" tabindex="0"><code><span><span>let</span><span> </span><span>m</span><span> </span><span>=</span><span> (module </span><span>SomeGodotTypeModule</span><span>)</span></span>
<span></span>
<span><span>let</span><span>%</span><span>gfunc </span><span>g</span><span> </span><span>=</span><span> </span><span>[|</span><span> </span><span>ClassMethodFlags</span><span>.</span><span>default </span><span>|]</span><span> m m m (</span><span>fun</span><span> </span><span>x</span><span> </span><span>y</span><span> </span><span>-&gt;</span><span> x)</span></span></code></pre>
<p>or something, but that wouldn’t work.  This is due to the way I generate code, but basically you have to consider the <code>module</code> as part of the “syntax” for <code>gfunc</code>; it directly takes the packed module out of the expression and codegens using whatever the <code>module</code> operator is applied to (i.e. at <em>parse</em> time, not <em>run</em> time).  This is pretty messed up and not ideal, but the way I justify it to myself is that <code>let%gfunc</code> introduces it’s own syntactic form for declaring a <em>type signature</em> and <em>implementation</em> of a method.  This is, as far as I can tell, <em>not fixable</em>, but I’d love to hear your thoughts if you think it is.  (Briefly, the problem is that if you try to use real first-class modules, their types escape the scope of the function because the implementation function contains them.)</p>
<ol start="5">
<li><strong>General Clean-up:</strong> This implementation was designed sort of ad-hoc and in-the-moment, so some of the stuff doesn’t quite make sense in the module architecture.  This is fixable but lower priority until I iron out the rest of the implementation details of the other features.</li>
<li><strong>Better Build System Integration:</strong> I don’t know dune very well, so I got it <em>working</em>, but it’s not exactly nice to use and develop on.  Lots of ad-hoc calls to <code>dune exec ./gen_api.exe</code> when something has changed, and then trying to remember to format with <code>ocamlformat -i *.ml</code> — that sort of thing; I’m sure dune can help with it, but I didn’t invest the time into learning properly (but I will).  Fixable.</li>
<li><strong>Hot-Reloading:</strong> This should be possible, as Rust somehow manages it in Godot 4.2+ but I haven’t even begun to look into it.  Right now, if you change an OCaml file and recompile the extension, you probably need to restart the editor to see the effects.  Fixable.</li>
<li><strong>Name-mangling for Custom Operators:</strong> I just haven’t done this, but it wouldn’t be hard (and would probably make a good first issue, if you’re looking to contribute).  Right now custom operators defined as <code>gfunc</code> methods in OCaml probably can’t be used from GDScript, or most other languages.  Perhaps at all!  I haven’t even tried.  Fixable.</li>
<li><strong>Finishing the C API:</strong> These represent a work-in-progress set of bindings that are by no means complete at the moment.  That needs to change eventually.  Fixable.</li>
<li><strong>Embedding a TopLevel:</strong> I’d like to be able to interact with the Godot world, well, interactively, from an OCaml Toplevel.  This is on my backburner, but it’s harder than it looks at first, because the shared_object you have to build must of course be native code, but Toplevel and friends are (seemingly?) only available as bytecode.  Ping me if you have ideas here! Fixability unknown!</li>
<li><strong>Reliably Not Segfault:</strong> This is an unsafe C api, and so it’s extremely sharp, and the interface is very rough around the edges, as I figure out what exactly each value is supposed to actually do.  DO NOT TRY TO MAKE A PRODUCTION GAME IN GODOTCAML RIGHT NOW!  I’m going to fix things, but they definitely aren’t ready for prime-time at the moment.  This is fixable, but will take time and testing.</li>
<li><strong>Testing:</strong> Speaking of testing, I have none.  If you’d like to contribute here, I’d be happy to hear from you; I’m personally going to prioritize other above areas until things are a little more stable in the API, so I’m not constantly changing things and fixing <em>broken tests</em> (i.e. as opposed to <em>broken code that is being tested</em>).  Fixable.</li>
<li><strong>Type Safety Concerns:</strong> Right now all classes have the same object type.  This makes it nice for when you’re inheriting from them, as module inclusion “just works”, but obviously have negative effects on the type safety of the system.  Destructive updates of the module types during inclusion is one possible solution, but this requires you to have a module type for every Godot class from which you wish to inherit — a tall order.  I believe this is fixable with some thought — and indeed, it <em>must</em> be fixed in my eyes, even if it means more code gen for the module signatures — but I haven’t given it much thought beyond that.</li>
<li><strong>Support Multiple Native Type Sizes:</strong> Godot’s api supports multiple configurations, depending on if you want float64 or float32, and 64-bit and 32-bit systems.  Right now, I’m concentrating on the float64 + 64-bit configuration, but this should be expanded at some point in the future.  Fixable.</li>
</ol>
<p>For more, check the issue page on GitHub, as that is where I’ll be doing the development.</p>
<h2 id="to-be-continued">To Be Continued</h2>
<p>More details and a setup guide to come!  If you’d like to get involved, I’d love to hear from you — best place to find me is either GitHub or the OCaml Discourse currently.  Beware, the code is pretty funky at the moment, but it’ll get there!</p>
<p>Best,</p>
<p><em>Matt</em></p>

        <p><span>#open-source</span><span>#ocaml</span><span>#godot</span><span>#godotcaml</span><span>#announcement</span>
        </p>
      </div><div>
    <p><img alt="Author Photo" width="96" height="96" src="https://fizzixnerd.com/_astro/fizzixnerd_Z1RMnIC.png" loading="eager" decoding="async">
    </p>
    <div>
      <p>
          About Matt Walker
        </p>
      <p>Matt Walker is a software engineer with a love for all things Functional, DevOps, and Typed, currently residing in Toronto, Canada.</p>
    </div>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Live Stream of VC Funded Startups – Use It for Research and Sales (503 pts)]]></title>
            <link>https://old.reddit.com/r/SaaSMarketing/comments/1e4ktjy/i_creatd_a_tool_to_track_all_live_vc_investments/</link>
            <guid>40975351</guid>
            <pubDate>Tue, 16 Jul 2024 10:59:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/SaaSMarketing/comments/1e4ktjy/i_creatd_a_tool_to_track_all_live_vc_investments/">https://old.reddit.com/r/SaaSMarketing/comments/1e4ktjy/i_creatd_a_tool_to_track_all_live_vc_investments/</a>, See on <a href="https://news.ycombinator.com/item?id=40975351">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="searchexpando"><p><label>limit my search to r/SaaSMarketing</label></p><div id="moresearchinfo"><p>use the following search parameters to narrow your results:</p><dl><dt>subreddit:<i>subreddit</i></dt><dd>find submissions in "subreddit"</dd><dt>author:<i>username</i></dt><dd>find submissions by "username"</dd><dt>site:<i>example.com</i></dt><dd>find submissions from "example.com"</dd><dt>url:<i>text</i></dt><dd>search for "text" in url</dd><dt>selftext:<i>text</i></dt><dd>search for "text" in self post contents</dd><dt>self:yes (or self:no)</dt><dd>include (or exclude) self posts</dd><dt>nsfw:yes (or nsfw:no)</dt><dd>include (or exclude) results marked as NSFW</dd></dl><p>e.g. <code>subreddit:aww site:imgur.com dog</code></p><p><a href="https://www.reddit.com/wiki/search">see the search faq for details.</a></p></div><p><a href="https://www.reddit.com/wiki/search" id="search_showmore">advanced search: by author, subreddit...</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Engineer's Guide to Deep Learning: Understanding the Transformer Model (240 pts)]]></title>
            <link>https://www.interdb.jp/dl/</link>
            <guid>40974193</guid>
            <pubDate>Tue, 16 Jul 2024 07:01:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.interdb.jp/dl/">https://www.interdb.jp/dl/</a>, See on <a href="https://news.ycombinator.com/item?id=40974193">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body-inner" tabindex="-1">
          <article>
            


<h2 id="the-engineers-guide-to-deep-learning">The Engineer’s Guide To Deep Learning</h2>
<figure><a href="https://www.flickr.com/photos/x-ray_delta_one/35257686756">
    <img src="https://www.interdb.jp/dl/metropolis.jpg" width="480"> </a>
</figure>


<p>We are in the third golden age of AI.</p>
<p>In the previous two golden ages (1950s-1960s and the 1980s),
our expectations outpaced the capabilities of the technology at the time, leading to disappointment.
In contrast, the AI technology of the current golden age, which began in the mid-2010s, has consistently exceeded our expectations.</p>
<p>Among AI technologies, the Transformer, introduced in 2017, stands as a groundbreaking breakthrough.
Initially developed as a machine translation model, its impact has extended to permeate nearly every field.
Today, the Transformer model is considered essential knowledge for modern engineers.</p>
<p>The first goal of this document is to provide the shortest path for engineers to understand the Transformer.</p>
<h6 id="what-is-this-document">What is this document</h6>
<ul>
<li>A concise guidebook:<br>
This document provides just enough information to learn the Transformer.</li>
</ul>
<!--
Unlike a textbook, it avoids detailed explanations.
However, I will explain where appropriate documentation is not available or where readers find the material difficult to understand.
-->
<h6 id="what-this-document-provides">What this document provides</h6>
<ul>
<li>
<p>Working <a href="https://github.com/s-hironobu/guide2dl/tree/main" target="_blank">Python code examples</a> for hands-on learning:<br>
To enhance comprehension, this document provides working Python code examples that readers can run themselves.</p>
</li>
<li>
<p>References for further exploration:<br>
This document introduces readers to a variety of documentation options, recognizing that different individuals find different resources more accessible.</p>
</li>
</ul>

<div>
  <p>Contents</p>
  <div>

<ul>
<li>Part 1: <a href="https://www.interdb.jp/dl/part01.html">Neural Networks</a><br>Introduces the fundamental concepts of neural networks.</li>
<li>Part 2: <a href="https://www.interdb.jp/dl/part02.html">Recurrent Neural Networks (RNNs)</a><br>Explores RNNs, including LSTM and GRU.</li>
<li>Part 3: <a href="https://www.interdb.jp/dl/part03.html">Natural Language Processing (NLP) and Attention Mechanisms</a><br>Provides the essential principles of NLP, encompassing machine translation and attention mechanisms.</li>
<li>Part 4: <a href="https://www.interdb.jp/dl/part04.html">Transformer</a><br>Unravels the Transformer model.</li>
<li>Appendix: <a href="https://www.interdb.jp/dl/L-00">Basic Knowledge</a><br>Provides the minimum knowledge of Python and mathematics required to understand the Transformer.</li>
</ul>
</div>
</div>

<div>
    <p>
    <label for="expand-05cf400c8aed19a98bb9d3ea186aea44">
        <i></i>
        <i></i>
        Change History (since 21st May, 2024)
    </label></p>
</div>
<h5 id="next-goal">Next goal</h5>
<p>Many Transformer-based technologies are currently being developed.
There will definitely be another major breakthrough in the near future.
I might write about them if I have time.</p>
<h5 id="copyright">Copyright</h5>
<p>© Copyright ALL Right Reserved, Hironobu SUZUKI.</p>
<p>For any inquiries regarding the use of this document or any of its figures, please contact me after reading the following FAQ:</p>

<div>
    <p>
    <label for="expand-988bd57a55344cb0d300647b4c8666b0">
        <i></i>
        <i></i>
        FAQ
    </label></p><div id="expandcontent-988bd57a55344cb0d300647b4c8666b0">

<p>Since publishing my content, I’ve been fortunate to receive a lot of positive feedback, which is truly gratifying.
However, I’ve also encountered a few instances where people tried to misuse my content for self-promotion in the past.</p>
<!--
(For more details, refer to the FAQ in "[the internals of PostgreSQL](https://www.interdb.jp/pg/)")
-->
<p>These experiences have shaped the approach I’ve outlined below:</p>
<ol>
<li>Who can use this document freely?<br>
If you are a teacher or a student belonging to an educational organization, you can freely use this document and figures in your study.
Anyone can use this document and figures with noncommercial meetings and lectures, if you state the link to this site and the copyright; otherwise, contact me.</li>
<li>Is it available for commercial contents?<br>
This content can be used under two options:
<ul>
<li>Revenue Share:
You can leverage this content after a revenue share agreement is signed.
Under this agreement, you’ll share 20% of the sales generated from using this content including the github repository.</li>
<li>Full Buyout:
In very rare cases, I consider requests for full commercial use of all content on this site (and the github repository).
For a complete buyout of all content rights, the cost is €10,000,000.</li>
</ul>
</li>
<li>Why doesn’t the author waive the copyright of this document or use the creative commons license?<br>
I’d like to ask you what problems you have by that I keep on having the copyright of my document.</li>
</ol>
</div>
</div>
<p>When you send me an email, please <strong>provide at least two SNS addresses (e.g. LinkedIn, Twitter) for verification purposes</strong>.
Due to the XZ backdoor incident, I no longer accept contact from anonymous individuals.</p>
<p><span><span><i></i></span><span>Exception</span></span> Educational institutions can use this document freely.</p>

<h6 id="hironobu-suzuki">Hironobu SUZUKI</h6>
<p>I am a software programmer/engineer, the author of:</p>
<ul>
<li><a href="http://www.interdb.jp/pg/" target="_blank">The Internals of PostgreSQL</a></li>
<li><a href="http://www.interdb.jp/dl/" target="_blank">The Engineer’s Guide To Deep Learning</a></li>
<li><a href="https://github.com/s-hironobu/pg_plan_inspector" target="_blank">pg_plan_inspector</a></li>
<li><a href="https://github.com/s-hironobu/pg_tuner" target="_blank">pg_tuner</a></li>
</ul>
<p>I graduated from graduate school in information engineering (M.S. in Information Engineering),
had worked for several companies as a software developer and technical manager/director,
and published seven books (4 PostgreSQL books and 3 MySQL books) in Japanese and a Chinese book.</p>
<p>As a director of the Japan PostgreSQL Users Group (2010-2016),
I organized the largest (non-commercial) technical seminar/lecture on PostgreSQL in Japan for more than six years,
and also served as the program committee chair of the Japan PostgreSQL Conference in 2013 and as a member in 2008 and 2009.
In June 2022, <a href="https://postgresql.life/post/hironobu_suzuki/" target="_blank">my interview article</a> was published.</p>
<p>Cuando era joven, vivió en Sudamérica por unos años. Recientemente, a veces vuelve a allí.</p>
<p>I am looking for a new job, applying ML and AI technologies to DBMS.</p>
<p>I’m interested in History, Animal Rights, Cosmology, Social Issues, Environment Issues. I play the piano and guitar. Vegetarian. I love animals, music, science.</p>

            
          </article>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI illegally barred staff from airing safety risks, whistleblowers say (155 pts)]]></title>
            <link>https://www.washingtonpost.com/technology/2024/07/13/openai-safety-risks-whistleblower-sec/</link>
            <guid>40974154</guid>
            <pubDate>Tue, 16 Jul 2024 06:51:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.washingtonpost.com/technology/2024/07/13/openai-safety-risks-whistleblower-sec/">https://www.washingtonpost.com/technology/2024/07/13/openai-safety-risks-whistleblower-sec/</a>, See on <a href="https://news.ycombinator.com/item?id=40974154">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="PBIKSWH2CJH27NQS6BVBMESB24" data-el="text" dir="null">OpenAI whistleblowers have filed a complaint with the Securities and Exchange Commission alleging the artificial intelligence company illegally prohibited its employees from <a href="https://www.washingtonpost.com/technology/2024/06/04/openai-employees-ai-whistleblowers/?itid=lk_inline_manual_2" target="_blank">warning </a>regulators about the grave risks its technology may pose to humanity, calling for an investigation.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="I2BEYH4RENHTBLYB2G5VYLGMDU" data-el="text" dir="null">The whistleblowers said OpenAI issued its employees overly restrictive employment, severance and nondisclosure agreements that could have led to penalties against workers who raised concerns about OpenAI to federal regulators,<a href="https://www.washingtonpost.com/documents/83df0e55-546c-498a-9efc-06fac591904e.pdf?itid=lk_inline_manual_4" target="_blank"> according to a seven-page letter</a> sent to the SEC commissioner earlier this month that referred to the formal complaint. The letter was obtained exclusively by The Washington Post.</p></div><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="U7EFJOVSBJANHMID5ZODY4GNNI" data-el="text" dir="null">OpenAI made staff sign employee agreements that required them to waive their federal rights to whistleblower compensation, the letter said. These agreements also required OpenAI staff to get prior consent from the company if they wished to disclose information to federal authorities. OpenAI did not create exemptions in its employee nondisparagement clauses for disclosing securities violations to the SEC.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="WADFSEVK5NHI3JTJURT65MRGLI" data-el="text" dir="null">These overly broad agreements violated long-standing federal laws and regulations meant to protect whistleblowers who wish to reveal damning information about their company anonymously and without fear of retaliation, the letter said.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="57HMR3A3IJG7ND4UJF2ZKP76AY" data-el="text" dir="null">“These contracts sent a message that ‘we don’t want … employees talking to federal regulators,’” said one of the whistleblowers, who spoke on the condition of anonymity for fear of retaliation. “I don’t think that AI companies can build technology that is safe and in the public interest if they shield themselves from scrutiny and dissent.”</p><div role="group" aria-roledescription="carousel" data-qa="article-body"><h2>GET CAUGHT UP<p>Stories to keep you informed</p></h2></div><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="QBGXLIXUFJGQFD7HASNP3JPDIY" data-el="text" dir="null">In a statement, Hannah Wong, a spokesperson for OpenAI said, “Our whistleblower policy protects employees’ rights to make protected disclosures. Additionally, we believe rigorous debate about this technology is essential and have already made important changes to our departure process to remove nondisparagement terms.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="MLHFUMOBBFEELIFOVC4BKMCAQY" data-el="text" dir="null">The whistleblowers’ letter comes amid concerns that OpenAI, which started as a<b> </b>nonprofit with an altruistic mission, is putting profit before safety in creating its technology. The Post <a href="https://www.washingtonpost.com/technology/2024/07/12/openai-ai-safety-regulation-gpt4/?itid=lk_inline_manual_13" target="_blank">reported Friday</a> that OpenAI rushed out its latest AI model that fuels ChatGPT to meet a May release date set by company leaders, despite employee concerns that the company “failed” to live up to its own security testing protocol that it said would keep its AI safe from catastrophic harms, like teaching users to build bioweapons or helping hackers develop new kinds of cyberattacks. In a statement, OpenAI spokesperson Lindsey Held said the company “didn’t cut corners on our safety process, though we recognize the launch was stressful for our teams.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="TH3SN445UVE7PC4L3TFGYHYC5U" data-el="text" dir="null">Tech companies’ strict confidentiality agreements have long vexed workers and regulators. During the #MeToo movement and national protests in response to the murder of George Floyd, <a href="https://www.washingtonpost.com/news/powerpost/paloma/the-technology-202/2020/07/02/the-technology-202-here-s-what-facebook-ad-boycott-organizers-plan-to-tell-mark-zuckerberg/5efcc6f388e0fa7b44f6c2fd/?itid=lk_inline_manual_14" target="_blank">workers warned</a> that such legal agreements limited their ability to report sexual misconduct or racial discrimination. Regulators, meanwhile, have worried that the terms muzzle tech employees who could alert them to misconduct in the opaque<b> </b>tech sector, especially amid allegations that companies’ algorithms promote content that undermines elections, public health and children’s safety.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="PBRT3ZCHC10B7ATVQ4TZ16V3MW" data-el="text" dir="null">The rapid advance of artificial intelligence sharpened<b> </b>policymakers’ concerns about the power of the tech industry, prompting a flood of calls for regulation. In the United States, AI companies are largely operating in a legal vacuum, and policymakers say they cannot effectively create new AI policies without the help of whistleblowers, who can help explain the potential threats posed by the fast-moving technology.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="1MJTVFK93N5C5ANZ20PUGG5TW8" data-el="text" dir="null">“OpenAI’s policies and practices appear to cast a chilling effect on whistleblowers’ right to speak up and receive due compensation for their protected disclosures,” said Sen. Chuck Grassley (R-Iowa) in a statement to The Post. “In order for the federal government to stay one step ahead of artificial intelligence, OpenAI’s nondisclosure agreements must change.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="652ZX455XZAFFCRJF54SSPIDIU" data-el="text" dir="null">A copy of the letter, addressed to SEC chairman Gary Gensler, was sent to Congress. The Post obtained the whistleblower letter from Grassley’s office.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="TNEC468CUD38VEBX5T95KAUFTW" data-el="text" dir="null">The official complaints referred to<b> </b>in the letter were submitted to the SEC in June. Stephen Kohn, a lawyer representing the OpenAI whistleblowers, said the SEC has responded to the complaint.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="7YWSZPAHU5ESVBEY4UCH3TVOAQ" data-el="text" dir="null">It could not be determined whether the SEC has launched an investigation. The agency declined to comment.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="25MGV72VVNFVPAU2QHSNMBBD7U" data-el="text" dir="null">The SEC must take “swift and aggressive” steps to address these illegal agreements, the letter says, as they might be relevant to the wider AI sector and could violate the October <a href="https://www.washingtonpost.com/technology/2023/10/30/biden-artificial-intelligence-executive-order/?itid=lk_inline_manual_24" target="_blank">White House executive order</a> that demands AI companies develop the technology safely.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="YF6YXKL6QJABXFSKJOMEJK2LIA" data-el="text" dir="null">“At the heart of any such enforcement effort is the recognition that insiders … must be free to report concerns to federal authorities,” the letter said. “Employees are in the best position to detect and warn against the types of dangers referenced in the Executive Order and are also in the best position to help ensure that AI benefits humanity, instead of having the opposite effect.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="VBLIBHJZ7RH67L564Q47DMO2TI" data-el="text" dir="null">These agreements threatened employees with criminal prosecutions if they reported violations of law to federal authorities under trade secret laws, Kohn said. Employees were instructed to keep company information confidential and threatened with “severe sanctions” without recognition of their right to report such information to the government, he said.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="MTDF47KSXFFCHGKVEQ6P24ZLXQ" data-el="text" dir="null">“In terms of oversight of AI, we are at the very beginning,” Kohn said. “We need employees to step forward, and we need OpenAI to be open.”</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="63IGWQ6QEBERXD7HMFWTBTM6BY" data-el="text" dir="null">The SEC should require OpenAI to produce every employment, severance and investor agreement that contains nondisclosure clauses to ensure they don’t violate federal laws, the letter said. Federal regulators should require OpenAI to notify all past and current employees of the violations the company committed as well as notify them that they have the right to confidentially and anonymously report any violations of law to the SEC. The SEC should issue fines to OpenAI for “each improper agreement” under SEC law and direct OpenAI to cure the “chilling effect” of its past practices, according to the whistleblowers letter.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="T4GVHMFCIJBUZH4UKMEZ5GO34Y" data-el="text" dir="null">Multiple tech employees, including <a href="https://www.washingtonpost.com/technology/2021/10/26/frances-haugen-facebook-whistleblower-documents/?itid=lk_inline_manual_32" target="_blank">Facebook whistleblower Frances Haugen</a>, have filed complaints with the SEC, which established a whistleblower program in the wake of the 2008 financial crisis.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="5GFZHILTJRHHNJUGOYTAWL5ZKE" data-el="text" dir="null">Fighting back against Silicon Valley’s use of NDAs to “monopolize information” has been a protracted battle, said Chris Baker, a San Francisco lawyer. He won a $27 million settlement for Google employees in December against claims that the tech giant used onerous confidentiality agreements to block whistleblowing and other protected activity. Now tech companies are increasingly fighting back with clever ways to deter speech, he said.</p><p data-testid="drop-cap-letter" data-apitype="text" data-contentid="VQNXXU4DJ97DQ4N4YTY49QZ82C" data-el="text" dir="null">“Employers have learned that the cost of leaks is sometimes way greater than the cost of litigation, so they are willing to take the risk,” Baker said.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[For advertising: Firefox now collects user data by default (550 pts)]]></title>
            <link>https://www.heise.de/en/news/For-advertising-Firefox-now-collects-user-data-by-default-9801345.html</link>
            <guid>40974112</guid>
            <pubDate>Tue, 16 Jul 2024 06:41:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.heise.de/en/news/For-advertising-Firefox-now-collects-user-data-by-default-9801345.html">https://www.heise.de/en/news/For-advertising-Firefox-now-collects-user-data-by-default-9801345.html</a>, See on <a href="https://news.ycombinator.com/item?id=40974112">Hacker News</a></p>
Couldn't get https://www.heise.de/en/news/For-advertising-Firefox-now-collects-user-data-by-default-9801345.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Decoding DME aircraft radio navigation system with the LimeSDR (104 pts)]]></title>
            <link>https://destevez.net/2024/07/recording-dme-with-the-limesdr/</link>
            <guid>40974060</guid>
            <pubDate>Tue, 16 Jul 2024 06:31:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://destevez.net/2024/07/recording-dme-with-the-limesdr/">https://destevez.net/2024/07/recording-dme-with-the-limesdr/</a>, See on <a href="https://news.ycombinator.com/item?id=40974060">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-18438">

	<!-- .entry-header -->

	<div>
		
<p><a href="https://en.wikipedia.org/wiki/Distance_measuring_equipment">DME</a> (distance measuring equipment) is an aircraft radio navigation system that is used to measure the distance between an aircraft and a DME station on ground. DME is often colocated with a <a href="https://en.wikipedia.org/wiki/VHF_omnidirectional_range">VOR</a> station, in which case the VOR provides the bearing information. DME works by measuring the two-way time of flight of pulse pairs, which are first transmitted by the aircraft, then retransmitted with a fixed delay by the ground station, which acts as a transponder, and finally received back by the aircraft. DME operates between 960 and 1215 MHz. It is channelized in steps of 1 MHz, and the air-to-ground and ground-to-air frequencies always differ by 63 MHz (<a href="https://www.flightsim-corner.com/wp-content/uploads/VOR-Frequencies-to-TACAN-Channel-list.pdf">here</a> is a list of all the frequency channels).</p>



<p>I want to write a post explaining in detail how DME works by analysing a recording of DME that contains both the air-to-ground and the ground-to-air channels. Among other things, I want to show that the delay between the aircraft and ground station pulses matches the one calculated using the aircraft position (which I can get from ADS-B data on the internet), the ground station position, the position of the recorder, and the fixed delay applied by the ground station transponder.</p>



<p>Recording two channels 63 MHz apart is tricky with the kind of SDRs I have. Devices based on the AD9361 technically support a maximum sample rate of 61.44 Msps (although some people are <a href="https://destevez.net/2023/02/running-the-ad9361-at-122-88-msps/">running it at up to 122.88 Msps</a>). The <a href="https://limemicro.com/technology/lms7002m/">LMS7002M</a>, which is used by the <a href="https://limemicro.com/products/boards/limesdr/">LimeSDR</a> and other SDRs, is an interesting alternative, for two reasons. First, it supports more than 61.44 Msps. However, it isn’t clear what is the maximum sample rate supported by the LimeSDR. Some sources, including the <a href="https://limemicro.com/products/boards/limesdr/">LimeSDR webpage</a> mention 61.44 MHz bandwidth, but the <a href="https://limemicro.com/app/uploads/2017/07/LMS7002M-Data-Sheet-v3.1r00.pdf">LMS7002M datasheet</a> says that the maximum RF modulation bandwidth (whatever that means) through the digital interface in SISO mode is 96 MHz. In the case of the LimeSDR there is also the limitation of the USB3 data rate, but this should not be a problem if we use only 1 RX channel. I haven’t found clear information about the limitations of each of the components of the LMS7002M (ADC max sample rate, etc.).</p>



<p>The second interesting feature is that the LMS7002M has a DDC on the chip. The AD9361 has a series of decimating filters to reduce the ADC sample rate and deliver a lower sample rate through the digital interface. The LMS7002M, in addition to this, has an NCO and digital mixer that can be be used to apply a frequency shift to the ADC IQ signal before decimation.</p>



<p>I had two different ideas about how to use the LimeSDR to record the two DME channels. The first idea consisted in using a 70 Msps output sample rate. For this I used an ADC sample rate of 140 Msps, because I think it is necessary to have at least decimation by 2 after the ADC (the LMS7200M documentation does not explain this clearly, so figuring out how to use the chip often involves some trial and error using <code>LimeSuiteGUI</code>). This idea had two problems. The first problem is that some CGEN PLL occasionally failed to lock when using an ADC sample rate of 140 Msps. However the LimeSuite driver retried multiple times until the PLL locked, so in practice this wasn’t a problem. This approach worked well on my desktop PC, since in 70 Msps I had the two DME channels and then I could use GNU Radio to extract each of the two channels (for instance with the <a href="https://wiki.gnuradio.org/index.php/Frequency_Xlating_FIR_Filter">Frequency Xlating FIR Filter</a>). However, the laptop I planned to use to record on the field couldn’t keep up with 70 Msps.</p>



<p>The second idea was to use the on-chip DDC in the LMS7200M to extract the DME channel and deliver a much lower sample rate over the digital interface. The figure below shows how the LMS7200M digital signal processing datapath works. This datapath is called RXTSP. The RXI and RXQ signals are the digital signals coming from the ADC (here and below, by ADC I mean a dual-channel ADC, since the LMS7002M is a zero-if IQ transceiver). The RYI and RYQ are the signals delivered to the digital interface of the chip. Since the LMS7200M has two RX channels, there are two identical chains, one for each channel. The parameters of each chain can be programmed completely independently.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/rxtsp.png"><img fetchpriority="high" decoding="async" width="1368" height="294" src="https://destevez.net/wp-content/uploads/2024/07/rxtsp.png" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/rxtsp.png 1368w, https://destevez.net/wp-content/uploads/2024/07/rxtsp-300x64.png 300w, https://destevez.net/wp-content/uploads/2024/07/rxtsp-644x138.png 644w, https://destevez.net/wp-content/uploads/2024/07/rxtsp-768x165.png 768w" sizes="(max-width: 1368px) 100vw, 1368px"></a><figcaption>LMS7200M digital signal processing, extracted from the <a href="https://limemicro.com/app/uploads/2017/07/LMS7002M-Data-Sheet-v3.1r00.pdf">datasheet</a></figcaption></figure>



<p>There is no way of sending the signal of one ADC to the two RXTSPs. The connection between each ADC and its corresponding RXTSP is fixed. Therefore, we need to feed in the antenna signal through the two RX channels, but we can easily do this with an external splitter. Remember that both of the LMS7200M RX channels share the same LO, as illustrated by the block diagram below. So the point here is to tune the LO to a frequency between the two DME channels, set the sample rate high enough that both DME channels are present in the ADC output, and finally to use each of the two RXTSPs to extract one of the DME channels, sending it at a low sample rate through the digital interface.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/lms7200m_block.png"><img decoding="async" width="649" height="792" src="https://destevez.net/wp-content/uploads/2024/07/lms7200m_block.png" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/lms7200m_block.png 649w, https://destevez.net/wp-content/uploads/2024/07/lms7200m_block-246x300.png 246w, https://destevez.net/wp-content/uploads/2024/07/lms7200m_block-644x786.png 644w" sizes="(max-width: 649px) 100vw, 649px"></a><figcaption>LMS7200M block diagram, extracted from the <a href="https://limemicro.com/app/uploads/2017/07/LMS7002M-Data-Sheet-v3.1r00.pdf">datasheet</a></figcaption></figure>



<p>This approach has worked quite well. I have set the ADC to 80 Msps and used the RXTSPs to dowconvert and decimate the DME channels to 2.5 Msps, recording that data directly in GNU Radio.</p>



<p>I have done a two hour recording of DME and published it in the Zenodo dataset <a href="https://zenodo.org/records/12743623">Recording of Colmenar (CNR) VOR-DME air-to-ground and ground-to-air DME channels</a>.</p>



<p>In the rest of this post I explain the details of the recording set up and do a preliminary analysis of the recording quality.</p>



<h4>VOR-DME and recording location</h4>



<p>The VOR-DME station used in this recording is the <a href="https://ourairports.com/navaids/CNR/Colmenar_VOR-DME_ES/">Colmenar (CNR) VOR-DME</a>, which is the one closest to where I live. The VOR has a frequency of 117.3 MHz. The DME frequency is related to the VOR frequency, so according to <a href="https://www.flightsim-corner.com/wp-content/uploads/VOR-Frequencies-to-TACAN-Channel-list.pdf">the list</a>, it corresponds to TACAN channel 120X, which has an aircraft-to-ground frequency of 1144 MHz and a ground-to-aircraft frequency of 1207 MHz (DME is, in a sense, a civilian version of the military <a href="https://en.wikipedia.org/wiki/Tactical_air_navigation_system">TACAN</a> radio navigation system, which provides both range and bearing information).</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/cnr-vor.png"><img decoding="async" width="644" height="646" src="https://destevez.net/wp-content/uploads/2024/07/cnr-vor-644x646.png" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/cnr-vor-644x646.png 644w, https://destevez.net/wp-content/uploads/2024/07/cnr-vor-300x300.png 300w, https://destevez.net/wp-content/uploads/2024/07/cnr-vor-150x150.png 150w, https://destevez.net/wp-content/uploads/2024/07/cnr-vor-768x770.png 768w, https://destevez.net/wp-content/uploads/2024/07/cnr-vor.png 818w" sizes="(max-width: 644px) 100vw, 644px"></a><figcaption>CNR VOR and Madrid area in the <a href="https://skyvector.com/">Skyvector</a> world VFR map</figcaption></figure>



<p>According to the <a href="https://skyvector.com/">Skyvector</a> world high and world low IFR maps, the CNR VOR is not part of any airway. However, it is part of several instrument procedures of the Madrid Barajas airport. An aircraft only transmits to a DME station if that DME or VOR-DME station is selected in one of the aircraft navigation radios (most aircraft have two radios for VOR-DME). The pilot will usually tune the radios to the stations that are part of the procedure that the aircraft is flying (although the pilot is free to tune to other stations as a cross check), so the kind of aircraft that we expect to see in the recording are those operating on the Madrid Barajas airport, not those flying high en route.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/world_high.png"><img decoding="async" width="644" height="526" src="https://destevez.net/wp-content/uploads/2024/07/world_high-644x526.png" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/world_high-644x526.png 644w, https://destevez.net/wp-content/uploads/2024/07/world_high-300x245.png 300w, https://destevez.net/wp-content/uploads/2024/07/world_high-768x628.png 768w, https://destevez.net/wp-content/uploads/2024/07/world_high.png 1077w" sizes="(max-width: 644px) 100vw, 644px"></a><figcaption>CNR VOR in the <a href="https://skyvector.com/">Skyvector</a> world high map</figcaption></figure>



<p>In particular, aircraft departing from Madrid Barajas runway 36R and flying due west or southwest will fly over CNR.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/sid_36R.png"><img decoding="async" width="644" height="657" src="https://destevez.net/wp-content/uploads/2024/07/sid_36R-644x657.png" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/sid_36R-644x657.png 644w, https://destevez.net/wp-content/uploads/2024/07/sid_36R-294x300.png 294w, https://destevez.net/wp-content/uploads/2024/07/sid_36R-768x784.png 768w, https://destevez.net/wp-content/uploads/2024/07/sid_36R.png 825w" sizes="(max-width: 644px) 100vw, 644px"></a><figcaption>SID for LEMD RWY 36R using CNR VOR, taken from the <a href="https://aip.enaire.es/aip/contenido_AIP/AD/AD2/LEMD/LE_AD_2_LEMD_SID_7_en.pdf">SID chart</a></figcaption></figure>



<p>However, departures from runway 36L do not use the CNR VOR at all (see the <a href="https://aip.enaire.es/aip/contenido_AIP/AD/AD2/LEMD/LE_AD_2_LEMD_SID_5_en.pdf">SID chart</a>). I’m quite impressed at how much the instrument procedures for Madrid Barajas have changed since I did my <a href="https://destevez.net/2018/08/aircraft-reflections-of-a-2-3ghz-beacon/">2.3 GHz aircraft reflections recording</a> in 2018. This is due to the increasing prevalence of GPS navigation.</p>



<p>The location I have chosen to do the recording is in the countryside just outside my town. It is 4.58 km southeast of the CNR VOR-DME. There is almost line of sight to the VOR-DME station, but there are some buildings in between. This recording location also gives good geometry for TDOA (time difference of arrival) analysis for aircraft travelling along the path shown above. As they depart from Madrid Barajas, they are closer to my recorder than to the VOR-DME, but at some point they start being closer to the VOR-DME than to my recorder.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/recording_map.png"><img decoding="async" width="644" height="651" src="https://destevez.net/wp-content/uploads/2024/07/recording_map-644x651.png" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/recording_map-644x651.png 644w, https://destevez.net/wp-content/uploads/2024/07/recording_map-297x300.png 297w, https://destevez.net/wp-content/uploads/2024/07/recording_map-768x776.png 768w, https://destevez.net/wp-content/uploads/2024/07/recording_map.png 888w" sizes="(max-width: 644px) 100vw, 644px"></a><figcaption>Recording location (red pin) and distance to CNR VOR-DME</figcaption></figure>



<p>The <a href="https://www.heywhatsthat.com/">HeyWhatsThat</a> <a href="https://www.heywhatsthat.com/profiler.html?show_grade=0&amp;show_rise=0&amp;include_grade=1&amp;decimal_places=0&amp;elev_source=0&amp;ll1=40.620582,-3.69318&amp;ll2=40.645972,-3.73583">profile</a> shows that there are no significant terrain obstructions.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/profile-0904.png"><img decoding="async" width="644" height="129" src="https://destevez.net/wp-content/uploads/2024/07/profile-0904-644x129.png" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/profile-0904-644x129.png 644w, https://destevez.net/wp-content/uploads/2024/07/profile-0904-300x60.png 300w, https://destevez.net/wp-content/uploads/2024/07/profile-0904-768x154.png 768w, https://destevez.net/wp-content/uploads/2024/07/profile-0904.png 800w" sizes="(max-width: 644px) 100vw, 644px"></a><figcaption>HeyWhatsThat profile from the recording location to the CNR VOR-DME</figcaption></figure>



<h4>Antenna</h4>



<p>I have made an antenna from scratch for this recording. I have chosen a quarter-wave groundplane monopole because it’s easy to make and gives a roughly omnidirectional pattern, except for a null in the zenith (which is useful, because I’ll be recording both the ground signal and signals from aircraft at high elevation angles). I made the antenna from a spare SMA female connector and some thick enamelled copper wire.</p>



<p>The following photo shows the antenna and the LNA, with a ruler in cm for scale.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/dme_antenna.jpg"><img decoding="async" width="644" height="884" src="https://destevez.net/wp-content/uploads/2024/07/dme_antenna-644x884.jpg" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/dme_antenna-644x884.jpg 644w, https://destevez.net/wp-content/uploads/2024/07/dme_antenna-218x300.jpg 218w, https://destevez.net/wp-content/uploads/2024/07/dme_antenna-768x1055.jpg 768w, https://destevez.net/wp-content/uploads/2024/07/dme_antenna.jpg 932w" sizes="(max-width: 644px) 100vw, 644px"></a><figcaption>Antenna and LNA</figcaption></figure>



<p>The antenna is resonant somewhere around 1.1 to 1.2 GHz, depending on the presence of nearby objects. It provides a good impedance match to 50Ω in all the DME band.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/dme_antenna_vna.png"><img decoding="async" width="1920" height="1063" src="https://destevez.net/wp-content/uploads/2024/07/dme_antenna_vna.png" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/dme_antenna_vna.png 1920w, https://destevez.net/wp-content/uploads/2024/07/dme_antenna_vna-300x166.png 300w, https://destevez.net/wp-content/uploads/2024/07/dme_antenna_vna-644x357.png 644w, https://destevez.net/wp-content/uploads/2024/07/dme_antenna_vna-768x425.png 768w, https://destevez.net/wp-content/uploads/2024/07/dme_antenna_vna-1536x850.png 1536w, https://destevez.net/wp-content/uploads/2024/07/dme_antenna_vna-1568x868.png 1568w" sizes="(max-width: 1920px) 100vw, 1920px"></a><figcaption>Antenna S11 measured with NanoVNA V2</figcaption></figure>



<h4>LNA</h4>



<p>Since the antenna will be connected to the two RX channels of the LimeSDR through a splitter, I have used an LNA to overcome the splitter losses. The LNA is a <a href="https://www.minicircuits.com/WebStore/dashboard.html?model=GALI-39%2B">GALI-39</a> <a href="https://www.minikits.com.au/Gali-39-Amplifier">board from Minikits</a> that I had around. The GALI-39 is a wideband LNA MMIC from MiniCircuits that has a gain of around 20 dB and a noise figure of around 2.4 dB. I talked about this LNA board kit in <a href="https://destevez.net/2016/02/building-the-gali-39-amplifier-kit-from-minikits/">an old post</a>. Note that I built the biasing network for the MMIC using the option that covers from 500 kHz to 850 MHz, but this should still be fine at 1.2 GHz.</p>



<h4>Splitter</h4>



<p>To split the antenna signal into the two LimeSDR RX channels, I’m using a cheap Wilkinson splitter board that I originally got to feed two GPS receivers from the same antenna. I installed a DC blocking capacitor in one of the two ports, since GPS receivers often provide a DC bias for an active antenna.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/splitter.jpg"><img decoding="async" width="644" height="508" src="https://destevez.net/wp-content/uploads/2024/07/splitter-644x508.jpg" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/splitter-644x508.jpg 644w, https://destevez.net/wp-content/uploads/2024/07/splitter-300x237.jpg 300w, https://destevez.net/wp-content/uploads/2024/07/splitter-768x606.jpg 768w, https://destevez.net/wp-content/uploads/2024/07/splitter.jpg 1200w" sizes="(max-width: 644px) 100vw, 644px"></a><figcaption>Wilkinson splitter</figcaption></figure>



<p>The figure below shows a measurement of the splitter in the VNA, with the third port terminated with a load. It performs quite well in the DME band, but even though I think it was marketed for GPS, it doesn’t perform so well at 1575.42 MHz (the GPS L1 frequency).</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/splitter_vna.png"><img decoding="async" width="644" height="357" src="https://destevez.net/wp-content/uploads/2024/07/splitter_vna-644x357.png" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/splitter_vna-644x357.png 644w, https://destevez.net/wp-content/uploads/2024/07/splitter_vna-300x166.png 300w, https://destevez.net/wp-content/uploads/2024/07/splitter_vna-768x425.png 768w, https://destevez.net/wp-content/uploads/2024/07/splitter_vna-1536x850.png 1536w, https://destevez.net/wp-content/uploads/2024/07/splitter_vna-1568x868.png 1568w, https://destevez.net/wp-content/uploads/2024/07/splitter_vna.png 1920w" sizes="(max-width: 644px) 100vw, 644px"></a><figcaption>Splitter S11 and S21 measured with the NanoVNA V2 (third port terminated)</figcaption></figure>



<h4>Complete hardware set up</h4>



<p>The antenna is connected through the LNA and the splitter to the two RX channels of the LimeSDR. I placed everything on a camera tripod on the roof of my car and used a LiPo battery to power the LNA. The following photo shows how everything looked during the recording.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/dme_receiver.jpg"><img decoding="async" width="644" height="860" src="https://destevez.net/wp-content/uploads/2024/07/dme_receiver-644x860.jpg" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/dme_receiver-644x860.jpg 644w, https://destevez.net/wp-content/uploads/2024/07/dme_receiver-225x300.jpg 225w, https://destevez.net/wp-content/uploads/2024/07/dme_receiver-768x1026.jpg 768w, https://destevez.net/wp-content/uploads/2024/07/dme_receiver-1150x1536.jpg 1150w, https://destevez.net/wp-content/uploads/2024/07/dme_receiver.jpg 1437w" sizes="(max-width: 644px) 100vw, 644px"></a><figcaption>DME receiver hardware set up</figcaption></figure>



<p>This shows the view towards the CNR VOR-DME station. The buildings are in the line of sight towards the VOR-DME, but there are no terrain obstructions behind them.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/dme_receiver2.jpg"><img decoding="async" width="644" height="860" src="https://destevez.net/wp-content/uploads/2024/07/dme_receiver2-644x860.jpg" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/dme_receiver2-644x860.jpg 644w, https://destevez.net/wp-content/uploads/2024/07/dme_receiver2-225x300.jpg 225w, https://destevez.net/wp-content/uploads/2024/07/dme_receiver2-768x1026.jpg 768w, https://destevez.net/wp-content/uploads/2024/07/dme_receiver2-1150x1536.jpg 1150w, https://destevez.net/wp-content/uploads/2024/07/dme_receiver2.jpg 1437w" sizes="(max-width: 644px) 100vw, 644px"></a><figcaption>Hardware set up looking towards the CNR VOR-DME station</figcaption></figure>



<h4>LimeSDR configuration</h4>



<p>I have used LimeSuiteGUI to prepare the LimeSDR configuration. This configuration can then be loaded in GNU Radio using <a href="https://github.com/myriadrf/gr-limesdr">gr-limesdr</a>. The configuration file can be found <a href="https://github.com/daniestevez/jupyter_notebooks/blob/master/DME/limesdr_config.ini">here</a>.</p>



<p>I have found that it is somewhat tricky to set up from scratch a configuration in LimeSuiteGUI that uses decimation in the RXTSP. The HBD ratio in the RxTSP tab (which controls the decimation applied in the RXTSP) and the Rx clock divider in the LimeLight &amp; PAD tab (which controls when the digital interface outputs a sample) need to be set to matching values. Otherwise samples are duplicated or missing in the output. However, it turns out that for a HBD ratio of 2^5 (which is what I’m using to reduce the 80 Msps ADC sample rate to 2.5 Msps), the Rx clock divider needs to be set to 7. The way I’ve figured this out is by configuring the LimeSDR to 2.5 Msps in GQRX (which automatically sets a decimation of 2^5) and then stopping GQRX and examining the configuration in LimeSuiteGUI. This works because GQRX doesn’t clear the configuration when it stops.</p>



<p>The important points to configure in LimeSuiteGUI are the following:</p>



<ul>
<li>In the CLKGEN tab, the CLK_H is set to 320 MHz. Together with a decimation of 4 this produces an RXTSP and TXTSP frequency of 80 MHz.</li>



<li>In the SXR tab, the RX LO frequency is set to 1175 MHz, which is the midpoint of the aircarft-to-ground and ground-to-aircraft frequencies of this DME channel.</li>



<li>In RxTSP tab, the HBD ratio is set to 2^5, and the NCO is set to 31.5 MHz. Channel A is set to upconvert (which shifts the frequency up), and channel B is set to downconvert (which shifts the frequency down)</li>



<li>In the RBB tab, the RX filter bandwidth is tuned to 70 MHz. Experimentally it seems that this bandwidth refers to the full IQ bandwidth, so the lowpass filter cut-off is at 35 MHz.</li>



<li>The gains are adjusted in the field for appropriate signal levels. I have ended up using a PGA gain of 6 dB (RBB tab), an LNA gain of Gmax-12 dB, and a TIA gain of Gmax-3 dB (RFE tab).</li>



<li>RX IQ imbalance calibration is done in the field.</li>
</ul>



<p>Note that, except for the CLKGEN and SXR setttings, all these settings need to be applied independently to each of the two channels.</p>



<p>In hindsight, the decision to set the LO frequency exactly at the midpoint between the air-to-ground and ground-to-air channels might have not been the best. Since this causes the two channels to be at symmetric frequencies in baseband, any IQ imbalance will cause some power from the signals from one channel to appear in the other. Offsetting the LO frequency by a few MHz would prevent this. However, this doesn’t seem to be a problem in practice. An advantage of setting the LO frequency in the midpoint is that the two channels then have a more similar response in the analog baseband, since they have the same analog baseband frequency.</p>



<h4>GNU Radio flowgraph</h4>



<p>A <a href="https://github.com/daniestevez/jupyter_notebooks/blob/master/DME/dme_limesdr_record.grc">GNU Radio flowgraph</a> is used to load the LimeSDR configuration and record data to disk.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/dme_limesdr_record.png"><img decoding="async" width="644" height="262" src="https://destevez.net/wp-content/uploads/2024/07/dme_limesdr_record-644x262.png" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/dme_limesdr_record-644x262.png 644w, https://destevez.net/wp-content/uploads/2024/07/dme_limesdr_record-300x122.png 300w, https://destevez.net/wp-content/uploads/2024/07/dme_limesdr_record-768x313.png 768w, https://destevez.net/wp-content/uploads/2024/07/dme_limesdr_record.png 992w" sizes="(max-width: 644px) 100vw, 644px"></a><figcaption>GNU Radio flowgraph to record DME aircraft-to-ground and ground-to-aircraft channels</figcaption></figure>



<p>The output of the LimeSDR has a small DC spike even though we are using the NCO (so it is not the ADC DC spike). I think the reason is poor rounding in the digital signal chain. Adding <code>1/2**12</code> to each channel fixes this. This is why the Add Const blocks are present.</p>



<p>The Tag Debug block is used to detect lost samples, since the LimeSDR Source emits an <code>rx_time</code> each time that samples are lost.</p>



<p>The LimeSDR configuration file is loaded in the LimeSDR Source block in the Advanced tab, File option. Since we are using a configuration file, most of the other parameters in this block do not matter, as they are overwritten by the configuration file.</p>



<p>Since the LimeSDR doesn’t have a nice timestamp synchronization API as UHD does, timestamping of the files is done by writing the time at which the flowgraph starts running as part of the filename and also by comparing this to the modification timestamp of the file obtained using <code>stat</code> once the recording has stopped (this timestamp corresponds to the end of the file). This method is accurate to about 1 second, which is good enough to compare the recorded data with ADS-B data.</p>



<h4>Recording preliminary analysis</h4>



<p>I have performed a preliminary analysis of the recording in <a href="https://github.com/daniestevez/jupyter_notebooks/blob/master/DME/DME%20recording%20quick%20analysis.ipynb">a Jupyter notebook</a> to validate the recording quality. A more detailed analysis will be done in a future post.</p>



<p>The following figure shows the signal amplitude of the ground-to-air and air-to-ground channels in the first 100 ms of the recording. Each of the spikes we see is a pulse pair. Instead of single pulses, DME uses pulse pairs with a well determined separation between the two pulses so that the receiver can discriminate against pulsed interference.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/dme_amplitude.png"><img decoding="async" width="622" height="353" src="https://destevez.net/wp-content/uploads/2024/07/dme_amplitude.png" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/dme_amplitude.png 622w, https://destevez.net/wp-content/uploads/2024/07/dme_amplitude-300x170.png 300w" sizes="(max-width: 622px) 100vw, 622px"></a></figure>



<p>There are many more pulse pairs in the ground-to-air channel. The reason is something known as <a href="https://en.wikipedia.org/wiki/Squitter">squitter</a>. The DME transponder on ground has an AGC system that adjusts the detection threshold so that on average the transponder outputs 2700 pulse pairs per second. If there are not enough pulses received from aircraft, the transponder will trigger on random noise to generate the required amount of pulse pairs. This has several goals. It provides a constant output power to the transponder, it automatically sets the transponder threshold to a value that optimizes the sensitivity, and it degrades gracefully if the number of aircraft increases. If there are so many aircraft that the transponder receives more than 2700 pulse pairs per second, the threshold will be set high enough that the weaker aircraft are ignored. This kind of AGC system is also relatively simple to implement in hardware, so it is a very clever solution.</p>



<p>In this recording there are few aircraft using the DME. Each aircraft transmits around 150 pulse pairs per second when in search mode and less than 30 pulse pairs per second when in track mode, so most of the pulse pairs we see in the ground-to-air channel are squitter.</p>



<p>The following shows the spectrum of the first 100 seconds of the recording, using an FFT size of 4096 and a rectangular window. Both an average and a peak hold spectrum are shown for each channel. Due to the low duty cycle of the pulses (specially for the air-to-ground channel), the peak hold is often better than the average to detect the DME signal.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/dme_spectrum.png"><img decoding="async" width="621" height="353" src="https://destevez.net/wp-content/uploads/2024/07/dme_spectrum.png" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/dme_spectrum.png 621w, https://destevez.net/wp-content/uploads/2024/07/dme_spectrum-300x171.png 300w" sizes="(max-width: 621px) 100vw, 621px"></a></figure>



<p>The following two figures show a pulse pair in the ground-to-air channel and in the air-to-ground channel. In both cases, I have plotted the pulse with largest amplitude in the first 100 seconds of the recording. The amplitude of the pulse is around 0.3 with respect to ADC full scale (or rather with respect to the full scale of the LMS7200M digital interface), so the ADC is probably still far from saturation.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/dme_ground_pulse.png"><img decoding="async" width="625" height="353" src="https://destevez.net/wp-content/uploads/2024/07/dme_ground_pulse.png" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/dme_ground_pulse.png 625w, https://destevez.net/wp-content/uploads/2024/07/dme_ground_pulse-300x169.png 300w" sizes="(max-width: 625px) 100vw, 625px"></a></figure>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/dme_air_pulse.png"><img decoding="async" width="625" height="353" src="https://destevez.net/wp-content/uploads/2024/07/dme_air_pulse.png" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/dme_air_pulse.png 625w, https://destevez.net/wp-content/uploads/2024/07/dme_air_pulse-300x169.png 300w" sizes="(max-width: 625px) 100vw, 625px"></a></figure>



<p>There are two DME modes: mode X and mode Y. Each mode has different parameters for the pulse pair timing. The two modes are mainly a way to support twice as many channels, since a given frequency can be used with mode X or with mode Y (although whether a given frequency is air-to-ground or ground-to-air also changes with the mode). This DME is mode X (recall that it is TACAN channel 120X), so as indicated in <a href="https://www.airnav.eu/index.php?stranka=DMEen">this webpage</a>, the distance between the two pulses in the pair is 12 usec both for the air-to-ground and ground-to-air channels (mode Y uses different distances for air-to-ground and ground-to-air) and the transponder retransmits the pulses with a delay of 50 usec.</p>



<p>The shape of the pulses is somewhat ugly. The SNR is very high, so this is caused by signal distortion, not noise. In the ground-to-air channel perhaps this distortion is multipath, although in the air-to-ground pulse there seems to be an echo with a delay of ~5 usec. This corresponds to 1.5 km. I can’t think how this receiver set up could have 1.5 km of multipath for pulses coming from aircraft, so maybe the distortion is generated in the receiver hardware. Or maybe the aircraft signal is reflected in a building. Who knows.</p>



<p>The next plot shows the same air-to-ground pulse pair, and its corresponding reply on the ground-to-air channel. The reply is received with a delay of approximately 70 usec. The transponder has a delay of 50 usec, and since my receiver is 4.58 km away from the VOR-DME station, the transponder signal takes another 15 usec to reach the receiver. Therefore, this means that the aircraft signal arrived to my receiver 5 usec before it arrived to the VOR-DME station. If we can identify which aircraft transmitted this pulse, we should be able to check that it was 1.5 km closer to my station than to the VOR-DME in this moment. This will be done in the next post.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/dme_ground_and_air_pulse.png"><img decoding="async" width="669" height="353" src="https://destevez.net/wp-content/uploads/2024/07/dme_ground_and_air_pulse.png" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/dme_ground_and_air_pulse.png 669w, https://destevez.net/wp-content/uploads/2024/07/dme_ground_and_air_pulse-300x158.png 300w, https://destevez.net/wp-content/uploads/2024/07/dme_ground_and_air_pulse-644x340.png 644w" sizes="(max-width: 669px) 100vw, 669px"></a></figure>



<p>Another interesting feature of the DME ground station is that it periodically sends an identification in Morse code. The way in which this is done is quite ingenious. To transmit a Morse dash or dot, the transponder stops listening to aircraft transmissions and replaces the squitter by a train of 1350 equally spaced pulse pairs per second. This is shown in a plot here. The transition from the squitter to the regular train of pulses is apparent.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2024/07/dme_cw_id.png"><img decoding="async" width="622" height="353" src="https://destevez.net/wp-content/uploads/2024/07/dme_cw_id.png" alt="" srcset="https://destevez.net/wp-content/uploads/2024/07/dme_cw_id.png 622w, https://destevez.net/wp-content/uploads/2024/07/dme_cw_id-300x170.png 300w" sizes="(max-width: 622px) 100vw, 622px"></a></figure>



<p>When a DME signal is received with an AM receiver such as the ones that are used for aircraft communications, even though the bandwidth of a typical AM receiver is 10 kHz and the bandwidth of the DME signal is 1 MHz, the DME signal causes a distinct sound in the AM receiver audio output. Each DME pulse pair causes a single pulse in the AM receiver audio output. This single pulse is longer, around 100 usec long, due to the reduced bandwidth of the AM receiver. The train of regularly spaced pulses sounds quite similar to a 1350 Hz rectangular wave. The value of 1350 Hz is chosen to be distinct from the tone used by VOR for its Morse ID, which is 1020 Hz.</p>



<p>The audio clip below shows a few seconds of DME squittter and then the Morse ID. The Morse 1350 Hz tone can be heard quite naturally. Observe that there is squitter between the Morse dots and dashes. Also note that the squitter sounds quite different from AWGN, so an AM receiver can be used to detect the presence of a DME ground-to-air signal and even to get a rough idea of its SNR, since as the signal becomes weaker, more and more AWGN will be heard in the AM receiver. This audio file was obtained with <a href="https://github.com/daniestevez/jupyter_notebooks/blob/master/DME/dme_am_demod.grc">this GNU Radio flowgraph</a>.<br></p>



<figure><audio controls="" src="https://destevez.net/wp-content/uploads/2024/07/dme_am_demod.wav"></audio><figcaption>DME Morse code ID (demodulated as AM with 10 kHz bandwidth)</figcaption></figure>



<p>During the Morse identification, aircraft don’t receive valid response pulses from the DME transponder, so the DME receiver on the aircraft stops updating its distance measurement until valid transponder replies are received again.</p>



<h4>Code and data</h4>



<p>The GNU Radio flowgraphs and the Jupyter notebook used in this post are in <a href="https://github.com/daniestevez/jupyter_notebooks/tree/master/DME">this repository</a>. The recording is published as a <a href="https://zenodo.org/records/12743623">dataset in Zenodo</a>.</p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

				
</article></div>]]></description>
        </item>
    </channel>
</rss>