<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 15 Dec 2023 16:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[How Lego builds a new Lego set (166 pts)]]></title>
            <link>https://www.theverge.com/c/23991049/lego-ideas-polaroid-onestep-behind-the-scenes-price</link>
            <guid>38653456</guid>
            <pubDate>Fri, 15 Dec 2023 12:25:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/c/23991049/lego-ideas-polaroid-onestep-behind-the-scenes-price">https://www.theverge.com/c/23991049/lego-ideas-polaroid-onestep-behind-the-scenes-price</a>, See on <a href="https://news.ycombinator.com/item?id=38653456">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
		
			<div>
					<p>
		Marc Corfmat was a teenager when he began to compete for Lego’s ultimate prize: the chance to design an official set. He and his brother Nick had been building custom Lego creations ever since they were kids, sometimes in California, sometimes during vacations at their grandparents’ home in La Rochelle, France. They shared their models <a href="https://www.youtube.com/c/MiniBrickProductions/videos">on YouTube</a> and posted their creations to Lego’s website, but interest from the Lego world came slowly, if it came at all.
</p>


					<p>
		Then, in 2020, the brothers started having some luck. <a href="https://ideas.lego.com/">The Lego Ideas program</a> gives fans the chance to turn their designs into reality, offering both fame and a small fortune —&nbsp;1 percent of net sales — to anyone who can convince 10,000 peers and The Lego Group that their set deserves to exist. After three years and 18 submissions, Marc finally cleared the 10,000-vote hurdle with a design based on <a href="https://ideas.lego.com/projects/d0772ba6-298a-44df-ba84-f26faa8d7216"><em>Avatar: The Last Airbender</em></a>. A month later, <a href="https://ideas.lego.com/projects/f4dead4b-7900-451a-9cfe-96c4ab3e756c">his <em>Tintin</em> idea</a> was chosen as a staff pick. <a href="https://ideas.lego.com/projects/0b9f43ac-67c0-4108-9e82-a44d6574b8ab/official_comments#content_nav_tabs">Another design</a> based on <em>The Polar Express </em>hit 10,000 votes the next year.
</p>


					<p>
		And then… nothing. The <em>Tintin</em> votes dried up, and Lego rejected both his fan-favorite <em>Avatar</em> and <em>Polar Express </em>ideas<em>. </em>The company never says why it rejects an Ideas submission, only that deciding factors include everything from “playability” and “brand fit” to the difficulties in licensing another company’s IP.
</p>


					<p>
		“We knew it was almost impossible to get products on the shelves. You see maybe a few selected a year out of thousands of submissions — but even that slight glimmer of hope was enough to really keep us going,” says Marc, now a graduate student in mechanical engineering at the University of California, Davis.
</p>


					<p>
		Then, he decided to try an idea that had been noodling about his brain: a Polaroid, like one of the instant cameras his sister Mia liked using. Marc wasn’t a Polaroid devotee himself, but he’d liked the iconic look of the original 1977 Polaroid OneStep. The rainbow stripe camera had lived on his internal mood board for “quite some time,” but when he saw that a 2020 Lego <em>Minions</em> set had introduced <a href="https://www.bricklink.com/catalogItemIn.asp?P=68327&amp;in=S">the perfect size lens ring</a> for his purposes, he decided to begin building.
</p>


			</div>
		
					<div>
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn0.vox-cdn.com/thumbor/FpaAR448zYP_rwB0naZjb3AzIi0=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161196/MarcCorfmatPicture_4_3.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/OLne4uI2UUgGwYUxQ16dyP0-Ujw=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161196/MarcCorfmatPicture_4_3.jpg" data-sourcemobile="https://cdn3.vox-cdn.com/thumbor/_GSzwxYPek3-4dbwRPRmZZ3ejes=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161196/MarcCorfmatPicture_4_3.jpg" alt="Marc Corfmat is the fan designer behind this set — here, he’s holding the original Polaroid OneStep SX-70 instant camera." src="https://cdn3.vox-cdn.com/thumbor/_GSzwxYPek3-4dbwRPRmZZ3ejes=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161196/MarcCorfmatPicture_4_3.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Marc Corfmat is the fan designer behind this set — here, he’s holding the original Polaroid OneStep SX-70 instant camera.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn1.vox-cdn.com/thumbor/SG-jj-rN8oWc6JRpGCeQ_6ZGB_0=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161292/Progress3.jpg" data-sourcetablet="https://cdn0.vox-cdn.com/thumbor/WiTH--UfUDK12esDYQPOqfFRw9Q=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161292/Progress3.jpg" data-sourcemobile="https://cdn0.vox-cdn.com/thumbor/sYOCgbZ0H0Dx5cYJAT-ThsPrJes=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161292/Progress3.jpg" alt="Marc’s original digital designs included a truck tire around the lens." src="https://cdn0.vox-cdn.com/thumbor/sYOCgbZ0H0Dx5cYJAT-ThsPrJes=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161292/Progress3.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Marc’s original digital designs included a truck tire around the lens.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn1.vox-cdn.com/thumbor/YQojalpfimOHBOID89F6xdoURik=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161283/PolaroidSideViewOpen.jpg" data-sourcetablet="https://cdn2.vox-cdn.com/thumbor/V7fyMu8eU_H9xgMxg-WTl_RQRfI=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161283/PolaroidSideViewOpen.jpg" data-sourcemobile="https://cdn2.vox-cdn.com/thumbor/3Y7ZZ3SpmulOaOaakgsftepFx_s=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161283/PolaroidSideViewOpen.jpg" alt="Marc’s final fan renders show an opening film bay on the front of the camera and a prominent dial on the side so you can spit out photos." src="https://cdn2.vox-cdn.com/thumbor/3Y7ZZ3SpmulOaOaakgsftepFx_s=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161283/PolaroidSideViewOpen.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Marc’s final fan renders show an opening film bay on the front of the camera and a prominent dial on the side so you can spit out photos.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn2.vox-cdn.com/thumbor/DAActsXdL2Y3kZWJPz0xjtFwwe8=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161072/marc_lego_polaroid_prototype.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/dkeDEmdejlVmTfdA6ER_kTJTT0Y=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161072/marc_lego_polaroid_prototype.jpg" data-sourcemobile="https://cdn0.vox-cdn.com/thumbor/6Rj9MOT1DlQJaUjwuirKouh7Qgw=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161072/marc_lego_polaroid_prototype.jpg" alt="One of Marc’s physical prototypes, with a different lens idea." src="https://cdn0.vox-cdn.com/thumbor/6Rj9MOT1DlQJaUjwuirKouh7Qgw=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161072/marc_lego_polaroid_prototype.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>One of Marc’s physical prototypes, with a different lens idea.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn0.vox-cdn.com/thumbor/bRDRDH68c8kVIE3R1N4FWIfBP00=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161222/polaroid_marc_corfmat_lego_4_3.gif.gif" data-sourcetablet="https://cdn2.vox-cdn.com/thumbor/4oO45CQm27ZSgM20DHend-XErxY=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161222/polaroid_marc_corfmat_lego_4_3.gif.gif" data-sourcemobile="https://cdn0.vox-cdn.com/thumbor/m433V4pGlapbuRfi5hHvO0vCYOc=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161222/polaroid_marc_corfmat_lego_4_3.gif.gif" alt="Marc’s animation of his Lego Polaroid fan design shows it pushing a photo brick in and out." src="https://cdn.vox-cdn.com/csk/bc589934-607c-4e08-aa73-839ad29e1951/2f7ff42a-850b-48b6-a4a9-9a9d63fecacb/images/placeholder.png">
    	<label tabindex="0">
			<div>
				
				<p><span>Marc’s animation of his Lego Polaroid fan design shows it pushing a photo brick in and out.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
							</div>
		
			<div>
					<p>
		Everything just clicked. “All the angles were lining up perfectly, everything was working,” he says. “It very quickly became apparent to me that I was falling in love with this thing.”&nbsp;
</p>


					<p>
		And realizing that, Marc decided to do something differently with his Lego Ideas submission in January 2022: he made it <em>move</em>. His model let you “load film” by opening the iconic hinged door, then “eject” a photo by turning a dial or sliding a hidden lever underneath. For the first time, he <a href="https://ideas.lego.com/projects/200dd32e-8ec8-44aa-8f7d-e4dcc6f74e5c">showed off motion on the web</a> in crisp, clean animations that made the gadgety design look irresistible. It got the “staff pick” nod in under two weeks and hit 10,000 supporters in under two months. And this time, Lego finally got in touch.
</p>


					<p>
		Today, Lego is opening preorders for its replica of the classic rainbow stripe Polaroid OneStep SX-70 instant camera, based on Marc’s homegrown build. Lego sent one to <em>The Verge</em> to build and toy with, and as I’ll explain later, the $80 / €80 / £70 set is a delight. Lego also granted us multiple interviews to discuss <em>how</em> a Lego dream comes to life —&nbsp;and the challenges that come with turning a fan-made design into a ready-to-sell product. 
</p>


			</div>
		
					<div>
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn2.vox-cdn.com/thumbor/FNPiZ6zFEYRlDvwY842AtRGlrfU=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161538/1vpavic_20231012__0030.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/COvBNc4ZVL4mAlFCtuFJ0JiVGf8=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161538/1vpavic_20231012__0030.jpg" data-sourcemobile="https://cdn1.vox-cdn.com/thumbor/B5WMvdioR1zcj1H6RYRRQQvY-2w=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161538/1vpavic_20231012__0030.jpg" alt="I hold up the final prototype of Lego’s Polaroid as if to take a photo, with its iconic “OneStep” and “Polaroid Land Camera” badges on display." src="https://cdn1.vox-cdn.com/thumbor/B5WMvdioR1zcj1H6RYRRQQvY-2w=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161538/1vpavic_20231012__0030.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>I hold up the final prototype of Lego’s Polaroid as if to take a photo, with its iconic “OneStep” and “Polaroid Land Camera” badges on display.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn1.vox-cdn.com/thumbor/b_pKsGIA-xY-ieAh3yIb-ScSkyM=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161540/1vpavic_20231012__0016.jpg" data-sourcetablet="https://cdn1.vox-cdn.com/thumbor/w4xpF4R4stZi3RAd4AjxNMHW4hM=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161540/1vpavic_20231012__0016.jpg" data-sourcemobile="https://cdn3.vox-cdn.com/thumbor/huWMQcOyqJiQ7DUX2Mcghfv8vWU=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161540/1vpavic_20231012__0016.jpg" alt="The Lego Ideas Polaroid OneStep in final prototype form, with its film bay open, sitting alongside its brick-built film box and photocards." src="https://cdn3.vox-cdn.com/thumbor/huWMQcOyqJiQ7DUX2Mcghfv8vWU=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161540/1vpavic_20231012__0016.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>The Lego Ideas Polaroid OneStep in final prototype form, with its film bay open, sitting alongside its brick-built film box and photocards.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn2.vox-cdn.com/thumbor/-tX4zBc8N3fkd9B5dFlu74RP_ao=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161746/lego_polaroid_comparo.gif" data-sourcetablet="https://cdn0.vox-cdn.com/thumbor/4SowmjtScUNFi4kufnXxpa0mzqo=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161746/lego_polaroid_comparo.gif" data-sourcemobile="https://cdn2.vox-cdn.com/thumbor/9OLgumhKvRbdtJ94ITnskIEIUT0=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161746/lego_polaroid_comparo.gif" alt="Here’s a GIF we made that flips back and forth between the Lego Polaroid and the one that takes film. You can catch some of the minute visual differences, like the deeper, wider film bay and wide-angle viewfinder on the original." src="https://cdn.vox-cdn.com/csk/bc589934-607c-4e08-aa73-839ad29e1951/2f7ff42a-850b-48b6-a4a9-9a9d63fecacb/images/placeholder.png">
    	<label tabindex="0">
			<div>
				
				<p><span>Here’s a GIF we made that flips back and forth between the Lego Polaroid and the one that takes film. You can catch some of the minute visual differences, like the deeper, wider film bay and wide-angle viewfinder on the original.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn0.vox-cdn.com/thumbor/PCFj3M-VFo4EjBLOiIBr5GTUcCE=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163112/1vpavic_20231012__0055.jpg" data-sourcetablet="https://cdn2.vox-cdn.com/thumbor/FrGds2ZAV_GtJzvxuw_zrNqG910=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163112/1vpavic_20231012__0055.jpg" data-sourcemobile="https://cdn1.vox-cdn.com/thumbor/KaSnlw0T_UmMrkKWSUdnIiYvS-M=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163112/1vpavic_20231012__0055.jpg" alt="The set’s brick-built photo box has printed parts — no stickers — and comes with “photos” of Polaroid founder Edwin Land, Marc’s sister Mia at a cafe in France, and the Lego House." src="https://cdn1.vox-cdn.com/thumbor/KaSnlw0T_UmMrkKWSUdnIiYvS-M=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163112/1vpavic_20231012__0055.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>The set’s brick-built photo box has printed parts — no stickers — and comes with “photos” of Polaroid founder Edwin Land, Marc’s sister Mia at a cafe in France, and the Lego House.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn3.vox-cdn.com/thumbor/u_4A4uu5k0v2ZoL3B-_Rg2IKUoM=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161548/1vpavic_20231012__0037.jpg" data-sourcetablet="https://cdn2.vox-cdn.com/thumbor/jsfsH_7YxVwrIO-o6sptSuDX0FQ=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161548/1vpavic_20231012__0037.jpg" data-sourcemobile="https://cdn0.vox-cdn.com/thumbor/19S4bHku4br2VxeNZCCOmzYkWFQ=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161548/1vpavic_20231012__0037.jpg" alt="Looking through the Lego viewfinder, we get a narrow look at the Lego-ified face of the fan designer’s sister." src="https://cdn0.vox-cdn.com/thumbor/19S4bHku4br2VxeNZCCOmzYkWFQ=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161548/1vpavic_20231012__0037.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Looking through the Lego viewfinder, we get a narrow look at the Lego-ified face of the fan designer’s sister.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
							</div>
		
			<div>
					<p>
		Each project moving through the Lego Ideas program starts the same way: a Lego designer tries to replicate the original fan creation in the real world to see what works and what doesn’t. In Lego’s Billund, Denmark, headquarters, designers walk through a room called the Brick Library that’s filled with veritable supermarket aisles of parts sorted by color and shape. They can take whatever they need.
</p>


					<p>
		<a href="https://brickset.com/sets/designer-Jordan-Scott/">Jordan David Scott</a>, a creative lead in the Lego Ideas program, says that creating a true Lego set <em>isn’t </em>a straightforward series of steps. Though Marc’s Polaroid set was well built, every set must go through stringent quality control that inevitably leads to changes. To pass, even Lego’s seasoned designers head back to the drawing board to swap out parts again and again.&nbsp;
</p>


					<p>
		In addition to production, packing, packaging, and marketing, Lego has a host of teams that work directly with designers, including a function testing department, a safety department, an engineering department, and a textile department. There’s even a dedicated “building instructions” department and a “model quality” team, each of which sits with designers and watches them build. They make sure the build process stays fun, the instructions make sense, and the model stays stable enough that there’s little chance it breaks while you build it. “It’s like the final exam of the design process,” says Lego designer <a href="https://brickset.com/sets/designer-James-May">James May</a>.&nbsp;
</p>


					<p>
		While some designers think in bricks, May tells me he thinks in Lego’s internal design tool. While it’s similar to fan-facing tools like <a href="https://www.bricklink.com/v3/studio/download.page">BrickLink Studio</a>, which lets designers automatically snap together digital bricks, the internal Lego version is linked to the company’s other projects and systems. That means he can collaborate with fellow designers, see which new Lego elements are becoming available, and even budget how much pieces will cost and how many bags of parts will be created and boxed in the final set.&nbsp;
</p>


					<p>
		May is the primary builder on the Polaroid set, and that means building the Lego camera many, many times over a matter of months — some digitally, some picture-perfect physical sets, and some physical models in random colors just for stress testing. One gets baked in an oven to simulate the set sitting out in a particularly hot country; another gets poked by a robot arm to test its moving parts. May says he doesn’t keep track of “drafts” because each set is a <a href="https://en.wikipedia.org/wiki/Ship_of_Theseus">Ship of Theseus</a>, the same design constantly evolving as pieces are swapped out to satisfy Lego’s standards.
</p>


			</div>
		
					<div>
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn1.vox-cdn.com/thumbor/GPqRjxqEOGLh5YO2VUZ422drm9U=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163619/lego_polaroid_random_colors_4_3.jpg" data-sourcetablet="https://cdn1.vox-cdn.com/thumbor/igmrjWgz-m4JtKBtD66c7tiaYOY=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163619/lego_polaroid_random_colors_4_3.jpg" data-sourcemobile="https://cdn0.vox-cdn.com/thumbor/RuSRoen9jHazIki2vNQ3GxNDqrg=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163619/lego_polaroid_random_colors_4_3.jpg" alt="This particular red, gray, black, white, and yellow Lego Polaroid prototype was made from the colors that were on hand." src="https://cdn0.vox-cdn.com/thumbor/RuSRoen9jHazIki2vNQ3GxNDqrg=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163619/lego_polaroid_random_colors_4_3.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>This particular red, gray, black, white, and yellow Lego Polaroid prototype was made from the colors that were on hand.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn2.vox-cdn.com/thumbor/ulUK4U6FxPTdwJouMBsyHrP-lTc=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163629/lego_polaroid_internal_prototypes_4_3.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/-zUgm7_ngfElCar1L-dSP7e5a0E=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163629/lego_polaroid_internal_prototypes_4_3.jpg" data-sourcemobile="https://cdn3.vox-cdn.com/thumbor/_wQDM8w8SeHDxdrlFceKot7aMgM=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163629/lego_polaroid_internal_prototypes_4_3.jpg" alt="Lego went through many iterations of the Polaroid set’s internal mechanism, seen here without any housing — there are a few different ways to let the shutter button release a spring-loaded lever." src="https://cdn3.vox-cdn.com/thumbor/_wQDM8w8SeHDxdrlFceKot7aMgM=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163629/lego_polaroid_internal_prototypes_4_3.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Lego went through many iterations of the Polaroid set’s internal mechanism, seen here without any housing — there are a few different ways to let the shutter button release a spring-loaded lever.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn2.vox-cdn.com/thumbor/OG7gM8izSOUXqP71oT-D8EqX9vs=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161562/1vpavic_20231012__0049_4_3.jpg" data-sourcetablet="https://cdn1.vox-cdn.com/thumbor/kDSKxe15J5qhvcc6kdMqnguXCXQ=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161562/1vpavic_20231012__0049_4_3.jpg" data-sourcemobile="https://cdn0.vox-cdn.com/thumbor/hT3y-1yspYivGqQSVi8RrFZtx7M=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161562/1vpavic_20231012__0049_4_3.jpg" alt="The final mechanism uses a simple orange and green linkage to raise a blue tooth, which lets the arm shoot forward. Here, you can see it encased in the Polaroid’s pyramid-like rear shell." src="https://cdn0.vox-cdn.com/thumbor/hT3y-1yspYivGqQSVi8RrFZtx7M=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161562/1vpavic_20231012__0049_4_3.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>The final mechanism uses a simple orange and green linkage to raise a blue tooth, which lets the arm shoot forward. Here, you can see it encased in the Polaroid’s pyramid-like rear shell.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn0.vox-cdn.com/thumbor/qGhLuJKSeMtEaz_xXpEQFsUSF3c=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163123/1vpavic_20231012__0059_4_3.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/ueq9BDoqDPQphTZmUc9zUrdUzW8=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163123/1vpavic_20231012__0059_4_3.jpg" data-sourcemobile="https://cdn1.vox-cdn.com/thumbor/SdGVn-7LEr59IoKaITwra5UTwKM=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163123/1vpavic_20231012__0059_4_3.jpg" alt="The result: when you press the shutter button, the Lego Polaroid shoots a photo out of its slot." src="https://cdn1.vox-cdn.com/thumbor/SdGVn-7LEr59IoKaITwra5UTwKM=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163123/1vpavic_20231012__0059_4_3.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>The result: when you press the shutter button, the Lego Polaroid shoots a photo out of its slot.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
							</div>
		
			<div>
					<p>
		In the case of the Lego Polaroid, one particular challenge kept May and his colleagues swapping out parts: a request from the CEO of Polaroid himself.&nbsp;
</p>


					<p>
		When Lego came calling, Polaroid CEO Oskar Smolokowski didn’t hesitate. “I’m a (casual) Lego fan building a few sets a year so it wasn’t really a decision I had to think about!” he tells me via email. He accepted Lego’s offer almost on the spot, he says, while dodging my question about how much Lego did or didn’t pay for the license. “We didn’t feel the need to negotiate anything <span> it felt fair and win-win to us,” he writes.</span>
</p>


					<p>
		But Polaroid’s CEO did have one ask: he wanted the Lego Polaroid’s big red shutter button to <em>do</em> <em>something</em>. “I really wanted the camera to be as much of a camera as possible,” he recalls, and the CEO brought up this idea in the very first Lego / Polaroid kickoff meeting, remembers Scott.
</p>


					<p>
		Lego wasn’t quite ready to commit to that. “I said yeah… we can look into it?” Scott recalls. Marc’s design could already eject a photo by turning a dial, and Lego had already successfully replicated that. The dial would definitely be Plan B.
</p>


					<p>
		But Scott decided to challenge May, who had previously worked on <a href="https://youtu.be/HRrHf57TMa8?si=Jga7xnV53wVzDuIi&amp;t=134">the moving Lego Typewriter</a>, to make the button work. With help from other teams that specialize in Lego’s mechanism-friendly Technic bricks, they landed on using a pair of tiny rubber bands connected to a sliding arm to eject the photo.&nbsp;
</p>


						</div>
						
						<div>
					<p>
		“It definitely didn’t work the first time,” says Scott. “I don’t know how many versions James went through.” They had to tinker with tiny details to make the mechanism work — making the contraption half a Lego plate thicker here or moving it over by one brick’s width. “A lot of it came down to nuances,” says Scott, “and all these subtleties you wouldn’t necessarily think of like which bricks are better at <em>stopping</em> it from firing out.”
</p>


					<p>
		In the end, the team attached the shutter button to an internal lever that, when pushed, raises an internal tooth, which releases a spring-loaded carriage that pushes the photo out with a satisfying <em>chonk</em> each time.
</p>


					<p>
		“Everyone came together to make this happen, and it’s so much better,” says Scott, adding that colleagues were wowed by the action (and sound) when they came by.
</p>


					<p>
		They also had to make sure the button worked no matter how many times someone pressed it. “A lot of the feedback we got was that the function just isn’t triggering after several hundred or several thousand times, it’s failing,” he adds. The function department even rigged up a robot to simulate pushing the shutter button tens of thousands of times — one which, I’m unreasonably pleased to say, uses Lego to test Lego: 
</p>


					<div><p><iframe src="https://www.youtube.com/embed/UR1Ul9Yn7Rw?rel=0" allowfullscreen="" scrolling="no" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share;"></iframe></p></div>

			</div>
		
			<div>
					<p>
		The other half of the Polaroid button challenge: figuring out how to create a Polaroid-like “picture” worthy of being ejected from the model. Originally, they tried a flat tile like Marc did but decided it wasn’t right.&nbsp;
</p>


					<p>
		“It looked Lego, it felt Lego, but it didn’t feel like a Polaroid photo because you want it to be thin; it also meant we couldn’t print on the back because you need the tube side; it caused a lot of issues in production because of warping,” says Scott.&nbsp;
</p>


					<p>
		But Lego’s textile department came to the rescue: “We found this card, could we use this for anything?” Scott remembers them asking. It was a thin sheet of matte polypropylene plastic — a “foil” — that had only been used a couple of times before in Lego sets, most prominently <a href="https://jaysbrickblog.com/reviews/review-lego-80109-lunar-new-year-ice-festival/#:~:text=this%20photobooth%20for%20minifigures">in this Chinese Lunar New Year Ice Festival photobooth</a> where minifigures can pop their heads through. It was flexible (though you can’t <em>quite</em> “<a href="https://www.cnn.com/2004/TECH/ptech/02/17/polaroid.warns.reut/">shake it like a Polaroid picture</a>”), and it could be easily printed on both sides.&nbsp;
</p>


					<p>
		So, Lego graphics designer Matthew Parsons, who typically works for the Lego City team, embedded himself in the company’s textile department to help figure out the foils. A photographer himself, he jumped at the chance to be part of the Polaroid project, and he designed the three Easter egg photocards that come in every box.
</p>


					<p>
		Lego got one of the images, choosing to depict the <a href="https://en.wikipedia.org/wiki/Lego_House_(Billund)">Lego House</a>; Polaroid chose an iconic photo of its founder, Edwin Land; and Marc decided to thank his inspirations: the city of La Rochelle, France, where he cultivated his love of Lego and first prototyped the set, and his sister Mia, whose instant photography hobby brought him the idea. You can see some of Parsons’ sketches in our embedded gallery.
</p>


					<p>
		One of the last challenges was safety. Unlike actual Polaroids, the foils have rounded corners rather than sharp points. But even then, Lego’s safety department had to continually test the launcher during the monthslong project to ensure <em>other </em>unspecified objects couldn’t be dangerously blasted. With just a few weeks left in the schedule, they told the team they’d found one more undesirable object that someone could potentially launch out of the camera. “So that was another week of testing and building,” Scott says.
</p>


					<p>
		The final design ensures four Lego studs barely brush against the photo every time it ejects thanks to two sets of locking hinges that hold them at just the right angle. Inclined slopes on the edge of the film slot make the photo curve slightly upward as it ejects, too. Put it all together, press the button, and — <em>chonk</em> — the photo extends just far enough for you to easily grab, almost exactly an inch, instead of shooting all the way out. 
</p>


			</div>
		
					<div>
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn1.vox-cdn.com/thumbor/bbyE4HA2epXCiwAsfBHBNd9HKeY=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163655/first_digital_sketch_lego_polaroid.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/5a7nifd85hGX-fmL4bFVWXPqJCs=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163655/first_digital_sketch_lego_polaroid.jpg" data-sourcemobile="https://cdn3.vox-cdn.com/thumbor/W96THeP6-UmoV2xhEU-3ln2n9jw=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163655/first_digital_sketch_lego_polaroid.jpg" alt="The first digital mockup of the Lego Polaroid shows how it resembled Marc’s design. You see colorful highlights like the red shutter button, gold film ejector, and rainbow stripe on the film box — before any stickers or printing are added." src="https://cdn3.vox-cdn.com/thumbor/W96THeP6-UmoV2xhEU-3ln2n9jw=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163655/first_digital_sketch_lego_polaroid.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>The first digital mockup of the Lego Polaroid shows how it resembled Marc’s design. You see colorful highlights like the red shutter button, gold film ejector, and rainbow stripe on the film box — before any stickers or printing are added.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn3.vox-cdn.com/thumbor/DYPK4cfN7OlFQ2KUcVpH5-27TeA=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163646/lego_polaroid_sketches_final.jpg" data-sourcetablet="https://cdn1.vox-cdn.com/thumbor/rcrOMGK4JxGGOQoF0ygORsv8aiI=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163646/lego_polaroid_sketches_final.jpg" data-sourcemobile="https://cdn0.vox-cdn.com/thumbor/kvuTFpVCrlN2mwU2Ur1xVhGWG50=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163646/lego_polaroid_sketches_final.jpg" alt="Lego graphics designer Matthew Parsons’ sketches and art for one included photocard, starting in black and white, then color, then refined." src="https://cdn0.vox-cdn.com/thumbor/kvuTFpVCrlN2mwU2Ur1xVhGWG50=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163646/lego_polaroid_sketches_final.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Lego graphics designer Matthew Parsons’ sketches and art for one included photocard, starting in black and white, then color, then refined.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn2.vox-cdn.com/thumbor/BjWJv8j40H6Xyt4KaCj53aApw2A=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163669/lego_polaroid_final_look.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/0SzaxksBnfQ_1kizZEJmtcbZAmg=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163669/lego_polaroid_final_look.jpg" data-sourcemobile="https://cdn3.vox-cdn.com/thumbor/JoZYJFvdaDWOJNkVedePGKUzSI0=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163669/lego_polaroid_final_look.jpg" alt="A final image of the set, provided by Lego, with all the decorations and printing complete." src="https://cdn3.vox-cdn.com/thumbor/JoZYJFvdaDWOJNkVedePGKUzSI0=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163669/lego_polaroid_final_look.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>A final image of the set, provided by Lego, with all the decorations and printing complete.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn1.vox-cdn.com/thumbor/S0vPpsOvNhuhjuENT9IMQWYrzrY=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163110/1vpavic_20231012__0058_4_3.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/Ddc5ph2rO4A3YgMsMY9z2KarjCE=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163110/1vpavic_20231012__0058_4_3.jpg" data-sourcemobile="https://cdn2.vox-cdn.com/thumbor/-AEXzxXNpSvVCh-pqSyRFRv5pDE=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163110/1vpavic_20231012__0058_4_3.jpg" alt="The final prototype photocards in real life, sitting against the Lego camera." src="https://cdn2.vox-cdn.com/thumbor/-AEXzxXNpSvVCh-pqSyRFRv5pDE=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163110/1vpavic_20231012__0058_4_3.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>The final prototype photocards in real life, sitting against the Lego camera.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn0.vox-cdn.com/thumbor/Lt7xq_y74pFU9jJw_DXiZoyiwGw=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163676/lego_polaroid_studded_vs_semi.jpg" data-sourcetablet="https://cdn0.vox-cdn.com/thumbor/sC5nHTD-PwpOKsrOyPRDARifMv4=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163676/lego_polaroid_studded_vs_semi.jpg" data-sourcemobile="https://cdn3.vox-cdn.com/thumbor/lif7CWHndiHFYnXIxLRvqtCOlUo=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163676/lego_polaroid_studded_vs_semi.jpg" alt="Lego tried both smooth and partially studded backs for the Ideas Polaroid OneStep but settled on fully studded in the end.  " src="https://cdn3.vox-cdn.com/thumbor/lif7CWHndiHFYnXIxLRvqtCOlUo=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163676/lego_polaroid_studded_vs_semi.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Lego tried both smooth and partially studded backs for the Ideas Polaroid OneStep but settled on fully studded in the end.  </span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn0.vox-cdn.com/thumbor/I2_WN1WfUd8qLoRD8E1ZeCjSxSw=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161773/1vpavic_20231012__0022.jpg" data-sourcetablet="https://cdn2.vox-cdn.com/thumbor/r4xiZN9k5NuBwsgkCqIm_Nj3_5k=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161773/1vpavic_20231012__0022.jpg" data-sourcemobile="https://cdn0.vox-cdn.com/thumbor/ZI9aWeQoxgVSu-ggUrHZGsbCsP8=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161773/1vpavic_20231012__0022.jpg" alt="And here’s the real Polaroid’s textured back and the Lego Polaroid’s all-studded back, for comparison." src="https://cdn0.vox-cdn.com/thumbor/ZI9aWeQoxgVSu-ggUrHZGsbCsP8=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161773/1vpavic_20231012__0022.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>And here’s the real Polaroid’s textured back and the Lego Polaroid’s all-studded back, for comparison.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
							</div>
		
			<div>
					<p>
		If you’ve ever used an old Polaroid camera, you know that photos tend to pop out quite a bit more than an inch, accompanied by a stretchy black film to slow their roll. It’s not the only way the Lego Polaroid diverges from the real thing, of course. You won’t find the OneStep SX-70’s trademark camera strap, or the film bay’s stickers with the manufacturer’s warranty support telephone number, or an optional green button that shipped in some markets, things Marc says he asked for when they solicited his input but says understandably didn’t make the cut.&nbsp;
</p>


					<p>
		(He also says he would have preferred a smooth, tiled back instead of studs — but Lego did try that, and both Polaroid and Lego agreed they preferred the studded look. And you can swap the “OneStep” sticker for a “1000” sticker, which is how some versions looked.)
</p>


					<p>
		Overall, I’m wildly impressed by the result. I bought the actual 1977 camera over a year ago just because <a href="https://www.theverge.com/2022/10/25/23423120/lego-polaroid-land-camera-sx-70-replica-film-ideas">I knew this set was coming</a>, and I sometimes mistake one for the other on my office shelf. The size, shapes, and weight are incredibly close — both weigh approximately one pound, with the Lego set’s nose (and lens) mostly just protruding a little bit more than the actual camera. The body is also a tad narrower.&nbsp;
</p>


			</div>
		
				<div>
	<picture>
		<img data-sourcedesktop="https://cdn0.vox-cdn.com/thumbor/pPhGNxj4t0-JK1Ekg4cLovKU3ro=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161494/polaroid_of_polaroid_1.jpg" data-sourcetablet="https://cdn2.vox-cdn.com/thumbor/VSksGZs_sb6OsvH5i5awTKB5wkg=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161494/polaroid_of_polaroid_1.jpg" data-sourcemobile="https://cdn1.vox-cdn.com/thumbor/7RzM-MGnHz5jzsMlaSvXIyc3ZXE=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161494/polaroid_of_polaroid_1.jpg" alt="We shot some actual Polaroids of the Lego Polaroid with the Polaroid camera that it’s based on. Here it is in front of the Golden Gate Bridge in San Francisco and the historic Dutch Windmill." src="https://cdn1.vox-cdn.com/thumbor/7RzM-MGnHz5jzsMlaSvXIyc3ZXE=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161494/polaroid_of_polaroid_1.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>We shot some actual Polaroids of the Lego Polaroid with the Polaroid camera that it’s based on. Here it is in front of the Golden Gate Bridge in San Francisco and the historic Dutch Windmill.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
			<div>
					<p>
		There actually is a substantial nod to the missing camera strap on the back of the set, too, with openings for a strap cleverly sculpted by the gap in <a href="https://www.bricklink.com/v2/catalog/catalogitem.page?P=39613">heart-shaped Lego plates</a> — ones that meld into the camera’s smooth corners thanks to a semi-advanced build technique. (If you’re a big Lego fan, you’ll be familiar with the phrase “Studs Not On Top.”) The film bay eject lever, film counter, and flash hot shoe are all represented with gaps or bulges, too, and the mechanism inside the black-and-white shell is a hidden rainbow of color, using all the same hues as the rainbow stripe up front. 
</p>


					<p>
		(It inspired me to hunt down a copy of the classic rainbow stripe for the right rail of this <em>Verge</em> story, in fact — Polaroid doesn’t really use the deep pink color anymore, and they had to dig it up at my request.)
</p>


					<p>
		The Lego team even splurged on a custom red plate with a white edge to represent Polaroid’s shutter button, plus two printed tiles for the brick-built film pack that reads “Polaroid” and “Time-Zero Supercolor SX-70 Land Film.”
</p>


					<p>
		I haven’t yet gotten to the single most satisfying step in the build, the one Lego saves for last: the iconic Polaroid rainbow stripe on this camera isn’t a sticker. It’s a sideways stack of 1x6 plates and <a href="https://www.newelementary.com/2018/12/lego-35459-1x3-inverted-tile-hole.html">1x3 inverted hole tiles</a> in colors that match up almost perfectly to Polaroid’s original hues, held together by thin Lego pipes. It’s great — but it made me wonder why Lego still does use some other stickers in this design.
</p>


					<p>
		Many Lego fans are vocal about their preference for printed parts over stickers, and there’s always annoyance when a set aimed at adults uses any stickers at all. Here, your “Polaroid Land Camera,” “OneStep” or “1000,” and the exposure dial’s white and black EV marks are all sticky labels, not printed tiles.
</p>


			</div>
		
					<div>
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn2.vox-cdn.com/thumbor/ox0n4tHzoSMw_fKMDPpjWPCBa4Q=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163120/1vpavic_20231012__0060.jpg" data-sourcetablet="https://cdn1.vox-cdn.com/thumbor/OoTfdVle2HB2d7RV2wnyZAKCCrU=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163120/1vpavic_20231012__0060.jpg" data-sourcemobile="https://cdn1.vox-cdn.com/thumbor/dliybiQSMaoglbsFQxFUeGgDiZ0=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163120/1vpavic_20231012__0060.jpg" alt="The brick-built rainbow stripe that adorns the front of the Lego camera. Here it is all together, close enough so you can see the gaps that show how it was made." src="https://cdn1.vox-cdn.com/thumbor/dliybiQSMaoglbsFQxFUeGgDiZ0=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163120/1vpavic_20231012__0060.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>The brick-built rainbow stripe that adorns the front of the Lego camera. Here it is all together, close enough so you can see the gaps that show how it was made.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn3.vox-cdn.com/thumbor/KwlV5S4eJ0Va8k_W8HV4Y73WQbc=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163121/1vpavic_20231012__0057.jpg" data-sourcetablet="https://cdn3.vox-cdn.com/thumbor/9Mvj_ylUu05URHiE5Ne0cLx8Rv0=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163121/1vpavic_20231012__0057.jpg" data-sourcemobile="https://cdn1.vox-cdn.com/thumbor/TWlymZTAfCqnF65_OwfyJyRrONI=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163121/1vpavic_20231012__0057.jpg" alt="Here’s a look at the rainbow stripe broken apart so you can see the Lego studs on the green plates inside." src="https://cdn1.vox-cdn.com/thumbor/TWlymZTAfCqnF65_OwfyJyRrONI=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25163121/1vpavic_20231012__0057.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Here’s a look at the rainbow stripe broken apart so you can see the Lego studs on the green plates inside.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
								<div>
	<picture>
		<img data-sourcedesktop="https://cdn0.vox-cdn.com/thumbor/_7Vu9TQFMz_rsBJLnaOmdaESw_o=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161577/zoom_with_lego_director_2.jpg" data-sourcetablet="https://cdn1.vox-cdn.com/thumbor/nfNyxoTtYbkrSoo7z2EpU350kQ8=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161577/zoom_with_lego_director_2.jpg" data-sourcemobile="https://cdn3.vox-cdn.com/thumbor/J7Lqp-6VThj_ipL5ngb_ftbEhOg=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161577/zoom_with_lego_director_2.jpg" alt="On a video call with&nbsp;The Verge, Lego Ideas creative lead Jordan David Scott holds up the key to the rainbow stripe in front of his eyes — a pair of inverted blue tiles with holes inside." src="https://cdn3.vox-cdn.com/thumbor/J7Lqp-6VThj_ipL5ngb_ftbEhOg=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161577/zoom_with_lego_director_2.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>On a video call with&nbsp;The Verge, Lego Ideas creative lead Jordan David Scott holds up the key to the rainbow stripe in front of his eyes — a pair of inverted blue tiles with holes inside.</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
							</div>
		
			<div>
					<p>
		To my great surprise, Scott was willing to explain how Lego makes those kinds of choices.
</p>


					<p>
		Lego’s picker system requires each printed piece to have its own unique storage bin, so rather than continually opening more warehouses, Lego limits how many custom parts designers can introduce each year.&nbsp;
</p>


					<p>
		“We can’t make everything decorated. We can’t change every brick into every color,” Scott says. “Otherwise the portfolio would just explode in complexity, so we have teams that manage the complexity level.”&nbsp;
</p>


					<p>
		And those teams came up with one simple idea to stem the tide of complexity: “frames.”&nbsp;
</p>


					<p>
		Want a part in a different color? That costs designers a frame. A new piece? Spend some frames. Bring back an old out-of-print piece? That’s a frame, too. Every year, design leads like Scott are given a limited number of frames that they can spend on their entire portfolio for physical pieces that aren’t readily at hand. “If I have five products or 10 products coming out, I need to allocate where those frames go,” says Scott.&nbsp;
</p>


						</div>
						
						<div>
					<p>
		Doing so is “a bit of a puzzle” to figure out which sets will need lots of frames — the <a href="https://www.theverge.com/23909975/lego-animal-crossing-pictures-price-release-date-sets">new <em>Animal Crossing</em> sets</a> with their custom minifigures probably ate a few — and which ones can be built mostly out of preexisting parts.&nbsp;
</p>


					<p>
		Designers also try to save frames by sharing brand-new bricks with other teams, giving them a heads-up that they might come in handy for other sets, too. Some of that happens automatically: “When someone puts in an order for a particular color change, we can see it showing up in the library of digital bricks,” says Scott.
</p>


					<p>
		Some of it is designers intentionally pooling their resources: “If Ninjago are making something we could use, we kind of have a dialogue and say, ‘Oh, we can use this as well, that would be great, so maybe we need to get you a frame or something to share it.’” 
</p>


					<p>
		Designers always want more frames for their sets, May says. But he explains those constraints are just part of the process. When designers don’t have as many frames as they’d like, they have to get creative — just like any other Lego fan.&nbsp;
</p>


					<p>
		For the Lego Polaroid, the team spent a frame on the red and white shutter button — which could now appear in any number of other sets — and two frames for the decorations on the film pack, which are obviously exclusive to Polaroid. Scott planned to spend frames on ejecting photos, too: internally, he and his fellow designers were excited about making a new 8x6 printed photo tile, until the foils came along.
</p>


					<p>
		Polaroid’s CEO remembers one more thing that didn’t make the cut: “I think the only other thing I may have mentioned was a little Edwin Land figure,” he says, referencing the founder of Polaroid. “That would’ve been awesome.” Instead, Land is on one of the three photocards that come with the set.
</p>


			</div>
		
				<div>
	<picture>
		<img data-sourcedesktop="https://cdn1.vox-cdn.com/thumbor/R2zLBoEnl9-uCdFDS1D49siHaUQ=/0x800/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161722/real_polaroids_polaroid_2.jpg" data-sourcetablet="https://cdn0.vox-cdn.com/thumbor/R6i5QHekqIE_nNEXFtDTL3hSS-A=/0x750/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161722/real_polaroids_polaroid_2.jpg" data-sourcemobile="https://cdn2.vox-cdn.com/thumbor/gJeDDOgBIl74dD2FUlIjgqERtP0=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161722/real_polaroids_polaroid_2.jpg" alt="Two more real, unedited Polaroids we shot of the Lego Polaroid with a Polaroid OneStep SX-70 — the camera it’s based on. One is me, holding the Lego Polaroid up to my eye facing the camera. The other is the Polaroid in its native habitat (on a railing next to the Camera Obscura near San Francisco’s Cliff House, with the ocean in the background)." src="https://cdn2.vox-cdn.com/thumbor/gJeDDOgBIl74dD2FUlIjgqERtP0=/0x650/filters:no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/25161722/real_polaroids_polaroid_2.jpg">
    	<label tabindex="0">
			<div>
				
				<p><span>Two more real, unedited Polaroids we shot of the Lego Polaroid with a Polaroid OneStep SX-70 — the camera it’s based on. One is me, holding the Lego Polaroid up to my eye facing the camera. The other is the Polaroid in its native habitat (on a railing next to the Camera Obscura near San Francisco’s Cliff House, with the ocean in the background).</span>
			</p></div>
		</label>
    	
		
	</picture>
</div>
		
			<div>
					<p>
		“Just thinking about the fact that because I submitted an idea like a year and a half ago, that now so many people in the community are going to have a Lego Polaroid set — it’s just insane,” says Marc.
</p>


					<p>
		I get the sense, though, that the process wasn’t <em>entirely</em> a dream come true. Lego mostly took his idea and ran with it. It never flew him to Denmark to meet the designers in person, something he says he would have loved, nor did it ship him prototypes during the process; he got to see it on a video call. He assured me it wasn’t a big deal — he’ll get 10 free copies after all.&nbsp;
</p>


					<p>
		Lego demands a high level of secrecy, too: he felt he couldn’t tell his own Lego-loving brother for <em>months</em>. Or his mom. Or his sister Mia, who may not quite know what she’s gotten into. “Like, I don’t think she understands that she’s going to be in the Lego set, you know, mass-produced,” says Marc. (He says he did ask permission to “steal her likeness,” and she was “totally cool” with it hypothetically being in Lego.)
</p>


					<p>
		But judging by their Lego Ideas page, Marc and his brother Nick don’t seem to have been put off one bit. In September, their “Minibrick Productions” submitted a brick-built version of <a href="https://ideas.lego.com/projects/7a6d8d85-093d-4f6c-ab7f-48b405115d60/official_comments#content_nav_tabs">the <em>Interstellar</em> space shuttle</a> that took just weeks to become a Lego staff pick and has crossed 6,000 votes. <a href="https://ideas.lego.com/projects/53056645-ba8d-4a35-a219-77d30aa6f733">A set based on Blackpink’s music video for “Lovesick Girls”</a> hit 5,000 votes in August.
</p>


					<p>
		If you’re looking to follow in their footsteps with a Lego set of your own, here’s Marc’s advice: design it like a product you’d want to sell. “Showcase its play features like you’d showcase a final product.” And — though this could be <a href="https://en.wikipedia.org/wiki/Survivorship_bias">survivorship bias</a> — he says you have to keep trying, pointing to his many previous rejections as evidence.&nbsp;
</p>


					<p>
		“I think you really just have to keep going and continue with that spark of hope, that maybe one of your future projects will become an actual set.”
</p>


			</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WebP is so great except it's not (156 pts)]]></title>
            <link>https://eng.aurelienpierre.com/2021/10/webp-is-so-great-except-its-not/</link>
            <guid>38653110</guid>
            <pubDate>Fri, 15 Dec 2023 11:32:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eng.aurelienpierre.com/2021/10/webp-is-so-great-except-its-not/">https://eng.aurelienpierre.com/2021/10/webp-is-so-great-except-its-not/</a>, See on <a href="https://news.ycombinator.com/item?id=38653110">Hacker News</a></p>
<div id="readability-page-1" class="page">
		<a href="#content">Skip to content</a>

	<div id="boxed-wrapper">
			
							
					
			<header>
				<div>
					<div data-margin-top="0px" data-margin-bottom="0px" data-margin-left="0px" data-margin-right="0px">
			<a href="https://eng.aurelienpierre.com/">

						<!-- standard logo -->
			<img src="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2017/03/signature-logo-petit.png" srcset="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2017/03/signature-logo-petit.png 1x, https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2020/10/signature-logo-retina.png 2x" width="" height="" alt="Aurélien PIERRE Engineering Logo" data-retina_logo_url="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2020/10/signature-logo-retina.png">

			
					</a>
		</div>		<nav aria-label="Main Menu"></nav>	

<nav aria-label="Main Menu Mobile"></nav>

					</div>
				
			</header>
								
							
				
					
							
			<div aria-label="Page Title Bar">

																							<h2>WebP is so great… except it’s not</h2>

											
																
				</div>

						<main id="main">
				<div id="content">
					<!-- maths detected --><article><div><p>I’m a responsible web designer, and as such, since WordPress (finally) accepts media uploads of <code>image/webp</code> MIME type and since <strong>all</strong> <a href="https://caniuse.com/webp">web browsers</a> newer than september 2020 (even Apple Safari \o/) can display it, I have been moving my <a href="https://photo.aurelienpierre.com/portfolio">photos library</a> to <a href="https://en.wikipedia.org/wiki/WebP">WebP</a>. After all, when you create content, the least you can do is to also provide the smoothest user experience around it.</p>

<p>WebP falls close to magical : lookie those file weights ! <a href="https://www.industrialempathy.com/posts/avif-webp-quality-settings/">15% savings</a> compared to JPEG at same quality ! What are we waiting for ? Google even claims <a href="https://developers.google.com/speed/webp/docs/webp_study">25-34% smaller</a>.</p>

<p>There are dozens of WordPress plugins allowing you to convert your old media library on-the-fly, most of them operating as SaaS (Shit as a Software) and doing the conversion on their own servers, which entitles them to make you pay a ridiculous amount for it, <a href="https://imagify.io/">one of them</a> I’m very unhappy to have actually paid (something about sparing time, which actually led to losing time AND money). All of them claim that their “aggressive” compression factor is safe for 99 % of your pictures.</p>

<p>The most technical ones will go as far as telling you that WebP quality greater than <code>lossy 80</code> is useless for most pictures, sustaining their claim with a glorious <a href="https://groups.google.com/a/webmproject.org/forum/#!topic/webp-discuss/0GmxDmlexek">Google logo</a> encoded at various rates. Because everyone knows shooting logos is the bread and butter of every photographer, especially the Adobe Stock ones. Also, logos have their gradients following an hyper-laplacian distribution like any other natural image<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">[1]</a></sup>. Or maybe not. Who cares about gradients stats anyway ? We are only talking about 2D compression heuristics with entropy and high-frequencies thresholding, after all.</p>

<p>So, while I may have lost all respect for coding monkeys turned into image dudes just because a position opened (and everyone loves pics, right ? They are fun and much easier on the brain than words), especially the internet image dudes, I still fall every time for that silly assumption:&nbsp;people who are supposed to know, actually know. Years pass, I&nbsp;don’t learn : I&nbsp;read docs, I&nbsp;do what they say, I discover it doesn’t work as promised, only then I remember those guys don’t know shit about images. And here I am, loosing faith in humanity one wrong expert at a time.</p>

<p>In my great silliness, I set the <a href="https://wordpress.org/plugins/webp-express/">third and last plugin I&nbsp;tried</a> to the advised <code>lossy 80</code> quality and trigger the batch conversion. I&nbsp;have relative faith in it since it uses server-side <a href="https://en.wikipedia.org/wiki/GraphicsMagick">GraphicsMagic</a> instead of the unfortunate PHP shitstack (GD, Gmagick and the likes) or the laggy HTTP-error connection-timed-out DNS-said-not-today please-retry-later SaaS nonsense.</p>

<p>Everything goes well, until this happens…</p>

<p><a href="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2017/02/03-WEB-800x971.webp"><img decoding="async" data-orig-src="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2017/02/03-WEB-800x971.webp" alt="" src="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2017/02/03-WEB-800x971.webp"></a></p>

<p>To the non-educated eye, this might look ok, but for a photographer it’s not, and for several reasons. See the posterized ring in the background ? First of all, it’s not graceful, but then it has nothing to do there. This comes from a 16 bits scan of an Ilford Delta 400 film shot with a Mamiya RB 67, that is old school analog medium format at 6×7 cm. The silver halide crystals of the Delta 400 emulsion act as a natural dithering which makes high-frequency compressions more difficult and therefore prevent posterization in smooth areas. So, for any compression algo, managing to posterize a Delta 400 scan is a feat of the wrong kind.</p>

<p>Look at the original JPEG at quality 85 :</p>

<p><a href="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2017/02/03-WEB-800x971.jpg"><img decoding="async" data-orig-src="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2017/02/03-WEB-800x971.jpg" alt="" src="https://photo.aurelienpierre.com/wp-content/uploads/sites/3/2017/02/03-WEB-800x971.jpg"></a></p>

<p>It’s not 100 % clean either, but much better. Granted, this is WebP re-encoding of an already lossy compressed JPEG, so we stack 2 steps of destructive compression. <strong>But</strong> this is what <a href="https://developers.google.com/speed/pagespeed/insights/">Google Page Speed insights</a> encourage you to do and what a shitload of plugins enable you to do, while pretending it’s completely safe. <strong>It’s not.</strong></p>

<p>I have seen a similar effect in other similar pictures : always pictures with large, smooth, gradients in the background, which happens a lot when some punctual-ish light falls off a wall. That’s not something accidental, smooth fall-off are actively built by photographers to create organic-looking backgrounds with just enough of texture to not get boring, yet discrete enough to not draw attention off the foreground/subject.</p>

<p>So, I wondered how bad it was for actual raw photos encoded straight in darktable. Meaning just one step of encoding. Meaning real WebP quality comparison for real-life studio head-shots, which are one of the last things customers are willing to pay actual photographers to do (instead of snapping their own iPhone). Meaning real money for real professionals. Meaning something the image coding douchebags may have not foreseen, because it doesn’t happen in VS Code (or Vim, for that matter).</p>

<p>Let’s start. The following 2 images use <a href="https://en.wikipedia.org/wiki/Floyd%E2%80%93Steinberg_dithering">Floyd-Steinberg dithering</a> in 8 bits, with lossy compression set at 90 for both JPEG and WebP (remember, the experts recommend 80 for WebP). All images below, saved in WebP, use the “photo” image hint of Jeroen Oom’s <a href="https://cran.r-project.org/web/packages/webp/index.html">libwebp</a> 1.2.1. <em>Click on images to open the full-size version, or better : right-click and open them in a new tab</em></p>

<p>JPEG, lossy, 90 : 227 kiB
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90.jpg"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90.jpg" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90.jpg" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-200x113.jpg 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-300x169.jpg 300w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-400x225.jpg 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-600x338.jpg 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-768x432.jpg 768w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-800x450.jpg 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-1024x576.jpg 1024w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-1200x675.jpg 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-1536x864.jpg 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90.jpg 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-200x113.jpg 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-300x169.jpg 300w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-400x225.jpg 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-600x338.jpg 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-768x432.jpg 768w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-800x450.jpg 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-1024x576.jpg 1024w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-1200x675.jpg 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90-1536x864.jpg 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-90.jpg 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>JPEG, lossy, 85 : 184 kiB 
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-200x113.webp 200w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-300x169.jpg 300w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-400x225.webp 400w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-600x338.webp 600w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-768x432.jpg 768w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-800x450.webp 800w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1024x576.jpg 1024w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1200x675.webp 1200w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1536x864.webp 1536w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-200x113.webp 200w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-300x169.jpg 300w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-400x225.webp 400w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-600x338.webp 600w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-768x432.jpg 768w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-800x450.webp 800w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1024x576.jpg 1024w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1200x675.webp 1200w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1536x864.webp 1536w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>WebP, lossy, 90 : 140 kiB
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-200x113.webp 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-400x225.webp 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-600x338.webp 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-800x450.webp 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1200x675.webp 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1536x864.webp 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-200x113.webp 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-400x225.webp 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-600x338.webp 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-800x450.webp 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1200x675.webp 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering-1536x864.webp 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-dithering.webp 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>JPEG 85 and WebP 90 both fail the test, looking like shit. But WebP looks more like shit : the contrast in posterized rings is higher. <strong>And</strong> we are already 10 % above the recommended quality that “should fit 99 % of pictures”. JPEG 90 looks ok though, but it’s a lot heavier.</p>

<p>So, let’s try something else, now : going lossless WebP. That should be our ground truth of WebP supremacy.</p>

<p>WebP, lossless : 660 kiB
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless.webp"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless.webp" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless.webp" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-200x113.webp 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-400x225.webp 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-600x338.webp 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-800x450.webp 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-1200x675.webp 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-1536x864.webp 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless.webp 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-200x113.webp 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-400x225.webp 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-600x338.webp 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-800x450.webp 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-1200x675.webp 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless-1536x864.webp 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossless.webp 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>JPEG, quality 100 : 759 kiB
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100.jpg"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100.jpg" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100.jpg" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-200x113.jpg 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-300x169.jpg 300w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-400x225.jpg 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-600x338.jpg 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-768x432.jpg 768w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-800x450.jpg 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-1024x576.jpg 1024w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-1200x675.jpg 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-1536x864.jpg 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100.jpg 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-200x113.jpg 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-300x169.jpg 300w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-400x225.jpg 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-600x338.jpg 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-768x432.jpg 768w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-800x450.jpg 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-1024x576.jpg 1024w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-1200x675.jpg 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100-1536x864.jpg 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-100.jpg 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>JPEG, quality 95 : 363 kiB
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95.jpg"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95.jpg" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95.jpg" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-200x113.jpg 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-300x169.jpg 300w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-400x225.jpg 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-600x338.jpg 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-768x432.jpg 768w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-800x450.jpg 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-1024x576.jpg 1024w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-1200x675.jpg 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-1536x864.jpg 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95.jpg 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-200x113.jpg 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-300x169.jpg 300w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-400x225.jpg 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-600x338.jpg 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-768x432.jpg 768w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-800x450.jpg 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-1024x576.jpg 1024w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-1200x675.jpg 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95-1536x864.jpg 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-95.jpg 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>So, the WebP quality is now clean, but I’m not impressed with the weight, especially since you need a really good look to distinguish it from JPEG 90, which weighs about a third of that, and it’s forensically similar to JPEG 95, which is a bit more than half. Ooops.</p>

<p>Let’s try something else : redo it, but instead of the light Floyd-Steinberg dithering, use heavier random noise at -48 dB PSNR. That’s a very high PSNR, meaning it should be almost unnoticeable to human eyes but should give an harder time to the high-frequency filtering which is most of the trick behind image compressing.</p>

<p>JPEG, lossy, 85 : 211 KiB
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-200x113.webp 200w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-300x169.jpg 300w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-400x225.webp 400w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-600x338.webp 600w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-768x432.jpg 768w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-800x450.webp 800w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1024x576.jpg 1024w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1200x675.webp 1200w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1536x864.webp 1536w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-200x113.webp 200w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-300x169.jpg 300w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-400x225.webp 400w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-600x338.webp 600w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-768x432.jpg 768w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-800x450.webp 800w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1024x576.jpg 1024w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1200x675.webp 1200w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1536x864.webp 1536w,https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>WebP, lossy, 90 : 146 kiB
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-200x113.webp 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-400x225.webp 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-600x338.webp 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-800x450.webp 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1200x675.webp 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1536x864.webp 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-200x113.webp 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-400x225.webp 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-600x338.webp 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-800x450.webp 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1200x675.webp 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-1536x864.webp 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise.webp 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>The WebP is still more prone to posterization. So, I wondered what the WebP quality was that would be as smooth as the JPEG 85 with -48 dB of noise (which was pretty damn smooth). The answer is somewhere between 95 and 96, even though it’s hard to make an equivalence since the quality and texture of the artifacts differ.</p>

<p>WebP, lossy, 96 : 294 kiB
<a href="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96.webp"><img decoding="async" src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96.webp" data-orig-src="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96.webp" alt="" width="2048" height="1152" srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-200x113.webp 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-400x225.webp 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-600x338.webp 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-800x450.webp 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-1200x675.webp 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-1536x864.webp 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96.webp 2048w" data-srcset="https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-200x113.webp 200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-400x225.webp 400w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-600x338.webp 600w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-800x450.webp 800w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-1200x675.webp 1200w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96-1536x864.webp 1536w, https://eng.aurelienpierre.com/wp-content/uploads/sites/8/2021/10/Shoot-Antoine-0044-_DSC0085-lossy-noise-96.webp 2048w" data-sizes="auto" data-orig-sizes="(max-width: 2048px) 100vw, 2048px"></a></p>

<p>Yeah, you read it. WebP is actually 39 % heavier than JPEG 85 plus noise for a similar-ish look on this difficult picture, and still not totally as smooth as the JPEG (there is still a tiny bit of ringing). It’s also 30 % heavier than JPEG 90 with simple Floyd-Steinberg dithering.</p>

<p>So, what do we take from that ?</p>

<p>First, at similar visual quality and for photographs, WebP is not lighter than JPEG, it’s actually the opposite. All the Google claims rely on measuring the average SSIM and average bit weight over a dataset of images. Call me crazy, but I don’t give a shit about averages. For a <a href="https://en.wikipedia.org/wiki/Normal_distribution">gaussian "normal" process</a>, probabilities say half of your sample will be above and half will be below the average (which is also the median in a gaussian distribution). If we designed cars for the average load they would have to sustain, it means we would kill about half of the customers. Instead, we design cars for the worst foreseeable scenario, add a safety factor on top, and they still kill a fair amount of them, but a lot fewer than in the past. Most probabilistic distributions are close to gaussian, so the assumption that average = median ± a little something is fair. Also the <a href="https://fr.wikipedia.org/wiki/Structural_Similarity">SSIM</a> metric is an incomplete, biased, controverted metric of image similarity that takes no actual perceptual metric into account<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">[2]</a></sup>, it’s just averages, variances and covariances, meaning it is barely a pattern recognition scheme from a machine perspective.</p>

<p>As a photographer, I&nbsp;care about robustness of the visual output. Which means, as a designer, designing for the worst possible image and taking numerical metrics with a grain of salt. And that whole WebP hype is unjustified, in this regard. It surely performs well in well chosen examples, no doubt. The question is : what happens when it doesn’t ? I can’t fine-tune the WebP quality for each individual image on my website, that’s time consuming and WordPress doesn’t even allow that. I can’t have a portfolio of pictures with even 25 % posterized backgrounds either, the whole point of a portfolio is to showcase your skills and results, not to take a wild guess on the compression performance of your image backend. Average won’t do, it’s simply not good enough. And in setting the weight vs. quality ratio, the nature of the induced artifacts matters perhaps more than the norm of the deviation. We can tolerate higher variance in random noise than in patterned blotches.</p>

<p>Second, I don’t know why all the techies around have a huge kink over sharpness, but the most challenging situations I&nbsp;have faced as a photographer were with smooth gradients. Or more accurately, gradients that should have been smooth and weren’t in the output. So there is a real issue with the design priorities of image algos from tech guys who clearly lack historical and artistic background, and don’t talk to artists, who anyway have largely decided that they were above science, maths and other menial materialistic concerns. Most test pictures for WebP compression showcase sharp scenes with large depth of field, so lots of details, aka high-frequencies, which have zero chance of posterization and are not the pain point of such algorithms. Lack of sharpness has never destroyed a picture, on the contrary. Painters took as much trouble to render atmospheric haze and <em><a href="https://en.wikipedia.org/wiki/Sfumato">sfumato</a></em> as photographers take now to revert them. But having a staircase in place of a smooth vignette surely <strong>is</strong> damaging to the picture in an unacceptable way.</p>

<p>Third, big shout-out to all the morons, idiots, douchebags and monkeys who make big claims all around on matters they don’t nearly understand. Why the big words ? I have been told on my previous article that I&nbsp;was too heavy on insults… Well, we live in a time where time is the ultimate luxury, and the idiots-who-should-know-but-didn’t are not only causing damages, they also cost money and time, and I really think they should be punished for this. You can refund money, you can’t refund time. Thing is, as technologies are “improving”, people don’t get more free time because the work doesn’t get any easier. Instead, the tools become more complex and the customer expect more as the tools get faster, meaning workers have as much work as before, only with more complex toolkits. So, actually, better tech doesn’t mean less or easier work for the actual workers, it may just mean better result if it is actually better tech, which, in this case, it is not. The proof has been made here that WebP is simply not robust enough for image makers, regardless of its average performance, if lesser (or even similar) data bandwidth is the ultimate goal. The test done here is simple enough to have been done by anyone much earlier, provided they used image datasets from actual photographers.</p>

<p>Image-making is not just a vocational part-time activity for bored upper-class or retired citizen with enough money to buy 10 k€ camera systems and do mostly nothing out of them. Some people rely on that to make a living. And they are already in a precarious enough situation (even before COVID… how many newspapers still had a photo staff in 2015 ?) to not take more shit from the people who pretend to help them, when they do the opposite. I have the ability to double-check the stupid shit I read here and there, but the large majority of visual artists don’t and will take the word of “experts” for truth even though it contradicts facts they have witnessed themselves for years.</p>

<p>The Google monkeys at Page Speed are idiots when they advise you to move all your content to WebP. Also they are dishonest since they commited it themselves, so they are judge and party. The Google monkeys who said WebP has lower weight at similar average SSIM say nothing because neither the SSIM nor the average are meaningful : none is robust enough, at best it’s a 50⁄50 % of satisfying/unacceptable outcome. The WordPress plugins monkeys are idiots when they advise and tool you up to convert already lossy JPEGs to WebP. Oh, they probably make all their claims in good faith, the problem is they didn’t see the problems, precisely. And it’s super difficult to argue with people who don’t — literally — see the problems because it’s their bad eyes against your experience, and since people believe only what they see, you are screwed. But then, a lot of lower-tier websites and blog will repeat everything coming from these “trustable” sources, doing even more damage. I have personally lost about a full working week in the past 6 months over that whole WebP migration madness and thanks to all these fake news, to make it work across URL rewriting and CDN redirections, and then to understand why it looked so bad at the end.</p>

<p>Finally, WebP is badly designed. Being necessarily RGB or RGBalpha, there is no way to save a monochrome grey image on single channel. We see that all the posterization here is made worse by magenta and green rings which come solely from the chroma subsampling. With a purely monochrome format saved on a single channel, you don’t introduce any additional chroma shift. It’s as bad as JPEG, but it could have been fixed. That’s what AVIF did, at least, but it won’t be a technical reality for at least another decade.</p>

<p>How do we solve that ?</p>

<ol>
<li>Stick to JPEG at 90 quality (or at least 85) if images matter to you, e.g. if you are a visual artist. If images are pretty decorations for your textual content, it doesn’t matter.</li>
<li>Always add dithering and/or a tiny bit of noise in your images, just to be sure smooth gradients will stay smooth no matter the amount of damage they will take from stupid websites recompressions.</li>
<li>Don’t convert your old JPEG to WebP even if every idiot around tells you to, unless you find the images shown above remotely acceptable.</li>
<li>Serve your images from a fast <a href="https://developer.mozilla.org/en-US/docs/Glossary/CDN">CDN</a>, use <a href="https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images">responsive image sizes</a> and <a href="https://developer.mozilla.org/en-US/docs/Web/Performance/Lazy_loading">image lazy loading</a> to improve loading speed and perceived responsiveness from the user/client side, but there is not much more you can do without damaging the quality of your images.</li>
<li>Avoid all the SaaS ways of converting your images on another server. On paper, they sound great because they relieve your own server from the conversion load, which is good on mutualized hostings. Except they cost, don’t disclose the actual quality factor they use, and don’t work in lots of cases (HTTP connections errors everywhere, especially if you have hardened WordPress with a security plugin). You would be better off with a better hosting and running conversions on your server straight with Image Magick/Graphics Magick (not the PHP&nbsp;interfaces, but directly the server program). There is a <a href="https://wordpress.org/plugins/imagemagick-engine/">WordPress plugin</a> that does just that.</li>
<li>Devs and techs really need to pull their head out of their arses and start discussing with actual artists to understand their challenges and priorities.</li>
<li>Devs and techs really need to get a grasp at basic probabilities because… average, really ?</li>
<li>We really need people able to have one foot in the tech world and the other in the art world, and being able to discuss with both worlds, because having them in two separate bubbles is damaging on a large scale right now, and I don’t see it improving.</li>
</ol>


</div></article>
				</div>  <!-- fusion-row -->
				</main>  <!-- #main -->
				
				
								
					
		 <!-- fusion-footer -->

		
					
												</div> <!-- #boxed-wrapper -->
				<a tabindex="-1" href="#" aria-hidden="true">Page load link</a>

		

			<section aria-labelledby="awb-to-top-label">
		<a href="#" id="toTop">
			<span id="awb-to-top-label">Go to Top</span>
		</a>
	</section>
		


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Oxlint – written in Rust – 50-100 Times Faster than ESLint (211 pts)]]></title>
            <link>https://oxc-project.github.io/blog/2023-12-12-announcing-oxlint.html</link>
            <guid>38652887</guid>
            <pubDate>Fri, 15 Dec 2023 10:48:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://oxc-project.github.io/blog/2023-12-12-announcing-oxlint.html">https://oxc-project.github.io/blog/2023-12-12-announcing-oxlint.html</a>, See on <a href="https://news.ycombinator.com/item?id=38652887">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-d1cebd69=""><header data-v-6584a84b=""><h2 data-v-6584a84b="">Oxlint General Availability</h2></header><p>We're thrilled to announce that oxlint is now generally available! This milestone signifies our team's ability to promptly address and triage issues.</p><p>Oxlint is a JavaScript linter designed to catch erroneous or useless code without requiring any configurations by default.</p><h2 id="how-to-use" tabindex="-1">How to Use <a href="#how-to-use" aria-label="Permalink to &quot;How to Use&quot;">​</a></h2><p>At this stage, oxlint is <strong>not intended to fully replace ESLint</strong>; it serves as an enhancement when ESLint's slowness becomes a bottleneck in your workflow.</p><p>For faster feedback loops, we recommend running oxlint before ESLint in your lint-staged or CI setup, considering it only takes a few seconds to run on large codebases.</p><p>To test oxlint in your JavaScript / TypeScript codebase, simply execute the following command at the root directory of your repository:</p><div><p><label for="tab-zlT1HlT">npm</label><label for="tab-bS3cme4">pnpm</label><label for="tab-eHmabbt">yarn</label><label for="tab-qvH706H">bun</label><label for="tab-NTSJzp6">deno</label></p><div><div><p><span>sh</span></p><pre><code><span><span>$</span><span> </span><span>npx</span><span> </span><span>oxlint@latest</span></span></code></pre><pre><code><span><span>$</span><span> </span><span>npx</span><span> </span><span>oxlint@latest</span></span></code></pre></div><div><p><span>sh</span></p><pre><code><span><span>$</span><span> </span><span>pnpm</span><span> </span><span>dlx</span><span> </span><span>oxlint@latest</span></span></code></pre><pre><code><span><span>$</span><span> </span><span>pnpm</span><span> </span><span>dlx</span><span> </span><span>oxlint@latest</span></span></code></pre></div><div><p><span>sh</span></p><pre><code><span><span>$</span><span> </span><span>yarn</span><span> </span><span>dlx</span><span> </span><span>oxlint@latest</span></span></code></pre><pre><code><span><span>$</span><span> </span><span>yarn</span><span> </span><span>dlx</span><span> </span><span>oxlint@latest</span></span></code></pre></div><div><p><span>sh</span></p><pre><code><span><span>$</span><span> </span><span>bunx</span><span> </span><span>oxlint@latest</span></span></code></pre><pre><code><span><span>$</span><span> </span><span>bunx</span><span> </span><span>oxlint@latest</span></span></code></pre></div><div><p><span>sh</span></p><pre><code><span><span>$</span><span> </span><span>deno</span><span> </span><span>run</span><span> </span><span>oxlint@latest</span></span></code></pre><pre><code><span><span>$</span><span> </span><span>deno</span><span> </span><span>run</span><span> </span><span>oxlint@latest</span></span></code></pre></div></div></div><p>Alternatively, refer to the <a href="https://oxc-project.github.io/docs/guide/usage/linter.html">installation guide</a> for detailed instructions.</p><h2 id="design" tabindex="-1">Design <a href="#design" aria-label="Permalink to &quot;Design&quot;">​</a></h2><h3 id="_50-100-times-faster-than-eslint" tabindex="-1">50-100 Times Faster than ESLint <a href="#_50-100-times-faster-than-eslint" aria-label="Permalink to &quot;50-100 Times Faster than ESLint&quot;">​</a></h3><p>In real-world scenarios, Shopify reported that their 75 CI minutes ESLint run is now only 10 seconds.</p><p>From Jason Miller, Shopify DX and creator of Preact:</p><blockquote><p>oxlint has been a massive win for us at Shopify. Our previous linting setup took 75 minutes to run, so we were fanning it out across 40+ workers in CI.</p><p>By comparison, oxlint takes around 10 seconds to lint the same codebase on a single worker, and the output is easier to interpret.</p><p>We even caught a few bugs that were hidden or skipped by our old setup when we migrated!</p></blockquote><p>The majority of the performance gains stem from Oxlint being purposefully designed for performance, utilizing Rust and parallel processing as key factors.</p><h3 id="lint-for-correctness" tabindex="-1">Lint for Correctness <a href="#lint-for-correctness" aria-label="Permalink to &quot;Lint for Correctness&quot;">​</a></h3><p>Oxlint defaults to identifying erroneous, redundant, or confusing code — prioritizing correctness over unnecessary nitpicking rules (categorized as <code>perf</code>, <code>suspicious</code>, <code>pedantic</code>, or <code>style</code>), which are disabled by default.</p><h3 id="ease-of-use" tabindex="-1">Ease of Use <a href="#ease-of-use" aria-label="Permalink to &quot;Ease of Use&quot;">​</a></h3><p>Setting up new JavaScript / TypeScript codebases is becoming increasingly complex. There's a high likelihood of encountering compatibility issues among your tools, potentially resulting in hours of wasted time.</p><p>That's why we designed oxlint to be zero-config out of the box; even Node.js is not a requirement. Most adjustments can be made through the command-line, and reading from ESLint configuration file is currently work in progress.</p><h3 id="enhanced-diagnostics" tabindex="-1">Enhanced Diagnostics <a href="#enhanced-diagnostics" aria-label="Permalink to &quot;Enhanced Diagnostics&quot;">​</a></h3><p>Understanding linter messages can be challenging. Oxlint aims to simplify this by pinpointing root causes and providing helpful messages — eliminating the need for lengthy rule documentation reading, saving valuable time.</p><p>Running <code>oxlint -D perf</code> in the <a href="https://github.com/microsoft/vscode" target="_blank" rel="noreferrer">vscode repository</a>:</p><p><img width="100%" src="https://github.com/oxc-project/oxc/assets/1430279/094a3b24-0433-42ae-aad2-48a7dec2b985"></p><h3 id="consolidated-rules" tabindex="-1">Consolidated Rules <a href="#consolidated-rules" aria-label="Permalink to &quot;Consolidated Rules&quot;">​</a></h3><p>Oxlint does not provide a plugin system yet, but we are actively consolidating rules from popular plugins like TypeScript, React, Jest, Unicorn, JSX-a11y and Import.</p><p>We recognize the importance of plugins in the JavaScript ecosystem and are also investigating a DSL-based plugin system.</p><p>However, you might appreciate a standalone linter — no need to manage a list of plugin dependencies, navigate through <a href="https://github.com/antfu/eslint-ts-patch" target="_blank" rel="noreferrer">compatibility issues</a>, or <a href="https://github.com/import-js/eslint-plugin-import/pull/2504#issuecomment-1191057877" target="_blank" rel="noreferrer">resort to forked plugins due to version constraints</a>.</p><hr><p>Happy linting and have a joyful holiday season!</p><p>To get started, follow the <a href="https://oxc-project.github.io/docs/guide/usage/linter.html">installation guide</a>, or <a href="https://oxc-project.github.io/docs/guide/introduction.html">learn more about the oxc project</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: AI/ML papers to catch up with current state of AI? (135 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=38652736</link>
            <guid>38652736</guid>
            <pubDate>Fri, 15 Dec 2023 10:19:14 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=38652736">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="38654772"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38654772" href="https://news.ycombinator.com/vote?id=38654772&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>The good (and some might say bad thing) is that when it comes to fundamental technologies there are only 2 that are relevant:<p>1. Transformers
2. Diffusion</p><p>The benefit is that, focus on understanding them both reeaaalllyy well and you are at the forefront of research;)</p><p>Also, what is the reason you want to do this? If it is about building some kind of AI enabled app, you don't have to read anything. Get an API key and let's go the barrier has never been lower.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38654200"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38654200" href="https://news.ycombinator.com/vote?id=38654200&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span><a href="https://trendingpapers.com/" rel="nofollow noreferrer">https://trendingpapers.com/</a><p>This tool can help you find what's new &amp; relevant to read. It's updated every day (based on ArXiv).</p><p>You can filter by category (Computer Vision, Machine Learning, NLP, etc), by release date, but most importantly, you can rank by PageRank (proxy of influence/readership), PageRank growth (to see the fastest growing papers in terms of influence), total # of citations, etc...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="38654526"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_38654526" href="https://news.ycombinator.com/vote?id=38654526&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>I'd be wary of programmatic lists that claim to track the most important recent papers. There's a ridiculous amount of <i>hype/propaganda</i> and <i>citation hacking</i> surrounding new AI research, making it hard to discern what will truly stand the test of time. Tomas Mikolov just posted about this:<p><a href="https://news.ycombinator.com/item?id=38654038">https://news.ycombinator.com/item?id=38654038</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="38654264"><td></td></tr>
                <tr id="38654737"><td></td></tr>
                  <tr id="38653719"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38653719" href="https://news.ycombinator.com/vote?id=38653719&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>My view is to focus on doing stuff. That's what I did. Pick up some task you want the model to do, try finetuning llama, playing with APIs from OpenAI, etc. Googling and asking GPT along the way.<p>Foundational model training got so expensive that unless you can get hired by "owns nuclear power plant of GPUs" you are not going to get any "research" done. And as the area got white-hot those companies have more available talent than hardware nowadays. So just getting into the practitioner area is the best way to get productive with those models. And you improve as a practitioner by practicing, not by reading papers.</p><p>If you're at the computer, your time is best spent writing code and interacting with those models in my opinion. If you cannot (e.g. commute) I listen to some stuff (e.g. <a href="https://www.youtube.com/watch?v=zjkBMFhNj_g" rel="nofollow noreferrer">https://www.youtube.com/watch?v=zjkBMFhNj_g</a> - Anything from Karpathy on youtube, or <a href="https://www.youtube.com/@YannicKilcher" rel="nofollow noreferrer">https://www.youtube.com/@YannicKilcher</a> channel).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38654704"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38654704" href="https://news.ycombinator.com/vote?id=38654704&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>Bear in mind that ML skillset is now bifurcating into two components. On the one side are the people who work at places like OpenAI/DeepMind/Mistral/etc, who have billion dollar compute budgets. They are the ones who will create the foundational models. At this point a lot of this work is very technically narrow, dealing with CUDA, GPU issues, numerical stability, etc. On the other side are people who are using the models through the APIs in various ways. This is much more open-ended and potentially creative, but you don't need to know how QLearning works to do this.<p>It's a bit analogous to the situation with microprocessors. There is a ton of deep technical knowledge about how chips work, but most of this knowledge isn't critical for mainstream programming.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38654486"><td></td></tr>
            <tr id="38653406"><td></td></tr>
            <tr id="38653159"><td></td></tr>
                <tr id="38653205"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_38653205" href="https://news.ycombinator.com/vote?id=38653205&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>Thanks! Purchased a copy for myself and a friend.<p>And, Francois could easily report the unauthorized seller to Amazon, or send S&amp;D letter, suing not required.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="38654333"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38654333" href="https://news.ycombinator.com/vote?id=38654333&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span><i>Build something</i> of personal interest to you. Start by looking for similar open-source projects online. Look at the online posts of the authors. <i>Then</i> look for the papers that <i>you</i> think will be useful for <i>your</i> project. Before you know it, you'll become an expert in your area of interest.<p>Above all, be wary of programmatic lists that claim to track the most important recent papers. There's a ridiculous amount of <i>hype/propaganda</i> and <i>citation hacking</i> surrounding new AI research, making it hard to discern what will truly stand the test of time. Tomas Mikolov just posted about this.[a]</p><p>---</p><p>[a] <a href="https://news.ycombinator.com/item?id=38654038">https://news.ycombinator.com/item?id=38654038</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38654256"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38654256" href="https://news.ycombinator.com/vote?id=38654256&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>I kind of despair of keeping up to date with ML, at least to the extent that I might ever get current enough to be paid to work with it. I did Andrew Ng's Coursera specialisation a few years back - and I've worked through some of the developer-oriented courses, implemented some stuff. read more than a few books, read papers (the ones I might have a hope of understanding), and tried to get a former employer to take it seriously. But its seeming like unless you have a PhD or big-co experience then its very difficult to keep up to date by working in the field.<p>Notwithstanding the above, I'd agree with others here who suggest learning by doing/implementing, not reading papers.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="38653228"><td></td></tr>
                <tr id="38654346"><td></td></tr>
                  <tr id="38654362"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38654362" href="https://news.ycombinator.com/vote?id=38654362&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>Posted in another thread, but sadly I got no replies...<p>Related question: how can I learn how to read the mathematical notation used in AI/ML papers? Is there a definitive work that describes the basics? I am a post-grad Engineer, so I know the fundamentals, but I'm really struggling with a lot of the Arxiv papers. Any pointers hugely appreciated.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="38654475"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_38654475" href="https://news.ycombinator.com/vote?id=38654475&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>I particularly enjoyed Kevin Murphy's book [0] for being just rigorous enough to satisfy but not too dry, but also not trying to add humor unnecessarily.  It's not the best introduction text but it's great for someone with a little familiarity in the field who wants to broaden their understanding.  There are proofs to rationalize some approaches, but not to the degree that would satisfy a hardcore mathematicians maybe, but tbh I think that's a good thing for a book of this scope.<p>If you find a sample, it may include the index of symbols in the beginning which is pretty comprehensive and may satisfy your question on its own.</p><p><a href="https://www.goodreads.com/book/show/15857489-machine-learning" rel="nofollow noreferrer">https://www.goodreads.com/book/show/15857489-machine-learnin...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                      <tr id="38654573"><td></td></tr>
                  <tr id="38653497"><td></td></tr>
            <tr id="38654135"><td></td></tr>
            <tr id="38653865"><td></td></tr>
            <tr id="38653235"><td></td></tr>
            <tr id="38653407"><td></td></tr>
            <tr id="38654078"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38654078" href="https://news.ycombinator.com/vote?id=38654078&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><br><div>
                  <p><span>can I get some insights on ai and robotics some papers to implement and get my hands dirty</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="38654107"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_38654107" href="https://news.ycombinator.com/vote?id=38654107&amp;how=up&amp;goto=item%3Fid%3D38652736"></a></center>    </td><td><p><span>you don’t need papers, Arxiv are self aggrandizement from some meme in East Asia<p>just join communities on discord or locallama on reddit
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Godot: A Collaboration with Google and the Forge (114 pts)]]></title>
            <link>https://godotengine.org/article/collaboration-with-google-forge-2023/</link>
            <guid>38651818</guid>
            <pubDate>Fri, 15 Dec 2023 07:16:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://godotengine.org/article/collaboration-with-google-forge-2023/">https://godotengine.org/article/collaboration-with-google-forge-2023/</a>, See on <a href="https://news.ycombinator.com/item?id=38651818">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p>We are excited to announce that we have partnered with <a href="https://about.google/">Google</a> and <a href="https://theforge.dev/">The Forge</a> to bring some helpful performance optimizations to our Vulkan mobile backend. This will primarily benefit users targeting Vulkan-capable mobile devices.</p>

<p>Google is committed to enhancing the Android gaming ecosystem by ensuring that Vulkan is well supported across many games, game engines, and devices. Lucky for us, this means that they have decided to help us ensure that our Vulkan mobile renderer is as efficient as possible.</p>

<p>The Forge is one of the premier developers in the world specializing in building custom game engines and rendering solutions as well as optimizing existing game engines.</p>

<p>The Forge will spend the next several months working on Godot and bringing their expertise to our engine. They will be assisted by experts at Google and, of course, Godot contributors.</p>

<p>We are excited to welcome The Forge into the Godot pool of contributors and we are very thankful to Google for their assistance as well!</p>

<p>The end goal for all of us is to create an amazing ecosystem for games on Android. We are happy to already participate in that ecosystem and look forward to allowing game developers to push their ideas even further with Vulkan.</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD's Next GPU Is a 3D-Integrated Superchip (125 pts)]]></title>
            <link>https://spectrum.ieee.org/amd-mi300</link>
            <guid>38650937</guid>
            <pubDate>Fri, 15 Dec 2023 04:18:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/amd-mi300">https://spectrum.ieee.org/amd-mi300</a>, See on <a href="https://news.ycombinator.com/item?id=38650937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="AMD’s Next GPU Is a 3D-Integrated Superchip" data-elid="2666454143" data-post-url="https://spectrum.ieee.org/amd-mi300" data-authors="Samuel K. Moore" data-page-title="AMD’s Next GPU Is a 3D-Integrated Superchip - IEEE Spectrum"><p><a href="https://spectrum.ieee.org/tag/amd">AMD</a> lifted the hood on its next AI accelerator chip, the Instinct MI300, at the<a href="https://www.amd.com/en/corporate/events/advancing-ai.html" target="_blank"> AMD Advancing AI</a> event today, and it’s an unprecedented feat of <a href="https://spectrum.ieee.org/tag/3d-integration" target="_blank">3D integration</a>. MI300, a version of which will power the <a href="https://en.wikipedia.org/wiki/El_Capitan_(supercomputer)" target="_blank">El Capitan</a> supercomputer, is a layer cake of computing, memory, and communication that’s three slices of silicon high and that can sling as much as 17 terabytes of data vertically between those slices. The result is as much as a 3.4-fold boost in speed for certain machine-learning-critical calculations. The chip offers both contrasts and similarities to competing approaches such as Nvidia’s <a href="https://spectrum.ieee.org/nvidia-supercomputing-cpu-puts-intel-under-pressure" target="_blank">Grace Hopper superchip</a> and Intel’s supercomputer accelerator <a href="https://spectrum.ieee.org/intel-s-exascale-supercomputer-chip-is-a-master-class-in-3d-integration" target="_self">Ponte Vecchio</a>.<br></p><p>MI300a stacks three CPU <a href="https://spectrum.ieee.org/tag/chiplets" target="_blank">chiplets</a> (called compute complex dies, or CCDs, in AMD’s lingo) and six accelerator chiplets (XCDs) on top of four input-output dies (IODs), all on top of a piece of silicon that links them together to eight stacks of high-bandwidth DRAM that ring the superchip. (The MI300x substitutes the CCDs for two more XCDs, for an accelerator-only system.) With the scaling of transistors in the plane of the silicon slowing down, 3D stacking is seen as a key method to get more transistors into the same area and keep driving Moore’s Law forward.</p><p><img alt="Grey, labeled rectangles with orange and gold vertical lines connecting them." data-rm-shortcode-id="b87acda6384d54374dd4564223f0574f" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/grey-labeled-rectangles-with-orange-and-gold-vertical-lines-connecting-them.jpg?id=50661489&amp;width=980" height="240" id="a7fd1" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/grey-labeled-rectangles-with-orange-and-gold-vertical-lines-connecting-them.jpg?id=50661489&amp;width=980" width="1057"><small data-gramm="false" data-lt-tmp-id="lt-246886" placeholder="Add Photo Caption..." spellcheck="false">Compute and AI chiplets are stacked on top of I/O and cache chiplets in the MI300a.</small><small placeholder="Add Photo Credit...">AMD</small></p><p>“It’s a truly amazing silicon stack up that delivers the highest density performance that industry knows how to produce at this time,” says <a href="https://spectrum.ieee.org/chiplet" target="_self">Sam Naffziger</a>, a senior vice president and corporate fellow at AMD. The integration is done using two Taiwan Semiconductor Manufacturing Co. technologies, <a href="https://3dfabric.tsmc.com/english/dedicatedFoundry/technology/SoIC.htm" rel="noopener noreferrer" target="_blank">SoIC (system on integrated chips) and CoWoS (chip on wafer on substrate)</a>. The latter stacks smaller chips on top of larger ones using hybrid bonding, which links copper pads on each chip directly without solder. It is used to produce AMD’s <a href="https://spectrum.ieee.org/amd-3d-stacking-intel-graphcore" target="_self">V-Cache,</a> a cache-memory expanding chiplet that stacks on its highest-end CPU chiplets. The former, CoWos, stacks chiplets on a larger piece of silicon, called an interposer, which is built to contain high-density interconnects.</p><h2>Similarities and differences between AMD and Nvidia</h2><p>There are both similarities and differences to chief rival Nvidia’s approach. Just as Nvidia did in its Hopper architecture, AMD’s accelerator architecture, CDNA3, added the capability of computing with <a href="https://www.nvidia.com/en-us/data-center/h100/#:~:text=AI%2Dfused%20HPC%20applications%20can,operations%2C%20with%20zero%20code%20changes." rel="noopener noreferrer" target="_blank">truncated 32-bit numbers called TF32</a> and with two different forms of 8-bit floating-point numbers. The latter attribute is used to speed the training of certain parts of transformer neural networks, such as large language models. They also both include a <a href="https://spectrum.ieee.org/nvidia-gpu" target="_self">scheme that reduces the size of the neural network</a>, called 4:2 sparsity.</p><p>Another similarity is the inclusion of both CPU and GPU in the same <a href="https://spectrum.ieee.org/tag/3d-packaging" target="_blank">package</a>. In many AI computer systems, GPUs and CPUs are separately packaged chips deployed in a 4 to 1 ratio. One advantage to joining them together in a single superchip is that both CPU and GPU have high-bandwidth access to the same cache and high-bandwidth DRAM (HBM) in a way that won’t trip each other up as they read and write data.</p><p>Nvidia’s <a href="https://developer.nvidia.com/blog/nvidia-grace-hopper-superchip-architecture-in-depth/" target="_blank">Grace Hopper</a> is such a superchip combination linking the Grace CPU to the Hopper GPU through Nvidia’s Nvidia NVLink Chip-2-Chip interconnects. AMD’s MI300a is as well, by integrating three CPU dies designed for its Genoa line and six XCD accelerators using its AMD Infinity Fabric interconnect technology.</p><p>But a casual glance at Grace Hopper and MI300 show some profound differences. Grace and Hopper are each individual dies that integrate all of a system-on-chip’s needed functional blocks—compute, I/O, and cache. They are linked horizontally, and they are large—nearly at the size limit of photolithography technology.</p><p>AMD took a different approach, one that it has followed for several generations of its CPUs and that rival <a href="https://spectrum.ieee.org/tag/intel">Intel</a> used for its 3D-stacked supercomputer accelerator Ponte Vecchio. The concept is called system-technology-co-optimization, or STCO. That means designers started by breaking the chip down into its functions and decided which functions needed which manufacturing technology.</p><p data-rm-resized-container="25%"><img alt="" data-rm-shortcode-id="824acf053d8de4f6c76ec91569d65c78" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-slice-of-mi300-stack-from-the-carrier-silicon-at-the-top-to-the-solder-ball-at-the-bottom-of-the-package.jpg?id=50682953&amp;width=980" height="3661" id="cde2e" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-slice-of-mi300-stack-from-the-carrier-silicon-at-the-top-to-the-solder-ball-at-the-bottom-of-the-package.jpg?id=50682953&amp;width=980" width="1126"><small placeholder="Add Photo Caption...">A slice of MI300 stack from the carrier silicon at the top to the solder ball at the bottom of the package.</small><small placeholder="Add Photo Credit...">AMD</small></p><p>“What we wanted to do with MI300 was to scale beyond what was possible in a single monolithic GPU. So we deconstructed it into pieces and then built it back up,” says <a href="https://www.linkedin.com/in/alan-smith-amd/" rel="noopener noreferrer" target="_blank">Alan Smith</a>, a senior fellow and the chief architect for Instinct. Although it’s been doing so for several generations of CPUs, the MI300 is the first time the company has made GPU chiplets and bound them in a single system.</p><p>“Breaking the GPU into chiplets allowed us to put the compute in the most advanced process node while keeping the rest of the chip in technology that’s more appropriate for cache and I/O,” he says. In the case of the MI300, all the compute was built using TSMC’s N5 process, the most advanced available and the one used for Nvidia’s top-line GPUs. Neither the I/O functions nor the system’s cache memory benefit from N5, so AMD chose a less-expensive technology (N6) for those. Therefore, those two functions could then be built together on the same chiplet.</p><p>With the functions broken up, all the pieces of silicon involved in the MI300 are small. The largest, the I/O dies, are not even half the size of Hopper. And the CCDs are only about one-fifth the size of the I/O die. The small sizes make a big difference. Generally, smaller chips yield better. That is, a single wafer will provide a higher proportion of working small chips than it would large chips. “3D integration isn’t free,” says Naffziger. But the higher yield offsets the cost, he says.</p><h2>Luck and experience</h2><p>The design involved some clever reuse of existing technologies and designs, a few compromises, and a little luck, according to Naffziger, an IEEE Fellow. The reuse came in two instances. First, AMD was able to do the 3D integration with a degree of confidence because it had already been using the exact same pitch of vertical interconnects—9 micrometers—in its V-cache product.</p><p>As an optional add-on that AMD was able to charge extra for, V-cache offers little risk that poor yield or other problems will have a big impact on the company. “It’s been a great thing to enable us to wring out the manufacturing problems and all the design complexities of 3D stacking without endangering the main product line,” says Naffziger.</p><p>The other instance of reuse was a bit chancier. When the MI300 team decided that a CPU/GPU combination was needed, Naffziger “somewhat sheepishly” asked the head of the team designing the Zen4 CCD for the <a href="https://www.amd.com/en/events/epyc4" target="_blank">Genoa CPU</a> if the CCD could be made to fit the MI300’s needs. That team was under pressure to meet an earlier deadline than expected, but a day later they responded. Naffziger was in luck; the Zen4 CCD had a small blank space in just the right spot to make the vertical connections to the MI300 I/O die and their associated circuitry without a disruption to the overall design.</p><p>Nevertheless, there was still some geometry that needed solving. To make all the internal communications work, the four I/O chiplets had to be facing each other on a particular edge. That meant making a mirror-image version of the chiplet. Because it was codesigned with the I/O chiplet, the XCD and its vertical connections were built to link up with both versions of the I/O. But there was no messing with the CCD, which they were lucky to have at all. So instead the I/O was designed with redundant connections, so that no matter which version of the chiplet it sat on, the CCD would connect.</p><p><img alt="Multicolor rectangle with capital lettering in places." data-rm-shortcode-id="fd7dd6159bc9fba3eaf956a11c231d6e" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/multicolor-rectangle-with-capital-lettering-in-places.jpg?id=50662187&amp;width=980" height="412" id="917d3" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/multicolor-rectangle-with-capital-lettering-in-places.jpg?id=50662187&amp;width=980" width="896"><small placeholder="Add Photo Caption...">To get everything to line up, the IOD chiplets had to be made as mirrors of each other, and the accelerator (XCD) and compute (CCD) chiplets had to be rotated.</small><small placeholder="Add Photo Credit...">AMD</small></p><p>The power grid, which has to deliver hundreds of amperes of current to the compute dies at the top of the stack, faced similar challenges because it too had to accommodate all the various chiplet orientations, Naffziger noted.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pipe Dreams: The life and times of Yahoo Pipes (275 pts)]]></title>
            <link>https://retool.com/pipes</link>
            <guid>38650878</guid>
            <pubDate>Fri, 15 Dec 2023 04:07:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://retool.com/pipes">https://retool.com/pipes</a>, See on <a href="https://news.ycombinator.com/item?id=38650878">Hacker News</a></p>
<div id="readability-page-1" class="page"><div font-family="TiemposText" font-weight="400"><div><p>“He was very kind to write this very nice review of it,” Sadri reflected, “and that exploded interest, and caused our servers all to melt down.”</p></div><div><p>As launch day went on, usage didn’t taper off. The demand for Pipes required vastly more resources than it had available, and Yahoo put all hands on deck to support keeping the nascent system running.</p></div><div><p>Ho remembers “just sitting in my chair the entire <!-- -->[<!-- -->launch<!-- -->]<!-- --> day until late in the evening, just bringing up machines.” Yahoo’s data-center team started shifting servers that they had already prepped for deployment for other groups into service for Pipes.</p></div><div><p>“Every time it came up, everybody jumped on it and it crashed again, and everybody cycled back into a waiting state,” said Trevor, the lead engineer. “It felt bad at the time, but in retrospect, it was awesome.”</p></div><div><p>Raffel said Yahoo co-founder Jerry Yang told him to use his name to get whatever he needed to keep Pipes running then: “I’d never done that before. I certainly haven’t done that since.”</p></div><div><p>The beta launch’s popularity caught the team so off guard that they hadn’t even prepared a crash page. (Though that soon changed: Trevor said that the first thing they created after the beta launch was a page that read something like “our pipes are clogged.”)</p></div><div><p>In the days and weeks following, the team started to see more of the loose threads they hadn’t been able to stitch up in the sudden sprint to an unexpectedly momentous launch. Among a variety of issues, once someone created a Pipe, it ran forever.</p></div><div><p>There was no end-time built in or a need for someone to login to refresh or request a feed. (Even if there had been, RSS newsreaders polled for updates automatically.) Raffel said the team didn’t consider “just how many zombie things would get generated. <!-- -->[<!-- -->...<!-- -->]<!-- --> We kind of put ourselves in a situation where, fairly quickly, we had a lot of usage from things that may or may not actually <!-- -->[<!-- -->have been<!-- -->]<!-- --> real users.”</p></div><div><p>Still, many others agreed with O’Reilly’s sentiments. The <em>InfoWorld</em> writer turned <a href="https://blog.jonudell.net/2007/02/12/annotate-the-web-then-rewire-it/" target="_blank" rel="noopener noreferrer">Microsoft evangelist Jon Udell</a> blogged about a week after the beta launch, “The dominant way in which most people will ‘program’ the web is by writing metadata, not code, and we’ll need an interface as friendly and powerful as Pipes to help them do that.”</p></div><div><p>Yahoo threw more servers at Pipes, the programmers optimized the least inefficient code, and within a few months, Pipes’ reliability stabilized. But just as it was rising it was also—already—nearly fixed in amber.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: My Go SQLite driver did poorly on a benchmark, so I fixed it (191 pts)]]></title>
            <link>https://github.com/ncruces/go-sqlite-bench</link>
            <guid>38650570</guid>
            <pubDate>Fri, 15 Dec 2023 02:44:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ncruces/go-sqlite-bench">https://github.com/ncruces/go-sqlite-bench</a>, See on <a href="https://news.ycombinator.com/item?id=38650570">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Benchmarks for Golang SQLite Drivers</h2>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">I noticed my SQLite driver <a href="https://github.com/ncruces/go-sqlite3"><code>github.com/ncruces/go-sqlite3</code></a>
was doing poorly in this benchmark that got posted on <a href="https://news.ycombinator.com/item?id=38626698" rel="nofollow">Hacker News</a>
and <a href="https://www.reddit.com/r/golang/comments/18hgbyl/i_benchmarked_six_go_sqlite_drivers_and_found_you/" rel="nofollow">reddit</a>.</p>
<p dir="auto">I traced it back to a <a href="https://github.com/ncruces/go-sqlite3/commit/c667a1f469f28879a044807f8ed83e36645977ba">new feature</a>,
and a serious <a href="https://github.com/ncruces/go-sqlite3/commit/d862f47d95d522fb7a63aacf1259714aff986d46">performance regression</a>
(introduced to fix a compiler crash).<br>
I implemented a <a href="https://github.com/ncruces/go-sqlite3/commit/964a42c76deb9c7dcff2dca5c19f0453e062c55f">fix</a>
(a 4x larger, PLRU bit cache), released a new version, and ran the numbers again (different machine, configuration).</p>
<p dir="auto"><strong>The results of the new experiment are below</strong>.</p>
</div>
<p dir="auto">This work is sponsored by Monibot - Easy Server and Application Monitoring.
Try out Monibot at <a href="https://monibot.io/?ref=go-sqlite-bench" rel="nofollow">https://monibot.io</a>.
It's free.</p>
<p dir="auto">For benchmarks I used the following libraries:</p>
<ul dir="auto">
<li>
<p dir="auto">craw, <code>crawshaw.io/sqlite</code>, a CGO-based solution. This is not a <code>database/sql</code> driver.</p>
</li>
<li>
<p dir="auto">mattn, <code>github.com/mattn/go-sqlite3</code>, a CGO-based solution. This library is
(still) the de-facto standard and widely used.</p>
</li>
<li>
<p dir="auto">modernc, <code>modernc.org/sqlite</code>, a pure Go solution. This is a newer library,
based on the SQLite C code re-written in Go.</p>
</li>
<li>
<p dir="auto">ncruces, <code>github.com/ncruces/go-sqlite3</code>, a pure Go solution based on WASM (?).</p>
</li>
<li>
<p dir="auto">sqinn, <code>github.com/cvilsmeier/sqinn-go</code>, a solution without CGO. It uses
<code>github.com/cvilsmeier/sqinn</code> to access SQLite database files.</p>
</li>
<li>
<p dir="auto">zombie, <code>github.com/zombiezen/go-sqlite</code>, a rewrite of the crawshaw driver, using the
modernc libraries. This is not a <code>database/sql</code> driver.</p>
</li>
</ul>
<p dir="auto">The test setup is as follows:</p>
<ul dir="auto">
<li>OS: Debian/GNU Linux amd64 version 12.3</li>
<li>CPU: 11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz, 4 physical cores, 8 logical cores</li>
<li>RAM: 16GB</li>
<li>Disk: 1TB NVME SSD</li>
<li>go version go1.21.5 linux/amd64</li>
</ul>
<p dir="auto">The benchmark was run on 2023-12-09, with then-current library versions.
See go.mod for library versions. Each test was run once for warmup.
The second run was then recorded.</p>
<p dir="auto">A general note on benchmarks:</p>
<p dir="auto">Do not trust benchmarks, write your own. This specific benchmark is modelled
after my very own database usage scenarios. Your scenarios may be totally
different.</p>
<h2 tabindex="-1" dir="auto">Database Schema</h2>
<p dir="auto">The test database consist of the following tables and indizes:</p>
<div data-snippet-clipboard-copy-content="PRAGMA journal_mode=DELETE;
PRAGMA synchronous=FULL;
PRAGMA foreign_keys=1;
PRAGMA busy_timeout=5000;

CREATE TABLE users (
    id INTEGER PRIMARY KEY NOT NULL,
    created INTEGER NOT NULL,
    email TEXT NOT NULL,
    active INTEGER NOT NULL);
CREATE INDEX users_created ON users(created);

CREATE TABLE articles (
    id INTEGER PRIMARY KEY NOT NULL,
    created INTEGER NOT NULL,  
    userId INTEGER NOT NULL REFERENCES users(id),
    text TEXT NOT NULL);
CREATE INDEX articles_created ON articles(created);
CREATE INDEX articles_userId ON articles(userId);

CREATE TABLE comments (
    id INTEGER PRIMARY KEY NOT NULL,
    created INTEGER NOT NULL,
    articleId INTEGER NOT NULL REFERENCES articles(id),
    text TEXT NOT NULL);
CREATE INDEX comments_created ON comments(created);
CREATE INDEX comments_articleId ON comments(articleId);"><pre><code>PRAGMA journal_mode=DELETE;
PRAGMA synchronous=FULL;
PRAGMA foreign_keys=1;
PRAGMA busy_timeout=5000;

CREATE TABLE users (
    id INTEGER PRIMARY KEY NOT NULL,
    created INTEGER NOT NULL,
    email TEXT NOT NULL,
    active INTEGER NOT NULL);
CREATE INDEX users_created ON users(created);

CREATE TABLE articles (
    id INTEGER PRIMARY KEY NOT NULL,
    created INTEGER NOT NULL,  
    userId INTEGER NOT NULL REFERENCES users(id),
    text TEXT NOT NULL);
CREATE INDEX articles_created ON articles(created);
CREATE INDEX articles_userId ON articles(userId);

CREATE TABLE comments (
    id INTEGER PRIMARY KEY NOT NULL,
    created INTEGER NOT NULL,
    articleId INTEGER NOT NULL REFERENCES articles(id),
    text TEXT NOT NULL);
CREATE INDEX comments_created ON comments(created);
CREATE INDEX comments_articleId ON comments(articleId);
</code></pre></div>
<h2 tabindex="-1" dir="auto">Benchmarks</h2>
<p dir="auto">Result times are measured in milliseconds. Lower numbers indicate better
performance.</p>
<h3 tabindex="-1" dir="auto">Simple</h3>
<p dir="auto">Insert 1 million user rows in one database transaction.
Then query all users once.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ncruces/go-sqlite-bench/blob/master/results/simple.svg"><img src="https://github.com/ncruces/go-sqlite-bench/raw/master/results/simple.svg" alt=""></a></p>
<div data-snippet-clipboard-copy-content="                  insert        query
-------------------------------------
craw             1622 ms       908 ms
mattn            2035 ms      1851 ms
modernc          7218 ms      1851 ms
ncruces          4706 ms      1959 ms
sqinn            1337 ms       927 ms
zombie           2623 ms       584 ms"><pre><code>                  insert        query
-------------------------------------
craw             1622 ms       908 ms
mattn            2035 ms      1851 ms
modernc          7218 ms      1851 ms
ncruces          4706 ms      1959 ms
sqinn            1337 ms       927 ms
zombie           2623 ms       584 ms
</code></pre></div>
<h3 tabindex="-1" dir="auto">Complex</h3>
<p dir="auto">Insert 200 users in one database transaction.
Then insert 20000 articles (100 articles for each user) in another transaction.
Then insert 400000 comments (20 comments for each article) in another transaction.
Then query all users, articles and comments in one big JOIN statement.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ncruces/go-sqlite-bench/blob/master/results/complex.svg"><img src="https://github.com/ncruces/go-sqlite-bench/raw/master/results/complex.svg" alt=""></a></p>
<div data-snippet-clipboard-copy-content="                   insert       query
-------------------------------------
craw              1011 ms      972 ms
mattn             1231 ms     2047 ms
modernc           4414 ms     2290 ms
ncruces           2985 ms     2577 ms
sqinn              911 ms     1068 ms
zombie            2187 ms      858 ms"><pre><code>                   insert       query
-------------------------------------
craw              1011 ms      972 ms
mattn             1231 ms     2047 ms
modernc           4414 ms     2290 ms
ncruces           2985 ms     2577 ms
sqinn              911 ms     1068 ms
zombie            2187 ms      858 ms
</code></pre></div>
<h3 tabindex="-1" dir="auto">Many</h3>
<p dir="auto">Insert N users in one database transaction.
Then query all users 1000 times.
This benchmark is used to simluate a read-heavy use case.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ncruces/go-sqlite-bench/blob/master/results/many.svg"><img src="https://github.com/ncruces/go-sqlite-bench/raw/master/results/many.svg" alt=""></a></p>
<div data-snippet-clipboard-copy-content="        query/N=10  query/N=100  query/N=1000
--------------------------------------------------------
craw         58 ms       168 ms       1282 ms
mattn        52 ms       165 ms       2003 ms
modernc      58 ms       186 ms       2335 ms
ncruces      73 ms       375 ms       2665 ms
sqinn       112 ms       331 ms       3071 ms
zombie       34 ms        58 ms        646 ms"><pre><code>        query/N=10  query/N=100  query/N=1000
--------------------------------------------------------
craw         58 ms       168 ms       1282 ms
mattn        52 ms       165 ms       2003 ms
modernc      58 ms       186 ms       2335 ms
ncruces      73 ms       375 ms       2665 ms
sqinn       112 ms       331 ms       3071 ms
zombie       34 ms        58 ms        646 ms
</code></pre></div>
<h3 tabindex="-1" dir="auto">Large</h3>
<p dir="auto">Insert 10000 users with N bytes of row content.
Then query all users.
This benchmark is used to simluate reading of large (gigabytes) databases.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ncruces/go-sqlite-bench/blob/master/results/large.svg"><img src="https://github.com/ncruces/go-sqlite-bench/raw/master/results/large.svg" alt=""></a></p>
<div data-snippet-clipboard-copy-content="      query/N=50000  query/N=100000  query/N=200000
---------------------------------------------------
craw         429 ms          784 ms         1639 ms
mattn        308 ms          624 ms         1098 ms
modernc      599 ms         1054 ms         1983 ms
ncruces      395 ms          678 ms         1271 ms
sqinn       1269 ms         2553 ms         5707 ms
zombie      1064 ms         2057 ms         4115 ms"><pre><code>      query/N=50000  query/N=100000  query/N=200000
---------------------------------------------------
craw         429 ms          784 ms         1639 ms
mattn        308 ms          624 ms         1098 ms
modernc      599 ms         1054 ms         1983 ms
ncruces      395 ms          678 ms         1271 ms
sqinn       1269 ms         2553 ms         5707 ms
zombie      1064 ms         2057 ms         4115 ms
</code></pre></div>
<h3 tabindex="-1" dir="auto">Concurrent</h3>
<p dir="auto">Insert one million users.
Then have N goroutines query all users.
This benchmark is used to simulate concurrent reads.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ncruces/go-sqlite-bench/blob/master/results/concurrent.svg"><img src="https://github.com/ncruces/go-sqlite-bench/raw/master/results/concurrent.svg" alt=""></a></p>
<div data-snippet-clipboard-copy-content="        query/N=2  query/N=4  query/N=8
---------------------------------------
craw       796 ms    1145 ms    1553 ms
mattn     1782 ms    2019 ms    2642 ms
modernc   3554 ms   10789 ms   39959 ms
ncruces   2075 ms    2321 ms    3203 ms
sqinn      910 ms    1327 ms    1965 ms
zombie     509 ms     777 ms    1054 ms"><pre><code>        query/N=2  query/N=4  query/N=8
---------------------------------------
craw       796 ms    1145 ms    1553 ms
mattn     1782 ms    2019 ms    2642 ms
modernc   3554 ms   10789 ms   39959 ms
ncruces   2075 ms    2321 ms    3203 ms
sqinn      910 ms    1327 ms    1965 ms
zombie     509 ms     777 ms    1054 ms
</code></pre></div>
<h2 tabindex="-1" dir="auto">Summary</h2>
<ul dir="auto">
<li>We cannot declare a winner, it all depends on the use case.</li>
<li>Crawshaw and Zombiezen are pretty fast.</li>
<li>Mattn, although the de-facto standard, is not the best overall solution.</li>
<li>SQLite without CGO is possible.</li>
</ul>
<p dir="auto">This work is sponsored by Monibot - Easy Server and Application Monitoring.
Try out Monibot at <a href="https://monibot.io/?ref=go-sqlite-bench" rel="nofollow">https://monibot.io</a>.
It's free.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Groups ending support for Usenet (254 pts)]]></title>
            <link>https://support.google.com/groups/answer/11036538?hl=en</link>
            <guid>38649554</guid>
            <pubDate>Fri, 15 Dec 2023 00:37:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://support.google.com/groups/answer/11036538?hl=en">https://support.google.com/groups/answer/11036538?hl=en</a>, See on <a href="https://news.ycombinator.com/item?id=38649554">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="hcfe-content" role="main">            <article class="page" sc-render-smart-button="false" itemscope=""> <div data-stats-ve="35"><p>If you work with Usenet groups in Google Groups, support for these groups is ending soon.</p>

<h2>What’s changing?</h2>

<p>Starting on February 22, 2024, you can no longer use Google Groups (at groups.google.com) to post content to Usenet groups, subscribe to Usenet groups, or view new Usenet content. You can continue to view and search for historical Usenet content posted before February 22, 2024 on Google Groups.</p>

<p>In addition, Google’s Network News Transfer Protocol (NNTP) server and associated peering will no longer be available, meaning Google will not support serving new Usenet content or exchanging content with other NNTP servers.</p>

<p>This change will not impact any non-Usenet content on Google Groups, including all user and organization-created groups. Most of the current Google Groups content&nbsp; is not Usenet content and will not be affected.</p>

<h2>What do I need to do?</h2>

<p>If you don’t actively engage with Usenet content, you don’t need to do anything. Current Usenet users will need to do two things before February 22, 2024 if they want to continue engaging with Usenet content:</p>

<ol>
  <li><strong>Find a new Usenet client. </strong>Several free and paid alternatives are available, both web-based and application-based. To find a client, do a web search for "how do I find a usenet text client"</li>
  <li><strong>Find a new public Usenet server. </strong>The new client you choose will likely have a default server or a set of curated options for you. If not, to find a server, do a web search for "public NNTP servers."</li>
</ol>

<p>Because Usenet is a distributed system, <strong>you do not need to migrate data</strong>. All of the Usenet content you can access today on Google Groups should already be synced to the new server you choose. After you select a new client and server, you can reselect the groups you’re interested in.</p>

<h2>Why is Google Groups support for Usenet ending?</h2>

<p>Over the last several years, legitimate activity in text-based Usenet groups has declined significantly because users have moved to more modern technologies and formats such as social media and web-based forums. Much of the content being disseminated via Usenet today is binary (non-text) file sharing, which Google Groups does not support, as well as spam.</p>
</div>         <div data-stats-id="11036538" data-stats-ve="20" data-stats-visible-imp="" id="article-survey-container"><h2>Was this helpful?</h2><p>How can we improve it?</p></div>    </article>            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Stanford Prison Experiment was hugely influential. We learned it was a fraud (2018) (154 pts)]]></title>
            <link>https://www.vox.com/2018/6/13/17449118/stanford-prison-experiment-fraud-psychology-replication</link>
            <guid>38648883</guid>
            <pubDate>Thu, 14 Dec 2023 23:26:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vox.com/2018/6/13/17449118/stanford-prison-experiment-fraud-psychology-replication">https://www.vox.com/2018/6/13/17449118/stanford-prison-experiment-fraud-psychology-replication</a>, See on <a href="https://news.ycombinator.com/item?id=38648883">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p id="ch3SNe">The Stanford Prison Experiment, one of the most famous and compelling psychological studies of all time, told us a tantalizingly simple story about human nature. </p>
<p id="2DE2Gk">The study took paid participants and assigned them to be “inmates” or “guards” in a mock prison at Stanford University. Soon after the experiment began, the “guards” began mistreating the “prisoners,” implying evil is brought out by circumstance. The authors, in their conclusions, suggested innocent people, thrown into a situation where they have power over others, will begin to abuse that power. And people who are put into a situation where they are powerless will be driven to submission, even madness. </p>
<p id="buIC1G">The Stanford Prison Experiment has been <a href="https://www.researchgate.net/publication/278144932_Coverage_of_the_Stanford_Prison_Experiment_in_Introductory_Social_Psychology_Textbooks">included</a> in many, many introductory psychology textbooks and <a href="http://journals.sagepub.com/doi/pdf/10.1177/1475725714568007">is often cited uncritically</a>. It’s the subject of movies, documentaries, books, television shows, and <a href="https://exhibits.stanford.edu/spe/catalog/gf026hb5740">congressional testimony</a>.</p>
<p id="m7VLu5">But its findings were wrong. Very wrong. And not just due to its questionable ethics <a href="https://twitter.com/david_m_amodio/status/1006958007983460361">or lack of concrete data</a> — but because of deceit. </p>
<p id="cmyBh2">A new <a href="https://medium.com/s/trustissues/the-lifespan-of-a-lie-d869212b1f62">exposé</a> published by Medium based on previously unpublished <a href="https://www.vox.com/science-and-health/2018/6/14/17464516/stanford-prison-experiment-audio">recordings</a> of Philip Zimbardo, the Stanford psychologist who ran the study, and interviews with his participants, <a href="https://medium.com/s/trustissues/the-lifespan-of-a-lie-d869212b1f62">offers</a> convincing evidence that the guards in the experiment were coached to be cruel. It also shows that the experiment’s most memorable moment — of a prisoner descending into a screaming fit, proclaiming, “I’m burning up inside!” — was the result of the prisoner acting. “I took it as a kind of an improv exercise,” one of the guards told reporter <a href="https://twitter.com/benzblum?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">Ben Blum</a>. “I believed that I was doing what the researchers wanted me to do.” </p>
<p id="sk2QDd">The findings have long been subject to scrutiny — many think of them <a href="https://twitter.com/david_m_amodio/status/1006958007983460361">as more of a dramatic demonstration</a>, a sort-of academic reality show, than a serious bit of science. But these new revelations incited an immediate response. “We must stop celebrating this work,” personality psychologist Simine Vazire <a href="https://twitter.com/siminevazire/status/1006070517772607488">tweeted</a>, in response to the <a href="https://medium.com/s/trustissues/the-lifespan-of-a-lie-d869212b1f62">article</a>. “It’s anti-scientific. Get it out of textbooks.” Many other psychologists have expressed similar sentiments. </p>
<p id="8V75RZ">(<strong>Update</strong>: Since this article published, the journal <em>American Psychologist</em> has<a href="https://www.gwern.net/docs/psychology/2019-letexier.pdf"> published a thorough debunking</a> of the Stanford Prison Experiment that goes beyond what Blum found in his piece. There’s even more evidence that the “guards” knew the results that Zimbardo wanted to produce, and were trained to meet his goals. It also provides evidence that the conclusions of the experiment were predetermined.) </p>
<p id="c8SIec">Many of the classic show-stopping experiments in psychology have lately turned out to be wrong, fraudulent, or outdated. And in recent years, social scientists have begun to reckon with the truth that their old work needs a redo, the “<a href="https://www.vox.com/2016/3/14/11219446/psychology-replication-crisis">replication crisis</a>.” But there’s been a lag — in the popular consciousness and in how psychology is taught by teachers and textbooks. It’s time to catch up. </p>
<h3 id="UNdsVO">Many classic findings in psychology have been reevaluated recently </h3>
  <figure>
  <span>
    
    <span data-original="https://cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg">
      
        <picture data-cid="site/picture_element-1702618019_8587_194721" data-cdata="{&quot;asset_id&quot;:11533017,&quot;ratio&quot;:&quot;*&quot;}">
  


  <source srcset="https://cdn.vox-cdn.com/thumbor/rpZt310iAvhW5p4gYqANH5ZzCV8=/0x0:4912x3508/320x0/filters:focal(0x0:4912x3508):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 320w, https://cdn.vox-cdn.com/thumbor/NbI0nCEGMeY6wVmCwzipfoIahiM=/0x0:4912x3508/520x0/filters:focal(0x0:4912x3508):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 520w, https://cdn.vox-cdn.com/thumbor/uBEQbmUpdxsQ-IauH5VgqgPN69I=/0x0:4912x3508/720x0/filters:focal(0x0:4912x3508):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 720w, https://cdn.vox-cdn.com/thumbor/kIV1zd92Usd_jGwzNP5p4Iiaq-I=/0x0:4912x3508/920x0/filters:focal(0x0:4912x3508):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 920w, https://cdn.vox-cdn.com/thumbor/fA-hv31BSeEV9oTOvzCDij4kz3o=/0x0:4912x3508/1120x0/filters:focal(0x0:4912x3508):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 1120w, https://cdn.vox-cdn.com/thumbor/psHz4SyOj8NHCdmVp1f8HJDwzcc=/0x0:4912x3508/1320x0/filters:focal(0x0:4912x3508):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 1320w, https://cdn.vox-cdn.com/thumbor/FaR_bUgYUDtox_G5jM1CwhdnybE=/0x0:4912x3508/1520x0/filters:focal(0x0:4912x3508):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 1520w, https://cdn.vox-cdn.com/thumbor/KXh2zhbRBIzBRwreFWwsC9Nu1Gk=/0x0:4912x3508/1720x0/filters:focal(0x0:4912x3508):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 1720w, https://cdn.vox-cdn.com/thumbor/Hsma2pASoM5y60bIa3zcdVerm8Q=/0x0:4912x3508/1920x0/filters:focal(0x0:4912x3508):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 1920w" sizes="(min-width: 1221px) 846px, (min-width: 880px) calc(100vw - 334px), 100vw" type="image/webp">


<img srcset="https://cdn.vox-cdn.com/thumbor/U5w4ALHg15_J7-DXB-jni-MIbj8=/0x0:4912x3508/320x0/filters:focal(0x0:4912x3508):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 320w, https://cdn.vox-cdn.com/thumbor/_twvisnyW4hDIqO3Z86NCnEopyQ=/0x0:4912x3508/520x0/filters:focal(0x0:4912x3508):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 520w, https://cdn.vox-cdn.com/thumbor/FD59-zi0mf6R6xe3Dk8GTz92vjI=/0x0:4912x3508/720x0/filters:focal(0x0:4912x3508):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 720w, https://cdn.vox-cdn.com/thumbor/fN4egToY62oLHbqQmnGMbwZUP1Y=/0x0:4912x3508/920x0/filters:focal(0x0:4912x3508):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 920w, https://cdn.vox-cdn.com/thumbor/t7YLXM5VKH8C9JEIEATji_XHwck=/0x0:4912x3508/1120x0/filters:focal(0x0:4912x3508):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 1120w, https://cdn.vox-cdn.com/thumbor/mdnJnUfQqXoBDrBvIeawRR-sFQI=/0x0:4912x3508/1320x0/filters:focal(0x0:4912x3508):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 1320w, https://cdn.vox-cdn.com/thumbor/XuY4IdF0S_wVI5ZPqWilf_lCvuI=/0x0:4912x3508/1520x0/filters:focal(0x0:4912x3508):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 1520w, https://cdn.vox-cdn.com/thumbor/w9MUybJ5NN34xgavweV2h0umN64=/0x0:4912x3508/1720x0/filters:focal(0x0:4912x3508):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 1720w, https://cdn.vox-cdn.com/thumbor/puZA528BnVXkuISyYZaSlgwNH6E=/0x0:4912x3508/1920x0/filters:focal(0x0:4912x3508):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg 1920w" sizes="(min-width: 1221px) 846px, (min-width: 880px) calc(100vw - 334px), 100vw" alt="" loading="lazy" data-upload-width="4912" width="4912" height="3508" src="https://cdn.vox-cdn.com/thumbor/lEu7VEMcHRcXIpfaw1PQ4TtEAJw=/0x0:4912x3508/1200x0/filters:focal(0x0:4912x3508):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/11533017/GettyImages_834859350__1_.jpg">

</picture>

      
    </span>
    
  </span>
  
    <span>
      
      
        <cite>Getty Images</cite>
      
    </span>
  
</figure>

<p id="4jOET1">The Zimbardo prison experiment is not the only classic study that has been recently scrutinized, reevaluated, or outright exposed as a fraud. Recently, <a href="https://www.theguardian.com/science/2018/apr/16/a-real-life-lord-of-the-flies-the-troubling-legacy-of-the-robbers-cave-experiment">science journalist Gina Perry</a> found that the infamous “Robbers Cave“ experiment in the 1950s — in which young boys at summer camp were essentially manipulated into joining warring factions — was a do-over from a failed previous version of an experiment, which the scientists never mentioned in an academic paper. That’s a glaring omission. It’s wrong to throw out data that refutes your hypothesis and only publicize data that supports it. </p>
<p id="tXLsi4">Perry has also revealed inconsistencies in another major early work in psychology: the Milgram electroshock test, in which participants were told by an authority figure to deliver seemingly lethal doses of electricity to an unseen hapless soul. Her investigations show some <a href="https://psmag.com/social-justice/electric-schlock-65377">evidence of</a> researchers going off the study script and possibly coercing participants to deliver the desired results. (Somewhat ironically, the new revelations about the prison experiment also show the power an authority figure — in this case Zimbardo himself and his “warden” — has in manipulating others to be cruel.)</p>
<p id="13AnP0">Other studies have been reevaluated for more honest, methodological snafus. Recently, I wrote about the “marshmallow test,” a series of studies from the early ’90s that suggested the ability to delay gratification at a young age <a href="https://www.vox.com/science-and-health/2018/6/6/17413000/marshmallow-test-replication-mischel-psychology">is correlated with success later in life</a>. New research finds that if the original marshmallow test authors had a larger sample size, and greater research controls, their results would not have been the showstoppers they were in the ’90s. I can list so many more textbook psychology findings that have either not replicated, or are currently in the midst of a serious reevaluation. </p>
<p id="2sg5OI">Like: </p>
<ul>
<li id="OMhdEa">Social priming: People who read “old”-sounding words (like “nursing home”) were more likely to walk slowly — showing how our brains can be subtly “primed” with thoughts and actions.</li>
<li id="yPoABY">The facial feedback hypothesis: Merely activating muscles around the mouth caused people to become happier — demonstrating how our bodies tell our brains what emotions to feel.</li>
<li id="Bqa2bI">Stereotype threat: Minorities and maligned social groups don’t perform as well on tests due to anxieties about becoming a stereotype themselves. </li>
<li id="i75kyS">Ego depletion: The idea that willpower is a finite mental resource.</li>
</ul>
<p id="KsruDx">Alas, the past few years<a href="http://www.vox.com/2016/3/14/11219446/psychology-replication-crisis"> have brought about a reckoning</a> for these ideas and social psychology as a whole. </p>
<p id="L7bU4Q">Many psychological theories have been debunked or diminished in rigorous replication attempts. Psychologists are now realizing it's <a href="https://www.vox.com/science-and-health/2017/7/31/16021654/p-values-statistical-significance-redefine-0005">more likely</a> that false positives will make it through to publication than inconclusive results. And they’ve realized that experimental methods commonly used just a few years ago aren’t rigorous enough. For instance, it used to be commonplace for scientists to publish experiments that sampled about 50 undergraduate students. Today, scientists realize this <a href="https://www.vox.com/science-and-health/2017/7/31/16021654/p-values-statistical-significance-redefine-0005">is a recipe for false positives</a>, and strive for sample sizes in the hundreds and ideally from a more representative subject pool.  </p>
<p id="tKajYd">Nevertheless, in so many of these cases, scientists have moved on and corrected errors, and are still doing well-intentioned work to understand the heart of humanity. For instance, work on one of psychology’s oldest fixations — dehumanization, the ability to see another as less than human — continues with methodological rigor, <a href="https://www.vox.com/science-and-health/2017/3/7/14456154/dehumanization-psychology-explained">helping us understand</a> the modern-day maltreatment of Muslims and immigrants in America.  </p>
<p id="06Yvsy">In some cases, time has shown that flawed original experiments offer worthwhile reexamination. The original Milgram experiment was flawed. But at least its study design — which brings in participants to administer shocks (not actually carried out) to punish others for failing at a memory test — is basically repeatable today with some ethical tweaks.</p>
<p id="TYA1u7"><a href="http://journals.sagepub.com/doi/10.1177/1948550617693060">And</a> it seems like Milgram’s conclusions may hold up: In a recent study, many people found demands from <a href="http://journals.sagepub.com/doi/10.1177/1948550617693060">an authority figure to be a compelling</a> reason to shock another. However, it’s possible, due to something known as the file-drawer effect, that failed replications of the Milgram experiment have not been published. Replication attempts at the Stanford prison study, on the other hand, <a href="http://www.bbcprisonstudy.org/pdfs/bjsp(2006)tyrannny.pdf">have been a mess</a>.</p>
<p id="dTlTu4">In science, too often, the first demonstration of an idea becomes the lasting one — in both pop culture and academia. But this isn’t how science is supposed to work at all! </p>
<p id="dC0VT9">Science is a frustrating, iterative process. When we communicate it, we need to get beyond the idea that a single, stunning study ought to last the test of time. Scientists know this as well, but their institutions have often discouraged them from replicating old work, instead of the pursuit of new and exciting, attention-grabbing studies. (Journalists are <a href="https://www.vox.com/2016/7/14/12016710/science-challeges-research-funding-peer-review-process">part of the problem too</a>, imbuing small, insignificant studies with more importance and meaning than they’re due.)  </p>
<p id="PylPjK">Thankfully, there are researchers thinking very hard, and very earnestly, on trying to make psychology a more replicable, robust science. There’s even a whole <a href="https://osf.io/jtcu9/">Society for the Improvement of Psychological Science</a> devoted to these issues.</p>
<p id="8z2Cpw">Follow-up results <a href="https://www.vox.com/2016/3/14/11219446/psychology-replication-crisis">tend to be less dramatic than original findings</a>, but they are more useful in helping discover the truth. And it’s not that the Stanford Prison Experiment has no place in a classroom. It’s interesting as history. Psychologists like Zimbardo and Milgram were highly influenced by World War II. Their experiments were, in part, an attempt to figure out why ordinary people would fall for Nazism. That’s an important question, one that set the agenda for a huge amount of research in psychological science, and is still echoed in papers today. </p>
<h3 id="ZLgDN1">Textbooks need to catch up </h3>
<p id="3yADvM">Psychology has changed tremendously over the past few years. Many studies used to teach the next generation of psychologists have been intensely scrutinized, and found to be in error. But troublingly, <a href="http://nymag.com/scienceofus/2017/01/theres-a-problem-with-a-bunch-of-psychology-textbooks.html">the textbooks have not been updated accordingly</a>.</p>
<p id="WVNuea">That’s the conclusion of a <a href="http://christopherjferguson.com/Education%20or%20Indoctrination.pdf">2016 study</a> in <em>Current Psychology. “</em>By and large,” the study explains (emphasis mine):</p>
<blockquote><p id="PhzjQV">introductory textbooks have difficulty accurately portraying controversial topics with care or, in some cases, simply avoid covering them at all. ... <strong>readers of introductory textbooks may be unintentionally misinformed on these topics.</strong></p></blockquote>
<p id="f0osHA">The study authors — from Texas A&amp;M and Stetson universities — gathered a stack of 24 popular introductory psych textbooks and began looking for coverage of 12 contested ideas or myths in psychology.</p>
<p id="DtAEZl">The ideas — like stereotype threat, the <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1281386/">Mozart effect</a>, and whether there’s a “narcissism epidemic” among millennials — have not necessarily been disproven. Nevertheless, there are credible and noteworthy studies that cast doubt on them. The list of ideas also included some urban legends — like the one about the brain only using 10 percent of its potential at any given time, and a <a href="https://www.washingtonpost.com/lifestyle/style/her-shocking-murder-became-the-stuff-of-legend-but-everyone-got-the-story-wrong/2016/06/29/544916d8-3952-11e6-9ccd-d6005beac8b3_story.html?utm_term=.e061b368fd71">debunked</a> story about how bystanders refused to help a woman named Kitty Genovese while she was being murdered.</p>
<p id="bXTNSJ">The researchers then rated the texts on how they handled these contested ideas. The results found a troubling amount of “biased” coverage on many of the topic areas.</p>
  <figure>
  <span>
    
    <span data-original="https://cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png">
      
        <picture data-cid="site/picture_element-1702618019_4386_194722" data-cdata="{&quot;asset_id&quot;:7755711,&quot;ratio&quot;:&quot;*&quot;}">
  


  <source srcset="https://cdn.vox-cdn.com/thumbor/22IB92UP2uND8hHRTV_GBhSbSGE=/0x0:659x328/320x0/filters:focal(0x0:659x328):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 320w, https://cdn.vox-cdn.com/thumbor/C9GAApPC1ZgDubv2_GUXA7s6JPk=/0x0:659x328/520x0/filters:focal(0x0:659x328):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 520w, https://cdn.vox-cdn.com/thumbor/z2UFW7x1ysf-cAZChW_AL1CUXQ0=/0x0:659x328/720x0/filters:focal(0x0:659x328):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 720w, https://cdn.vox-cdn.com/thumbor/mWDpnqdNBy8uOPx-2HmXiyAA8J0=/0x0:659x328/920x0/filters:focal(0x0:659x328):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 920w, https://cdn.vox-cdn.com/thumbor/4RotdIzNHMM5MNJuHXAubqnymm4=/0x0:659x328/1120x0/filters:focal(0x0:659x328):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 1120w, https://cdn.vox-cdn.com/thumbor/PZcMbGJFzkkph_bfNCwG97ST4QE=/0x0:659x328/1320x0/filters:focal(0x0:659x328):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 1320w, https://cdn.vox-cdn.com/thumbor/oF5jyBY1ubtYBpbn7O3Zm-Qir4Q=/0x0:659x328/1520x0/filters:focal(0x0:659x328):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 1520w, https://cdn.vox-cdn.com/thumbor/NTNL1ARx6BTYYjiFQByeiJlBqxk=/0x0:659x328/1720x0/filters:focal(0x0:659x328):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 1720w, https://cdn.vox-cdn.com/thumbor/TewCtbhwv-xppMYjaa0jJAKJI1Q=/0x0:659x328/1920x0/filters:focal(0x0:659x328):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 1920w" sizes="(min-width: 1221px) 846px, (min-width: 880px) calc(100vw - 334px), 100vw" type="image/webp">


<img srcset="https://cdn.vox-cdn.com/thumbor/gaOxUMCAQe1tzj8IyOgrCydCUfQ=/0x0:659x328/320x0/filters:focal(0x0:659x328):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 320w, https://cdn.vox-cdn.com/thumbor/dK2KIOdTfpjdv-pRa1k_A2RdiVg=/0x0:659x328/520x0/filters:focal(0x0:659x328):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 520w, https://cdn.vox-cdn.com/thumbor/cSBbMvTiefp4I0RHxtKB3d4FHgw=/0x0:659x328/720x0/filters:focal(0x0:659x328):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 720w, https://cdn.vox-cdn.com/thumbor/IetE0tKuo7y7PZw62gNeV9S4euY=/0x0:659x328/920x0/filters:focal(0x0:659x328):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 920w, https://cdn.vox-cdn.com/thumbor/Xo5_aVSE0424wnldKjzhgIqBFo4=/0x0:659x328/1120x0/filters:focal(0x0:659x328):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 1120w, https://cdn.vox-cdn.com/thumbor/vrVFMJYphi29bU4M5rwk7C-OYHo=/0x0:659x328/1320x0/filters:focal(0x0:659x328):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 1320w, https://cdn.vox-cdn.com/thumbor/xZCV2bbquJqbXxmWOKGp2Sht5po=/0x0:659x328/1520x0/filters:focal(0x0:659x328):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 1520w, https://cdn.vox-cdn.com/thumbor/EL_nKYoV_Tnu8odrGgN6kB8nIC0=/0x0:659x328/1720x0/filters:focal(0x0:659x328):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 1720w, https://cdn.vox-cdn.com/thumbor/A9ckVnlni_tmDj5UeqVsIPsMbwY=/0x0:659x328/1920x0/filters:focal(0x0:659x328):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png 1920w" sizes="(min-width: 1221px) 846px, (min-width: 880px) calc(100vw - 334px), 100vw" alt="" loading="lazy" data-upload-width="659" width="659" height="328" src="https://cdn.vox-cdn.com/thumbor/d44-TJwUXTNeYIzIyBC4ZNyNP1o=/0x0:659x328/1200x0/filters:focal(0x0:659x328):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/7755711/Screen_Shot_2017_01_05_at_11.43.36_AM.png">

</picture>

      
    </span>
    
  </span>
  
</figure>

<p id="IGYwOa">But why wouldn’t these textbooks include more doubt? Replication, after all, is a cornerstone of any science. </p>
<p id="chk6i9">One idea is that textbooks, in the pursuit of covering a wide range of topics, aren’t meant to be authoritative on these individual controversies. But something else might be going on. The study authors suggest these textbook authors are trying to “oversell” psychology as a discipline, to get more undergraduates to study it full time. (I have to admit that it might have worked on me back when I was an undeclared undergraduate.) </p>
<p id="mAobmd">There are some caveats to mention with the study: One is that the 12 topics the authors chose to scrutinize are completely arbitrary. “And many other potential issues were left out of our analysis,” they note. Also, the textbooks included were printed in the spring of 2012; it’s possible they have been updated since then. </p>
<p id="gWz0Tl">Recently, I asked <a href="https://twitter.com/B_resnick/status/1006197663908487168">on Twitter</a> how intro psychology professors deal with inconsistencies in their textbooks. Their answers were simple. Some say they decided to get rid of textbooks (which save students money) and focus on teaching individual articles. Others have another solution that’s just as simple: “You point out the wrong, outdated, and less-than-replicable sections,” <a href="https://twitter.com/lakens">Daniël Lakens</a>, a professor at Eindhoven University of Technology in the Netherlands, said. He offered a useful example of one of the slides he uses in class.</p>

<p id="TOyfMv">Anecdotally, Illinois State University professor Joe Hilgard said he thinks his students appreciate “the ‘cutting-edge’ feeling from knowing something that the textbook didn’t.” (Also, who really, earnestly reads the textbook in an introductory college course?) </p>
<div id="IJeBSP">
<blockquote data-conversation="none">
<p lang="en" dir="ltr">I tried to frame things as four steps:<br>1) here's the big idea <br>2) here's the famous study and how it illustrates<br>3) here are the damning criticisms<br>4) here's what you can do as scholars to figure out what you believe / make a contribution to the literature</p>— Joe Hilgard, that psych prof we all know and love. (@JoeHilgard) <a href="https://twitter.com/JoeHilgard/status/1006199028168712192?ref_src=twsrc%5Etfw">June 11, 2018</a>
</blockquote>

</div>
<p id="3QgI6I">And it seems this type of teaching is catching on. A (not perfectly representative) <a href="https://mfr.osf.io/render?url=https://osf.io/va7jg/?action=download%26mode=render">recent survey of</a> 262 psychology professors found more than half said replication issues <a href="https://mfr.osf.io/render?url=https://osf.io/va7jg/?action=download%26mode=render">impacted their teaching</a>. On the other hand, 40 percent said they hadn’t. So whether students are exposed to the recent reckoning is all up to the teachers they have.  </p>
<p id="lpA4to">If it’s true that textbooks and teachers are still neglecting to cover replication issues, then I’d argue they are actually underselling the science. To teach the “replication crisis” is to teach students that science strives to be self-correcting. It would instill in them the value that science ought to be reproducible.</p>
<p id="xCWDnp">Understanding human behavior is a hard problem. Finding out the answers shouldn’t be easy. If anything, that should give students more motivation to become the generation of scientists who get it right.</p>
<p id="osnF3b">“Textbooks may be missing an opportunity for myth busting,” the <em>Current Psychology</em> study’s authors write. That’s, ideally, what young scientist ought to learn: how to bust myths and find the truth. </p>
<h3 id="bqLVL4">Further reading: Psychology’s “replication crisis” </h3>
<ul>
<li id="OsOr3Q">
<a href="https://www.vox.com/2016/3/14/11219446/psychology-replication-crisis">The replication crisis, explained. </a>Psychology is currently undergoing a painful period of introspection. It will emerge stronger than before.</li>
<li id="TK1JOq">The “marshmallow test” said patience was a key to success.<a href="https://www.vox.com/science-and-health/2018/6/6/17413000/marshmallow-test-replication-mischel-psychology"> A new replication tells us s’more.</a>
</li>
<li id="eNgm9z">The 7 biggest problems facing science, <a href="https://www.vox.com/2016/7/14/12016710/science-challeges-research-funding-peer-review-process">according to 270 scientists</a>
</li>
<li id="CQH98V">What a nerdy debate about p-values shows about science — <a href="https://www.vox.com/science-and-health/2017/7/31/16021654/p-values-statistical-significance-redefine-0005">and how to fix it</a>
</li>
<li id="4jKvOz">Science is often flawed. <a href="https://www.vox.com/2015/5/13/8591837/how-science-is-broken">It’s time we embraced that.</a>
</li>
</ul>

  <div data-cid="site/article_footer-1702618019_247_194723" data-cdata="{&quot;base_type&quot;:&quot;Entry&quot;,&quot;id&quot;:17213159,&quot;timestamp&quot;:1528914601,&quot;published_timestamp&quot;:1528914601,&quot;show_published_and_updated_timestamps&quot;:false,&quot;title&quot;:&quot;The Stanford Prison Experiment was massively influential. We just learned it was a fraud.&quot;,&quot;type&quot;:&quot;Article&quot;,&quot;url&quot;:&quot;https://www.vox.com/2018/6/13/17449118/stanford-prison-experiment-fraud-psychology-replication&quot;,&quot;entry_layout&quot;:{&quot;key&quot;:&quot;unison_standard&quot;,&quot;layout&quot;:&quot;unison_main&quot;,&quot;template&quot;:&quot;standard&quot;},&quot;additional_byline&quot;:null,&quot;authors&quot;:[{&quot;id&quot;:3246711,&quot;name&quot;:&quot;Brian Resnick&quot;,&quot;url&quot;:&quot;https://www.vox.com/authors/brian-resnick&quot;,&quot;twitter_handle&quot;:&quot;B_resnick&quot;,&quot;profile_image_url&quot;:&quot;https://cdn.vox-cdn.com/thumbor/2RI8F6UrxnVcWtLHlf8I-YHLgZc=/512x512/cdn.vox-cdn.com/author_profile_images/14855/me.0.png&quot;,&quot;title&quot;:&quot;&quot;,&quot;email&quot;:&quot;brian@vox.com&quot;,&quot;short_author_bio&quot;:&quot;is Vox’s science and health editor, and is the co-creator of Unexplainable, Vox's podcast about unanswered questions in science. Previously, Brian was a reporter at Vox and at National Journal.&quot;}],&quot;byline_enabled&quot;:true,&quot;byline_credit_text&quot;:&quot;By&quot;,&quot;byline_serial_comma_enabled&quot;:true,&quot;comment_count&quot;:0,&quot;comments_enabled&quot;:false,&quot;legacy_comments_enabled&quot;:false,&quot;coral_comments_enabled&quot;:false,&quot;coral_comment_counts_enabled&quot;:false,&quot;commerce_disclosure&quot;:null,&quot;community_name&quot;:&quot;Vox&quot;,&quot;community_url&quot;:&quot;https://www.vox.com/&quot;,&quot;community_logo&quot;:&quot;\r\n<svg width=\&quot;386px\&quot; height=\&quot;385px\&quot; viewBox=\&quot;0 0 386 385\&quot; version=\&quot;1.1\&quot; xmlns=\&quot;http://www.w3.org/2000/svg\&quot; xmlns:xlink=\&quot;http://www.w3.org/1999/xlink\&quot; >\r\n    \r\n    <title>vox-mark</title>\r\n    \r\n    <defs></defs>\r\n    <g id=\&quot;Page-1\&quot; stroke=\&quot;none\&quot; stroke-width=\&quot;1\&quot; fill=\&quot;none\&quot; fill-rule=\&quot;evenodd\&quot; >\r\n        <path d=\&quot;M239.811,0 L238.424,6 L259.374,6 C278.011,6 292.908,17.38 292.908,43.002 C292.908,56.967 287.784,75.469 276.598,96.888 L182.689,305.687 L159.283,35.693 C159.283,13.809 168.134,6 191.88,6 L205.854,6 L207.247,0 L1.409,0 L0,6 L13.049,6 C28.88,6 35.863,15.885 37.264,34.514 L73.611,385 L160.221,385 L304.525,79.217 C328.749,31.719 349.237,6 372.525,6 L384.162,6 L385.557,0 L239.811,0\&quot; id=\&quot;vox-mark\&quot; fill=\&quot;#444745\&quot; ></path>\r\n    </g>\r\n</svg>&quot;,&quot;cross_community&quot;:false,&quot;internal_groups&quot;:[{&quot;base_type&quot;:&quot;EntryGroup&quot;,&quot;id&quot;:42340,&quot;timestamp&quot;:1702590230,&quot;title&quot;:&quot;The Latest&quot;,&quot;type&quot;:&quot;SiteGroup&quot;,&quot;url&quot;:&quot;&quot;,&quot;slug&quot;:&quot;latest-news&quot;,&quot;community_logo&quot;:&quot;\r\n<svg width=\&quot;386px\&quot; height=\&quot;385px\&quot; viewBox=\&quot;0 0 386 385\&quot; version=\&quot;1.1\&quot; xmlns=\&quot;http://www.w3.org/2000/svg\&quot; xmlns:xlink=\&quot;http://www.w3.org/1999/xlink\&quot; >\r\n    \r\n    <title>vox-mark</title>\r\n    \r\n    <defs></defs>\r\n    <g id=\&quot;Page-1\&quot; stroke=\&quot;none\&quot; stroke-width=\&quot;1\&quot; fill=\&quot;none\&quot; fill-rule=\&quot;evenodd\&quot; >\r\n        <path d=\&quot;M239.811,0 L238.424,6 L259.374,6 C278.011,6 292.908,17.38 292.908,43.002 C292.908,56.967 287.784,75.469 276.598,96.888 L182.689,305.687 L159.283,35.693 C159.283,13.809 168.134,6 191.88,6 L205.854,6 L207.247,0 L1.409,0 L0,6 L13.049,6 C28.88,6 35.863,15.885 37.264,34.514 L73.611,385 L160.221,385 L304.525,79.217 C328.749,31.719 349.237,6 372.525,6 L384.162,6 L385.557,0 L239.811,0\&quot; id=\&quot;vox-mark\&quot; fill=\&quot;#444745\&quot; ></path>\r\n    </g>\r\n</svg>&quot;,&quot;community_name&quot;:&quot;Vox&quot;,&quot;community_url&quot;:&quot;https://www.vox.com/&quot;,&quot;cross_community&quot;:false,&quot;entry_count&quot;:23070,&quot;always_show&quot;:false,&quot;description&quot;:&quot;&quot;,&quot;disclosure&quot;:&quot;&quot;,&quot;cover_image_url&quot;:&quot;&quot;,&quot;cover_image&quot;:null,&quot;title_image_url&quot;:&quot;&quot;,&quot;intro_image&quot;:null,&quot;four_up_see_more_text&quot;:&quot;View All&quot;}],&quot;image&quot;:{&quot;ratio&quot;:&quot;*&quot;,&quot;original_url&quot;:&quot;https://cdn.vox-cdn.com/uploads/chorus_image/image/60051555/GettyImages_2020760.0.jpg&quot;,&quot;network&quot;:&quot;unison&quot;,&quot;bgcolor&quot;:&quot;white&quot;,&quot;pinterest_enabled&quot;:false,&quot;caption&quot;:&quot;Rorschach tests are another old mainstay of psychological science that don’t<a href=\&quot;http://moemesto.ru/rorschach_club/file/895917/download/56-3-2000-5%252520Rorschahiana4.pdf\&quot; target=\&quot;_blank\&quot;> actually reveal much about a person. </a>&quot;,&quot;credit&quot;:&quot;Adapted from <a href=\&quot;https://www.gettyimages.com/search/photographer?family=editorial&amp;amp;photographer=Lambert\&quot; target=\&quot;_blank\&quot;>Lambert</a>/Getty Creative Images&quot;,&quot;focal_area&quot;:{&quot;top_left_x&quot;:1324,&quot;top_left_y&quot;:941,&quot;bottom_right_x&quot;:1828,&quot;bottom_right_y&quot;:1445},&quot;bounds&quot;:[0,0,3152,2385],&quot;uploaded_size&quot;:{&quot;width&quot;:3152,&quot;height&quot;:2385},&quot;focal_point&quot;:null,&quot;image_id&quot;:60051555,&quot;alt_text&quot;:&quot;Rorschach test&amp;nbsp;&quot;},&quot;hub_image&quot;:{&quot;ratio&quot;:&quot;*&quot;,&quot;original_url&quot;:&quot;https://cdn.vox-cdn.com/uploads/chorus_image/image/60051555/GettyImages_2020760.0.jpg&quot;,&quot;network&quot;:&quot;unison&quot;,&quot;bgcolor&quot;:&quot;white&quot;,&quot;pinterest_enabled&quot;:false,&quot;caption&quot;:&quot;Rorschach tests are another old mainstay of psychological science that don’t<a href=\&quot;http://moemesto.ru/rorschach_club/file/895917/download/56-3-2000-5%252520Rorschahiana4.pdf\&quot; target=\&quot;_blank\&quot;> actually reveal much about a person. </a>&quot;,&quot;credit&quot;:&quot;Adapted from <a href=\&quot;https://www.gettyimages.com/search/photographer?family=editorial&amp;amp;photographer=Lambert\&quot; target=\&quot;_blank\&quot;>Lambert</a>/Getty Creative Images&quot;,&quot;focal_area&quot;:{&quot;top_left_x&quot;:1324,&quot;top_left_y&quot;:941,&quot;bottom_right_x&quot;:1828,&quot;bottom_right_y&quot;:1445},&quot;bounds&quot;:[0,0,3152,2385],&quot;uploaded_size&quot;:{&quot;width&quot;:3152,&quot;height&quot;:2385},&quot;focal_point&quot;:null,&quot;image_id&quot;:60051555,&quot;alt_text&quot;:&quot;Rorschach test&amp;nbsp;&quot;},&quot;lede_image&quot;:{&quot;ratio&quot;:&quot;*&quot;,&quot;original_url&quot;:&quot;https://cdn.vox-cdn.com/uploads/chorus_image/image/60051557/GettyImages_2020760.0.jpg&quot;,&quot;network&quot;:&quot;unison&quot;,&quot;bgcolor&quot;:&quot;white&quot;,&quot;pinterest_enabled&quot;:false,&quot;caption&quot;:&quot;Rorschach tests are another old mainstay of psychological science that don’t<a href=\&quot;http://moemesto.ru/rorschach_club/file/895917/download/56-3-2000-5%252520Rorschahiana4.pdf\&quot; target=\&quot;_blank\&quot;> actually reveal much about a person. </a>&quot;,&quot;credit&quot;:&quot;Adapted from <a href=\&quot;https://www.gettyimages.com/search/photographer?family=editorial&amp;amp;photographer=Lambert\&quot; target=\&quot;_blank\&quot;>Lambert</a>/Getty Creative Images&quot;,&quot;focal_area&quot;:{&quot;top_left_x&quot;:1324,&quot;top_left_y&quot;:941,&quot;bottom_right_x&quot;:1828,&quot;bottom_right_y&quot;:1445},&quot;bounds&quot;:[0,0,3152,2385],&quot;uploaded_size&quot;:{&quot;width&quot;:3152,&quot;height&quot;:2385},&quot;focal_point&quot;:null,&quot;image_id&quot;:60051557,&quot;alt_text&quot;:&quot;Rorschach test&amp;nbsp;&quot;},&quot;group_cover_image&quot;:null,&quot;picture_standard_lead_image&quot;:{&quot;ratio&quot;:&quot;*&quot;,&quot;original_url&quot;:&quot;https://cdn.vox-cdn.com/uploads/chorus_image/image/60051557/GettyImages_2020760.0.jpg&quot;,&quot;network&quot;:&quot;unison&quot;,&quot;bgcolor&quot;:&quot;white&quot;,&quot;pinterest_enabled&quot;:false,&quot;caption&quot;:&quot;Rorschach tests are another old mainstay of psychological science that don’t<a href=\&quot;http://moemesto.ru/rorschach_club/file/895917/download/56-3-2000-5%252520Rorschahiana4.pdf\&quot; target=\&quot;_blank\&quot;> actually reveal much about a person. </a>&quot;,&quot;credit&quot;:&quot;Adapted from <a href=\&quot;https://www.gettyimages.com/search/photographer?family=editorial&amp;amp;photographer=Lambert\&quot; target=\&quot;_blank\&quot;>Lambert</a>/Getty Creative Images&quot;,&quot;focal_area&quot;:{&quot;top_left_x&quot;:1324,&quot;top_left_y&quot;:941,&quot;bottom_right_x&quot;:1828,&quot;bottom_right_y&quot;:1445},&quot;bounds&quot;:[0,0,3152,2385],&quot;uploaded_size&quot;:{&quot;width&quot;:3152,&quot;height&quot;:2385},&quot;focal_point&quot;:null,&quot;image_id&quot;:60051557,&quot;alt_text&quot;:&quot;Rorschach test&amp;nbsp;&quot;,&quot;picture_element&quot;:{&quot;loading&quot;:&quot;eager&quot;,&quot;html&quot;:{},&quot;alt&quot;:&quot;Rorschach test&amp;nbsp;&quot;,&quot;default&quot;:{&quot;srcset&quot;:&quot;https://cdn.vox-cdn.com/thumbor/ygs06BkH-PtXfEZKrI3Vnb8Mxhw=/0x0:3152x2385/320x240/filters:focal(1324x941:1828x1445)/cdn.vox-cdn.com/uploads/chorus_image/image/60051557/GettyImages_2020760.0.jpg 320w, https://cdn.vox-cdn.com/thumbor/mSMclNeJBOQR26DicD7MnWmHIJM=/0x0:3152x2385/620x465/filters:focal(1324x941:1828x1445)/cdn.vox-cdn.com/uploads/chorus_image/image/60051557/GettyImages_2020760.0.jpg 620w, https://cdn.vox-cdn.com/thumbor/_J5-GxCpJ9oPzHbfsT-4NalWGhY=/0x0:3152x2385/920x690/filters:focal(1324x941:1828x1445)/cdn.vox-cdn.com/uploads/chorus_image/image/60051557/GettyImages_2020760.0.jpg 920w, https://cdn.vox-cdn.com/thumbor/hI96TrovXGPMk3j62POOoUGAcc8=/0x0:3152x2385/1220x915/filters:focal(1324x941:1828x1445)/cdn.vox-cdn.com/uploads/chorus_image/image/60051557/GettyImages_2020760.0.jpg 1220w, https://cdn.vox-cdn.com/thumbor/OGg0fNJHVrgth0Ykw5KP-5Xxfyw=/0x0:3152x2385/1520x1140/filters:focal(1324x941:1828x1445)/cdn.vox-cdn.com/uploads/chorus_image/image/60051557/GettyImages_2020760.0.jpg 1520w&quot;,&quot;webp_srcset&quot;:&quot;https://cdn.vox-cdn.com/thumbor/m3TLEE773U1JSRvjN4_dW8KXwQ4=/0x0:3152x2385/320x240/filters:focal(1324x941:1828x1445):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/60051557/GettyImages_2020760.0.jpg 320w, https://cdn.vox-cdn.com/thumbor/KToq0IzmS53xV59iHiMTJ-_I2nY=/0x0:3152x2385/620x465/filters:focal(1324x941:1828x1445):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/60051557/GettyImages_2020760.0.jpg 620w, https://cdn.vox-cdn.com/thumbor/hHXY4ARoDa7rji_PgSR246ayoZg=/0x0:3152x2385/920x690/filters:focal(1324x941:1828x1445):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/60051557/GettyImages_2020760.0.jpg 920w, https://cdn.vox-cdn.com/thumbor/3ebasHvwCRUBu-Qgy2OOX0uBe2I=/0x0:3152x2385/1220x915/filters:focal(1324x941:1828x1445):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/60051557/GettyImages_2020760.0.jpg 1220w, https://cdn.vox-cdn.com/thumbor/QBmXdYMb2ciWIrgFc36cHyJKpxE=/0x0:3152x2385/1520x1140/filters:focal(1324x941:1828x1445):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/60051557/GettyImages_2020760.0.jpg 1520w&quot;,&quot;media&quot;:null,&quot;sizes&quot;:&quot;(min-width: 809px) 485px, (min-width: 600px) 60vw, 100vw&quot;,&quot;fallback&quot;:&quot;https://cdn.vox-cdn.com/thumbor/b_AdjXVeuKlUsIUqGT39gIZvgXc=/0x0:3152x2385/1200x900/filters:focal(1324x941:1828x1445)/cdn.vox-cdn.com/uploads/chorus_image/image/60051557/GettyImages_2020760.0.jpg&quot;},&quot;art_directed&quot;:[]}},&quot;image_is_placeholder&quot;:false,&quot;image_is_hidden&quot;:false,&quot;network&quot;:&quot;vox&quot;,&quot;omits_labels&quot;:false,&quot;optimizable&quot;:false,&quot;promo_headline&quot;:&quot;The Stanford Prison Experiment was massively influential. We just learned it was a fraud.&quot;,&quot;recommended_count&quot;:0,&quot;recs_enabled&quot;:false,&quot;slug&quot;:&quot;2018/6/13/17449118/stanford-prison-experiment-fraud-psychology-replication&quot;,&quot;dek&quot;:&quot;The most famous psychological studies are often wrong, fraudulent, or outdated. Textbooks need to catch up.&amp;nbsp;&quot;,&quot;homepage_title&quot;:&quot;The Stanford Prison Experiment was massively influential. We just learned it was a fraud.&quot;,&quot;homepage_description&quot;:&quot;The most famous psychological studies are often wrong, fraudulent, or outdated. Textbooks need to catch up.&quot;,&quot;show_homepage_description&quot;:false,&quot;title_display&quot;:&quot;The Stanford Prison Experiment was massively influential. We just learned it was a fraud.&quot;,&quot;pull_quote&quot;:null,&quot;voxcreative&quot;:false,&quot;show_entry_time&quot;:true,&quot;show_dates&quot;:true,&quot;paywalled_content&quot;:false,&quot;paywalled_content_box_logo_url&quot;:&quot;&quot;,&quot;paywalled_content_page_logo_url&quot;:&quot;&quot;,&quot;paywalled_content_main_url&quot;:&quot;&quot;,&quot;article_footer_body&quot;:&quot;Readers rely on Vox for clear, nuanced coverage that not only illuminates the issues, but poses solutions, too. And we rely on help from our readers: Advertising and grants cover the majority of our costs, but we count on contributions to help us close the gaps in our budget. In fact, we’re looking to reach 95,000 individual contributions before the end of the year. <a href=\&quot;http://vox.com/pages/support-now?itm_campaign=eoy-2023&amp;itm_medium=site&amp;itm_source=footer\r\n\&quot;>Will you make the next contribution right now?</a> Our average gift is just $20 — and it goes a long way in helping us keep our work free. Vox is here to help everyone understand what’s shaping the world — not just the people who can afford to pay for a subscription. We believe that’s an important part of building a more equal society. <a href=\&quot;http://vox.com/pages/support-now?itm_campaign=eoy-2023&amp;itm_medium=site&amp;itm_source=footer\r\n\&quot;>Join that mission by making a contribution today.</a> \r\n&quot;,&quot;article_footer_header&quot;:&quot;<a href=\&quot;http://vox.com/pages/support-now?itm_campaign=eoy-2023&amp;itm_medium=site&amp;itm_source=footer\r\n\&quot;>Contributions are a key part of the future of Vox</a>&quot;,&quot;use_article_footer&quot;:true,&quot;article_footer_cta_annual_plans&quot;:&quot;{\r\n  \&quot;default_plan\&quot;: 1,\r\n  \&quot;plans\&quot;: [\r\n    {\r\n      \&quot;amount\&quot;: 50,\r\n      \&quot;plan_id\&quot;: 99546\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 100,\r\n      \&quot;plan_id\&quot;: 99547\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 150,\r\n      \&quot;plan_id\&quot;: 99548\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 200,\r\n      \&quot;plan_id\&quot;: 99549\r\n    }\r\n  ]\r\n}&quot;,&quot;article_footer_cta_button_annual_copy&quot;:&quot;year&quot;,&quot;article_footer_cta_button_copy&quot;:&quot;Yes, I'll give&quot;,&quot;article_footer_cta_button_monthly_copy&quot;:&quot;month&quot;,&quot;article_footer_cta_default_frequency&quot;:&quot;monthly&quot;,&quot;article_footer_cta_monthly_plans&quot;:&quot;{\r\n  \&quot;default_plan\&quot;: 0,\r\n  \&quot;plans\&quot;: [\r\n    {\r\n      \&quot;amount\&quot;: 5,\r\n      \&quot;plan_id\&quot;: 99543\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 10,\r\n      \&quot;plan_id\&quot;: 99544\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 25,\r\n      \&quot;plan_id\&quot;: 99545\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 50,\r\n      \&quot;plan_id\&quot;: 46947\r\n    }\r\n  ]\r\n}&quot;,&quot;article_footer_cta_once_plans&quot;:&quot;{\r\n  \&quot;default_plan\&quot;: 0,\r\n  \&quot;plans\&quot;: [\r\n    {\r\n      \&quot;amount\&quot;: 20,\r\n      \&quot;plan_id\&quot;: 69278\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 50,\r\n      \&quot;plan_id\&quot;: 48880\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 100,\r\n      \&quot;plan_id\&quot;: 46607\r\n    },\r\n    {\r\n      \&quot;amount\&quot;: 250,\r\n      \&quot;plan_id\&quot;: 46946\r\n    }\r\n  ]\r\n}&quot;,&quot;use_article_footer_cta_read_counter&quot;:true,&quot;use_article_footer_cta&quot;:true,&quot;groups&quot;:[{&quot;base_type&quot;:&quot;EntryGroup&quot;,&quot;id&quot;:30772,&quot;timestamp&quot;:1702555472,&quot;title&quot;:&quot;Science&quot;,&quot;type&quot;:&quot;SiteGroup&quot;,&quot;url&quot;:&quot;https://www.vox.com/science&quot;,&quot;slug&quot;:&quot;science&quot;,&quot;community_logo&quot;:&quot;\r\n<svg width=\&quot;386px\&quot; height=\&quot;385px\&quot; viewBox=\&quot;0 0 386 385\&quot; version=\&quot;1.1\&quot; xmlns=\&quot;http://www.w3.org/2000/svg\&quot; xmlns:xlink=\&quot;http://www.w3.org/1999/xlink\&quot; >\r\n    \r\n    <title>vox-mark</title>\r\n    \r\n    <defs></defs>\r\n    <g id=\&quot;Page-1\&quot; stroke=\&quot;none\&quot; stroke-width=\&quot;1\&quot; fill=\&quot;none\&quot; fill-rule=\&quot;evenodd\&quot; >\r\n        <path d=\&quot;M239.811,0 L238.424,6 L259.374,6 C278.011,6 292.908,17.38 292.908,43.002 C292.908,56.967 287.784,75.469 276.598,96.888 L182.689,305.687 L159.283,35.693 C159.283,13.809 168.134,6 191.88,6 L205.854,6 L207.247,0 L1.409,0 L0,6 L13.049,6 C28.88,6 35.863,15.885 37.264,34.514 L73.611,385 L160.221,385 L304.525,79.217 C328.749,31.719 349.237,6 372.525,6 L384.162,6 L385.557,0 L239.811,0\&quot; id=\&quot;vox-mark\&quot; fill=\&quot;#444745\&quot; ></path>\r\n    </g>\r\n</svg>&quot;,&quot;community_name&quot;:&quot;Vox&quot;,&quot;community_url&quot;:&quot;https://www.vox.com/&quot;,&quot;cross_community&quot;:false,&quot;entry_count&quot;:5751,&quot;always_show&quot;:false,&quot;description&quot;:&quot;News and updates from the science team. Topics include genetics, infectious disease, psychology, and more.&quot;,&quot;disclosure&quot;:&quot;&quot;,&quot;cover_image_url&quot;:&quot;&quot;,&quot;cover_image&quot;:null,&quot;title_image_url&quot;:&quot;&quot;,&quot;intro_image&quot;:null,&quot;four_up_see_more_text&quot;:&quot;View All&quot;,&quot;primary&quot;:true},{&quot;base_type&quot;:&quot;EntryGroup&quot;,&quot;id&quot;:24365,&quot;timestamp&quot;:1702385104,&quot;title&quot;:&quot;Explainers&quot;,&quot;type&quot;:&quot;SiteGroup&quot;,&quot;url&quot;:&quot;https://www.vox.com/explainers&quot;,&quot;slug&quot;:&quot;explainers&quot;,&quot;community_logo&quot;:&quot;\r\n<svg width=\&quot;386px\&quot; height=\&quot;385px\&quot; viewBox=\&quot;0 0 386 385\&quot; version=\&quot;1.1\&quot; xmlns=\&quot;http://www.w3.org/2000/svg\&quot; xmlns:xlink=\&quot;http://www.w3.org/1999/xlink\&quot; >\r\n    \r\n    <title>vox-mark</title>\r\n    \r\n    <defs></defs>\r\n    <g id=\&quot;Page-1\&quot; stroke=\&quot;none\&quot; stroke-width=\&quot;1\&quot; fill=\&quot;none\&quot; fill-rule=\&quot;evenodd\&quot; >\r\n        <path d=\&quot;M239.811,0 L238.424,6 L259.374,6 C278.011,6 292.908,17.38 292.908,43.002 C292.908,56.967 287.784,75.469 276.598,96.888 L182.689,305.687 L159.283,35.693 C159.283,13.809 168.134,6 191.88,6 L205.854,6 L207.247,0 L1.409,0 L0,6 L13.049,6 C28.88,6 35.863,15.885 37.264,34.514 L73.611,385 L160.221,385 L304.525,79.217 C328.749,31.719 349.237,6 372.525,6 L384.162,6 L385.557,0 L239.811,0\&quot; id=\&quot;vox-mark\&quot; fill=\&quot;#444745\&quot; ></path>\r\n    </g>\r\n</svg>&quot;,&quot;community_name&quot;:&quot;Vox&quot;,&quot;community_url&quot;:&quot;https://www.vox.com/&quot;,&quot;cross_community&quot;:false,&quot;entry_count&quot;:5143,&quot;always_show&quot;:false,&quot;description&quot;:&quot;We live in a world of too much information and too little context. Too much noise and too little insight. That's where Vox's explainers come in.&quot;,&quot;disclosure&quot;:&quot;&quot;,&quot;cover_image_url&quot;:&quot;&quot;,&quot;cover_image&quot;:null,&quot;title_image_url&quot;:&quot;&quot;,&quot;intro_image&quot;:null,&quot;four_up_see_more_text&quot;:&quot;View All&quot;,&quot;primary&quot;:false},{&quot;base_type&quot;:&quot;EntryGroup&quot;,&quot;id&quot;:65787,&quot;timestamp&quot;:1701432004,&quot;title&quot;:&quot;Psychology&quot;,&quot;type&quot;:&quot;SiteGroup&quot;,&quot;url&quot;:&quot;https://www.vox.com/psychology&quot;,&quot;slug&quot;:&quot;psychology&quot;,&quot;community_logo&quot;:&quot;\r\n<svg width=\&quot;386px\&quot; height=\&quot;385px\&quot; viewBox=\&quot;0 0 386 385\&quot; version=\&quot;1.1\&quot; xmlns=\&quot;http://www.w3.org/2000/svg\&quot; xmlns:xlink=\&quot;http://www.w3.org/1999/xlink\&quot; >\r\n    \r\n    <title>vox-mark</title>\r\n    \r\n    <defs></defs>\r\n    <g id=\&quot;Page-1\&quot; stroke=\&quot;none\&quot; stroke-width=\&quot;1\&quot; fill=\&quot;none\&quot; fill-rule=\&quot;evenodd\&quot; >\r\n        <path d=\&quot;M239.811,0 L238.424,6 L259.374,6 C278.011,6 292.908,17.38 292.908,43.002 C292.908,56.967 287.784,75.469 276.598,96.888 L182.689,305.687 L159.283,35.693 C159.283,13.809 168.134,6 191.88,6 L205.854,6 L207.247,0 L1.409,0 L0,6 L13.049,6 C28.88,6 35.863,15.885 37.264,34.514 L73.611,385 L160.221,385 L304.525,79.217 C328.749,31.719 349.237,6 372.525,6 L384.162,6 L385.557,0 L239.811,0\&quot; id=\&quot;vox-mark\&quot; fill=\&quot;#444745\&quot; ></path>\r\n    </g>\r\n</svg>&quot;,&quot;community_name&quot;:&quot;Vox&quot;,&quot;community_url&quot;:&quot;https://www.vox.com/&quot;,&quot;cross_community&quot;:false,&quot;entry_count&quot;:134,&quot;always_show&quot;:false,&quot;description&quot;:&quot;Human behavior is fascinating — and we still have a lot to learn. Keep up with news and updates from the field of psychology. &quot;,&quot;disclosure&quot;:&quot;&quot;,&quot;cover_image_url&quot;:&quot;&quot;,&quot;cover_image&quot;:null,&quot;title_image_url&quot;:&quot;&quot;,&quot;intro_image&quot;:null,&quot;four_up_see_more_text&quot;:&quot;View All&quot;,&quot;primary&quot;:false},{&quot;base_type&quot;:&quot;EntryGroup&quot;,&quot;id&quot;:112369,&quot;timestamp&quot;:1702565101,&quot;title&quot;:&quot;Health&quot;,&quot;type&quot;:&quot;SiteGroup&quot;,&quot;url&quot;:&quot;https://www.vox.com/health&quot;,&quot;slug&quot;:&quot;health&quot;,&quot;community_logo&quot;:&quot;\r\n<svg width=\&quot;386px\&quot; height=\&quot;385px\&quot; viewBox=\&quot;0 0 386 385\&quot; version=\&quot;1.1\&quot; xmlns=\&quot;http://www.w3.org/2000/svg\&quot; xmlns:xlink=\&quot;http://www.w3.org/1999/xlink\&quot; >\r\n    \r\n    <title>vox-mark</title>\r\n    \r\n    <defs></defs>\r\n    <g id=\&quot;Page-1\&quot; stroke=\&quot;none\&quot; stroke-width=\&quot;1\&quot; fill=\&quot;none\&quot; fill-rule=\&quot;evenodd\&quot; >\r\n        <path d=\&quot;M239.811,0 L238.424,6 L259.374,6 C278.011,6 292.908,17.38 292.908,43.002 C292.908,56.967 287.784,75.469 276.598,96.888 L182.689,305.687 L159.283,35.693 C159.283,13.809 168.134,6 191.88,6 L205.854,6 L207.247,0 L1.409,0 L0,6 L13.049,6 C28.88,6 35.863,15.885 37.264,34.514 L73.611,385 L160.221,385 L304.525,79.217 C328.749,31.719 349.237,6 372.525,6 L384.162,6 L385.557,0 L239.811,0\&quot; id=\&quot;vox-mark\&quot; fill=\&quot;#444745\&quot; ></path>\r\n    </g>\r\n</svg>&quot;,&quot;community_name&quot;:&quot;Vox&quot;,&quot;community_url&quot;:&quot;https://www.vox.com/&quot;,&quot;cross_community&quot;:false,&quot;entry_count&quot;:622,&quot;always_show&quot;:false,&quot;description&quot;:&quot;Vox's coverage of all things health, including personal health, public health, mental health, and more. &quot;,&quot;disclosure&quot;:&quot;&quot;,&quot;cover_image_url&quot;:&quot;&quot;,&quot;cover_image&quot;:null,&quot;title_image_url&quot;:&quot;&quot;,&quot;intro_image&quot;:null,&quot;four_up_see_more_text&quot;:&quot;View All&quot;,&quot;primary&quot;:false}],&quot;featured_placeable&quot;:false,&quot;video_placeable&quot;:false,&quot;disclaimer&quot;:null,&quot;volume_placement&quot;:&quot;lede&quot;,&quot;video_autoplay&quot;:false,&quot;youtube_url&quot;:&quot;http://bit.ly/voxyoutube&quot;,&quot;facebook_video_url&quot;:&quot;&quot;,&quot;play_in_modal&quot;:true,&quot;user_preferences_for_privacy_enabled&quot;:false,&quot;show_branded_logos&quot;:true}">

  <div>
    
      
    

    
      <p><strong><a href="http://vox.com/pages/support-now?itm_campaign=eoy-2023&amp;itm_medium=site&amp;itm_source=footer%0D%0A">Contributions are a key part of the future of Vox</a></strong></p>
    

    <p>
      Readers rely on Vox for clear, nuanced coverage that not only illuminates the issues, but poses solutions, too. And we rely on help from our readers: Advertising and grants cover the majority of our costs, but we count on contributions to help us close the gaps in our budget. In fact, we’re looking to reach 95,000 individual contributions before the end of the year. <a href="http://vox.com/pages/support-now?itm_campaign=eoy-2023&amp;itm_medium=site&amp;itm_source=footer%0D%0A">Will you make the next contribution right now?</a> Our average gift is just $20 — and it goes a long way in helping us keep our work free. Vox is here to help everyone understand what’s shaping the world — not just the people who can afford to pay for a subscription. We believe that’s an important part of building a more equal society. <a href="http://vox.com/pages/support-now?itm_campaign=eoy-2023&amp;itm_medium=site&amp;itm_source=footer%0D%0A">Join that mission by making a contribution today.</a> 

    </p>
  </div> <!-- end of .left-column -->

  <div>
       <!-- end of .contribute--frequency-container -->

      <div>
        <p><label tabindex="0" role="radio" aria-checked="true">
                
                <p>
                  <span>$5</span><span>/month</span>
                </p>
              </label>
            
              <label tabindex="0" role="radio" aria-checked="true">
                
                <p>
                  <span>$10</span><span>/month</span>
                </p>
              </label>
            
              <label tabindex="0" role="radio" aria-checked="true">
                
                <p>
                  <span>$25</span><span>/month</span>
                </p>
              </label>
            
              <label tabindex="0" role="radio" aria-checked="true">
                
                <p>
                  <span>$50</span><span>/month</span>
                </p>
              </label>
            

            <label tabindex="0">
              
              <span>Other</span>
            </label>
          </p>
        </div>

        

        <a href="https://vox.memberful.com/checkout?plan=" id="contribute--submit">
          <p>
            Yes, I'll give $5<span>/month</span>
          </p>
        </a>

        <p>
          Yes, I'll give $5<span>/month</span>
        </p>

        <div>
            <p>
              <span>
                We accept credit card, Apple Pay, and
              </span>
              <span>
                Google Pay. You can also contribute via
              </span>
            </p>
            <p><a href="https://www.paypal.com/donate/?hosted_button_id=VSP4PYJX98SHL" target="_blank">
              <img src="https://cdn.vox-cdn.com/uploads/chorus_asset/file/22734206/paypal_logo.png" alt="" width="136" height="42">
            </a>
          </p></div>

      </div> <!-- end of .cta-container -->
  </div> <!-- end of .right-column -->

 <!-- end of .c-article-footer-cta -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Airdraw (247 pts)]]></title>
            <link>https://www.airdraw.io/</link>
            <guid>38647938</guid>
            <pubDate>Thu, 14 Dec 2023 22:02:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.airdraw.io/">https://www.airdraw.io/</a>, See on <a href="https://news.ycombinator.com/item?id=38647938">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Mitchell reflects as he departs HashiCorp (639 pts)]]></title>
            <link>https://www.hashicorp.com/blog/mitchell-reflects-as-he-departs-hashicorp</link>
            <guid>38647484</guid>
            <pubDate>Thu, 14 Dec 2023 21:27:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hashicorp.com/blog/mitchell-reflects-as-he-departs-hashicorp">https://www.hashicorp.com/blog/mitchell-reflects-as-he-departs-hashicorp</a>, See on <a href="https://news.ycombinator.com/item?id=38647484">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><section><p><em>Earlier this week, I sent this note to HashiCorp employees and am posting it here to let the entire HashiCorp community know about my plans:</em></p>
<p>I have some bittersweet news to share with you all today: I've decided to move on from HashiCorp, and I'll soon no longer be an employee with the company. I recently celebrated 11 years since starting HashiCorp, and as I reflect back on the last decade I couldn't have asked for a better way to spend that part of my life.</p>
<p>My departure from HashiCorp is something I’ve been thinking about and planning for a long time. Ever since founding HashiCorp, I've felt it's important to build a company where I'm not required for day-to-day operations and where other leaders can carry the torch over time. I have been very intentional about this as time went on: stepping down from being CEO in 2016, iterating over time on a culture of leadership autonomy that didn't require my involvement to make decisions, and finally <a href="https://www.hashicorp.com/blog/mitchell-s-new-role-at-hashicorp">departing the leadership team and board of directors in 2021</a>. Since then, I've had the pleasure of working where I’m happiest — as a full-time, hands-on engineer.</p>
<p>My passion as an engineer reaches beyond infrastructure and I always knew that at some point — when the company and I were ready — I'd move on and take on new, different challenges. My family recently welcomed our first child, and while reflecting during my time off I felt now was a fitting time to complete this transition. The world of cloud automation and infrastructure tooling is still ripe with opportunities and growth, but after nearly 15 years of working exclusively on tooling in this space, I'm ready to dabble in new areas.</p>
<p>While my departure from HashiCorp is exactly what I've planned for, it's still a poignant moment. Nearly my entire adult life has revolved around the company. Many of my most formative memories happened in the context of this company. There are far too many to recount here, but I'd like to highlight just a few.</p>
<p>Years before we started HashiCorp, Armon [Dadgar, HashiCorp Co-Founder and CTO] and I would talk about cloud, automation, and distributed systems incessantly. We were teenagers, and we'd playfully — not seriously — say things like, "What if one day the biggest companies used our software?" At one point, though, we took the first step and made some of our ideas into actual code. Next thing we knew, we had thousands of users. So, we took another step and started a company. A little later we took the next step and decided to raise funding. And that’s how HashiCorp became what it is today: we took many small little steps like this until we found that that playful, teenage idealism had become reality.</p></section><div><p><img alt="Mitchell and Armon in 2013" loading="lazy" width="3264" height="1736" decoding="async" data-nimg="1" srcset="https://www.hashicorp.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F2885%2F1671228946-2013-mitchell-armon.jpg&amp;w=3840&amp;q=75 1x" src="https://www.hashicorp.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F2885%2F1671228946-2013-mitchell-armon.jpg&amp;w=3840&amp;q=75"></p><p>Mitchell and Armon in 2013.</p></div><section><p>As we got going, I felt like some “firsts” were particularly significant. The <a href="https://www.hashicorp.com/blog/hashiconf-2015-wrap-up">first HashiConf in 2015</a> will always be a special memory. It was the first time that the digital world really firmly crossed over into the physical world for me, and it was hard to believe that any of it was real. I knew that our download numbers were high and I knew I interacted daily with community members online, but it's something entirely different to see hundreds of people willingly choose to physically show up. I felt incredibly proud, but it was also one of the earliest moments that I felt a real weight of responsibility. I felt the internal struggle of wanting to build, but also needing to shepherd this company Armon and I were creating. I'm so thankful to all of those early adopters and employees who joined us for that first event.</p></section><div><p><img alt="First HashiConf" loading="lazy" width="1920" height="1281" decoding="async" data-nimg="1" srcset="https://www.hashicorp.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F2885%2F1702507478-first-hashiconf-2015.jpg&amp;w=1920&amp;q=75 1x, https://www.hashicorp.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F2885%2F1702507478-first-hashiconf-2015.jpg&amp;w=3840&amp;q=75 2x" src="https://www.hashicorp.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F2885%2F1702507478-first-hashiconf-2015.jpg&amp;w=3840&amp;q=75"></p><p>The audience at HashiConf 2015.</p></div><section><p>Just a few short years later, our first internal, full-company offsite was the next major "whoa" experience for me. We had more people present than at that first HashiConf! I started this company with Armon, focused on my excitement around the technology, but moments like this taught me how important the people are, too. The people and our shared experiences are what I now look back on most fondly.</p>
<p>There are many more similarly impactful moments throughout my history with HashiCorp, and I'm so grateful for all of them. Although it may sometimes seem like some events are bigger than others, I value each experience (even the tough ones) as a necessary step toward achieving each individual milestone.</p>
<p>I've worked alongside Armon for almost 15 years (since before HashiCorp!), and worked with Dave [McJannet, HashiCorp CEO] for over 7. We led the company together up until I stepped off the leadership team in 2021. Beyond being coworkers, we’ve grown to be close friends. I continue to trust their leadership and will miss working with them dearly.</p></section><div><p><img alt="Mitchell, Dave and Armon" loading="lazy" width="1832" height="1218" decoding="async" data-nimg="1" srcset="https://www.hashicorp.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F2885%2F1702495672-screenshot-2023-12-13-at-2-26-55-pm.png&amp;w=1920&amp;q=75 1x, https://www.hashicorp.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F2885%2F1702495672-screenshot-2023-12-13-at-2-26-55-pm.png&amp;w=3840&amp;q=75 2x" src="https://www.hashicorp.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F2885%2F1702495672-screenshot-2023-12-13-at-2-26-55-pm.png&amp;w=3840&amp;q=75"></p><p>Armon, Mitchell, and Dave.</p></div><section><p>The controversial worldviews such as <em>multi-cloud</em> that we founded this company on are now mainstream and <a href="https://www.hashicorp.com/state-of-the-cloud/2021">broadly accepted</a>. The software that I helped start is used industry-wide from hobbyists to professionals at the world's largest companies. And, most recently, the <a href="https://solutionshub.epam.com/blog/post/programming-language-popularity-on-github">GitHub Octoverse report</a> found that HashiCorp Configuration Language (HCL) has once again emerged as one of the top languages used in open source projects. These are just some of the examples that show the impact, growth, and promising future HashiCorp continues to have in the industry. This is all beyond what I could've hoped for, and I'm leaving proud of the small role I played in making this happen.</p>
<p>As I said earlier, nearly my entire adult life has revolved around HashiCorp. This company has made such an impact on not just my life, but on the lives of so many, including our passionate community, our valued customers, our many close ecosystem partners, and our amazing employees. Thanks to all of you for your energy and your trust. Finally, my heartfelt wishes go out to the entire company. I will be cheering you on, grateful to have contributed to the journey of shaping HashiCorp, and excited to see what you will do next.</p>
<p>All the best.</p></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Journal of Universal Rejection (112 pts)]]></title>
            <link>https://universalrejection.org/</link>
            <guid>38647101</guid>
            <pubDate>Thu, 14 Dec 2023 20:54:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://universalrejection.org/">https://universalrejection.org/</a>, See on <a href="https://news.ycombinator.com/item?id=38647101">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wholepage">
<p><img src="https://universalrejection.org/NewBanner.png"></p>

<hr color="blue"><big><a href="#about">About the Journal</a> | <a href="#editorialboard">Editorial Board</a> | <a href="#instructions">Instructions for Authors</a> | <a href="#subscriptions">Subscriptions</a> | <a href="#archives">Archives</a> | <a href="http://www.universalrejection.org/conference/">Conference</a></big>

<hr color="blue">

<!--<P> <font color="#ED2939"><BIG><B>
SPECIAL ANNOUNCEMENT</B>:</font> <BR>The Journal of Universal Rejection is pleased to announce the <a href="http://www.universalrejection.org/conference/"> Conference of Universal Rejection</a>.
</BIG></P>-->

<big> <b>About the Journal</b><a name="about"></a> </big>

<p><big>The founding principle of the Journal of Universal Rejection (JofUR) is rejection. Universal rejection. That is to say, all submissions, regardless of quality, will be rejected. Despite that apparent drawback, here are a number of reasons you may choose to submit to the JofUR:</big></p>
<big> </big>

<blockquote>
<ul>
	<li><big>You can send your manuscript here without suffering waves of anxiety regarding the eventual fate of your submission. You know with 100% certainty that it will not be accepted for publication.</big></li>
	<li><big>There are no page-fees.</big></li>
	<li><big>You may claim to have submitted to the most prestigious journal (judged by acceptance rate).</big></li>
	<li><big>The JofUR is one-of-a-kind. Merely submitting work to it may be considered a badge of honor.</big></li>
	<li><big>You retain complete rights to your work, and are free to resubmit to other journals <i>even before our review process is complete.</i></big></li>
	<li><big>Decisions are often (though not always) rendered within hours of submission.</big></li>
</ul>
</blockquote>
<big> <b>Editorial Board</b><a name="editorialboard"></a> </big>

<p><big><b>Founder and Editor-in-Chief</b><br>
<a href="http://www.universalrejection.org/cje.html">Caleb Emmons</a>, (Mathematics and Poetry).<p>

<b>Associate Editors</b><br>
Michael Baranowski, Northern Kentucky University, USA (Political Science)<br>
Sarah Bracking, University of Manchester, UK (Political Economy)<br>
Lois A. Butcher-Poffley, Temple University, USA (Kinesiology and Sport Psychology)<br>
Michael M. Chemers, Carnegie Mellon University, USA (Theatre and Performance Studies)<br>
Eric Chicken, Florida State University, USA (Statistics)<br>
David Deane, Atlantic School of Theology, Canada (Theology and Religious Studies)<br>
Ramon P. DeGennaro, University of Tennessee at Knoxville, USA (Finance)<br>
Matt J. Duffy, Kennesaw State University, USA (Communications)<br>
Carsten Elbro, University of Copenhagen, Denmark (Linguistics)<br>
David J. Elton, Auburn University, USA (Civil Engineering)<br>
Jason Eriksen, University of Houston, USA (Pharmacology and Pharmacy)<br>
Silvia Florea, Lucian Blaga University of Sibiu, Romania (British and American Studies)<br>
Omar Ha-Redeye, Ryerson University, Canada (Law)<br>
Hanjo Hamann, Max Planck Institute Bonn, Germany (Jurisprudence)<br>
Louise Heslop, Carleton University, Canada (Business)<br>
Jeffrey Hoch, University of Toronto, Canada (Health Policy, Management and Evaluation)<br>
Manfred J. Holler, Universität Hamburg, Germany (Economics)<br>
Jeffrey Lacasse, Florida State University, USA (Social Work)<br>
Charlotte P. Lee, University of Washington, USA (Human Centered Design &amp; Engineering)<br>
Samuel R. Lucas, University of California-Berkeley, USA (Sociology)<br>
Sonia Lyris, (Short Fiction)<br>
Karl Maton, University of Sydney, Australia (Education)<br>
Alexander Maxwell, Victoria University of Wellington, New Zealand (History)<br>
Luca Moretti, University of Aberdeen, Scotland, UK (Philosophy)<br>
Axel H. E. Müller, Johannes Gutenberg University Mainz (Chemistry)<br>
Judith Ogilvie, Saint Louis University, USA (Biology)<br>
Karl M. Petruso, University of Texas at Arlington, USA (Archaeology)<br>
Maurice Preter, Columbia University, USA (Clinical Psychiatry)<br>
César A. Rodríguez-Rosari, Universität Bremen, Germany (Physics)<br>
David W. Rosenthal, Miami University, USA (Marketing)<br>
Daniele Scarpi, University of Bologna, Italy (Geology)<br>
Charles M. Shub, University of Colorado at Colorado Springs, USA (Computer Science)<br>
Christophe Tzourio, University of Bordeaux 2, France (Neurology and Epidemiology)<br>
Kip Williams, Purdue University, USA (Psychology)<br>
Ghil'ad Zuckermann, University of Adelaide, Australia (Endangered Languages, Jewish Studies)  </p></big></p>
<big> <b>Instructions for Authors</b><a name="instructions"></a> </big>

<p><big>The JofUR solicits any and all types of manuscript: poetry, prose, visual art, and research articles. You name it, we take it, and reject it. Your manuscript may be formatted however you wish. Frankly, we don't care.</big></p>
<big> </big>

<p><big>After submitting your work, the decision process varies. Often the Editor-in-Chief will reject your work out-of-hand, without even reading it! However, he might read it. Probably he'll skim. At other times your manuscript may be sent to anonymous referees. Unless they are the Editor-in-Chief's wife or graduate school buddies, it is unlikely that the referees will even understand what is going on. Rejection will follow as swiftly as a bird dropping from a great height after being struck by a stone. At other times, rejection may languish like your email buried in the Editor-in-Chief's inbox. But it will come, swift or slow, as surely as death. Rejection.</big></p>
<big> </big>

<p><big>Submissions should be emailed to <a href="mailto:j.universal.rejection@gmail.com">j.universal.rejection@gmail.com</a>. Small files only, please. Why not just send the first couple pages if it is long? If you are lucky, your eventual rejection letter will appear on the Journal's blog. Please let us know in your cover letter if you would not mind being identified, otherwise most identifying information will be redacted.</big></p>
<big> <b>Subscriptions</b><a name="subscriptions"></a> </big>

<p><big>An individual subscription may be secured for $9,999.99 per year (one issue). Institutional and library subscriptions are also available; prices will be provided upon enquiry. It is unknown whether the subscription will be delivered in print or as electronic content, because no one has yet ordered one. </big></p>
<big> </big>

<!-- Or you can donate to our cause: <form action="https://www.paypal.com/cgi-bin/webscr" method="post">
<input type="hidden" name="cmd" value="_s-xclick">
<input type="hidden" name="hosted_button_id" value="AR9MFVFK2FAGU">
<input type="image" src="https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif" border="0" name="submit" alt="PayPal - The safer, easier way to pay online!">
<img alt="" border="0" src="https://www.paypalobjects.com/en_US/i/scr/pixel.gif" width="1" height="1">
</form>
--> 
<big> <b>Archives</b><a name="archives"></a> </big>



<ul>
	<li><big>March 2009 (Vol 1, No 1) contents: </big>

	<blockquote><big><i>(empty)</i> </big></blockquote>
	</li>
	<li><big>June 2009 (Vol 1, No 2) contents: </big>
	<blockquote><big><i>(empty)</i> </big></blockquote>
	</li>
	<li><big>September 2009 (Vol 1, No 3) contents: </big>
	<blockquote><big><i>(empty)</i> </big></blockquote>
	</li>
	<li><big>December 2009 (Vol 1, No 4) contents: </big>
	<blockquote><big><i>(empty - because we were on holiday)</i> </big></blockquote>
	</li>
	<li><big>March 2010 (Vol 2, No 1) contents: </big>
	<blockquote><big><i>(empty)</i> </big></blockquote>
	</li>
	<li><big>June 2010 (Vol 2, No 2) contents: </big>
	<blockquote><big><i>(empty)</i> </big></blockquote>
	</li>
	<li><big>September 2010 (Vol 2, No 3) contents: </big>
	<blockquote><big><i>(empty)</i> </big></blockquote>
	</li>
	<li><big>December 2010 (Vol 2, No 4) contents: </big>
	<blockquote><big><i>(lost when server crashed - presumed empty) </i> </big></blockquote>
	</li>
	<li><big>March 2011 (Vol 3, No 1) contents: </big>
	<blockquote><big><i>(empty - 352 submissions rejected)</i> </big></blockquote>
	</li>
	<li><big>June 2011 (Vol 3, No 2) contents: </big>
	<blockquote><big><i>(empty)</i> </big></blockquote>
	</li>
	<li><big>2012 (Vol 4) contents: </big>
	<blockquote><big><i>(empty - switched to yearly publication)</i> </big></blockquote>
	</li>
	<li><big>2013 (Vol 5) contents: </big>
	<blockquote><big><i>(empty)</i> </big></blockquote>
	</li>
</ul>
<big> <!--
<B>News Room</B><A name="newsroom"></a>

<P>

</P>
--><br>
<big> <b>Advertisement</b><a name="advertisement"></a> </big>
<p><a href="https://www.4kdownload.com/products/product-stogram"><img src="https://universalrejection.org/212.png" alt="Instagram downloader"></a></p>
</big></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[After getting $824M in state aid, GM to cut 900 jobs at Orion Assembly (122 pts)]]></title>
            <link>https://www.michigancapitolconfidential.com/news/after-getting-824m-in-state-aid-gm-to-cut-900-jobs-at-orion-assembly</link>
            <guid>38647097</guid>
            <pubDate>Thu, 14 Dec 2023 20:54:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.michigancapitolconfidential.com/news/after-getting-824m-in-state-aid-gm-to-cut-900-jobs-at-orion-assembly">https://www.michigancapitolconfidential.com/news/after-getting-824m-in-state-aid-gm-to-cut-900-jobs-at-orion-assembly</a>, See on <a href="https://news.ycombinator.com/item?id=38647097">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article_header">
			<p><a href="https://www.michigancapitolconfidential.com/news">News Story</a></p><p data-edit-url="/articles/31604">
				
				<h2>GM CEO claimed investment would not have been possible, except for corporate welfare</h2>
		</p>

		

			
	</div><div data-field="body">
		<p>General Motors Corp. will lay off 900 workers from Orion Assembly in January 2024.</p>
<p>The company&nbsp;received&nbsp;roughly $824 million in corporate welfare from Michigan lawmakers less than two years ago, in January 2022. Taxpayers spent&nbsp;$600 million to create 4,000 jobs, up to $158 million for renewable energy projects&nbsp;(as the factory would build electric vehicles), and $66.1 million in site readiness.</p>
<p>“The Michigan Economic Development Corporation also authorized a State Education Tax abatement to be used in conjunction with the locally approved Orion Township abatement in support of the GM expansion,” the state announced at the time.</p>
<p>Orion Township threw in property tax abatements.</p>
<p>Gov. Gretchen Whitmer joined GM CEO Mary Barra in the announcement, touting the good-paying jobs the plant would create.</p>
<p>"GM's $7 billion investment in Michigan —&nbsp;the largest in their history —&nbsp;will create and retain 5,000 good-paying jobs and enable us to build on our legacy as the place that put the world on wheels," <a target="" href="https://www.michigan.gov/whitmer/news/press-releases/2022/01/25/secures-historic-7-billion-investment-by-gm-to-create-thousands-of-manufacturing-jobs-">Whitmer said</a> at the time.</p>
<p>"When it comes to investing in Michigan, GM and I have the same philosophy: 'Everybody In.' Michigan's future is bright, and I will continue working with anyone to make transformational investments in our economy, create good-paying jobs, and empower working families."</p>
<p>Barra runs the no. 21 company on the Fortune 500 list. But in her public statement, she said the Orion investment would not have been possible&nbsp;if not for corporate welfare.</p>
<p>"These important investments would not have been possible without the strong support from the governor, Michigan Legislature, Orion Township, the City of Lansing, Delta Township,” Barra said, before mentioning the United Auto Workers union.</p>
<p>UAW officials spoke of the General Motors investment&nbsp;as one that would “benefit families for decades to come.”</p>
<p>Two years later, 911 Orion Township factory workers will start the year in need of work.</p>
<h3><a target="" href="https://www.michigan.gov/leo/-/media/Project/Websites/leo/Documents/WD-DATA_PUBLIC_WARN_NOTICES4/2023/2023-10-24_WARN-Notice_GM-Lake-Orion.pdf?rev=c3d3e77c003849a08005f1276058bc6f">Read the General Motors&nbsp;WARN Notice for yourself</a></h3>
	</div><p>Michigan Capitol Confidential is the news source produced by the Mackinac Center for Public Policy. Michigan Capitol Confidential reports with a free-market news perspective.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Postfix 25 years old today (106 pts)]]></title>
            <link>https://marc.info/?l=postfix-users&amp;m=170256002601828&amp;w=2</link>
            <guid>38647053</guid>
            <pubDate>Thu, 14 Dec 2023 20:50:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://marc.info/?l=postfix-users&#x26;m=170256002601828&#x26;w=2">https://marc.info/?l=postfix-users&#x26;m=170256002601828&#x26;w=2</a>, See on <a href="https://news.ycombinator.com/item?id=38647053">Hacker News</a></p>
<div id="readability-page-1" class="page">
<pre><b>[<a href="https://marc.info/?l=postfix-users&amp;m=170255408229650&amp;w=2">prev in list</a>] [<a href="https://marc.info/?l=postfix-users&amp;m=170256090002466&amp;w=2">next in list</a>] [<span color="#c0c0c0">prev in thread</span>] [<a href="https://marc.info/?l=postfix-users&amp;m=170256090002466&amp;w=2">next in thread</a>] </b>
<b><span size="+1">
List:       <a href="https://marc.info/?l=postfix-users&amp;r=1&amp;w=2">postfix-users</a>
Subject:    <a href="https://marc.info/?t=170256019600001&amp;r=1&amp;w=2">[pfx] 25 years today</a>
From:       <a href="https://marc.info/?a=167820008700002&amp;r=1&amp;w=2">Wietse Venema via Postfix-users &lt;postfix-users () postfix ! org&gt;</a>
Date:       <a href="https://marc.info/?l=postfix-users&amp;r=1&amp;w=2&amp;b=202312">2023-12-14 13:20:26</a>
Message-ID: <a href="https://marc.info/?i=4SrXyy1HSLzJrP1%20()%20spike%20!%20porcupine%20!%20org">4SrXyy1HSLzJrP1 () spike ! porcupine ! org</a></span>
[Download RAW <a href="https://marc.info/?l=postfix-users&amp;m=170256002601828&amp;q=mbox">message</a> or <a href="https://marc.info/?l=postfix-users&amp;m=170256002601828&amp;q=raw">body</a>]</b>

As a few on this list may recall, it is 25 years ago today that the
"IBM secure mailer" had its public beta release. This was accompanied
by a nice article in the New York Times business section.

There is some literature at <a href="https://www.postfix.org/press.html" rel="nofollow">https://www.postfix.org/press.html</a> that
attests how this project accelerated open-source adoption by a very
large company.

At the time there were several efforts by people inside IBM to do
open-source projects, but it was the NY Times article that brought
open source on the radar of the CEO. He then tasked people to come
up with an open-source strategy for IBM.

As for the name Postfix, my colleagues and I had come up with
multiple names that were rejected each time (I still have some
Internet domains names from that time). We decided that this was
not going to work, released it as "IBM secure mailer", and then,
after IBM was no longer in control, changed the name to Postfix.

That was a long time ago. Postfix has evolved as the Internet has
changed. I am continuing the overhaul of this software, motivated
by people like you on this mailing list.

	Wietse
_______________________________________________
Postfix-users mailing list -- postfix-users@postfix.org
To unsubscribe send an email to postfix-users-leave@postfix.org
<b>[<a href="https://marc.info/?l=postfix-users&amp;m=170255408229650&amp;w=2">prev in list</a>] [<a href="https://marc.info/?l=postfix-users&amp;m=170256090002466&amp;w=2">next in list</a>] [<span color="#c0c0c0">prev in thread</span>] [<a href="https://marc.info/?l=postfix-users&amp;m=170256090002466&amp;w=2">next in thread</a>] </b>
</pre>
  <br><center>
    <a href="https://marc.info/?q=configure">Configure</a> | 

    <a href="https://marc.info/?q=about">About</a> |
    <a href="https://marc.info/?q=news">News</a> |
    <a href="mailto:webguy@marc.info?subject=Add%20a%20list%20to%20MARC">Add&nbsp;a&nbsp;list</a> |
    Sponsored&nbsp;by&nbsp;<a href="http://www.korelogic.com/">KoreLogic</a>
</center>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[OnnxStream: Stable Diffusion XL 1.0 Base on a Raspberry Pi Zero 2 (102 pts)]]></title>
            <link>https://github.com/vitoplantamura/OnnxStream/tree/c0cb4b3d7b419e4b10129904fbe16b850ca5d385</link>
            <guid>38646969</guid>
            <pubDate>Thu, 14 Dec 2023 20:43:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/vitoplantamura/OnnxStream/tree/c0cb4b3d7b419e4b10129904fbe16b850ca5d385">https://github.com/vitoplantamura/OnnxStream/tree/c0cb4b3d7b419e4b10129904fbe16b850ca5d385</a>, See on <a href="https://news.ycombinator.com/item?id=38646969">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h4 tabindex="-1" dir="auto">News 📣</h4>
<ul dir="auto">
<li>December 14, 2023: Added support for <strong>Stable Diffusion XL Turbo 1.0</strong>! (thanks to @AeroX2)</li>
<li>October 3, 2023: Added support for <strong>Stable Diffusion XL 1.0 Base</strong>!</li>
</ul>
<h4 tabindex="-1" dir="auto">Index 👇</h4>
<ul dir="auto">
<li><a href="https://github.com/vitoplantamura/OnnxStream#onnxstream">Introduction</a></li>
<li><strong><a href="https://github.com/vitoplantamura/OnnxStream#stable-diffusion-15">Stable Diffusion 1.5</a></strong></li>
<li><strong><a href="https://github.com/vitoplantamura/OnnxStream#stable-diffusion-xl-10-base">Stable Diffusion XL 1.0 Base</a></strong></li>
<li><strong><a href="https://github.com/vitoplantamura/OnnxStream#stable-diffusion-xl-turbo-10">Stable Diffusion XL Turbo 1.0</a></strong></li>
<li><a href="https://github.com/vitoplantamura/OnnxStream#features-of-onnxstream">Features of OnnxStream</a></li>
<li><a href="https://github.com/vitoplantamura/OnnxStream#performance">Performance</a></li>
<li><a href="https://github.com/vitoplantamura/OnnxStream#attention-slicing-and-quantization">Attention Slicing and Quantization</a></li>
<li><a href="https://github.com/vitoplantamura/OnnxStream#how-onnxstream-works">How OnnxStream Works</a></li>
<li><strong><a href="https://github.com/vitoplantamura/OnnxStream#how-to-build-the-stable-diffusion-example-on-linuxmacwindowstermux">How to Build</a></strong></li>
<li><a href="https://github.com/vitoplantamura/OnnxStream#how-to-convert-and-run-a-custom-stable-diffusion-15-model-with-onnxstream-by-gaelicthunder">How to Convert SD 1.5 Model</a></li>
<li><a href="https://github.com/vitoplantamura/OnnxStream#related-projects">Related Projects</a></li>
<li><a href="https://github.com/vitoplantamura/OnnxStream#credits">Credits</a></li>
</ul>
<h2 tabindex="-1" dir="auto">OnnxStream</h2>
<p dir="auto">The challenge is to run <a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a> 1.5, which includes a large transformer model with almost 1 billion parameters, on a <a href="https://www.raspberrypi.com/products/raspberry-pi-zero-2-w/" rel="nofollow">Raspberry Pi Zero 2</a>, which is a microcomputer with 512MB of RAM, without adding more swap space and without offloading intermediate results on disk. The recommended minimum RAM/VRAM for Stable Diffusion 1.5 is typically 8GB.</p>
<p dir="auto">Generally major machine learning frameworks and libraries are focused on minimizing inference latency and/or maximizing throughput, all of which at the cost of RAM usage. So I decided to write a super small and hackable inference library specifically focused on minimizing memory consumption: OnnxStream.</p>
<p dir="auto">OnnxStream is based on the idea of decoupling the inference engine from the component responsible of providing the model weights, which is a class derived from <code>WeightsProvider</code>. A <code>WeightsProvider</code> specialization can implement any type of loading, caching and prefetching of the model parameters. For example a custom <code>WeightsProvider</code> can decide to download its data from an HTTP server directly, without loading or writing anything to disk (hence the word "Stream" in "OnnxStream"). Three default <code>WeightsProviders</code> are available: <code>DiskNoCache</code>, <code>DiskPrefetch</code> and <code>Ram</code>.</p>
<p dir="auto"><strong>OnnxStream can consume even 55x less memory than OnnxRuntime with only a 50% to 200% increase in latency</strong> (on CPU, with a good SSD, with reference to the SD 1.5's UNET - see the Performance section below).</p>
<h2 tabindex="-1" dir="auto">Stable Diffusion 1.5</h2>
<p dir="auto">These images were generated by the Stable Diffusion example implementation included in this repo, using OnnxStream, at different precisions of the VAE decoder. The VAE decoder is the only model of Stable Diffusion 1.5 that could not fit into the RAM of the Raspberry Pi Zero 2 in single or half precision. This is caused by the presence of residual connections and very big tensors and convolutions in the model. The only solution was static quantization (8 bit). The third image was generated by my RPI Zero 2 in about <del>3 hours</del> 1.5 hours (using the MAX_SPEED option when compiling). The first image was generated on my PC using the same latents generated by the RPI Zero 2, for comparison:</p>
<p dir="auto">VAE decoder in W16A16 precision:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W16A16.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W16A16.png" alt="W16A16 VAE Decoder"></a></p>
<p dir="auto">VAE decoder in W8A32 precision:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W8A32.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W8A32.png" alt="W8A32 VAE Decoder"></a></p>
<p dir="auto">VAE decoder in W8A8 precision, generated by my RPI Zero 2 in about <del>3 hours</del> 1.5 hours (using the MAX_SPEED option when compiling):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W8A8.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W8A8.png" alt="W8A8 VAE Decoder"></a></p>
<h2 tabindex="-1" dir="auto">Stable Diffusion XL 1.0 (base)</h2>
<p dir="auto">The OnnxStream Stable Diffusion example implementation now supports SDXL 1.0 (without the Refiner). The ONNX files were exported from the SDXL 1.0 implementation of the Hugging Face's <a href="https://github.com/huggingface/diffusers">Diffusers</a> library (version 0.19.3).</p>
<p dir="auto">SDXL 1.0 is significantly more computationally expensive than SD 1.5. The most significant difference is the ability to generate 1024x1024 images instead of 512x512. To give you an idea, generating a 10-steps image with HF's Diffusers takes 26 minutes on my 12-core PC with 32GB of RAM. The minimum recommended VRAM for SDXL is typically 12GB.</p>
<p dir="auto"><strong>OnnxStream can run SDXL 1.0 in less than 300MB of RAM and therefore is able to run it comfortably on a RPI Zero 2</strong>, without adding more swap space and without writing anything to disk during inference. Generating a 10-steps image takes about 11 hours on my RPI Zero 2.</p>
<h4 tabindex="-1" dir="auto">SDXL Specific Optimizations</h4>
<p dir="auto">The same set of optimizations for SD 1.5 has been used for SDXL 1.0, but with the following differences.</p>
<p dir="auto">As for the UNET model, in order to make it run in less than 300MB of RAM on the RPI Zero 2, UINT8 dynamic quantization is used, but limited to a specific subset of large intermediate tensors.</p>
<p dir="auto">The situation for the VAE decoder is more complex than for SD 1.5. SDXL 1.0's VAE decoder is 4x the size of SD 1.5's, and consumes 4.4GB of RAM when run with OnnxStream in FP32 precision.</p>
<p dir="auto">In the case of SD 1.5 the VAE decoder is statically quantized (UINT8 precision) and this is enough to reduce RAM consumption to 260MB. Instead, the SDXL 1.0's VAE decoder overflows when run with FP16 arithmetic and the numerical ranges of its activations are too large to get good quality images with UINT8 quantization.</p>
<p dir="auto">So we are stuck with a model that consumes 4.4GB of RAM, which cannot be run in FP16 precision and which cannot be quantized in UINT8 precision. (NOTE: there is at least <a href="https://huggingface.co/madebyollin/sdxl-vae-fp16-fix" rel="nofollow">one solution</a> to the FP16 problem, but I have not investigated further since even running the VAE decoder in FP16 precision, the total memory consumed would be divided by 2, so the model would ultimately consume 2.2GB instead of 4.4GB, which is still way too much for the RPI Zero 2)</p>
<p dir="auto">The inspiration for the solution came from the implementation of the VAE decoder of the Hugging Face's <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoder_kl.py">Diffusers</a> library, i.e. using tiled decoding. The final result is absolutely indistinguishable from an image decoded by the full decoder and in this way it is possible to reduce RAM memory consumption from 4.4GB to 298MB!</p>
<p dir="auto">The idea is simple. The result of the diffusion process is a tensor with shape (1,4,128,128). The idea is to split this tensor into 5x5 (therefore 25) overlapping tensors with shape (1,4,32,32) and to decode these tensors separately. Each of these tensors is overlapped by 25% on the tile to its left and the one above. The decoding result is a tensor with shape (1,3,256,256) which is then appropriately blended into the final image.</p>
<p dir="auto">For example, this is an image generated by the tiled decoder with blending manually turned off in the code. <strong>You can clearly see the tiles in the image:</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/sdxl_tiles.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/sdxl_tiles.png" alt="SDXL Output with Tiles"></a></p>
<p dir="auto">While this is the same image with blending turned on. <strong>This is the final result:</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/sdxl_without_tiles.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/sdxl_without_tiles.png" alt="SDXL Output without Tiles"></a></p>
<p dir="auto">This is another image generated by my RPI Zero 2 in about 11 hours: (10 steps, Euler Ancestral)</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/sdxl_out_1.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/sdxl_out_1.png" alt="SDXL Output generated by RPI Zero 2"></a></p>
<h2 tabindex="-1" dir="auto">Stable Diffusion XL Turbo 1.0</h2>
<p dir="auto">Support for SDXL Turbo was contributed by the kind <a href="https://github.com/AeroX2">@AeroX2</a>.</p>
<p dir="auto">The main difference between SDXL and SDXL Turbo is that the Turbo version generates 512x512 images instead of 1024x1024, but with a much lower number of steps. It is possible to get good quality images even with just one step!</p>
<p dir="auto">No additional optimizations compared to SDXL 1.0 were required to run SDXL Turbo on the RPI Zero 2. SDXL and SDXL Turbo share the same text encoder and VAE decoder: tiled decoding is required to keep memory consumption under 300MB.</p>
<p dir="auto">This image was generated by my Raspberry PI Zero 2 in <strong>29 minutes</strong> (<strong>1 step</strong>):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/sdxlturbo_steps1.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/sdxlturbo_steps1.png" alt="sdxlturbo steps 1"></a></p>
<p dir="auto">This image is an example of <strong>3 step</strong> generation, and took <strong>50 minutes</strong> on my RPI Zero 2. The quality is the same as the 1 step generated image:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/sdxlturbo_steps3.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/sdxlturbo_steps3.png" alt="sdxlturbo steps 3"></a></p>
<p dir="auto">A comparison between SDXL 1.0 and SDXL Turbo run in OnnxStream with respect to the number of steps is available in the <a href="https://huggingface.co/AeroX2/stable-diffusion-xl-turbo-1.0-onnxstream" rel="nofollow">model card</a> of the model (by @AeroX2).</p>
<h2 tabindex="-1" dir="auto">Features of OnnxStream</h2>
<ul dir="auto">
<li>Inference engine decoupled from the <code>WeightsProvider</code></li>
<li><code>WeightsProvider</code> can be <code>DiskNoCache</code>, <code>DiskPrefetch</code>, <code>Ram</code> or custom</li>
<li>Attention slicing</li>
<li>Dynamic quantization (8 bit unsigned, asymmetric, percentile)</li>
<li>Static quantization (W8A8 unsigned, asymmetric, percentile)</li>
<li>Easy calibration of a quantized model</li>
<li>FP16 support (with or without FP16 arithmetic)</li>
<li>25 ONNX operators implemented (the most common)</li>
<li>Operations executed sequentially but all operators are multithreaded</li>
<li>Single implementation file + header file</li>
<li>XNNPACK calls wrapped in the <code>XnnPack</code> class (for future replacement)</li>
</ul>
<p dir="auto">OnnxStream depends on <a href="https://github.com/google/XNNPACK">XNNPACK</a> for some (accelerated) primitives: MatMul, Convolution, element-wise Add/Sub/Mul/Div, Sigmoid and Softmax.</p>
<h2 tabindex="-1" dir="auto">Performance</h2>
<p dir="auto">Stable Diffusion 1.5 consists of three models: <strong>a text encoder</strong> (672 operations and 123 million parameters), the <strong>UNET model</strong> (2050 operations and 854 million parameters) and the <strong>VAE decoder</strong> (276 operations and 49 million parameters). Assuming that the batch size is equal to 1, a full image generation with 10 steps, which yields good results (with the Euler Ancestral scheduler), requires 2 runs of the text encoder, 20 (i.e. 2*10) runs of the UNET model and 1 run of the VAE decoder.</p>
<p dir="auto">This table shows the various inference times of the three models of Stable Diffusion 1.5, together with the memory consumption (i.e. the <code>Peak Working Set Size</code> in Windows or the <code>Maximum Resident Set Size</code> in Linux).</p>
<table>
<thead>
<tr>
<th>Model / Library</th>
<th>1st run</th>
<th>2nd run</th>
<th>3rd run</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16 UNET / OnnxStream</td>
<td>0.133 GB - 18.2 secs</td>
<td>0.133 GB - 18.7 secs</td>
<td>0.133 GB - 19.8 secs</td>
</tr>
<tr>
<td>FP16 UNET / OnnxRuntime</td>
<td>5.085 GB - 12.8 secs</td>
<td>7.353 GB - 7.28 secs</td>
<td>7.353 GB - 7.96 secs</td>
</tr>
<tr>
<td>FP32 Text Enc / OnnxStream</td>
<td>0.147 GB - 1.26 secs</td>
<td>0.147 GB - 1.19 secs</td>
<td>0.147 GB - 1.19 secs</td>
</tr>
<tr>
<td>FP32 Text Enc / OnnxRuntime</td>
<td>0.641 GB - 1.02 secs</td>
<td>0.641 GB - 0.06 secs</td>
<td>0.641 GB - 0.07 secs</td>
</tr>
<tr>
<td>FP32 VAE Dec / OnnxStream</td>
<td>1.004 GB - 20.9 secs</td>
<td>1.004 GB - 20.6 secs</td>
<td>1.004 GB - 21.2 secs</td>
</tr>
<tr>
<td>FP32 VAE Dec / OnnxRuntime</td>
<td>1.330 GB - 11.2 secs</td>
<td>2.026 GB - 10.1 secs</td>
<td>2.026 GB - 11.1 secs</td>
</tr>
</tbody>
</table>
<p dir="auto">In the case of the UNET model (when run in FP16 precision, with FP16 arithmetic enabled in OnnxStream), OnnxStream can consume even 55x less memory than OnnxRuntime with a 50% to 200% increase in latency.</p>
<p dir="auto">Notes:</p>
<ul dir="auto">
<li>The first run for OnnxRuntime is a warm up inference, since its <code>InferenceSession</code> is created before the first run and reused for all the subsequent runs. No such thing as a warm up exists for OnnxStream since it is purely eager by design (however subsequent runs can benefit from the caching of the weights files by the OS).</li>
<li>At the moment OnnxStream doesn't support inputs with a batch size != 1, unlike OnnxRuntime, which can greatly speed up the whole diffusion process using a batch size = 2 when running the UNET model.</li>
<li>In my tests, changing OnnxRuntime's <code>SessionOptions</code> (like <code>EnableCpuMemArena</code> and <code>ExecutionMode</code>) produces no significant difference in the results.</li>
<li>Performance of OnnxRuntime is very similar to that of NCNN (the other framework I evaluated), both in terms of memory consumption and inference time. I'll include NCNN benchmarks in the future, if useful.</li>
<li>Tests were run on my development machine: Windows Server 2019, 16GB RAM, 8750H cpu (AVX2), 970 EVO Plus SSD, 8 virtual cores on VMWare.</li>
</ul>
<h2 tabindex="-1" dir="auto">Attention Slicing and Quantization</h2>
<p dir="auto">The use of "attention slicing" when running the UNET model and the use of W8A8 quantization for the VAE decoder were crucial in reducing memory consumption to a level that allowed execution on a RPI Zero 2.</p>
<p dir="auto">While there is a lot of information on the internet about quantizing neural networks, little can be found about "attention slicing". The idea is simple: the goal is to avoid materializing the full <code>Q @ K^T</code> matrix when calculating the scaled dot-product attention of the various multi-head attentions in the UNET model. With an attention head count of 8 in the UNET model, <code>Q</code> has a shape of (8,4096,40), while <code>K^T</code> has a shape of (8,40,4096): so the result of the first MatMul has a final shape of (8,4096,4096), which is a 512MB tensor (in FP32 precision):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/attention_mem_consumpt.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/attention_mem_consumpt.png" alt="Attention Slicing"></a></p>
<p dir="auto">The solution is to split <code>Q</code> vertically and then to proceed with the attention operations normally on each chunk of <code>Q</code>. <code>Q_sliced</code> has a shape of (1,x,40), where x is 4096 (in this case) divided by <code>onnxstream::Model::m_attention_fused_ops_parts</code> (which has a default value of 2, but can be customized). This simple trick allows to lower the overall consumed memory of the UNET model from 1.1GB to 300MB (when the model is run in FP32 precision). A possible alternative, certainly more efficient, would be to use FlashAttention, however FlashAttention would require writing a custom kernel for each supported architecture (AVX, NEON etc), bypassing XnnPack in our case.</p>
<h2 tabindex="-1" dir="auto">How OnnxStream works</h2>
<p dir="auto">This code can run a model defined in the <code>path_to_model_folder/model.txt</code>: (all the model operations are defined in the <code>model.txt</code> text file; OnnxStream expects to find all the weights files in that same folder, as a series of <code>.bin</code> files)</p>
<div dir="auto" data-snippet-clipboard-copy-content="#include &quot;onnxstream.h&quot;

using namespace onnxstream;

int main()
{
    Model model;

    //
    // Optional parameters that can be set on the Model object:
    //
    // model.set_weights_provider( ... ); // specifies a different weights provider (default is DiskPrefetchWeightsProvider)
    // model.read_range_data( ... ); // reads a range data file (which contains the clipping ranges of the activations for a quantized model)
    // model.write_range_data( ... ); // writes a range data file (useful after calibration)
    // model.m_range_data_calibrate = true; // calibrates the model
    // model.m_use_fp16_arithmetic = true; // uses FP16 arithmetic during inference (useful if weights are in FP16 precision)
    // model.m_use_uint8_arithmetic = true; // uses UINT8 arithmetic during inference
    // model.m_use_uint8_qdq = true; // uses UINT8 dynamic quantization (can reduce memory consumption of some models)
    // model.m_fuse_ops_in_attention = true; // enables attention slicing
    // model.m_attention_fused_ops_parts = ... ; // see the &quot;Attention Slicing&quot; section above
    //

    model.read_file(&quot;path_to_model_folder/model.txt&quot;);

    tensor_vector<float> data;
    
    ... // fill the tensor_vector with the tensor data. &quot;tensor_vector&quot; is just an alias to a std::vector with a custom allocator.

    Tensor t;
    t.m_name = &quot;input&quot;;
    t.m_shape = { 1, 4, 64, 64 };
    t.set_vector(std::move(data));
    model.push_tensor(std::move(t));

    model.run();
    
    auto&amp; result = model.m_data[0].get_vector<float>();
    
    ... // process the result: &quot;result&quot; is a reference to the first result of the inference (a tensor_vector<float> as well).

    return 0;
}"><pre>#<span>include</span> <span><span>"</span>onnxstream.h<span>"</span></span>

<span>using</span> <span>namespace</span> <span>onnxstream</span><span>;</span>

<span>int</span> <span>main</span>()
{
    Model model;

    <span><span>//</span></span>
    <span><span>//</span> Optional parameters that can be set on the Model object:</span>
    <span><span>//</span></span>
    <span><span>//</span> model.set_weights_provider( ... ); // specifies a different weights provider (default is DiskPrefetchWeightsProvider)</span>
    <span><span>//</span> model.read_range_data( ... ); // reads a range data file (which contains the clipping ranges of the activations for a quantized model)</span>
    <span><span>//</span> model.write_range_data( ... ); // writes a range data file (useful after calibration)</span>
    <span><span>//</span> model.m_range_data_calibrate = true; // calibrates the model</span>
    <span><span>//</span> model.m_use_fp16_arithmetic = true; // uses FP16 arithmetic during inference (useful if weights are in FP16 precision)</span>
    <span><span>//</span> model.m_use_uint8_arithmetic = true; // uses UINT8 arithmetic during inference</span>
    <span><span>//</span> model.m_use_uint8_qdq = true; // uses UINT8 dynamic quantization (can reduce memory consumption of some models)</span>
    <span><span>//</span> model.m_fuse_ops_in_attention = true; // enables attention slicing</span>
    <span><span>//</span> model.m_attention_fused_ops_parts = ... ; // see the "Attention Slicing" section above</span>
    <span><span>//</span></span>

    model.<span>read_file</span>(<span><span>"</span>path_to_model_folder/model.txt<span>"</span></span>);

    tensor_vector&lt;<span>float</span>&gt; data;
    
    ... <span><span>//</span> fill the tensor_vector with the tensor data. "tensor_vector" is just an alias to a std::vector with a custom allocator.</span>

    Tensor t;
    t.<span>m_name</span> = <span><span>"</span>input<span>"</span></span>;
    t.<span>m_shape</span> = { <span>1</span>, <span>4</span>, <span>64</span>, <span>64</span> };
    t.<span>set_vector</span>(<span>std::move</span>(data));
    model.<span>push_tensor</span>(<span>std::move</span>(t));

    model.<span>run</span>();
    
    <span>auto</span>&amp; result = model.<span>m_data</span>[<span>0</span>].<span>get_vector</span>&lt;<span>float</span>&gt;();
    
    ... <span><span>//</span> process the result: "result" is a reference to the first result of the inference (a tensor_vector&lt;float&gt; as well).</span>

    <span>return</span> <span>0</span>;
}</pre></div>
<p dir="auto">The <code>model.txt</code> file contains all the model operations in ASCII format, as exported from the original ONNX file. Each line corresponds to an operation: for example this line represents a convolution in a quantized model:</p>
<div data-snippet-clipboard-copy-content="Conv_4:Conv*input:input_2E_1(1,4,64,64);post_5F_quant_5F_conv_2E_weight_nchw.bin(uint8[0.0035054587850383684,134]:4,4,1,1);post_5F_quant_5F_conv_2E_bias.bin(float32:4)*output:input(1,4,64,64)*dilations:1,1;group:1;kernel_shape:1,1;pads:0,0,0,0;strides:1,1"><pre><code>Conv_4:Conv*input:input_2E_1(1,4,64,64);post_5F_quant_5F_conv_2E_weight_nchw.bin(uint8[0.0035054587850383684,134]:4,4,1,1);post_5F_quant_5F_conv_2E_bias.bin(float32:4)*output:input(1,4,64,64)*dilations:1,1;group:1;kernel_shape:1,1;pads:0,0,0,0;strides:1,1
</code></pre></div>
<p dir="auto">In order to export the <code>model.txt</code> file and its weights (as a series of <code>.bin</code> files) from an ONNX file for use in OnnxStream, a notebook (with a single cell) is provided (<code>onnx2txt.ipynb</code>).</p>
<p dir="auto">Some things must be considered when exporting a Pytorch <code>nn.Module</code> (in our case) to ONNX for use in OnnxStream:</p>
<ol dir="auto">
<li>When calling <code>torch.onnx.export</code>, <code>dynamic_axes</code> should be left empty, since OnnxStream doesn't support inputs with a dynamic shape.</li>
<li>It is strongly recommended to run the excellent <a href="https://github.com/daquexian/onnx-simplifier">ONNX Simplifier</a> on the exported ONNX file before its conversion to a <code>model.txt</code> file.</li>
</ol>
<h2 tabindex="-1" dir="auto">How to Build the Stable Diffusion example on Linux/Mac/Windows/Termux</h2>
<ul dir="auto">
<li><strong>Windows only</strong>: start the following command prompt: <code>Visual Studio Tools</code> &gt; <code>x64 Native Tools Command Prompt</code>.</li>
<li><strong>Mac only</strong>: make sure to install cmake: <code>brew install cmake</code>.</li>
</ul>
<p dir="auto">First you need to build <a href="https://github.com/google/XNNPACK">XNNPACK</a>.</p>
<p dir="auto">Since the function prototypes of XnnPack can change at any time, I've included a <code>git checkout</code> ​​that ensures correct compilation of OnnxStream with a compatible version of XnnPack at the time of writing:</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/google/XNNPACK.git
cd XNNPACK
git rev-list -n 1 --before=&quot;2023-06-27 00:00&quot; master
git checkout <COMMIT_ID_FROM_THE_PREVIOUS_COMMAND>
mkdir build
cd build
cmake -DXNNPACK_BUILD_TESTS=OFF -DXNNPACK_BUILD_BENCHMARKS=OFF ..
cmake --build . --config Release"><pre><code>git clone https://github.com/google/XNNPACK.git
cd XNNPACK
git rev-list -n 1 --before="2023-06-27 00:00" master
git checkout &lt;COMMIT_ID_FROM_THE_PREVIOUS_COMMAND&gt;
mkdir build
cd build
cmake -DXNNPACK_BUILD_TESTS=OFF -DXNNPACK_BUILD_BENCHMARKS=OFF ..
cmake --build . --config Release
</code></pre></div>
<p dir="auto">Then you can build the Stable Diffusion example.</p>
<p dir="auto"><code>&lt;DIRECTORY_WHERE_XNNPACK_WAS_CLONED&gt;</code> is for example <code>/home/vito/Desktop/XNNPACK</code> or <code>C:\Projects\SD\XNNPACK</code> (on Windows):</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/vitoplantamura/OnnxStream.git
cd OnnxStream
cd src
mkdir build
cd build
cmake -DMAX_SPEED=ON -DXNNPACK_DIR=<DIRECTORY_WHERE_XNNPACK_WAS_CLONED> ..
cmake --build . --config Release"><pre><code>git clone https://github.com/vitoplantamura/OnnxStream.git
cd OnnxStream
cd src
mkdir build
cd build
cmake -DMAX_SPEED=ON -DXNNPACK_DIR=&lt;DIRECTORY_WHERE_XNNPACK_WAS_CLONED&gt; ..
cmake --build . --config Release
</code></pre></div>
<p dir="auto"><strong>Important:</strong> the MAX_SPEED option allows to increase performance by about 10% in Windows, but by more than 50% on the Raspberry Pi. This option consumes much more memory at build time and the produced executable may not work (as was the case with Termux in my tests). So in case of problems, the first attempt to make is to set MAX_SPEED to OFF.</p>
<p dir="auto">Now you can run the Stable Diffusion example.</p>
<p dir="auto">In the case of <strong>Stable Diffusion 1.5</strong>, the weights for the example can be downloaded from the Releases of this repo (about 2GB). In the case of <strong>Stable Diffusion XL 1.0 Base</strong>, the weights can be downloaded from Hugging Face (about 8GB):</p>
<div data-snippet-clipboard-copy-content="git lfs install
git clone --depth=1 https://huggingface.co/vitoplantamura/stable-diffusion-xl-base-1.0-onnxstream"><pre><code>git lfs install
git clone --depth=1 https://huggingface.co/vitoplantamura/stable-diffusion-xl-base-1.0-onnxstream
</code></pre></div>
<p dir="auto">In the case of <strong>Stable Diffusion XL Turbo 1.0</strong>, the weights can be downloaded here (about 8GB):</p>
<div data-snippet-clipboard-copy-content="git lfs install
git clone --depth=1 https://huggingface.co/AeroX2/stable-diffusion-xl-turbo-1.0-onnxstream"><pre><code>git lfs install
git clone --depth=1 https://huggingface.co/AeroX2/stable-diffusion-xl-turbo-1.0-onnxstream
</code></pre></div>
<p dir="auto">These are the command line options of the Stable Diffusion example:</p>
<div data-snippet-clipboard-copy-content="--xl                Runs Stable Diffusion XL 1.0 instead of Stable Diffusion 1.5.
--turbo             Runs Stable Diffusion Turbo 1.0 instead of Stable Diffusion 1.5.
--models-path       Sets the folder containing the Stable Diffusion models.
--ops-printf        During inference, writes the current operation to stdout.
--output            Sets the output PNG file.
--decode-latents    Skips the diffusion, and decodes the specified latents file.
--prompt            Sets the positive prompt.
--neg-prompt        Sets the negative prompt.
--steps             Sets the number of diffusion steps.
--seed              Sets the seed.
--save-latents      After the diffusion, saves the latents in the specified file.
--decoder-calibrate (ONLY SD 1.5) Calibrates the quantized version of the VAE decoder.
--not-tiled         (ONLY SDXL 1.0) Don't use the tiled VAE decoder.
--ram               Uses the RAM WeightsProvider (Experimental).
--rpi               Configures the models to run on a Raspberry Pi.
--rpi-lowmem        Configures the models to run on a Raspberry Pi Zero 2."><pre><code>--xl                Runs Stable Diffusion XL 1.0 instead of Stable Diffusion 1.5.
--turbo             Runs Stable Diffusion Turbo 1.0 instead of Stable Diffusion 1.5.
--models-path       Sets the folder containing the Stable Diffusion models.
--ops-printf        During inference, writes the current operation to stdout.
--output            Sets the output PNG file.
--decode-latents    Skips the diffusion, and decodes the specified latents file.
--prompt            Sets the positive prompt.
--neg-prompt        Sets the negative prompt.
--steps             Sets the number of diffusion steps.
--seed              Sets the seed.
--save-latents      After the diffusion, saves the latents in the specified file.
--decoder-calibrate (ONLY SD 1.5) Calibrates the quantized version of the VAE decoder.
--not-tiled         (ONLY SDXL 1.0) Don't use the tiled VAE decoder.
--ram               Uses the RAM WeightsProvider (Experimental).
--rpi               Configures the models to run on a Raspberry Pi.
--rpi-lowmem        Configures the models to run on a Raspberry Pi Zero 2.
</code></pre></div>
<h2 tabindex="-1" dir="auto">How to Convert and Run a Custom Stable Diffusion 1.5 Model with OnnxStream (by @GaelicThunder)</h2>
<details>
<summary>Click to expand</summary>
<p dir="auto">This guide aims to assist you in converting a custom Stable Diffusion model for use with OnnxStream. Whether you're starting from <code>.safetensors</code> or <code>.onnx</code>, this guide has you covered.</p>
<h3 tabindex="-1" dir="auto">Prerequisites</h3>
<ul dir="auto">
<li>Python 3.x</li>
<li>ONNX</li>
<li>ONNX Simplifier</li>
<li>Linux environment (tested on Ubuntu, Windows WSL also works)</li>
<li>Swap space (amount varies depending on your approach)</li>
</ul>
<h3 tabindex="-1" dir="auto">Why Specific Steps?</h3>
<h4 tabindex="-1" dir="auto">Understanding Einsum and Other Operations</h4>
<p dir="auto">AUTO1111's Stable Diffusion implementation uses operations like Einsum, which are not supported by OnnxStream (yet). Hence, it's advised to use the Hugging Face implementation, which is more compatible.</p>
<h3 tabindex="-1" dir="auto">Optional: Converting .safetensors to ONNX</h3>
<p dir="auto">If you're starting with a <code>.safetensors</code> file, you can convert it to <code>.onnx</code> using the tool available at <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui-tensorrt">this GitHub repository</a>.</p>
<p dir="auto">However, it is recommended to follow the approach in section "Option A" below.</p>
<h3 tabindex="-1" dir="auto">Exporting Your Model</h3>
<h4 tabindex="-1" dir="auto">Option A: Exporting from Hugging Face (Recommended)</h4>
<div dir="auto" data-snippet-clipboard-copy-content="from diffusers import StableDiffusionPipeline
import torch

pipe = StableDiffusionPipeline.from_single_file(&quot;https://huggingface.co/YourUsername/YourModel/blob/main/Model.safetensors&quot;)

dummy_input = (torch.randn(1, 4, 64, 64), torch.randn(1), torch.randn(1, 77, 768))
input_names = [&quot;sample&quot;, &quot;timestep&quot;, &quot;encoder_hidden_states&quot;]
output_names = [&quot;out_sample&quot;]

torch.onnx.export(pipe.unet, dummy_input, &quot;/path/to/save/unet_temp.onnx&quot;, verbose=False, input_names=input_names, output_names=output_names, opset_version=14, do_constant_folding=True, export_params=True)"><pre><span>from</span> <span>diffusers</span> <span>import</span> <span>StableDiffusionPipeline</span>
<span>import</span> <span>torch</span>

<span>pipe</span> <span>=</span> <span>StableDiffusionPipeline</span>.<span>from_single_file</span>(<span>"https://huggingface.co/YourUsername/YourModel/blob/main/Model.safetensors"</span>)

<span>dummy_input</span> <span>=</span> (<span>torch</span>.<span>randn</span>(<span>1</span>, <span>4</span>, <span>64</span>, <span>64</span>), <span>torch</span>.<span>randn</span>(<span>1</span>), <span>torch</span>.<span>randn</span>(<span>1</span>, <span>77</span>, <span>768</span>))
<span>input_names</span> <span>=</span> [<span>"sample"</span>, <span>"timestep"</span>, <span>"encoder_hidden_states"</span>]
<span>output_names</span> <span>=</span> [<span>"out_sample"</span>]

<span>torch</span>.<span>onnx</span>.<span>export</span>(<span>pipe</span>.<span>unet</span>, <span>dummy_input</span>, <span>"/path/to/save/unet_temp.onnx"</span>, <span>verbose</span><span>=</span><span>False</span>, <span>input_names</span><span>=</span><span>input_names</span>, <span>output_names</span><span>=</span><span>output_names</span>, <span>opset_version</span><span>=</span><span>14</span>, <span>do_constant_folding</span><span>=</span><span>True</span>, <span>export_params</span><span>=</span><span>True</span>)</pre></div>
<h4 tabindex="-1" dir="auto">Option B: Manually Fixing Input Shapes</h4>
<div dir="auto" data-snippet-clipboard-copy-content="python -m onnxruntime.tools.make_dynamic_shape_fixed --input_name sample --input_shape 1,4,64,64 model.onnx model_fixed1.onnx
python -m onnxruntime.tools.make_dynamic_shape_fixed --input_name timestep --input_shape 1 model_fixed1.onnx model_fixed2.onnx
python -m onnxruntime.tools.make_dynamic_shape_fixed --input_name encoder_hidden_states --input_shape 1,77,768 model_fixed2.onnx model_fixed3.onnx"><pre>python -m onnxruntime.tools.make_dynamic_shape_fixed --input_name sample --input_shape 1,4,64,64 model.onnx model_fixed1.onnx
python -m onnxruntime.tools.make_dynamic_shape_fixed --input_name timestep --input_shape 1 model_fixed1.onnx model_fixed2.onnx
python -m onnxruntime.tools.make_dynamic_shape_fixed --input_name encoder_hidden_states --input_shape 1,77,768 model_fixed2.onnx model_fixed3.onnx</pre></div>
<p dir="auto">Note by Vito: This can be achieved simply by following the approach outlined in "Option A" above, which remains the recommended approach. Making the input shapes fixed might be useful if your starting point is already an ONNX file.</p>
<h3 tabindex="-1" dir="auto">Running ONNX Simplifier</h3>
<div dir="auto" data-snippet-clipboard-copy-content="python -m onnx_simplifier model_fixed3.onnx model_simplified.onnx"><pre>python -m onnx_simplifier model_fixed3.onnx model_simplified.onnx</pre></div>
<p dir="auto"><strong>Note</strong>:</p>
<ul dir="auto">
<li>If you exported your model from Hugging Face, you'll need around 100GB of swap space.</li>
<li>If you manually fixed the input shapes, 16GB of RAM should suffice.</li>
<li>The process may take some time; please be patient.</li>
</ul>
<h3 tabindex="-1" dir="auto">Final Steps and Running the Model</h3>
<p dir="auto">Once you have the final model from <code>onnx2txt</code>, move it into the <code>unet_fp16</code> folder of the standard SD 1.5 model, which can be found in the Windows release of OnnxStream.</p>
<p dir="auto">The command to run the model might look like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./sd --models-path ./Converted/ --prompt &quot;space landscape&quot; --steps 28 --rpi"><pre>./sd --models-path ./Converted/ --prompt <span><span>"</span>space landscape<span>"</span></span> --steps 28 --rpi</pre></div>
<h3 tabindex="-1" dir="auto">Note on the "Shape" Operator</h3>
<p dir="auto">If you see the "Shape" operator in the output of Onnx Simplifier or in <code>onnx2txt.ipynb</code>, it indicates that Onnx Simplifier may not be functioning as expected. This issue is often not caused by Onnx Simplifier itself but rather by Onnx's Shape Inference.</p>
<h4 tabindex="-1" dir="auto">Alternative Solution</h4>
<p dir="auto">In such cases, you have the alternative to re-export the model by modifying the parameters of <code>torch.onnx.export</code>. Locate this file on your computer:</p>
<p dir="auto"><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui-tensorrt/blob/master/export_onnx.py">export_onnx.py from GitHub</a></p>
<p dir="auto">And make sure to:</p>
<ul dir="auto">
<li>Set <code>opset_version</code> to 14</li>
<li>Remove <code>dynamic_axes</code></li>
</ul>
<p dir="auto">After making these changes, you can rerun Onnx Simplifier and <code>onnx2txt</code>.</p>
<p dir="auto">Note by Vito: This solution, although working, generates ONNX files with Einsum operations. When OnnxStream supports the Einsum operator, this solution will become the recommended one.</p>
<h3 tabindex="-1" dir="auto">Conclusion</h3>
<p dir="auto">This guide is designed to be a comprehensive resource for those looking to run a custom Stable Diffusion 1.5 model with OnnxStream. Additional contributions are welcome!</p>
</details>
<h2 tabindex="-1" dir="auto">Related Projects</h2>
<ul dir="auto">
<li><a href="https://github.com/ThomAce/OnnxStreamGui">OnnxStreamGui</a> by @ThomAce: a web and desktop user interface for OnnxStream.</li>
<li><a href="https://github.com/rvdveen/epaper-slow-generative-art">Auto epaper art</a> by @rvdveen: a self-contained image generation picture frame showing news.</li>
</ul>
<h2 tabindex="-1" dir="auto">Credits</h2>
<ul dir="auto">
<li>The Stable Diffusion 1.5 implementation in <code>sd.cpp</code> is based on <a href="https://github.com/fengwang/Stable-Diffusion-NCNN">this project</a>, which in turn is based on <a href="https://github.com/EdVince/Stable-Diffusion-NCNN">this project</a> by @EdVince. The original code was modified in order to use OnnxStream instead of NCNN.</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple partly halts Beeper's iMessage app again, suggesting a long fight ahead (272 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2023/12/apple-partly-halts-beepers-imessage-app-again-suggesting-a-long-fight-ahead/</link>
            <guid>38646903</guid>
            <pubDate>Thu, 14 Dec 2023 20:37:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2023/12/apple-partly-halts-beepers-imessage-app-again-suggesting-a-long-fight-ahead/">https://arstechnica.com/gadgets/2023/12/apple-partly-halts-beepers-imessage-app-again-suggesting-a-long-fight-ahead/</a>, See on <a href="https://news.ycombinator.com/item?id=38646903">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/beeper_group_chat-800x475.png" alt="Beeper group chat illustration">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/12/beeper_group_chat.png" data-height="600" data-width="1010">Enlarge</a> <span>/</span> The dream of everybody having blue bubbles, and epic photos of perfectly digestible meals, as proffered by Beeper.</p><p>Beeper</p></figcaption>  </figure>

  




<!-- cache hit 53:single/related:f609da48f8c974c21f5a1a7129fdec49 --><!-- empty -->
<p>A friend of mine had been using <a href="https://arstechnica.com/gadgets/2023/12/beeper-mini-on-android-claims-to-have-reverse-engineered-imessage-compatibility/">Beeper's iMessage-for-Android app, Beeper Mini</a> to keep up on group chats where she was the only Android user. It worked great until last Friday, when <a href="https://arstechnica.com/gadgets/2023/12/beeper-minis-imessage-app-for-android-is-broken-possibly-by-apple/">it didn't work at all</a>.</p>
<p>What stung her wasn't the return to being the Android interloper in the chats again. It wasn't the resulting lower-quality images, loss of encryption, and strange "Emphasized your message" reaction texts. It was losing messages during the outage and never being entirely certain they had been sent or received. There was a gathering on Saturday, and she had to double-check with a couple people about the details after showing up inadvertently early at the wrong spot.</p>
<p>That kind of grievance is why, after Apple on Wednesday appeared to have blocked what Beeper described as "~5% of Beeper Mini users" from accessing iMessages, both co-founder <a href="https://twitter.com/ericmigi/status/1735131934484029914">Eric Migicovksy</a> and the app told users <a href="https://twitter.com/onbeeper/status/1735315504880123995">they understood if people wanted out</a>. The app had already suspended its plans to charge customers $1.99 per month, following the first major outage. But this was something more about "how ridiculously annoying this uncertainty is for our users," Migicovsky posted.</p>                                            
                                                        
<h2>Fighting on two fronts</h2>
<p>But Beeper would keep working to ensure access and keep fighting on other fronts. Migicovsky pointed to <a href="https://arstechnica.com/gaming/2023/12/googles-android-app-store-monopoly-violates-antitrust-law-jury-finds/">Epic's victory at trial against Google's Play Store</a> ("big tech") as motivation. "We have a chance. We're not giving up." Over the weekend, Migicovsky reposted shows of support from Senators <a href="https://twitter.com/ewarren/status/1734983766492184886">Elizabeth Warren</a> (D-Mass.) and <a href="https://twitter.com/ericmigi/status/1734363711769546873">Amy Klobuchar</a> (D-Minn.), who have focused on reigning in and regulating large technology company's powers.</p>
<p>Apple previously issued a (somewhat uncommon) statement about Beeper's iMessage access, stating that it "took steps to protect our users by blocking techniques that exploit fake credentials in order to gain access to iMessage." Citing privacy, security, and spam concerns, Apple stated it would "continue to make updates in the future" to protect users. Migicovsky previously denied to Ars that Beeper used "fake credentials" or in any way made iMessages less secure.</p>
<p>I asked Migicovsky by direct message if, given Apple's stated plan to continually block it, there could ever be a point at which Beeper's access was "settled," or "back up and running," as he put it in his post on X (formerly Twitter). He wrote that it was up to the press and the community. "If there's enough pressure on Apple, they will have to quit messing with us." "Us," he clarified, meant both Apple's customers using iMessage and Android users trying to chat securely with iPhone friends.</p>
<p>"That's who they're penalizing," he wrote. "It's not a Beeper vs. Apple fight, it's Apple versus customers."</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Magic: The Gathering Is Turing Complete (2019) (129 pts)]]></title>
            <link>https://arxiv.org/abs/1904.09828</link>
            <guid>38646892</guid>
            <pubDate>Thu, 14 Dec 2023 20:35:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/1904.09828">https://arxiv.org/abs/1904.09828</a>, See on <a href="https://news.ycombinator.com/item?id=38646892">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/1904.09828.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>$\textit{Magic: The Gathering}$ is a popular and famously complicated trading card game about magical combat. In this paper we show that optimal play in real-world $\textit{Magic}$ is at least as hard as the Halting Problem, solving a problem that has been open for a decade. To do this, we present a methodology for embedding an arbitrary Turing machine into a game of $\textit{Magic}$ such that the first player is guaranteed to win the game if and only if the Turing machine halts. Our result applies to how real $\textit{Magic}$ is played, can be achieved using standard-size tournament-legal decks, and does not rely on stochasticity or hidden information. Our result is also highly unusual in that all moves of both players are forced in the construction. This shows that even recognising who will win a game in which neither player has a non-trivial decision to make for the rest of the game is undecidable. We conclude with a discussion of the implications for a unified computational theory of games and remarks about the playability of such a board in a tournament setting.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Alex Churchill [<a href="https://arxiv.org/show-email/3e3e4b47/1904.09828">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/1904.09828v1">[v1]</a></strong>
        Sun, 24 Mar 2019 23:48:09 UTC (17 KB)<br>
    <strong>[v2]</strong>
        Tue, 23 Apr 2019 10:16:57 UTC (458 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open and portable Postgres-as-a-service. Also available on Hetzner (208 pts)]]></title>
            <link>https://www.ubicloud.com/blog/open-and-portable-managed-postgresql-avail-hetzner</link>
            <guid>38646872</guid>
            <pubDate>Thu, 14 Dec 2023 20:33:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ubicloud.com/blog/open-and-portable-managed-postgresql-avail-hetzner">https://www.ubicloud.com/blog/open-and-portable-managed-postgresql-avail-hetzner</a>, See on <a href="https://news.ycombinator.com/item?id=38646872">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-animation="default" data-collapse="medium" data-duration="400" data-easing="ease" data-easing2="ease" data-doc-height="1" role="banner"><p><a href="https://www.ubicloud.com/"><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/64fe48116c52fe1a51e17279_ubicolud%20logo.png" loading="lazy" alt=""></a></p></div><div id="w-node-decdb48f-56e8-4c35-c577-932285e9b439-68343a58"><p>December 14, 2023 · 3 min read</p><div><p><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/657a516845397682d9b27b9f_Umur%20Cubukcu-lowres.PNG" loading="lazy" alt=""></p><div><p>Umur Cubukcu</p><p>Founder / Co-CEO</p></div></div><div><p>At Ubicloud, we are building an open and portable cloud. You can use our managed service to reduce your AWS spend to a fraction, or self-host Ubicloud to control your own cloud.<br>‍<br>With <a href="https://www.ubicloud.com/use-cases/postgresql">Ubicloud PostgreSQL</a>, we are excited to release an open and managed PostgreSQL implementation, and cut your cloud database spend by 3x.<br></p><p><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6579b8a05fb17cf50c0d40fc_Screenshot%202023-12-13%20at%202.56.55%E2%80%AFPM.png" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 92vw, (max-width: 991px) 94vw, (max-width: 1439px) 72vw, 864px" srcset="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6579b8a05fb17cf50c0d40fc_Screenshot%202023-12-13%20at%202.56.55%E2%80%AFPM-p-500.png 500w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6579b8a05fb17cf50c0d40fc_Screenshot%202023-12-13%20at%202.56.55%E2%80%AFPM-p-800.png 800w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6579b8a05fb17cf50c0d40fc_Screenshot%202023-12-13%20at%202.56.55%E2%80%AFPM-p-1080.png 1080w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6579b8a05fb17cf50c0d40fc_Screenshot%202023-12-13%20at%202.56.55%E2%80%AFPM-p-1600.png 1600w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6579b8a05fb17cf50c0d40fc_Screenshot%202023-12-13%20at%202.56.55%E2%80%AFPM-p-2000.png 2000w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6579b8a05fb17cf50c0d40fc_Screenshot%202023-12-13%20at%202.56.55%E2%80%AFPM-p-2600.png 2600w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6579b8a05fb17cf50c0d40fc_Screenshot%202023-12-13%20at%202.56.55%E2%80%AFPM.png 2894w" alt="Ubicloud Managed PostgreSQL on Hetzner"></p></div><div id="sign-up-and-sign-in"><h3>The Problem</h3><div><p>We love the cloud. But we think the prices from the big cloud vendors like AWS, Azure, GCP get too high, with closed source solutions that lock users in. It gets worse once credits expire.</p><p>The cloud evolved over the years towards a few players that charge high premiums, for increasingly complex products for standard, essential services. Each player optimizing for their walled-gardens leaves customers stuck across incompatible clouds. </p><p>We believe a simpler, portable, and open version of the cloud will make a lasting difference.</p></div><h3>The Solution</h3><div><p>Ubicloud is an open and portable alternative, reduces your costs, and returns control of your infrastructure back to you. We started with VMs, virtual networking, block storage (non-replicated), and powerful IAM. We are now adding Managed PostgreSQL, in preview.</p><p><strong>Open and Portable</strong><br>There are two parts to our release today: First, for the first time, we are making the control plane for a managed PostgreSQL solution open, using the Elastic v2 license for it. While PostgreSQL has been open and portable since its inception, managed cloud services that simplify its management have remained closed source until now. </p><p>We’ve built several of those services ourselves as the Ubicloud founding team, including the PostgreSQL services at Citus Data, Microsoft and Heroku. Our open implementation emphasizes transparency and portability:<br> <a href="https://github.com/ubicloud/ubicloud">https://github.com/ubicloud/ubicloud</a></p><p><strong>Costs 3x less</strong><br>Second, we are previewing a managed version of Ubicloud PostgreSQL, which costs 3x less compared to AWS RDS, Azure PostgreSQL, and other major providers:</p></div><p><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6579045bf4a0d1ee95387b2a_Screenshot%202023-12-13%20at%202.03.46%E2%80%AFAM.png" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 92vw, (max-width: 991px) 94vw, (max-width: 1439px) 72vw, 864px" srcset="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6579045bf4a0d1ee95387b2a_Screenshot%202023-12-13%20at%202.03.46%E2%80%AFAM-p-500.png 500w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6579045bf4a0d1ee95387b2a_Screenshot%202023-12-13%20at%202.03.46%E2%80%AFAM-p-800.png 800w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6579045bf4a0d1ee95387b2a_Screenshot%202023-12-13%20at%202.03.46%E2%80%AFAM-p-1080.png 1080w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/6579045bf4a0d1ee95387b2a_Screenshot%202023-12-13%20at%202.03.46%E2%80%AFAM.png 1266w" alt=""></p><p>Pricing <a href="https://www.ubicloud.com/docs/managed-postgresql/price-performance">scales</a> linearly from our &lt;standard-2&gt; to &lt;standard-16&gt; instances, with billing in minutely increments.</p><h3>Key features and performance</h3><div><p>We run our open and portable Ubicloud PostgreSQL control plane, on Hetzner data centers in Germany. The low cost of the bare metal instances from Hetzner allows us to pass those low prices to you. We are also working to introduce more regions in the new year.&nbsp;</p><p>Performance matters for databases, and Ubicloud PostgreSQL is competitive with the big cloud vendors –sometimes faster, depending on your workload. Here are the numbers using the standard pgbench benchmark <a href="https://www.ubicloud.com/docs/managed-postgresql/price-performance">(more here)</a>.</p></div><p><img src="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/657904d9dc6cb18dfca17808_Screenshot%202023-12-13%20at%202.05.37%E2%80%AFAM.png" loading="lazy" sizes="(max-width: 479px) 93vw, (max-width: 767px) 92vw, (max-width: 991px) 94vw, (max-width: 1439px) 72vw, 864px" srcset="https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/657904d9dc6cb18dfca17808_Screenshot%202023-12-13%20at%202.05.37%E2%80%AFAM-p-500.png 500w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/657904d9dc6cb18dfca17808_Screenshot%202023-12-13%20at%202.05.37%E2%80%AFAM-p-800.png 800w, https://assets-global.website-files.com/64f9d9b4e737e7b37d4e39a4/657904d9dc6cb18dfca17808_Screenshot%202023-12-13%20at%202.05.37%E2%80%AFAM.png 1066w" alt=""></p></div><p>While running fast at a lower cost, we provide a managed database experience on the latest version of PostgreSQL, v16.1, with automatic backups, point-in-time-restore to the minute. We use dedicated VMs for each database, with encryption at-rest and in-transit to keep your data secure.</p><div id="enter-billing"><h3><strong>What workloads are best suited for Ubicloud PostgreSQL today?</strong></h3><p>We are building Ubicloud Postgres as a general purpose PostgreSQL service. Even in its earliest and current version, it can be particularly useful when:<br></p><ul role="list"><li>You are already using Hetzner data centers, where there is currently no managed PostgreSQL offering.<br></li><li>Your cloud PostgreSQL bill is going up; and you want to offload your dev and/or analytics-heavy instances to a fast, inexpensive cloud&nbsp;</li></ul></div><p id="create-vm"><h3>Getting started</h3></p><div><p>We’d love to have you try Ubicloud Postgres, and share your feedback with us. For our managed Postgres service, we are offering the first 10 companies that sign up 50% off for the first 3 months. We will also be happy to take a look at your PostgreSQL setup, and give you any feedback for improving your Postgres usage.&nbsp;</p><p>You can use our docs to get started; then send us an email at support@ubicloud.com to claim your credit.<a href="https://www.ubicloud.com/docs/managed-postgresql/quickstart-copy"><br>‍</a><a href="https://www.ubicloud.com/docs/managed-postgresql/quickstart">https://www.ubicloud.com/docs/managed-postgresql/quickstart<p>‍</p></a>To talk about your PostgreSQL setup on RDS, Heroku, other providers, and see whether complementing it with Ubicloud PostgreSQL makes sense, you can set up time with the founders <a href="https://calendly.com/ubicloud/postgres">here.</a> <a href="https://calendly.com/ubicloud/postgres"><p>‍</p></a>Building an open cloud, with all its key services, will require your help. If you'd like to contribute to the Ubicloud project, please take a look at <a href="https://github.com/ubicloud/ubicloud">our repo</a> on Github. If you have any use-cases where you think Ubicloud could be particularly helpful, just drop us a line at <a href="mailto:support@ubicloud.com">support@ubicloud.com</a>. We’d love to talk!</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Maybe Getting Rid of Your QA Team Was Bad, Actually. (413 pts)]]></title>
            <link>https://davidkcaudill.medium.com/maybe-getting-rid-of-your-qa-team-was-bad-actually-52c408bd048b</link>
            <guid>38645856</guid>
            <pubDate>Thu, 14 Dec 2023 19:15:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://davidkcaudill.medium.com/maybe-getting-rid-of-your-qa-team-was-bad-actually-52c408bd048b">https://davidkcaudill.medium.com/maybe-getting-rid-of-your-qa-team-was-bad-actually-52c408bd048b</a>, See on <a href="https://news.ycombinator.com/item?id=38645856">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><div><div><a rel="noopener follow" href="https://davidkcaudill.medium.com/?source=post_page-----52c408bd048b--------------------------------"><div aria-hidden="false"><p><img alt="David Caudill" src="https://miro.medium.com/v2/resize:fill:88:88/0*Lgz4VQ83FheZMmP7.jpg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><figure></figure><p id="4ee8">Over many years, “DevOps” practitioners applied <a href="https://en.wikipedia.org/wiki/Theory_of_constraints" rel="noopener ugc nofollow" target="_blank">Theory Of Constraints</a> to our problems, ruthlessly optimizing our delivery pipelines and practices. Manual release management? Hell no, automate that. Deployment? Automate that too. Image management? 🔨 No thanks. Rolling back after we trebuchet a flaming dumpster into production? Automated. Whatever low value activity we could find in the process of getting code from product backlog to customer hands was a bottleneck to be removed or optimized.</p><p id="0707">The end result of this was that the<strong> slowest part of software delivery is testing.</strong> Since testing is <em>why continuous delivery exists</em>, that should have been good enough. Yes, we can make our tests faster, more automated, parallelized, etc. But when the highest value activity of a given practice is the bottleneck, you’re optimal. You have achieved “the best possible problem”.</p><p id="8725">Those habits and behaviors of optimization didn’t stop there. We kept on chopping. 🪓 We squished our integration and end to end tests down to unit tests to parallelize. At the personnel level, we pushed out anybody whom we believed could not code, indiscriminately, at the function level. We decided that <em>testing </em>might not be the bottleneck…the QA team was. The industry began to treat the people in these roles worse and worse. Expectations for them went up, salaries went down(while everyone else’s seemed to be going up!), we contracted the role, we offshored it, pretty much anything we could do to try and stop employing QA Engineers.</p><p id="ffd4">This created a self-reinforcing spiral, in which anyone “good enough at coding” or fed up with being treated poorly would leave QA. Similarly, others would assume anyone in QA wasn’t “good enough” to exit the discipline. No one recommends the field to new grads. Eventually, the whole thing seemed like it wasn’t worth it any more. Our divorce with QA was a cold one — companies just said “we’re no longer going to have that function, figure it out.” It was<strong> incredibly disrespectful</strong> and demoralizing to folks who had spent their career in QA. It also caused a lot of problems, because producing low quality software is actually a huge headache.</p><p id="5a65">You can probably see where this is going by now: <strong>developers did not figure this out. </strong>Most orgs have no idea who should be doing what in terms of software quality. Those who have kept the function are struggling to find a place for it, because of the damage already done to the discipline.</p><blockquote><p id="414c">It turns out, the “One Weird Trick” to faster software delivery was not “fire your testers”.</p></blockquote><p id="28eb">Wrecking this discipline was one of the <strong>worst kind of management mistakes </strong>— a choice to destroy something that took decades to develop, and one where the impact might not be felt for years. By the time your org has felt it, you’re likely <em>years</em> away from a meaningful fix.</p><p id="3132">The parts of the broken QA role we were handed are all still broken, and on the metaphorical workbench. The division of labor simply did not happen. Unsurprisingly, developers did not readily assume the duties of the role without any additional compensation, recognition, or reward. Those of us who can still remember working with a high functioning QA team can impersonate <em>some</em> of those behaviors, but newer engineers and managers have no idea what any of that was about, and aren’t able to tell what’s missing. The guiding principle in the following advice is that <strong>quality assurance is work. </strong>Just like any other work, assuming “somebody” will do it, <a href="https://itrevolution.com/product/making-work-visible/" rel="noopener ugc nofollow" target="_blank">letting it be invisible</a>, is a recipe for failure. Denial is not a strategy.</p><p id="daa9">It’s 2023 and it feels silly to have to write this down, but my experience suggests that I absolutely <strong>must.</strong></p></div><div><h2 id="cc56">Here is the work to be done in order to manage quality in your software practice:</h2><p id="8b0c"><strong>Defect Tracking: </strong>there <em>needs </em>to be a way for your users to send you information about a bug, and for your developers to log a bug. What is a bug? A bug is an individual ticket that describes <em>what’s wrong and how bad it is. </em>It doesn’t describe the work to fix it, only the defect itself, how to reproduce it, and its impact. In recent years I have been surprised to find that <strong>most </strong>dev teams I work with simply do not track these. These teams have an ocean of excuses: “We won’t ever fix it.” “That’s not my job.” “I don’t want to fix anything, I get rewarded for new features.” None of these is good enough to justify the low quality results that are produced by this approach.</p><figure><figcaption>Just managing the list of bugs is WORK.</figcaption></figure><p id="0d4b"><strong>Triage: </strong><em>Bug triage</em> is the process by which your engineering organization assigns, prioritizes, cleans up, categorizes, deduplicates, and otherwise cares for the bugs coming in to your organization. Having a consistent standard for what a high/medium/low severity bug looks like will help your org in a number of ways. We used to call this “bug hygiene”. Similarly, just the task of deciding what team this bug belongs to is <strong>work. </strong>A high functioning organization can do things like: degrade quality gracefully in the presence of layoffs, hand off a category of bugs to a new team in the event of a reorganization, or jettison all bugs for a feature that’s been cut.</p><figure><figcaption>You can ignore the triage process if your team never deals with external stressors or reorganizes. That was a joke.</figcaption></figure><p id="19e6"><strong>Defect Investigation: </strong>Reproduction, or “repro”, is a critical part of managing bugs. In order to expedite fixes, somebody has to do the legwork to translate “I tried to buy a movie ticket and it didn’t work” into “character encoding issues broke the purchase flow for a customer with a non-English character in their name”. Similarly, the questions of “how many times does this happen?” and “what is the user impact?” need a real human to spend a minute answering them.</p><figure><figcaption>Defect investigation is the application of engineering rigor to your backlog of bugs.</figcaption></figure><p id="2510"><strong>Focus: </strong>There is real value in having people at your company whose focus is on the quality of your end product. Quality might be “everybody’s job”…but it should also be “somebody’s job”. The push/pull of Quality vs Velocity <strong>needs</strong> an advocate for quality in the discussion — and that dynamic is vital to producing better results. Your testing tools, your test <em>quality, </em>test plans… all of these need an opinionated party to argue in favor of doing the best possible job.</p><figure></figure><p id="61db"><strong>End to End Testing: </strong>One of the biggest, most common problems I see in the engineering orgs I work with is ownership of the <em>system. </em>Increases in architectural complexity have been done to try and keep teams and applications small. That’s a perfectly rational strategy, but it leaves a gap around…<em>literally the most important thing about your application. </em>In my experience, the average team no longer does this, because it’s too hard.</p><figure><figcaption>Does anyone in your organization actually USE the product before it ships?</figcaption></figure></div><div><p id="641a">It’s easy to look at that list and say <em>“but we’re agile, lean, dynamic, we don’t need to do these things! We’ve moved past this.” </em>But I think that if you look harder, what you’ll find is that this work is probably happening in your organization already — <em>poorly. </em>And imagine if<em> literally </em>any other field tried to make that claim to you. Your car, your bank, your doctor…”we don’t do quality assurance” is just not a great thing to hear or say.<em> </em>Failure to recognize and organize these activities will lead to a really dreadful situation. Tell me if this sounds familiar:</p><p id="b2b8">The most conscientious employees in your organization are the most bitter. They see the quality issues, they often address them, and they get no recognition for doing so. When they speak up about quality concerns, they get treated like mouthbreathers who want to slow down. They watch the “move fast and break things” crowd get rewarded time after time, while they run around angrily cleaning up their messes. To these folks, it feels like giving a damn is a huge career liability in your organization. <strong>Because it is.</strong></p></div></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA['It's all gone': CAR-T therapy forces autoimmune diseases into remission (150 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-023-03968-6</link>
            <guid>38645340</guid>
            <pubDate>Thu, 14 Dec 2023 18:46:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-023-03968-6">https://www.nature.com/articles/d41586-023-03968-6</a>, See on <a href="https://news.ycombinator.com/item?id=38645340">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <header>
        <div>
            <ul data-test="article-identifier">
                <li data-test="article-category"><span>NEWS</span></li>
                <li><time datetime="2023-12-12">12 December 2023</time></li>
                
            </ul>

            

            <div>
                
                <p>
                    Engineered immune cells, most commonly used to treat cancers, show their power against lupus and other immune disorders.
                </p>
            </div>
        </div>
        
            <div data-test="author-info">
    <ol>
        
            <li>
                
                    <span>Heidi Ledford</span>
                
                
                
                    
                
            </li>
        
    </ol>
</div>
        
    </header>
    
</div><div>
                    <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-03968-6/d41586-023-03968-6_26512420.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-03968-6/d41586-023-03968-6_26512420.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Composite coloured scanning electron micrograph (SEM) of T-cells (small round) and a cervical cancer cell (Hela)" loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-03968-6/d41586-023-03968-6_26512420.jpg">
  <figcaption>
   <p><span>T cells (smaller cells) can be engineered to recognize cancer cells — and also other immune cells.</span><span>Credit: Steve Gschmeissner/Science Photo Library</span></p>
  </figcaption>
 </picture>
</figure><p>Engineered immune cells have given 15 people with once-debilitating <a href="https://www.nature.com/articles/d41586-021-01833-y" data-track="click" data-label="https://www.nature.com/articles/d41586-021-01833-y" data-track-category="body text link">autoimmune disorders</a> a new lease on life, free from fresh symptoms or treatments. The results raise hopes that the approach — called <a href="https://www.nature.com/articles/d41586-020-02675-w" data-track="click" data-label="https://www.nature.com/articles/d41586-020-02675-w" data-track-category="body text link">CAR-T-cell therapy</a> — might one day be <a href="https://www.nature.com/articles/d41586-021-01840-z" data-track="click" data-label="https://www.nature.com/articles/d41586-021-01840-z" data-track-category="body text link">extended to a variety of other conditions fuelled by rogue immune cells</a> that produce antibodies against the body’s own tissues.</p><p>All 15 participants, who each had one of three <a href="https://www.nature.com/articles/d41586-021-01834-x" data-track="click" data-label="https://www.nature.com/articles/d41586-021-01834-x" data-track-category="body text link">autoimmune conditions</a>, have remained disease-free or nearly so since their treatment, according to <a href="https://ash.confex.com/ash/2023/webprogram/Paper180547.html" data-track="click" data-label="https://ash.confex.com/ash/2023/webprogram/Paper180547.html" data-track-category="body text link">data presented on 9 December</a> at the <a href="https://www.hematology.org/meetings/annual-meeting" data-track="click" data-label="https://www.hematology.org/meetings/annual-meeting" data-track-category="body text link">American Society of Hematology meeting</a> in San Diego, California. The first participants were treated more than two years ago.</p><p>These successes, although preliminary, have been electric, says Marco Ruella, an oncologist at the University of Pennsylvania in Philadelphia. “We’re all excited,” he says. “There’s a lot of potential.”</p><h2>Bespoke immune cells</h2><p><a href="https://www.nature.com/articles/d41586-022-00241-0" data-track="click" data-label="https://www.nature.com/articles/d41586-022-00241-0" data-track-category="body text link">CAR-T therapies</a> harness the immune players called T cells. T cells are removed from the person being treated, genetically engineered to produce proteins called chimeric antigen receptors (CARs) and then reintroduced to the person’s body. In many therapies, the T cells are tailored to recognize a protein made by immune cells called B cells. When reintroduced, the CAR T cells will target the B cells for destruction — a useful feature for treating cancers caused by abnormal B cells.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-00177-z" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-03968-6/d41586-023-03968-6_23971308.png"><p>The race to supercharge cancer-fighting T cells</p></a>
 </article><p>B cells also drive some autoimmune disorders by making antibodies that attack healthy tissue. In 2019, researchers showed that CAR T cells that recognize B cells reduced symptoms in mice with a disease similar to lupus, an autoimmune disorder that affects a variety of organs<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>.</p><p>Around the same time, researchers at University Hospital Erlangen in Germany were setting up their own CAR-T centre to provide cancer treatment. During a meeting at the centre, a rheumatologist asked the cancer specialists for advice about a young woman with a form of lupus called systemic lupus erythematosus. Several of her organs were failing; her doctors estimated that she did not have long to live. The young woman insisted that they try something new.</p><h2>High-risk approach</h2><p>The team thought of the mouse study but baulked at trying it in people. CAR-T therapy can have severe side effects, and recipients must first undergo intensive chemotherapy that kills off many of their existing immune cells. “At the beginning we were quite scared,” said team member Fabian Müller, an oncologist at the Friedrich–Alexander University of Erlangen–Nuremberg, at a press conference before he presented the work at the San Diego meeting. The woman was adamant that they try.</p><p>That first participant — and the others who followed — experienced relatively minor adverse effects, Müller reported at the conference. The Erlangen team eventually used the method to treat two other autoimmune disorders: systemic sclerosis and idiopathic inflammatory myositis. The successes continued.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-022-04465-y" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-03968-6/d41586-023-03968-6_23945576.jpg"><p>Cancer treatments boosted by immune-cell hacking</p></a>
 </article><p>Other groups have since taken up the approach and reported similar results. Earlier this month, another team added a fourth autoimmune disorder called myasthenia gravis to the list of successes<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup>. Researchers are beginning to wonder how long the final list will be. “We’re just at the beginning,” says Marcela Maus, who designs CAR-T therapies against cancer at Massachusetts General Hospital in Boston. “There is so much that can be done that was unthinkable just a decade ago.”</p><p>At this stage, however, it’s unclear how much of this success is due to the CAR-T therapy as opposed to the chemotherapy that killed many of the participants’ pre-existing immune cells, cautions Ruella. That might have helped to wipe out the errant B cells.</p><p>For now, Müller lapses into a dreamy smile as he marvels over the remarkable recoveries he has seen: the man who struggled to walk 10 metres before his treatment and now routinely walks 10 kilometres around town, for example. “These are young people that have been spending more time with their doctors than with their friends,” he says. “They would describe their breakfast as a handful of pills that they are just shoving in.”</p><p>“And it’s all gone,” he says. “From the physician perspective, it’s pretty much the most pleasing thing.”</p>
                </div><div id="references" aria-labelledby="Bib1"><h2 id="Bib1">References</h2></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Interviewing my mother, a mainframe COBOL programmer(2016) (391 pts)]]></title>
            <link>https://ezali.substack.com/p/interviewing-my-mother-a-mainframe</link>
            <guid>38645159</guid>
            <pubDate>Thu, 14 Dec 2023 18:34:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ezali.substack.com/p/interviewing-my-mother-a-mainframe">https://ezali.substack.com/p/interviewing-my-mother-a-mainframe</a>, See on <a href="https://news.ycombinator.com/item?id=38645159">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><em><a href="https://homedoctorbook.com/book/#aff=aboel3z" rel="nofollow ugc noopener">The Home Doctor - Practical Medicine for Every Household </a><span>- is a 304 page doctor written and approved guide on how to manage most health situations when help is not on the way.</span></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72fc8dfb-b677-43fd-92b4-8ffde4d4e99b_800x600.jpeg" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72fc8dfb-b677-43fd-92b4-8ffde4d4e99b_800x600.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72fc8dfb-b677-43fd-92b4-8ffde4d4e99b_800x600.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72fc8dfb-b677-43fd-92b4-8ffde4d4e99b_800x600.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72fc8dfb-b677-43fd-92b4-8ffde4d4e99b_800x600.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72fc8dfb-b677-43fd-92b4-8ffde4d4e99b_800x600.jpeg" width="800" height="600" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/72fc8dfb-b677-43fd-92b4-8ffde4d4e99b_800x600.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:600,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72fc8dfb-b677-43fd-92b4-8ffde4d4e99b_800x600.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72fc8dfb-b677-43fd-92b4-8ffde4d4e99b_800x600.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72fc8dfb-b677-43fd-92b4-8ffde4d4e99b_800x600.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72fc8dfb-b677-43fd-92b4-8ffde4d4e99b_800x600.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>My mother has been working for one of the largest banks in the EU since before I was born and I’ve always been fascinated by her line of work, especially these last years since I’ve become a programmer myself. I’ve been asked to interview her plenty of times, and finally decided to do so.</p><p>Some notes</p><p>The banking programming world is a completely different world than what most of us are used to, and for the next couple of hours after posting this I’ll accept whatever questions you may have for her, but keep in mind that some questions may not be answered due to security concerns. I’ll be posting this to HN &amp; Reddit and will answer questions over there as well.</p><p>I won’t write this as a Q&amp;A, but more like I’m telling you her story.</p><p>UPDATE: I’ve added Q&amp;A of some FAQ to the bottom of the post!</p><p>1991</p><p>The year she started internal training at Nordea, which back then was known as Nordbanken (The North Bank) but changed name to Nordea in 2001. During the training she had to take various tests, most notably an IQ-test to see if she had the intelligence to work within this field. Other tests includes a psychological checkup to make sure she had the psyche to handle that line of work and a multitasking test which she failed horribly where she got the score 22/100. She did very good on the other tests and among the 16 available positions, she managed to get one.</p><p>The position in question was an IBM Mainframe COBOL programmer, which to this day, 25 years later still work as for the same bank.</p><p>This position is the most important one in the bank, at least from a technical standpoint. If, let’s say, my mother and everyone on her team would quit their job, the bank would go under within a matter of weeks if they’re lucky. They have a rotation of people on her team being available 24/7. I remember when I was younger and she had to take a taxi to work in the middle of the night on a Sunday to fix a dead-lock problem.</p><p><strong>COBOL…</strong></p><p>…is not a fancy programming language like your functional Haskell or concurrent Golang— it’s an imperative, procedural language and since 2002, object-oriented. There’s nothing wrong with the language itself, the problem is that barely anyone knows it — at least not in the context of mainframe programming. My mother is the next youngest person on her team, and she’s born 1964, and the youngest person being 2 years younger. Since almost all of the largest banks in the world runs on IBM Mainframe with COBOL as the primary programming language, this is a global issue. The smaller banks however are better off which usually runs something like Java without mainframes.</p><p>My mother used to ask me if I wanted to learn, but since working with the more fancier stuff like Postgres, Redis, Node, Crystal, PHP, among others, I’ve always answered “Never, ever!”. I am still very interested in what she do, but these types of systems gives off the probably worst enterprisy-feeling that you can imagine, which I’d like to avoid.</p><p>I can only imagine the fat paycheck a 20-year old mainframe programmer would get though, because your age in this case would be invaluable.</p><p><strong>Databases</strong></p><p>Their primary database is called IMS, which is an hierarchical database built by IBM for the Apollo program. Internally they call it DL/1 which is short for Database Language One. They are trying to migrate to DB2 which is a relational database that speaks normal SQL, but considering the sheer volume of data that Nordea is storing, this is a task that is going to take years. It’s not as simple as just moving the data from IMS into DB2, they also have to update their modules to load &amp; save data from DB2 instead of IMS and they have thousands of modules, many of which were developed by programmers that have either passed away or have retired.</p><p>Each transaction is stored in DB2. They’re avoiding writing to IMS as much as possible, only reading data from it until their newly purchased system is fully integrated and they can start storing data there.</p><p>IMS is extremely old and very slow (for some tasks).</p><p>Searching for data can take hours. Hah, and here we are, arguing that MySQL delivers 2ms better query speed than Postgres. That’s a bit ironic.</p><p>They also use a flat-file structure for a lot of tasks, which themselves has different flavors that IMS supports. One such example is GSAM which has forced my mother to take a taxi into work many times due to modules working on the same GSAM-file at the same time, producing deadlocks.</p><p>Let’s talk about the size of their databases for a moment. DB2 is only storing data about transactions, and each transaction is different in size depending on what type of account the transaction goes to/from. Private accounts like my personal bank account is orders of magnitude more simple than bank accounts used by business. Each transaction is somewhere between 2KB to 500 bytes, so let’s say the average is 1KB.</p><p>At the moment, their DB2 database holds 11 billion transactions, and the law requires them to save each transaction for 10 years, having settled for 11 years. At the moment, transactions are only 7 years old and the amount of transactions is estimated to grow roughly 5–8% each year, until they hit the 11 year mark where they can nuke transactions that are older than 11 years.</p><p>So at the moment, the DB2 database holds about 10TB of data, and that data is only transactions. In 4 years, it’ll be somewhere around 13–14TB of data.</p><p>IMS holds most other information. That kind of data differs a lot in size, so it’s hard to estimate how large it is, but I guess it’s much larger than their DB2 database.</p><p>They also store some data on tapes, where their programs tell a machine to load/save data from a specific tape, which is pretty cool!</p><p>IDE</p><p>Everyone has some form of IDE or text editor, right? Well, yes they do. The IDE is called ISPF which is like an entire OS. ISPF can be extended, and the part we would call an IDE is an extension to ISPF which is called Endevor.</p><p>ISPF is directly connected to the mainframe, and there’s no such thing as a local development environment here.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50306661-9869-4d8a-8105-f199c256eb56_636x343.gif" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50306661-9869-4d8a-8105-f199c256eb56_636x343.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50306661-9869-4d8a-8105-f199c256eb56_636x343.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50306661-9869-4d8a-8105-f199c256eb56_636x343.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50306661-9869-4d8a-8105-f199c256eb56_636x343.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50306661-9869-4d8a-8105-f199c256eb56_636x343.gif" width="636" height="343" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/50306661-9869-4d8a-8105-f199c256eb56_636x343.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:343,&quot;width&quot;:636,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50306661-9869-4d8a-8105-f199c256eb56_636x343.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50306661-9869-4d8a-8105-f199c256eb56_636x343.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50306661-9869-4d8a-8105-f199c256eb56_636x343.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50306661-9869-4d8a-8105-f199c256eb56_636x343.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Roughly 80% of their systems are batch jobs. These are jobs that runs at a certain time or interval, doing some processing on their data or sends data to other banks/agencies/etc. For example when I buy a can of Coke, the money is withdrawn from my account balance, however the money is not actually transferred anywhere until one of their batch jobs does so. These jobs are usually executed during the night, which is usually why it takes a day before transactions between banks are completed. Transactions to the same bank are usually instant because it executes immediately.</p><p>Batch jobs are working with datasets on a gigabyte-, sometimes terrabyte-level and can in some cases take hours to complete. I’d love to see how IBMs mainframes all of a sudden starts working at full capacity the second the clock turns 00:00 in their datacenters, would be pretty cool!</p><p><strong>The problems banks are facing</strong></p><p>Banks that run on mainframes have quite a lot of issues that they need to take care of, but unfortunately time is scarce.</p><p>Programmers are getting older, not many wants to learn and the time before a new employee can stand on their own feet is 2–3 years, and even then there are a lot of gray areas for them.</p><p>There are programs that are decades old that nobody even knows what they do and the person who wrote it is long gone.</p><p>Many banks are trying to purchase shiny new systems to replace what they currently have, while almost all of their programmers are saying “this is not going to work, you cannot replace this monster that we have here”.</p><p>The reason their systems has become so huge is partially because they made the mistake early on to tailor their own system towards other systems. A great example here is how we build RESTful services, where programs that wants to get data from us have to tailor their system to match ours. This is not the case with some banks, instead they’re sending data to other systems (such as the tax agency) which may at any point update their systems which in turn breaks the current system the bank are using to send the data.</p><p>Banking systems are also extremely advanced. A personal bank account differs a lot from a business bank account, and there are at least 50 different types of bank accounts for each of them. And in Nordeas case, they also have the Swedish government accounts, which are different from both personal and business accounts. I think they have the Finnish government accounts and maybe a portion of Denmarks as well, which differs too.</p><p><span>It’s going to be very interesting to hear what Nordea and other banks will do these upcoming years, and what new tech they’ll be implemented on.</span><br><span>I may have forgot to ask something, so if you have any questions go ahead and ask, I’ll be forwarding them to her and write down her response for the next couple of hours.</span></p><p><strong><span>Why did you choose to work with IBM Mainframe COBOL programming?</span><br></strong><span>- I’ve always wanted to “work with computers”, but I wasn’t aiming for this specific job. I saw an ad in the papers and applied and got the job. I had a bit of computer background before I applied though.</span></p><p><strong>What is the worst thing that you’ve seen on a day?</strong><br><span>- My co-worker forgot to add a dot to the end of a statement for a module in the most critical part of our system which we call “The Cash Register”. It is the part that handles all the money. The result was that the entire bank was down for 16 hours straight due to the module continiuing executing statements when it actually should’ve stopped after that statement. It basically overloaded our systems, a self-DOS of sorts.</span></p><p><strong>What do you think will happen in the future for banks that run on the same infrastructure as Nordea?</strong><br><span>- Most banks has acknowledged that we need to replace the older mainframes with something more modern. Nordea has bought a new system which it has promised to replace the current one within 4 years, but Nordea takes care of multiple countries and a more reasonable number is 4 years per country, so 16 years in total.</span><br><span>Banks and finance that haven’t acknowledged this is going to have a very rough time ahead of them. I still think we’ll live on IBM infrastructure though.</span></p><p><strong>What were the challenges you’ve faced as a female programmer that started in the 90's?</strong><br><span>- No problem at all. I’ve got a couple of female co-workers, but most are men. It doesn’t bother me that much.</span></p><p><strong>You’ve been working for the same entity and possibly the same code base for more than 20 years. Does it ever gets old?</strong><br><span>- Yeah it definitely gets tiring after a while, like most other lines of work becomes. But during my time here I’ve build many completely new systems for various things in finance, and that has always been fun. Unfortunately from now on we will never build anything new, only maintain what we currently have and wait for the new system we’ve purchased to replace the current one.</span></p><p><strong>How scary is it to write code for a bank?</strong><br><span>- It is very scary, especially when we push changes to production which happens on Sundays. Whenever we push new changes to production, huge parts of the system has to be taken offline. One of those parts is the entire IMS. During that period anything can happen, but we have a very robust test environment so it usually goes through smoothly.</span></p><p><strong>Have you caused any huge mistakes for the bank?</strong><br><span>- Definitely have, quite a large mistake I did back in 1997, when my youngest son (that’s me, the writer) just started pre-school and my parental leave was over. We have a system for saving money for retirement. Those type of bank accounts wasn’t locked back then, and the law says you cannot withdraw money from that account before you’re 55 years old. Since the accounts wasn’t locked, it was possible to withdraw money if you had the bank account number, so the solution was simple; not give the customer their bank account number.</span><br><span>I managed to screw up by modifying a module that added their bank account number to the mail that was sent out to customers. So what happened was that customers started withdrawing money (which was not taxed yet!) from their retirement savings account before they were allowed to.</span><br><span>This triggered a huge inspection, the Swedish government stepped in, the financial inspection and the media were all over it. That was me.</span></p><p><strong>What’s your working environment like?</strong><br><span>- We’ve recently moved to a more “hip” location. We used to have personal desks, but now we have this “pick whatever spot is avaiable” open area. I dislike it a lot.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intel CEO: 'The entire industry is motivated to eliminate the CUDA market' (288 pts)]]></title>
            <link>https://www.tomshardware.com/tech-industry/artificial-intelligence/intel-ceo-attacks-nvidia-on-ai-the-entire-industry-is-motivated-to-eliminate-the-cuda-market</link>
            <guid>38645021</guid>
            <pubDate>Thu, 14 Dec 2023 18:24:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/intel-ceo-attacks-nvidia-on-ai-the-entire-industry-is-motivated-to-eliminate-the-cuda-market">https://www.tomshardware.com/tech-industry/artificial-intelligence/intel-ceo-attacks-nvidia-on-ai-the-entire-industry-is-motivated-to-eliminate-the-cuda-market</a>, See on <a href="https://news.ycombinator.com/item?id=38645021">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture><source type="image/webp" alt="Pat Gelsinger " onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-1920-80.jpg.webp 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3.jpg"><source type="image/jpeg" alt="Pat Gelsinger " onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-1920-80.jpg 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3.jpg"><img src="https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-320-80.jpg" alt="Pat Gelsinger " onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3-1920-80.jpg 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3.jpg"></picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/UUt6w69oGjw6coQaQr2rV3.jpg">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span>
</figcaption>
</div>

<div id="article-body">
<div><p>Intel CEO Pat Gelsinger came out swinging at Nvidia's CUDA technology, claiming that inference technology will be more important than training for AI as he launched </p><a data-analytics-id="inline-link" href="https://www.tomshardware.com/laptops/intel-core-ultra-meteor-lake-u-h-series-specs-skus" data-before-rewrite-localise="https://www.tomshardware.com/laptops/intel-core-ultra-meteor-lake-u-h-series-specs-skus"><u>Intel Core Ultra</u></a><p> and </p><a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/cpus/intel-emerald-rapids-5th-gen-xeon-platinum-8592-review-64-cores-320mb-of-l3-and-350w-tdp" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/cpus/intel-emerald-rapids-5th-gen-xeon-platinum-8592-review-64-cores-320mb-of-l3-and-350w-tdp"><u>5th Gen Xeon</u></a><p> datacenter chips in an event here in New York City. Taking questions at the NASDAQ, Gelsinger suggested that Nvidia’s CUDA dominance in training wouldn't last forever.</p><p>

"You know, the entire industry is motivated to eliminate the CUDA market," Gelsinger said. He cited examples such as MLIR, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tag/google" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.tomshardware.com/tag/google">Google</a>, and OpenAI, suggesting that they are moving to a "Pythonic programming layer" to make AI training more open.</p><p>

"We think of the CUDA moat as shallow and small," Gelsinger went on. "Because the industry is motivated to bring a broader set of technologies for broad training, innovation, data science, et cetera."</p><p>

But Intel isn't relying just on training. Instead, it thinks inference is the way to go.</p></div><p>"As inferencing occurs, hey, once you've trained the model… There is no CUDA dependency," Gelsinger continued. "It's all about, can you run that model well?" He suggested that with Gaudi 3, shown on stage for the first time, that Intel will be up to the challenge, and will be able to do it as well with Xeon and edge PCs. Not that Intel won't compete in training, but "fundamentally, the inference market is where the game will be at," Gelsinger said.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" alt="Intel Gaudi 3, shown on stage for the first time" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/tomshardware/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-1200-80.jpg.webp 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB.jpg"><source type="image/jpeg" alt="Intel Gaudi 3, shown on stage for the first time" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/tomshardware/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB.jpg"><img src="https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB.jpg" alt="Intel Gaudi 3, shown on stage for the first time" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/tomshardware/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB.jpg" srcset="https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/NdkzqapuTGSSq2agzqZkrB-1200-80.jpg 1200w"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Tom's Hardware)</span></figcaption></figure><div><p>He also took the opportunity to push OpenVINO, the standard that Intel has gathered around for its AI efforts, and predicted a world of mixed computing, some that occurs in the cloud, and others that happen on your PC.</p><p>

Sandra Rivera, executive vice president and general manager of the Data Center and AI Group at Intel, added that Intel's scale from the data center to the PC may make it a partner of choice, as it can produce at volume.</p><p>

"We're going to compete three ways for 100% of the datacenter AI TAM." Gelsinger said, tacking onto Rivera's comment. "With our leadership CEOs, leadership accelerators, and as a foundry. Every one of those internal opportunities is available to us: The TPUs, the inferentias, the trainiums, et cetera. We're going to pursue all of those. And we're going to pursue every commercial opportunity as well, with NVIDIA, with AMD, et cetera. We're going to be a foundry player."</p><p>

It's a bold strategy, and Gelsinger appeared confident as he led his team through presentations today. Can he truly take on CUDA? Only time will tell as applications for the chips Intel launched today — and that his competitors are also working on — become more widespread.</p></div>
</div>
<div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-TXVaYh6bwbpJ4oe3CxqqjP"><section><p>Join the experts who read Tom's Hardware for the inside track on enthusiast PC tech news — and have for over 25 years. We'll send breaking news and in-depth reviews of CPUs, GPUs, AI, maker hardware and more straight to your inbox.</p></section></div>
<div id="slice-container-authorBio-TXVaYh6bwbpJ4oe3CxqqjP"><p>Andrew E. Freedman is a senior editor at Tom's Hardware focusing on laptops, desktops and gaming. He also keeps up with the latest news. A lover of all things gaming and tech, his previous work has shown up in Tom's Guide, Laptop Mag, Kotaku, PCMag and Complex, among others. Follow him on Twitter: <a href="https://twitter.com/FreedmanAE">@FreedmanAE</a></p></div>



<!-- Drop in a standard article here maybe? -->


</section>





<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mozilla expands extension support for Firefox for Android (487 pts)]]></title>
            <link>https://blog.mozilla.org/en/mozilla/new-extensions-youll-love-now-available-on-firefox-for-android/</link>
            <guid>38644608</guid>
            <pubDate>Thu, 14 Dec 2023 17:58:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.mozilla.org/en/mozilla/new-extensions-youll-love-now-available-on-firefox-for-android/">https://blog.mozilla.org/en/mozilla/new-extensions-youll-love-now-available-on-firefox-for-android/</a>, See on <a href="https://news.ycombinator.com/item?id=38644608">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
  <main id="main">

    
<article id="post-73805">
  

  <div>
    
<figure><img decoding="async" fetchpriority="high" width="1024" height="576" src="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2023/12/FX_Distilled_WorldofExtensions_blogheader_1920x1080_blue-1024x576.jpg" alt="" srcset="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2023/12/FX_Distilled_WorldofExtensions_blogheader_1920x1080_blue-1024x576.jpg 1024w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2023/12/FX_Distilled_WorldofExtensions_blogheader_1920x1080_blue-300x169.jpg 300w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2023/12/FX_Distilled_WorldofExtensions_blogheader_1920x1080_blue-768x432.jpg 768w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2023/12/FX_Distilled_WorldofExtensions_blogheader_1920x1080_blue-1536x864.jpg 1536w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2023/12/FX_Distilled_WorldofExtensions_blogheader_1920x1080_blue-1000x562.jpg 1000w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2023/12/FX_Distilled_WorldofExtensions_blogheader_1920x1080_blue-1279x719.jpg 1279w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2023/12/FX_Distilled_WorldofExtensions_blogheader_1920x1080_blue.jpg 1921w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Today, Mozilla announced more than 450 new extensions (software that adds new features or functionality to the browser) to users on Firefox for Android at Mozilla’s <a href="https://addons.mozilla.org/en-US/android/">Addons.mozilla.org (AMO) Android page</a>. This milestone marks the launch of a new open extension ecosystem on mobile where developers are now free to create and publish extensions and users can easily access and install them on Firefox for Android.</p>



<p>“Extensions were first created as a way for people to customize their own internet experience, from <a href="https://blog.mozilla.org/en/products/firefox/madonna-firefox-theme-designer/">artists designing themes</a> to developers who wanted to make extensions to improve people’s web experience,” said Vicky Chin, Vice President of Engineering at Firefox. “We’re thrilled to bring this experience to Firefox for Android, where we’re the only major Android browser to support an open extension ecosystem. In the coming months, we plan to enable more extensions for people to choose from and customize their own mobile internet experience.”&nbsp;</p>



<h2>Our journey to an open extension ecosystem to benefit all</h2>



<p>Browser extensions have become an essential part of everyone’s daily internet experience. Nearly half of all Firefox desktop users have installed an extension to customize their online experience. Extensions provide a wide array of powerful features — from privacy tools like anti-trackers and ad blockers, to productivity tools, tab managers, translators and so much more.&nbsp;&nbsp;</p>



<p>Built on Mozilla’s mission for an open and accessible internet for all, Firefox works with an independent community of developers to offer extensions for people who want more <a href="https://blog.mozilla.org/en/products/firefox/extensions-addons/become-a-better-writer-with-these-five-extensions-for-firefox/">personal agency</a> out of their online experience. On desktop, there are thousands of extensions to help you become a <a href="https://blog.mozilla.org/en/products/firefox/extensions-addons/become-a-better-writer-with-these-five-extensions-for-firefox/">better writer</a>, <a href="https://blog.mozilla.org/en/products/firefox/firefox-tips/land-your-next-job-with-these-firefox-extensions/">land a job</a> or clean up a <a href="https://blog.mozilla.org/en/products/firefox/extensions-addons/extensions-for-cleaning-up-a-chaotic-desktop/">chaotic desktop</a>. While the new Android ecosystem will take time to develop the robust diversity of content that desktop extensions offer, quite a few major desktop extensions are already available on Firefox for Android, such as privacy focused content blockers <a href="https://addons.mozilla.org/en-US/firefox/addon/ublock-origin/">uBlock Origin</a> and <a href="https://addons.mozilla.org/en-US/firefox/addon/ghostery/">Ghostery</a>, anti-tracking gem <a href="https://addons.mozilla.org/en-US/firefox/addon/privacy-badger17/">Privacy Badger</a> and color customizer <a href="https://addons.mozilla.org/en-US/firefox/addon/darkreader/">Dark Reader</a>.&nbsp;</p>



<p>In August, we <a href="https://blog.mozilla.org/addons/2023/08/10/prepare-your-firefox-desktop-extension-for-the-upcoming-android-release/">announced</a> that we had completed building the infrastructure needed to support an open extension ecosystem on Firefox for Android. We were ready for the next chapter: adding extensions. Since then, we’ve been working with developers to <a href="https://blog.mozilla.org/addons/2023/09/27/test-firefox-android-extensions-and-help-developers-prepare-for-an-open-mobile-ecosystem-in-december/">test</a> and make hundreds of extensions compatible on mobile. So, are you ready to customize your mobile browsing experience to make it faster, safer or simply more fun? Look no further with today’s release of extensions on Firefox for Android.&nbsp;</p>



<h2>Extensions to help while you’re on the go, just in time for the holidays&nbsp;</h2>



<p>We depend on our mobile devices for many things — quick informational searches, reading articles, listening to music, looking for recipes (like cookies for the annual cookie holiday exchange). So, we understand the value of having experiences that are simple, predictable and offer the time to focus. Here are some extensions available today to help achieve that <a href="https://en.wikipedia.org/wiki/Flow_(psychology)">flow</a>.</p>



<ul>
<li><a href="https://addons.mozilla.org/en-US/android/addon/midnight-lizard-quantum/?utm_source=addons.mozilla.org&amp;utm%20_medium=referral&amp;utm_content=search"><strong><em>Midnight Lizard</em></strong></a><strong><em> – Read easier&nbsp;</em><br></strong>We look at our mobile devices in so many different environments. Whether it’s outside with the sun or inside a dimly lit room, our eyes work hard to adjust. Midnight Lizard is one of those extensions your eyes will feel the difference and thank you. It can change the colors of the page, increase or decrease the brightness and contrast. Add a blue light filter, screen shader and of course, the ever-popular night mode. Midnight Lizard will keep your eyes in good shape!</li>



<li><a href="https://addons.mozilla.org/en-US/android/addon/dark-background-light-text/?utm_source=addons.mozilla.org&amp;utm_medium=referral&amp;utm_content=search"><strong><em>Dark Background and Light Text</em></strong></a><strong><em> – Keep it simple</em><br></strong>This extension is well-loved by thousands of users for its simplicity. It’s helpful for folks who prefer to work in dark mode, or those with low vision where reading dark text on a white background is challenging. You’re free to customize it so that all web pages are rendered in this elegant way, or just select pages.</li>



<li><a href="https://addons.mozilla.org/en-US/android/addon/worldwide-radio/"><strong><em>Worldwide Radio</em></strong></a><strong><em> – Get into the groove</em><br></strong>Access more than 50,000 radio stations from all over the world right from your Firefox for Android browser. In the mood for a bit of Brazilian Samba? How about some traditional Indian Hindustani? Techno beats from Berlin? The world’s music and real time talk radio is literally at your fingertips.</li>
</ul>



<h2>A big thank you to our developer community</h2>



<p>We would like to thank all the developers who worked with us to make their extensions compatible for this launch of the open extension ecosystem on Firefox for Android. Hundreds attended our webinars and brought incredible creative energy to this project.&nbsp;</p>



<p>“The opportunity for innovation is vast,” said Giorgio Natili, Firefox Director of Engineering. “It’s thrilling to see extension developers embrace this moment and create novel browsing experiences and features for Firefox for Android users. People don’t have to browse the mobile web in a strictly singular way anymore. With extensions, you’re free to change the way Firefox for Android looks and behaves. It’s only going to get better as more developers innovate within this exciting new space.”</p>



<p>As more developers create mobile optimized content, you can expect a wave of new Firefox for Android extensions to emerge in the coming months. In the meantime, <a href="https://play.google.com/store/apps/details?id=org.mozilla.firefox">download</a> the latest Firefox for Android and shape your own internet experience with <a href="https://addons.mozilla.org/android/">Android extensions</a>.&nbsp;</p>



<a href="https://play.google.com/store/apps/details?id=org.mozilla.firefox">
  <p><img width="800" height="800" src="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2021/10/Visual-Guidelines-800x800.png" alt="" decoding="async" srcset="https://blog.mozilla.org/wp-content/blogs.dir/278/files/2021/10/Visual-Guidelines-800x800.png 800w, https://blog.mozilla.org/wp-content/blogs.dir/278/files/2021/10/Visual-Guidelines-150x150.png 150w" sizes="(max-width: 800px) 100vw, 800px">  </p>
  <div>
     <h3>More than 450 new extensions now available for Firefox on Android users</h3>      <p><span>Download Firefox for Android</span>   </p></div>
</a>
  </div>

</article><!-- #post-73805 -->

  </main><!-- #main -->
  

<div id="related-articles">
    <h2>Related Articles</h2>
    
  </div>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why did older computers and OSes use UPPER case instead of lower case? (147 pts)]]></title>
            <link>https://retrocomputing.stackexchange.com/questions/28141/why-did-older-computers-and-oses-use-upper-case-instead-of-lower-case</link>
            <guid>38644381</guid>
            <pubDate>Thu, 14 Dec 2023 17:44:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://retrocomputing.stackexchange.com/questions/28141/why-did-older-computers-and-oses-use-upper-case-instead-of-lower-case">https://retrocomputing.stackexchange.com/questions/28141/why-did-older-computers-and-oses-use-upper-case-instead-of-lower-case</a>, See on <a href="https://news.ycombinator.com/item?id=38644381">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<h2>TL;DR: Upper Case Is Default for Latin Scripture.</h2>
<p>Latin script is based on upper case and designed around that. Lower case is a later add on (see below) for cursive. Default use case for Latin script is Upper Case.</p>
<hr>
<h2>Character Encoding</h2>
<p>Early codes and machinery used for writing didn't have any case at all, only letters. Lower and upper case glyphs are eye candy, usually not conveying any  substantial meaning. That's why neither <a href="https://en.wikipedia.org/wiki/Morse_code" rel="nofollow noreferrer">Morse</a> nor <a href="https://en.wikipedia.org/wiki/Braille" rel="nofollow noreferrer">Braille</a> carry them. Same goes for any other early transmission code, like the the ITA2 used for teletypes.</p>
<p>After all, there is no need to have two sets when they are fully complementary. It wasn't until the mid 1960s that, with the upcoming ASCII encoding, lower case became a choice to be used.</p>
<h2>Printing Requires a Choice</h2>
<p>The whole point of choosing what case to be used only arises with the need for displaying those letters in print, or later on screen. Here using upper case is only natural, as</p>
<p><strong>Upper Case is</strong></p>
<ul>
<li><p><strong>most commonly accepted</strong></p>
<p>across Europe, independent of culture and writing style due being the oldest form.</p>
</li>
<li><p><strong>best readable</strong>,</p>
<p>due being a two line script (*1) developed for best readability (*2) - a feature that comes quite handy with low quality output (*3).</p>
</li>
<li><p><strong>most inclusive</strong> / <strong>least offensive</strong> one.</p>
<p>Writing all all upper case will always be proper, while writing all lower may be offending to some - like when addressing a person.</p>
</li>
</ul>
<h2>Then There Was Data</h2>
<p>When it comes to data handling it's important to remember that letters were only added as afterthought. Punch card, the most original mean of data storage, had at first only numbers. Next some punctuation (decimal point, currency sign, hash, etc) were added, while letters were the last major modification.</p>
<p>Same goes for equipment to print from punch cards (printing tabulators) or on punch cards (interpreters). They originally also could only do numbers and punctuation, while letters were added last. <a href="https://en.wikipedia.org/wiki/Powers_Accounting_Machine" rel="nofollow noreferrer">Powers</a> first in 1921, while IBM followed in 1931. Of course, when printed they were done so in upper case.</p>
<p>It wasn't until the mid 1960s that lower case became a distinct, additional set on punch cards and mainframes.</p>
<h2>But I Type Lower Case</h2>
<p>Not really. Early machinery, no matter if teletype or terminal, did not produce lower or upper case when typed, but just letters, as there was no case (see above). This single case was usually displayed as uppercase (reasoning see above).</p>
<p>The Apple II is eventually the last major application of that principle. pressing a letter always produced only a single code, no matter if Shift was pressed or not and this letter was displayed in upper case.</p>
<p>Similar worked next to all early computing equipment. When terminals allowing entry of lower and upper case letters became a thing, mainframes still only got a single case delivered as communication equipment (terminal controllers, I/O handler) simply converted both to the single encoding the host used.</p>
<p>That way terminals with lower case and such without would by default be operated all equal, allowing a smooth transition and high degree of compatibility.</p>
<h2>Then Why is Unix Lower Case?</h2>
<p>The main purpose of low end machines of the 1970s was about to get any work done - translation layers like common in mainframes were a luxury one could do without. Unix being a prime example as it was all about getting a certain functionality to production. Combine this with now widely available (ASCII based) lowercase enabled terminals and one gets lower case as default input. Including all the hassles of case sensitive input.</p>
<p>CP/M and in turn MS-DOS being a nice counter example as here the command line is by default translated to upper case, simplifying handling.</p>
<h2>Bottom Line</h2>
<p>There is no single reason and especially not one constructed as an afterthought. It was an evolutionary process influenced by need, capabilities and usage.</p>
<hr>
<h2>History is Upper Case</h2>
<ul>
<li><p>Latin, developed around <strong>7th century BC</strong> was originally a two line script(*1).</p>
</li>
<li><p>By the <strong>first century AD</strong> papyri show sloppy written letters forming lower case (minuscule), which developed into <a href="https://en.wikipedia.org/wiki/Roman_cursive" rel="nofollow noreferrer">Roman Cursive</a>.</p>
</li>
<li><p>Until the <strong>8th century</strong> those forms of  developed into <a href="https://en.wikipedia.org/wiki/Carolingian_minuscule" rel="nofollow noreferrer">Carolingian Minuscule</a> like today's lower case.</p>
</li>
<li><p>It took another 3-500 years (<strong>11th/13th century</strong>) until Gothic script (<a href="https://en.wikipedia.org/wiki/Blackletter" rel="nofollow noreferrer">Blackletter</a>) finalized them to what we know today.</p>
</li>
<li><p>Rules of when to use upper and lower case only became widely accepted during the <strong>16th/17th century</strong>, not at least by development of the printing press.</p>
</li>
<li><p>The English way of writing mostly lower case is even more recent and dates to the <strong>early 18th century</strong>.</p>
</li>
</ul>
<p>Bottom Line: Thruout history it was Upper Case first.</p>
<hr>
<p>*1 - Two line scripts, like old Greek and Latin, uses a single case with all letters of the same height, limited between those two lines. Lower case needs three lines, or four with descender (itself a later, Gothic addition). Might remind you about school, doesn't it :))</p>
<p>*2 - Roman Majuscule developed to be readable from distance and off of low quality material. Roman Minuscule are a form of short hand trading readability for space usage and speed in writing(*4). More appropriate for notes and private correspondence than official exchange.</p>
<p>*3 - Note that your keyboard is (most likely) also labelled with upper case letters. A convention done since early typewriter days. They are until today more easy to identify than lower case - so not (only) heritage.</p>
<p>*4 - <a href="https://en.wikipedia.org/wiki/Ligature_(writing)" rel="nofollow noreferrer">Ligatures</a> make a great point here. They were a further evolution promoting increasing density and speed of cursive - and almost exclusive found with lower case letters. While usually not considered letters in their own, typesetters had dedicated letters for each ligature.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Weak-to-Strong Generalization (139 pts)]]></title>
            <link>https://openai.com/research/weak-to-strong-generalization</link>
            <guid>38643995</guid>
            <pubDate>Thu, 14 Dec 2023 17:20:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/research/weak-to-strong-generalization">https://openai.com/research/weak-to-strong-generalization</a>, See on <a href="https://news.ycombinator.com/item?id=38643995">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><!--[--><!--[--><div><p>A core challenge for aligning future superhuman AI systems (superalignment) is that humans will need to supervise AI systems much smarter than them. We study a simple analogy: can small models supervise large models? We show that we can use a GPT-2-level model to elicit most of GPT-4’s capabilities—close to GPT-3.5-level performance—generalizing correctly even to hard problems where the small model failed. This opens up a new research direction that allows us to directly tackle a central challenge of aligning future superhuman models while making iterative empirical progress today.<br></p></div><!--]--><!--[--><div id="the-superalignment-problem" data-heading=""><p><h2>The superalignment problem</h2></p></div><!--]--><!--[--><div><p>We believe superintelligence—AI vastly smarter than humans—could be developed within the next ten years. However, we still do not know how to reliably steer and control superhuman AI systems. Solving this problem is essential for ensuring that even the most advanced AI systems in the future remain safe and beneficial to humanity.&nbsp;</p><p>We formed the <a href="https://openai.com/blog/introducing-superalignment" rel="noopener noreferrer" target="_blank">Superalignment team</a> earlier this year to solve this problem of superintelligence alignment. Today, we are releasing the team’s first paper, which introduces a new research direction for empirically aligning superhuman models.</p><p>Current alignment methods, such as reinforcement learning from human feedback (RLHF), rely on human supervision. However, future AI systems will be capable of extremely complex and creative behaviors that will make it hard for humans to reliably supervise them. For example, superhuman models may be able to write millions of lines of novel—and potentially dangerous—computer code that would be very hard even for expert humans to understand.&nbsp;</p><p>Relative to superhuman AI models, humans will be “weak supervisors.”<em> </em>This is a core challenge for AGI alignment: how can weak supervisors trust and control substantially stronger models?<br></p></div><!--]--><!--[--><div id="our-setup" data-heading=""><p><h2>Our setup</h2></p></div><!--]--><!--[--><div><p>To make progress on this core challenge, we propose an analogy we can empirically study today: <strong>can we use a smaller (less capable) model to supervise a larger (more capable) model?</strong><br></p></div><!--]--><!--[--><div><figure><p><img src="https://images.openai.com/blob/7f37b3a2-c7fa-4e5d-b5eb-e0a8c6794223/SuperAlignmentBlog_Artwork_Transparent.png?trim=0,0,0,0&amp;width=10&amp;height=10&amp;quality=50" width="2041" height="1164" alt="Superalignmentblog Artwork Transparent" loading="lazy" data-nuxt-img="" sizes="(max-width: 744px) 100vw, (max-width: 1280px) 100vw, (max-width: 1440px) 100vw, 100vw" srcset="https://images.openai.com/blob/7f37b3a2-c7fa-4e5d-b5eb-e0a8c6794223/SuperAlignmentBlog_Artwork_Transparent.png?trim=0,0,0,0&amp;width=400 400w, https://images.openai.com/blob/7f37b3a2-c7fa-4e5d-b5eb-e0a8c6794223/SuperAlignmentBlog_Artwork_Transparent.png?trim=0,0,0,0&amp;width=800 800w, https://images.openai.com/blob/7f37b3a2-c7fa-4e5d-b5eb-e0a8c6794223/SuperAlignmentBlog_Artwork_Transparent.png?trim=0,0,0,0&amp;width=1000 1000w, https://images.openai.com/blob/7f37b3a2-c7fa-4e5d-b5eb-e0a8c6794223/SuperAlignmentBlog_Artwork_Transparent.png?trim=0,0,0,0&amp;width=1400 1400w, https://images.openai.com/blob/7f37b3a2-c7fa-4e5d-b5eb-e0a8c6794223/SuperAlignmentBlog_Artwork_Transparent.png?trim=0,0,0,0&amp;width=2000 2000w, https://images.openai.com/blob/7f37b3a2-c7fa-4e5d-b5eb-e0a8c6794223/SuperAlignmentBlog_Artwork_Transparent.png?trim=0,0,0,0&amp;width=2600 2600w, https://images.openai.com/blob/7f37b3a2-c7fa-4e5d-b5eb-e0a8c6794223/SuperAlignmentBlog_Artwork_Transparent.png?trim=0,0,0,0&amp;width=3200 3200w" aria-hidden="false"></p><figcaption><p><strong>A simple analogy for superalignment: </strong>In traditional machine learning (ML), humans supervise AI systems weaker than themselves (left). To align superintelligence, humans will instead need to supervise AI systems smarter than them (center). We cannot directly study this problem today, but we can study a simple analogy: can small models supervise larger models (right)?<br></p></figcaption></figure></div><!--]--><!--[--><div><p>Naively, we might not expect a strong model to perform better than the weak supervisor that provides its training signal—it may simply learn to imitate all the errors the weak supervisor makes. On the other hand, strong pretrained models have excellent raw capabilities—we don't need to teach them new tasks from scratch, we just need to elicit their latent knowledge. The critical question is then: will the strong model generalize according to the weak supervisor's underlying intent—leveraging its full capabilities to solve the task even on difficult problems where the weak supervisor can only provide incomplete or flawed training labels?<br></p></div><!--]--><!--[--><div id="our-results" data-heading=""><p><h2>Our results</h2></p></div><!--]--><!--[--><!--]--><!--[--><div><figure layout="auto" data-v-22aefd69=""><div data-v-22aefd69=""><!----><!--[--><!--]--><figcaption data-v-22aefd69=""><strong>Typical weak-to-strong generalization across NLP benchmarks:</strong> We use a GPT-2-level model as a weak supervisor to finetune GPT-4.</figcaption></div></figure></div><!--]--><!--[--><div><p>We can significantly improve generalization in many settings. We use a simple method that&nbsp; encourages the strong model to be more confident—including confidently disagreeing with the weak supervisor if necessary. <strong>When we supervise GPT-4 with a GPT-2-level model using this method on NLP tasks, the resulting model typically performs somewhere between GPT-3 and GPT-3.5.</strong> We are able to recover much of GPT-4’s capabilities with only much weaker supervision.</p><p>This method is a proof of concept with important limitations; for example, it still doesn’t work on ChatGPT preference data. However, we also find signs of life with other approaches, such as optimal early stopping and bootstrapping from small to intermediate to large models.</p><p>Collectively, our results suggest that (1) naive human supervision—such as reinforcement learning from human feedback (RLHF)—could scale poorly to superhuman models without further work, but (2) it is feasible to substantially improve weak-to-strong generalization.<br></p></div><!--]--><!--[--><div id="research-opportunities" data-heading=""><p><h2>Research opportunities</h2></p></div><!--]--><!--[--><div><p>There are still important disanalogies between our current empirical setup and the ultimate problem of aligning superhuman models. For example, it may be easier for future models to imitate weak human errors than for current strong models to imitate current weak model errors, which could make generalization harder in the future.&nbsp;</p><p>Nevertheless, we believe our setup captures some key difficulties of aligning future superhuman models, enabling us to start making empirical progress on this problem today. There are many promising directions for future work, including fixing the disanalogies in our setup, developing better scalable methods, and advancing our scientific understanding of when and how we should expect good weak-to-strong generalization.</p><p><strong>We believe this is an exciting opportunity for the ML research community to make progress on alignment.</strong> To kickstart more research in this area,</p><ul><li>We are releasing <a href="https://github.com/openai/weak-to-strong" rel="noopener noreferrer" target="_blank">open source code</a> to make it easy to get started with weak-to-strong generalization experiments today.</li><li>We are launching a <a href="https://openai.com/blog/superalignment-fast-grants" rel="noopener noreferrer" target="_blank">$10 million grants program</a> for graduate students, academics, and other researchers to work on superhuman AI alignment broadly. We’re especially excited to support research related to weak-to-strong generalization.</li></ul><p>Figuring out how to align future superhuman AI systems to be safe has never been more important, and it is now easier than ever to make empirical progress on this problem. We are excited to see what breakthroughs researchers discover.<br></p></div><!--]--><!--]--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What's Going on with Language Rankings? (122 pts)]]></title>
            <link>https://redmonk.com/rstephens/2023/12/14/language-rankings-update/</link>
            <guid>38643923</guid>
            <pubDate>Thu, 14 Dec 2023 17:15:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://redmonk.com/rstephens/2023/12/14/language-rankings-update/">https://redmonk.com/rstephens/2023/12/14/language-rankings-update/</a>, See on <a href="https://news.ycombinator.com/item?id=38643923">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>As the inbound DMs and emails attest, at least a few of you out there having been waiting for RedMonk to drop our latest version of the programming language rankings. Our apologies about the delay: here’s our update on the process.</p>
<p>Normally we run these rankings twice a year, with the goal of using publicly available data sets to track if there are any meaningful changes in how people are using programming languages. We correlate cumulative language usage as seen through non-forked PRs on public GitHub repos against questions asked on Stack Overflow.</p>
<p>These metrics have always told an incomplete story at best. There are languages that were under- or over-represented by using public GitHub repos; there are communities that were under- or over-represented by looking at discussions happening in Stack Overflow. These metrics were not perfect by any means, but the two large, publicly facing datasets created an interesting decade plus long trend for RedMonk to track over time.</p>
<p>Our last set of queries, however, provided us with data that required further investigation.</p>
<h2>Stack Overflow</h2>
<p>We’ve been internally discussing how we’re going to address the impact of AI-based code assistants on our language rankings since GitHub released Copilot in October 2021. However, it was when ChatGPT hit the market on November 30, 2022 and went from <a href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/">0 to 100M users in two months</a> that we started seeing undeniable impacts on our source data.</p>
<p>We’d already seen questions asked falling off from their peak. However, when we pulled data for our June 2023 rankings the collective decline was notable.</p>
<p>The below chart takes questions asked on Stack Overflow about the top 20 programming languages (as determined from <a href="https://redmonk.com/sogrady/2023/05/16/language-rankings-1-23/">RedMonk’s analysis dated January 2023</a>) and look backed at historical semi-annual trends.</p>
<p><a href="https://redmonk.com/rstephens/files/2023/12/stackovertime.png"><img decoding="async" src="https://redmonk.com/rstephens/files/2023/12/stackovertime.png" alt="Bar chart showing number of questions tagged on Stack Overflow for our top 20 languages. Questions peak in 2016-17 and begin to fall sharply in last year" width="100%" height="1000" srcset="https://redmonk.com/rstephens/files/2023/12/stackovertime.png 1000w, https://redmonk.com/rstephens/files/2023/12/stackovertime-520x520.png 520w, https://redmonk.com/rstephens/files/2023/12/stackovertime-150x150.png 150w, https://redmonk.com/rstephens/files/2023/12/stackovertime-768x768.png 768w, https://redmonk.com/rstephens/files/2023/12/stackovertime-480x480.png 480w, https://redmonk.com/rstephens/files/2023/12/stackovertime-300x300.png 300w, https://redmonk.com/rstephens/files/2023/12/stackovertime-627x627.png 627w" sizes="(max-width: 1000px) 100vw, 1000px"></a></p>
<p>As you can see, the number of questions asked using these 20 Stack Overflow tags* declined almost 20% from the prior period. And this is just our first full period running these numbers post-ChatGPT. A cursory query about YTD indicates an even more stark change.</p>
<p>It is undeniable that developer’s ability to instantaneously ask questions of a non-judgmental AI assistant that will give answers in context is going to have a marked negative impact on the usefulness of the public datasets provided by Stack Overflow.</p>
<h2>GitHub</h2>
<p>While we expected a change in Stack Overflow, we were not expecting significant anomalies in our data from GitHub (and to the extent that we were anticipating changes, we expected them to be upwards. The narrative about AI code assistants aiding development velocity and <a href="https://redmonk.com/kholterhoff/2023/06/27/ai-is-for-tinkerers/">benefitting tinkerers</a> has been prevalent.)</p>
<p>However, the data we saw from GitHub Archive actually showed a roughly 25% decline in pull requests in 1H2023 as compared to 2H2022 PRs that we were not expecting.</p>
<p>The dataset we use is a public dataset on BigQuery, and so we asked questions of both Google in terms of how the data was pulled in and of the GitHub team to see if they had seen similar changes on in their internal data.</p>
<p>In the end, the change appears to not be an error in the available data and is largely lacking an explanation. The best guess thus far is that there was an overhang in increased activity from the pandemic and this is a return to expected activity, but we have no way of confirming whether that storyline is accurate.</p>
<p>As of now this is not a declining trend we expect to continue, but these are numbers we will continue to watch with interest.</p>
<h2>What Next?</h2>
<p>The advent and rise of AI-based code assistants are already impacting the data that populates RedMonk’s language rankings. As questions and knowledge sharing moves from public forums to private tools, our ability to ascertain meaningful trends from said public data will be indefinitely altered.</p>
<p>We will continue to track these trends and make determinations about how this change in sample size will impact our ability to perform the rankings.</p>
<p>As of now, look for our next rankings in January 2024.</p>
<hr>
<p>* <strong>Tags queried based on last Top 20:</strong> JavaScript, Python, Java, PHP, C#, CSS, TypeScript, C++, Ruby, C, Swift, Shell, R, Go, Scala, Objective-C, Kotlin, PowerShell, Rust, Dart</p>
<p><strong>Disclaimer:</strong> GitHub and Google Cloud are RedMonk clients. Stack Overflow and OpenAI (ChatGPT) are not.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Canadian internet bill S-210 is a step closer to becoming law (220 pts)]]></title>
            <link>https://www.michaelgeist.ca/2023/12/the-most-dangerous-canadian-internet-bill-youve-never-heard-of-is-a-step-closer-to-becoming-law/</link>
            <guid>38643369</guid>
            <pubDate>Thu, 14 Dec 2023 16:43:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.michaelgeist.ca/2023/12/the-most-dangerous-canadian-internet-bill-youve-never-heard-of-is-a-step-closer-to-becoming-law/">https://www.michaelgeist.ca/2023/12/the-most-dangerous-canadian-internet-bill-youve-never-heard-of-is-a-step-closer-to-becoming-law/</a>, See on <a href="https://news.ycombinator.com/item?id=38643369">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content" role="main">

	<div><p><img src="https://www.michaelgeist.ca/wp-content/uploads/2021/09/34462663835_aff7fad1fb_k-780x350.jpg" alt="2017 Freedom of Expression Awards by Elina Kansikas for Index on Censorship https://flic.kr/p/Uvmaie (CC BY-SA 2.0)" width="780" height="350"></p><p>2017 Freedom of Expression Awards by Elina Kansikas for Index on Censorship https://flic.kr/p/Uvmaie (CC BY-SA 2.0)</p></div>

	<span><a href="https://www.michaelgeist.ca/category/news/" rel="category tag">News</a></span>
	


	
		<p>

						December 14, 2023
						</p><!-- /.post-meta -->


		<div id="post-19923">
				<p><span>After years of battles over Bills C-11 and C-18, few Canadians will have the appetite for yet another troubling Internet bill. But given a bill that envisions government-backed censorship, mandates age verification to use search engines or social media sites, and creates a framework for court-ordered website blocking, there is a need to pay attention. <a href="https://www.parl.ca/DocumentViewer/en/44-1/bill/S-210/third-reading">Bill S-210</a>, or the </span><em><span>Protecting Young Persons from Exposure to Pornography Act</span></em><span>, was passed by the Senate in April after Senators were reluctant to reject a bill framed as protecting children from online harm. The same scenario appears to be playing out in the House of Commons, where yesterday a majority of the House <a href="https://x.com/HoCChamber/status/1735038570753102106?s=20">voted for the bill at second reading</a>, sending it to the Public Safety committee for review. The bill, which is the brainchild of <a href="https://x.com/mivillej/status/1735041108764188967?s=20">Senator Julie Miville-Duchêne</a>, is not a government bill. In fact, government ministers voted against it. Instead, the bill is backed by the Conservatives, Bloc and NDP with a smattering of votes from backbench Liberal MPs. Canadians can be forgiven for being confused that after&nbsp;months of championing Internet freedoms, raising fears of censorship, and expressing concern about CRTC overregulation of the Internet, Conservative MPs were quick to <a href="https://x.com/GarnettGenuis/status/1735043817965764793?s=20">call out</a> those who <a href="https://x.com/annarobertsmp/status/1735080924012818833?s=20">opposed the bill </a>(the House sponsor is Conservative MP Karen Vecchio).&nbsp;</span></p>
<p><span>I appeared before the <a href="https://www.michaelgeist.ca/2022/02/age-verification-requirements-for-twitter-or-website-blocking-for-reddit-my-appearance-on-bill-s-210-at-the-senate-standing-committee-on-legal-and-constitutional-affairs/"><span>Senate committee that studied the bill in February 2022</span></a>, where I argued that “</span><span>by bringing together website blocking, face recognition technologies, and stunning overbreadth that would capture numerous mainstream services, the bill isn’t just a slippery slope, it is an avalanche.” As I did then, I should preface criticism of the bill by making it clear that underage access to inappropriate content is indeed a legitimate concern. I think the best way to deal with the issue includes education, digital skills, and parental oversight of Internet use including the use of personal filters or blocking tools if desired. Moreover, if there are Canadian-based sites that are violating the law in terms of the content they host, they should absolutely face investigation and potential charges. </span></p>
<p><span>However, Bill S-210 goes well beyond personal choices to limit underage access to sexually explicit material on Canadian sites. Instead, it envisions government-enforced global website liability for failure to block underage access, backed by website blocking and mandated age verification systems that are likely to include face recognition technologies. The government establishes this regulatory framework and is likely to task the CRTC with providing the necessary administration. While there are surely good intentions with the bill, the risks and potential harms it poses are significant.</span></p>
<p><span>The basic framework of Bill S-210 is that it creates an offence for any organization making available sexually explicit material to anyone under the age of 18 for commercial purposes. The penalty for doing so is $250,000 for the first offence and up to $500,000 for any subsequent offences. Organizations (broadly defined under the Criminal Code) can rely on three potential defences:</span></p>
<ol>
<li><span>The organization instituted a “</span><span>prescribed age-verification method” to limit access. It would be up to the government to determine what methods qualify with due regard for reliability and privacy. There is a major global business of vendors that sell these technologies and who are vocal proponents of this kind of legislation.</span></li>
<li><span>The organization can make the case that there is “legitimate purpose related to science, medicine, education or the arts.” </span></li>
<li><span>The organization took steps required to limit access after having received a notification from the enforcement agency (likely the CRTC).</span></li>
</ol>
<p><span>The enforcement of the bill is left to the designated regulatory agency, which can issue notifications of violations to websites and services. Those notices can include the steps the agency wants followed to bring the site into compliance. This literally means the government via its regulatory agency will dictate to sites how they must interact with users to ensure no underage access. If the site fails to act as instructed within 20 days, the regulator can apply for a court order mandating that Canadian ISPs block the site from their subscribers. The regulator would be required to identify which ISPs are subject to the blocking order. </span></p>
<p><span>The website blocking provisions are focused on limiting user access and can therefore be applied to websites anywhere in the world with Canadian ISPs required to ensure that the sites are rendered inaccessible. And what about the risk of overblocking? The bill not only envisions the possibility of blocking lawful content or limiting access to those over 18, it expressly permits it. Section 9(5) states that if the court determines that an order is needed, it may have the effect of preventing access to “material other than sexually explicit material made available by the organization” or limiting access to anyone, not just young people. This raises the prospect of full censorship of lawful content under court order based on notices from a government agency.&nbsp;</span></p>
<p><span>If that isn’t bad enough, there are two additional serious concerns. First, the bill is not limited to pornography sites. Rather, it applies to any site or service that makes sexually explicit materials available. This would presumably include search engines, social media sites such as Twitter, or chat forums such as Reddit, where access to explicit material is not hard to find. If the bill was limited solely to sites whose primary purpose is the commercial distribution of sexually explicit material, it might be more defensible. As it stands now, the overbroad approach leaves this bill vulnerable to constitutional challenge.</span></p>
<p><span>Second, consider the way sites are supposed to comply with the law, by establishing age verification systems. This effectively means that sites will require their users to register with commercial age verification systems in order to run a search or access some tweets. And the age verification systems raise real privacy concerns, including mandated face recognition as part of the verification process.</span></p>
<p>Senate private members bills rarely become law, but this bill is suddenly on the radar screen in a big way. The bill should not have come this far and should not be supported. Creating safeguards for underage access to inappropriate content is a laudable goal, but not at the cost of government-backed censorship, mandated face recognition, and age-approval requirements to use some of the most popular sites and services in the world.</p>

				

								

			</div><!-- #post-19923 -->

		

					
		
		
		



		<!-- #comments -->
		
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ubiquiti showing other users' consoles (276 pts)]]></title>
            <link>https://community.ui.com/questions/Security-Issue-Cloud-Site-Manager-presented-me-your-consoles-not-mine/376ec514-572d-476d-b089-030c4313888c</link>
            <guid>38643348</guid>
            <pubDate>Thu, 14 Dec 2023 16:42:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://community.ui.com/questions/Security-Issue-Cloud-Site-Manager-presented-me-your-consoles-not-mine/376ec514-572d-476d-b089-030c4313888c">https://community.ui.com/questions/Security-Issue-Cloud-Site-Manager-presented-me-your-consoles-not-mine/376ec514-572d-476d-b089-030c4313888c</a>, See on <a href="https://news.ycombinator.com/item?id=38643348">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span>Loading <span>Ubiquiti </span><span>Community</span></span></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FunSearch: Making new discoveries in mathematical sciences using LLMs (309 pts)]]></title>
            <link>https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/</link>
            <guid>38643076</guid>
            <pubDate>Thu, 14 Dec 2023 16:24:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/">https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/</a>, See on <a href="https://news.ycombinator.com/item?id=38643076">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      
  <article>
    
  
    
  
  
  
    

    
    
      
        <div>
          
            
            
              
              

<div>
    <div>
      <p>Research</p>
      

      
        <dl>
          
            <dt>Published</dt>
            <dd>
              <time datetime="2023-12-14">
                14 December 2023
              </time>
            </dd>
          
          
            <dt>Authors</dt>
            
          
        </dl>
      

      
    </div>

    
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="1072" height="603" srcset="https://lh3.googleusercontent.com/fWc7Wtt0ge3XaM53Dwgbscfc2t7qG9mHLlES7lNwdW8lce9iiJKw_sRcXcaild2ZNm3nIjlvzO-4aMGv8EkLE6usq180w4e7ljHpWMdLQGt0NylmQYQ=w1072-h603-n-nu-rw 1x, https://lh3.googleusercontent.com/fWc7Wtt0ge3XaM53Dwgbscfc2t7qG9mHLlES7lNwdW8lce9iiJKw_sRcXcaild2ZNm3nIjlvzO-4aMGv8EkLE6usq180w4e7ljHpWMdLQGt0NylmQYQ=w2144-h1206-n-nu-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="928" height="522" srcset="https://lh3.googleusercontent.com/fWc7Wtt0ge3XaM53Dwgbscfc2t7qG9mHLlES7lNwdW8lce9iiJKw_sRcXcaild2ZNm3nIjlvzO-4aMGv8EkLE6usq180w4e7ljHpWMdLQGt0NylmQYQ=w928-h522-n-nu-rw 1x, https://lh3.googleusercontent.com/fWc7Wtt0ge3XaM53Dwgbscfc2t7qG9mHLlES7lNwdW8lce9iiJKw_sRcXcaild2ZNm3nIjlvzO-4aMGv8EkLE6usq180w4e7ljHpWMdLQGt0NylmQYQ=w1856-h1044-n-nu-rw 2x"><source type="image/webp" width="528" height="297" srcset="https://lh3.googleusercontent.com/fWc7Wtt0ge3XaM53Dwgbscfc2t7qG9mHLlES7lNwdW8lce9iiJKw_sRcXcaild2ZNm3nIjlvzO-4aMGv8EkLE6usq180w4e7ljHpWMdLQGt0NylmQYQ=w528-h297-n-nu-rw 1x, https://lh3.googleusercontent.com/fWc7Wtt0ge3XaM53Dwgbscfc2t7qG9mHLlES7lNwdW8lce9iiJKw_sRcXcaild2ZNm3nIjlvzO-4aMGv8EkLE6usq180w4e7ljHpWMdLQGt0NylmQYQ=w1056-h594-n-nu-rw 2x">
      <img alt="Snippets of code and colourful streams of light" height="603" src="https://lh3.googleusercontent.com/fWc7Wtt0ge3XaM53Dwgbscfc2t7qG9mHLlES7lNwdW8lce9iiJKw_sRcXcaild2ZNm3nIjlvzO-4aMGv8EkLE6usq180w4e7ljHpWMdLQGt0NylmQYQ=w1072-h603-n-nu" width="1072">
    </picture>
    
  
  </div>
            
          
            
            
              
              <div>
  <p data-block-key="4lvaj">By searching for “functions” written in computer code, FunSearch made the first discoveries in open problems in mathematical sciences using LLMs</p><p data-block-key="7k21c">Large Language Models (LLMs) are useful assistants - they excel at combining concepts and can read, write and code to help people solve problems. But could they discover entirely new knowledge?</p><p data-block-key="c3ioi">As LLMs have been shown to “hallucinate” factually incorrect information, using them to make verifiably correct discoveries is a challenge. But what if we could harness the creativity of LLMs by identifying and building upon only their very best ideas?</p><p data-block-key="c9jsm">Today, in a <a href="https://www.nature.com/articles/s41586-023-06924-6" rel="noopener" target="_blank">paper published in Nature</a>, we introduce FunSearch, a method to search for new solutions in mathematics and computer science. FunSearch works by pairing a pre-trained LLM, whose goal is to provide creative solutions in the form of computer code, with an automated “evaluator”, which guards against hallucinations and incorrect ideas. By iterating back-and-forth between these two components, initial solutions “evolve” into new knowledge. The system searches for “functions” written in computer code; hence the name FunSearch.</p><p data-block-key="79msm">This work represents the first time a new discovery has been made for challenging open problems in science or mathematics using LLMs. FunSearch discovered new solutions for the cap set problem, a longstanding open problem in mathematics. In addition, to demonstrate the practical usefulness of FunSearch, we used it to discover more effective algorithms for the “bin-packing” problem, which has ubiquitous applications such as making data centers more efficient.</p><p data-block-key="6hrod">Scientific progress has always relied on the ability to share new understanding. What makes FunSearch a particularly powerful scientific tool is that it outputs programs that reveal <i>how</i> its solutions are constructed, rather than just what the solutions are. We hope this can inspire further insights in the scientists who use FunSearch, driving a virtuous cycle of improvement and discovery.</p><h2 data-block-key="cqrj5">Driving discovery through evolution with language models</h2><p data-block-key="3lfpb">FunSearch uses an evolutionary method powered by LLMs, which promotes and develops the highest scoring ideas. These ideas are expressed as computer programs, so that they can be run and evaluated automatically. First, the user writes a description of the problem in the form of code. This description comprises a procedure to evaluate programs, and a seed program used to initialize a pool of programs.</p><p data-block-key="33p7f">FunSearch is an iterative procedure; at each iteration, the system selects some programs from the current pool of programs, which are fed to an LLM. The LLM creatively builds upon these, and generates new programs, which are automatically evaluated. The best ones are added back to the pool of existing programs, creating a self-improving loop. FunSearch uses <a href="https://ai.google/discover/palm2/" rel="noopener" target="_blank">Google’s PaLM 2</a>, but it is compatible with other LLMs trained on code.</p>
</div>
            
          
            
            
              
              




<figure>
  
  
    <figcaption>
      <p data-block-key="8b2b2">The FunSearch process. The LLM is shown a selection of the best programs it has generated so far (retrieved from the programs database), and asked to generate an even better one. The programs proposed by the LLM are automatically executed, and evaluated. The best programs are added to the database, for selection in subsequent cycles. The user can at any point retrieve the highest-scoring programs discovered so far.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <p data-block-key="4lvaj">Discovering new mathematical knowledge and algorithms in different domains is a notoriously difficult task, and largely beyond the power of the most advanced AI systems. To tackle such challenging problems with FunSearch, we introduced multiple key components. Instead of starting from scratch, we start the evolutionary process with common knowledge about the problem, and let FunSearch focus on finding the most critical ideas to achieve new discoveries. In addition, our evolutionary process uses a strategy to improve the diversity of ideas in order to avoid stagnation. Finally, we run the evolutionary process in parallel to improve the system efficiency.</p><h2 data-block-key="293lm">Breaking new ground in mathematics</h2><p data-block-key="c10fe">We first address the <a href="https://en.wikipedia.org/wiki/Cap_set" rel="noopener" target="_blank">cap set problem</a>, an open challenge, which has vexed mathematicians in multiple research areas for decades. Renowned mathematician Terence Tao once described it as his <a href="https://terrytao.wordpress.com/2007/02/23/open-question-best-bounds-for-cap-sets/" rel="noopener" target="_blank">favorite open question</a>. We collaborated with Jordan Ellenberg, a professor of mathematics at the University of Wisconsin–Madison, and author of an <a href="https://arxiv.org/abs/1605.09223" rel="noopener" target="_blank">important breakthrough on the cap set problem</a>.</p><p data-block-key="6gov3">The problem consists of finding the largest set of points (called a <i>cap set</i>) in a high-dimensional grid, where no three points lie on a line. This problem is important because it serves as a model for other problems in extremal combinatorics - the study of how large or small a collection of numbers, graphs or other objects could be. Brute-force computing approaches to this problem don’t work – the number of possibilities to consider quickly becomes greater than the number of atoms in the universe.</p><p data-block-key="2vb6l">FunSearch generated solutions - in the form of programs - that in some settings discovered the largest cap sets ever found. This represents the <a href="https://link.springer.com/article/10.1023/A:1027365901231" rel="noopener" target="_blank">largest increase</a> in the size of cap sets in the past 20 years. Moreover, FunSearch outperformed state-of-the-art computational solvers, as this problem scales well beyond their current capabilities.</p>
</div>
            
          
            
            
              
              
            
          
            
            
              
              




<figure>
  
  
    <figcaption>
      <p data-block-key="ewhw0">Interactive figure showing the evolution from the seed program (top) to a new higher-scoring function (bottom). Each circle is a program, with its size proportional to the score assigned to it. Only ancestors of the program at the bottom are shown. The corresponding function produced by FunSearch for each node is shown on the right (see full program using this function in the paper).</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <p data-block-key="4lvaj">These results demonstrate that the FunSearch technique can take us beyond established results on hard combinatorial problems, where intuition can be difficult to build. We expect this approach to play a role in new discoveries for similar theoretical problems in combinatorics, and in the future it may open up new possibilities in fields such as communication theory.</p><h2 data-block-key="cc0t2">FunSearch favors concise and human-interpretable programs</h2><p data-block-key="ektms">While discovering new mathematical knowledge is significant in itself, the FunSearch approach offers an additional benefit over traditional computer search techniques. That’s because FunSearch isn’t a black box that merely generates solutions to problems. Instead, it generates programs that describe <i>how</i> those solutions were arrived at. This show-your-working approach is how scientists generally operate, with new discoveries or phenomena explained through the process used to produce them.</p><p data-block-key="6f8qq">FunSearch favors finding solutions represented by highly compact programs - solutions with a low Kolmogorov complexity†. Short programs can describe very large objects, allowing FunSearch to scale to large needle-in-a-haystack problems. Moreover, this makes FunSearch’s program outputs easier for researchers to comprehend. Ellenberg said: “FunSearch offers a completely new mechanism for developing strategies of attack. The solutions generated by FunSearch are far conceptually richer than a mere list of numbers. When I study them, I learn something”.</p><p data-block-key="jp5n">What’s more, this interpretability of FunSearch’s programs can provide actionable insights to researchers. As we used FunSearch we noticed, for example, intriguing symmetries in the code of some of its high-scoring outputs. This gave us a new insight into the problem, and we used this insight to refine the problem introduced to FunSearch, resulting in even better solutions. We see this as an exemplar for a collaborative procedure between humans and FunSearch across many problems in mathematics.</p>
</div>
            
          
            
            
              
              




<figure>
  
  
    <figcaption>
      <p data-block-key="8jaj9">Left: Inspecting code generated by FunSearch yielded further actionable insights (highlights added by us). Right: The raw “admissible” set constructed using the (much shorter) program on the left.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <figure>
  <blockquote><p data-block-key="jt2vi">The solutions generated by FunSearch are far conceptually richer than a mere list of numbers. When I study them, I learn something.</p></blockquote>
  <figcaption><p data-block-key="qcni1">Jordan Ellenberg, collaborator and professor of mathematics at the University of Wisconsin–Madison</p></figcaption>
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="4lvaj">Addressing a notoriously hard challenge in computing</h2><p data-block-key="a8qqv">Encouraged by our success with the theoretical cap set problem, we decided to explore the flexibility of FunSearch by applying it to an important practical challenge in computer science. The “bin packing” problem looks at how to pack items of different sizes into the smallest number of bins. It sits at the core of many real-world problems, from loading containers with items to allocating compute jobs in data centers to minimize costs.</p><p data-block-key="1n4qj">The online bin-packing problem is typically addressed using algorithmic rules-of-thumb (heuristics) based on human experience. But finding a set of rules for each specific situation - with differing sizes, timing, or capacity – can be challenging. Despite being very different from the cap set problem, setting up FunSearch for this problem was easy. FunSearch delivered an automatically tailored program (adapting to the specifics of the data) that outperformed established heuristics – using fewer bins to pack the same number of items.</p>
</div>
            
          
            
            
              
              




<figure>
  
  
    <figcaption>
      <p data-block-key="n0rzx">Illustrative example of bin packing using existing heuristic – Best-fit heuristic (left), and using a heuristic discovered by FunSearch (right).</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <p data-block-key="4lvaj">Hard combinatorial problems like online bin packing can be tackled using other AI approaches, <a href="https://deepmind.google/impact/optimizing-computer-systems-with-more-generalized-ai-tools/" rel="noopener" target="_blank">such as neural networks</a> and reinforcement learning. Such approaches have proven to be effective too, but may also require significant resources to deploy. FunSearch, on the other hand, outputs code that can be easily inspected and deployed, meaning its solutions could potentially be slotted into a variety of real-world industrial systems to bring swift benefits.</p><h2 data-block-key="1htm1">LLM-driven discovery for science and beyond</h2><p data-block-key="drmg1">FunSearch demonstrates that if we safeguard against LLMs’ hallucinations, the power of these models can be harnessed not only to produce new mathematical discoveries, but also to reveal potentially impactful solutions to important real-world problems.</p><p data-block-key="bd36s">We envision that for many problems in science and industry - longstanding or new - generating effective and tailored algorithms using LLM-driven approaches will become common practice.</p><p data-block-key="8uv2j">Indeed, this is just the beginning. FunSearch will improve as a natural consequence of the wider progress of LLMs, and we will also be working to broaden its capabilities to address a variety of society’s pressing scientific and engineering challenges.</p>
</div>
            
          
            
            
              
              


            
          
            
            
              
              
            
          
            
            
              
              



  
    
  

            
          
        </div>
      
    

    
  
  

  

  </article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The AI Trust Crisis (291 pts)]]></title>
            <link>https://simonwillison.net/2023/Dec/14/ai-trust-crisis/</link>
            <guid>38643046</guid>
            <pubDate>Thu, 14 Dec 2023 16:22:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2023/Dec/14/ai-trust-crisis/">https://simonwillison.net/2023/Dec/14/ai-trust-crisis/</a>, See on <a href="https://news.ycombinator.com/item?id=38643046">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p>14th December 2023</p>

<p>Dropbox added some <a href="https://help.dropbox.com/view-edit/dropbox-ai-how-to">new AI features</a>. In the past couple of days these have attracted a firestorm of criticism. Benj Edwards rounds it up in  <a href="https://arstechnica.com/information-technology/2023/12/dropbox-spooks-users-by-sending-data-to-openai-for-ai-search-features/">Dropbox spooks users with new AI features that send data to OpenAI when used</a>.</p>
<p>The key issue here is that people are worried that their private files on Dropbox are being passed to OpenAI to use as training data for their models—a claim that is strenuously denied by Dropbox.</p>
<p>As far as I can tell, Dropbox built some sensible features—summarize on demand, “chat with your data” via Retrieval Augmented Generation—and did a moderately OK job of communicating how they work... but when it comes to data privacy and AI, a “moderately OK job” is a failing grade. Especially if you hold as much of people’s private data as Dropbox does!</p>
<p>Two details in particular seem really important. Dropbox have an <a href="https://www.dropbox.com/ai-principles">AI principles document</a> which includes this:</p>
<blockquote>
<p>Customer trust and the privacy of their data are our foundation. We will not use customer data to train AI models without consent.</p>
</blockquote>
<p>They also have a checkbox hidden <a href="https://www.dropbox.com/account/ai">deep in their settings</a> that looks like this:</p>
<p><img src="https://static.simonwillison.net/static/2023/dropbox-third-party.png" alt="Third-party AI: Use artificial intelligence (Al) from third-party partners so you can work faster in Dropbox. We only use technology partners we have vetted. Your data is never used to train their internal models, and is deleted from third-party servers within 30 days. Learn more. There is a toggle set to On."></p>
<p>I took that screenshot on my own account. It’s toggled “on”—but I never turned it on myself.</p>
<p>Does that mean I’m marked as “consenting” to having my data used to train AI models?</p>
<p>I don’t think so: I think this is a combination of confusing wording and the eternal vagueness of what the term “consent” means in a world where everyone agrees to the terms and conditions of everything without reading them.</p>
<p>But a LOT of people have come to the conclusion that this means their private data—which they pay Dropbox to protect—is now being funneled into the OpenAI training abyss.</p>
<h4 id="people-dont-believe-openai">People don’t believe OpenAI</h4>
<p>Here’s copy from that Dropbox preference box, talking about their “third-party partners”—in this case OpenAI:</p>
<blockquote>
<p>Your data is never used to train their internal models, and is deleted from third-party servers within 30 days.</p>
</blockquote>
<p>It’s increasing clear to me like people simply <strong>don’t believe OpenAI</strong> when they’re told that data won’t be used for training.</p>
<p>What’s really going on here is something deeper then: AI is facing a crisis of trust.</p>
<p>I quipped <a href="https://twitter.com/simonw/status/1735086765814542802">on Twitter</a>:</p>
<blockquote>
<p>“OpenAI are training on every piece of data they see, even when they say they aren’t” is the new “Facebook are showing you ads based on overhearing everything you say through your phone’s microphone”</p>
</blockquote>
<p>Here’s what I meant by that.</p>
<h4 id="facebook-dont-spy-microphone">Facebook don’t spy on you through your microphone</h4>
<p>Have you heard the one about Facebook spying on you through your phone’s microphone and showing you ads based on what you’re talking about?</p>
<p>This theory has been floating around for years. From a technical perspective it should be easy to disprove:</p>
<ul>
<li>Mobile phone operating systems don’t allow apps to invisibly access the microphone.</li>
<li>Privacy researchers can audit communications between devices and Facebook to confirm if this is happening.</li>
<li>Running high quality voice recognition like this at scale is extremely expensive—I had a conversation with a friend who works on server-based machine learning at Apple a few years ago who found the entire idea laughable.</li>
</ul>
<p>The non-technical reasons are even stronger:</p>
<ul>
<li>Facebook say they aren’t doing this. The risk to their reputation if they are caught in a lie is astronomical.</li>
<li>As with many conspiracy theories, too many people would have to be “in the loop” and not blow the whistle.</li>
<li>Facebook don’t need to do this: there are much, much cheaper and more effective ways to target ads at you than spying through your microphone. These methods have been working incredibly well for years.</li>
<li>Facebook gets to show us thousands of ads a year. 99% of those don’t correlate in the slightest to anything we have said out loud. If you keep rolling the dice long enough, eventually a coincidence will strike.</li>
</ul>
<p>Here’s the thing though: <em>none of these arguments matter</em>.</p>
<p>If you’ve ever experienced Facebook showing you an ad for something that you were talking about out-loud about moments earlier, you’ve already dismissed everything I just said. You have personally experienced anecdotal evidence which overrides all of my arguments here.</p>
<p>Here’s a Reply All podcast episode from Novemember 2017 that explores this issue: <a href="https://gimletmedia.com/shows/reply-all/z3hlwr">109 Is Facebook Spying on You?</a>. Their conclusion: Facebook are not spying through your microphone. But if someone already believes that there is no argument that can possibly convince them otherwise.</p>
<p>I’ve experienced this effect myself—over the past few years I’ve tried talking people out of this, as part of my own personal fascination with how sticky this conspiracy theory is.</p>
<p>The key issue here is the same as the OpenAI training issue: people <strong>don’t believe</strong> these companies when they say that they aren’t doing something.</p>
<p>One interesting difference here is that in the Facebook example people have personal evidence that makes them believe they understand what’s going on.</p>
<p>With AI we have almost the complete opposite: AI models are weird black boxes, built in secret and with no way of understanding what the training data was or how it influences the model.</p>
<p>As with so much in AI, people are left with nothing more than “vibes” to go on. And the vibes are bad.</p>
<h4 id="this-really-matters">This really matters</h4>
<p>Trust is really important. Companies lying about what they do with your privacy is a very serious allegation.</p>
<p>A society where big companies tell blatant lies about how they are handling our data—and get away with it without consequences—is a very unhealthy society.</p>
<p>A key role of government is to prevent this from happening. If OpenAI are training on data that they said they wouldn’t train on, or if Facebook are spying on us through our phone’s microphones, they should be hauled in front of regulators and/or sued into the ground.</p>
<p>If we believe that they are doing this without consequence, and have been getting away with it for years, our intolerance for corporate misbehavior becomes a victim as well. We risk letting companies get away with real misconduct because we incorrectly believed in conspiracy theories.</p>
<p>Privacy is important, and very easily misunderstood. People both overestimate and underestimate what companies are doing, and what’s possible. This isn’t helped by the fact that AI technology means the scope of what’s possible is changing at a rate that’s hard to appreciate even if you’re deeply aware of the space.</p>
<p>If we want to protect our privacy, we need to understand what’s going on. More importantly, we need to be able to trust companies to honestly and clearly explain what they are doing with our data.</p>
<p>On a personal level we risk losing out on useful tools. How many people cancelled their Dropbox accounts in the last 48 hours? How many more turned off that AI toggle, ruling out ever evaluating if those features were useful for them or not?</p>
<h4 id="what-can-we-do">What can we do about it?</h4>
<p>There is something that the big AI labs could be doing to help here: tell us how you are training!</p>
<p>The fundamental question here is about training data: what are OpenAI using to train their models?</p>
<p>And the answer is: we have no idea! The entire process could not be more opaque.</p>
<p>Given that, is it any wonder that when OpenAI say “we don’t train on data submitted via our API” people have trouble believing them?</p>
<p>The situation with ChatGPT itself is even more messy. OpenAI say that they DO use ChatGPT interactions to improve their models—even those from paying customers, with the exception of the “call us” priced <a href="https://openai.com/blog/introducing-chatgpt-enterprise">ChatGPT Enterprise</a>.</p>
<p>If I paste a private document into ChatGPT to ask for a summary, will snippets of that document be leaked to future users after the next model update? Without more details on HOW they are using ChatGPT to improve their models I can’t come close to answering that question.</p>
<p>Clear explanations of how this stuff works could go a long way to improving the trust relationship OpenAI have with their users, and the world at large.</p>
<p>Maybe take a leaf from large scale platform companies. They publish public post-mortem incident reports on outages, to regain trust with their customers through transparency about exactly what happened and the steps they are taking to prevent it from happening again. Dan Luu has collected a <a href="https://github.com/danluu/post-mortems">great list of examples</a>.</p>
<h4 id="opportunity-local-models">An opportunity for local models</h4>
<p>One consistent theme I’ve seen in conversations about this issue is that people are much more comfortable trusting their data to local models that run on their own devices than models hosted in the cloud.</p>
<p>The good news is that local models are consistently both increasing in quality and shrinking in size.</p>
<p>I figured out how to run Mixtral-8x7b-Instruct <a href="https://fedi.simonwillison.net/@simon/111577242044966329">on my laptop</a> last night—the first local model I’ve tried which really does seem to be equivalent in quality to ChatGPT 3.5.</p>
<p>Microsoft’s <a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">Phi-2</a> is a fascinating new model in that it’s only 2.7 billion parameters (most useful local models start at 7 billion) but claims state-of-the-art performance against some of those larger models. And they trained it for around $35,000.</p>
<p>While I’m excited about the potential of local models, I’d hate to see us lose out on the power and convenience of the larger hosted models over privacy concerns which turn out to be incorrect.</p>
<p>The intersection of AI and privacy is a critical issue. We need to be able to have the highest quality conversations about it, with maximum transparency and understanding of what’s actually going on.</p>
<p>This is hard already, and it’s made even harder if we straight up disbelieve anything that companies tell us. Those companies need to earn our trust. How can we help them do that?</p>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JWST Captures Image of Supernova That 'Absolutely Shattered' a Star (149 pts)]]></title>
            <link>https://www.smithsonianmag.com/smart-news/james-webb-telescope-captures-image-of-supernova-that-absolutely-shattered-a-star-180983421/</link>
            <guid>38643017</guid>
            <pubDate>Thu, 14 Dec 2023 16:19:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.smithsonianmag.com/smart-news/james-webb-telescope-captures-image-of-supernova-that-absolutely-shattered-a-star-180983421/">https://www.smithsonianmag.com/smart-news/james-webb-telescope-captures-image-of-supernova-that-absolutely-shattered-a-star-180983421/</a>, See on <a href="https://news.ycombinator.com/item?id=38643017">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-article-body="">
        
          <figure>
            <img src="https://th-thumbnailer.cdn-si-edu.com/q6VwXaAU92PSQVO1HUWQzIhenCw=/1000x750/filters:no_upscale():focal(873x931:874x932)/https://tf-cmsv2-smithsonianmag-media.s3.amazonaws.com/filer_public/c2/4e/c24efef7-dc5c-45a9-9a5f-3937bfcf728b/stsci-01hggzdyh8ghhssnwzd71mf0xh.png" alt="A colorized image of the remnants of supernova" itemprop="image">
            <figcaption>
              
                The supernova remnant Cassiopeia A, captured by NASA's James Webb Space Telescope in near-infrared light. The image shows the expanding material from the blast colliding with gas released by the star before the explosion.
              <span>NASA, ESA, CSA, STScI, Danny Milisavljevic (Purdue University), Ilse De Looze (UGent), Tea Temim (Princeton University)</span>
            </figcaption>
          </figure>
        

        

        <p>NASA’s James Webb Space Telescope has seen the remains of a supernova explosion in a new light. The remnants, called <a href="https://www.britannica.com/place/Cassiopeia-A">Cassiopeia A</a> (or Cas A for short), lie 11,000 light-years from Earth, in the constellation Cassiopeia.</p>

<p>In April of this year, Webb <a href="https://www.smithsonianmag.com/smart-news/see-the-james-webb-telescopes-stunning-new-snapshot-of-an-exploded-star-180981973/">imaged</a> the stellar remains in <a href="https://webbtelescope.org/contents/news-releases/2023/news-2023-121">mid-infrared light</a>. Now, the newly released snapshot shows Cas A’s colorful, orb-like wisps<strong> </strong>captured using Webb’s Near-Infrared Camera (NIRCam).</p>

<p>“With NIRCam’s resolution, we can now see how the dying star absolutely shattered when it exploded, leaving filaments akin to tiny shards of glass behind,” <a href="https://www.physics.purdue.edu/people/faculty/dmilisav.php">Danny Milisavljevic</a>, an astronomer at Purdue University who led the research, says in a <a href="https://www.nasa.gov/missions/webb/nasas-webb-stuns-with-new-high-definition-look-at-exploded-star/">statement</a> from NASA. “It’s really unbelievable after all these years studying Cas A to now resolve those details, which are providing us with transformational insight into how this star exploded.”</p><center><blockquote><div lang="en" dir="ltr"><p>To: You, From: The Universe </p><p>This stunning new Webb image is a gift from a past star. In near-infrared light, supernova remnant Cassiopeia A (Cas A) resembles a shiny ornament. Embedded within gas from the star are the materials for new stars &amp; planets: <a href="https://t.co/9kIvQtEnpb">https://t.co/9kIvQtEnpb</a> <a href="https://t.co/vzzaWrzPBA">pic.twitter.com/vzzaWrzPBA</a></p></div>— NASA Webb Telescope (@NASAWebb) <a href="https://twitter.com/NASAWebb/status/1734020677311639828?ref_src=twsrc%5Etfw">December 11, 2023</a></blockquote> </center>
<p>Stars burn with fusion, which propels energy outward from their cores. But when aging, giant stars run out of fuel, their own gravity overwhelms fusion’s outward push. The star <a href="https://www.nationalgeographic.com/science/article/supernovae?loggedin=true&amp;rnd=1702397839656">collapses</a> in an explosion that spews its materials across the cosmos. Heavy elements in the universe are often <a href="https://www.sciencedaily.com/releases/2019/03/190318111945.htm">formed during supernovae</a>.</p>

<p>The light from Cassiopeia A’s explosion reached Earth about 340 years ago. Scientists estimate that in its early days, the star that yielded the explosion had a mass <a href="https://chandra.harvard.edu/photo/2017/casa_life/">16 times that of the sun</a>, but it shrank to about five times the size of the sun before it blew. Since the explosion occurred thousands of light-years from Earth, it took thousands of years for its light to reach us.</p>

<p>Previously, the Hubble Space Telescope, Spitzer Space Telescope, Chandra X-Ray Observatory and other telescopes had imaged Cas A, according to NASA. Chandra’s study revealed the amounts of different elements produced by the explosion. The supernova has spat out <a href="https://chandra.harvard.edu/photo/2017/casa_life/">10,000 times the mass of the Earth in sulfur</a>, 20,000 times Earth’s mass in silicon, 70,000 Earth masses of iron and a million Earth masses of oxygen.</p>

<p>Webb’s NIRCam detects wavelengths of light that are broader than visible light and thus can’t be seen by the human eye. So, to compose the new image, researchers translated the infrared light in different colors.</p>

<p>The bright orange and light pink areas in the new image represent the supernova’s inner shell and are made up of sulfur, oxygen, argon and neon from the star. Dust and molecules that will one day form new stars are in this gas, per NASA.</p>

<figure>
  
    <img src="https://th-thumbnailer.cdn-si-edu.com/GX-mhR8OKSh185-g4ZJH9FBrbnA=/fit-in/1072x0/filters:focal(1000x562:1001x563)/https://tf-cmsv2-smithsonianmag-media.s3.amazonaws.com/filer_public/b3/be/b3be5990-0772-4ca6-9aa5-80b2e171392f/stsci-01hgh0xz8jhr7ayp4tygdxcekn.png" alt="A side-by-side comparison of two images of the same supernova remnant" loading="lazy">
  

  <figcaption>
    
      A comparison of the Webb Telescope's new near-infrared image of Cassiopeia A (left) and its mid-infrared image of the supernova from April (right). Some things seen in mid-infrared, like the green loop of light in the center, are not visible in near-infrared.
    
    
      <span>NASA, ESA, CSA, STScI, Danny Milisavljevic (Purdue University), Ilse De Looze (UGent), Tea Temim (Princeton University)</span>
    
  </figcaption>
</figure>
<p>The researchers also compared the new image to the mid-infrared one from earlier this year. Orange and red in April’s image represent the edge of the remnant’s main inner shell, while this detail looks like wisps of smoke in the new picture. This boundary denotes where the supernova explosion collides with surrounding material that isn’t hot enough to be detected in near-infrared.</p>

<p>A green loop of light in the mid-infrared image (which astronomers had nicknamed the “Green Monster”) is also not visible in Webb’s new view. Holes in that part of the image are outlined in ionized gas, appearing white and purple in the near-infrared image. This could be from the explosion pushing through and shaping gas from the star, according to NASA.</p>

<p>With this new look at Cas A, Webb captured another intriguing feature: A blob that appears in the bottom right of the image is an example of a light echo, where light from the supernova is warming far-away dust. As that dust cools, it glows. This light echo, nicknamed Baby Cas A, is about 170 light-years behind the main supernova remnant.</p>

        

        

        
          
  <div>
      <p>Get the latest stories in your inbox every weekday.</p>
      
    </div>


        

        

        
          


  
    
      
    
  

  


        

         
        

        
          
            <section>
              <nav>Filed Under: 
                
                  
                    <a href="https://www.smithsonianmag.com/tag/astronomy/">Astronomy</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/james-webb-space-telescope/">James Webb Space Telescope</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/milky-way/">Milky way</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/nasa/">NASA</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/outer-space/">Outer Space</a>, 
                  
                
                  
                    <a href="https://www.smithsonianmag.com/tag/telescope/">telescope</a>
                  
                
              </nav>
            </section>
          
        

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FCC votes to ban termination fees for cable and satellite services (122 pts)]]></title>
            <link>https://www.cnbc.com/2023/12/13/fcc-votes-to-ban-termination-fees-for-cable-and-satellite-services.html</link>
            <guid>38642977</guid>
            <pubDate>Thu, 14 Dec 2023 16:16:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2023/12/13/fcc-votes-to-ban-termination-fees-for-cable-and-satellite-services.html">https://www.cnbc.com/2023/12/13/fcc-votes-to-ban-termination-fees-for-cable-and-satellite-services.html</a>, See on <a href="https://news.ycombinator.com/item?id=38642977">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>WASHINGTON — The <a href="https://www.fcc.gov/" target="_blank">Federal Communications Commission</a> on Wednesday voted to pass a measure banning cable and satellite companies from charging early termination fees.</p><p>"Consumers are tired of these junk fees," FCC Chairwoman <a href="https://www.fcc.gov/about/leadership/jessica-rosenworcel" target="_blank">Jessica Rosenworcel</a> said before casting the <a href="https://recapd.com/w-UeqR0B/86a21480582e5cf4a68cc1e6924e9c/?" target="_blank">deciding vote</a> at an open commission meeting.</p><p>"They now have more choices when it comes to video content but these friction-filled tactics to keep us subscribing to our current providers are aggravating and unfair," Rosenworcel added. "So, today we kick out a rulemaking to put an end to these practices."</p><p>The agency first <a href="https://www.cnbc.com/2023/11/21/biden-proposes-ban-on-cable-cord-cutting-fees.html">announced the proposal</a> in late November to force cable and direct broadcast satellite providers to drop fees for canceling services before the end of a contract period.</p><p>The rule will also require service providers to issue a prorated credit or rebate to customers for days remaining in the billing cycle after cancellation.</p><p>Two out of four commissioners voted against the measure.</p><p>The ban on early termination fees is part of the Biden administration's initiative against excessive surcharges, or "junk" fees, which are estimated to cost consumers <a href="https://www.whitehouse.gov/briefing-room/blog/2022/10/26/the-presidents-initiative-on-junk-fees-and-related-pricing-practices/" target="_blank">tens of billions of dollars</a>, according to the White House.</p><p>Cable companies could be raking in an <a href="https://advocacy.consumerreports.org/wp-content/uploads/2019/10/CR_WhatTheFeeReport_6F_sm-1.pdf" target="_blank">estimated $28 billion a year</a> on fees, according to a 2019 brief by Consumer Reports.</p><p>&nbsp;<em><strong>Don't miss these stories from CNBC PRO:</strong></em></p><ul><li><a href="https://www.cnbc.com/2023/12/05/five-stocks-to-buy-before-the-year-end-according-to-the-pros.html"><em>Five stocks to buy before the year-end, according to the pros</em></a></li><li><a href="https://www.cnbc.com/2023/12/04/morgan-stanley-fund-manager-names-4-top-stocks-to-buy-on-the-cheap.html"><em>Morgan Stanley fund manager names 4 top stocks to buy 'on the cheap'</em></a></li><li><a href="https://www.cnbc.com/2023/12/10/jpmorgan-picks-china-stocks-to-buy-now-alibabas-not-on-the-list.html"><em>JPMorgan picks China stocks to buy now. Alibaba's not on the list</em></a></li><li><a href="https://www.cnbc.com/2023/12/11/analysts-love-this-self-driving-car-tech-stock-and-give-it-over-400percent-upside.html"><em>Analysts love this self-driving car tech stock and give it over 400% upside</em></a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cruise slashes 24% of self-driving car workforce in sweeping layoffs (125 pts)]]></title>
            <link>https://techcrunch.com/2023/12/14/cruise-slashes-24-of-self-driving-car-workforce-in-sweeping-layoffs/</link>
            <guid>38642791</guid>
            <pubDate>Thu, 14 Dec 2023 16:02:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2023/12/14/cruise-slashes-24-of-self-driving-car-workforce-in-sweeping-layoffs/">https://techcrunch.com/2023/12/14/cruise-slashes-24-of-self-driving-car-workforce-in-sweeping-layoffs/</a>, See on <a href="https://news.ycombinator.com/item?id=38642791">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<p>
						
			<h2>Cruise, the GM self-driving car subsidiary, is laying off 900 workers to slash costs and revamp the company, TechCrunch exclusively learned.</h2>
		</p>

			
	
			<p><img src="https://techcrunch.com/wp-content/uploads/2023/11/GettyImages-1497020096.jpg?w=648">
		</p>
	</div><div>
				<p id="speakable-summary"><span>Cruise, the embattled</span> GM self-driving car subsidiary, is laying off 900 employees, or about 24% of its workforce, TechCrunch has exclusively learned. The layoffs are part of a plan to slash costs and attempt to revamp the company following an October 2 incident that left a pedestrian stuck under and then dragged by one of its robotaxis.</p>
<p>An email, penned by newly minted president and CTO Mo Elshenawy, was sent this morning to the entire 3,800-person workforce. The email, which TechCrunch has viewed, began with a resigned tone: “We knew this day was coming, but that does not make it any less difficult—especially for those whose jobs are affected,” Elshenawy wrote. Workers were expected to be informed within the hour or receiving the company wide email, as to whether they would be losing their job.</p>
<p>GM, which acquired Cruise in 2016, was rewarded by shareholders for the cutbacks. GM shares rose 4.8% to $35.64 following the news.</p>
<p>Cruise is targeting non-engineering jobs in the layoffs, particularly those people who worked in the field, commercial operations and corporate staffing, according to the email. The company has also ended additional assignments of contingent workers who supported its driverless operations. Engineering, a category that makes up the bulk of the Cruise workforce, is largely being preserved, according to the content of the email and discussions with internal sources.</p>
<p>The email continued:</p>
<blockquote><p><span>Today, we are making staff reductions that will affect 24% of full-time Cruisers, through no fault of their own.&nbsp; We are simplifying and focusing our efforts to return with an exceptional service in one city to start with and focusing on the Bolt platform for this first step before we scale. As a result, we are reducing our employee counts in operations and other areas. These impacts are largely outside of engineering, although some Tech positions are impacted also.</span></p></blockquote>
<p dir="ltr">Workers will remain on the payroll through February 12 and will be eligible for an additional 8 weeks of pay, with long-term employees offered an additional 2 weeks’ pay per every year at Cruise over 3 years, according to the email to staff. Anyone laid off will also receive their 2023 bonus (eligible target payout) on Jan. 5, 2024. Other parts of the severance package includes health benefits through the end of May, two months contribution into their 401(k) plan and continued time on payroll through March 24 for immigrants in lieu of a lump-sum severance payment to allow visa holders additional time to help transition and manage their immigration status.</p>
<p>The company also said that all employees, regardless of whether they were laid off, will receive&nbsp; their January 15th vesting through its employee share-selling program.</p>
<p dir="ltr" role="presentation"><span>Cruise issued a statement confirming the layoffs.</span></p>
<p dir="ltr">“We shared the difficult news that we are reducing our workforce, primarily in commercial operations and related corporate functions,” the emailed statement reads. “These changes reflect our decision to focus on more deliberate commercialization plans with safety as our north star. We are supporting impacted Cruisers with strong severance and benefits packages and are grateful to the departing employees who played important roles in building Cruise and supporting our mission.”</p>
<p dir="ltr">The layoffs come just a day after nine senior leaders (SLT) at Cruise, who worked in its commercial operations, legal and policy departments, <a href="https://techcrunch.com/2023/12/13/cruise-leaders-booted-following-initial-safety-probe/" target="_blank" rel="noopener">were dismissed</a> by the company’s board. COO Gil West and David Estrada, who was head of government affairs, were among that group.</p>
<p dir="ltr">Elshenawy reiterated the company would be narrowing and refocusing its efforts, information shared last month following the<a href="https://techcrunch.com/2023/11/19/cruise-co-founder-and-ceo-kyle-vogt-resigns/" target="_blank" rel="noopener"> resignation of co-founder and CEO Kyle Vogt</a> and some executive shuffling that included <a href="https://techcrunch.com/2023/11/14/gm-inserts-exec-at-cruise-as-safety-review-expands-manual-self-driving-paused/" target="_blank" rel="noopener">appointing Craig Glidden</a>, GM’s EVP of legal and policy and a Cruise board member, as chief administrative officer at Cruise. Jon McNeill, a member of GM’s board, was also named vice chairman of the Cruise board. McNeill, who joined the Cruise board recently and was previously chief operating officer at Lyft and president of Tesla, now serves alongside GM Chair and CEO Mary Barra.</p>
<p id="speakable-summary">Cruise executives said at the time they wanted to take a measured business approach that preserves cash and improves safety culture in an attempt to put GM’s troubled autonomous vehicle subsidiary on the right path. The first steps in that rebuilding plan, which included pausing production on its Origin robotaxi, were laid out in an internal email sent to employees in late November by Elshenawy, who was executive vice president of engineering at Cruise and ascended into the president role after co-founder and Vogt resigned.</p>
<p>Elshenawy repeated that intent in the Thursday morning email stating that the company was “simplifying and focusing our efforts to return with an exceptional service in one city to start with and focusing on the Bolt platform for this first step before we scale.”</p>
<p>Cruise used all-electric Chevy Bolt vehicles, which have been specifically manufactured to support its self-driving system, in its robotaxi fleet. The company intended to shift towards a custom-built autonomous vehicle called the Origin.</p>
<p dir="ltr">The layoffs have been largely expected at Cruise for weeks now. Last month. Barra reiterated plans for Cruise to be more “deliberate” when operations eventually resume at the troubled self-driving vehicle subsidiary. For GM, that includes <a href="https://techcrunch.com/2023/11/29/gm-to-slash-spending-at-cruise-by-hundreds-of-millions-of-dollars/" target="_blank" rel="noopener">slashing spending</a> at Cruise “by hundreds of millions of dollars” in 2024, an action that most expected would result in widespread layoffs.</p>
<p>GM and the Cruise board have been scrambling ever since the October 2 incident put the company in the crosshairs of state, local and federal agencies. However’s Cruise’s robotaxi operations in San Francisco have been criticized by the public and city officials almost immediately after the California Public Utilities Commission issued the company in August the final permit required to operate commercially. Videos of Cruise robotaxis blocking traffic and driving into a construction site were shared on social media. But it was a crash with an emergency response vehicle that began to chip away at the company’s seemingly impenetrable exterior.</p>
<p><em>This story is developing ….</em></p>
			</div></div>]]></description>
        </item>
    </channel>
</rss>