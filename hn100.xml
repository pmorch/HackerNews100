<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 15 Jun 2025 16:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Canyon.mid (108 pts)]]></title>
            <link>https://canyonmid.com/</link>
            <guid>44282177</guid>
            <pubDate>Sun, 15 Jun 2025 13:23:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://canyonmid.com/">https://canyonmid.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44282177">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[How to modify Starlink Mini to run without the built-in WiFi router (114 pts)]]></title>
            <link>https://olegkutkov.me/2025/06/15/how-to-modify-starlink-mini-to-run-without-the-built-in-wifi-router/</link>
            <guid>44282017</guid>
            <pubDate>Sun, 15 Jun 2025 12:40:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://olegkutkov.me/2025/06/15/how-to-modify-starlink-mini-to-run-without-the-built-in-wifi-router/">https://olegkutkov.me/2025/06/15/how-to-modify-starlink-mini-to-run-without-the-built-in-wifi-router/</a>, See on <a href="https://news.ycombinator.com/item?id=44282017">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-3678">

	<div>
			<p><a href="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_no_router_thumb.jpg"><img decoding="async" src="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_no_router_thumb.jpg" alt="" width="132" height="113"></a> The Starlink Mini terminal is designed as a compact, all-in-one solution with an integrated Wi-Fi router. While this design is ideal for typical consumer use, certain applications—such as custom networking setups, embedded installations, or power-constrained environments—may benefit from removing the internal router entirely. In this article, I’ll detail the process of physically removing the built-in Wi-Fi router board from the Starlink Mini, allowing the terminal to operate solely via Ethernet and offering greater flexibility for advanced users.</p>
<p>Please note that this modification applies only to the Starlink Mini 1 (as of June 14, 2025). Hardware changes in future models, such as the expected Mini 2, may render this process invalid.</p>

<h3>Starlink Mini teardown</h3>
<p>The disassembly process requires patience and accuracy. I recommend using metal spudgers and a plastic prying tool.</p>
<p><a href="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_teardown_recommended_tools.jpg"><img fetchpriority="high" decoding="async" src="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_teardown_recommended_tools.jpg" alt="" width="720" height="322" srcset="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_teardown_recommended_tools.jpg 720w, https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_teardown_recommended_tools-400x179.jpg 400w" sizes="(max-width: 720px) 100vw, 720px"></a></p>







<p>Additionally, you will need a thin, flexible knife or a thin metal wire to remove the router’s PCB.</p>
<p>I prepared a video manual about the teardown process.<br>
Please note that there is no need to remove the metal plate from the Starlink PCB. You can stop the teardown process after removing the router’s PCB.</p>
<p><iframe title="How to take apart the Starlink Mini terminal non-destructively." width="850" height="478" src="https://www.youtube.com/embed/LRhQ3FCJ5ng?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>Removing the metal plate is <strong>strongly discouraged</strong>. This plate serves as both a <strong>heatsink</strong> and an <strong>EMI shield</strong>.<br>
The Starlink CPU runs hot, and without proper cooling, it may throttle the CPU or antenna array. In addition, the plate is glued around the corners with <strong>conductive adhesive</strong> to ensure effective electromagnetic shielding. The Starlink Mini PCB generates <strong>significant EMI</strong>, and SpaceX reportedly faced challenges achieving compliance.<br>
Removing the plate can lead to increased emissions and potential <strong>interference</strong> with nearby electronics.</p>
<h3>Starlink Mini PCB connector</h3>
<p>The exact type of the connector is unknown. It might be a custom order. But the pitch of the connector is 2 mm, so any <a href="https://www.digikey.com/en/products/detail/gct/BF100-18-A-D-1-0640-L-C/16396976">standard</a> 2 mm header will fit here nicely.</p>
<p><a href="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_to_board_connector.jpg"><img loading="lazy" decoding="async" src="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_to_board_connector.jpg" alt="" width="800" height="600" srcset="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_to_board_connector.jpg 800w, https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_to_board_connector-400x300.jpg 400w, https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_to_board_connector-768x576.jpg 768w" sizes="auto, (max-width: 800px) 100vw, 800px"></a></p>
<p>You may notice that this connector is secured with a similar conductive adhesive and a large non-masked grounding area. Additionally, a shield is placed over the connector on the router’s PCB.<br>
This is all done to help contain the EMI.</p>
<h3>Connector pinout</h3>
<p>Starlink Mini uses a 1 Gbps Ethernet link between the primary unit and the router. Please note that there is no Ethernet transformer; instead, a direct <strong>PHY-to-PHY</strong> connection with some decoupling is used. This is acceptable for short distances. With any custom design installation, the Ethernet transformer is mandatory.<br>
The primary voltage bus is 12 VDC.</p>
<p>The connector pinout (Starlink Mini side) is in the picture below. The connector pinout (Starlink Mini side) is in the image below.<br>
Ethernet line signals are mapped to the corresponding T568B twisted cable colors. Pins 11, 14, 16, 17, and 18 are not used in the current modification and mainly serve Starlink-Router monitoring purposes.<br>
It’s recommended to use all 12 VDC and GND lines.</p>
<p><a href="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_connector.png"><img loading="lazy" decoding="async" src="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_connector.png" alt="" width="773" height="834" srcset="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_connector.png 773w, https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_connector-278x300.png 278w, https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_connector-768x829.png 768w" sizes="auto, (max-width: 773px) 100vw, 773px"></a></p>
<p>A reference image to help you identify pins:</p>
<p><a href="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_connector_view_pins.jpg"><img loading="lazy" decoding="async" src="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_connector_view_pins.jpg" alt="" width="700" height="418" srcset="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_connector_view_pins.jpg 700w, https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_connector_view_pins-400x239.jpg 400w" sizes="auto, (max-width: 700px) 100vw, 700px"></a></p>
<h3>Direct Ethernet connection</h3>
<p>Below, you can see an example schematic of the direct Ethernet connection to the Starlink Mini_1 PCB connector. This schematic provides required Ethernet isolation and minimal power filtering to ensure proper operation.</p>
<p><a href="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_direct_ethernet_sch.png"><img loading="lazy" decoding="async" src="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_direct_ethernet_sch.png" alt="" width="1791" height="585" srcset="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_direct_ethernet_sch.png 1791w, https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_direct_ethernet_sch-400x131.png 400w, https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_direct_ethernet_sch-1024x334.png 1024w, https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_direct_ethernet_sch-768x251.png 768w, https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_direct_ethernet_sch-1536x502.png 1536w" sizes="auto, (max-width: 1791px) 100vw, 1791px"></a></p>
<p>It’s recommended to put a guard ground around the U1 connector. The most optimal design includes conductive adhesive and shielding.<br>
Please, keep wires between the connector and Ethernet transformer as short as possible.</p>
<p>Nominal runtime current over <strong>12V</strong> is <strong>~3A</strong> with short spikes up to <strong>5A</strong>.<br>
Please select the <strong>L1</strong> with the appropriate rated current to avoid overheating.</p>
<p>Proof-of-concept using my <a href="https://olegkutkov.me/2022/04/30/how-to-add-ethernet-port-to-the-gen-2-starlink-router/">Ethermod</a> adapter:</p>
<p><a href="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_connector_example_ethermod.jpg"><img loading="lazy" decoding="async" src="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_connector_example_ethermod.jpg" alt="" width="700" height="437" srcset="https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_connector_example_ethermod.jpg 700w, https://olegkutkov.me/wp-content/uploads/2025/06/starlink_mini1_board_connector_example_ethermod-400x250.jpg 400w" sizes="auto, (max-width: 700px) 100vw, 700px"></a></p>
<h3>Network configuration</h3>
<p>The Starlink terminal provides a <strong>DHCP</strong> IP address in the <strong>192.168.100.0/24</strong> network when not connected to Starlink satellites.<br>
The terminal itself is available at address <a href="http://192.168.100.1/">192.168.100.1</a>, running a simple web UI and gRPC monitoring/control server.</p>
<p>It’s convenient to get Starlink debug data using <a href="https://github.com/fullstorydev/grpcurl">grpcurl</a>. Example request (getting the current status of the terminal):</p>
<pre>grpcurl -plaintext -d {\"get_status\":{}} 192.168.100.1:9200
SpaceX.API.Device.Device/Handle</pre>
<p>After connecting to the Starlink network, the Ethernet interface provides a <strong>tunneled DHCP</strong> service, assigning clients IP addresses from the Starlink pool – typically a CGNAT IPv4 address (e.g., 100.72.116.102) and a link-global IPv6 address. This network configuration provides access to the Internet.<br>
Please note that the Starlink DHCP server provides only a single IP address. Therefore, you can connect only one host or an upstream router directly to share the connection with multiple devices.</p>
<p>After acquiring the “external” IP address, your client naturally loses access to the <strong>192.168.100.1</strong> host.<br>
To fix this, you can add a static route to the terminal:</p>
<pre>sudo ip route add 192.168.100.1 dev ethX</pre>
<p>Replace <strong>ethX</strong> with the name of your Ethernet interface connected to Starlink.</p>
<p>No additional configuration is required. Once your host obtains an IP address after joining the Starlink network, it should have internet access.<br>
Don’t forget to monitor account status and connection state via the gRPC output.</p>
<h3>Bonus – important gRPC status codes</h3>
<p>The gRPC <strong>get_status</strong> output contains a lot of helpful information.<br>
In case of connection issues, the “outage” section should appear in the gRPC output. Example:</p>
<pre>"outage": {
"cause": "NO_SCHEDULE",
"startTimestampNs": "1815683934050410150",
"durationNs": "4320001119",
"didSwitch": true
},</pre>
<p>Possible values of the “cause”:</p>
<p><strong data-start="76" data-end="87">BOOTING</strong> – The terminal is starting up, initializing modules, and waiting for a GPS fix.<br data-start="167" data-end="170"><strong data-start="170" data-end="190">THERMAL_SHUTDOWN</strong> – The terminal has overheated and shut down to protect components.<br data-start="257" data-end="260"><strong data-start="260" data-end="275" data-is-only-node="">NO_SCHEDULE</strong> – The terminal is unable to communicate with satellites. This could be due to a weak signal, incorrect GPS data, or other issues.<br data-start="405" data-end="408"><strong data-start="408" data-end="419">NO_SATS</strong> – The terminal failed to detect any satellites in the sky.<br data-start="478" data-end="481"><strong data-start="481" data-end="495">OBSTRUCTED</strong> – An obstruction is detected along the radio beam path.<br data-start="551" data-end="554"><strong data-start="554" data-end="569">NO_DOWNLINK</strong> – The terminal cannot receive data from the satellite.<br data-start="624" data-end="627"><strong data-start="627" data-end="639">NO_PINGS</strong> – The satellite has lost connection to the <strong>ground segment</strong>, although the terminal’s <strong>connection to the satellite remains intact.</strong></p>
<p>You can monitor the status of your Starlink account in the section “<strong>disablementCode</strong>“.</p>
<p>Possible disabement codes:</p>
<p><strong data-start="88" data-end="105">UNKNOWN_STATE</strong> – Terminal reported an undefined or unrecognized state.<br data-start="161" data-end="164"><strong data-start="164" data-end="172">OKAY</strong> – The Account is active and should provide access to the internet.<br data-start="235" data-end="238"><strong data-start="238" data-end="259" data-is-only-node="">NO_ACTIVE_ACCOUNT</strong> – Starlink account has been removed or the service is on pause.<br data-start="310" data-end="313"><strong data-start="313" data-end="345">TOO_FAR_FROM_SERVICE_ADDRESS</strong> – The terminal is operating outside the registered service address region.<br data-start="419" data-end="422"><strong data-start="422" data-end="434">IN_OCEAN</strong> – Terminal is located in an oceanic zone not covered by the current service plan.<br data-start="516" data-end="519"><strong data-start="519" data-end="538">BLOCKED_COUNTRY</strong> – Terminal is located in a country where Starlink service is not permitted.<br data-start="614" data-end="617"><strong data-start="617" data-end="648">DATA_OVERAGE_SANDBOX_POLICY</strong> – Data cap exceeded; Change service plan to get more data.<br data-start="718" data-end="721"><strong data-start="721" data-end="741">CELL_IS_DISABLED</strong> – The local service cell is disabled in the Starlink system.<br data-start="798" data-end="801"><strong data-start="801" data-end="820">ROAM_RESTRICTED</strong> – Roaming is not allowed under the current account or plan.<br data-start="880" data-end="883"><strong data-start="883" data-end="903">UNKNOWN_LOCATION</strong> – Terminal location cannot be determined by the Starlink satellite.<br data-start="972" data-end="975"><strong data-start="975" data-end="995">ACCOUNT_DISABLED</strong> – The Starlink account has been suspended or permanently disabled.<br data-start="1062" data-end="1065"><strong data-start="1065" data-end="1088">UNSUPPORTED_VERSION</strong> – Terminal firmware is incompatible or outdated.<br data-start="1137" data-end="1140"><strong data-start="1140" data-end="1170">MOVING_TOO_FAST_FOR_POLICY</strong> – Terminal is moving too quickly (e.g., in a vehicle or aircraft), violating policy limits.<br data-start="1262" data-end="1265"><strong data-start="1265" data-end="1298">UNDER_AVIATION_FLYOVER_LIMITS</strong> – Terminal is under an aircraft flyover zone where service is restricted.<br data-start="1372" data-end="1375"><strong data-start="1375" data-end="1394">INVALID_COUNTRY</strong> – Terminal is in a country not supported by Starlink.<br data-start="1448" data-end="1451"><strong data-start="1451" data-end="1473">UNLICENSED_COUNTRY</strong> – The terminal is located in a country where Starlink lacks regulatory approval.</p>
<p>The disablement code should always be available after a successful connection to the satellites. The Starlink system provides this information. The user terminal itself has no knowledge of service plans, countries, regional, or velocity restrictions – it simply follows commands received from the Starlink satellite.</p>
<p>Thanks for reading!</p>
<p><span>Tagged <a href="https://olegkutkov.me/tag/ethernet/" rel="tag">ethernet</a>, <a href="https://olegkutkov.me/tag/router/" rel="tag">router</a>, <a href="https://olegkutkov.me/tag/starlink/" rel="tag">starlink</a>, <a href="https://olegkutkov.me/tag/starlink-mini/" rel="tag">starlink mini</a>, <a href="https://olegkutkov.me/tag/wifi/" rel="tag">wifi</a></span>		</p></div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Datalog in Rust (144 pts)]]></title>
            <link>https://github.com/frankmcsherry/blog/blob/master/posts/2025-06-03.md</link>
            <guid>44281727</guid>
            <pubDate>Sun, 15 Jun 2025 11:18:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/frankmcsherry/blog/blob/master/posts/2025-06-03.md">https://github.com/frankmcsherry/blog/blob/master/posts/2025-06-03.md</a>, See on <a href="https://news.ycombinator.com/item?id=44281727">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>


                <li>
      

      <div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
          <p>
            GitHub Copilot
          </p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_product_navbar&quot;}" href="https://github.com/features/models">
      
      <div>
          <p>
            GitHub Models
              <span>
                New
              </span>
          </p><p>
        Manage and compare prompts
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_product_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
          <p>
            GitHub Advanced Security
          </p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
          <p>
            Actions
          </p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
          <p>
            Codespaces
          </p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                </ul>
              </div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
          <p>
            Issues
          </p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
          <p>
            Code Review
          </p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
          <p>
            Discussions
          </p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
          <p>
            Code Search
          </p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
          

      </div>
</li>


                <li>
      

      
</li>


                <li>
      

      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      Events &amp; Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      

      <div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
          <p>
            GitHub Sponsors
          </p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
          <p>
            The ReadME Project
          </p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      

      <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
          <p>
            Enterprise platform
          </p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:frankmcsherry/blog" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="GwwwokTeo9He2ovhUuidGFclxbPT_GWeVIQrZ5QoIX1iMF4N_OR0UPAzC9s-tNN9DnxqPzCFwP8Ei52zs3Aeew" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="frankmcsherry/blog" data-current-org="" data-current-owner="frankmcsherry" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=frankmcsherry%2Fblog" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/frankmcsherry/blog/blob/master/posts/2025-06-03.md&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="e2c7d38cd93d4f103f28153fd0b89ee5cb82474995e39ae1fa15163029f0b65a" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-b457d053-400b-4028-b8cf-2afa57fb2f5b" for="icon-button-de701988-0def-4064-a1a9-00ac019d07f8" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.c212c596cc6bbefb1798.module.css">
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.4e1ca273f504ba849f8c.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>

      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Art of Lisp and Writing (123 pts)]]></title>
            <link>https://www.dreamsongs.com/ArtOfLisp.html</link>
            <guid>44281016</guid>
            <pubDate>Sun, 15 Jun 2025 07:07:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dreamsongs.com/ArtOfLisp.html">https://www.dreamsongs.com/ArtOfLisp.html</a>, See on <a href="https://news.ycombinator.com/item?id=44281016">Hacker News</a></p>
<div id="readability-page-1" class="page">
		<p><a href="https://www.dreamsongs.com/index.html"><img src="https://www.dreamsongs.com/Files/DreamsongsBannerWeb.jpg" alt="" width="800" height="95"></a></p>
			
		<blockquote>Ignorance more frequently begets confidence than does knowledge.</blockquote>
		<p>–Charles Darwin</p>
		<p>Lisp is the language of loveliness. With it a great programmer can make a beautiful, operating thing, a thing organically created and formed through the interaction of a programmer/artist and a medium of expression that happens to execute on a computer.</p>
		<p>Taught that programming—or the worse "developing software"—is like a routine engineering activity, many find difficulty seeing writing as a model or even a metaphor for programming. Writing is creative, it is self-expression, it is art, which is to say it isn't a science and unlike science and engineering, it isn't a serious activity. Judgments like this, though, are easiest made by people who don't seriously engage in making both science and art. Art, engineering, and science are—in that order—part of a continuum of finding truth in the world and about ourselves.</p>
		<p>Artists make things and have always done so, gathering knowledge. There is sometimes no other purpose for what artists make than the need to embody an artistic statement in some form—perhaps in an artifact or perhaps on paper. When an artist makes something, he or she is drawing on two parts (at least) of the world at the same time: the part that is driving the construction which could be thought of as being within the artist in a sense, and the part that is in the world providing the substances that combine under physical laws to become the artistic artifact. The least bit of artistic knowledge-making is that the artist is, in making the thing, making a map of what is possible and how the physical world relates to the metaphorical, imaginative, and interior worlds of people. And the other extreme is that in trying to make something perhaps grand, the artist is stretching what people know about the world. Even today there is debate about how some artistic structures were made by ancient artists. Artists discover the properties of the world.</p>
		<p>Artists who create by writing or producing other representations of what the world is or could be are also laying out a map for how the world could become, and in cases where artists need technological explanations in their stories and myths, scientists and engineers in many cases explore how to build what they were delighted to learn about as children or adults from the dream-makers. For example, in 1587 someone published a set of tall tales about a possibly historical man known as "Faustus" in a book that came to be known as <em>German Faustbuch</em> or <em>Historia von D. Johan Faustus</em>. The Faustbuch was translated and changed by a man known only as "P. F." in 1592 into the English as <em>The History of the Damnable Life and Deserved Death of Doctor John Faustus</em>. In this book Dr. John Faustus rides a wagon pulled by dragons to the sun. And as time went on, engineers and scientists worked on and devised flying machines, as commonplace today as were sailing ships in the 16<sup>th</sup> century. Is it a coincidence that cellphones look suspiciously like Star Trek communicators? We can see an early fascination with traveling beneath the sea in the story of Jonah and in the legend of Atlantis. Greeks and Romans wrote about diving bells, and so did medieval writers.</p>
		<p>As people need or want to do things with materials and the world, people with special skill take the fore and devise or discover how to manipulate the physical world to make those things. To avoid future mistakes, these makers write down rules of thumb, patterns of creation and making, and safety factors as a practical matter. Today we call them engineers. When we think of engineering today we think of carefully planned scientific engineering such as building bridges, where it is a fairly linear though costly and complicated process to go from the planning stage to a completed bridge. We forget the centuries of tinkering with bridge design in prehistoric and ancient times when bridges were gingerly tested as designers searched for principles. Even still, on November 7, 1940, at 11:00AM, the Tacoma Narrows Bridge collapsed from wind-caused vibrations after being open to the public for a few months, showing that even sophisticated engineering techniques—one could even call them contemporary engineering techniques—can fail.<sup>†</sup></p>
		
		<p>Nevertheless, engineering knowledge has usually preceded scientific knowledge and often has remained constant while scientific theories about the engineering materials came and went. "Engineers" were able to create sophisticated fires for cooking and metal working using chimneys and other forms of forced oxygen for centuries while scientists flailed from theory to theory. For example, while advanced fire-based engineering was going on in the 6<sup>th</sup> century BC in Greece, Anaximenes's theory was that the source or nature of the cosmos was <em><strong>aer</strong></em> (air). <em>Aer</em> was constantly in motion. When <em>aer</em> expanded it became <em><strong>aither</strong></em> (ether: bright fiery air as on a clear day); when it expanded further, it became fire. As aer condensed it became clouds; further condensation led to water, ice, earth, and finally rock. Throughout history there have been various theories of what fire was; nevertheless, the chimneys and other artifacts of working with fire remained constant and working properly.</p>
		<p>Scientists come along at the end; they take what artists and engineers have found out or dreamed about the world and try to weave a simplified narrative that explains it all. Scientists need to find the small set of facts, conditions, laws, forces, and principles that cause the beautiful complexity we experience. Since Galileo they have agreed to use mathematics as the language of scientific knowledge.</p>
		<p>Scientists, though, perhaps because of their desire for accuracy and precision have had the worst luck of all in pinning down the truth of the universe. Philosophers and historians of science have pointed out and speculated on how scientific theories come and go—through refinement, through revolution, or even through anarchy.</p>
		<p>As the 20<sup>th</sup> century proceeded, for example, the theory of quantum mechanics and the expanding plausibility of complexity science threw everything we knew about the world into doubt, rendering many of our best theories approximations at best, or at least opening the door for a new kind of formulation (other than mathematics) for describing the truth about reality.</p>
		<p>Perhaps it's because they are the most consistent losers when it comes to understanding the world that we love scientists so much, tell them constantly that they are the only true beacons we have pointing out what is true; we shower them with grants and money for large experiments and take away funding from the arts.</p>
		<p>Well, let's look at the more successful branches of human endeavor: art and engineering. I was recently reading about writers and mapmakers. Writers, of course, are among the most imaginative creators of false realities, meaning that they create worlds and characters that are almost like the real thing, but nowhere will we ever find those places or people or circumstances. We might be tempted to call them liars, but even though all the details are in essence not representative of a particular reality, what is contained in their work is essentially true—there may not be a Nick Adams and neither a Big Two-Hearted River in Seney, but Heminway captured something about real life that makes his story not a lie.</p>
		<p>A mapmaker, though, is a kind of engineer—designing and creating representations of the real world for a purpose, usually the purpose of understanding the lay of the land or how to get from one place to another. A mapmaker strives for accuracy and precision in representing the geography of the world. Right?</p>
		<p>Not really. First, maps of regions of the earth involve some sort of mathematical projection which, unless understood in a way probably few people can, distorts some aspect of reality significantly. The Mercator projection represents constant bearing paths such as sea routes as straight lines, but whereas meridians converge at the poles, in the Mercator projection they are parallel. Further, length and area are not preserved.</p>
		<p>If a mapmaker makes a local map, the surrounding areas are left out, as if they don't exist. If the map is a of a political area, it might leave out places important to people living there—especially in gerrymandered political regions.</p>
		<p>A mapmaker's skill lies in knowing how much detail to show and where to leave things out or even tell lies. The following is perhaps one of the most useful maps in the world today. It depicts the London underground transportation system—the Tube.</p>
		<p><img src="https://www.dreamsongs.com/Files/tubemap2.gif" alt="" height="400" width="600"></p>
		<p>This map is indispensable even while it totally distorts the geography and geometry of London. Using a ruler to measure distances or using a compass to determine headings is useless. Almost all of London is left out. The Thames, for example, more closely resembles heat ducting in a remodeled London warehouse than it does the famous river or any river for that matter.</p>
		<p>Perhaps mapmakers are liars, too.</p>
		<p>Novelists and especially short story writers—not to mention poets who do it the most—are in the same business. There simply cannot be enough detail in a story to be any sort of accurate representation of the actual world. Many characters in stories apparently don't work, don't eat, don't go to the bathroom, don't pay bills, don't work out, don't shop—in fact, all that they do is what's connected with the story and what we need to know as readers about them. We don't know where some characters live, what their childhoods were like, where they grew up, what they look like, or even sometimes their names. Fictional worlds are partly real, mainly made up, and almost entirely left out. In this, writers' work resembles mapmakers'.</p>
		<p>These skills are the skills of guiding—they are the skills of perfecting the presentation of the finished product. For the mapmaker, the available tools include existing maps, computer-aided design tools, and the talent and skill to know what a map is for and how to create one that serves that purpose for human consumption.</p>
		<p>In contrast, the writer has only his or her skills; pens, pencils, computers, and paper; and the language—for simplicity let's say it's English—he or she is writing in. The act of creating a good presentation of a story is the act of writing followed by revision. The writer is guided by the many thousands of writers and stories that have come before, the conventions of the culture, and the extensive reading skills the audience can be expected to have.</p>
		<p>But neither writing nor mapmaking are simply guiding. Guides need to have knowledge of their landscapes, and such knowledge comes from exploration and discovery.</p>
		<p>Obviously gathering the knowledge needed to make a map is discovery—in fact, when asked to list the best known explorers we think of people who have explicitly set out to complete a map, where a map includes everything that is found there—not simply the land and its features but the people, towns, cities, roads, resources, minerals, crops, climate, and local flora and fauna. Christopher Columbus, Ferdinand Magellan, Marco Polo, Lewis and Clark. To explore, one simply goes and observes. To find out what's on the other side of a mountain range, one just goes there; to see what an island has to offer one simply wanders. But these acts of going and wandering require also noticing and noting.</p>
		<p>Most of exploration is in the nature of the locally expected: What is on the other side of that hill is likely to be a lot like what's on this side. Only occasionally is the explorer taken totally by surprise, and it is for these times that many explorers live. Similarly for writers: What a writer thinks up in the next minute is likely to be a lot like what is being thought this minute—but not always: Sometimes an idea so initially apparently unrelated pops up that the writer is as surprised as anyone. And that's why writers write.</p>
		<p>Richard Hugo, the 20<sup>th</sup> century poet and writing teacher wrote about the use of <em><strong>triggers</strong></em> in creative work. A trigger is any idea, scene, image, thought, sound, smell, or memory that by its "appearance" in a mind causes that mind to create something. A trigger can end up being a metaphor, a reminder, a stepping stone, an association, or anything that sparks connections to the creative mind. Hugo almost goes so far as saying that all creativity is really just an ability and willingness to take up the thread a trigger provides.</p>
		<blockquote>
		A poem can be said to have two subjects, the initiating or triggering subject, which starts the poem or "causes" the poem to be written, and the real or generated subject, which the poem comes to say or mean, and which is generated or discovered in the poem during the writing. That's not quite right because it suggests that the poet recognizes the real subject. The poet may not be aware of what the real subject is but only [has] some instinctive feeling that the poem is done.
		</blockquote>
		<p>–Richard Hugo, <strong>The Triggering Town</strong></p>
		<p>Writing, then, can be viewed as two acts put together: the act of discovery and the act of perfecting the presentation. One seems creative and the other analytic, and therefore, it would seem to make sense to separate them, using the best tools for each, maybe even using the best people for each. This is where some of our software methodologists have landed.</p>
		<blockquote>But for almost all writers, the act of discovery is mixed up with the act of perfecting the presentation, because the actual words and the need to select and prune act as triggers and forces on the material in ways that can both require more discovery and cause it whether required or not. And so while each writer may always start with an act of discovery and always end with an act of perfecting, the proportion of each type of act and the order in which they appear will be otherwise as varied as possible. We might be able to say that the statistical behavior of writers shows that more acts of discovery take place near the beginning and more acts of perfecting take place at the end, but this is unimportant information for any particular writer. It can be argued that all writing is creative writing, since if one is writing the way one should, one does not know what will be on the page until it is there. Discovery remains the ideal.</blockquote>
		<p>–Richard Hugo, <strong>The Triggering Town</strong></p>
		<p>Writing a beautiful text takes sitting before a medium that one can revise easily, and allowing a combination of flow and revision to take place as the outlines and then the details of the piece come into view. Changes are made as features of the writing emerge—first the noise, then the sense, the force, and then the meaning—and the work of the writer is to continually improve the work until it represents the best words in the best order. Great writing is never accomplished through planning followed by implementation in words, because the nature of the word choices, phrasings, sentence structures, paragraph structures, and narrative structures interact in ways that depend of each other, and the only possible plan that can be made with which a writer can "reason" about the piece is the piece itself.</p>
		<p>This is true of all creative making—flow and revision are the essences. Flow is the preferred state of mind for the writer doing discovery. Flow means that there are no barriers between the movement of the mind and the placement of words on the page. When in flow, the writer feels as if he or she is in a vivid and continuous dream in which the relevant parts of the dream flow onto the page.</p>
		<p>Revision includes reading material already written and perfecting it as presentation, but as the name suggests, it is more. Discovery is always mixed in with perfecting, because, as Hugo says, "one does not know what will be on the page until it is there." Moreover, revision—when performed courageously—involves determining what to leave out, just as the mapmaker determines what in the real world can remain unmentioned and unrepresented for a map made for a particular purpose.</p>
		<p>For many, this will not sound as if I am describing programming—or as some like to put it: software design and implementation. To satisfy the biases of such folks, I will use untypical words and phrases for concepts that have acquired too strict a meaning.</p>
		<p>Following the lead of Paul Graham, I will call Lisp a <em>programming medium</em> to distinguish it from languages like Java™ which is a programming language.</p>
		<blockquote>Very dynamic languages like Lisp, TCL, and Smalltalk are often used for prototyping. One of the reasons for their success at this is that they are very robust.... Another reason ... is that they don't require you to pin down decisions early on. Java has exactly the opposite property: it forces you to make choices explicitly.</blockquote>
		<p>–James Gosling, <strong>Java: An Overview</strong></p>
		<p>The difference between Lisp and Java, as Paul Graham has pointed out, is that Lisp is for working with computational ideas and expression, whereas Java is for expressing completed programs. As James says, Java requires you to pin down decisions early on. And once pinned down, the system which is the set of type declarations, the compiler, and the runtime system make it as hard as it can for you to change those assumptions, on the assumption that all such changes are mistakes you're inadvertently making.</p>
		<p>There are, of course, many situations when making change more difficult is the best thing to do: Once a program is perfected, for example, or when it is put into light-maintenance mode. But when we are exploring what to create given a trigger or other impetus—when we are in flow—we need to change things frequently, even while we want the system to be robust in the face of such changes. In fact, most design problems we face in creating software can be resolved only through experimentation with a partially running system. Engineering is and always has been fundamentally such an enterprise, no matter how much we would like it to be more like science than like art. And the reason is that the requirements for a system come not only from the outside in the form of descriptions of behavior useful for the people using it, but also from within the system as it has been constructed, from the interactions of its parts and the interactions of its parts separately with the outside world. That is, requirements emerge from the constructed system which can affect how the system is put together and also what the system does. Furthermore, once a system is working and becomes observable, it becomes a trigger for subsequent improvement. The Wright Brothers' first flying machine likely satisfied all the requirements they placed on it, but they were unwilling to settle for such modest ambitions—and neither was the world—and even today we see the requirements for manned flight expanding as scientific, engineering, and artistic advances are made.</p>
		<p>Christopher Alexander spoke of these issues as follows while discussing an initial Bay Area Rapid Transit ticket booth design study he did early in his career:</p>
		<blockquote>
			So it became clear that the free functioning of the system did not purely depend on meeting a set of requirements. It had to do, rather, with the system coming to terms with itself and being in balance with the forces that were generated internal to the system, not in accordance with some arbitrary set of requirements we stated.... What bothered me was that the correct analysis of the ticket booth could not be based purely on one's goals, that there were realities emerging from the center of the system itself and that whether you succeeded or not had to do with whether you created a configuration that was stable with respect to these realities.</blockquote>
		<p>–Christopher Alexander</p>
		<p>What bothers and surprises me that so many software developers, managers, and thinkers don't appear to understand this point. James Gosling in talking about Java acknowledges that at least the object-oriented notion of late or dynamic binding is important to support shipping new (though constrained) versions of libraries and to make derived classes, but he seems to forget that the malleability of the medium while programming is part of the act of discovery that goes into understanding all the requirements and forces—internal or not—that a system must be designed around.</p>
		<p>As a system is implemented and subjected to human use, all sorts of adjustments are required. What some call the "user interface" can undergo radical reformulation when designers watch the system in use. Activities that are possible using the system sometimes require significant redesign to make them convenient or unconscious. As people learn what a system does do, they start to imagine what further it could do. This is the ongoing nature of design.</p>
		<p>We see it at work in writing as well. As the writer reads what's been written, changes come to mind—improvements based on writing gone awry, better words popping up, imperfections, wrong turns, too much left out, too much detail creating distractions, better approaches, or even better ideas and directions. The acts of discovery and perfecting merge for the writer, and for this to happen it is essential that the medium in which the creation is built be the medium in which it is delivered. No writer could even begin to conceive of using one language for drafting a novel, say, and another to make its final form.</p>
		<p>Many writers share drafts with other writers to get more eyes and minds involved. There is a tradition of writers' workshops for helping a writer see what the work really is, to understand how readers will approach it, what will be clear and what obscure. Like the mapmaker, the writer wants to find out how travelers will get on in the newly created world. And at any point, the process of discovery can kick in again.</p>
		<p>Like many types of artist, the writer is manipulating during the discovery and perfecting stages the very material that represents the work when complete. In the old days, the manuscript would be retyped or typeset, but not so much anymore. The word "manuscript" itself refers to the (hand written) copy of the work before printing and publication, which the writer constantly revises. But the manuscript—which is constantly worked over—differs from a published work only by finally pinning down decisions like fonts, page breaks, illustrations, and design elements.</p>
				<p><img src="https://www.dreamsongs.com/Files/Break2.gif" width="59" height="18"></p>
		<p>The distinction some make between prototyping and software development has vexed me my whole career. I have never seen a version 1.0 of a system that was acceptable. I find that every software group messes with the software up until the very time it's deployed or delivered. At the outset the developers want to get something running as soon as possible—they are interested in leaving things out that are not necessary to the purpose of understanding the internal forces and external reactions affecting what has been built. When prototyping was considered part of the development process, people spoke of prototyping languages or languages good for prototyping as an ideal. Though these times are long past, software developers still need to go through the same steps as prototypers did and for the same reasons, but what these developers are using are delivery languages or languages designed to describe tightly a finished program.</p>
		<p>The idea behind a description language is to create the most complete description of a runnable program, primarily for the benefit of a compiler and computer to create an executable version of it that is as efficient as it can be, but sometimes also for readers who will perhaps be enlightened by knowing all the details of the program without having to do a lot of thinking.</p>
		<p>I like to think of a program written in one of these languages as a "measured drawing." A measured drawing is a set of plans made by a craftsman of a piece of furniture, for example, which enables an amateur to make an exact copy. Whereas a master craftsman will be able to take a rough drawing or an idea of such a piece and by virtue of his or her expertise make a perfect specimen, an amateur may not be able to make all the joints perfectly and in the perfect positions without having all the dimensions and distances spelled out. Often, a measured drawing requires the craftsman to build a prototype to understand the piece and then a second to make one perfect enough to provide the measurements.</p>
		<p>The problem with the idea of using a programming language designed to describe finished programs is that programs are just like poems. Paul Valéry wrote, "a poem is never finished, only abandoned." Developers begin work on a project and deliver it when it seems to work well enough—they abandon it, at least temporarily. And hence the series of released versions: 1.0, 1.1, 1.5, 1.6, 2.0, 2.05, etc. Trying to write a program in manuscript using a finished-program programming language would be like trying to write a poem using a Linotype machine. Perhaps using a Linotype machine and thereby producing a beautiful printed book of poetry is good for readers, critics, and therefore literature, but it doesn't help poets and would make their writing lives intolerable with lousy poetry the result.</p>
		<p>The screwed-up way we approach software development is because of what we have done with programming languages. With some exceptions, we have opted for optimizing the description of programs for compilers, computers, and casual human readers over providing programming media. Early on, computers were insanely slow, and so performance was the key characteristic of a system, and performance was easiest to achieve when the language required you to "pin down decisions early on." Because of this, much work was poured into defining such languages and producing excellent compilers for them. Eventually, some of these languages won out over ones more suitable for drafting and exploration both in the commercial world and in academia, and as a result it is politically difficult if not impossible to use these so-called "very dynamic" languages for serious programming and system building.</p>
		<p>Interestingly, the results of this optimization are well-described by Hoare's Dictum, often mistakenly attributed to Donald Knuth, which states:</p>
		<blockquote>
			Premature optimization is the root of all evil in programming.</blockquote>
		<p>–C. A. R. Hoare</p>
		<p>Professor Sir C. A. R. Hoare was referring to the practice of worrying about performance of an algorithm before worrying about correctness, but the dictum can be taken to refer to any design problem where optimization is an eventual concern. In this case, the design problem was to design a usable programming medium that excels at enabling developers and designers to explore and discover, and to continue to enable discovery and exploration once well into the perfecting stage. Instead of waiting until we understood the ramifications of large system design and implementation using computer programming media, we decided to prematurely optimize for performance and optimization. And we got program description (or programming) languages instead—the root of all evil.</p>
		<p>We can tell this analysis is correct by observing the current fad of Agile methodologies including Extreme Programming. These methodologies all emphasize practices in which code is delivered early in order to enable exploring the design space in the presence of the forces generated by the system itself. As the process continues, the system grows and matures in the context of both internal (system-derived) and external forces.</p>
		<p>And we can tell that program description languages have taken the field by observing that very dynamic languages like Smalltalk and Lisp have faded into the background.</p>
			<p><img src="https://www.dreamsongs.com/Files/Break2.gif" width="59" height="18"></p>
		<p>Lisp is a medium for working with a computation until it is in balance with its external and internal requirements. At that point it can be decorated with performance-enhancing declarations and perhaps additionally modularized. In this it is more like an artist's medium than what many think of as a programming language.</p>
		<p>Lisp, viewed this way, is a good vehicle for understanding how programming and software development really take place. Because programming Lisp is more like writing than like describing algorithms, it fits with how people work better than the alternatives. The problem, of course, is that writing is considered a "creative" activity, involving "self-expression," and creativity is a gift while self-expression is hokey. Not so:</p>
		<blockquote>
			The conventional wisdom here is that while "craft" can be taught, "art" remains a magical gift bestowed only by the gods. Not so. In large measure becoming an artist consists of learning to accept yourself, which makes your work personal, and in following your own voice, which makes your work distinctive. Clearly these qualities can be nurtured by others.... Even talent is rarely distinguishable, over the long run, from perseverance and lots of hard work.</blockquote>
		<p>–David Bayles &amp; Ted Orland, <strong>Art &amp; Fear</strong></p>
		<p>Writing and programming are creative acts, yet we've tried to label programming as engineering (as if engineering weren't creative). Instead of fearing creativity, we need to embrace it.</p>
		<p>Moreover, the key to exploratory programming is to be open to triggers and what we happen upon, because the balance a great system needs is not from purely external considerations. It is ok to have been wrong in one's assumptions as long as one is willing to change and revise. In the end one may need to become more of a guide than an explorer, but one must always be open to being an explorer.</p>
				<p><img src="https://www.dreamsongs.com/Files/Break2.gif" width="59" height="18"></p>
		<p>Lisp, of course, and especially Common Lisp are my languages. Not since the 1960s have I programmed professionally in a language other than Lisp. Sure, some of that was when I was doing AI research, but most of it was writing system software such as the commercial implementations of Common Lisp my company, Lucid, Inc., produced in the 1980s and early 1990s. I worked on programming environments, debuggers, compilers, memory managers, assemblers, and language extensions. In many cases I was exploring what could be achieved with the goal of productizing the results when the ideas were proved out. Just as I explored the ideas in this essay before revising and polishing it.</p>
		<p>Perhaps it's the coincidence of the same mind apprehending different things, but I used to describe AI as "trying to program what cannot be programmed," and I currently define poetry as "trying to say what cannot be said."</p>
		<p>In 1995 I decided to return to school and get an Master of Fine Arts in creative writing (poetry) at a low-residency program where most of the work is done at home with only one 10-day residency per semester. At that time, Lucid had been out of business for only a year, and Lisp-based systems had been winning various software-of-the-year awards. I had been consulting at a company that was doing a Lisp-based product, but they were exploring other implementation languages for business reasons.</p>
		<p>The school I attended was Warren Wilson College, which is located in Swannanoa in the western part of North Carolina in the Blue Ridge Mountains. It is a small school with a world-class writing program. One of the quaint things about the school is that it has a working farm students are expected to work. The writing world is different from the computing world in many ways, and soon I was immersed in learning how to write poetry. Because I had never had a literature background, I needed to do a lot of catching up, so I spent about 60 hours a week working on schoolwork. I was lucky to find a position as a consultant to Bill Joy on a couple of small projects, and he essentially subsidized me for 2 years learning to write.</p>
		<p>When I graduated in 1998, I expected to come back to computing to support myself, preferably by doing research. I had made my 25-year career in Lisp, starting the Common Lisp effort, helping design CLOS, doing research in programming environments, compilation, and parallel languages. I thought that though Lisp might not have a great commercial future, university and industry labs would still recognize Lisp as a viable research vehicle.</p>
		<p>I was shocked to learn that I was wrong. As examples, the scientific journal Guy Steele and I founded called <em>Lisp and Symbolic Computation</em> (LASC) had been renamed <em>Higher-Order and Symbolic Computation</em> (HOSC), and my primary scientific conference called <em>Lisp and Functional Programming</em> (L&amp;FP) had been renamed the <em>International Conference on Functional Programming</em> (ICFP). I was discouraged for two reasons. One was that during the 1980s when functional programming researchers had few outlets, many of us Lispers kept the lights on at L&amp;FP for them, and later when Lisp was on the decline, they shut the door. And the other is that this seemed to signal that my field of inquiry had been deleted by the academy.</p>
		<p>Now, 5 years later, things are no better even though we face a crisis in computing wherein we cannot build the sorts of systems we need because we as a discipline have embraced a philosophy of inflexibility as part of a fear of large systems when in fact the opposite philosophy is needed. As Jaron Lanier points out, we have not taken seriously the idea that a system or module should be liberal in trying to accept commands or arguments, in trying to understand what the caller wanted done. As Jon Postel remarked:</p>
		<blockquote>
			In general, an implementation must be conservative in its sending behavior, and liberal in its receiving behavior.</blockquote>
		<p>–Jon Postel, <strong>RFC 791</strong></p>
		<p>This is a statement about flexibility, about keeping a system in balance. The perfect bridge is not perfectly rigid, nor is the perfect building. Rigidity works when nothing moves or changes. Flexibility is required when the ground is prone to shake or the wind to blow. Had Web browsers required perfectly formed HTML from the outset, there would not have been the Web. But they were designed to encounter and deal with almost anything that came along.</p>
		<p>Discouraged by having no computing career and lamenting static thinking, I was originally heartened by the rising importance of XML—which amounts to some fundamental Lisp data structures reinterpreted by people with bad taste brainwashed by inflexibility. When I mentioned to some programmers that it might be a good idea for a program receiving XML to use pattern recognition techniques like the ones we used in the old AI days to try to understand what the XML was trying to say, they scrunched their faces as if I had told them that poetry is as true as any scientific theory.</p>
		<p><img src="https://www.dreamsongs.com/Files/Break2.gif" width="59" height="18"></p>
		<p>I'm glad to see a new book on Lisp. I'm glad there are starting to be some highly innovative conferences on new thinking about computing. And finally I'm glad to see some projects that are throwing off the shackles of inflexible programming: autonomic computing, organic computing, phenotropic computing, acceptability-oriented computing, and the Feyerabend project.</p>
		<p>I believe William Stafford captured best what programming is, especially using Lisp, when he defined art this way:</p>
		<blockquote>
			Art is a reckless encounter with whatever comes along.</blockquote>
		<p>–William Stafford, <strong>Writing the Australian Crawl</strong></p>
		<hr>
		<hr>
		<p>† Since I wrote this I found two other more contemporary failures worth noting: the Millennium Bridge in London, which opened June 10, 2000 and closed on June 12, 2000 after enduring unexpected swaying and wobbling; and the new terminal at the Charles de Gaulle airport in Paris, whose roof collapsed on May 23<sup>rd</sup> 2004 shortly after opening.</p>
		<p>The Millenium Bridge was a surprising failure given the high profile of the project and the reputations of the designers and engineering firms involved. Software pundits are sometimes fond of using bridge engineering as an exemplar that software engineering should mimic.</p>
		<blockquote>The design of the bridge was decided by a competition organised in 1996 by Southwark council. The winning entry was a innovative “blade of light” effort from Arup, Foster and Partners and Sir Anthony Caro. Due to height restrictions, and to improve the view, the bridge’s suspension design had the supporting cables below the deck level, giving a very shallow profile. The bridge has two river piers and is made of three main sections of 81m, 144m and 108m (North to South) with a total structure length of 325m; the aluminum deck is 4m wide.</blockquote>
		<blockquote>
			Construction began in late 1998 with the main works beginning on April 28, 1999 by Monberg Thorsen and McAlpine. The bridge was completed at a cost of £18.2m (£2.2m over budget) and opened on June 10, 2000 (2 months late) but unexpected lateral vibration (resonant structural response) caused the bridge to be closed on June 12 for modifications. The movements were produced by the sheer numbers of pedestrians (90,000 users [[ Note the use of the term.]] in the first day, with up to 2,000 on the bridge at any one time). The initial small vibrations encouraged the users to walk in synchronization with the sway, increasing the effect. This swaying motion earned it the nickname the Wobbly Bridge.</blockquote>
		<blockquote>
		Resonant vibrational modes have been well understood in bridge designs following the failure of the Tacoma Narrows Bridge. However this was the first time a bridge had displayed this type of pedestrian excited lateral motion. As such the motion was not anticipated by the computational analysis of the bridge prior to construction. It is often thought that the unusually low profile of the suspension cables contributed to the problem, but an analysis by the structural engineer, Arup, shows that it can occur in any suspension bridge which happens to have the appropriate resonant frequencies. After extensive analysis, the problem was fixed by the retrofitting of 37 fluid-viscous dampers (energy dissipating) to control horizontal movement and 52 tuned mass dampers (inertial) to control vertical movement. This took from May 2001 to January 2002 and cost £5m. After a period of testing the bridge was successfully re-opened on February 22, 2002.
		</blockquote>
		<p>–http://en.wikipedia.org/wiki/London_Millennium_Bridge</p>
		<p>Some architectural failures are the result of a set of small design and construction problems which combine to tragic results:</p>
		<blockquote>
			Terminal 2E, with a daring design and wide open spaces, was CDG's newest addition. However, on 23 May 2004, not long after its inauguration, a portion of Terminal 2’s ceiling collapsed early in the day, near Gate E50, killing four people. Terminal 2E had been inaugurated in 2003 after some delays in construction and was also designed by Paul Andreu. Administrative and judicial enquiries were started. Coincidentally, Andreu had also designed Terminal 3 at Dubai International Airport, which collapsed while under construction on September 28, 2004.
		</blockquote>
		<blockquote>
			In February 2005, the results from the administrative enquiry were published. The experts pointed out that the there existed no single fault, but rather a multiplicity of causes to the collapse, in a design that had little margins of safety. According to them, the concrete vaulted roof was not resilient enough and had been pierced by metallic pillars; and some openings weakened the structure. Sources close to the enquiry also disclosed that the whole building chain had worked as close to the limits as possible, so as to reduce costs. Paul Andreu denounced the building companies for having not correctly prepared the reinforced concrete.
		</blockquote>
		<blockquote>
			On March 17, 2005, ADP decided to tear down and rebuild the whole part of Terminal 2E (the “jetty”) of which a section had collapsed, at a cost of approximately €100 million.
					</blockquote>
		<p>–http://en.wikipedia.org/wiki/Charles_de_Gaulle_International_Airport</p>
		<p><em>[[In 2003 I wrote a foreword for "Successful Lisp: How to Understand and Use Common Lisp," by David B. Lamkins. This essay is that foreword.]]</em></p>
	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Q-learning is not yet scalable (187 pts)]]></title>
            <link>https://seohong.me/blog/q-learning-is-not-yet-scalable/</link>
            <guid>44279850</guid>
            <pubDate>Sun, 15 Jun 2025 00:56:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://seohong.me/blog/q-learning-is-not-yet-scalable/">https://seohong.me/blog/q-learning-is-not-yet-scalable/</a>, See on <a href="https://news.ycombinator.com/item?id=44279850">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <div>
            <h2>Does RL scale?</h2>
            <p>
                Over the past few years,
                we've seen that next-token prediction scales, denoising diffusion scales, contrastive learning scales,
                and so on, all the way to the point where we can train models with billions of parameters
                with a <i>scalable</i> objective that can eat up as much data as we can throw at it.
                Then, what about reinforcement learning (RL)?
                <b>Does RL also <i>scale</i> like all the other objectives?</b>
            </p>
            <p>
                Apparently, it does.
                In 2016, RL achieved superhuman-level performance in games like Go and Chess.
                Now, RL is solving complex reasoning tasks in math and coding with large language models (LLMs).
                This is great. However, there is one important caveat:
                most of the current real-world successes of RL have been achieved with <b>on-policy RL</b> algorithms
                (<i>e.g.</i>, REINFORCE, PPO, GRPO, etc.),
                which <i>always</i> require fresh, newly sampled rollouts from the current policy,
                and cannot reuse previous data
                (<i>note: while PPO-like methods can technically reuse data to some (limited) degree, I'll classify them as on-policy RL,
                as in <a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html">OpenAI's documentation</a></i>).
                This is not a problem in <i>some</i> settings like board games and LLMs,
                where we can cheaply generate as many rollouts as we want.
                However, it is a significant limitation in <i>most</i> real-world problems.
                For example, in robotics, it takes <a href="https://x.com/KyleStachowicz/status/1885359401546162638">more than several months</a> in the real world to generate
                the amount of samples used to post-train a language model with RL,
                not to mention that a human must be present 24/7 next to the robot to reset it during the entire training time!
            </p>
        </div>
        <div>
                <p>
                    <img src="https://seohong.me/blog/q-learning-is-not-yet-scalable/assets/on_off.png">
                </p>
                <p><span>
                    On-policy RL can only use fresh data collected by the current policy \(\pi\).
                    Off-policy RL can use <i>any</i> data \(\mathcal{D}\).
                </span>
            </p></div>
        <div>
            <p>
                This is where <b>off-policy RL</b> comes to the rescue.
                In principle, off-policy RL algorithms can use <i>any</i> data, regardless of when and how it was collected.
                Hence, they generally lead to much better sample efficiency, by reusing data many times.
                For example, off-policy RL can train <a href="https://sites.google.com/berkeley.edu/walk-in-the-park">a dog robot to walk in 20 minutes from scratch in the real world</a>.
                <b>Q-learning</b> is the most widely used off-policy RL algorithm.
                It minimizes the following temporal difference (TD) loss:
                <span>
                    $$\begin{aligned}
                    \mathbb{E}_{(s, a, r, s') \sim \mathcal{D}} \bigg[ \Big( Q_\theta(s, a) - \big(r + \gamma \max_{a'} Q_{\bar \theta}(s', a') \big) \Big)^2 \bigg],
                    \end{aligned}$$
                </span>
                where \(\bar \theta\) is the parameter of the target network.
                Most practical (model-free) off-policy RL algorithms are based on some variants of the TD loss above.
                So, to apply RL to many real-world problems,
                the question becomes: <b>does Q-learning (TD learning) scale?</b>
                If the answer is yes, this would lead to at least an equivalent level of impact as the successes of AlphaGo and LLMs,
                enabling RL to solve far more diverse and complex real-world tasks very efficiently,
                in robotics, computer-using agents, and so on.
            </p>
            <h2>Q-learning is not yet scalable</h2>
            <p>
                Unfortunately, my current belief is that the answer is <b>not yet</b>.
                I believe current Q-learning algorithms are not readily scalable, at least to <i>long-horizon</i> problems that require more than (say) 100 semantic decision steps.
            </p>
            <p>
                Let me clarify. My definition of scalability here is the ability to solve <i>more challenging, longer-horizon</i> problems
                with more data (of sufficient coverage), compute, and time.
                This notion is different from the ability to solve merely a <i>larger number</i> of (but not necessarily harder) tasks with a single model,
                which many excellent <a href="https://sites.google.com/view/scaling-offlinerl/home">prior</a> <a href="https://sites.google.com/view/perceiver-actor-critic">scaling</a> <a href="https://arxiv.org/abs/2505.23150">studies</a> have shown to be possible.
                You can think of the former as the "depth" axis and the latter as the "width" axis.
                The depth axis is more important and harder to push, because it requires developing more advanced decision-making capabilities.
            </p>
            <p>
                I claim that Q-learning, in its current form, is <i>not</i> highly scalable along the depth axis.
                In other words, I believe we still need <i>algorithmic breakthroughs</i> to scale up Q-learning (and off-policy RL) to complex, long-horizon problems.
                Below, I'll explain two main reasons why I think so:
                one is anecdotal, and the other is based on our <a href="https://arxiv.org/abs/2506.04168">recent scaling study</a>.
            </p>
        </div>
        <div>
                <p>
                    <img src="https://seohong.me/blog/q-learning-is-not-yet-scalable/assets/logos.png">
                </p>
                <p><span>
                    Both AlphaGo and DeepSeek are based on <i>on-policy</i> RL and do not use TD learning.
                </span>
            </p></div>
        <div>
            <p>
                Anecdotal evidence first.
                As mentioned earlier, most real-world successes of RL are based on on-policy RL algorithms.
                AlphaGo, AlphaZero, and MuZero are based on model-based RL and Monte Carlo tree search, and do not use TD learning on board games
                (see 15p of the <a href="https://arxiv.org/abs/1911.08265">MuZero</a> paper).
                OpenAI Five achieves superhuman performance in Dota 2 with PPO
                (see footnote 6 of the <a href="https://arxiv.org/abs/1912.06680">OpenAI Five</a> paper).
                RL for LLMs is currently dominated by variants of on-policy policy gradient methods, such as PPO and GRPO.
                Let me ask: do we know of any <i>real-world</i> successes of off-policy RL (1-step TD learning, in particular) on a similar scale to AlphaGo or LLMs?
                If you do, please let me know and I'll happily update this post.
            </p>
            <p>
                Of course, I'm not making this claim based only on anecdotal evidence.
                As said before, I'll show concrete experiments to empirically prove this point later in this post.
                Also, please don't get me wrong, I'm still highly optimistic about off-policy RL and Q-learning (as an RL researcher who mainly works in off-policy RL!).
                I just think that we are not there yet, and <b>the purpose of this post is to call for research in RL algorithms, rather than to discourage it!</b>
            </p>
            <h2>What's the problem?</h2>
            <p>
                Then, what fundamentally makes Q-learning not readily scalable to complex, long-horizon problems, unlike other objectives?
                Here is my answer:
                <span>
                    $$\begin{aligned}
                    \definecolor{myblue}{RGB}{89, 139, 231}
                    \mathbb{E}_{(s, a, r, s') \sim \mathcal{D}} \bigg[ \Big( Q_\theta(s, a) - \underbrace{\big(r + \gamma \max_{a'} Q_{\bar \theta}(s', a') \big)}_{{\color{myblue}\texttt{Biased }} (\textit{i.e., }\neq Q^*(s, a))} \Big)^2 \bigg]
                    \end{aligned}$$
                </span>
                Q-learning struggles to scale because <b>the prediction targets are biased, and these biases <i>accumulate</i> over the horizon.</b>
                The presence of <b>bias accumulation</b> is a fundamental limitation that is <i>unique</i> to Q-learning (TD learning).
                For example, there are no biases in prediction targets in other scalable objectives
                (<i>e.g.</i>, next-token prediction, denoising diffusion, contrastive learning, etc.)
                or at least these biases do not accumulate over the horizon (<i>e.g.</i>, BYOL, DINO, etc.).
            </p>
        </div>
        <div>
                <p>
                    <img src="https://seohong.me/blog/q-learning-is-not-yet-scalable/assets/toy_accumulation_simple.png">
                </p>
                <p><span>
                    Biases accumulate over the horizon.
                </span>
            </p></div>
        <div>
            <p>
                As the problem becomes more complex and the horizon gets longer, the biases in bootstrapped targets accumulate more and more severely,
                to the point where we cannot easily mitigate them with more data and larger models.
                I believe this is the main reason why we almost never use larger discount factors (\(\gamma &gt; 0.999\)) in practice,
                and why it is challenging to scale up Q-learning.
                Note that policy gradient methods suffer much less from this issue.
                This is because <a href="https://arxiv.org/abs/1506.02438">GAE</a> or similar on-policy value estimation techniques
                can deal with longer horizons relatively more easily (though at the expense of higher variance), without strict 1-step recursions.
            </p>
            <h2>Empirical scaling study</h2>
            <p>
                In <a href="https://arxiv.org/abs/2506.04168">our recent paper</a>, we empirically verified the above claim via diverse, controlled scaling studies.
            </p>
            <p>
                We wanted to see whether current off-policy RL methods can solve highly challenging tasks by just scaling up data and compute.
                To do this, we first prepared highly complex, previously unsolved tasks in <a href="https://seohong.me/projects/ogbench/">OGBench</a>.
                Here are some videos:
            </p>
        </div>
        <div>
                
                <p><span>
                    <span>humanoidmaze</span><br>
                </span>
                <span>
                    <span>humanoidmaze</span><br>
                </span>
            </p></div>
        <div>
            <p>
                These tasks are really difficult.
                To solve them, the agent must learn complex goal-reaching behaviors from unstructured, random (play-style) demonstrations.
                At test time, the agent must perform precise manipulation, combinatorial puzzle-solving, or long-horizon navigation,
                over 1,000 environment steps.
            </p>
            <p>
                We then collected <i>near-infinite</i> data on these environments, to the degree that overfitting is virtually impossible.
                We also removed as many confounding factors as possible.
                For example, we focused on offline RL to abstract away exploration.
                We ensured that the datasets had sufficient coverage, and that all the tasks were solvable from the given datasets.
                We directly provided the agent with the ground-truth state observations to reduce the burden of representation learning.
            </p>
            <p>
                Hence, a "scalable" RL algorithm must really be able to solve these tasks, given sufficient data and compute.
                If Q-learning does not scale <b>even in this controlled setting with near-infinite data</b>,
                there is little hope that it will scale in more realistic settings,
                where we have limited data, noisy observations, and so on.
            </p>
        </div>
        <div>
                <p>
                    <img src="https://seohong.me/blog/q-learning-is-not-yet-scalable/assets/teaser1.png">
                </p>
                <p><span>
                    Standard offline RL methods struggle to scale on complex tasks, even with \(1000\times\) more data.
                </span>
            </p></div>
        <div>
            <p>
                So, how did the existing algorithms work?
                The results were a bit disappointing.
                None of the standard, widely used offline RL algorithms (flow BC, IQL, CRL, and SAC+BC) were able to solve all of these tasks,
                even with 1B-sized datasets, which are \(1000 \times\) larger than typical datasets used in offline RL.
                More importantly, their performance often plateaued far below the optimal performance.
                In other words, they didn't scale well on these complex, long-horizon tasks.
            </p>
            <p>
                You might ask:
                Are you really sure these tasks are solvable? Did you try larger models?
                Did you train them for longer? Did you try different hyperparameters? And so on.
                In the paper, we tried our best to address as many questions as possible with a number of ablations and controlled experiments,
                showing that <b>none</b> of these fixes worked...
                <b>except for one:</b>
            </p>
            <h2>Horizon reduction makes RL scalable</h2>
            <p>
                Recall that my claim earlier was that the <b>horizon</b> (and bias accumulation thereof) is the main obstacle to scaling up off-policy RL.
                To verify this,
                we tried diverse <i>horizon reduction</i> techniques (<i>e.g.</i>, n-step returns, hierarchical RL, etc.) that reduce the number of biased TD backups.
            </p>
        </div>
        <div>
                <p>
                    <img src="https://seohong.me/blog/q-learning-is-not-yet-scalable/assets/teaser2.png">
                </p>
                <p><span>
                    Horizon reduction was the only technique we found that substantially improved scaling.
                </span>
            </p></div>
        <div>
            <p>
                The results were promising!
                Even simple tricks like n-step returns
                significantly improved scalability and even <i>asymptotic performance</i>
                (so it is <i>not</i> just a "trick" that merely makes training faster!).
                Full-fledged hierarchical methods worked even better.
                More importantly, horizon reduction is the <b>only</b> technique that worked across the board in our experiments.
                This suggests that simply scaling up data and compute is <i>not</i> enough to address the curse of horizon.
                In other words, we need <i>better algorithms</i> that directly address this fundamental horizon problem.
            </p>
            <h2>Call for research: find a <i>scalable</i> off-policy RL objective</h2>
            <p>
                We saw that horizon reduction unlocks the scalability of Q-learning.
                So are we done? Can we now just scale up Q-learning?
                I'd say this is only the beginning.
                While it is great to know the cause and have some solutions,
                most of the current horizon reduction techniques (n-step returns, hierarchical RL, etc.) only <i>mitigate</i> the issue by a constant factor,
                and do not fundamentally solve the problem.
                I think <b>we're currently missing an off-policy RL algorithm that scales to arbitrarily complex, long-horizon problems</b>
                (or perhaps we may already have a solution, but just haven't stress-tested it enough yet!).
                I believe finding such a scalable off-policy RL algorithm is <b>the most important missing piece in machine learning today</b>.
                This will enable solving <i>much</i> more diverse real-world problems,
                including robotics, language models, agents, and basically any data-driven decision-making tasks.
            </p>
            <p>
                I'll conclude this post with my thoughts about potential solutions to scalable off-policy RL.
            </p>
            <ul>
                <li>
                    Can we find a simple, scalable way to extend beyond two-level hierarchies to deal with horizons of arbitrary lengths?
                    Such a solution should be able to naturally form a recursive hierarchical structure,
                    while being <i>simple enough</i> to be scalable.
                    One great example of this (though in a different field) is chain-of-thought in LLMs.
                </li>
                <li>
                    Another completely different approach (which I intentionally didn't mention so far for simplicity)
                    is <i>model-based RL</i>.
                    We know that model learning is scalable, because it's just supervised learning.
                    We also know that on-policy RL is scalable.
                    So why don't we combine the two, where we first learn a model and run on-policy RL within the model?
                    Would model-based RL indeed <i>scale</i> better than TD-based Q-learning?
                </li>
                <li>
                    Or is there a way to just completely avoid TD learning?
                    Among the methods that I know of,
                    one such example is <a href="https://www.tongzhouwang.info/quasimetric_rl/">quasimetric RL</a>,
                    which is essentially based on the LP formulation of RL.
                    Perhaps this sort of "exotic" RL methods, or MC-based methods like <a href="https://ben-eysenbach.github.io/contrastive_rl/">contrastive RL</a>, might eventually scale better than TD-based approaches?
                </li>
            </ul>
            <p>
                Our setup above can be a great starting point for testing these ideas.
                We have already designed a set of highly challenging robotic tasks, made the datasets, and verified that they are solvable.
                One can even make the tasks arbitrarily difficult (<i>e.g.</i>, by adding more cubes)
                and further stress-test the scalability of algorithms in a controlled way.
                We also put our efforts into making the code as clean as possible.
                Check out <a href="https://github.com/seohongpark/horizon-reduction">our code</a>!
            </p>
            <p>
                Feel free to let me know via email/Twitter/X or reach out to me at conferences if you have any questions, comments, or feedback.
                I hope that at some point, I can write another post about off-policy RL with a more positive title in the near future!
            </p>
            <h3>Acknowledgments</h3>
            <p>
                I would like to thank
                <a href="https://kvfrans.com/">Kevin Frans</a>,
                <a href="https://hongsukchoi.github.io/">Hongsuk Choi</a>,
                <a href="https://ben-eysenbach.github.io/">Ben Eysenbach</a>,
                <a href="https://aviralkumar2907.github.io/">Aviral Kumar</a>,
                and <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
                for their helpful feedback on this post.
                This post is partly based on our recent work, <a href="https://arxiv.org/abs/2506.04168">Horizon Reduction Makes RL Scalable</a>.
                The views in this post are my own, and do not necessarily reflect those of my coauthors.
            </p>
        </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Infinite Grid of Resistors (195 pts)]]></title>
            <link>https://www.mathpages.com/home/kmath668/kmath668.htm</link>
            <guid>44279181</guid>
            <pubDate>Sat, 14 Jun 2025 22:12:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mathpages.com/home/kmath668/kmath668.htm">https://www.mathpages.com/home/kmath668/kmath668.htm</a>, See on <a href="https://news.ycombinator.com/item?id=44279181">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
 <tbody><tr>
  <td>
  <p><b><span>Infinite Grid of
  Resistors</span></b></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><i><span>�����������������
  Remain, remain thou here,</span></i></p>
  </td>
 </tr>
 <tr>
  <td>
  <p><i><span>While
  sense can keep it on. And, sweetest, fairest,</span></i></p>
  </td>
 </tr>
 <tr>
  <td>
  <p><i><span>As
  I my poor self did exchange for you,</span></i></p>
  </td>
 </tr>
 <tr>
  <td>
  <p><i><span>To
  your so infinite loss, so in our trifles</span></i></p>
  </td>
 </tr>
 <tr>
  <td>
  <p><i><span>I
  still win of you: for my sake wear this...</span></i></p>
  </td>
 </tr>
 <tr>
  <td>
  <p><i><span>������������������������� ������������������������������� Shakespeare</span></i></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>There is a well-known puzzle based on the premise of an
  �infinite� grid of resistors connecting adjacent nodes of a square lattice. A
  small portion of such a grid is illustrated below.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><img width="232" height="229" src="https://www.mathpages.com/home/kmath668/kmath668_files/image001.png"></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Between every pair of adjacent nodes is a resistance R,
  and we�re told that this grid of resistors extends �to infinity� in all
  direction, and we�re asked to determine the effective resistance between two
  adjacent nodes, or, more generally, between any two specified nodes of the
  lattice.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>For adjacent nodes, the usual solution of this puzzle is
  to consider the current flow field as the sum of two components, one being
  the flow field of a grid with current injected into a single node, and the
  other being the flow field of a grid with current extracted from a single
  (adjacent) node. The symmetry of the two individual cases then enables us to
  infer the flow rates through the immediately adjacent resistors, and hence we
  can conclude (as explained in more detail below) that the effective
  resistance between two adjacent nodes is R/2. This solution has a certain
  intuitive plausibility, since it�s similar to how the potential field of an
  electric dipole can be expressed as the sum of the fields of a positive and a
  negative charge, each of which is spherically symmetrical about its
  respective charge. Just as the electric potential satisfies the Laplace equation, the voltages of the grid nodes satisfy the discrete from of the Laplace equation, which is to say, the voltage at each node is the average of the voltages
  of the four surrounding nodes. It�s also easy to see that solutions are
  additive, in the sense that the sum of any two solutions for given boundary
  conditions is a solution for the sum of the boundary conditions.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>If we accept the premise of an infinite grid of resistors,
  along with some tacit assumption about the behavior of the voltages and
  currents �at infinity�, and if we accept the idea that we can treat the
  current fields for the positive and negative nodes separately, and that
  applying a voltage to a single node of the infinite grid will result in some
  current flow into the grid, the puzzle is easily solved by simple symmetry
  considerations. We assert (somewhat naively) that if we inject (say) four
  Amperes of current into a given node, with no removal of current at any
  finite point of the grid, the current will flow equally out through the four
  resistors, so one Ampere will flow toward each of the four adjacent nodes.
  This one Ampere must flow out through the three other lines emanating from
  that adjacent node, as indicated in the left hand figure below.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><img width="375" height="155" src="https://www.mathpages.com/home/kmath668/kmath668_files/image002.png"></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>The figure on the right shows the four nodes surrounding
  the �negative� node, assuming we are extracting four Amperes from that node
  (with no current injected at any finite node of the grid). Again, simple
  symmetry dictates the distribution of currents indicated in the figure.
  Adding the two current fields together, we see that the link between the
  positive and negative nodes carries a total of 2 Amperes away from the
  positive node, and the other three links emanating from the positive node
  carry away a combined total of 1 + 1 + 1 <span>-</span>
  (2α <span>+</span> β<span>)</span> = 2 Amperes. Thus the direct link carries
  the same current as all the other paths, so the resistance of the direct link
  equals the effective resistance of the entire grid excluding that link. The
  direct link is in parallel with the remainder of the grid, so the combined
  resistance is simply R/2.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>This is an appealing argument, and it certainly gives the
  �right� answer (as can be verified by other methods), but the premises are
  somewhat questionable, and the reasoning involves some subtle issues that
  need to be addressed before it can qualify as a rigorous proof. The
  fundamental problem with the simple argument, as stated above, is that it
  relies on the notion of forcing current into a node of an infinite grid,
  without satisfactorily explaining where this current goes. One �hand-waving�
  explanation is that we may regard the grid as being grounded �at infinity�,
  but this isn�t strictly valid, because the resistance from any given node to
  �infinity� is infinite. This is easily seen from the fact that a given node
  is surrounded by a sequence of concentric squares, and the number of
  resistive links beginning from the central node and expanding outward to
  successive concentric squares are 4, 12, 20, 28, �, etc., which implies that
  the total resistance to infinity is (at least roughly)</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="381" height="45" src="https://www.mathpages.com/home/kmath668/kmath668_files/image003.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>The odd harmonic series diverges, so this resistance is
  infinite. Therefore, in order for current to enter the grid at a node and
  exit at infinity, we would require infinite potential (i.e., voltage) between
  the node and the grid points �at infinity�. To make the argument rigorous, we
  could consider a large but finite grid, and convince ourselves that the
  behavior approaches the expected result in the limit as the grid size
  increases. This is not entirely trivial, because we must be sure the two
  sequences of expanding grids, one concentric about the positive node and one
  concentric about the negative node, approach mutually compatible boundary
  conditions in the limit, giving zero net flow �to infinity�. To evaluate this
  limit, the voltage at the central node (relative to the voltage at infinity)
  must approach infinity to provide a fixed amount of current. This is
  discussed further in another <span><a href="https://www.mathpages.com/home/kmath669/kmath669.htm">note</a></span>, where we also discuss the
  arbitrariness of the solution for a truly �infinite� grid. Strictly speaking,
  for a truly infinite grid, the solution is indeterminate unless some
  asymptotic boundary conditions are imposed (which are not specified in the
  usual statements of the problem). </p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>The unphysical aspect of the problem can also be seen in
  the fact that the flow field is assumed to be fully developed, to infinity, a
  situation that could not have been established by any realistic physical
  process in any finite amount of time. Of course, the postulated grid consists
  purely of ideal resistances, with no capacitances or inductances, so there
  are no dynamics to consider, and hence one could argue that the entire
  current field is established instantaneously to infinity - but this merely
  illustrates that the postulated grid is idealized to the point of violating
  the laws of physics. All real circuits have capacitance and inductance, which
  is why the propagation speed cannot be infinite. One might think that such
  idealizations are harmless for this problem, but they actually render the
  problem totally indeterminate if we apply them rigorously. Our intuitive
  sense that there is a unique answer comes precisely from our unconscious
  imposition of �physically reasonable� asymptotic behavior emanating from a
  localized source, based on the asymptotic behavior of a finite grid as the
  size increases � a conception that arises from our physical notions of
  locality and finite propagation of effects, notions which are not justified
  in the idealized setting.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Setting aside these issues, and just naively adopting the
  usual tacit assumptions about the asymptotic conditions of the grid, we can
  consider the more general problem of determining the resistance between any
  two nodes. The most common method is based on superimposing solutions of the
  basic difference equation. Again this method tacitly imposes plausible
  boundary conditions to force a unique answer, essentially by requiring that
  the grid behaves like the limit of a large finite grid. Consider first the
  trivial example of a one-dimensional �grid� of unit resistors, in which the
  net current emanating from the nth node is given by</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="152" height="23" src="https://www.mathpages.com/home/kmath668/kmath668_files/image004.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>where we stipulate that I<sub>0</sub> = <span>-</span>1 Amp and I<sub>n</sub> = 0 for all n
  ≠ 0. Notice that for all n ≠ 0 would could negate the signs of
  the indices on the right hand side without affecting the equation, because
  the net current is zero at those nodes, and the equations above and below the
  origin are symmetrical. However, the case n = 0 is different if we stipulate
  that V<sub>1</sub> = V<sub><span>-</span>1</sub>,
  and if we stipulate that I<sub>0</sub> = �1, which implies</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="173" height="23" src="https://www.mathpages.com/home/kmath668/kmath668_files/image005.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>It follows that, for unit resistors, if we stipulate V<sub>0</sub>
  = 0, we must have V<sub>1</sub> = 1/2. Therefore, if we imagine current being
  extracted from just the node at the origin, while all the other nodes have
  zero net current flow, then for all n ≠ 0 we have the relation</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="149" height="23" src="https://www.mathpages.com/home/kmath668/kmath668_files/image006.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>The characteristic polynomial is</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="95" height="44" src="https://www.mathpages.com/home/kmath668/kmath668_files/image007.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>For any value of μ that satisfies this equation, it�s
  clear that one solution of the preceding difference equation is V<sub>n</sub>
  = Aμ<sup>n</sup> for any constant A. However, the characteristic
  equation has the repeated roots μ<sub>1</sub> = μ<sub>2</sub> = 1,
  so we have a resonance term, and the general form of the solution of the
  discrete difference equation is</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="267" height="31" src="https://www.mathpages.com/home/kmath668/kmath668_files/image008.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>for some constants A and B, chosen to make the solution
  consistent with the specified boundary conditions. We want V<sub>0</sub> = 0,
  so we must put A = 0. Also, since the recurrence relation doesn�t apply at n
  = 0, we can chose B equal to +1/2 for positive n, and �1/2 for negative n,
  which amounts to taking the absolute value of n, giving the result V<sub>n</sub>
  = |n|/2. This implies that the effective resistance between nodes separated
  by k resistors is (as expected) simply kR, where R is the resistance of an
  individual resistor.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>We can similarly consider the difference equation for
  grids of higher dimensions. For a two-dimensional grid, the current emanating
  from node (m,n) for all m,n &gt; 0 is</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="516" height="27" src="https://www.mathpages.com/home/kmath668/kmath668_files/image009.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>We stipulate that I<sub>m,n</sub> = 0 for all m,n &gt; 0,
  so the characteristic equation for this two-dimensional difference equation
  is</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="440" height="44" src="https://www.mathpages.com/home/kmath668/kmath668_files/image010.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>This shows that there are infinitely many �eigenvalues�.
  Indeed, for any value of μ we can solve for the corresponding value
  ν, and vice versa. For any such pair of values μ,ν satisfying
  this equation it�s clear that a solution of the preceding difference equation
  is given by V<sub>m,n</sub> = A(μ,ν) μ<sup>m</sup>ν<sup>n</sup>
  for any constant A(μ,ν). If we define parameters α,β such
  that iα = ln(μ) and iβ = ln(ν), then these solutions can
  be written as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="475" height="27" src="https://www.mathpages.com/home/kmath668/kmath668_files/image011.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>In terms of α and β the characteristic equation
  can be written as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="460" height="27" src="https://www.mathpages.com/home/kmath668/kmath668_files/image012.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Now, any combination of these solutions will satisfy the
  original difference equation for nodes with zero net current, but we want I<sub>0,0</sub>
  = <span>-</span>1 Amp, and to achieve this we
  (again) take the absolute values of the indices, i.e., we define</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="524" height="28" src="https://www.mathpages.com/home/kmath668/kmath668_files/image013.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>For values of m and n different from zero, taking the absolute
  values has no effect on the difference equation, i.e., it still gives zero
  net current. However, for m = n = 0 we get</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="424" height="24" src="https://www.mathpages.com/home/kmath668/kmath668_files/image014.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>This shows that if we put V<sub>0,0</sub> = 0 then for �1
  Amp of current we must have V<sub>1,0</sub> = 1/4 volts, which is consistent
  with the fact that the resistance between two adjacent nodes of 1/2 ohms,
  because we superimpose this solution with an equal and opposite solution
  centered on the adjacent node with +1 Amp current.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>We don�t yet have a definite solution satisfying all the
  conditions, because with the absolute values of the indices we generally get
  non-zero currents for I<sub>m,0</sub> and I<sub>0,n</sub>. To solve this
  problem, we will simply superimpose several of these solutions, and impose
  the requirement that the net currents of the form I<sub>m,0</sub> and I<sub>0,n</sub>
  all vanish. To do this, we integrate the solutions of the form (4) over
  α ranging from �π to π. Thus we have</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="517" height="53" src="https://www.mathpages.com/home/kmath668/kmath668_files/image015.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>with the understanding that β is given as a function
  of α by equation (3). We will find that this determines the function
  A(α,β). Consider first the requirement I<sub>m,0</sub> = 0.
  Inserting the expression for V<sub>m,0</sub> from equation (4) into equation
  (1), we have, for all m &gt; 0, </p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="305" height="72" src="https://www.mathpages.com/home/kmath668/kmath668_files/image016.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Making use of the characteristic equation (3), this can be
  written as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="281" height="72" src="https://www.mathpages.com/home/kmath668/kmath668_files/image017.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Now we seek to superimpose many of these solutions such
  that the net current for these expressions is zero. To do this, we will
  integrate this expression over α ranging from �π to π. Thus we
  have</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="531" height="53" src="https://www.mathpages.com/home/kmath668/kmath668_files/image018.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>At this point we recall that the exponential Fourier
  series for an arbitrary function f(x) is</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="397" height="53" src="https://www.mathpages.com/home/kmath668/kmath668_files/image019.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Therefore we have</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="308" height="48" src="https://www.mathpages.com/home/kmath668/kmath668_files/image020.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>where we�ve made use of the fact that I<sub>0,0</sub> = <span>-</span>1 and I<sub>m,0</sub> = 0 for all m
  ≠ 0. From this we infer that</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="168" height="47" src="https://www.mathpages.com/home/kmath668/kmath668_files/image021.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Inserting this into equation (5), we get</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="540" height="53" src="https://www.mathpages.com/home/kmath668/kmath668_files/image022.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Again, it�s understood that β is given as a function
  of α by equation (3). It might seem as if we cannot now force the
  currents I<sub>0,n</sub> to equal zero. If we had imposed that requirement
  first, instead of I<sub>m,0</sub> = 0, by symmetry we would have found</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="540" height="53" src="https://www.mathpages.com/home/kmath668/kmath668_files/image023.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>which seems superficially different from (7). However,
  notice that the limits of integration are reversed, because α and β
  progress in opposite directions. Furthermore, if we take the differential of
  the characteristic equation (3) we get</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="132" height="47" src="https://www.mathpages.com/home/kmath668/kmath668_files/image024.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>which proves that (7) and (8) are equivalent. As discussed
  previously, the resistance between the origin and the node (m,n) is twice the
  value of V<sub>m,n</sub>� <span>-</span> V<sub>0,0</sub>,
  so we have the formula</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="273" height="56" src="https://www.mathpages.com/home/kmath668/kmath668_files/image025.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>This can be split into two integrals, one ranging from
  �π to 0, and the other ranging from 0 to +π, as follows:</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="485" height="61" src="https://www.mathpages.com/home/kmath668/kmath668_files/image026.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Reversing the sign of α in the first integral, and
  noting that cos(�α) = cos(α),� this can be written as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="441" height="61" src="https://www.mathpages.com/home/kmath668/kmath668_files/image027.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>and hence we have</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="516" height="61" src="https://www.mathpages.com/home/kmath668/kmath668_files/image028.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>For example, the resistance between two diagonal corners
  of a lattice square is given by</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="267" height="59" src="https://www.mathpages.com/home/kmath668/kmath668_files/image029.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>If we define the variable s = cos(α) we have α =
  acos(s) and </p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="87" height="44" src="https://www.mathpages.com/home/kmath668/kmath668_files/image030.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>As α ranges from 0 to π, the parameter s ranges
  from 1 to �1, so the preceding integral can be written as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="272" height="53" src="https://www.mathpages.com/home/kmath668/kmath668_files/image031.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>We now make use of the identity</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="213" height="29" src="https://www.mathpages.com/home/kmath668/kmath668_files/image032.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>to re-write the integral as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="239" height="52" src="https://www.mathpages.com/home/kmath668/kmath668_files/image033.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Also, from the identity</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="301" height="43" src="https://www.mathpages.com/home/kmath668/kmath668_files/image034.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>we have</p>
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="380" height="48" src="https://www.mathpages.com/home/kmath668/kmath668_files/image035.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>so the integral can be written as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="387" height="203" src="https://www.mathpages.com/home/kmath668/kmath668_files/image036.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>(This same result can be derived purely algebraically,
  without the use of Fourier series, as described in another <span><a href="https://www.mathpages.com/home/kmath673/kmath673.htm">note</a></span>.)
  Knowing this resistance value for diagonal nodes, and the resistance value
  1/2 for adjacent nodes, we can immediately compute the resistances to several
  other nodes by simple application of the basic difference equation. Thus we
  have the resistances relative to the origin shown below.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="372" height="307" src="https://www.mathpages.com/home/kmath668/kmath668_files/image037.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Returning to equation (9), we see that the same
  substitutions and identities that we used to simplify R<sub>1,1</sub> enable
  us to write the general expression as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="324" height="61" src="https://www.mathpages.com/home/kmath668/kmath668_files/image038.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>where h<sub>m</sub>(s) denote the trigonometric
  polynomials giving cos(mα) as a function of s = cos(α). The first
  several of these polynomials are</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="155" height="187" src="https://www.mathpages.com/home/kmath668/kmath668_files/image039.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>The coefficients of these polynomials are given by a
  simple recurrence relation, and they also have a simple trigonometric
  expression. However, we don�t actually need to deal with these polynomials,
  because it is sufficient to determine the values of R<sub>0,n</sub> by the
  above integral, and then all the remaining resistances are easily determined
  by simple algebra. Hence we can focus on just the integrals</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="279" height="61" src="https://www.mathpages.com/home/kmath668/kmath668_files/image040.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>To simplify this still further, we can define the
  parameter</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="169" height="28" src="https://www.mathpages.com/home/kmath668/kmath668_files/image041.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>in terms of which s and ds are given by</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="276" height="41" src="https://www.mathpages.com/home/kmath668/kmath668_files/image042.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Re-writing the expression for R<sub>0,n</sub> in terms of
  the parameter σ, we get</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="259" height="55" src="https://www.mathpages.com/home/kmath668/kmath668_files/image043.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Making use of the indefinite integrals</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="385" height="192" src="https://www.mathpages.com/home/kmath668/kmath668_files/image044.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>where</p>
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="113" height="24" src="https://www.mathpages.com/home/kmath668/kmath668_files/image045.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>we can determine the values</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="468" height="41" src="https://www.mathpages.com/home/kmath668/kmath668_files/image046.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>and so on.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Another way of simplifying the integrals involved in the
  infinite grid solution is to return to equation (7), focusing on the case of
  positive m and n with m &gt; n, and recalling that the resistance from the
  origin to the node (m,n) is <i>twice</i> the voltage given by (7), so we have</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="501" height="53" src="https://www.mathpages.com/home/kmath668/kmath668_files/image047.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>where, for convenience, we�ve shifted the limits of
  integration from the range [�π,+π] to the range [0,2π]. Now
  suppose we define new variables r,s such that α = r + s and β = r �
  s. Substituting for α and β in equation (3) gives the relation
  cos(r)cos(s) = 1. In terms of these parameters equation (10) can be written
  as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="540" height="56" src="https://www.mathpages.com/home/kmath668/kmath668_files/image048.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Obviously we have dα = dr + ds, and differentiating
  the relation cos(r)cos(s) = 1 gives</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="147" height="49" src="https://www.mathpages.com/home/kmath668/kmath668_files/image049.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>so we have</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="509" height="53" src="https://www.mathpages.com/home/kmath668/kmath668_files/image050.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Making this substitution into (11) gives</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="544" height="56" src="https://www.mathpages.com/home/kmath668/kmath668_files/image051.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Now, since cos(r)cos(s)=1 where cos(z)=(e<sup>iz</sup>+e<sup>−iz</sup>)/2,
  and in view of the identity</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="261" height="67" src="https://www.mathpages.com/home/kmath668/kmath668_files/image052.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>we see that r and s can be expressed parametrically in
  terms of a single parameter t such that</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="201" height="41" src="https://www.mathpages.com/home/kmath668/kmath668_files/image053.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>From this we get</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="271" height="41" src="https://www.mathpages.com/home/kmath668/kmath668_files/image054.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>With these substitutions, equation (12) becomes</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="529" height="59" src="https://www.mathpages.com/home/kmath668/kmath668_files/image055.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>To this point we have continued to specify the limits of
  integration in terms of α, but now we note as α ranges over the
  real values from 0 to 2π we have t = (1�i)τ where τ is a
  real-valued parameter ranging from infinity to 0. Hence we make this change
  of variable to express the result as</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="551" height="56" src="https://www.mathpages.com/home/kmath668/kmath668_files/image056.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>where we�ve made use of the fact that the variable τ
  can be multiplied by an arbitrary factor inside the curved parentheses
  without affecting the integral. For the first diagonal node we have m = n = 1
  and the integral is simply</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="417" height="56" src="https://www.mathpages.com/home/kmath668/kmath668_files/image057.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>For the resistances to the other nodes along the diagonal
  of the lattice, notice that for any m we have</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="453" height="243" src="https://www.mathpages.com/home/kmath668/kmath668_files/image058.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>Consequently we have the well-known result</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><img width="247" height="45" src="https://www.mathpages.com/home/kmath668/kmath668_files/image059.png"></span></p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p>from which all the other resistances are easily computed
  using the basic recurrence relation (1). In another <span><a href="https://www.mathpages.com/home/kmath669/kmath669.htm">note</a></span> we consider the same problem
  from a more algebraic standpoint.</p>
  </td>
 </tr>
 <tr>
  <td>
  
  </td>
 </tr>
 <tr>
  <td>
  <p><span><a href="https://www.mathpages.com/home/index.htm">Return to
  MathPages Main Menu</a></span></p>
  </td>
 </tr>
</tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD's AI Future Is Rack Scale 'Helios' (110 pts)]]></title>
            <link>https://morethanmoore.substack.com/p/amds-ai-future-is-rack-scale-helios</link>
            <guid>44278746</guid>
            <pubDate>Sat, 14 Jun 2025 20:51:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://morethanmoore.substack.com/p/amds-ai-future-is-rack-scale-helios">https://morethanmoore.substack.com/p/amds-ai-future-is-rack-scale-helios</a>, See on <a href="https://news.ycombinator.com/item?id=44278746">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Only have a minute? Here are our key takeaways.</p><p><strong>🚀 New MI355X GPU</strong><span>: 2x AI FLOPs, more HBM, 40% better tokens/$ than NVIDIA.</span><br><strong>🧠 Software Wins: </strong><span>ROCm 7 with big performance boosts and day-0 support.</span><br><strong>🖧 Rack-Scale Wins</strong><span>: New turnkey solutions using AMD CPU + GPU + Network.</span><br><strong>📈 Roadmap Wins: </strong><span>Next-Gen in 2026 with 4x performance, HBM4 and scale.</span><br><strong>🌱 Efficiency Wins</strong><span>: Roadmap to 20× rack-scale energy efficiency by 2030.</span></p><div data-attrs="{&quot;url&quot;:&quot;https://morethanmoore.substack.com/p/amds-ai-future-is-rack-scale-helios?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="CaptionedButtonToDOM"><p>Thanks for reading More Than Moore! This post is public so feel free to share it.</p><p data-attrs="{&quot;url&quot;:&quot;https://morethanmoore.substack.com/p/amds-ai-future-is-rack-scale-helios?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="ButtonCreateButton"><a href="https://morethanmoore.substack.com/p/amds-ai-future-is-rack-scale-helios?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div><p>With the demand for hardware for artificial intelligence continuing to go through the roof, the various chip designers and other hardware providers within the industry are still in full-on growth mode. The name of the game is not just developing new products to woo deep-pocketed customers looking to build large-scale training and inference clusters, but to keep up with the rapid growth of the market and not fall behind in market share simply by not growing as fast as the overall market. The gold rush is still in full effect, and everyone wants their piece.</p><p>As the second-largest of the industry’s major GPU providers, AMD has been able to capture a slice of the AI market relatively quickly in the last couple of years, building on the back of its Instinct MI300X series of GPU-based accelerators. While far from AMD’s first foray into server GPUs, as MI300X uses the third iteration of their CDNA server GPU architecture, but MI300X was AMD’s first architecture to truly go all-in on the kind of features required for large-scale AI systems, all of which has translated into significant and high-margin sales for the company. </p><div data-component-name="DigestPostEmbed"><a href="https://morethanmoore.substack.com/p/amd-2025-q1-financials" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaea129d-cb78-41f4-8cfd-21a75bbc247f_1423x1033.png"><img src="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaea129d-cb78-41f4-8cfd-21a75bbc247f_1423x1033.png" sizes="100vw" alt="AMD 2025 Q1 Financials" width="140" height="140"></picture></div></a></div><p>And now with that momentum behind them, AMD is aiming to continue to ramp up their presence in the AI market with their next generation of AI server hardware.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c77617a-fcc9-4993-ac01-95486788e030_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c77617a-fcc9-4993-ac01-95486788e030_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c77617a-fcc9-4993-ac01-95486788e030_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c77617a-fcc9-4993-ac01-95486788e030_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c77617a-fcc9-4993-ac01-95486788e030_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c77617a-fcc9-4993-ac01-95486788e030_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5c77617a-fcc9-4993-ac01-95486788e030_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:487500,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c77617a-fcc9-4993-ac01-95486788e030_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c77617a-fcc9-4993-ac01-95486788e030_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c77617a-fcc9-4993-ac01-95486788e030_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c77617a-fcc9-4993-ac01-95486788e030_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5c77617a-fcc9-4993-ac01-95486788e030_4000x2250.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>During the company’s Advancing AI 2025 presentation earlier this morning, AMD launched the next generation of what has become a full-on AI ecosystem for the company. Built around the company’s fourth-generation CDNA architecture, CDNA4, and the new Instinct MI350 series of accelerators based on it, AMD is looking to dive even deeper into the AI market with a faster accelerator even more optimized for AI workloads and the unique math behind them. And running on that new hardware is the latest iteration of AMD’s ROCm platform, now up to version 7, which sees AMD touting numerous performance improvements coupled with a fresh focus on day-0 software support.</p><p>Meanwhile, with the wider industry’s focus shifting from buying individual server systems to buying whole racks of systems, AMD is also assembling its own ecosystem for rack-scale AI compute, which it has brought to the forefront today, enabled through its acquisitions of companies like ZT Systems. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60060b96-270b-406c-ac45-fd9837d4c66f_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60060b96-270b-406c-ac45-fd9837d4c66f_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60060b96-270b-406c-ac45-fd9837d4c66f_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60060b96-270b-406c-ac45-fd9837d4c66f_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60060b96-270b-406c-ac45-fd9837d4c66f_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60060b96-270b-406c-ac45-fd9837d4c66f_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/60060b96-270b-406c-ac45-fd9837d4c66f_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:489687,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60060b96-270b-406c-ac45-fd9837d4c66f_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60060b96-270b-406c-ac45-fd9837d4c66f_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60060b96-270b-406c-ac45-fd9837d4c66f_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60060b96-270b-406c-ac45-fd9837d4c66f_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F60060b96-270b-406c-ac45-fd9837d4c66f_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>By combining Instinct MI350 accelerators with AMD’s recently-launched Pollara 400 AI NIC and Turin EPYC CPUs, AMD is now offering customers all the major components they need to build whole racks based on AMD hardware. And like AMD’s plans for CPUs and GPUs, AMD is planning for multiple generations of rack-scale hardware, with this year’s equipment being the tip of the iceberg to a much bigger push in 2026 with the Helios AI rack, which will be packed full of next-generation AMD technologies.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59307578-13eb-4cca-be9e-068211ac3156_4608x2592.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59307578-13eb-4cca-be9e-068211ac3156_4608x2592.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59307578-13eb-4cca-be9e-068211ac3156_4608x2592.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59307578-13eb-4cca-be9e-068211ac3156_4608x2592.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59307578-13eb-4cca-be9e-068211ac3156_4608x2592.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59307578-13eb-4cca-be9e-068211ac3156_4608x2592.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/59307578-13eb-4cca-be9e-068211ac3156_4608x2592.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3606975,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59307578-13eb-4cca-be9e-068211ac3156_4608x2592.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59307578-13eb-4cca-be9e-068211ac3156_4608x2592.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59307578-13eb-4cca-be9e-068211ac3156_4608x2592.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59307578-13eb-4cca-be9e-068211ac3156_4608x2592.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59307578-13eb-4cca-be9e-068211ac3156_4608x2592.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>But before we get too far ahead of ourselves, let’s take a look at AMD’s major announcements today.</p><h5><strong>Doubling Down on AI, Damn Near Literally</strong></h5><p>First and foremost, we have the star of today’s announcements; AMD’s Instinct MI350 family of GPU-based accelerators. Based on AMD’s new CDNA4 architecture, these accelerators push even harder into the AI field, with AMD effectively doubling up on their matrix (tensor) operation performance per clock versus the MI300X accelerator. And with AMD supporting lower precision FP6 and FP4 formats, the peak throughput of these new accelerators is even higher. Under ideal accelerators, the MI350 series stands to be up to four times as fast as MI300X – a significant growth spurt that comes barely 18 months after AMD first launched the MI300X.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb33a220f-d61c-4645-a996-556ad28508d8_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb33a220f-d61c-4645-a996-556ad28508d8_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb33a220f-d61c-4645-a996-556ad28508d8_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb33a220f-d61c-4645-a996-556ad28508d8_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb33a220f-d61c-4645-a996-556ad28508d8_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb33a220f-d61c-4645-a996-556ad28508d8_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b33a220f-d61c-4645-a996-556ad28508d8_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:395138,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb33a220f-d61c-4645-a996-556ad28508d8_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb33a220f-d61c-4645-a996-556ad28508d8_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb33a220f-d61c-4645-a996-556ad28508d8_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb33a220f-d61c-4645-a996-556ad28508d8_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb33a220f-d61c-4645-a996-556ad28508d8_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Under the hood, the Instinct MI350 series is based on AMD’s CDNA4 architecture. A further evolution of the CDNA3 architecture that was used in the MI300 series, CDNA4 is an incremental design that keeps around most of CDNA3’s underpinnings, while making some important upgrades elsewhere with a specific focus on improving matrix performance for AI workloads.</p><p>The biggest change here is that AMD has doubled the throughput of the matrix engines that are responsible for providing matrix operation support. So clock-for-clock, for FP16 and below data types, a CDNA4 compute unit (CU) can process twice as many matrix operations as a CDNA3 compute unit. Meanwhile, there has been no equivalent scale-up of traditional vector performance, underscoring how MI350’s transistor budget is far more shifted to AI hardware than MI300 before it.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a73c888-8a87-4859-bfe1-5f9dfd703b11_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a73c888-8a87-4859-bfe1-5f9dfd703b11_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a73c888-8a87-4859-bfe1-5f9dfd703b11_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a73c888-8a87-4859-bfe1-5f9dfd703b11_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a73c888-8a87-4859-bfe1-5f9dfd703b11_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a73c888-8a87-4859-bfe1-5f9dfd703b11_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4a73c888-8a87-4859-bfe1-5f9dfd703b11_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:612319,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a73c888-8a87-4859-bfe1-5f9dfd703b11_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a73c888-8a87-4859-bfe1-5f9dfd703b11_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a73c888-8a87-4859-bfe1-5f9dfd703b11_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a73c888-8a87-4859-bfe1-5f9dfd703b11_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a73c888-8a87-4859-bfe1-5f9dfd703b11_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Meanwhile, CDNA4 also brings native support for FP6 and FP4 data types to AMD’s accelerators for the first time. One of the marquee features of rival NVIDIA’s Blackwell architecture, FP6 and FP4 have become a new target for AI inference, as developers look to wring every TOP/FLOP of performance from these expensive and power-hungry GPUs. And, aiming to one-up NVIDIA at their own game here, AMD has even beefed up FP6 performance on their architecture so that it processes at twice the rate of FP8, unlike NVIDIA’s architecture where it processes at the same rate as FP8. AMD in essence built a better FP4 unit to support FP6, rather than reusing an FP8 unit to support FP6. This carries a die area penalty, but the upshot is double the performance.</p><p>Feeding all of this, the MI350 series is being paired with 288GB of HBM3E memory, eight stacks in all. This is the same number of stacks as on the MI300, with AMD benefitting from both the greater memory capacity of HBM3E and its higher bandwidth as well. All told, the MI350 accelerators offer a cumulative 8 TB/sec of memory bandwidth, up from the 192GB of memory and 5.3 TB/sec of bandwidth available on the MI300X.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb98cd258-8260-4014-a8e6-ad1e59ef2ed0_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb98cd258-8260-4014-a8e6-ad1e59ef2ed0_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb98cd258-8260-4014-a8e6-ad1e59ef2ed0_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb98cd258-8260-4014-a8e6-ad1e59ef2ed0_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb98cd258-8260-4014-a8e6-ad1e59ef2ed0_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb98cd258-8260-4014-a8e6-ad1e59ef2ed0_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b98cd258-8260-4014-a8e6-ad1e59ef2ed0_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:527373,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb98cd258-8260-4014-a8e6-ad1e59ef2ed0_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb98cd258-8260-4014-a8e6-ad1e59ef2ed0_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb98cd258-8260-4014-a8e6-ad1e59ef2ed0_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb98cd258-8260-4014-a8e6-ad1e59ef2ed0_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb98cd258-8260-4014-a8e6-ad1e59ef2ed0_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>And while we’ll get into the nitty-gritty of the architecture in our separate CDNA4 architecture piece, the MI350 series also sees a meaningful change in how the chips are constructed. AMD is still using a 3D hybrid bonded die stacking technique, stacking the compute ‘XCD’ dies on top of I/O and memory IOD dies. The MI350 series continues to use 8 XCDs – this time with 32 CUs each – but these are now being stacked on top of just two large IODs, versus the four smaller IODs used on MI300X. In terms of process technologies, the CDNA4 XCDs are using the latest and greatest node from TSMC, N3P. This is a notable advantage over NVIDIA’s N4-built hardware. AMD tells us that there is a total of 185B transistors in a completed chip, up from 153B for MI300.</p><p>This time around, the Instinct MI350 series is being split up into a pair of accelerator SKUs, based on power consumption and clockspeeds. </p><p>The lead SKU is the MI355X, which is specifically intended for liquid cooled systems. This is the highest performing part, with a peak FP16 performance of 5 PFLOPS, and a clockspeed of around 2.4 GHz. </p><p>The air-cooled counterpart will be the MI350X, which is hardware-identical but with a lower peak clockspeed of around 2.2 GHz (and a lower power consumption to match). In terms of AI performance, this would give the MI350X around 4.6 PFLOPS of peak FP16 performance.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9b40dbb-c815-4e2f-9df6-238bf79072a3_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9b40dbb-c815-4e2f-9df6-238bf79072a3_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9b40dbb-c815-4e2f-9df6-238bf79072a3_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9b40dbb-c815-4e2f-9df6-238bf79072a3_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9b40dbb-c815-4e2f-9df6-238bf79072a3_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9b40dbb-c815-4e2f-9df6-238bf79072a3_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f9b40dbb-c815-4e2f-9df6-238bf79072a3_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:424120,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9b40dbb-c815-4e2f-9df6-238bf79072a3_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9b40dbb-c815-4e2f-9df6-238bf79072a3_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9b40dbb-c815-4e2f-9df6-238bf79072a3_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9b40dbb-c815-4e2f-9df6-238bf79072a3_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9b40dbb-c815-4e2f-9df6-238bf79072a3_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Otherwise, as hinted at by AMD’s decision to split SKUs based on power consumption, the power consumption of this generation of accelerators is going to be very high. Coming from the 750 Watt MI300X, the similarly air-cooled MI350X is designed for 1000 Watt systems, and the liquid-cooled MI355X will be able to chug 1400 Watts. AMD is pushing these parts hard to maximize performance, but according to the company, it’s what their customers want - extreme scale-up by pushing each individual processor as hard as possible. Compared to the cost of buying the hardware and maintaining these systems, the extra performance is worth the additional power consumption. </p><p>Also worth noting here is the uptick in ‘rack’ level power consumption. With up to 128 MI355X accelerators fitting into a single rack thanks to the density afforded by liquid cooling, this means a single rack can potentially draw upwards of 180 kW just from the GPUs alone.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ed322ea-ddbb-49cf-8ae5-62b37cfdca4e_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ed322ea-ddbb-49cf-8ae5-62b37cfdca4e_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ed322ea-ddbb-49cf-8ae5-62b37cfdca4e_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ed322ea-ddbb-49cf-8ae5-62b37cfdca4e_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ed322ea-ddbb-49cf-8ae5-62b37cfdca4e_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ed322ea-ddbb-49cf-8ae5-62b37cfdca4e_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1ed322ea-ddbb-49cf-8ae5-62b37cfdca4e_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:601490,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ed322ea-ddbb-49cf-8ae5-62b37cfdca4e_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ed322ea-ddbb-49cf-8ae5-62b37cfdca4e_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ed322ea-ddbb-49cf-8ae5-62b37cfdca4e_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ed322ea-ddbb-49cf-8ae5-62b37cfdca4e_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1ed322ea-ddbb-49cf-8ae5-62b37cfdca4e_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Ultimately, just about everything about the MI350 is rooted in economics in some form or another. As AMD is looking to grow their AI market share – and particularly at the expense of NVIDIA – the company is looking to not just meet or beat its rival on performance, but to undercut them on pricing as well. Specific pricing is not being published, but AMD reckons they can offer upwards of 40% more tokens per dollar – or around 30% lower cost per token – than NIVIDA’s GB200 platform. Though as with all performance and cost claims, this remains to be seen, and is going to be highly workload dependent.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd913ad08-339e-4fb8-90c2-27d96d8e62e7_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd913ad08-339e-4fb8-90c2-27d96d8e62e7_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd913ad08-339e-4fb8-90c2-27d96d8e62e7_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd913ad08-339e-4fb8-90c2-27d96d8e62e7_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd913ad08-339e-4fb8-90c2-27d96d8e62e7_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd913ad08-339e-4fb8-90c2-27d96d8e62e7_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d913ad08-339e-4fb8-90c2-27d96d8e62e7_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:286660,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd913ad08-339e-4fb8-90c2-27d96d8e62e7_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd913ad08-339e-4fb8-90c2-27d96d8e62e7_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd913ad08-339e-4fb8-90c2-27d96d8e62e7_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd913ad08-339e-4fb8-90c2-27d96d8e62e7_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd913ad08-339e-4fb8-90c2-27d96d8e62e7_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>According to AMD, Instinct MI350 accelerators are already shipping to their partners. The company expects those partners to start offering MI350 solutions – cloud and hardware – in Q3 of this year. The company has not outlined the ramp schedule for the product, however, so it’s unclear just how quickly AMD will be able to get all the systems their customers want into their waiting hands.</p><p>The software counterpart to AMD’s hardware ambitions is the ROCm software stack. At its inception nearly a decade ago, like its competitor, was focused mostly as a high performance computing stack for scientific research. But the recent pivot to AI means that AMD has been gearing towards wide support, and through that, building an open software ecosystem. That continues to be their direction to this day. With a relatively late start on assembling a software stack, ROCm has traditionally been the more fraught piece of the puzzle of AMD, but the software side of the business has been making some good progress as of late, and is reaching some important milestones in terms of software support and reaction times.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6da63557-114d-44f5-9e5a-a6676f913afe_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6da63557-114d-44f5-9e5a-a6676f913afe_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6da63557-114d-44f5-9e5a-a6676f913afe_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6da63557-114d-44f5-9e5a-a6676f913afe_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6da63557-114d-44f5-9e5a-a6676f913afe_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6da63557-114d-44f5-9e5a-a6676f913afe_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6da63557-114d-44f5-9e5a-a6676f913afe_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:299613,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6da63557-114d-44f5-9e5a-a6676f913afe_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6da63557-114d-44f5-9e5a-a6676f913afe_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6da63557-114d-44f5-9e5a-a6676f913afe_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6da63557-114d-44f5-9e5a-a6676f913afe_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6da63557-114d-44f5-9e5a-a6676f913afe_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The big announcement on the software front today is the upcoming release of ROCm 7, the latest iteration of the software stack, up from 6.4 earlier this year. The marquee addition is introducing support for the CDNA4 architecture and MI350 series accelerators. But AMD’s software developers have also been focusing on everything from improving performance and better supporting cluster-scale operations to laying down the framework for enterprise-grade management and AI lifecycle features. Most noticeably, for those following the key people at AMD on social media, has been out-of-the-box support for popular models and integration methods.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26243d18-3049-4e78-8be0-ace5128f5e62_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26243d18-3049-4e78-8be0-ace5128f5e62_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26243d18-3049-4e78-8be0-ace5128f5e62_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26243d18-3049-4e78-8be0-ace5128f5e62_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26243d18-3049-4e78-8be0-ace5128f5e62_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26243d18-3049-4e78-8be0-ace5128f5e62_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/26243d18-3049-4e78-8be0-ace5128f5e62_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:324865,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26243d18-3049-4e78-8be0-ace5128f5e62_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26243d18-3049-4e78-8be0-ace5128f5e62_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26243d18-3049-4e78-8be0-ace5128f5e62_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26243d18-3049-4e78-8be0-ace5128f5e62_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26243d18-3049-4e78-8be0-ace5128f5e62_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>At a high level, AMD is indicating that the ROCm 7 release marks the pivot point for the company, moving from focusing on catching up to NVIDIA to focusing to what comes next – being there to support new frameworks and services as early as possible. In other words, focusing on bringing day-0 support where possible, especially for the ever-popular Pytorch.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a9c1649-9a9a-48fe-8373-78274692a2dd_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a9c1649-9a9a-48fe-8373-78274692a2dd_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a9c1649-9a9a-48fe-8373-78274692a2dd_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a9c1649-9a9a-48fe-8373-78274692a2dd_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a9c1649-9a9a-48fe-8373-78274692a2dd_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a9c1649-9a9a-48fe-8373-78274692a2dd_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2a9c1649-9a9a-48fe-8373-78274692a2dd_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:330174,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a9c1649-9a9a-48fe-8373-78274692a2dd_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a9c1649-9a9a-48fe-8373-78274692a2dd_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a9c1649-9a9a-48fe-8373-78274692a2dd_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a9c1649-9a9a-48fe-8373-78274692a2dd_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2a9c1649-9a9a-48fe-8373-78274692a2dd_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>This also includes AMD’s previously announced efforts to bring ROCm to first-class citizen status under Windows – therefore helping them bootstrap the next generation of ROCm developers. While bits and pieces have been available under Windows up until recently, such as HIP and various tools under WSL, in Q3 of this year AMD will finally begin previewing native Pytorch support under Windows, and ONNX runtime support as well. And perhaps most importantly of all, this next Windows release will bring support for ROCm software development on AMD’s latest generation of discrete (RDNA 4) and integrated (RDNA 3) GPU hardware.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d4ef0f-af35-4b7a-bdb0-9471524aef12_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d4ef0f-af35-4b7a-bdb0-9471524aef12_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d4ef0f-af35-4b7a-bdb0-9471524aef12_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d4ef0f-af35-4b7a-bdb0-9471524aef12_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d4ef0f-af35-4b7a-bdb0-9471524aef12_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d4ef0f-af35-4b7a-bdb0-9471524aef12_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a5d4ef0f-af35-4b7a-bdb0-9471524aef12_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:303933,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d4ef0f-af35-4b7a-bdb0-9471524aef12_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d4ef0f-af35-4b7a-bdb0-9471524aef12_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d4ef0f-af35-4b7a-bdb0-9471524aef12_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d4ef0f-af35-4b7a-bdb0-9471524aef12_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5d4ef0f-af35-4b7a-bdb0-9471524aef12_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Elsewhere, there is an ever-continuing push for improved performance from software, which over the years is where most of the total performance gains from new hardware generations has come from. Case in point, AMD is touting that both inference and training performance on the existing MI300X is upwards of 3.8x faster under ROCm 7 than ROCm 6.0, all thanks to software improvements.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1649430f-2f64-4f19-9453-14e8f69efefa_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1649430f-2f64-4f19-9453-14e8f69efefa_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1649430f-2f64-4f19-9453-14e8f69efefa_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1649430f-2f64-4f19-9453-14e8f69efefa_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1649430f-2f64-4f19-9453-14e8f69efefa_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1649430f-2f64-4f19-9453-14e8f69efefa_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1649430f-2f64-4f19-9453-14e8f69efefa_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:266575,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1649430f-2f64-4f19-9453-14e8f69efefa_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1649430f-2f64-4f19-9453-14e8f69efefa_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1649430f-2f64-4f19-9453-14e8f69efefa_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1649430f-2f64-4f19-9453-14e8f69efefa_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1649430f-2f64-4f19-9453-14e8f69efefa_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Notably, these are the cumulative improvements of all the software improvements AMD has made since the start of ROCm 6, so these improvements are not solely due to ROCm 7; but rather it’s meant to illustrate how AMD has improved software performance since the MI300X launch in late 2023. AMD’s software team has stated that they aren’t holding back software performance improvements – they will deploy them as soon as possible – so ROCm 7 is more of a rolling release than a major milestone on the performance front.</p><p>What is new are a suite of features across the stack. Everything from distributed inference to GEMM tuning to GPU direct access has been touched in ROCm 7, all adding new tools and features for developers to further improve their ROCm programs.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F692f6b5f-f2c3-4631-a447-9f3171b9bd59_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F692f6b5f-f2c3-4631-a447-9f3171b9bd59_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F692f6b5f-f2c3-4631-a447-9f3171b9bd59_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F692f6b5f-f2c3-4631-a447-9f3171b9bd59_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F692f6b5f-f2c3-4631-a447-9f3171b9bd59_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F692f6b5f-f2c3-4631-a447-9f3171b9bd59_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/692f6b5f-f2c3-4631-a447-9f3171b9bd59_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:338266,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F692f6b5f-f2c3-4631-a447-9f3171b9bd59_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F692f6b5f-f2c3-4631-a447-9f3171b9bd59_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F692f6b5f-f2c3-4631-a447-9f3171b9bd59_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F692f6b5f-f2c3-4631-a447-9f3171b9bd59_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F692f6b5f-f2c3-4631-a447-9f3171b9bd59_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>And, as briefly mentioned earlier, AMD is finally paying more focus to enterprise management needs as well with ROCm Enterprise AI. This operational stack is intended to provide the tools that large enterprises need to actually manage and orchestrate AMD-based AI clusters, encompassing everything from provisioning to model fine-tuning.</p><p>ROCm 7 is being released in a preview format today. The final release will come at some point in the second-half of the year.</p><p>While not strictly a new product being announced or launched today, AMD also used part of today’s keynote to talk about their networking technologies, as well as talking up the recently-released Pollara 400 AI NIC. A product of AMD’s Pensando division, a previous acquisition, the Pollara 400 AI NIC is AMD’s first post-acqusition NIC, and an important cornerstone in developing a complete AMD hardware ecosystem. By offering AMD CPUs, AMD accelerators, and now suitable high-performance NICs, AMD’s partners can build whole racks of AMD-based systems.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab661e62-f1ad-4ebb-b880-f888e168203f_4608x2592.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab661e62-f1ad-4ebb-b880-f888e168203f_4608x2592.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab661e62-f1ad-4ebb-b880-f888e168203f_4608x2592.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab661e62-f1ad-4ebb-b880-f888e168203f_4608x2592.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab661e62-f1ad-4ebb-b880-f888e168203f_4608x2592.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab661e62-f1ad-4ebb-b880-f888e168203f_4608x2592.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ab661e62-f1ad-4ebb-b880-f888e168203f_4608x2592.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3126522,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab661e62-f1ad-4ebb-b880-f888e168203f_4608x2592.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab661e62-f1ad-4ebb-b880-f888e168203f_4608x2592.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab661e62-f1ad-4ebb-b880-f888e168203f_4608x2592.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab661e62-f1ad-4ebb-b880-f888e168203f_4608x2592.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fab661e62-f1ad-4ebb-b880-f888e168203f_4608x2592.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>As alluded to by the name, the Pollara 400 AI NIC is a 400G Ethernet card. With the central controller built on TSMC’s N4 process, the Pollara 400 is meant to be used as a highly programmable P4 NIC for scaling out AMD compute clusters, taking AMD from an 8-way system (their current scale-up limit) to as large as customers need.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd97c2c43-e337-4dd2-a8cf-1ec69800efa6_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd97c2c43-e337-4dd2-a8cf-1ec69800efa6_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd97c2c43-e337-4dd2-a8cf-1ec69800efa6_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd97c2c43-e337-4dd2-a8cf-1ec69800efa6_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd97c2c43-e337-4dd2-a8cf-1ec69800efa6_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd97c2c43-e337-4dd2-a8cf-1ec69800efa6_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d97c2c43-e337-4dd2-a8cf-1ec69800efa6_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:442702,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd97c2c43-e337-4dd2-a8cf-1ec69800efa6_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd97c2c43-e337-4dd2-a8cf-1ec69800efa6_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd97c2c43-e337-4dd2-a8cf-1ec69800efa6_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd97c2c43-e337-4dd2-a8cf-1ec69800efa6_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd97c2c43-e337-4dd2-a8cf-1ec69800efa6_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Notably, this is also the first Ultra Ethernet Consortium-ready AI NIC, with AMD being one of the steering members of what is the group pushing forward the next generation of Ethernet. We’re still very early in the days of UEC, and AMD will be but one of hopefully many players, but the company is securely hitching its horse on this next generation of Ethernet to provide the scale-out features and performance that current Ethernet technologies cannot provide.</p><p>Last, but certainly not least, is the matter of AMD’s broader pivot to rack-scale solutions. With large customers increasingly buying systems by the whole rack – networking hardware and all – instead of by the individual server, AMD is looking to meet customers (and rival NVIDIA) in offering rack-scale systems.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6446ac4-b5a5-4bc9-a4bb-3198f419eb02_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6446ac4-b5a5-4bc9-a4bb-3198f419eb02_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6446ac4-b5a5-4bc9-a4bb-3198f419eb02_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6446ac4-b5a5-4bc9-a4bb-3198f419eb02_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6446ac4-b5a5-4bc9-a4bb-3198f419eb02_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6446ac4-b5a5-4bc9-a4bb-3198f419eb02_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b6446ac4-b5a5-4bc9-a4bb-3198f419eb02_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:489687,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6446ac4-b5a5-4bc9-a4bb-3198f419eb02_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6446ac4-b5a5-4bc9-a4bb-3198f419eb02_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6446ac4-b5a5-4bc9-a4bb-3198f419eb02_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6446ac4-b5a5-4bc9-a4bb-3198f419eb02_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6446ac4-b5a5-4bc9-a4bb-3198f419eb02_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Today that is in the form of MI350 + Turin 5th Gen EPYC + Pollara 400. This is the first generation of such systems where AMD has an Ethernet offering to pair with their CPUs and GPUs. However the big prize is on racks-scale systems that can truly scale-up (rather than scale-out) to the whole rack, and this is where AMD is laying out a series of fresh roadmaps for both their GPUs and their rack-scale systems. As we’ve seen with other AI vendors, the long hardware cycle time and significant investments required means that hardware vendors are starting to offer an open look at their plans for the next few years, and AMD is among this crowd.</p><p>First and foremost, let’s talk about the GPU side of matters. AMD has previously published GPU roadmaps leading up to the MI400 in 2026, calling it a true next-generation GPU architecture with little in the way of hard details. Now that the MI400 is only around a year off, AMD is outlining for the first time some of the features and the performance targets for their future accelerator.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34cad41b-2f6d-4da3-a6be-24ee29a51105_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34cad41b-2f6d-4da3-a6be-24ee29a51105_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34cad41b-2f6d-4da3-a6be-24ee29a51105_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34cad41b-2f6d-4da3-a6be-24ee29a51105_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34cad41b-2f6d-4da3-a6be-24ee29a51105_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34cad41b-2f6d-4da3-a6be-24ee29a51105_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/34cad41b-2f6d-4da3-a6be-24ee29a51105_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:334060,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34cad41b-2f6d-4da3-a6be-24ee29a51105_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34cad41b-2f6d-4da3-a6be-24ee29a51105_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34cad41b-2f6d-4da3-a6be-24ee29a51105_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34cad41b-2f6d-4da3-a6be-24ee29a51105_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34cad41b-2f6d-4da3-a6be-24ee29a51105_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>With a performance target of 20 PFLOPS of FP8, MI400 is slated to double MI355X’s AI performance at low precision. Feeding the beast will be 432GB of next-generation HBM4 memory, with AMD touting a true generational jump in memory bandwidth of 19.6 TB/second – more than double MI355X’s. Based on what we know about HBM4 thus far, these figures allude to 12 stacks of memory on a single accelerator, but this remains to be confirmed. AMD has not previously named the underlying architecture for the chip, but in previous roadmaps it has been described as “CDNA Next.”</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e690d6-31fe-4812-9d48-6ab22b3cb559_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e690d6-31fe-4812-9d48-6ab22b3cb559_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e690d6-31fe-4812-9d48-6ab22b3cb559_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e690d6-31fe-4812-9d48-6ab22b3cb559_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e690d6-31fe-4812-9d48-6ab22b3cb559_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e690d6-31fe-4812-9d48-6ab22b3cb559_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/06e690d6-31fe-4812-9d48-6ab22b3cb559_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:279196,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e690d6-31fe-4812-9d48-6ab22b3cb559_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e690d6-31fe-4812-9d48-6ab22b3cb559_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e690d6-31fe-4812-9d48-6ab22b3cb559_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e690d6-31fe-4812-9d48-6ab22b3cb559_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06e690d6-31fe-4812-9d48-6ab22b3cb559_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Compute performance aside, the other notable revelation today is that MI400 will support Ultra Accelerator Link, the open industry standard for developing scale-up fabrics. If all goes according to plan, UAL will give AMD the scale-up bandwidth and flexibility that they lack today with the MI350 series, allowing AMD to go from 8 GPUs to a whopping 1024 GPUs in a scale-up configuration.</p><p>MI400, in turn, will be the heart of AMD’s next-generation rack-scale system, Helios. Combining MI400, 6th gen EPYC “Venice,” and the “Vulcano” NIC, Helios is meant to be AMD’s proper answer to today’s (and tomorrow’s) rack-scale systems from NVIDIA and others. With NVIDIA having already shown a bit of their hand with their own roadmaps, AMD thinks that a 72 GPU Helios rack will be able to match NVIDIA’s next-gen Vera Rubin racks on AI performance while beating it in memory capacity, memory bandwidth, and even scale-out network bandwidth. In short, AMD wants to capture performance leadership at the GPU level and at the rack level.</p><blockquote><p><em>Venice is the name for Zen 6 based EPYC CPUs built on TSMC N2. AMD showed off a wafer of this only a few weeks ago with TSMC to showcase the first N2 silicon being developed. We expect Venice to be launched in 1H 2026.</em></p><p><em>Vulcano is an 800G networking solution that incorporates PCIe 6.0 support as well as Ultra Accelerator Link support, built on TSMC N3.</em></p></blockquote><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4300f62f-736c-4cfa-be3f-7c1274488fb1_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4300f62f-736c-4cfa-be3f-7c1274488fb1_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4300f62f-736c-4cfa-be3f-7c1274488fb1_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4300f62f-736c-4cfa-be3f-7c1274488fb1_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4300f62f-736c-4cfa-be3f-7c1274488fb1_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4300f62f-736c-4cfa-be3f-7c1274488fb1_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4300f62f-736c-4cfa-be3f-7c1274488fb1_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:478322,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4300f62f-736c-4cfa-be3f-7c1274488fb1_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4300f62f-736c-4cfa-be3f-7c1274488fb1_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4300f62f-736c-4cfa-be3f-7c1274488fb1_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4300f62f-736c-4cfa-be3f-7c1274488fb1_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4300f62f-736c-4cfa-be3f-7c1274488fb1_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><em>Edit from Ian: This rack looks to be double width. I’ve been saying for a while that a single rack width isn’t sufficient in today’s scale-up, and I’m fully expecting team red or green to redefine the base rack specification in the next year or two to compensate. We already see dual-width rack designs from some EDA companies and their emulation tools (eg Siemens), but if this image is a true-ish render, it could be coming to AI next year for sure.</em></p><p>And AMD’s rack plans don’t stop there. The company is publishing a whole rack roadmap, that takes the company’s AI offerings through 2027. Two years for now, the hereto-unnamed next-gen AI rack will combine an even newer MI500 accelerator with EPYC “Verano” and the “Vulcano” NIC.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d14a372-207b-434a-954f-24c13e0b00c8_4000x2250.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d14a372-207b-434a-954f-24c13e0b00c8_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d14a372-207b-434a-954f-24c13e0b00c8_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d14a372-207b-434a-954f-24c13e0b00c8_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d14a372-207b-434a-954f-24c13e0b00c8_4000x2250.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d14a372-207b-434a-954f-24c13e0b00c8_4000x2250.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0d14a372-207b-434a-954f-24c13e0b00c8_4000x2250.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:473295,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://morethanmoore.substack.com/i/165779277?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d14a372-207b-434a-954f-24c13e0b00c8_4000x2250.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d14a372-207b-434a-954f-24c13e0b00c8_4000x2250.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d14a372-207b-434a-954f-24c13e0b00c8_4000x2250.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d14a372-207b-434a-954f-24c13e0b00c8_4000x2250.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d14a372-207b-434a-954f-24c13e0b00c8_4000x2250.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>AMD’s goal is to iterate on their core architectures on a yearly basis, and so long as they’re able to hold to their roadmap published here, they’ll be able to do just that, with multiple new CPU and GPU families to build ever-faster racks.</p><p>Ultimately, AMD is setting a new 5 year goal to reach a 20-fold increase in rack-scale energy efficiency versus MI300X by 2030. And coupled with software optimizations to help drive down the amount of computational work required to actually train a model, AMD is ambitiously eyeing a possible 100x improvement in overall energy efficiency by that time.</p><p>Suffice it to say, if all goes according to plan, then AMD has some very big plans for their AI offerings over the next several years. But for the immediate future, AMD’s AI offerings are going to be rooted in the new instinct MI350 series, the CDNA 4 architecture, and all the performance that kilowatts of power and low precision number can unlock.</p><p>We’re planning deeper dives into today’s announcements, such as the architecture and the benchmarks, so subscribe to ensure you do not miss out!</p><p>Want to read more right now? How about this article from earlier in 2025 where AMD CEO Dr. Lisa Su laid out her plans for AMD this year.</p><div data-component-name="DigestPostEmbed"><a href="https://morethanmoore.substack.com/p/seven-highlights-from-amds-ceo-dr" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4147723-ab35-46cc-b5d5-858be57e95a1_2378x1034.jpeg"><img src="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb4147723-ab35-46cc-b5d5-858be57e95a1_2378x1034.jpeg" sizes="100vw" alt="Seven Highlights from AMD's CEO Dr Lisa Su" width="140" height="140"></picture></div></a></div><h6>More Than Moore, as with other research and analyst firms, provides or has provided paid research, analysis, advising, or consulting to many high-tech companies in the industry, which may include advertising on the More Than Moore newsletter or TechTechPotato YouTube channel and related social media. The companies that fall under this banner include AMD, Applied Materials, Armari, ASM, Ayar Labs, Baidu, Dialectica, Facebook, GLG, Guidepoint, IBM, Impala, Infineon, Intel, Kuehne+Nagel, Lattice Semi, Linode, MediaTek, NextSilicon, NordPass, NVIDIA, ProteanTecs, Qualcomm, SiFive, SIG, SiTime, Supermicro, Synopsys, Tenstorrent, Third Bridge, TSMC, Untether AI, Ventana Micro.</h6></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Seven replies to the viral Apple reasoning paper and why they fall short (313 pts)]]></title>
            <link>https://garymarcus.substack.com/p/seven-replies-to-the-viral-apple</link>
            <guid>44278403</guid>
            <pubDate>Sat, 14 Jun 2025 19:52:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://garymarcus.substack.com/p/seven-replies-to-the-viral-apple">https://garymarcus.substack.com/p/seven-replies-to-the-viral-apple</a>, See on <a href="https://news.ycombinator.com/item?id=44278403">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>The Apple paper on </span><a href="https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf" rel="">limitations in the “reasoning” of Large Reasoning Models</a><span>, which raised challenges for the latest scaling hypothesis, has clearly touched a nerve. Tons of media outlets covered it; huge numbers of people on social media are discussing. </span></p><p><span>M</span><a href="https://open.substack.com/pub/garymarcus/p/a-knockout-blow-for-llms?r=8tdk6&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false" rel="">y own post here laying out the Apple paper in historical and scientific context </a><span>was so popular that well over 150,000 people read it, biggest in this newsletter’s history. </span><em>The Guardian</em><span> published an adaptation of my post (“</span><a href="https://www.theguardian.com/commentisfree/2025/jun/10/billion-dollar-ai-puzzle-break-down" rel="">When billion-dollar AIs break down over puzzles a child can do, it’s time to rethink the hype</a><span>”) The editor tells me readers spent a long time reading it, notably longer than usual, as if people really wanted to understand the arguments in detail.  (The ACM computer science society is reposting the essay, too, and </span><a href="https://legrandcontinent.eu/fr/2025/06/10/ia-llm-marcus/" rel="">there is now a French version as well</a><span>).</span></p><p>Tons of GenAI optimists took cracks at the Apple paper (see below), and it is worth considering their arguments. Overall I have seen roughly seven different efforts at rebuttal, ranging from nitpicking and ad hominem to the genuinely clever. Most (not all) are based on grains of truth, but are any of them actually compelling?</p><p> Let’s consider.</p><ol><li><p><strong>Humans have trouble with complex problems and memory demands</strong><span>. True! But incomplete.  We have every right to expect machines to do things we can’t. Cars have more stamina, calculators don’t make arithmetical errors. That’s why we invented computers: to do repetitive calculation without errors. And in many cases (including the Tower of Hanoi, which featured prominently in the paper)) we have existing systems that work perfectly without errors. AGI should be a step forward; in many cases LLMs are a step backwards. And note the bait and switch from “we’re going to build AGI that can revolutionize the world” to “give us some credit, our systems make errors and humans do, too”. The real takeaway from the Apple paper is that LLMs can’t be trusted to run algorithms as complexity and distance from the training distribution grows (just as humans shouldn’t serve as calculators). If we want to get to AGI, we will have to better.</span></p></li><li><p><strong>The Large Reasoning Models (LRMs) couldn’t possibly solve the problem, because the outputs would require too many output tokens</strong><span> (which is to say the correct answer would be too long for the LRMs to produce). Partial truth, and a clever observation: LRMs (which are enhanced LLMs) have a shortcoming, which is a limit on how long their outputs can be. The correct answer to Tower of Hanoi with 12 moves would be too long for some LRMs to spit out, and the authors should have addressed that.  But crucially (i) this objection, clever as it is, doesn’t actually explain the overall pattern of results. The LRMs failed on  Tower of Hanoi with 8 discs, where the optimal solution is 255 moves, well within so-called token limits; (ii) well-written symbolic AI systems generally don’t suffer from this problem, and AGI should not either. The length limit on LLM is a bug, and most certainly not a feature. And look, if an LLM can’t reliably execute something as basic as Hanoi, what makes you think it is going to compute military strategy (especially with the fog of war) or molecular biology (with many unknowns) correctly? What the Apple team asked for was way easier than what the real world often demands.</span></p></li><li><p><strong>The paper was written by an intern</strong><span>. This one, which infuriated me because is a form of ad hominem argument rather than substance, is misguided and only barely true – and utterly lacking in context. It is </span><em>true</em><span> that the first author was an intern at Apple, Parshin Shojaee, but (i) she (not </span><em>he</em><span> as some fool I won’t name presumed, without spending two seconds of research) also happens to be </span><a href="https://parshinsh.github.io/" rel="">a very promising third year Ph.D. student with many paper presentations at leading conferences</a><span>, (ii) the article, if you actually read it, makes it clear she shared lead responsibility with </span><a href="https://imirzadeh.me/" rel="">Iman Mirzadeh</a><span>, who has a Ph.D.;  (iii) the paper actually has six authors, not one, and four have Ph.D.s; for good measure one is Yoshua Bengio’s brother </span><a href="https://bengio.abracadoudou.com/" rel="">Samy Bengio</a><span>, well-known and very distinguished in his own right within the machine learning community; (iv) it is a common practice in many scientific fields to put the junior author first, senior author last, as this paper did; thousands of major articles have done the same (and never been criticized for doing so — it’s a true desperation measure); (v) what really matters is the quality of the work. Alfred Sturtevant was an </span><em>undergraduate</em><span> when he invented gene maps. </span></p></li><li><p><strong>Bigger models might to do better</strong><span>. True, and this is always the case (I have seen one report that o3-pro can do at least one of the problems, at least some of the time; I have not seen a thorough study yet).  Bigger models sometimes do better because of genuine improvements in the model, sometimes because of problem-specific training. (From the outside we can never know which). But here’s the thing, we can’t know in advance what model is big enough (if any) for any given problem. And Apple’s result that some pretty large models could succeed at 6 discs, giving the illusion of mastery, but collapse by 8, is ominous.  One is left simply having to test everything, all the time, with little guarantees of anything. Some model might be big enough for task T of size S and fail on the next size, or on Task T’ that is slightly different, etc. It all becomes a crapshoot. Not good.</span></p></li><li><p><strong>The systems can solve the puzzles with code.</strong><span> True in some cases, and a huge win for neurosymbolic AI, given that they can’t reliably solve the puzzles without code, and given that code is symbolic.  </span><em>Huge</em><span> vindication for what I have been saying all along: we need AI that integrates both neural networks and symbolic algorithms and representations (such as logic, code, knowledge graphs, etc). But also, we need to do so reliably, and in a general way and we haven’t yet crossed that threshold. (Importantly, the point of the Apple paper goal was to see how LRM’s unaided explore a space of solutions via reasoning and backtracking, not see how well it could use preexisting code retrieved from the web.) An analogy: A student might complain about a math exam requiring integration or differentiation by hand, even though math software can produce the correct answer instantly. The teacher’s goal in assigning the problem, though, isn’t finding the answer to that question (presumably the teacher already know the answer),  but to assess the student’s conceptual understanding. Do LLM’s </span><em>conceptually</em><span> understand Hanoi?  That’s what the Apple team was getting at. (Can LLMs download the right code? Sure. But downloading code without conceptual understanding  is of less help in the case of new problems, dynamically changing environments, and so on.)</span></p></li><li><p><strong>The paper has only four examples, and at least one of them (Hanoi) isn’t perfect</strong><span>. I doubt any of them are perfect, but the four together provide converging evidence with dozens of other prior papers (including some of my own), and I am confident many more examples will be uncovered.  I already found a couple of analogous failures in algorithm application myself, which I will write about in a few days. Tal Linzen at NYU </span><a href="https://x.com/tallinzen/status/1933184078821360084?s=61" rel="">just published yet another example</a><span>, with “</span><em>models .. able to do the right thing for simple versions of [a language] problem (small grammars, short strings), but [with] accuracy drop[ping] very quickly as the problem becomes more complex</em><span>.” Mark my words: in time, we will see scores of papers reinforcing the Apple results. </span></p></li><li><p><strong>The paper is not news; we already knew these models generalize poorly</strong><span>. True! (I personally have been trying to tell people this for almost thirty years; Subbarao Rao Kambhampati has been trying his best, too). But then why do we think these models are the royal road to AGI? The real news here, aside from the fact that this was a clever study nailing down an important point, is that </span><em>people are finally starting to pay attention,</em><span> to (one of the) two biggest Achilles’ Heels of generative AI, and to appreciate its significance. (Tune in to this newsletter on the weekend to hear about the other.)  Hilarious, by the way, is hearing both “it’s wrong” and “we knew it all along”, simultaneously. In at least one case I saw a single person say both, minutes apart!</span></p></li></ol><p>Bottom line? None of the rejoinders are compelling. If people like Sam Altman are sweating, it’s because they should. The Apple paper is yet another clear sign that scaling is not the answer; for once, people are paying attention. </p><p>§</p><p>The kicker? A Salesforce paper also just posted, that many people missed:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg" width="1251" height="1791" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1791,&quot;width&quot;:1251,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:523217,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://garymarcus.substack.com/i/165791446?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc7dcc58c-c588-4f54-907e-7d7198090090_1251x1791.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>In the “multi-turn” condition, which presumably would require reasoning and algorithmic precision, performance was only 35%. </p><p>Talk about convergence evidence. Taking the SalesForce report together with the Apple paper, it’s clear the current tech is not to be trusted.</p><p><em><strong>Gary Marcus</strong><span>, professor emeritus at NYU, and author of The Algebraic Mind and “Deep learning is hitting a wall”, both of which anticipated the correct results, is thrilled to see people finally realize that scaling is not enough to get us to AGI. Now perhaps we can begin to build better AI.</span></em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Waymo's market share in San Francisco exceeds Lyft's (137 pts)]]></title>
            <link>https://underscoresf.com/in-san-francisco-waymo-has-now-bested-lyft-uber-is-next/</link>
            <guid>44277355</guid>
            <pubDate>Sat, 14 Jun 2025 16:44:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://underscoresf.com/in-san-francisco-waymo-has-now-bested-lyft-uber-is-next/">https://underscoresf.com/in-san-francisco-waymo-has-now-bested-lyft-uber-is-next/</a>, See on <a href="https://news.ycombinator.com/item?id=44277355">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="primary"><article id="post-141765"><div><p><img width="810" height="608" src="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?fit=810%2C608&amp;ssl=1" alt="" decoding="async" srcset="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?w=5356&amp;ssl=1 5356w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?resize=300%2C225&amp;ssl=1 300w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?resize=1500%2C1125&amp;ssl=1 1500w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?resize=768%2C576&amp;ssl=1 768w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?resize=1536%2C1152&amp;ssl=1 1536w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?resize=2048%2C1536&amp;ssl=1 2048w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?w=1620&amp;ssl=1 1620w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?w=2430&amp;ssl=1 2430w" sizes="(max-width: 810px) 100vw, 810px" data-attachment-id="141769" data-permalink="https://underscoresf.com/in-san-francisco-waymo-has-now-bested-lyft-uber-is-next/san-francisco-ca-usa-california-street-autonomes-fahrzeug/" data-orig-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?fit=5356%2C4017&amp;ssl=1" data-orig-size="5356,4017" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;8&quot;,&quot;credit&quot;:&quot;Dietmar Rabich&quot;,&quot;camera&quot;:&quot;Canon EOS 5D Mark IV&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1656635759&quot;,&quot;copyright&quot;:&quot;Dietmar Rabich, D\u00fclmen&quot;,&quot;focal_length&quot;:&quot;105&quot;,&quot;iso&quot;:&quot;100&quot;,&quot;shutter_speed&quot;:&quot;0.005&quot;,&quot;title&quot;:&quot;San Francisco (CA, USA), California Street, autonomes Fahrzeug (&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="San Francisco (CA, USA), California Street, autonomes Fahrzeug (" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?fit=300%2C225&amp;ssl=1" data-large-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/San_Francisco_CA_USA_California_Street_autonomes_Fahrzeug_Waymo_-_2022_-_2925.jpg?fit=810%2C608&amp;ssl=1"></p><h2><span>If a data-backed trend plays out, Waymo could become San Francisco’s biggest ride-hailing service before the year ends.</span></h2><p><a href="https://www.wsj.com/tech/waymo-cars-self-driving-robotaxi-tesla-uber-0777f570?gaa_at=eafs&amp;gaa_n=ASWzDAhnWF-iL0nB_VD1P9sE3oEvTzxfPRciScEXu2atNMFFerpPTdeL9vKlknl-ljg%3D&amp;gaa_ts=684048ca&amp;gaa_sig=T-9NFT-D_HBZgQzoyvZnU-0sp1-gS4FSVJMIo57FFlhxVVVMK8UFbvp6Eu_ndwTO23KoDxeZ9sZoC09rHkiDCg%3D%3D"><span>Waymo’s celestial ascent into the cultural zeitgeist</span></a><span> — a rise that has been propelled by dystopian memes and sheer, futuristic novelty — has only been matched by how it continues swallowing its competition. The Alphabet-owned autonomous driving company saw explosive exponential growth in ridership in 2024, with driverless rides increasing from </span><a href="https://underscoresf.com/self-driving-world-record-was-quietly-broken-in-sf-bay-area-last-week/"><span>77,000 to more than 312,000 lifts</span></a><span> by August of last year alone, according to the California DMV; as of publishing, Waymo asserts that 30% of their rides are to local small businesses.</span></p><figure id="attachment_141767" aria-describedby="caption-attachment-141767"><img data-recalc-dims="1" decoding="async" data-attachment-id="141767" data-permalink="https://underscoresf.com/in-san-francisco-waymo-has-now-bested-lyft-uber-is-next/502321336_17879766099336220_8766268074646908687_n/" data-orig-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?fit=1236%2C919&amp;ssl=1" data-orig-size="1236,919" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="502321336_17879766099336220_8766268074646908687_n" data-image-description="<p>Screenshot: Courtesy of YipitData</p>
" data-image-caption="" data-medium-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?fit=300%2C223&amp;ssl=1" data-large-file="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?fit=810%2C602&amp;ssl=1" src="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?resize=810%2C602&amp;ssl=1" alt="" width="810" height="602" srcset="https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?w=1236&amp;ssl=1 1236w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?resize=300%2C223&amp;ssl=1 300w, https://i0.wp.com/underscoresf.com/wp-content/uploads/2025/06/502321336_17879766099336220_8766268074646908687_n.jpg?resize=768%2C571&amp;ssl=1 768w" sizes="(max-width: 810px) 100vw, 810px"><figcaption id="caption-attachment-141767">Screenshot: Courtesy of YipitData</figcaption></figure><p><span>Independent contractors for Lyft and Uber have been </span><a href="https://www.theregister.com/2024/09/07/uber_driver_waymo/"><span>saying they’re “cooked”</span></a><span> for a while, citing massive declines in available requests as a result of Waymo’s success. (This, however, is a tandem issue: Waymo’s ride-hailing operations in San Francisco coincided with the increased number of regional rideshare drivers that began working during and after the pandemic.) But now factual data is showing that the aforementioned broiling is, indeed, happening … and at a rate quicker than once thought.</span></p><p><span>According to YipitData, a data and analytics firm based out of New York City, Waymo’s gross bookings from August of 2023 to April of this year have surpassed Lyft’s in market share. The twenty-month data analysis highlighted Uber’s dominance in San Francisco ridership — well over 50% of all trips booked via a ridesharing application were done on Uber throughout the analysis — but showed, perhaps more surprisingly, how quickly Waymo clambered into the commonplace. Waymo is also currently beating Lyft, a company that has operated rides in San Francisco since 2012, in total gross bookings.&nbsp;</span></p><p><span>In a staggeringly short amount of time, Waymo, which is about to celebrate its first anniversary of city-wide ride-hailing operations, has gone from effectively 0% market share of bookings in San Francisco to over 25%. Lyft has continuously gathered fewer bookings than Uber, but still managed to hold onto a roughly 30% market share since 2023. </span><span><br> </span><span><br> </span><span>Waymo has now flipped that stake, surpassing Lyft to become San Francisco’s second most-popular ride-hailing service. If the research published by YipitData is extrapolated outwardly, Waymo could easily beat Uber to become SF’s foremost taxi-like service by early next year. Or sooner.</span></p><p><span>What does this mean for San Francisco, the city that launched ridesharing services as we know them today? On the roads, not much; self-driving Jaguar iPaces would become even more prominent. But on an economic level, a subset of blue-collar workers (which numbers in the tens of thousands in San Francisco) would find themselves either regionally displaced or outright vocationally exterminated by a branch of artificial intelligence.&nbsp;</span></p><p><span>We don’t need a graph to tell you how that window into a </span><a href="https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic"><span>looming dystopian landscape plays out.</span></a><span> But hey, at least you won’t have to surrender to mind-numbing small talk on </span><a href="https://missionlocal.org/2025/03/sf-waymo-sfo-airport-robotaxis-autonomous-vehicles-teamsters/"><span>your way to SFO.</span></a></p><hr><p><em>Photo: Courtesy of Wikimedia Commons</em></p></div></article></div></div>]]></description>
        </item>
    </channel>
</rss>