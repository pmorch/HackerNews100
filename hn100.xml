<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 06 Dec 2023 22:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Quad9 wins appeal against Sony (207 pts)]]></title>
            <link>https://quad9.net/news/blog/quad9-turns-the-sony-case-around-in-dresden/</link>
            <guid>38548209</guid>
            <pubDate>Wed, 06 Dec 2023 19:11:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quad9.net/news/blog/quad9-turns-the-sony-case-around-in-dresden/">https://quad9.net/news/blog/quad9-turns-the-sony-case-around-in-dresden/</a>, See on <a href="https://news.ycombinator.com/item?id=38548209">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-13a3556e=""><p>Today marks a bright moment in the efforts to keep the internet a neutral and trusted resource for everyone.</p>
<p>Quad9 has received word from the courts in Dresden, Germany in the appeal of our case versus Sony Entertainment (Germany). The court has ruled in favor of Quad9, clearly and unequivocally. The text from the court is available here <a href="https://quad9.net/uploads/URT_05_12_2023_redacted_de2_1880f3f6c4.pdf">in German</a> and <a href="https://quad9.net/uploads/URT_05_12_2023_en_Korr_MH_en2_2e629b1f7b.pdf">in English</a>.</p>
<p>Needless to say, we are elated at the news.</p>
<h2>Background</h2>
<p>Sony Entertainment (Germany) started a legal proceeding against Quad9 more than two years ago to force Quad9 to stop resolving certain domain names which they claimed were involved in copyright infringement behavior.</p>
<ul>
<li><a href="https://www.quad9.net/news/blog/quad9-and-sony-music-german-injunction-update-for-july-2023/" target="_blank">07/2023</a></li>
<li><a href="https://quad9.net/news/blog/sony-s-legal-attack-on-quad9-censorship-and-freedom-of-speech/" target="_blank">03/2023</a></li>
<li><a href="https://quad9.net/news/blog/quad9-and-sony-music-german-injunction-update-for-february-2023/" target="_blank">02/2023</a></li>
<li><a href="https://quad9.net/news/blog/an-update-to-the-quad9-and-sony-music-german-court-injunction-august-2022/" target="_blank">08/2022</a></li>
<li><a href="https://quad9.net/news/blog/quad9-files-official-objection-opposing-sony-music-s-german-court-ruling/" target="_blank">09/2021</a></li>
</ul>
<p>We believe this lawsuit was an attempt to set a precedent, such that commercial rights holders could demand that sites on the internet be made unreachable by forcing recursive resolvers to block content. We contended that recursive resolvers have no commercial or even remotely indirect relationship to any of the infringing parties, and that Sony’s demand for blocking was ineffective, inappropriately specified, and not related to Quad9.</p>
<p>What made this case more problematic, in our view, was that the servers in question in this case were not located in Germany, and the links they pointed to were on servers also not in Germany. The domain name (<code>canna.to</code>) was not registered in Germany and was under the top-level-domain operated by the nation of Tonga. Sony Entertainment further asserted that we block the domains globally, not just in Germany, as geoIP does not block for users based in Germany with certainty. For that matter, Quad9 has no office or standing in Germany (we are a Swiss entity), but due to the Lugano Convention treaty it was possible for Sony to serve an injunction in Switzerland and drag Quad9 into legal proceedings.</p>
<h2>Details on Judgement</h2>
<p>The appeal with the Higher Regional Court in Dresden follows a decision by the Regional Court in Leipzig, in which Sony prevailed, and Quad9 was convicted as a wrongdoer. Before that, Sony successfully obtained a preliminary junction against Quad9 with the Regional Court in Hamburg. The objection against the preliminary injunction by Quad9 was unsuccessful, and the appeal with the Higher Regional Court in Hamburg was withdrawn by Quad9 since a decision in the main proceeding was expected to be made earlier than the conclusion of the appeal in the preliminary proceedings.</p>
<p>Please find below a table summarizing the main take-aways from the decision in Dresden. For more information on these points, please go <a href="https://quad9.net/uploads/2023_12_06_Comparison_of_processes_aad2df4618.pdf">here</a>.</p>
<table>
<thead>
<tr>
<th>Leipzig</th>
<th>Dresden</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>Extending the motion to “and/or the other domain(s) is ok</td>
<td>“and/or the other domain(s)” is too unspecific</td>
<td>Win</td>
</tr>
<tr>
<td>DNS resolver plays a "central role" in the publication of the copyrighted material leading to liability as a wrongdoer</td>
<td>No liability as a perpetrator: DNS resolver does not play a “central role”</td>
<td>Win</td>
</tr>
<tr>
<td>No Störerhaftung as Quad9 was held liable as a wrongdoer</td>
<td>No Störerhaftung</td>
<td>Win</td>
</tr>
<tr>
<td>Liability privileges are not applicable as DNS resolver operators are not deemed service providers under the Telemedia Services Act</td>
<td>DNS resolver operators enjoy liability privileges as access providers</td>
<td>Win</td>
</tr>
<tr>
<td>Subsidiarity criteria fulfilled</td>
<td>Subsidiarity criteria not fulfilled: Sony has not done enough to go after the hosting company</td>
<td>Win</td>
</tr>
</tbody>
</table>
<p>The court has also ruled that the case cannot be taken to a higher court and their decision is the final word in this particular case. Sony may appeal the appeal closure via a complaint against the denial of leave of appeal and then would have to appeal the case itself with the German Federal Court. So while there is still a possibility that this case could continue, Sony would have to win twice to turn the decision around again.</p>
<p>We would also like to clarify that even though Quad9 benefits from the liability privileges as a mere conduit, it is possible that a DNS resolver operator can be required to block as a matter of last resort if the claiming party has taken appropriate means to go after the wrongdoer and the hosting company unsuccessfully. Such measures could be legal action by applying for a preliminary injunction against a hosting company within the EU. These uncertainties still linger, and we expect that this ongoing question of what circumstances require what actions, by what parties, will continue to be argued in court and in policy circles over the next few years.</p>
<p>We remain committed to the concept that resolving a domain name is not an action that should be prohibited for commercial goals. The DNS does not contain content - it is a system designed for delivery only of pointers, not for data transport.</p>
<p>The courts in Cologne also recently ruled in favor of Cloudflare in a <a href="https://blog.cloudflare.com/latest-copyright-decision-in-germany-rejects-blocking-through-global-dns-resolvers/" target="_blank">similar case</a> involving DNS recursive resolution (though that case is more complex as it involves content hosting or proxying) and we are pleased to have consistent and clear statements from both courts in this matter of DNS recursive resolution.</p>
<p>Today was a significant win in Germany, but there is some disappointment as well. We received a notice from a consortium of Italian rightsholders (Sony Music Italy, Universal Music Italy, Warner Music Italy, and Federation of Italian Music Industry) who have demanded that Quad9 block domains in Italy, and there is potentially another court process ahead of us. We have a blog post on that <a href="https://quad9.net/news/blog/italian-blocking-demands-following-a-bad-example">here</a>, also published today.</p>
<h2>Actions taken</h2>
<p>From a practical perspective, Quad9 has removed the blocks on all domains previously noted by Sony and documented in the lower courts. Those domains are listed below. Currently, there are no domains now being blocked in Germany or anywhere else related to this case.</p>
<h2>Thanks to those who believed in us and in your rights</h2>
<p>There are hundreds of individual supporters that we wish to thank for their monetary support, advice, and willingness to speak up in public - social media, blogs, and in industry settings. In particular, we would like to thank our legal team at <a href="https://rickert.law/" target="_blank">Rickert.law</a> - Thomas Rickert and Sandra Schulte - who gave us an enormously generous amount of effort and time spent on achieving this win. We'd also like to thank the <a href="https://freiheitsrechte.org/" target="_blank">GFF</a> for their legal advice, continued advisement on German media issues, and for being a sounding board for our legal team and management. We would also like to thank the Kahle Austin Family Foundation, and <a href="https://www.stiftung-mercator.ch/" target="_blank">Stiftung Mercator Schweiz</a>, and <a href="https://www.eco.de/" target="_blank">eco</a> for their support.</p>
<p>To those who have donated to Quad9: By helping us, you helped yourself. It is clear to us that expansionism of internet censorship will never stop and letting minor inappropriate removals of digital sovereignty go without challenge will ultimately end with heavy-handed suppression of ideas much closer to the user. Those boundaries are constantly being tested, and we have won this round. You have kept the front lines from moving closer to you - congratulations!</p>
<p>Quad9 is a non-profit organization, and the costs of pursuing court cases is a heavy burden which takes away our ability to expand the network as quickly as we need. We will continue to need that help in the next phases of our defense of the DNS. Please consider <a href="https://www.quad9.net/donate" target="_blank">donating</a> - every euro counts as we continue to fight for your ability to use the DNS. Thank you for making this possible for us.</p>
<p>Regardless of our victory in this case, the costs to Quad9 have been extensive and excessive from a real monetary perspective as well as a time and attention drain. If you feel like we have done service for you in this court case, please consider <a href="https://www.quad9.net/donate" target="_blank">donating</a> to Quad9 so that we can catch up to where we should have been by now with other costs related to our continued service.</p>
<h2>Documents / Further Reading / Details</h2>
<p>You may find the original case documents <a href="https://quad9.net/uploads/URT_05_12_2023_redacted_de2_1880f3f6c4.pdf">here</a>.<br>
You may find an English machine-translated version <a href="https://quad9.net/uploads/URT_05_12_2023_en_Korr_MH_en2_2e629b1f7b.pdf">here</a>. Please note the English version may not be exact, and final references should be made to the German original.</p>
<p>In addition to the case documentation itself issued by the Court, Rickert.law has broken the arguments down into more specific discussions on several points, which may be of interest to readers who wish a more complete understanding of the points of law, history of the case, or understanding of the judgment.  A German-language version may be found <a href="https://quad9.net/uploads/2023_12_06_Verfahrensuebersicht_dfe6a95669.pdf">here</a> and the English version is <a href="https://quad9.net/uploads/2023_12_06_Comparison_of_processes_aad2df4618.pdf">here</a>.</p>
<p>List of domains now unblocked: <code>canna.to</code>, <code>www.canna.to</code>, <code>uu.canna.to</code>, <code>www.uu.canna.to</code>, <code>canna-power.to</code>, <code>www.canna-power.to</code>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wikifunctions (102 pts)]]></title>
            <link>https://wikimediafoundation.org/news/2023/12/05/introducing-wikifunctions-first-wikimedia-project-to-launch-in-a-decade-creates-new-forms-of-knowledge/</link>
            <guid>38548130</guid>
            <pubDate>Wed, 06 Dec 2023 19:04:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wikimediafoundation.org/news/2023/12/05/introducing-wikifunctions-first-wikimedia-project-to-launch-in-a-decade-creates-new-forms-of-knowledge/">https://wikimediafoundation.org/news/2023/12/05/introducing-wikifunctions-first-wikimedia-project-to-launch-in-a-decade-creates-new-forms-of-knowledge/</a>, See on <a href="https://news.ycombinator.com/item?id=38548130">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>The Wikimedia Foundation — the nonprofit that hosts Wikipedia and other <a href="https://wikimediafoundation.org/our-work/wikimedia-projects/" rel="noreferrer noopener">Wikimedia projects</a> — has announced the launch of <a href="https://www.wikifunctions.org/wiki/Wikifunctions:Main_Page" rel="noreferrer noopener">Wikifunctions</a>, its first new project in over a decade. The project will enable volunteer editors to collaboratively create and maintain a library of functions to answer questions and enhance knowledge on Wikimedia projects and beyond.</p><article>
		


<p>A “function” is a sequence of programming instructions that makes a calculation based on data provided. Internet users most commonly encounter functions when entering queries on search engines, such as the time difference between two cities, the distance between two locations, or the volume of an object. Functions operate behind-the-scenes to produce answers to these queries. For the first time, Wikifunctions will provide a library of functions that everyone, everywhere can access and contribute to.&nbsp;</p>



<p>Notably, these functions can exist in any language; therefore, for many Wikifunctions users, this will be the first project where they can read and write functions in their native language.</p>



<blockquote>
<p>“Functions are pathways to knowledge. Wikifunctions aims to make these pathways more accessible than ever before.&nbsp; Imagine a programming system where a single line of code can be run from anywhere by anyone — even by programs written in different programming languages. That is the promise and potential of Wikifunctions,” said Dr. Denny Vrandečić, Head of Special Projects at the Wikimedia Foundation. “Wikifunctions will be built in the same community-led fashion as Wikipedia, with volunteer editors donating their time and energy to the cause of building a freely editable library of valuable functions. We are excited for this project to continue to grow with their contributions.”</p>
</blockquote>



<p>Currently, functions on Wikimedia projects are complex, siloed, and vary by language versions of various projects (Wikipedia alone has over 300 language versions). Wikifunctions will place functions in a single shared space, simplifying the work of the volunteers who maintain them and increasing their accessibility. Wikifunctions will eventually integrate with Wikipedia and other Wikimedia projects, opening new opportunities for knowledge creation.</p>



<blockquote>
<p>&nbsp;“Wikifunctions will bring together different data sources from other Wikimedia projects in new and powerful ways, ultimately leveraging functional code to create new forms of knowledge,”&nbsp; said <a href="https://wikimediafoundation.org/en/?profile=selena-deckelmann" target="_blank" rel="noreferrer noopener">Selena Deckelmann</a>, Chief Product and Technology Officer at the Wikimedia Foundation. “We are excited to announce Wikifunctions as yet another step in getting us closer to <a href="https://wikimediafoundation.org/about/vision/" target="_blank" rel="noreferrer noopener">our vision</a> of a world where everyone can freely share in the sum of all knowledge.”</p>
</blockquote>



<p>Wikifunctions is the underlying technical infrastructure that will support a <a href="https://meta.wikimedia.org/wiki/Abstract_Wikipedia" target="_blank" rel="noreferrer noopener">wider initiative</a> by the Wikimedia Foundation to enable people to share more knowledge in more languages across Wikipedia.&nbsp; Through this initiative, users will be able to create and maintain content in their native language, which others can access in over 300 languages available on Wikimedia projects. The long-term aim of this effort is to create knowledge that is independent of language, and easier for Wikipedia editors to share, add, translate, and improve across languages on the online encyclopedia. The work is being supported by grants from <a href="https://www.google.org/" target="_blank" rel="noreferrer noopener">Google.org</a>, <a href="https://www.google.org/" target="_blank" rel="noreferrer noopener">The Rockefeller Foundation</a>, and <a href="https://wikimediaendowment.org/" target="_blank" rel="noreferrer noopener">the Wikimedia Endowment</a>.</p>



<p>Wikifunctions was approved by the Wikimedia Foundation’s Board of Trustees <a href="https://meta.wikimedia.org/wiki/Special:MyLanguage/Abstract_Wikipedia/July_2020_announcement" target="_blank" rel="noreferrer noopener">in 2020</a>. The project went live as a read-only site earlier this year, and it is now available so that anyone, anywhere can use it. It is the fourteenth Wikimedia project — the first new project in a decade. Learn more about the project at <a href="https://www.wikifunctions.org/wiki/Wikifunctions:Main_Page" target="_blank" rel="noreferrer noopener">wikifunctions.org</a>.</p>



<h3><strong>About the Wikimedia Foundation</strong></h3>



<p><a href="https://wikimediafoundation.org/" target="_blank" rel="noreferrer noopener">The Wikimedia Foundation</a> is the nonprofit organization that operates Wikipedia and other Wikimedia free knowledge projects. Our vision is a world in which every single human can freely share in the sum of all knowledge. We believe that everyone has the potential to contribute something to our shared knowledge and that everyone should be able to access that knowledge freely. We host Wikipedia and the Wikimedia projects; build software experiences for reading, contributing, and sharing Wikimedia content; support the volunteer communities and partners who make Wikimedia possible. The Wikimedia Foundation is a United States 501(c)(3) tax-exempt organization with offices in San Francisco, California, USA.</p>



<p><strong>For media inquiries, please contact </strong><a href="mailto:press@wikimedia.org"><strong>press@wikimedia.org</strong></a><strong>.</strong></p>

			
	
	
		</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[W3C Leaves Twitter (173 pts)]]></title>
            <link>https://w3c.social/@w3c/111534700276754588</link>
            <guid>38547203</guid>
            <pubDate>Wed, 06 Dec 2023 17:49:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://w3c.social/@w3c/111534700276754588">https://w3c.social/@w3c/111534700276754588</a>, See on <a href="https://news.ycombinator.com/item?id=38547203">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Databases are the endgame for data-oriented design (137 pts)]]></title>
            <link>https://spacetimedb.com/blog/databases-and-data-oriented-design</link>
            <guid>38545417</guid>
            <pubDate>Wed, 06 Dec 2023 15:48:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spacetimedb.com/blog/databases-and-data-oriented-design">https://spacetimedb.com/blog/databases-and-data-oriented-design</a>, See on <a href="https://news.ycombinator.com/item?id=38545417">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: CopilotKit- Build in-app AI chatbots and AI-powered Textareas (123 pts)]]></title>
            <link>https://github.com/CopilotKit/CopilotKit</link>
            <guid>38545207</guid>
            <pubDate>Wed, 06 Dec 2023 15:35:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/CopilotKit/CopilotKit">https://github.com/CopilotKit/CopilotKit</a>, See on <a href="https://news.ycombinator.com/item?id=38545207">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><div dir="auto">
  <themed-picture data-catalyst-inline="true"><picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://github.com/CopilotKit/CopilotKit/assets/746397/5890217b-524e-49c5-a89e-b8743d2acd51">
    <img alt="CopilotKit Logo" src="https://github.com/CopilotKit/CopilotKit/assets/746397/bd5c9079-929b-4d55-bdc9-16d1c8181b71" width="450px">
  </picture></themed-picture>
  
</div>
<p dir="auto">
  <a href="https://discord.gg/6dffbvGU3D" rel="nofollow">
      <img src="https://camo.githubusercontent.com/9e53fcff931b057d78cdd2ff3d11fe9d14ba6592809e4a57291f75384a03625c/68747470733a2f2f646362616467652e76657263656c2e6170702f6170692f7365727665722f366466666276475533443f636f6d706163743d74727565267374796c653d666c6174" alt="Discord" data-canonical-src="https://dcbadge.vercel.app/api/server/6dffbvGU3D?compact=true&amp;style=flat">
  </a>
  <a href="https://github.com/CopilotKit/CopilotKit/actions/workflows/ci.yml">
      <img src="https://github.com/CopilotKit/CopilotKit/actions/workflows/ci.yml/badge.svg" alt="GitHub CI">
  </a>
  <a href="https://www.npmjs.com/package/@copilotkit/react-core" rel="nofollow">
    <img src="https://camo.githubusercontent.com/eccd94d8a2a65d87cf0881a6f3632a1a36e698319cb2435bd8675679bf7a39d1/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f40636f70696c6f746b69742f72656163742d636f7265" alt="NPM" data-canonical-src="https://img.shields.io/npm/v/@copilotkit/react-core">
  <img src="https://camo.githubusercontent.com/64457ba31ad77e9143b4f8f76944a92401a587f6f64dfa59525770aa56e27195/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f436f70696c6f744b69742f436f70696c6f744b6974" alt="MIT" data-canonical-src="https://img.shields.io/github/license/CopilotKit/CopilotKit">
</a></p>
<h2 tabindex="-1" dir="auto">
The Open-Source Copilot Platform
</h2>
<h3 tabindex="-1" dir="auto">
In-app chatbots, and AI-enabled TextArea.
</h3>

 <p dir="auto">
   Questions?
    <a href="https://calendly.com/atai_/copilotkit" rel="nofollow"><strong> Book a call with us  »</strong></a>
    <br>
</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/746397/288405135-1aa17608-46a5-4e2f-aad5-19c8f5c5f1bd.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDE4OTY3MDUsIm5iZiI6MTcwMTg5NjQwNSwicGF0aCI6Ii83NDYzOTcvMjg4NDA1MTM1LTFhYTE3NjA4LTQ2YTUtNGUyZi1hYWQ1LTE5YzhmNWM1ZjFiZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBSVdOSllBWDRDU1ZFSDUzQSUyRjIwMjMxMjA2JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIzMTIwNlQyMTAwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04NmUyMmUzYTlhODc5NzNjOTQ0ZTQ1NDFhNzAxMGExYmRlZGViYzlhODFmYWYzZTI5NDU2ODIyNmJhNjY3ZmMyJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.SkcsNxW2MJFl52Yt-iyqbr0vAxr7Va7SERgmzO-QSXo"><img src="https://private-user-images.githubusercontent.com/746397/288405135-1aa17608-46a5-4e2f-aad5-19c8f5c5f1bd.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDE4OTY3MDUsIm5iZiI6MTcwMTg5NjQwNSwicGF0aCI6Ii83NDYzOTcvMjg4NDA1MTM1LTFhYTE3NjA4LTQ2YTUtNGUyZi1hYWQ1LTE5YzhmNWM1ZjFiZC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBSVdOSllBWDRDU1ZFSDUzQSUyRjIwMjMxMjA2JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIzMTIwNlQyMTAwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT04NmUyMmUzYTlhODc5NzNjOTQ0ZTQ1NDFhNzAxMGExYmRlZGViYzlhODFmYWYzZTI5NDU2ODIyNmJhNjY3ZmMyJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.SkcsNxW2MJFl52Yt-iyqbr0vAxr7Va7SERgmzO-QSXo" height="220px"></a>
</p>

<p dir="auto">🌟 <strong>&lt;CopilotPortal /&gt;:</strong> <br>
Build <strong>in-app AI chatbots</strong> that can "see" the current app state + take action inside your app. <br>
The AI chatbot can talk to your app frontend &amp; backend, and to 3rd party services (Salesforce, Dropbox, etc.) via plugins. <br>
AI "second brain" for your users, on tap.</p>
<p dir="auto">🌟 <strong>&lt;CopilotTextarea /&gt;:</strong> <br>
AI-assisted text generation. Drop-in replacement for any <code>&lt;textarea /&gt;.</code><br>
Autocompletions + AI editing + generate from scratch. Indexed on your users' content.<br>
Starting with React. Use any LLM. <br></p>
<p dir="auto">Combines frontend SDKs, backend SDKs, and (optional) cloud infrastructure. Open-source 🪁</p>
<h2 tabindex="-1" dir="auto">🎯 Features Overview</h2>
<h3 tabindex="-1" dir="auto">CopilotTextarea: AI-assisted text generation + editing.</h3>
<ul dir="auto">
<li>✅ A a drop-in <code>&lt;textarea /&gt;</code> replacement. Supports all <code>&lt;textarea /&gt;</code> customizations.</li>
<li>✅ Context-aware autocompletions ✨ (like in GitHub Copilot)</li>
<li>✅ AI editing ✨ - "list the client's top 3 pain points from the last call using @SalesforceData"</li>
<li>🟩 Generate from scratch ✨ - automatically populate the initial content based on given context</li>
<li>✅ App context &amp; 3rd party context with <code>useMakeCopilotReadable</code> and <code>useMakeCopilotDocumentReadable</code></li>
<li>✅ Fully custsomizable prompt engineering</li>
<li>🟩 Arbitrary LLM chains.</li>
<li>🟩 Bold + italics.</li>
</ul>
<h3 tabindex="-1" dir="auto">Copilot Chatbot: (frontend + backend) runtimes for in-app copilots.</h3>
<ul dir="auto">
<li>✅ Index on frontend app state (via <code>useMakeCopilotReadable</code> and <code>useMakeCopilotDocumentReadable</code>)</li>
<li>🟩 Index on backend state</li>
<li>✅ frontend function calling runtime (in-app actions) (via <code>useMakeCopilotActionable</code>)</li>
<li>🟩 backend function calling runtime (auth enabled)</li>
<li>🟩 Autorun vs. "sensitive" functions (require user approval before execution).</li>
<li>✅ Cursor-style @document-referecing.</li>
<li>✅ Bring your own model</li>
<li>🟩 3rd party plugins</li>
<li>🟩 execute arbitrary LLM chains</li>
<li>🟩 OpenAI <em>assistants</em> api</li>
<li>✅ Fully customize UI</li>
</ul>
<h2 tabindex="-1" dir="auto">Demo</h2>
<p dir="auto"><strong>2-min showcase + 2-min implementation tutorial:</strong></p>
<details open="">
  <summary>
    
    <span aria-label="Video description copilot_full_demo_nxxpbr.3.mp4">copilot_full_demo_nxxpbr.3.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/746397/266699772-b0cdf38b-ec5c-4e95-8623-364bafb70907.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDE4OTY3MDUsIm5iZiI6MTcwMTg5NjQwNSwicGF0aCI6Ii83NDYzOTcvMjY2Njk5NzcyLWIwY2RmMzhiLWVjNWMtNGU5NS04NjIzLTM2NGJhZmI3MDkwNy5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBSVdOSllBWDRDU1ZFSDUzQSUyRjIwMjMxMjA2JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIzMTIwNlQyMTAwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1lM2JlNzViZmFjOTE4NzZiYzgxYTBjNWQzY2NiZWEzYjcxOWE2NTg3NGZlNGJjY2EyMmNhNzA1YzljNTA3ZTVmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.d3kLjQUfKCJq8EetRtGphZDa1HA-7LSgEDEEdw8Ytok" data-canonical-src="https://private-user-images.githubusercontent.com/746397/266699772-b0cdf38b-ec5c-4e95-8623-364bafb70907.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDE4OTY3MDUsIm5iZiI6MTcwMTg5NjQwNSwicGF0aCI6Ii83NDYzOTcvMjY2Njk5NzcyLWIwY2RmMzhiLWVjNWMtNGU5NS04NjIzLTM2NGJhZmI3MDkwNy5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBSVdOSllBWDRDU1ZFSDUzQSUyRjIwMjMxMjA2JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIzMTIwNlQyMTAwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1lM2JlNzViZmFjOTE4NzZiYzgxYTBjNWQzY2NiZWEzYjcxOWE2NTg3NGZlNGJjY2EyMmNhNzA1YzljNTA3ZTVmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.d3kLjQUfKCJq8EetRtGphZDa1HA-7LSgEDEEdw8Ytok" controls="controls" muted="muted">

  </video>
</details>

<h2 tabindex="-1" dir="auto">Installation</h2>
<div dir="auto" data-snippet-clipboard-copy-content="npm i @copilotkit/react-core @copilotkit/react-ui @copilotkit/react-textarea"><pre>npm i @copilotkit/react-core @copilotkit/react-ui @copilotkit/react-textarea</pre></div>
<h2 tabindex="-1" dir="auto">Getting started</h2>
<p dir="auto">See quickstart in the <a href="https://docs.copilotkit.ai/" rel="nofollow">docs</a></p>
<h2 tabindex="-1" dir="auto">Examples</h2>
<h3 tabindex="-1" dir="auto"><code>&lt;CopilotTextarea /&gt;</code></h3>
<p dir="auto">A drop-in &lt;textarea /&gt; replacement with context-aware Copilot autocompletions.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/CopilotKit/CopilotKit/blob/main/assets/CopilotTextarea.gif"><img src="https://github.com/CopilotKit/CopilotKit/raw/main/assets/CopilotTextarea.gif" width="400" height="400" data-animated-image=""></a>
</p>
<h4 tabindex="-1" dir="auto">Features</h4>
<ol dir="auto">
<li>Customizable <code>purpose</code> prompt.</li>
<li>Provide arbitrary context to inform autocompletions using <code>useMakeCopilotReadable</code></li>
<li>Works with any backend/LLM, using <code>ChatlikeApiEndpoint</code></li>
<li>Supports all <code>&lt;textarea /&gt;</code> customizations</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="import &quot;@copilotkit/react-textarea/styles.css&quot;; // add to the app-global css
import { CopilotTextarea } from &quot;@copilotkit/react-textarea&quot;;
import { CopilotProvider } from &quot;@copilotkit/react-core&quot;;

// call ANYWHERE in your app to provide external context (make sure you wrap the app with a <CopilotProvider >):
// See below for more features (parent/child hierarchy, categories, etc.)
useMakeCopilotReadable(relevantInformation)
useMakeCopilotDocumentReadable(document)

return (
  <CopilotProvider chatApiEndpoint=&quot;/api/copilotkit/chat&quot;> {/* Global state &amp; copilot logic. Put this around the entire app */}
    <CopilotTextarea
      className=&quot;p-4 w-1/2 aspect-square font-bold text-3xl bg-slate-800 text-white rounded-lg resize-none&quot;
      placeholder=&quot;A CopilotTextarea!&quot;
      autosuggestionsConfig={{
        purposePrompt: &quot;A COOL &amp; SMOOTH announcement post about CopilotTextarea. Be brief. Be clear. Be cool.&quot;,
        forwardedParams: { // additional arguments to customize autocompletions
          max_tokens: 25,
          stop: [&quot;\n&quot;, &quot;.&quot;, &quot;,&quot;],
        },
      }}
    />
  </CopilotProvider>
);"><pre><span>import</span> <span>"@copilotkit/react-textarea/styles.css"</span><span>;</span> <span>// add to the app-global css</span>
<span>import</span> <span>{</span> <span>CopilotTextarea</span> <span>}</span> <span>from</span> <span>"@copilotkit/react-textarea"</span><span>;</span>
<span>import</span> <span>{</span> <span>CopilotProvider</span> <span>}</span> <span>from</span> <span>"@copilotkit/react-core"</span><span>;</span>

<span>// call ANYWHERE in your app to provide external context (make sure you wrap the app with a &lt;CopilotProvider &gt;):</span>
<span>// See below for more features (parent/child hierarchy, categories, etc.)</span>
<span>useMakeCopilotReadable</span><span>(</span><span>relevantInformation</span><span>)</span>
<span>useMakeCopilotDocumentReadable</span><span>(</span><span>document</span><span>)</span>

<span>return</span> <span>(</span>
  <span>&lt;</span><span>CopilotProvider</span><span></span> <span>chatApiEndpoint</span><span>=</span><span>"/api/copilotkit/chat"</span><span>&gt;</span> <span>{</span><span>/* Global state &amp; copilot logic. Put this around the entire app */</span><span>}</span>
    <span>&lt;</span><span>CopilotTextarea</span>
      <span>className</span><span>=</span><span>"p-4 w-1/2 aspect-square font-bold text-3xl bg-slate-800 text-white rounded-lg resize-none"</span>
      <span>placeholder</span><span>=</span><span>"A CopilotTextarea!"</span>
      <span>autosuggestionsConfig</span><span>=</span><span>{</span><span>{</span>
        <span>purposePrompt</span>: <span>"A COOL &amp; SMOOTH announcement post about CopilotTextarea. Be brief. Be clear. Be cool."</span><span></span><span>,</span>
        <span>forwardedParams</span>: <span>{</span> <span>// additional arguments to customize autocompletions</span>
          <span>max_tokens</span>: <span>25</span><span>,</span>
          <span>stop</span>: <span>[</span><span>"\n"</span><span>,</span> <span>"."</span><span>,</span> <span>","</span><span>]</span><span>,</span>
        <span>}</span><span></span><span>,</span>
      <span>}</span><span>}</span>
    <span>/</span><span>&gt;</span>
  <span>&lt;</span><span>/</span><span>CopilotProvider</span><span>&gt;</span>
<span>)</span><span>;</span></pre></div>
<h3 tabindex="-1" dir="auto">Integrate copilot</h3>
<div dir="auto" data-snippet-clipboard-copy-content="import &quot;@copilotkit/react-ui/styles.css&quot;; // add to the app-global css
import { CopilotProvider } from &quot;@copilotkit/react-core&quot;;
import { CopilotSidebarUIProvider } from &quot;@copilotkit/react-ui&quot;;

export default function App(): JSX.Element {
  return (
  <CopilotProvider chatApiEndpoint=&quot;/api/copilotkit/chat&quot;> {/* Global state &amp; copilot logic. Put this around the entire app */}
      <CopilotSidebarUIProvider> {/* A built-in Copilot UI (or bring your own UI). Put around individual pages, or the entire app. */}

        <YourContent />

      </CopilotSidebarUIProvider>
    </CopilotProvider>
  );
}"><pre><span>import</span> <span>"@copilotkit/react-ui/styles.css"</span><span>;</span> <span>// add to the app-global css</span>
<span>import</span> <span>{</span> <span>CopilotProvider</span> <span>}</span> <span>from</span> <span>"@copilotkit/react-core"</span><span>;</span>
<span>import</span> <span>{</span> <span>CopilotSidebarUIProvider</span> <span>}</span> <span>from</span> <span>"@copilotkit/react-ui"</span><span>;</span>

<span>export</span> <span>default</span> <span>function</span> <span>App</span><span>(</span><span>)</span>: <span>JSX</span><span>.</span><span>Element</span> <span>{</span>
  <span>return</span> <span>(</span>
  <span>&lt;</span><span>CopilotProvider</span><span></span> <span>chatApiEndpoint</span><span>=</span><span>"/api/copilotkit/chat"</span><span>&gt;</span> <span>{</span><span>/* Global state &amp; copilot logic. Put this around the entire app */</span><span>}</span>
      <span>&lt;</span><span>CopilotSidebarUIProvider</span><span>&gt;</span> <span>{</span><span>/* A built-in Copilot UI (or bring your own UI). Put around individual pages, or the entire app. */</span><span>}</span>

        <span>&lt;</span><span>YourContent</span> <span>/</span><span>&gt;</span>

      <span>&lt;</span><span>/</span><span>CopilotSidebarUIProvider</span><span>&gt;</span>
    <span>&lt;</span><span>/</span><span>CopilotProvider</span><span>&gt;</span>
  <span>)</span><span>;</span>
<span>}</span></pre></div>
<h4 tabindex="-1" dir="auto">Features</h4>
<ol dir="auto">
<li>Batteries included. Add 2 React components, and your Copilot is live.</li>
<li>Customize the built-in <code>CopilotSidebarUIProvider</code> UI -- or bring your own UI component.</li>
<li>Extremely hackable. Should the need arise, you can define 1st-class extensions just as powerful as <code>useMakeCopilotReadable</code>, <code>useMakeCopilotActionable</code>, etc.</li>
</ol>
<h3 tabindex="-1" dir="auto">Give the copilot read permissions</h3>
<h4 tabindex="-1" dir="auto">Features</h4>
<ol dir="auto">
<li>Propagate useful information &amp; granular app-state to the Copilot</li>
<li>Easily maintain the hierarchical structure of information with <code>parentId</code></li>
<li>One call to rule them all: <code>useMakeCopilotReadable</code> works both with the sidekick, and with CopilotTextarea.
<ul dir="auto">
<li>Use the <code>contextCategories: string[]</code> param to route information to different places.</li>
</ul>
</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="import { useMakeCopilotReadable } from &quot;@copilotkit/react-core&quot;;


function Employee(props: EmployeeProps): JSX.Element {
  const { employeeName, workProfile, metadata } = props;

  // propagate any information copilot
  const employeeContextId = useMakeCopilotReadable(employeeName);

  // Pass a parentID to maintain a hiearchical structure.
  // Especially useful with child React components, list elements, etc.
  useMakeCopilotReadable(workProfile.description(), employeeContextId);
  useMakeCopilotReadable(metadata.description(), employeeContextId);
  
  return (
    // Render as usual...
  );
}
"><pre><span>import</span> <span>{</span> <span>useMakeCopilotReadable</span> <span>}</span> <span>from</span> <span>"@copilotkit/react-core"</span><span>;</span>


<span>function</span> <span>Employee</span><span>(</span><span>props</span>: <span>EmployeeProps</span><span>)</span>: <span>JSX</span><span>.</span><span>Element</span> <span>{</span>
  <span>const</span> <span>{</span> employeeName<span>,</span> workProfile<span>,</span> metadata <span>}</span> <span>=</span> <span>props</span><span>;</span>

  <span>// propagate any information copilot</span>
  <span>const</span> <span>employeeContextId</span> <span>=</span> <span>useMakeCopilotReadable</span><span>(</span><span>employeeName</span><span>)</span><span>;</span>

  <span>// Pass a parentID to maintain a hiearchical structure.</span>
  <span>// Especially useful with child React components, list elements, etc.</span>
  <span>useMakeCopilotReadable</span><span>(</span><span>workProfile</span><span>.</span><span>description</span><span>(</span><span>)</span><span>,</span> <span>employeeContextId</span><span>)</span><span>;</span>
  <span>useMakeCopilotReadable</span><span>(</span><span>metadata</span><span>.</span><span>description</span><span>(</span><span>)</span><span>,</span> <span>employeeContextId</span><span>)</span><span>;</span>
  
  <span>return</span> <span>(</span>
    <span>// Render as usual...</span>
  <span>)</span><span>;</span>
<span>}</span></pre></div>
<h3 tabindex="-1" dir="auto">Give the copilot write permissions</h3>
<div dir="auto" data-snippet-clipboard-copy-content="import { useMakeCopilotActionable } from &quot;@copilotkit/react-core&quot;;

function Department(props: DepartmentProps): JSX.Element {
  // ...

  // Let the copilot take action on behalf of the user.
  useMakeCopilotActionable(
    {
      name: &quot;setEmployeesAsSelected&quot;,
      description: &quot;Set the given employees as 'selected'&quot;,
      argumentAnnotations: [
        {
          name: &quot;employeeIds&quot;,
          type: &quot;array&quot;, items: { type: &quot;string&quot; }
          description: &quot;The IDs of employees to set as selected&quot;,
          required: true
        }
      ],
      implementation: async (employeeIds) => setEmployeesAsSelected(employeeIds),
    },
    []
  );

  // ...
}"><pre><span>import</span> <span>{</span> <span>useMakeCopilotActionable</span> <span>}</span> <span>from</span> <span>"@copilotkit/react-core"</span><span>;</span>

<span>function</span> <span>Department</span><span>(</span><span>props</span>: <span>DepartmentProps</span><span>)</span>: <span>JSX</span><span>.</span><span>Element</span> <span>{</span>
  <span>// ...</span>

  <span>// Let the copilot take action on behalf of the user.</span>
  <span>useMakeCopilotActionable</span><span>(</span>
    <span>{</span>
      <span>name</span>: <span>"setEmployeesAsSelected"</span><span>,</span>
      <span>description</span>: <span>"Set the given employees as 'selected'"</span><span>,</span>
      <span>argumentAnnotations</span>: <span>[</span>
        <span>{</span>
          <span>name</span>: <span>"employeeIds"</span><span>,</span>
          <span>type</span>: <span>"array"</span><span>,</span> <span>items</span>: <span>{</span> <span>type</span>: <span>"string"</span> <span>}</span>
          <span>description</span>: <span>"The IDs of employees to set as selected"</span><span>,</span>
          <span>required</span>: <span>true</span>
        <span>}</span>
      <span>]</span><span>,</span>
      <span>implementation</span>: <span>async</span> <span>(</span><span>employeeIds</span><span>)</span> <span>=&gt;</span> <span>setEmployeesAsSelected</span><span>(</span><span>employeeIds</span><span>)</span><span>,</span>
    <span>}</span><span>,</span>
    <span>[</span><span>]</span>
  <span>)</span><span>;</span>

  <span>// ...</span>
<span>}</span></pre></div>
<h4 tabindex="-1" dir="auto">Features</h4>
<ol dir="auto">
<li>Plain typescript actions. Edit a textbox, navigate to a new page, or anythign you can think of.</li>
<li>Specify arbitrary input types.</li>
</ol>
<h2 tabindex="-1" dir="auto">Near-Term Roadmap</h2>
<h3 tabindex="-1" dir="auto">📊 Please vote on features via the Issues tab!</h3>
<h3 tabindex="-1" dir="auto">Copilot-App Interaction</h3>
<ul dir="auto">
<li>✅ <code>useMakeCopilotReadable</code>: give static information to the copilot, in sync with on-screen state</li>
<li>✅ <code>useMakeCopilotActionable</code>: Let the copilot take action on behalf of the user</li>
<li>🚧 <code>useMakeCopilotAskable</code>: let the copilot ask for additional information when needed (coming soon)</li>
<li>🚧 <code>useEditCopilotMessage</code>: edit the (unsent) typed user message to the copilot (coming soon)</li>
<li>🚧 copilot-assisted navigation: go to the best page to achieve some objective.</li>
<li>🚧 CopilotCloudKit: integrate arbitrary LLM logic / chains / RAG, using plain code.</li>
</ul>
<h3 tabindex="-1" dir="auto">UI components</h3>
<ul dir="auto">
<li>✅ <code>&lt;CopilotSidebarUIProvider&gt;</code>: Built in, hackable Copilot UI (optional - you can bring your own UI).</li>
<li>✅ <code>&lt;CopilotTextarea /&gt;</code>: drop-in <code>&lt;textarea /&gt;</code> replacement with Copilot autocompletions.</li>
</ul>
<h3 tabindex="-1" dir="auto">Integrations</h3>
<ul dir="auto">
<li>✅ Vercel AI SDK</li>
<li>✅ OpenAI APIs</li>
<li>🚧 Langchain</li>
<li>🚧 Additional LLM providers</li>
</ul>
<h3 tabindex="-1" dir="auto">Frameworks</h3>
<ul dir="auto">
<li>✅ React</li>
<li>🚧 Vue</li>
<li>🚧 Svelte</li>
<li>🚧 Swift (Mac + iOS)</li>
</ul>
<h2 tabindex="-1" dir="auto">Contribute</h2>
<p dir="auto">Contributions are welcome! 🎉</p>
<p dir="auto"><a href="https://discord.gg/6dffbvGU3D" rel="nofollow">Join the Discord</a>
<a href="https://discord.gg/6dffbvGU3D" rel="nofollow"><img src="https://camo.githubusercontent.com/9e53fcff931b057d78cdd2ff3d11fe9d14ba6592809e4a57291f75384a03625c/68747470733a2f2f646362616467652e76657263656c2e6170702f6170692f7365727665722f366466666276475533443f636f6d706163743d74727565267374796c653d666c6174" alt="Discord" data-canonical-src="https://dcbadge.vercel.app/api/server/6dffbvGU3D?compact=true&amp;style=flat"></a></p>

<h2 tabindex="-1" dir="auto">Contact</h2>
<p dir="auto">atai <code>&lt;at&gt;</code> copilotkit.ai</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Just about every Windows/Linux device vulnerable to new LogoFAIL firmware attack (202 pts)]]></title>
            <link>https://arstechnica.com/security/2023/12/just-about-every-windows-and-linux-device-vulnerable-to-new-logofail-firmware-attack/</link>
            <guid>38545022</guid>
            <pubDate>Wed, 06 Dec 2023 15:23:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/security/2023/12/just-about-every-windows-and-linux-device-vulnerable-to-new-logofail-firmware-attack/">https://arstechnica.com/security/2023/12/just-about-every-windows-and-linux-device-vulnerable-to-new-logofail-firmware-attack/</a>, See on <a href="https://news.ycombinator.com/item?id=38545022">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/computer-power-button-800x534.jpg" alt="Just about every Windows and Linux device vulnerable to new LogoFAIL firmware attack">
      <figcaption><p>Getty Images</p></figcaption>  </figure>

  




<!-- cache miss 456:single/related:310c989e205eee0f590738e2cb404abd --><!-- empty -->
<p>Hundreds of Windows and Linux computer models from virtually all hardware makers are vulnerable to a new attack that executes malicious firmware early in the boot-up sequence, a feat that allows infections that are nearly impossible to detect or remove using current defense mechanisms.</p>
<p>The attack—dubbed LogoFAIL by the researchers who devised it—is notable for the relative ease in carrying it out, the breadth of both consumer- and enterprise-grade models that are susceptible, and the high level of control it gains over them. In many cases, LogoFAIL can be remotely executed in post-exploit situations using techniques that can’t be spotted by traditional endpoint security products. And because exploits run during the earliest stages of the boot process, they are able to bypass a host of defenses, including the industry-wide Secure Boot, Intel’s Secure Boot, and similar protections from other companies that are devised to prevent so-called bootkit infections.</p>
<h2>Game over for platform security</h2>
<p>LogoFAIL is a constellation of two dozen newly discovered vulnerabilities that have lurked for years, if not decades, in Unified Extensible Firmware Interfaces responsible for booting modern devices that run Windows or Linux. The vulnerabilities are the product of almost a year’s worth of work by Binarly, a firm that helps customers identify and secure vulnerable firmware.</p>
<p>The vulnerabilities are the subject of a coordinated mass disclosure released Wednesday. The participating companies comprise nearly the entirety of the x64 and ARM CPU ecosystem, starting with UEFI suppliers AMI, Insyde, and Phoenix (sometimes still called IBVs or independent BIOS vendors); device manufacturers such as Lenovo, Dell, and HP; and the makers of the CPUs that go inside the devices, usually Intel, AMD or designers of ARM CPUs. The researchers unveiled the attack on Wednesday at the Black Hat Security Conference in London.</p>                                            
                                                        
<p>The affected parties are releasing advisories that disclose which of their products are vulnerable and where to obtain security patches. Links to advisories and a list of vulnerability designations appears at the end of this article.</p>
<p>As its name suggests, LogoFAIL involves logos, specifically those of the hardware seller that are displayed on the device screen early in the boot process, while the UEFI is still running. Image parsers in UEFIs from all three major IBVs are riddled with roughly a dozen critical vulnerabilities that have gone unnoticed until now. By replacing the legitimate logo images with identical-looking ones that have been specially crafted to exploit these bugs, LogoFAIL makes it possible to execute malicious code at the most sensitive stage of the boot process, which is known as DXE, short for Driver Execution Environment.</p>
<p>“Once arbitrary code execution is achieved during the DXE phase, it’s game over for platform security,” researchers from Binarly, the security firm that discovered the vulnerabilities, wrote in a whitepaper. “From this stage, we have full control over the memory and the disk of the target device, thus including the operating system that will be started.”</p>
<p>From there, LogoFAIL can deliver a second-stage payload that drops an executable onto the hard drive before the main OS has even started. The following video demonstrates a proof-of-concept exploit created by the researchers. The infected device—a Gen 2 Lenovo ThinkCentre M70s running an 11th-Gen Intel Core with a UEFI released in June—runs standard firmware defenses, including Secure Boot and Intel Boot Guard.</p>
<figure><p><iframe type="text/html" width="560" height="315" src="https://www.youtube.com/embed/EufeOPe6eqk?si=Mf2uRls9-tVytbJx?start=0&amp;wmode=transparent" frameborder="0" allowfullscreen=""></iframe></p><figcaption><p>LogoFAIL.</p></figcaption></figure>
<p>In an email, Binarly founder and CEO Alex Matrosov wrote:</p>
<blockquote><p>LogoFAIL is a newly discovered set of high-impact security vulnerabilities affecting different image parsing libraries used in the system firmware by various vendors during the device boot process. These vulnerabilities are present in most cases inside reference code, impacting not a single vendor but the entire ecosystem across this code and device vendors where it is used. This attack can give a threat actor an advantage in bypassing most endpoint security solutions and delivering a stealth firmware bootkit that will persist in a firmware capsule with a modified logo image.</p></blockquote>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Enabling next-generation AI workloads: Announcing TPU v5p and AI Hypercomputer (144 pts)]]></title>
            <link>https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer</link>
            <guid>38544824</guid>
            <pubDate>Wed, 06 Dec 2023 15:10:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer">https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer</a>, See on <a href="https://news.ycombinator.com/item?id=38544824">Hacker News</a></p>
<div id="readability-page-1" class="page"><div jsname="tx2NYc"><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Generative AI (gen AI) models are rapidly evolving, offering unparalleled sophistication and capability. This advancement empowers enterprises and developers across various industries to solve complex problems and unlock new opportunities. However, the growth in gen AI models — <a href="https://cloud.google.com/blog/products/compute/announcing-cloud-tpu-v5e-and-a3-gpus-in-ga">with a tenfold increase in parameters annually over the past five years</a> — brings heightened requirements for training, tuning, and inference. Today's larger models, featuring hundreds of billions or even trillions of parameters, require extensive training periods, sometimes spanning months, even on the most specialized systems. Additionally, efficient AI workload management necessitates a coherently integrated AI stack consisting of optimized compute, storage, networking, software and development frameworks.</p><p>Today, to address these challenges, we are excited to announce Cloud TPU v5p, our most powerful, scalable, and flexible AI accelerator thus far. TPUs have long been the basis for training and serving AI-powered products like YouTube, Gmail, Google Maps, Google Play, and Android. In fact, Gemini, Google’s most capable and general AI model <a href="https://blog.google/technology/ai/google-gemini-ai" target="_blank">announced today</a>, was trained on, and is served, using TPUs.</p><p>In addition, we are also announcing AI Hypercomputer from Google Cloud, a groundbreaking supercomputer architecture that employs an integrated system of performance-optimized hardware, open software, leading ML frameworks, and flexible consumption models. Traditional methods often tackle demanding AI workloads through piecemeal, component-level enhancements, which can lead to inefficiencies and bottlenecks. In contrast, AI Hypercomputer employs systems-level codesign to boost efficiency and productivity across AI training, tuning, and serving.</p></span></section><div jsaction="rcuQ6b:npT2md" jscontroller="wJu6E" data-video-url="https://www.youtube.com/watch?v=hszd5UqnfLk"><picture><section><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Thumbnail_-_AI_Infra_Launch_v1.max-2000x2000.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/Thumbnail_-_AI_Infra_Launch_v1.max-2000x2000.png" loading="lazy"></section></picture></div><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><h3><b>Inside Cloud TPU v5p, our most powerful and scalable TPU accelerator to date</b></h3><p>Earlier this year, <a href="https://cloud.google.com/blog/products/compute/announcing-cloud-tpu-v5e-in-ga">we announced</a> the general availability of Cloud TPU v5e. With 2.3X price performance improvements over the previous generation TPU v4<sup>1</sup>, it is our most <i>cost-efficient</i> TPU to date. By contrast, Cloud TPU v5p, is our most <i>powerful</i> TPU thus far. Each TPU v5p pod <b>composes together 8,960 chips</b> over our <b>highest-bandwidth inter-chip interconnect (ICI) at 4,800 Gbps/chip in a 3D torus topology</b>. Compared to TPU v4, TPU v5p features more than<b> 2X greater FLOPS and 3X more high-bandwidth memory (HBM)</b>.</p><p>Designed for performance, flexibility, and scale, TPU v5p can <b>train large LLM models 2.8X faster</b> than the previous-generation TPU v4. Moreover, with second-generation <a href="https://cloud.google.com/tpu">SparseCores</a>, TPU v5p can <b>train embedding-dense models 1.9X faster</b> than TPU v4<sup>2</sup>.</p></span></section><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_next-generation_AI_workloads.max-2000x2000.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_next-generation_AI_workloads.max-2000x2000.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><p><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Source: TPU v5p and v4 are based on Google Internal Data. As of November, 2023: All numbers normalized per chip seq-len=2048 for GPT-3 175 billion parameter model.</p></span></p></section><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_next-generation_AI_workloads.max-2000x2000.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_next-generation_AI_workloads.max-2000x2000.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><p><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Source: TPU v5e data is from MLPerf™ 3.1 Training Closed results for v5e. TPU v5p and v4 are based on Google internal training runs. As of November, 2023: All numbers normalized per chip seq-len=2048 for GPT-3 175 billion parameter model. It shows relative performance per dollar using the public list price of TPU v4 ($3.22/chip/hour), TPU v5e ( $1.2/chip/hour) and TPU v5p ($4.2/chip/hour).</p></span></p></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>In addition to performance improvements, <b>TPU v5p is also 4X more scalable than TPU v4 in terms of total available FLOPs per pod.</b> Doubling the floating-point operations per second (FLOPS) over TPU v4 and doubling the number of chips in a single pod provides considerable improvement in relative performance in training speed.</p></span></section><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/3_next-generation_AI_workloads_v1.max-2000x2000.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/3_next-generation_AI_workloads_v1.max-2000x2000.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><h3><b>Google AI Hypercomputer delivers peak performance and efficiency at large scale</b></h3><p>Achieving both scale and speed is necessary, but not sufficient to meet the needs of modern AI/ML applications and services. The hardware and software components must come together into an integrated, easy-to-use, secure, and reliable computing system. At Google, we’ve done decades of research and development on this very problem, culminating in AI Hypercomputer, a system of technologies optimized to work in concert to enable modern AI workloads.</p></span></section><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/4_next-generation_AI_workloads.max-800x800.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/4_next-generation_AI_workloads.max-800x800.png" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><ul><li><b>Performance-optimized hardware:</b> AI Hypercomputer features performance-optimized compute, storage, and networking built over an ultrascale data center infrastructure, leveraging a high-density footprint, liquid cooling, and our <a href="https://cloud.google.com/blog/topics/systems/the-evolution-of-googles-jupiter-data-center-network">Jupiter data center network</a> technology. All of this is predicated on technologies that are built with <a href="https://www.google.com/about/datacenters/efficiency/" target="_blank">efficiency</a> at their core; leveraging <a href="https://cloud.google.com/blog/topics/sustainability/a-smarter-way-to-buy-clean-energy">clean energy</a> and <a href="https://blog.google/outreach-initiatives/sustainability/replenishing-water/?_ga=2.140272307.1460901017.1631498684-1474825438.1628277680" target="_blank">a deep commitment to water stewardship</a>, and that are <a href="https://blog.google/outreach-initiatives/sustainability/our-third-decade-climate-action-realizing-carbon-free-future/" target="_blank">helping us move toward a carbon-free future</a>.</li><li><b>Open software:</b> AI Hypercomputer enables developers to access our performance-optimized hardware through the use of open software to tune, manage, and dynamically orchestrate AI training and inference workloads on top of performance-optimized AI hardware.<ul><li>Extensive support for popular ML frameworks such as JAX, TensorFlow, and PyTorch are available right out of the box. Both JAX and PyTorch are powered by <a href="https://github.com/openxla/xla" target="_blank">OpenXLA</a> compiler for building sophisticated LLMs. XLA serves as a foundational backbone, enabling the creation of complex multi-layered models (<a href="https://pytorch.org/blog/high-performance-llama-2/" target="_blank">Llama 2 training and inference on Cloud TPUs with PyTorch/XLA</a>). It optimizes distributed architectures across a wide range of hardware platforms, ensuring easy-to-use and efficient model development for diverse AI use cases (<a href="https://cloud.google.com/blog/products/compute/assemblyai-on-cloud-tpu-v5e-price-performance">AssemblyAI leverages JAX/XLA and Cloud TPUs for large-scale AI speech</a>).</li><li>Open and unique <a href="https://cloud.google.com/blog/products/compute/using-cloud-tpu-multislice-to-scale-ai-workloads">Multislice Training</a> and <a href="https://cloud.google.com/tpu/docs/v5e-inference">Multihost Inferencing</a> software, respectively, make scaling, training, and serving workloads smooth and easy. Developers can scale to tens of thousands of chips to support demanding AI workloads.</li><li>Deep integration with <a href="https://cloud.google.com/kubernetes-engine?hl=en">Google Kubernetes Engine (GKE)</a> and <a href="https://cloud.google.com/compute?hl=en">Google Compute Engine</a>, to deliver efficient resource management, consistent ops environments, autoscaling, node-pool auto-provisioning, auto-checkpointing, auto-resumption, and timely failure recovery.</li></ul></li><li><b>Flexible consumption</b>: AI Hypercomputer offers a wide range of flexible and dynamic consumption choices. In addition to classic options, such as Committed Use Discounts (CUD), on-demand pricing, and spot pricing, AI Hypercomputer provides consumption models tailored for AI workloads via <a href="https://cloud.google.com/blog/products/compute/introducing-dynamic-workload-scheduler">Dynamic Workload Scheduler.</a> Dynamic Workload Scheduler introduces two models: Flex Start mode for higher resource obtainability and optimized economics, as well as Calendar mode, which targets workloads with higher predictability on job-start times.</li></ul><h3><b>Leveraging Google’s deep experience to help power the future of AI</b></h3><p>Customers like Salesforce and Lightricks are already training and serving large AI models with Google Cloud’s TPU v5p AI Hypercomputer — and already seeing a difference:</p><p><i>“We’ve been leveraging Google Cloud TPU v5p for pre-training Salesforce’s foundational models that will serve as the core engine for specialized production use cases, and we’re seeing considerable improvements in our training speed. In fact, Cloud TPU v5p compute outperforms the previous generation TPU v4 by as much as 2X. We also love how seamless and easy the transition has been from Cloud TPU v4 to v5p using JAX. We’re excited to take these speed gains even further by leveraging the native support for INT8 precision format via the Accurate Quantized Training (AQT) library to optimize our models.” -</i> Erik Nijkamp, Senior Research Scientist, Salesforce</p><p><i>“Leveraging the remarkable performance and ample memory capacity of Google Cloud TPU v5p, we successfully trained our generative text-to-video model without splitting it into separate processes. This optimal hardware utilization significantly accelerates each training cycle, allowing us to swiftly conduct a series of experiments. The ability to train our model quickly in each experiment facilitates rapid iteration, which is an invaluable advantage for our research team in this competitive field of generative AI.”</i> - Yoav HaCohen, PhD, Core Generative AI Research Team Lead, Lightricks</p><p><i>“In our early-stage usage, Google DeepMind and Google Research have observed 2X speedups for LLM training workloads using TPU v5p chips compared to the performance on our TPU v4 generation. The robust support for ML Frameworks (JAX, PyTorch, TensorFlow) and orchestration tools enables us to scale even more efficiently on v5p. With the 2nd generation of SparseCores we also see significant improvement in the performance of embeddings-heavy workloads. TPUs are vital to enabling our largest-scale research and engineering efforts on cutting edge models like Gemini.” -</i> Jeff Dean, Chief Scientist, Google DeepMind and Google Research</p><p>At Google, we’ve long believed in the power of AI to help solve challenging problems. Until very recently, training large foundation models and serving them at scale was too complicated and expensive for many organizations. Today, with Cloud TPU v5p and AI Hypercomputer, we’re excited to extend the result of decades of research in AI and systems design with our customers, so they can innovate with AI faster, more efficiently, and more cost effectively.</p><p>To request access to Cloud TPU v5p and AI Hypercomputer, please reach out to your <a href="https://cloud.google.com/contact/">Google Cloud account manager</a>. To learn more about Google Cloud’s AI infrastructure, <a href="https://cloudonair.withgoogle.com/events/summit-applied-ml-summit-23" target="_blank">register to attend Google Cloud Applied AI Summit</a>.</p><hr><p><i><sup>1: MLPerf™ v3.1 Training Closed, multiple benchmarks as shown. Retrieved November 8th, 2023 from</sup></i> <a href="http://mlcommons.org/" target="_blank"><i><sup>mlcommons.org</sup></i></a><i><sup>. Results 3.1-2004. Performance per dollar is not an MLPerf metric. TPU v4 results are unverified: not verified by MLCommons Association. The MLPerf™ name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See</sup></i> <a href="http://www.mlcommons.org/" target="_blank"><i><sup>www.mlcommons.org</sup></i></a> <i><sup>for more information.<br>2: Google Internal Data for TPU v5p as of November, 2023: E2E steptime, SearchAds pCTR, batch size per TPU core 16,384, 125 vp5 chips</sup></i></p></span></section><section><span>Posted in</span><ul><li><a href="https://cloud.google.com/blog/products/ai-machine-learning" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/ai-machine-learning" track-metadata-module="tag list" track-metadata-module_headline="posted in">AI &amp; Machine Learning</a></li><li><a href="https://cloud.google.com/blog/products/compute" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/compute" track-metadata-module="tag list" track-metadata-module_headline="posted in">Compute</a></li><li><a href="https://cloud.google.com/blog/products/infrastructure-modernization" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/infrastructure-modernization" track-metadata-module="tag list" track-metadata-module_headline="posted in">Infrastructure Modernization</a></li><li><a href="https://cloud.google.com/blog/topics/systems" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/topics/systems" track-metadata-module="tag list" track-metadata-module_headline="posted in">Systems</a></li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini: Google's most capable AI model yet (763 pts)]]></title>
            <link>https://blog.google/technology/ai/google-gemini-ai/</link>
            <guid>38544746</guid>
            <pubDate>Wed, 06 Dec 2023 15:05:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/ai/google-gemini-ai/">https://blog.google/technology/ai/google-gemini-ai/</a>, See on <a href="https://news.ycombinator.com/item?id=38544746">Hacker News</a></p>
<div id="readability-page-1" class="page"><article ng-init="drawerToggle = {'open': true}">

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
  }">
      
      
        <p>
          Making AI more helpful for everyone
        </p>
      
    </div>

    

    
      







<div>
    <figure>
      <div>
  <p><img srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_header.width-600.format-webp.webp 600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_header.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_header.width-1600.format-webp.webp 1600w" sizes="(max-width: 599px) 100vw, (max-width: 1023px) 600px, 1024px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_header.width-1200.format-webp.webp" fetchpriority="high" alt="The word “Gemini” above five separate threads, each a different color, converge from the left into a three-dimensional central helix before separating back out toward the right into five individual strands once more.">
  </p>
</div>

      
    </figure>
  </div>


    

    
    <div>
        
          
            <div data-component="uni-article-jumplinks" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Jumplinks&quot;,
    &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
  }">
  <nav aria-label="Article Jumplinks">
    <p><span>In this story</span>
    </p>
    
    
    <div>
      <ul id="article-jumplinks__list">
        
        <li>
          <a aria-label="link to Note from Sundar" href="#sundar-note" id="sundar-note-anchor">Note from Sundar</a>
        </li>
        
        <li>
          <a aria-label="link to Introducing Gemini" href="#introducing-gemini" id="introducing-gemini-anchor">Introducing Gemini</a>
        </li>
        
        <li>
          <a aria-label="link to State-of-the-art performance" href="#performance" id="performance-anchor">State-of-the-art performance</a>
        </li>
        
        <li>
          <a aria-label="link to Next-generation capabilities" href="#capabilities" id="capabilities-anchor">Next-generation capabilities</a>
        </li>
        
        <li>
          <a aria-label="link to Scalable and efficient" href="#scalable-efficient" id="scalable-efficient-anchor">Scalable and efficient</a>
        </li>
        
        <li>
          <a aria-label="link to Responsibility and safety" href="#responsibility-safety" id="responsibility-safety-anchor">Responsibility and safety</a>
        </li>
        
        <li>
          <a aria-label="link to Availability" href="#availability" id="availability-anchor">Availability</a>
        </li>
        
      </ul>
    </div>
    
  </nav>
</div>
          
          
          <div data-reading-time="true" data-component="uni-drop-cap|uni-tombstone">
            
            
<!--article text-->

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><p data-block-key="xdxwr"><i>A note from Google and Alphabet CEO Sundar Pichai:</i></p><p data-block-key="6590u">Every technology shift is an opportunity to advance scientific discovery, accelerate human progress, and improve lives. I believe the transition we are seeing right now with AI will be the most profound in our lifetimes, far bigger than the shift to mobile or to the web before it. AI has the potential to create opportunities — from the everyday to the extraordinary — for people everywhere. It will bring new waves of innovation and economic progress and drive knowledge, learning, creativity and productivity on a scale we haven’t seen before.</p><p data-block-key="3ffsc">That’s what excites me: the chance to make AI helpful for everyone, everywhere in the world.</p><p data-block-key="6sa5">Nearly eight years into our journey as an AI-first company, the pace of progress is only accelerating: Millions of people are now using generative AI across our products to do things they couldn’t even a year ago, from finding answers to more complex questions to using new tools to collaborate and create. At the same time, developers are using our models and infrastructure to build new generative AI applications, and startups and enterprises around the world are growing with our AI tools.</p><p data-block-key="fafvp">This is incredible momentum, and yet, we’re only beginning to scratch the surface of what’s possible.</p><p data-block-key="chghs">We’re approaching this work boldly and responsibly. That means being ambitious in our research and pursuing the capabilities that will bring enormous benefits to people and society, while building in safeguards and working collaboratively with governments and experts to address risks as AI becomes more capable. And we continue to invest in the very best tools, foundation models and infrastructure and bring them to our products and to others, guided by our <a href="https://ai.google/responsibility/principles/" rt-link-type="external">AI Principles</a>.</p><p data-block-key="akdk1">Now, we’re taking the next step on our journey with Gemini, our most capable and general model yet, with state-of-the-art performance across many leading benchmarks. Our first version, Gemini 1.0, is optimized for different sizes: Ultra, Pro and Nano. These are the first models of the Gemini era and the first realization of the vision we had when we formed Google DeepMind earlier this year. This new era of models represents one of the biggest science and engineering efforts we’ve undertaken as a company. I’m genuinely excited for what’s ahead, and for the opportunities Gemini will unlock for people everywhere.</p><p data-block-key="87ult">– Sundar</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="ede3f">Introducing Gemini</h2><p data-block-key="copss"><i>By Demis Hassabis, CEO and Co-Founder of Google DeepMind, on behalf of the Gemini team</i></p><p data-block-key="ff9rj">AI has been the focus of my life's work, as for many of my research colleagues. Ever since programming AI for computer games as a teenager, and throughout my years as a neuroscience researcher trying to understand the workings of the brain, I’ve always believed that if we could build smarter machines, we could harness them to benefit humanity in incredible ways.</p><p data-block-key="i7i7">This promise of a world responsibly empowered by AI continues to drive our work at Google DeepMind. For a long time, we’ve wanted to build a new generation of AI models, inspired by the way people understand and interact with the world. AI that feels less like a smart piece of software and more like something useful and intuitive — an expert helper or assistant.</p><p data-block-key="98mu4">Today, we’re a step closer to this vision as <a href="https://deepmind.google/technologies/gemini" rt-link-type="external">we introduce Gemini</a>, the most capable and general model we’ve ever built.</p><p data-block-key="aka6e">Gemini is the result of large-scale collaborative efforts by teams across Google, including our colleagues at Google Research. It was built from the ground up to be multimodal, which means it can generalize and seamlessly understand, operate across and combine different types of information including text, code, audio, image and video.</p></div>
  

  
    
  
    


<div data-component="uni-article-yt-player" data-page-title="Introducing Gemini: our largest and most capable AI model" data-video-id="jV1vkHv4zq8" data-index-id="5" data-analytics-module="{
    &quot;module_name&quot;: &quot;Youtube Video&quot;,
    &quot;section_header&quot;: &quot;undefined&quot;
  }">

    

    <a role="video" tabindex="0">
      <div>
        
          
          
          
          <p><img alt="Introducing Gemini: our largest and most capable AI model." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/06_Foundation_01.width-100.format-webp.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/06_Foundation_01.width-500.format-webp.webp&quot;,
                &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/06_Foundation_01.width-1000.format-webp.webp&quot;
              }"></p>

        
        <svg role="presentation">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button_no_hole"></use>
          
        </svg>
        <svg role="img">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button"></use>
          
        </svg>

        
        
        
        
      </div>
    </a>

    
      <p>Introducing Gemini: our largest and most capable AI model</p>
    

    
  </div>

  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><p data-block-key="39bmn">Gemini is also our most flexible model yet — able to efficiently run on everything from data centers to mobile devices. Its state-of-the-art capabilities will significantly enhance the way developers and enterprise customers build and scale with AI.</p><p data-block-key="8j5hi">We’ve optimized Gemini 1.0, our first version, for three different sizes:</p><ul><li data-block-key="103ti"><b>Gemini Ultra</b> — our largest and most capable model for highly complex tasks.</li><li data-block-key="498p0"><b>Gemini Pro</b> — our best model for scaling across a wide range of tasks.</li><li data-block-key="2sl86"><b>Gemini Nano</b> — our most efficient model for on-device tasks.</li></ul></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="4rd18">State-of-the-art performance</h2><p data-block-key="7ormq">We've been rigorously testing our Gemini models and evaluating their performance on a wide variety of tasks. From natural image, audio and video understanding to mathematical reasoning, Gemini Ultra’s performance exceeds current state-of-the-art results on 30 of the 32 widely-used academic benchmarks used in large language model (LLM) research and development.</p><p data-block-key="61bg2">With a score of 90.0%, Gemini Ultra is the first model to outperform human experts on <a href="https://arxiv.org/abs/2009.03300" rt-link-type="external">MMLU</a> (massive multitask language understanding), which uses a combination of 57 subjects such as math, physics, history, law, medicine and ethics for testing both world knowledge and problem-solving abilities.</p><p data-block-key="at1df">Our new benchmark approach to MMLU enables Gemini to use its reasoning capabilities to think more carefully before answering difficult questions, leading to significant improvements over just using its first impression.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
        }">
  

  <p><img alt="A chart showing Gemini Ultra’s performance on common text benchmarks, compared to GPT-4 (API numbers calculated where reported numbers were missing)." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_final_text_table_bigger_font_amendment_lines.gif">
        
      
    
    </p>
    
      <figcaption><p data-block-key="a6ort">Gemini surpasses state-of-the-art performance on a range of benchmarks including text and coding.</p></figcaption>
    
  
    </div>
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><p data-block-key="39bmn">Gemini Ultra also achieves a state-of-the-art score of 59.4% on the new <a href="https://arxiv.org/abs/2311.16502" rt-link-type="external">MMMU</a> benchmark, which consists of multimodal tasks spanning different domains requiring deliberate reasoning.</p><p data-block-key="2pfr3">With the image benchmarks we tested, Gemini Ultra outperformed previous state-of-the-art models, without assistance from object character recognition (OCR) systems that extract text from images for further processing. These benchmarks highlight Gemini’s native multimodality and indicate early signs of Gemini's more complex reasoning abilities.</p><p data-block-key="cihqv">See more details in our <a href="https://goo.gle/GeminiPaper" rt-link-type="external">Gemini technical report</a>.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
        }">
  

  <p><img alt="A chart showing Gemini Ultra’s performance on multimodal benchmarks compared to GPT-4V, with previous SOTA models listed in places where capabilities are not supported in GPT-4V." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_final_multimodal_table_bigger_font_amendment_lines.gif">
        
      
    
    </p>
    
      <figcaption><p data-block-key="a6ort">Gemini surpasses state-of-the-art performance on a range of multimodal benchmarks.</p></figcaption>
    
  
    </div>
  



  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="4rd18">Next-generation capabilities</h2><p data-block-key="90lfq">Until now, the standard approach to creating multimodal models involved training separate components for different modalities and then stitching them together to roughly mimic some of this functionality. These models can sometimes be good at performing certain tasks, like describing images, but struggle with more conceptual and complex reasoning.</p><p data-block-key="c2vjn">We designed Gemini to be natively multimodal, pre-trained from the start on different modalities. Then we fine-tuned it with additional multimodal data to further refine its effectiveness. This helps Gemini seamlessly understand and reason about all kinds of inputs from the ground up, far better than existing multimodal models — and its capabilities are stateof the art in nearly every domain.</p><p data-block-key="9ad54">Learn more about <a href="https://deepmind.google/technologies/gemini" rt-link-type="external">Gemini’s capabilities and see how it works</a>.</p><h3 data-block-key="4mlv7">Sophisticated reasoning</h3><p data-block-key="1s4j6">Gemini 1.0’s sophisticated multimodal reasoning capabilities can help make sense of complex written and visual information. This makes it uniquely skilled at uncovering knowledge that can be difficult to discern amid vast amounts of data.</p><p data-block-key="3lp95">Its remarkable ability to extract insights from hundreds of thousands of documents through reading, filtering and understanding information will help deliver new breakthroughs at digital speeds in many fields from science to finance.</p></div>
  

  
    
  
    


<div data-component="uni-article-yt-player" data-page-title="Introducing Gemini: our largest and most capable AI model" data-video-id="sPiOP_CB54A" data-index-id="14" data-analytics-module="{
    &quot;module_name&quot;: &quot;Youtube Video&quot;,
    &quot;section_header&quot;: &quot;undefined&quot;
  }">

    

    <a role="video" tabindex="0">
      <div>
        
          
          
          
          <p><img alt="Gemini unlocks new scientific insights." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ScienceDemo_TaylorSebastian.width-100.format-webp.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ScienceDemo_TaylorSebastian.width-500.format-webp.webp&quot;,
                &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ScienceDemo_TaylorSebastia.width-1000.format-webp.webp&quot;
              }"></p>

        
        <svg role="presentation">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button_no_hole"></use>
          
        </svg>
        <svg role="img">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button"></use>
          
        </svg>

        
        
        
        
      </div>
    </a>

    
      <p>Gemini unlocks new scientific insights</p>
    

    
  </div>

  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h3 data-block-key="39bmn">Understanding text, images, audio and more</h3><p data-block-key="acvam">Gemini 1.0 was trained to recognize and understand text, images, audio and more at the same time, so it better understands nuanced information and can answer questions relating to complicated topics. This makes it especially good at explaining reasoning in complex subjects like math and physics.</p></div>
  

  
    
  
    


<div data-component="uni-article-yt-player" data-page-title="Introducing Gemini: our largest and most capable AI model" data-video-id="K4pX1VAxaAI" data-index-id="16" data-analytics-module="{
    &quot;module_name&quot;: &quot;Youtube Video&quot;,
    &quot;section_header&quot;: &quot;undefined&quot;
  }">

    

    <a role="video" tabindex="0">
      <div>
        
          
          
          
          <p><img alt="Gemini explains reasoning in math and physics." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_PhysicsHomework_Sam.width-100.format-webp.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_PhysicsHomework_Sam.width-500.format-webp.webp&quot;,
                &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_PhysicsHomework_Sam.width-1000.format-webp.webp&quot;
              }"></p>

        
        <svg role="presentation">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button_no_hole"></use>
          
        </svg>
        <svg role="img">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button"></use>
          
        </svg>

        
        
        
        
      </div>
    </a>

    
      <p>Gemini explains reasoning in math and physics</p>
    

    
  </div>

  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h3 data-block-key="39bmn">Advanced coding</h3><p data-block-key="f2037">Our first version of Gemini can understand, explain and generate high-quality code in the world’s most popular programming languages, like Python, Java, C++, and Go. Its ability to work across languages and reason about complex information makes it one of the leading foundation models for coding in the world.</p><p data-block-key="6rsks">Gemini Ultra excels in several coding benchmarks, including <a href="https://arxiv.org/abs/2107.03374" rt-link-type="external">HumanEval</a>, an important industry-standard for evaluating performance on coding tasks, and Natural2Code, our internal held-out dataset, which uses author-generated sources instead of web-based information.</p><p data-block-key="38e8l">Gemini can also be used as the engine for more advanced coding systems. Two years ago we presented <a href="https://deepmind.google/discover/blog/competitive-programming-with-alphacode/" rt-link-type="external">AlphaCode</a>, the first AI code generation system to reach a competitive level of performance in programming competitions.</p><p data-block-key="915ut">Using a specialized version of Gemini, we created a more advanced code generation system, <a href="https://goo.gle/AlphaCode2" rt-link-type="external">AlphaCode 2</a>, which excels at solving competitive programming problems that go beyond coding to involve complex math and theoretical computer science.</p></div>
  

  
    
  
    


<div data-component="uni-article-yt-player" data-page-title="Introducing Gemini: our largest and most capable AI model" data-video-id="LvGmVmHv69s" data-index-id="18" data-analytics-module="{
    &quot;module_name&quot;: &quot;Youtube Video&quot;,
    &quot;section_header&quot;: &quot;undefined&quot;
  }">

    

    <a role="video" tabindex="0">
      <div>
        
          
          
          
          <p><img alt="Gemini excels at coding and competitive programming." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ACDemo_RemiGabi_v001.width-100.format-webp.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ACDemo_RemiGabi_v001.width-500.format-webp.webp&quot;,
                &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ACDemo_RemiGabi_v001.width-1000.format-webp.webp&quot;
              }"></p>

        
        <svg role="presentation">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button_no_hole"></use>
          
        </svg>
        <svg role="img">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button"></use>
          
        </svg>

        
        
        
        
      </div>
    </a>

    
      <p>Gemini excels at coding and competitive programming</p>
    

    
  </div>

  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><p data-block-key="39bmn">When evaluated on the same platform as the original AlphaCode, AlphaCode 2 shows massive improvements, solving nearly twice as many problems, and we estimate that it performs better than 85% of competition participants — up from nearly 50% for AlphaCode. When programmers collaborate with AlphaCode 2 by defining certain properties for the code samples to follow, it performs even better.</p><p data-block-key="brofo">We’re excited for programmers to increasingly use highly capable AI models as collaborative tools that can help them reason about the problems, propose code designs and assist with implementation — so they can release apps and design better services, faster.</p><p data-block-key="3jkkn">See more details in our <a href="https://goo.gle/AlphaCode2" rt-link-type="external">AlphaCode 2 technical report</a>.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="4rd18">More reliable, scalable and efficient</h2><p data-block-key="85utf">We trained Gemini 1.0 at scale on our AI-optimized infrastructure using Google’s in-house designed <a href="https://cloud.google.com/tpu?hl=en" rt-link-type="external">Tensor Processing Units</a> (TPUs) v4 and v5e. And we designed it to be our most reliable and scalable model to train, and our most efficient to serve.</p><p data-block-key="5ujmc">On TPUs, Gemini runs significantly faster than earlier, smaller and less-capable models. These custom-designed AI accelerators have been at the heart of Google's AI-powered products that serve billions of users like Search, YouTube, Gmail, Google Maps, Google Play and Android. They’ve also enabled companies around the world to train large-scale AI models cost-efficiently.</p><p data-block-key="6kgb1">Today, we’re announcing the most powerful, efficient and scalable TPU system to date, <a href="https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer" rt-link-type="external">Cloud TPU v5p</a>, designed for training cutting-edge AI models. This next generation TPU will accelerate Gemini’s development and help developers and enterprise customers train large-scale generative AI models faster, allowing new products and capabilities to reach customers sooner.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
        }">
  

  <p><img alt="A row of Cloud TPU v5p AI accelerator supercomputers in a Google data center." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_tpu.width-100.format-webp.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_tpu.width-500.format-webp.webp&quot;,
                &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_tpu.width-1000.format-webp.webp&quot;
              }">
        
      
    
    </p>
    
      <figcaption><p data-block-key="a6ort">A row of Cloud TPU v5p AI accelerator supercomputers in a Google data center.</p></figcaption>
    
  
    </div>
  



  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="dapif">Built with responsibility and safety at the core</h2><p data-block-key="6pvob">At Google, we’re committed to advancing bold and responsible AI in everything we do. Building upon Google’s <a href="https://ai.google/responsibility/principles/" rt-link-type="external">AI Principles</a> and the robust safety policies across our products, we’re adding new protections to account for Gemini’s multimodal capabilities. At each stage of development, we’re considering potential risks and working to test and mitigate them.</p><p data-block-key="4rc0o">Gemini has the most comprehensive safety evaluations of any Google AI model to date, including for bias and toxicity. We’ve conducted <a href="https://deepmind.google/discover/blog/an-early-warning-system-for-novel-ai-risks/" rt-link-type="external">novel research into potential risk areas</a> like cyber-offense, persuasion and autonomy, and have applied Google Research’s best-in-class <a href="https://blog.research.google/2023/11/responsible-ai-at-google-research_16.html" rt-link-type="external">adversarial testing techniques</a> to help identify critical safety issues in advance of Gemini’s deployment.</p><p data-block-key="1tti2">To identify blindspots in our internal evaluation approach, we’re working with a diverse group of external experts and partners to stress-test our models across a range of issues.</p><p data-block-key="9hpod">To diagnose content safety issues during Gemini’s training phases and ensure its output follows our policies, we’re using benchmarks such as <a href="https://allenai.org/data/real-toxicity-prompts" rt-link-type="external">Real Toxicity Prompts</a>, a set of 100,000 prompts with varying degrees of toxicity pulled from the web, developed by experts at the Allen Institute for AI. Further details on this work are coming soon.</p><p data-block-key="1ovh">To limit harm, we built dedicated safety classifiers to identify, label and sort out content involving violence or negative stereotypes, for example. Combined with robust filters, this layered approach is designed to make Gemini safer and more inclusive for everyone. Additionally, we’re continuing to address known challenges for models such as factuality, grounding, attribution and corroboration.</p><p data-block-key="6cdjb">Responsibility and safety will always be central to the development and deployment of our models. This is a long-term commitment that requires building collaboratively, so we’re partnering with the industry and broader ecosystem on defining best practices and setting safety and security benchmarks through organizations like <a href="https://mlcommons.org/" rt-link-type="external">MLCommons</a>, the <a href="https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/" rt-link-type="external">Frontier Model Forum</a> <a href="https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/" rt-link-type="external">and</a> its <a href="https://blog.google/outreach-initiatives/public-policy/google-microsoft-anthropic-open-ai-frontier-model-forum-executive-director/" rt-link-type="external">AI Safety Fund</a>, and our <a href="https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/" rt-link-type="external">Secure AI Framework (SAIF)</a>, which was designed to help mitigate security risks specific to AI systems across the public and private sectors. We’ll continue partnering with researchers, governments and civil society groups around the world as we develop Gemini.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="4rd18">Making Gemini available to the world</h2><p data-block-key="5l85j">Gemini 1.0 is now rolling out across a range of products and platforms:<br></p><h3 data-block-key="a245r">Gemini Pro in Google products</h3><p data-block-key="e5h3k">We’re bringing Gemini to billions of people through Google products.</p><p data-block-key="dp2ls">Starting today, <a href="https://blog.google/products/bard/google-bard-try-gemini-ai" rt-link-type="external">Bard will use a fine-tuned version of Gemini Pro</a> for more advanced reasoning, planning, understanding and more. This is the biggest upgrade to Bard since it launched. It will be available in English in more than 170 countries and territories, and we plan to expand to different modalities and support new languages and locations in the near future.</p><p data-block-key="5erfe">We’re also <a href="https://blog.google/products/pixel/pixel-feature-drop-december-2023/" rt-link-type="internal">bringing Gemini to Pixel</a>. Pixel 8 Pro is the first smartphone engineered to run Gemini Nano, which is powering new features like Summarize in the Recorder app and rolling out in Smart Reply in Gboard, starting with WhatsApp — with more messaging apps coming next year.</p><p data-block-key="3ah01">In the coming months, Gemini will be available in more of our products and services like Search, Ads, Chrome and Duet AI.</p><p data-block-key="7s7gn">We’re already starting to experiment with Gemini in Search, where it's making our <a href="https://labs.google/sge/" rt-link-type="external">Search Generative Experience</a> (SGE) faster for users, with a 40% reduction in latency in English in the U.S., alongside improvements in quality.</p><h3 data-block-key="fpjq5">Building with Gemini</h3><p data-block-key="3e6sk">Starting on December 13, developers and enterprise customers can access Gemini Pro via the Gemini API in Google AI Studio or <a href="https://cloud.google.com/vertex-ai" rt-link-type="external">Google Cloud Vertex AI</a>.</p><p data-block-key="51fbs">Google AI Studio is a free, web-based developer tool to prototype and launch apps quickly with an API key. When it's time for a fully-managed AI platform, Vertex AI allows customization of Gemini with full data control and benefits from additional Google Cloud features for enterprise security, safety, privacy and data governance and compliance.</p><p data-block-key="5lr43">Android developers will also be able to build with Gemini Nano, our most efficient model for on-device tasks, via AICore, a new system capability available in Android 14, starting on Pixel 8 Pro devices. Sign up for an <a href="https://android-developers.googleblog.com/2023/12/a-new-foundation-for-ai-on-android.html" rt-link-type="external">early preview of AICore</a>.</p><h3 data-block-key="a0kru">Gemini Ultra coming soon</h3><p data-block-key="hhfg">For Gemini Ultra, we’re currently completing extensive trust and safety checks, including red-teaming by trusted external parties, and further refining the model using fine-tuning and reinforcement learning from human feedback (RLHF) before making it broadly available.</p><p data-block-key="aubeq">As part of this process, we’ll make Gemini Ultra available to select customers, developers, partners and safety and responsibility experts for early experimentation and feedback before rolling it out to developers and enterprise customers early next year.</p><p data-block-key="22un0">Early next year, we’ll also launch <a href="https://blog.google/products/bard/google-bard-try-gemini-ai" rt-link-type="external">Bard Advanced</a>, a new, cutting-edge AI experience that gives you access to our best models and capabilities, starting with Gemini Ultra.</p></div>
  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="4rd18">The Gemini era: enabling a future of innovation</h2><p data-block-key="ausk2">This is a significant milestone in the development of AI, and the start of a new era for us at Google as we continue to rapidly innovate and responsibly advance the capabilities of our models.</p><p data-block-key="936sj">We’ve made great progress on Gemini so far and we’re working hard to further extend its capabilities for future versions, including advances in planning and memory, and increasing the context window for processing even more information to give better responses.</p><p data-block-key="2i1b7">We’re excited by the amazing possibilities of a world responsibly empowered by AI — a future of innovation that will enhance creativity, extend knowledge, advance science and transform the way billions of people live and work around the world.</p></div>
  


            
            

            
              




            
          </div>
        
      </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini – Google DeepMind (980 pts)]]></title>
            <link>https://deepmind.google/technologies/gemini/</link>
            <guid>38544729</guid>
            <pubDate>Wed, 06 Dec 2023 15:03:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/technologies/gemini/">https://deepmind.google/technologies/gemini/</a>, See on <a href="https://news.ycombinator.com/item?id=38544729">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><tr><th><p>Image</p></th><td></td><td></td><td scope="row"><p>MMMU<span>Multi-discipline college-level reasoning problems</span></p></td><td><p>Multi-discipline college-level reasoning problems</p></td><td><p><span data-no-percent="false">59.4%</span><span>0-shot pass@1 <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">56.8%</span><span>0-shot pass@1 <br> GPT-4V</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>VQAv2<span>Natural image understanding</span></p></td><td><p>Natural image understanding</p></td><td><p><span data-no-percent="false">77.8%</span><span>0-shot <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">77.2%</span><span>0-shot <br> GPT-4V</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>TextVQA<span>OCR on natural images</span></p></td><td><p>OCR on natural images</p></td><td><p><span data-no-percent="false">82.3%</span><span>0-shot <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">78%</span><span>0-shot <br> GPT-4V</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>DocVQA<span>Document understanding</span></p></td><td><p>Document understanding</p></td><td><p><span data-no-percent="false">90.9%</span><span>0-shot <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">88.4%</span><span>0-shot <br> GPT-4V (pixel only)</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>Infographic VQA<span>Infographic understanding</span></p></td><td><p>Infographic understanding</p></td><td><p><span data-no-percent="false">80.3%</span><span>0-shot <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">75.1%</span><span>0-shot <br> GPT-4V (pixel only)</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>MathVista<span>Mathematical reasoning in visual contexts</span></p></td><td><p>Mathematical reasoning in visual contexts</p></td><td><p><span data-no-percent="false">53%</span><span>0-shot <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">49.9%</span><span>0-shot <br> GPT-4V</span></p></td></tr><tr><th><p>Video</p></th><td></td><td></td><td scope="row"><p>VATEX<span>English video captioning <br>(CIDEr)</span></p></td><td><p>English video captioning <br>(CIDEr)</p></td><td><p><span data-no-percent="true">62.7</span><span>4-shot <br> Gemini Ultra</span></p></td><td><p><span data-no-percent="true">56</span><span>4-shot <br> DeepMind Flamingo</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>Perception Test MCQA<span>Video question answering</span></p></td><td><p>Video question answering</p></td><td><p><span data-no-percent="false">54.7%</span><span>0-shot <br> Gemini Ultra</span></p></td><td><p><span data-no-percent="false">46.3%</span><span>0-shot <br> SeViLA</span></p></td></tr><tr><th><p>Audio</p></th><td></td><td></td><td scope="row"><p>CoVoST 2 (21 languages)<span>Automatic speech translation <br>(BLUE score)</span></p></td><td><p>Automatic speech translation <br>(BLUE score)</p></td><td><p><span data-no-percent="true">40.1</span><span>Gemini Pro</span></p></td><td><p><span data-no-percent="true">29.1</span><span>Whisper v2</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>FLEURS (62 languages)<span>Automatic speech recognition <br>(based on word error rate, lower is better)</span></p></td><td><p>Automatic speech recognition <br>(based on word error rate, lower is better)</p></td><td><p><span data-no-percent="false">7.6%</span><span>Gemini Pro</span></p></td><td><p><span data-no-percent="false">17.6%</span><span>Whisper v3</span></p></td></tr></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Confirms Governments Using Push Notifications to Surveil Users (538 pts)]]></title>
            <link>https://www.macrumors.com/2023/12/06/apple-governments-surveil-push-notifications/</link>
            <guid>38543587</guid>
            <pubDate>Wed, 06 Dec 2023 13:29:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2023/12/06/apple-governments-surveil-push-notifications/">https://www.macrumors.com/2023/12/06/apple-governments-surveil-push-notifications/</a>, See on <a href="https://news.ycombinator.com/item?id=38543587">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2023/12/06/apple-governments-surveil-push-notifications/"><p>Unidentified governments are surveilling smartphone users by tracking push notifications that move through Google's and Apple's servers, a US senator warned on Wednesday (via <em><a href="https://www.reuters.com/technology/cybersecurity/governments-spying-apple-google-users-through-push-notifications-us-senator-2023-12-06/">Reuters</a></em>).</p>
<p><img src="https://images.macrumors.com/t/JdCJ-AJ3Ort9AETOl6rhY0h-yKg=/400x0/article-new/2023/02/iOS-16-4-Web-Push.jpeg?lossy" srcset="https://images.macrumors.com/t/JdCJ-AJ3Ort9AETOl6rhY0h-yKg=/400x0/article-new/2023/02/iOS-16-4-Web-Push.jpeg?lossy 400w,https://images.macrumors.com/t/SXvAzJ10FPOQRZ5HpBBKDyjgeF8=/800x0/article-new/2023/02/iOS-16-4-Web-Push.jpeg?lossy 800w,https://images.macrumors.com/t/esEjyyoBJArH9DuUVOxzSi_hc5g=/1600x0/article-new/2023/02/iOS-16-4-Web-Push.jpeg 1600w,https://images.macrumors.com/t/TjaJpIne8IG3rDlqM3tgUYkDAc8=/2500x0/filters:no_upscale()/article-new/2023/02/iOS-16-4-Web-Push.jpeg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="iOS 16 4 Web Push" width="794" height="450"><br>In a letter to the Department of Justice, Senator Ron Wyden said foreign officials were demanding the data from the tech giants to track smartphones. The traffic flowing from apps that send push notifications put the companies "in a unique position to facilitate government surveillance of how users are using particular apps," Wyden said. He asked the Department of Justice to "repeal or modify any policies" that hindered public discussions of push notification spying.</p>
<p>In a statement given to <em>Reuters</em>, Apple said that Wyden's letter gave them the opening they needed to share more details with the public about how governments monitored push notifications.<br>
</p>
<blockquote><p>"In this case, the federal government prohibited us from sharing any information," the company said in a statement. "Now that this method has become public we are updating our transparency reporting to detail these kinds of requests."</p></blockquote>
<p>According to the report, Wyden's letter said a "tip" was the source of the information about the surveillance. A source familiar with the matter confirmed that both foreign and U.S. government agencies have been asking Apple and Google for metadata related to push notifications. The data is said to have been used to attempt to tie anonymous users of messaging apps to specific Apple or Google accounts.</p>
<p><em>Reuters</em>' source would not identify which governments were making the data requests but described them as "democracies allied to the United States." They did not know how long the requests had been going on for. </p>
<p>Apple <a href="https://developer.apple.com/documentation/usernotifications/setting_up_a_remote_notification_server/generating_a_remote_notification">advises developers</a> not to include sensitive data in notifications and to encrypt any data before adding it to a notification payload. However, this requires action on the developers' part. Likewise, metadata (like which apps are sending notifications and how often) is not encrypted, potentially giving anyone with access to the information insight into users' app usage.</p>
<p><small>Note: Due to the political or social nature of the discussion regarding this topic, the discussion thread is located in our <a href="https://forums.macrumors.com/forums/political-news.218/">Political News</a> forum. All forum members and site visitors are welcome to read and follow the thread, but posting is limited to forum members with at least 100 posts.</small></p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2023/12/01/ios-17-2-list-of-12-new-features/">iOS 17.2 Will Add These 12 New Features to Your iPhone</a></h3><p>Friday December 1, 2023 12:19 pm PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>iOS 17.2 has been in beta testing for over a month, and it should be released to all users in a few more weeks. The software update includes many new features and changes for iPhones, including the dozen that we have highlighted below. iOS 17.2 is expected to be released to the public in mid-December. To learn about even more features coming in the update, check out our full list. Journal ...</p></div><div><h3><a href="https://www.macrumors.com/2023/12/01/ankers-cyber-week-sale-final-days/">Anker's Cyber Week Sale Enters Final Days With Up to 60% Off Sitewide</a></h3><p>Anker's Black Friday/Cyber Week event is entering its final days this weekend, and it's still offering up to 60 percent off sitewide. There are also a few "mystery boxes" that can include hundreds of dollars in savings, if you're willing to risk not knowing what you're buying ahead of time. All of these sales will end on December 3. Note: MacRumors is an affiliate partner with Anker. When you...</p></div><div><h3><a href="https://www.macrumors.com/2023/11/30/iphone-green-bubbles-rcs-support/">Green Bubbles on iPhone to Gain These 7 New Features Next Year</a></h3><p>Thursday November 30, 2023 9:00 am PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Earlier this month, Apple announced that it will finally support RCS in the Messages app on the iPhone starting later next year. This change will result in several improvements to the messaging experience between iPhones and Android devices. RCS will become the new default standard for messaging between iPhones and Android devices, but these conversations will still have green bubbles like...</p></div><div><h3><a href="https://www.macrumors.com/2023/12/02/top-stories-ios-17-1-2-released/">Top Stories: iOS 17.1.2 Released, NameDrop Misinformation, and More</a></h3><p>Apple employees are back to work following a Thanksgiving break, and that means this week saw a number of new operating system updates for both public release and beta testing. This week also saw some misinformation about Apple's new NameDrop feature making the rounds, while Apple and Goldman Sachs appear to be on the verge of a break-up in their Apple Card and savings account partnership,...</p></div><div><h3><a href="https://www.macrumors.com/2023/12/05/instagram-messenger-chats-disconnecting/">Instagram and Facebook Messenger Chats to Disconnect This Month</a></h3><p>Tuesday December 5, 2023 1:57 am PST by <a href="https://www.macrumors.com/author/tim-hardwick/" rel="author">Tim Hardwick</a></p><p>Meta has revealed plans to end Instagram users' ability to chat with Facebook accounts later this month, rolling back a feature that it introduced over three years ago. In September 2020, Meta (then Facebook) announced it was merging its Facebook Messenger service with Instagram direct messaging, allowing Instagram users to chat with Facebook users and vice versa using the same platform....</p></div><div><h3><a href="https://www.macrumors.com/2023/12/04/apple-work-on-6g-expanding/">Apple's Work on 6G Connectivity Already Expanding</a></h3><p>Apple's work on implementing 6G cellular connectivity on its devices appears to be ramping up, according to Bloomberg's Mark Gurman. In the latest edition of his "Power On" newsletter, Gurman explained that Apple is increasingly turning its attention to 6G, even amid its widely reported difficulties developing a custom 5G cellular modem. In 2021, the first highly specific Apple job...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mastering Nim, 2nd edition (117 pts)]]></title>
            <link>https://nim-lang.org/blog/2023/09/19/mastering-nim.html</link>
            <guid>38543491</guid>
            <pubDate>Wed, 06 Dec 2023 13:21:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nim-lang.org/blog/2023/09/19/mastering-nim.html">https://nim-lang.org/blog/2023/09/19/mastering-nim.html</a>, See on <a href="https://news.ycombinator.com/item?id=38543491">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          <h3>
            <span>
              
              19 September 2023
            </span>
            
            <span>
              
              The Nim Team
            </span>
            
          </h3>
          <p>Discover the secret of Nim!</p>

<p>The definite guide on Nim!
Written by the inventor himself.</p>

<p>Now with updated content for version 2.0 which solves the biggest pain point of Nim 1.0, shared memory in a multi-threaded setting.</p>

<p>Please have a look at its cover image:</p>
<p>
  <img width="auto" height="600" src="https://nim-lang.org//assets/img/mastering_nim_2.jpg">
</p>

<p><strong>But Nim’s logo is a crown!
Where is the crown?</strong>
That’s the secret of Nim!</p>

<p>Send us your reply to <a href="https://nim-lang.org/cdn-cgi/l/email-protection" data-cfemail="c9babcb9b9a6bbbd89a7a0a4e4a5a8a7aee7a6bbae">[email&nbsp;protected]</a> until December 6th 2023.
Among the correct answers we will select 3 winners by randomization.
The winners will receive a signed hardcover!</p>

<p>“Mastering Nim” is available here:</p>

<ul>
  <li><a href="https://www.amazon.com/dp/B0B4R7B9YX">amazon.com</a></li>
  <li><a href="https://www.amazon.de/dp/B0B4R7B9YX">amazon.de</a></li>
</ul>


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mold Course (282 pts)]]></title>
            <link>https://www.epa.gov/mold/mold-course-introduction</link>
            <guid>38543229</guid>
            <pubDate>Wed, 06 Dec 2023 12:58:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.epa.gov/mold/mold-course-introduction">https://www.epa.gov/mold/mold-course-introduction</a>, See on <a href="https://news.ycombinator.com/item?id=38543229">Hacker News</a></p>
Couldn't get https://www.epa.gov/mold/mold-course-introduction: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Playstation keeps reminding us why digital ownership sucks (322 pts)]]></title>
            <link>https://www.theverge.com/2023/12/5/23989290/playstation-digital-ownership-sucks</link>
            <guid>38543196</guid>
            <pubDate>Wed, 06 Dec 2023 12:54:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2023/12/5/23989290/playstation-digital-ownership-sucks">https://www.theverge.com/2023/12/5/23989290/playstation-digital-ownership-sucks</a>, See on <a href="https://news.ycombinator.com/item?id=38543196">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In less than a week, Sony has given us two timely reminders of the tenuousness of digital “ownership” — and both reminders involve things on PlayStation.</p><p>Last week, Sony said that, because of content licensing “arrangements,” users wouldn’t be able to watch Discovery content they’ve purchased <em>and</em> that the content would be removed from their libraries as of December 31st, 2023. The resulting list of shows that will suddenly disappear because of corporate agreements is <a href="https://www.playstation.com/en-us/legal/psvideocontent/?et_rid=&amp;et_cid=231130-VIDREMVL-AM-CSA-B-FLX&amp;Linkid=231130-VIDREMVL-AM-CSA-B-FLX&amp;emcid=em-pl-500377">very long</a>. Shows disappearing from streaming services is commonplace, but in this case, people are losing access to shows they bought to watch on demand whenever they wanted.</p><p>Then, on Monday, many users were unexpectedly banned from their <a href="https://www.theverge.com/2023/12/4/23988621/sony-playstation-account-ps5-bans-permanent-suspension">PlayStation Network accounts</a>, meaning that not only were they blocked from playing multiplayer games or using cloud streaming but they were also locked out of games they purchased digitally from Sony’s PlayStation marketplace. Affected users who may have spent years building a robust digital library were suddenly left without access to content they had bought through no fault of their own. It appears that Sony has since restored account access to people who were accidentally banned, but the company hasn’t explained what happened or said how it might prevent similar unexpected bans in the future. (Sony hasn’t replied to our multiple requests for comment.)</p><p>The ephemerality of digital “ownership” isn’t a new issue. Even though downloading and accessing digital content is often easier than trudging to a retail store to buy a physical copy of a game, you’re putting your faith in the platform holders to maintain their digital storefronts, the content on those storefronts, and their account systems so that your access keeps working.</p><p>The recent closure <a href="https://www.theverge.com/2023/3/26/23657431/wii-u-nintendo-3ds-eshops-shut-down">of Nintendo’s Wii U and 3DS eShops</a> was a stark reminder that companies have the power to decide when you can buy digital content. While you can still redownload Wii U and 3DS games that you’ve purchased, it seems inevitable that Nintendo will stop letting you do that one day. (It’s already planning to shut down online services for those platforms, <a href="https://www.theverge.com/2023/10/4/23902615/wii-u-nintendo-3ds-online-shut-down">after all</a>.) And remember <a href="https://www.theverge.com/2022/9/29/23378713/google-stadia-shutting-down-game-streaming-january-2023">when Google shut down Stadia</a>?</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="I’m considering switching back to physical games." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/376x251/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/384x256/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/415x277/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/480x320/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/540x360/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/640x427/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/750x500/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/828x552/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1080x720/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1200x800/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1440x960/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1920x1280/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2048x1365/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>I’m considering switching back to physical games.</em></figcaption> <p><cite>Photo by James Bareham / The Verge</cite></p></div></div><p>These recent PlayStation incidents are more aggravating, however, because of how sudden and seemingly unfair they are. With the Discovery content, Sony is giving users a matter of weeks to watch their purchased shows for the last time before the shows are yanked from their library entirely. And Sony isn’t offering any compensation for titles you’ve already bought or a way to transfer those purchases to another store. The PlayStation account bans were as swift as they were unexpected, and while resolution for most arrived within a few hours, Sony still hasn’t shared any public communication about what happened or why users should continue to trust the platform.</p><p>I’ve been all in on digital content for years. I don’t like the clutter of physical boxes, and I enjoy being able to switch games and movies without having to get off the couch. But after seeing more instances of companies removing “purchased” digital content — essentially making things I buy digitally a long-term rental — I’m seriously considering going back to buying discs and cartridges.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Governments spying on Apple, Google users through push notifications -US senator (577 pts)]]></title>
            <link>https://www.reuters.com/technology/cybersecurity/governments-spying-apple-google-users-through-push-notifications-us-senator-2023-12-06/</link>
            <guid>38543155</guid>
            <pubDate>Wed, 06 Dec 2023 12:49:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/cybersecurity/governments-spying-apple-google-users-through-push-notifications-us-senator-2023-12-06/">https://www.reuters.com/technology/cybersecurity/governments-spying-apple-google-users-through-push-notifications-us-senator-2023-12-06/</a>, See on <a href="https://news.ycombinator.com/item?id=38543155">Hacker News</a></p>
Couldn't get https://www.reuters.com/technology/cybersecurity/governments-spying-apple-google-users-through-push-notifications-us-senator-2023-12-06/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Norway Joins Denmark in Swedish Tesla Strike/Blockade (191 pts)]]></title>
            <link>https://www.svt.se/nyheter/utrikes/aven-norge-dras-in-i-teslastrejken--3vcx0b</link>
            <guid>38542892</guid>
            <pubDate>Wed, 06 Dec 2023 12:21:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.svt.se/nyheter/utrikes/aven-norge-dras-in-i-teslastrejken--3vcx0b">https://www.svt.se/nyheter/utrikes/aven-norge-dras-in-i-teslastrejken--3vcx0b</a>, See on <a href="https://news.ycombinator.com/item?id=38542892">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody"><p>– De tar striden mot ett fackfientligt bolag på hela arbetsmarknaden vägnar. Nu måste Tesla omedelbart acceptera IF Metalls krav om ett kollektivavtal, säger fackbasen Jørn Eggum till den norska tidningen Dagbladets Børsen.</p><p>Om Tesla inte tillmötesgått IF Metalls krav till den 20 december kommer Fellesforbundet verkställa bojkotten, enligt Eggum.</p><h2>Danskt fack varslar om sympatiåtgärder</h2><p>Även Danmarks största fackförbund 3F Transport, som organiserar hamnarbetare och chaufförer, varslade under tisdagen om sympatiåtgärder. Om 13 dagar kommer de inte ta emot eller transportera Teslas bilar som ska till Sverige.</p><p>”När de ber om vårt stöd backar vi naturligtvis upp. Liksom företagen är fackföreningsrörelsen global i kampen för att skydda arbetarna”, sade Jan Villadsen, ordförande i 3F Transport, i ett pressmeddelande.</p><figure><div><div><p><img alt="" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p></div><div><p>Javascript måste vara påslaget för att kunna spela video</p></div></div><figcaption><span>Därför bråkar IF Metall och Tesla om kollektivavtal. <span>Foto: <!-- -->SVT</span></span></figcaption></figure></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Compressing Gaussian Splats (114 pts)]]></title>
            <link>https://blog.playcanvas.com//compressing-gaussian-splats/</link>
            <guid>38542875</guid>
            <pubDate>Wed, 06 Dec 2023 12:18:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.playcanvas.com//compressing-gaussian-splats/">https://blog.playcanvas.com//compressing-gaussian-splats/</a>, See on <a href="https://news.ycombinator.com/item?id=38542875">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <h3 id="introduction">Introduction</h3>

<p><a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/"><strong>3D Gaussian Splatting</strong></a> is a new method for digitizing and rendering real world objects. With gaussian splatting, you can digitize a scene from a few photos using services like <a href="https://lumalabs.ai/">Luma Labs</a> or <a href="https://poly.cam/">Polycam</a>. These services take the set of photos and generate a 3d Gaussian Splat scene in <a href="https://en.wikipedia.org/wiki/PLY_(file_format)">PLY format</a>.</p>

<p>For example, this is a Gaussian Splat scene rendered in PlayCanvas.</p>



<h3 id="what-is-a-splat">What is a Splat?</h3>

<p>Gaussian Splat Scenes are not made up of polygons and textures. Instead, they are made up of many (up to millions) of individual, unconnected blobs called <em>splats</em>. A splat is just a particle in space with size, orientation, color and opacity.</p>

<p>Below you can see a single brown splat selected. The splat bounding box shows its orientation and size:</p>

<p><img src="https://blog.playcanvas.com/assets/media/splat-example.gif" alt="Splat Example"></p>

<p>The gaussian part of the name comes from the shape of splat itself: the splat opacity has a gaussian falloff from its center to its edge.</p>

<h3 id="engine-support">Engine Support</h3>

<p>The PlayCanvas team has been adding support to the engine for loading and rendering Gaussian Splat PLY files:</p>

<p><a href="https://playcanvas.github.io/#/loaders/splat-many"><img src="https://blog.playcanvas.com/assets/media/gaussian-splat-example.gif" alt="Engine Example"></a></p>

<p>Since the resulting files are often messy and require cleaning, we released <a href="https://playcanvas.com/super-splat">SuperSplat</a>, a tool for cleaning and processing gaussian splat PLY files:</p>

<p><a href="https://playcanvas.com/super-splat?load=https://code.playcanvas.com/viewer/guitar-cleaned.ply"><img src="https://blog.playcanvas.com/assets/media/super-splat-example.gif" alt="SuperSplat Example"></a></p>

<h3 id="ply-format">PLY Format</h3>

<p>However, the default gaussian splat PLY format as exported by training tools is large.</p>

<p>This is because the uncompressed format stores a large amount of data <em>per splat</em>:</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Data Format</th>
      <th>Bytes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Position</td>
      <td>3 x float</td>
      <td>12</td>
    </tr>
    <tr>
      <td>Orientation</td>
      <td>4 x float</td>
      <td>16</td>
    </tr>
    <tr>
      <td>Scale</td>
      <td>3 x float</td>
      <td>12</td>
    </tr>
    <tr>
      <td>Spherical harmonics / color</td>
      <td>48 x float</td>
      <td>192</td>
    </tr>
    <tr>
      <td>Total</td>
      <td>&nbsp;</td>
      <td>232</td>
    </tr>
  </tbody>
</table>

<p>For example, the original <code>guitar.ply</code> scene file takes <strong>132.8 MB</strong> (<strong>32 MB</strong> excluding spherical harmonic data).</p>

<h3 id="compressed-ply-format">Compressed PLY Format</h3>

<p>So we introduced a <em>compressed PLY</em> format for use in runtime applications. The compressed PLY file format ignores the unused spherical harmonic data and stores the rest of the elements in quantized integers.</p>

<p>The format can be summarized as follows:</p>
<ul>
  <li>Split the scene into chunks of 256 splats</li>
  <li>For each chunk, store the min and max (x, y, z) for position and scale in floating point</li>
  <li>For each splat in the chunk, store a normalized and quantized value for position and scale (relative to chunk extents) and orientation and color</li>
</ul>

<p>This data layout results in the following data <em>per chunk</em>:</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Data Format</th>
      <th>Bytes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Position bound</td>
      <td>6 x float</td>
      <td>24</td>
    </tr>
    <tr>
      <td>Scale bound</td>
      <td>6 x float</td>
      <td>24</td>
    </tr>
    <tr>
      <td>Total</td>
      <td>&nbsp;</td>
      <td>48</td>
    </tr>
  </tbody>
</table>

<p>And the following data <em>per splat</em>:</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Data Format</th>
      <th>Bytes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Position</td>
      <td>uint32 (11, 10, 11)</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Orientation</td>
      <td>uint32 (2, 10, 10, 10)</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Scale</td>
      <td>uint32 (11, 10, 11)</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Color</td>
      <td>uint32 (8, 8, 8, 8)</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Total</td>
      <td>&nbsp;</td>
      <td>16</td>
    </tr>
  </tbody>
</table>

<p>As a result, the compressed version of <code>guitar.ply</code> takes only <strong>8.7 MB</strong>.</p>

<h3 id="do-it-yourself">Do It Yourself</h3>

<p>The easiest way to generate a compressed PLY file yourself is using the <a href="https://playcanvas.com/super-splat">SuperSplat tool</a>. Load the PLY file into SuperSplat and export it again using the ‘Compressed Ply File’ option:</p>

<p><a href="https://playcanvas.com/super-splat"><img src="https://blog.playcanvas.com/assets/media/super-splat-export.png" alt="SuperSplat Export"></a></p>

<p>If you are interested in the file format specifics, see <a href="https://github.com/playcanvas/engine/blob/a86bd8be0cfd4e39e9ba5e5466acb6875ab9906e/extras/splat/splat-data.js#L257">this code</a> which demonstrates how to decompress the file data.</p>

<p>See <a href="https://playcanvas.com/project/1165904/overview/gaussiansplatdemo">this editor project</a> for an example of loading and rendering a compressed gaussian splat PLY file. Or you can <a href="https://playcanv.as/p/69cnpevQ/">run it here</a>.</p>

<h3 id="summary-and-future">Summary and Future</h3>

<p>We have introduced a new compressed PLY format for gaussian splatting which is roughly 4x smaller than uncompressed data and can be used in realtime applications.</p>

<p>In future we hope to:</p>

<ul>
  <li>store splats hierarchically for optimized rendering and culling</li>
  <li>implement realtime splat LOD</li>
  <li>test skinning and animation of gaussian splats</li>
  <li>further compress gaussian splat data</li>
  <li>optimize WebGPU rendering</li>
</ul>

<h3 id="references">References</h3>

<p>The compressed format is largely based on the fine work of Aras Pranckevičius and his <a href="https://aras-p.info/">blog posts</a>.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rethinking Serverless with Flame (293 pts)]]></title>
            <link>https://fly.io/blog/rethinking-serverless-with-flame/</link>
            <guid>38542764</guid>
            <pubDate>Wed, 06 Dec 2023 12:03:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fly.io/blog/rethinking-serverless-with-flame/">https://fly.io/blog/rethinking-serverless-with-flame/</a>, See on <a href="https://news.ycombinator.com/item?id=38542764">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
         <dl>
             <dt>Author</dt>
             <dd>
                 <img alt="Chris McCord" src="https://fly.io/static/images/chris-m.webp">
               <dl>
                 <dt>Name</dt>
                 <dd>
                   Chris McCord
                 </dd>
                   <dt>Twitter</dt>
                   <dd>
                     <a href="https://twitter.com/chris_mccord" target="_blank">
                       @chris_mccord
                     </a>
                   </dd>
               </dl>
             </dd>
         </dl>

        <section>
            <figure>
                <img src="https://fly.io/blog/rethinking-serverless-with-flame/assets/flame-cover.webp" alt="FLAME logo">
            </figure>
          <blockquote>Imagine if you could auto scale simply by wrapping any existing app code in a function and have that block of code run in a temporary copy of your app.</blockquote>


<p>The pursuit of elastic, auto-scaling applications has taken us to silly places.</p>

<p>Serverless/FaaS had a couple things going for it. Elastic Scale™ is hard. It’s even harder when you need to manage those pesky servers. It also promised pay-what-you-use costs to avoid idle usage. Good stuff, right?</p>

<p>Well the charade is over. You offload scaling concerns and the complexities of scaling, just to end up needing <em>more complexity</em>. Additional queues, storage, and glue code to communicate back to our app is just the starting point. Dev, test, and CI complexity balloons as fast as your costs. Oh, and you often have to rewrite your app in proprietary JavaScript – even if it’s already written in JavaScript!</p>

<p>At the same time, the rest of us have elastically scaled by starting more webservers. Or we’ve dumped on complexity with microservices. This doesn’t make sense. Piling on more webservers to transcode more videos or serve up more ML tasks isn’t what we want. And granular scale shouldn’t require slicing our apps into bespoke operational units with their own APIs and deployments to manage.</p>

<p>Enough is enough. There’s a better way to elastically scale applications.</p>
<h2 id="the-flame-pattern"><a href="#the-flame-pattern" aria-label="Anchor"></a>The FLAME pattern</h2>
<p>Here’s what we really want:</p>

<ul>
<li>We don’t want to manage those pesky servers. We already have this for our app deployments via <code>fly deploy</code>, <code>git push heroku</code>, <code>kubectl</code>, etc
</li><li>We want on-demand, <em>granular</em> elastic scale of specific parts of our app code
</li><li>We don’t want to rewrite our application or write parts of it in proprietary runtimes
</li></ul>

<p>Imagine if we could auto scale simply by wrapping any existing app code in a function and have that block of code run in a temporary copy of the app.</p>

<p>Enter the FLAME pattern.</p>
<blockquote>FLAME - Fleeting Lambda Application for Modular Execution</blockquote>


<p>With FLAME, you treat your <em>entire application</em> as a lambda, where modular parts can be executed on short-lived infrastructure.</p>

<p>No rewrites. No bespoke runtimes. No outrageous layers of complexity. Need to insert the results of an expensive operation to the database? PubSub broadcast the result of some expensive work? No problem! It’s your whole app so of course you can do it.</p>

<p>The Elixir <a href="https://github.com/phoenixframework/flame">flame library</a> implements the FLAME pattern. It has a backend adapter for Fly.io, but you can use it on any cloud that gives you an API to spin up an instance with your app code running on it. We’ll talk more about backends in a bit, as well as implementing FLAME in other languages.</p>

<p>First, lets watch a realtime thumbnail generation example to see FLAME + Elixir in action:</p>
<div><p>
          <iframe width="100%" height="100%" src="https://www.youtube.com/embed/l1xt_rkWdic" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
          </iframe>
        </p>
</div>


<p>Now let’s walk thru something a little more basic. Imagine we have a function to transcode video to thumbnails in our Elixir application after they are uploaded:</p>
<div>
  <pre><code><span>def</span> <span>generate_thumbnails</span><span>(%</span><span>Video</span><span>{}</span> <span>=</span> <span>vid</span><span>,</span> <span>interval</span><span>)</span> <span>do</span>
  <span>tmp</span> <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>System</span><span>.</span><span>tmp_dir!</span><span>(),</span> <span>Ecto</span><span>.</span><span>UUID</span><span>.</span><span>generate</span><span>())</span>
  <span>File</span><span>.</span><span>mkdir!</span><span>(</span><span>tmp</span><span>)</span>
  <span>args</span> <span>=</span> <span>~w(-i #{vid.url} -vf fps=1/#{interval} #{tmp}/%02d.png)</span><span>)</span>
  <span>System</span><span>.</span><span>cmd</span><span>(</span><span>"ffmpeg"</span><span>,</span> <span>args</span><span>)</span>
  <span>urls</span> <span>=</span> <span>VidStore</span><span>.</span><span>put_thumbnails</span><span>(</span><span>vid</span><span>,</span> <span>Path</span><span>.</span><span>wildcard</span><span>(</span><span>tmp</span> <span>&lt;&gt;</span> <span>"/*.png"</span><span>))</span>
  <span>Repo</span><span>.</span><span>insert_all</span><span>(</span><span>Thumb</span><span>,</span> <span>Enum</span><span>.</span><span>map</span><span>(</span><span>urls</span><span>,</span> <span>&amp;</span><span>%{</span><span>vid_id:</span> <span>vid</span><span>.</span><span>id</span><span>,</span> <span>url:</span> <span>&amp;1</span><span>}))</span>
<span>end</span>
</code></pre>
</div>

<p>Our <code>generate_thumbnails</code> function accepts a video struct. We shell out to <code>ffmpeg</code> to take the video URL and generate thumbnails at a given interval. We then write the temporary thumbnail paths to durable storage. Finally, we insert the generated thumbnail URLs into the database.</p>

<p>This works great locally, but CPU bound work like video transcoding can quickly bring our entire service to a halt in production. Instead of rewriting large swaths of our app to move this into microservices or some FaaS, we can simply wrap it in a FLAME call:</p>
<div>
  <pre><code><span>def</span> <span>generate_thumbnails</span><span>(%</span><span>Video</span><span>{}</span> <span>=</span> <span>vid</span><span>,</span> <span>interval</span><span>)</span> <span>do</span>
  <span>FLAME</span><span>.</span><span>call</span><span>(</span><span>MyApp</span><span>.</span><span>FFMpegRunner</span><span>,</span> <span>fn</span> <span>-&gt;</span>
    <span>tmp</span> <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>System</span><span>.</span><span>tmp_dir!</span><span>(),</span> <span>Ecto</span><span>.</span><span>UUID</span><span>.</span><span>generate</span><span>())</span>
    <span>File</span><span>.</span><span>mkdir!</span><span>(</span><span>tmp</span><span>)</span>
    <span>args</span> <span>=</span> <span>~w(-i #{vid.url} -vf fps=1/#{interval} #{tmp}/%02d.png)</span><span>)</span>
    <span>System</span><span>.</span><span>cmd</span><span>(</span><span>"ffmpeg"</span><span>,</span> <span>args</span><span>)</span>
    <span>urls</span> <span>=</span> <span>VidStore</span><span>.</span><span>put_thumbnails</span><span>(</span><span>vid</span><span>,</span> <span>Path</span><span>.</span><span>wildcard</span><span>(</span><span>tmp</span> <span>&lt;&gt;</span> <span>"/*.png"</span><span>))</span>
    <span>Repo</span><span>.</span><span>insert_all</span><span>(</span><span>Thumb</span><span>,</span> <span>Enum</span><span>.</span><span>map</span><span>(</span><span>urls</span><span>,</span> <span>&amp;</span><span>%{</span><span>vid_id:</span> <span>vid</span><span>.</span><span>id</span><span>,</span> <span>url:</span> <span>&amp;1</span><span>}))</span>
  <span>end</span><span>)</span>
<span>end</span>
</code></pre>
</div>

<p>That’s it! <code>FLAME.call</code> accepts the name of a runner pool, and a function. It then finds or boots a new copy of our entire application and runs the function there. Any variables the function closes over (like our <code>%Video{}</code> struct and <code>interval</code>) are passed along automatically.</p>

<p>When the FLAME runner boots up, it connects back to the parent node, receives the function to run, executes it, and returns the result to the caller. Based on configuration, the booted runner either waits happily for more work before idling down, or extinguishes itself immediately.</p>

<p>Let’s visualize the flow:</p>

<p><img alt="visualizing the flow" src="https://fly.io/blog/rethinking-serverless-with-flame/assets/visual.webp?centered"></p>

<p>We changed no other code and issued our DB write with <code>Repo.insert_all</code> just like before, because we are running our <em>entire</em> <em>application</em>. Database connection(s) and all. Except this fleeting application only runs that little function after startup and nothing else.</p>

<p>In practice, a FLAME implementation will support a pool of runners for hot startup, scale-to-zero, and elastic growth. More on that later.</p>
<h2 id="solving-a-problem-vs-removing-the-problem"><a href="#solving-a-problem-vs-removing-the-problem" aria-label="Anchor"></a>Solving a problem vs removing the problem</h2><blockquote>FaaS solutions help you solve a problem. FLAME removes the problem.</blockquote>


<p>The FaaS labyrinth of complexity defies reason. And it’s unavoidable. Let’s walkthrough the thumbnail use-case to see how.</p>

<p>We try to start with the simplest building block like request/response AWS Lambda Function URL’s.</p>

<p>The complexity hits immediately.</p>

<p>We start writing custom encoders/decoders on both sides to handle streaming the thumbnails back to the app over HTTP. Phew that’s done. Wait, is our video transcoding or user uploads going to take longer than 15 minutes? Sorry, hard timeout limit&nbsp;–&nbsp;time to split our videos into chunks to stay within the timeout, which means more lambdas to do that. Now we’re orchestrating lambda workflows and relying on additional services, such as SQS and S3, to enable this.</p>

<p>All the FaaS is doing is adding layers of communication between your code and the parts you want to run elastically. Each layer has its own glue integration price to pay.</p>

<p>Ultimately handling this kind of use-case looks something like this:</p>

<ul>
<li>Trigger the lambda via HTTP endpoint, S3, or API gateway ($)
</li><li>Write the bespoke lambda to transcode the video ($)
</li><li>Place the thumbnail results into SQS ($)
</li><li>Write the SQS consumer in our app (dev $)
</li><li>Persist to DB and figure out how to get events back to active subscribers that may well be connected to other instances than the SQS consumer (dev $)
</li></ul>

<p>This is nuts. We pay the FaaS toll at every step. We shouldn’t have to do any of this!</p>

<p>FaaS provides a bunch of offerings to build a solution on top of. FLAME removes the problem entirely.</p>
<h2 id="flame-backends"><a href="#flame-backends" aria-label="Anchor"></a>FLAME Backends</h2><blockquote>On Fly.io infrastructure the <code>FLAME.FlyBackend</code> can boot a copy of your application on a new <a href="https://fly.io/docs/machines/">Machine</a> and have it connect back to the parent for work within ~3s.</blockquote>


<p>By default, FLAME ships with a <code>LocalBackend</code> and <code>FlyBackend</code>, but any host that provides an API to provision a server and run your app code can work as a FLAME backend. Erlang and Elixir primitives are doing all the heavy lifting here. The entire <code>FLAME.FlyBackend</code> is <a href="https://github.com/phoenixframework/flame/blob/main/lib/flame/fly_backend.ex">&lt; 200 LOC with docs</a>. The library has a single dependency, <code>req</code>, which is an HTTP client.</p>

<p>Because Fly.io runs our applications as a packaged up docker image, we simply ask the Fly API to boot a new Machine for us with the same image that our app is currently running. Also thanks to Fly infrastructure, we can guarantee the FLAME runners are started in the same region as the parent. This optimizes latency and lets you ship whatever data back and forth between parent and runner without having to think about it.</p>
<h2 id="look-at-everything-were-not-doing"><a href="#look-at-everything-were-not-doing" aria-label="Anchor"></a>Look at everything we’re not doing</h2>
<p>With FaaS, just imagine how quickly the dev and testing story becomes a fate worse than death.</p>

<p>To run the app locally, we either need to add some huge dev dependencies to simulate the entire FaaS pipeline, or worse, connect up our dev and test environments directly to the FaaS provider.</p>

<p>With FLAME, your dev and test runners simply run on the local backend.</p>

<p>Remember, this is your app. FLAME just controls where modular parts of it run. In dev or test, those parts simply run on the existing runtime on&nbsp;your laptop or CI server.</p>

<p>Using Elixir, we can even send a file across to the remote FLAME application thanks to the distributed features of the Erlang VM:</p>
<div>
  <pre><code><span>def</span> <span>generate_thumbnails</span><span>(%</span><span>Video</span><span>{}</span> <span>=</span> <span>vid</span><span>,</span> <span>interval</span><span>)</span> <span>do</span>
  <span>parent_stream</span> <span>=</span> <span>File</span><span>.</span><span>stream!</span><span>(</span><span>vid</span><span>.</span><span>filepath</span><span>,</span> <span>[],</span> <span>2048</span><span>)</span>
  <span>FLAME</span><span>.</span><span>call</span><span>(</span><span>MyApp</span><span>.</span><span>FFMpegRunner</span><span>,</span> <span>fn</span> <span>-&gt;</span>
    <span>tmp_file</span> <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>System</span><span>.</span><span>tmp_dir!</span><span>(),</span> <span>Ecto</span><span>.</span><span>UUID</span><span>.</span><span>generate</span><span>())</span>
    <span>flame_stream</span> <span>=</span> <span>File</span><span>.</span><span>stream!</span><span>(</span><span>tmp_file</span><span>)</span>
    <span>Enum</span><span>.</span><span>into</span><span>(</span><span>parent_stream</span><span>,</span> <span>flame_stream</span><span>)</span>

    <span>tmp</span> <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>System</span><span>.</span><span>tmp_dir!</span><span>(),</span> <span>Ecto</span><span>.</span><span>UUID</span><span>.</span><span>generate</span><span>())</span>
    <span>File</span><span>.</span><span>mkdir!</span><span>(</span><span>tmp</span><span>)</span>
    <span>args</span> <span>=</span> <span>~w(-i #{tmp_file} -vf fps=1/#{interval} #{tmp}/%02d.png)</span>
    <span>System</span><span>.</span><span>cmd</span><span>(</span><span>"ffmpeg"</span><span>,</span> <span>args</span><span>)</span>
    <span>urls</span> <span>=</span> <span>VidStore</span><span>.</span><span>put_thumbnails</span><span>(</span><span>vid</span><span>,</span> <span>Path</span><span>.</span><span>wildcard</span><span>(</span><span>tmp</span> <span>&lt;&gt;</span> <span>"/*.png"</span><span>))</span>
    <span>Repo</span><span>.</span><span>insert_all</span><span>(</span><span>Thumb</span><span>,</span> <span>Enum</span><span>.</span><span>map</span><span>(</span><span>urls</span><span>,</span> <span>&amp;</span><span>%{</span><span>vid_id:</span> <span>vid</span><span>.</span><span>id</span><span>,</span> <span>url:</span> <span>&amp;1</span><span>}))</span>
  <span>end</span><span>)</span>
<span>end</span>
</code></pre>
</div>

<p>On line 2 we open a file on the parent node to the video path. Then in the FLAME child, we stream the file from the parent node to the FLAME server in only a couple lines of code. That’s it! No setup of S3 or HTTP interfaces required.</p>

<p>With FLAME it’s easy to miss everything we’re not doing:</p>

<ul>
<li>We don’t need to write code outside of our application. We can reuse business logic, database setup, PubSub, and all the features of our respective platforms
</li><li>We don’t need to manage deploys of separate services or endpoints
</li><li>We don’t need to write results to S3 or SQS just to pick up values back in our app
</li><li>We skip the dev, test, and CI dependency dance
</li></ul>
<h2 id="flame-outside-elixir"><a href="#flame-outside-elixir" aria-label="Anchor"></a>FLAME outside Elixir</h2>
<p>Elixir is fantastically well suited for the FLAME model because we get so much <a href="https://fly.io/phoenix-files/elixir-and-phoenix-can-do-it-all/">for free</a> like process supervision and distributed messaging. That said, any language with reasonable concurrency primitives can take advantage of this pattern. For example, my teammate, Lubien, created a proof of concept example for breaking out functions in your JavaScript application and running them inside a new Fly Machine: <a href="https://github.com/lubien/fly-run-this-function-on-another-machine">https://github.com/lubien/fly-run-this-function-on-another-machine</a></p>

<p>So the general flow for a JavaScript-based FLAME call would be to move the modular executions to a new file, which is executed on a runner pool. Provided the arguments are JSON serializable, the general FLAME flow is similar to what we’ve outlined here. Your application, your code, running on fleeting instances.</p>

<p>A complete FLAME library will need to handle the following concerns:</p>

<ul>
<li>Elastic pool scale-up and scale-down logic
</li><li>Hot vs cold startup with pools
</li><li>Remote runner monitoring to avoid orphaned resources
</li><li>How to monitor and keep deployments fresh
</li></ul>

<p>For the rest of this post we’ll see how the Elixir FLAME library handles these concerns as well as features uniquely suited to Elixir applications. But first, you might be wondering about your background job queues.</p>
<h2 id="what-about-my-background-job-processor"><a href="#what-about-my-background-job-processor" aria-label="Anchor"></a>What about my background job processor?</h2>
<p>FLAME works great inside your background job processor, but you may have noticed some overlap. If your job library handles scaling the worker pool, what is FLAME doing for you? There’s a couple important distinctions here.</p>

<p>First, we reach for these queues when we need <em>durability guarantees</em>. We often can turn knobs to have the queues scale to handle more jobs as load changes. But durable operations are separate from elastic execution. Conflating these concerns can send you down a similar path to lambda complexity. Leaning on your worker queue purely for offloaded execution means writing all the glue code to get the data into and out of the job, and back to the caller or end-user’s device somehow.</p>

<p>For example, if we want to guarantee we successfully generated thumbnails for a video after the user upload, then a job queue makes sense as the <em>dispatch, commit, and retry</em> <em>mechanism</em> for this operation. The actual transcoding could be a FLAME call inside the job itself, so we decouple the ideas of durability and scaled execution.</p>

<p>On the other side, we have operations we don’t need durability for. Take the screencast above where the user hasn’t yet saved their video. Or an ML model execution where there’s no need to waste resources churning a prompt if the user has already left the app. In those cases, it doesn’t make sense to write to a durable store to pick up a job for work that will go right into the ether.</p>
<h2 id="pooling-for-elastic-scale"><a href="#pooling-for-elastic-scale" aria-label="Anchor"></a>Pooling for Elastic Scale</h2>
<p>With the Elixir implementation of FLAME, you define elastic pools of runners. This allows scale-to-zero behavior while also elastically scaling up FLAME servers with max concurrency limits.</p>

<p>For example, lets take a look at the <code>start/2</code> callback, which is the entry point of all Elixir applications. We can drop in a <code>FLAME.Pool</code> for video transcriptions and say we want it to scale to zero, boot a max of 10, and support 5 concurrent <code>ffmpeg</code> operations per runner:</p>
<div>
  <pre><code><span>def</span> <span>start</span><span>(</span><span>_type</span><span>,</span> <span>_args</span><span>)</span> <span>do</span>
  <span>flame_parent</span> <span>=</span> <span>FLAME</span><span>.</span><span>Parent</span><span>.</span><span>get</span><span>()</span>

  <span>children</span> <span>=</span> <span>[</span>
    <span>...</span><span>,</span>
    <span>MyApp</span><span>.</span><span>Repo</span><span>,</span>
    <span>{</span><span>FLAME</span><span>.</span><span>Pool</span><span>,</span>
      <span>name:</span> <span>Thumbs</span><span>.</span><span>FFMpegRunner</span><span>,</span>
      <span>min:</span> <span>0</span><span>,</span>
      <span>max:</span> <span>10</span><span>,</span>
      <span>max_concurrency:</span> <span>5</span><span>,</span>
      <span>idle_shutdown_after:</span> <span>30_000</span><span>},</span>
    <span>!flame_parent</span> <span>&amp;&amp;</span> <span>MyAppWeb</span><span>.</span><span>Endpoint</span>
  <span>]</span>
  <span>|&gt;</span> <span>Enum</span><span>.</span><span>filter</span><span>(</span><span>&amp;</span> <span>&amp;1</span><span>)</span>

  <span>opts</span> <span>=</span> <span>[</span><span>strategy:</span> <span>:one_for_one</span><span>,</span> <span>name:</span> <span>MyApp</span><span>.</span><span>Supervisor</span><span>]</span>
  <span>Supervisor</span><span>.</span><span>start_link</span><span>(</span><span>children</span><span>,</span> <span>opts</span><span>)</span>
<span>end</span>
</code></pre>
</div>

<p>We use the presence of a FLAME parent to conditionally start our Phoenix webserver when booting the app. There’s no reason to start a webserver if we aren’t serving web traffic. Note we leave other services like the database <code>MyApp.Repo</code> alone because we want to make use of those services inside FLAME runners.</p>

<p>Elixir’s supervised process approach to applications is uniquely great for turning these kinds of knobs.</p>

<p>We also set our pool to idle down after 30 seconds of no caller operations. This keeps our runners hot for a short while before discarding them. We could also pass a <code>min: 1</code> to always ensure at least one <code>ffmpeg</code> runner is hot and ready for work by the time our application is started.</p>
<h2 id="process-placement"><a href="#process-placement" aria-label="Anchor"></a>Process Placement</h2>
<p>In Elixir, stateful bits of our applications are built around the <em>process</em> primitive –&nbsp;lightweight greenthreads with message mailboxes. Wrapping our otherwise stateless app code in a synchronous <code>FLAME.call</code>‘s or async <code>FLAME.cast</code>’s works great, but what about the stateful parts of our app?</p>

<p><code>FLAME.place_child</code> exists to take an existing process specification in your Elixir app and start it on a FLAME runner instead of locally. You can use it anywhere you’d use <code>Task.Supervisor.start_child</code> , <code>DynamicSupervisor.start_child</code>, or similar interfaces. Just like <code>FLAME.call</code>, the process is run on an elastic pool and runners handle idle down when the process completes its work.</p>

<p>And like <code>FLAME.call</code>, it lets us take existing app code, change a single LOC, and continue shipping features.</p>

<p>Let’s walk thru the example from the screencast above. Imagine we want to generate video thumbnails for a video <em>as it is being uploaded</em>. Elixir and LiveView make this easy. We won’t cover all the code here, but you can view the <a href="https://github.com/fly-apps/thumbnail_generator/blob/main/lib/thumbs/thumbnail_generator.ex">full app implementation</a>.</p>

<p>Our first pass would be to write a LiveView upload writer that calls into a <code>ThumbnailGenerator</code>:</p>
<div>
  <pre><code><span>defmodule</span> <span>ThumbsWeb</span><span>.</span><span>ThumbnailUploadWriter</span> <span>do</span>
  <span>@behaviour</span> <span>Phoenix</span><span>.</span><span>LiveView</span><span>.</span><span>UploadWriter</span>

  <span>alias</span> <span>Thumbs</span><span>.</span><span>ThumbnailGenerator</span>

  <span>def</span> <span>init</span><span>(</span><span>opts</span><span>)</span> <span>do</span>
    <span>generator</span> <span>=</span> <span>ThumbnailGenerator</span><span>.</span><span>open</span><span>(</span><span>opts</span><span>)</span>
    <span>{</span><span>:ok</span><span>,</span> <span>%{</span><span>gen:</span> <span>generator</span><span>}}</span>
  <span>end</span>

  <span>def</span> <span>write_chunk</span><span>(</span><span>data</span><span>,</span> <span>state</span><span>)</span> <span>do</span>
    <span>ThumbnailGenerator</span><span>.</span><span>stream_chunk!</span><span>(</span><span>state</span><span>.</span><span>gen</span><span>,</span> <span>data</span><span>)</span>
    <span>{</span><span>:ok</span><span>,</span> <span>state</span><span>}</span>
  <span>end</span>

  <span>def</span> <span>meta</span><span>(</span><span>state</span><span>),</span> <span>do</span><span>:</span> <span>%{</span><span>gen:</span> <span>state</span><span>.</span><span>gen</span><span>}</span>

  <span>def</span> <span>close</span><span>(</span><span>state</span><span>,</span> <span>_reason</span><span>)</span> <span>do</span>
    <span>ThumbnailGenerator</span><span>.</span><span>close</span><span>(</span><span>state</span><span>.</span><span>gen</span><span>)</span>
    <span>{</span><span>:ok</span><span>,</span> <span>state</span><span>}</span>
  <span>end</span>
<span>end</span>
</code></pre>
</div>

<p>An upload writer is a behavior that simply ferries the uploaded chunks from the client into whatever we’d like to do with them. Here we have a <code>ThumbnailGenerator.open/1</code> which starts a process that communicates with an <code>ffmpeg</code> shell. Inside <code>ThumbnailGenerator.open/1</code>, we use regular elixir process primitives:</p>
<div>
  <pre><code>  <span># thumbnail_generator.ex</span>
  <span>def</span> <span>open</span><span>(</span><span>opts</span> <span>\\</span> <span>[])</span> <span>do</span>
    <span>Keyword</span><span>.</span><span>validate!</span><span>(</span><span>opts</span><span>,</span> <span>[</span><span>:timeout</span><span>,</span> <span>:caller</span><span>,</span> <span>:fps</span><span>])</span>
    <span>timeout</span> <span>=</span> <span>Keyword</span><span>.</span><span>get</span><span>(</span><span>opts</span><span>,</span> <span>:timeout</span><span>,</span> <span>5_000</span><span>)</span>
    <span>caller</span> <span>=</span> <span>Keyword</span><span>.</span><span>get</span><span>(</span><span>opts</span><span>,</span> <span>:caller</span><span>,</span> <span>self</span><span>())</span>
    <span>ref</span> <span>=</span> <span>make_ref</span><span>()</span>
    <span>parent</span> <span>=</span> <span>self</span><span>()</span>

    <span>spec</span> <span>=</span> <span>{</span><span>__MODULE__</span><span>,</span> <span>{</span><span>caller</span><span>,</span> <span>ref</span><span>,</span> <span>parent</span><span>,</span> <span>opts</span><span>}}</span>
    <span>{</span><span>:ok</span><span>,</span> <span>pid</span><span>}</span> <span>=</span> <span>DynamicSupervisor</span><span>.</span><span>start_child</span><span>(</span><span>@sup</span><span>,</span> <span>spec</span><span>)</span>

    <span>receive</span> <span>do</span>
      <span>{</span><span>^</span><span>ref</span><span>,</span> <span>%</span><span>ThumbnailGenerator</span><span>{}</span> <span>=</span> <span>gen</span><span>}</span> <span>-&gt;</span>
        <span>%</span><span>ThumbnailGenerator</span><span>{</span><span>gen</span> <span>|</span> <span>pid:</span> <span>pid</span><span>}</span>
    <span>after</span>
      <span>timeout</span> <span>-&gt;</span> <span>exit</span><span>(</span><span>:timeout</span><span>)</span>
    <span>end</span>
  <span>end</span>
</code></pre>
</div>

<p>The details aren’t super important here, except line 10 where we call <code>{:ok, pid} = DynamicSupervisor.start_child(@sup, spec)</code>, which starts a supervised<code>ThumbnailGenerator</code> process. The rest of the implementation simply ferries chunks as stdin into <code>ffmpeg</code> and parses png’s from stdout. Once a PNG delimiter is found in stdout, we send the <code>caller</code> process (our LiveView process) a message saying “hey, here’s an image”:</p>
<div>
  <pre><code><span># thumbnail_generator.ex</span>
<span>@png_begin</span> <span>&lt;&lt;</span><span>137</span><span>,</span> <span>80</span><span>,</span> <span>78</span><span>,</span> <span>71</span><span>,</span> <span>13</span><span>,</span> <span>10</span><span>,</span> <span>26</span><span>,</span> <span>10</span><span>&gt;&gt;</span>
<span>defp</span> <span>handle_stdout</span><span>(</span><span>state</span><span>,</span> <span>ref</span><span>,</span> <span>bin</span><span>)</span> <span>do</span>
  <span>%</span><span>ThumbnailGenerator</span><span>{</span><span>ref:</span> <span>^</span><span>ref</span><span>,</span> <span>caller:</span> <span>caller</span><span>}</span> <span>=</span> <span>state</span><span>.</span><span>gen</span>

  <span>case</span> <span>bin</span> <span>do</span>
    <span>&lt;&lt;</span><span>@png_begin</span><span>,</span> <span>_rest</span><span>::</span><span>binary</span><span>&gt;&gt;</span> <span>-&gt;</span>
      <span>if</span> <span>state</span><span>.</span><span>current</span> <span>do</span>
        <span>send</span><span>(</span><span>caller</span><span>,</span> <span>{</span><span>ref</span><span>,</span> <span>:image</span><span>,</span> <span>state</span><span>.</span><span>count</span><span>,</span> <span>encode</span><span>(</span><span>state</span><span>)})</span>
      <span>end</span>

      <span>%{</span><span>state</span> <span>|</span> <span>count:</span> <span>state</span><span>.</span><span>count</span> <span>+</span> <span>1</span><span>,</span> <span>current:</span> <span>[</span><span>bin</span><span>]}</span>

    <span>_</span> <span>-&gt;</span>
      <span>%{</span><span>state</span> <span>|</span> <span>current:</span> <span>[</span><span>bin</span> <span>|</span> <span>state</span><span>.</span><span>current</span><span>]}</span>
  <span>end</span>
<span>end</span>
</code></pre>
</div>

<p>The <code>caller</code> LiveView process then picks up the message in a <code>handle_info</code> callback and updates the UI:</p>
<div>
  <pre><code><span># thumb_live.ex</span>
<span>def</span> <span>handle_info</span><span>({</span><span>_ref</span><span>,</span> <span>:image</span><span>,</span> <span>_count</span><span>,</span> <span>encoded</span><span>},</span> <span>socket</span><span>)</span> <span>do</span>
  <span>%{</span><span>count:</span> <span>count</span><span>}</span> <span>=</span> <span>socket</span><span>.</span><span>assigns</span>

  <span>{</span><span>:noreply</span><span>,</span>
   <span>socket</span>
   <span>|&gt;</span> <span>assign</span><span>(</span><span>count:</span> <span>count</span> <span>+</span> <span>1</span><span>,</span> <span>message:</span> <span>"Generating (</span><span>#{</span><span>count</span> <span>+</span> <span>1</span><span>}</span><span>)"</span><span>)</span>
   <span>|&gt;</span> <span>stream_insert</span><span>(</span><span>:thumbs</span><span>,</span> <span>%{</span><span>id:</span> <span>count</span><span>,</span> <span>encoded:</span> <span>encoded</span><span>})}</span>
<span>end</span>
</code></pre>
</div>

<p>The <code>send(caller, {ref, :image, state.count, encode(state)}</code> is one magic part about Elixir. Everything is a process, and we can message those processes, regardless of their location in the cluster.</p>

<p>It’s like if every instantiation of an object in your favorite OO lang included a cluster-global unique identifier to work with methods on that object. The LiveView (a process) simply receives the image message and updates the UI with new images.</p>

<p>Now let’s head back over to our <code>ThumbnailGenerator.open/1</code> function and make this elastically scalable.</p>
<div>
  <pre><code><span>-    {:ok, pid} = DynamicSupervisor.start_child(@sup, spec)
</span><span>+    {:ok, pid} = FLAME.place_child(Thumbs.FFMpegRunner, spec)
</span></code></pre>
</div>

<p>That’s it! Because everything is a process and processes can live anywhere, it doesn’t matter what server our <code>ThumbnailGenerator</code> process lives on. It simply messages the caller with <code>send(caller, …)</code> and the messages are sent across the cluster if needed.</p>

<p>Once the process exits, either from an explicit close, after the upload is done, or from the end-user closing their browser tab, the FLAME server will note the exit and idle down if no other work is being done.</p>

<p>Check out the <a href="https://github.com/fly-apps/thumbnail_generator/blob/main/lib/thumbs/thumbnail_generator.ex">full implementation</a> if you’re interested.</p>
<h2 id="remote-monitoring"><a href="#remote-monitoring" aria-label="Anchor"></a>Remote Monitoring</h2>
<p>All this transient infrastructure needs failsafe mechanisms to avoid orphaning resources. If a parent spins up a runner, that runner must take care of idling itself down when no work is present and handle failsafe shutdowns if it can no longer contact the parent node.</p>

<p>Likewise, we need to shutdown runners when parents are rolled for new deploys as we must guarantee we’re running the same code across the cluster.</p>

<p>We also have active callers in many cases that are awaiting the result of work on runners that could go down for any reason.</p>

<p>There’s a lot to monitor here.</p>

<p>There’s also a number of failure modes that make this sound like a harrowing experience to implement. Fortunately Elixir has all the primitives to make this an easy task thanks to the Erlang VM. Namely, we get the following for free:</p>

<ul>
<li>Process monitoring and supervision –&nbsp;we know when things go bad. Whether on a node-local process, or one across the cluster
</li><li>Node monitoring – we know when nodes come up, and when nodes go away
</li><li>Declarative and controlled app startup and shutdown - we carefully control the startup and shutdown sequence of applications as a matter of course. This allows us to gracefully shutdown active runners when a fresh deploy is triggered, while giving them time to finish their work
</li></ul>

<p>We’ll cover the internal implementation details in a future deep-dive post. For now, feel free to poke around <a href="https://github.com/phoenixframework/flame">the flame source</a>.</p>
<h2 id="whats-next"><a href="#whats-next" aria-label="Anchor"></a>What’s Next</h2>
<p>We’re just getting started with the Elixir FLAME library, but it’s ready to try out now. In the future  look for more advance pool growth techniques, and deep dives into how the Elixir implementation works. You can also find me <a href="https://twitter.com/chris_mccord">@chris_mccord</a> to chat about implementing the FLAME pattern in your language of choice.</p>

<p>Happy coding!</p>

<p>–Chris</p>

          
        </section>
        <dl>
            <dt>
              Next post  ↑
            </dt>
            <dd>
              <a href="https://fly.io/blog/scaling-llm-ollama/">
                Scaling Large Language Models to zero with Ollama
              </a>
            </dd>
            <dt>
              Previous post  ↓
            </dt>
            <dd>
              <a href="https://fly.io/blog/the-risks-of-building-apps-on-chatgpt/">
                The risks of building apps on ChatGPT
              </a>
            </dd>
        </dl>
      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Open Letter to the Python Software Foundation (292 pts)]]></title>
            <link>https://pythonafrica.blogspot.com/2023/12/an-open-letter-to-python-software_5.html</link>
            <guid>38542330</guid>
            <pubDate>Wed, 06 Dec 2023 10:45:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pythonafrica.blogspot.com/2023/12/an-open-letter-to-python-software_5.html">https://pythonafrica.blogspot.com/2023/12/an-open-letter-to-python-software_5.html</a>, See on <a href="https://news.ycombinator.com/item?id=38542330">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-9208814100498476329">
<p>Dear PSF,</p><p>We, organisers in the pan-African Python community, would like to raise some concerns and frustrations that have been brought to a head by recent events.</p><p>We held the first-ever DjangoCon Africa in November, a flagship event for our community, and the first Africa-wide Python event since 2019’s PyCon Africa.</p><p>The PSF Board approved a grant of USD 9000 for the conference (six days, 200 people). The event was a significant success, and the PSF’s support was key to that.</p><h2>Delays in the grant award process&nbsp;</h2><p>As acknowledged in the PSF’s article, “<a href="https://pyfound.blogspot.com/2023/10/september-october-board-votes.html" target="_blank">September &amp; October Board Votes</a>”, there were however some problems leading up to the grant decision.</p><p>First, the Grants Working Group was unable to come to a consensus on the request, so the decision was passed to the PSF Board. The Board was also unable to come to a consensus in its September meeting. Finally in the Board’s October meeting, the grant was approved. That was just a few weeks before the event started, nearly three months after its submission.</p><h2>Effect of the delays</h2><p>We would like to share a statement from the organisers of DjangoCon Africa describing the effect of these delays on the event and on them personally.&nbsp;</p><blockquote><p>For a considerable period of time, we were in doubt about a significant portion of our expected conference budget - from the PSF, whom we expected to be a steadfast ally and backer.&nbsp;</p><p>During that time, we felt quite stuck - unable to make decisions. Our daily conversations became centred around anxious what-if calculations. At one point we had reduced our budget for catering to a total of 5 USD per person per day, for breakfast, lunch and refreshments - even in Zanzibar, this is an unfeasibly low figure. We jettisoned one item after another from our budget.</p><p>We were unable to make decisions about financial assistance, which in turn delayed our ability to make decisions about the programme (how could we invite a speaker whom we knew would require some funds to travel if we couldn’t provide the funds?). We watched as the air-fares we had expected to cover for many of those people rose.</p><p>We had to answer all the people who couldn’t understand why we hadn’t decided on their talk proposals and grant applications, or who wondered why an event starting so soon had not yet even published a programme of talks. “What are we supposed to say to these people?” became another anxious topic in our meetings.</p><p>We felt unable to advertise or promote the event, because we simply didn’t know what we could promise people.</p><p>Some of the people affected had applied for visas - incurring expense - well in advance, on our advice; they too were waiting to hear back from us.</p><p>Locally, we had caterers and other businesses waiting for deposits and confirmation of contracts. Some lost patience with us. The local PyCon Tanzania organisation bore the brunt of this.&nbsp;</p><p>The organisation of DjangoCon Africa must have looked lazy, or incompetent, or worse, to someone looking at it from the outside.</p><p>The delay in a decision on funding from the PSF also made it harder for us to approach other organisations for funds - “Is the PSF sponsoring DjangoCon Africa? Why not?”.</p><p>It’s hard to describe the embarrassment we felt sometimes.</p><p>We had sleepless nights with worry - literally, not figuratively. More than one of us confided in another that we wished we had never started the project.</p><p>We started a fundraising campaign on GoFundMe to help cover the cost of financial assistance. It was a comfort to know that members of the international Python/Django community would stand up to support us, but the pleasure and gratitude we felt about that was overlaid with a feeling of humiliation that once again, a major African open-source software event had been obliged to publicly extend a begging-bowl.</p><p>At one point, gaps in our funding meant that the organisers faced a personal liability of almost USD 10,000 - funds actually spent, or committed, to make the event possible. &nbsp;</p><p>For any volunteer conference organiser, the weeks in the run-up to an event are full of hard work. This experience went far beyond that. Much of the pride and joy of staging DjangoCon Africa was sucked out of it for us.&nbsp;</p><p>Eventually, we received the grant funding we had applied for, though even this seemed to come with a humiliation: it happened after a white European spoke up publicly on behalf of the African Python community.</p><p>By the time we were able to start taking care of travellers who needed financial assistance, many of the air tickets had gone up significantly in price. Amongst the hard choices we had to make: one of our own organisers - a student, who has worked tirelessly in multiple events - was unable to attend because we could not afford to pay for her travel.</p><p>The organisers personally contributed well over USD 4000 to make the event possible in the form it took - funds contributed to a cause that we believe in, but it is not right that volunteer organisers of community events should be forced to make such choices.&nbsp;</p><p>We are genuinely grateful for the support we received from the PSF. The event was a success, and we are proud of what we achieved, but we remain perplexed and hurt by the problems we faced, and how our grant request was treated.</p></blockquote><h2>Problems within the PSF?</h2><p>It’s not clear to the wider African Python community why events unfolded in this way, though we are aware of some things that we have found very troubling.</p><p>We know that there are some extraordinary attitudes at work within the PSF. A PSF Board member once openly expressed the opinion that Anglo cultures always seem to be the ones that take the moral lead around the world, leaving others to follow their example. From any non-western perspective, this is an astounding idea to receive.</p><p>In the case of DjangoCon Africa, the first public response to our event on Mastodon was a negative response from a PSF Director, that in effect, cast doubt on the whole idea of a DjangoCon in Tanzania.</p><p>That’s not a solitary episode. We understand that (notwithstanding the PSF’s ambition to support Python in Africa) a PSF Director has consistently spoken out against funding for African events, over a period of years.&nbsp;</p><p>Our grant request was handled by the PSF’s Grants Working Group. We understand that one member of this group was able effectively to stall its decision-making long enough that the grant request had to be passed to the PSF Board.</p><p>At the PSF Board meeting in September 2023, a board member strategically used an abstention to ensure that a resolution to support our request could not pass. (Under the PSF rules, had they voted against the resolution, it would have passed 4-1. In the circumstances, other abstentions for different reasons - including one person who was required to abstain, as an organiser of &nbsp;DjangoCon Africa - meant that the resolution could not pass.) We are genuinely shocked by this. It’s one thing for a PSF Board member to vote against something they don’t believe in. It is quite another that someone has been able to weaponise the PSF’s voting system against an African event.</p><p>We are deeply troubled that such behaviours and values are actively at work inside the PSF. As an organisation, the PSF (and its Board and Working Groups and their processes) should be robust enough to stand up to individual prejudices, and not allow decision-making and deliberation to be derailed by individuals, however influential.</p><h2>The PSF and marginalised and at-risk groups</h2><p>We understand that the argument against support for DjangoCon Africa was that the host country, Tanzania, is not a safe place for the LGBTQIA+ community.</p><p>The PSF represents a global community, and has for years upheld high standards of inclusion and protection, paying special attention to the needs of those in marginalised and at-risk groups. Python community events around the world are effective safe spaces, that give strength to people who do not always find guarantees of safety elsewhere.</p><p>It is therefore especially shocking to have observed an attempt, coming from within the PSF, to pit the well-being and interests of two different excluded groups against each other, as if somehow the interests of members of the LGBTQIA+ community and of Africans are mutually exclusive.</p><p>Many questions can be asked about this reasoning.</p><p>What counts as “safety”? Which places in the world are truly safe for LGBTQIA+ community? How much of a city, or state, or country needs to be LGBTQIA+ hostile for the whole of it to be declared unworthy of PSF support? What does the PSF have to say to LGBTQIA+ community members in such locations? Are the LGBTQIA+ communities who are worthy of the PSF’s consideration only those who live in western countries, or do others count too? What does the PSF have to say to Python community organisers around the world who assert the community’s standards of inclusion, even in countries where it takes an act of bravery to do so?</p><p>And we would like to ask: when have questions been raised to check on whether western events present potential safety risks to non-western attendees, and when have non-western people been asked for their experiences?</p><p>All across the world, including the west, there are countries and places that are genuinely unsafe for members of particular groups, on the basis of their religion, ethnicity, language, gender, sexuality, nationality and other characteristics. Some of these risks may be obvious to westerners, or native English speakers, or men, and some of them may not. Simplistic judgements made from narrow perspectives will not enhance the safety of anyone in our community.</p><h2>Risk and the law</h2><p>The PSF and its directors quite correctly also observe the laws that apply to them. Yet we have witnessed discussions in which it has been proposed that volunteer organisers take public stances in their own countries that are not just contentious or socially unacceptable, but would actually violate local laws.&nbsp;</p><p>In one recent example, voices on the PSF Board were demanding that a condition of funding for a particular PyCon be the formal adoption of a “human rights plan” - a measure that would pose a significant legal and personal risk to its organisers.</p><p>The entitlement and assumption of cultural superiority embodied in these ideas are absurd and offensive.&nbsp;</p><h2>Guidance and consideration for non-western Python events grant awards</h2><p>At a meeting earlier this year, the PSF expressed concern that barely 16% of grants go to African communities.&nbsp;</p><p>At the same time, the perception within some African Python communities is that the PSF is less likely to award a grant to an African event, or will scrutinise it more harshly, or take longer to make an award.</p><p>For example, in 2019 and 2020 one Ugandan Python community made two grant requests that we understand received literally no response. In 2022, another grant request finally received attention from the working group when - with the event coming up in a matter of days - one of the PSF Directors connected to the community raised the issue with PSF staff, and a vote was initiated immediately.&nbsp;</p><p>This can be contrasted with the way a grant request for a European event was handled, at around the same time; the European request was made later, and dealt with sooner.</p><p>Inconsistency, lack of transparency and lack of clarity around expectations serve to undermine trust and confidence. The general perception within the Ugandan Python community is now that their events will only be given consideration if a PSF Director happens to take a personal interest in it.</p><p>In fact, other African organisers have reported timely responses and good communication from the PSF, so what is happening here?&nbsp;</p><p>Do some African grant requests lack quality or detail, because organisers failed to understand what was required? Are there enough people in the PSF with an adequate understanding of the challenges faced by non-western events? ​Is there a pattern where weaknesses in a grant request made from some regions in the world are given the benefit of the doubt, while others are treated less favourably?&nbsp;</p><p>We simply don’t know, and there could be a whole range of explanations. Whatever the underlying reasons, we need to understand and work together to address them, because the effects are harmful.&nbsp;</p><h2>Our requests to the Python Software Foundation&nbsp;</h2><h3>Transparency</h3><p>We request that the PSF undertake and publish a review of actual grant applications, to determine whether there indeed are differences in grant responsiveness, approval times, rejection rates and so on in response to requests from different regions.&nbsp;</p><p>We would like the PSF to publish clear expectations of timelines for handling grant requests, and for each final decision to be accompanied by a report showing how the case was actually handled.</p><h3>Guidance and feedback</h3><p>Organisers, and especially those operating without the benefit of long-standing networks of knowledge and shared expectations, need more guidance, and feedback they can act upon, especially in the case when a grant is rejected.&nbsp;</p><p>We ask that the PSF commits to developing - in collaboration with organisers, especially those in non-western regions - further materials and guidance to help organisers put in the best possible requests for funding. This could include a more proactive approach to working with those organisers to help them understand the PSF’s expectations and standards.</p><p>We request that the PSF institute a practice of providing clear feedback to grant applicants, to help improve and motivate subsequent applications. In cases of delay or doubt, we would like a practice of prompt, direct engagement with organisers to help clarify.</p><h3>Understanding of global needs</h3><p>We ask that the PSF as an organisation commits to a better understanding of global diversity and the realities, needs and challenges of non-western events and organisers.</p><p>This includes an understanding of financial realities. For example, the organisers of African events face the combined difficulties of lesser commercial sponsorship prospects, the expense of intra-African travel, vast geographical distances and so on. We need the PSF to understand these realities in its decision-making about financial awards to events.</p><h3>The law and marginalised groups</h3><p>We request that the PSF undertake a formal review of policies and bylaws, that incorporates expert legal advice and takes full account of the realities of laws and legal regimes across the world that apply to volunteer Python community organisers.</p><p>We recognise that all across the world, Python events are proposed in places where laws and practices mean that the rights of some individuals will be in jeopardy. We would like the PSF to recognise, formally and in its actual practices, that this includes the west, and that it is not only non-western events that should be subjected to critical scrutiny over this.</p><p>We also ask the PSF to adopt a constructive stance that requires all local organisers, wherever they may be, to consider the safeguarding of marginalised groups, and actively helps them improve safety, without ever demanding that volunteers be willing to violate local law or place themselves at risk while doing unpaid work on behalf of the PSF.&nbsp;</p><h2>Progress, prejudice and confidence</h2><p>In the past ten years, Python in Africa has developed with remarkable speed and success. In 2014 there was just one African PyCon, in South Africa. Since then PyCons and other events have been held all over the continent, and the communities behind them have grown in size, confidence, expertise and influence.</p><p>We can trace the introduction of Python teaching in universities across Africa and its spread across multiple commercial and non-commercial sectors to our work.</p><p>We have been generously supported, financially and morally, by the Python Software Foundation. Leaders like Ewa Jodlowska and Naomi Ceder have been part of that growth due to intentional support for our communities.</p><p>This has been a story of growth, motivation and courage.</p><p>More recent experiences have left us feeling hurt and angry. We hear voices, openly and confidently raised within the PSF, that denigrate us and our communities, that dismiss our experiences, that doubt our values, and harm us materially.</p><p>Our confidence in the PSF, and the confidence of many other people in our communities, has been shaken. Our motivation has taken some hammer blows. The work of the last decade risks being set back.</p><h2>Constructive collaboration</h2><p>We want to work with the PSF on everything addressed in this letter, in the spirit of constructive collaboration. We are willing to put our energies into building better practices and understanding. We want to be part of a solution to the concerns.</p><p>We ask the PSF to recognise our concerns, and not just to take them seriously, but to commit to working with the African Python community to address them, so that we do it together. &nbsp;</p><p>Sincerely,</p><p><b>Python Communities in Ghana, Namibia, Nigeria, Uganda, Tanzania, Mozambique &nbsp;South Africa, and Zimbabwe.</b></p><p>Abigail Mesrenyame Dogbe (Python Ghana)<br>Aisha Bello (Python Nigeria)<br>Anna Makarudze (Python Zimbabwe)<br>Chukwudi Nwachukwu (Python Nigeria)<br>Daniele Procida (Python Namibia)<br>Eusebio Simango (Python Mozambique)<br>Jessica Upani (Python Namibia)<br>Joannah Nanjekye (Python Uganda)<br>Julius Moshiro (Python Tanzania)<br>Mannie Young (Python Ghana)<br>Marlene Mhangami (Python Zimbabwe)<br>Noah Maina (Python Tanzania)<br>Sheena O’Connell (Python South Africa)</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[World's largest tokamak fusion reactor powers up (128 pts)]]></title>
            <link>https://newatlas.com/energy/worlds-largest-tokamak-fusion-reactor-powers-up/</link>
            <guid>38541451</guid>
            <pubDate>Wed, 06 Dec 2023 07:53:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/energy/worlds-largest-tokamak-fusion-reactor-powers-up/">https://newatlas.com/energy/worlds-largest-tokamak-fusion-reactor-powers-up/</a>, See on <a href="https://news.ycombinator.com/item?id=38541451">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                        <article>

                            
    


                            
    



                            

                            
                                
                                
                                    <div><ps-carousel data-direction="vertical" data-scroll-indicator="1" data-carousel-has-aside="">
    <template>
        <div class="FullscreenCarousel-slide" aria-hidden="true" tabindex="-1" style="position: absolute;" data-gallery-slide-index="inline">
            <div class="CarouselSlide" itemprop="image" itemtype="http://schema.org/ImageObject">
                <div class="CarouselSlide-media">
                    
                    <figure>
                        <img class="Image lazy" alt="Inline image" src="data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 16 9%22 /%3E">
                    </figure>
                </div>
                <div class="CarouselSlide-info">
                    <div class="CarouselSlide-numbers">
                        <span class="CarouselSlide-currentSlide"></span>/<span class="CarouselSlide-slidesLength"></span>
                    </div>
                    <div class="CarouselSlide-caption">
                        <div class="CarouselSlide-infoDescription" itemprop="caption"></div>
                        <div class="CarouselSlide-infoAttribution" itemprop="author"></div>
                    </div>
                </div>
            </div>
        </div>
    </template>

    

    

    <div itemprop="image" itemtype="http://schema.org/ImageObject"><p>JT-60SA has been under development since 1970</p><p>European Commission</p></div>

    <div itemprop="image" itemtype="http://schema.org/ImageObject" id="slide-1" data-gallery-slide-index="0">
        
            <p><span>1</span>/<span>1</span>
            </p>
        
        
            <div><p>JT-60SA has been under development since 1970</p><p>European Commission</p></div>
        
    </div>

    
        
    

    
</ps-carousel></div>
                                

                                
                            


                            


                            <div><p>The world's largest and most advanced tokamak fusion reactor has gone online as the EU/Japanese 370-tonne JT-60SA reactor was fired up for the first time during an inauguration ceremony in Ibaraki Prefecture, Japan.</p><p>First conceived by Soviet scientists in the 1950s, tokamaks are toroidal reactors that are one of the leading contenders to become the first commercially viable fusion power plants. The name is a Russian acronym for Toroidal Chamber with Magnetic Coils and consists of a large doughnut-shaped chamber surrounded by magnetic coils that compress a plasma made of hydrogen isotopes until it reaches pressures and temperatures that are only found in the interior of the Sun to initiate fusion.</p><p>In concept, it's a simple machine and achieving fusion is relatively easy, but in practice it's extremely difficult to build a reactor that can maintain a sustained fusion reaction that generates more power than is fed into it. The Japan Torus-60 (JT-60) project has been running since 1970 and the JT-60SA is that latest and biggest iteration.</p><p>The JT-60SA is currently a joint project by the EU and Japan, with participation by Britain, which signed a separate agreement after leaving the Union. The original reactor was upgraded several times as technology evolved, resulting in a complete disassembly and reassembly in 2013, with work finishing in 2020. Unfortunately, this was followed by a massive electrical short in 2021 that necessitated two years of repairs.</p><p>The initiation of operations for JT-60SA was inaugurated on December 1, 2023 by the EU’s Commissioner for Energy Kadri Simson and Japan’s Minister of Education, Culture, Sports, Science and Technology (MEXT) Masahito Moriyama in a formal ceremony. Though the upgraded reactor still isn't anywhere near to being a practical power generator, it will be used to overcome many outstanding problems as well as testing materials and procedures that will be needed for commercial stations.</p><p>For 75 years we've been told that fusion power was only 25 years away and billions of dollars have been spent to make it practical. However, since successful fusion power would provide humanity with unlimited clean power forever, a little patience might be in order.</p><p>The video below shows the upgrade of the JT-60SA.</p><div data-video-disable-history="" data-align-center="">
    
        <p><ps-youtubeplayer data-video-player="" data-player-id="f458ac38c71f54ff8a7266a4dfd187990" data-video-id="Eno5410tO0g" data-video-title="JT 60SA組立動画">

    <iframe id="YouTubeVideoPlayer-f458ac38c71f54ff8a7266a4dfd187990" role="application" title="YouTube embedded video player" allowfullscreen="" loading="lazy" src="https://www.youtube.com/embed/Eno5410tO0g?enablejsapi=1"></iframe>
</ps-youtubeplayer>
</p>
    
    
        <p>JT60-SA</p>
    
</div><p>Source: <a href="https://energy.ec.europa.eu/news/eu-and-japan-celebrate-start-operations-jt-60sa-fusion-reactor-and-reaffirm-close-cooperation-fusion-2023-12-01_en" target="_blank" data-cms-ai="0">European Commission</a></p></div>

                            
                                    
                            
                        </article>

                        

                        

<div>
<h2>Tags</h2>
    
</div>



                        
                            
                        

                        
    <div>
        
        
        <p><a aria-label="David Szondy" href="https://newatlas.com/author/david-szondy/" data-cms-ai="0">
            <img alt="David Szondy" loading="lazy" src="https://assets.newatlas.com/dims4/default/2737b65/2147483647/strip/true/crop/500x500+0+0/resize/100x100!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F58%2Feb%2Fc0a71279440998ce939ec841c996%2Fd-szondy-use1.png" srcset="https://assets.newatlas.com/dims4/default/f7e82ae/2147483647/strip/true/crop/500x500+0+0/resize/80x80!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F58%2Feb%2Fc0a71279440998ce939ec841c996%2Fd-szondy-use1.png 80w,https://assets.newatlas.com/dims4/default/9f95b82/2147483647/strip/true/crop/500x500+0+0/resize/160x160!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F58%2Feb%2Fc0a71279440998ce939ec841c996%2Fd-szondy-use1.png 160w" sizes="(min-width: 768px) 60px, 80px" width="100" height="100">
            </a>
        </p>
        
        

        <div>
            
                
            
            <p>
                
                    David Szondy is a playwright, author and journalist based in Seattle, Washington. A retired field archaeologist and university lecturer, he has a background in the history of science, technology, and medicine with a particular emphasis on aerospace, military, and cybernetic subjects. In addition, he is the author of four award-winning plays, a novel, reviews, and a plethora of scholarly works ranging from industrial archaeology to law. David has worked as a feature writer for many international magazines and has been a feature writer for New Atlas since 2011.
                
            </p>
        </div>
    </div>



                        
    


                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Outlook Blocking All Email from Tutanota.com Domain as Spam (214 pts)]]></title>
            <link>https://tuta.com/blog/posts/outlook-falsely-marks-tutanota-emails-as-junk</link>
            <guid>38541355</guid>
            <pubDate>Wed, 06 Dec 2023 07:39:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tuta.com/blog/posts/outlook-falsely-marks-tutanota-emails-as-junk">https://tuta.com/blog/posts/outlook-falsely-marks-tutanota-emails-as-junk</a>, See on <a href="https://news.ycombinator.com/item?id=38541355">Hacker News</a></p>
Couldn't get https://tuta.com/blog/posts/outlook-falsely-marks-tutanota-emails-as-junk: Error: Request failed with status code 404]]></description>
        </item>
        <item>
            <title><![CDATA[Why is Jepsen written in Clojure? (415 pts)]]></title>
            <link>https://aphyr.com/posts/367-why-is-jepsen-written-in-clojure</link>
            <guid>38540761</guid>
            <pubDate>Wed, 06 Dec 2023 05:32:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aphyr.com/posts/367-why-is-jepsen-written-in-clojure">https://aphyr.com/posts/367-why-is-jepsen-written-in-clojure</a>, See on <a href="https://news.ycombinator.com/item?id=38540761">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>People keep asking why <a href="https://jepsen.io/">Jepsen</a> is written in <a href="https://clojure.org/">Clojure</a>, so I figure it’s worth having a referencable answer. I’ve programmed in something like twenty languages. Why choose a Weird Lisp?</p>
<p>Jepsen is built for testing concurrent systems–mostly databases. Because it tests concurrent systems, the language itself needs good support for concurrency. Clojure’s immutable, persistent data structures make it easier to write correct concurrent programs, and the language and runtime have excellent concurrency support: real threads, promises, futures, atoms, locks, queues, cyclic barriers, all of java.util.concurrent, etc. I also considered languages (like Haskell) with more rigorous control over side effects, but decided that Clojure’s less-dogmatic approach was preferable.</p>
<p>Because Jepsen tests databases, it needs broad client support. Almost every database has a JVM client, typically written in Java, and Clojure has decent Java interop.</p>
<p>Because testing is experimental work, I needed a language which was concise, adaptable, and well-suited to prototyping. Clojure is terse, and its syntactic flexibility–in particular, its macro system–work well for that. In particular the threading macros make chained transformations readable, and macros enable re-usable error handling and easy control of resource scopes. The Clojure REPL is really handy for exploring the data a test run produces.</p>
<p>Tests involve representing, transforming, and inspecting complex, nested data structures. Clojure’s data structures and standard library functions are possibly the best I’ve ever seen. I also print a lot of structures to the console and files: Clojure’s data syntax (EDN) is fantastic for this.</p>
<p>Because tests involve manipulating a decent, but not huge, chunk of data, I needed a language with “good enough” performance. Clojure’s certainly not the fastest language out there, but idiomatic Clojure is usually within an order of magnitude or two of Java, and I can shave off the difference where critical. The JVM has excellent profiling tools, and these work well with Clojure.</p>
<p>Jepsen’s (gosh) about a decade old now: I wanted a language with a mature core and emphasis on stability. Clojure is remarkably stable, both in terms of JVM target and the language itself. Libraries don’t “rot” anywhere near as quickly as in Scala or Ruby.</p>
<p>Clojure does have significant drawbacks. It has a small engineering community and no (broadly-accepted, successful) static typing system. Both of these would constrain a large team, but Jepsen’s maintained and used by only 1-3 people at a time. Working with JVM primitives can be frustrating without dropping to Java; I do this on occasion. Some aspects of the polymorphism system are lacking, but these can be worked around with libraries. The error messages are terrible. I have no apologetics for this. ;-)</p>
<p>I prototyped Jepsen in a few different languages before settling on Clojure. A decade in, I think it was a pretty good tradeoff.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JSONB Has Landed in SQLite (586 pts)]]></title>
            <link>https://sqlite.org/forum/forumpost/fa6f64e3dc1a5d97</link>
            <guid>38540421</guid>
            <pubDate>Wed, 06 Dec 2023 04:16:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sqlite.org/forum/forumpost/fa6f64e3dc1a5d97">https://sqlite.org/forum/forumpost/fa6f64e3dc1a5d97</a>, See on <a href="https://news.ycombinator.com/item?id=38540421">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>JSONB is a rewrite of the <a href="https://sqlite.org/draft/json1.html">SQLite JSON functions</a>
that, depending on usage patterns, could be several times faster
than the original JSON functions.  This enhancement has now
<a href="https://sqlite.org/src/info/7f0c79b94e8f55e5">landed on trunk</a>.</p>

<p>Developers who use JSON heavily in their applications are encouraged
to download a <a href="https://sqlite.org/download.html">pre-release snapshot</a>
and give the new code a try.</p>

<h2>How Is This Different?</h2>
<p>Functions that deal with text JSON use a three-step process:</p>

<ol>
<li><p>Parse the text JSON into an internal binary format that is
  more accessible to C-code.</p></li>
<li><p>Carry out the requested operation.  Maybe this is looking up
  a field in an object, or perhaps it is modifying the JSON in
  some way.</p></li>
<li><p>If the processing involved changing the JSON, convert the
  internal binary format back into 
  <a href="https://www.rfc-editor.org/rfc/rfc8259">RFC-8279</a> JSON text for
  output and/or storage.</p></li>
</ol>

<p>Step 2 is the essential operation that you want to accomplish.  Steps
1 and 3 are just overhead.</p>

<p>Historically, SQLite used an internal binary
representation of JSON that involved lots of pointers.  This fits
will into C programs, but it is difficult to serialize.
The JSONB rewrite changes the internal-use binary representation of
JSON into a contiguous byte array that can read or written as an SQL BLOB.
This allows the internal-use representation of JSON to potentially be
saved to the database, in place of JSON text, eliminating the overhead
of steps 1 and 3.</p>

<h2>What has changed?</h2>
<p>All legacy functionality is preserved.  The only change has been to add
new capabilities.</p>

<p>Any JSON function that accepts JSON text as an input will now also accept
JSONB binary content for that same parameter.  You do not have to tell the
function if it getting text or binary data.  It figures that out for itself.</p>

<p>JSON functions that output JSON now come in two versions.  The historical
"<tt>json_</tt>" functions works as before.  But there is now a corresponding
"<tt>jsonb_</tt>" function that returns JSONB rather than text JSON, thus
omitting step 3 in the normal processing.</p>

<p>If you don't make any changes to your application, everything should
continue to work as it always has, though perhaps slightly (1%) faster.</p>

<p>But if you modify your application to start storing JSONB instead of text
JSON, you might see a 3-times performance improvement, at least for the
JSON-intensive operations.  JSONB is also slightly smaller than text JSON
in most cases (about 5% or 10% smaller) so you might also see a modest
reduction in your database size if you use a lot of JSON.</p>

<h2>Migrating</h2>
<p>Note that all functions accept both text JSON and JSONB.  So to start using
JSONB, you do <u>not</u> have to modify your database files to convert
legacy text JSON into JSONB.  Just start writing out JSONB for new entries.
The old entries will continue to work.  The new entries will just work
faster.</p>

<p>Or, if you do want to convert all your legacy data to JSONB, you can just
run an update operation like:</p>

<blockquote>
<pre><code>UPDATE bigtable SET jsonColumn = jsonb(jsonColumn);
</code></pre></blockquote>

<h2>Please provide comments</h2>
<p>If you find this enhancement useful, or if you try it out and see performance
regressions or bugs, please let us know.  Leave a follow-up post here, or
contact me directly at drh at sqlite dot org.</p>

<p>The current plan is to release the JSONB enhancement in the next
major release of SQLite - version 3.45.0.  That will probably occur
in a month or two.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A new quantum algorithm for classical mechanics with an exponential speedup (119 pts)]]></title>
            <link>https://blog.research.google/2023/12/a-new-quantum-algorithm-for-classical.html</link>
            <guid>38539374</guid>
            <pubDate>Wed, 06 Dec 2023 01:23:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.research.google/2023/12/a-new-quantum-algorithm-for-classical.html">https://blog.research.google/2023/12/a-new-quantum-algorithm-for-classical.html</a>, See on <a href="https://news.ycombinator.com/item?id=38539374">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-3665865722098768988">
<p><span>Posted by Robin Kothari and Rolando Somma, Research Scientists, Google Research, Quantum AI Team</span>

</p><p>
Quantum computers promise to solve some problems exponentially faster than classical computers, but there are only a handful of examples with such a dramatic speedup, such as <a href="https://en.wikipedia.org/wiki/Shor%27s_algorithm">Shor’s factoring algorithm</a> and <a href="https://blog.research.google/2023/10/developing-industrial-use-cases-for.html">quantum simulation</a>. Of those few examples, the majority of them involve simulating physical systems that are inherently quantum mechanical — a natural application for quantum computers. But what about simulating systems that are not inherently quantum? Can quantum computers offer an exponential advantage for this?
</p>
<p>
In “<a href="https://link.aps.org/doi/10.1103/PhysRevX.13.041041">Exponential quantum speedup in simulating coupled classical oscillators</a>”, published in <a href="https://journals.aps.org/prx/">Physical Review X</a> (PRX) and presented at the <a href="https://focs.computer.org/2023/">Symposium on Foundations of Computer Science</a> (FOCS 2023), we report on the discovery of a new quantum algorithm that offers an exponential advantage for simulating coupled <a href="https://en.wikipedia.org/wiki/Harmonic_oscillator">classical harmonic oscillators</a>. These are some of the most fundamental, ubiquitous systems in nature and can describe the physics of countless natural systems, from electrical circuits to molecular vibrations to the mechanics of bridges. In collaboration with Dominic Berry of Macquarie University and Nathan Wiebe of the University of Toronto, we found a mapping that can transform any system involving coupled oscillators into a problem describing the time evolution of a quantum system. Given certain constraints, this problem can be solved with a quantum computer exponentially faster than it can with a classical computer. Further, we use this mapping to prove that any problem efficiently solvable by a quantum algorithm can be recast as a problem involving a network of coupled oscillators, albeit exponentially many of them. In addition to unlocking previously unknown applications of quantum computers, this result provides a new method of designing new quantum algorithms by reasoning purely about classical systems.
</p>
<br> 

<h2>Simulating coupled oscillators</h2>


<p>
The systems we consider consist of classical harmonic oscillators. An example of a single harmonic oscillator is a mass (such as a ball) attached to a spring. If you displace the mass from its rest position, then the spring will induce a restoring force, pushing or pulling the mass in the opposite direction. This restoring force causes the mass to oscillate back and forth.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE19xRL5DLh42HtY6Q-mDXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/s359/oscillator.gif"><img data-original-height="116" data-original-width="359" height="129" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE19xRL5DLh42HtY6Q-mDXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/w400-h129/oscillator.gif" width="400"></a></td></tr><tr><td>A simple example of a harmonic oscillator is a mass connected to a wall by a spring. [Image Source: <a href="https://commons.wikimedia.org/wiki/File:Simple_harmonic_oscillator.gif">Wikimedia</a>]</td></tr></tbody></table>

<p>
Now consider <em>coupled </em>harmonic oscillators, where <em>multiple</em> masses are attached to one another through springs. Displace one mass, and it will induce a wave of oscillations to pulse through the system. As one might expect, simulating the oscillations of a large number of masses on a classical computer gets increasingly difficult.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IKfNVVBX3Z2Dm8sVxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkUKE-mFxJ3e9sdM-ct3lU81Gs1/s1129/image4.png"><img data-original-height="832" data-original-width="1129" height="295" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IKfNVVBX3Z2Dm8sVxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkUKE-mFxJ3e9sdM-ct3lU81Gs1/w400-h295/image4.png" width="400"></a></td></tr><tr><td>An example system of masses connected by springs that can be simulated with the quantum algorithm.</td></tr></tbody></table>

<p>
To enable the simulation of a large number of coupled harmonic oscillators, we came up with a mapping that encodes the positions and velocities of all masses and springs into the quantum wavefunction of a system of qubits. Since the number of parameters describing the wavefunction of a system of qubits grows exponentially with the number of qubits, we can encode the information of <em>N</em> balls into a quantum mechanical system of only about log(<em>N</em>) qubits. As long as there is a compact description of the system (i.e., the properties of the masses and the springs), we can evolve the wavefunction to learn coordinates of the balls and springs at a later time with far fewer resources than if we had used a naïve classical approach to simulate the balls and springs.
</p>
<p>
We showed that a certain class of coupled-classical oscillator systems can be efficiently simulated on a quantum computer. But this alone does not rule out the possibility that there exists some as-yet-unknown clever classical algorithm that is similarly efficient in its use of resources. To show that our quantum algorithm achieves an exponential speedup over <em>any</em> possible classical algorithm, we provide two additional pieces of evidence.
</p>
<br> 

<h2>The glued-trees problem and the quantum oracle</h2>


<p>
For the first piece of evidence, we use our mapping to show that the quantum algorithm can efficiently solve a famous problem about graphs known to be difficult to solve classically, called the <a href="https://arxiv.org/abs/quant-ph/0209131">glued-trees problem</a>. The problem takes two branching trees — a graph whose nodes each branch to two more nodes, resembling the branching paths of a tree — and glues their branches together through a random set of edges, as shown in the figure below.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6mVvFQ-twhlL3EjLKDHKHDw0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s930/image2.png"><img data-original-height="556" data-original-width="930" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6mVvFQ-twhlL3EjLKDHKHDw0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s16000/image2.png"></a></td></tr><tr><td>A visual representation of the glued trees problem. Here we start at the node labeled ENTRANCE and are allowed to locally explore the graph, which is obtained by randomly gluing together two binary trees. The goal is to find the node labeled EXIT.</td></tr></tbody></table>

<p>
The goal of the glued-trees problem is to find the exit node — the “root” of the second tree — as efficiently as possible. But the exact configuration of the nodes and edges of the glued trees are initially hidden from us. To learn about the system, we must query an <a href="https://en.wikipedia.org/wiki/Oracle_machine">oracle</a>, which can answer specific questions about the setup. This oracle allows us to explore the trees, but only locally. Decades ago, <a href="https://doi.org/10.1145/780542.780552">it was shown</a> that the number of queries required to find the exit node on a classical computer is proportional to a polynomial factor of <em>N</em>, the total number of nodes. 
</p>
<p>
But recasting this as a problem with balls and springs, we can imagine each node as a ball and each connection between two nodes as a spring. Pluck the entrance node (the root of the first tree), and the oscillations will pulse through the trees. It only takes a time that scales with the <em>depth</em> of the tree — which is exponentially smaller than <em>N</em> — to reach the exit node. So, by mapping the glued-trees ball-and-spring system to a quantum system and evolving it for that time, we can detect the vibrations of the exit node and determine it exponentially faster than we could using a classical computer.
</p>
<br> 

<h2>BQP-completeness</h2>


<p>
The second and strongest piece of evidence that our algorithm is exponentially more efficient than any possible classical algorithm is revealed by examination of the set of problems a quantum computer can solve efficiently (i.e., solvable in <a href="https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time">polynomial time</a>), referred to as <a href="https://en.wikipedia.org/wiki/BQP">bounded-error quantum polynomial time</a> or BQP. The hardest problems in BQP are called “BQP-complete”. 
</p>
<p>
While it is generally accepted that there exist some problems that a quantum algorithm can solve efficiently and a classical algorithm cannot, this has not yet been proven. So, the best evidence we can provide is that our problem is BQP-complete, that is, it is among the hardest problems in BQP. If someone were to find an efficient classical algorithm for solving our problem, then every problem solved by a quantum computer efficiently would be classically solvable! Not even the <a href="https://en.wikipedia.org/wiki/Integer_factorization">factoring problem</a> (finding the prime factors of a given large number), which forms the basis of <a href="https://en.wikipedia.org/wiki/RSA_(cryptosystem)">modern encryption</a> and was famously solved by Shor’s algorithm, is expected to be BQP-complete. 
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0ZnDD4buC25_mmmwAmuXiWgCEKNuhJGOzpbBEt4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s574/image1.png"><img data-original-height="563" data-original-width="574" height="314" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0ZnDD4buC25_mmmwAmuXiWgCEKNuhJGOzpbBEt4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s320/image1.png" width="320"></a></td></tr><tr><td>A diagram showing the believed relationships of the classes BPP and BQP, which are the set of problems that can be efficiently solved on a classical computer and quantum computer, respectively. BQP-complete problems are the hardest problems in BQP.</td></tr></tbody></table>

<p>
To show that our problem of simulating balls and springs is indeed BQP-complete, we start with a standard BQP-complete problem of simulating universal quantum circuits, and show that every quantum circuit can be expressed as a system of many balls coupled with springs. Therefore, our problem is also BQP-complete. 
</p>
<br> 

<h2>Implications and future work</h2>


<p>
This effort also sheds light on work from 2002, when theoretical computer scientist Lov K. Grover and his colleague, Anirvan M. Sengupta, used an <a href="https://journals.aps.org/pra/abstract/10.1103/PhysRevA.65.032319">analogy to coupled</a> pendulums to illustrate how Grover’s famous quantum <a href="https://en.wikipedia.org/wiki/Grover%27s_algorithm">search algorithm</a> could find the correct element in an unsorted database quadratically faster than could be done classically. With the proper setup and initial conditions, it would be possible to tell whether one of <em>N</em> pendulums was different from the others — the analogue of finding the correct element in a database — after the system had evolved for time that was only ~√(<em>N)</em>. While this hints at a connection between certain classical oscillating systems and quantum algorithms, it falls short of explaining why Grover’s quantum algorithm achieves a quantum advantage.
</p>
<p>
Our results make that connection precise. We showed that the dynamics of any classical system of harmonic oscillators can indeed be equivalently understood as the dynamics of a corresponding quantum system of exponentially smaller size. In this way we can simulate Grover and Sengupta’s system of pendulums on a quantum computer of log(<em>N</em>) qubits, and find a different quantum algorithm that can find the correct element in time ~√(<em>N</em>). The analogy we discovered between classical and quantum systems can be used to construct other quantum algorithms offering exponential speedups, where the reason for the speedups is now more evident from the way that classical waves propagate.
</p>
<p>
Our work also reveals that every quantum algorithm can be equivalently understood as the propagation of a classical wave in a system of coupled oscillators. This would imply that, for example, we can in principle build a classical system that solves the factoring problem after it has evolved for time that is exponentially smaller than the runtime of any known classical algorithm that solves factoring. This may look like an efficient classical algorithm for factoring, but the catch is that the number of oscillators is exponentially large, making it an impractical way to solve factoring.
</p>
<p>
Coupled harmonic oscillators are ubiquitous in nature, describing a broad range of systems from electrical circuits to chains of molecules to structures such as bridges. While our work here focuses on the fundamental complexity of this broad class of problems, we expect that it will guide us in searching for real-world examples of harmonic oscillator problems in which a quantum computer could offer an exponential advantage. 
</p>
<br> 

<h2>Acknowledgements</h2>


<p>
<em>We would like to thank our Quantum Computing Science Communicator, Katie McCormick, for helping to write this blog post.</em>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Go Testing by Example [video] (164 pts)]]></title>
            <link>https://research.swtch.com/testing</link>
            <guid>38539174</guid>
            <pubDate>Wed, 06 Dec 2023 00:58:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.swtch.com/testing">https://research.swtch.com/testing</a>, See on <a href="https://news.ycombinator.com/item?id=38539174">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <h2>Go Testing By Example
        
        <div>
        <p>
          
            Posted on Tuesday, December 5, 2023.
            
          
        </p>
        </div>
        </h2>
        

<p>
I opened GopherCon Australia in early November with the talk “Go Testing By Example”.
Being the first talk, there were some A/V issues, so I re-recorded it at home and have posted it here:
</p><p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/X4rxi9jStLo?si=DJiEGUPNxPlYnlWL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p>


<p>
Here are the 20 tips from the talk:
</p><ol>
<li>
Make it easy to add new test cases.
</li><li>
Use test coverage to find untested code.
</li><li>
Coverage is no substitute for thought.
</li><li>
Write exhaustive tests.
</li><li>
Separate test cases from test logic.
</li><li>
Look for special cases.
</li><li>
If you didn’t add a test, you didn’t fix the bug.
</li><li>
Not everything fits in a table.
</li><li>
Test cases can be in testdata files.
</li><li>
Compare against other implementations.
</li><li>
Make test failures readable.
</li><li>
If the answer can change, write code to update them.
</li><li>
Use <a href="https://pkg.go.dev/golang.org/x/tools/txtar">txtar</a> for multi-file test cases.
</li><li>
Annotate existing formats to create testing mini-languages.
</li><li>
Write parsers and printers to simplify tests.
</li><li>
Code quality is limited by test quality.
</li><li>
Scripts make good tests.
</li><li>
Try <a href="https://pkg.go.dev/rsc.io/script">rsc.io/script</a> for your own script-based test cases.
</li><li>
Improve your tests over time.
</li><li>
Aim for continuous deployment.</li></ol>


<p>
Enjoy!
      </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An update on Twitch in Korea (200 pts)]]></title>
            <link>https://blog.twitch.tv/en/2023/12/05/an-update-on-twitch-in-korea/</link>
            <guid>38539167</guid>
            <pubDate>Wed, 06 Dec 2023 00:57:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.twitch.tv/en/2023/12/05/an-update-on-twitch-in-korea/">https://blog.twitch.tv/en/2023/12/05/an-update-on-twitch-in-korea/</a>, See on <a href="https://news.ycombinator.com/item?id=38539167">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tw-impression-id="body"><p>This morning, I shared with our community in Korea that we’ve made the difficult decision to shut down the Twitch business in Korea on February 27, 2024 KST. We understand that this is extremely disappointing news, and we want to explain why we made this decision and how we are planning to support those impacted.</p><p>Ultimately, the cost to operate Twitch in Korea is prohibitively expensive and we have spent significant effort working to reduce these costs so that we could find a way for the Twitch business to remain in Korea. First, we experimented with a peer-to-peer model for source quality. Then, we adjusted source quality to a maximum of 720p. While we have lowered costs from these efforts, our network fees in Korea are still 10 times more expensive than in most other countries. Twitch has been operating in Korea at a significant loss, and unfortunately there is no pathway forward for our business to run more sustainably in that country.</p><p>To all of our global communities, we want to make it clear that this is a unique situation. Operating costs in Korea are significantly higher than they are in other countries and we have been open about this challenge for some time.</p><p>Twitch streamers in Korea have devoted significant time and effort into building their communities, and we plan to help these communities find new homes — even if it’s regrettably not on Twitch. We will work to help Twitch streamers in Korea move their communities to alternative livestreaming services in Korea. We are also reaching out to several of these services to help with the transition and will communicate with impacted streamers as those discussions progress.</p><p>I want to reiterate that this was a very difficult decision and one we are very disappointed we had to make. Korea has always and will continue to play a special role in the international esports community and we are incredibly grateful for the communities they built on Twitch.</p><p>For more information, please see our <a href="https://twitch-web.app.link/e/NLGoBvzBYEb">Help article</a> or join our live stream where I’ll be taking the community’s questions. We will host a stream for our Korean community on /TwitchKR today, December 6 at 9:30 am KST (December 5, at 4:30pm PT). For people outside of the Korean community, I will host another session on /Twitch today, December 6 at 11am KST (December 5, at 6pm PT) to answer questions about this decision or other topics.</p><p>Dan Clancy, Twitch CEO</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MLX: An array framework for Apple Silicon (208 pts)]]></title>
            <link>https://github.com/ml-explore/mlx</link>
            <guid>38539153</guid>
            <pubDate>Wed, 06 Dec 2023 00:56:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ml-explore/mlx">https://github.com/ml-explore/mlx</a>, See on <a href="https://news.ycombinator.com/item?id=38539153">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">MLX</h2>
<p dir="auto"><a href="#quickstart"><strong>Quickstart</strong></a> | <a href="#installation"><strong>Installation</strong></a> |
<a href="https://ml-explore.github.io/mlx/build/html/index.html" rel="nofollow"><strong>Documentation</strong></a> |
<a href="#examples"><strong>Examples</strong></a></p>
<p dir="auto">MLX is an array framework for machine learning on Apple silicon, brought to you
by Apple machine learning research.</p>
<p dir="auto">Some key features of MLX include:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Familiar APIs</strong>: MLX has a Python API that closely follows NumPy.
MLX also has a fully featured C++ API, which closely mirrors the Python API.
MLX has higher-level packages like <code>mlx.nn</code> and <code>mlx.optimizers</code> with APIs
that closely follow PyTorch to simplify building more complex models.</p>
</li>
<li>
<p dir="auto"><strong>Composable function transformations</strong>: MLX has composable function
transformations for automatic differentiation, automatic vectorization,
and computation graph optimization.</p>
</li>
<li>
<p dir="auto"><strong>Lazy computation</strong>: Computations in MLX are lazy. Arrays are only
materialized when needed.</p>
</li>
<li>
<p dir="auto"><strong>Dynamic graph construction</strong>: Computation graphs in MLX are built
dynamically. Changing the shapes of function arguments does not trigger
slow compilations, and debugging is simple and intuitive.</p>
</li>
<li>
<p dir="auto"><strong>Multi-device</strong>: Operations can run on any of the supported devices
(currently, the CPU and GPU).</p>
</li>
<li>
<p dir="auto"><strong>Unified memory</strong>: A notable difference from MLX and other frameworks
is the <em>unified memory model</em>. Arrays in MLX live in shared memory.
Operations on MLX arrays can be performed on any of the supported
device types without moving data.</p>
</li>
</ul>
<p dir="auto">MLX is designed by machine learning researchers for machine learning
researchers. The framework is intended to be user-friendly, but still efficient
to train and deploy models. The design of the framework itself is also
conceptually simple. We intend to make it easy for researchers to extend and
improve MLX with the goal of quickly exploring new ideas.</p>
<p dir="auto">The design of MLX is inspired by frameworks like
<a href="https://numpy.org/doc/stable/index.html" rel="nofollow">NumPy</a>,
<a href="https://pytorch.org/" rel="nofollow">PyTorch</a>, <a href="https://github.com/google/jax">Jax</a>, and
<a href="https://arrayfire.org/" rel="nofollow">ArrayFire</a>.</p>
<h2 tabindex="-1" dir="auto">Examples</h2>
<p dir="auto">The <a href="https://github.com/ml-explore/mlx-examples">MLX examples repo</a> has a
variety of examples, including:</p>
<ul dir="auto">
<li><a href="https://github.com/ml-explore/mlx-examples/tree/main/transformer_lm">Transformer language model</a> training.</li>
<li>Large-scale text generation with
<a href="https://github.com/ml-explore/mlx-examples/tree/main/llama">LLaMA</a> and
finetuning with <a href="https://github.com/ml-explore/mlx-examples/tree/main/lora">LoRA</a>.</li>
<li>Generating images with <a href="https://github.com/ml-explore/mlx-examples/tree/main/stable_diffusion">Stable Diffusion</a>.</li>
<li>Speech recognition with <a href="https://github.com/ml-explore/mlx-examples/tree/main/whisper">OpenAI's Whisper</a>.</li>
</ul>
<h2 tabindex="-1" dir="auto">Quickstart</h2>
<p dir="auto">See the <a href="https://ml-explore.github.io/mlx/build/html/quick_start.html" rel="nofollow">quick start
guide</a>
in the documentation.</p>
<h2 tabindex="-1" dir="auto">Installation</h2>
<p dir="auto">MLX is available on <a href="https://pypi.org/project/mlx/" rel="nofollow">PyPi</a>. To install the Python API, run:</p>

<p dir="auto">Checkout the
<a href="https://ml-explore.github.io/mlx/build/html/install.html#" rel="nofollow">documentation</a>
for more information on building the C++ and Python APIs from source.</p>
<h2 tabindex="-1" dir="auto">Contributing</h2>
<p dir="auto">Check out the <a href="https://github.com/ml-explore/mlx/blob/main/CONTRIBUTING.md">contribution guidelines</a> for more information
on contributing to MLX.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MLX: NumPy like framework for Apple Silicon by Apple (145 pts)]]></title>
            <link>https://ml-explore.github.io/mlx/build/html/index.html</link>
            <guid>38539020</guid>
            <pubDate>Wed, 06 Dec 2023 00:40:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ml-explore.github.io/mlx/build/html/index.html">https://ml-explore.github.io/mlx/build/html/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=38539020">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      
      
      <main id="main-content">
        
        



          <div>
              
              
              
              



              
                

                <article role="main">
                  
  <section id="mlx">
<h2>MLX<a href="#mlx" title="Permalink to this heading">#</a></h2>
<p>MLX is a NumPy-like array framework designed for efficient and flexible machine
learning on Apple silicon, brought to you by Apple machine learning research.</p>
<p>The Python API closely follows NumPy with a few exceptions. MLX also has a
fully featured C++ API which closely follows the Python API.</p>
<p>The main differences between MLX and NumPy are:</p>
<blockquote>
<div><ul>
<li><p><strong>Composable function transformations</strong>: MLX has composable function
transformations for automatic differentiation, automatic vectorization,
and computation graph optimization.</p></li>
<li><p><strong>Lazy computation</strong>: Computations in MLX are lazy. Arrays are only
materialized when needed.</p></li>
<li><p><strong>Multi-device</strong>: Operations can run on any of the supported devices (CPU,
GPU, …)</p></li>
</ul>
</div></blockquote>
<p>The design of MLX is inspired by frameworks like <a href="https://pytorch.org/">PyTorch</a>, <a href="https://github.com/google/jax">Jax</a>, and
<a href="https://arrayfire.org/">ArrayFire</a>. A noteable difference from these
frameworks and MLX is the <em>unified memory model</em>. Arrays in MLX live in shared
memory. Operations on MLX arrays can be performed on any of the supported
device types without performing data copies. Currently supported device types
are the CPU and GPU.</p>






</section>


                </article>
              

              
              
              
              
                
              
            </div>
          
        

      </main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Does uBlock Origin bypass the latest YouTube anti-adblock script? (238 pts)]]></title>
            <link>https://drhyperion451.github.io/does-uBO-bypass-yt/</link>
            <guid>38538236</guid>
            <pubDate>Tue, 05 Dec 2023 23:11:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://drhyperion451.github.io/does-uBO-bypass-yt/">https://drhyperion451.github.io/does-uBO-bypass-yt/</a>, See on <a href="https://news.ycombinator.com/item?id=38538236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-detector">
        <p><span id="main-question">Does uBlock Origin (uBO) bypass the latest YouTube anti-adblock script?</span></p>
        <p><span id="main-answer">Not sure</span>
        </p>
        
         
    </div><div id="about">
        <!-- class ="aa-blocked" -> Activated when the website response is 'yes'-->
        <!-- class ="not-aa-blocked" -> Activated when the website response is 'no'-->
        <h2>What does it mean?</h2>
        <p>We are currently evaluating if uBlock Origin's filters have been updated to deal with the latest Anti-Adblocker script. This website doesn't check your device. It simply compares text files provided by the uBO team to let you know the current status of uBO's solutions for YouTube.</p>
        
        
            
            
            
        <h2>What should I do now?</h2>
        <p>Please come back later.</p>
                        
        <!--Button to auto-update quick filters.-->
        
        

        <h2>What does this website do?</h2>
        <p>It simply gets the info of the latest <a href="https://raw.githubusercontent.com/stephenhawk8054/misc/main/yt-fix.txt">YT script ID solved by uBlock Origin</a> and compares it against the latest <a href="https://pastefy.app/G1Txv5su/raw">YouTube Anti-Adblocker script ID</a>.
            If it's the same, then the uBlock Origin team has finally updated their filters. If it's not, a fix is on the way. This website does not check if your own uBlock Origin version is up-to-date.</p>

        <h2>How can I contribute to this website?</h2>
        <p>You can make changes and pull request to the <a href="https://github.com/drHyperion451/does-uBO-bypass-yt/tree/dev"><code>dev</code></a> branch.</p>
        <p>Please keep in mind we are all volunteers. We don't get any revenue from this and we cannot be online 24/7. Thanks for understanding!</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Playstation is erasing 1,318 seasons of Discovery shows from customer libraries (188 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2023/12/playstation-is-erasing-1318-seasons-of-discovery-shows-from-customer-libraries/</link>
            <guid>38538162</guid>
            <pubDate>Tue, 05 Dec 2023 23:01:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2023/12/playstation-is-erasing-1318-seasons-of-discovery-shows-from-customer-libraries/">https://arstechnica.com/gadgets/2023/12/playstation-is-erasing-1318-seasons-of-discovery-shows-from-customer-libraries/</a>, See on <a href="https://news.ycombinator.com/item?id=38538162">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      No refunds mentioned    —
</h4>
            
            <h2 itemprop="description">The change comes as Warner Bros. tries to add subscribers to Max, Discovery+ apps.</h2>
                    </header>
        <section>
            <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/mythbusters-800x447.jpg" alt="mythbusters">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/12/mythbusters.jpg" data-height="1073" data-width="1920">Enlarge</a> <span>/</span> Myth: You own the digital content you buy.</p></figcaption>  </figure>

  




<!-- cache hit 419:single/related:d9041d325cf5f210d63753721da064aa --><!-- empty -->
<p>If you purchased any Discovery shows from the PlayStation Store, Sony has some bad news for you to discover.</p>
<p>The company recently announced that all Discovery content purchased on the PlayStation Store will be erased before 2024. The brief <a href="https://www.playstation.com/en-us/legal/psvideocontent/">notice</a>, signed by the PlayStation Store, says:</p>
<blockquote><p>As of 31 December 2023, due to our content licensing arrangements with content providers, you will no longer be able to watch any of your previously purchased Discovery content and the content will be removed from your video library.</p>
<p>We sincerely thank you for your continued support.</p></blockquote>
<p>PlayStation Network started selling TV shows and movies with 2008's PlayStation 3, and at the time you were allowed to transfer content to different Sony devices, <a href="https://kotaku.com/sony-ps4-ps5-discovery-mythbusters-tv-1851066164">Kotaku</a> noted. That feature went away with the PlayStation 4. With the growth of <a href="https://arstechnica.com/culture/2023/08/the-tv-streaming-apps-broke-their-promises-and-now-theyre-jacking-the-price/">streaming TV apps</a>, many of which could be accessed through a PlayStation, the PlayStation Store <a href="https://blog.playstation.com/2021/03/02/playstation-store-to-discontinue-movie-and-tv-purchases-and-rentals/">stopped selling</a> movies and TV shows in 2021.</p>
<p>But there were users who had already purchased stuff from the PlayStation Store and, believe it or not, expect to be able to watch it when they want, since they paid money to buy (rather than rent) it. I admit that I haven't heard a lot of the shows being deleted post-purchase. Shows getting axed from user libraries include <em>Wives With Knives</em>,<em> An Idiot Abroad</em>,<em> Evil Twins</em>, and <i>Body Bizarre</i>. And when it came to deadly docuseries, PlayStation Store offered plenty, whether you were after<em> Deadly Affairs</em>,<em> Demands</em>,<em> Dentists</em>,<em> Devotion</em>,<em> Sins</em>, or, of course, <em>Women</em>.</p>
<p>I'm having fun with some of the most bizarre titles, of course. But there are also plenty of more well-known titles on the list of purchased content being revoked, including <em>American Chopper</em>, <em>Cake Boss</em>,<em> MythBusters</em>, <em>Shark Week</em>, and <em>Say Yes to the Dress.&nbsp;</em></p>
<p>While some of the content listed sounds, shall we say, a bit niche, there are in total 1,318 seasons of shows listed for deletion. That means there's a good chance numerous users will be affected by Sony's announcement.</p>

                                                </div>

            
            
                            <nav>Page: <span>1 <a href="https://arstechnica.com/gadgets/2023/12/playstation-is-erasing-1318-seasons-of-discovery-shows-from-customer-libraries/2/">2</a> <a href="https://arstechnica.com/gadgets/2023/12/playstation-is-erasing-1318-seasons-of-discovery-shows-from-customer-libraries/2/"><span>Next <span>→</span></span></a></span></nav>
            
        </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[All my favorite tracing tools (206 pts)]]></title>
            <link>https://thume.ca/2023/12/02/tracing-methods/</link>
            <guid>38538111</guid>
            <pubDate>Tue, 05 Dec 2023 22:56:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thume.ca/2023/12/02/tracing-methods/">https://thume.ca/2023/12/02/tracing-methods/</a>, See on <a href="https://news.ycombinator.com/item?id=38538111">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<p>Ever wanted more different ways to understand what’s going on in a program? Here I catalogue a huge variety of tracing methods you can use for varying types of problems. Tracing has been such a long-standing interest (and job) of mine that some of these will novel and interesting to anyone who reads this. I’ll guarantee it by including 2 novel tracing tools I’ve made and haven’t shared before (look for this: <span><em>Tooling drop!</em></span>).</p>
<p>What I see as the key parts of tracing are collecting timestamped data on what happened in a system, and then ideally visualizing it in a timeline UI instead of just as a text log. First I’ll cover my favorite ways of really easily getting trace data into a nice timeline UI, because it’s a superpower that makes all the other tracing tools more interesting. Then I’ll go over ways to get that data, everything from instrumentation to binary patching to processor hardware features.</p>
<p>I’ll also give a real-life example of combining eBPF tracing with Perfetto visualization to diagnose tail latency issues in huge traces by using a number of neat tricks. Look for the “eBPF Example” section.</p>
<p><strong>Note:</strong> I’m hiring for my accelerator optimization team at Anthropic! See <a href="#conclusion-if-you-liked-this-you-may-like-my-team-at-anthropic">the bottom of the post</a> for more detail.</p>
<h2 id="easily-visualizing-data-on-a-trace-timeline">Easily visualizing data on a trace timeline</h2>
<p>Getting event data onto a nice zoomable timeline UI is way easier than most people think. Here’s my favorite method I do all the time which can take you from logging your data to visualizing it in minutes:</p>
<div><pre><code><span># from:
</span><span>print</span><span>(</span><span>"%d: %s %d"</span> <span>%</span> <span>(</span><span>event_name</span><span>,</span> <span>timestamp</span><span>,</span> <span>duration</span><span>))</span>
<span># to:
</span><span>with</span> <span>open</span><span>(</span><span>'trace.json'</span><span>,</span><span>'w'</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
  <span>f</span><span>.</span><span>print</span><span>(</span><span>"["</span><span>)</span>
  <span>f</span><span>.</span><span>print</span><span>(</span><span>'{"name": "%s", "ts": %d, "dur": %d, "cat": "hi", "ph": "X", "pid": 1, "tid": 1, "args": {}}</span><span>\n</span><span>'</span> <span>%</span>
    <span>(</span><span>event_name</span><span>,</span> <span>timestamp</span><span>,</span> <span>duration</span><span>))</span>
  <span>f</span><span>.</span><span>print</span><span>(</span><span>"]"</span><span>)</span> <span># this closing ] isn't actually required
</span></code></pre></div>
<p>This is the power of the <a href="https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview">Chromium Event JSON Format</a>. It’s a super simple JSON format that supports a bunch of different kinds of events, and is supported by a lot of different profile visualizer tools.</p>
<p>You can view the resulting tracing files in Google’s Perfetto trace viewer by going to <a href="https://ui.perfetto.dev/">https://ui.perfetto.dev/</a>, or in the older Catapult viewer (which is nicer for some traces) by going to <code>chrome://tracing</code> in Chrome. You can play around with the UI by <a href="https://ui.perfetto.dev/">going to Perfetto</a> and clicking “Open Chrome Example” in the sidebar. Here’s a screenshot showing an event annotated with arguments and flow event arrows:</p>
<p><a href="https://thume.ca/assets/postassets/tracing/perfetto.png"><img src="https://thume.ca/assets/postassets/tracing/perfetto.png" alt="Perfetto Screenshot"></a></p>
<p>Me and my coworkers do this all the time at work, whip up trace visualizations for new data sources in under an hour and add them to our growing set of trace tools. We have a Python utility to turn a trace file into a clickable permanently-saved intranet link we can share with coworkers in Slack. This is easy to set up by building a copy of Perfetto and uploading to a file hosting server you control, and then putting trace files on that server and generating links using Perfetto’s <code>?url=</code> parameter. We also write custom trace analysis scripts by loading the simple JSON into a Pandas dataframe.</p>
<p>I like Perfetto as its use of WebAssembly lets it scale to about 10x more events than Catapult (although it gets laggy), and you have the escape hatch of the native backend for even bigger traces. Its <a href="https://perfetto.dev/docs/analysis/common-queries">SQL query feature</a> also lets you find events and annotate them in the UI using arbitrary predicates, including <a href="https://perfetto.dev/docs/analysis/stdlib-docs">special SQL functions</a> for dealing with trace stacks.</p>
<p><strong>UI protip</strong>: Press <code>?</code> in Perfetto to see the shortcuts. I use both <code>WASD</code> and <code>CTRL+scroll</code> to move around.</p>
<h3 id="advanced-format-fuchsia-trace-format">Advanced Format: Fuchsia Trace Format</h3>
<p>The Chromium JSON format can produce gigantic files and be very slow for large traces, because it repeats both the field names and string values for every event. Perfetto also supports the <a href="https://fuchsia.dev/fuchsia-src/reference/tracing/trace-format">Fuchsia Trace Format (FTF)</a> which is a simple compact binary format with an incredible spec doc that makes it easy to produce binary traces. It supports interning strings to avoid repeating event names, and is designed around 64 byte words and supports clock bases so that you can directly write timestamp counters and have the UI compute the true time.</p>
<p>When I worked at Jane Street I <a href="https://github.com/janestreet/tracing/blob/master/zero/writer.ml">used this to log instrumentation events to a buffer directly in FTF</a> as they occurred in &lt;10ns per span (it would have been closer to 4ns if it wasn’t for OCaml limitations).</p>
<h3 id="advanced-format-perfetto-protobuf">Advanced Format: Perfetto Protobuf</h3>
<p>Another format which is similarly compact, and also supports more features, is <a href="https://github.com/google/perfetto/blob/master/protos/perfetto/trace/perfetto_trace.proto">Perfetto’s native Protobuf trace format</a>. It’s documented only in comments in the proto files and is a bit trickier to figure out, but might be a bit easier to generate if you have access to a protobuf library. It enables access to advanced Perfetto features like including callstack samples in a trace, which aren’t available with other formats. It’s slower to write than FTF, although Perfetto has a <a href="https://perfetto.dev/docs/design-docs/protozero">ProtoZero</a> library to make it somewhat faster.</p>
<p>This can be really tricky to get right though and I had to reference the Perfetto source code to figure out error codes in the “info and stats” tab a lot. The biggest gotchas are you need to set <code>trusted_packet_sequence_id</code> on every packet, have a <code>TrackDescriptor</code> for every track, and set <code>sequence_flags=SEQ_INCREMENTAl_STATE_CLEARED</code> on the first packet.</p>
<h3 id="other-tools">Other tools</h3>
<p>Some other nice trace visualization tools are <a href="https://github.com/jlfwong/speedscope">Speedscope</a> which is better for a hybrid between profile and trace visualization, <a href="https://github.com/google/pprof">pprof</a> for pure profile call graph visualization, and <a href="https://www.rerun.io/">Rerun</a> for multimodal 3D visualization. Other profile viewers I like less but which have some nice parts include <a href="https://eclipse.dev/tracecompass/">Trace Compass</a> and the <a href="https://profiler.firefox.com/docs/#/">Firefox Profiler</a>.</p>
<h2 id="tracing-methods">Tracing Methods</h2>
<p>Now lets go over all sorts of different neat tracing methods! I’ll start with some obscure and interesting low level ones but I promise I’ll get to some more broadly usable ones after.</p>
<h2 id="hardware-breakpoints">Hardware breakpoints</h2>
<p>For ages, processors have supported <strong>hardware breakpoint registers</strong> which let you put in a small number of memory addresses and have the processor interrupt itself when any of them are accessed or executed.</p>
<h3 id="perf-and-perftrace">perf and perftrace</h3>
<p>Linux exposes this functionality through <code>ptrace</code> but also through the <a href="https://man7.org/linux/man-pages/man2/perf_event_open.2.html"><code>perf_event_open</code> syscall</a> and the <a href="https://man7.org/linux/man-pages/man1/perf-record.1.html"><code>perf record</code> command</a>. You can record a process like <code>perf record -e \mem:0x1000/8:rwx my_command</code> and view the results with <code>perf script</code>. It costs about 3us of overhead every time a breakpoint is hit.</p>
<p><span><em>Tooling drop!</em></span> I wrote <a href="https://github.com/trishume/perftrace">a tiny Python library called perftrace</a> with a C stub which calls the <code>perf_event_open</code> syscall to record timestamps and register values when the breakpoints were hit.</p>
<p>It currently only supports execution breakpoints but you can also breakpoint on reads or writes of any memory and it would be <a href="https://github.com/trishume/perftrace/blob/d074e65bf71e8af10335164111969f96263d283a/perftrace.c#L61">easy to modify the code to do that</a>. Hardware breakpoints are basically the only way to watch for accessing a specific memory address at a fine granularity which doesn’t add overhead to code which doesn’t touch that memory.</p>
<h3 id="gdb-scripting">GDB scripting</h3>
<p>In addition to using it manually, you can automate the process of following the execution of a program using debugger breakpoints by using GDB’s Python scripting interface. This is slower than perf breakpoints but gives you the ability to inspect and modify memory when you hit breakpoints. <a href="https://github.com/hugsy/gef">GEF</a> is an extension to GDB that in addition to making it much nicer in general, also extends the Python API with a bunch of handy utilities.</p>
<p><span><em>Tooling drop!</em></span> <a href="https://gist.github.com/trishume/fe3b3b90a7e524c976ecb98053bb7f86">Here’s an example GDB script I wrote using GEF which gives examples of how to puppeteer, trace and inspect a program</a></p>
<h2 id="intel-processor-trace">Intel Processor Trace</h2>
<p><a href="https://easyperf.net/blog/2019/08/23/Intel-Processor-Trace">Intel Processor Trace</a> is a hardware technology on Intel chips since Skylake which allows recording a trace of <em>every instruction the processor executes</em> via recording enough info to reconstruct the control flow in a super-compact format, along with fine-grained timing info. It has extremely low overhead since it’s done by hardware and writes bypass the cache so the only overhead is reducing main memory bandwidth by about 1GB/s. I see no noticeable overhead at all on most program benchmarks I’ve tested.</p>
<p>You can access a dump of the assembly instructions executed in a recorded region using <a href="https://man7.org/linux/man-pages/man1/perf-intel-pt.1.html"><code>perf</code></a>, <a href="https://lldb.llvm.org/use/intel_pt.html"><code>lldb</code></a> and <a href="https://easyperf.net/blog/2019/08/30/Intel-PT-part2"><code>gdb</code></a>.</p>
<h3 id="magic-trace">magic-trace</h3>
<p>However assembly traces aren’t useful to most people, so when at Jane Street I created <a href="https://github.com/janestreet/magic-trace">magic-trace</a> along with my intern Chris Lambert, which generates a trace file (using FTF and Perfetto as described above) which visualizes <em>every function call</em> in a program execution. Jane Street generously open-sourced it so anyone can use it! Since then it’s been extended to support tracing into the kernel as well. I wrote <a href="https://blog.janestreet.com/magic-trace/">a blog post about how it works for the Jane Street tech blog</a>.</p>
<p><img src="https://github.com/janestreet/magic-trace/raw/master/docs/assets/stage-3.gif" alt="magic-trace demo"></p>
<p>Processor Trace can record to a ring buffer, and <code>magic-trace</code> uses the hardware breakpoint feature described earlier to let you trigger capture of the last 10ms whenever some function that signals an event you want to look at happened, or when the program ends. This makes it great for a bunch of scenarios:</p>
<ul>
<li>Debugging rare tail latency events: Add a trigger function call after something takes unusually long, and then leave magic-trace attached in production. Because it captures everything you’ll never have not logged enough data to identify the slow part.</li>
<li>Everyday performance analysis: A full trace timeline can be easier to interpret than a sampling profiler visualization, especially because it displays the difference between a million fast calls to a function and one slow call.
<ul>
<li>It’s typical to find performance problems on systems that had only ever been analyzed with a sampling profiler by noticing the first time you magic-trace the program that many functions are being called more times than expected or in locations you didn’t expect.</li>
</ul>
</li>
<li>Debugging crashes: When a program crashes for reasons you don’t understand, you can just run it under magic-trace and see every function call leading up to the crash, which is often enough to figure out why the crash happened without adding extra logging or using a debugger!</li>
</ul>
<p>If you want to modify magic-trace to suit your needs, it’s open-source OCaml. And if you like Rust more than OCaml someone made a simple Rust port called <a href="https://github.com/michoecho/perf2perfetto">perf2perfetto</a>.</p>
<p>Unfortunately, Processor Trace isn’t supported on many virtual machines that use compatible Intel Hardware. Complain to your cloud provider to add support in their hypervisor or try bare-metal instances!</p>
<h2 id="instrumentation-based-tracing-profilers">Instrumentation-based tracing profilers</h2>
<p>What most people use to get similar benefits to magic-trace traces, especially in the gamedev industry, is low-overhead instrumentation-based profilers with custom UIs. One major advantage of instrumentation-based traces is they can contain extra information about data and not just control flow, putting arguments from your functions into the trace can be key for figuring out what’s going on. These tools often support including other data sources such as OS scheduling info, CPU samples and GPU trace data. Here’s my favorite tools like this and their pros/cons:</p>
<h3 id="tracy"><a href="https://github.com/wolfpld/tracy">Tracy</a></h3>
<p><a href="https://github.com/wolfpld/tracy"><img src="https://github.com/wolfpld/tracy/raw/master/doc/profiler.png" alt="Tracy screenshot"></a></p>
<ul>
<li>Cross platform, including good Linux sampling and scheduling capture</li>
<li>Overhead of only 2ns/span, supports giant traces with hundreds of millions of events</li>
<li>Really nice and fast UI with tons of features (check out the <a href="https://www.youtube.com/watch?v=30wpRpHTTag">demo</a> <a href="https://www.youtube.com/watch?v=_hU7vw00MZ4">videos</a> in the readme)</li>
<li>Integrates CPU sampling with detailed source and assembly analysis</li>
<li>Popular so there are bindings in non-C++ languages like <a href="https://docs.rs/tracing-tracy/latest/tracing_tracy/">Rust</a> and <a href="https://github.com/nektro/zig-tracy">Zig</a>.</li>
<li>Con: Only supports a single string/number argument to events</li>
<li>Con: Timeline is overly aggressive in collapsing small events into squiggles (<a href="https://thume.ca/2021/03/14/iforests/">see my post on this</a>).</li>
</ul>
<h3 id="optick"><a href="https://github.com/bombomby/optick">Optick</a></h3>
<p><a href="https://www.youtube.com/watch?v=p57TV5342fo"><img src="https://github.com/bombomby/brofiler/raw/gh-pages/images/VideoThumbnail.jpg" alt="Optick screenshot"></a></p>
<ul>
<li>Cross-platform, lots of features, very nice UI</li>
<li>Supports multiple named arguments per event</li>
<li>Con: Not as fleshed-out for non-game applications</li>
<li>Con: sampling integration only works on Windows</li>
</ul>
<h3 id="perfetto"><a href="https://perfetto.dev/docs/instrumentation/tracing-sdk">Perfetto</a></h3>
<ul>
<li>Perfetto UI is nice, events can include arguments and flow event arrows</li>
<li>Integrates with other Perfetto data sources like OS events and sampling</li>
<li>Con: Higher overhead of around 600ns/span when tracing enabled</li>
<li>Con: UI doesn’t scale to traces as large as the above two programs</li>
</ul>
<h3 id="other-programs">Other programs</h3>
<p>There’s a bunch more similar small programs that generally come with their own instrumentation library and their own WebGL profile viewer. These are generally more lightweight and can be easier to integrate. For example <a href="https://gravitymoth.com/spall/spall-web.html">Spall</a>, <a href="https://github.com/jonasmr/microprofile">microprofile</a>, <a href="https://github.com/Celtoys/Remotery">Remotery</a>, <a href="https://github.com/EmbarkStudios/puffin">Puffin (Rust-native)</a>, <a href="https://github.com/mikesart/gpuvis">gpuviz</a>. I must also mention the <a href="https://github.com/janestreet/tracing">OCaml tracing instrumentation library I wrote for Jane Street</a> which has overheads under 10ns/span via a compile-time macro like the C++ libraries.</p>
<h2 id="ebpf">eBPF</h2>
<p>If you want to trace things using the Linux kernel there’s a new game in town, and it’s awesome. The eBPF subsystem allows you to attach complex programs to all sorts of different things in the kernel and efficiently shuttle data back to userspace, basically subsuming all the legacy facilities like ftrace and kprobes such that I won’t talk about them.</p>
<p>Things you can trace include: syscalls, low overhead tracepoints throughout the kernel, hardware performance counters, any kernel function call and arbitrary breakpoints or function calls/returns in userspace code. Combined these basically let you see anything on the system in or out of userspace.</p>
<p>You normally write BPF programs in C but there are perhaps even nicer toolkits for using <a href="https://github.com/tw4452852/zbpf">Zig</a> and <a href="https://aya-rs.dev/">Rust</a>.</p>
<p>There’s <a href="https://ebpf.io/applications/">a whole bunch of ways to use eBPF</a> and I’ll talk about some of my favorites here. Some other favorites I won’t go into in detail are <a href="https://rubrikinc.github.io/wachy/">Wachy</a> and <a href="https://github.com/anakryiko/retsnoop">retsnoop</a>.</p>
<h3 id="bcc-easy-python-api-for-ebpf">BCC: Easy Python API for eBPF</h3>
<p>The <a href="https://github.com/iovisor/bcc">BPF Compiler Collection (BCC)</a> is a library with really nice Python bindings for compiling eBPF programs from C source code, injecting them, and getting the data back. It has a really nice feature where you can write a C struct to hold the event data you want to record, and then it will parse that and expose it so you can access the fields in Python. Check out <a href="https://github.com/iovisor/bcc/blob/master/examples/ringbuf/ringbuf_output.py">how simple this syscall tracing example is</a>.</p>
<p>I really like having the full power of Python to control my tracing scripts. BCC scripts often use Python string templating to do compile time metaprogramming of the C to compose the exact probe script you want, and then do data post-processing in Python to present things nicely.</p>
<h3 id="bpftrace-terse-dsl-for-ebpf-tracing">bpftrace: terse DSL for eBPF tracing</h3>
<p>If you want a terser way to compose tracing programs, in the style of dtrace, check out <a href="https://github.com/iovisor/bpftrace">bpftrace</a>. It lets you write one liners like these:</p>
<div><pre><code><span># Files opened by process</span>
bpftrace <span>-e</span> <span>'tracepoint:syscalls:sys_enter_open { printf("%s %s\n", comm, str(args-&gt;filename)); }'</span>

<span># Count LLC cache misses by process name and PID (uses PMCs):</span>
bpftrace <span>-e</span> <span>'hardware:cache-misses:1000000 { @[comm, pid] = count(); }'</span>
</code></pre></div>
<h3 id="ply-simpler-bpftrace">ply: simpler bpftrace</h3>
<p>If you want something like bpftrace but simpler and faster with no LLVM dependencies. Check out <a href="https://wkz.github.io/ply/">ply</a>.</p>
<div><pre><code><span># Which processes are receiving errors when reading from the VFS?</span>
ply <span>'kretprobe:vfs_read if (retval &lt; 0) { @[pid, comm, retval] = count(); }'</span>
</code></pre></div>
<h2 id="ebpf-example-anthropics-perfetto-based-packet-and-user-event-tracing">eBPF Example: Anthropic’s Perfetto-based packet and user event tracing</h2>
<p>For work at Anthropic I wanted to analyze tail latency of some networking code so I used BCC and hooked into low-overhead kernel probe points to trace info from every single packet into a ring buffer. I could even include fields pulled from the packet header and NIC queue information, all at 1 million packets per second with no noticeable overhead.</p>
<h3 id="trick-for-tracing-userspace-events-with-low-overhead-in-ebpf">Trick for tracing userspace events with low overhead in eBPF</h3>
<p>I wanted to correlate packets with userspace events from a Python program, so I used a fun trick: Find a syscall which has an early-exit error path and bindings in most languages, and then trace calls to that which have specific arguments which produce an error. I traced the <code>faccessat2</code> syscall such that in Python <code>os.access(event_name, -932, dir_fd=-event_type)</code> where <code>event_type</code> was an enum for start, stop and instant events would log spans to my Perfetto trace. This had an overhead of around 700ns/event, which is in a similar league to Perfetto’s full-userspace C++ instrumentation, and a lot of that is Python call overhead. The <code>os.access</code> function is especially good because when the syscall errors it doesn’t incur overhead by generating a Python exception like most other syscall wrappers do.</p>
<h3 id="how-to-process-events-more-quickly-using-a-c-helper-with-bcc">How to process events more quickly using a C helper with BCC</h3>
<p>With 1 million packets per second I had a problem that with rare tail latency events, my traces quickly got huge and lagged Perfetto. I wanted to only keep data from shortly before one of my userspace send events took too long. Normally you’d do this with a circular buffer that gets snapshotted, and it would be possible to implement that in eBPF. But I didn’t want to implement my own ringbuf and the included ones don’t support wraparound overwriting. So instead I used the internal <code>_open_ring_buffer</code> function to register a ctypes C function as a ringbuffer callback instead of a Python function, and wrote an efficient C callback to filter out packets near a tail latency event before passing those to Python.</p>
<h3 id="perks-of-perfetto-visualization">Perks of Perfetto visualization</h3>
<p>I used the Perfetto Protobuf format with interned strings in order to keep trace size down to a few bytes per packet.</p>
<p>I could use Perfetto’s SQL support in the resulting trace to query for send events above a certain time threshold after startup in a specific process. Here’s a screenshot showing a long send event coinciding with packets starting to be paced out with larger gaps on one of the queues, including the ability to have line graph tracks:</p>
<p><a href="https://thume.ca/assets/postassets/tracing/packettrace.png"><img src="https://thume.ca/assets/postassets/tracing/packettrace.png" alt="Perfetto Packet Trace"></a></p>
<p>I think it’s kinda crazy that we have all these different mostly-text-based BPF tools rather than a framework that lets you put all sorts of different kinds of system events into a trace UI, including easily scripting your own new events. It’s so much easier to investigate this kind of thing with a timeline UI. I started building that framework at Anthropic, but only spent a week on it since I’ve had higher priority things to do since I did the packet latency investigation.</p>
<h2 id="binary-instrumentation">Binary Instrumentation</h2>
<p>When you’re instrumenting userspace programs in a way where the overhead of kernel breakpoints is too high, but you don’t have access to the source code, perhaps because you’re reverse-engineering something, then it may be time for binary instrumentation.</p>
<h3 id="bpftime-ebpf-based-binary-instrumentation">bpftime: eBPF-based binary instrumentation</h3>
<p>One easy way that’s a good segue is <a href="https://github.com/eunomia-bpf/bpftime">bpftime</a> which takes your existing eBPF programs with userspace probes, and runs them much faster by patching the instructions to run the BPF program inside the process rather than incurring 3us of kernel interrupt overhead every time.</p>
<h3 id="e9patch">E9Patch</h3>
<p>For more sophisticated binary patching on x86, look to <a href="https://github.com/GJDuck/e9patch">E9Patch</a>.</p>
<p>On some architectures, patching can be really easy since you just patch the instruction you want to trace with a jump to a piece of “trampoline” code which has your instrumentation, and then the original instruction and a jump back.</p>
<p>It’s much harder on x86 since instructions are variable length, so if you just patch a jump over a target instruction, occasionally that’ll cause problems since some other instruction jumps to an instruction your longer jump had to stomp over.</p>
<p>People have invented all kinds of clever tricks to get around these issues including “instruction punning” where you put your patch code at addresses which are also valid x86 nop or trap instructions. E9Patch implements very advanced versions of these tricks such that the patching should basically always work.</p>
<p>It comes with an API as well as a tool called <a href="https://github.com/GJDuck/e9patch/blob/master/doc/e9tool-user-guide.md">E9Tool</a> which lets you patch using a command line interface:</p>
<div><pre><code><span># print all jump instructions in the xterm binary</span>
<span>$ </span>e9tool <span>-M</span> jmp <span>-P</span> print xterm
jz 0x4064d5
jz 0x452c36
...
</code></pre></div>
<h3 id="frida">Frida</h3>
<p>The other way to get around the difficulty of static patching, when you have to be conservative around how jumps you don’t know about could be messed up by your patches, is dynamic binary instrumentation, where you basically puppeteer the execution of the program. This is the technique used by JIT VMs like Rosetta and QEMU to basically recompile your program as you run it.</p>
<p><a href="https://frida.re/">Frida</a> exposes this incredibly powerful technique in a general way you can script in Javascript using its “Stalker” interface. Allowing you to attach JS snippets to pieces of code or rewrite the assembly as it is run. It also lets you do more standard patching, although it doesn’t work as well on x86 as E9Patch.</p>
<h2 id="ld_preload">LD_PRELOAD</h2>
<p>If you just want to trace a function in a dynamic library like libc, you can use <code>LD_PRELOAD</code> to inject a library of your own to replace any functions you like. You can use <code>dlsym(RTLD_NEXT, "fn_name")</code> to get the old implementation in order to wrap it. Check out <a href="https://axcheron.github.io/playing-with-ld_preload/">this tutorial post</a> for how.</p>
<h2 id="distributed-tracing">Distributed Tracing</h2>
<p>Distributed Tracing is where you can trace across different services via attaching special headers to requests and sending all the timing data back to a trace server. Some popular solutions are <a href="https://opentelemetry.io/">OpenTelemetry</a> (of which there are many implementations and UIs) and <a href="https://zipkin.io/">Zipkin</a>.</p>
<p>There’s some cool new solutions like <a href="https://odigos.io/">Odigos</a> that use eBPF to add distributed tracing support without any instrumentation.</p>
<h2 id="sampling-profilers">Sampling Profilers</h2>
<p>Sampling profilers take a sample of the full call stack of your program periodically. Typical profiler UIs don’t have the time axis I’d think of as part of “tracing”, but some UIs do. For example <a href="https://github.com/jlfwong/speedscope">Speedscope</a> accepts many profiler data formats and can visualize with a time axis, and <a href="https://github.com/mstange/samply">Samply</a> is an easy to use profiler which uses the Firefox Profiler UI, which also has a timeline view.</p>
<p>One neat sampling method used by <a href="https://github.com/benfred/py-spy">py-spy</a> and <a href="https://rbspy.github.io/">rbspy</a> is to use the <a href="https://man7.org/linux/man-pages/man2/process_vm_readv.2.html"><code>process_vm_readv</code> syscall</a> to read memory out of a process without interrupting it. If like an interpreter the process stores info about what it’s doing in memory, this can allow you to follow it with no overhead on the target process. You could even use this trick for low-overhead native program instrumentation: set up a little stack data structure where you push and pop pointers to span names or other context info, and then sample it from another program when needed using eBPF or <code>process_vm_readv</code>.</p>
<h2 id="qemu-instrumentation">QEMU Instrumentation</h2>
<p>When all other tracing tools fail, sometimes you have to fall back on the most powerful tool in the tracing toolbox: Full emulation and hooking into QEMU’s JIT compiler. This theoretically allows you to trace and patch both control flow <em>and</em> memory, in both userspace and the kernel, including snapshot and restore, across many architectures and operating systems.</p>
<p>However, actually doing this is not for the faint of heart and the tooling for it only barely exists.</p>
<h3 id="cannoli">Cannoli</h3>
<p><a href="https://github.com/MarginResearch/cannoli">Cannoli</a> is a tracing engine for qemu-user (so no kernel stuff) which patches QEMU to log execution and memory events to a high-performance ringbuffer read by a Rust extension you compile. This lets it trace with very low overhead by spreading the load of following the trace over many cores, at the cost of not being able to modify the execution.</p>
<p>It’s a bit tricky to use, you have to compile QEMU and Cannoli yourself at the moment, and it’s kind of a prototype so when I’ve used it in the past for CTFs I’ve often had to add new features to it.</p>
<h3 id="qemu-tcg-plugins">QEMU TCG Plugins</h3>
<p>QEMU has recently added <a href="https://www.qemu.org/docs/master/devel/tcg-plugins.html">plugin support for its TCG JIT</a>. Like Cannoli this is read-only for now, and its likely slower than Cannoli, but it works in qemu-system mode and exposes slightly different functionality.</p>
<h3 id="usercorn">usercorn</h3>
<p>My friend has an old project called <a href="https://github.com/lunixbochs/usercorn">usercorn</a> that is mostly bitrotted but has the ability to trace programs using QEMU and analyze them with Lua scripts and all sorts of fancy trace analysis. Someone (possibly him eventually) could theoretically revive it and rebase it on top of something like QEMU TCG plugins.</p>
<h2 id="conclusion-if-you-liked-this-you-may-like-my-team-at-anthropic">Conclusion: If you liked this you may like my team at Anthropic</h2>
<p>If you made it to the bottom and enjoyed all those different tracing strategies, you may also be interested in working on my team!</p>
<p>I lead the performance optimization team at <a href="https://www.anthropic.com/">Anthropic</a> (we build one of the world’s leading large language models, and have a heavy focus on figuring out how future more powerful models can go well for the world). We’ll be doing accelerator kernel optimization across GPUs, TPUs and Trainium. TPUs and Trainium are cool in that they’re simpler architectures where optimization is more like a cycle-counting puzzle, and they also have <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-profile-user-guide.html">amazing tracing tools</a>. Almost nobody knows these new architectures, so we’re currently hiring high potential people with other kinds of low-level optimization experience who are willing to learn.</p>
<p>I plan for us to do a bunch of optimization work as compiler-style transformation passes over IRs, but simpler via being bespoke to the ML architecture we’re optimizing. These will parallelize architectures across machines, within a machine, and within a chip in similar ways. We also work closely with an amazing ML research team to do experiments together and come up with architectures that jointly optimize for ML and hardware performance.</p>
<p>Anthropic recently received ~$6B in funding commitments, and are investing it heavily in compute. We currently have ~5 performance specialists, with each one making an immense contribution in helping us have models that exhibit interesting capabilities for our alignment researcher and policy teams.</p>
<p>AI now is still missing a lot, but progress is incredibly fast. It’s hard for me to say the coming decade of progress won’t lead to AI as good as us at nearly all jobs, which would be the biggest event in history. Anthropic is unusually full of people who joined because they really care about ensuring this goes well. I think we have the world’s best alignment, interpretability research, and AI policy teams, and I personally work on performance optimization here because I think it’s the best way to leverage my comparative advantage to help the rest of our efforts succeed at steering towards AI going well for the world in the event it keeps up this pace.</p>
<p>If you too would like to do fun low-level optimization on what I think will be the most important technology of this decade and want to chat: Email me at <a href="https://thume.ca/cdn-cgi/l/email-protection" data-cfemail="394d4b504a4d58577958574d514b5649505a175a5654">[email&nbsp;protected]</a> with a link or paragraph about the most impressive low-level or performance thing you’ve done. And feel free to check out some of
<a href="https://thume.ca/2023/01/02/one-machine-twitter/">my other</a> <a href="https://thume.ca/2021/03/14/iforests/">performance</a> <a href="https://thume.ca/2022/05/15/latency-testing-streaming/">writing</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Virtual Machine as a core Android Primitive (231 pts)]]></title>
            <link>https://android-developers.googleblog.com/2023/12/virtual-machines-as-core-android-primitive.html</link>
            <guid>38538100</guid>
            <pubDate>Tue, 05 Dec 2023 22:55:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://android-developers.googleblog.com/2023/12/virtual-machines-as-core-android-primitive.html">https://android-developers.googleblog.com/2023/12/virtual-machines-as-core-android-primitive.html</a>, See on <a href="https://news.ycombinator.com/item?id=38538100">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<meta name="twitter:image" content="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3K8ZNCgF96aUEngG-N-ilXHjMZv2WidIhseXT1JIua_Z8KiVapdoSViSky-4ChuQ1_hgs6ktkobMo0Jh1OLk4vejenA7mt1gjSi_VQqXr3gLeR8g3aCGForCLlTmZ9-4PQg0GL7Gn1w_F_OYGoUjvywqFf-3ZDe0LCETPDkZLkHjTn93MZ9Fwjkq05dI/s1600/social-Android-Virtualization-as-a-core-Android-Primitive.png">
<p>

<em>Posted by Sandeep Patil – Principal Software Engineer, and Irene Ang – Product Manager</em>

<a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQUgLI8VNIOZk5Bf6wOTHe-4hrSSwlxck3cwoTbFYyy5uG219Ira8WsI8euGWfx20d3aNbWTGj5aCJX3XuQOdMZv6zS9PRI9HseNAoUwN42t4EjctfvbN_04Gk5vwZDaABvHToYMibcLBHrimTrEPWYbPGbE8hqOKuJoDRFBiezCClclCjLKrNhSOvdzA/s1600/Android-Virtualization-as-a-core-Android-Primitive.png" imageanchor="1"><img data-original-height="800" data-original-width="100%" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQUgLI8VNIOZk5Bf6wOTHe-4hrSSwlxck3cwoTbFYyy5uG219Ira8WsI8euGWfx20d3aNbWTGj5aCJX3XuQOdMZv6zS9PRI9HseNAoUwN42t4EjctfvbN_04Gk5vwZDaABvHToYMibcLBHrimTrEPWYbPGbE8hqOKuJoDRFBiezCClclCjLKrNhSOvdzA/s1600/Android-Virtualization-as-a-core-Android-Primitive.png"></a></p><p>The <b>Android Virtualization Framework (AVF)</b> will be available on upcoming select Android 14 devices. The AVF, first introduced in Android 13 on Pixel devices, provides new capabilities for platform developers working on privileged applications. </p>


<p>With AVF, we are more broadly supporting virtualization to Android. Virtualization is widely used and deployed to isolate workloads and operating systems from each other. It enables efficient scaling of infrastructure, testing environments, legacy software compatibility, creating virtual desktops and much more.</p>

<p>With AVF virtual machines become a core construct of the Android operating system, similar to the way Android utilizes Linux processes. Developers have the flexibility to choose the level of isolation for a virtual machine:</p>
<ul><ul>
<li><b>One-way isolation:</b> Android (the host) can control and inspect the contents of the VM. These are most commonly used for sandboxing and separation, enabling multiple operating systems to run on the same machine / device, with one operating system host (Android) controlling and watching over all others.</li></ul></ul><ul><ul>


<li><b>Two-way isolation (Isolated VM):</b> Android (the host) and the virtual machine (the guest) are completely isolated from each other. Developers who deal with or store sensitive data may benefit  from an isolated virtual machine. An isolated virtual machine has a two-way barrier, where neither the host (Android) nor the VM have access to each other, except via explicitly-agreed-upon communication channels.  This has 2 main properties:</li></ul></ul><blockquote><blockquote><ol><li>The workload and data inside the VM is inaccessible (confidential) from the host (Android).</li><li>Even if Android is compromised all the way up to (and including) the host kernel, the isolated VM remains uncompromised.</li></ol></blockquote></blockquote>

<h3>Benefits of AVF</h3>
<h4><span>Isolation</span></h4> 
<p>With an isolated VM, developers now have an alternative to Trustzone for use cases that need isolation from Android without escalated privilege.</p>


<h4><span>Portability</span></h4>  
<p>Virtual machines and the applications running inside them are far more portable than trusted applets. For example, a Linux-based virtual machine with a Linux-application payload will work on all devices that support AVF. This means that developers can build an application once and deploy it everywhere. VMs also make porting of existing Linux based applications seamless and easy, compared to porting into a Trustzone operating system.</p>  

<h4><span>Performance</span></h4>
<p>AVF is designed to be lightweight, efficient and flexible. Virtual machines can:</p>
<ul><ul>
<li>be as small as a single C program and as big as an entire operating system depending on the developer’s need;</li>
<li>be persistent or intermittent;</li>
<li>grow in memory or shrink depending on the overall system health; and</li>
<li>honor Android’s scheduler hints and low-memory warnings.</li>
</ul></ul>

<h4><span>Extensibility</span></h4>
<p>AVF is designed with developers in mind. Virtual machines can be customized to meet specific use-case needs. Developers can deploy any VM payload as long as it conforms to certain boot and communication protocols specified by AVF. </p> 


<p>In addition to bringing the power of virtualization to Android and enabling all the possibilities of virtual desktops, sandboxing, AVF’s use of isolated virtual machines can benefit the following common Android use cases (and many more):</p>
<ul><ul>
  <li><b>Biometrics:</b> By deploying biometric trusted applets in an isolated virtual machine, developers will have the isolation guarantee, access to more compute power for biometric algorithms, easy updatability regardless of the Trustzone operating system, and a more streamlined deployment.</li></ul></ul><ul><ul>
  <li><b>DRM:</b> Widevine enables streaming DRM on Android devices. Once deployed in an isolated Virtual Machine, updates to Widevine become much easier across Android devices, regardless of the details of the various Trustzone operating systems being deployed on those devices.</li></ul></ul><ul><ul>
</ul></ul>

<h3>AVF Usage</h3>

<p>AVF provides easy <a href="https://cs.android.com/android/platform/superproject/main/+/main:packages/modules/Virtualization/javalib/README.md" target="_blank">APIs</a> to query the device’s ability to create virtual machines and their supported types, and to set up secure communication channels with these virtual machines from applications and services that create them.</p>

<p>For example, to check for the availability of the AVF APIs, and of isolated and regular VM:</p>

<div><pre><span>VirtualMachineManager manager <span>=</span>
     (VirtualMachineManager)context<span>.</span>
          getSystemService(VirtualMachineManager<span>.</span>class)<span>;</span>
<span>if</span> (manager <span>==</span> null) {
    <span>//</span> AVF <span>not</span> supported
} <span>else</span> {
    <span>int</span> capabilities <span>=</span> manager<span>.</span>getCapabilities()<span>;</span>
    <span>if</span> ((capabilities <span>&amp;</span> CAPABILITY_PROTECTED_VM) <span>!=</span> <span>0</span>) {
        <span>//</span> protected VM is supported
    }
    <span>if</span> ((capabilities <span>&amp;</span> CAPABILITY_NON_PROTECTED_VM) <span>!=</span> <span>0</span>) {
        <span>//</span> non protected VM is supported
    }
}</span>
</pre></div>

<p>Please find additional documentation on AVF and its APIs <a href="https://source.android.com/docs/core/virtualization" target="_blank">here</a>. </p>

<h3>AVF Components</h3>

<p><img alt="AVF Component architecture" id="imgCaption" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqoflrFdKllbCL-ksao2ozMc0Vwp3eAXNadr58iaiCyd4noAKVuwaetGcVVoU1s1s_g3FA94wCHv_wg8AYbnczEY518U98tUwpYqkyLM-H2IbKpJcNY55xg6yCw5KO2Nk5HW2uAxAGNBXVdDVm8dNdG4das2y7RBWaQCWleykRilaLbd0sfFnb0JOXgC0/s1600/image1.png"></p>

<p>AVF consists of the framework APIs, the hypervisor, and the Virtual Machine Manager. The hypervisor guarantees virtual machines (including Android) are isolated from each other, much like how the Linux kernel does it for processes. The AVF hypervisor (pKVM), however, does that with a significantly smaller (~50x) code base compared to the Linux kernel.</p>


<h4><span>The Hypervisor (<a href="https://source.android.com/docs/core/virtualization/architecture#hypervisor" target="_blank">pKVM</a>)</span></h4>
<p>The hypervisor is focused on open source availability, security, device assignment to VMs and  security by isolation between virtual machines. It has a small attack surface that meets a higher security assurance level. AVF APIs and features are fully supported by the protected KVM hypervisor (pKVM). </p>

<p>pKVM is built on top of the industry standard Kernel-based Virtual Machine (KVM) in Linux. It means all existing operating systems and workloads that rely on KVM-based virtual machines can work seamlessly on Android devices with pKVM.</p>

<h4><span>Virtual Machine Manager (<a href="https://android.googlesource.com/platform/external/crosvm/" target="_blank">crosvm</a>)</span></h4>
<p><a href="https://android.googlesource.com/platform/external/crosvm/" target="_blank">crosvm</a>, a Rust-based Virtual Machine Manager (VMM), provides the glue between the hypervisor and the AVF framework. It is responsible for  creating, managing and destroying  virtual machines. In addition, it provides an abstraction layer across multiple hypervisor implementations.</p>

<h4><span>Isolated Virtual Machines</span></h4>
<p>Isolated virtual machines are invisible to Android i.e. any process running in Android cannot inspect, see, tamper with the content of such a virtual machine. This guarantee is provided by the <a href="https://source.android.com/docs/core/virtualization/architecture#hypervisor" target="_blank">hypervisor</a>.</p>

<h4><span>Virtual Machines</span></h4>
<p>Virtual machines are the same as isolated VMs, except they are accessible to Android processes with the right permissions and privilege.</p>


<h4><span><a href="https://source.android.com/docs/core/virtualization/microdroid" target="_blank">Microdroid</a></span></h4>
<p>Microdroid is a trimmed down Android OS package that is created to serve as a template for starting a virtual machine (VM). It provides developers with a familiar environment to build and run their workloads in a VM. Microdroid uses familiar Android tools and libraries, such as Bionic, Binder IPC and keystore support.</p>


<h4><span><a href="https://source.android.com/docs/core/virtualization/virtualization-service" target="_blank">Virtualization Service</a></span></h4>
<p>VirtualizationService manages all guest VMs, isolated or otherwise. It does so, primarily by managing instances of crosvm. It also exposes an AIDL API, which system services or privileged apps can use to start, monitor, and stop VMs.</p>


<h4><span>RpcBinder</span></h4>
<p><b>RpcBinder</b> is an all-new backend developed for the Android Interface Definition Language (AIDL). RpcBinder enables communication to and from virtual machines using the existing binder wire protocol. This means:</p>
<ol>
<li>Developers can write interfaces to virtual machines using the language and infrastructure they are already familiar with - AIDL.</li>
<li>Simply continue using existing AIDL interfaces even if the binder endpoint moves into a virtual machine.</li>
</ol>

<h3>What’s new in Android 14?</h3>

<p>Android 14, not only makes AVF available on more devices, it also provides a new toolkit to enable building more with AVF and its components:</p>

<div><ul><ul><li><b>Android System API for AVF</b>&nbsp;</li></ul></ul></div>
<blockquote><p>Privileged applications can now use VMs for executing their critical workload needing isolation;&nbsp;</p></blockquote>


<div><ul><ul><li><b>Hypervisor DevEx toolkit</b>&nbsp;</li></ul></ul></div>
<blockquote><p>Added tracing capability, improved debuggability and monitoring capabilities to provide insights and assist platform developers in developing inside Isolated VMs;&nbsp;</p></blockquote>


<div><ul><ul><li><b>Hypervisor Vendor Modules&nbsp;</b></li></ul></ul></div>
<blockquote><p>With vendor module extensions, our partners can customize Google’s hypervisor (pKVM) to meet their specific need and differentiate themselves;&nbsp;</p></blockquote>

<div><ul><ul><li><b>System Health Improvements</b>&nbsp;</li></ul></ul></div>
<blockquote><p>With Android 14, a microdroid based VM boots 2 times faster compared to Android 13 while using half the memory.</p></blockquote>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Report: YouTube adding user-traceable ID tag to links shared off-platform (199 pts)]]></title>
            <link>https://twitter.com/OldRowSwig/status/1732112446943269347?s=20</link>
            <guid>38537977</guid>
            <pubDate>Tue, 05 Dec 2023 22:44:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/OldRowSwig/status/1732112446943269347?s=20">https://twitter.com/OldRowSwig/status/1732112446943269347?s=20</a>, See on <a href="https://news.ycombinator.com/item?id=38537977">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ScriptLoadFailure"><form action="" method="GET"><div><p><span>Something went wrong, but don’t fret — let’s give it another shot.</span></p><br></div></form></div></div>]]></description>
        </item>
    </channel>
</rss>