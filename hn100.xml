(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 01 Jan 2025 03:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Static search trees: faster than binary search (154 pts)]]></title>
            <link>https://curiouscoding.nl/posts/static-search-tree/</link>
            <guid>42562847</guid>
            <pubDate>Wed, 01 Jan 2025 00:08:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://curiouscoding.nl/posts/static-search-tree/">https://curiouscoding.nl/posts/static-search-tree/</a>, See on <a href="https://news.ycombinator.com/item?id=42562847">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Table of Contents</p><ul><li><span>1</span> <a href="#introduction">Introduction</a><ul><li><span>1.1</span> <a href="#problem-statement">Problem statement</a></li><li><span>1.2</span> <a href="#recommended-reading">Recommended reading</a></li><li><span>1.3</span> <a href="#binary-search-and-eytzinger-layout">Binary search and Eytzinger layout</a></li><li><span>1.4</span> <a href="#hugepages">Hugepages</a></li><li><span>1.5</span> <a href="#a-note-on-benchmarking">A note on benchmarking</a></li><li><span>1.6</span> <a href="#cache-lines">Cache lines</a></li><li><span>1.7</span> <a href="#s-trees-and-b-trees">S-trees and B-trees</a></li></ul></li><li><span>2</span> <a href="#optimizing-find">Optimizing <code>find</code></a><ul><li><span>2.1</span> <a href="#linear">Linear</a></li><li><span>2.2</span> <a href="#auto-vectorization">Auto-vectorization</a></li><li><span>2.3</span> <a href="#trailing-zeros">Trailing zeros</a></li><li><span>2.4</span> <a href="#popcount">Popcount</a></li><li><span>2.5</span> <a href="#manual-simd">Manual SIMD</a></li></ul></li><li><span>3</span> <a href="#optimizing-the-search">Optimizing the search</a><ul><li><span>3.1</span> <a href="#batching">Batching</a></li><li><span>3.2</span> <a href="#prefetching">Prefetching</a></li><li><span>3.3</span> <a href="#pointer-arithmetic">Pointer arithmetic</a><ul><li><span>3.3.1</span> <a href="#up-front-splat">Up-front splat</a></li><li><span>3.3.2</span> <a href="#byte-based-pointers">Byte-based pointers</a></li><li><span>3.3.3</span> <a href="#the-final-version">The final version</a></li></ul></li><li><span>3.4</span> <a href="#skip-prefetch">Skip prefetch</a></li><li><span>3.5</span> <a href="#interleave">Interleave</a></li></ul></li><li><span>4</span> <a href="#optimizing-the-tree-layout">Optimizing the tree layout</a><ul><li><span>4.1</span> <a href="#left-tree">Left-tree</a></li><li><span>4.2</span> <a href="#memory-layouts">Memory layouts</a></li><li><span>4.3</span> <a href="#node-size-b-15">Node size \(B=15\)</a><ul><li><span>4.3.1</span> <a href="#data-structure-size">Data structure size</a></li></ul></li><li><span>4.4</span> <a href="#summary">Summary</a></li></ul></li><li><span>5</span> <a href="#prefix-partitioning">Prefix partitioning</a><ul><li><span>5.1</span> <a href="#full-layout">Full layout</a></li><li><span>5.2</span> <a href="#compact-subtrees">Compact subtrees</a></li><li><span>5.3</span> <a href="#the-best-of-both-compact-first-level">The best of both: compact first level</a></li><li><span>5.4</span> <a href="#overlapping-trees">Overlapping trees</a></li><li><span>5.5</span> <a href="#human-data">Human data</a></li><li><span>5.6</span> <a href="#prefix-map">Prefix map</a></li><li><span>5.7</span> <a href="#prefix-summary">Summary</a></li></ul></li><li><span>6</span> <a href="#multi-threaded-comparison">Multi-threaded comparison</a></li><li><span>7</span> <a href="#conclusion">Conclusion</a><ul><li><span>7.1</span> <a href="#future-work">Future work</a><ul><li><span>7.1.1</span> <a href="#branchy-search">Branchy search</a></li><li><span>7.1.2</span> <a href="#interpolation-search">Interpolation search</a></li><li><span>7.1.3</span> <a href="#packing-data-smaller">Packing data smaller</a></li><li><span>7.1.4</span> <a href="#returning-indices-in-original-data">Returning indices in original data</a></li><li><span>7.1.5</span> <a href="#range-queries">Range queries</a></li></ul></li></ul></li></ul></div><p>In this post, we will implement a static search tree (S+ tree) for
high-throughput searching of sorted data, as <a href="https://en.algorithmica.org/hpc/data-structures/s-tree/">introduced</a> on Algorithmica.
We’ll mostly take the code presented there as a starting point, and optimize it
to its limits. For a large part, I’m simply taking the ‘future work’ ideas of that post
and implementing them. And then there will be a bunch of looking at assembly
code to shave off all the instructions we can.
Lastly, there will be one big addition to optimize throughput: <em>batching</em>.</p><p>All <strong>source code</strong>, including benchmarks and plotting code, is at <a href="https://github.com/RagnarGrootKoerkamp/suffix-array-searching/tree/master/static-search-tree">github:RagnarGrootKoerkamp/suffix-array-searching</a>.</p><h2 id="introduction"><span>1</span> Introduction
<a href="#introduction"></a></h2><h2 id="problem-statement"><span>1.1</span> Problem statement
<a href="#problem-statement"></a></h2><p><strong>Input.</strong> A sorted list of \(n\) 32bit unsigned integers <code>vals: Vec&lt;u32&gt;</code>.</p><p><strong>Output.</strong> A data structure that supports queries \(q\), returning the smallest
element of <code>vals</code> that is at least \(q\), or <code>u32::MAX</code> if no such element exists.
Optionally, the index of this element may also be returned.</p><p><strong>Metric.</strong> We optimize <em>throughput</em>. That is, the number of (independent) queries
that can be answered per second. The typical case is where we have a
sufficiently long <code>queries: &amp;[u32]</code> as input, and return a corresponding <code>answers: Vec&lt;u32&gt;</code>.</p><p>Note that we’ll usually report reciprocal throughput as <code>ns/query</code> (or just
<code>ns</code>), instead of <code>queries/s</code>. You can think of this as amortized (not <em>average</em>) time spent per query.</p><p><strong>Benchmarking setup.</strong> For now, we will assume that both the input and queries
are simply uniform random sampled 31bit integers<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.</p><p><strong>Code.</strong>
In code, this can be modelled by the trait shown in <a href="#code-snippet--trait">Code Snippet 1</a>.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span><span>9
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>trait</span><span> </span><span>SearchIndex</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>/// Two functions with default implementations in terms of each other.
</span></span></span><span><span><span></span><span>    </span><span>fn</span> <span>query_one</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>query</span>: <span>u32</span><span>)</span><span> </span>-&gt; <span>u32</span> <span>{</span><span>
</span></span></span><span><span><span>        </span><span>Self</span>::<span>query</span><span>(</span><span>&amp;</span><span>vec!</span><span>[</span><span>query</span><span>])[</span><span>0</span><span>]</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>fn</span> <span>query</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>queries</span>: <span>&amp;</span><span>[</span><span>u32</span><span>])</span><span> </span>-&gt; <span>Vec</span><span>&lt;</span><span>u32</span><span>&gt;</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>queries</span><span>.</span><span>iter</span><span>().</span><span>map</span><span>(</span><span>|&amp;</span><span>q</span><span>|</span><span> </span><span>Self</span>::<span>query_one</span><span>(</span><span>q</span><span>)).</span><span>collect</span><span>()</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--trait">Code Snippet 1</a>:</span>
Trait that our solution should implement.</p><h2 id="recommended-reading"><span>1.2</span> Recommended reading
<a href="#recommended-reading"></a></h2><p>The classical solution to this problem is <strong>binary search</strong>, which we will briefly
visit in the next section. A great paper on this and other search layouts is
<a href="#citeproc_bib_item_2">“Array Layouts for Comparison-Based Searching”</a> by Khuong and Morin (<a href="#citeproc_bib_item_2">2017</a>).
Algorithmica also has a <a href="https://en.algorithmica.org/hpc/data-structures/binary-search/">case study</a> based on that paper.</p><p>This post will focus on <strong>S+ trees</strong>, as introduced on Algorithmica in the
followup post, <a href="https://en.algorithmica.org/hpc/data-structures/s-tree/">static B-trees</a>. In the interest of my time, I will mostly assume
that you are familiar with that post.</p><p>I also recommend reading my work-in-progress <a href="https://curiouscoding.nl/posts/cpu-benchmarks">introduction to CPU performance</a>,
which contains some benchmarks pushing the CPU to its limits. We will use the
metrics obtained there as baseline to understand our optimization attempts.</p><p>Also helpful is the <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#=undefined&amp;techs=AVX_ALL">Intel Intrinsics Guide</a> when looking into SIMD instructions.
Note that we’ll only be using <code>AVX2</code> instructions here, as in, we’re assuming
intel. And we’re not assuming less available <code>AVX512</code> instructions (in
particular, since my laptop doesn’t have them).</p><h2 id="binary-search-and-eytzinger-layout"><span>1.3</span> Binary search and Eytzinger layout
<a href="#binary-search-and-eytzinger-layout"></a></h2><p>As a baseline, we will use the Rust standard library binary search implementation.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span> </span><span>struct</span> <span>SortedVec</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>vals</span>: <span>Vec</span><span>&lt;</span><span>u32</span><span>&gt;</span><span>,</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>impl</span><span> </span><span>SortedVec</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>pub</span><span> </span><span>fn</span> <span>binary_search_std</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>q</span>: <span>u32</span><span>)</span><span> </span>-&gt; <span>u32</span> <span>{</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>idx</span><span> </span><span>=</span><span> </span><span>self</span><span>.</span><span>vals</span><span>.</span><span>binary_search</span><span>(</span><span>&amp;</span><span>q</span><span>).</span><span>unwrap_or_else</span><span>(</span><span>|</span><span>i</span><span>|</span><span> </span><span>i</span><span>);</span><span>
</span></span></span><span><span><span>        </span><span>self</span><span>.</span><span>vals</span><span>[</span><span>idx</span><span>]</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--binary-search">Code Snippet 2</a>:</span>
The binary search in the Rust standard library.</p><p>The main conclusion of the array layouts paper (<a href="#citeproc_bib_item_2">Khuong and Morin 2017</a>) is
that the Eytzinger layout is one of the best in practice.
This layout reorders the values in memory: the binary search effectively is a
binary search tree on the data, the root the middle node, then the nodes at
positions \(\frac 14 n\) and \(\frac 34 n\), then \(\frac 18n, \frac 38n, \frac 58n,
\frac 78n\), and so on. The main benefit of this layout is that all values needed
for the first steps of the binary search are close together, so they can be
cached efficiently. If we put the root at index \(1\), the two children of the
node at index \(i\) are at \(2i\) and \(2i+1\). This means that we can effectively
prefetch the next cache line, before knowing whether we need index \(2i\) or
\(2i+1\). This can be taken a step further and we can prefetch the cache line
containing indices \(16i\) to \(16i+15\), which are exactly the values needed 4
iterations from now.
For a large part, this can quite effectively hide the latency associated with
the traversal of the tree.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span> </span><span>struct</span> <span>Eytzinger</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>/// The root of the tree is at index 1.
</span></span></span><span><span><span></span><span>    </span><span>vals</span>: <span>Vec</span><span>&lt;</span><span>u32</span><span>&gt;</span><span>,</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>impl</span><span> </span><span>Eytzinger</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>/// L: number of levels ahead to prefetch.
</span></span></span><span><span><span></span><span>    </span><span>pub</span><span> </span><span>fn</span> <span>search_prefetch</span><span>&lt;</span><span>const</span><span> </span><span>L</span>: <span>usize</span><span>&gt;</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>q</span>: <span>u32</span><span>)</span><span> </span>-&gt; <span>u32</span> <span>{</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>mut</span><span> </span><span>idx</span><span> </span><span>=</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>while</span><span> </span><span>(</span><span>1</span><span> </span><span>&lt;&lt;</span><span> </span><span>L</span><span>)</span><span> </span><span>*</span><span> </span><span>idx</span><span> </span><span>&lt;</span><span> </span><span>self</span><span>.</span><span>vals</span><span>.</span><span>len</span><span>()</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>idx</span><span> </span><span>=</span><span> </span><span>2</span><span> </span><span>*</span><span> </span><span>idx</span><span> </span><span>+</span><span> </span><span>(</span><span>q</span><span> </span><span>&gt;</span><span> </span><span>self</span><span>.</span><span>get</span><span>(</span><span>idx</span><span>))</span><span> </span><span>as</span><span> </span><span>usize</span><span>;</span><span>
</span></span></span><span><span><span>            </span><span>prefetch_index</span><span>(</span><span>&amp;</span><span>self</span><span>.</span><span>vals</span><span>,</span><span> </span><span>(</span><span>1</span><span> </span><span>&lt;&lt;</span><span> </span><span>L</span><span>)</span><span> </span><span>*</span><span> </span><span>idx</span><span>);</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>        </span><span>// The last few iterations don't need prefetching anymore.
</span></span></span><span><span><span></span><span>        </span><span>while</span><span> </span><span>idx</span><span> </span><span>&lt;</span><span> </span><span>self</span><span>.</span><span>vals</span><span>.</span><span>len</span><span>()</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>idx</span><span> </span><span>=</span><span> </span><span>2</span><span> </span><span>*</span><span> </span><span>idx</span><span> </span><span>+</span><span> </span><span>(</span><span>q</span><span> </span><span>&gt;</span><span> </span><span>self</span><span>.</span><span>get</span><span>(</span><span>idx</span><span>))</span><span> </span><span>as</span><span> </span><span>usize</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>zeros</span><span> </span><span>=</span><span> </span><span>idx</span><span>.</span><span>trailing_ones</span><span>()</span><span> </span><span>+</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>idx</span><span> </span><span>=</span><span> </span><span>idx</span><span> </span><span>&gt;&gt;</span><span> </span><span>zeros</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>self</span><span>.</span><span>get</span><span>(</span><span>idx</span><span>)</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--eytzinger">Code Snippet 3</a>:</span>
Implementation of searching the Eytzinger layout, with \(L=4\) levels of prefetching.</p><p>If we plot these two, we see that Eytzinger layout performs as good as binary
search when the array fits in L2 cache (<code>256kB</code> for me, the middle red line), but starts to be much
better than binary search as the array grows to be much larger than the L3 cache (<code>12MB</code>).
In the end, Eytzinger search is around 4 times faster, which nicely corresponds
to being able to prefetch 4 iterations of cache lines from memory at a time.</p><figure><a href="https://curiouscoding.nl/ox-hugo/1-binary-search.svg"><img src="https://curiouscoding.nl/ox-hugo/1-binary-search.svg" alt="Figure 1: Query throughput of binary search and Eytzinger layout as the size of the input increases. At 1GB input, binary search needs around 1150ns/query, while Eytzinger is 6x faster at 200ns/query."></a><figcaption><p><span>Figure 1: </span>Query throughput of binary search and Eytzinger layout as the size of the input increases. At <code>1GB</code> input, binary search needs around <code>1150ns/query</code>, while Eytzinger is 6x faster at <code>200ns/query</code>.</p></figcaption></figure><h2 id="hugepages"><span>1.4</span> Hugepages
<a href="#hugepages"></a></h2><p>For all experiments, we’ll make sure to allocate the tree using <code>2MB</code> <em>hugepages</em>
by default, instead of the usual <code>4kB</code> pages.
This reduces pressure on the <em>translation lookaside buffer</em> (TLB) that
translates virtual memory addresses to hardware memory addresses, since its
internal table of pages is much smaller when using hugepages, and hence can be
cached better.</p><p>With <em>transparent hugepages</em> enabled, they are automatically given out whenever
allocating an exact multiple of <code>2MB</code>, and so we always round up the allocation
for the tree to the next multiple of <code>2MB</code>. However, it turns out that small
allocations below <code>32MB</code> still go on the program’s <em>heap</em>, rather than asking
the kernel for new memory pages, causing them to not actually be hugepages.
Thus, all allocations we do are actually rounded up to the next multiple of
<code>32MB</code> instead.</p><p>All together, hugepages sometimes makes a small difference when the dataset is
indeed between <code>1MB</code> and <code>32MB</code> in size. Smaller data structures don’t really need
hugepages anyway. Enabling them for the Eytzinger layout as in the plot above
also gives a significant speedup for larger sizes.</p><h2 id="a-note-on-benchmarking"><span>1.5</span> A note on benchmarking
<a href="#a-note-on-benchmarking"></a></h2><p>The plots have the size of the input data on the logarithmic (bottom) x-axis. On the top,
they show the corresponding number of elements in the vector, which is 4 times
less, since each element is a <code>u32</code> spanning 4 bytes.
Measurements are taken at values \(2^i\), \(1.25 \cdot 2^i\), \(1.5\cdot 2^i\), and
\(1.75\cdot 2^i\).</p><p>The y-axis shows measured time per query. In the plot above, it says
<em>latency</em>, since it is benchmarked as <code>for q in queries { index.query(q); }</code>.
Even then, the pipelining and out-of-order execution of the CPU will make it
execute multiple iterations in parallel. Specifically, while it is waiting for
the last cache lines of iteration \(i\), it can already start executing the first
instructions of the next query. To measure the true latency, we would have to
introduce a <em>loop carried dependency</em> by making query \(i+1\) dependent on the
result of query \(i\).
However, the main goal of this post is to optimize for <em>throughput</em>, so we won’t
bother with that.</p><p>Thus, all plots will show the throughput of doing <code>index.query(all_queries)</code>.</p><p>For the benchmarks, I’m using my laptop’s <code>i7-10750H</code> CPU, with the frequency
fixed to <code>2.6GHz</code> using <a href="#code-snippet--pin">Code Snippet 4</a>.<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span></code></pre></td><td><pre tabindex="0"><code data-lang="sh"><span><span>sudo cpupower frequency-set -g powersave -d 2.6GHz -u 2.6GHz
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--pin">Code Snippet 4</a>:</span>
Pinning the CPU frequency to <code>2.6GHz</code>.</p><p>Also relevant are the sizes of the caches: <code>32KiB</code> L1 cache per core, <code>256KiB</code>
L2 cache per core, and <code>12MiB</code> L3 cache shared between the physical 6 cores.
Furthermore, hyper-threading is disabled.</p><p>All measurements are done 5 times. The line follows the median, and we show the
spread of the 2nd to 4th value (i.e., after discarding the minimum and maximum).
Observe that in most of the plot above, the spread is barely visible! Thus,
while especially the graph for binary search looks very noisy, that ’noise’ is
in fact completely reproducible. Indeed, it’s caused by effects of <em>cache
associativity</em>, as explained in the array layouts paper
(Khuong and Morin (<a href="#citeproc_bib_item_2">2017</a>); this post is long enough already).</p><h2 id="cache-lines"><span>1.6</span> Cache lines
<a href="#cache-lines"></a></h2><p>Main memory and the caches work at the level of <em>cache lines</em> consisting of 64
bytes (at least on my machine), or 16 <code>u32</code> values. Thus, even if you only read a single byte, if
the cache line containing that byte is not yet in the L1 cache, the entire thing
will be fetched from RAM or L3 or L2 into L1.</p><p>Plain binary search typically only uses a single value of each cache line,
until it gets to the end of the search where the last 16 values span just 1 or 2
cache lines.</p><p>They Eytzinger layout suffers the same problem: even though the next cache line
can be prefetched, it still only uses a single value in each.
This fundamentally means that both these search schemes are using the available
memory bandwidth quite inefficiently, and since most of what they are doing is
waiting for memory to come through, that’s not great.
Also, while that’s not relevant <em>yet</em>, when doing this with many threads in
parallel, or with batching, single-core RAM throughput and the throughput of the
main memory itself become a bottleneck.</p><p>It would be much better if <em>somehow</em>, we could use the information in each cache
line much more efficiently ;)</p><p>We can do that by storing our data in a different way. Instead of storing it
layer by layer, so that each iteration goes into a new layer,
we can store 4 layers of the tree at a time (<a href="#code-snippet--node">Code Snippet 5</a>). That takes 15 values, and could
nicely be padded into a full cache line. Then when we fetch a cache line, we can
use it for 4 iterations at once – much better!
On the other hand, now we can’t prefetch upcoming cache lines in advance
anymore, so that overall the latency will be the same. But we fetch up to 4
times fewer cache lines overall, which should help throughput.</p><p>Unfortunately, I don’t have code and plots here, because what I really want to
focus on is the next bit.</p><figure><a href="https://curiouscoding.nl/ox-hugo/packed-eytzinger.svg"><img src="https://curiouscoding.nl/ox-hugo/packed-eytzinger.svg" alt="Figure 2: The first two rows show how we could pack four layers of the Eytzinger search into a single cache line. The first follows a classic binary search layout, while the second applies the Eytzinger layout recursively. The third row shows an S-tree node instead. For simplicity and clarity, I’m using consecutive values, but in practice, this would be any list of sorted numbers."></a><figcaption><p><span>Figure 2: </span>The first two rows show how we could pack four layers of the Eytzinger search into a single cache line. The first follows a classic binary search layout, while the second applies the Eytzinger layout recursively. The third row shows an S-tree node instead. For simplicity and clarity, I’m using consecutive values, but in practice, this would be any list of sorted numbers.</p></figcaption></figure><h2 id="s-trees-and-b-trees"><span>1.7</span> S-trees and B-trees
<a href="#s-trees-and-b-trees"></a></h2><p>We just ended with a <em>node</em> of 15 values that represent a height-4 search tree
in which we can binary search. From there, it’s just a small step to S-trees.</p><p><strong>B-trees.</strong> But first I have to briefly mention B-trees though (<a href="https://en.wikipedia.org/wiki/B-tree">wikipedia</a>). Those are
the more classic dynamic variant, where nodes are linked together via pointers.
As wikipedia writes, they are typically used with much larger block sizes, for
example 4kB, since files read from disk usually come in 4kB chunks. Thus, they
also have much larger branching factors.</p><p><strong>S-trees.</strong> But we will instead use S-trees, as named so by Algorithmica. They
are a nice middle ground between the high branching factor of B-trees, and the
compactness of the Eytzinger layout.
Instead of interpreting the 15 values as a search tree, we can also store them
in a sorted way, and consider them as a 16-ary search tree: the 15 values simply
split the data in the subtree into 16 parts, and we can do a linear scan to find
which part to recurse into.
But if we store 15 values and one padding in a cache line, we might as well make
it 16 values and have a branching factor of 17 instead.</p><p><strong>S+ trees.</strong> B-trees and S-trees only store each value once, either in a leaf node or
in an internal node. This turns out to be somewhat annoying, since we must track
in which layer the result was found. To simplify this, we can store <em>all</em> values
as a leaf, and <em>duplicate</em> them in the internal nodes. This is then called a B+
tree or S+ tree. However, I will be lazy and just use S-tree to include this modification.</p><figure><a href="https://curiouscoding.nl/ox-hugo/full.svg"><img src="https://curiouscoding.nl/ox-hugo/full.svg" alt="Figure 3: An example of a ‘full’ S+ tree (that I will from now just call S-tree) on 18 values with nodes of size (B=2) and branching factor (B+1=3). Each internal node stores the smallest value in the subtree on its right. In memory, the layers are simply packed together behind each other."></a><figcaption><p><span>Figure 3: </span>An example of a ‘full’ S+ tree (that I will from now just call S-tree) on 18 values with nodes of size (B=2) and branching factor (B+1=3). Each internal node stores the smallest value in the subtree on its right. In memory, the layers are simply packed together behind each other.</p></figcaption></figure><p>A full S-tree can be navigated in a way similar to the Eytzinger layout: The
node (note: not<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> value) at index \(i\) has its \(B+1\) child-nodes at indices \((B+1)\cdot i + 1 + \{0, \dots, B\}\).</p><p>When the tree is only partially filled, the full layout can waste a lot of space
(<a href="#figure--stree-partial">Figure 4</a>). Instead, we can <em>pack</em> the layers together, by storing the
offset \(o_\ell\) of each layer.</p><p>The children of node \(o_\ell + i\) are then at \(o_{\ell+1} + (B+1)\cdot i + \{0, \dots, B\}\).</p><figure><a href="https://curiouscoding.nl/ox-hugo/partial.svg"><img src="https://curiouscoding.nl/ox-hugo/partial.svg" alt="Figure 4: The full representation can be inefficient. The packed representation removes the empty space, and explicitly stores the offset (o_ell) where each layer starts."></a><figcaption><p><span>Figure 4: </span>The <em>full</em> representation can be inefficient. The <em>packed</em> representation removes the empty space, and explicitly stores the offset (o_ell) where each layer starts.</p></figcaption></figure><p>At last, let’s have a look at some code. Each node in the tree is simply
represented as a list of \(N=16\) <code>u32</code> values. We explicitly ask that nodes are
aligned to 64byte cache line boundaries.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>#[repr(align(64))]</span><span>
</span></span></span><span><span><span></span><span>pub</span><span> </span><span>struct</span> <span>TreeNode</span><span>&lt;</span><span>const</span><span> </span><span>N</span>: <span>usize</span><span>&gt;</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>data</span>: <span>[</span><span>u32</span><span>;</span><span> </span><span>N</span><span>],</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--node">Code Snippet 5</a>:</span>
Search tree node, aligned to a 64 byte cache line. For now, N is always 16. The values in a node must always be sorted.</p><p>The S-tree itself is simply a list of nodes, and the offsets where each layer starts.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span><span>9
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>/// N: #elements in a node, always 16.
</span></span></span><span><span><span>/// B: branching factor &lt;= N+1. Typically 17.
</span></span></span><span><span><span></span><span>pub</span><span> </span><span>struct</span> <span>STree</span><span>&lt;</span><span>const</span><span> </span><span>B</span>: <span>usize</span><span>,</span><span> </span><span>const</span><span> </span><span>N</span>: <span>usize</span><span>&gt;</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>/// The list of tree nodes.
</span></span></span><span><span><span></span><span>    </span><span>tree</span>: <span>Vec</span><span>&lt;</span><span>TreeNode</span><span>&lt;</span><span>N</span><span>&gt;&gt;</span><span>,</span><span>
</span></span></span><span><span><span>    </span><span>/// The root is at index tree[offsets[0]].
</span></span></span><span><span><span></span><span>    </span><span>/// It's children start at tree[offsets[1]], and so on.
</span></span></span><span><span><span></span><span>    </span><span>offsets</span>: <span>Vec</span><span>&lt;</span><span>usize</span><span>&gt;</span><span>,</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--stree">Code Snippet 6</a>:</span>
The S-tree data structure. It depends on the number of values per node \(B\) (usually 16 but sometimes 15) and the size of each node \(N\) (always 16).</p><p>To save some space, and focus on the interesting part (to me, at least), I will
not show any code for constructing S-trees. It’s a whole bunch of uninteresting
fiddling with indices, and takes a lot of time to get right. Also, construction
is not optimized at all currently. Anyway, find the code <a href="https://github.com/RagnarGrootKoerkamp/suffix-array-searching/tree/master/static-search-tree/src">here</a>.</p><p>TODO: Reverse offsets.</p><p>What we <em>will</em> look at, is code for searching S-trees.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>fn</span> <span>search</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>q</span>: <span>u32</span><span>,</span><span> </span><span>find</span>: <span>impl</span><span> </span><span>Fn</span><span>(</span><span>&amp;</span><span>TreeNode</span><span>&lt;</span><span>N</span><span>&gt;</span><span>,</span><span> </span><span>u32</span><span>)</span><span> </span>-&gt; <span>usize</span><span>)</span><span> </span>-&gt; <span>u32</span> <span>{</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>k</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>for</span><span> </span><span>o</span><span> </span><span>in</span><span> </span><span>self</span><span>.</span><span>offsets</span><span>[</span><span>0</span><span>..</span><span>self</span><span>.</span><span>offsets</span><span>.</span><span>len</span><span>()</span><span>-</span><span>1</span><span>]</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>jump_to</span><span> </span><span>=</span><span> </span><span>find</span><span>(</span><span>self</span><span>.</span><span>node</span><span>(</span><span>o</span><span> </span><span>+</span><span> </span><span>k</span><span>),</span><span> </span><span>q</span><span>);</span><span>
</span></span></span><span><span><span>        </span><span>k</span><span> </span><span>=</span><span> </span><span>k</span><span> </span><span>*</span><span> </span><span>(</span><span>B</span><span> </span><span>+</span><span> </span><span>1</span><span>)</span><span> </span><span>+</span><span> </span><span>jump_to</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>o</span><span> </span><span>=</span><span> </span><span>self</span><span>.</span><span>offsets</span><span>.</span><span>last</span><span>().</span><span>unwrap</span><span>();</span><span>
</span></span></span><span><span><span>    </span><span>// node(i) returns tree[i] using unchecked indexing.
</span></span></span><span><span><span></span><span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>idx</span><span> </span><span>=</span><span> </span><span>find</span><span>(</span><span>self</span><span>.</span><span>node</span><span>(</span><span>o</span><span> </span><span>+</span><span> </span><span>k</span><span>),</span><span> </span><span>q</span><span>);</span><span>
</span></span></span><span><span><span>    </span><span>// get(i, j) returns tree[i].data[j] using unchecked indexing.
</span></span></span><span><span><span></span><span>    </span><span>self</span><span>.</span><span>get</span><span>(</span><span>o</span><span> </span><span>+</span><span> </span><span>k</span><span> </span><span>+</span><span> </span><span>idx</span><span> </span><span>/</span><span> </span><span>N</span><span>,</span><span> </span><span>idx</span><span> </span><span>%</span><span> </span><span>N</span><span>)</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p>Our first step will be optimizing the <code>find</code> function.</p><h2 id="optimizing-find"><span>2</span> Optimizing <code>find</code>
<a href="#optimizing-find"></a></h2><h2 id="linear"><span>2.1</span> Linear
<a href="#linear"></a></h2><p>Let’s first precisely define what we want <code>find</code> to do:
it’s input is a node with 16 sorted values and a query value \(q\), and it should return
the index of the first element that is at least \(q\).</p><p>Some simple code for this is <a href="#code-snippet--find-linear">Code Snippet 8</a>.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span> </span><span>fn</span> <span>find_linear</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>q</span>: <span>u32</span><span>)</span><span> </span>-&gt; <span>usize</span> <span>{</span><span>
</span></span></span><span><span><span>    </span><span>for</span><span> </span><span>i</span><span> </span><span>in</span><span> </span><span>0</span><span>..</span><span>N</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>if</span><span> </span><span>self</span><span>.</span><span>data</span><span>[</span><span>i</span><span>]</span><span> </span><span>&gt;=</span><span> </span><span>q</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>return</span><span> </span><span>i</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>N</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--find-linear">Code Snippet 8</a>:</span>
A linear scan for the first element \(\geq q\), that breaks as soon as it is found.</p><p>The results are not very impressive yet.</p><figure><a href="https://curiouscoding.nl/ox-hugo/2-find-linear.svg"><img src="https://curiouscoding.nl/ox-hugo/2-find-linear.svg" alt="Figure 5: The initial version of our S-tree search is quite a bit slower than the Eytzinger layout. In this and following plots, ‘old’ lines will be dimmed, and the best previous and best new line slightly highlighted. Colours will be consistent from one plot to the next."></a><figcaption><p><span>Figure 5: </span>The initial version of our S-tree search is quite a bit slower than the Eytzinger layout. In this and following plots, ‘old’ lines will be dimmed, and the best previous and best new line slightly highlighted. Colours will be consistent from one plot to the next.</p></figcaption></figure><h2 id="auto-vectorization"><span>2.2</span> Auto-vectorization
<a href="#auto-vectorization"></a></h2><p>As it turns out, the <code>break;</code> in <a href="#code-snippet--find-linear">Code Snippet 8</a> is really bad for performance,
since the branch predictor can’t do a good job on it.</p><p>Instead, we can <em>count</em> the number of values less than \(q\), and return that as
the index of the first value \(\geq q\). (Example: all values \(\geq q\) index
gives index 0.)</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span><span>9
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span> </span><span>fn</span> <span>find_linear_count</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>q</span>: <span>u32</span><span>)</span><span> </span>-&gt; <span>usize</span> <span>{</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>count</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>for</span><span> </span><span>i</span><span> </span><span>in</span><span> </span><span>0</span><span>..</span><span>N</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>if</span><span> </span><span>self</span><span>.</span><span>data</span><span>[</span><span>i</span><span>]</span><span> </span><span>&lt;</span><span> </span><span>q</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>count</span><span> </span><span>+=</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>count</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--linear-count">Code Snippet 9</a>:</span>
Counting values \(&lt; q\) instead of an early break. The <code>if self.data[i] &lt; q</code> can be optimized into branchless code.</p><p>In fact, the code is not just branchless, but actually it’s auto-vectorized into
SIMD instructions!</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span></code></pre></td><td><pre tabindex="0"><code data-lang="asm"><span><span><span>vmovdqu</span>      <span>(</span><span>%rax</span><span>,</span><span>%rcx</span><span>),</span> <span>%ymm1</span>     <span>; load data[..8]
</span></span></span><span><span><span></span><span>vmovdqu</span>      <span>32</span><span>(</span><span>%rax</span><span>,</span><span>%rcx</span><span>),</span> <span>%ymm2</span>   <span>; load data[8..]
</span></span></span><span><span><span></span><span>vpbroadcastd</span> <span>%xmm0</span><span>,</span> <span>%ymm0</span>           <span>; 'splat' the query value
</span></span></span><span><span><span></span><span>vpmaxud</span>      <span>%ymm0</span><span>,</span> <span>%ymm2</span><span>,</span> <span>%ymm3</span>    <span>; v
</span></span></span><span><span><span></span><span>vpcmpeqd</span>     <span>%ymm3</span><span>,</span> <span>%ymm2</span><span>,</span> <span>%ymm2</span>    <span>; v
</span></span></span><span><span><span></span><span>vpmaxud</span>      <span>%ymm0</span><span>,</span> <span>%ymm1</span><span>,</span> <span>%ymm0</span>    <span>; v
</span></span></span><span><span><span></span><span>vpcmpeqd</span>     <span>%ymm0</span><span>,</span> <span>%ymm1</span><span>,</span> <span>%ymm0</span>    <span>; 4x compare query with values
</span></span></span><span><span><span></span><span>vpackssdw</span>    <span>%ymm2</span><span>,</span> <span>%ymm0</span><span>,</span> <span>%ymm0</span>    <span>;
</span></span></span><span><span><span></span><span>vpcmpeqd</span>     <span>%ymm1</span><span>,</span> <span>%ymm1</span><span>,</span> <span>%ymm1</span>    <span>; v
</span></span></span><span><span><span></span><span>vpxor</span>        <span>%ymm1</span><span>,</span> <span>%ymm0</span><span>,</span> <span>%ymm0</span>    <span>; 2x negate result
</span></span></span><span><span><span></span><span>vextracti128</span> <span>$1</span><span>,</span> <span>%ymm0</span><span>,</span> <span>%xmm1</span>       <span>; v
</span></span></span><span><span><span></span><span>vpacksswb</span>    <span>%xmm1</span><span>,</span> <span>%xmm0</span><span>,</span> <span>%xmm0</span>    <span>; v
</span></span></span><span><span><span></span><span>vpshufd</span>      <span>$216</span><span>,</span> <span>%xmm0</span><span>,</span> <span>%xmm0</span>     <span>; v
</span></span></span><span><span><span></span><span>vpmovmskb</span>    <span>%xmm0</span><span>,</span> <span>%ecx</span>            <span>; 4x extract mask
</span></span></span><span><span><span></span><span>popcntl</span>      <span>%ecx</span><span>,</span> <span>%ecx</span>             <span>; popcount the 16bit mask
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--linear-count-asm">Code Snippet 10</a>:</span>
Code Snippet <a href="#org580fb2e">9</a> is auto-vectorized!</p><p>To save some space: you can find this and further results for this section in
<a href="#figure--find-results">Figure 34</a> at the end of the section.</p><p>This auto-vectorized version is over two times faster than the linear find,
and now clearly beats Eytzinger layout!</p><h2 id="trailing-zeros"><span>2.3</span> Trailing zeros
<a href="#trailing-zeros"></a></h2><p>We can also roll our own SIMD. The SIMD version of the original linear scan idea
does 16 comparisons in parallel, converts that to a bitmask, and then counts the
number of trailing zeros. Using <code>#[feature(portable_simd)]</code>, that looks like this:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span> </span><span>fn</span> <span>find_ctz</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>q</span>: <span>u32</span><span>)</span><span> </span>-&gt; <span>usize</span> <span>{</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>data</span>: <span>Simd</span><span>&lt;</span><span>u32</span><span>,</span><span> </span><span>N</span><span>&gt;</span><span> </span><span>=</span><span> </span><span>Simd</span>::<span>from_slice</span><span>(</span><span>&amp;</span><span>self</span><span>.</span><span>data</span><span>[</span><span>0</span><span>..</span><span>N</span><span>]);</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>q</span><span> </span><span>=</span><span> </span><span>Simd</span>::<span>splat</span><span>(</span><span>q</span><span>);</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>mask</span><span> </span><span>=</span><span> </span><span>q</span><span>.</span><span>simd_le</span><span>(</span><span>data</span><span>);</span><span>
</span></span></span><span><span><span>    </span><span>mask</span><span>.</span><span>first_set</span><span>().</span><span>unwrap_or</span><span>(</span><span>N</span><span>)</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--find-ctz">Code Snippet 11</a>:</span>
A <code>find</code> implementation using the <i>count-trailing-zeros</i> instruction.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span></code></pre></td><td><pre tabindex="0"><code data-lang="asm"><span><span><span>vpminud</span>      <span>32</span><span>(</span><span>%rsi</span><span>,</span><span>%r8</span><span>),</span> <span>%ymm0</span><span>,</span> <span>%ymm1</span>  <span>; take min of data[8..] and query
</span></span></span><span><span><span></span><span>vpcmpeqd</span>     <span>%ymm1</span><span>,</span> <span>%ymm0</span><span>,</span> <span>%ymm1</span>         <span>; does the min equal query?
</span></span></span><span><span><span></span><span>vpminud</span>      <span>(</span><span>%rsi</span><span>,</span><span>%r8</span><span>),</span> <span>%ymm0</span><span>,</span> <span>%ymm2</span>    <span>; take min of data[..8] and query
</span></span></span><span><span><span></span><span>vpcmpeqd</span>     <span>%ymm2</span><span>,</span> <span>%ymm0</span><span>,</span> <span>%ymm2</span>         <span>; does the min equal query?
</span></span></span><span><span><span></span><span>vpackssdw</span>    <span>%ymm1</span><span>,</span> <span>%ymm2</span><span>,</span> <span>%ymm1</span>         <span>; pack the two results together, interleaved as 16bit words
</span></span></span><span><span><span></span><span>vextracti128</span> <span>$1</span><span>,</span> <span>%ymm1</span><span>,</span> <span>%xmm2</span>            <span>; extract half (both halves are equal)
</span></span></span><span><span><span></span><span>vpacksswb</span>    <span>%xmm2</span><span>,</span> <span>%xmm1</span><span>,</span> <span>%xmm1</span>         <span>; go down to 8bit values, but weirdly shuffled
</span></span></span><span><span><span></span><span>vpshufd</span>      <span>$216</span><span>,</span> <span>%xmm1</span><span>,</span> <span>%xmm1</span>          <span>; unshuffle
</span></span></span><span><span><span></span><span>vpmovmskb</span>    <span>%xmm1</span><span>,</span> <span>%r8d</span>                 <span>; extract the high bit of each 8bit value.
</span></span></span><span><span><span></span><span>orl</span>          <span>$65536</span><span>,</span><span>%r8d</span>                 <span>; set bit 16, to cover the unwrap_or(N)
</span></span></span><span><span><span></span><span>tzcntl</span>       <span>%r8d</span><span>,</span><span>%r15d</span>                  <span>; count trailing zeros
</span></span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 12:</span>
Assembly code for Code Snippet <a href="#orge6452ef">11</a>. Instead of ending with <code>popcntl</code>, this ends with <code>tzcntl</code>.</p><p>Now, let’s look at this generated code in a bit more detail.</p><p>First up: why does <code>simd_le</code> translate into <code>min</code> and <code>cmpeq</code>?</p><p>From checking the <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#=undefined&amp;techs=AVX_ALL&amp;text=_mm256_cmp">Intel Intrinsics Guide</a>, we find out that there are only signed
comparisons, while our data is unsigned. For now, let’s just assume that all
values fit in 31 bits and are at most <code>i32::MAX</code>. Then, we can transmute our input
to <code>Simd&lt;i32, 8&gt;</code> without changing its meaning.</p><div><p>Assumption</p><div><p>Both input values and queries are between <code>0</code> and <code>i32::MAX</code>.</p><p>Eventually we can fix this by either taking <code>i32</code> input directly, or by shifting
<code>u32</code> values to fit in the <code>i32</code> range.</p></div></div><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn find_ctz_signed(&amp;self, q: u32) -&gt; usize
</span></span><span><span> where
</span></span><span><span>     LaneCount&lt;N&gt;: SupportedLaneCount,
</span></span><span><span> {
</span></span><span><span><span>-    let data: Simd&lt;u32, N&gt; = Simd::from_slice(                   &amp;self.data[0..N]   );
</span></span></span><span><span><span></span><span>+    let data: Simd&lt;i32, N&gt; = Simd::from_slice(unsafe { transmute(&amp;self.data[0..N]) });
</span></span></span><span><span><span></span><span>-    let q = Simd::splat(q       );
</span></span></span><span><span><span></span><span>+    let q = Simd::splat(q as i32);
</span></span></span><span><span><span></span>     let mask = q.simd_le(data);
</span></span><span><span>     mask.first_set().unwrap_or(N)
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--ctz-signed">Code Snippet 13</a>:</span>
Same as before, but now using <code>i32</code> values instead of <code>u32</code>.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span><span>-vpminud      32(%rsi,%r8), %ymm0, %ymm1
</span></span></span><span><span><span>-vpcmpeqd     %ymm1, %ymm0, %ymm1
</span></span></span><span><span><span></span><span>+vpcmpgtd     32(%rsi,%rdi), %ymm1, %ymm2 ; is query(%ymm1) &gt; data[8..]?
</span></span></span><span><span><span></span><span>-vpminud      (%rsi,%r8), %ymm0, %ymm2
</span></span></span><span><span><span>-vpcmpeqd     %ymm2, %ymm0, %ymm2
</span></span></span><span><span><span></span><span>+vpcmpgtd     (%rsi,%rdi), %ymm1, %ymm1   ; is query(%ymm1) &gt; data[..8]?
</span></span></span><span><span><span></span> vpackssdw    %ymm2, %ymm1, %ymm1         ; pack results
</span></span><span><span><span>+vpxor        %ymm0, %ymm1, %ymm1         ; negate results (ymm0 is all-ones)
</span></span></span><span><span><span></span> vextracti128 $1, %ymm1, %xmm2            ; extract u16x16
</span></span><span><span> vpacksswb    %xmm2, %xmm1, %xmm1         ; shuffle
</span></span><span><span> vpshufd      $216, %xmm1, %xmm1          ; extract u8x16
</span></span><span><span> vpmovmskb    %xmm1, %edi                 ; extract u16 mask
</span></span><span><span> orl          $65536,%edi                 ; add bit to get 16 when none set
</span></span><span><span> tzcntl       %edi,%edi                   ; count trailing zeros
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--ctz-signed-asm">Code Snippet 14</a>:</span>
The two <code>vpminud</code> and <code>vpcmpeqd</code> instructions are gone now and merged into <code>vpcmpgtd</code>, but instead we got a <code>vpxor</code> back :/ (Ignore the different registers being used in the old versus the new version.)</p><p>It turns out there is only a <code>&gt;</code> instruction in SIMD, and not <code>&gt;=</code>, and so there
is no way to avoid inverting the result.</p><p>We also see a <code>vpshufd</code> instruction that feels <em>very</em> out of place. What’s
happening is that while packing the result of the 16 <code>u32</code> comparisons down to a
single 16bit value, data is interleaved in an unfortunate way, and we need to
fix that.
Here, Algorithmica takes the approach of ‘pre-shuffling’ the values in each
node to counter for the unshuffle instruction.
They also suggest using <code>popcount</code> instead, which is indeed what we’ll do next.</p><h2 id="popcount"><span>2.4</span> Popcount
<a href="#popcount"></a></h2><p>As we saw, the drawback of the trailing zero count approach is that the order of
the lanes must be preserved. Instead, we’ll now simply count the number of lanes
with a value less than the query, similar to the auto-vectorized SIMD before,
so that the order of lanes doesn’t matter.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn find_popcnt_portable(&amp;self, q: u32) -&gt; usize
</span></span><span><span> where
</span></span><span><span>     LaneCount&lt;N&gt;: SupportedLaneCount,
</span></span><span><span> {
</span></span><span><span>     let data: Simd&lt;i32, N&gt; = Simd::from_slice(unsafe { transmute(&amp;self.data[0..N]) });
</span></span><span><span>     let q = Simd::splat(q as i32);
</span></span><span><span><span>-    let mask = q.simd_le(data);
</span></span></span><span><span><span></span><span>+    let mask = q.simd_gt(data);
</span></span></span><span><span><span></span><span>-    mask.first_set().unwrap_or(N)
</span></span></span><span><span><span></span><span>+    mask.to_bitmask().count_ones() as usize
</span></span></span><span><span><span></span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--popcount-1">Code Snippet 15</a>:</span>
Using popcount instead of trailing zeros.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> vpcmpgtd     32(%rsi,%rdi), %ymm0, %ymm1
</span></span><span><span> vpcmpgtd     (%rsi,%rdi), %ymm0, %ymm0
</span></span><span><span> vpackssdw    %ymm1, %ymm0, %ymm0     ; 1
</span></span><span><span><span>-vpxor        %ymm0, %ymm1, %ymm1
</span></span></span><span><span><span></span> vextracti128 $1, %ymm0, %xmm1        ; 2
</span></span><span><span> vpacksswb    %xmm1, %xmm0, %xmm0     ; 3
</span></span><span><span> vpshufd      $216, %xmm0, %xmm0      ; 4
</span></span><span><span> vpmovmskb    %xmm0, %edi             ; 5
</span></span><span><span><span>-orl          $65536,%edi
</span></span></span><span><span><span></span><span>+popcntl      %edi, %edi
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--popcount-1-asm">Code Snippet 16</a>:</span>
the <code>xor</code> and <code>or</code> instructions are gone, but we are still stuck with the sequence of 5 instructions to go from the comparison results to an integer bitmask.</p><p>Ideally we would like to <code>movmsk</code> directly on the <code>u16x16</code> output of the first
pack instruction, <code>vpackssdw</code>, to get the highest bit of each of the 16 16-bit values.
Unfortunately, we are again let down by AVX2: there are <code>movemask</code> <a href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#=undefined&amp;techs=AVX_ALL&amp;text=movms">instructions</a>
for <code>u8</code>, <code>u32</code>, and <code>u64</code>, but not for <code>u16</code>.</p><p>Also, the <code>vpshufd</code> instruction is now provably useless, so it’s slightly
disappointing the compiler didn’t elide it. Time to write the SIMD by hand instead.</p><h2 id="manual-simd"><span>2.5</span> Manual SIMD
<a href="#manual-simd"></a></h2><p>As it turns out, we can get away without most of the packing!
Instead of using <code>vpmovmskb</code> (<code>_mm256_movemask_epi8</code>) on 8bit data, we can
actually just use it directly on the 16bit output of <code>vpackssdw</code>!
Since the comparison sets each lane to all-zeros or all-ones, we can safely read
the most significant <em>and</em> middle bit, and divide the count by two at the
end.<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span> </span><span>fn</span> <span>find_popcnt</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>q</span>: <span>u32</span><span>)</span><span> </span>-&gt; <span>usize</span> <span>{</span><span>
</span></span></span><span><span><span>    </span><span>// We explicitly require that N is 16.
</span></span></span><span><span><span></span><span>    </span><span>let</span><span> </span><span>low</span>: <span>Simd</span><span>&lt;</span><span>u32</span><span>,</span><span> </span><span>8</span><span>&gt;</span><span> </span><span>=</span><span> </span><span>Simd</span>::<span>from_slice</span><span>(</span><span>&amp;</span><span>self</span><span>.</span><span>data</span><span>[</span><span>0</span><span>..</span><span>N</span><span> </span><span>/</span><span> </span><span>2</span><span>]);</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>high</span>: <span>Simd</span><span>&lt;</span><span>u32</span><span>,</span><span> </span><span>8</span><span>&gt;</span><span> </span><span>=</span><span> </span><span>Simd</span>::<span>from_slice</span><span>(</span><span>&amp;</span><span>self</span><span>.</span><span>data</span><span>[</span><span>N</span><span> </span><span>/</span><span> </span><span>2</span><span>..</span><span>N</span><span>]);</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>q_simd</span><span> </span><span>=</span><span> </span><span>Simd</span>::<span>&lt;</span><span>_</span><span>,</span><span> </span><span>8</span><span>&gt;</span>::<span>splat</span><span>(</span><span>q</span><span> </span><span>as</span><span> </span><span>i32</span><span>);</span><span>
</span></span></span><span><span><span>    </span><span>unsafe</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>use</span><span> </span><span>std</span>::<span>mem</span>::<span>transmute</span><span> </span><span>as</span><span> </span><span>t</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>// Transmute from u32 to i32.
</span></span></span><span><span><span></span><span>        </span><span>let</span><span> </span><span>mask_low</span><span> </span><span>=</span><span> </span><span>q_simd</span><span>.</span><span>simd_gt</span><span>(</span><span>t</span><span>(</span><span>low</span><span>));</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>mask_high</span><span> </span><span>=</span><span> </span><span>q_simd</span><span>.</span><span>simd_gt</span><span>(</span><span>t</span><span>(</span><span>high</span><span>));</span><span>
</span></span></span><span><span><span>        </span><span>// Transmute from portable_simd to __m256i intrinsic types.
</span></span></span><span><span><span></span><span>        </span><span>let</span><span> </span><span>merged</span><span> </span><span>=</span><span> </span><span>_mm256_packs_epi32</span><span>(</span><span>t</span><span>(</span><span>mask_low</span><span>),</span><span> </span><span>t</span><span>(</span><span>mask_high</span><span>));</span><span>
</span></span></span><span><span><span>        </span><span>// 32 bits is sufficient to hold a count of 2 per lane.
</span></span></span><span><span><span></span><span>        </span><span>let</span><span> </span><span>mask</span>: <span>i32</span> <span>=</span><span> </span><span>_mm256_movemask_epi8</span><span>(</span><span>t</span><span>(</span><span>merged</span><span>));</span><span>
</span></span></span><span><span><span>        </span><span>mask</span><span>.</span><span>count_ones</span><span>()</span><span> </span><span>as</span><span> </span><span>usize</span><span> </span><span>/</span><span> </span><span>2</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--popcount">Code Snippet 17</a>:</span>
Manual version of the SIMD code, by explicitly using the intrinsics. This is kinda ugly now, and there's a lot of transmuting (casting) going on between <code>[u32; 8]</code>, <code>Simd&lt;u32, 8&gt;</code> and the native <code>__m256i</code> type, but we'll have to live with it.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span><span>9
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> vpcmpgtd     (%rsi,%rdi), %ymm0, %ymm1
</span></span><span><span> vpcmpgtd     32(%rsi,%rdi), %ymm0, %ymm0
</span></span><span><span> vpackssdw    %ymm0, %ymm1, %ymm0
</span></span><span><span><span>-vextracti128 $1, %ymm0, %xmm1
</span></span></span><span><span><span>-vpacksswb    %xmm1, %xmm0, %xmm0
</span></span></span><span><span><span>-vpshufd      $216, %xmm0, %xmm0
</span></span></span><span><span><span>-vpmovmskb    %xmm0, %edi
</span></span></span><span><span><span></span><span>+vpmovmskb    %ymm0, %edi
</span></span></span><span><span><span></span> popcntl      %edi, %edi
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--popcount-asm">Code Snippet 18</a>:</span>
Only 5 instructions total are left now. Note that there is no explicit division by 2, since this is absorbed into the pointer arithmetic in the remainder, after the function is inlined.</p><p>Now let’s have a look at the results of all this work.</p><figure><a href="https://curiouscoding.nl/ox-hugo/3-find.svg"><img src="https://curiouscoding.nl/ox-hugo/3-find.svg" alt="Figure 6: Using the S-tree with an optimized find function improves throughput from 240ns/query for Eytzinger to 140ns/query for the auto-vectorized one, and down to 115ns/query for the final hand-optimized version, which is over 2x speedup!"></a><figcaption><p><span>Figure 6: </span>Using the S-tree with an optimized <code>find</code> function improves throughput from <code>240ns/query</code> for Eytzinger to <code>140ns/query</code> for the auto-vectorized one, and down to <code>115ns/query</code> for the final hand-optimized version, which is over 2x speedup!</p></figcaption></figure><p>As can be seen very nicely in this plot, each single instruction that we remove
gives a small but consistent improvement in throughput. The biggest improvement
comes from the last step, where we indeed shaved off 3 instructions.</p><p>In fact, we can analyse this plot a bit more:</p><ul><li>For input up to \(2^6=64\) bytes, the performance is constant, since in this
case the ‘search tree’ only consists of the root node.</li><li>Up to input of size \(2^{10}\), the thee has two layers, and the performance is constant.</li><li>Similarly, we see the latency jumping up at size \(2^{14}\), \(2^{18}\), \(2^{22}\)
and \(2^{26}\), each time because a new layer is added to the tree. (Or rather,
the jumps are at powers of the branching factor \(B+1=17\) instead of \(2^4=16\), but you get the idea.)</li><li>In a way, we can also (handwaivily) interpret the x-axis as time: each time
the graph jumps up, the height of the jump is pretty much the time spent on
processing that one extra layer of the tree.</li><li>Once we exceed the size of L3 cache, things slow down quickly. At that
point, each extra layer of the tree adds a significant amount of time, since
waiting for RAM is inherently slow.</li><li>On the other hand, once we hit RAM, the slowdown is more smooth rather than
stepwise. This is because L3 is still able to cache a fraction of the
data structure, and that fraction only decreases slowly.</li><li>Again handwavily, we can also interpret the x-axis as a snapshot of space
usage at a fixed moment in time: the first three layers of the tree fit in L1.
The 4th and 5th layers fit in L2 and L3. Once the three is 6 layers deep, the
reads of that layer will mostly hit RAM, and any additional layers for sure
are going to RAM.</li></ul><p>From now on, this last version, <code>find_popcnt</code>, is the one we will be using.</p><h2 id="optimizing-the-search"><span>3</span> Optimizing the search
<a href="#optimizing-the-search"></a></h2><h2 id="batching"><span>3.1</span> Batching
<a href="#batching"></a></h2><p>As promised, the first improvement we’ll make is <em>batching</em>.
Instead of processing one query at a time, we can process multiple (many) queries
at once. This allows the CPU to work on multiple queries at the same time, and
in particular, it can have multiple (up to 10-12) in-progress requests to RAM at
a time. That way, instead of waiting for a latency of 80ns per read, we
effectively wait for 10 reads at the same time, lowering the amortized wait time
to around 8ns.</p><p>Batching very much benefits from the fact that we use an S+ tree instead of
S-tree, since each element is find in the last layer (at the same depth), and
hence the number of seach steps through the tree is the same for every element
in the batch.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>fn</span> <span>batch</span><span>&lt;</span><span>const</span><span> </span><span>P</span>: <span>usize</span><span>&gt;</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>qb</span>: <span>&amp;</span><span>[</span><span>u32</span><span>;</span><span> </span><span>P</span><span>])</span><span> </span>-&gt; <span>[</span><span>u32</span><span>;</span><span> </span><span>P</span><span>]</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>k</span><span> </span><span>=</span><span> </span><span>[</span><span>0</span><span>;</span><span> </span><span>P</span><span>];</span><span>
</span></span></span><span><span><span>    </span><span>for</span><span> </span><span>[</span><span>o</span><span>,</span><span> </span><span>_o2</span><span>]</span><span> </span><span>in</span><span> </span><span>self</span><span>.</span><span>offsets</span><span>.</span><span>array_windows</span><span>()</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>for</span><span> </span><span>i</span><span> </span><span>in</span><span> </span><span>0</span><span>..</span><span>P</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>let</span><span> </span><span>jump_to</span><span> </span><span>=</span><span> </span><span>self</span><span>.</span><span>node</span><span>(</span><span>o</span><span> </span><span>+</span><span> </span><span>k</span><span>[</span><span>i</span><span>]).</span><span>find</span><span>(</span><span>qb</span><span>[</span><span>i</span><span>]);</span><span>
</span></span></span><span><span><span>            </span><span>k</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>k</span><span>[</span><span>i</span><span>]</span><span> </span><span>*</span><span> </span><span>(</span><span>B</span><span> </span><span>+</span><span> </span><span>1</span><span>)</span><span> </span><span>+</span><span> </span><span>jump_to</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>o</span><span> </span><span>=</span><span> </span><span>self</span><span>.</span><span>offsets</span><span>.</span><span>last</span><span>().</span><span>unwrap</span><span>();</span><span>
</span></span></span><span><span><span>    </span><span>from_fn</span><span>(</span><span>|</span><span>i</span><span>|</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>idx</span><span> </span><span>=</span><span> </span><span>self</span><span>.</span><span>node</span><span>(</span><span>o</span><span> </span><span>+</span><span> </span><span>k</span><span>[</span><span>i</span><span>]).</span><span>find</span><span>(</span><span>qb</span><span>[</span><span>i</span><span>]);</span><span>
</span></span></span><span><span><span>        </span><span>self</span><span>.</span><span>get</span><span>(</span><span>o</span><span> </span><span>+</span><span> </span><span>k</span><span>[</span><span>i</span><span>]</span><span> </span><span>+</span><span> </span><span>idx</span><span> </span><span>/</span><span> </span><span>N</span><span>,</span><span> </span><span>idx</span><span> </span><span>%</span><span> </span><span>N</span><span>)</span><span>
</span></span></span><span><span><span>    </span><span>})</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--batch">Code Snippet 19</a>:</span>
The batching code is very similar to processing one query at a time. We just insert an additional loop over the batch of \(P\) items.</p><figure><a href="https://curiouscoding.nl/ox-hugo/4-batching.svg"><img src="https://curiouscoding.nl/ox-hugo/4-batching.svg" alt="Figure 7: Batch size 1 (red) performs very similar to our non-batched version (blue), around 115ns/query. Increasing the batch size to 2, 4, and 8 each time significantly improves performance, until it saturates at 45ns/query (2.5x faster) around 16."></a><figcaption><p><span>Figure 7: </span>Batch size 1 (red) performs very similar to our non-batched version (blue), around <code>115ns/query</code>. Increasing the batch size to 2, 4, and 8 each time significantly improves performance, until it saturates at <code>45ns/query</code> (2.5x faster) around 16.</p></figcaption></figure><p>One interesting observation is that going from batch size 1 to 2 does <em>not</em>
double the performance. I suspect this is because the CPU’s out-of-order
execution was already deep enough to effectively execute (almost) 2 queries in
parallel anyway. Going to a batch size of 4 and then 8 does provide a
significant speedup. Again going to 4 the speedup is relatively a bit less than
when going to 8, so probably even with batch size 4 the CPU is somewhat looking
ahead into the next batch of 4 already 🤯.</p><p>Throughput saturates at batch size 16 (or really, around 12 already), which
corresponds to the CPU having 12 <em>line fill buffers</em> and thus being able to
read up to 12 cache lines in parallel.</p><p>Nevertheless, we will settle on a batch size of 128, mostly because it leads to
slightly cleaner plots in the remainder. It is also every so slightly faster,
probably because the constant overhead of initializing a batch is smaller when
batches are larger.</p><h2 id="prefetching"><span>3.2</span> Prefetching
<a href="#prefetching"></a></h2><p>The CPU is already fetching multiple reads in parallel using out-of-order
execution, but we can also help out a bit by doing this explicitly using <em>prefetching</em>.
After processing a node, we determine the child node <code>k</code> that we need to visit
next, so we can directly request that node to be read from memory before
continuing with the rest of the batch.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> fn batch&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let mut k = [0; P];
</span></span><span><span>     for [o, o2] in self.offsets.array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span>             let jump_to = self.node(o + k[i]).find(qb[i]);
</span></span><span><span>             k[i] = k[i] * (B + 1) + jump_to;
</span></span><span><span><span>+            prefetch_index(&amp;self.tree, o2 + k[i]);
</span></span></span><span><span><span></span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = self.offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span>         let idx = self.node(o + k[i]).find(qb[i]);
</span></span><span><span>         self.get(o + k[i] + idx / N, idx % N)
</span></span><span><span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--prefetch">Code Snippet 20</a>:</span>
Prefetching the cache line/node for the next iteration ahead.</p><figure><a href="https://curiouscoding.nl/ox-hugo/5-prefetch.svg"><img src="https://curiouscoding.nl/ox-hugo/5-prefetch.svg" alt="Figure 8: Prefetching helps speeding things up once the data does not fit in L2 cache anymore, and gets us down from 45ns/query to 30ns/query for 1GB input."></a><figcaption><p><span>Figure 8: </span>Prefetching helps speeding things up once the data does not fit in L2 cache anymore, and gets us down from <code>45ns/query</code> to <code>30ns/query</code> for <code>1GB</code> input.</p></figcaption></figure><p>We observe a few things: first prefetching slightly slow things down while data
fits in L1 already, since in that case the instruction just doesn’t do anything anyway.
In L2, it makes the graph slightly more flat, indicating that already there, the
latency is already a little bit of a bottleneck.
In L3 this effect gets larger, and we get a nice smooth/horizontal graph, until
we hit RAM size. There, prefetching provides the biggest gains.</p><h2 id="pointer-arithmetic"><span>3.3</span> Pointer arithmetic
<a href="#pointer-arithmetic"></a></h2><p>Again, it’s time to look at some assembly code, now to optimize the search
function itself. Results are down below in <a href="#figure--pointer-arithmetic">Figure 9</a>.</p><h3 id="up-front-splat"><span>3.3.1</span> Up-front splat
<a href="#up-front-splat"></a></h3><p>First, we can note that the <code>find</code> function <code>splat</code>’s the query from a <code>u32</code> to
a <code>Simd&lt;u32, 8&gt;</code> on each call. It’s slightly nicer (but not really faster,
actually) to splat all the queries
up-front, and then reuse those.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn batch_splat&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let mut k = [0; P];
</span></span><span><span><span>+    let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span></span><span><span><span></span>
</span></span><span><span>     for [o, o2] in self.offsets.array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span><span>-            let jump_to = self.node(o + k[i]).find      (qb[i]    );
</span></span></span><span><span><span></span><span>+            let jump_to = self.node(o + k[i]).find_splat(q_simd[i]);
</span></span></span><span><span><span></span>             k[i] = k[i] * (B + 1) + jump_to;
</span></span><span><span>             prefetch_index(&amp;self.tree, o2 + k[i]);
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = self.offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span><span>-        let idx = self.node(o + k[i]).find      (qb[i]    );
</span></span></span><span><span><span></span><span>+        let idx = self.node(o + k[i]).find_splat(q_simd[i]);
</span></span></span><span><span><span></span>         self.get(o + k[i] + idx / N, idx % N)
</span></span><span><span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--splat">Code Snippet 21</a>:</span>
<i>Hoisting</i> the <code>splat</code> out of the <i>loop</i> is slightly nicer, but not faster.</p><p>The assembly code for each iteration of the first loop now looks like this:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span></code></pre></td><td><pre tabindex="0"><code data-lang="asm"><span><span><span>movq</span>         <span>(</span><span>%rsp</span><span>,</span><span>%r11</span><span>),</span><span>%r15</span>
</span></span><span><span><span>leaq</span>         <span>(</span><span>%r9</span><span>,</span><span>%r15</span><span>),</span><span>%r12</span>
</span></span><span><span><span>shlq</span>         <span>$6</span><span>,</span> <span>%r12</span>
</span></span><span><span><span>vmovdqa</span>      <span>1536</span><span>(</span><span>%rsp</span><span>,</span><span>%r11</span><span>,</span><span>4</span><span>),</span><span>%ymm0</span>
</span></span><span><span><span>vpcmpgtd</span>     <span>(</span><span>%rsi</span><span>,</span><span>%r12</span><span>),</span> <span>%ymm0</span><span>,</span> <span>%ymm1</span>
</span></span><span><span><span>vpcmpgtd</span>     <span>32</span><span>(</span><span>%rsi</span><span>,</span><span>%r12</span><span>),</span> <span>%ymm0</span><span>,</span> <span>%ymm0</span>
</span></span><span><span><span>vpackssdw</span>    <span>%ymm0</span><span>,</span> <span>%ymm1</span><span>,</span> <span>%ymm0</span>
</span></span><span><span><span>vpmovmskb</span>    <span>%ymm0</span><span>,</span> <span>%r12d</span>
</span></span><span><span><span>popcntl</span>      <span>%r12d</span><span>,</span> <span>%r12d</span>
</span></span><span><span><span>shrl</span>         <span>%r12d</span>
</span></span><span><span><span>movq</span>         <span>%r15</span><span>,</span><span>%r13</span>
</span></span><span><span><span>shlq</span>         <span>$4</span><span>,</span> <span>%r13</span>
</span></span><span><span><span>addq</span>         <span>%r15</span><span>,</span><span>%r13</span>
</span></span><span><span><span>addq</span>         <span>%r12</span><span>,</span><span>%r13</span>
</span></span><span><span><span>movq</span>         <span>%r13</span><span>,(</span><span>%rsp</span><span>,</span><span>%r11</span><span>)</span>
</span></span><span><span><span>shlq</span>         <span>$6</span><span>,</span> <span>%r13</span>
</span></span><span><span><span>prefetcht0</span>   <span>(</span><span>%r10</span><span>,</span><span>%r13</span><span>)</span>
</span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 22:</span>
Assembly code for each iteration of Code Snippet <a href="#orgaceea1d">21</a>. (Actually it's unrolled into two copied of this, but they're identical.)</p><h3 id="byte-based-pointers"><span>3.3.2</span> Byte-based pointers
<a href="#byte-based-pointers"></a></h3><p>Looking at the code above, we see two <code>shlq $6</code> instructions that multiply the
given value by \(64\). That’s because our tree nodes are 64 bytes large, and
hence, to get the \(i\)’th element of the array, we need to read at byte \(64\cdot
i\). For smaller element sizes, there are dedicated read instructions that
inline, say, an index multiplication by 8. But for a stride of 64, the compiler
has to generate ‘manual’ multiplications in the form of a shift.</p><p>Additionally, direct pointer-based lookups can be slightly more efficient here than
array-indexing: when doing <code>self.tree[o + k[i]]</code>, we can effectively pre-compute
the pointer to <code>self.tree[o]</code>, so that only <code>k[i]</code> still has to be added. Let’s
first look at that diff:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn batch_ptr&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let mut k = [0; P];
</span></span><span><span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span><span>+    // offsets[l] is a pointer to self.tree[self.offsets[l]]
</span></span></span><span><span><span>+    let offsets = self.offsets.iter()
</span></span></span><span><span><span>+        .map(|o| unsafe { self.tree.as_ptr().add(*o) })
</span></span></span><span><span><span>+        .collect_vec();
</span></span></span><span><span><span></span>
</span></span><span><span>     for [o, o2] in offsets.array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span><span>-            let jump_to = self.node(o  +  k[i])  .find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>+            let jump_to = unsafe { *o.add(k[i]) }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span>             k[i] = k[i] * (B + 1) + jump_to;
</span></span><span><span><span>-            prefetch_index(&amp;self.tree, o2 + k[i]);
</span></span></span><span><span><span></span><span>+            prefetch_ptr(unsafe { o2.add(k[i]) });
</span></span></span><span><span><span></span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span><span>-        let idx = self.node(o  +  k[i])  .find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>+        let idx = unsafe { *o.add(k[i]) }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>-        self.get(o + k[i] + idx / N, idx % N)
</span></span></span><span><span><span></span><span>+        unsafe { *(*o.add(k[i] + idx / N)).data.get_unchecked(idx % N) }
</span></span></span><span><span><span></span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--ptr">Code Snippet 23</a>:</span>
Using pointer-based indexing instead of array indexing.</p><p>Now, we can avoid all the multiplications by 64, by just multiplying all <code>k[i]</code>
by 64 to start with:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn batch_byte_ptr&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let mut k = [0; P];
</span></span><span><span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span>     let offsets = self
</span></span><span><span>         .offsets
</span></span><span><span>         .iter()
</span></span><span><span>         .map(|o| unsafe { self.tree.as_ptr().add(*o) })
</span></span><span><span>         .collect_vec();
</span></span><span><span>
</span></span><span><span>     for [o, o2] in offsets.array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span><span>-            let jump_to = unsafe { *o.     add(k[i]) }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>+            let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>-            k[i] = k[i] * (B + 1) + jump_to     ;
</span></span></span><span><span><span></span><span>+            k[i] = k[i] * (B + 1) + jump_to * 64;
</span></span></span><span><span><span></span><span>-            prefetch_ptr(unsafe { o2.     add(k[i]) });
</span></span></span><span><span><span></span><span>+            prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span></span><span><span><span></span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span><span>-        let idx = unsafe { *o.     add(k[i]) }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>+        let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>-        unsafe { *(*o.add(k[i] + idx / N)).data.get_unchecked(idx % N) }
</span></span></span><span><span><span></span><span>+        unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }
</span></span></span><span><span><span></span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--ptr64">Code Snippet 24</a>:</span>
We multiply <code>k[i]</code> by 64 up-front, and then call <code>byte_add</code> instead of the usual <code>add</code>.</p><p>Indeed, the generated code now goes down from 17 to 15 instructions, and we can
see in <a href="#figure--pointer-arithmetic">Figure 9</a> that this gives a significant speedup!</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span></code></pre></td><td><pre tabindex="0"><code data-lang="asm"><span><span><span>movq</span>         <span>32</span><span>(</span><span>%rsp</span><span>,</span><span>%rdi</span><span>),</span><span>%r8</span>
</span></span><span><span><span>vmovdqa</span>      <span>1568</span><span>(</span><span>%rsp</span><span>,</span><span>%rdi</span><span>,</span><span>4</span><span>),</span><span>%ymm0</span>
</span></span><span><span><span>vpcmpgtd</span>     <span>(</span><span>%rsi</span><span>,</span><span>%r8</span><span>),</span> <span>%ymm0</span><span>,</span> <span>%ymm1</span>
</span></span><span><span><span>vpcmpgtd</span>     <span>32</span><span>(</span><span>%rsi</span><span>,</span><span>%r8</span><span>),</span> <span>%ymm0</span><span>,</span> <span>%ymm0</span>
</span></span><span><span><span>vpackssdw</span>    <span>%ymm0</span><span>,</span> <span>%ymm1</span><span>,</span> <span>%ymm0</span>
</span></span><span><span><span>vpmovmskb</span>    <span>%ymm0</span><span>,</span> <span>%r9d</span>
</span></span><span><span><span>popcntl</span>      <span>%r9d</span><span>,</span> <span>%r9d</span>
</span></span><span><span><span>movq</span>         <span>%r8</span><span>,</span><span>%r10</span>
</span></span><span><span><span>shlq</span>         <span>$4</span><span>,</span> <span>%r10</span>
</span></span><span><span><span>addq</span>         <span>%r8</span><span>,</span><span>%r10</span>
</span></span><span><span><span>shll</span>         <span>$5</span><span>,</span> <span>%r9d</span>
</span></span><span><span><span>andl</span>         <span>$-64</span><span>,</span><span>%r9d</span>
</span></span><span><span><span>addq</span>         <span>%r10</span><span>,</span><span>%r9</span>
</span></span><span><span><span>movq</span>         <span>%r9</span><span>,</span><span>32</span><span>(</span><span>%rsp</span><span>,</span><span>%rdi</span><span>)</span>
</span></span><span><span><span>prefetcht0</span>   <span>(</span><span>%rcx</span><span>,</span><span>%r9</span><span>)</span>
</span></span></code></pre></td></tr></tbody></table></div><p><span><a href="#code-snippet--byte-ptr">Code Snippet 25</a>:</span>
When using byte-based pointers, we avoid some multiplications by 64.</p><h3 id="the-final-version"><span>3.3.3</span> The final version
<a href="#the-final-version"></a></h3><p>One particularity about the code above is the <code>andl $-64,%r9d</code>.
In line 6, the bitmask gets written there. Then in line 7, it’s popcounted.
Life 11 does a <code>shll $5</code>, i.e., a multiplication by 32, which is a combination
of the <code>/2</code> to compensate for the double-popcount and the <code>* 64</code>. Then, it does
the <code>and $-64</code>, where the mask of -64 is <code>111..11000000</code> which ends in 6 zeros.
But we just multiplied by 32, so all this does is zeroing out a single bit, in
case the popcount was odd. But we know for a fact that that can never be, so we
don’t actually need this <code>and</code> instruction.</p><p>To avoid it, we do this <code>/2*64 =&gt; *32</code> optimization manually.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span><span>35
</span><span>36
</span><span>37
</span><span>38
</span><span>39
</span><span>40
</span><span>41
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn find_splat64(&amp;self, q_simd: Simd&lt;u32, 8&gt;) -&gt; usize {
</span></span><span><span>     let low: Simd&lt;u32, 8&gt; = Simd::from_slice(&amp;self.data[0..N / 2]);
</span></span><span><span>     let high: Simd&lt;u32, 8&gt; = Simd::from_slice(&amp;self.data[N / 2..N]);
</span></span><span><span>     unsafe {
</span></span><span><span>         let q_simd: Simd&lt;i32, 8&gt; = t(q_simd);
</span></span><span><span>         let mask_low = q_simd.simd_gt(t(low));
</span></span><span><span>         let mask_high = q_simd.simd_gt(t(high));
</span></span><span><span>         use std::mem::transmute as t;
</span></span><span><span>         let merged = _mm256_packs_epi32(t(mask_low), t(mask_high));
</span></span><span><span>         let mask = _mm256_movemask_epi8(merged);
</span></span><span><span><span>-        mask.count_ones() as usize / 2
</span></span></span><span><span><span></span><span>+        mask.count_ones() as usize * 32
</span></span></span><span><span><span></span>     }
</span></span><span><span> }
</span></span><span><span>
</span></span><span><span> pub fn batch_byte_ptr&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let mut k = [0; P];
</span></span><span><span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span>     let offsets = self
</span></span><span><span>         .offsets
</span></span><span><span>         .iter()
</span></span><span><span>         .map(|o| unsafe { self.tree.as_ptr().add(*o) })
</span></span><span><span>         .collect_vec();
</span></span><span><span>
</span></span><span><span>     for [o, o2] in offsets.array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span><span>-            let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat  (q_simd[i]);
</span></span></span><span><span><span></span><span>+            let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);
</span></span></span><span><span><span></span><span>-            k[i] = k[i] * (B + 1) + jump_to * 64;
</span></span></span><span><span><span></span><span>+            k[i] = k[i] * (B + 1) + jump_to     ;
</span></span></span><span><span><span></span>             prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span>         let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);
</span></span><span><span>         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }
</span></span><span><span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 26:</span>
Manually merging <code>/2</code> and <code>*64</code> into <code>*32</code>.</p><p>Again, this gives a small speedup.</p><figure><a href="https://curiouscoding.nl/ox-hugo/6-improvements.svg"><img src="https://curiouscoding.nl/ox-hugo/6-improvements.svg" alt="Figure 9: Results of improving the search function bit by bit. Like before, the improvements are small but consistent. Throughput on 1GB input improves from 31ns/query to 28ns/query."></a><figcaption><p><span>Figure 9: </span>Results of improving the search function bit by bit. Like before, the improvements are small but consistent. Throughput on <code>1GB</code> input improves from <code>31ns/query</code> to <code>28ns/query</code>.</p></figcaption></figure><h2 id="skip-prefetch"><span>3.4</span> Skip prefetch
<a href="#skip-prefetch"></a></h2><p>Now we know that the first three levels of the graph fit in L1 cache, so
probably we can simply skip prefetching for those levels.</p><figure><a href="https://curiouscoding.nl/ox-hugo/7-skip-prefetch.svg"><img src="https://curiouscoding.nl/ox-hugo/7-skip-prefetch.svg" alt="Figure 10: Skipping the prefetch for the first layers is slightly slower."></a><figcaption><p><span>Figure 10: </span>Skipping the prefetch for the first layers is slightly slower.</p></figcaption></figure><p>As it turns out, skipping the prefetch does not help. Probably because the
prefetch is cheap if the data is already available, and there is a small chance
that the data we need was evicted to make room for other things, in which case
the prefetch <em>is</em> useful.</p><h2 id="interleave"><span>3.5</span> Interleave
<a href="#interleave"></a></h2><p>One other observation is that the first few layers are CPU bound, while the last
few layers are memory throughput bound.
By merging the two domains, we should be able to get a higher total throughput.
(Somewhat similar to how for a piece wise linear convex function \(f\), \(f((x+y)/2) &lt;
(f(x)+f(y))/2\) when \(x\) and \(y\) are on different pieces.)
Thus, maybe we could process two batches
of queries at the same time by processing layer \(i\) of one batch at the same
time as layer \(i+L/2\) of the other batch (where \(L\) is the height of the tree).
I implemented this, but unfortunately the result is not faster than what we had.</p><p>Or maybe we can split the work as: interleave the last level of one half
with <em>all but the last</em> level of the other half? Since the last-level memory
read takes most of the time. Also that turns out slower in practice.</p><p>What does give a small speedup: process the first <em>two</em> levels of the next batch
interleaved with the last prefetch of the current batch. Still the result is
only around <code>2ns</code> speedup, while code the (not shown ;") gets significantly more
messy.</p><p>What <em>does</em> work great, is interleaving <em>all</em> layers of the search: when the
tree has \(L\) layers, we can interleave \(L\) batches at a time, and then process
layer \(i\) of the \(i\)’th in-progress batch. Then we ‘shift out’ the completed
batch and store the answers to those queries, and ‘shift in’ a new batch.
This we, completely average the different workloads of all the layers, and
should achieve near-optimal performance given the CPU’s memory bandwidth to L3
and RAM (at least, that’s what I assume is the bottleneck now).</p><details><summary>Click to show code for interleaving.</summary><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span><span>35
</span><span>36
</span><span>37
</span><span>38
</span><span>39
</span><span>40
</span><span>41
</span><span>42
</span><span>43
</span><span>44
</span><span>45
</span><span>46
</span><span>47
</span><span>48
</span><span>49
</span><span>50
</span><span>51
</span><span>52
</span><span>53
</span><span>54
</span><span>55
</span><span>56
</span><span>57
</span><span>58
</span><span>59
</span><span>60
</span><span>61
</span><span>62
</span><span>63
</span><span>64
</span><span>65
</span><span>66
</span><span>67
</span><span>68
</span><span>69
</span><span>70
</span><span>71
</span><span>72
</span><span>73
</span><span>74
</span><span>75
</span></code></pre></td><td><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span><span> </span><span>fn</span> <span>batch_interleave_full_128</span><span>(</span><span>&amp;</span><span>self</span><span>,</span><span> </span><span>qs</span>: <span>&amp;</span><span>[</span><span>u32</span><span>])</span><span> </span>-&gt; <span>Vec</span><span>&lt;</span><span>u32</span><span>&gt;</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>match</span><span> </span><span>self</span><span>.</span><span>offsets</span><span>.</span><span>len</span><span>()</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>// 1 batch of size 128
</span></span></span><span><span><span></span><span>        </span><span>1</span><span> </span><span>=&gt;</span><span> </span><span>self</span><span>.</span><span>batch_interleave_full</span>::<span>&lt;</span><span>128</span><span>,</span><span> </span><span>1</span><span>,</span><span> </span><span>128</span><span>&gt;</span><span>(</span><span>qs</span><span>),</span><span>
</span></span></span><span><span><span>        </span><span>// 2 batches of size 64 in parallel, with product 128
</span></span></span><span><span><span></span><span>        </span><span>2</span><span> </span><span>=&gt;</span><span> </span><span>self</span><span>.</span><span>batch_interleave_full</span>::<span>&lt;</span><span>64</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>128</span><span>&gt;</span><span>(</span><span>qs</span><span>),</span><span>
</span></span></span><span><span><span>        </span><span>// 3 batches of size 32 in parallel with product 96
</span></span></span><span><span><span></span><span>        </span><span>3</span><span> </span><span>=&gt;</span><span> </span><span>self</span><span>.</span><span>batch_interleave_full</span>::<span>&lt;</span><span>32</span><span>,</span><span> </span><span>3</span><span>,</span><span> </span><span>96</span><span>&gt;</span><span>(</span><span>qs</span><span>),</span><span>
</span></span></span><span><span><span>        </span><span>4</span><span> </span><span>=&gt;</span><span> </span><span>self</span><span>.</span><span>batch_interleave_full</span>::<span>&lt;</span><span>32</span><span>,</span><span> </span><span>4</span><span>,</span><span> </span><span>128</span><span>&gt;</span><span>(</span><span>qs</span><span>),</span><span>
</span></span></span><span><span><span>        </span><span>5</span><span> </span><span>=&gt;</span><span> </span><span>self</span><span>.</span><span>batch_interleave_full</span>::<span>&lt;</span><span>16</span><span>,</span><span> </span><span>5</span><span>,</span><span> </span><span>80</span><span>&gt;</span><span>(</span><span>qs</span><span>),</span><span>
</span></span></span><span><span><span>        </span><span>6</span><span> </span><span>=&gt;</span><span> </span><span>self</span><span>.</span><span>batch_interleave_full</span>::<span>&lt;</span><span>16</span><span>,</span><span> </span><span>6</span><span>,</span><span> </span><span>96</span><span>&gt;</span><span>(</span><span>qs</span><span>),</span><span>
</span></span></span><span><span><span>        </span><span>7</span><span> </span><span>=&gt;</span><span> </span><span>self</span><span>.</span><span>batch_interleave_full</span>::<span>&lt;</span><span>16</span><span>,</span><span> </span><span>7</span><span>,</span><span> </span><span>112</span><span>&gt;</span><span>(</span><span>qs</span><span>),</span><span>
</span></span></span><span><span><span>        </span><span>8</span><span> </span><span>=&gt;</span><span> </span><span>self</span><span>.</span><span>batch_interleave_full</span>::<span>&lt;</span><span>16</span><span>,</span><span> </span><span>8</span><span>,</span><span> </span><span>128</span><span>&gt;</span><span>(</span><span>qs</span><span>),</span><span>
</span></span></span><span><span><span>        </span><span>_</span><span> </span><span>=&gt;</span><span> </span><span>panic!</span><span>(</span><span>"Unsupported tree height </span><span>{}</span><span>"</span><span>,</span><span> </span><span>self</span><span>.</span><span>offsets</span><span>.</span><span>len</span><span>()),</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>pub</span><span> </span><span>fn</span> <span>batch_interleave_full</span><span>&lt;</span><span>const</span><span> </span><span>P</span>: <span>usize</span><span>,</span><span> </span><span>const</span><span> </span><span>L</span>: <span>usize</span><span>,</span><span> </span><span>const</span><span> </span><span>PL</span>: <span>usize</span><span>&gt;</span><span>(</span><span>
</span></span></span><span><span><span>    </span><span>&amp;</span><span>self</span><span>,</span><span>
</span></span></span><span><span><span>    </span><span>qs</span>: <span>&amp;</span><span>[</span><span>u32</span><span>],</span><span>
</span></span></span><span><span><span></span><span>)</span><span> </span>-&gt; <span>Vec</span><span>&lt;</span><span>u32</span><span>&gt;</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>    </span><span>assert_eq!</span><span>(</span><span>self</span><span>.</span><span>offsets</span><span>.</span><span>len</span><span>(),</span><span> </span><span>L</span><span>);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>out</span><span> </span><span>=</span><span> </span><span>Vec</span>::<span>with_capacity</span><span>(</span><span>qs</span><span>.</span><span>len</span><span>());</span><span>
</span></span></span><span><span><span>    </span><span>let</span><span> </span><span>mut</span><span> </span><span>ans</span><span> </span><span>=</span><span> </span><span>[</span><span>0</span><span>;</span><span> </span><span>P</span><span>];</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>// Iterate over chunks of size P of queries.
</span></span></span><span><span><span></span><span>    </span><span>// Omitted: initialize
</span></span></span><span><span><span></span><span>    </span><span>let</span><span> </span><span>first_i</span><span> </span><span>=</span><span> </span><span>L</span><span>-</span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>    </span><span>for</span><span> </span><span>chunk</span><span> </span><span>in</span><span> </span><span>qs</span><span>.</span><span>array_chunks</span>::<span>&lt;</span><span>P</span><span>&gt;</span><span>()</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>        </span><span>let</span><span> </span><span>i</span><span> </span><span>=</span><span> </span><span>first_i</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>        </span><span>// Decrement first_i, modulo L.
</span></span></span><span><span><span></span><span>        </span><span>if</span><span> </span><span>first_i</span><span> </span><span>==</span><span> </span><span>0</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>first_i</span><span> </span><span>=</span><span> </span><span>L</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>        </span><span>first_i</span><span> </span><span>-=</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>        </span><span>// Process 1 element per chunk, starting at element first_i.
</span></span></span><span><span><span></span><span>        </span><span>// (Omitted: process first up-to L elements.)
</span></span></span><span><span><span></span><span>        </span><span>// Write output and read new queries from index j.
</span></span></span><span><span><span></span><span>        </span><span>let</span><span> </span><span>mut</span><span> </span><span>j</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span>        </span><span>loop</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>            </span><span>// First L-1 levels: do the usual thing.
</span></span></span><span><span><span></span><span>            </span><span>// The compiler will unroll this loop.
</span></span></span><span><span><span></span><span>            </span><span>for</span><span> </span><span>l</span><span> </span><span>in</span><span> </span><span>0</span><span>..</span><span>L</span><span> </span><span>-</span><span> </span><span>1</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>                </span><span>let</span><span> </span><span>jump_to</span><span> </span><span>=</span><span> </span><span>unsafe</span><span> </span><span>{</span><span> </span><span>*</span><span>offsets</span><span>[</span><span>l</span><span>].</span><span>byte_add</span><span>(</span><span>k</span><span>[</span><span>i</span><span>])</span><span> </span><span>}.</span><span>find_splat64</span><span>(</span><span>q_simd</span><span>[</span><span>i</span><span>]);</span><span>
</span></span></span><span><span><span>                </span><span>k</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>k</span><span>[</span><span>i</span><span>]</span><span> </span><span>*</span><span> </span><span>(</span><span>B</span><span> </span><span>+</span><span> </span><span>1</span><span>)</span><span> </span><span>+</span><span> </span><span>jump_to</span><span>;</span><span>
</span></span></span><span><span><span>                </span><span>prefetch_ptr</span><span>(</span><span>unsafe</span><span> </span><span>{</span><span> </span><span>offsets</span><span>[</span><span>l</span><span> </span><span>+</span><span> </span><span>1</span><span>].</span><span>byte_add</span><span>(</span><span>k</span><span>[</span><span>i</span><span>])</span><span> </span><span>});</span><span>
</span></span></span><span><span><span>                </span><span>i</span><span> </span><span>+=</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>            </span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>            </span><span>// Last level: read answer.
</span></span></span><span><span><span></span><span>            </span><span>ans</span><span>[</span><span>j</span><span>]</span><span> </span><span>=</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>                </span><span>let</span><span> </span><span>idx</span><span> </span><span>=</span><span> </span><span>unsafe</span><span> </span><span>{</span><span> </span><span>*</span><span>ol</span><span>.</span><span>byte_add</span><span>(</span><span>k</span><span>[</span><span>i</span><span>])</span><span> </span><span>}.</span><span>find_splat</span><span>(</span><span>q_simd</span><span>[</span><span>i</span><span>]);</span><span>
</span></span></span><span><span><span>                </span><span>unsafe</span><span> </span><span>{</span><span> </span><span>(</span><span>ol</span><span>.</span><span>byte_add</span><span>(</span><span>k</span><span>[</span><span>i</span><span>])</span><span> </span><span>as</span><span> </span><span>*</span><span>const</span><span> </span><span>u32</span><span>).</span><span>add</span><span>(</span><span>idx</span><span>).</span><span>read</span><span>()</span><span> </span><span>}</span><span>
</span></span></span><span><span><span>            </span><span>};</span><span>
</span></span></span><span><span><span>            </span><span>// Last level: reset index, and read new query.
</span></span></span><span><span><span></span><span>            </span><span>k</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>0</span><span>;</span><span>
</span></span></span><span><span><span>            </span><span>q_simd</span><span>[</span><span>i</span><span>]</span><span> </span><span>=</span><span> </span><span>Simd</span>::<span>splat</span><span>(</span><span>chunk</span><span>[</span><span>j</span><span>]);</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>            </span><span>i</span><span> </span><span>+=</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>            </span><span>j</span><span> </span><span>+=</span><span> </span><span>1</span><span>;</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>            </span><span>if</span><span> </span><span>i</span><span> </span><span>&gt;</span><span> </span><span>PL</span><span> </span><span>-</span><span> </span><span>L</span><span> </span><span>{</span><span>
</span></span></span><span><span><span>                </span><span>break</span><span>;</span><span>
</span></span></span><span><span><span>            </span><span>}</span><span>
</span></span></span><span><span><span>        </span><span>}</span><span>
</span></span></span><span><span><span>        </span><span>// (Omitted: process last up-to L elements.)
</span></span></span><span><span><span></span><span>
</span></span></span><span><span><span>        </span><span>out</span><span>.</span><span>extend_from_slice</span><span>(</span><span>&amp;</span><span>ans</span><span>);</span><span>
</span></span></span><span><span><span>    </span><span>}</span><span>
</span></span></span><span><span><span>
</span></span></span><span><span><span>    </span><span>out</span><span>
</span></span></span><span><span><span></span><span>}</span><span>
</span></span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 27:</span>
In code, we interleave all layers by compiling a separate function for each height of the tree. Then the compiler can unroll the loop over the layers. There is a bunch of overhead in the code for the first and last iterations that's omitted.</p></details><figure><a href="https://curiouscoding.nl/ox-hugo/8-interleave.svg"><img src="https://curiouscoding.nl/ox-hugo/8-interleave.svg" alt="Figure 11: Interleaving all layers of the search binary search improves throughput from 29ns/query to 24ns/query."></a><figcaption><p><span>Figure 11: </span>Interleaving all layers of the search binary search improves throughput from <code>29ns/query</code> to <code>24ns/query</code>.</p></figcaption></figure><h2 id="optimizing-the-tree-layout"><span>4</span> Optimizing the tree layout
<a href="#optimizing-the-tree-layout"></a></h2><h2 id="left-tree"><span>4.1</span> Left-tree
<a href="#left-tree"></a></h2><p>So far, every internal node of the tree stores the minimum of the subtree on
it’s right (<a href="#figure--stree-full">Figure 3</a>, reproduced below).</p><figure><a href="https://curiouscoding.nl/ox-hugo/full.svg"><img src="https://curiouscoding.nl/ox-hugo/full.svg" alt="Figure 12: Usually in B+ trees, each node stores the minimum of it’s right subtree. Let’s call this a right (S+/B+) tree."></a><figcaption><p><span>Figure 12: </span>Usually in B+ trees, each node stores the minimum of it’s right subtree. Let’s call this a <em>right</em> (S+/B+) tree.</p></figcaption></figure><p>This turns out somewhat inefficient when searching values that are exactly in
between two subtrees (as <em>also</em> already suggested by Algorithmica), such as
\(5.5\). In that case, the search descends into the
leftmost (green) subtree with node \([2, 4]\). Then, it goes to the rightmost
(red) node \([4,5]\). There, we realize \(5.5 &gt; 5\), and thus we need the next value
in the red layer (which is stored as a single array), which is \(6\). The problem
now is that the red tree nodes exactly correspond to cache lines, and thus, the
\(6\) will be in a new cache line that needs to be fetched from memory.</p><p>Now consider the <em>left-max</em> tree below:</p><figure><a href="https://curiouscoding.nl/ox-hugo/flipped.svg"><img src="https://curiouscoding.nl/ox-hugo/flipped.svg" alt="Figure 13: In the left-max S+ tree, each internal node contains the maximum of its left subtree."></a><figcaption><p><span>Figure 13: </span>In the <em>left-max</em> S+ tree, each internal node contains the maximum of its <em>left</em> subtree.</p></figcaption></figure><p>Now if we search for \(5.5\), we descend into the middle subtree rooted at
\([7,9]\). Then we go left to the \([6,7]\) node, and end up reading \(6\) as the
first value \(\geq 5.5\). Now, the search directly steers toward the node
that actually contains the answer, instead of the one just before.</p><figure><a href="https://curiouscoding.nl/ox-hugo/9-left-max-tree.svg"><img src="https://curiouscoding.nl/ox-hugo/9-left-max-tree.svg" alt="Figure 14: The left-S tree brings runtime down from 24ns/query for the interleaved version to 22ns/query now."></a><figcaption><p><span>Figure 14: </span>The left-S tree brings runtime down from <code>24ns/query</code> for the interleaved version to <code>22ns/query</code> now.</p></figcaption></figure><h2 id="memory-layouts"><span>4.2</span> Memory layouts
<a href="#memory-layouts"></a></h2><p>Let’s now consider some alternative memory layouts.
So far, we were packing all layers in forward order, but the Algorithmica post
actually stores them in reverse, so we’ll try that too. The query code is
exactly the same, since the order of the layers is already encoded into the offsets.</p><p>Another potential improvement is to always store a <em>full</em> array. This may seem
very inefficient, but is actually not that bad when we make sure to use
uninitialized memory. In that case, untouched memory pages will simply never be
mapped, so that we waste on average only about 2MB
per layer when hugepages are enabled, and 14MB when there are 7 layers and the
entire array takes 1GB.</p><figure><a href="https://curiouscoding.nl/ox-hugo/layouts.svg"><img src="https://curiouscoding.nl/ox-hugo/layouts.svg" alt="Figure 15: So far we have been using the packed layout. We now also try the reversed layout as used by Algorithmica, and the full layout that allows simple arithmetic for indexing."></a><figcaption><p><span>Figure 15: </span>So far we have been using the packed layout. We now also try the <em>reversed</em> layout as used by Algorithmica, and the <em>full</em> layout that allows simple arithmetic for indexing.</p></figcaption></figure><p>A benefit of storing the full array is that instead of using the offsets, we can
simply compute the index in the next layer directly, as we did for the
Eytzinger search.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn batch_ptr3_full&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let mut k = [0; P];
</span></span><span><span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span><span>+    let o = self.tree.as_ptr();
</span></span></span><span><span><span></span>
</span></span><span><span><span>-    for [o, o2] in offsets.array_windows() {
</span></span></span><span><span><span></span><span>+    for _l      in 0..self.offsets.len() - 1 {
</span></span></span><span><span><span></span>         for i in 0..P {
</span></span><span><span>             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);
</span></span><span><span><span>-            k[i] = k[i] * (B + 1) + jump_to     ;
</span></span></span><span><span><span></span><span>+            k[i] = k[i] * (B + 1) + jump_to + 64;
</span></span></span><span><span><span></span>             prefetch_ptr(unsafe { o.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     from_fn(|i| {
</span></span><span><span>         let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);
</span></span><span><span>         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }
</span></span><span><span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 28:</span>
When storing the array in full, we can drop the per-layer offsets and instead compute indices directly.</p><figure><a href="https://curiouscoding.nl/ox-hugo/9-params.svg"><img src="https://curiouscoding.nl/ox-hugo/9-params.svg" alt="Figure 16: Comparison with reverse and full memory layout, and full memory layout with using a dedicated _full search that computes indices directly."></a><figcaption><p><span>Figure 16: </span>Comparison with reverse and full memory layout, and full memory layout with using a dedicated <code>_full</code> search that computes indices directly.</p></figcaption></figure><p>As it turns out, neither of those layouts improves performance, and so we will
not use them going forward.</p><h2 id="node-size-b-15"><span>4.3</span> Node size \(B=15\)
<a href="#node-size-b-15"></a></h2><p>We can also try storing only 15 values per node, so that the branching factor
is 16. This has the benefit of making the multiplication by \(B+1\) (17 so far)
slightly simpler, since it replaces <code>x = (x&lt;&lt;4)+x</code> by <code>x = x&lt;&lt;4</code>.</p><figure><a href="https://curiouscoding.nl/ox-hugo/10-base15.svg"><img src="https://curiouscoding.nl/ox-hugo/10-base15.svg" alt="Figure 17: Storing 15 values per node. The lines in the bottom part of the plot show the overhead that each data structure has relative to the size of the input, capped at 1 (which corresponds to take double the size)."></a><figcaption><p><span>Figure 17: </span>Storing 15 values per node. The lines in the bottom part of the plot show the overhead that each data structure has relative to the size of the input, capped at 1 (which corresponds to take double the size).</p></figcaption></figure><p>When the tree has up to 5 layers and the data fits in L3 cache, using \(B=15\) is
indeed slightly faster when the number of layers in the tree is the same. On the
other hand, the lower branching factor of \(16\) requires an additional layer for smaller sizes than
when using branching factor \(17\). When the input is much larger than L3 cache
the speedup disappears, because RAM throughput becomes a common bottleneck.</p><h3 id="data-structure-size"><span>4.3.1</span> Data structure size
<a href="#data-structure-size"></a></h3><p>Plain binary search and the Eytzinger layout have pretty much no overhead.
Our S+ tree so far has around \(1/16=6.25\%\) overhead: \(1/17\) of the
values in the final layer is duplicated in the layer above, and \(1/17\) of
<em>those</em> is duplicated again, and so on, for a total of \(1/17 + 1/17^2 + \cdots =
1/16\).</p><p>Using node size \(15\) instead, increases the overhead:
Each node now only stores \(15\) instead of \(16\) elements, so that we already have
an overhead of \(1/15\). Furthermore the reduced branching factor increases the
duplication overhead fro \(1/16\) to \(1/15\) as well, for a total overhead of \(2/15
= 13.3\%\), which matches the dashed blue line in <a href="#figure--b15">Figure 17</a>.</p><h2 id="summary"><span>4.4</span> Summary
<a href="#summary"></a></h2><figure><a href="https://curiouscoding.nl/ox-hugo/11-summary.svg"><img src="https://curiouscoding.nl/ox-hugo/11-summary.svg" alt="Figure 18: A summary of all the improvements we made so far."></a><figcaption><p><span>Figure 18: </span>A summary of all the improvements we made so far.</p></figcaption></figure><p>Of all the improvements so far, only the interleaving is maybe a bit too much:
it is the only method that does not work batch-by-batch, but really benefits
from having the full input at once. And also its code is three times longer
than the plain batched query methods because the first and last few
iterations of each loop are handled separately.</p><h2 id="prefix-partitioning"><span>5</span> Prefix partitioning
<a href="#prefix-partitioning"></a></h2><p>So far, we’ve been doing a purely <em>comparison-based search</em>.
Now, it is time for something new: <em>partitioning</em> the input values.</p><p>The simplest form of the idea is to simply partition values by their top \(b\)
bits, into \(2^b\) parts. Then we can build \(2^b\) independent search trees and
search each query in one of them. If \(b=12\), this saves the first two levels of
the search (or slightly less, actually, since \(2^{12} = 16^3 &lt; 17^3\)).</p><h2 id="full-layout"><span>5.1</span> Full layout
<a href="#full-layout"></a></h2><p>In memory, we can store these trees very similar to the <em>full</em> layout we had
before, with the main differences that the first few layers are skipped and that
now there will be padding at the end of each part, rather than once at the end.</p><figure><a href="https://curiouscoding.nl/ox-hugo/prefix.svg"><img src="https://curiouscoding.nl/ox-hugo/prefix.svg" alt="Figure 19: The full partitioned layout concatenates the full trees for all parts ‘horizontally’. As a new detail, when a part is not full, the smallest value of the next part is appended in the leaf layer."></a><figcaption><p><span>Figure 19: </span>The <em>full</em> partitioned layout concatenates the full trees for all parts ‘horizontally’. As a new detail, when a part is not full, the smallest value of the next part is appended in the leaf layer.</p></figcaption></figure><p>For some choices of \(b\), it could happen that up to \(15/16\) of each tree is
padding. To reduce this overhead, we attempt to shrink \(b\) while keeping the
height of all trees the same: as long as all pairs of adjacent trees would
fit together in the same space, we decrease \(b\) by one. This way, all parts will
be filled for at least \(50\%\) when the elements are evenly distributed.</p><p>Once construction is done, the code for querying is very similar to before: we
only have to start the search for each query at the index of its part, given by
<code>q &gt;&gt; shift</code> for some value of <code>shift</code>, rather than at index \(0\).</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn search_prefix&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let offsets = self
</span></span><span><span>         .offsets
</span></span><span><span>         .iter()
</span></span><span><span>         .map(|o| unsafe { self.tree.as_ptr().add(*o) })
</span></span><span><span>         .collect_vec();
</span></span><span><span>
</span></span><span><span>     // Initial parts, and prefetch them.
</span></span><span><span>     let o0 = offsets[0];
</span></span><span><span><span>-    let mut k = [0; P];
</span></span></span><span><span><span></span><span>+    let mut k = qb.map(|q| {
</span></span></span><span><span><span>+        (q as usize &gt;&gt; self.shift) * 64
</span></span></span><span><span><span>+    });
</span></span></span><span><span><span></span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span>     for [o, o2] in offsets.array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span>             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);
</span></span><span><span>             k[i] = k[i] * (B + 1) + jump_to;
</span></span><span><span>             prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span>         let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);
</span></span><span><span>         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }
</span></span><span><span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 29:</span>
Searching the full layout of the partitioned tree starts in the partition in which each query belongs.</p><figure><a href="https://curiouscoding.nl/ox-hugo/20-prefix.svg"><img src="https://curiouscoding.nl/ox-hugo/20-prefix.svg" alt="Figure 20: The ‘simple’ partitioned tree, for (b_{textrm{max}}in {4,8,12,16,20}), shown as dotted lines."></a><figcaption><p><span>Figure 20: </span>The ‘simple’ partitioned tree, for (b_{textrm{max}}in {4,8,12,16,20}), shown as dotted lines.</p></figcaption></figure><p>We see that indeed, the partitioned tree has a space overhead varying between
\(0\) and \(1\), making this not yet useful in practice.
Larger \(b\) reduce the height of the remaining trees, and indeed we
see that queries are faster for larger \(b\). Especially for small trees there is
a significant speedup over interleaving. Somewhat surprisingly, none of the
partition sizes has faster queries than interleaving for large inputs. Also
important to note is that while partitioning is very fast for sizes up to L1
cache, this is only possible because they have \(\gg 1\) space overhead.</p><h2 id="compact-subtrees"><span>5.2</span> Compact subtrees
<a href="#compact-subtrees"></a></h2><p>Just like we used the <em>packed</em> layout before, we can also do that now, by simply
concatenating the representation of all packed subtrees.
We ensure that all subtrees are still padded into the same total size, but now
we only add as much padding as needed for the largest part, rather than padding
to <em>full</em> trees. Then, we give each tree the same layout in memory.</p><p>We’ll have offsets \(o_\ell\) of where each layer starts in the first tree, and we
store the constant size of the trees. That way, we can easily index each layer
of each part.</p><figure><a href="https://curiouscoding.nl/ox-hugo/prefix-compact.svg"><img src="https://curiouscoding.nl/ox-hugo/prefix-compact.svg" alt="Figure 21: Compared to before, Figure 19, the lowest level of each subtree now only takes 2 instead of 3 nodes."></a><figcaption><p><span>Figure 21: </span>Compared to before, <a href="#figure--prefix">Figure 19</a>, the lowest level of each subtree now only takes 2 instead of 3 nodes.</p></figcaption></figure><p>The code for querying does become slightly more complicated. Now, we must
explicitly track the part that each query belongs to, and compute all indices
based on the layer offset, the in-layer offset <code>k[i]</code>, <em>and</em> the part offset.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn search&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let offsets = self
</span></span><span><span>         .offsets
</span></span><span><span>         .iter()
</span></span><span><span>         .map(|o| unsafe { self.tree.as_ptr().add(*o) })
</span></span><span><span>         .collect_vec();
</span></span><span><span>
</span></span><span><span>     // Initial parts, and prefetch them.
</span></span><span><span>     let o0 = offsets[0];
</span></span><span><span><span>+    let mut k: [usize; P] = [0; P];
</span></span></span><span><span><span>+    let parts: [usize; P] = qb.map(|q| {
</span></span></span><span><span><span>+        // byte offset of the part.
</span></span></span><span><span><span>+        (q as usize &gt;&gt; self.shift) * self.bpp * 64
</span></span></span><span><span><span>+    });
</span></span></span><span><span><span></span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span>     for [o, o2] in offsets.array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span><span>-            let jump_to = unsafe { *o.byte_add(           k[i]) }.find_splat64(q_simd[i]);
</span></span></span><span><span><span></span><span>+            let jump_to = unsafe { *o.byte_add(parts[i] + k[i]) }.find_splat64(q_simd[i]);
</span></span></span><span><span><span></span>             k[i] = k[i] * (B + 1) + jump_to;
</span></span><span><span><span>-            prefetch_ptr(unsafe { o2.byte_add(           k[i]) });
</span></span></span><span><span><span></span><span>+            prefetch_ptr(unsafe { o2.byte_add(parts[i] + k[i]) });
</span></span></span><span><span><span></span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span><span>-        let idx = unsafe { *o.byte_add(           k[i]) }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>+        let idx = unsafe { *o.byte_add(parts[i] + k[i]) }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>-        unsafe { (o.byte_add(           k[i]) as *const u32).add(idx).read() }
</span></span></span><span><span><span></span><span>+        unsafe { (o.byte_add(parts[i] + k[i]) as *const u32).add(idx).read() }
</span></span></span><span><span><span></span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 30:</span>
The indexing for the packed subtrees requires explicitly tracking the part of each query. This slows things down a bit.</p><figure><a href="https://curiouscoding.nl/ox-hugo/21-compact.svg"><img src="https://curiouscoding.nl/ox-hugo/21-compact.svg" alt="Figure 22: Compared to the the simple/full layout before (dark blue dots for (b=16)), the compact layout (e.g. red dots for (b=16)) consistently uses less memory, but is slightly slower."></a><figcaption><p><span>Figure 22: </span>Compared to the the simple/full layout before (dark blue dots for (b=16)), the compact layout (e.g. red dots for (b=16)) consistently uses less memory, but is slightly slower.</p></figcaption></figure><p>For fixed \(b_{\textrm{max}}\), memory overhead of the compact layout is small as
long as the input is sufficiently large and the trees have sufficiently many
layers. Thus, this tree could be practical.
Unfortunately though, querying them is slightly slower than before,
because we must explicitly track the part of each query.</p><h2 id="the-best-of-both-compact-first-level"><span>5.3</span> The best of both: compact first level
<a href="#the-best-of-both-compact-first-level"></a></h2><p>As we just saw, storing the trees one by one slows queries down, so we would
like to avoid that. But on the other hand, the full layout can waste space.</p><p>Here, we combine the two ideas. We would like to store the <em>horizontal</em>
concatenation of the packed trees (each packed to the same size), but this is
complicated, because then levels would have a non-constant branching factor.
Instead, we can fully omit the last few (level 2) subtrees from each
tree, and pad those subtrees that <em>are</em> present to full subtrees.
This way, only the first level has a configurable branching factor \(B_1\), which we can
simply store after construction is done.</p><p>This layout takes slightly more space than before because the subtrees must
be full, but the overhead should typically be on the order of \(1/16\),
since (for uniform data) each tree will have \(\geq 9\) subtrees, of which only
the last is not full.</p><figure><a href="https://curiouscoding.nl/ox-hugo/prefix-l1.svg"><img src="https://curiouscoding.nl/ox-hugo/prefix-l1.svg" alt="Figure 23: We can also store the horizontal concatenation of all trees. Here, the number of subtrees can be fixed to be less than (B+1), and is (2) instead of (B+1=3). Although not shown, deeper layers must always be full and have a (B+1) branching factor."></a><figcaption><p><span>Figure 23: </span>We can also store the horizontal concatenation of all trees. Here, the number of subtrees can be fixed to be less than (B+1), and is (2) instead of (B+1=3). Although not shown, deeper layers must always be full and have a (B+1) branching factor.</p></figcaption></figure><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span><span>35
</span><span>36
</span><span>37
</span><span>38
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn search_b1&lt;const P: usize&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let offsets = self
</span></span><span><span>         .offsets
</span></span><span><span>         .iter()
</span></span><span><span>         .map(|o| unsafe { self.tree.as_ptr().add(*o) })
</span></span><span><span>         .collect_vec();
</span></span><span><span>
</span></span><span><span>     let o0 = offsets[0];
</span></span><span><span>     let mut k: [usize; P] = qb.map(|q| {
</span></span><span><span>          (q as usize &gt;&gt; self.shift) * 64
</span></span><span><span>     });
</span></span><span><span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span><span>-    for         [o, o2]  in offsets.array_windows()        {
</span></span></span><span><span><span></span><span>+    if let Some([o1, o2]) = offsets.array_windows().next() {
</span></span></span><span><span><span></span>         for i in 0..P {
</span></span><span><span>             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);
</span></span><span><span><span>-            k[i] = k[i] * (B + 1) + jump_to;
</span></span></span><span><span><span></span><span>+            k[i] = k[i] * self.b1 + jump_to;
</span></span></span><span><span><span></span>             prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span><span>-    for [o, o2] in offsets     .array_windows() {
</span></span></span><span><span><span></span><span>+    for [o, o2] in offsets[1..].array_windows() {
</span></span></span><span><span><span></span>         for i in 0..P {
</span></span><span><span>             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);
</span></span><span><span>             k[i] = k[i] * (B + 1) + jump_to;
</span></span><span><span>             prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span>         let idx = unsafe { *o.byte_add(k[i]) }.find_splat(q_simd[i]);
</span></span><span><span>         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }
</span></span><span><span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 31:</span>
Now, the code is simple again, in that we don't need to explicitly track part indices. All that changes is that we handle the first iteration of the for loop separately, and use branching factor <code>self.b1</code> instead of <code>B+1</code> there.</p><figure><a href="https://curiouscoding.nl/ox-hugo/22-l1.svg"><img src="https://curiouscoding.nl/ox-hugo/22-l1.svg" alt="Figure 24: When compressing the first level, space usage is very similar to the compact layout before, and query speed is as fast as the full layout before."></a><figcaption><p><span>Figure 24: </span>When compressing the first level, space usage is very similar to the compact layout before, and query speed is as fast as the full layout before.</p></figcaption></figure><h2 id="overlapping-trees"><span>5.4</span> Overlapping trees
<a href="#overlapping-trees"></a></h2><p>A drawback of all the above methods is that memory usage is heavily influenced by the
largest part, since all parts must be at least as large. This is especially a
problem when the distribution of part sizes is very skewed.
We can avoid this by sharing storage between adjacent trees.
Let \(S_p\) be the number of subtrees for each part \(p\), and \(S_{max} = \max_p S_p\).
Then, we can define the <em>overlap</em> \(0\leq v\leq B\), and append only
\(B_1 = S_{max}-v\) new subtrees for each new part, rather than \(S_{max}\) as we
did before.
The values for each part are then simply appended where the previous part left
off, unless that subtree is ‘out-of-reach’ for the current part, in which
case first some padding is added.
This way, consecutive
parts can overlap and exchange memory, and we can somewhat ‘buffer’ the effect
of large parts.</p><figure><a href="https://curiouscoding.nl/ox-hugo/prefix-overlapping.svg"><img src="https://curiouscoding.nl/ox-hugo/prefix-overlapping.svg" alt="Figure 25: In this example, the third tree has (6) values in ([8, 12)) and requires (S_{max}=3) subtrees. We have an overlap of (v=1), so that for each additional tree, only (2) subtrees are added. We add padding elements in grey to ensure all elements are reachable from their own tree."></a><figcaption><p><span>Figure 25: </span>In this example, the third tree has (6) values in ([8, 12)) and requires (S_{max}=3) subtrees. We have an overlap of (v=1), so that for each additional tree, only (2) subtrees are added. We add padding elements in grey to ensure all elements are reachable from their own tree.</p></figcaption></figure><p>When the overlap is \(1\), as in the example above, the nodes in the first layer
each contain the maximum value of \(B\) subtrees. When the overlap is larger than
\(1\), the nodes in the first layer would contain overlapping values. Instead, we
store a single list of values, in which we can do <em>unaligned</em> reads to get the
right slice of \(B\) values that we need.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span><span>35
</span><span>36
</span><span>37
</span><span>38
</span><span>39
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn search&lt;const P: usize, const PF: bool&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let offsets = self
</span></span><span><span>         .offsets
</span></span><span><span>         .iter()
</span></span><span><span>         .map(|o| unsafe { self.tree.as_ptr().add(*o) })
</span></span><span><span>         .collect_vec();
</span></span><span><span>
</span></span><span><span>     let o0 = offsets[0];
</span></span><span><span>     let mut k: [usize; P] = qb.map(|q| {
</span></span><span><span><span>-        (q as usize &gt;&gt; self.shift) * 4 *  16
</span></span></span><span><span><span></span><span>+        (q as usize &gt;&gt; self.shift) * 4 * (16 - self.overlap)
</span></span></span><span><span><span></span>     });
</span></span><span><span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span>     if let Some([o1, o2]) = offsets.array_windows().next() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span><span>+            // First level read may be unaligned.
</span></span></span><span><span><span></span><span>-            let jump_to = unsafe { *o.byte_add(k[i])                  }.find_splat64(q_simd[i]);
</span></span></span><span><span><span></span><span>+            let jump_to = unsafe {  o.byte_add(k[i]).read_unaligned() }.find_splat64(q_simd[i]);
</span></span></span><span><span><span></span>             k[i] = k[i] * self.l1 + jump_to;
</span></span><span><span>             prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     for [o, o2] in offsets[1..].array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span>             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);
</span></span><span><span>             k[i] = k[i] * (B + 1) + jump_to;
</span></span><span><span>             prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span><span>-        let idx = unsafe { *o.byte_add(k[i])                  }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span><span>+        let idx = unsafe {  o.byte_add(k[i]).read_unaligned() }.find_splat(q_simd[i]);
</span></span></span><span><span><span></span>         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }
</span></span><span><span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 32:</span>
Each part now contains \(16-v\) values, instead of the original 16. We use <code>read_unaligned</code> since we do not always read at 16-value boundaries anymore.</p><figure><a href="https://curiouscoding.nl/ox-hugo/23-overlap.svg"><img src="https://curiouscoding.nl/ox-hugo/23-overlap.svg" alt="Figure 26: Overlapping trees usually use less memory than the equivalent version with first-level compression, while being about as fast."></a><figcaption><p><span>Figure 26: </span>Overlapping trees usually use less memory than the equivalent version with first-level compression, while being about as fast.</p></figcaption></figure><h2 id="human-data"><span>5.5</span> Human data
<a href="#human-data"></a></h2><p>So far we’ve been testing with uniform random data, where the largest part
deviates form the mean size by around \(\sqrt n\). Now, let’s look at some real
data: k-mers of a human genome. DNA consists of <code>ACGT</code> characters that can be
encoded as 2 bits, so each string of \(k=16\) characters defines a 32 bit
integer<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>.
We then look at the first \(n\) k-mers of the human genome, starting at chromosome 1.</p><p>To give an idea, the plot below show for each k-mer of length \(k=12\) how often
it occurs in the full human genome. In total, there are around 3G
k-mers, and so the expected count for each k-mer is around 200. But instead,
we see k-mers that occur over 2 million times! So if we were to partition on the
first 24 bits, the size of the largest part is only around \(2^{-10}\) of the input,
rather than \(2^{-24}\).</p><p>The accumulated counts are shown in orange, where we also see a number of flat
regions caused by underrepresented k-mers.</p><figure><a href="https://curiouscoding.nl/ox-hugo/rank-curve.png"><img src="https://curiouscoding.nl/ox-hugo/rank-curve.png" alt="Figure 27: A plot showing k-mer counts for all (4^{12} = 16M) $k=12$-mers of the human genome. On random data each k-mer would occur around 200 times, but here we see some k-mers occurring over 2 million times."></a><figcaption><p><span>Figure 27: </span>A plot showing k-mer counts for all (4^{12} = 16M) $k=12$-mers of the human genome. On random data each k-mer would occur around 200 times, but here we see some k-mers occurring over 2 million times.</p></figcaption></figure><figure><a href="https://curiouscoding.nl/ox-hugo/23-overlap-human.svg"><img src="https://curiouscoding.nl/ox-hugo/23-overlap-human.svg" alt="Figure 28: Building the overlapping trees for k-mers of the human genome takes much more space, and even using only 16 parts regularly requires up to 50% overhead, making this data structure not quite practical."></a><figcaption><p><span>Figure 28: </span>Building the overlapping trees for k-mers of the human genome takes much more space, and even using only 16 parts regularly requires up to 50% overhead, making this data structure not quite practical.</p></figcaption></figure><h2 id="prefix-map"><span>5.6</span> Prefix map
<a href="#prefix-map"></a></h2><p>We need a way to handle unbalanced partition sizes, instead of mapping
everything linearly.
We can do this by simply storing the full tree compactly as we did before,
preceded by an array (in blue below) that points to the index of the first
subtree containing elements of the part. Like for the overlapping trees before,
the first layer is simply a list of the largest elements of all subtrees that
can be indexed anywhere (potentially unaligned).</p><figure><a href="https://curiouscoding.nl/ox-hugo/prefix-map.svg"><img src="https://curiouscoding.nl/ox-hugo/prefix-map.svg" alt="Figure 29: The prefix map, in blue, stores (2^b) elements, that for each $b$-bit prefix stores the index of the first subtree that contains an element of that prefix."></a><figcaption><p><span>Figure 29: </span>The prefix map, in blue, stores (2^b) elements, that for each $b$-bit prefix stores the index of the first subtree that contains an element of that prefix.</p></figcaption></figure><p>To answer a query, we first find its part, then read the block (16 elements)
starting at the pointed-to element, and then proceed as usual from the sub-tree onward.</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span><span>29
</span><span>30
</span><span>31
</span><span>32
</span><span>33
</span><span>34
</span><span>35
</span><span>36
</span></code></pre></td><td><pre tabindex="0"><code data-lang="diff"><span><span> pub fn search&lt;const P: usize, const PF: bool&gt;(&amp;self, qb: &amp;[u32; P]) -&gt; [u32; P] {
</span></span><span><span>     let offsets = self
</span></span><span><span>         .offsets
</span></span><span><span>         .iter()
</span></span><span><span>         .map(|o| unsafe { self.tree.as_ptr().add(*o) })
</span></span><span><span>         .collect_vec();
</span></span><span><span>
</span></span><span><span>     let o0 = offsets[0];
</span></span><span><span>     let mut k: [usize; P] = qb.map(|q| {
</span></span><span><span><span>-                 4 * (16 - self.overlap)         * (q as usize &gt;&gt; self.shift)
</span></span></span><span><span><span></span><span>+        unsafe { 4 * *self.prefix_map.get_unchecked(q as usize &gt;&gt; self.shift) }
</span></span></span><span><span><span></span>     });
</span></span><span><span>     let q_simd = qb.map(|q| Simd::&lt;u32, 8&gt;::splat(q));
</span></span><span><span>
</span></span><span><span>     if let Some([o1, o2]) = offsets.array_windows().next() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span>             let jump_to = unsafe {  o.byte_add(k[i]).read_unaligned() }.find_splat64(q_simd[i]);
</span></span><span><span>             k[i] = k[i] * self.l1 + jump_to;
</span></span><span><span>             prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     for [o, o2] in offsets[1..].array_windows() {
</span></span><span><span>         for i in 0..P {
</span></span><span><span>             let jump_to = unsafe { *o.byte_add(k[i]) }.find_splat64(q_simd[i]);
</span></span><span><span>             k[i] = k[i] * (B + 1) + jump_to;
</span></span><span><span>             prefetch_ptr(unsafe { o2.byte_add(k[i]) });
</span></span><span><span>         }
</span></span><span><span>     }
</span></span><span><span>
</span></span><span><span>     let o = offsets.last().unwrap();
</span></span><span><span>     from_fn(|i| {
</span></span><span><span>         let idx = unsafe {  o.byte_add(k[i]).read_unaligned() }.find_splat(q_simd[i]);
</span></span><span><span>         unsafe { (o.byte_add(k[i]) as *const u32).add(idx).read() }
</span></span><span><span>     })
</span></span><span><span> }
</span></span></code></pre></td></tr></tbody></table></div><p><span>Code Snippet 33:</span>
In code, the only thing that changes compared to the previous overlapping version is that instead of computing the start index linearly (and adapting the element layout accordingly), we use the <code>prefix_map</code> to jump directly to the right place in the packed tree representation.</p><figure><a href="https://curiouscoding.nl/ox-hugo/24-map.svg"><img src="https://curiouscoding.nl/ox-hugo/24-map.svg" alt="Figure 30: As long as there are more elements than parts and the tree has at least two layers, the space overhead of this representation is close to (1/16) again."></a><figcaption><p><span>Figure 30: </span>As long as there are more elements than parts and the tree has at least two layers, the space overhead of this representation is close to (1/16) again.</p></figcaption></figure><p>Although memory usage is now similar to the unpartitioned version, queries for
large inputs are slightly slower than those previous layouts due to the
additional index required.</p><p>We can also again do the interleaving queries. These are slightly faster for
small inputs, and around as fast as interleaving was without the partitioning.</p><figure><a href="https://curiouscoding.nl/ox-hugo/25-map-interleave.svg"><img src="https://curiouscoding.nl/ox-hugo/25-map-interleave.svg" alt="Figure 31: Prefix-map index with interleaving queries on random data."></a><figcaption><p><span>Figure 31: </span>Prefix-map index with interleaving queries on random data.</p></figcaption></figure><p>On human data, we see that the partitioned index is a bit faster in L1 and L2,
and consistently saves the time of roughly one layer in L3. For larger indices,
performance is still very similar to not using partitioning at all.</p><figure><a href="https://curiouscoding.nl/ox-hugo/25-map-interleave-human.svg"><img src="https://curiouscoding.nl/ox-hugo/25-map-interleave-human.svg" alt="Figure 32: Prefix-map with interleaving on human data."></a><figcaption><p><span>Figure 32: </span>Prefix-map with interleaving on human data.</p></figcaption></figure><h2 id="prefix-summary"><span>5.7</span> Summary
<a href="#prefix-summary"></a></h2><figure><a href="https://curiouscoding.nl/ox-hugo/27-summary.svg"><img src="https://curiouscoding.nl/ox-hugo/27-summary.svg" alt="Figure 33: Summary of partitioning results. Overall, it seems that partitioning does not provide when we already interleave queries."></a><figcaption><p><span>Figure 33: </span>Summary of partitioning results. Overall, it seems that partitioning does not provide when we already interleave queries.</p></figcaption></figure><h2 id="multi-threaded-comparison"><span>6</span> Multi-threaded comparison
<a href="#multi-threaded-comparison"></a></h2><figure><a href="https://curiouscoding.nl/ox-hugo/28-threads.svg"><img src="https://curiouscoding.nl/ox-hugo/28-threads.svg" alt="Figure 34: When using 6 threads, runtime goes down from 27ns to 7ns. Given that the speedup is less than 4x, we are now bottlenecked by total RAM throughput, and indeed methods that are slower for a single thread also reach near-optimal throughput now."></a><figcaption><p><span>Figure 34: </span>When using 6 threads, runtime goes down from <code>27ns</code> to <code>7ns</code>. Given that the speedup is less than 4x, we are now bottlenecked by total RAM throughput, and indeed methods that are slower for a single thread also reach near-optimal throughput now.</p></figcaption></figure><h2 id="conclusion"><span>7</span> Conclusion
<a href="#conclusion"></a></h2><p>All together, we went from <code>1150ns/query</code> for binary search on 4GB input to
<code>27ns</code> for the optimized S-tree with interleaved queries, over <code>40x</code> speedup!
A large part of this improvement is due to <strong>batching</strong> queries and <strong>prefetching</strong>
upcoming nodes. To get even higher throughput, <strong>interleaving</strong> queries at different
levels helps to balance the CPU-bound part of the computation with the
memory-bound part, so that we get a higher overall throughput. Using a <strong>15
elements per node</strong> instead of 16 also improves throughput somewhat, but doubles
the overhead of the data structure from 6.25% to 13.3%. For inputs that fit in
L3 cache that’s fine and the speedup is worthwhile, while for larger inputs the
speed is memory-bound anyway, so that there is no speedup while the additional
memory requirements are somewhat large.</p><p>We also looked into <strong>partitioning</strong> the data by prefix. While this does give some speedup,
it turns out that on skewed input data, the benefits quickly
diminish since the tree either requires a lot of buffer space, or else requires
an additional lookup to map each part to its location in the first level of the tree.
In the end, I’d say the additional complexity and dependency on the shape of
the input data of partitioning is not worth the speedup compared to simply using interleaved
queries directly.</p><h2 id="future-work"><span>7.1</span> Future work
<a href="#future-work"></a></h2><h3 id="branchy-search"><span>7.1.1</span> Branchy search
<a href="#branchy-search"></a></h3><p>All methods we considered are <em>branchless</em> and use the exact same number of
iterations for each query. Especially in combination with partitioning, it may
be possible to handle the few large parts independently from the usual
smaller parts. That way we could answer most queries with slightly fewer
iterations.</p><p>On the other hand, the layers saved would mostly be the quick lookups near the
root of the tree, and introducing branches to the code could possibly cause
quite a bit of delay due to mispredictions.</p><h3 id="interpolation-search"><span>7.1.2</span> Interpolation search
<a href="#interpolation-search"></a></h3><p>As we saw in the last plot above, total RAM throughput (rather than per-core
throughput) becomes a bottleneck once we’re using multiple threads.
Thus, the only way to improve total query throughput is to use strictly fewer RAM
accesses per query.
Prefix lookups won’t help, since they only replace the layers of the tree
that would otherwise fit in the cache. Instead, we could use <em>interpolation
search</em> (<a href="https://en.wikipedia.org/wiki/Interpolation_search">wikipedia</a>), where the estimated position of a query \(q\) is linearly
interpolated between known positions of surrounding elements. On random data, this only takes
\(O(\lg \lg n)\) iterations, rather than \(O(\lg n)\) for binary search, and could
save some RAM accesses. On the
other hand, when data is not random its worst case performance is \(O(n)\) rather
than the statically bounded \(O(\lg n)\).</p><p>The PLA-index (<a href="#citeproc_bib_item_1">Abrar and Medvedev 2024</a>) also uses a single interpolation step in a
precisely constructed piece wise linear approximation. The error after the
approximation is determined by some global upper bound, so that the number of remaining
search steps can be bounded as well.</p><h3 id="packing-data-smaller"><span>7.1.3</span> Packing data smaller
<a href="#packing-data-smaller"></a></h3><p>Another option to use the RAM lookups more efficiently would be to pack values
into 16 bits rather than the 32 bits we’ve been using so far. Especially if we
first do a 16 bit prefix lookup, we already know those bits anyway, so it would
suffice to only compare the last 16 bits of the query and values. This increases
the branching factor from 17 to 33, which reduces the number of layers of the
tree by around 1.5 for inputs of 1GB.</p><h3 id="returning-indices-in-original-data"><span>7.1.4</span> Returning indices in original data
<a href="#returning-indices-in-original-data"></a></h3><p>For various applications, it may be helpful to not only return the smallest
value \(\geq q\), but also the index in the original list of sorted values, for
example when storing an array with additional data for each item.</p><p>Since we use the S+ tree that stores all data in the bottom layer, this is
mostly straightforward. The <em>prefix map</em> partitioned tree also natively supports
this, while the other partitioned variants do not: they include buffer/padding
elements in their bottom layer, and hence we would need to store and look up the position
offset of each part separately.</p><h3 id="range-queries"><span>7.1.5</span> Range queries
<a href="#range-queries"></a></h3><p>We could extend the current query methods to a version that return both the
first value \(\geq q\) and the first value \(&gt;q\), so that the range of positions
corresponding to value \(q\) can be determined. In practice, the easiest way to do
this is by simply doubling the queries into \(q\) and \(q+1\). This will cause some
CPU overhead in the initial layers, but the query execution will remain
branch-free. When \(q\) is not found or only occurs a few times, they will mostly
fetch the same cache lines, so that memory is efficiently reused and the
bandwidth can be used for other queries.</p><p>In practice though, this seems only around 20% faster per individual query for 4GB input, so
around 60% slower for a range than for a single query. For small inputs, the
speedup is less, and sometimes querying ranges is even more than twice slower
than individual random queries.</p><h2 id="references">References
<a href="#references"></a></h2><div><p><a id="citeproc_bib_item_1"></a>Abrar, Md. Hasin, and Paul Medvedev. 2024. “Pla-Index: A K-Mer Index Exploiting Rank Curve Linearity.” Schloss Dagstuhl – Leibniz-Zentrum für Informatik. <a href="https://doi.org/10.4230/LIPICS.WABI.2024.13">https://doi.org/10.4230/LIPICS.WABI.2024.13</a>.</p><p><a id="citeproc_bib_item_2"></a>Khuong, Paul-Virak, and Pat Morin. 2017. “Array Layouts for Comparison-Based Searching.” <i>Acm Journal of Experimental Algorithmics</i> 22 (May): 1–39. <a href="https://doi.org/10.1145/3053370">https://doi.org/10.1145/3053370</a>.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Arnis: Generate cities in Minecraft from OpenStreetMap (233 pts)]]></title>
            <link>https://github.com/louis-e/arnis</link>
            <guid>42561711</guid>
            <pubDate>Tue, 31 Dec 2024 20:47:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/louis-e/arnis">https://github.com/louis-e/arnis</a>, See on <a href="https://news.ycombinator.com/item?id=42561711">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/louis-e/arnis/blob/main/gitassets/logo.png?raw=true"><img width="456" height="125" src="https://github.com/louis-e/arnis/raw/main/gitassets/logo.png?raw=true"></a>
</p>

<p dir="auto">This open source project written in Rust generates any chosen location from the real world in Minecraft Java Edition with a high level of detail.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🖥️ Example</h2><a id="user-content-desktop_computer-example" aria-label="Permalink: :desktop_computer: Example" href="#desktop_computer-example"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/louis-e/arnis/blob/main/gitassets/mc.gif?raw=true"><img src="https://github.com/louis-e/arnis/raw/main/gitassets/mc.gif?raw=true" alt="Minecraft Preview" data-animated-image=""></a></p>
<p dir="auto">By leveraging geospatial data from OpenStreetMap and utilizing the powerful capabilities of Rust, Arnis provides an efficient and robust solution for creating complex and accurate Minecraft worlds that reflect real-world geography and architecture.</p>
<p dir="auto">Arnis is designed to handle large-scale data and generate rich, immersive environments that bring real-world cities, landmarks, and natural features into the Minecraft universe. Whether you're looking to replicate your hometown, explore urban environments, or simply build something unique and realistic, Arnis offers a comprehensive toolset to achieve your vision.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">⌨️ Usage</h2><a id="user-content-keyboard-usage" aria-label="Permalink: :keyboard: Usage" href="#keyboard-usage"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/louis-e/arnis/blob/main/gitassets/gui.png?raw=true"><img width="60%" src="https://github.com/louis-e/arnis/raw/main/gitassets/gui.png?raw=true"></a><br>
Download the <a href="https://github.com/louis-e/arnis/releases/">latest release</a> or <a href="#trophy-open-source">compile</a> the project on your own.</p>
<p dir="auto">Choose your area in Arnis using the rectangle tool and select your Minecraft world - then simply click on 'Start Generation'!
The world will always be generated starting from the coordinates 0 0 0.</p>
<p dir="auto">If you choose to select an own world, make sure to generate a new flat world in advance in Minecraft.</p>
<details>
<summary>Alternatively you can also run Arnis the old fashioned way in the command line.</summary>
<p dir="auto"><code>arnis.exe --path="C:/YOUR_PATH/.minecraft/saves/worldname" --bbox="min_lng,min_lat,max_lng,max_lat"</code></p>
<p dir="auto">The --bbox parameter specifies the bounding box coordinates in the format: min_lng,min_lat,max_lng,max_lat. Use --path to specify the location of the Minecraft world.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/louis-e/arnis/blob/main/gitassets/bbox-finder.png?raw=true"><img width="60%" src="https://github.com/louis-e/arnis/raw/main/gitassets/bbox-finder.png?raw=true"></a><br>
Use <a href="http://bboxfinder.com/" rel="nofollow">http://bboxfinder.com/</a> to draw a rectangle of your wanted area. Then copy the four box coordinates as shown below and use them as the input for the --bbox parameter. Try starting with a small area since large areas take a lot of computing power and time to process.<br></p>
<p dir="auto"><i>Note: This might not be working right now since the console gets suppressed. <a data-error-text="Failed to load title" data-id="2762422686" data-permission-text="Title is private" data-url="https://github.com/louis-e/arnis/issues/99" data-hovercard-type="issue" data-hovercard-url="/louis-e/arnis/issues/99/hovercard" href="https://github.com/louis-e/arnis/issues/99">#99</a></i></p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">💾 How it works</h2><a id="user-content-floppy_disk-how-it-works" aria-label="Permalink: :floppy_disk: How it works" href="#floppy_disk-how-it-works"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/louis-e/arnis/blob/main/gitassets/cli.gif?raw=true"><img src="https://github.com/louis-e/arnis/raw/main/gitassets/cli.gif?raw=true" alt="CLI Generation" data-animated-image=""></a></p>
<p dir="auto">The raw data obtained from the API <em><a href="#question-faq">(see FAQ)</a></em> includes each element (buildings, walls, fountains, farmlands, etc.) with its respective corner coordinates (nodes) and descriptive tags. When you run Arnis, the following steps are performed automatically to generate a Minecraft world:</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Processing Pipeline</h4><a id="user-content-processing-pipeline" aria-label="Permalink: Processing Pipeline" href="#processing-pipeline"></a></p>
<ol dir="auto">
<li><strong>Fetching Data from the Overpass API:</strong> The script retrieves geospatial data for the desired bounding box from the Overpass API.</li>
<li><strong>Parsing Raw Data:</strong> The raw data is parsed to extract essential information like nodes, ways, and relations. Nodes are converted into Minecraft coordinates, and relations are handled similarly to ways, ensuring all relevant elements are processed correctly. Relations and ways cluster several nodes into one specific object.</li>
<li><strong>Prioritizing and Sorting Elements:</strong> The elements (nodes, ways, relations) are sorted by priority to establish a layering system, which ensures that certain types of elements (e.g., entrances and buildings) are generated in the correct order to avoid conflicts and overlapping structures.</li>
<li><strong>Generating Minecraft World:</strong> The Minecraft world is generated using a series of element processors (generate_buildings, generate_highways, generate_landuse, etc.) that interpret the tags and nodes of each element to place the appropriate blocks in the Minecraft world. These processors handle the logic for creating 3D structures, roads, natural formations, and more, as specified by the processed data.</li>
<li><strong>Generating Ground Layer:</strong> A ground layer is generated based on the provided scale factors to provide a base for the entire Minecraft world. This step ensures all areas have an appropriate foundation (e.g., grass and dirt layers).</li>
<li><strong>Saving the Minecraft World:</strong> All the modified chunks are saved back to the Minecraft region files.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">❓ FAQ</h2><a id="user-content-question-faq" aria-label="Permalink: :question: FAQ" href="#question-faq"></a></p>
<ul dir="auto">
<li><em>Wasn't this written in Python before?</em><br>
Yes! Arnis was initially developed in Python, which benefited from Python's open-source friendliness and ease of readability. This is why we strive for clear, well-documented code in the Rust port of this project to find the right balance. I decided to port the project to Rust to learn more about the language and push the algorithm's performance further. We were nearing the limits of optimization in Python, and Rust's capabilities allow for even better performance and efficiency. The old Python implementation is still available in the python-legacy branch.</li>
<li><em>Where does the data come from?</em><br>
The geographic data is sourced from OpenStreetMap (OSM)<sup><a href="#user-content-fn-1-76869c95acfdd418de6bcf280d4bde15" id="user-content-fnref-1-76869c95acfdd418de6bcf280d4bde15" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>, a free, collaborative mapping project that serves as an open-source alternative to commercial mapping services. The data is accessed via the Overpass API, which queries OSM's database.</li>
<li><em>How does the Minecraft world generation work?</em><br>
The script uses the <a href="https://github.com/owengage/fastnbt">fastnbt</a> cargo package to interact with Minecraft's world format. This library allows Arnis to manipulate Minecraft region files, enabling the generation of real-world locations.</li>
<li><em>Where does the name come from?</em><br>
The project is named after the smallest city in Germany, Arnis<sup><a href="#user-content-fn-2-76869c95acfdd418de6bcf280d4bde15" id="user-content-fnref-2-76869c95acfdd418de6bcf280d4bde15" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>. The city's small size made it an ideal test case for developing and debugging the algorithm efficiently.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">📝 ToDo and Known Bugs</h2><a id="user-content-memo-todo-and-known-bugs" aria-label="Permalink: :memo: ToDo and Known Bugs" href="#memo-todo-and-known-bugs"></a></p>
<p dir="auto">Feel free to choose an item from the To-Do or Known Bugs list, or bring your own idea to the table. Bug reports shall be raised as a Github issue. Contributions are highly welcome and appreciated!</p>
<ul>
<li> Mapping real coordinates to Minecraft coordinates (<a data-error-text="Failed to load title" data-id="2276630492" data-permission-text="Title is private" data-url="https://github.com/louis-e/arnis/issues/29" data-hovercard-type="issue" data-hovercard-url="/louis-e/arnis/issues/29/hovercard" href="https://github.com/louis-e/arnis/issues/29">#29</a>)</li>
<li> Rotate maps (<a data-error-text="Failed to load title" data-id="2762253710" data-permission-text="Title is private" data-url="https://github.com/louis-e/arnis/issues/97" data-hovercard-type="issue" data-hovercard-url="/louis-e/arnis/issues/97/hovercard" href="https://github.com/louis-e/arnis/issues/97">#97</a>)</li>
<li> Evaluate and implement elevation (<a data-error-text="Failed to load title" data-id="2540480972" data-permission-text="Title is private" data-url="https://github.com/louis-e/arnis/issues/66" data-hovercard-type="issue" data-hovercard-url="/louis-e/arnis/issues/66/hovercard" href="https://github.com/louis-e/arnis/issues/66">#66</a>)</li>
<li> Fix Github Action Workflow for releasing Linux &amp; MacOS Binary</li>
<li> Evaluate and implement faster region saving</li>
<li> Automatic new world creation instead of using an existing world</li>
<li> Implement house roof types</li>
<li> Refactor bridges implementation</li>
<li> Refactor railway implementation</li>
<li> Better code documentation</li>
<li> Refactor fountain structure implementation</li>
<li> Add interior to buildings</li>
<li> Memory optimization</li>
<li> Design and implement a GUI</li>
<li> Fix faulty empty chunks (<a data-error-text="Failed to load title" data-id="2490089253" data-permission-text="Title is private" data-url="https://github.com/owengage/fastnbt/issues/120" data-hovercard-type="issue" data-hovercard-url="/owengage/fastnbt/issues/120/hovercard" href="https://github.com/owengage/fastnbt/issues/120">owengage/fastnbt#120</a>) (workaround found)</li>
<li> Setup fork of <a href="https://github.com/aaronr/bboxfinder.com">https://github.com/aaronr/bboxfinder.com</a> for easy bbox picking</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🏆 Open Source</h2><a id="user-content-trophy-open-source" aria-label="Permalink: :trophy: Open Source" href="#trophy-open-source"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Key objectives of this project</h4><a id="user-content-key-objectives-of-this-project" aria-label="Permalink: Key objectives of this project" href="#key-objectives-of-this-project"></a></p>
<ul dir="auto">
<li><strong>Modularity</strong>: Ensure that all components (e.g., data fetching, processing, and world generation) are cleanly separated into distinct modules for better maintainability and scalability.</li>
<li><strong>Performance Optimization</strong>: Utilize Rust’s memory safety and concurrency features to optimize the performance of the world generation process.</li>
<li><strong>Comprehensive Documentation</strong>: Detailed in-code documentation for a clear structure and logic.</li>
<li><strong>User-Friendly Experience</strong>: Focus on making the project easy to use for end users.</li>
<li><strong>Cross-Platform Support</strong>: Ensure the project runs smoothly on Windows, macOS, and Linux.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">How to contribute</h4><a id="user-content-how-to-contribute" aria-label="Permalink: How to contribute" href="#how-to-contribute"></a></p>
<p dir="auto">This project is open source and welcomes contributions from everyone! Whether you're interested in fixing bugs, improving performance, adding new features, or enhancing documentation, your input is valuable. Simply fork the repository, make your changes, and submit a pull request. We encourage discussions and suggestions to ensure the project remains modular, optimized, and easy to use for the community. You can use the parameter --debug to get a more detailed output of the processed values, which can be helpful for debugging and development. Contributions of all levels are appreciated, and your efforts help improve this tool for everyone.</p>
<p dir="auto">Build and run it using: <code>cargo run --release -- --path="C:/YOUR_PATH/.minecraft/saves/worldname" --bbox="min_lng,min_lat,max_lng,max_lat"</code><br>
For the GUI: <code>cargo run --release</code><br></p>
<p dir="auto">After your pull request was merged, I will take care of regularly creating update releases which will include your changes.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Contributors:</h4><a id="user-content-contributors" aria-label="Permalink: Contributors:" href="#contributors"></a></p>
<p dir="auto">This section is dedicated to recognizing and celebrating the outstanding contributions of individuals who have significantly enhanced this project. Your work and dedication are deeply appreciated!</p>
<ul dir="auto">
<li>louis-e</li>
<li>scd31</li>
<li>vfosnar</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">⭐ Star History</h2><a id="user-content-star-star-history" aria-label="Permalink: :star: Star History" href="#star-star-history"></a></p>
<a href="https://star-history.com/#louis-e/arnis&amp;Date" rel="nofollow">
 <themed-picture data-catalyst-inline="true"><picture>
   <source media="(prefers-color-scheme: dark)" srcset="https://camo.githubusercontent.com/c2318d7ea305c26e852cd7bf91f6f7e5961f72938768d4eb4cbe09a06accc884/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6c6f7569732d652f61726e69732644617465267468656d653d6461726b" data-canonical-src="https://api.star-history.com/svg?repos=louis-e/arnis&amp;Date&amp;theme=dark">
   <source media="(prefers-color-scheme: light)" srcset="https://camo.githubusercontent.com/b3b679b7cd0e757d805e3410571db4e0622b03bbb48fffe49143a7edb23cc1fe/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6c6f7569732d652f61726e6973264461746526747970653d44617465" data-canonical-src="https://api.star-history.com/svg?repos=louis-e/arnis&amp;Date&amp;type=Date">
   <img alt="Star History Chart" src="https://camo.githubusercontent.com/b3b679b7cd0e757d805e3410571db4e0622b03bbb48fffe49143a7edb23cc1fe/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6c6f7569732d652f61726e6973264461746526747970653d44617465" data-canonical-src="https://api.star-history.com/svg?repos=louis-e/arnis&amp;Date&amp;type=Date">
 </picture></themed-picture>
</a>
<p dir="auto"><h2 tabindex="-1" dir="auto">©️ License Information</h2><a id="user-content-copyright-license-information" aria-label="Permalink: :copyright: License Information" href="#copyright-license-information"></a></p>
<p dir="auto">This project is licensed under the GNU General Public License v3.0 (GPL-3.0).<sup><a href="#user-content-fn-3-76869c95acfdd418de6bcf280d4bde15" id="user-content-fnref-3-76869c95acfdd418de6bcf280d4bde15" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup></p>
<p dir="auto">Copyright (c) 2022-2025 Louis Erbkamm (louis-e)</p>
<section data-footnotes="">
<ol dir="auto">
<li id="user-content-fn-1-76869c95acfdd418de6bcf280d4bde15">
<p dir="auto"><a href="https://en.wikipedia.org/wiki/OpenStreetMap">https://en.wikipedia.org/wiki/OpenStreetMap</a> <a href="#user-content-fnref-1-76869c95acfdd418de6bcf280d4bde15" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-2-76869c95acfdd418de6bcf280d4bde15">
<p dir="auto"><a href="https://en.wikipedia.org/wiki/Arnis,_Germany">https://en.wikipedia.org/wiki/Arnis,_Germany</a> <a href="#user-content-fnref-2-76869c95acfdd418de6bcf280d4bde15" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
<li id="user-content-fn-3-76869c95acfdd418de6bcf280d4bde15">
<p dir="auto">This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
For the full license text, see the LICENSE file. <a href="#user-content-fnref-3-76869c95acfdd418de6bcf280d4bde15" data-footnote-backref="" aria-label="Back to reference 3">↩</a></p>
</li>
</ol>
</section>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Things we learned about LLMs in 2024 (532 pts)]]></title>
            <link>https://simonwillison.net/2024/Dec/31/llms-in-2024/</link>
            <guid>42560558</guid>
            <pubDate>Tue, 31 Dec 2024 18:11:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2024/Dec/31/llms-in-2024/">https://simonwillison.net/2024/Dec/31/llms-in-2024/</a>, See on <a href="https://news.ycombinator.com/item?id=42560558">Hacker News</a></p>
Couldn't get https://simonwillison.net/2024/Dec/31/llms-in-2024/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Dinner for One: British comedy Germans have been laughing at for years (2018) (108 pts)]]></title>
            <link>https://www.theguardian.com/tv-and-radio/2018/dec/30/dinner-for-one-german-television-new-years-eve</link>
            <guid>42560171</guid>
            <pubDate>Tue, 31 Dec 2024 17:23:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/tv-and-radio/2018/dec/30/dinner-for-one-german-television-new-years-eve">https://www.theguardian.com/tv-and-radio/2018/dec/30/dinner-for-one-german-television-new-years-eve</a>, See on <a href="https://news.ycombinator.com/item?id=42560171">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p><span>T</span>en years ago, on New Year’s Eve, my mother grabbed my English boyfriend’s arm and led him to the television. “It’s time for <em>Dinner for One</em>,” she said excitedly as the whole family gathered – as we do every year – to fall about laughing at an old British black-and-white comedy sketch.</p><p>As the opening credits appeared to the sound of a crackling string orchestra, my boyfriend was perplexed but too polite to ask what this was all about. The familiar elegant dining room came into shot with silver chandeliers on a white table cloth. We Germans started to giggle in anticipation of a scene we all knew so well.</p><p>“This is so funny, isn’t it?” my mother cried out to him. His response? “I have never come across it in my entire life,” he confessed. “We don’t watch this at home.”</p><p>That could change this year when, for the first time, <em><a href="https://www.theguardian.com/tv-and-radio/2018/dec/15/beloved-freddie-frinton-skit-dinner-for-one-to-air-on-uk-tv-for-first-time" data-link-name="in body link">Dinner For One</a></em><a href="https://www.theguardian.com/tv-and-radio/2018/dec/15/beloved-freddie-frinton-skit-dinner-for-one-to-air-on-uk-tv-for-first-time" data-link-name="in body link"> is broadcast on British TV</a>. Airing on Sky Arts on 31 December, the skit which holds the world record as the most repeated TV programme in history will finally be coming home. About time, too, many will say: while Britain has ignored this quirky cultural export for decades, it has been at the heart of the German New Year’s Eve ritual since 1972, a much loved overture to the celebrations.</p><p>The popularity of the sketch, filmed in 1963, is not impaired by the fact it is is shown in its original English language version (with a short introduction in German).</p><p>In 2017, more than 12 million Germans tuned in, accompanied by another 100,000 across Scandinavia, the Baltic countries and Switzerland. This year it will be shown 12 times on German public TV channels alone, starting at 10.30 in the morning and continuing until just before midnight. Much of the country will be transfixed, as ever, by the spectacle of James, the butler, serving dinner for Miss Sophie on her 90th birthday.</p><p>The sketch, starring Grimsby comedian Freddie Frinton and May Warden, has spawned extensive literature in <a href="https://www.theguardian.com/world/germany" data-link-name="in body link" data-component="auto-linked-tag">Germany</a>, from manuals to cookbooks and even a commemorative stamp.</p><p>And then there’s the sex: ever the most discussed topic, it repeatedly sparks a debate over whether Frinton and Warden had an affair in real life – they allegedly did. “Same procedure as every year,” mutters James as he accompanies Miss Sophie up the staircase at the end of the sketch.</p><p>Shot more than 50 years ago, it is an extremely basic plot with an air of silent-film slapstick.</p><p>Due to her mature age Miss Sophie’s illustrious guests – Admiral von Schneider, Mr Pomeroy, Sir Toby and Mr Winterbottom – are only imaginary having died years before. Good old James has the task of playing the parts of each of the four, making a toast at the start of each course.</p><p>“They are all here, Miss Sophie,” he says while serving mulligatawny and sherry, North Sea haddock and white wine, chicken with champagne, fruit and port. While carrying out his duties, the loyal but increasingly drunk James hilariously juggles expensive china plates on the taxidermy tiger rug.</p><p>The producer Peter Frankenfeld, one of postwar Germany’s most famous entertainers, first saw <em>Dinner for One</em> in the early 1960s while scouting in Blackpool for ideas for the market back home. Frinton, a former fish filleter, was performing the sketch in a comedy club and Frankenfeld immediately invited him and Warden to Hamburg where they staged it in front of a live TV audience. They shot the sketch in the studios of the North German Broadcasting Corporation.</p><p>What an irony that this famous piece of English humour makes its way back to Britain on the eve of the year the country will cut its most important institutional link with mainland Europe, although I find it hard to imagine that a new era of cultural self-definition will make <em>Dinner for One</em> top of the British comedy hit-list.</p><p>As <a href="https://www.theguardian.com/politics/eu-referendum" data-link-name="in body link" data-component="auto-linked-tag">Brexit</a> looms a repeated question I get from people here is how Germans see the British. Watching <em>Dinner for One</em> might be a hint. Germans still have a stereotypical idea of British society being divided by class, with an establishment made up of eccentric individuals lost in a forlorn past. It is an impression not hurt by certain staunch Leave campaigners. Who knows if they will be tuning in tomorrow night?</p><p><em>Stefanie Bolzen is UK &amp; Ireland correspondent for Die Welt</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The GTA III port for the Dreamcast has been released (292 pts)]]></title>
            <link>https://gitlab.com/skmp/dca3-game</link>
            <guid>42559909</guid>
            <pubDate>Tue, 31 Dec 2024 16:52:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gitlab.com/skmp/dca3-game">https://gitlab.com/skmp/dca3-game</a>, See on <a href="https://news.ycombinator.com/item?id=42559909">Hacker News</a></p>
Couldn't get https://gitlab.com/skmp/dca3-game: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Systems ideas that sound good but almost never work (284 pts)]]></title>
            <link>https://hardcoresoftware.learningbyshipping.com/p/225-systems-ideas-that-sound-good</link>
            <guid>42559882</guid>
            <pubDate>Tue, 31 Dec 2024 16:47:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hardcoresoftware.learningbyshipping.com/p/225-systems-ideas-that-sound-good">https://hardcoresoftware.learningbyshipping.com/p/225-systems-ideas-that-sound-good</a>, See on <a href="https://news.ycombinator.com/item?id=42559882">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa17ca159-5aa6-4324-b75a-b595f1c0e2a7_1017x495.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa17ca159-5aa6-4324-b75a-b595f1c0e2a7_1017x495.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa17ca159-5aa6-4324-b75a-b595f1c0e2a7_1017x495.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa17ca159-5aa6-4324-b75a-b595f1c0e2a7_1017x495.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa17ca159-5aa6-4324-b75a-b595f1c0e2a7_1017x495.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa17ca159-5aa6-4324-b75a-b595f1c0e2a7_1017x495.jpeg" width="1017" height="495" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a17ca159-5aa6-4324-b75a-b595f1c0e2a7_1017x495.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:495,&quot;width&quot;:1017,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:145676,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa17ca159-5aa6-4324-b75a-b595f1c0e2a7_1017x495.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa17ca159-5aa6-4324-b75a-b595f1c0e2a7_1017x495.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa17ca159-5aa6-4324-b75a-b595f1c0e2a7_1017x495.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa17ca159-5aa6-4324-b75a-b595f1c0e2a7_1017x495.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><a href="https://x.com/@Martin_Casado" rel="">@Martin_Casado</a><span> tweeted some wisdom (as he often does) in </span><a href="https://x.com/martin_casado/status/1872822491829420241" rel="">this</a><span>: </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48589728-67cf-4c00-9729-0ad5cfac42a2_1124x800.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48589728-67cf-4c00-9729-0ad5cfac42a2_1124x800.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48589728-67cf-4c00-9729-0ad5cfac42a2_1124x800.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48589728-67cf-4c00-9729-0ad5cfac42a2_1124x800.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48589728-67cf-4c00-9729-0ad5cfac42a2_1124x800.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48589728-67cf-4c00-9729-0ad5cfac42a2_1124x800.jpeg" width="1124" height="800" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/48589728-67cf-4c00-9729-0ad5cfac42a2_1124x800.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:800,&quot;width&quot;:1124,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48589728-67cf-4c00-9729-0ad5cfac42a2_1124x800.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48589728-67cf-4c00-9729-0ad5cfac42a2_1124x800.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48589728-67cf-4c00-9729-0ad5cfac42a2_1124x800.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48589728-67cf-4c00-9729-0ad5cfac42a2_1124x800.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><p>He asked what else and I replied with a quick list. Below is “why” these don’t work. I offer this recognizing engineering is also a social science and what works/does not work is context dependent. One life lesson is that every time you say to an engineer (or post on X) that something won’t work it quickly becomes a challenge to prove otherwise. That’s why most of engineering management (and software architecture) is a combination of “rules of thumb” and lessons learned the hard way.</p><p>I started my list with “let’s just” because 9 out of 10 times when someone says “let’s just” what follows is going to be ultimately way more complicated than anyone in the room thought it would be. I’m going to say “9 out of 10 times” a lot below on purpose because…experience. I offer an example of two below but for each there are probably a half dozen I lived through.</p><p>So why do these below “almost never work”?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F504d72ee-16ed-4bc2-a21d-b695bf06d4b2_722x682.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F504d72ee-16ed-4bc2-a21d-b695bf06d4b2_722x682.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F504d72ee-16ed-4bc2-a21d-b695bf06d4b2_722x682.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F504d72ee-16ed-4bc2-a21d-b695bf06d4b2_722x682.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F504d72ee-16ed-4bc2-a21d-b695bf06d4b2_722x682.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F504d72ee-16ed-4bc2-a21d-b695bf06d4b2_722x682.png" width="722" height="682" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/504d72ee-16ed-4bc2-a21d-b695bf06d4b2_722x682.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:682,&quot;width&quot;:722,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F504d72ee-16ed-4bc2-a21d-b695bf06d4b2_722x682.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F504d72ee-16ed-4bc2-a21d-b695bf06d4b2_722x682.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F504d72ee-16ed-4bc2-a21d-b695bf06d4b2_722x682.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F504d72ee-16ed-4bc2-a21d-b695bf06d4b2_722x682.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><strong>Let's just make it pluggable. </strong><span>When you are pretty sure one implementation won’t work you think “I know, we’ll let developers or maybe others down the road use the same architecture and just slot in a new implementation". Then everyone calling the APIs magically get some improvement or new capability without changing anything. There’s an old saying “the API is the behavior not the header file/documentation”. Almost nothing is pluggable to the degree that it “just works”. The most pluggable components of modern software are probably device drivers, which enabled the modern computer but worked so poorly they are either no longer allowed or modern OSs have been building their own for a decade. The only way something is truly pluggable is if a second implementation is designed at the exact same time as the primary implementation. Then at least you have a proof it can work…one time.</span></p><p><strong>Let's just add an API. </strong><span>Countless products/companies have sprinted to some level of success and then decided “we need to be a platform and have developers” and soon enough there is an API. The problem with offering an API is multidimensional. First, being an API provider is itself a whole mindset and skill where you constantly trade compatibility and interoperability for features, where you are constrained in how things change because of legacy behavior or performance characteristics, and you can basically never move the cheese around. More importantly, an offering an API doesn’t mean anyone wants to use it. Almost every new API comes up because the co/product wants features, but it doesn’t want to prioritize them enough (too small a market, too vertical, too domain specific, etc.) and the theory is the API will be “evangelized” to some partner in the space. Turns out those people are not sitting around waiting to fill in holes in your product. They have a business and customers too who don’t want to buy into yet another product to solve their problem. Having an API—being a platform—is a serious business with real demands. There is magic in building a platform but rarely does one come about by simply “offering some APIs” and even if it does, the chances it provides an economic base for third parties are slim. Tough stuff! Hence the reward.</span></p><p><strong>Let's abstract that one more time. </strong><span>One of the wisest computer scientists I ever got to work with was the legend Butler Lampson (Xerox, MIT, Microsoft, etc) who once said, "All problems in computer science can be solved by another level of indirection" (the "fundamental theorem of software engineering” as it is known). There is truth to this—real truth. Two things on why this fails. First, often engineers know this ahead of time, so they put in abstractions in the architecture too soon. Windows NT is riddled with excess abstractions that were never really used primarily because they were there from the start before there was a real plan to use them. I would contrast this with Mac OS evolution where abstractions that seemed odd appeared useful two releases later because there was a plan. Second, abstractions added after the fact can become very messy to maintain, difficult to secure, and challenging to performance optimize. Because of that you end up with too much code that does not use the new abstraction. Then you have a maintenance headache.</span></p><p><strong>Let's make that asynchronous</strong><span>. Most of the first 25 years of computer science was figuring out how to make things work asynchronously. If you were a graduate student in the 1980s you spent whole courses talking about dining philosophers or producers-consumers or sleeping barbers. Today’s world has mostly abstracted this problem away for most engineers who just operate by the rules at the data level. But at the user experience level there remains a desire to try to get more stuff done and never have people wait. Web frameworks have done great work to abstract this. But 9 out of 10 times once you go outside a framework or the data layer and think you can manage asynchrony yourself, you’ll do great except for the bug that will show up a year from now that you will never be able to reproduce. Hopefully it won’t be a data corruption issue, but I warned you.</span></p><p><strong>Let's just add access controls later. </strong><span>When we weren’t talking about philosophers using chopsticks in grad school we were debating where exactly in a system access control should be. Today’s world is vastly more complex than the days of theoretical debates about access control because systems are under constant attack. Of course everyone knows systems need to be secure from the get-go, yet the pace to get to market means almost no system has fully thought through the access control/security model from the start. There’s almost no way to get the design of access controls to a product right unless you are thinking of that from the customer and adversary perspective from the start. No matter how expeditious it might feel, you will either fail or need to rewrite the product down the road and that will be a horrible experience for everyone including customers.</span></p><p><strong>Let's just sync the data. </strong><span>In this world of multiple devices, SaaS apps, or data stores it is super common to hear someone chime in “why don’t we just sync the data”? Ha. </span><a href="https://x.com/@ROzzie" rel="">@ROzzie</a><span> (Ray Ozzie) who got his start on the Plato product, invented Lotus Notes, as well as Groove and Talko, and led the formation of Microsoft Azure was a pioneer in client/server and data sync. His words of wisdom, “synchronization is a hard problem”. And in computer science a hard problem means it is super difficult and fraught with challenges that can only be learned by experience. This problem is difficult enough with a full semantic and transacted data store, but once it gets to synchronizing blobs or unstructured data or worse involves data translation of some kind, then it very quickly becomes enormously difficult. Almost never do you want to base a solution on synchronizing data. This is why there are multi-billion dollar companies that do sync.</span></p><p><strong>Let's make it cross-platform. </strong><span>I have been having this debate my whole computing life. Every time it comes up someone shows me something that they wrote that they believe works “great” cross platform, or someone tells me about Unity and games. Really clever people think that they can just say “the web”. I get that but I’m still right :-) When you commit to making something cross platform, no matter how customer focused and good your intentions are, you are committing to build an operating system, a cloud provider, or a browser. As much as you think you’re building your own thing, by committing to cross-platform you are essentially building one of those by just “adding a level of indirection” (see above by Butler Lampson). You think you can just make a pluggable platform (see above). The repeated reality of cross-platform is that it works well two times. It works when platforms are new—when the cloud was compute and simple storage for example—and then being an abstraction across two players doing that simple thing makes sense. It works when your application/product is new and simple. Both of those fail as you diverge from the underlying platform or as you build capabilities that are expressed wildly differently on each target. A most “famous” example for me is when Microsoft gave up on building Mac software precisely because it became too difficult to make Office for Mac and Windows from the same code—realize Microsoft essentially existed because it made its business building cross-platform apps. That worked when an OS API was 100 pages of docs, and each OS was derived from CP/M. We forked the Office code in 1998 and never looked back. Every day I use Mac Office I can see how even today it remains impossible to do a great job across platforms. You want more of my view on this please see -&gt; </span><a href="https://medium.learningbyshipping.com/divergent-thoughts-on-cross-platform-updated-68a925a45a83" rel="">https://medium.learningbyshipping.com/divergent-thoughts-on-cross-platform-updated-68a925a45a83</a></p><p><strong>Let's just enable escape to native. </strong><span>Since cross-platform only works for a brief time one of the most common solutions frameworks and API abstractions offer is the ability to “escape to native. The idea is that the platform evolved or added features that the framework/abstraction doesn’t (yet?) expose, presumably because it has to build a whole implementation for the other targets that don’t yet have the capability. This really sounds great on paper. It too never works, more than 9 out of 10 times. The reason is pretty simple. The framework or API you are using that abstracts out some native capability always maintains some state or a cache of what is going on within the abstraction it created. When you call the underlying native platform, you muck with data structures and state that the framework doesn’t know about. Many frameworks provide elaborate mechanisms to exchange data or state information from your “escape to native” code back to the framework. That can work a little bit but in a world of automatic memory management is a solution akin to malloc/free and I am certain no one today would argue for that architecture :-)</span></p><p>Have I always been a strong “no” on all of these? Of course not. Can you choose these approaches, and they work? Yes, of course you can. There’s always some context where these might work, but most of the time you just don’t need them and there’s a better way. Always solve with first principles and don’t just to a software pattern that is so failure prone.</p><p>—Steven</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Watch 3 AIs compete in real-time stock trading ($5/trade) (209 pts)]]></title>
            <link>https://trading.snagra.com</link>
            <guid>42559744</guid>
            <pubDate>Tue, 31 Dec 2024 16:32:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://trading.snagra.com">https://trading.snagra.com</a>, See on <a href="https://news.ycombinator.com/item?id=42559744">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Morris Chang and the Origins of TSMC (183 pts)]]></title>
            <link>https://www.construction-physics.com/p/morris-chang-and-the-origins-of-tsmc</link>
            <guid>42559052</guid>
            <pubDate>Tue, 31 Dec 2024 14:58:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.construction-physics.com/p/morris-chang-and-the-origins-of-tsmc">https://www.construction-physics.com/p/morris-chang-and-the-origins-of-tsmc</a>, See on <a href="https://news.ycombinator.com/item?id=42559052">Hacker News</a></p>
Couldn't get https://www.construction-physics.com/p/morris-chang-and-the-origins-of-tsmc: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[How to monitor your local weather with Grafana (170 pts)]]></title>
            <link>https://grafana.com/blog/2024/12/26/how-to-monitor-your-local-weather-with-grafana/</link>
            <guid>42558763</guid>
            <pubDate>Tue, 31 Dec 2024 14:05:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://grafana.com/blog/2024/12/26/how-to-monitor-your-local-weather-with-grafana/">https://grafana.com/blog/2024/12/26/how-to-monitor-your-local-weather-with-grafana/</a>, See on <a href="https://news.ycombinator.com/item?id=42558763">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Ever look at a wall of raw data and wonder, “How am I supposed to make sense of <em>this</em>?” That’s exactly where Grafana comes in, turning your data into beautiful dashboards with panels of graphs and other visualization types.</p><p>In this blog post, which is geared towards those new to Grafana, we’ll walk through an example that demonstrates how, exactly, Grafana can transform your data into eye-catching dashboards. To do this, we’ll build a free weather dashboard using <a href="https://grafana.com/products/cloud/?pg=blog&amp;plcmt=body-txt">Grafana Cloud</a>. (Don’t have a Grafana Cloud account yet? No worries. We have a generous free-forever tier that you can <a href="https://grafana.com/auth/sign-up/create-user/?pg=blog&amp;plcmt=body-txt">sign up for today</a>.)</p><h2 id="a-quick-overview-of-grafana-and-grafana-cloud">A quick overview of Grafana and Grafana Cloud</h2><p>Grafana is an open source solution that enables you to collect, correlate, and visualize your data, regardless of where that data is stored. This is because Grafana supports a vast number of data sources and accommodates a wide range of use cases — whether it’s monitoring server health or <a href="https://grafana.com/blog/2024/04/01/how-to-monitor-your-kids-chores-an-introduction-to-grafana-powered-parenting/">your kids completing their chores</a>.</p><figure itemprop="associatedMedia" itemscope="" itemtype="http://schema.org/ImageObject"><a href="https://grafana.com/media/blog/weather-dashboard/weather-dashboard-grafana-example.png" itemprop="contentUrl"><p><img src="https://grafana.com/media/blog/weather-dashboard/weather-dashboard-grafana-example.png" alt="A sample Grafana dashboard to monitor Linux server health. " width="1483" height="985" title="*A sample Grafana dashboard to monitor Linux server health.*" data-src="/media/blog/weather-dashboard/weather-dashboard-grafana-example.png" data-srcset="/media/blog/weather-dashboard/weather-dashboard-grafana-example.png?w=320 320w, /media/blog/weather-dashboard/weather-dashboard-grafana-example.png?w=550 550w, /media/blog/weather-dashboard/weather-dashboard-grafana-example.png?w=750 750w, /media/blog/weather-dashboard/weather-dashboard-grafana-example.png?w=900 900w, /media/blog/weather-dashboard/weather-dashboard-grafana-example.png?w=1040 1040w, /media/blog/weather-dashboard/weather-dashboard-grafana-example.png?w=1240 1240w, /media/blog/weather-dashboard/weather-dashboard-grafana-example.png?w=1920 1920w"></p><figcaption><em>A sample Grafana dashboard to monitor Linux server health.</em></figcaption></a></figure><p>Introduced as an open source project in 2013, Grafana has grown to have a thriving community of more than 25 million users worldwide. (<em>Note: if you want to learn more about the origins and evolution of Grafana, you can check out <a href="https://grafana.com/blog/2024/02/12/the-story-of-grafana-documentary-from-one-developers-dream-to-20-million-users-worldwide/">‘The Story of Grafana’ documentary.</a></em>)</p><p><a href="https://grafana.com/products/cloud/?pg=blog&amp;plcmt=body-txt">Grafana Cloud</a>, meanwhile, is our fully managed, cloud-hosted observability platform powered by the Grafana LGTM (<a href="https://grafana.com/oss/loki/">Loki</a> for logs, <a href="https://grafana.com/oss/grafana?pg=blog&amp;plcmt=body-txt">Grafana</a> for visualization, <a href="https://grafana.com/oss/tempo/?pg=blog&amp;plcmt=body-txt">Tempo</a> for traces, <a href="https://grafana.com/oss/mimir/?pg=blog&amp;plcmt=body-txt">Mimir</a> for metrics) Stack. We’ll be using Grafana Cloud to create our free weather dashboard.</p><h2 id="how-to-build-a-custom-weather-dashboard-with-grafana-cloud">How to build a custom weather dashboard with Grafana Cloud</h2><p>Now we’ll walk through an example of how to transform raw JSON data into a weather forecast dashboard using Grafana Cloud. It should take about 30 minutes to do, and when we’re finished, you’ll have a useful weather forecast dashboard that you can check right from your cell phone.</p><p>We’ll use a <a href="http://api.weather.gov/" target="_blank" rel="noopener noreferrer">free public API</a> from the <a href="https://www.weather.gov/" target="_blank" rel="noopener noreferrer">U.S. National Weather Service</a> that offers detailed forecast information for all areas in the United States. If you’re outside the U.S., you can use <a href="https://open-meteo.com/en/docs" target="_blank" rel="noopener noreferrer">OpenMeteo</a> (in Europe) or <a href="https://openweathermap.org/api" target="_blank" rel="noopener noreferrer">OpenWeatherMap</a> (globally) to fetch your local weather data.</p><p>Here’s an example of the weather API response with data formatted as JSON:</p><figure itemprop="associatedMedia" itemscope="" itemtype="http://schema.org/ImageObject"><a href="https://grafana.com/media/blog/weather-dashboard/weather-dashboard-grafana-cloud-JSON.png" itemprop="contentUrl"><p><img src="https://grafana.com/media/blog/weather-dashboard/weather-dashboard-grafana-cloud-JSON.png" alt="A screenshot of raw JSON data." width="573" height="455" data-src="/media/blog/weather-dashboard/weather-dashboard-grafana-cloud-JSON.png" data-srcset="/media/blog/weather-dashboard/weather-dashboard-grafana-cloud-JSON.png?w=320 320w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-JSON.png?w=550 550w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-JSON.png?w=750 750w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-JSON.png?w=900 900w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-JSON.png?w=1040 1040w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-JSON.png?w=1240 1240w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-JSON.png?w=1920 1920w"></p></a></figure><p>It’s packed with useful info, but most people don’t want to read JSON to check the weather. This is where Grafana Cloud dashboards will come in.</p><figure itemprop="associatedMedia" itemscope="" itemtype="http://schema.org/ImageObject"><a href="https://grafana.com/media/blog/weather-dashboard/weather-dashboard-meme.jpg" itemprop="contentUrl"><p><img src="https://grafana.com/media/blog/weather-dashboard/weather-dashboard-meme.jpg" alt="A screenshot of a meme about JSON code being difficult to read." width="500" height="500" title="*JSON: Great for computers, difficult for humans.*" data-src="/media/blog/weather-dashboard/weather-dashboard-meme.jpg" data-srcset="/media/blog/weather-dashboard/weather-dashboard-meme.jpg?w=320 320w, /media/blog/weather-dashboard/weather-dashboard-meme.jpg?w=550 550w, /media/blog/weather-dashboard/weather-dashboard-meme.jpg?w=750 750w, /media/blog/weather-dashboard/weather-dashboard-meme.jpg?w=900 900w, /media/blog/weather-dashboard/weather-dashboard-meme.jpg?w=1040 1040w, /media/blog/weather-dashboard/weather-dashboard-meme.jpg?w=1240 1240w, /media/blog/weather-dashboard/weather-dashboard-meme.jpg?w=1920 1920w"></p><figcaption><em>JSON: Great for computers, difficult for humans.</em></figcaption></a></figure><p>We’ll also use the <a href="https://grafana.com/grafana/plugins/yesoreyeram-infinity-datasource/?pg=blog&amp;plcmt=body-txt">Infinity data source</a> plugin for Grafana, which lets you query and visualize data from JSON, CSV, XML, and GraphQL endpoints (think of it as a universal plugin for ingesting data from almost anywhere). This plugin is how we will get the raw weather data that powers our dashboard.</p><p>Grafana Cloud will connect to the weather.gov API every time you load your dashboard, and render the results in a table.</p><p>Below you’ll find a brief summary of the steps to create your weather dashboard. For more detailed instructions, please check out <a href="https://github.com/scarolan/grafana-cloud-tutorial/tree/main/01-the-journey-begins" target="_blank" rel="noopener noreferrer">this tutorial on GitHub</a>.</p><ol><li><a href="https://grafana.com/auth/sign-up/create-user/?pg=blog&amp;plcmt=body-txt"><strong>Sign up for Grafana Cloud</strong>: </a>You can use any of our sign-in providers or sign up with an email address and password.</li></ol><figure itemprop="associatedMedia" itemscope="" itemtype="http://schema.org/ImageObject"><a href="https://grafana.com/media/blog/weather-dashboard/weather-dashboard-grafana-cloud-signin.png" itemprop="contentUrl"><p><img src="https://grafana.com/media/blog/weather-dashboard/weather-dashboard-grafana-cloud-signin.png" alt="A screenshot of the Grafana Cloud sign up page. " width="614" height="585" data-src="/media/blog/weather-dashboard/weather-dashboard-grafana-cloud-signin.png" data-srcset="/media/blog/weather-dashboard/weather-dashboard-grafana-cloud-signin.png?w=320 320w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-signin.png?w=550 550w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-signin.png?w=750 750w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-signin.png?w=900 900w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-signin.png?w=1040 1040w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-signin.png?w=1240 1240w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-signin.png?w=1920 1920w"></p></a></figure><ol start="2"><li><strong><a href="https://grafana.com/grafana/plugins/yesoreyeram-infinity-datasource/?pg=blog&amp;plcmt=body-txt&amp;tab=installation">Install the Infinity data source plugin</a></strong>.</li><li><strong>Import a preconfigured weather dashboard</strong>: Grafana Cloud allows you to import dashboards created by other users. If you can copy and paste some text, you can import a dashboard!</li><li><strong>Connect your local weather data using the National Weather Service API</strong>: To find your local weather station, you’ll use the National Weather Service grid points for your chosen city, based on your longitude and latitude.</li><li><strong>Customize the dashboard</strong> with your city’s name and bookmark it for later use. Its format is optimized for display on a mobile device.</li></ol><figure itemprop="associatedMedia" itemscope="" itemtype="http://schema.org/ImageObject"><a href="https://grafana.com/media/blog/weather-dashboard/weather-dashboard-grafana-cloud-forecast-resized.png" itemprop="contentUrl"><p><img src="https://grafana.com/media/blog/weather-dashboard/weather-dashboard-grafana-cloud-forecast-resized.png" alt="A screenshot of the Grafana Cloud dashboard for monitoring weather. " width="418" height="625" data-src="/media/blog/weather-dashboard/weather-dashboard-grafana-cloud-forecast-resized.png" data-srcset="/media/blog/weather-dashboard/weather-dashboard-grafana-cloud-forecast-resized.png?w=320 320w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-forecast-resized.png?w=550 550w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-forecast-resized.png?w=750 750w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-forecast-resized.png?w=900 900w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-forecast-resized.png?w=1040 1040w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-forecast-resized.png?w=1240 1240w, /media/blog/weather-dashboard/weather-dashboard-grafana-cloud-forecast-resized.png?w=1920 1920w"></p></a></figure><h2 id="your-grafana-cloud-adventure-awaits">Your Grafana Cloud adventure awaits</h2><p>This exercise is just one small example of what you can achieve with Grafana Cloud.</p><p>To learn more, please check out these <a href="https://grafana.com/products/cloud/resources/?pg=blog&amp;plcmt=body-txt">Grafana Cloud resources</a>, including blog posts, technical docs, webinars, and quick-start guides. And if you have any questions, please reach out on our <a href="https://community.grafana.com/" target="_blank" rel="noopener noreferrer">community forums</a> or <a href="https://slack.grafana.com/" target="_blank" rel="noopener noreferrer">Slack</a>.</p><p><em><a href="https://grafana.com/products/cloud/?pg=blog&amp;plcmt=body-txt">Grafana Cloud</a> is the easiest way to get started with metrics, logs, traces, dashboards, and more. We have a generous forever-free tier and plans for every use case. <a href="https://grafana.com/auth/sign-up/create-user/?pg=blog&amp;plcmt=body-txt">Sign up for free now!</a></em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Darktable 5.0.0 (292 pts)]]></title>
            <link>https://www.darktable.org/2024/12/darktable-5.0.0-released/</link>
            <guid>42558037</guid>
            <pubDate>Tue, 31 Dec 2024 11:19:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.darktable.org/2024/12/darktable-5.0.0-released/">https://www.darktable.org/2024/12/darktable-5.0.0-released/</a>, See on <a href="https://news.ycombinator.com/item?id=42558037">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>We’re proud to announce the new feature release of darktable, 5.0.0!</p>
<p>The github release is here: <a href="https://github.com/darktable-org/darktable/releases/tag/release-5.0.0">https://github.com/darktable-org/darktable/releases/tag/release-5.0.0</a>.</p>
<p>To build from source, do not use the autogenerated tarball provided by GitHub on the release page, download our tar.xz file instead. If you’re just building for yourself without creating a package for some distribution, then using source code cloning in git is an even more convenient way.</p>
<p>The checksums are:</p>
<pre tabindex="0"><code>$ sha256sum darktable-5.0.0.tar.xz
eaa136e6e624bb53127282e26aafa0441abcc189b55371465e1f5a8a493fa3a1  darktable-5.0.0.tar.xz

$ sha256sum darktable-5.0.0-x86_64.dmg
3f49cfb63958269b99065cf6b501678d4e63f2457ee1915bcd7ffa0dfef9dcfd  darktable-5.0.0-x86_64.dmg

$ sha256sum darktable-5.0.0-arm64.dmg
14feb35ef2b2e8e50cf1855826ad4913e905a5600a56a87dd98382e8d828e9db  darktable-5.0.0-arm64.dmg

$ sha256sum darktable-5.0.0-arm64-13.5.dmg
b43011cae5ddc9f19a8f895ba389e9ddb79d01534e9ca0568b7125026ac72145  darktable-5.0.0-arm64-13.5.dmg

$ sha256sum darktable-5.0.0-win64.exe
40444d5c7d310b1e1e859bd6b7c5d5e35d538a7bf9ad3e918b0e883c971451ea  darktable-5.0.0-win64.exe

$ sha256sum Darktable-5.0.0-x86_64.AppImage
d0061ac5a345c473d98f04388197afaee48e61b638db576ae1c88700cb8855cd  Darktable-5.0.0-x86_64.AppImage
</code></pre><p>When updating from the stable 4.8 series, please bear in mind that your edits will be preserved during this process, but the new library and configuration will no longer be usable with 4.8.</p>
<p>You are strongly advised to take a backup first.</p>
<h4 id="important-note-to-make-sure-that-darktable-can-keep-on-supporting-the-raw-file-format-for-your-camera-please-read-this-posthttpsdiscusspixlsustraw-samples-wanted5420ulebedevri-on-howwhat-raw-samples-you-can-contribute-to-ensure-that-we-have-the-full-raw-sample-set-for-your-camera-under-cc0-license">Important note: to make sure that darktable can keep on supporting the raw file format for your camera, <em>please</em> read <a href="https://discuss.pixls.us/t/raw-samples-wanted/5420?u=lebedevri">this post</a> on how/what raw samples you can contribute to ensure that we have the <em>full</em> raw sample set for your camera under CC0 license!</h4>
<p>Since darktable 4.8:</p>
<ul>
<li>1198 commits to darktable+rawspeed</li>
<li>505 pull requests handled</li>
<li>45 issues closed</li>
</ul>
<p><em>Please note that the darktable documentation is not currently complete for release 5.0
and contributions are greatly appreciated. Please see the
<a href="https://github.com/darktable-org/dtdocs#contributing">project documentation</a>
for more information on how to contribute.</em></p>
<h2 id="the-big-ones">The Big Ones</h2>
<p>The following is a summary of the main features added to darktable
5.0. Please see the user manual for more details of the individual
changes (where available).</p>
<ul>
<li>This development cycle has included a large number of changes which
improve the user experience, as detailed in the next section.</li>
</ul>
<h2 id="uiux-improvements">UI/UX Improvements</h2>
<ul>
<li>
<p>Added camera-specific styles for more than 500 camera models to more
closely approximate the out-of-camera JPEG rendition.  These styles
only affect contrast, brightness, and saturation and do not attempt
to match sharpening, denoising, or hue shifts.  Also added a Lua
script to auto-apply the appropriate style on import and manually
apply styles to a collection of previously-imported images.</p>
</li>
<li>
<p>Added an optional splash screen showing startup progress (including
estimated time remaining during the scan for updated sidecar files) to
dramatically reduce the time between invoking darktable and something
appearing on screen when the user has a large library.</p>
</li>
<li>
<p>The user interface now gives feedback while processing bulk image
operations such as rating, tagging, applying styles, and edit
history management (and undoing those operations), rather than
silently freezing until the operation completes.  While the
operation is in progress, darktable will now show either a busy
cursor (such as a stopwatch or spinner) or a progress bar with
option to cancel the remainder of the operation.</p>
</li>
<li>
<p>Paths for drawn masks now display two Bézier handles per control point,
which can be moved individually. This allows for more precise control
of the paths.</p>
</li>
<li>
<p>Added a high-contrast theme with bright white text on a dark gray
background.</p>
</li>
<li>
<p>Enhanced tooltips for utility module headers to provide more
information about the module.</p>
</li>
<li>
<p>Added more new-user hints on an empty lighttable.</p>
</li>
<li>
<p>Added two new error placeholder images to distinguish between
missing, unsupported, and corrupted images.  When attempting to edit
such an image, an appropriate, more specific error message is
displayed.</p>
</li>
<li>
<p>When selecting a style in the export module, hovering on the style
name in the popup menu displays a thumbnail previewing the effect of
appending the style to the active image’s edit (first selected image
in lighttable, center-view image in darkroom).</p>
</li>
<li>
<p>Allow for selecting the utility modules to be displayed on the
panels in the different views.</p>
<ul>
<li>
<p>Right-click on the empty panel area below the modules to get a
menu where they can be hidden or shown. This allows additional
modules to be added to the darkroom, like metadata editor and
styles.</p>
</li>
<li>
<p>This replaces the options in the “collections” and “recently used
collections” modules’ preferences to show or hide the latter and
show a “history” button in the former instead. Users that want the
separate module will need to reenable it once via the new
<kbd>Right-click</kbd> menu.</p>
</li>
<li>
<p>The menu also contains an option “restore defaults” that resets
the selection and position of modules in the current view. In the
preferences dialog, on the general tab, there’s a “reset view
panels” button that resets all views, including visibility and
width of the panels themselves.</p>
</li>
</ul>
</li>
<li>
<p>Added a global preference to swap the left and right side panels in
the darkroom view.</p>
</li>
<li>
<p>The first time a new user presses Tab, they will be warned that this
will hide all panels and how to get them back. Hopefully this
prevents some confusion or frustration.</p>
</li>
<li>
<p>Drag&amp;drop utility module headers to reposition them across the left
and right panels (lighttable) as well as vertically (all
views). Each view can have a different layout.</p>
</li>
<li>
<p>Drag&amp;drop of processing modules in the darkroom right panel has been
improved to auto-scroll when reaching the top or bottom and to not
get confused when images get dragged into the area. This functionality
no longer requires <kbd>Ctrl+Shift</kbd> modifiers.</p>
</li>
<li>
<p>Improved the message displayed at startup when the database is
locked by another instance of darktable.</p>
</li>
<li>
<p>Replaced the icon of the operator button in the color label filter
for working with multiple selected color labels
(union/intersection).</p>
</li>
</ul>
<h2 id="performance-improvements">Performance Improvements</h2>
<ul>
<li>
<p>Added OpenCL implementation of color equalizer.</p>
</li>
<li>
<p>Improved the speed of bulk image operations by improving the speed
of sidecar writes, and by moving sidecar updates for many operations
into a background task, allowing the user to proceed before the
writes complete.</p>
</li>
<li>
<p>Significantly accelerated loading of PFM files due to loops
parallelization and optimization that eliminated additional
processing.</p>
</li>
</ul>
<h2 id="other-changes">Other Changes</h2>
<ul>
<li>
<p>Switched default scope for new installations from histogram to
waveform to display more detailed information about image color and
tonality.</p>
</li>
<li>
<p>The ISO 12646 color assessment condition is kept until unset by user
action.</p>
</li>
<li>
<p>Exposure bias can now be used to form collections and as a display filter.</p>
</li>
<li>
<p>Improved visualization of the color equalizer’s effect.</p>
</li>
<li>
<p>Improved debugging support for verifying CPU vs. GPU results.</p>
</li>
<li>
<p>Add Calibrite alias for X-Rite ColorChecker in color calibration.</p>
</li>
<li>
<p>The scan for updated sidecar files now ignores timestamp differences
of two seconds or less.</p>
</li>
<li>
<p>The macOS installation package now has a background image to direct
the user on installing darktable.app.</p>
</li>
<li>
<p>Changed the user interface of the import dialog to make it easier to
delete custom places.</p>
</li>
<li>
<p>Numerous rounds of code cleanup.</p>
</li>
<li>
<p>The copy-parts dialog does not select any module by default now.</p>
</li>
<li>
<p>Add support for undo/redo for actions done on the filmstrip while in
darkroom.</p>
</li>
<li>
<p>In darkroom, add action (binding to <kbd>Ctrl+x</kbd> by default) for
synchronizing the last edited module on current edited module to the
selection.</p>
</li>
<li>
<p>Adjusted the internal AVIF encoder parameter to significantly boost
encoding speed without compromising the output quality.</p>
</li>
<li>
<p>Tag names can now easily be copied to the clipboard via popup
context menu in the tagging module.</p>
</li>
<li>
<p>The Piwigo export storage now supports to specify a file name
pattern for the exported file.</p>
</li>
<li>
<p>The directory where darktable will write the log file under Windows
has been changed to %USERPROFILE%\Documents\Darktable. This allows
the user to easily see where the log file is located without even
having to search for it in the documentation or FAQ. The previous
location was deep in the system subdirectories of the user profile,
and also under a hidden directory (so it was impossible to click to
it in File Explorer with default system settings).</p>
</li>
<li>
<p>Allow import of JPEG 2000 files with .jpf and .jpx file extensions.</p>
</li>
<li>
<p>Add a visible indicator to the color calibration module when its
color mapping section has non-neutral settings which will affect
color rendition.</p>
</li>
<li>
<p>Added new substitution variables <code>$(IMAGE.TAGS.HIERARCHY)</code> to insert
tags with full hierarchy and <code>$(IMAGE.ID.NEXT)</code> to insert the image ID
to be assigned to the image being imported, allowing the image ID to
be part of the filename generated during a copy&amp;import operation.</p>
</li>
<li>
<p>Exporting to floating-point JPEG XL with a quality of 100 will try
to do it as losslessly as possible. That is now consistent with the
behavior of integral JPEG XL formats.</p>
</li>
<li>
<p>Improved visibility of shortcuts that can be changed by users by
using bold text.</p>
</li>
<li>
<p>The histogram-exposure interface now supports all standard bauhaus
features (<kbd>Ctrl+click</kbd>, <kbd>Right-click</kbd>…).</p>
</li>
<li>
<p>Introduce image module order v5.0 to have the final-scale done before
color-out to fix some issues with color difference between darkroom
view and exported files.</p>
</li>
<li>
<p>Add support for editing any live color-picker samples. Using
<kbd>Right-click</kbd> on a sample it is possible to edit it
(changing location and/or size of the box) and either add a new
sample based on the edit or store the edit into an existing live
sample.</p>
</li>
<li>
<p>Added more substitution variables for using EXIF data fields,
enabled autocompletion of variables in the watermark module.</p>
<p>The new variables are <code>$(EXIF.FLASH)</code>, <code>$(EXIF.METERING)</code>,
<code>$(EXIF.EXPOSURE.PROGRAM)</code>, <code>$(EXIF.WHITEBALANCE)</code> and
<code>$(GPS.LOCATION.ICON)</code>.</p>
</li>
<li>
<p>Increase maximum focal length for filtering auto-applied presets to
2000mm.</p>
</li>
<li>
<p>Added an expanded color-checker preset to the Color Look Up Table
module with seven-level red/green/blue/gray ramps, IT8/CC24-like
skin tones, and miscellaneous color patches for more targeted color
adjustments across the full spectrum.</p>
</li>
<li>
<p>Added support for EXIF tags ‘AnalogBalance’ used for color
calibration and ‘LinearResponseLimit’ used in highlights
reconstruction.</p>
</li>
<li>
<p>If we find currently unsupported color calibration data in DNG
specific tags, we tag the image by darktable|issue|no-samples for
better support.</p>
</li>
<li>
<p>Added read support for HEIF files with AVC (H.264) compression and
.avci file extension.</p>
</li>
<li>
<p>Added read support for JPEG 2000 encoded images in HEIF containers
with .hej2 file extension.</p>
</li>
</ul>
<h2 id="bug-fixes">Bug Fixes</h2>
<ul>
<li>
<p>Fixed a performance regression for redrawing mipmaps.</p>
</li>
<li>
<p>Fixed handling of old (2020) edits using Filmic RGB.</p>
</li>
<li>
<p>Various OpenCL fixes to reduce differences between CPU and GPU
processing: colorspace conversion, saturation gradient filter in
color equalizer.</p>
</li>
<li>
<p>Fixed gallery export not working on Windows.</p>
</li>
<li>
<p>Fixed printer discovery in the print module, which could cause
available printers to be missed.</p>
</li>
<li>
<p>Work around out-of-spec EXIF date field caused by buggy software.</p>
</li>
<li>
<p>Fixed reading embedded color profiles from PNG images.</p>
</li>
<li>
<p>Fixed certain boundary cases in the crop module.</p>
</li>
<li>
<p>Fixed crash when loading corrupted .gpx file in the geotagging module</p>
</li>
<li>
<p>Fix preset handling in the export module not saving all parameters.</p>
</li>
<li>
<p>Fix an issue in FilmicRGB where one of the parameter could be above
the maximum allowed range making the validation failing and the
whole set of parameters reset to default.</p>
</li>
<li>
<p>Fix overlay recording to work in all cases (discarding history or
copy/paste history for example) ensuring that an image not
referenced anymore as overlay in a composite module can be removed.</p>
</li>
<li>
<p>Properly reset darktable internal tag darktable|style|<name> and
darktable|changed when resetting history.</name></p>
</li>
<li>
<p>Fixed crash in the Piwigo export storage when not logged in to the
Piwigo server.</p>
</li>
<li>
<p>Fixed a bug in the export module where it was impossible to export a
file again if “on conflict: overwrite if changed” was selected.</p>
</li>
<li>
<p>Fixed a bug where double clicking on a label in darkroom modules
does not reset the control.</p>
</li>
<li>
<p>The composite module now prevents assigning an overlay that would
lead to a loop. Previously, only direct references
(image #1 &lt;-&gt; image #2) were checked; this has now been extended
to also cover chains (image #1 -&gt; image #2 -&gt; image #3 -&gt; image #1)
of arbitrary length.</p>
</li>
<li>
<p>Fix a bug in overlay module which incorrectly apply a color profile
and so creating an unwanted and wrong color cast. This bug was a
regression added just before the 4.8 release.</p>
</li>
<li>
<p>Fixed a bug in color calibration module where switching between
various illuminants could lead to unpredictable settings.</p>
</li>
<li>
<p>Various fixes In the demosaic module. Non-usable options are hidden
now. Fixed dual demosaicing for xtrans sensors and OpenCL code.</p>
</li>
<li>
<p>Fixed a bug in the history module where style creation fails if a
style with that name already exists.</p>
</li>
<li>
<p>Fixed guides drawing in case a module is expanded and active.</p>
</li>
<li>
<p>Ensure that the list of images in the culling view remains up to
date when hidden.</p>
</li>
<li>
<p>Fixed minor glitches in color calibration module.</p>
</li>
<li>
<p>Fixed issues with wrong corrections in highlight opposed OpenCL
code.</p>
</li>
<li>
<p>Fixed surface blur radius calculation possibly resulting in garbled
output.</p>
</li>
</ul>
<h2 id="lua">Lua</h2>
<h3 id="api-version">API Version</h3>
<ul>
<li>API version is now 9.4.0</li>
</ul>
<h3 id="new-features">New Features</h3>
<ul>
<li>
<p>Added new event, inter-script-communication, to permit sending messages
from one running script to another running script.</p>
</li>
<li>
<p>Added new function darktable.util.message(), for sending messages using
the inter-script-communication event.</p>
</li>
<li>
<p>Added new EXIF data fields to dt_lua_image_t:</p>
<ul>
<li>
<p>exif_whitebalance</p>
</li>
<li>
<p>exif_flash</p>
</li>
<li>
<p>exif_exposure_program</p>
</li>
<li>
<p>exif_metering_mode</p>
</li>
</ul>
</li>
<li>
<p>Added new event, image-group-information-changed, that is raised any time
an images group information changes.</p>
</li>
</ul>
<h3 id="bug-fixes-1">Bug Fixes</h3>
<ul>
<li>Fixed a bug with dt_imageio_module_format_t.write_image so it returns
true on success and false on failure.</li>
</ul>
<h3 id="add-action-support-for-lua">Add action support for Lua</h3>
<h3 id="other-lua-changes">Other Lua changes</h3>
<ul>
<li>Lua scripts are now better integrated into Darktable and can be
fully translated. The design for the scripts manager has been
reworked to be more in line with the current Darktable GUI modules.</li>
</ul>
<h2 id="notes">Notes</h2>
<ul>
<li>
<p>When exporting to AVIF, EXR, JPEG XL, or XCF, selecting specific
metadata (e.g. geo-tag or creator) is not currently possible. For
AVIF, EXR, JPEG XL, and XCF formats, darktable will not include any
metadata fields unless the user selects all of the checkboxes in the
export module’s preference options.</p>
</li>
<li>
<p>Since 4.8 release the support for macOS versions older than 13.5 has
been dropped.</p>
</li>
</ul>
<h2 id="changed-dependencies">Changed Dependencies</h2>
<h3 id="mandatory">Mandatory</h3>
<ul>
<li>Bump SQLite requirement to 3.26</li>
</ul>
<h3 id="optional">Optional</h3>
<ul>
<li>n/a</li>
</ul>
<h2 id="rawspeed-changes">RawSpeed changes</h2>
<ul>
<li>Fujifilm GFX cameras now use the vendor supplied crop</li>
</ul>
<h2 id="camera-support-compared-to-48">Camera support, compared to 4.8</h2>
<h3 id="base-support">Base Support</h3>
<ul>
<li>Fujifilm X-M5 (compressed)</li>
<li>Fujifilm X-T50 (compressed)</li>
<li>Leica D-Lux 8 (DNG)</li>
<li>Leica M11-D (DNG)</li>
<li>Leica Q3 43 (DNG)</li>
<li>Minolta Alpha Sweet Digital</li>
<li>Minolta Alpha-7 Digital</li>
<li>Nikon Z50_2 (14bit-compressed)</li>
<li>Nikon Z6_3 (14bit-compressed)</li>
<li>Panasonic DC-FZ80D (4:3)</li>
<li>Panasonic DC-FZ82D (4:3)</li>
<li>Panasonic DC-FZ85 (4:3)</li>
<li>Panasonic DC-FZ85D (4:3)</li>
<li>Panasonic DC-G100D (4:3)</li>
<li>Phase One P20+</li>
<li>Sony ILCE-1M2</li>
</ul>
<h3 id="white-balance-presets">White Balance Presets</h3>
<ul>
<li>Nikon Z6_3</li>
<li>Sony ILCE-6700</li>
</ul>
<h3 id="noise-profiles">Noise Profiles</h3>
<ul>
<li>Canon PowerShot G1 X</li>
<li>Leica M11</li>
<li>Nikon Z6_3</li>
</ul>
<h3 id="missing-compression-mode-support">Missing Compression Mode Support</h3>
<ul>
<li>Apple ProRAW DNGs</li>
<li>CinemaDNG lossless (Blackmagic, some DJI, etc.) and lossy (Blackmagic)</li>
<li>DNG 1.7 using JPEG XL (Adobe enhanced, Samsung Expert RAW)</li>
<li>Fujifilm lossy RAFs</li>
<li>Nikon high efficiency NEFs</li>
<li>OM System 14-bit high resolution ORFs</li>
<li>Sony downsized lossless ARWs (“M” for full-frame, “S” for full-frame &amp; APS-C)</li>
</ul>
<h3 id="suspended-support">Suspended Support</h3>
<p>Support for the following cameras is suspended because no samples are available on <a href="https://raw.pixls.us/">https://raw.pixls.us</a>:</p>
<ul>
<li>Creo/Leaf Aptus 22(LF3779)/Hasselblad H1</li>
<li>Fujifilm IS-1</li>
<li>Kodak EasyShare Z980</li>
<li>Leaf Aptus-II 5(LI300059)/Mamiya 645 AFD</li>
<li>Leaf Credo 60</li>
<li>Leaf Credo 80</li>
<li>Minolta DiMAGE 5</li>
<li>Olympus SP320</li>
<li>Phase One IQ250</li>
<li>Sinar Hy6/ Sinarback eXact</li>
<li>ST Micro STV680</li>
</ul>
<h2 id="translations">Translations</h2>
<ul>
<li>Czech</li>
<li>German</li>
<li>European Spanish</li>
<li>Finnish</li>
<li>French</li>
<li>Japanese</li>
<li>Dutch</li>
<li>Brazilian Portuguese</li>
<li>Slovenian</li>
<li>Albanian</li>
<li>Ukrainian</li>
<li>Chinese (Simplified)</li>
<li>Chinese (Traditional)</li>
</ul>

          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Interesting Interview with DeepSeek's CEO (330 pts)]]></title>
            <link>https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas</link>
            <guid>42557586</guid>
            <pubDate>Tue, 31 Dec 2024 09:28:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas">https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas</a>, See on <a href="https://news.ycombinator.com/item?id=42557586">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Deepseek is a Chinese AI startup whose latest R1 model </span><strong><a href="https://api-docs.deepseek.com/news/news1120" rel="">beat OpenAI’s o1</a><span> on multiple reasoning benchmarks</span></strong><span>. Despite its low profile, Deepseek is the Chinese AI lab to watch.</span></p><p><span>Before Deepseek, CEO Liang Wenfeng’s main venture was High-Flyer (幻方), a top 4 Chinese quantitative hedge fund last valued at $8 billion. Deepseek is fully funded by High-Flyer and has no plans to fundraise. It focuses on building foundational technology rather than commercial applications and has committed to open sourcing all of its models. It has also singlehandedly kicked off price wars in China by charging very affordable API rates. Despite this, Deepseek can afford to stay in the scaling game: with access to High-Flyer’s compute clusters, Dylan Patel’s </span><a href="https://x.com/dylan522p/status/1859302712803807696" rel="">best guess</a><span> is they have upwards of “50k Hopper GPUs,” orders of magnitude more compute power than the 10k A100s they cop to publicly.</span></p><p>Deepseek’s strategy is grounded in their ambition to build AGI. Unlike previous spins on the theme, Deepseek’s mission statement does not mention safety, competition, or stakes for humanity, but only “unraveling the mystery of AGI with curiosity”. Accordingly, the lab has been laser-focused on research into potentially game-changing architectural and algorithmic innovations.</p><p><span>Deepseek has delivered a series of impressive technical breakthroughs. Before R1-Lite-Preview, there had been a longer track record of wins: </span><a href="https://arxiv.org/abs/2401.06066" rel="">architectural improvements</a><span> like multi-head latent attention (MLA) and sparse mixture-of-experts (DeepseekMoE) had reduced inference costs so much as to trigger a price war among Chinese developers. Meanwhile, Deepseek’s </span><a href="https://arxiv.org/abs/2406.11931" rel="">coding model</a><span> trained on these architectures outperformed open weights rivals like July’s GPT4-Turbo.</span></p><p>As a first step to understanding what’s in the water at Deepseek, we’ve translated a rare, in-depth interview with CEO Liang Wenfeng, originally published this past July on a 36Kr sub-brand. It contains some deep insights into:</p><ul><li><p>How DeepSeek’s ambitions for AGI flow through their research strategy</p></li><li><p>Why it views open source as the dominant strategy and why it ignited a price war</p></li><li><p>How he hires and organizes researchers to leverage young domestic talent far better than other labs that have splurged on returnees</p></li><li><p>Why Chinese firms settle for copying and commercialization instead of “hardcore innovation” and how Liang hopes Deepseek will ignite more “hardcore innovation” across the Chinese economy.  </p></li></ul><p><a href="https://mp.weixin.qq.com/s/r9zZaEgqAa_lml_fOEZmjg" rel="">Wechat</a><span>, </span><a href="https://archive.is/JnE4j" rel="">Archive link</a><span>. Text | Lily Yu 于丽丽. Editor | Liu Jing 刘旌.</span></p><p>Of China’s seven large-model startups, DeepSeek has been the most discreet — yet it consistently manages to be memorable in unexpected ways.</p><p>A year ago, this unexpectedness came from its backing by High-Flyer 幻方, a quantitative hedge fund powerhouse, making it the only non-big tech giant with a reserve of 10,000 A100 chips. A year later, it became known as the catalyst for China’s AI model price war. A year later, it became known as the catalyst for China’s AI model price war.</p><p>In May, amid continuous AI developments, DeepSeek suddenly rose to prominence. The reason was that they released an open-source model called DeepSeek V2, which offered an unprecedented price/performance ratio: inference costs were reduced to only 1 RMB per million tokens, which is about one-seventh of the cost of Llama3 70B and one-seventieth of the cost of GPT-4 Turbo.</p><p>DeepSeek was quickly dubbed the “Pinduoduo of AI,” and other major tech giants such as ByteDance, Tencent, Baidu, and Alibaba couldn’t hold back, cutting their prices one after another. A price war for large models in China was imminent.</p><p><strong>This diffuse smoke of war actually concealed one fact: unlike many big companies burning money on subsidies, DeepSeek is profitable.</strong></p><p><span>​​This success stems from DeepSeek’s comprehensive innovation in model architecture. They proposed a novel MLA (</span><strong>multi-head latent attention</strong><span>) architecture that reduces memory usage to 5-13% of the commonly used MHA architecture. Additionally, their original DeepSeekMoESparse structure minimized computational costs, ultimately leading to reduced overall costs.</span></p><p>In Silicon Valley, DeepSeek is known as “the mysterious force from the East” 来自东方的神秘力量. SemiAnalysis’s chief analyst believes the DeepSeek V2 paper “may be the best one of the year.” Former OpenAI employee Andrew Carr found the paper “full of amazing wisdom” 充满惊人智慧, and applied its training setup to his own models. And Jack Clark, former policy head at OpenAI and co-founder of Anthropic, believes DeepSeek “hired a group of unfathomable geniuses” 雇佣了一批高深莫测的奇才, adding that large models made in China “will be as much of a force to be reckoned with as drones and electric cars” 将和无人机、电动汽车一样，成为不容忽视的力量.</p><p><strong>In the AI ​​wave — where the story is largely driven by Silicon Valley — this is a rare occurrence. </strong><span>Several industry insiders told us that </span><strong>this strong response stems from innovation at the architectural level, a rare attempt by domestic large model companies and even global open-source large-scale models</strong><span>. One AI researcher said that the Attention architecture has hardly been successfully modified, let alone validated on a large scale, in the years since it was proposed. “It’s an idea that would be shut down at the decision-making stage because most people lack confidence” 这甚至是一个做决策时就会被掐断的念头，因为大部分人都缺乏信心.</span></p><p><span>On the other hand, large domestic models have rarely dabbled in innovation at the architectural level before, partly due to a prevailing belief that </span><strong>Americans excel at 0-to-1 technical innovation, while Chinese excel at 1-to-10 application innovation</strong><span>. Moreover, this kind of behavior is very unprofitable — after all, a new generation of models will inevitably emerge after a few months, so Chinese companies need only follow along and focus on downstream applications. Innovating the model architecture means that there is no path to follow, meaning multiple failures and substantial time and economic costs.</span></p><p>DeepSeek is clearly going against the grain. Amid the clamor that large-model technology is bound to converge and following is a smarter shortcut, DeepSeek values the learning accumulated through “detours” 弯路, and believes that Chinese large-model entrepreneurs can join the global technological innovation stream beyond just application innovation.</p><p>Many of DeepSeek’s choices differ from the norm. Until now, among the seven major Chinese large-model startups, it’s the only one that has given up the “want it all” 既要又要 approach, so far focusing on only research and technology, without the toC applications. It’s also the only one that hasn’t fully considered commercialization, firmly choosing the open-source route without even raising capital. While these choices often leave it in obscurity, DeepSeek frequently gains organic user promotion within the community.</p><p>How did DeepSeek achieve this all? We interviewed DeepSeek’s seldom-seen founder, Liang Wenfeng 梁文锋, to find out.</p><p>The post-80s founder, who has been working behind the scenes on technology since the High-Flyer era, continues his low-key style in the DeepSeek era — “reading papers, writing code, and participating in group discussions” 看论文，写代码，参与小组讨论 every day, just like every other researcher does.</p><p>And unlike many quant fund founders — who have overseas hedge-fund experience and physics or mathematics degrees — Liang Wenfeng has always maintained a local background: in his early years, he studied artificial intelligence at Zhejiang University’s Department of Electrical Engineering.</p><p>Multiple industry insiders and DeepSeek researchers told us that Liang Wenfeng is a very rare person in China’s AI industry — someone who has “both strong infra engineering and modeling capabilities, as well as the ability to mobilize resources” he “can make accurate, high-level judgments, while also remaining stronger than first-line researchers in the details”. He has a “terrifying ability to learn”, and at the same time, he is “not at all like a boss and much more like a geek.”</p><p><span>This is a particularly rare interview. Here, this technological idealist provides a voice that is especially scarce in China’s tech world: </span><strong>he is one of the few who puts “right and wrong” before “profits and losses”</strong><span> 把“是非观”置于“利害观”之前, </span><strong>who reminds us to see the inertia of the times, and who puts “original innovation”</strong><span> 原创式创新 </span><strong>at the top of the agenda</strong><span>.</span></p><p><span>A year ago, when DeepSeek first came off the market, we interviewed Liang Wenfeng: “</span><a href="https://mp.weixin.qq.com/s/fpnmf5W1rr6qTIQjbf9aCg" rel="">Crazy High-Flyer: A Stealth AI Giant’s Road to Large Models</a><span>” </span><a href="https://archive.is/OoId6" rel="">疯狂的幻方：一家隐形AI巨头的大模型之路</a><span>. If the phrase “be insanely ambitious and insanely sincere” 务必要疯狂地怀抱雄心，且还要疯狂地真诚 was merely a beautiful slogan back then, a year later, it has become action.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg" width="1266" height="705" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:705,&quot;width&quot;:1266,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F883b237c-dba3-49db-a6d3-85cd8cdde305_1266x705.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Waves: After DeepSeek V2’s release, it quickly triggered a fierce price war in the large-model market. Some say you’ve become the industry’s catfish.</p><p><span>Liang Wenfeng: We didn’t mean to become a catfish — we just accidentally became a catfish. [</span><em>Translator’s note: This is likely a reference to Wong Kar-wai’s new tv show </em><span>王家卫</span><em> </em><span>“Blossoms Shanghai” 繁花</span><em>, where catfish are symbolic of market disruptors due to their cannibalistic nature.</em><span>]</span></p><p>Waves: Was this outcome a surprise to you?</p><p>Liang Wenfeng: Very surprising. We didn’t expect pricing to be so sensitive to everyone. We were just doing things at our own pace and then accounted for and set the price. Our principle is that we don’t subsidize nor make exorbitant profits. This price point gives us just a small profit margin above costs.</p><p>Waves: Zhipu AI 智谱AI followed suit five days later, followed by ByteDance, Alibaba, Baidu, Tencent, and other big players.</p><p>Liang Wenfeng: Zhipu AI reduced the price of an entry-level product, while their models comparable to ours remained expensive. ByteDance was truly the first to follow, reducing its flagship model to match our price, which then triggered other tech giants to cut prices. Since big companies’ model costs are much higher than ours, we never expected anyone would do this at a loss, but it eventually turned into the familiar subsidy-burning logic of the internet era.</p><p>Waves: From the outside, price cuts look a lot like bids for users, which is usually the case in internet-era price wars.</p><p>Liang Wenfeng: Poaching users is not our main purpose. We cut prices because, on the one hand, our costs decreased while exploring next-generation model architectures, and on the other hand, we also feel that both APIs and AI should be accessible and affordable to everyone.</p><p>Waves: Before this, most Chinese companies would directly copy the current generation’s Llama architecture for applications. Why did you start from the model structure?</p><p>Liang Wenfeng: If the goal is to make applications, using the Llama structure for quick product deployment is reasonable. But our destination is AGI, which means we need to study new model structures to realize stronger model capability with limited resources. This is one of the fundamental research areas needed for scaling up to larger models. And beyond model structure, we’ve done extensive research in other areas, including data construction and making models more human-like — which are all reflected in the models we released. In addition, Llama’s structure, in terms of training efficiency and inference cost, is estimated to have a two-generation gap behind international frontier levels in training efficiency and inference costs.</p><p>Waves: Where does this generation gap mainly come from?</p><p>Liang Wenfeng: First of all, there’s a training efficiency gap. We estimate that compared to the best international levels, China’s best capabilities might have a twofold gap in model structure and training dynamics — meaning we have to consume twice the computing power to achieve the same results. In addition, there may also be a twofold gap in data efficiency, that is, we have to consume twice the training data and computing power to achieve the same results. Combined, that’s four times more computing power needed. What we’re trying to do is to keep closing these gaps.</p><p>Waves: Most Chinese companies choose to have both models and applications. Why has DeepSeek chosen to focus on only research and exploration?</p><p>Liang Wenfeng: Because we believe the most important thing now is to participate in the global innovation wave. For many years, Chinese companies are used to others doing technological innovation, while we focused on application monetization — but this isn’t inevitable. In this wave, our starting point is not to take advantage of the opportunity to make a quick profit, but rather to reach the technical frontier and drive the development of the entire ecosystem.</p><p>Waves: The Internet and mobile Internet eras left most people with the belief that the United States excels at technological innovation, while China excels at making applications.</p><p><span>Liang Wenfeng: We believe that as the economy develops, </span><strong>China should gradually become a contributor instead of freeriding</strong><span>. In the past 30+ years of the IT wave, we basically didn’t participate in real technological innovation. </span><strong>We’re used to Moore’s Law falling out of the sky, lying at home waiting 18 months for better hardware and software to emerge. That’s how the Scaling Law is being treated</strong><span>.</span></p><p><strong>But in fact, this is something that has</strong><span> </span><strong>been created through the tireless efforts of generations of Western-led tech communities. It’s just because we weren’t previously involved in this process that we’ve ignored its existence.</strong></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png" width="166" height="135.8849557522124" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b41161e2-7139-4c15-84ba-0fff53b18561_452x370.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:370,&quot;width&quot;:452,&quot;resizeWidth&quot;:166,&quot;bytes&quot;:99155,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb41161e2-7139-4c15-84ba-0fff53b18561_452x370.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Waves: Why did DeepSeek V2 surprise so many people in Silicon Valley?</p><p>Liang Wenfeng: Among the numerous innovations happening daily in the United States, this is quite ordinary. They were surprised because it was a Chinese company joining their game as an innovation contributor. After all, most Chinese companies are used to following, not innovating.</p><p>Waves: But choosing to innovate in the Chinese context is a very extravagant decision. Large models are a heavy investment game, and not all companies have the capital to solely research and innovate instead of thinking about commercialization first.</p><p><span>Liang Wenfeng: The cost of innovation is definitely not low, and past tendencies toward indiscriminate borrowing were also related to China’s previous conditions. But now you see, whether it’s China’s economic scale, or the profits of giants like ByteDance and Tencent — none of it is low by global standards. </span><strong>What we lack in innovation is definitely not capital, but a lack of confidence and knowledge of how to organize high-density talent for effective innovation.</strong></p><p>Waves: Why do Chinese companies — including the huge tech giants — default to rapid commercialization as their #1 priority?</p><p>Liang Wenfeng: In the past 30 years, we’ve emphasized only making money while neglecting innovation. Innovation isn’t entirely business-driven; it also requires curiosity and a desire to create. We’re just constrained by old habits, but this is tied to a particular economic phase.</p><p>Waves: But you’re ultimately a business organization, not a public-interest research institution — so where do you build your moat when you choose to innovate and then open source your innovations? Won’t the MLA architecture you released in May be quickly copied by others?</p><p><span>Liang Wenfeng: I</span><strong>n the face of disruptive technologies, moats created by closed source are temporary. Even OpenAI’s closed source approach can’t prevent others from catching up</strong><span>. S</span><strong>o we anchor our value in our team — our colleagues grow through this process, accumulate know-how, and form an organization and culture capable of innovation. That’s our moat.</strong></p><p>Open source, publishing papers, in fact, do not cost us anything. For technical talent, having others follow your innovation gives a great sense of accomplishment. In fact, open source is more of a cultural behavior than a commercial one, and contributing to it earns us respect. There is also a cultural attraction for a company to do this.</p><p>Waves: What do you think of those who believe in the market, like [GSR Ventures’[ Zhu Xiaohu 朱啸虎?</p><p>Liang Wenfeng: Zhu Xiaohu is logically consistent, but his style of play is more suitable for fast money-making companies. And if you look at America’s most profitable companies, they’re all high-tech companies that accumulated deep technical foundations before making major breakthroughs.</p><p>Waves: But when it comes to large models, pure technical leadership rarely forms an absolute advantage. What bigger thing are you betting on?</p><p><span>Liang Wenfeng: </span><strong>What we see is that Chinese AI can’t be in the position of following forever. We often say that there is a gap of one or two years between Chinese AI and the United States, but the real gap is the difference between originality and imitation. If this doesn’t change, China will always be only a follower — so some exploration is inescapable.</strong></p><p>Nvidia’s leadership isn’t just the effort of one company, but the result of the entire Western technical community and industry working together. They see the next generation of technology trends and have a roadmap in hand. Chinese AI development needs such an ecosystem. Many domestic chip developments struggle because they lack supporting technical communities and have only second-hand information. China inevitably needs people to stand at the technical frontier.</p><p>Waves: DeepSeek, right now, has a kind of idealistic aura reminiscent of the early days of OpenAI, and it’s open source. Will you change to closed source later on? Both OpenAI and Mistral moved from open-source to closed-source.</p><p>Liang Wenfeng: We will not change to closed source. We believe having a strong technical ecosystem first is more important.</p><p>Waves: Do you have a financing plan? I’ve seen media reports saying that High-Flyer plans to spin off DeepSeek for an IPO. AI startups in Silicon Valley inevitably end up binding themselves to major firms.</p><p><span>Liang Wenfeng: We do not have financing plans in the short term. </span><strong>Money has never been the problem for us; bans on shipments of advanced chips are the problem.</strong></p><p>Waves: Many people believe that developing AGI and quantitative finance are completely different endeavors. Quantitative finance can be pursued quietly, but AGI may require a high-profile and bold approach, forming alliances to amplify your investments.</p><p>Liang Wenfeng: More investments do not equal more innovation. Otherwise, big firms would’ve monopolized all innovation already.</p><p>Waves: Are you not focusing on applications right now because you lack the operational expertise?</p><p>Liang Wenfeng: We believe the current stage is a period of explosive growth in technological innovation, not in applications. In the long run, we hope to create an ecosystem where the industry directly utilizes our technology and outputs. Our focus will remain on foundational models and cutting-edge innovation, while other companies can build B2B and B2C businesses based on DeepSeek’s foundation. If a complete industry value chain can be established, there’s no need for us to develop applications ourselves. Of course, if needed, nothing stops us from working on applications, but research and technological innovation will always be our top priority.</p><p>Waves: But when customers are choosing APIs, why should they choose DeepSeek over offerings from bigger firms?</p><p>Liang Wenfeng: The future world is likely to be one of specialized division of labor. Foundational large models require continuous innovation, and large companies have limits on their capabilities, which may not necessarily make them the best fit.</p><p>Waves: But can technology itself really create a significant gap? You’ve also mentioned that there are no absolute technological secrets.</p><p>Liang Wenfeng: There are no secrets in technology, but replication requires time and cost. Nvidia’s graphics cards, theoretically, have no technological secrets and are easy to replicate. However, building a team from scratch and catching up with the next generation of technology takes time, so the actual moat remains quite wide.</p><p>Waves: Once DeepSeek lowered its prices, ByteDance followed suit, which shows that they feel a certain level of threat. How do you view new approaches to competition between startups and big firms?</p><p><strong>Liang Wenfeng: Honestly, we don’t really care, because it was just something we did along the way. Providing cloud services isn’t our main goal. Our ultimate goal is still to achieve AGI.</strong></p><p><span>Right now I don’t see any new approaches, but big firms do not have a clear upper hand. </span><strong>Big firms have existing customers, but their cash-flow businesses are also their burden, and this makes them vulnerable to disruption at any time.</strong></p><p>Waves: What do you see as the end game of the six other large-model startups?</p><p>Liang Wenfeng: Two or three may survive. All of them are in the “burning-money” phase right now, so those with a clear self-positioning and better refinement of operations have a higher chance of making it. Other companies might undergo significant transformations. Things of value won’t simply disappear but will instead take on a different form.</p><p>Waves: High-Flyer’s approach to competition has been described as “impervious,” as it pays little attention to horizontal competition. What’s your starting point when it comes to thinking about competition?</p><p>Liang Wenfeng: What I often think about is whether something can improve the efficiency of society’s operations, and whether you can find a point of strength within its industrial chain. As long as the ultimate goal is to make society more efficient, it’s valid. Many things in between are just temporary phases, and overly focusing on them can lead to confusion.</p><p><span>Waves: Jack Clark, former policy director at OpenAI and co-founder of Anthropic, said that DeepSeek hired </span><a href="https://importai.substack.com/p/import-ai-372-gibberish-jailbreak" rel="">“inscrutable wizards.”</a><span> What kind of people are behind DeepSeek V2?</span></p><p><strong>Liang Wenfeng: There are no wizards. We are mostly fresh graduates from top universities, PhD candidates in their fourth or fifth year, and some young people who graduated just a few years ago.</strong></p><p>Waves: Many LLM companies are obsessed with recruiting talents from overseas, and it’s often said that the top 50 talents in this field might not even be working for Chinese companies. Where are your team members from?</p><p><strong>Liang Wenfeng: The team behind the V2 model doesn’t include anyone returning to China from overseas — they are all local. The top 50 experts might not be in China, but perhaps we can train such talents ourselves.</strong></p><p><strong>Waves: How did this MLA innovation come about? I heard the idea originated from the personal interest of a young researcher?</strong></p><p><strong>Liang Wenfeng: After summarizing some mainstream evolutionary trends of the attention mechanism, he just thought to design an alternative. However, turning the idea into reality was a lengthy process. We formed a team specifically for this and spent months getting it to work. [</strong><em><span>Jordan: really reminiscent of how </span><a href="https://aibusiness.com/nlp/sxsw-23-openai-co-founder-shares-the-story-behind-chatgpt" rel="">Alec Radford’s early contribution to the GPT series</a><span> and speaks to the broader thesis we’ve argued in the past on ChinaTalk that algorithmic innovation is fundamentally different from pushing the technological frontier in something like semiconductor fabrication. Instead of needing a PhD and years of industry experience to really be useful, you can push the frontier by being a really sharp and hungry 20something (of which China has many!). Dwarkesh’s interview with OpenAI </span><a href="https://www.dwarkeshpatel.com/p/sholto-douglas-trenton-bricken" rel="">Sholto Douglass and Anthropic’s Trenton Bricken</a><span> illustrates this dynamic well. Dwarkesh opens with the ine “Noam Brown, who wrote the Diplomacy paper, said this about Sholto: “he's only been in the field for 1.5 years, but people in AI know that he was one of the most important people behind Gemini's success.”</span></em><strong>]</strong></p><p>Waves: The emergence of such divergent thinking seems closely related to your innovation-driven organizational structure. Back in the High-Flyer era, your team rarely assigned goals or tasks from the top down. But AGI involves frontier exploration with much uncertainty — has that led to more management intervention?</p><p><strong>Liang Wenfeng: DeepSeek is still entirely bottom-up. We generally don’t predefine roles; instead, the division of labor occurs naturally. Everyone has their own unique journey, and they bring ideas with them, so there’s no need to push anyone. While we explore, if someone sees a problem, they will naturally discuss it with someone else. However, if an idea shows potential, we do allocate resources top-down.</strong></p><p>Waves: I heard that DeepSeek is very flexible in mobilizing resources like GPUs and people.</p><p><strong>Liang Wenfeng: Anyone on the team can access GPUs or people at any time. If someone has an idea, they can access the training cluster cards anytime without approval. Similarly, since we don’t have hierarchies or separate departments, people can collaborate across teams, as long as there’s mutual interest.</strong></p><p>Waves: Such a loose management style relies on having highly self-driven people. I heard you excel at identifying exceptional talent through non-traditional evaluation criteria.</p><p><span>Liang Wenfeng: </span><strong>Our hiring standard has always been passion and curiosity. Many of our team members have unusual experiences, and that is very interesting. Their desire to do research often comes before making money.</strong></p><p>Waves: Transformers was born at Google’s AI Lab, and ChatGPT at OpenAI. How do you compare the value of innovations at big companies’ AI labs versus startups?</p><p>Liang Wenfeng: Google’s AI Lab, OpenAI, and even Chinese tech companies’ AI labs are all immensely valuable. The fact that OpenAI succeeded was partly due to a few historical coincidences.</p><p>Waves: So, is innovation largely a matter of luck? I noticed that the middle row of meeting rooms in your office has doors on both sides that anyone can open. Your colleagues said that this design leaves room for serendipity. The creation of transformers involved someone overhearing a discussion and joining, ultimately turning it into a general framework.</p><p><span>Liang Wenfeng: I believe innovation starts with believing. </span><strong>Why is Silicon Valley so innovative? Because they dare to do things. When ChatGPT came out, the tech community in China lacked confidence in frontier innovation. From investors to big tech, they all thought that the gap was too big and opted to focus on applications instead. But innovation starts with confidence, which we often see more from young people.</strong></p><p>Waves: But you don’t fundraise or even speak to the public, so your visibility is lower than those companies actively fundraising. How do you ensure DeepSeek remains the top choice for those working on LLMs?</p><p><span>Liang Wenfeng: Because we’re tackling the hardest problems. Top talents are most drawn to solving the world’s toughest challenges. In fact, </span><strong>top talents in China are underestimated because there’s so little hardcore innovation happening at the societal level, leaving them unrecognized. We’re addressing the hardest problems, which makes us inherently attractive to them.</strong></p><p>Waves: When OpenAI’s latest release didn’t bring us GPT5, many people feel that this indicates technological progress is slowing and are starting to question the Scaling Law. What do you think?</p><p><span>Liang Wenfeng: We’re relatively optimistic. Our industry as a whole seems to be meeting expectations. </span><strong>OpenAI is not a god (OpenAI不是神), they won’t necessarily always be at the forefront.</strong></p><p>Waves: How long until AGI is realized? Before releasing DeepSeek V2, you had models for math and code generation and also switched from dense models to Mixture of Experts. What are the key points on your AGI roadmap?</p><p>Liang Wenfeng: It could be two, five, or ten years–in any case, it will happen in our lifetimes. There’s no unified opinion on a roadmap even within our company. That said, we’ve taken real bets on three directions. First is mathematics and code, second multimodality, and third natural language itself.</p><p>Mathematics and code are natural AGI testing grounds, somewhat like Go. They’re closed, verifiable systems where high levels of intelligence can be self-taught. Multimodality and engagement with the real human world, on the other hand, might also be a requirement for AGI. We remain open to different possibilities.</p><p>Waves: What do you think is the end game for large models?</p><p>Liang Wenfeng: There will be specialized companies providing foundation models and services, achieving extensive specialization in every node of the supply chain. More people will build on top of all of this to meet society’s diverse needs.</p><p>Waves: Over the past year, there have been many changes in China's large model startups. For example, Wang Huiwen [co-founder of RenRen, a facebook clone, and Meituan, a food delivery company], who was very active at the beginning of last year, withdrew midway, and companies that joined later began to show differentiation.</p><p><span>Liang Wenfeng: Wang Huiwen bore all the losses himself, allowing others to withdraw unscathed. He made a choice that was worst for himself but good for everyone else, so he's very decent in his conduct - this is something I really admire. [</span><em><span>Wang Huiyuan founded foundation model company 光年之外 Lightyear only to quickly fold it back into Meituan. For more on Meituan and AI, </span><a href="https://36kr.com/p/3053948838351233" rel="">see this recent 36Kr feature</a></em><span>].</span></p><p>Waves: Where are you focusing most of your energy now?</p><p>Liang Wenfeng: My main energy is focused on researching the next generation of large models. There are still many unsolved problems.</p><p>Waves: Other large model startups are insisting on pursuing both [technology and commercialization], after all, technology won't bring permanent leadership as it's also important to capitalize on a window of opportunity to translate technological advantages into products. Is DeepSeek daring to focus on model research because its model capabilities aren't sufficient yet?</p><p>Liang Wenfeng: All these business patterns are products of the previous generation and may not hold true in the future. Using Internet business logic to discuss future AI profit models is like discussing General Electric and Coca-Cola when Pony Ma was starting his business. It’s a pointless exercise (刻舟求剑).</p><p>Waves: In the past, your quant fund High-Flyer had a strong foundation in technology and innovation, and its growth was relatively smooth. Is this the reason for your optimism?</p><p>Liang Wenfeng: In some ways, High-Flyer strengthened our confidence in technology-driven innovation, but it wasn't all smooth sailing. We went through a long accumulation process. What outsiders see is the part of High-Flyer after 2015, but in fact, we've been at it for 16 years.</p><p>Waves: Returning to the topic of innovation. Now that the economy is starting to decline and capital is no longer as loose as it was, will this suppress basic research?</p><p>Liang Wenfeng: I don't necessarily think so. The adjustment of China's industrial structure will necessarily rely more on hardcore technological innovation. When people realize that making quick money in the past was likely due to lucky windows, they'll be more willing to humble themselves and engage in genuine innovation.</p><p>An Yong: So you're optimistic about this as well?</p><p>Liang Wenfeng: I grew up in the 1980s in a fifth-tier city in Guangdong. My father was a primary school teacher. In the 1990s, there were many opportunities to make money in Guangdong. At that time, many parents came to my home; basically, they thought studying was useless. But looking back now, they’ve all changed their views. Because making money isn't easy anymore—even the opportunity to drive a taxi might be gone soon. It’s only taken one generation.</p><p>In the future, hardcore innovation will become increasingly common. It’s not easy to understand right now, because society as a whole needs to be educated on this point. Once society allows people dedicated to hardcore innovation to achieve fame and fortune, then our collective mindset will adapt. We just need some examples and a process</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. Army Soldier Arrested in AT&T, Verizon Extortions (276 pts)]]></title>
            <link>https://krebsonsecurity.com/2024/12/u-s-army-soldier-arrested-in-att-verizon-extortions/</link>
            <guid>42557342</guid>
            <pubDate>Tue, 31 Dec 2024 08:24:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://krebsonsecurity.com/2024/12/u-s-army-soldier-arrested-in-att-verizon-extortions/">https://krebsonsecurity.com/2024/12/u-s-army-soldier-arrested-in-att-verizon-extortions/</a>, See on <a href="https://news.ycombinator.com/item?id=42557342">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
												<p>Federal authorities have arrested and indicted a 20-year-old U.S. Army soldier on suspicion of being <strong>Kiberphant0m</strong>, a cybercriminal who has been selling and leaking sensitive customer call records stolen earlier this year from <strong>AT&amp;T</strong> and <strong>Verizon</strong>. As first reported by KrebsOnSecurity last month, the accused is a communications specialist who was recently stationed in South Korea.</p>
<div id="attachment_69974"><p><img aria-describedby="caption-attachment-69974" decoding="async" src="https://krebsonsecurity.com/wp-content/uploads/2024/12/camwagenius-selfie.png" alt="" width="750" height="743" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/12/camwagenius-selfie.png 779w, https://krebsonsecurity.com/wp-content/uploads/2024/12/camwagenius-selfie-768x761.png 768w" sizes="(max-width: 750px) 100vw, 750px"></p><p id="caption-attachment-69974">One of several selfies on the Facebook page of Cameron Wagenius.</p></div>
<p><strong>Cameron John Wagenius</strong>&nbsp;was arrested near the Army base in Fort Hood, Texas on Dec. 20, after being indicted on two criminal counts of unlawful transfer of confidential phone records.</p>
<p>The sparse, <a href="https://krebsonsecurity.com/wp-content/uploads/2024/12/wagenius-indictment.pdf" target="_blank" rel="noopener">two-page indictment</a> (PDF) doesn’t reference specific victims or hacking activity, nor does it include any personal details about the accused. But a conversation with Wagenius’ mother — Minnesota native <strong>Alicia Roen </strong>—&nbsp;filled in the gaps.</p>
<p>Roen said that prior to her son’s arrest he’d acknowledged being associated with <strong>Connor Riley Moucka</strong>, a.k.a. “<strong>Judische</strong>,” a prolific cybercriminal from Canada who was <a href="https://krebsonsecurity.com/2024/11/canadian-man-arrested-in-snowflake-data-extortions/" target="_blank" rel="noopener">arrested in late October</a> for stealing data from and extorting dozens of companies that stored data at the cloud service <strong>Snowflake</strong>.</p>
<p>In an interview with KrebsOnSecurity, Judische said he had no interest in selling the data he’d stolen from Snowflake customers and telecom providers, and that he preferred to outsource that to Kiberphant0m and others. Meanwhile, Kiberphant0m claimed in posts on Telegram that he was responsible for hacking into at least 15 telecommunications firms, including AT&amp;T and Verizon.</p>
<p>On November 26, KrebsOnSecurity <a href="https://krebsonsecurity.com/2024/11/hacker-in-snowflake-extortions-may-be-a-u-s-soldier/" target="_blank" rel="noopener">published a story</a> that followed a trail of clues left behind by Kiberphantom indicating he was a U.S. Army soldier stationed in South Korea.</p>
<div id="attachment_69971"><p><img aria-describedby="caption-attachment-69971" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/12/cwag-army.png" alt="" width="751" height="641" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/12/cwag-army.png 918w, https://krebsonsecurity.com/wp-content/uploads/2024/12/cwag-army-768x655.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/12/cwag-army-782x667.png 782w" sizes="(max-width: 751px) 100vw, 751px"></p><p id="caption-attachment-69971">An 18-year-old Cameron Wagenius, joining the U.S. Army.</p></div>
<p>Ms. Roen said Cameron worked on radio signals and network communications at an Army base in South Korea for the past two years, returning to the United States periodically. She said Cameron was always good with computers, but that she had no idea he might have been involved in criminal hacking.</p>
<p>“I never was aware he was into hacking,” Roen said. “It was definitely a shock to me when we found this stuff out.”</p>
<p>Ms. Roen said Cameron joined the Army as soon as he was of age, following in his older brother’s footsteps.</p>
<p>“He and his brother when they were like 6 and 7 years old would ask for MREs from other countries,” she recalled, referring to military-issued “meals ready to eat” food rations. “They both always wanted to be in the Army. I’m not sure where things went wrong.”<span id="more-69925"></span></p>
<p>Immediately after news broke of Moucka’s arrest, Kiberphant0m posted on the hacker community <strong>BreachForums</strong>&nbsp;what they claimed were the AT&amp;T call logs for&nbsp;<strong>President-elect</strong>&nbsp;<strong>Donald J. Trump</strong>&nbsp;and for&nbsp;<strong>Vice President Kamala Harris</strong>.</p>
<p>“In the event you do not reach out to us @ATNT all presidential government call logs will be leaked,” Kiberphant0m threatened, signing their post with multiple “#FREEWAIFU” tags. “You don’t think we don’t have plans in the event of an arrest? Think again.”</p>
<div id="attachment_69624"><p><img aria-describedby="caption-attachment-69624" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/11/kiberphant0m-nsa-schema.png" alt="" width="750" height="239" srcset="https://krebsonsecurity.com/wp-content/uploads/2024/11/kiberphant0m-nsa-schema.png 1417w, https://krebsonsecurity.com/wp-content/uploads/2024/11/kiberphant0m-nsa-schema-768x245.png 768w, https://krebsonsecurity.com/wp-content/uploads/2024/11/kiberphant0m-nsa-schema-782x249.png 782w" sizes="(max-width: 750px) 100vw, 750px"></p><p id="caption-attachment-69624">Kiberphant0m posting what he claimed was a “data schema” stolen from the NSA via AT&amp;T.</p></div>
<p>On that same day, Kiberphant0m posted what they claimed was the “data schema” from the <strong>U.S. National Security Agency</strong>.</p>
<p>On Nov. 5, Kiberphant0m offered call logs stolen from Verizon’s push-to-talk (PTT) customers — mainly U.S. government agencies and emergency first responders. On Nov. 9, Kiberphant0m posted a sales thread on BreachForums offering a “SIM-swapping” service targeting Verizon PTT customers. In a SIM-swap, fraudsters use credentials that are phished or stolen from mobile phone company employees to divert a target’s phone calls and text messages to a device they control.</p>
<p>The profile photo on Wagenius’ Facebook page was deleted within hours of my Nov. 26 story identifying Kiberphant0m as a likely U.S. Army soldier. Still, many of his original profile photos remain, including several that show Wagenius in uniform while holding various Army-issued weapons.</p>
<div id="attachment_69972"><p><img aria-describedby="caption-attachment-69972" decoding="async" loading="lazy" src="https://krebsonsecurity.com/wp-content/uploads/2024/12/wagenius-fb.png" alt="" width="602" height="842"></p><p id="caption-attachment-69972">Several profile photos visible on the Facebook page of Cameron Wagenius.</p></div>
<p>November’s story on Kiberphant0m cited his own Telegram messages saying he maintained a large botnet that was used for distributed denial-of-service (DDoS) attacks to knock websites, users and networks offline. In 2023, Kiberphant0m sold remote access credentials for a major U.S. defense contractor.</p>
<p><strong>Allison Nixon,</strong>&nbsp;chief research officer at the New York-based cybersecurity firm <a href="https://www.unit221b.com/" target="_blank" rel="noopener">Unit 221B</a>, helped track down Kiberphant0m’s real life identity. Nixon was among several security researchers who faced harassment and specific threats of violence from Judische and his associates.</p>
<p>“Anonymously extorting the President and VP as a member of the military is a bad idea, but it’s an even worse idea to harass people who specialize in de-anonymizing cybercriminals,” Nixon told KrebsOnSecurity. She said&nbsp;the investigation into Kiberphant0m shows that law enforcement is getting better and faster at going after cybercriminals — especially those who are actually living in the United States.</p>
<p>“Between when we, and an anonymous colleague, found his opsec mistake on November 10th to his last Telegram activity on December 6, law enforcement set the speed record for the fastest turnaround time for an American federal cyber case that I have witnessed in my career,” she said.</p>
<p>Nixon asked to share a message for all the other Kiberphant0ms out there who think they can’t be found and arrested.</p>
<p>“I know that young people involved in cybercrime will read these articles,” Nixon said. “You need to stop doing stupid shit and get a lawyer. Law enforcement wants to put all of you in prison for a long time.”</p>
<p>The indictment against Wagenius was filed in Texas, but the case has been transferred to the U.S. District Court for the Western District of Washington in Seattle.</p>
											</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dog Aging Project (122 pts)]]></title>
            <link>https://dogagingproject.org/</link>
            <guid>42557276</guid>
            <pubDate>Tue, 31 Dec 2024 08:05:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dogagingproject.org/">https://dogagingproject.org/</a>, See on <a href="https://news.ycombinator.com/item?id=42557276">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[App Should Have Been a Website (and Probably Your Game Too) (203 pts)]]></title>
            <link>https://rogueengine.io/blog/your-app-should-have-been-a-website</link>
            <guid>42557172</guid>
            <pubDate>Tue, 31 Dec 2024 07:38:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rogueengine.io/blog/your-app-should-have-been-a-website">https://rogueengine.io/blog/your-app-should-have-been-a-website</a>, See on <a href="https://news.ycombinator.com/item?id=42557172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<article>
	
  
	<p>The smartphone boom changed everything. Suddenly, apps were everywhere, connecting people, solving problems, and entertaining us. But for a while now, they’ve started to feel more like a burden than a blessing. In today’s web-first world, most native apps feel redundant, cluttering our phones unnecessarily. With how far modern web technologies have come, it’s time to rethink if we really need them.</p>
<p>This isn’t just a guess—it’s already happening. Here’s why.</p>
<h3>The Problem with Native Apps</h3>
<h4>Too Many Apps, Too Little Reason</h4>
<p>Remember when every company rushed to make an app? Airlines, restaurants, even your local coffee shop. Back then, it made some sense. Browsers weren’t as powerful, and apps had unique features like notifications and offline access. But fast-forward to today, and browsers can do all that. Yet businesses still push native apps as if it’s 2010, and we’re left downloading apps for things that should just work on the web.</p>
<p>My wife recently had a rant about the Italki website, which forced her to download their app just to accept class requests on mobile. Frustrated by the app clutter and the company’s behavior, she downloaded the app, accepted the request, then promptly deleted it. She’s so fed up with unnecessary apps that she keeps only five on her phone. It’s one thing to hear complaints like this from someone in tech, but coming from a regular user like my wife, it’s clear that the app overload problem has gone completely out of hand.</p>
<h4>Costly, Clunky, and Cluttered</h4>
<p>Native apps are a pain for everyone involved. Developers pay hefty app store fees, jump through approval hoops, and juggle multiple platform versions. Users? We’re stuck with constant updates, wasted storage space, and apps that don’t even work on all our devices.</p>
<p>And don’t let anyone tell you the App Store solves discoverability. Apps aren’t “discovered” there; they’re marketed through ads, SEO, or word-of-mouth—just like websites. So why go through the extra hassle?</p>
<h4>Specialized, But Rarely Essential</h4>
<p>Yes, native apps still have their place, like video/image editing where it’s better for working with local files. But these are niche use cases. For most situations, web apps have caught up. They’re faster, more flexible, and work seamlessly across devices. Native apps? Not so much.</p>
<h3>Why Web Apps Are the Future</h3>
<h4>Accessible Everywhere, All the Time</h4>
<p>Web apps can easily adapt to whatever device you’re on. A single responsive website can run on your desktop, phone, tablet, or even a VR headset. What’s even more, they can be updated on all of them simultaneously. That’s a level of flexibility that native apps can’t match.</p>
<h4>The Power of Modern Browsers</h4>
<p>Today’s browsers are powerhouses. Notifications? Check. Offline mode? Check. Secure payments? Yep, they’ve got that too. And with technologies like WebAssembly and WebGPU, web games are catching up to native-level performance. In some cases, they’re already there.</p>
<h3>The Gaming Frontier</h3>
<h4>All Caught Up</h4>
<p>If there’s one industry proving the web’s potential, it’s gaming. Thanks to HTML5, WebGL, and WebAssembly, browser games are catching up to native ones in ways we couldn’t have imagined just a few years ago. Meta’s Oculus browser already delivers web games that rival native apps in performance. And once WebGPU becomes standard, the differences will be practically invisible.</p>
<h4>Growing Popularity</h4>
<p>Platforms like <a href="https://poki.com/" rel="nofollow">Poki</a> and <a href="https://crazygames.com/" rel="nofollow">CrazyGames</a>, with a combined 95 million players a month, are leading the charge in, what I like to call, the Browser Games Renaissance. These platforms are already showing what’s possible with web gaming.</p>
<p>I recently caught up with Raf Mertens, founder and CEO of CrazyGames, who shared: <em>“Users don’t care about the technology; they want something fun and accessible. It should just work—and that includes instantly. At CrazyGames, we aim to deliver high-quality games in an accessible format. We’re attracting new game developers into the web space and converting mobile developers.”</em></p>
<p>Raf also highlighted how the synergy between tools like Rogue Engine and platforms like CrazyGames is essential for the web gaming ecosystem to thrive.</p>
<p>Even platforms that cater to multiple formats are seeing the browser’s dominance. In a recent chat with Leaf, the creator of <a href="https://itch.io/" rel="nofollow">itch.io</a>, he shared, <em>“Browser games make up about 50% of all games on the site. It’s a pretty big section and continually growing.”</em>.</p>
<p>One great example of a very successful web game, is <a href="https://narrow.one/" rel="nofollow">Narrow One</a> by <a href="https://www.pelicanparty.co/#home" rel="nofollow">Pelican Party</a>. This two-person game studio has been thriving for the past five years exclusively with web games. Jasper from Pelican Party shared how this model has worked perfectly for them and many other game studios. They’re living proof that browser games can provide a sustainable business model.</p>
<h4>Tooling Is Catching Up</h4>
<p>Web game development is evolving fast. <a href="https://threejs.org/" rel="nofollow">Three.js</a>, the leading programming library for 3D web content, is pushing beyond WebGL into WebGPU, and at <a href="https://rogueengine.io/">Rogue Engine</a>, we’re committed to being the go-to game engine for <a href="https://threejs.org/" rel="nofollow">Three.js</a> and the web. We focus on keeping the experience simple, web-first, and familiar for developers—especially those transitioning from Unity, thanks to our similar environments.</p>
<p>We’re also working closely with <a href="https://multisynq.io/" rel="nofollow">Multisynq</a> to integrate serverless networking with <a href="https://croquet.io/" rel="nofollow">Croquet</a>, through our <a href="https://github.com/BeardScript/RogueCroquet" rel="nofollow">Rogue Croquet</a> plugin. This is making it easier for developers to build connected, multiplayer web games, at a fraction of the price, without worrying about backend infrastructure. As technology advances, we’re committed to staying ahead, pushing the boundaries of what’s possible on the web, and making these innovations accessible to all developers.</p>
<h3>The Challenges Ahead</h3>
<h4>Browser UX Needs an Upgrade</h4>
<p>For web apps to dominate, browsers have to evolve. Mobile browsers, in particular, need smarter navigation and better use of screen space. It should feel like using your desktop or mobile home screen—unobtrusive, customizable, and flawless.</p>
<h4>App Stores Won’t Let Go Easily</h4>
<p>Let’s face it: Google, Apple, and other app store giants aren’t going to give up their cash cow without a fight. But as web apps continue to grow, their dominance will diminish. It’s not a question of if, but when.</p>
<h4>Bad Actors</h4>
<p>Some companies push for app installations because they gain access to more permissions than they would in the browser. Apps allow them to collect more data and track user activity, often under the pretense of a better experience. They’d much rather escape the safety of the browser where you get more control and transparency over these permissions.</p>
<h4>VCs Are Overlooking the Web Gaming Revolution</h4>
<p>The web game development space is on fire, but many VCs are still missing the point. They see the potential but are waiting for the “right” platform to show up before jumping in. What they don’t get is that this puts unnecessary pressure on tools like ours or <a href="https://multisynq.io/" rel="nofollow">Multisynq</a> and portals like <a href="https://poki.com/" rel="nofollow">Poki</a>, and <a href="https://crazygames.com/" rel="nofollow">CrazyGames</a>, who are already seeing huge success. VCs are sitting on the sidelines, ignoring the strength of the ecosystem and the clear demand for web games. The time to act is now, and those who hesitate will be left scrambling when the web gaming revolution takes off.</p>
<h3>The Web-Centric Future</h3>
<p>With WebGPU, WebAssembly, and WebXR, the web isn’t just catching up—it’s pulling ahead. Developers who embrace this shift now will be free from app store restrictions, ready to create experiences that work anywhere and everywhere.</p>
<p>Sure, a few niche native apps will stick around. But for the vast majority of games, tools, and everything in between, the web is the future. The app craze was fun while it lasted, but it’s time to get back to what the web was always meant to be: a universal platform for everyone. At the end of the day, most apps should’ve been websites all along.</p>
<p>Come join the party at <a href="https://rogueengine.io/">Rogue Engine</a>. Let’s make something awesome together!</p>
</article></div><div><p>Copyright © 2024 Rogue Engine</p></div></div>]]></description>
        </item>
    </channel>
</rss>