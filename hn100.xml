<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 29 Jan 2025 19:30:13 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[An analysis of DeepSeek's R1-Zero and R1 (124 pts)]]></title>
            <link>https://arcprize.org/blog/r1-zero-r1-results-analysis</link>
            <guid>42868390</guid>
            <pubDate>Wed, 29 Jan 2025 17:44:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arcprize.org/blog/r1-zero-r1-results-analysis">https://arcprize.org/blog/r1-zero-r1-results-analysis</a>, See on <a href="https://news.ycombinator.com/item?id=42868390">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
            <h2>ARC Prize remains undefeated.<br>New ideas still needed<span>.</span></h2>
        </p><div>
<h2 id="an-analysis-of-deepseeks-r1-zero-and-r1">An Analysis of DeepSeek's R1-Zero and R1</h2>
<h2 id="r1-zero-is-more-important-than-r1">R1-Zero is more important than R1</h2>

<blockquote>
  <p>Special thanks to <a href="https://x.com/tuhinone">Tuhin</a> and <a href="https://www.linkedin.com/in/abuqader/">Abu</a> from <a href="https://www.baseten.co/">Baseten</a> and <a href="https://x.com/yuchenj_uw">Yuchen</a> from <a href="https://hyperbolic.xyz/">Hyperbolic Labs</a> for hosting r1-zero for us. Hardly any providers are hosting this model variant, and its availability is important for research purposes.</p>
</blockquote>

<p>ARC Prize Foundation’s goal is to define, measure, and inspire new ideas towards AGI. To this end, we strive to create the strongest global innovation environment possible.</p>

<p>We do not have AGI yet and are still innovation constrained – scaling up pure LLM pretraining is not the path, despite this being the dominant AI industry narrative and mainstream public view as of last summer.</p>

<p>The reason narratives are important is they end up driving economic activity, like investment, research focus, funding, geopolitics, trade, etc. For example, in 2023-24 there was ~$20B invested into new LLM startups compared to only ~$200M into new AGI startups.</p>

<p>We <a href="https://arcprize.org/blog/launch">launched ARC Prize 2024 last June</a> to grow awareness of limits of scaling LLMs and promote a useful benchmark, ARC-AGI-1, towards a new direction that requires AI systems to adapt to novel, unseen problems instead of being able to rely strictly on memorization.</p>

<figure>
  <img src="https://arcprize.org/media/images/blog/r1-arch.jpg" alt="R1 Training Architecture">
  <figcaption>DeepSeek R1 architecture by <a href="https://x.com/SirrahChan/status/1881488738473357753" target="_blank">@SirrahChan</a>.</figcaption>
</figure>

<p>Last week, DeepSeek <a href="https://arxiv.org/abs/2501.12948">published</a> their new R1-Zero and R1 “reasoner” systems that is <a href="https://x.com/arcprize/status/1881761987090325517">competitive with OpenAI’s o1 system</a> on ARC-AGI-1. R1-Zero, R1, and o1 (low compute) all score around 15-20% – in contrast to <code>GPT-4o</code>’s 5%, the pinnacle of years of pure LLM scaling. Based on this week’s <a href="https://www.reuters.com/technology/chinas-deepseek-sets-off-ai-market-rout-2025-01-27/">US market reaction</a>, the public is starting to understand the limits of scaling pure LLMs too. However, there is still broad public ignorance about impending inference demand.</p>

<p>In December 2024, OpenAI announced a <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">new breakthrough o3 system that we verified</a>. It scored 76% in a low compute mode and 88% in a high compute mode. The o3 system demonstrates the first practical, general implementation of a computer adapting to novel unseen problems.</p>

<p>Despite being <a href="https://www.techmeme.com/241220/h2200">huge tech news</a>, o3 beating ARC-AGI-1 <a href="https://x.com/benspringwater/status/1881507009184530449">went largely unnoticed and unreported</a> by mainstream press.</p>

<p>This is an incredibly important moment for the field of AI and for computer science and these systems demand study. But due to the closed nature of o1/o3, we’re forced to rely on speculation. Thanks to ARC-AGI-1 and now (nearly) open source R1-Zero and R1, we can add to our understanding. In particular, R1-Zero is significantly more important than R1.</p>

<blockquote>
  <p>“Nearly” because DeepSeek did not publish a reproducible way to generate their model weights from scratch</p>
</blockquote>

<h2 id="r1-zero-removes-the-human-bottleneck">R1-Zero removes the human bottleneck</h2>

<p>In our <a href="https://arcprize.org/blog/openai-o1-results-arc-prize">o1</a> and <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">o3 analysis</a>, we speculated how these reasoning systems work. The key ideas:</p>

<ol>
  <li>Generate chains-of-thought (CoT) for a problem domain.</li>
  <li>Label the intermediary CoT steps using a combination of human experts (“supervised fine tuning” or SFT) and automated machines (“reinforcement learning” or RL).</li>
  <li>Train base model using (2).</li>
  <li>At test time, iteratively inference from the process model.</li>
</ol>

<p>Techniques used to iterative sample, along with ARC-AGI-1 scores, are reviewed below:</p>

<div>
  <table>
    <thead>
      <tr>
        <th>System</th>
        <th>ARC-AGI-1</th>
        <th>Method</th>
        <th>Avg Tokens</th>
        <th>Avg Cost</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>r1-zero</td>
        <td>14%</td>
        <td>No SFT / no search</td>
        <td>11K</td>
        <td>$.11</td>
      </tr>
      <tr>
        <td>r1</td>
        <td>15.8%</td>
        <td>SFT / no search</td>
        <td>6K</td>
        <td>$.06</td>
      </tr>
      <tr>
        <td>o1 (low)</td>
        <td>20.5%</td>
        <td>SFT / no search</td>
        <td>7K</td>
        <td>$.43</td>
      </tr>
      <tr>
        <td>o1 (med)</td>
        <td>31%</td>
        <td>SFT / no search</td>
        <td>13K</td>
        <td>$.79</td>
      </tr>
      <tr>
        <td>o1 (high)</td>
        <td>35%</td>
        <td>SFT / no search</td>
        <td>22K</td>
        <td>$1.31</td>
      </tr>
      <tr>
        <td>o3 (low)</td>
        <td>75.7%</td>
        <td>SFT / search + sampling</td>
        <td>335K</td>
        <td>$20</td>
      </tr>
      <tr>
        <td>o3 (high)</td>
        <td>87.5%</td>
        <td>SFT / search + sampling</td>
        <td>57M</td>
        <td>$3.4K</td>
      </tr>
    </tbody>
  </table>
</div>

<p><em>Note: ARC-AGI-1 semi-private score shown.</em></p>

<p>With DeepSeek’s new published research, we can better inform our speculation. The key insight is that higher degrees of novelty adaptation (and reliability) for LLM reasoning systems are achieved along three dimensions:</p>

<ol>
  <li>Adding human labels aka SFT to CoT process model training</li>
  <li>CoT search instead of linear inference (parallel per-step CoT inference)</li>
  <li>Whole CoT sampling (parallel trajectory inference)</li>
</ol>

<p>Item (1) is bottlenecked by human data generation and constrains which domains these reasoning systems benefit most. For example, the <a href="https://openai.com/index/learning-to-reason-with-llms/">MMLU professional law category</a> is surprisingly much lower than the math and logic on o1.</p>

<p>Items (2) and (3) are bottlenecked by efficiency. o1 and o3 both <a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">show logarithmic improvement</a> in benchmark accuracy on ARC-AGI-1 as they spend more inference compute at test time, while the different ways to spend that compute adjust the x-axis of the curve.</p>

<p>In my opinion, the most interesting thing DeepSeek has done is to publish R1-Zero separately. R1-Zero is a model which does not use SFT, the (1) item. Instead it relies purely on reinforcement learning.</p>

<p>R1-Zero and R1 show strong score agreement on  ARC-AGI-1, scoring 14% and 15% respectively. DeepSeeks’s own reported benchmark scores also show strong agreement between R1-Zero and R1, eg. on MATH AIME 2024 scores are 71% and 76% respectively (up from ~40% on the base DeepSeek V3).</p>

<p>In the paper, R1-Zero authors say “DeepSeek-R1-Zero encounters challenges such as poor readability, and language mixing” and <a href="https://www.reddit.com/r/LocalLLaMA/comments/1i765q0/r1zero_pure_rl_creates_a_mind_we_cant_decodeis/">has been corroborated online</a>. However in our testing, we found little to no evidence of incoherence when testing R1-Zero on ARC-AGI-1 which is similar to the math and coding domains the system was RL’d on.</p>

<p>Taken together, these findings suggest:</p>

<ol>
  <li>SFT (eg. human expert labeling) is not necessary for accurate and legible CoT reasoning in domains with strong verification.</li>
  <li>The R1-Zero training process is capable of creating its own internal domain specific language (“DSL”) in token space via RL optimization.</li>
  <li>SFT is necessary for increasing CoT reasoning domain generality.</li>
</ol>

<p>This makes intuitive sense, as language itself is effectively a reasoning DSL. The exact same “words” can be learned in one domain and applied in another, like a program. The pure RL approach can not yet discover a broad shared vocabulary and I expect this will be a strong focus for future research.</p>

<p>Ultimately, R1-Zero demonstrates the prototype of a potential scaling regime with zero human bottlenecks – even in the training data acquisition itself.</p>

<p>Almost certainly DeepSeek has set its sights on OpenAI’s o3 system. It is important to watch whether SFT ends up being a requirement to add CoT search and sampling, or whether a hypothetical “R2-Zero” could exist along the same logarithmic accuracy vs inference scaling curve. Based on R1-Zero results, I believe SFT will not be required to beat ARC-AGI-1 in this hypothetical scaled up version.</p>

<h2 id="dollars-for-reliability">Dollars for reliability</h2>

<p>There are two major shifts happening in AI, economically speaking:</p>

<ol>
  <li>You can now spend more $ to get higher accuracy and reliability</li>
  <li>Training $ is moving to inference $</li>
</ol>

<p>Both are going to drive a massive amount of demand for inference and neither will curtail the demand for more compute. In fact, they will increase the demand for compute.</p>

<p>AI reasoning systems promise much greater returns than simply higher accuracy on benchmarks. The number one issue preventing more AI automation use (e.g. inference demand) is reliability. I’ve spoken with hundreds of Zapier’s customers trying to deploy AI agents in their businesses and the feedback is strongly consistent: “I don’t trust them yet because they don’t work reliably”.</p>

<p><a href="https://www.cognitiverevolution.ai/the-arc-prize-efficiency-intuition-and-agi-with-mike-knoop-co-founder-of-zapier/">Previously</a> I’ve argued that progress towards ARC-AGI would result in higher reliability. The challenge with LLM agents is they need strong local domain steering to work reliability. Stronger generalization capability requires the ability to adapt to unseen situations. We’re now starting to <a href="https://x.com/woj_zaremba/status/1882290021778313272">see evidence</a> this view is correct. And so it’s no surprise several companies are now introducing agents (Anthropic, OpenAI, Apple, …)</p>

<p>Agents will drive significant near-term demand inference due the reliability needs. More broadly, developers can choose to spend more compute to increase user trust in the system. More reliability does not mean 100% accuracy though – but you’d expect to be more <a href="https://commons.wikimedia.org/wiki/File:Statistical_bias_and_statistical_noise_illustration.png">consistently inaccurate</a>. This is okay because users and developers can now more confidently steer behavior via prompting when accuracy is low.</p>

<p>Problems that were impossible for computers previously now have dollar amounts attached to them. And as efficiency climbs, those dollar amounts will go down.</p>

<h2 id="inference-as-training">Inference as training</h2>

<p>The other major shift occurring is in the provenance of data going into LLM systems for pretraining. Previously, most data was either purchased, scraped, or synthetically generated from an existing LLM (eg. distilling or augmenting).</p>

<p>These reasoning systems offer a new option which is to generate “real” data as opposed to “synthetic”. The AI industry uses the term synthetic to identify low quality data that is typically recycled through an LLM to boost the overall amount of training data – with diminishing returns.</p>

<p>But now with reasoning systems and verifiers, we can create brand new legitimate data to train on. This can either be done offline where the developer pays to create the data or at inference time where the end user pays!</p>

<p>This is a fascinating shift in economics and suggests there could be a runaway power concentrating moment for AI system developers who have the largest number of paying customers. Those customers are footing the bill to create new high quality data … which improves the model … which becomes better and more preferred by users … you get the idea.</p>

<p>If we can break through the human expert CoT barrier and create an extremely efficient system to create new data via search/synthesis and verification, then we should expect a massive influx of compute to go into these inference systems as they quite literally get better just by inputting dollars and raw data. Eventually this type of AI training will eclipse pretraining on human generated data altogether.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We will continue to see market corrections as increased inference demand becomes clear. AI system efficiency is only going to drive more usage, not just due to <a href="https://en.wikipedia.org/wiki/Jevons_paradox">Jevons Paradox</a> but because new regimes of training are unlocked as efficiency increases.</p>

<p>With R1 being open and reproducible, more people and teams will be pushing CoT and search to the limits. This will more quickly tell us where the frontier actually lies and will fuel a wave of innovation that increases the chance of reaching AGI quickly.</p>

<p>Several people have already told me they plan to use R1-style systems for <a href="https://arcprize.org/blog/arc-prize-2025">ARC Prize 2025</a> and I’m excited to see the results.</p>

<p>The fact that R1 is open is a great thing for the world. DeepSeek has pushed the frontier of science forward.</p>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[On DeepSeek and export controls (107 pts)]]></title>
            <link>https://darioamodei.com/on-deepseek-and-export-controls</link>
            <guid>42866905</guid>
            <pubDate>Wed, 29 Jan 2025 16:19:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://darioamodei.com/on-deepseek-and-export-controls">https://darioamodei.com/on-deepseek-and-export-controls</a>, See on <a href="https://news.ycombinator.com/item?id=42866905">Hacker News</a></p>
Couldn't get https://darioamodei.com/on-deepseek-and-export-controls: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek proves the future of LLMs is open-source (450 pts)]]></title>
            <link>https://www.getlago.com/blog/deepseek-open-source</link>
            <guid>42866201</guid>
            <pubDate>Wed, 29 Jan 2025 15:37:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.getlago.com/blog/deepseek-open-source">https://www.getlago.com/blog/deepseek-open-source</a>, See on <a href="https://news.ycombinator.com/item?id=42866201">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><a href="https://www.getlago.com/blog"><p>Blog</p></a><p>Why DeepSeek had to be open-source (and why it won't defeat OpenAI)</p></div><div><p>By now, you’ve heard of DeepSeek. It’s the Chinese AI lab that trained R1, an open-source reasoning model as good as OpenAI’s o1, but trained on inferior hardware for a fraction of the price. </p><p>‍</p><p>They achieved this with novel training methods that are more efficient than the ones OpenAI, Anthropic and other well-funded competitors use to train their proprietary models. But why would they open-source it? </p><p>‍</p><p>On the surface, it goes against every business textbook you’ve read: If you innovate to build a market-leading product at a fraction of the cost, you should exploit that advantage. Coca-Cola doesn’t open-source its recipe, right? </p><p>‍</p><p>Not in the world of LLMs. I believe DeepSeek almost had to open-source its models—and that open-source models will become more and more dominant as time goes on. </p><h3>‍<br>Why DeepSeek had to go open-source</h3><p>‍<br>DeepSeek is in a unique position. It’s a Chinese company, which probably makes businesses feel uneasy about building with them, especially when you start to deal with customer data—and even more so when you want to be HIPAA compliant or SOC2-certified. </p><p>‍</p><p>A Chinese AI API would likely receive skepticism in the West. But an open-source model instantly builds trust. You have full control if you self-host or use an AI vendor like Together AI. </p><p>‍</p><p>To gain a foothold in Western markets, DeepSeek had to open-source its models. But that’s not just an economic decision. But it’s not just a political decision. I recently heard the quote that “Open-source is not just a technological behavior, it’s also a cultural one”. </p><p>‍</p><p>And open-source companies (at least in the beginning) have to do more with less. It’s precisely because DeepSeek has to deal with export control on cutting-edge chips like Nvidia H100s and GB10s that they had to find more efficient ways of training models. </p><p>‍</p><p>OpenAI, Meta, Google etc. have billions of dollars, massive compute resources and world-class distribution. They don’t need to find a more efficient way to train models when their expensive solution is the only one. In fact, making it easier and cheaper to build LLMs would erode their advantages!</p><p>‍</p><p>Now that has changed. </p><h3>‍<br>Models are getting commoditized</h3><p>‍<br>It feels like a new GPT-4-level LLM gets released every week. In the AI apps I use, I can’t tell if I’m using LLaMa, GPT, Claude or Mistral models. They’re pretty equal in performance both in my personal experience and in benchmarks. </p><p>‍</p><p>OpenAI is still the leader. They were the first to release a reasoning model and the first to release GPT-4. But models are getting commoditized—and it’s worth asking whether it’s worth paying the premium the OpenAI API charges compared to open-source models. </p><p>‍</p><p>DeepSeek might be the starkest example of this. Compare $60 per million output tokens for OpenAI o1 to $7 per million output tokens on Together AI for DeepSeek R1. </p><p>‍</p><p>If your end user doesn’t know the difference, why would you pay that much more? This is especially important in infrastructure.</p><h3>‍<br>In infrastructure, open-source wins (eventually)</h3><p>‍<br>There’s often a tradeoff with using open-source and proprietary software: Open-source is cheaper and more customizable, but ties up more resources (and requires technical knowledge) because you have to maintain it yourself. Proprietary costs more, but offers a smoother (if more rigid) experience.</p><p>For many product categories, this tradeoff is not worth making for most companies. You don’t want to lose your knowledge base because your self-hosted Notion alternative made an error. </p><p>‍</p><p>But infrastructure is always custom. It always requires work from you. Even a proprietary Oracle database requires a ton of work to set up and maintain. This is why open-source databases are more and more popular. </p><p>‍</p><p>The advantage of proprietary software (No maintenance, no technical knowledge required, etc.) is much lower for infrastructure. It’s actually the opposite: The more technical a product, the better it is for the user (engineers) to work with open-source because they can audit the codebase.</p><p>‍</p><p>This is also why we’re building Lago as an open-source company. We know billing gets complex whether you build your own or use a vendor, so the engineers prefer to work with Lago.</p><p>‍</p><p>The same is true for LLMs. To build any useful product, you’ll be doing a lot of custom prompting and engineering anyway, so you may as well use DeepSeek’s R1 over OpenAI’s o1. </p><p>‍</p><p>This is why there are a lot of successful open-source infrastructure companies and almost no successful open source consumer companies. </p><p>‍</p><p>Does that mean proprietary AI is done? No. </p><h3>‍<br>OpenAI is far from over</h3><p>‍<br>There’s a lot of talk about how OpenAI will be obsolete because of DeepSeek R1 or other open-source models. But that’s not true. First, OpenAI has always been first to market, both with LLMs like GPT-4 and reasoning models like o1. </p><p>‍</p><p>Without OpenAI’s models, DeepSeek R1 and many other models wouldn’t exist (because of LLM distillation). This does beg the question of whether it’s still worth it to build new frontier models if you provide the breakthrough and someone else ships something similar for much cheaper. </p><p>‍</p><p>But R1 might also wake up the well-funded incumbents and force them to find more efficient methods—and who knows what they can build when they have both efficiency and all the resources in the world.</p><p>‍</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Pixel 4a old firmware is gone, trapping users on the buggy battery update (160 pts)]]></title>
            <link>https://www.androidcentral.com/phones/google-pixel-4as-old-firmware-is-gone-trapping-users-on-the-buggy-battery-update</link>
            <guid>42865619</guid>
            <pubDate>Wed, 29 Jan 2025 14:59:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.androidcentral.com/phones/google-pixel-4as-old-firmware-is-gone-trapping-users-on-the-buggy-battery-update">https://www.androidcentral.com/phones/google-pixel-4as-old-firmware-is-gone-trapping-users-on-the-buggy-battery-update</a>, See on <a href="https://news.ycombinator.com/item?id=42865619">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg" alt="Google Pixel 4a on a black metal bench" srcset="https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-799-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/zGATq9gTtzMk74khXzFkaF.jpg" data-pin-nopin="true" fetchpriority="high" crossorigin="anonymous">
</picture>
</div>
<figcaption>
<span>(Image credit: Android Central)</span>
</figcaption>
</div>

<div id="article-body">
<h2 id="what-you-need-to-know-3">What you need to know</h2><ul><li>Google’s Pixel 4a battery update is a disaster, causing crazy battery drain for many users.</li><li>The update also wiped older firmware, leaving no way to roll back to the previous version.</li><li>Google has acknowledged the mess and offered compensation: a free battery replacement, $50 cash, or $100 credit toward a new Pixel.</li></ul><hr><p>Google recently <a data-analytics-id="inline-link" href="https://www.androidcentral.com/apps-software/older-pixel-4a-to-get-an-all-new-battery-performance-update" data-before-rewrite-localise="https://www.androidcentral.com/apps-software/older-pixel-4a-to-get-an-all-new-battery-performance-update">rolled out a battery update for the Pixel 4a</a>, and it’s been a bit of a disaster. A bunch of users are reporting crazy battery drain since installing it. To add insult to injury, Google wiped the older firmware as part of the update process, so there’s no official way to go back to the previous version. If you’re dealing with this mess, you’re pretty much stuck at the moment.</p><p>After discontinuing the Pixel 4a last year, Google had left the device without any software updates. But out of nowhere, about two weeks ago, the company released a surprise update meant to <a data-analytics-id="inline-link" href="https://www.androidcentral.com/phones/google-tips-pixel-smartphone-battery-health" data-before-rewrite-localise="https://www.androidcentral.com/phones/google-tips-pixel-smartphone-battery-health">improve battery performance</a>. Part of the Pixel 4a Battery Performance Program, it was supposed to help some units last longer on a charge. Instead, it ended up making battery problems worse for a ton of users.</p><p>In other words, the update was a total flop—lots of Pixel 4a owners are now stuck with awful battery life. And if you were thinking about rolling back to an older version to fix things, Google just shut that door by pulling the download links for previous firmware.</p><p>Someone on <a data-analytics-id="inline-link" href="https://old.reddit.com/r/GooglePixel/comments/1iajsu3/google_removed_pixel_4a_firmware_images_from/" target="_blank" data-url="https://old.reddit.com/r/GooglePixel/comments/1iajsu3/google_removed_pixel_4a_firmware_images_from/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Reddit</a> noticed that the only Android image you can get for the Pixel 4a is the new update that Google forced everyone to install in order to tackle unidentified battery issues (via <a data-analytics-id="inline-link" href="https://9to5google.com/2025/01/26/old-pixel-4a-updates/" target="_blank" data-url="https://9to5google.com/2025/01/26/old-pixel-4a-updates/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">9to5Google</a>).</p><p>The update might keep the Pixel 4a running, but it's essentially rendered it useless for many users.</p><p>One <a data-analytics-id="inline-link" href="https://www.reddit.com/r/Pixel4a/comments/1i6u7h8/comment/m8fbhnf/" target="_blank" data-url="https://www.reddit.com/r/Pixel4a/comments/1i6u7h8/comment/m8fbhnf/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Reddit user</a> reported their 4a draining from full to 2% in just five hours, despite minimal screen usage (under 20 minutes).</p><p><a data-analytics-id="inline-link" href="https://www.androidauthority.com/google-pixel-4a-firmware-removed-3520326/" target="_blank" data-url="https://www.androidauthority.com/google-pixel-4a-firmware-removed-3520326/" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Android Authority</a> points out that Google rarely removes older firmware, implying a serious underlying issue with the Pixel 4a update.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-ZHxyhSCQ88Eymxod3bdfsN"><section><p>Get the latest news from Android Central, your trusted companion in the world of Android</p></section></div><p>Google is owning up to the battery disaster caused by the recent Pixel 4a update and is trying to make things right. The company is offering compensation to anyone impacted. To check if you’re eligible, hop over to this <a data-analytics-id="inline-link" href="https://support.google.com/pixelphone/workflow/15642495" target="_blank" data-url="https://support.google.com/pixelphone/workflow/15642495" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">support page</a>, type in your phone’s IMEI number, and follow the steps. You’ll get to pick between a free battery replacement, $50 cash, or a $100 credit toward a new Pixel.</p>
</div>


<div id="slice-container-authorBio-ZHxyhSCQ88Eymxod3bdfsN"><p>Jay Bonggolto always keeps a nose for news. He has been writing about consumer tech and apps for as long as he can remember, and he has used a variety of Android phones since falling in love with Jelly Bean. Send him a direct message via Twitter or LinkedIn.</p></div>
</section>





<div id="slice-container-relatedArticles"><p><h3>Most Popular</h3></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Complete hardware and software setup for running Deepseek-R1 locally. ($6000) (180 pts)]]></title>
            <link>https://twitter.com/carrigmat/status/1884244369907278106</link>
            <guid>42865575</guid>
            <pubDate>Wed, 29 Jan 2025 14:56:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/carrigmat/status/1884244369907278106">https://twitter.com/carrigmat/status/1884244369907278106</a>, See on <a href="https://news.ycombinator.com/item?id=42865575">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI Furious DeepSeek Might Have Stolen All the Data OpenAI Stole from Us (1243 pts)]]></title>
            <link>https://www.404media.co/openai-furious-deepseek-might-have-stolen-all-the-data-openai-stole-from-us/</link>
            <guid>42865527</guid>
            <pubDate>Wed, 29 Jan 2025 14:52:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/openai-furious-deepseek-might-have-stolen-all-the-data-openai-stole-from-us/">https://www.404media.co/openai-furious-deepseek-might-have-stolen-all-the-data-openai-stole-from-us/</a>, See on <a href="https://news.ycombinator.com/item?id=42865527">Hacker News</a></p>
Couldn't get https://www.404media.co/openai-furious-deepseek-might-have-stolen-all-the-data-openai-stole-from-us/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Cali's AG Tells AI Companies Almost Everything They're Doing Might Be Illegal (151 pts)]]></title>
            <link>https://gizmodo.com/californias-ag-tells-ai-companies-practically-everything-theyre-doing-might-be-illegal-2000555896</link>
            <guid>42865174</guid>
            <pubDate>Wed, 29 Jan 2025 14:21:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/californias-ag-tells-ai-companies-practically-everything-theyre-doing-might-be-illegal-2000555896">https://gizmodo.com/californias-ag-tells-ai-companies-practically-everything-theyre-doing-might-be-illegal-2000555896</a>, See on <a href="https://news.ycombinator.com/item?id=42865174">Hacker News</a></p>
Couldn't get https://gizmodo.com/californias-ag-tells-ai-companies-practically-everything-theyre-doing-might-be-illegal-2000555896: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[I do not want AI to "polish" me (369 pts)]]></title>
            <link>https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/</link>
            <guid>42864854</guid>
            <pubDate>Wed, 29 Jan 2025 13:50:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/">https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/</a>, See on <a href="https://news.ycombinator.com/item?id=42864854">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content">

		<main id="main">

			
<article id="post-42430">

	

	
	<div>
		
<p>I was sending an email when a little magic wand popped up that said <strong>“Polish”</strong> and I thought that was weird because <em>why would I want to translate my email into Polish</em>?  </p>



<figure><a href="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?ssl=1"><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="750" height="295" data-attachment-id="42432" data-permalink="https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/screenshot-2025-01-28-at-3-27-41-pm/" data-orig-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?fit=1862%2C732&amp;ssl=1" data-orig-size="1862,732" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2025-01-28 at 3.27.41 PM" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?fit=300%2C118&amp;ssl=1" data-large-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?fit=750%2C295&amp;ssl=1" src="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?resize=750%2C295&amp;ssl=1" alt="" srcset="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?resize=1024%2C403&amp;ssl=1 1024w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?resize=300%2C118&amp;ssl=1 300w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?resize=768%2C302&amp;ssl=1 768w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?resize=1536%2C604&amp;ssl=1 1536w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?resize=1200%2C472&amp;ssl=1 1200w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?resize=1568%2C616&amp;ssl=1 1568w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.41-PM.png?w=1862&amp;ssl=1 1862w" sizes="(max-width: 750px) 100vw, 750px"></a></figure>



<p>I tried to click on it to make it go away but instead it changed the entire email because apparently it was saying that it needed to “polish” my email because I guess I’m too unsophisticated to use words:</p>



<figure><a href="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?ssl=1"><img data-recalc-dims="1" decoding="async" width="750" height="943" data-attachment-id="42434" data-permalink="https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/screenshot-2025-01-28-at-3-27-57-pm/" data-orig-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?fit=2044%2C2572&amp;ssl=1" data-orig-size="2044,2572" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2025-01-28 at 3.27.57 PM" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?fit=238%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?fit=750%2C943&amp;ssl=1" src="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?resize=750%2C943&amp;ssl=1" alt="" srcset="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?resize=814%2C1024&amp;ssl=1 814w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?resize=238%2C300&amp;ssl=1 238w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?resize=768%2C966&amp;ssl=1 768w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?resize=1221%2C1536&amp;ssl=1 1221w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?resize=1628%2C2048&amp;ssl=1 1628w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?resize=1200%2C1510&amp;ssl=1 1200w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?resize=1568%2C1973&amp;ssl=1 1568w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?w=2044&amp;ssl=1 2044w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.27.57-PM.png?w=1500&amp;ssl=1 1500w" sizes="(max-width: 750px) 100vw, 750px"></a></figure>



<p>There is no way in hell anyone who knows me would get that email and not think I’d been abducted so I deleted the suggested rewrite and updated my email:</p>



<figure><a href="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?ssl=1"><img data-recalc-dims="1" decoding="async" width="750" height="758" data-attachment-id="42436" data-permalink="https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/screenshot-2025-01-28-at-3-32-40-pm/" data-orig-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?fit=1938%2C1960&amp;ssl=1" data-orig-size="1938,1960" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2025-01-28 at 3.32.40 PM" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?fit=297%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?fit=750%2C758&amp;ssl=1" src="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?resize=750%2C758&amp;ssl=1" alt="" srcset="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?resize=1013%2C1024&amp;ssl=1 1013w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?resize=297%2C300&amp;ssl=1 297w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?resize=768%2C777&amp;ssl=1 768w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?resize=1519%2C1536&amp;ssl=1 1519w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?resize=1200%2C1214&amp;ssl=1 1200w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?resize=1568%2C1586&amp;ssl=1 1568w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.32.40-PM.png?w=1938&amp;ssl=1 1938w" sizes="(max-width: 750px) 100vw, 750px"></a></figure>



<p>But after I added the update gmail was like, “YOU’RE STILL DOING IT WRONG, IDIOT?” and the polish thing came up <em><strong>again</strong></em> and I was like, “Are you trying to AI fix a paragraph where I say how much I don’t want AI to fix shit?”  And turns out, yeah, that exactly what it meant because it gave me this:</p>



<figure><a href="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?ssl=1"><img data-recalc-dims="1" loading="lazy" decoding="async" width="750" height="753" data-attachment-id="42438" data-permalink="https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/screenshot-2025-01-28-at-3-28-15-pm/" data-orig-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?fit=2232%2C2240&amp;ssl=1" data-orig-size="2232,2240" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2025-01-28 at 3.28.15 PM" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?fit=300%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?fit=750%2C753&amp;ssl=1" src="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=750%2C753&amp;ssl=1" alt="" srcset="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=1020%2C1024&amp;ssl=1 1020w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=768%2C771&amp;ssl=1 768w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=1531%2C1536&amp;ssl=1 1531w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=2041%2C2048&amp;ssl=1 2041w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=200%2C200&amp;ssl=1 200w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=1200%2C1204&amp;ssl=1 1200w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.28.15-PM.png?resize=1568%2C1574&amp;ssl=1 1568w" sizes="auto, (max-width: 750px) 100vw, 750px"></a></figure>



<p><strong>Jesus.</strong> Y’all, if you get an email from me it will be signed with HUGS, LOVE, FIGHT THE PATRIARCHY, DOWN WITH POWDERED GRAVY or SORRY I SUCK SO MUCH. It will be filled with typos and rambling parentheticals and apologies for answering several months too late. This is how you know it’s me and not a robot. My only hope is that my constant declining of the suggestions will make the AI learn from me and spread my terrible etiquette throughout the world.</p>



<p>Also, I just realized when I tried to insert these pictures into this blog about how much I hate AI my blog was suddenly like, “HEY I KNOW YOU JUST CLICKED A BUTTOM SAYING YOU WANT TO ADD A SPECIFIC PICTURE BUT HOW ABOUT WE JUST MAKE AI IMAGES FOR YOU INSTEAD?”  <strong><em>AM I ON CANDID CAMERA?</em></strong>  It’s like my whole computer is a toddler screaming “LET ME DO IT!” every time I try to create something.</p>



<figure><a href="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?ssl=1"><img data-recalc-dims="1" loading="lazy" decoding="async" width="750" height="364" data-attachment-id="42446" data-permalink="https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/screenshot-2025-01-28-at-3-46-17-pm/" data-orig-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?fit=1170%2C568&amp;ssl=1" data-orig-size="1170,568" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2025-01-28 at 3.46.17 PM" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?fit=300%2C146&amp;ssl=1" data-large-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?fit=750%2C364&amp;ssl=1" src="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?resize=750%2C364&amp;ssl=1" alt="" srcset="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?resize=1024%2C497&amp;ssl=1 1024w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?resize=300%2C146&amp;ssl=1 300w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?resize=768%2C373&amp;ssl=1 768w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.46.17-PM.png?w=1170&amp;ssl=1 1170w" sizes="auto, (max-width: 750px) 100vw, 750px"></a></figure>



<p>And as much as I hate AI, I had to see what the program thought it could do so much better than me so I gave it the prompt “please stop giving me AI” and…all apologies.  Clearly I <strong><em>did</em></strong> need help because…fucking wow.  Nailed it:</p>



<figure><a href="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?ssl=1"><img data-recalc-dims="1" loading="lazy" decoding="async" width="750" height="644" data-attachment-id="42448" data-permalink="https://thebloggess.com/2025/01/28/no-i-do-not-want-ai-to-polish-me/screenshot-2025-01-28-at-3-48-20-pm-2/" data-orig-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?fit=1712%2C1470&amp;ssl=1" data-orig-size="1712,1470" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2025-01-28 at 3.48.20 PM" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?fit=300%2C258&amp;ssl=1" data-large-file="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?fit=750%2C644&amp;ssl=1" src="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?resize=750%2C644&amp;ssl=1" alt="" srcset="https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?resize=1024%2C879&amp;ssl=1 1024w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?resize=300%2C258&amp;ssl=1 300w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?resize=768%2C659&amp;ssl=1 768w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?resize=1536%2C1319&amp;ssl=1 1536w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?resize=1200%2C1030&amp;ssl=1 1200w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?resize=1568%2C1346&amp;ssl=1 1568w, https://i0.wp.com/thebloggess.com/wp-content/uploads/2025/01/Screenshot-2025-01-28-at-3.48.20-PM-1.png?w=1712&amp;ssl=1 1712w" sizes="auto, (max-width: 750px) 100vw, 750px"></a></figure>



<p>Anyway…this sucks.</p>



<p>Worst regards,</p>



<p>Jenny</p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

				
</article><!-- #post-${ID} -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
<!-- #comments -->

		</main><!-- #main -->

		



	</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Seagate: 'new' hard drives used for tens of thousands of hours (237 pts)]]></title>
            <link>https://www.tomshardware.com/pc-components/hdds/german-seagate-customers-say-their-new-hard-drives-were-actually-used-resold-hdds-reportedly-used-for-tens-of-thousands-of-hours</link>
            <guid>42864788</guid>
            <pubDate>Wed, 29 Jan 2025 13:42:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/pc-components/hdds/german-seagate-customers-say-their-new-hard-drives-were-actually-used-resold-hdds-reportedly-used-for-tens-of-thousands-of-hours">https://www.tomshardware.com/pc-components/hdds/german-seagate-customers-say-their-new-hard-drives-were-actually-used-resold-hdds-reportedly-used-for-tens-of-thousands-of-hours</a>, See on <a href="https://news.ycombinator.com/item?id=42864788">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-1600-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-1200-80.jpg" alt="" srcset="https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-1600-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/qCi53xjtZYf9U2GBj2NDfN.jpg" data-pin-nopin="true" fetchpriority="high" crossorigin="anonymous">
</picture>
</div>
<figcaption>
<span>(Image credit: Seagate)</span>
</figcaption>
</div>

<div id="article-body">
<p>German outlet Heise.de says it may have uncovered fraud after hearing from many of its readers who bought supposedly new <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tag/seagate" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.tomshardware.com/tag/seagate">Seagate</a> hard drives, which turned out to be very much used.</p><p>Last week, the publication <a data-analytics-id="inline-link" href="https://www.heise.de/news/Gebrauchte-Seagate-Festplatten-als-Neuware-im-Umlauf-10254276.html" data-url="https://www.heise.de/news/Gebrauchte-Seagate-Festplatten-als-Neuware-im-Umlauf-10254276.html" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">relayed</a> the experience of one of Heise.de’s readers, who said he had bought a couple of 14TB Seagate Exos HDDs that seemed a little strange. The drives had some minor signs of wear on the outside, but after a quick look at the SMART stats, everything appeared normal. Later, though, the reader did a more thorough Field Accessible Reliability Metrics (FARM) test and discovered that one drive had already been used for 10,000 hours, and the other 15,000 hours.</p><p>Naturally, he returned the drives to the store he bought them from, an official Seagate retailer, and decided to replace them with two 16TB Exos HDDs purchased from a different store. These drives also turned out to be heavily used: 22,000 hours logged on each one.</p><p>Although both HDD sellers, neither of which Heise.de identified, claimed the Exos drives were simply brand-new retail models, Seagate told the publication that all four drives were actually OEM models. This meant that the normal five-year warranty did not apply like it would to typical drives bought at retail.</p><p>The initial retailer eventually stopped selling the 14TB and 16TB HDDs at some point and even canceled an order that Heise.de had anonymously placed. According to the report, Seagate is looking into how this happened, especially as one of the retailers has the storage corporation’s endorsement as an official retailer.</p><p>After this report was published, the floodgates opened, and over fifty other Heise.de readers <a data-analytics-id="inline-link" href="https://www.heise.de/news/Betrug-mit-Seagate-Festplatten-Dutzende-Leser-melden-Verdachtsfaelle-10258657.html" target="_blank" data-url="https://www.heise.de/news/Betrug-mit-Seagate-Festplatten-Dutzende-Leser-melden-Verdachtsfaelle-10258657.html" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">said</a> they experienced the exact same thing after buying apparently new Seagate HDDs. While 50 is a small sample size, the issue might be widespread since they bought their drives at a dozen different retailers, some of which are on Seagate’s official “where-to-buy” list. Some of the impacted retailers are quite large, such as <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tag/amazon" data-auto-tag-linker="true" data-before-rewrite-localise="https://www.tomshardware.com/tag/amazon">Amazon</a> and Mindfactory.</p><p>Most readers report having 16TB Exos drives, but others have the 12TB model, and a few have non-Exos HDDs ranging from 4 to 18TB. The time used ranges from 15,000 to 36,000 hours except for two 4TB HDDs, which were both used for about 50,000 hours. Heise.de checked a few drives at random to see when their warranties expired, and most of them were for 2026. Assuming a five-year warranty, that means they were first made and sold in 2021. All of the readers who reported receiving a used Seagate drive had bought it in the past few weeks, meaning the issue is relatively recent.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-yuaCUcXynTznQQrKVaUXa7"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div><p>It’s hard to imagine this is just a simple mixup, not just because so many retailers are apparently involved but also because they’ve all had their SMART stats reset, which would be very useful to someone trying to pretend a used drive is new. Although it’s not entirely clear if actual fraud is happening here, something has definitely gone very wrong.</p><p>We reached out to Seagate for comment but haven’t received a reply yet.</p><p>Seagate does have a direct relationship with used hard drives. Nearly a year ago, the company <a data-analytics-id="inline-link" href="https://www.tomshardware.com/pc-components/hdds/seagate-opens-an-ebay-store-to-sell-refurbished-hard-drives" target="_blank" data-before-rewrite-localise="https://www.tomshardware.com/pc-components/hdds/seagate-opens-an-ebay-store-to-sell-refurbished-hard-drives">launched an official eBay store that sells refurbished drives. It</a> also has a Hard Drive Circularity Program to find as many refurbish-worthy drives as possible, including Exos models. However, this store only sells in the US, so it doesn’t seem likely that it has anything to do with the current situation.</p>
</div>



<!-- Drop in a standard article here maybe? -->



<div id="slice-container-authorBio-yuaCUcXynTznQQrKVaUXa7"><p>Matthew&nbsp;Connatser is a freelancing writer for Tom's Hardware US. He writes articles about CPUs, GPUs, SSDs, and computers in general.</p></div>
</section>





<div id="slice-container-relatedArticles"><p><h3>Most Popular</h3></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Our phones are killing our ability to feel sexy (2024) (197 pts)]]></title>
            <link>https://catherineshannon.substack.com/p/your-phone-is-why-you-dont-feel-sexy</link>
            <guid>42864595</guid>
            <pubDate>Wed, 29 Jan 2025 13:21:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://catherineshannon.substack.com/p/your-phone-is-why-you-dont-feel-sexy">https://catherineshannon.substack.com/p/your-phone-is-why-you-dont-feel-sexy</a>, See on <a href="https://news.ycombinator.com/item?id=42864595">Hacker News</a></p>
Couldn't get https://catherineshannon.substack.com/p/your-phone-is-why-you-dont-feel-sexy: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Jevons paradox (164 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Jevons_paradox</link>
            <guid>42863808</guid>
            <pubDate>Wed, 29 Jan 2025 11:34:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Jevons_paradox">https://en.wikipedia.org/wiki/Jevons_paradox</a>, See on <a href="https://news.ycombinator.com/item?id=42863808">Hacker News</a></p>
Couldn't get https://en.wikipedia.org/wiki/Jevons_paradox: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Bacteria (and Their Metabolites) and Depression (333 pts)]]></title>
            <link>https://www.science.org/content/blog-post/bacteria-and-their-metabolites-and-depression</link>
            <guid>42863262</guid>
            <pubDate>Wed, 29 Jan 2025 09:54:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/blog-post/bacteria-and-their-metabolites-and-depression">https://www.science.org/content/blog-post/bacteria-and-their-metabolites-and-depression</a>, See on <a href="https://news.ycombinator.com/item?id=42863262">Hacker News</a></p>
Couldn't get https://www.science.org/content/blog-post/bacteria-and-their-metabolites-and-depression: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[We got hit by an alarmingly well-prepared phish spammer (147 pts)]]></title>
            <link>https://utcc.utoronto.ca/~cks/space/blog/spam/WellPreparedPhishSpammer</link>
            <guid>42862873</guid>
            <pubDate>Wed, 29 Jan 2025 08:43:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://utcc.utoronto.ca/~cks/space/blog/spam/WellPreparedPhishSpammer">https://utcc.utoronto.ca/~cks/space/blog/spam/WellPreparedPhishSpammer</a>, See on <a href="https://news.ycombinator.com/item?id=42862873">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>We got hit by an alarmingly well-prepared phish spammer</h2>

	<p><small>January 28, 2025</small></p>
</div><div><p>Yesterday evening, <a href="https://support.cs.toronto.edu/">we</a> were hit
by a run of phish spam that I would call 'vaguely customized' for
us, for example the display name in the From: header was "U of T |
CS Dept" (but then the actual email address was that of the compromised
account elsewhere that was used to send the phish spam). The
destination addresses here weren't particularly well chosen, and
some of them didn't even exist. So far, so normal. One person here
fell for the phish spam that evening but realized it almost immediately
and promptly changed their password. Today that person got in touch with
us because they'd started receiving email bounces for (spam) email that
they hadn't sent. Investigation showed that the messages were being sent
through us, but in an alarmingly clever way.</p>

<p>We have a local VPN service for people, and this VPN service requires
a different password from your regular (Unix and IMAP and etc)
password. People connecting through our VPN have access to an
internal-only SMTP gateway machine that doesn't require SMTP
authentication. As far as we can tell, in the quite short interval
between when the person fell for the phish and then changed their
password, the phish spam attacker used the main password they'd
just stolen to register the person for our VPN and obtain a VPN
password (which we don't reset on Unix password changes). They then
connected to the VPN using their stolen credentials and used the
VPN to send spam email through our internal-only SMTP gateway
(initially last evening and then again today, at which point they
were detected).</p>

<p>Based on some log evidence, I think that the phish spammer first
tried to use authenticated SMTP but failed due to the password
change, then fell back on the VPN access. Even if VPN access hadn't
been their primary plan, they worked very fast to secure themselves
an additional access method. It seems extremely likely that the
attacker had already researched our mail and VPN environment before
they sent their initial phish spam, since they knew exactly where
to go and what to do.</p>

<p>If phish spammers are increasingly going to be this well prepared
and clever, we're going to have to be prepared for that on our side.
Until now, we hadn't really thought about the possibility of phish
spammers gaining VPN access; previous phish spammers have exploited
some combination of webmail and authenticated SMTP.</p>

<p>(We're also going to need to be more concerned about other methods
of obtaining persistent account access, such as adding new SSH
authorized keys to the Unix login. This attacker didn't attempt any
sort of SSH access.)</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I still like Sublime Text in 2025 (829 pts)]]></title>
            <link>https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/</link>
            <guid>42862246</guid>
            <pubDate>Wed, 29 Jan 2025 06:43:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/">https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=42862246">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="ArticleBody text"><p>I still get people asking me why I use Sublime Text in 2025 given there are <em>soooo</em> many other great editors out there.</p><p>My response: there is? Because I still think Sublime Text holds up as a great editor.</p><p><em>Table Of Contents</em></p><ul><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#it-fast">It fast</a></li><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#lsp">LSP</a></li><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#snippets">Snippets</a></li><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#project-workspaces">Project workspaces</a></li><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#build-systems">Build systems</a></li><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#multiple-cursors">Multiple cursors</a></li><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#key-mouse-bindings">Key/mouse bindings</a></li><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#included-niceness">Included niceness</a></li><li><a href="https://ohdoylerules.com/workflows/why-i-still-like-sublime-text-in-2025/#wish-list">Wish List</a></li></ul><p>I started with Sublime Text 2 back in 2010/2011 while I was in college. I mainly started using it because it was free, cross-platform, and came as a "portable app" that I could put on a USB and just use.</p><p>Back then, I had a really basic Toshiba laptop that dual booted Windows XP and Ubuntu (or maybe it was Mint?) so it was nice that it worked on both. I really liked how snappy it was compared to the tools our teacher suggested using. At that time it was Dreamweaver and maybe Notepad++.</p><p>Sublime, at that time, was pretty novel. It clearly took a lot of inspirations from <a rel="noopener nofollow noreferrer" href="https://github.com/textmate/textmate" target="_blank">TextMate</a>, another classic editor, considering that one is 4 years older than Sublime. It had multiple cursors, plugins, a build system. But the <a rel="noopener nofollow noreferrer" href="https://www.vendr.com/blog/consumer-dev-tools-command-palette" target="_blank">biggest claim to fame for Sublime, was the "command palette"</a>. I'm sure there is some other older app that had a precursor or similar feature to it, but generally speaking, it seems like that user experience pattern really kicked off with Sublime.</p><p>I built my web-dev chops on Sublime. The shortcuts are ingrained in my bones at this point. I'm not some key-combo-king, but I know a lot of the shortcuts that can help me get the UI and commands I need without thinking much at all.</p><blockquote><p>I have been, and continue to be, a Sublime user of about 15 years.</p></blockquote><p>So take all this with that in mind. I have been, and continue to be, a Sublime user of about 15 years.</p><p>So why do I keep using Sublime?</p><p>If you thought Sublime was dead, well you couldn't be more wrong! The latest build of Sublime as of this post is "4192" and was released 20th January 2025. So basically a week ago from this post. Not too bad.</p><p>It has regularly been updated with minor tweaks and fixes about a dozen times a year. I think the last major upgrade would be when Package Control (the plugin installer/manager) bumped to the next version which allowed plugins to install external dependencies.</p><p>You can nitpick here and say that Package Control is not part of Sublime. But most people won't use Sublime without it. So I am going to take some liberties and say it is part of it.</p><p>I think the thing to consider is how Sublime is basically "done" software. It has been around a <em>long</em> time. It was first released around 2008. It just passed it's 17-year anniversary actually. Congrats to them!</p><p>Before I dive into the details of <strong>why</strong> I still use it, consider this: if you are using a modern GUI-driven editor, it probably has taken inspiration from Sublime. So why not check out one of the OGs? You might find something you like.</p><p>Without further adieu, my reasons for still using Sublime in 2025:</p><h3 id="it-fast">It fast</h3><p>Sublime is fast. It starts instantly. Uses very few resources. Handles large files gracefully. Rarely crashes.</p><p>Nothing else to add here. A+ performance.</p><h3 id="lsp">LSP</h3><p><a rel="noopener nofollow noreferrer" href="https://github.com/sublimelsp" target="_blank">Sublime LSP</a> is really doing a lot for Sublime to keep it feeling modern and keeping up with other tools in the same class.</p><p>If you aren't aware of what an LSP is, this isn't the post to learn about it. But the gist is, it handles all that fancy code-aware completion and hover info you like from VS Code. If you want to learn more <a href="https://www.youtube.com/watch?v=LaS32vctfOY" rel="noopener nofollow noreferrer" target="_blank">give TJ 5 minutes to learn you</a>.</p><p>Some of the cool things about the Sublime LSP:</p><p><strong>Multiple servers per file</strong></p><p>You can enable as many LSP servers per file that you want. Restart them individually and configure them on a per-project basis (more on that later) which really helps bolster the capabilities of this already great editor.</p><p><strong>Detection on a scope level</strong></p><p>When configuring an LSP, outside of one installed with a plugin, you tell the LSP plugin which "scope" (think of this as an id for a syntax) to enable an LSP on.</p><p>Want your LSP to only turn on if you only open a file with a specific syntax? No problem. Want it to turn on only if a type of syntax is detected? Like a specific flavour of CSS? Sure. It is very configurable.</p><p><strong>Extensible configuration</strong></p><p>I know VS Code is the LSP king, which the tech originating with that editor, but I haven't seen the ability to just add an LSP installed as a binary in on your <code>usr/local/bin</code>.</p><p>There are a few "cutting-edge" LSPs that are installed via Cargo that are usually only targeting Neovim, but can easily be configured in Sublime with a simple JSON object.</p><p>Here is an example of configuring <a rel="noopener nofollow noreferrer" href="https://github.com/matkrin/md-lsp" target="_blank">md-lsp</a> (Markdown language server with support for GitHub flavored Markdown) in a few lines:</p><pre data-lang="json"><code data-lang="json"><span>// in the "LSP Settings" file, under "clients[]"
</span><span>"</span><span>md-lsp</span><span>": {
</span><span>  </span><span>// default enabled?
</span><span>  "</span><span>enabled</span><span>": </span><span>true</span><span>,
</span><span>  "</span><span>command</span><span>": [
</span><span>    </span><span>// as long as the system path is setup right, we can find this binary
</span><span>    "</span><span>md-lsp</span><span>",
</span><span>  ],
</span><span>  </span><span>// when we see this scope, the LSP will start this server
</span><span>  "</span><span>selector</span><span>": "</span><span>text.html.markdown</span><span>",
</span><span>},
</span></code></pre><p>It should be noted that md-lsp <strong>does not have a Sublime LSP plugin</strong> nor any mention of Sublime in their README. They only mention support for <em>Helix</em> and <em>Neovim</em>. Well, guess what? You support Sublime too!</p><h3 id="snippets">Snippets</h3><p>I write a lot of snippets. Right now, my snippets folder in Sublime has 123 snippets. The latest one was added <strong>today</strong>. It was a "TODO" snippet for Blade.</p><p>Sublime lets you create snippets from the <code>Tools &gt; Developer &gt; New Snippet</code> dropdown. They get sent to your "User" folder in the Sublime directory and are sourced on startup.</p><p><strong>Scope based</strong></p><p>Snippets are also scope-based. VS Code has scopes too, so there is nothing new here. I wonder where they got that from? 😮</p><p>A quick note on scopes: they can be very vague (like <code>source.txt</code>, targeting a whole <code>.txt</code> file) or super specific (like <code>text.html.basic.liquid text.html.basic meta.object.liquid</code> which targets a nested object in a liquid template) based on what you need.</p><p>I have found the Sublime scope integration to be straightforward to understand. A syntax defines scopes, and you can target those scopes in snippets, keybindings, and macros. More on those other two later.</p><p>I'm sure I didn't get the details perfect. But it doesn't really matter given the point I'm going to make: not all snippet systems work this way.</p><p>Specifically, I have found Helix, Neovim, and Zed snippets to be more based around "filetype" and not the scope of the where you are in the "syntax".</p><p>I'm sure this will change. Or perhaps I've missed something. From what I can see on the surface, snippets based on syntax-specific scopes seem to be the default in VS Code and Sublime.</p><p><strong>Tab stops with nesting, placeholders, and references</strong></p><p>Here is the snippet I made today:</p><pre data-lang="xml"><code data-lang="xml"><span>&lt;</span><span>snippet</span><span>&gt;
</span><span>    &lt;</span><span>content</span><span>&gt;&lt;![</span><span>CDATA</span><span>[
</span><span>&lt;!-- TODO: $1 --&gt;$0
</span><span>]]&gt;&lt;/</span><span>content</span><span>&gt;
</span><span>    &lt;</span><span>description</span><span>&gt;Blade todo comment&lt;/</span><span>description</span><span>&gt;
</span><span>    </span><span>&lt;!-- Optional: Set a tabTrigger to define how to trigger the snippet --&gt;
</span><span>    &lt;</span><span>tabTrigger</span><span>&gt;todo&lt;/</span><span>tabTrigger</span><span>&gt;
</span><span>    </span><span>&lt;!-- Optional: Set a scope to limit where the snippet will trigger --&gt;
</span><span>    &lt;</span><span>scope</span><span>&gt;text.html.blade&lt;/</span><span>scope</span><span>&gt;
</span><span>&lt;/</span><span>snippet</span><span>&gt;
</span></code></pre><p>Pretty simple. People who have written snippets before will recognize the syntax. The <code>$1</code> is where your caret is sent first, you can then type, then hit tab, and you get sent to <code>$0</code>.</p><p>Here is a TODO snippet I have for "javascript" files:</p><pre data-lang="xml"><code data-lang="xml"><span>&lt;</span><span>snippet</span><span>&gt;
</span><span>  &lt;</span><span>content</span><span>&gt;&lt;![</span><span>CDATA</span><span>[
</span><span>/** @todo ${1:this is my todo} */$0
</span><span>]]&gt;&lt;/</span><span>content</span><span>&gt;
</span><span>  </span><span>&lt;!-- Optional: Set a tabTrigger to define how to trigger the snippet --&gt;
</span><span>  &lt;</span><span>tabTrigger</span><span>&gt;todo&lt;/</span><span>tabTrigger</span><span>&gt;
</span><span>  &lt;</span><span>description</span><span>&gt;Insert a TODO JS comment&lt;/</span><span>description</span><span>&gt;
</span><span>  </span><span>&lt;!-- Optional: Set a scope to limit where the snippet will trigger --&gt;
</span><span>  &lt;</span><span>scope</span><span>&gt;source.js, source.ts, source.jsx meta.function.js meta.block.js meta.group.js meta.jsx.js meta.interpolation.js, source.tsx meta.function.js meta.block.js meta.group.js meta.jsx.js meta.interpolation.js&lt;/</span><span>scope</span><span>&gt;
</span><span>&lt;/</span><span>snippet</span><span>&gt;
</span></code></pre><p>Here you can see that the first tab stop has default content of "this is my todo". Here you can see a more complex scope setup that only expands this snippet under those conditions. Nothing really spectacular here.</p><p>But snippets in Sublime also support some transformations...</p><p><strong>Transformations (Vue component)</strong></p><p>Here is a much more complicated snippet:</p><pre data-lang="xml"><code data-lang="xml"><span>&lt;</span><span>snippet</span><span>&gt;
</span><span>  &lt;</span><span>content</span><span>&gt;&lt;![</span><span>CDATA</span><span>[
</span><span>&lt;!--
</span><span>  ${1:Namespace} Component
</span><span>  Usage:
</span><span>    &lt;${1/([a-zA-Z]+)(?:(\s+?)|\b)/\L\1(?2:\-)\E/g}&gt;&lt;/${1/([a-zA-Z]+)(?:(\s+?)|\b)/\L\1(?2:\-)\E/g}&gt;
</span><span>  ${2:Here is a description of my web component.}
</span><span>  @element ${3:div}
</span><span>  @fires change - This jsdoc tag makes it possible to document events.
</span><span>  @fires submit
</span><span>  @prop {String} title - You can use this jsdoc tag to document properties.
</span><span>  @slot - This is an unnamed slot (the default slot)
</span><span>  @slot start - This is a slot named "start".
</span><span>  @slot end
</span><span> --&gt;
</span><span>
</span><span>&lt;script setup lang="ts"&gt;
</span><span>import type { HTMLAttributes } from 'vue';
</span><span>export interface Props extends /* @vue-ignore */ HTMLAttributes {
</span><span>  ${4:title?: string;${5}}
</span><span>}
</span><span>
</span><span>withDefaults(defineProps&lt;Props&gt;(), {});
</span><span>&lt;/script&gt;
</span><span>
</span><span>&lt;template&gt;
</span><span>  &lt;${3:div} :key="${1/(\w+)(\W*)/\L\1\E(?2:\-)/g}" class="${1/(\w+)(\W*)/\L\1\E(?2:\-)/g}wrapper"&gt;
</span><span>    ${6:&lt;!-- content --&gt;}
</span><span>  &lt;/${3:div}&gt;
</span><span>&lt;/template&gt;
</span><span>]]&gt;&lt;/</span><span>content</span><span>&gt;
</span><span>  </span><span>&lt;!-- Optional: Set a tabTrigger to define how to trigger the snippet --&gt;
</span><span>  &lt;</span><span>tabTrigger</span><span>&gt;sfc&lt;/</span><span>tabTrigger</span><span>&gt;
</span><span>  </span><span>&lt;!-- Optional: Set a scope to limit where the snippet will trigger --&gt;
</span><span>  &lt;</span><span>scope</span><span>&gt;text.html.vue&lt;/</span><span>scope</span><span>&gt;
</span><span>  &lt;</span><span>description</span><span>&gt;Vue single-file component template&lt;/</span><span>description</span><span>&gt;
</span><span>&lt;/</span><span>snippet</span><span>&gt;
</span></code></pre><p>This snippet has some transformations in it. This means we can actually format what the content in the different tab stops will be.</p><p>I won't harp on what is going on too much. Just know that I can format the content in the "Usage" comment as <code>UpperCamelCase</code> and I can format the content in <code>:key</code> to be <code>lower-snake-case</code>. VS Code can do this. The syntax for it is a bit nicer. But I prefer authoring snippets in XML rather than JSON.</p><p>Obviously, I'm twisted.</p><h3 id="project-workspaces">Project workspaces</h3><p>Sublime supports the concept of workspaces under the banner of a "project". All without a plugin, by the way. You can open a folder and save that folder as a project.</p><p>This creates an empty <code>your-project-name.sublime-project</code> which you can really save wherever you like in your project as it has some features to target where the root of the project is.</p><p>This file is just a JSON file and contains editor settings that you are overriding for that specific project. You can target global settings, set rules for specific folders, create a build system, tweak/toggle LSP settings, etc. etc.</p><p>This is a lot like the <code>.vscode/settings.json</code> file from what I understand. I also believe you can do this in Vim with a <code>.vim</code> folder in the root of your project with a feature called "exrc". I haven't used it personally, so I can't speak to it much.</p><p>In my brief flirtations with Neovim and Helix, you need a plugin for this. In Zed, they also have a settings file that can be saved into a project root to get the same thing.</p><p><strong>Including/excluding files and folders</strong></p><p>I think all the editors I referenced above can do this. Not much to share. It is just nice to have an array of file configurations for a project that may or not be in that directory, can be matched with a glob, or just listed explicitly.</p><p>Here is an example of how I would use the project file in a Next.js site:</p><pre data-lang="json"><code data-lang="json"><span>{
</span><span>    "</span><span>folders</span><span>": [
</span><span>        {
</span><span>            "</span><span>file_exclude_patterns</span><span>": [
</span><span>                "</span><span>.gitkeep</span><span>",
</span><span>                "</span><span>*.min.*</span><span>",
</span><span>                "</span><span>*.snap</span><span>",
</span><span>                "</span><span>*.lock</span><span>",
</span><span>                "</span><span>*lock.json</span><span>"
</span><span>            ],
</span><span>            "</span><span>folder_exclude_patterns</span><span>": [
</span><span>                "</span><span>.sanity</span><span>",
</span><span>                "</span><span>.netlify</span><span>",
</span><span>                "</span><span>.next</span><span>",
</span><span>                "</span><span>.vercel</span><span>",
</span><span>                "</span><span>.cache</span><span>",
</span><span>                "</span><span>out</span><span>",
</span><span>                "</span><span>dist</span><span>",
</span><span>                "</span><span>node_modules</span><span>"
</span><span>            ],
</span><span>            "</span><span>path</span><span>": "</span><span>.</span><span>"
</span><span>        }
</span><span>    ],
</span><span>    "</span><span>build_systems</span><span>": [
</span><span>        {
</span><span>            "</span><span>name</span><span>": "</span><span>Project - Build</span><span>",
</span><span>            "</span><span>working_dir</span><span>": "</span><span>$project_path</span><span>",
</span><span>            "</span><span>shell_cmd</span><span>": "</span><span>pnpm run build</span><span>"
</span><span>        },
</span><span>        {
</span><span>            "</span><span>name</span><span>": "</span><span>Project - Test</span><span>",
</span><span>            "</span><span>working_dir</span><span>": "</span><span>$project_path</span><span>",
</span><span>            "</span><span>shell_cmd</span><span>": "</span><span>pnpm run test</span><span>"
</span><span>        },
</span><span>        {
</span><span>            "</span><span>name</span><span>": "</span><span>Project - Open Test</span><span>",
</span><span>            "</span><span>working_dir</span><span>": "</span><span>$project_path</span><span>",
</span><span>            "</span><span>shell_cmd</span><span>": "</span><span>find tests -print | grep $file_base_name.spec | sed -n 1p | xargs subl</span><span>
</span><span>        },
</span><span>        {
</span><span>            "</span><span>name</span><span>": "</span><span>Project - Test Snapshots</span><span>",
</span><span>            "</span><span>working_dir</span><span>": "</span><span>$project_path</span><span>",
</span><span>            "</span><span>shell_cmd</span><span>": "</span><span>pnpm run test:snapshots</span><span>"
</span><span>        },
</span><span>        {
</span><span>            "</span><span>name</span><span>": "</span><span>Project - Test File</span><span>",
</span><span>            "</span><span>working_dir</span><span>": "</span><span>$project_path</span><span>",
</span><span>            "</span><span>shell_cmd</span><span>": "</span><span>pnpm run test -- $file_base_name.spec</span><span>"
</span><span>        },
</span><span>        {
</span><span>            "</span><span>name</span><span>": "</span><span>Project - Test File Snapshot</span><span>",
</span><span>            "</span><span>working_dir</span><span>": "</span><span>$project_path</span><span>",
</span><span>            "</span><span>shell_cmd</span><span>": "</span><span>pnpm run test:snapshots -- $file_base_name.spec</span><span>"
</span><span>        },
</span><span>        {
</span><span>            "</span><span>name</span><span>": "</span><span>Project - Format README.md</span><span>",
</span><span>            "</span><span>working_dir</span><span>": "</span><span>$project_path</span><span>",
</span><span>            "</span><span>shell_cmd</span><span>": "</span><span>mdformat $file</span><span>"
</span><span>        }
</span><span>    ],
</span><span>    "</span><span>settings</span><span>": {
</span><span>        "</span><span>match_brackets_angle</span><span>": </span><span>true</span><span>,
</span><span>        "</span><span>tab_size</span><span>": </span><span>2</span><span>,
</span><span>        "</span><span>translate_tabs_to_spaces</span><span>": </span><span>true</span><span>,
</span><span>        "</span><span>jsdocs_return_tag</span><span>": "</span><span>@return</span><span>",
</span><span>        "</span><span>lsp_format_on_save</span><span>": </span><span>false</span><span>,
</span><span>        "</span><span>lsp_code_actions_on_save</span><span>": {
</span><span>            "</span><span>source.organizeImports</span><span>": </span><span>false</span><span>,
</span><span>            "</span><span>source.fixAll.eslint</span><span>": </span><span>true
</span><span>        },
</span><span>        "</span><span>LSP</span><span>": {
</span><span>            "</span><span>formatters</span><span>": {
</span><span>                "</span><span>source.ts</span><span>": "</span><span>LSP-biome</span><span>",
</span><span>                "</span><span>source.js</span><span>": "</span><span>LSP-biome</span><span>",
</span><span>                "</span><span>source.tsx</span><span>": "</span><span>LSP-biome</span><span>",
</span><span>                "</span><span>source.jsx</span><span>": "</span><span>LSP-biome</span><span>",
</span><span>                "</span><span>text.html.basic</span><span>": "</span><span>LSP-biome</span><span>"
</span><span>            },
</span><span>            "</span><span>LSP-eslint</span><span>": {
</span><span>                "</span><span>enabled</span><span>": </span><span>false
</span><span>            },
</span><span>            "</span><span>LSP-biome</span><span>": {
</span><span>                "</span><span>enabled</span><span>": </span><span>true
</span><span>            }
</span><span>        }
</span><span>    }
</span><span>}
</span></code></pre><p>I'm doing a lot here. Setting files to ignore, folders to exclude from indexing, setting build commands that use <code>pnpm</code> as well as <code>mdformat</code>, setting from editor setting for tab spacing, and finally a few LSP tweaks that make sense for a Next project.</p><p>VS Code can do all of what I've listed here. But they split it up into different files. Kinda annoying.</p><p><strong>Configure plugin settings per project</strong></p><p>You can also configure plugin settings per project. Here is how I would add project settings for the "syntax override" plugin. This plugin forces the editor to use a specific syntax for certain files that match a given pattern:</p><pre data-lang="json"><code data-lang="json"><span>{
</span><span>    </span><span>// everything is the same as above but this is added on the end
</span><span>    "</span><span>syntax_override</span><span>": {
</span><span>        "</span><span>\\</span><span>.env.*$</span><span>": [
</span><span>            "</span><span>ShellScript</span><span>",
</span><span>            "</span><span>Shell-Unix-Generic</span><span>"
</span><span>        ],
</span><span>        "</span><span>\\</span><span>.*rc$</span><span>": [
</span><span>            "</span><span>JSON</span><span>",
</span><span>            "</span><span>JSON</span><span>"
</span><span>        ],
</span><span>        "</span><span>\\</span><span>.ts.snap$</span><span>": [
</span><span>            "</span><span>JavaScript</span><span>",
</span><span>            "</span><span>TypeScript</span><span>"
</span><span>        ],
</span><span>        "</span><span>\\</span><span>.css$</span><span>": [
</span><span>            "</span><span>Tailwind CSS</span><span>",
</span><span>            "</span><span>Tailwind CSS</span><span>"
</span><span>        ]
</span><span>    }
</span><span>}
</span></code></pre><p>I don't always want all <code>.css</code> files to be highlighted with the Tailwind syntax. But in this project I do. So I can set it locally here, and when I open a <code>.css</code> file in this project, it will switch the syntax for me. Nice!</p><p><strong>Add build systems per project</strong></p><p>You can see in the example above that I have set build systems on this specific project. You can do this in VS Code with "tasks". Zed has this feature as well and calls it "tasks" too. I just find it annoying that they are in their own files. I guess that makes them more portable.</p><p>I just like my project configuration <em>in one central place</em>. I must be nuts.</p><h3 id="build-systems">Build systems</h3><p>I've touched on build systems a bit already. But in summation, they are just tasks you can run in your project.</p><p>Sometimes they call a global command (like <code>curl</code>), sometimes a local dependency is installed with a package manager (think some binary in <em>node_modules/.bin/</em>), or maybe just runs a command already setup in your other tools (like <code>composer run test</code> or <code>php artisan migrate:fresh</code>) that run in the root of the project but need some context.</p><p><strong>Can also be provided by a plugin</strong></p><p>A plugin can provide build systems. The neat thing is they are just Sublime files. JSON that ends in <code>.sublime-build</code>. Like snippets. So they are really portable too. Just like the other editors with their <code>tasks.json</code> file.</p><p><strong>Just a simple file</strong></p><p>Here is one I saved in my "User" directory called <code>dot-env-linter.sublime-build</code>:</p><pre data-lang="json"><code data-lang="json"><span>{
</span><span>  "</span><span>cmd</span><span>": ["</span><span>dotenv-linter</span><span>", "</span><span>$file</span><span>"],
</span><span>  "</span><span>selector</span><span>": "</span><span>source.shell</span><span>",
</span><span>  "</span><span>file_patterns</span><span>": ["</span><span>\\</span><span>.env.*$</span><span>"],
</span><span>  "</span><span>file_regex</span><span>": "</span><span>^(.*?)</span><span>\\</span><span>:(</span><span>\\</span><span>d+)(</span><span>\\</span><span>s)(.*+)</span><span>"
</span><span>}
</span></code></pre><p>You can see there are some special variables (like <code>$file</code>) that will expand based on context. In this case, that one is the full path to the currently active file.</p><p>Here is my <code>phpmd.sublime-build</code> with a bit more flavour:</p><pre data-lang="json"><code data-lang="json"><span>{
</span><span>  "</span><span>cmd</span><span>": ["</span><span>phpmd</span><span>", "</span><span>${file}</span><span>", "</span><span>text</span><span>", "</span><span>codesize,unusedcode,naming</span><span>"],
</span><span>  "</span><span>path</span><span>": "</span><span>${PATH}</span><span>",
</span><span>  "</span><span>working_dir</span><span>": "</span><span>$project_path</span><span>",
</span><span>  "</span><span>selector</span><span>": "</span><span>source.php</span><span>"
</span><span>}
</span></code></pre><p>You can see some more vars here for the project path as well as a reference to my system PATH. Of course, we got a nice lil scope as well.</p><p><strong>Build systems with 🐍</strong></p><p><a rel="noopener nofollow noreferrer" href="https://www.sublimetext.com/docs/build_systems.html#advanced-example" target="_blank">You can actually write build systems in Python as well</a>. So if you need something more complicated, you can reach for that.</p><p>You don't even need to make any semblance of a plugin. You can toss a <code>.py</code> file in your "User" directory and implement a class that takes a <code>sublime_plugin.WindowCommand</code>.</p><h3 id="multiple-cursors">Multiple cursors</h3><p>Yep. Multiple cursors. I use them all the time. I know that the "vim" way is to start recording a macro, apply the changes on a single line, and then replay that macro on all the lines you want to change. Or do some <code>s//g</code> fu for a fancy find and replace. I get it. I just don't like it.</p><p>Most of the editors these days have multiple cursors. Including some terminal editors like Helix. I have tried Helix and I think it is a lot closer to what I would want from a modern editor than my previous terminal editor of Neovim + LazyVim.</p><h3 id="key-mouse-bindings">Key/mouse bindings</h3><p>The key and mouse bindings are what you would expect from a modern editor. It is basically the same as VS Code. Nothing exhilarating here. But I do like the way conceptual key bindings are handled.</p><p><strong>Contextual key bindings</strong></p><p>Like any good editor, Sublime supports contextual key bindings.</p><p>I think the best usage I have seen for contextual key bindings is in <a rel="noopener nofollow noreferrer" href="https://github.com/sublimehq/Packages/blob/master/JavaScript/Default.sublime-keymap#L11-L17" target="_blank">the JavaScript language support package</a> that comes with Sublime.</p><p>When you have an active selection, and that selection is not empty, and you are inside a string-like scope, then ` will wrap that selection with `. Basically, it will wrap your selection as a template string. Handy!</p><p><strong>Just a simple file</strong></p><p>Like the build systems and snippets, key bindings are just saved in a file that ends with <code>.sublime-keymap</code>. They can be in your "User" directory or in a plugin. Unfortunately, unlike build systems, they cannot be saved on a per-project basis - from what I can tell.</p><p>You can also have key bindings for different platforms. They are named as follows:</p><pre><code><span>Default (Windows).sublime-keymap
</span><span>Default (OSX).sublime-keymap
</span><span>Default (Linux).sublime-keymap
</span></code></pre><h3 id="included-niceness">Included niceness</h3><p>These are just some notable mentions of things I like:</p><p><strong>Python all the way down</strong></p><p>Given how Python is probably the most popular language, at least <a rel="noopener nofollow noreferrer" href="https://www.theregister.com/2024/11/05/python_dethrones_javascript_github/" target="_blank">the last time GitHub checked</a>, I'm surprised this isn't the go-to editor. I don't think you need to know python to use Sublime but it helps if you want to craft a nice plugin.</p><p>One more thing to add around authoring plugins, they are super simple. It is just a <code>.py</code> file in a folder. No build system, external dependencies, or ever-changing APIs to navigate.</p><p>You can see <a rel="noopener nofollow noreferrer" href="https://github.com/james2doyle/sublime_scratchpad/blob/master/Scratchpad.py" target="_blank">my recent fork of the Scratchpad plugin</a> and how the whole thing is mostly powered by a single python file.</p><p>This plugin has been trucking along for 11 years. I can't imagine any VS Code plugin lasting that long!</p><p><strong>Macros</strong></p><p>Yep you can record macros in Sublime. You can also save them to... a file! Then put that file (<code>.sublime-macro</code>) in a plugin, or, of course, your "User" folder. The macro is just an array of key presses. They also support scopes and can be bound to a key combo. Pretty sweet.</p><p>This is one feature that VS Code has not stolen - I mean implemented - and that requires an additional plugin to have. Of course the Vimmers have had this for decades.</p><p><strong>Diff hunks (revert or show)</strong></p><p>Sublime supports viewing inline diff hunks. Handy for when you don't want to dive into the diff of a file. You can just ask to see the diff hunk for that line or group of lines. You can also revert just as easily.</p><p><strong>Case conversion and line permute functions</strong></p><p>There are some handy case conversion functions that are built in. Nice with multiple cursors. VS Code has a couple of conversion choices. Sublime has 8 different case conversions built in.</p><p><strong>Package control and repo URLs for packages</strong></p><p>Package control packages can be installed from the central repository. But you can also install them from a git repo URL. You can also clone a repo to your "Packages" folder, and it will work too. No marketplace or anything like that is required.</p><p>This is really handy if you have forked a package and just want to install your fork.</p><p><strong>All the config and settings are plain files</strong></p><p>Since the whole of a Sublime setup is mostly plain files, that makes it really easy to sync your setup across multiple computers. I actually symlink my Sublime folder to Dropbox. So any changes I make will be shared across all my computers that use it.</p><p>This isn't unique to Sublime. I think it is just a benefit of having tools that use plain text-driven configuration.</p><p><strong>Distraction-free mode</strong></p><p>Sublime has a "distraction free mode" which will full-screen your editor and focus the content to the middle of the screen. I am using it to write this post right now!</p><h3 id="wish-list">Wish List</h3><p>Of course there could be a few things that could be better.</p><p><strong>Better docs</strong></p><p>I do find the docs for developing plugins to be sparse. There are doc sites. There are two big ones. One for the "official" docs that document the APIs. There is also another site that is tagged as the "unofficial" docs.</p><p>Usually when I want to know how to do something in a plugin, I just read the source of another plugin that does something I want to emulate. It isn't a bad way to learn, it is just a bit tedious.</p><p><strong>Plugin development DX</strong></p><p>Speaking of building plugins, there does not seem to be any "stubs" for the Sublime python API. I am by no means a python guru. I only use it in Sublime, and I usually forget everything once I finish what I am trying to do. But I think there could be some "plugin starter template" that could include a Pyright setup and some basic guide for how to get going.</p><p>I also find that getting plugins on the Package Control site to be quite a chore. You need to open a PR to a repo and put your repo into a list based on where it goes into an alphabet. It doesn't allow forks and plugins that are too similar. There is also no way to mark a package as "abandoned".</p><p>I like the way Composer/Packagist does packages. You create a repo, submit that URL to the Packagist site, and it will automatically keep track of it for you. You release a new version by just using git tags and releases.</p><p>NPM is also a bit nicer. But forcing an NPM account and having to juggle the mixing of git tags and the <code>package.json</code> "version" key to be a bit unclear at times.</p><p><strong>Key/Mouse bindings per project</strong></p><p>Simple one here. It would be nice to have key and mouse bindings on a project level. I don't know how often I would really use it, but it would be nice to have just for those projects that have tedious tasks or macros that I want to run under specific scopes.</p><h2 id="in-summation">In Summation</h2><p>I like Sublime. I think it is still incredibly capable in 2025. If you are in search of something a little snappier, classier, and not riddled with AI slop, then give it a try.</p><p>I doubt I can pry your Vim from your <code>HJKL</code> riddled right hand, but if you have been let down or uninspired by the latest offerings when it comes to editors, you might find Sublime still has a lot to offer.</p><p><em>Note: if any of my information is wrong or outdated, I will update it accordingly</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Science YouTuber physicsgirl (Dianna Cowern) stands for the first time in 2 yrs (641 pts)]]></title>
            <link>https://www.youtube.com/shorts/2ntx91cOYEc</link>
            <guid>42862118</guid>
            <pubDate>Wed, 29 Jan 2025 06:16:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/shorts/2ntx91cOYEc">https://www.youtube.com/shorts/2ntx91cOYEc</a>, See on <a href="https://news.ycombinator.com/item?id=42862118">Hacker News</a></p>
Couldn't get https://www.youtube.com/shorts/2ntx91cOYEc: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI says it has evidence DeepSeek used its model to train competitor (122 pts)]]></title>
            <link>https://www.ft.com/content/a0dfedd1-5255-4fa9-8ccc-1fe01de87ea6</link>
            <guid>42861475</guid>
            <pubDate>Wed, 29 Jan 2025 04:21:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/a0dfedd1-5255-4fa9-8ccc-1fe01de87ea6">https://www.ft.com/content/a0dfedd1-5255-4fa9-8ccc-1fe01de87ea6</a>, See on <a href="https://news.ycombinator.com/item?id=42861475">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a data-trackable="a11y-skip-to-help" href="https://www.ft.com/accessibility">Accessibility help</a><a data-trackable="a11y-skip-to-navigation" href="#site-navigation">Skip to navigation</a><a data-trackable="a11y-skip-to-content" href="#site-content">Skip to content</a><a data-trackable="a11y-skip-to-footer" href="#site-footer">Skip to footer</a></p><div id="barrier-page"><div id="heroOffer-Hero offers-5bd4a3bf-069f-4fc1-bf4e-faf8ba92eb0b" data-component="heroOffer" data-component-unique-name="Hero offers"><div data-o-grid-colspan="12 L6"><p><span></span><span></span><span></span><span>Subscribe to unlock this article</span><span></span></p></div><div data-o-grid-colspan="12 L6"><p><h2><span>Join FT Edit</span></h2><h2><strong><span>Only </span><span>CHF50</span><span> a year</span></strong></h2></p><p><span>Get 2 months free with an annual subscription at </span><span>was </span><span>CHF60</span><span> </span><span>now </span><span>CHF50</span><span>.
Access to eight fresh articles a day, hand-picked by senior editors. Selected to feed your curiosity.</span></p></div></div><div id="recommendedOffers-Recommended Offers" data-component="recommendedOffers" data-component-unique-name="Recommended Offers"><p><h2 data-o-grid-colspan="12">Explore more offers.</h2></p><div data-o-grid-colspan="12"><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_trial.svg?source=next-barrier-page&amp;format=svg" alt=""></p></div><p><span>CHF1</span><span> for 4 weeks</span></p><p><span>Then </span><span>CHF85</span><span> per month. Complete digital access to quality FT journalism. Cancel anytime during your trial.</span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_standard.svg?source=next-barrier-page&amp;format=svg" alt=""></p><p><h3>Standard Digital</h3></p></div><p><span>was </span><span>CHF660</span><span> </span><span>now </span><span>CHF395</span><span> per year</span></p><p><span>Save now on essential digital access to quality FT journalism on any device.</span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_premium.svg?source=next-barrier-page&amp;format=svg" alt=""></p></div><p><span>CHF85</span><span> per month</span></p><p><span>Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.</span></p></div></div></div><div data-component="subscriptionOptions" data-component-unique-name="Subscription Options Offers API"><h2>Explore our full range of subscriptions.</h2><div><div><p>Discover all the plans currently available in your country</p></div><div><p>Digital access for organisations. Includes exclusive features and content.</p></div></div></div><div data-component="whyFT" data-component-unique-name="Why FT"><div><h2>Why the FT?</h2><p>See why over a million readers pay to read the Financial Times.</p></div><p><a href="https://subs.ft.com/whytheft?ft-content-uuid=a0dfedd1-5255-4fa9-8ccc-1fe01de87ea6">Find out why</a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Probing If DeepSeek-Linked Group Improperly Obtained OpenAI Data (117 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2025-01-29/microsoft-probing-if-deepseek-linked-group-improperly-obtained-openai-data</link>
            <guid>42861150</guid>
            <pubDate>Wed, 29 Jan 2025 03:23:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2025-01-29/microsoft-probing-if-deepseek-linked-group-improperly-obtained-openai-data">https://www.bloomberg.com/news/articles/2025-01-29/microsoft-probing-if-deepseek-linked-group-improperly-obtained-openai-data</a>, See on <a href="https://news.ycombinator.com/item?id=42861150">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/help">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple and SpaceX link up to support Starlink satellite network on iPhones (225 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2025-01-29/apple-and-spacex-link-up-to-support-starlink-satellite-network-on-iphones</link>
            <guid>42860752</guid>
            <pubDate>Wed, 29 Jan 2025 02:22:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2025-01-29/apple-and-spacex-link-up-to-support-starlink-satellite-network-on-iphones">https://www.bloomberg.com/news/articles/2025-01-29/apple-and-spacex-link-up-to-support-starlink-satellite-network-on-iphones</a>, See on <a href="https://news.ycombinator.com/item?id=42860752">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/help">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It's official: Research has found that libraries make everything better (222 pts)]]></title>
            <link>https://lithub.com/its-official-research-has-found-that-libraries-make-everything-better/</link>
            <guid>42860240</guid>
            <pubDate>Wed, 29 Jan 2025 01:11:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lithub.com/its-official-research-has-found-that-libraries-make-everything-better/">https://lithub.com/its-official-research-has-found-that-libraries-make-everything-better/</a>, See on <a href="https://news.ycombinator.com/item?id=42860240">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
				
				
				
				<p>Science has backed up what many of us have long been saying: the library <em>rocks</em>. <a href="https://www.nypl.org/spotlight/libraries-well-being-report" target="_blank">A study from the New York Public Library</a> surveyed 1,974 users on how the library makes them feel and how it affects their lives, and the results are overwhelmingly positive.</p>
<p>The researchers’ analysis (which used positive psychology’s PERMA model, if that means anything to you) discovered that libraries are good for people, their well-being, and their communities. Not only that, but the positive societal impacts are more pronounced in lower-income communities, even more reason to make sure we’re funding and supporting libraries. Don’t let the ghosts of Reagan and Thatcher tell you otherwise, government can help people!</p>
<p>Some top-line statistics from the study:</p>
<p>– 92% of respondents reported feeling somewhat to very “calm / peaceful” after visiting the Library<br>
– 74% of respondents reported that their library use positively affects how equipped they feel to cope with the world<br>
– 90% of respondents reported that their Library use positively affects how much they love to learn new things<br>
– 88% of respondents reported that their Library use has supported their personal growth</p>
<p>Those are some big numbers and some uniformly good news — people are not only feeling better about themselves and their world after a visit to the library, but they’re feeling more secure in their world too.</p>
<p>The individual outcomes are undeniable: 89% of respondents said that the library had a positive effect on them having “more appreciation for things [they] did not know much about before” and 77% said the library made them feel “that what [they] do in [their] life is valuable and worthwhile.” You can get books at the library, but you can also fight your existential dread.</p>
<p>People are also moving away from doomerism in the stacks: 82% of visitors said use of the library “positively affects how optimistic they are about the future.” That’s not just for people visiting the brick-and-mortar library either: 58% of e-only users also get a sense of optimism from library interactions. It honestly feels like a miracle that anything connected to the internet would make people feel good, so this is a big win.</p>
<p>The community feelings the library engenders are very encouraging too: 75% say libraries gave them more positive feelings of “empathy towards others who may be different from [them],” 72% said it made them feel more connected to others, 66% felt “seen and heard,” and 70% felt like they are “part of a community.” Most touching to me is that 59% said the library had a positive effect on their “feeling that there are people in your life who really care about [them].”</p>
<p>What I find most charming in this study are the quotes, which the researchers highlight in “Patron Voices” sections. They’re full of great little lines, like people calling the library “a touchstone” and “a place to rely on,” and that “knowing it’s there makes me feel better about my life in the city.”</p>
<p>I really had to hold myself back from including too many of these patron quotes, because in a month when I’ve been feeling so, so down, reading all the nice things people have to say about the library felt like a hug from an old friend. Here are just some of them:</p>
<p>– “Space where I can just be me”<br>
– “Books transport me”<br>
– “Islands of calm, and I find balance within them”<br>
– “It offers us hope that we can do something, that we can make a change, that we can advance”<br>
– “Surrounded me with life’s possibilities”<br>
– “Makes me feel useful”<br>
– “The library gives you a sense of direction”</p>
<p>Tell me you didn’t tear up at that, and pal, <em>I’ll show you a liar</em>. Also these quotes are a great opportunity for some uplifting found poetry, if anyone’s looking for a new chapbook project.</p>
<p>So the takeaway? If you’re feeling unmotivated and unconnected, the library has now been scientifically proven to improve your well-being, the perfect antidote to all the push alerts and doomscrolling that’s bringing you down.</p>
				
										
									
				

				

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Discovery Coding (228 pts)]]></title>
            <link>https://jimmyhmiller.github.io/discovery-coding</link>
            <guid>42860128</guid>
            <pubDate>Wed, 29 Jan 2025 00:53:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jimmyhmiller.github.io/discovery-coding">https://jimmyhmiller.github.io/discovery-coding</a>, See on <a href="https://news.ycombinator.com/item?id=42860128">Hacker News</a></p>
Couldn't get https://jimmyhmiller.github.io/discovery-coding: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Goodbye, Slopify (329 pts)]]></title>
            <link>https://alexeystar.com/blog/slopify/</link>
            <guid>42860113</guid>
            <pubDate>Wed, 29 Jan 2025 00:51:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alexeystar.com/blog/slopify/">https://alexeystar.com/blog/slopify/</a>, See on <a href="https://news.ycombinator.com/item?id=42860113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><figure><img src="https://alexeystar.com/blog/slopify/slopify2.png" alt="Screenshot from Spotify app suggesting AI music"><figcaption></figcaption></figure><p>My Discover Weekly playlist on Spotify is finally poisoned by AI slop.</p><p>There’s no way I’m going to continue a subscription to a streaming service that suggests AI-generated music with AI-generated album covers. A bloated, Electron-based desktop application with a terrible UI is only going to add up.</p><p>So goodbye, Slopify. Thank you for pushing me towards supporting more real artists and buying music that I can actually own.</p><p><time>2025-01-26</time></p><p><span><a href="https://alexeystar.com/blog/cats-2024/">←<br>Cat-alog of the Year</a></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek's AI breakthrough bypasses industry-standard CUDA, uses PTX (125 pts)]]></title>
            <link>https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead</link>
            <guid>42859909</guid>
            <pubDate>Wed, 29 Jan 2025 00:20:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead">https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead</a>, See on <a href="https://news.ycombinator.com/item?id=42859909">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA-1200-80.jpg" alt="Nvidia Hopper H100 GPU and DGX systems" srcset="https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/Wb8pUe4iWjBf2Gx8ky5rLA.jpg" data-pin-nopin="true" fetchpriority="high" crossorigin="anonymous">
</picture>
</div>
<figcaption>
<span>(Image credit: Nvidia)</span>
</figcaption>
</div>

<div id="article-body">
<p>DeepSeek made quite a splash in the AI industry by training its Mixture-of-Experts (MoE) language model with 671 billion parameters <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/chinese-ai-company-says-breakthroughs-enabled-creating-a-leading-edge-ai-model-with-11x-less-compute-deepseeks-optimizations-highlight-limits-of-us-sanctions" data-before-rewrite-localise="https://www.tomshardware.com/tech-industry/artificial-intelligence/chinese-ai-company-says-breakthroughs-enabled-creating-a-leading-edge-ai-model-with-11x-less-compute-deepseeks-optimizations-highlight-limits-of-us-sanctions">using a cluster featuring 2,048 Nvidia H800 GPUs in about two months</a>, showing 10X higher efficiency than AI industry leaders like Meta. The breakthrough was achieved by implementing tons of fine-grained optimizations and usage of Nvidia's assembly-like PTX (Parallel Thread Execution) programming instead of Nvidia's CUDA, according to an analysis from Mirae Asset Securities Korea cited by <a data-analytics-id="inline-link" href="https://x.com/Jukanlosreve/status/1883304958432624881" data-url="https://x.com/Jukanlosreve/status/1883304958432624881" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">@Jukanlosreve</a>.</p><p><a data-analytics-id="inline-link" href="https://docs.nvidia.com/cuda/parallel-thread-execution/" data-url="https://docs.nvidia.com/cuda/parallel-thread-execution/" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">Nvidia's PTX</a> (Parallel Thread Execution) is an intermediate instruction set architecture designed by Nvidia for its GPUs. PTX sits between higher-level GPU programming languages (like CUDA C/C++ or other language frontends) and the low-level machine code (streaming assembly, or SASS). PTX is a close-to-metal ISA that exposes the GPU as a data-parallel computing device and, therefore, allows fine-grained optimizations, such as register allocation and thread/warp-level adjustments, something that CUDA C/C++ and other languages cannot enable. Once PTX is into SASS, it is optimized for a specific generation of Nvidia GPUs.&nbsp;</p><p>For example, when training its V3 model, DeepSeek reconfigured Nvidia's H800 GPUs: out of 132 streaming multiprocessors, it allocated 20 for server-to-server communication, possibly for compressing and decompressing data to overcome connectivity limitations of the processor and speed up transactions. To maximize performance, DeepSeek also implemented advanced pipeline algorithms, possibly by making extra fine thread/warp-level adjustments.&nbsp;</p><p>These modifications go far beyond standard CUDA-level development, but they are notoriously difficult to maintain. Therefore, this level of optimization reflects the exceptional skill of DeepSeek's engineers. The global GPU shortage, amplified by U.S. restrictions, has compelled companies like DeepSeek to adopt innovative solutions, and DeepSeek has made a breakthrough. However, it is unclear how much money DeepSeek had to invest in development to achieve its results.&nbsp;</p><p>The breakthrough disrupted the market as some investors believed that the need for high-performance hardware for new AI models would get lower, hurting the sales of companies like Nvidia. Industry veterans, such as Intel Pat Gelsinger, ex-chief executive of Intel, <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/ex-intel-ceo-pat-gelsinger-loads-up-on-nvidia-stock-says-the-markets-reaction-to-deepseek-is-wrong" data-before-rewrite-localise="https://www.tomshardware.com/tech-industry/artificial-intelligence/ex-intel-ceo-pat-gelsinger-loads-up-on-nvidia-stock-says-the-markets-reaction-to-deepseek-is-wrong">believe that applications like AI can take advantage of all computing power they can access</a>. As for DeepSeek's breakthrough, Gelsinger sees it as a way to add AI to a broad set of inexpensive devices in the mass market.&nbsp;</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-8vJHmRYXBTFYuqGQaFazAe"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div>
</div>



<!-- Drop in a standard article here maybe? -->



<div id="slice-container-authorBio-8vJHmRYXBTFYuqGQaFazAe"><p>Anton Shilov is a contributing writer at Tom’s Hardware. Over the past couple of decades, he has covered everything from CPUs and GPUs to supercomputers and from modern process technologies and latest fab tools to high-tech industry trends.</p></div>
</section>





<div id="slice-container-relatedArticles"><p><h3>Most Popular</h3></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Departing the New York Times (157 pts)]]></title>
            <link>https://contrarian.substack.com/p/departing-the-new-york-times</link>
            <guid>42859669</guid>
            <pubDate>Tue, 28 Jan 2025 23:47:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://contrarian.substack.com/p/departing-the-new-york-times">https://contrarian.substack.com/p/departing-the-new-york-times</a>, See on <a href="https://news.ycombinator.com/item?id=42859669">Hacker News</a></p>
Couldn't get https://contrarian.substack.com/p/departing-the-new-york-times: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Deferred resignation email to federal employees (153 pts)]]></title>
            <link>https://www.opm.gov/fork</link>
            <guid>42859552</guid>
            <pubDate>Tue, 28 Jan 2025 23:31:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.opm.gov/fork">https://www.opm.gov/fork</a>, See on <a href="https://news.ycombinator.com/item?id=42859552">Hacker News</a></p>
Couldn't get https://www.opm.gov/fork: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Job trends of HN Who Is Hiring? (106 pts)]]></title>
            <link>https://hnhiring.com/trends</link>
            <guid>42858947</guid>
            <pubDate>Tue, 28 Jan 2025 22:28:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hnhiring.com/trends">https://hnhiring.com/trends</a>, See on <a href="https://news.ycombinator.com/item?id=42858947">Hacker News</a></p>
Couldn't get https://hnhiring.com/trends: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek's multi-head latent attention and other KV cache tricks (273 pts)]]></title>
            <link>https://www.pyspur.dev/blog/multi-head-latent-attention-kv-cache-paper-list</link>
            <guid>42858741</guid>
            <pubDate>Tue, 28 Jan 2025 22:11:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pyspur.dev/blog/multi-head-latent-attention-kv-cache-paper-list">https://www.pyspur.dev/blog/multi-head-latent-attention-kv-cache-paper-list</a>, See on <a href="https://news.ycombinator.com/item?id=42858741">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="blog"><div><!--$--><p><img alt="DeepSeek's Multi-Head Latent Attention and Other KV Cache Tricks" loading="lazy" width="1920" height="1080" decoding="async" data-nimg="1" srcset="https://www.pyspur.dev/_next/image?url=%2Fblog%2Fkv-cache%2Fmla.png&amp;w=1920&amp;q=75&amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP 1x, https://www.pyspur.dev/_next/image?url=%2Fblog%2Fkv-cache%2Fmla.png&amp;w=3840&amp;q=75&amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP 2x" src="https://www.pyspur.dev/_next/image?url=%2Fblog%2Fkv-cache%2Fmla.png&amp;w=3840&amp;q=75&amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP"></p><!--/$--><div><!--$--><p><time datetime="2025-01-21">January 21, 2025 (1w ago)</time><span>•</span></p><!--/$--></div><div><a target="_blank" rel="noopener noreferrer" href="https://twitter.com/jeankaddour"><img alt="Jean Kaddour" loading="lazy" width="40" height="40" decoding="async" data-nimg="1" srcset="https://www.pyspur.dev/_next/image?url=%2Fauthor.jpg&amp;w=48&amp;q=75&amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP 1x, https://www.pyspur.dev/_next/image?url=%2Fauthor.jpg&amp;w=96&amp;q=75&amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP 2x" src="https://www.pyspur.dev/_next/image?url=%2Fauthor.jpg&amp;w=96&amp;q=75&amp;dpl=dpl_3Agp3pPUHQkFT3ejk3F3i1kgTjfP"></a></div><article><p><strong>Overview</strong>:</p>
<ol>
<li><strong>Introduction</strong>: We'll explore how Key-Value (KV) caches make language models like ChatGPT and DeepSeek faster at generating text, by making a clever trade-off between memory usage and computation time.</li>
<li><strong>MLA and other Tricks</strong>: We'll then look at 11 recent research papers, including <strong>DeepSeek's Multi-head Latent Attention (MLA)</strong>, that build upon this basic idea to make LLM inference even more time-efficient.</li>
</ol>
<hr>
<h2>Understanding the Problem: Why Text Generation is Slow</h2>
<p>Let's start with a simple analogy. Imagine you're writing a story, and for each new word you write, you need to re-read the entire story so far to maintain consistency. The longer your story gets, the more time you spend re-reading. This is exactly what large language models face during text generation!</p>
<h3>The Basic Building Block: Self-Attention</h3>
<p>At the heart of modern language models is a mechanism called <strong>self-attention</strong>. For a sequence of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> tokens (think of tokens as roughly corresponding to words), each token needs to "look at" or "attend to" all other tokens to understand the context.</p>
<p>This looking-at-everything process has a computational cost that grows with the sequence length:</p>
<ul>
<li>For <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> tokens, each token needs to look at all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> tokens</li>
<li>This means the cost is proportional to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi><mo>=</mo><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n \times n = n^2</annotation></semantics></math></span></span></li>
<li>In mathematical notation, we write this as <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span></span> complexity</li>
</ul>
<h3>The Real Problem: Generating Text One Token at a Time</h3>
<p>When a language model generates text, it does so one token at a time, and this is where things get computationally expensive:</p>
<ol>
<li><strong>First token</strong>: Look at 1 token (cost: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>1</mn><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1^2)</annotation></semantics></math></span></span>)</li>
<li><strong>Second token</strong>: Look at 2 tokens (cost: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>2</mn><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(2^2)</annotation></semantics></math></span></span>)</li>
<li><strong>Third token</strong>: Look at 3 tokens (cost: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>3</mn><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(3^2)</annotation></semantics></math></span></span>)</li>
<li>And so on until the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span>-th token: Look at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> tokens (cost: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span></span>)</li>
</ol>
<p>If we add up all these costs for generating a sequence of length <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span>, we get:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>1</mn><mn>2</mn></msup><mo>+</mo><msup><mn>2</mn><mn>2</mn></msup><mo>+</mo><msup><mn>3</mn><mn>2</mn></msup><mo>+</mo><mo>⋯</mo><mo>+</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo><mo>≈</mo><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1^2 + 2^2 + 3^2 + \dots + n^2) \approx O(n^3)</annotation></semantics></math></span></span></span>
<p>This <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics></math></span></span> cost means that as your text gets longer, the generation time grows <strong>extremely quickly</strong>. For example, generating a sequence twice as long takes roughly <strong>eight times</strong> as long! Clearly, we need a better approach.</p>
<hr>
<h2>The Solution: Key-Value (KV) Cache</h2>
<p>The key insight behind KV caching is that we're doing a lot of redundant work. When generating each new token, we're recomputing things for all previous tokens that we've already processed before. Let's see how we can fix this.</p>
<h3>What is a Key-Value Cache?</h3>
<p>Think of a KV cache like a smart notepad where we write down important information about each token the first time we see it. For each token, we compute and store two things:</p>
<ol>
<li>A <strong>key</strong> (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span>): Think of this as an addressing mechanism - it helps determine how relevant this token is to future tokens</li>
<li>A <strong>value</strong> (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span>): Think of this as the actual information that gets used when this token is found to be relevant</li>
</ol>
<p>Mathematically, we compute these as:</p>
<ul>
<li>Key: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mi>x</mi><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">k = x W_K</annotation></semantics></math></span></span> (where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span></span> is the token and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>K</mi></msub></mrow><annotation encoding="application/x-tex">W_K</annotation></semantics></math></span></span> is a learned transformation)</li>
<li>Value: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mi>x</mi><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">v = x W_V</annotation></semantics></math></span></span> (where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_V</annotation></semantics></math></span></span> is another learned transformation)</li>
</ul>
<p>When generating a new token, we use its query (computed similarly to keys) to find relevant information in our cache by comparing it with all stored keys. The matching values are then used to help generate the token.</p>
<h3>How the KV Cache Makes Things Faster</h3>
<p>With a KV cache, the process becomes much more efficient:</p>
<ol>
<li>When we see a new token, we only need to compute its key and value <strong>once</strong></li>
<li>For all future tokens, we can just look up these pre-computed values from our cache</li>
<li>This means each new token only needs to do a small amount of new work, instead of redoing all previous computations</li>
</ol>
<p>The trade-off is clear:</p>
<ul>
<li>We use more memory to store all the keys and values. For a model with:
<ul>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span></span> layers</li>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> attention heads</li>
<li>Sequence length <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span></li>
<li>Key/value dimension <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span></span>
The total memory cost is <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>n</mi><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">L \times H \times n \times d_k \times 2</annotation></semantics></math></span></span> values (the factor of 2 accounts for both keys and values).
This grows linearly with sequence length (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span></span>), but the constant factors can be substantial for large models.</li>
</ul>
</li>
<li>But in return, we reduce the computation cost from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics></math></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span></span></li>
</ul>
<p>To understand why it's <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span></span>, let's look at the cost at each step:</p>
<ol>
<li><strong>Step 1</strong>: Process 1 token → cost <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math></span></span></li>
<li><strong>Step 2</strong>: Process 1 new token + look at 1 cached token → cost <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(2)</annotation></semantics></math></span></span></li>
<li><strong>Step 3</strong>: Process 1 new token + look at 2 cached tokens → cost <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(3)</annotation></semantics></math></span></span></li>
<li>And so on...</li>
</ol>
<p>Adding these up:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mn>2</mn><mo>+</mo><mn>3</mn><mo>+</mo><mo>⋯</mo><mo>+</mo><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1 + 2 + 3 + \dots + n) = O(n^2)</annotation></semantics></math></span></span></span>
<p>This is a dramatic improvement over <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics></math></span></span>! While we still have to do the fundamental work of looking at all previous tokens (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span></span>), we avoid the costly recomputation at each step.</p>
<hr>
<h2>The Memory Challenge: Why We Need Better Solutions</h2>
<p>While KV cache is a powerful optimization, it comes with a significant memory cost. Let's look at a concrete example using a modern large language model like Llama3 70B with:</p>
<ul>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mn>80</mn></mrow><annotation encoding="application/x-tex">L = 80</annotation></semantics></math></span></span> layers</li>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">H = 64</annotation></semantics></math></span></span> attention heads</li>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">B = 8</annotation></semantics></math></span></span> batch size of 8 sequences</li>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">d_k = 128</annotation></semantics></math></span></span> key/value dimension</li>
<li>16-bit precision</li>
</ul>
<p>The memory required for a batch of 8 sequences of 1000 tokens each would be:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>B</mi><mo>×</mo><mi>n</mi><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub><mo>×</mo><mn>2</mn><mo>×</mo><mn>2</mn><mtext>&nbsp;bytes</mtext><mo>=</mo><mn>80</mn><mo>×</mo><mn>64</mn><mo>×</mo><mn>8</mn><mo>×</mo><mn>1000</mn><mo>×</mo><mn>128</mn><mo>×</mo><mn>2</mn><mo>×</mo><mn>2</mn><mtext>&nbsp;bytes</mtext><mo>=</mo><mn>20.97</mn><mtext>GB</mtext></mrow><annotation encoding="application/x-tex">L \times H \times B \times n \times d_k \times 2 \times 2 \text{ bytes} = 80 \times 64 \times 8 \times 1000 \times 128 \times 2 \times 2 \text{ bytes} = 20.97\text{GB}</annotation></semantics></math></span></span></span>
<p>This substantial memory usage creates several challenges:</p>
<ol>
<li><strong>Scales linearly</strong> with sequence length</li>
<li><strong>Multiplies</strong> with batch size for parallel processing</li>
<li><strong>Limits</strong> the maximum context length we can handle</li>
<li><strong>Constrains</strong> deployment on memory-limited devices</li>
</ol>
<p>These challenges have sparked a wave of innovation in the research community, leading to various techniques for optimizing KV cache usage. Let's explore these cutting-edge solutions.</p>
<h2>Can we improve over naive KV caches?</h2>
<p>The following papers represent key innovations in KV cache optimization. We'll explore them through three main approaches: token selection, post-hoc compression techniques, and architectural redesigns.</p>
<h2>Token Selection and Pruning Approaches</h2>
<h3>1) <a href="https://arxiv.org/abs/2306.14048">Heavy-Hitter Oracle (H2O)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/h2o_alg.png" alt=""></p>
<p>H2O introduces the concept of identifying and preserving important tokens in the KV cache:</p>
<ul>
<li><strong>Heavy-Hitter Tokens</strong>: H2O identifies tokens with the highest accumulated attention scores during generation, following a power-law distribution. These tokens are critical for model functionality and are prioritized in the cache.</li>
<li><strong>Dynamic Submodular Eviction</strong>: The method frames cache management as an optimization problem with a submodular objective function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F(S)</annotation></semantics></math></span></span> that quantifies the importance of a token set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span></span>:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>S</mi></mrow></munder><msub><mi>A</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">F(S) = \sum_{i \in S} A_{i}</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">A_i</annotation></semantics></math></span></span> is the accumulated attention score for token <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span></span>. The cache <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">S_t</annotation></semantics></math></span></span> is updated by:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><msub><mtext>argmax</mtext><mrow><mi>S</mi><mo>⊆</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>∪</mo><mo stretchy="false">{</mo><mi>i</mi><mo stretchy="false">}</mo><mo separator="true">,</mo><mi mathvariant="normal">∣</mi><mi>S</mi><mi mathvariant="normal">∣</mi><mo>≤</mo><mi>k</mi></mrow></msub><mtext> </mtext><mi>F</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">S_t = \text{argmax}_{S \subseteq S_{t-1} \cup \{i\}, |S| \leq k} \, F(S)</annotation></semantics></math></span></span></span>
ensuring that at most one token is evicted per step. This greedy algorithm is computationally efficient and guarantees near-optimal performance under submodular constraints.</li>
<li><strong>Results</strong>: Achieves <strong>5× reduction</strong> in KV cache size with negligible accuracy loss and up to <strong>29×</strong> throughput improvement.</li>
</ul>
<h3>2) <a href="https://arxiv.org/abs/2309.17453">StreamLLM</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/streamingLLM.png" alt=""></p>
<ul>
<li>The authors observe the phenomenon of <strong>Attention Sinks</strong>: Initial tokens that act as natural attention anchors during decoding
<ul>
<li>Without these attention sink tokens, the performance of naive window attention drops</li>
</ul>
</li>
<li>Based on that observation, they introduce a <strong>Rolling Cache</strong> for recent context with retained initial tokens, enabling infinite-length sequence processing.</li>
<li>They show that these sink tokens can also be <strong>trained</strong>; serving as dedicated attention anchors, reducing reliance on multiple initial tokens.</li>
</ul>
<h3>3) <a href="https://arxiv.org/abs/2406.12335">Value-Aware Token Pruning (VATP)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/vatp.png" alt=""></p>
<p>VATP extends H2O's token importance concept by considering both attention patterns and value vector properties:</p>
<ul>
<li><strong>Importance Scoring</strong>: Combines attention scores with value vector information:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>I</mi><mi>k</mi><mi>t</mi></msubsup><mo>=</mo><msubsup><mi>S</mi><mi>k</mi><mi>t</mi></msubsup><mo>⋅</mo><mi mathvariant="normal">∥</mi><msub><mi>v</mi><mi>k</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>1</mn></msub><mo separator="true">,</mo><mspace width="1em"></mspace><msubsup><mi>S</mi><mi>k</mi><mi>t</mi></msubsup><mo>=</mo><munder><mo>∑</mo><mrow><mi>k</mi><mo>≤</mo><mi>j</mi><mo>≤</mo><mi>t</mi></mrow></munder><msub><mi>a</mi><mrow><mi>j</mi><mo separator="true">,</mo><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">I_k^t = S_k^t \cdot \|v_k\|_1, \quad S_k^t = \sum_{k \leq j \leq t} a_{j,k}</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>S</mi><mi>k</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">S_k^t</annotation></semantics></math></span></span> is the accumulated attention score and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∥</mi><msub><mi>v</mi><mi>k</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\|v_k\|_1</annotation></semantics></math></span></span> is the value vector's L1 norm.</li>
<li><strong>Token Pruning</strong>: Tokens are ranked by <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>I</mi><mi>k</mi><mi>t</mi></msubsup></mrow><annotation encoding="application/x-tex">I_k^t</annotation></semantics></math></span></span>, and those with the lowest scores are pruned, while <strong>attention sink tokens</strong> (e.g., start or newline tokens) are preserved to prevent performance degradation.</li>
<li><strong>Performance and Efficiency</strong>:
<ul>
<li>Outperforms baselines like H2O and Scissorhands in 12–14 out of 16 LongBench tasks.</li>
<li>Achieves effective <strong>50% compression</strong> with minimal performance loss.</li>
<li>Introduces negligible computational overhead and is compatible with FlashAttention when integrated with Scissorhands.</li>
</ul>
</li>
</ul>
<h2>Post-hoc Compression Techniques</h2>
<p>These methods compress or optimize the KV cache while preserving the standard transformer architecture.</p>
<h3>4) <a href="https://arxiv.org/pdf/2310.01801">Adaptive KV Compression (FastGen)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/fastgen_1.png" alt=""></p>
<p>FastGen introduces adaptive compression based on attention patterns observed at run-time:</p>
<ul>
<li><strong>Attention Profiling</strong>: during prompt encoding, FastGen identifies attention patterns and selects compression policies <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>C</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">C^*</annotation></semantics></math></span></span> that minimize memory cost while preserving attention recovery:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>C</mi><mo>∗</mo></msup><mo>=</mo><mi>arg</mi><mo>⁡</mo><munder><mrow><mi>min</mi><mo>⁡</mo></mrow><mrow><mi>C</mi><mo>∈</mo><mi mathvariant="script">C</mi></mrow></munder><mtext>CacheMemoryCost</mtext><mo stretchy="false">(</mo><mi>C</mi><mo stretchy="false">)</mo><mspace width="1em"></mspace><mtext>s.t.</mtext><mspace width="1em"></mspace><mi mathvariant="normal">∥</mi><mi>A</mi><mo>−</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mi>Q</mi><msubsup><mi>K</mi><mi>C</mi><mi>T</mi></msubsup><mo stretchy="false">)</mo><mi mathvariant="normal">∥</mi><mo>≤</mo><mn>1</mn><mo>−</mo><mi>T</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">C^* = \arg\min_{C \in \mathcal{C}} \text{CacheMemoryCost}(C) \quad \text{s.t.} \quad \|A - \text{softmax}(QK_C^T)\| \leq 1 - T.</annotation></semantics></math></span></span></span>
</li>
<li><strong>Adaptive Compression Policies</strong>:
<ul>
<li>Compression strategies include:
<ul>
<li><strong>Special Tokens</strong> (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mtext>special</mtext></msub></mrow><annotation encoding="application/x-tex">C_{\text{special}}</annotation></semantics></math></span></span>): Retain only special tokens.</li>
<li><strong>Locality</strong> (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mtext>local</mtext></msub></mrow><annotation encoding="application/x-tex">C_{\text{local}}</annotation></semantics></math></span></span>): Evict tokens beyond a relative distance <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">r_l</annotation></semantics></math></span></span>.</li>
<li><strong>Frequency</strong> (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mtext>frequent</mtext></msub></mrow><annotation encoding="application/x-tex">C_{\text{frequent}}</annotation></semantics></math></span></span>): Keep tokens with high cumulative attention scores (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>f</mi></msub></mrow><annotation encoding="application/x-tex">r_f</annotation></semantics></math></span></span>).</li>
<li><strong>Hybrid Policies</strong> combine strategies, starting with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mtext>special</mtext></msub></mrow><annotation encoding="application/x-tex">C_{\text{special}}</annotation></semantics></math></span></span>, and applies them adaptively to each head:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">C</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>C</mi><mtext>special</mtext></msub><mo separator="true">,</mo><msub><mi>C</mi><mtext>special</mtext></msub><mo>+</mo><msub><mi>C</mi><mtext>punct</mtext></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>C</mi><mtext>full</mtext></msub><mo stretchy="false">}</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\mathcal{C} = \{C_{\text{special}}, C_{\text{special}} + C_{\text{punct}}, \ldots, C_{\text{full}}\}.</annotation></semantics></math></span></span></span>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ol start="4">
<li><strong>Token Generation</strong>:
<ul>
<li>During decoding, pre-selected compression policies manage the KV cache efficiently:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><msub><mi>C</mi><mi>i</mi></msub></msub><mo separator="true">,</mo><msub><mi>V</mi><msub><mi>C</mi><mi>i</mi></msub></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo separator="true">,</mo><msub><mi>C</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K_{C_i}, V_{C_i} = f(K, V, C_i).</annotation></semantics></math></span></span></span>
</li>
</ul>
</li>
</ol>
<h3>5) <a href="https://arxiv.org/pdf/2403.09636">Dynamic Memory Compression (DMC)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/dmc.png" alt=""></p>
<p>DMC introduces adaptive token merging:</p>
<ul>
<li><strong>Decision Mechanism</strong>: At time <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span></span>, predicts merge decisions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_t</annotation></semantics></math></span></span> and weights <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\omega_t</annotation></semantics></math></span></span>:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>=</mo><mo stretchy="false">⌊</mo><mtext>sigmoid</mtext><mo stretchy="false">(</mo><msub><mi>k</mi><mi>t</mi></msub><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mo stretchy="false">⌉</mo><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>ω</mi><mi>t</mi></msub><mo>=</mo><mtext>sigmoid</mtext><mo stretchy="false">(</mo><msub><mi>q</mi><mi>t</mi></msub><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\alpha_t = \lfloor \text{sigmoid}(k_t[0]) \rceil, \quad \omega_t = \text{sigmoid}(q_t[0]).</annotation></semantics></math></span></span></span>
</li>
<li><strong>Weighted Merging</strong>: When <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha_t = 1</annotation></semantics></math></span></span>, merges current and previous entries:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>k</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mfrac><mrow><msub><mi>ω</mi><mi>t</mi></msub><msub><mi>k</mi><mi>t</mi></msub><mo>+</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi>k</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mrow><msub><mi>ω</mi><mi>t</mi></msub><mo>+</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfrac><mo separator="true">,</mo><mspace width="1em"></mspace><msup><mi>v</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mfrac><mrow><msub><mi>ω</mi><mi>t</mi></msub><msub><mi>v</mi><mi>t</mi></msub><mo>+</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><msub><mi>v</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><mrow><msub><mi>ω</mi><mi>t</mi></msub><mo>+</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfrac><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">k' = \frac{\omega_t k_t + z_{t-1} k_{t-1}}{\omega_t + z_{t-1}}, \quad v' = \frac{\omega_t v_t + z_{t-1} v_{t-1}}{\omega_t + z_{t-1}},</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub><mo>=</mo><msub><mi>z</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>ω</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t = z_{t-1} + \omega_t</annotation></semantics></math></span></span> accumulates importance weights.</li>
<li><strong>Training</strong>:
<ul>
<li>Uses a Gumbel-Sigmoid relaxation for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_t</annotation></semantics></math></span></span> to allow end-to-end training with gradient descent:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mo>∼</mo><mtext>Gumbel-Sigmoid</mtext><mo stretchy="false">(</mo><msub><mi>k</mi><mi>t</mi></msub><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo separator="true">,</mo><mi>τ</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\alpha_t \sim \text{Gumbel-Sigmoid}(k_t[0], \tau),</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span></span> is a temperature parameter.</li>
<li>Optimizes a combined objective:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">L</mi><mo>=</mo><msub><mi mathvariant="script">L</mi><mtext>LM</mtext></msub><mo>+</mo><mi>λ</mi><mi>max</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mn>0</mn><mo separator="true">,</mo><mfrac><mi>n</mi><mtext>CR</mtext></mfrac><mo>−</mo><munder><mo>∑</mo><mi>t</mi></munder><msub><mi>α</mi><mi>t</mi></msub><mo fence="true">)</mo></mrow><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L} = \mathcal{L}_{\text{LM}} + \lambda \max\left(0, \frac{n}{\text{CR}} - \sum_{t} \alpha_t \right),</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">L</mi><mtext>LM</mtext></msub></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{LM}}</annotation></semantics></math></span></span> is the language modeling loss, and the second term encourages the model to match a target compression ratio (CR).</li>
</ul>
</li>
<li><strong>Results</strong>: Up to <strong>8× compression</strong> with maintained performance.</li>
</ul>
<h3>6) <a href="https://arxiv.org/pdf/2406.11430"><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_2</annotation></semantics></math></span></span> Norm-Based Compression</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/l2.png" alt=""></p>
<p>This paper presents a surprising observation: A clear correlation between the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_2</annotation></semantics></math></span></span> norm and the attention scores over cached KV pairs, where a low <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">L_2</annotation></semantics></math></span></span> norm of a key embedding usually leads to a high attention score during decoding. Consequently, they introduce a simple but effective compression objective:</p>
<ul>
<li><strong>Norm-Based Selection</strong>: For a set of cached keys <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>k</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>k</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>k</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">K = \{k_1, k_2, \dots, k_n\}</annotation></semantics></math></span></span>, computes and sorts key norms:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">∥</mi><msub><mi>k</mi><mi>i</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo>=</mo><msqrt><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><msubsup><mi>k</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mn>2</mn></msubsup></mrow></msqrt></mrow><annotation encoding="application/x-tex">\|k_i\|_2 = \sqrt{\sum_{j=1}^d k_{i,j}^2}</annotation></semantics></math></span></span></span>
</li>
<li><strong>Sorting and Selection</strong>: To compress the KV cache, sort all keys by their L2 norm values:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><mtext>sorted</mtext></msub><mo>=</mo><mtext>Sort</mtext><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">(</mo><mo stretchy="false">{</mo><mi mathvariant="normal">∥</mi><msub><mi>k</mi><mn>1</mn></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">∥</mi><msub><mi>k</mi><mn>2</mn></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi mathvariant="normal">∥</mi><msub><mi>k</mi><mi>n</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo stretchy="false">}</mo><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">)</mo></mrow><annotation encoding="application/x-tex">K_{\text{sorted}} = \text{Sort}\big(\{\|k_1\|_2, \|k_2\|_2, \dots, \|k_n\|_2\}\big)</annotation></semantics></math></span></span></span>
Retain the top-<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span></span> keys with lowest norms, where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mo stretchy="false">⌊</mo><mi>c</mi><mo>⋅</mo><mi>n</mi><mo stretchy="false">⌋</mo></mrow><annotation encoding="application/x-tex">m = \lfloor c \cdot n \rfloor</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span></span> is the compression ratio.</li>
<li><strong>Compressed Cache</strong>: The compressed key-value cache consists of:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><mtext>compressed</mtext></msub><mo>=</mo><mo stretchy="false">{</mo><msub><mi>k</mi><mi>i</mi></msub><mo>∣</mo><mi mathvariant="normal">∥</mi><msub><mi>k</mi><mi>i</mi></msub><msub><mi mathvariant="normal">∥</mi><mn>2</mn></msub><mo>∈</mo><msub><mi>K</mi><mtext>sorted</mtext></msub><mo stretchy="false">[</mo><mn>1</mn><mo>:</mo><mi>m</mi><mo stretchy="false">]</mo><mo stretchy="false">}</mo><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>V</mi><mtext>compressed</mtext></msub><mo>=</mo><mo stretchy="false">{</mo><msub><mi>v</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>k</mi><mi>i</mi></msub><mo>∈</mo><msub><mi>K</mi><mtext>compressed</mtext></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">K_{\text{compressed}} = \{k_i \mid \|k_i\|_2 \in K_{\text{sorted}}[1:m]\}, \quad V_{\text{compressed}} = \{v_i \mid k_i \in K_{\text{compressed}}\}</annotation></semantics></math></span></span></span>
</li>
<li>Due to its simplicity, this approach maintains compatibility with <strong>FlashAttention</strong>.</li>
</ul>
<h2>Architectural Redesigns</h2>
<p>These approaches change the Transformers architecture to handle KV caches more efficiently, often incorporating compression directly into the architecture.</p>
<h3>7) <a href="https://arxiv.org/pdf/2305.13245">Multi-Query Attention (MQA)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/mqa.png" alt=""></p>
<ul>
<li><strong>Key Idea</strong>: MQA reduces the KV cache size by sharing a <strong>single key-value head</strong> across all query heads, replacing the traditional Multi-Head Attention (MHA):
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>K</mi></msub><mo separator="true">,</mo><mspace width="1em"></mspace><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>V</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">K = XW_K, \quad V = XW_V,</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K </annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V </annotation></semantics></math></span></span> are the shared key and value projections.</li>
<li><strong>Benefits</strong>: Reduces the KV cache size by a factor of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H </annotation></semantics></math></span></span> (the number of attention heads), significantly lowering memory bandwidth overhead.</li>
<li><strong>Trade-Off</strong>: While MQA is faster, it often suffers from <strong>quality degradation</strong>, especially in tasks requiring diverse attention patterns.</li>
</ul>
<h3>8) <a href="https://arxiv.org/abs/2305.13245">Group-Query Attention (GQA)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/gqa.png" alt=""></p>
<ul>
<li><strong>Key Idea</strong>: GQA interpolates between full multi-head attention and MQA to offering a scalable trade-off between inference speed and model quality. It divides query heads into <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G </annotation></semantics></math></span></span> groups, where each group shares a single key-value head:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><mtext>group</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant="normal">∣</mi><mi>G</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><munder><mo>∑</mo><mrow><mi>h</mi><mo>∈</mo><mi>G</mi></mrow></munder><msub><mi>K</mi><mi>h</mi></msub><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>V</mi><mtext>group</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi mathvariant="normal">∣</mi><mi>G</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><munder><mo>∑</mo><mrow><mi>h</mi><mo>∈</mo><mi>G</mi></mrow></munder><msub><mi>V</mi><mi>h</mi></msub></mrow><annotation encoding="application/x-tex">K_{\text{group}} = \frac{1}{|G|} \sum_{h \in G} K_h, \quad V_{\text{group}} = \frac{1}{|G|} \sum_{h \in G} V_h</annotation></semantics></math></span></span></span>
<ul>
<li><strong>GQA-1</strong>: Equivalent to MQA (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">G = 1 </annotation></semantics></math></span></span>).</li>
<li><strong>GQA-<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H </annotation></semantics></math></span></span></strong>: Equivalent to MHA (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mo>=</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">G = H </annotation></semantics></math></span></span>).</li>
</ul>
</li>
<li><strong>Uptraining</strong>: GQA can be introduced to existing pre-trained models through fine-tuning:
<ul>
<li>First, convert MHA checkpoints to GQA by <strong>mean pooling</strong> key and value heads into groups</li>
<li>Then fine-tune ("uptrain") the model briefly to adapt to the new attention pattern</li>
<li>This adaptation process requires only <strong>5% of the original pre-training compute</strong>, making it very efficient</li>
<li>The resulting model maintains quality while gaining GQA's memory benefits</li>
</ul>
</li>
</ul>
<h3>9) <a href="https://arxiv.org/abs/2405.04434">Multi-head Latent Attention (MLA)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/mla.png" alt=""></p>
<p>DeepSeek's <strong>Multi-Head Latent Attention (MLA)</strong> takes a novel approach to reducing KV cache overhead. While MQA and GQA achieve this through head-sharing, MLA instead employs a <strong>low-rank latent compression</strong> technique that maintains the benefits of multiple attention heads.</p>
<ul>
<li>MLA reduces KV cache size by compressing keys and values into low-dimensional latent vectors before reconstruction.</li>
<li>It down-project key-value embeddings into a compressed latent space:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>c</mi><mrow><mtext>KV</mtext><mo separator="true">,</mo><mi>t</mi></mrow></msub><mo>=</mo><msub><mi>W</mi><mtext>DKV</mtext></msub><msub><mi>h</mi><mi>t</mi></msub><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>k</mi><mi>C</mi></msub><mo>=</mo><msub><mi>W</mi><mtext>UK</mtext></msub><msub><mi>c</mi><mrow><mtext>KV</mtext><mo separator="true">,</mo><mi>t</mi></mrow></msub><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>v</mi><mi>C</mi></msub><mo>=</mo><msub><mi>W</mi><mtext>UV</mtext></msub><msub><mi>c</mi><mrow><mtext>KV</mtext><mo separator="true">,</mo><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">c_{\text{KV}, t} = W_{\text{DKV}} h_t, \quad k_C = W_{\text{UK}} c_{\text{KV}, t}, \quad v_C = W_{\text{UV}} c_{\text{KV}, t}</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mtext>DKV</mtext></msub></mrow><annotation encoding="application/x-tex">W_{\text{DKV}}</annotation></semantics></math></span></span> is the down-projection matrix, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mtext>UK</mtext></msub></mrow><annotation encoding="application/x-tex">W_{\text{UK}}</annotation></semantics></math></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mtext>UV</mtext></msub></mrow><annotation encoding="application/x-tex">W_{\text{UV}}</annotation></semantics></math></span></span> are up-projection matrices for keys and values.</li>
<li>It retains per-head flexibility through compressed representations, unlike MQA's complete head sharing.</li>
<li>It introduces <strong>Rotary Positional Embeddings (RoPE)</strong> for decoupling position-aware keys:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>k</mi><mi>R</mi></msub><mo>=</mo><mtext>RoPE</mtext><mo stretchy="false">(</mo><msub><mi>W</mi><mrow><mi>K</mi><mi>R</mi></mrow></msub><msub><mi>h</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>k</mi><mi>t</mi></msub><mo>=</mo><mo stretchy="false">[</mo><msub><mi>k</mi><mi>C</mi></msub><mo separator="true">;</mo><msub><mi>k</mi><mi>R</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">k_R = \text{RoPE}(W_{KR} h_t), \quad k_t = [k_C; k_R]</annotation></semantics></math></span></span></span>
This reduces KV cache storage further by caching only compressed latent vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mtext>KV</mtext></msub></mrow><annotation encoding="application/x-tex">c_{\text{KV}}</annotation></semantics></math></span></span> and positional keys <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>R</mi></msub></mrow><annotation encoding="application/x-tex">k_R</annotation></semantics></math></span></span>.</li>
</ul>
<h3>10) <a href="https://arxiv.org/pdf/2404.14469">SnapKV</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/snapKV.png" alt=""></p>
<ul>
<li>SnapKV introduces an <strong>Observation Window</strong>: Uses end-of-prompt tokens to identify attention patterns:
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>C</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><msub><mi>L</mi><mtext>obs</mtext></msub></munderover><msub><mi>W</mi><mtext>obs</mtext></msub><mo stretchy="false">[</mo><mo>:</mo><mo separator="true">,</mo><mi>i</mi><mo separator="true">,</mo><mo>:</mo><mo stretchy="false">]</mo><mo separator="true">,</mo><mspace width="1em"></mspace><mi>I</mi><mo>=</mo><msub><mtext>Top</mtext><mi>k</mi></msub><mo stretchy="false">(</mo><mi>C</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">C = \sum_{i=0}^{L_{\text{obs}}} W_{\text{obs}}[:, i, :], \quad I = \text{Top}_k(C, k)</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mtext>obs</mtext></msub></mrow><annotation encoding="application/x-tex">W_{\text{obs}}</annotation></semantics></math></span></span> represents the attention weights, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span> is determined by the compression rate.</li>
<li><strong>Compression</strong>: Clusters features around the selected positions using a pooling layer to preserve context completeness.</li>
</ul>
<h3>11) <a href="https://arxiv.org/pdf/2405.05254">You Only Cache Once (YOCO)</a></h3>
<p><img src="https://www.pyspur.dev/blog/kv-cache/yoco_2.png" alt=""></p>
<p>YOCO modifies the transformer architecture for caching:</p>
<ul>
<li><strong>Global Cache</strong>: Uses a decoder-decoder design with a single shared KV cache.</li>
<li><strong>Complexity Reduction</strong>: Reduces memory from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo>×</mo><mi>L</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N \times L)</annotation></semantics></math></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo>+</mo><mi>L</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N + L)</annotation></semantics></math></span></span>, where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> is sequence length and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span></span> is the number of layers.</li>
<li><strong>Efficient Attention</strong>: The self-decoder employs <strong>sliding-window attention</strong> or <strong>gated retention</strong>, enabling constant memory usage (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>C</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(C)</annotation></semantics></math></span></span>, where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span></span> is a small window size).</li>
</ul>
<hr>
<h2>Conclusion</h2>
<p>Key-Value caching techniques are central to scaling and optimizing Transformer-based models for real-world use. Innovations like dynamic eviction, compression, and structured approximations continue to push the boundaries on what is possible in long-context or resource-constrained scenarios. KV caching remains a lively research area, offering both theoretical insights and practical improvements.</p>
<p>PS: This blog post is mostly AI-generated using a PySpur workflow with minor human edits.</p></article></div></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Questions censored by DeepSeek (342 pts)]]></title>
            <link>https://www.promptfoo.dev/blog/deepseek-censorship/</link>
            <guid>42858552</guid>
            <pubDate>Tue, 28 Jan 2025 21:54:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.promptfoo.dev/blog/deepseek-censorship/">https://www.promptfoo.dev/blog/deepseek-censorship/</a>, See on <a href="https://news.ycombinator.com/item?id=42858552">Hacker News</a></p>
Couldn't get https://www.promptfoo.dev/blog/deepseek-censorship/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: DeepSeek Your HN Profile (113 pts)]]></title>
            <link>https://hn-wrapped.kadoa.com/</link>
            <guid>42857604</guid>
            <pubDate>Tue, 28 Jan 2025 20:35:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hn-wrapped.kadoa.com/">https://hn-wrapped.kadoa.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42857604">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Hacker News Wrapped</h2><p>Let DeepSeek analyze your HN profile to give you highlights and trends.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Parkinsons patient "feels cured" with new adaptive deep brain stimulation device (254 pts)]]></title>
            <link>https://www.bbc.com/news/articles/ckgn49r069wo</link>
            <guid>42857293</guid>
            <pubDate>Tue, 28 Jan 2025 20:07:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/ckgn49r069wo">https://www.bbc.com/news/articles/ckgn49r069wo</a>, See on <a href="https://news.ycombinator.com/item?id=42857293">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div data-testid="byline-new" data-component="byline-block"><p><span>Sharon Barbour</span></p><p><span>BBC North East &amp; Cumbria health correspondent<!-- --></span></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp" alt="BBC Kevin Hill is sitting on his sofa and has opened his blue shirt to show a lump in his chest. This is where a small computer has been implanted. It is connected to wires that go deep into his brain, to control his Parkinson's disease.    "><span>BBC</span></p></div><p data-component="caption-block"><figcaption>Kevin Hill said he is able to go for days now without thinking about his Parkinson's<!-- --></figcaption></p></figure><div data-component="text-block"><p>A man fitted with a pioneering, computer-controlled brain implant to tackle his Parkinson's disease says it works so well he is sometimes able to forget he has the condition.<!-- --></p><p>A small computer inserted into Kevin Hill's chest wall 12 months ago is connected to wires running into the brain which can send electrical signals and an update means it can now read his brain activity.<!-- --></p><p>The 65-year-old from Sunderland said it has been so successful he feels like he has "been cured".<!-- --></p><p>Surgeons in Newcastle hope an adapted version of the deep brain stimulation system will have a "huge impact" on the quality of life of patients with the disease.<!-- --></p></div><p>Mr Hill said: "I forget about Parkinson's for days and days and days."<!-- --></p><p data-component="subheadline-block"><h2>Kitchen ban<!-- --></h2></p><p><i id="warning---contains-a-distressing-image"><b id="warning---contains-a-distressing-image">Warning - contains a distressing image<!-- --></b></i></p><div data-component="text-block"><p>He began getting symptoms, including trembling in his thumb, in his 40s and started suffering nightmares and insomnia.<!-- --></p><p>He was banned by his wife from going into the kitchen because his hand shook so much he spilled or dropped hot drinks and even cut the end of his finger off.<!-- --></p><p>In 2017 he visited his GP and was diagnosed with Parkinson's.<!-- --></p><p>He was told there were medicines but no cure, but there was a new treatment – deep brain stimulation (DBS) – and tests proved he was suitable for the surgery.<!-- --></p><p>It involved an implant that runs deep into the brain to an area the size of a grain of rice. <!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp" alt="NEWCASTLE HOSPITALS Kevin is with a nurse at hospital as the new system is re-programmed and switched on. They are both looking at a computer screen which is wired to his chest. "><span>NEWCASTLE HOSPITALS</span></p></div><p data-component="caption-block"><figcaption>Mr Hill originally had to go to hospital to have the system reprogrammed, but with updates it can now do that automatically<!-- --></figcaption></p></figure><div data-component="text-block"><p>The computer in his chest is connected to two thin wires that thread up the back of his neck. <!-- --></p><p>It carries the electrical messages that can manage his Parkinson's symptoms.<!-- --></p><p>Mr Hill described the computer as the size and shape of "a Jaffa Cake".<!-- --></p><p>When it was switched on after surgery he said the impact was dramatic.<!-- --></p><p>After years of sleepless nights, and being unable to manage the uncontrollable shaking of his arm and leg, his tremors "stopped instantly".<!-- --></p><p>Mr Hill said he stared at his still hand and "couldn't believe it". His wife burst into tears.<!-- --></p><p>The life he once knew came back, meaning he was able to go to the pub and see his friends again.<!-- --></p><p>He bought a bike and was even allowed back into the kitchen.<!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp" alt="KEVIN HILL Kevin's head shaved after surgery. You can see the stitches in his skull where he had the operation to implant the wires into his brain.   "><span>KEVIN HILL</span></p></div><p data-component="caption-block"><figcaption>A brain implant links to the computer in Mr Hill's chest<!-- --></figcaption></p></figure><div data-component="text-block"><p>For the last year he has had to go to hospital regularly to have his system re-programmed to better control his symptoms.<!-- --></p><p>Now, a new updated version called "adaptive deep brain stimulation" has been designed to re-programme the system in real time.<!-- --></p><p>It can also read a patient's brain signals which doctors say should mean even better control of symptoms.<!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp" alt="NEWCASTLE HOSPITALS NHS TRUST A mid-shot of Mr Akbar Hussain, a neurosurgeon at Newcastle Hospitals. He is pictured wearing his blue surgical scrubs.
    "><span>NEWCASTLE HOSPITALS NHS TRUST</span></p></div><p data-component="caption-block"><figcaption>Neurosurgeon Akbar Hussain said recent changes to the device would be very significant to patients' quality of life<!-- --></figcaption></p></figure><div data-component="text-block"><p>Akbar Hussain, a neurosurgeon at Newcastle Hospitals, is one of the first doctors in the world to offer the new adaptive Brainsense treatment, developed by Medtronic.<!-- --></p><p>He said: "The amazing thing about the adaptive version is that the electrical impulses provided to the brain by the device are controlled and adjusted automatically, according to individual patient's recordings from the device in their chest.<!-- --></p><p>"The biological signals generated within the person themselves are enough to alter the treatment given by the implant. <!-- --></p><p>"These changes could be taking place by the minute or hour, meaning the treatment is truly responsive to the exact needs of each individual.<!-- --></p><p>"It's exciting. Hopefully this will have a huge impact and be very significant for the patients' quality of life."<!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp" alt="Kevin Hill wearing a fluourescent green jacket and holding his bike and a helmet and smiling."></p></div><p data-component="caption-block"><figcaption>Kevin Hill says his old life has returned since having the surgery<!-- --></figcaption></p></figure><div data-component="text-block"><p>Dr Becky Jones, from the charity Parkinson's UK, said: "Current DBS can be life changing and has the promise to be even more effective if it could be responsive to the needs of the individual. Brainsense represents a major step towards this.<!-- --></p><p>"While evidence is still being gathered to assess the benefits of adaptive DBS versus the standard type, it's great to see movement towards this becoming a new, more effective treatment for people with Parkinson's."<!-- --></p><p>About 153,000 people in the UK are living with Parkinson's disease, a progressive neurological disorder affecting the brain and nervous system. <!-- --></p><p>The number is expected to increase due to population growth and ageing. <!-- --></p></div><p><i id="follow-bbc-sunderland-on">Follow BBC Sunderland on <!-- --></i><a target="_blank" href="https://x.com/BBCNEandCumbria"><i id="x">X<!-- --></i></a><i id=",">, <!-- --></i><a target="_blank" href="https://www.facebook.com/BBCSunderland"><i id="facebook">Facebook<!-- --></i></a><i id=",">, <!-- --></i><a target="_blank" href="https://bbc.in/3yyMYUI"><i id="nextdoor">Nextdoor<!-- --></i></a><i id="and"> and <!-- --></i><a target="_blank" href="https://www.instagram.com/bbcneandcumbria/"><i id="instagram">Instagram<!-- --></i></a><i id=".-send-your-story-ideas-to-northeastandcumbria@bbc.co.uk.">. Send your story ideas to northeastandcumbria@bbc.co.uk.<!-- --></i></p><div data-component="links-block"><p><span data-testid="links-title">More stories on this topic</span></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Svelte 5 and the Future of Frameworks: A Chat with Rich Harris (108 pts)]]></title>
            <link>https://www.smashingmagazine.com/2025/01/svelte-5-future-frameworks-chat-rich-harris/</link>
            <guid>42857106</guid>
            <pubDate>Tue, 28 Jan 2025 19:53:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.smashingmagazine.com/2025/01/svelte-5-future-frameworks-chat-rich-harris/">https://www.smashingmagazine.com/2025/01/svelte-5-future-frameworks-chat-rich-harris/</a>, See on <a href="https://news.ycombinator.com/item?id=42857106">Hacker News</a></p>
Couldn't get https://www.smashingmagazine.com/2025/01/svelte-5-future-frameworks-chat-rich-harris/: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>