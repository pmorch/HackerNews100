<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 21 Apr 2024 00:00:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Bringing Exchange Support to Thunderbird (137 pts)]]></title>
            <link>https://blog.thunderbird.net/2024/04/adventures-in-rust-bringing-exchange-support-to-thunderbird/</link>
            <guid>40100672</guid>
            <pubDate>Sat, 20 Apr 2024 20:19:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.thunderbird.net/2024/04/adventures-in-rust-bringing-exchange-support-to-thunderbird/">https://blog.thunderbird.net/2024/04/adventures-in-rust-bringing-exchange-support-to-thunderbird/</a>, See on <a href="https://news.ycombinator.com/item?id=40100672">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
								<img src="https://blog.thunderbird.net/files/2024/04/Tb-rust1.png" alt="featured post title image">
						<section>
												

				
<p>Microsoft Exchange is a popular choice of email service for corporations and educational institutions, and so it’s no surprise that there’s demand among Thunderbird users to support Exchange. Until recently, this functionality was only available through an add-on. But, in the next ESR (Extended Support) release of Thunderbird in July 2024, we expect to provide this support natively within Thunderbird. Because of the size of this undertaking, the first roll-out of the Exchange support will <a href="https://youtu.be/7jNV1J2pdPc">initially cover only email</a>, with calendar and address book support coming at a later date.</p>



<p>This article will go into technical detail on how we are implementing support for the Microsoft Exchange Web Services mail protocol, and some idea of where we’re going next with the knowledge gained from this adventure.</p>



<p><em>Before we dive in, just a quick note that <strong>Brendan Abolivier, Ikey Doherty</strong>, and <strong>Sean Burke</strong> are the developers behind this effort, and are the authors of this post.</em></p>



<figure></figure>



<h2>Historical context</h2>



<p>Thunderbird is a long-lived project, which means there’s lots of old code. The current architecture for supporting mail protocols predates Thunderbird itself, having been developed more than 20 years ago as part of Netscape Communicator. There was also no paid maintainership from about 2012 — when Mozilla divested and&nbsp; transferred ownership of Thunderbird to its community — until 2017, when Thunderbird rejoined the Mozilla Foundation. That means years of ad hoc changes without a larger architectural vision and a lot of decaying C++ code that was not using modern standards.</p>



<p>Furthermore, in the entire 20 year lifetime of the Thunderbird project, no one has added support for a new mail protocol before. As such, no one has updated the architecture as mail protocols change and adapt to modern usage patterns, and a great deal of institutional knowledge has been lost. Implementing this much-needed feature is the first organization-led effort to actually understand and address limitations of Thunderbird’s architecture in an incremental fashion.</p>



<h2>Why we chose Rust</h2>



<p>Thunderbird is a large project maintained by a small team, so choosing a language for new work cannot be taken lightly. We need powerful tools to develop complex features relatively quickly, but we absolutely must balance this with long-term maintainability. Selecting Rust as the language for our new protocol support brings some important benefits:</p>



<ol>
<li><strong>Memory safety.</strong> Thunderbird takes input from anyone who sends an email, so we need to be diligent about keeping security bugs out.</li>



<li><strong>Performance.</strong> Rust runs as native code with all of the associated performance benefits.</li>



<li><strong>Modularity and Ecosystem.</strong> The built-in modularity of Rust gives us access to a large ecosystem where there are already a lot of people doing things related to email which we can benefit from.</li>
</ol>



<p>The above are all on the standard list of benefits when discussing Rust. However, there are some additional considerations for Thunderbird:</p>



<ol>
<li><strong>Firefox.</strong> Thunderbird is built on top of Firefox code and we use a shared CI infrastructure with Firefox which already enables Rust. Additionally, Firefox provides a language interop layer called XPCOM (Cross-Platform Component Object Model), which has Rust support and allows us to call between Rust, C++, and JavaScript.</li>



<li><strong>Powerful tools.</strong> Rust gives us a large toolbox for building APIs which are difficult to misuse by pushing logical errors into the domain of the compiler. We can easily avoid circular references or provide functions which simply cannot be called with values which don’t make sense, letting us have a high degree of confidence in features with a large scope. Rust also provides first-class tooling for documentation, which is critically important on a small team.</li>



<li><strong>Addressing architectural technical debt.</strong> Introducing a new language gives us a chance to reconsider some aging architectures while benefiting from a growing language community.</li>



<li><strong>Platform support and portability.</strong> Rust supports a broad set of host platforms. By building modular crates, we can reuse our work in other projects, such as Thunderbird for Android/K-9 Mail.</li>
</ol>



<h2>Some mishaps along the way</h2>



<p>Of course, the endeavor to introduce our first Rust component in Thunderbird is not without its challenges, mostly related to the size of the Thunderbird codebase. For example, there is a lot of existing code with idiosyncratic asynchronous patterns that don’t integrate nicely with idiomatic Rust. There are also lots of features and capabilities in the Firefox and Thunderbird codebase that don’t have any existing Rust bindings.</p>



<h3>The first roadblock: the build system</h3>



<p>Our first hurdle came with getting any Rust code to run in Thunderbird at all. There are two things you need to know to understand why:</p>



<p>First, since the Firefox code is a dependency of Thunderbird, you might expect that we pull in their code as a subtree of our own, or some similar mechanism. However, for historical reasons, it’s the other way around: building Thunderbird requires fetching Firefox’s code, fetching Thunderbird’s code as a subtree of Firefox’s, and using a build configuration file to point into that subtree.</p>



<p>Second, because Firefox’s entrypoint is written in C++ and Rust calls happen via an interoperability layer, there is no single point of entry for Rust. In order to create a tree-wide dependency graph for Cargo and avoid duplicate builds or version/feature conflicts, Firefox introduced a hack to generate a single Cargo workspace which aggregates all the individual crates in the tree.</p>



<p>In isolation, neither of these is a problem in itself. However, in order to build Rust into Thunderbird, we needed to define our own Cargo workspace which lives in our tree, and Cargo does not allow nesting workspaces. To solve this issue, we had to define our own workspace and add configuration to the upstream build tool, <code>mach</code>, to build from this workspace instead of Firefox’s. We then use a newly-added <code>mach</code> subcommand to sync our dependencies and lockfile with upstream and to vendor the resulting superset.</p>



<h3>XPCOM</h3>



<p>While the availability of language interop through XPCOM is important for integrating our frontend and backend, the developer experience has presented some challenges. Because XPCOM was originally designed with C++ in mind, implementing or consuming an XPCOM interface requires a lot of boilerplate and prevents us from taking full advantage of tools like rust-analyzer. Over time, Firefox has significantly reduced its reliance on XPCOM, making a clunky Rust+XPCOM experience a relatively minor consideration. However, as part of the previously-discussed maintenance gap, Thunderbird never undertook a similar project, and supporting a new mail protocol requires implementing hundreds of functions defined in XPCOM.</p>



<p>Existing protocol implementations ease this burden by inheriting C++ classes which provide the basis for most of the shared behavior. Since we can’t do this directly, we are instead implementing our protocol-specific logic in Rust and communicating with a bridge class in C++ which combines our Rust implementations (an internal crate called <code>ews_xpcom</code>) with the existing code for shared behavior, with as small an interface between the two as we can manage.</p>



<p>Please visit our&nbsp;<a href="https://source-docs.thunderbird.net/en/latest/rust/index.html" target="_blank" rel="noreferrer noopener">documentation</a>&nbsp;to learn more about how to create Rust components in Thunderbird.</p>



<h2>Implementing Exchange support with Rust</h2>



<p>Despite the technical hiccups experienced along the way, we were able to clear the hurdles, use, and build Rust within Thunderbird. Now we can talk about how we’re using it and the tools we’re building. Remember all the way back to the beginning of this blog post, where we stated that our goal is to support Microsoft’s Exchange Web Services (EWS) API. EWS communicates over HTTP with request and response bodies in XML.</p>



<h2>Sending HTTP requests</h2>



<p>Firefox already includes a full-featured HTTP stack via its <code>necko</code> networking component. However, <code>necko</code> is written in C++ and exposed over XPCOM, which as previously stated does not make for nice, idiomatic Rust. Simply sending a GET request requires a great deal of boilerplate, including nasty-looking unsafe blocks where we call into XPCOM. (XPCOM manages the lifetime of pointers and their referents, ensuring memory safety, but the Rust compiler doesn’t know this.) Additionally, the interfaces we need are callback-based. For making HTTP requests to be simple for developers, we need to do two things:</p>



<ol>
<li><strong>Support native Rust async/await syntax.</strong> For this, we added a new Thunderbird-internal crate, <code>xpcom_async</code>. This is a low-level crate which translates asynchronous operations in XPCOM into Rust’s native async syntax by defining callbacks to buffer incoming data and expose it by implementing Rust’s <code>Future</code> trait so that it can be awaited by consumers. (If you’re not familiar with the <code>Future</code> concept in Rust, it is similar to a JS <code>Promise</code> or a Python coroutine.)</li>



<li><strong>Provide an idiomatic HTTP API.</strong> Now that we had native <code>async</code>/<code>await</code> support, we created another internal crate (<code>moz_http</code>) which provides an HTTP client inspired by <code>reqwest</code>. This crate handles creating all of the necessary XPCOM objects and providing Rustic error handling (much nicer than the standard XPCOM error handling).</li>
</ol>



<h2>Handling XML requests and responses</h2>



<p>The hardest task in working with EWS is translating between our code’s own data structures and the XML expected/provided by EWS. Existing crates for serializing/deserializing XML didn’t meet our needs. <code>serde</code>’s data model doesn’t align well with XML, making distinguishing XML attributes and elements difficult. EWS is also sensitive to XML namespaces, which are completely foreign to <code>serde</code>. Various <code>serde</code>-inspired crates designed for XML exist, but these require explicit annotation of how to serialize every field. EWS defines hundreds of types which can have dozens of fields, making that amount of boilerplate untenable.</p>



<p>Ultimately, we found that existing <code>serde</code>-based implementations worked fine for deserializing XML into Rust, but we were unable to find a satisfactory tool for serialization. To that end, we introduced another new crate, <code>xml_struct</code>. This crate defines traits governing serialization behavior and uses Rust’s procedural derive macros to automatically generate implementations of these traits for Rust data structures. It is built on top of the existing <code>quick_xml</code> crate and designed to create a low-boilerplate, intuitive mapping between XML and Rust.&nbsp; While it is in the early stages of development, it does not make use of any Thunderbird/Firefox internals and is <a href="https://github.com/thunderbird/xml-struct-rs">available on GitHub</a>.</p>



<p>We have also introduced one more new crate, <code>ews</code>, which defines types for working with EWS and an API for XML serialization/deserialization, based on <code>xml_struct</code> and <code>serde</code>. Like <code>xml_struct</code>, it is in the early stages of development, but is <a href="https://github.com/thunderbird/ews-rs">available on GitHub</a>.</p>



<h2>Overall flow chart</h2>



<p>Below, you can find a handy flow chart to help understand the logical flow for making an Exchange request and handling the response.&nbsp;</p>



<figure><a href="https://blog.thunderbird.net/files/2024/04/pasted-image-.png"><img decoding="async" fetchpriority="high" width="1600" height="716" src="https://blog.thunderbird.net/files/2024/04/pasted-image-.png" alt="A bird's eye view of the flow" title="A bird’s eye view of the flow" srcset="https://blog.thunderbird.net/files/2024/04/pasted-image-.png 1600w, https://blog.thunderbird.net/files/2024/04/pasted-image--252x113.png 252w, https://blog.thunderbird.net/files/2024/04/pasted-image--600x269.png 600w, https://blog.thunderbird.net/files/2024/04/pasted-image--768x344.png 768w, https://blog.thunderbird.net/files/2024/04/pasted-image--1536x687.png 1536w" sizes="(max-width: 1600px) 100vw, 1600px"></a></figure>



<p>Fig 1. A bird’s eye view of the flow</p>



<h2>What’s next?</h2>



<h2>Testing all the things</h2>



<p>Before landing our next major features, we are taking some time to build out our automated tests. In addition to unit tests, we just landed a mock EWS server for integration testing. The current focus on testing is already paying dividends, having exposed a couple of crashes and some double-sync issues which have since been rectified. Going forward, new features can now be easily tested and verified.</p>



<h2>Improving error handling</h2>



<p>While we are working on testing, we are also busy improving the story around error handling. EWS’s error behavior is often poorly documented, and errors can occur at multiple levels (e.g., a request may fail as a whole due to throttling or incorrect structure, or parts of a request may succeed while other parts fail due to incorrect IDs). Some errors we can handle at the protocol level, while others may require user intervention or may be intractable. In taking the time now to improve error handling, we can provide a more polished implementation and set ourselves up for easier long-term maintenance.</p>



<h2>Expanding support</h2>



<p>We are working on expanding protocol support for EWS (via <code>ews</code> and the internal <code>ews_xpcom</code> crate) and hooking it into the Thunderbird UI. Earlier this month, we landed a series of patches which allow adding an EWS account to Thunderbird, syncing the account’s folder hierarchy from the remote server, and displaying those folders in the UI. (At present, this alpha-state functionality is gated behind a build flag and a preference.) Next up, we’ll work on fetching message lists from the remote server as well as generalizing outgoing mail support in Thunderbird.</p>



<h2>Documentation</h2>



<p>Of course, all of our work on maintainability is for naught if no one understands what the code does. To that end, we’re producing documentation on how all of the bits we have talked about here come together, as well as describing the existing architecture of mail protocols in Thunderbird and thoughts on future improvements, so that once the work of supporting EWS is done, we can continue building and improving on the Thunderbird you know and love.</p>
				

			</section>
			

			
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AltStore. The first Apple approved alternative App Store (127 pts)]]></title>
            <link>https://altstore.io/#Downloads</link>
            <guid>40100151</guid>
            <pubDate>Sat, 20 Apr 2024 19:17:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://altstore.io/#Downloads">https://altstore.io/#Downloads</a>, See on <a href="https://news.ycombinator.com/item?id=40100151">Hacker News</a></p>
<div id="readability-page-1" class="page">
  <div data-collapse="medium" data-animation="default" data-duration="400" data-easing="ease" data-easing2="ease" role="banner">
        <p><a href="#"><img src="https://altstore.io/images/logo_text.png" alt="" width="124"></a></p>
      </div>
  
  <div><p>AltStore PAL now available!<br>Read the <a href="https://rileytestut.com/blog/2024/04/17/introducing-altstore-pal/" target="_blank">announcement</a>
    </p></div>
  <section>
    <h2>Sideloading for Everyone</h2>
    <p>Discover apps that push the boundaries of iOS.</p>
    
    <p><img src="https://altstore.io/images/Tuesday-23-Jan-2024-165319.png" srcset="https://altstore.io/images/Tuesday-23-Jan-2024-165319-p-500.png 500w, https://altstore.io/images/Tuesday-23-Jan-2024-165319-p-800.png 800w, https://altstore.io/images/Tuesday-23-Jan-2024-165319-p-1080.png 1080w, https://altstore.io/images/Tuesday-23-Jan-2024-165319.png 1339w" width="400" sizes="(max-width: 479px) 90vw, 400px" alt=""></p>
    
  </section>
  
  <div>
        <div>
          <h2>A New Way to Sideload</h2>
          <div><p>AltStore is an app store designed for sideloading. Every app in AltStore gets a beautifully generated store page with detailed information to make sideloading fun and easy. Browse apps from trusted developers, or add additional "sources" to further increase your options.</p><p> Plus, AltStore is made with security in mind. You can view a full list of an app's permissions from its store page, and AltStore will even automatically alert you if they change so you can sideload with confidence.</p></div>
          <p><a href="https://faq.altstore.io/" target="_blank">Learn More</a>
        </p></div><p><img src="https://altstore.io/images/DeltaStorePage.PNG" srcset="https://altstore.io/images/DeltaStorePage-p-500.png 500w, https://altstore.io/images/DeltaStorePage-p-800.png 800w, https://altstore.io/images/DeltaStorePage-p-1080.png 1080w, https://altstore.io/images/DeltaStorePage.PNG 1339w" width="350" sizes="(max-width: 479px) 90vw, 350px" alt="">
      </p></div>
  <div><p><img src="https://altstore.io/images/Monday-08-May-2023-162140.PNG" srcset="https://altstore.io/images/Monday-08-May-2023-162140-p-500.png 500w, https://altstore.io/images/Monday-08-May-2023-162140-p-800.png 800w, https://altstore.io/images/Monday-08-May-2023-162140-p-1080.png 1080w, https://altstore.io/images/Monday-08-May-2023-162140.PNG 1339w" width="350" sizes="(max-width: 479px) 90vw, 350px" alt=""></p><div>
          <h2>Self-Published Apps</h2>
          <div><p>Anyone can distribute their apps with AltStore. All you need is to make a “source”, which you can do by hosting a text file with basic information about your apps. Users can then enter your source URL in AltStore and your apps will automatically appear.</p><p>Follow our complete guide to create your own source and start distributing your apps in minutes!</p></div>
          <p><a href="https://faq.altstore.io/sources/make-a-source" target="_blank">Publish Apps</a>
        </p></div>
      </div>
  <div>
      <h2>By Indies — For Indies</h2>
      <div>
        <p><a id="w-node-a1c5e89b-ef5e-812c-23ad-c7315bcc8782-ed6becd7" href="https://mastodon.social/@rileytestut" target="_blank"><img src="https://altstore.io/images/spaces_Afe8qEztjcTjsjjaMBY2_uploads_UWT8nOrICxoO7OFmzKo0_Untitled.webp" srcset="https://altstore.io/images/spaces_Afe8qEztjcTjsjjaMBY2_uploads_UWT8nOrICxoO7OFmzKo0_Untitled-p-500.webp 500w, https://altstore.io/images/spaces_Afe8qEztjcTjsjjaMBY2_uploads_UWT8nOrICxoO7OFmzKo0_Untitled-p-800.webp 800w, https://altstore.io/images/spaces_Afe8qEztjcTjsjjaMBY2_uploads_UWT8nOrICxoO7OFmzKo0_Untitled-p-1080.webp 1080w, https://altstore.io/images/spaces_Afe8qEztjcTjsjjaMBY2_uploads_UWT8nOrICxoO7OFmzKo0_Untitled.webp 1500w" id="w-node-f271e8bc-a871-6e22-2967-dc732ff82ca8-ed6becd7" sizes="(max-width: 479px) 100vw, (max-width: 767px) 24vw, (max-width: 991px) 17vw, (max-width: 1439px) 19vw, 188px" alt=""></a></p><a id="w-node-_86a4c6a7-f9c4-ad13-f7bf-6e63b1079e20-ed6becd7" href="https://mastodon.social/@rileytestut" target="_blank">
          
        </a>
        <a id="w-node-e12031f2-99ee-5f24-ac27-ea8e62bab55b-ed6becd7" href="https://twitter.com/shanegillio">
          
        </a>
        <p><a id="w-node-_705f1323-e749-6403-8f86-b86a8cfd6f50-ed6becd7" href="https://twitter.com/shanegillio" target="_blank"><img src="https://altstore.io/images/shaneprof.webp" srcset="https://altstore.io/images/shaneprof-p-500.webp 500w, https://altstore.io/images/shaneprof-p-800.webp 800w, https://altstore.io/images/shaneprof-p-1080.webp 1080w, https://altstore.io/images/shaneprof.webp 1500w" id="w-node-a4aad372-35cc-e00f-45b1-66adc412ef93-ed6becd7" sizes="(max-width: 479px) 100vw, (max-width: 767px) 24vw, (max-width: 991px) 17vw, (max-width: 1439px) 19vw, 188px" alt=""></a>
      </p></div>
      
      <div><p>AltStore is an open-source project developed by a dedicated team of two. We are supported entirely by donations from our community and you can follow along with our progress on GitHub.</p><p>We’re continuously working on new updates for our apps, and you can try out in-development features by joining our Patreon.</p></div>
      <p><a href="https://www.patreon.com/rileyshane" target="_blank">Join Patreon</a>
    </p></div>
  <section id="Downloads">
    <h2><span>Downloads</span></h2>
    <p>AltStore, Delta, and Clip are properties of AltStore LLC and are in no way associated with Nintendo Co., Ltd. or Apple Inc.</p>
    <div>
        <p>AltStore PAL</p>
        <p>Available only in Europe. Requires iOS 17.4 or later.</p>
        <p><a href="https://buy.stripe.com/6oEg2u80z5vI0Mg4gg">€1.50/year + VAT</a></p><div><p>Your subscription covers Apple's Core Technology Fee, payment processing, and server costs.</p><p>Don't want to pay, or not in the EU? Download the version of AltStore below.</p></div>
      </div>
    <p>AltStore (World)</p>
    <p>Requires AltServer to install. Follow our step-by-step <a href="http://faq.altstore.io/">Install Guide</a>
    </p>
    
    <div>
      <p>“[AltStore] is clever, has been verified by other developers, and the service has an active community of thousands of users who side-load apps on their devices. For the past few weeks, I’ve been one of them.”</p>
      
    </div>
    
  </section>
  
  
  <div>
        <p data-w-id="c10f4652-e98c-37f6-51a6-bb8725682d07">
          <h2 data-w-id="0fb59a4f-6d90-2333-688a-b06b291420c6">Experience Apps like Never Before</h2>
          <h3 data-w-id="332883b0-f4e4-5f0a-bd2e-026003ed9cc2">AltStore allows apps to exist on iOS&nbsp;that may not otherwise. <br>‍<br>Apple doesn't allow all apps on their store, so AltStore gives those apps a chance.</h3>
        </p><p><img src="https://altstore.io/images/AltStore_Delta_StorePage.png" width="416" alt="" sizes="100vw" data-w-id="ae8b192f-4a13-e725-9af4-854638dc4268" loading="lazy" srcset="https://altstore.io/images/AltStore_Delta_StorePage-p-500.png 500w, https://altstore.io/images/AltStore_Delta_StorePage-p-800.png 800w, https://altstore.io/images/AltStore_Delta_StorePage-p-1080.png 1080w, https://altstore.io/images/AltStore_Delta_StorePage.png 1400w">
      </p></div>
  
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why you Should Not Apply To YC (364 pts)]]></title>
            <link>https://twitter.com/dvassallo/status/1781751108211511680</link>
            <guid>40099585</guid>
            <pubDate>Sat, 20 Apr 2024 18:26:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/dvassallo/status/1781751108211511680">https://twitter.com/dvassallo/status/1781751108211511680</a>, See on <a href="https://news.ycombinator.com/item?id=40099585">Hacker News</a></p>
Couldn't get https://twitter.com/dvassallo/status/1781751108211511680: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Financial Market Applications of LLMs (104 pts)]]></title>
            <link>https://thegradient.pub/financial-market-applications-of-llms/</link>
            <guid>40099344</guid>
            <pubDate>Sat, 20 Apr 2024 18:03:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thegradient.pub/financial-market-applications-of-llms/">https://thegradient.pub/financial-market-applications-of-llms/</a>, See on <a href="https://news.ycombinator.com/item?id=40099344">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>The AI revolution drove frenzied investment in both private and public companies and captured the public’s imagination in 2023. Transformational consumer products like ChatGPT are powered by Large Language Models (LLMs) that excel at modeling sequences of tokens that represent words or parts of words [2]. Amazingly, structural understanding emerges from learning next-token prediction, and agents are able to complete tasks such as translation, question answering and generating human-like prose from simple user prompts.</p><p>Not surprisingly, quantitative traders have asked: can we turn these models into the next price or trade prediction [1,9,10]? That is, rather than modeling sequences of words, can we model sequences of prices or trades. This turns out to be an interesting line of inquiry that reveals much about both generative AI and financial time series modeling. Be warned this will get wonky.</p><p>LLMs are known as autoregressive learners -- those using previous tokens or elements in a sequence to predict the next element or token. In quantitative trading, for example in strategies like statistical arbitrage in stocks, most research is concerned with identifying autoregressive structure. That means finding sequences of news or orders or fundamental changes that best predict future prices.</p><p>Where things break down is in the quantity and information content of available data to train the models. At the 2023 NeurIPS conference, Hudson River Trading, a high frequency trading firm, presented a comparison of the number of input tokens used to train GPT-3 with the amount of trainable tokens available in the stock market data per year HRT estimated that, with 3,000 tradable stocks, 10 data points per stock per day, 252 trading days per year, and 23400 seconds in a trading day, there are 177 billion stock market tokens per year available as market data. GPT-3 was trained on 500 billion tokens, so not far off [6].</p><figure><img src="https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png" alt="" loading="lazy" width="2000" height="368" srcset="https://thegradient.pub/content/images/size/w600/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png 600w, https://thegradient.pub/content/images/size/w1000/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png 1000w, https://thegradient.pub/content/images/size/w1600/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png 1600w, https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.11.18-PM.png 2356w" sizes="(min-width: 720px) 720px"><figcaption>numbers courtesy of HRT 2023 NeuRIPS presentation</figcaption></figure><p>But, in the trading context the tokens will be prices or returns or trades rather than syllables or words; the former is much more difficult to predict. Language has an underlying linguistic structure (e.g., grammar) [7]. It’s not hard to imagine a human predicting the next word in a sentence, however that same human would find it extremely challenging to predict the next return given a sequence of previous trades, hence the lack of billionaire day traders. The challenge is that there are very smart people competing away any signal in the market, making it <em>almost </em>efficient (“efficiently inefficient”, in the words of economist Lasse Pedersen) and hence unpredictable. No adversary actively tries to make sentences more difficult to predict — if anything, authors usually seek to make their sentences easy to understand and hence <em>more</em> predictable.</p><p>Looked at from another angle, there is much more noise than signal in financial data. Individuals and institutions are trading for reasons that might not be rational or tied to any fundamental change in a business. The GameStop episode in 2021 is one such example. Financial time series are also constantly changing with new fundamental information, regulatory changes, and occasional large macroeconomic shifts such as currency devaluations. Language evolves at a much slower pace and over longer time horizons.</p><p>On the other hand, there are reasons to believe that ideas from AI will work well in financial markets. One emerging area of AI research with promising applications to finance is multimodal learning [5], which aims to use different modalities of data, for example both images and textual inputs to build a unified model. With OpenAI’s DALL-E 2 model, a user can enter text and the model will generate an image. In finance, multi-modal efforts could be useful to combine information classical sources such as technical time series data (prices, trades, volumes, etc.) with alternative data in different modes like sentiment or graphical interactions on twitter, natural language news articles and corporate reports, or the satellite images of shipping activity in a commodity centric port. Here, leveraging multi-modal AI, one could potentially incorporate all these types of non-price information to predict well.</p><p>Another strategy called ‘residualization’ holds prominence in both finance and AI, though it assumes different roles in the two domains. &nbsp;In finance, structural `factor’ models break down the contemporaneous observations of returns across different assets into a shared component (the market return, or more generally returns of common, market-wide factors) and an idiosyncratic component unique to each underlying asset. Market and factor returns are difficult to predict and create interdependence, so it is often helpful to remove the common element when making predictions at the individual asset level and to maximize the number of independent observations in the data. </p><p>In residual network architectures such as transformers, there’s a similar idea that we want to learn a function h(X) of an input X, but it might be easier to learn the residual of h(X) to the identity map, i.e., h(X) – X. Here, if the function h(X) is close to identity, its residual will be close to zero, and hence there will be less to learn and learning can be done more efficiently. In both cases the goal is to exploit structure to refine predictions: in the finance case, the idea is to focus on predicting innovations beyond what is implied by the overall market, for residual networks the focus is on predicting innovations to the identity map.</p><p>A key ingredient for the impressive performance of LLMs work is their ability to discern affinities or strengths between tokens over long horizons known as context windows. In financial markets, the ability to focus attention across long horizons enables analysis of multi-scale phenomena, with some aspects of market changes explained across very different time horizons. For example, at one extreme, fundamental information (e.g., earnings) may be incorporated into prices over months, technical phenomena (e.g., momentum) might be realized over days, and, at the other extreme, microstructure phenomena (e.g., order book imbalance) might have a time horizon of seconds to minutes.</p><p>Capturing all of these phenomena involves analysis of multiple time horizons across the context window. However, in finance, prediction over multiple <em>future</em> time horizons is also important. For example, a quantitative system may seek to trade to profit from multiple different anomalies that are realized over multiple time horizons (e.g., simultaneously betting on a microstructure event and an earnings event). This requires predicting not just the next period return of the stock, but the entire term structure or trajectory of expected returns, while current transformer-style predictive models only look one period in the future.</p><p>Another financial market application of LLMs might be synthetic data creation [4,8]. This could take a few directions. Simulated stock price trajectories can be generated that mimic characteristics observed in the market and can be extremely beneficial given that financial market data is scarce relative to other sources as highlighted above in the number of tokens available. Artificial data could open the door for meta-learning techniques which have successfully been applied, for example, in robotics. In the robotic setting controllers are first trained using cheap but not necessarily accurate physics simulators, before being better calibrated using expensive real world experiments with robots. In finance the simulators could be used to coarsely train and optimize trading strategies. The model would learn high level concepts like risk aversion and diversification and tactical concepts such as trading slowly to minimize the price impact of a trade. Then precious real market data could be employed to fine-tune the predictions and determine precisely the optimal speed to trade.</p><figure><img src="https://thegradient.pub/content/images/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png" alt="" loading="lazy" width="2000" height="258" srcset="https://thegradient.pub/content/images/size/w600/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png 600w, https://thegradient.pub/content/images/size/w1000/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png 1000w, https://thegradient.pub/content/images/size/w1600/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png 1600w, https://thegradient.pub/content/images/size/w2400/2024/04/Screenshot-2024-04-18-at-3.08.44-PM.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>Financial market practitioners are often interested in extreme events, the times when trading strategies are more likely to experience significant gains or losses. Generative models where it’s possible to sample from extreme scenarios could find use. However extreme events by definition occur rarely and hence determining the right parameters and sampling data from the corresponding distribution is fraught.</p><p>Despite the skepticism that LLMs will find use in quantitative trading, they might boost fundamental analysis. As AI models improve, it’s easy to imagine them helping analysts refine an investment thesis, uncover inconsistencies in management commentary or find latent relationships between tangential industries and businesses [3]. Essentially these models could provide a Charlie Munger for every investor.</p><p>The surprising thing about the current generative AI revolution is that it’s taken almost everyone – academic researchers, cutting edge technology firms and long-time observers – by surprise. The idea that building bigger and bigger models would lead to emergent capabilities like we see today was totally unexpected and still not fully understood.</p><p>The success of these AI models has supercharged the flow of human and financial capital into AI, which should in turn lead to even better and more capable models. So while the case for GPT-4 like models taking over quantitative trading is currently unlikely, we advocate keeping an open mind. Expecting the unexpected has been a profitable theme in the AI business.</p><hr><h3 id="references">References</h3><ol><li>“Applying Deep Neural Networks to Financial Time Series Forecasting” Allison Koenecke. 2022</li><li>“<a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=2960712678066186980&amp;btnI=1&amp;hl=en">Attention is all you need</a>.” A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones… &nbsp;Advances in Neural Information Processing Systems, 2017</li><li>“Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models” . Lopez-Lira, Alejandro and Tang, Yuehua, (April 6, 2023) Available at SSRN</li><li>“<a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=8154893591177160457&amp;btnI=1&amp;hl=en">Generating Synthetic Data in Finance: Opportunities, Challenges and Pitfalls</a>.” SA Assefa, D Dervovic, M Mahfouz, RE Tillman… - Proceedings of the First ACM International Conference …, 2020</li><li>“GPT-4V(ision) System Card.” OpenAI. September 2023</li><li>“<a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=15953747982133883426&amp;btnI=1&amp;hl=en">Language models are few-shot learners</a>.” T Brown, B Mann, N Ryder, M Subbiah, JD Kaplan… - Advances in Neural Information Processing Systems, 2020</li><li>“Sequence to Sequence Learning with Neural Networks.” I.Sutskever,O.Vinyals,and Q.V.Le in Advances in Neural Information Processing Systems, 2014, pp. 3104–3112.</li><li>“Synthetic Data Generation for Economists”. A Koenecke, H Varian &nbsp;- arXiv preprint arXiv:2011.01374, 2020</li><li>C. C. Moallemi, M. Wang. A reinforcement learning approach to optimal execution. Quantitative Finance, 22(6):1051–1069, March 2022.</li><li>C. Maglaras, C. C. Moallemi, M. Wang. A deep learning approach to estimating fill probabilities in a limit order book. Quantitative Finance, 22(11):1989–2003, October 2022.</li></ol><h3 id="citation">Citation</h3><p>For attribution in academic contexts or books, please cite this work as</p><pre><code>Richard Dewey and Ciamac Moallemi, "Financial Market Applications of LLMs," The Gradient, 2024</code></pre><pre><code>@article{dewey2024financial,
    author = {Richard Dewey and Ciamac Moallemi},
    title = {Financial Market Applications of LLMs},
    journal = {The Gradient},
    year = {2024},
    howpublished = {\url{https://thegradient.pub/financial-market-applications-of-llms},
}</code></pre>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Self-reasoning tokens: teaching models to think ahead (106 pts)]]></title>
            <link>https://reasoning-tokens.ghost.io/reasoning-tokens/</link>
            <guid>40099252</guid>
            <pubDate>Sat, 20 Apr 2024 17:54:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reasoning-tokens.ghost.io/reasoning-tokens/">https://reasoning-tokens.ghost.io/reasoning-tokens/</a>, See on <a href="https://news.ycombinator.com/item?id=40099252">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p><a href="https://reasoning-tokens.ghost.io/author/felipe/">
                                <img src="https://www.gravatar.com/avatar/c7a32029edda420c1bff08b48f99b2cb?s=250&amp;r=x&amp;d=mp" alt="Felipe Sens Bonetto">
                            </a>
                </p>
                <div>
                    
                    <p><time datetime="2024-04-20">Apr 20, 2024</time>
                            <span><span>—</span> 4 min read</span>
                    </p>
                </div>
            </div><section>
            <p>What is the mathematical formulation of reasoning? How can we make LLMs like chatGPT think before they speak? And how can we make that baked into the model so it can learn to think in a self-supervised way without having to "explain it step by step" (or another famous prompt we use when we want to improve chatGPT performance drastically)? How can we teach models to think ahead? I will share with you the results of some experiments that may cast light on the path of "Reasoning Tokens."</p><p><strong>Introduction</strong></p><p>As the authors of <a href="https://arxiv.org/abs/2211.00593?ref=reasoning-tokens.ghost.io" rel="noreferrer">"Interpretability in the wild"</a> have taught us, from looking inside transformers, we know that the computation of the next token includes some information computed in previous steps.  This may seem obvious at first glance, but there is more to this affirmation than what meets the eye. This means the language model expends some internal "cognitive power" processing and storing information that will be used, not for predicting the very next token but 2, 3, or even 10 tokens ahead.</p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/Screenshot-2024-04-20-at-12.11.57-AM.png" alt="" loading="lazy" width="1520" height="654" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-20-at-12.11.57-AM.png 600w, https://reasoning-tokens.ghost.io/content/images/size/w1000/2024/04/Screenshot-2024-04-20-at-12.11.57-AM.png 1000w, https://reasoning-tokens.ghost.io/content/images/2024/04/Screenshot-2024-04-20-at-12.11.57-AM.png 1520w" sizes="(min-width: 720px) 720px"><figcaption><span>Internal computation of GPT-2, extracted from the "Interpretability in the wild" paper </span></figcaption></figure><p>As we can see from the image above, the attention heads produce computations that will be helpful only in the far future, and even some calculations that "headge" against the wrong answers, exposed in the paper as "Negative Name Mover Heads" or attention heads that suppress specific tokens.</p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/Screenshot-2024-04-20-at-12.30.01-AM.png" alt="" loading="lazy" width="920" height="860" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/Screenshot-2024-04-20-at-12.30.01-AM.png 600w, https://reasoning-tokens.ghost.io/content/images/2024/04/Screenshot-2024-04-20-at-12.30.01-AM.png 920w" sizes="(min-width: 720px) 720px"><figcaption><span>Visual explanation extracted from the "Do Language Models Plan for Future Tokens?" paper</span></figcaption></figure><p>Further work has shown that LLMs indeed plan for future tokens. In the paper <a href="https://arxiv.org/abs/2404.00859?ref=reasoning-tokens.ghost.io" rel="noreferrer">"Do Language Models Plan for Future Tokens?"</a> the authors carefully crafted a mathematical formulation to impede what they call "Pre-Caching," or the ability of the model to make intermediary computations that would be useful beyond the very next token. Their experiments found a small performance gap when the model was "myopic" or incapable of planning for future tokens. This is promising but could be better. This indicates that while GPTs plan ahead, most of their power is used to predict only the next word in the sequence. As a sanity check, this gap should increase as the length of the predicted text grows because the model would have more tokens to produce said computations, and indeed, that was what they found in the paper.</p><p><strong>How do we leverage that?</strong></p><p>What if we incentivized those intermediary calculations, which are useful only in future tokens, teaching the model to think ahead in a self-supervised way? It turns out that the formulation for such a task doesn't need to be that complicated.</p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/image-1.png" alt="" loading="lazy" width="1312" height="504" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/image-1.png 600w, https://reasoning-tokens.ghost.io/content/images/size/w1000/2024/04/image-1.png 1000w, https://reasoning-tokens.ghost.io/content/images/2024/04/image-1.png 1312w" sizes="(min-width: 720px) 720px"><figcaption><span>Gradient flow of Reasoning tokens!</span></figcaption></figure><p>In this first experiment, we introduce <strong>reasoning tokens</strong>! The model will produce two tokens for each token in the original sequence. As usual, the first token will be used to predict the next token. The second token, however, duplicates the input of the first one and does not receive a gradient "answer" from the very next token, only from future tokens; in fact, this token doesn't even participate in the calculation of the very next token. This incentivizes the model to "pre-cache" or <em>only</em> put information that is useful for the future in this spot. <em>But talk is cheap. Show me the results.</em></p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/image-3.png" alt="" loading="lazy" width="2000" height="918" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/image-3.png 600w, https://reasoning-tokens.ghost.io/content/images/size/w1000/2024/04/image-3.png 1000w, https://reasoning-tokens.ghost.io/content/images/size/w1600/2024/04/image-3.png 1600w, https://reasoning-tokens.ghost.io/content/images/2024/04/image-3.png 2000w" sizes="(min-width: 720px) 720px"><figcaption><span>Mini GPT-2 (10M params) trained on 82M tokens.</span></figcaption></figure><p>And the results are very promising, showing a reduction of <strong>35% in the loss</strong>! From 0.621 to 0.401. The experiment also shows that the model benefits from having multiple tokens to do its "reasoning," forecasting the capability to form long-range dependencies. This validates the hypothesis that we can teach the models to plan for the future, an important first step to get to reasoning. </p><p>A GPT-2 Small (124M params) model was also trained on 300B tokens of the "Open Web Text Corpus," and its results were also very promising, resulting in a 0.04 validation loss reduction from 2.85 to 2.81. In context, going from GPT-2 Large (~700M) to GPT-2 XL (1.5B) drops the validation loss by 0.13 in the same dataset. All training code was derived from Andrej Karpathy amazing <a href="https://github.com/karpathy/nanoGPT/tree/master?ref=reasoning-tokens.ghost.io" rel="noreferrer">GPT-2 implementation</a>.</p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/image-4.png" alt="" loading="lazy" width="1088" height="628" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/image-4.png 600w, https://reasoning-tokens.ghost.io/content/images/size/w1000/2024/04/image-4.png 1000w, https://reasoning-tokens.ghost.io/content/images/2024/04/image-4.png 1088w" sizes="(min-width: 720px) 720px"><figcaption><span>GPT-2 Small trained on 300B params - 1 Reasoning token</span></figcaption></figure><p><strong>What is next for Reasoning Tokens?</strong></p><p>Currently, I'm experimenting with reasoning tokens in fine-tuned instruction following models, where planning can be much more useful. The formulation is very close to the first experiment. Still, this time, the model can choose when this internal reasoning will start, allowing it to choose when to reason before producing the next word in the sequence.</p><figure><img src="https://reasoning-tokens.ghost.io/content/images/2024/04/image-5.png" alt="" loading="lazy" width="1714" height="910" srcset="https://reasoning-tokens.ghost.io/content/images/size/w600/2024/04/image-5.png 600w, https://reasoning-tokens.ghost.io/content/images/size/w1000/2024/04/image-5.png 1000w, https://reasoning-tokens.ghost.io/content/images/size/w1600/2024/04/image-5.png 1600w, https://reasoning-tokens.ghost.io/content/images/2024/04/image-5.png 1714w" sizes="(min-width: 720px) 720px"><figcaption><span>Reasoning tokens in instruction tasks</span></figcaption></figure><p>The hypothesis being tested is that the addition of Reasoning Tokens can substitute and outperform models where a "step by step" explanation is included in the training phase. This would be useful because those explanations are expensive to produce/obtain. Although such explanations can be useful to the model, gradient descent could find other ways to do that reasoning using all the internal mathematical dimensions of the model in a way that does not necessarily make sense to us. It would be a great fit for "Mixture of Experts" (MoE) models, where we can have an expert just for the reasoning phase.</p><p>The future is bright. Stay tuned for the next advancements.</p>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VLC vs. the App Stores (123 pts)]]></title>
            <link>https://mjtsai.com/blog/2024/04/19/vlc-vs-the-app-stores/</link>
            <guid>40098867</guid>
            <pubDate>Sat, 20 Apr 2024 17:15:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mjtsai.com/blog/2024/04/19/vlc-vs-the-app-stores/">https://mjtsai.com/blog/2024/04/19/vlc-vs-the-app-stores/</a>, See on <a href="https://news.ycombinator.com/item?id=40098867">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p><a href="https://twitter.com/videolan/status/1771104206872555660">VideoLAN</a> (via <a href="https://news.ycombinator.com/item?id=39798565">Hacker News</a>):</p>
<blockquote cite="https://twitter.com/videolan/status/1771104206872555660"><p>App Stores were a mistake.</p><p>Currently, we cannot update VLC on Windows Store, and we cannot update VLC on Android Play Store, without reducing security or dropping a lot of users…</p><p>For now, iOS App Store still allows us to ship for iOS9, but until when?</p></blockquote>

<p><a href="https://twitter.com/videolan/status/1759688788232523788">VideoLAN</a>:</p>
<blockquote cite="https://twitter.com/videolan/status/1759688788232523788"><p>If you do wonder why we don’t update VLC on the Windows Store or why VLC/iOS can’t connect properly to OneDrive shares, it’s because Microsoft Kafkaïesque bureaucracy refuses to help us.</p><p>We’re only trying to contact someone since 2years…</p></blockquote>

<p><a href="https://twitter.com/videolan/status/1771102415279763909">VideoLAN</a> (<a href="https://social.treehouse.systems/@Aissen/112139649840297169">Anisse</a>, <a href="https://news.ycombinator.com/item?id=39827828">Hacker News</a>):</p>
<blockquote cite="https://twitter.com/videolan/status/1771102415279763909"><p>If you wonder why we can’t update the VLC on Android version, it’s because Google refuses to let us update:</p><ul><li>either we give them our private signing keys,</li><li>or we drop support for Android TV before API-30, and all our users on TV API&lt;30 can’t get fixes.</li></ul></blockquote>

<p><a href="https://twitter.com/videolan/status/1771123709366943875">VideoLAN</a>:</p>
<blockquote cite="https://twitter.com/videolan/status/1771123709366943875">
<p>VLC cannot even enter the Mac App Store, because of the restrictions…</p>
</blockquote>

<p>Look at all those platforms competing to benefit users.</p>

<p><a href="https://twitter.com/Florian4Gamers/status/1778742764567429366">Florian Mueller</a>:</p>
<blockquote cite="https://twitter.com/Florian4Gamers/status/1778742764567429366"><p>This here is a European app store for Android and Google’s <a href="https://twitter.com/luishg/status/1778717325652341075">YouTube has just killed their channel</a>. It’s obviously a problem if you depend on the incumbent’s platforms all the way.</p></blockquote>

<p>Previously:</p>
<ul>
<li><a href="https://mjtsai.com/blog/2024/03/21/u-s-sues-apple-over-iphone-monopoly/">U.S. Sues Apple Over iPhone Monopoly</a></li>
<li><a href="https://mjtsai.com/blog/2021/07/01/google-sunsets-the-apk-format-for-new-android-apps/">Google Sunsets the APK Format for New Android Apps</a></li>
</ul><p><a rel="tag" href="https://mjtsai.com/blog/tag/android/">Android</a> <a rel="tag" href="https://mjtsai.com/blog/tag/appstore/">App Store</a> <a rel="tag" href="https://mjtsai.com/blog/tag/google-play-store/">Google Play Store</a> <a rel="tag" href="https://mjtsai.com/blog/tag/ios/">iOS</a> <a rel="tag" href="https://mjtsai.com/blog/tag/ios-17/">iOS 17</a> <a rel="tag" href="https://mjtsai.com/blog/tag/ios-9/">iOS 9</a> <a rel="tag" href="https://mjtsai.com/blog/tag/macapp/">Mac App</a> <a rel="tag" href="https://mjtsai.com/blog/tag/macappstore/">Mac App Store</a> <a rel="tag" href="https://mjtsai.com/blog/tag/macos-14-sonoma/">macOS 14 Sonoma</a> <a rel="tag" href="https://mjtsai.com/blog/tag/onedrive/">Microsoft OneDrive</a> <a rel="tag" href="https://mjtsai.com/blog/tag/vlc/">VLC</a> <a rel="tag" href="https://mjtsai.com/blog/tag/windows-store/">Windows Store</a> <a rel="tag" href="https://mjtsai.com/blog/tag/youtube/">YouTube</a></p>





















</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stop Doing Cloud (101 pts)]]></title>
            <link>https://grski.pl/self-host</link>
            <guid>40098405</guid>
            <pubDate>Sat, 20 Apr 2024 16:12:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://grski.pl/self-host">https://grski.pl/self-host</a>, See on <a href="https://news.ycombinator.com/item?id=40098405">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h2>STOP DOING CLOUD</h2>
<p>This will be a feisty juicy article, a bit controversial. I think more than a half of the users of the cloud/kubernetes would be better off without it. AWS should stand for <code>how to have people pay for our infra we need once per year during black friday and actually make money out of it</code>. Declouding is a nice trend I'm seeing now. 37signals have some good stuff on it. I'll ad my share as to why what has once been something cool has evolved into an abomination that often adds more complexity and problems than it brings, at least for some people. </p>
<p><img alt="meme about the cloud" src="https://grski.pl/static/articles/cloud/fools.png"></p>
<p>Sure it has it's uses at a certain scale and so on. The problem is almost no one is at such a scale and never will be, but we are blindly following a trend, pretending it's not the reality. </p>
<p>Why not try... Simplicty? Boring old stuff that just works, is easily debuggable and that even one person can grasp? </p>
<p>No your startup with 100k monthly users probably doesn't need all the stuff AWS excells at. To be honest most of you will be fine running a single dedicated bare-metal server.</p>
<p>Cloud pricing is unclear often, performance is not that dependable, especially on shared resources. To bring down costs you need to often sign up for long-term plans. So on so forth. Layers of abstractions upon abstractions.</p>
<p>I still do use the cloud in some of my work, but there's an alternative people have forgotten about - actually hosting your shit. Owning your data. Your architecture. Everything. Today I'll show you an example how we can do that. In this article we will go through setting up a self-hosted postgres instance, replicated/scalable API, load balancer, automatic ssl management, simple deployment that can be automated in 10 minutes and lastly, we will do that for under $200 per month and within 15 minutes. With this setup in some cases I'd argue you can handle up to 1M monthly active users without a hitch. See why the cloud providers and gurus have...</p>
<h2>They have played us for absolute fools.</h2>
<p>In the article I assume you have a server running somewhere. Preferable a dedicated one. In my case it's 80 core 128 gb hetzner, that I got for $200 per month.</p>
<p>Before starting let's install some utils we will need and update our server.</p>
<div><pre><span></span><code>sudo<span> </span>apt<span> </span>update<span> </span><span>&amp;&amp;</span><span> </span>sudo<span> </span>apt<span> </span>upgrade
sudo<span> </span>apt<span> </span>install<span> </span>gnupg2<span> </span>wget<span> </span>vim<span> </span>ca-certificates<span> </span>curl<span> </span>gnupg<span> </span>lsb-release
</code></pre></div>
<h2>Installing postgres</h2>
<h3>Installing needed packages</h3>
<div><pre><span></span><code>sudo<span> </span>sh<span> </span>-c<span> </span><span>'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" &gt; /etc/apt/sources.list.d/pgdg.list'</span>
curl<span> </span>-fsSL<span> </span>https://www.postgresql.org/media/keys/ACCC4CF8.asc<span> </span><span>|</span><span> </span>sudo<span> </span>gpg<span> </span>--dearmor<span> </span>-o<span> </span>/etc/apt/trusted.gpg.d/postgresql.gpg
sudo<span> </span>apt-get<span> </span>--purge<span> </span>remove<span> </span>postgresql<span> </span>postgresql-*<span> </span><span># IF YOU HAD POSTGRES PREVIOUSLY</span>
sudo<span> </span>apt<span> </span>update
sudo<span> </span>apt<span> </span>install<span> </span>postgresql-16<span> </span>postgresql-contrib-16
sudo<span> </span>systemctl<span> </span>start<span> </span>postgresql
sudo<span> </span>systemctl<span> </span><span>enable</span><span> </span>postgresql
</code></pre></div>
<p>Let's quickly walk through what we did here. We've added newest postgres repos so that our server knows what and where to install from. In case of ubuntu 22.04, the default postgres version that the distro repos come with is postgres 14. We want the new fancy shiny stuff, so we had to make that extra step.</p>
<p>Then, we optionally uninstall previous postgres versions. I doubt you had any, but adding this step as it might be helpful for some of you. Be careful though. That <code>--purge</code> thing will purge a lot of stuff. Your data from databases included. If you want to ugprade from existing postgres installation, this guide is not for you.</p>
<p>After that we update our sources and install postgres + some needed packages. </p>
<p>Lastly we start the postgresql service and make it enabled - so it boots after startup. </p>
<p>Now, we have to... Well, actually that's it. AWS marketers have done a good job in making you think installing and running a database was hard and a complex task. In some cases it is, indeed. But for the average IT Joe/startup? I wouldn't say so.</p>
<h3>Creating new database and user</h3>
<p>Now we need to do a little bit of setup on our database. In order to do that, let's connect to it using <code>psql</code>. How?</p>

<p>now you should see something like:</p>
<div><pre><span></span><code>psql<span> </span><span>(</span><span>16</span>.0<span> </span><span>(</span>Ubuntu<span> </span><span>16</span>.0-1.pgdg22.04+1<span>))</span>
Type<span> </span><span>"help"</span><span> </span><span>for</span><span> </span>help.

<span>postgres</span><span>=</span><span>#</span>
</code></pre></div>
<p>Boom. There we are. To test if our efforts in installing the newest postgres version have not failed, type:</p>
<div><pre><span></span><code><span>psql</span><span> </span><span>(</span><span>16</span><span>.</span><span>0</span><span> </span><span>(</span><span>Ubuntu</span><span> </span><span>16</span><span>.</span><span>0</span><span>-</span><span>1</span><span>.</span><span>pgdg22</span><span>.</span><span>04</span><span>+</span><span>1</span><span>))</span>
<span>Type</span><span> </span><span>"help"</span><span> </span><span>for</span><span> </span><span>help</span><span>.</span>

<span>postgres</span><span>=#</span><span> </span><span>SELECT</span><span> </span><span>version</span><span>();</span>
<span>                                                              </span><span>version</span>
<span>-----------------------------------------------------------------------------------------------------------------------------------</span>
<span> </span><span>PostgreSQL</span><span> </span><span>16</span><span>.</span><span>0</span><span> </span><span>(</span><span>Ubuntu</span><span> </span><span>16</span><span>.</span><span>0</span><span>-</span><span>1</span><span>.</span><span>pgdg22</span><span>.</span><span>04</span><span>+</span><span>1</span><span>)</span><span> </span><span>on</span><span> </span><span>x86_64</span><span>-</span><span>pc</span><span>-</span><span>linux</span><span>-</span><span>gnu</span><span>,</span><span> </span><span>compiled</span><span> </span><span>by</span><span> </span><span>gcc</span><span> </span><span>(</span><span>Ubuntu</span><span> </span><span>11</span><span>.</span><span>4</span><span>.</span><span>0</span><span>-</span><span>1</span><span>ubuntu1</span><span>~</span><span>22</span><span>.</span><span>04</span><span>)</span><span> </span><span>11</span><span>.</span><span>4</span><span>.</span><span>0</span><span>,</span><span> </span><span>64</span><span>-</span><span>bit</span>
<span>(</span><span>1</span><span> </span><span>row</span><span>)</span>
</code></pre></div>
<p>As you can see, <code>PostgreSQL 16.0</code>. Congrats, we made it brahs. </p>
<p>Currently you are inside your postgres, running as the default allmighty postgres user on postgres database. Now - we DO NOT EVER want to run our apps on this database. Don't be a lazy bum. It's a big security breach potentailly. So what do we do instead one might ask? That is a trivial question - we need to create a seprate database and a separate user for that database. Usually you have separate db (or multiple dbs actually) for an app/service couple with user just for that db.</p>
<p>That way if someone ever manages to break into your DB, in case you are hosting multiple dbs with data from multiple apps, they only get access to that one particular db. Is it hard? Nope. Check this out:</p>
<div><pre><span></span><code><span>postgres</span><span>=#</span><span> </span><span>CREATE</span><span> </span><span>DATABASE</span><span> </span><span>yourdbname</span><span>;</span>
<span>CREATE</span><span> </span><span>DATABASE</span>
<span>postgres</span><span>=#</span><span> </span><span>CREATE</span><span> </span><span>USER</span><span> </span><span>youruser</span><span> </span><span>WITH</span><span> </span><span>ENCRYPTED</span><span> </span><span>PASSWORD</span><span> </span><span>'yourpass'</span><span>;</span>
<span>CREATE</span><span> </span><span>ROLE</span>
<span>postgres</span><span>=#</span><span> </span><span>GRANT</span><span> </span><span>ALL</span><span> </span><span>PRIVILEGES</span><span> </span><span>ON</span><span> </span><span>DATABASE</span><span> </span><span>yourdbname</span><span> </span><span>TO</span><span> </span><span>youruser</span><span>;</span>
<span>GRANT</span>
<span>postgres</span><span>=#</span><span> </span><span>ALTER</span><span> </span><span>DATABASE</span><span> </span><span>yourdbname</span><span> </span><span>OWNER</span><span> </span><span>TO</span><span> </span><span>youruser</span><span>;</span>
<span>ALTER</span><span> </span><span>DATABASE</span>
</code></pre></div>
<p>We created a new user with a particular password, created a new database. Then we assigned the user privilages to perform all operations on said database, but only on that database.</p>
<p>The last line is needed because we created the database as the postgres user. Which means that while the user can perform actions on the database, he can only perform actions on the database objects that are his own. Because we created the database as the postgres user, and during db creation it gets created with some default schemas/tables, by default the owner of these is the user that created it. In our case - postgres. So other than allowing our new user to perform any action on the said database, we need to now make him an owner of the stuff that's already existing in the database so he can modify it too if needed, and it will be. </p>
<p>By the way interesting concept right? Even when you create an empty database, it's already populated with some stuff, so it ain't empty. IT, right?</p>
<p>That's pretty much it. Or is it? </p>
<h3>Connecting to postgres from outside of localhost</h3>
<p>Postgres, by default, only allows you to connect to itself from localhost/local machine to simplify. Meaning - any connections from other ips, networks etc. will be rejected. It's a very needed security measure that prevents random people from the internet to try and brute force their way into your database. That is the last thing you want.</p>
<p>However we live in a world where everything is running in a container. Containers have their own networks (usually) and when we make requests from inside of the container, the network we are in will be a bit different, meaning we won't be 'marked' as localhost, which in turn currently will make postgres reject our connection, even if we specify correct credentials.</p>
<p>I know it may sound tricky - how come, we are on the same machine, local machine. Why is our request treated as it isn't? This relats to how docker, containers and their networking works. Docker has it's own private network for all the stuff it does, sometimes sharing it with the host (in the host network mode) or having a 'bridge' that acts as a, well, bridge, between the local network and docker network, which allows you to, for example, call services hosted on the host machine, from within docker container.</p>
<p>This way you can have multiple docker containers or docker-composes running, some of which internally are using the same ports, without conflicts and so on. They are usually put in other networks created eg. per docker-compose (unless you specify otherwise).</p>
<p>It's a great thing, however in this case it complicates stuff for us, but not by much. What do we have to do?</p>
<p>Well, first of all, install docker! It'll come in handy, right?</p>
<h2>Installing docker on ubuntu 22.04</h2>
<p>First off, again - if you tried to install something before hand you might want to do this to purge everything and have a clean slate. Again - be careful.</p>
<div><pre><span></span><code>sudo<span> </span>apt<span> </span>remove<span> </span>docker-desktop
rm<span> </span>-r<span> </span><span>$HOME</span>/.docker/desktop
sudo<span> </span>rm<span> </span>/usr/local/bin/com.docker.cli
sudo<span> </span>apt<span> </span>purge<span> </span>docker-desktop
</code></pre></div>
<p>Once we have that, we will add new sources to our repos, this time for docker, similarly as we did for our postgres.</p>
<div><pre><span></span><code>curl<span> </span>-fsSL<span> </span>https://download.docker.com/linux/ubuntu/gpg<span> </span><span>|</span><span> </span>sudo<span> </span>gpg<span> </span>--dearmor<span> </span>-o<span> </span>/etc/apt/keyrings/docker.gpg
<span>echo</span><span> </span><span>\</span>
<span>  </span><span>"deb [arch=</span><span>$(</span>dpkg<span> </span>--print-architecture<span>)</span><span> signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \</span>
<span>  </span><span>$(</span>lsb_release<span> </span>-cs<span>)</span><span> stable"</span><span> </span><span>|</span><span> </span>sudo<span> </span>tee<span> </span>/etc/apt/sources.list.d/docker.list<span> </span>&gt;<span> </span>/dev/null
sudo<span> </span>apt<span> </span>update
</code></pre></div>
<p>Now, let's see what versions are available to us:</p>
<div><pre><span></span><code>apt-cache<span> </span>madison<span> </span>docker-ce<span> </span><span>|</span><span> </span>awk<span> </span><span>'{ print $3 }'</span>
<span>5</span>:24.0.6-1~ubuntu.22.04~jammy
<span>5</span>:24.0.5-1~ubuntu.22.04~jammy
<span>5</span>:24.0.4-1~ubuntu.22.04~jammy
<span>5</span>:24.0.3-1~ubuntu.22.04~jammy
<span>5</span>:24.0.2-1~ubuntu.22.04~jammy
<span>5</span>:24.0.1-1~ubuntu.22.04~jammy
<span>5</span>:24.0.0-1~ubuntu.22.04~jammy
<span>5</span>:23.0.6-1~ubuntu.22.04~jammy
<span>5</span>:23.0.5-1~ubuntu.22.04~jammy
<span>5</span>:23.0.4-1~ubuntu.22.04~jammy
<span>5</span>:23.0.3-1~ubuntu.22.04~jammy
<span>5</span>:23.0.2-1~ubuntu.22.04~jammy
<span>5</span>:23.0.1-1~ubuntu.22.04~jammy
<span>(</span>...<span>)</span>
</code></pre></div>
<p>I decided to go with the newest one, if you for some reason want to install another, feel free.</p>
<div><pre><span></span><code><span>VERSION_STRING</span><span>=</span><span>5</span>:24.0.6-1~ubuntu.22.04~jammy
sudo<span> </span>apt<span> </span>install<span> </span>docker-ce<span>=</span><span>$VERSION_STRING</span><span> </span>docker-ce-cli<span>=</span><span>$VERSION_STRING</span><span> </span>containerd.io<span> </span>docker-compose-plugin
</code></pre></div>
<p>aaand done. Let's test our docker installation.</p>
<div><pre><span></span><code>docker<span> </span>run<span> </span>hello-world
Unable<span> </span>to<span> </span>find<span> </span>image<span> </span><span>'hello-world:latest'</span><span> </span>locally
latest:<span> </span>Pulling<span> </span>from<span> </span>library/hello-world
719385e32844:<span> </span>Pull<span> </span><span>complete</span>
Digest:<span> </span>sha256:88ec0acaa3ec199d3b7eaf73588f4518c25f9d34f58ce9a0df68429c5af48e8d
Status:<span> </span>Downloaded<span> </span>newer<span> </span>image<span> </span><span>for</span><span> </span>hello-world:latest

Hello<span> </span>from<span> </span>Docker!
This<span> </span>message<span> </span>shows<span> </span>that<span> </span>your<span> </span>installation<span> </span>appears<span> </span>to<span> </span>be<span> </span>working<span> </span>correctly.

To<span> </span>generate<span> </span>this<span> </span>message,<span> </span>Docker<span> </span>took<span> </span>the<span> </span>following<span> </span>steps:
<span> </span><span>1</span>.<span> </span>The<span> </span>Docker<span> </span>client<span> </span>contacted<span> </span>the<span> </span>Docker<span> </span>daemon.
<span> </span><span>2</span>.<span> </span>The<span> </span>Docker<span> </span>daemon<span> </span>pulled<span> </span>the<span> </span><span>"hello-world"</span><span> </span>image<span> </span>from<span> </span>the<span> </span>Docker<span> </span>Hub.
<span>    </span><span>(</span>amd64<span>)</span>
<span> </span><span>3</span>.<span> </span>The<span> </span>Docker<span> </span>daemon<span> </span>created<span> </span>a<span> </span>new<span> </span>container<span> </span>from<span> </span>that<span> </span>image<span> </span>which<span> </span>runs<span> </span>the
<span>    </span>executable<span> </span>that<span> </span>produces<span> </span>the<span> </span>output<span> </span>you<span> </span>are<span> </span>currently<span> </span>reading.
<span> </span><span>4</span>.<span> </span>The<span> </span>Docker<span> </span>daemon<span> </span>streamed<span> </span>that<span> </span>output<span> </span>to<span> </span>the<span> </span>Docker<span> </span>client,<span> </span>which<span> </span>sent<span> </span>it
<span>    </span>to<span> </span>your<span> </span>terminal.

To<span> </span>try<span> </span>something<span> </span>more<span> </span>ambitious,<span> </span>you<span> </span>can<span> </span>run<span> </span>an<span> </span>Ubuntu<span> </span>container<span> </span>with:
<span> </span>$<span> </span>docker<span> </span>run<span> </span>-it<span> </span>ubuntu<span> </span>bash

Share<span> </span>images,<span> </span>automate<span> </span>workflows,<span> </span>and<span> </span>more<span> </span>with<span> </span>a<span> </span>free<span> </span>Docker<span> </span>ID:
<span> </span>https://hub.docker.com/

For<span> </span>more<span> </span>examples<span> </span>and<span> </span>ideas,<span> </span>visit:
<span> </span>https://docs.docker.com/get-started/
</code></pre></div>
<p>Seems to be working. How about <code>docker-compose</code>?</p>
<div><pre><span></span><code>docker-compose
Command<span> </span><span>'docker-compose'</span><span> </span>not<span> </span>found,<span> </span>but<span> </span>can<span> </span>be<span> </span>installed<span> </span>with:
apt<span> </span>install<span> </span>docker-compose
</code></pre></div>
<p>Not there, weird? Nope. Some of you might be still used to the old <code>docker-compose</code> thingy, however some time ago it got moved to be a part of the docker itself, which means that now instead of <code>docker-compose</code> you do:</p>
<div><pre><span></span><code>docker<span> </span>compose

Usage:<span>  </span>docker<span> </span>compose<span> </span><span>[</span>OPTIONS<span>]</span><span> </span>COMMAND

Define<span> </span>and<span> </span>run<span> </span>multi-container<span> </span>applications<span> </span>with<span> </span>Docker.
</code></pre></div>
<p>Alright! We set. Almost.</p>
<p>Currently, if you sshed on a clean server, which I assume you did, you are running as the root user. You can check this by typing:</p>

<p>The problem with that is similar to the situation with our postgres and the almighty postgres user. </p>
<p>Ideally, we do not want to run our containers as root, to prevent attackers from being able to do bad stuff to the whole server. Let's create a new user where we will be running our containers. You can have user per app or service, but not sure if you need that. Just not running on root is usually good enough.</p>
<p>How?</p>
<h3>Running docker on non-user or rootless docker</h3>
<p>That's all quite simple.</p>
<p>We need to create a new user, add it to the sudoers grup, set a password for it and lastly add it to the docker group. In our case we will create a prod user and then add it to the sudo group and docker group.</p>
<div><pre><span></span><code>sudo<span> </span>useradd<span> </span>prod
sudo<span> </span>usermod<span> </span>-aG<span> </span>sudo<span> </span>prod
sudo<span> </span>passwd<span> </span>prod
sudo<span> </span>usermod<span> </span>-aG<span> </span>docker<span> </span>docker
</code></pre></div>
<p>That's quite much it for now.</p>
<h3>Enabling docker containers to connect to host postgres</h3>
<p>The sane way. Some people deal with the issue described before, the one regarding connections from outside of localhost, by allowing <code>*</code> which means any and all networks/ips. This is a NO GO for production, really. The sane way is, as mentioned, to only allow specific networks. In our case docker network. How to do that?</p>
<p>We have to find out what is the local ip address that our docker network got assigned and simply allow traffic from that network to access. Sounds tricky, but ain't.</p>
<p>Now, before we proceed, I'm not that proficient in networking to be frank. Which means that my solution, while working, might not be the ideal one. Happy for feedback from someone more knowledgeable in the topic.</p>
<p>We want to add our docker network to those permitted inside our postgres. This implies we need to know the docker network address. How to get it?</p>
<div><pre><span></span><code>ip<span> </span>addr<span> </span><span>|</span><span> </span>grep<span> </span>docker
<span>3</span>:<span> </span>docker0:<span> </span>&lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt;<span> </span>mtu<span> </span><span>1500</span><span> </span>qdisc<span> </span>noqueue<span> </span>state<span> </span>DOWN<span> </span>group<span> </span>default
<span>    </span>inet<span> </span><span>172</span>.17.0.1/16<span> </span>brd<span> </span><span>172</span>.17.255.255<span> </span>scope<span> </span>global<span> </span>docker0
</code></pre></div>
<p><code>172.17.0.1</code> in this case is the network address we need. It'll probably be the same in your case, but doesn't have to be. Now that we have it, let's move on. How to edit postgres config?</p>
<p>First, my young padawan, we will need to find the location of our postgresql.conf file - which, surprisingly, is the file used to configure postgres.</p>
<p>We can do that with:</p>
<div><pre><span></span><code>sudo<span> </span>find<span> </span>/<span> </span>-type<span> </span>f<span> </span>-name<span> </span>postgresql.conf
/etc/postgresql/16/main/postgresql.conf
</code></pre></div>
<p>In my case it's in <code>/etc/postgresql/16/main/postgresql.conf</code>. The probability of it being the same for you, if you are running ubuntu 22.04 and followed this guide, as the probability of us living in a simulation or being at the beginning of an AI bubble. Get back to the topic Olaf. Gosh.</p>
<p>Okay, we know where the file is, we need to edit it. Type in:</p>
<div><pre><span></span><code>sudo<span> </span>vim<span> </span>/etc/postgresql/16/main/postgresql.conf
</code></pre></div>
<p>and look for <code>listen_addresses</code> part. In vim you can do a search by typing <code>/{phrase}</code> so <code>/listen_addresses</code> should navigate you to the proper line. In my case it looks like this:</p>
<div><pre><span></span><code><span>#</span><span>listen_addresses</span><span> </span><span>=</span><span> </span><span>'</span><span>localhost</span><span>'</span><span>         </span><span>#</span><span> </span><span>what</span><span> </span><span>IP</span><span> </span><span>address</span><span>(</span><span>es</span><span>)</span><span> </span><span>to</span><span> </span><span>listen</span><span> </span><span>on</span><span>;</span>
</code></pre></div>
<p>we need to uncomment the line and edit it so it allows connections from our docker network ip. So:</p>
<div><pre><span></span><code><span>listen_addresses</span><span> </span><span>=</span><span> </span><span>"localhost,172.17.0.1"</span>
</code></pre></div>
<p>then <code>:wq</code> and done.</p>
<p>Now we also need to edit <code>pg_hba.conf</code> to also allow this particular network to acces our database while authenticating with a password.</p>
<p>First let's find it:</p>
<div><pre><span></span><code>sudo<span> </span>find<span> </span>/<span> </span>-type<span> </span>f<span> </span>-name<span> </span>pg_hba.conf
/etc/postgresql/16/main/pg_hba.conf
</code></pre></div>
<p>and edit it with:</p>
<div><pre><span></span><code>sudo<span> </span>vim<span> </span>/etc/postgresql/16/main/pg_hba.conf
</code></pre></div>
<p>now again, navigate to a section containing "IPv4 local connections":</p>
<div><pre><span></span><code><span># IPv4 local connections:</span>
host<span>    </span>all<span>             </span>all<span>             </span><span>127</span>.0.0.1/32<span>            </span>scram-sha-256
<span># IPv6 local connections:</span>
host<span>    </span>all<span>             </span>all<span>             </span>::1/128<span>                 </span>scram-sha-256
</code></pre></div>
<p>we need to edit the IPv4 section to look like this:</p>
<div><pre><span></span><code><span># IPv4 local connections:</span>
host<span>    </span>all<span>             </span>all<span>             </span><span>127</span>.0.0.1/32<span>            </span>scram-sha-256
host<span>    </span>all<span>             </span>all<span>             </span><span>172</span>.17.0.0/16<span>           </span>scram-sha-256
</code></pre></div>
<p>What does the /16 after the netowrk address mean? Match the first 16 bytes of the address, so the 2 first. numbers, rest can change.</p>
<p>Now, let's restart our postgres.</p>
<div><pre><span></span><code>sudo<span> </span>/etc/init.d/postgresql<span> </span>restart
</code></pre></div>
<p>Important note. You might need to add something like this to your docker-compose:</p>
<div><pre><span></span><code><span>    </span><span>extra_hosts</span><span>:</span>
<span>      </span><span>-</span><span> </span><span>"host.docker.internal:172.17.0.1"</span>
</code></pre></div>
<p>For each service that will access it, and edit the connection string for postgres host to be: host.docker.internal. Either that or just use 172.17.0.1 value directly.</p>
<p>With docker and postgres set up, the world is yours to take. But is this it? I wouldn't be myself if i just ended with this. Let's take it up a notch. I mean we usually want to have something in front of our API, some reverse proxy, maybe capability to scale, have multiple replicas and so on. Performance and scaling stuff. Simple solution for that too.</p>
<h2>BUT MUH SCALABILITY, LOAD BALANCING, SSL and whatnot</h2>
<p>Ye I hear you, all the folks with 1k monthly active users, serving up to 2 requests per second, usually screaming about scalability the loudest. WHERE"S KUBERNETES? WHERE"S MY CLOUD SCALING STUFF, REEE!!one! AND THE DEVOPS TEAM? ARE YOU A FOOL?</p>
<p>I'll give you some love too, fret not.</p>
<p>For the database, the case is simple. With a dedicated bare metal server that I recommend you get, unless you do some horrendeous things in the schema or query, well, you can handle TONS of data &amp; traffic on a single machine.With just one instance of $200 ARM Hetzner with 80 dedicated cores, 128 GB of RAM, 2TB NVME PCIe SSD, how much more do you need in most cases? </p>
<p>Yeah, availability zones and so on, but let's take a step back. How many of you are truly running multi region &amp; multi availability zones DB deployments? HMMM? Thought so. Sorry to break it to you, but hype/conference driven development isn't the only way to go. I'd argue that at least 90-95% of current startups could probably run just fine with this single instance only. Okay, maybe some of you would need something like S3 (Cloudflare R2 maybe?). Outgrowing this setup will probably mean you already got enough tracton, customers and money to actually start your own colocation thingy with a dedicated team. Backups? Survavibility? We'll talk about that part later.</p>
<p>So, in this post, we won't cover how to horizontally scale the db, as I think it's simply not needed for the audience i target this too. What is needed though, is probably replication of the apis/scaling them and then reverse proxy/managing ssl/load balancing. That's what we will do. Let's start with replicating our api to an arbitrary size. How can we do that?</p>
<p>Docker-compose lol.</p>
<h2>Ditch Kubernetes, docker compose for the win</h2>
<p>This part will be quite short, sweet and simple. Probably not many of you know, but docker compose supports replication out of the box. Why wouldn't it. How do we go about it?</p>
<div><pre><span></span><code><span>  </span><span>api</span><span>:</span>
<span>    </span><span>build</span><span>:</span>
<span>      </span><span>context</span><span>:</span><span> </span><span>.</span>
<span>    </span><span>depends_on</span><span>:</span>
<span>      </span><span>database</span><span>:</span>
<span>        </span><span>condition</span><span>:</span><span> </span><span>service_healthy</span>
<span>    </span><span>restart</span><span>:</span><span> </span><span>always</span>
<span>    </span><span>deploy</span><span>:</span>
<span>      </span><span>replicas</span><span>:</span><span> </span><span>4</span>
<span>    </span><span>ports</span><span>:</span>
<span>      </span><span>-</span><span> </span><span>"8000-8003:8000"</span>
</code></pre></div>
<p>the key part here being:</p>
<div><pre><span></span><code><span>    </span><span>deploy</span><span>:</span>
<span>      </span><span>replicas</span><span>:</span><span> </span><span>4</span>
<span>    </span><span>ports</span><span>:</span>
<span>      </span><span>-</span><span> </span><span>"8000-8003:8000"</span>
</code></pre></div>
<p>after that, just do <code>docker compose up</code>. And then boom. You done. Multiple replicas of your api service up and running. With 4 lines of code, 2 of them you already probably have in your code.</p>
<p>Remember what we said - we do not want to run docker as root, so ssh/login into the user we created <code>prod</code> for this purpose.</p>
<p>once you there, just clone the repo and docker compose up.</p>
<p>You'll be amazed how fast the deployments can happen. Also about the secrets. 1password offers some nice options here for such use cases, or in fact, you can even just create .env file, specify it in the docker-compose and be done with it.</p>
<p>Logs can be easily checked with a simple <code>docker compose logs</code> + docker saves them to a file iether way.</p>
<p>But, what about load balancing, reverse proxy and ssl stuff? </p>
<h2>Load Balancing &amp; automatic ssl with Caddyserver</h2>
<p>We will use caddyserver to act as a reverse proxy, load balancer and to automatically take care of the certificates for us. It's a bit less performant than nginx, true, but the ease of use and convenience it provides is well worth it. That plus usually it's not the proxy that will die first. Quite the opposite.</p>
<p>So how do we go about this? Probably complicated? Nope.</p>
<p>We will let ansible handle all the work for us. Ansible? Yes, you read that right. Not terraform.</p>
<p>In order to do that we will need to create a new user for our ansible to run on, enable ssh access and do a bit of ansible dev. Let's go. You already know the drill.</p>
<div><pre><span></span><code>sudo<span> </span>useradd<span> </span>ingres
sudo<span> </span>passwd<span> </span>ingres
sudo<span> </span>usermod<span> </span>-aG<span> </span>sudo<span> </span>ingres
</code></pre></div>
<p>Now, a small change to what we did before.</p>
<p>We make sure our user can do sudo operations without password. How?</p>

<p>then find this piece:</p>
<div><pre><span></span><code><span># User privilege specification</span>
root<span>    </span><span>ALL</span><span>=(</span>ALL:ALL<span>)</span><span> </span>ALL
</code></pre></div>
<p>and add this below (or at the bottom of the file):</p>
<div><pre><span></span><code>ingres<span> </span><span>ALL</span><span>=(</span>ALL<span>)</span><span> </span>NOPASSWD:<span> </span>ALL
</code></pre></div>
<p>We could be more granular about permissions here and what it has access to, but that could come in a 2nd iteration, I consider this good enough.</p>
<p>Let's enable SSH access now. This might not be needed on your machine, depends on the server. I had to do it on my hetzner.</p>
<p>We need to edit the sshd_config file. How to find where it is? You should know by now ;) </p>

<p>and find something like this:</p>
<div><pre><span></span><code><span>#AuthorizedKeysFile      .ssh/authorized_keys</span>
</code></pre></div>
<p>turn it into:</p>
<div><pre><span></span><code>AuthorizedKeysFile<span>      </span>.ssh/authorized_keys
AllowUsers<span> </span>root<span> </span>prod<span> </span>dev
</code></pre></div>
<p>add your ssh key to <code>/home/ingres/.ssh/authorized_keys</code> in order to do that and eg. add the same ssh key you use for your root account (not ideal):</p>
<p>```bashand lastly:
su ingres # we switch the user to make it the owner of the directory we create
mkdir -p /home/ingres/.ssh
cat ~/.ssh/authorized_keys &gt; /home/ingres/.ssh/authorized_keys</p>
<div><pre><span></span><code>aaand lastly:

```bash
service sshd restart
</code></pre></div>
<p>Setup done. Time to install caddy with ansible. But before that, we need to setup ansible.</p>
<p>I'll assume you have pyenv installed and set up running on your local machine. You can read about that <a href="https://grski.pl/pyenv-en">here</a> in my article, or <a href="https://grski.pl/pdf-brag">here</a>.</p>
<p>With that we can:</p>
<div><pre><span></span><code>pyenv<span> </span>virtualenv<span> </span><span>3</span>.11<span> </span>infrastructure-deployment-3-11
mkdir<span> </span>infrastructure-deployment
<span>cd</span><span> </span>infrastructure-deployment
pyenv<span> </span><span>local</span><span> </span>infrastructure-deployment-3-11
python<span> </span>-m<span> </span>pip<span> </span>install<span> </span>ansible
</code></pre></div>
<p>Pyenv set up. Ansible set up. We will need one more thing - install custom role from ansible galaxy.</p>
<div><pre><span></span><code>python<span> </span>-m<span> </span>ansible<span> </span>galaxy<span> </span>role<span> </span>install<span> </span>caddy_ansible.caddy_ansible<span>  </span>
</code></pre></div>
<p>Now inside our <code>infrastructure-deployment</code> directory on our local machine create a file called <code>inventory.yml</code></p>
<div><pre><span></span><code><span>all</span><span>:</span>
<span>  </span><span>hosts</span><span>:</span>
<span>    </span><span>bare-metal-hetzner</span><span>:</span>
<span>      </span><span>ansible_host</span><span>:</span><span> </span><span>"your-host-ip"</span>
<span>      </span><span>ansible_user</span><span>:</span><span> </span><span>"ingres"</span>
<span>      </span><span>ansible_port</span><span>:</span><span> </span><span>22</span>
</code></pre></div>
<p>aaand <code>caddy_install.yml</code>:</p>
<div><pre><span></span><code><span>---</span>
<span>-</span><span> </span><span>name</span><span>:</span><span> </span><span>Install Caddy Server</span>
<span>  </span><span>hosts</span><span>:</span><span> </span><span>all</span>
<span>  </span><span>become</span><span>:</span><span> </span><span>true</span>
<span>  </span><span>roles</span><span>:</span>
<span>     </span><span>-</span><span> </span><span>role</span><span>:</span><span> </span><span>caddy_ansible.caddy_ansible</span>
<span>       </span><span>caddy_conf_filename</span><span>:</span><span> </span><span>Caddyfile</span>
<span>       </span><span>caddy_update</span><span>:</span><span> </span><span>true</span>
<span>       </span><span>caddy_systemd_capabilities_enabled</span><span>:</span><span> </span><span>true</span>
<span>       </span><span>caddy_systemd_capabilities</span><span>:</span><span> </span><span>"CAP_NET_BIND_SERVICE"</span>
<span>       </span><span>caddy_config</span><span>:</span><span> </span><span>|</span>
<span>        </span><span>your-fancy-startup-domain.com {                 </span>
<span>          </span><span># Compress responses according to Accept-Encoding headers</span>
<span>          </span><span>encode gzip zstd</span>

<span>          </span><span># Send API requests to backend</span>
<span>          </span><span>reverse_proxy 127.0.0.1:8000 127.0.0.1:8301 127.0.0.1:8302 127.0.0.1:8303</span>
<span>        </span><span>}</span>
</code></pre></div>
<p>run </p>
<div><pre><span></span><code>python<span> </span>-m<span> </span>ansible<span> </span>playbook<span> </span>-i<span> </span>inventory.yml<span> </span>caddy_install.yml<span>   </span>
</code></pre></div>
<p>aaand done.</p>
<p>Now if you go to your-fancy-startup-domain.com, given that proper docker containers are running, you'll get them.</p>
<p>Automatic SSL. Automatic load balancing. EVERYTHING WORKS.</p>
<h2>BACKUPS, SURVAVIBILITY</h2>
<p>You can have hourly backups with BorgBackup. How? Brilliant tutorial can be found in <a href="https://community.hetzner.com/tutorials/install-and-configure-borgbackup">hetzner docs</a>. Go read them.</p>
<p>On top of that add $4 1 TB <a href="https://www.hetzner.com/storage/storage-box">Hetzner Storage Box</a> linked to your server. Boom. Done. You might want to think about adding pg_dump, but IMO just the BorgBackup for starters is ok.</p>
<p>My borg-backup script looks more or less like this:</p>
<div><pre><span></span><code><span>    </span><span>#!/bin/sh</span>
<span># First init the repo</span>
<span># ssh -p23 ssh://xxxxx.your-storagebox.de mkdir /home/backup</span>
<span># ssh -p23 ssh://<a href="https://grski.pl/cdn-cgi/l/email-protection" data-cfemail="4830303030300830303030306631273d3a653b3c273a292f2d2a2730662c2d">[email&nbsp;protected]</a> mkdir /home/backup/main</span>
<span># borg init --encryption=repokey ssh://<a href="https://grski.pl/cdn-cgi/l/email-protection" data-cfemail="a6dededededee6dedededede88dfc9d3d48bd5d2c9d4c7c1c3c4c9de88c2c3">[email&nbsp;protected]</a>:23/~/backup/main</span>
<span># Setting this, so the repo does not need to be given on the commandline:</span>
<span>export</span><span> </span><span>BORG_REPO</span><span>=</span>ssh://<a href="https://grski.pl/cdn-cgi/l/email-protection" data-cfemail="245c645c0a5d4b51560957504b56454341464b5c0a4041">[email&nbsp;protected]</a>:23/~/backup/main

<span># See the section "Passphrase notes" for more infos.</span>
<span>export</span><span> </span><span>BORG_PASSPHRASE</span><span>=</span>

<span># some helpers and error handling:</span>
info<span>()</span><span> </span><span>{</span><span> </span><span>printf</span><span> </span><span>"\n%s %s\n\n"</span><span> </span><span>"</span><span>$(</span><span> </span>date<span> </span><span>)</span><span>"</span><span> </span><span>"</span><span>$*</span><span>"</span><span> </span>&gt;<span>&amp;</span><span>2</span><span>;</span><span> </span><span>}</span>
<span>trap</span><span> </span><span>'echo $( date ) Backup interrupted &gt;&amp;2; exit 2'</span><span> </span>INT<span> </span>TERM

info<span> </span><span>"Starting backup"</span>

<span># Backup the most important directories into an archive named after</span>
<span># the machine this script is currently running on:</span>

borg<span> </span>create<span>                         </span><span>\</span>
<span>    </span>--verbose<span>                       </span><span>\</span>
<span>    </span>--filter<span> </span>AME<span>                    </span><span>\</span>
<span>    </span>--list<span>                          </span><span>\</span>
<span>    </span>--stats<span>                         </span><span>\</span>
<span>    </span>--show-rc<span>                       </span><span>\</span>
<span>    </span>--compression<span> </span>lz4<span>               </span><span>\</span>
<span>    </span>--exclude-caches<span>                </span><span>\</span>
<span>    </span>--exclude<span> </span><span>'home/*/.cache/*'</span><span>     </span><span>\</span>
<span>    </span>--exclude<span> </span><span>'var/tmp/*'</span><span>           </span><span>\</span>
<span>    </span>--exclude<span> </span><span>'*__pycache__*'</span><span>       </span><span>\</span>
<span>    </span>--exclude<span> </span><span>'*.pyenv*'</span><span>            </span><span>\</span>
<span>                                    </span><span>\</span>
<span>    </span>::<span>'{hostname}-{now}'</span><span>            </span><span>\</span>
<span>    </span>/etc<span>                            </span><span>\</span>
<span>    </span>/home<span>                           </span><span>\</span>
<span>    </span>/root<span>                           </span><span>\</span>
<span>    </span>/var

<span>backup_exit</span><span>=</span><span>$?</span>

info<span> </span><span>"Pruning repository"</span>

<span># Use the `prune` subcommand to maintain 7 daily, 4 weekly and 6 monthly</span>
<span># archives of THIS machine. The '{hostname}-*' matching is very important to</span>
<span># limit prune's operation to this machine's archives and not apply to</span>
<span># other machines' archives also:</span>

borg<span> </span>prune<span>                          </span><span>\</span>
<span>    </span>--list<span>                          </span><span>\</span>
<span>    </span>--glob-archives<span> </span><span>'{hostname}-*'</span><span>  </span><span>\</span>
<span>    </span>--show-rc<span>                       </span><span>\</span>
<span>    </span>--keep-daily<span>    </span><span>7</span><span>               </span><span>\</span>
<span>    </span>--keep-weekly<span>   </span><span>4</span><span>               </span><span>\</span>
<span>    </span>--keep-monthly<span>  </span><span>6</span>

<span>prune_exit</span><span>=</span><span>$?</span>

<span># actually free repo disk space by compacting segments</span>

info<span> </span><span>"Compacting repository"</span>

borg<span> </span>compact

<span>compact_exit</span><span>=</span><span>$?</span>

<span># use highest exit code as global exit code</span>
<span>global_exit</span><span>=</span><span>$((</span><span> </span><span>backup_exit</span><span> </span><span>&gt;</span><span> </span><span>prune_exit</span><span> </span>?<span> </span><span>backup_exit</span><span> </span>:<span> </span><span>prune_exit</span><span> </span><span>))</span>
<span>global_exit</span><span>=</span><span>$((</span><span> </span><span>compact_exit</span><span> </span><span>&gt;</span><span> </span><span>global_exit</span><span> </span>?<span> </span><span>compact_exit</span><span> </span>:<span> </span><span>global_exit</span><span> </span><span>))</span>

<span>if</span><span> </span><span>[</span><span> </span><span>${</span><span>global_exit</span><span>}</span><span> </span>-eq<span> </span><span>0</span><span> </span><span>]</span><span>;</span><span> </span><span>then</span>
<span>    </span>info<span> </span><span>"Backup, Prune, and Compact finished successfully"</span>
<span>elif</span><span> </span><span>[</span><span> </span><span>${</span><span>global_exit</span><span>}</span><span> </span>-eq<span> </span><span>1</span><span> </span><span>]</span><span>;</span><span> </span><span>then</span>
<span>    </span>info<span> </span><span>"Backup, Prune, and/or Compact finished with warnings"</span>
<span>else</span>
<span>    </span>info<span> </span><span>"Backup, Prune, and/or Compact finished with errors"</span>
<span>fi</span>

<span>exit</span><span> </span><span>${</span><span>global_exit</span><span>}</span>
</code></pre></div>
<p>To run it periodically type in <code>crontab -e</code></p>
<p>and then</p>
<div><pre><span></span><code><span>00</span><span> </span><span>2</span><span> </span>*<span> </span>*<span> </span>*<span> </span>/root/borg-backup.sh
</code></pre></div>
<h2>Potential improvements</h2>
<p>Ofcourse the permissions here and there could be more fine-grained for sure. </p>
<p>We could also add a bastion in front of the server. </p>
<p>Automate the deployment so that after each merge stuff gets built &amp; deployed. </p>
<p>Add monitoring, observability, alerts. (Ain't that hard tbh, we will explore that one day)</p>
<p>There's much more than that ofcourse but these are the starters.</p>
<h2>Summary</h2>
<p>We have set up: </p>
<ol>
<li>self-hosted postgres instance with passable initail configuration</li>
<li>replicated api-service with as many replicas as we want</li>
<li>proper load balancing and reverse proxy in front of them</li>
<li>https everywhere</li>
<li>proper certifcates, all handled automatically</li>
<li>1 click deployment of our reverse proxy</li>
<li>blazing fast deployments/build times in the future (for now manual, but can easily be automated)</li>
<li>ability to potentially handle hundreds of thousands of users</li>
<li>very predictable cost &amp; performance</li>
<li>regular FULL backups</li>
<li>no additional deployment code</li>
<li>Absolutely stunning performance with 80 dedicated cores, 128 gb of ram, 2 TB NVMe SSD (you'd be amazed)</li>
</ol>
<p>What more can I say. Cloud IS NOT the solution for everything. Sometimes you can try the alternative path.</p>
<p>Similar setup on AWS would be probably $6-10k upwards just for the postgres. That plus it wouldn't match the performance we have here. One thing not covered here is how much performance you gain when all the services are within one network. No calls outside your network == blazing fast shit.</p>
<p>All of this in 15 minutes and for $200 monthly. </p>
<p>Want some copium cloud bro? </p>
<p>Ofcourse this doesn't adhere to some of you and your companies, but you know that. I've simplified lots of things or generlised. However, for the general public and their needs, I think it's worth rethink the whole cloud sometimes.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Doomscroller.xyz (291 pts)]]></title>
            <link>https://doomscroller.xyz/</link>
            <guid>40098178</guid>
            <pubDate>Sat, 20 Apr 2024 15:40:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://doomscroller.xyz/">https://doomscroller.xyz/</a>, See on <a href="https://news.ycombinator.com/item?id=40098178">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <center>
      <p><img src="https://doomscroller.xyz/static/Header.PNG" alt="Header Image">
    </p>
    <!-- <p class="logo-text">Doomscroller</p> -->

    <div>
        <div>
            <p>What does it do???   </p>
            <p>this and <br> only this.  </p>
            <p><img src="https://doomscroller.xyz/static/arrow_blue.png" alt="">
            <img src="https://doomscroller.xyz/static/wheel.png" alt="Wheel"></p>

          <div>
                 
           
      
                     
            <stripe-buy-button buy-button-id="buy_btn_1P5Y1328PU2Wr5nx37n76Me7" publishable-key="pk_live_QHHTMsoPgQZg6Oz7JujHn8pa">
          </stripe-buy-button>


            <stripe-buy-button buy-button-id="buy_btn_1P5Xx628PU2Wr5nxO5bFsP22" publishable-key="pk_live_QHHTMsoPgQZg6Oz7JujHn8pa">
            </stripe-buy-button>
        </div>

      
          <div>
            <p><b>Disclaimers:</b></p><ul>
              <li>Shipping in ~2 weeks</li>
              <li>Only works with Android/PC. Sorry iOS</li>
              <li>Code on github</li>
              <li>Fairly sure the battery won't explode</li>
            </ul>
          </div>
          
      </div>
        <div>
          <p><a data-width="350" data-theme="dark" data-min-width="300" data-max-width="600" href="https://twitter.com/AndrewMcCalip?ref_src=twsrc%5Etfw">Timeline</a>
        </p></div>
    </div>

</center></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U-M finds students with alphabetically lower-ranked names receive lower grades (337 pts)]]></title>
            <link>https://record.umich.edu/articles/study-alphabetical-order-of-surnames-may-affect-grading/</link>
            <guid>40097375</guid>
            <pubDate>Sat, 20 Apr 2024 13:53:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://record.umich.edu/articles/study-alphabetical-order-of-surnames-may-affect-grading/">https://record.umich.edu/articles/study-alphabetical-order-of-surnames-may-affect-grading/</a>, See on <a href="https://news.ycombinator.com/item?id=40097375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    
<p>Knowing your ABCs is essential to academic success, but having a last name starting with A, B or C might also help make the grade.</p>



<p>An analysis by University of Michigan researchers of more than 30 million grading records from U-M finds students with alphabetically lower-ranked names receive lower grades. This is due to sequential grading biases and the default order of students’ submissions in Canvas — the most widely used online learning management system — which is based on alphabetical rank of their surnames.</p>





<p>What’s more, the researchers found, those alphabetically disadvantaged students receive comments that are notably more negative and less polite, and exhibit lower grading quality measured by post-grade complaints from students.</p>



<p>“We spend a lot of time thinking about how to make the grading fair and accurate but even for me it was really surprising,” said Jun Li, associate professor of technology and operations at the Stephen M. Ross School of Business. “It didn’t occur to us until we looked at the data and realized that sequence makes a difference.”</p>



<p>Li co-authored the study with doctoral students Jiaxin Pei from the School of Information and Helen (Zhihan) Wang from Ross. It is under review by the journal Management Science.</p>



<p>The researchers collected available historical data of all programs, students and assignments on Canvas from the fall 2014 semester to the summer 2022 semester. They supplemented the Canvas data with university registrar data, which contains detailed information about students’ backgrounds, demographics and learning trajectories at the university.</p>



<figure><p>
<iframe width="500" height="281" src="https://www.youtube.com/embed/Dxdq1xXFi_M?feature=oembed&amp;rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="U-M Researchers find a correlation between student grades and alphabetical ordering of last names"></iframe>
</p></figure>



<p>Although the data is from U-M, the researchers say the findings can be generalized across institutions and courses. They are driven by a common design issue of learning-management systems — the default setting of ranking students’ assignments alphabetically by their names.</p>



<p>Their research uncovered a clear pattern of a decline in grading quality as graders evaluate more assignments. Wang said students whose surnames start with A, B, C, D or E received a 0.3-point higher grade out of 100 possible points than compared with when they were graded randomly. Likewise, students with later-in-the-alphabet surnames received a 0.3-point lower grade — creating a 0.6-point gap.</p>



<p>Wang noted that for a small group of graders (about 5%) that grade from Z to A, the grade gap flips as expected: A-E students are worse off, while W-Z students receive higher grades relative to what they would receive when graded randomly. The researchers said such observations confirm their hypothesis that it’s the order of grading that leads to the initial gap in grades.</p>



<p>A 0.6-point difference might seem small, but such a disparity did affect students’ course grade-point averages, which negatively influences opportunities in their respective career paths.</p>



<p>“Our conclusion is this may be something that happened unconsciously by the graders that’s actually creating a real social impact,” Wang said.</p>



<p>Pei said the idea for the study came up during a discussion he had with Wang in which they were talking about their research: Wang studies educational technology and he studies artificial intelligence. He observed that a fundamental task of machine learning is data labeling, also a sequential task that can be long and tedious, but one that is randomized.</p>



<p>It got them thinking about educational systems like Canvas and led to some pilot studies to see if there was any disparity among grades based on the amount of time spent in the task of grading.</p>



<p>“We kind of suspect that fatigue is one of the major factors that is driving this effect, because when you’re working on something for a long period of time, you get tired and then you start to lose your attention and your cognitive abilities are dropping,” Pei said.</p>



<p>The researchers note the option exists to grade the assignments in a random order, and some educators do, but alphabetical order is the default mode in Canvas and other online learning-management systems. One simple fix would be to make random order the default setting.</p>



<p>They also suggest academic institutions could hire more graders for larger classes, distribute the workload among more people or train them to be aware of and lessen the bias while grading.</p>



<p>Li, Wang and Pei have been sharing their research at conferences and it’s been positively received — many are impressed by their work although it confirms suspicions many harbor. One reaction in particular stands out to Li, no doubt an information-age wrinkle on “the dog ate my homework” excuse.</p>



<p>“A college student emailed us afterward asking us to share the paper with him,” she said. “He mentioned that his last name started with W. He’s going to tell his parents it’s not because of him — it’s because of his last name.”</p>
                    
                                        <dl>
                        <dt>Tags:</dt>
                        <dd>
                            <ul>
                                                                <li><a href="https://record.umich.edu/tags/canvas/">Canvas</a></li>
                                                                <li><a href="https://record.umich.edu/tags/grading/">grading</a></li>
                                                                <li><a href="https://record.umich.edu/tags/school-of-information/">School of Information</a></li>
                                                                <li><a href="https://record.umich.edu/tags/stephen-m-ross-school-of-business/">Stephen M. Ross School of Business</a></li>
                                                            </ul>
                        </dd>
                    </dl>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Senate passes reauthorization of key US surveillance program after midnight (248 pts)]]></title>
            <link>https://apnews.com/article/fisa-donald-trump-surveillance-congress-johnson-81e991c9f82e77b2fe13f8a3e0e25349</link>
            <guid>40096575</guid>
            <pubDate>Sat, 20 Apr 2024 11:40:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/fisa-donald-trump-surveillance-congress-johnson-81e991c9f82e77b2fe13f8a3e0e25349">https://apnews.com/article/fisa-donald-trump-surveillance-congress-johnson-81e991c9f82e77b2fe13f8a3e0e25349</a>, See on <a href="https://news.ycombinator.com/item?id=40096575">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>WASHINGTON (AP) — Barely missing its midnight deadline, the Senate voted early Saturday to reauthorize a key <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/hub/government-surveillance">U.S. surveillance law</a></span> after divisions over whether the FBI should be restricted from using the program to search for Americans’ data nearly forced the statute to lapse.</p><p>The legislation approved 60-34 with bipartisan support would extend for two years the program known as Section 702 of the <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/fbi-surveillance-section-702-c69db6741ca9f4c26f24ebf5941a5cb0">Foreign Intelligence Surveillance Act</a></span>. It now goes to President Joe Biden’s desk to become law. White House national security adviser Jake Sullivan said Biden “will swiftly sign the bill.”</p><p>“In the nick of time, we are reauthorizing FISA right before it expires at midnight,” Senate Majority Leader Chuck Schumer said when voting on final passage began 15 minutes before the deadline. “All day long, we persisted and we persisted in trying to reach a breakthrough and in the end, we have succeeded.” </p>
    

<p>U.S. officials have said the surveillance tool, first authorized in 2008 and renewed several times since then, is crucial in disrupting terror attacks, cyber intrusions, and foreign espionage and has also produced intelligence that the U.S. has relied on for specific operations, such as the 2022 killing of <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/hub/ayman-al-zawahri">al-Qaida leader Ayman al-Zawahri.</a></span></p>



<p>“If you miss a key piece of intelligence, you may miss some event overseas or put troops in harm’s way,” Sen. Marco Rubio, the top Republican on the Senate Intelligence Committee, said. “You may miss a plot to harm the country here, domestically, or somewhere else. So in this particular case, there’s real-life implications.”</p>
    
<p>The proposal would renew the program, which permits the U.S. government to collect without a warrant the communications of non-Americans located outside the country to gather foreign intelligence. The reauthorization faced a long and bumpy road to final passage Friday after months of clashes between privacy advocates and national security hawks pushed consideration of the legislation to the brink of expiration.</p>
    

<p>Though the spy program was technically set to expire at midnight, the Biden administration had said it expected its authority to collect intelligence to remain operational for at least another year, thanks to an opinion earlier this month from the Foreign Intelligence Surveillance Court, which receives surveillance applications.</p><p>Still, officials had said that court approval shouldn’t be a substitute for congressional authorization, especially since communications companies could cease cooperation with the government if the program is allowed to lapse.</p><p>Hours before the law was set to expire, U.S. officials were already scrambling after two major U.S. communication providers said they would stop complying with orders through the surveillance program, according to a person familiar with the matter, who spoke on the condition of anonymity to discuss private negotiations. </p><p>Attorney General Merrick Garland praised the reauthorization and reiterated how “indispensable” the tool is to the Justice Department. </p><p>“This reauthorization of Section 702 gives the United States the authority to continue to collect foreign intelligence information about non-U.S. persons located outside the United States, while at the same time codifying important reforms the Justice Department has adopted to ensure the protection of Americans’ privacy and civil liberties,” Garland said in a statement Saturday. </p>
    

<p>But despite the Biden administration’s urging and classified briefings to senators this week on the crucial role they say the spy program plays in protecting national security, a group of progressive and conservative lawmakers who were agitating for further changes had refused to accept the version of the bill the House sent over last week.</p><p>The lawmakers had demanded that Majority Leader Chuck Schumer allow votes on amendments to the legislation that would seek to address what they see as civil liberty loopholes in the bill. In the end, Schumer was able to cut a deal that would allow critics to receive floor votes on their amendments in exchange for speeding up the process for passage.</p>
    

<p>The six amendments ultimately failed to garner the necessary support on the floor to be included in the final passage. </p><p>One of the major changes detractors had proposed centered around restricting the FBI’s access to information about Americans through the program. Though the surveillance tool only targets non-Americans in other countries, it also collects communications of Americans when they are in contact with those targeted foreigners. Sen. Dick Durbin, the No. 2 Democrat in the chamber, had been pushing a proposal that would require U.S. officials to get a warrant before accessing American communications. </p><p>“If the government wants to spy on my private communications or the private communications of any American, they should be required to get approval from a judge, just as our Founding Fathers intended in writing the Constitution,” Durbin said. </p>
    

<p>In the past year, U.S. officials have revealed a series of abuses and mistakes by FBI analysts in improperly querying the intelligence repository for information about Americans or others in the U.S., including a <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/fisa-foreign-surveillance-fbi-3f7d4cc0ef413cdf20bc0b70548cde84">member of Congress</a></span> and participants in the <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/justice-department-fbi-surveillance-75c466a64e838ab12eaef96f6335f3cd">racial justice protests of 2020</a></span> and the Jan. 6, 2021, riot at the U.S. Capitol.</p><p>But members on both the House and Senate intelligence committees as well as the Justice Department warned requiring a warrant would severely handicap officials from quickly responding to imminent national security threats. </p><p>“I think that is a risk that we cannot afford to take with the vast array of challenges our nation faces around the world,” Democratic Sen. Mark Warner, chair of the Senate Intelligence Committee, said Friday. </p><h2>__</h2><p>Associated Press writer Eric Tucker contributed to this report. </p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Do not buy a Hisense TV (or at least keep them offline) (167 pts)]]></title>
            <link>https://cohost.org/ghoulnoise/post/5286766-do-not-buy-hisense-t</link>
            <guid>40096253</guid>
            <pubDate>Sat, 20 Apr 2024 10:31:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cohost.org/ghoulnoise/post/5286766-do-not-buy-hisense-t">https://cohost.org/ghoulnoise/post/5286766-do-not-buy-hisense-t</a>, See on <a href="https://news.ycombinator.com/item?id=40096253">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-post-body="true" data-testid="post-body"><p>holy fucking shit!!!!!!!!!!<!-- --></p>
<!-- --><p><em>I've had this in my drafts to finish writing up for a few weeks but today I made another discovery that has pushed me to finally write it up lol.<!-- --></em></p>
<!-- --><p>My PC had a few hiccups over the past couple of years. Nothing so serious that I was truly concerned (at first) but, annoyances, to be sure. ("PC? Weren't you talking about a TV??" you might be thinking to yourself.  Yes. I'll get there. Oh, will I <!-- --><em>ever<!-- --></em> get there.)<!-- --></p>
<!-- --><p>For a long time, the most serious hiccup with my PC was being unable to open Display Settings on my PC. I had to use Nvidia control panel to make adjustments. Whatever. Didn't affect my ability to work or anything. I had just updated to Windows 11 so I thought maybe something went wrong in the update. So I did another install (which meant I had to re-install a ton of music plugin stuff on my computer which takes ages given all the stuff I use.)<!-- --></p>
<!-- --><p>The fresh install still didn't fix it. So I simply ignored it for the better part of 2 years.<!-- --></p>
<!-- --><p>Over time, so slowly I didn't really clock them as being related, other things started to fail.<!-- --></p>
<!-- --><p>I have a Komplete Kontrol S88 midi Keyboard that interfaces with Kontakt and Komplete Kontrol in my DAW. I can use the keyboard to adjust settings or select instruments without having to look at my computer screen. At some point last year this stopped working. I could still use it to input midi, and I was afraid I'd have to do a fresh install of all the Native Instruments stuff I use, so I kept putting off trying to fix it until I had less going on with work. It worked, just not as well as it should!<!-- --></p>
<!-- --><p>Then, Task manager started to hang in weird ways. It wouldn't close unless I forced it closed with ProcExp. Whatever! Computers are weird!<!-- --></p>
<!-- --><p>I had trouble getting video capture cards to connect when I considered streaming Splatoon. Whatever! I'm not a streamer, I should probably use that energy on something else.<!-- --></p>
<!-- --><p>But then, in March, I had some things fail that, turns out, are pretty necessary to using the computer.<!-- --></p>
<!-- --><p>I was trying to get remote desktop to work on my tablet so I could work in the living room closer to my cat, Grendel, who was still very much in mourning for his best friend (&lt;3 Guts). Grendel is at his happiest when he's napping on the couch next to a human. So I thought, hey, I could do some stuff in remote desktop mode! It wasn't something I used very often because it doesn't play super nice with audio, it'd been probably over a year since I last used it, but I knew it <!-- --><em>had<!-- --></em> worked and had been easy to set up.<!-- --></p>
<!-- --><p>Well. It refused to connect despite appearing as an option on my tablet. And as I was trying to troubleshoot this problem...<!-- --></p>
<!-- --><p>My task bars on my PC disappeared! If I moused over the spot they should have been I got the loading circle. And my monitors would jitter!<!-- --></p>
<!-- --><p>System settings were nowhere to be found. To navigate I had to open Task Manager (which if you recall was already acting strange and sluggish for the past year) in order to open the command panel, which I'd then use to launch programs. Or, attempt to. If I tried to open the system settings, my PC spat out "Uhhh what's "Settings"? We ain't got settings" (in command I got "C:\Windows\System32&gt;start ms-settings: Access is denied." <!-- --><em>despite being in admin mode<!-- --></em>)<!-- --></p>
<!-- --><p>SO I started to panic a wee bit. It's not a great time for me to possibly need a new PC! Plus, it works great <!-- --><em>when it works<!-- --></em>.<!-- --></p>
<!-- --><p>I manually backed up everything important from my main PC drive (as I could not access the windows backup program because it lived in settings!!!!!!!!) and downloaded the Windows 11 installer to a drive in case I needed to completely wipe my computer and start fresh.<!-- --></p>
<!-- --><p>In a last ditch effort, I tried updating my Nvidia GeForce graphics driver and restarted.<!-- --></p>
<!-- --><p>Lo and behold, my taskbars came back and I could access settings again! Huzzah!<!-- --></p>
<!-- --><p>This lasted for 6 days. And then the taskbars disappeared AGAIN along with Settings.<!-- --></p>
<!-- --><p>At least by now I'd become somewhat comfortable using command to get around, so I was less panicked than I was in round 1. I at least knew the computer wouldn't crash and delete all my shit suddenly (plus I had my backups that were up-to-date).<!-- --></p>
<!-- --><p>I asked my friends if they'd ever heard of anything like this, and Cohost's own <!-- --><a data-testid="mention" href="https://cohost.org/vogon">@<!-- -->vogon<!-- --></a> helped me poke around some additional graphics driver stuff in case updating to the new Nvidia program that's in beta would solve my issue. This felt super promising when I went into safe mode and saw taskbars. So, I did the update, and, as before, when I updated, the taskbars came back! But there were gone again almost immediately. My screen literally started flickering and they vanished. It was so fucking bizarre.<!-- --></p>
<!-- --><p>I turned to google once more, hoping that somehow, as useless as google is these days, maybe I'd find what I needed to fix my damn computer!<!-- --></p>
<!-- --><p>Somehow, against all odds, I found it. I happened to use the exact right string in my search to pull up some reddit threads. The first one didn't have anything useful, but a few results down I spotted "no taskbar and task manager freezes" holy moly!!!<!-- --></p>
<!-- --><p>At first glance however, there wasn't anything useful. But then I spotted some collapsed comments at the bottom of the thread. I knew more than likely they just had comments like "sucks bro" or "this is why i use linux :/"  But I expanded them and inside was a link to a Microsoft forum post.  I kept my expectations in check and clicked on the link. What I saw had my nearly vibrating in my seat.<!-- --></p>
<!-- --><p><img src="https://staging.cohostcdn.org/attachment/d9f5719e-fe05-4247-9c27-39c3381f0648/screenshot%20collage.png?width=675&amp;auto=webp&amp;dpr=1" alt="Screenshots showing the path from my google search to reddit to a minimized comment that contained a link to a microsoft forum post with the solution to my several year long problem - caused by our TV"></p><!-- --><p><a href="https://learn.microsoft.com/en-us/answers/questions/1339707/help-with-figuring-out-what-is-causing-waitchain-d" target="_blank" rel="nofollow noopener">This was the solution.<!-- --></a></p>
<!-- --><p><em>Narayan B<!-- --></em><br>
<!-- --><em>Nov 16, 2023, 12:20 AM<!-- --></em></p>
<!-- --><p><em>So I finally solved it!<!-- --></em></p>
<!-- --><p><em>Source of the problem: an Android TV (HiSense model) connected to my network. Yes. A TV caused this issue.<!-- --></em></p>
<!-- --><p>HISENSE? LIKE THE SAME BRAND OF TV I HAVE HAD SINCE 2020???<!-- --></p>
<!-- --><p>I followed the instructions. I deleted keys generated by our TV for 5 straight minutes. 5 Minutes of like 200BPM clicking. I restarted. Everything worked again. I laughed so hard I cried. I felt like I'd solved a murder. The main suspect was the PC but the culprit was the TV in the other room.  And he almost got away with it!!! If I had spent a few days carrying out a clean install and re-installing all my work stuff, my problem would have come back. If I had taken the PC out back and shot it and replaced it with a fancy new computer, the problem would have come back.<!-- --></p>
<!-- --><p>Because the problem was never the PC. The Problem was my Hisense TV in the next room.<!-- --></p>
<!-- --><p>Once I carried this out, I was able to open display settings for the first time (outside of safe mode) in 2 years. When I deleted the keys, my Task Manager started behaving normally again. I turned around and saw that my keyboard was once again displaying the VST controls on its screen. The fancy midi keyboard was back at full functionality. I was able to connect my CRT as a display again using the HDMI -&gt; RCA converters I'd assumed had stopped working (nope! they still worked, they always worked!)<!-- --></p>
<!-- --><p>Which brings me to today. Almost a month later everything still works. So, I decided to see if remote desktop would magically work again. The answer is: Yes. Yes, in fact, my TV was the reason my remote desktop connection had failed the month before.<!-- --></p>
<!-- --><p>So here I am, sitting on my couch with my tablet and bluetooth keeb and mouse, writing this post in remote desktop mode so I could attach screenshots of this saga to my post without sitting at my PC. Grendel is snoring beside me as I write.<!-- --></p>
<!-- --><p>As a treat for reading this far, please enjoy two screenshots of my friends reacting to the solution of my great mystery.<!-- --></p>
<!-- --><p><img src="https://staging.cohostcdn.org/attachment/a23624ba-f7bc-4b5b-aa53-68b1e16bfd2e/cursed%20TV%20PC%20reactions.png" alt="friends reacting in discord, the overall sentiment: what the fuck that is so cursed"></p><!-- --><p><img src="https://staging.cohostcdn.org/attachment/b9d4234d-4a60-4bb0-b2a9-92b898e75fba/cursed%20TV%20PC%20reactions2.png" alt="friends reacting in discord, the overall sentiment: what the fuck that is so cursed"></p><!-- --><p>adding: the TV in question, in case you're curious: Hisense 50Q8G- 50" Smart 4K ULED™ Android TV with Quantum Dot Technology (Canada Model)<!-- --></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MuPDF WASM Viewer Demo (256 pts)]]></title>
            <link>https://mupdf.com/wasm/demo/index.html?file=../../docs/mupdf_explored.pdf</link>
            <guid>40096113</guid>
            <pubDate>Sat, 20 Apr 2024 09:52:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mupdf.com/wasm/demo/index.html?file=../../docs/mupdf_explored.pdf">https://mupdf.com/wasm/demo/index.html?file=../../docs/mupdf_explored.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=40096113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="menubar-panel">
		<details>
			<summary>File</summary>
			<menu>
				<li onclick="document.getElementById('open-file-input').click()">Open File...</li>
			</menu>
		</details>
		<details>
			<summary>Edit</summary>
			<menu>
				<li onclick="show_search_panel()">Search...</li>
			</menu>
		</details>
		<details>
			<summary>View</summary>
			<menu>
				<li onclick="toggle_fullscreen()">Fullscreen</li>
				<li onclick="toggle_outline_panel()">Outline</li>
				<li onclick="zoom_to(48)">50%</li>
				<li onclick="zoom_to(72)">75% (72 dpi)</li>
				<li onclick="zoom_to(96)">100% (96 dpi)</li>
				<li onclick="zoom_to(120)">125%</li>
				<li onclick="zoom_to(144)">150%</li>
				<li onclick="zoom_to(192)">200%</li>
			</menu>
		</details>

		<div>
			<p><a href="https://www.npmjs.com/package/mupdf">MuPDF.js on NPM</a>
			<a href="https://mupdfjs.readthedocs.io/">API docs</a>
			<a href="https://github.com/ArtifexSoftware/mupdf.js">Source code on Github</a>
		</p></div>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ceefax Simulator (149 pts)]]></title>
            <link>https://www.nathanmediaservices.co.uk/ceefax/</link>
            <guid>40095657</guid>
            <pubDate>Sat, 20 Apr 2024 08:03:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nathanmediaservices.co.uk/ceefax/">https://www.nathanmediaservices.co.uk/ceefax/</a>, See on <a href="https://news.ycombinator.com/item?id=40095657">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>Interactive Viewer by genius <b><a href="https://zxnet.co.uk/">Alistair Cree</a></b>, a.k.a. <b><a href="https://twitter.com/ZXGuesser">ZXGuesser</a></b></p>
			<br>
			<h2>How To Use</h2>
			<p>Remember teletext? This is exactly the same.<br>
			Each page is assigned a three digit number - you'll see navigation lines that give a page description followed by a number (e.g. "Sport Headlines  302")<br>Use the number keys on the on-screen remote (or your keyboard if you're on a PC) to enter a number. The top row turns green as we wait for the page to load.<br>You can also use the "channel up/down" buttons to move up or down one page at a time.</p>
			<hr>
			<div id="shadowed"><ul><a href="https://www.youtube.com/channel/UCNnD5PSSlPGnT3kAa-ljzWw/live"><li><h2>Pages From Ceefax</h2><img src="https://www.nathanmediaservices.co.uk/assets/images/ceefax-pi.jpeg" alt="TV showing Ceefax" width="1200" height="627"><p>View live Pages from Ceefax as shown on BBC2 overnight</p></li></a><a href="https://www.nathanmediaservices.co.uk/ceefax/inthenews/"><li><h2>In The News</h2><img src="https://www.nathanmediaservices.co.uk/assets/images/ceefax-news.jpeg" alt="Newspaper story on Ceefax re-creation" width="1200" height="627"><p>News stories featuring Ceefax</p></li></a><a href="https://www.nathanmediaservices.co.uk/ceefax/history/"><li><h2>History</h2><img src="https://www.nathanmediaservices.co.uk/assets/images/ceefax-history.jpeg" alt="In vision teletext decoder prototype" width="1200" height="627"><p>How this re-creation came to exist</p></li></a></ul></div>
<div id="shadowed"><a href="https://www.nathanmediaservices.co.uk/"><h2>More from NMS</h2></a><ul><a href="https://www.nathanmediaservices.co.uk/stuffimade/"><li><h2>Stuff I Made</h2><img src="https://www.nathanmediaservices.co.uk/assets/images/stuffimade-subrack-inserter.jpeg" alt="Two aluminium-fronted subrack modules. They each have four LEDs labelled 'power', 'act', 'sync' and 'text', a USB port, and an ethernet port." width="1200" height="627"><p>Electronic devices I've created</p></li></a><a href="https://www.nathanmediaservices.co.uk/projects/"><li><h2>Projects</h2><img src="https://www.nathanmediaservices.co.uk/assets/images/projects-connectivity.jpg" alt="A bunch of cables lie on the ground, ready to be pulled through a duct." width="1200" height="627"><p>Projects I've undertaken</p></li></a><a href="https://www.nathanmediaservices.co.uk/about/contact.php"><li><h2>Contact</h2><img src="https://www.nathanmediaservices.co.uk/assets/navigation/nms-logo.png" alt="NMS" width="1200" height="627"><p>How to get in touch and other account information</p></li></a></ul></div>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dehydrated: Letsencrypt/acme client implemented as a shell-script (116 pts)]]></title>
            <link>https://github.com/dehydrated-io/dehydrated</link>
            <guid>40095325</guid>
            <pubDate>Sat, 20 Apr 2024 06:36:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/dehydrated-io/dehydrated">https://github.com/dehydrated-io/dehydrated</a>, See on <a href="https://news.ycombinator.com/item?id=40095325">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">dehydrated <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=23P9DSJBTY7C8" rel="nofollow"><img src="https://camo.githubusercontent.com/0283ea90498d8ea623c07906a5e07e9e6c2a5eaa6911d52033687c60cfa8d22f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f6e6174652d50617950616c2d677265656e2e737667" alt="Donate" data-canonical-src="https://img.shields.io/badge/Donate-PayPal-green.svg"></a></h2><a id="user-content-dehydrated-" aria-label="Permalink: dehydrated " href="#dehydrated-"></a></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/dehydrated-io/dehydrated/blob/master/docs/logo.png"><img src="https://github.com/dehydrated-io/dehydrated/raw/master/docs/logo.png" alt=""></a></p>
<p dir="auto">Dehydrated is a client for signing certificates with an ACME-server (e.g. Let's Encrypt) implemented as a relatively simple (zsh-compatible) bash-script.
This client supports both ACME v1 and the new ACME v2 including support for wildcard certificates!</p>
<p dir="auto">It uses the <code>openssl</code> utility for everything related to actually handling keys and certificates, so you need to have that installed.</p>
<p dir="auto">Other dependencies are: cURL, sed, grep, awk, mktemp (all found pre-installed on almost any system, cURL being the only exception).</p>
<p dir="auto">Current features:</p>
<ul dir="auto">
<li>Signing of a list of domains (including wildcard domains!)</li>
<li>Signing of a custom CSR (either standalone or completely automated using hooks!)</li>
<li>Renewal if a certificate is about to expire or defined set of domains changed</li>
<li>Certificate revocation</li>
<li>and lots more..</li>
</ul>
<p dir="auto">Please keep in mind that this software, the ACME-protocol and all supported CA servers out there are relatively young and there might be a few issues. Feel free to report any issues you find with this script or contribute by submitting a pull request,
but please check for duplicates first (feel free to comment on those to get things rolling).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto">For getting started I recommend taking a look at <a href="https://github.com/dehydrated-io/dehydrated/blob/master/docs/domains_txt.md">docs/domains_txt.md</a>, <a href="https://github.com/dehydrated-io/dehydrated/blob/master/docs/wellknown.md">docs/wellknown.md</a> and the <a href="#usage">Usage</a> section on this page (you'll probably only need the <code>-c</code> option).</p>
<p dir="auto">Generally you want to set up your WELLKNOWN path first, and then fill in domains.txt.</p>
<p dir="auto"><strong>Please note that you should use the staging URL when experimenting with this script to not hit Let's Encrypt's rate limits.</strong> See <a href="https://github.com/dehydrated-io/dehydrated/blob/master/docs/staging.md">docs/staging.md</a>.</p>
<p dir="auto">If you have any problems take a look at our <a href="https://github.com/dehydrated-io/dehydrated/blob/master/docs/troubleshooting.md">Troubleshooting</a> guide.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Config</h2><a id="user-content-config" aria-label="Permalink: Config" href="#config"></a></p>
<p dir="auto">dehydrated is looking for a config file in a few different places, it will use the first one it can find in this order:</p>
<ul dir="auto">
<li><code>/etc/dehydrated/config</code></li>
<li><code>/usr/local/etc/dehydrated/config</code></li>
<li>The current working directory of your shell</li>
<li>The directory from which dehydrated was run</li>
</ul>
<p dir="auto">Have a look at <a href="https://github.com/dehydrated-io/dehydrated/blob/master/docs/examples/config">docs/examples/config</a> to get started, copy it to e.g. <code>/etc/dehydrated/config</code>
and edit it to fit your needs.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage:</h2><a id="user-content-usage" aria-label="Permalink: Usage:" href="#usage"></a></p>
<div data-snippet-clipboard-copy-content="Usage: ./dehydrated [-h] [command [argument]] [parameter [argument]] [parameter [argument]] ...

Default command: help

Commands:
 --version (-v)                   Print version information
 --display-terms                  Display current terms of service
 --register                       Register account key
 --account                        Update account contact information
 --cron (-c)                      Sign/renew non-existent/changed/expiring certificates.
 --signcsr (-s) path/to/csr.pem   Sign a given CSR, output CRT on stdout (advanced usage)
 --revoke (-r) path/to/cert.pem   Revoke specified certificate
 --deactivate                     Deactivate account
 --cleanup (-gc)                  Move unused certificate files to archive directory
 --cleanup-delete (-gcd)          Deletes (!) unused certificate files
 --help (-h)                      Show help text
 --env (-e)                       Output configuration variables for use in other scripts

Parameters:
 --accept-terms                   Accept CAs terms of service
 --full-chain (-fc)               Print full chain when using --signcsr
 --ipv4 (-4)                      Resolve names to IPv4 addresses only
 --ipv6 (-6)                      Resolve names to IPv6 addresses only
 --domain (-d) domain.tld         Use specified domain name(s) instead of domains.txt entry (one certificate!)
 --ca url/preset                  Use specified CA URL or preset
 --alias certalias                Use specified name for certificate directory (and per-certificate config) instead of the primary domain (only used if --domain is specified)
 --keep-going (-g)                Keep going after encountering an error while creating/renewing multiple certificates in cron mode
 --force (-x)                     Force certificate renewal even if it is not due to expire within RENEW_DAYS
 --force-validation               Force revalidation of domain names (used in combination with --force)
 --no-lock (-n)                   Don't use lockfile (potentially dangerous!)
 --lock-suffix example.com        Suffix lockfile name with a string (useful for with -d)
 --ocsp                           Sets option in CSR indicating OCSP stapling to be mandatory
 --privkey (-p) path/to/key.pem   Use specified private key instead of account key (useful for revocation)
 --domains-txt path/to/domains.txt Use specified domains.txt instead of default/configured one
 --config (-f) path/to/config     Use specified config file
 --hook (-k) path/to/hook.sh      Use specified script for hooks
 --preferred-chain issuer-cn      Use alternative certificate chain identified by issuer CN
 --out (-o) certs/directory       Output certificates into the specified directory
 --alpn alpn-certs/directory      Output alpn verification certificates into the specified directory
 --challenge (-t) http-01|dns-01|tls-alpn-01 Which challenge should be used? Currently http-01, dns-01, and tls-alpn-01 are supported
 --algo (-a) rsa|prime256v1|secp384r1 Which public key algorithm should be used? Supported: rsa, prime256v1 and secp384r1"><pre lang="text"><code>Usage: ./dehydrated [-h] [command [argument]] [parameter [argument]] [parameter [argument]] ...

Default command: help

Commands:
 --version (-v)                   Print version information
 --display-terms                  Display current terms of service
 --register                       Register account key
 --account                        Update account contact information
 --cron (-c)                      Sign/renew non-existent/changed/expiring certificates.
 --signcsr (-s) path/to/csr.pem   Sign a given CSR, output CRT on stdout (advanced usage)
 --revoke (-r) path/to/cert.pem   Revoke specified certificate
 --deactivate                     Deactivate account
 --cleanup (-gc)                  Move unused certificate files to archive directory
 --cleanup-delete (-gcd)          Deletes (!) unused certificate files
 --help (-h)                      Show help text
 --env (-e)                       Output configuration variables for use in other scripts

Parameters:
 --accept-terms                   Accept CAs terms of service
 --full-chain (-fc)               Print full chain when using --signcsr
 --ipv4 (-4)                      Resolve names to IPv4 addresses only
 --ipv6 (-6)                      Resolve names to IPv6 addresses only
 --domain (-d) domain.tld         Use specified domain name(s) instead of domains.txt entry (one certificate!)
 --ca url/preset                  Use specified CA URL or preset
 --alias certalias                Use specified name for certificate directory (and per-certificate config) instead of the primary domain (only used if --domain is specified)
 --keep-going (-g)                Keep going after encountering an error while creating/renewing multiple certificates in cron mode
 --force (-x)                     Force certificate renewal even if it is not due to expire within RENEW_DAYS
 --force-validation               Force revalidation of domain names (used in combination with --force)
 --no-lock (-n)                   Don't use lockfile (potentially dangerous!)
 --lock-suffix example.com        Suffix lockfile name with a string (useful for with -d)
 --ocsp                           Sets option in CSR indicating OCSP stapling to be mandatory
 --privkey (-p) path/to/key.pem   Use specified private key instead of account key (useful for revocation)
 --domains-txt path/to/domains.txt Use specified domains.txt instead of default/configured one
 --config (-f) path/to/config     Use specified config file
 --hook (-k) path/to/hook.sh      Use specified script for hooks
 --preferred-chain issuer-cn      Use alternative certificate chain identified by issuer CN
 --out (-o) certs/directory       Output certificates into the specified directory
 --alpn alpn-certs/directory      Output alpn verification certificates into the specified directory
 --challenge (-t) http-01|dns-01|tls-alpn-01 Which challenge should be used? Currently http-01, dns-01, and tls-alpn-01 are supported
 --algo (-a) rsa|prime256v1|secp384r1 Which public key algorithm should be used? Supported: rsa, prime256v1 and secp384r1
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Chat</h2><a id="user-content-chat" aria-label="Permalink: Chat" href="#chat"></a></p>
<p dir="auto">Dehydrated has an official IRC-channel <code>#dehydrated</code> on libera.chat that can be used for general discussion and suggestions.</p>
<p dir="auto">The channel can also be accessed with Matrix using the official libera.chat bridge at <code>#dehydrated:libera.chat</code>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Children Need Neighborhoods Where They Can Walk and Bike (127 pts)]]></title>
            <link>https://www.wsj.com/health/wellness/children-need-neighborhoods-where-they-can-walk-and-bike-5f3a9b4a</link>
            <guid>40095217</guid>
            <pubDate>Sat, 20 Apr 2024 06:11:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/health/wellness/children-need-neighborhoods-where-they-can-walk-and-bike-5f3a9b4a">https://www.wsj.com/health/wellness/children-need-neighborhoods-where-they-can-walk-and-bike-5f3a9b4a</a>, See on <a href="https://news.ycombinator.com/item?id=40095217">Hacker News</a></p>
Couldn't get https://www.wsj.com/health/wellness/children-need-neighborhoods-where-they-can-walk-and-bike-5f3a9b4a: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Volkswagen workers vote overwhelmingly to join the UAW (137 pts)]]></title>
            <link>https://www.cnn.com/2024/04/19/business/volkswagen-uaw-vote/index.html</link>
            <guid>40095189</guid>
            <pubDate>Sat, 20 Apr 2024 06:03:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2024/04/19/business/volkswagen-uaw-vote/index.html">https://www.cnn.com/2024/04/19/business/volkswagen-uaw-vote/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=40095189">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300}" data-uri="cms.cnn.com/_components/image/instances/clv7czy9600023b6hbzwa7xr0@published" data-name="11718-VolkswagenChattanoogaproducesone-millionthvehicle.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.5916666666666667" data-original-height="994" data-original-width="1680" data-url="https://media.cnn.com/api/v1/images/stellar/prod/11718-volkswagenchattanoogaproducesone-millionthvehicle.jpg?c=original" data-editable="lede" data-freewheel-lede="true">
       <picture><source height="383" width="680" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/11718-volkswagenchattanoogaproducesone-millionthvehicle.jpg?c=16x9&amp;q=h_383,w_680,c_fill/f_webp" type="image/webp"><source height="653" width="1160" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/11718-volkswagenchattanoogaproducesone-millionthvehicle.jpg?c=16x9&amp;q=h_653,w_1160,c_fill/f_webp" type="image/webp"><source height="605" width="1075" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/11718-volkswagenchattanoogaproducesone-millionthvehicle.jpg?c=16x9&amp;q=h_605,w_1075,c_fill/f_webp" type="image/webp"><source height="833" width="1480" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/11718-volkswagenchattanoogaproducesone-millionthvehicle.jpg?c=16x9&amp;q=h_833,w_1480,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/11718-volkswagenchattanoogaproducesone-millionthvehicle.jpg?c=16x9&amp;q=h_833,w_1480,c_fill" alt="Workers at Volkswagen's American factory in Chattanooga, Tennessee voted overwhelmingly to join the United Auto Workers union in an election concluded Friday." onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="994" width="1680"></picture>
    </div><div data-editable="content" itemprop="articleBody" data-reorderable="content">
                    <p><cite>
      <span data-editable="location">New York</span>
      <span data-editable="source">CNN</span>
        &nbsp;—&nbsp;
    </cite>
</p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv721hjx000j45ns0bvq3ogw@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Hourly workers at Volkswagen’s plant in Chattanooga, Tennessee, overwhelming voted to join the United Auto Workers late Friday, a major breakthrough in the union’s effort to organize workers at plants nationwide.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7gsxr300073b6hu9szx63r@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Shortly after 11 pm ET<strong> </strong>on Friday the National Labor Relations Board, the federal body that oversees such votes, announced that 73% of the 3,600 workers at the plant who cast ballots had voted in favor of joining the union. There was an 84% turnout among eligible voters.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hn15l000e3b6h2zu57i98@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “This election is big,” said Kelcey Smith, a worker in the paint department at Volkswagen, in a UAW statement. “This is the time; this is the place. Southern workers are ready to stand up and win a better life.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hmx6v000a3b6hwf1235hu@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            There are roughly 150,000 workers at nonunion auto plants in the United States today, roughly the same number as at the American plants of the three unionized automakers — Gener﻿al Motors, Ford and Stellantis. If the union can win the right to represent workers across the broad swath of the nonunion auto plants, it could increase their leverage in future contract negotiations.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hmy5e000c3b6h10cgwvr6@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The UAW’s victory could also provide a high-profile beachhead for unions in Southern states, which have a much lower level of union representation among workers than in Northern industrial states. Most of the nonunion auto plants are spread across the south.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hvnjn000g3b6heg92e70g@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The union has announced an effort to represent workers at not only Volkswagen, but also <a href="https://www.cnn.com/2023/11/29/business/uaw-organize-nonunion-automakers/index.html">nine other foreign automakers</a> with American plants — BMW, Honda, Hyundai, Mazda, Mercedes, Nissan, Subaru, Toyota and Volvo. It has already filed to have another election at the <a href="https://www.cnn.com/2024/04/05/business/uaw-mercedes-union-vote/index.html">Mercedes plant in Vance, Alabama</a>, just outside of Tuscaloosa. That vote is set to take place next month and be concluded on May 17.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hvz0a000i3b6h1gan135c@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            It is also seeking to represent workers at three American automakers making electric vehicles — Tesla, Rivian and Lucid. But it has yet to file to hold votes at those American EV makers or at the American plants of the eight foreign automakers other than Volkswagen and Mercedes.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hycpo000k3b6hvswn9zq5@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Unlike many employers who conduct campaigns against union membership when faced with an organizing effort, Volkswagen had remained neutral in this campaign. Its statement once the vote was announced was similarly even-handed, stating only the vote results and that “We will await certification of the results by the NLRB. Volkswagen thanks its Chattanooga workers for voting in this election.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7jf3wa00193b6hm56yhxxw@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            That certification is expected to come within five days if Volkswagen does not file objections to the vote, according to the NLRB. The agency said the company is expected to begin bargaining in good faith with the union at that time.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7jhmic001b3b6hjufkmdao@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            One reason the company was more neutral than many employers facing a union vote is the strength of unions in its home country of Germany. The main union for its plants there has a seat on the company’s board.
    </p>

  <h2 data-editable="text" data-uri="cms.cnn.com/_components/subheader/instances/clv7jith8001d3b6heu0mqlcw@published" data-component-name="subheader" id="auto-strikes-and-contracts-helped-win-vote" data-article-gutter="true">
    Auto strikes and contracts helped win vote
</h2>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hygvr000n3b6hib1yfd97@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The union’s organizing efforts follow a <a href="https://www.cnn.com/2023/09/15/business/auto-workers-strike/index.html">six-week strike</a> against the three unionized automakers last fall, which won <a href="https://www.cnn.com/2023/10/30/business/gm-uaw-tentative-agreement/index.html">record pay increases</a> for UAW members at the three companies. They received immediate raises of at least 11% and pay increases totaling more than 30% over the life of the contract, which runs through April 2028.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hygvr000o3b6h1uby897o@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Many of the nonunion automakers, including Volkswagen, gave their workers similar raises in the wake of the UAW contracts. But the workers at the nonunion plants generally earn less than their counterparts at the unionized automakers.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hygvr000p3b6h2c98ljud@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The average worker in Volkswagen’s Chattanooga plant makes about $60,000 a year before bonuses and benefits, according to the company. Production workers working under the recent UAW contract now make about $36 an hour, or about $75,000 a year before overtime, bonuses and benefits.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7jrx1y001m3b6ha6yiu59g@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Volkswagen once before had had an American plant where workers were represented by the UAW, in Pennsylvania. But that plant closed in 1988 in the face of weak American sales by Volkswagen.&nbsp;And the UAW has had little success winning the right to represent nonunion auto workers since then, until Friday’s vote.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7i52kc000s3b6hx7dksole@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            But the union’s effort had been opposed by a coalition of six southern Republican governors who have nonunion auto plants in their states.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7i5hf2000u3b6h33w47xhh@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The six, including Tennessee Gov. Bill Lee as well as the governors of Alabama, Georgia, Mississippi, South Carolina and Texas, signed a letter this week arguing those nonunion jobs would be at risk if the union won Friday’s vote.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7i5hf2000v3b6hmrj1tvvo@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “The reality is companies have choices when it comes to where to invest and bring jobs and opportunity. We have worked tirelessly on behalf of our constituents to bring good-paying jobs to our states,” said the letter. “Unionization would certainly put our states’ jobs in jeopardy.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7jlcye001f3b6huw8ugqf4@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            But the vote represents a run of success by the nation’s unions, which won <a href="https://www.cnn.com/2023/11/21/business/big-paydays-union-members/index.html">pay increases of 10% or more</a> for nearly a million union members last year, according to an analysis by CNN. <a href="https://www.cnn.com/2024/02/21/business/2023-strike-summary">Strikes</a> were at a decade high in 2023, and organizing activity also increased.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7jpqas001j3b6hxf6wfjmg@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Friday’s vote results were praised by the AFL-CIO, the main federation of unions in the country.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7jncps001h3b6hqgd0xugk@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “This victory sends a powerful message to corporate interests everywhere: Workers will no longer tolerate exploitation and mistreatment,” said the AFL-CIO’s statement. “Whether it’s autoworkers in Tennessee, film crews in Hollywood, hotel workers in Las Vegas or baristas at the local coffee shop, when working people stand together in solidarity, we have the power to enact meaningful change and usher in a brighter future for all.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clv7hf8wi00033b6hn1t69rop@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            <em>This story has been updated with additional context and developments.</em>
    </p>


                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[On Terry A. Davis (109 pts)]]></title>
            <link>https://schizophrenic.io/blog/on-terry-a-davis</link>
            <guid>40095070</guid>
            <pubDate>Sat, 20 Apr 2024 05:34:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://schizophrenic.io/blog/on-terry-a-davis">https://schizophrenic.io/blog/on-terry-a-davis</a>, See on <a href="https://news.ycombinator.com/item?id=40095070">Hacker News</a></p>
<div id="readability-page-1" class="page"><div lang="en"><header></header><p>As someone formally diagnosed with schizophrenia who happens to be a
programmer, I often get questions such as, “What do you think of Terry A. Davis
and his OS, TempleOS?“ While people who ask me this question often mean well,
it is often a catalyst for some complicated thoughts on mental illness and
product development.</p><p>I want to be delicate while also being realistic about Terry. He was, indeed, a
brilliant programmer. Where I disagree with him is in his insistence that there
was a grand conspiracy to hold back his operating system, perhaps orchestrated
by the CIA. Anyone who used his operating system could tell that it was, at the
very least, unpolished. While it did have some recreational value, it did not
provide a product that people could use that would enrich their lives.</p><p>While I certainly can relate to Terry’s sense of fleeting success, I do not
ascribe any agency or conspiracy as the cause of my failures, nor my lack of
successes. Any failures I have made in my life are strictly my own and, if
nobody wants to read my posts or hire me to do a job, that is on me. Of course,
I also disagree with Terry’s use of racial and ethnic slurs. Let’s be clear:
schizophrenia does not cause racism. That failure was his own, as well. [Edit:
Several people from the HN thread mentioned that his usage of these terms might
have stemmed from his persecutory delusions. Well, maybe so. Honestly, that’s
still not good, but I can at least see where that perspective comes from. Sorry
if I appeared judgmental here. :) ]</p><p>So what is the take away from all this? First, that Terry was a complicated
figure, often brilliant, but certainly flawed. Second, that it is not my
intention to present my lack of (current) success as anything other than the
failures (so far) to produce a useful product.</p><p>With that said, I wish to send regards to Terry’s family and say a prayer that
he is now in a better place. May he find many successes in (if it exists) the
afterlife. :)</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wikipedia Has Cancer (2017) (104 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/User:Guy_Macon/Wikipedia_has_Cancer</link>
            <guid>40093801</guid>
            <pubDate>Sat, 20 Apr 2024 01:33:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/User:Guy_Macon/Wikipedia_has_Cancer">https://en.wikipedia.org/wiki/User:Guy_Macon/Wikipedia_has_Cancer</a>, See on <a href="https://news.ycombinator.com/item?id=40093801">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div lang="en" dir="ltr" id="mw-content-text">
<p>Alternative title: <i>Just because you have some money, that doesn't mean that you have to spend it</i>.
</p>

<table role="presentation"><tbody><tr><td><span typeof="mw:File"><span><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Walnut.png/30px-Walnut.png" decoding="async" width="30" height="30" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Walnut.png/45px-Walnut.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Walnut.png/60px-Walnut.png 2x" data-file-width="192" data-file-height="192"></span></span></td><td><b>This page in a nutshell:</b> The Wikimedia Foundation's ever-increasing spending may not be sustainable in the long run.</td></tr></tbody></table>

<p>In biology, the <a href="https://en.wikipedia.org/wiki/The_Hallmarks_of_Cancer" title="The Hallmarks of Cancer">hallmarks of an aggressive cancer</a> include <a href="https://en.wikipedia.org/wiki/Hayflick_limit" title="Hayflick limit">limitless multiplication</a> of ordinarily beneficial cells, even when the body signals that further multiplication is no longer needed. The Wikipedia page on the <a href="https://en.wikipedia.org/wiki/Wheat_and_chessboard_problem" title="Wheat and chessboard problem">wheat and chessboard problem</a> explains that nothing can keep growing forever. In biology, the unwanted growth usually terminates with the death of the host. Ever-increasing spending can often lead to the same undesirable result in organizations.
</p><p>Consider the following example of runaway spending growth:
</p>

<table>

<tbody><tr>
<th>Year
</th>
<th>Support and revenue
</th>
<th>Expenses
</th>
<th>Net assets at year end
</th></tr>
<tr>
<td><b>2003–2004</b><sup id="cite_ref-2004-2006_1-0"><a href="#cite_note-2004-2006-1">[1]</a></sup>
</td>
<td><span>$80,129</span>
</td>
<td><span>$23,463</span>
</td>
<td><span>$56,666</span>
</td></tr>
<tr>
<td><b>2004–2005</b><sup id="cite_ref-2004-2006_1-1"><a href="#cite_note-2004-2006-1">[1]</a></sup>
</td>
<td><span>$379,088</span>
</td>
<td><span>$177,670</span>
</td>
<td><span>$268,084</span>
</td></tr>
<tr>
<td><b>2005–2006</b><sup id="cite_ref-2004-2006_1-2"><a href="#cite_note-2004-2006-1">[1]</a></sup>
</td>
<td><span>$1,508,039</span>
</td>
<td><span>$791,907</span>
</td>
<td><span>$1,004,216</span>
</td></tr>
<tr>
<td><b>2006–2007</b><sup id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup>
</td>
<td><span>$2,734,909</span>
</td>
<td><span>$2,077,843</span>
</td>
<td><span>$1,658,282</span>
</td></tr>
<tr>
<td><b>2007–2008</b><sup id="cite_ref-3"><a href="#cite_note-3">[3]</a></sup>
</td>
<td><span>$5,032,981</span>
</td>
<td><span>$3,540,724</span>
</td>
<td><span>$5,178,168</span>
</td></tr>
<tr>
<td><b>2008–2009</b><sup id="cite_ref-4"><a href="#cite_note-4">[4]</a></sup>
</td>
<td><span>$8,658,006</span>
</td>
<td><span>$5,617,236</span>
</td>
<td><span>$8,231,767</span>
</td></tr>
<tr>
<td><b>2009–2010</b><sup id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup>
</td>
<td><span>$17,979,312</span>
</td>
<td><span>$10,266,793</span>
</td>
<td><span>$14,542,731</span>
</td></tr>
<tr>
<td><b>2010–2011</b><sup id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup>
</td>
<td><span>$24,785,092</span>
</td>
<td><span>$17,889,794</span>
</td>
<td><span>$24,192,144</span>
</td></tr>
<tr>
<td><b>2011–2012</b><sup id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup>
</td>
<td><span>$38,479,665</span>
</td>
<td><span>$29,260,652</span>
</td>
<td><span>$34,929,058</span>
</td></tr>
<tr>
<td><b>2012–2013</b><sup id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup>
</td>
<td><span>$48,635,408</span>
</td>
<td><span>$35,704,796</span>
</td>
<td><span>$45,189,124</span>
</td></tr>
<tr>
<td><b>2013–2014</b><sup id="cite_ref-2013-14_9-0"><a href="#cite_note-2013-14-9">[9]</a></sup>
</td>
<td><span>$52,465,287</span>
</td>
<td><span>$45,900,745</span>
</td>
<td><span>$53,475,021</span>
</td></tr>
<tr>
<td><b>2014–2015</b><sup id="cite_ref-2014-15_10-0"><a href="#cite_note-2014-15-10">[10]</a></sup>
</td>
<td><span>$75,797,223</span>
</td>
<td><span>$52,596,782</span>
</td>
<td><span>$77,820,298</span>
</td></tr>
<tr>
<td><b>2015–2016</b><sup id="cite_ref-2015-16_11-0"><a href="#cite_note-2015-16-11">[11]</a></sup>
</td>
<td><span>$81,862,724</span>
</td>
<td><span>$65,947,465</span>
</td>
<td><span>$91,782,795</span>
</td></tr>
<tr>
<td><b>2016–2017</b><sup id="cite_ref-2016-17_12-0"><a href="#cite_note-2016-17-12">[12]</a></sup>
</td>
<td><span>$91,242,418</span>
</td>
<td><span>$69,136,758</span>
</td>
<td><span>$113,330,197</span>
</td></tr>
<tr>
<td><b>2017–2018</b><sup id="cite_ref-2017-18_13-0"><a href="#cite_note-2017-18-13">[13]</a></sup>
</td>
<td><span>$104,505,783</span>
</td>
<td><span>$81,442,265</span>
</td>
<td><span>$134,949,570</span>
</td></tr>
<tr>
<td><b>2018–2019</b><sup id="cite_ref-2018-19_14-0"><a href="#cite_note-2018-19-14">[14]</a></sup>
</td>
<td><span>$120,067,266</span>
</td>
<td><span>$91,414,010</span>
</td>
<td><span>$165,641,425</span>
</td></tr>
<tr>
<td><b>2019–2020</b><sup id="cite_ref-2019-20_15-0"><a href="#cite_note-2019-20-15">[15]</a></sup>
</td>
<td><span>$129,234,327</span>
</td>
<td><span>$112,489,397</span>
</td>
<td><span>$180,315,725</span>
</td></tr>
<tr>
<td><b>2020–2021</b><sup id="cite_ref-2020-21_16-0"><a href="#cite_note-2020-21-16">[16]</a></sup>
</td>
<td><span>$162,886,686</span>
</td>
<td><span>$111,839,819</span>
</td>
<td><span>$231,177,536</span>
</td></tr>
<tr>
<td><b>2021–2022</b><sup id="cite_ref-2021-22_17-0"><a href="#cite_note-2021-22-17">[17]</a></sup>
</td>
<td><span>$154,686,521</span>
</td>
<td><span>$145,970,915</span>
</td>
<td><span>$239,351,532</span>
</td></tr>
<tr>
<td><b>2022–2023</b><sup id="cite_ref-2022-23_18-0"><a href="#cite_note-2022-23-18">[18]</a></sup>
</td>
<td><span>$180,174,103</span>
</td>
<td><span>$169,095,381</span>
</td>
<td><span>$254,971,336</span>
</td></tr></tbody></table>




<p>In 2005, Wikipedia co-founder and Wikimedia Foundation founder Jimmy Wales <a rel="nofollow" href="https://www.youtube.com/watch?v=WQR0gx0QBZ4#t=275">told a TED audience:</a>
</p>
<blockquote><p><i>"So, we're doing around 1.4 billion page views monthly. So, it's really gotten to be a huge thing. And everything is managed by the volunteers and the total monthly cost for our bandwidth is about <span>US$</span>5,000, and that's essentially our main cost. We could actually do without the employee […] We actually hired Brion because he was working part-time for two years and full-time at Wikipedia so we actually hired him so he could get a life and go to the movies sometimes."</i></p></blockquote>
<p>According to the WMF, Wikipedia (in all language editions) now receives 16 billion page views per month.<sup id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup> The WMF spends roughly $2 million USD per year on Internet hosting<sup id="cite_ref-2015-16_11-1"><a href="#cite_note-2015-16-11">[11]</a></sup> and employs some 300 staff.<sup id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup> 
</p><p>The modern Wikipedia has 11-12 times as many page views than it had in 2005,<sup id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup> but the WMF is spending 33 times as much to serve up these pages to the readers.<sup id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup> This seems reasonable given that they have improved reliability, redundancy and backups. More concerning is the fact that since 2005 the WMF has hired hundreds of extra employees and is now spending <b>1,250 times</b> as much overall,<sup id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup> which seems rather excessive considering that the actual amount of work they have to do is pretty much the same. WMF's spending has gone up by 85% over the past three years.<sup id="cite_ref-24"><a href="#cite_note-24">[24]</a></sup>
</p><p>Sounds a lot like cancer, doesn't it? For those readers who were around three years ago, did you notice at the time any unmet needs that would have caused you to conclude that the WMF needed to increase spending by $30 million dollars? I certainly didn't.
</p><p>From 2005 to 2015, annual inflation in the US was between 1% and 3% per year, and cumulative inflation for the entire decade was 21.4%—far less than the increase in WMF spending. We are even <a href="https://en.wikipedia.org/wiki/Metastasis" title="Metastasis">metastasizing</a> the cancer by bankrolling local chapters, rewarding them for finding new ways to spend money.<sup id="cite_ref-25"><a href="#cite_note-25">[25]</a></sup><sup id="cite_ref-26"><a href="#cite_note-26">[26]</a></sup>
</p><p>Nothing can grow forever. Sooner or later, something is going to happen that causes the donations to decline instead of increase. It could be a scandal (real or perceived). It could be the WMF taking a political position that offends many donors. Or it could be a recession, leaving people with less money to give. It might even be a lawsuit that forces the WMF to pay out a judgement that is larger than the reserve. Whatever the reason is, it <em>will</em> happen. It would be naïve to think that the WMF, which up to this point has never seriously considered any sort of spending limits, will suddenly discover fiscal prudence when the revenues start to decline. It is far more likely that the WMF will not react to a drop in donations by decreasing spending, but instead will ramp up fund-raising efforts while burning through our reserves and our endowment.
</p><p>Although this essay focuses on spending, not fundraising, it could be argued that the ever-increasing spending is a direct cause of the kind of fund-raising that has generated a storm of criticism.<sup id="cite_ref-27"><a href="#cite_note-27">[27]</a></sup><sup id="cite_ref-28"><a href="#cite_note-28">[28]</a></sup><sup id="cite_ref-29"><a href="#cite_note-29">[29]</a></sup><sup id="cite_ref-30"><a href="#cite_note-30">[30]</a></sup><sup id="cite_ref-31"><a href="#cite_note-31">[31]</a></sup><sup id="cite_ref-32"><a href="#cite_note-32">[32]</a></sup><sup id="cite_ref-33"><a href="#cite_note-33">[33]</a></sup><sup id="cite_ref-34"><a href="#cite_note-34">[34]</a></sup><sup id="cite_ref-35"><a href="#cite_note-35">[35]</a></sup> These complaints have been around for years,<sup id="cite_ref-36"><a href="#cite_note-36">[36]</a></sup> leading one member of a major Wikimedia mailing list to automate his yearly complaint about the dishonesty he sees every year in our fundraising banners.<sup id="cite_ref-37"><a href="#cite_note-37">[37]</a></sup>
</p><p>No organization can sustain this sort of spending on a long-term basis. We should have leveled off our spending years ago. Like cancer, WMF spending is growing at an ever-increasing rate. Like cancer, this will kill the patient unless the growth is stopped. Some charities can safely grow without limits. If you are feeding 1,000 starving orphans per week, a ten times increase in revenue means that you will be able to feed 10,000 orphans per week. Wikipedia isn't like that. It costs a certain amount to have reliable servers, run a good legal team, maintain the core software, etc. But none of the things that the WMF needs to do require ever-expanding spending.
</p><p>The reason I have so little faith in the WMF's ability to adapt to declining revenues (note that I specified the WMF; I think <em>Wikipedia</em> has shown an excellent ability to adapt to multiple problems) is the horrific track record they have regarding adapting to other kinds of problems.
</p><p>In particular, their poor handling of software development has been well known for many years. The answer to the WMF's problems with software development is extensively documented in books such as <i><a href="https://en.wikipedia.org/wiki/The_Mythical_Man-Month" title="The Mythical Man-Month">The Mythical Man-Month</a></i> and <i><a href="https://en.wikipedia.org/wiki/Peopleware:_Productive_Projects_and_Teams" title="Peopleware: Productive Projects and Teams">Peopleware: Productive Projects and Teams</a></i>, yet I have never seen any evidence that the WMF has been following standard software engineering principles that were well-known when <i>Mythical Man-Month</i> was first published in 1975. If they had, we would be seeing things like requirements documents and schedules with measurable milestones. This failure is almost certainly a systemic problem directly caused by top management, not by the developers doing the actual work.
</p><p>This is not to imply that decades-old software development methods are somehow superior to modern ones, but rather that the WMF is violating basic principles that are common to both. Nothing about Agile or SCRUM means that the developers do not have to talk to end users, create requirements, or meet milestones. In fact, modern software development methods require <em>more</em> communication and interaction with the final end users. Take as an example the way Visual Editor was developed. There are many pages of documentation on the WMF servers and mailing lists, but no evidence that any developer had any serious discussions with the actual editors of Wikipedia who would be using the software. Instead, the role of "customer" was played by paid WMF staffers who thought that they knew what Wikipedia editors need better than the editors themselves do. Then they threw the result over the wall, and the community of Wikipedia editors largely rejected it. Or Knowledge Engine, which was developed in secret before being cancelled when word got out about what the WMF was planning. Another example: The MediaWiki edit toolbar ended up being used by a whopping 0.03% of active editors.<sup id="cite_ref-38"><a href="#cite_note-38">[38]</a></sup>
</p><p>After we burn through our reserves, it seems likely that the next step for the WMF will be going into debt to support continued runaway spending, followed by bankruptcy. At that point there are several large corporations (<a href="https://en.wikipedia.org/wiki/Privacy_concerns_regarding_Google" title="Privacy concerns regarding Google">Google</a> and <a href="https://en.wikipedia.org/wiki/Privacy_concerns_with_Facebook" title="Privacy concerns with Facebook">Facebook</a> come to mind) that will be more than happy to pay off the debts, take over the encyclopedia, fire the WMF staff, and start running Wikipedia as a profit-making platform. There are a lot of ways to monetize Wikipedia, all undesirable. The new owners could sell banner advertising, allow uneditable "sponsored articles" for those willing to pay for the privilege, or even sell information about editors and users.
</p><p>If we want to avoid disaster, we need to start shrinking the cancer now, before it is too late. We should make spending transparent, publish a detailed account of what the money is being spent on and answer any reasonable questions asking for more details. We should limit spending increases to no more than inflation plus some percentage (adjusted for any increases in page views), build up our endowment, and structure the endowment so that the WMF cannot legally dip into the principal when times get bad.
</p><p>If we do these things now, in a few short years we could be in a position to do everything we are doing now, while living off of the endowment interest, and would have no need for further fundraising. Or we could keep fundraising, using the donations to do many new and useful things, knowing that whatever we do there is a guaranteed income stream from the endowment that will keep the servers running indefinitely.
</p>
<h2><span id="2016.E2.80.932017_update"></span><span id="2016–2017_update">2016–2017 update</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=User:Guy_Macon/Wikipedia_has_Cancer&amp;action=edit&amp;section=1" title="Edit section: 2016–2017 update"><span>edit</span></a><span>]</span></span></h2>
<p>The above chart and graph have been updated to reflect the 2016–2017 results.
</p><p>Observations as of July of 2018:
</p>
<ul><li>It is difficult to derive a trend from one year's data, but it appears that the rate of spending is beginning to level off. How much influence this page (and the previous posting of the same argument on various pages) had on this is an interesting question.</li>
<li>We still have a marked lack of transparency on spending. For example, <a href="https://upload.wikimedia.org/wikipedia/foundation/d/da/Wikimedia_Foundation_Audit_Report_-_FY16-17.pdf">[2]</a> has numbers for "Grants and awards" and "Professional service expenses" but there is no obvious way of finding out the details of those expenditures (please note that this information may very well be in one of the many, many documents the WMF publishes each year).</li>
<li>All efforts to persuade the WMF to enact any spending cap, even "limit spending to no more than double last years spending" have failed.</li>
<li>We appear to be building up our endowment, but it is unclear whether the WMF has structured the endowment so that the WMF cannot legally dip into the principal when times get bad. Without this we have no protection from a sudden drop in revenue while the WMF maintains the current spending levels in the hope that revenue will recover. It is also unclear whether the endowment is legally protected against a large payout as a result of a lawsuit. The current management of the WMF appears to be committed to making immediate and drastic cuts to spending if revenue suddenly drops. Hopefully we will never have to find out.</li></ul>
<h2><span id="2017.E2.80.932018_update"></span><span id="2017–2018_update">2017–2018 update</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=User:Guy_Macon/Wikipedia_has_Cancer&amp;action=edit&amp;section=2" title="Edit section: 2017–2018 update"><span>edit</span></a><span>]</span></span></h2>
<p>The above chart and graph have been updated to reflect the 2017–2018 results.
</p><p>Observations as of August of 2019:
</p>
<ul><li>It turns out that the rate of spending is <b>not</b> beginning to level off. Not even close.</li>
<li>The good news is that the donations have also continued to grow, but anyone who thinks that this will continue forever simply does not understand basic economics. I really ought to calculate how soon the present rates of increase will bring us to the point where donations to the WMF are larger than the amount of money in the world. Anyone willing to run those numbers is invited to discuss them on <a href="https://en.wikipedia.org/wiki/User_talk:Guy_Macon/Wikipedia_has_Cancer" title="User talk:Guy Macon/Wikipedia has Cancer">the talk page for this essay</a>.</li>
<li>As for the rest, absolutely nothing has changed. There has been zero actual effort by the WMF to increase transparency on spending. Spending caps are not even something that is being discussed. And it really does look like the WMF has structured the endowment so that they can raid it instead of reducing spending when times get bad -- so in essence it isn't substantively different from any other WMF bank account other than a bit of extra paperwork needed to start draining it. Or maybe not; in another example of the WMF refusing to be be financially transparent, despite repeated requests the WMF has never revealed the actual language of any binding rules (if there are any) on when they can and cannot grab the endowment and use it to keep spending should the rate of donations fall off a cliff.</li></ul>
<p>From 2007–2008 to 2017–2018 donations went from $5,032,981.00 USD to $104,505,783.00 USD -- 20.76 times higher.
</p><p>From 2007–2008 to 2017–2018 spending went from $3,540,724.00 USD to $81,442,265.00 USD -- 23 times higher.
</p><p>In 2008 Wikipedia had over 5 million registered editors, 250 language editions, and 7.5 million articles. Wikipedia.org was the 10th-busiest website in the world. We had already started Wiktionary, Wikibooks, Wikinews, Wikiquote, Wikiversity and Wikispecies, we had already opened chapters in multiple countries, and we had already moved from Florida to San Fransisco.
</p><p>I was here in 2008. I did not notice any pressing needs that were not funded because we were spending 4.3% of what we are spending now. What, exactly, are we doing now that we were not doing ten years ago that justifies us spending twenty-three times as much money?
</p>
<h2><span id="2018.E2.80.932019_update"></span><span id="2018–2019_update">2018–2019 update</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=User:Guy_Macon/Wikipedia_has_Cancer&amp;action=edit&amp;section=3" title="Edit section: 2018–2019 update"><span>edit</span></a><span>]</span></span></h2>
<p>The above chart and graph have been updated to reflect the 2018–2019 results.
</p><p>Observations as of March of 2020:
</p><p>Same old story. Ever increasing spending, based upon the assumption of ever-increasing donations. Not looking forward to the day when it all collapses and everybody in the mass media starts pointing to this page and saying "you were warned years ago".
</p><p>WMF fundraising keeps getting more and more aggressive, because they are trying to lengthen an unfinished railway tunnel while a runaway train is heading down the tracks at full speed.
</p><p>Nobody at the WMF is even willing to discuss making the endowment a true endowment instead of a piggy bank that the WMF is free to break open and loot when the donations stop increasing.
</p><p>Nothing but the usual empty promises regarding spending transparency.
</p><p>Fun challenge #1: pick any <a href="https://en.wikipedia.org/wiki/Wikimania" title="Wikimania">Wikimania</a> from any time in the past. Ask for an accounting of the total spent on it. Ask as many places as possible. See if you get an answer.
</p><p>Fun challenge #2: The 2020 Wikimania was cancelled because of the <a href="https://en.wikipedia.org/wiki/COVID-19_pandemic" title="COVID-19 pandemic">COVID-19 pandemic</a>. Try asking for specifics documenting any bad thing that happened because we didn't spend money on it this year.
</p><p>I have personally abandoned all efforts to try to get a response on any of this, but I invite anyone who thinks that they can get answers if only they somehow ask the right questions on the right page to do so and report the results on <a href="https://en.wikipedia.org/wiki/User_talk:Guy_Macon/Wikipedia_has_Cancer" title="User talk:Guy Macon/Wikipedia has Cancer">the talk page</a>.
</p>
<h2><span id="See_also">See also</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=User:Guy_Macon/Wikipedia_has_Cancer&amp;action=edit&amp;section=4" title="Edit section: See also"><span>edit</span></a><span>]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2017-02-27/Op-ed" title="Wikipedia:Wikipedia Signpost/2017-02-27/Op-ed">Wikipedia Signpost February 2017 Opinion/Editorial</a> (a now-obsolete version of this page as edited by the <i>Signpost</i> editors. I gave them permission to make the changes and approved the result despite disliking the changes, most of which involved removing citations. The page you are reading is the original version, and is the version that I keep updating and improving.)</li></ul>
<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=User:Guy_Macon/Wikipedia_has_Cancer&amp;action=edit&amp;section=5" title="Edit section: References"><span>edit</span></a><span>]</span></span></h2>
<div><ol>
<li id="cite_note-2004-2006-1"><span>^ <a href="#cite_ref-2004-2006_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-2004-2006_1-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-2004-2006_1-2"><sup><i><b>c</b></i></sup></a></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/2/28/Wikimedia_2006_fs.pdf">Financial Statements, June 30, 2006, 2005, and 2004</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-2"><span><b><a href="#cite_ref-2">^</a></b></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/4/49/Wikimedia_2007_fs.pdf">Financial Statements, June 30, 2007 And 2006</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-3"><span><b><a href="#cite_ref-3">^</a></b></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/4/4c/Wikimedia_20072008_fs.pdf">Financial Statements, June 30, 2008</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-4"><span><b><a href="#cite_ref-4">^</a></b></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/4/4f/FINAL_08_09From_KPMG.pdf">Financial Statements, June 30, 2009 and 2008</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/c/cc/FINAL_09_10From_KPMG.pdf">Financial Statements, June 30, 2010 and 2009</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-6"><span><b><a href="#cite_ref-6">^</a></b></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/a/ac/FINAL_10_11From_KPMG.pdf">Financial Statements, June 30, 2011 and 2010</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-7"><span><b><a href="#cite_ref-7">^</a></b></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/0/09/FINAL_11_12From_KPMG.pdf">Financial Statements, June 30, 2012 and 2011</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-8"><span><b><a href="#cite_ref-8">^</a></b></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/6/6e/FINAL_12_13From_KPMG.pdf">Financial Statements, June 30, 2013 and 2012</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-2013-14-9"><span><b><a href="#cite_ref-2013-14_9-0">^</a></b></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/b/bf/Audit_Report_-_FY_13-14_-_Final_v2.pdf">Financial Statements, June 30, 2014 and 2013</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-2014-15-10"><span><b><a href="#cite_ref-2014-15_10-0">^</a></b></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/0/0b/Audit_Report_-_FY_14-15_-_Final.PDF">Financial Statements, June 30, 2015 and 2014</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-2015-16-11"><span>^ <a href="#cite_ref-2015-16_11-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-2015-16_11-1"><sup><i><b>b</b></i></sup></a></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/4/43/Wikimedia_Foundation_Audit_Report_-_FY15-16.pdf">Financial Statements, June 30, 2016 and 2015</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-2016-17-12"><span><b><a href="#cite_ref-2016-17_12-0">^</a></b></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/d/da/Wikimedia_Foundation_Audit_Report_-_FY16-17.pdf">Financial Statements, June 30, 2017 and 2016</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-2017-18-13"><span><b><a href="#cite_ref-2017-18_13-0">^</a></b></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/6/60/FY17-18_-_Independent_Auditors%27_Report.pdf">Financial Statements, June 30, 2018 and 2017</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-2018-19-14"><span><b><a href="#cite_ref-2018-19_14-0">^</a></b></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/3/31/Wikimedia_Foundation_Audit_Report_-_FY18-19.pdf">Financial Statements, June 30, 2019 and 2018</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-2019-20-15"><span><b><a href="#cite_ref-2019-20_15-0">^</a></b></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/f/f7/Wikimedia_Foundation_FY2019-2020_Audit_Report.pdf">Financial Statements, June 30, 2020 and 2019</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-2020-21-16"><span><b><a href="#cite_ref-2020-21_16-0">^</a></b></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/1/1e/Wikimedia_Foundation_FY2020-2021_Audit_Report.pdf">Financial Statements, June 30, 2021 and 2020</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-2021-22-17"><span><b><a href="#cite_ref-2021-22_17-0">^</a></b></span> <span><a href="https://upload.wikimedia.org/wikipedia/foundation/2/26/Wikimedia_Foundation_FY2021-2022_Audit_Report.pdf">Financial Statements, June 30, 2022 and 2021</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-2022-23-18"><span><b><a href="#cite_ref-2022-23_18-0">^</a></b></span> <span><a rel="nofollow" href="https://wikimediafoundation.org/wp-content/uploads/2023/11/Wikimedia_Foundation_FS_FY2022-2023_Audit_Report.pdf">Financial Statements, June 30, 2023 and 2022</a> <i>Wikimedia Foundation</i></span>
</li>
<li id="cite_note-19"><span><b><a href="#cite_ref-19">^</a></b></span> <span><a href="https://stats.wikimedia.org/EN/TablesPageViewsMonthlyCombined.htm">Page views for Wikipedia</a></span>
</li>
<li id="cite_note-20"><span><b><a href="#cite_ref-20">^</a></b></span> <span><a href="https://en.wikipedia.org/wiki/Wikimedia_Foundation">Wikimedia Foundation Wikipedia page</a></span>
</li>
<li id="cite_note-21"><span><b><a href="#cite_ref-21">^</a></b></span> <span>1,400,000 x 11 ˜ = 15,400,000, 1,400,000 x 12 ˜ = 16,800,000</span>
</li>
<li id="cite_note-22"><span><b><a href="#cite_ref-22">^</a></b></span> <span>($5000 x 12) x 33.3... = $2,000,000</span>
</li>
<li id="cite_note-23"><span><b><a href="#cite_ref-23">^</a></b></span> <span>($5,000 x 12) x 1250 = $75,000,000</span>
</li>
<li id="cite_note-24"><span><b><a href="#cite_ref-24">^</a></b></span> <span>$35,704,796 x 1.85 ~= $65,947,465</span>
</li>
<li id="cite_note-25"><span><b><a href="#cite_ref-25">^</a></b></span> <span><a rel="nofollow" href="http://www.dailydot.com/business/sue-gardner-log-rolling-corruption-wikimedia-chapters/">Where does your Wikipedia donation go? Outgoing chief warns of potential corruption</a> <i><a href="https://en.wikipedia.org/wiki/The_Daily_Dot" title="The Daily Dot">The Daily Dot</a></i></span>
</li>
<li id="cite_note-26"><span><b><a href="#cite_ref-26">^</a></b></span> <span><a rel="nofollow" href="http://www.theregister.co.uk/2013/10/08/wikipedia_foundation_money_in_wrong_place/">Wikipedia Foundation exec: Yes, we've been wasting your money</a> <i><a href="https://en.wikipedia.org/wiki/The_Register" title="The Register">The Register</a></i></span>
</li>
<li id="cite_note-27"><span><b><a href="#cite_ref-27">^</a></b></span> <span><a rel="nofollow" href="https://www.washingtonpost.com/news/the-intersect/wp/2015/12/02/wikipedia-has-a-ton-of-money-so-why-is-it-begging-you-to-donate-yours/">Wikipedia has a ton of money. So why is it begging you to donate yours?</a> <i><a href="https://en.wikipedia.org/wiki/The_Washington_Post" title="The Washington Post">The Washington Post</a></i></span>
</li>
<li id="cite_note-28"><span><b><a href="#cite_ref-28">^</a></b></span> <span><a href="https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2015-10-14/News_and_notes">2015–2016 Q1 fundraising update sparks mailing list debate</a> <i><a href="https://en.wikipedia.org/wiki/Wikipedia_Signpost" title="Wikipedia Signpost">Wikipedia Signpost</a></i></span>
</li>
<li id="cite_note-29"><span><b><a href="#cite_ref-29">^</a></b></span> <span><a rel="nofollow" href="http://www.ibtimes.co.uk/wikipedia-fundraising-drive-should-you-donate-money-wikipedia-foundation-1531912">Wikipedia fundraising drive: Should you donate money to the Wikimedia Foundation?</a> <i><a href="https://en.wikipedia.org/wiki/International_Business_Times" title="International Business Times">International Business Times</a></i></span>
</li>
<li id="cite_note-30"><span><b><a href="#cite_ref-30">^</a></b></span> <span><a rel="nofollow" href="http://wikipediocracy.com/2014/12/11/the-wikipedia-fundraising-banner-sad-but-untrue/">The Wikipedia Fundraising Banner: Sad but Untrue</a> <i>Wikipediocracy</i></span>
</li>
<li id="cite_note-31"><span><b><a href="#cite_ref-31">^</a></b></span> <span><a rel="nofollow" href="http://www.makeuseof.com/tag/wikipedia-millions-bank-beg/">Wikipedia Has Millions In The Bank – Why Beg For More?</a> <i>MakeUseOf</i></span>
</li>
<li id="cite_note-32"><span><b><a href="#cite_ref-32">^</a></b></span> <span><a rel="nofollow" href="http://wikipediocracy.com/2014/09/21/wikipedia-keeping-it-free-just-pay-us-our-salaries/">Wikipedia – keeping it free. Just pay us our salaries.</a> <i>Wikipediocracy</i></span>
</li>
<li id="cite_note-33"><span><b><a href="#cite_ref-33">^</a></b></span> <span><a rel="nofollow" href="http://effective-altruism.com/ea/c8/should_you_donate_to_the_wikimedia_foundation/">Should you donate to the Wikimedia Foundation?</a> <i>Effective Altruism</i></span>
</li>
<li id="cite_note-34"><span><b><a href="#cite_ref-34">^</a></b></span> <span><a rel="nofollow" href="https://www.quora.com/Wikipedia-in-2015-Why-does-Wikipedia-ask-for-donations-even-though-it-has-a-huge-reserve-60M-of-value-cash-investments-etc">Why does Wikipedia ask for donations even though it has a huge reserve?</a> <i><a href="https://en.wikipedia.org/wiki/Quora" title="Quora">Quora</a></i></span>
</li>
<li id="cite_note-35"><span><b><a href="#cite_ref-35">^</a></b></span> <span><a rel="nofollow" href="http://thewikipedian.net/2016/03/11/modest-proposal-wikimedia-future/">A Modest Proposal for Wikimedia’s Future</a> <i>The Wikipedian</i></span>
</li>
<li id="cite_note-36"><span><b><a href="#cite_ref-36">^</a></b></span> <span><a rel="nofollow" href="http://www.theregister.co.uk/2012/12/20/cash_rich_wikipedia_chugging/">Wikipedia doesn't need your money - so why does it keep pestering you?</a> <i>The Register</i></span>
</li>
<li id="cite_note-37"><span><b><a href="#cite_ref-37">^</a></b></span> <span><a rel="nofollow" href="https://lists.archive.carbon60.com/wiki/foundation/534599">Fundraising banners (again)</a> <i>Wikimedia-l</i></span>
</li>
<li id="cite_note-38"><span><b><a href="#cite_ref-38">^</a></b></span> <span><a href="https://en.wikipedia.org/w/index.php?title=User_talk:Jimbo_Wales&amp;oldid=779581521#Future_changes">[1]</a> <i>Wikimedia Foundation</i></span>
</li>
</ol></div>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=User:Guy_Macon/Wikipedia_has_Cancer&amp;action=edit&amp;section=6" title="Edit section: External links"><span>edit</span></a><span>]</span></span></h2>
<ul><li><a rel="nofollow" href="https://web.archive.org/web/20190429044449/https://mollywhite.net/wikimedia-timeline/">Wikimedia timeline of events</a> by <a href="https://en.wikipedia.org/wiki/User:GorillaWarfare" title="User:GorillaWarfare">Molly White</a></li></ul>
<!-- 
NewPP limit report
Parsed by mw‐web.codfw.main‐6b6f4c49c‐bcmqq
Cached time: 20240406035011
Cache expiry: 2592000
Reduced expiry: false
Complications: [no‐toc]
CPU time usage: 0.191 seconds
Real time usage: 0.274 seconds
Preprocessor visited node count: 1213/1000000
Post‐expand include size: 25334/2097152 bytes
Template argument size: 2254/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 2/500
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 25586/5000000 bytes
Lua time usage: 0.055/10.000 seconds
Lua memory usage: 1559120/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  228.760      1 -total
 23.77%   54.371      1 Template:Essay
 20.94%   47.895      1 Template:Ombox
 18.67%   42.702      1 Template:Graph:Chart
 15.29%   34.981      1 Template:Redirect
 14.60%   33.410      1 Template:Annual_readership
 12.99%   29.725      1 Template:Tmbox
 12.83%   29.353      1 Template:Ombox/shortcut
  8.57%   19.616      1 Template:Reflist
  7.83%   17.905      1 Template:Hidden
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:53306026-0!canonical and timestamp 20240406035011 and revision id 1213777095. Rendering was triggered because: page-view
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bitcoin Block 840000 (184 pts)]]></title>
            <link>https://mempool.space/block/0000000000000000000320283a032748cef8227873ff4872689bf23f1cda83a5</link>
            <guid>40093263</guid>
            <pubDate>Sat, 20 Apr 2024 00:14:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mempool.space/block/0000000000000000000320283a032748cef8227873ff4872689bf23f1cda83a5">https://mempool.space/block/0000000000000000000320283a032748cef8227873ff4872689bf23f1cda83a5</a>, See on <a href="https://news.ycombinator.com/item?id=40093263">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
    </channel>
</rss>