<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 15 Nov 2023 06:00:09 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[U.S. reenters the nuclear fuel game (177 pts)]]></title>
            <link>https://spectrum.ieee.org/nuclear-power-plant-2666199640</link>
            <guid>38272132</guid>
            <pubDate>Wed, 15 Nov 2023 01:12:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/nuclear-power-plant-2666199640">https://spectrum.ieee.org/nuclear-power-plant-2666199640</a>, See on <a href="https://news.ycombinator.com/item?id=38272132">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="U.S. Reenters the Nuclear Fuel Game" data-elid="2666199640" data-post-url="https://spectrum.ieee.org/nuclear-power-plant-2666199640" data-authors="Prachi Patel" data-page-title="U.S. Reenters the Nuclear Fuel Game - IEEE Spectrum"><p>The 93 currently active nuclear-power reactors in the United States burn about 2,000 tonnes of uranium fuel each year. However, the type of uranium fuel those reactors use is not going to cut it for the <a href="https://spectrum.ieee.org/terrapowers-nuclear-reactor-could-power-the-21st-century" target="_self">advanced reactors</a> expected to go on line in the coming years, as part of the effort to meet the country’s goal of <a href="https://www.washingtonpost.com/climate-environment/2020/07/30/biden-calls-100-percent-clean-electricity-by-2035-heres-how-far-we-have-go/" target="_blank">100 percent clean electricity</a> by 2035. The specialized fuel these advanced reactors will need is currently <a href="https://www.ft.com/content/2c9c325e-e734-4a9f-b089-2f64deebc658" target="_blank">made on a commercial scale only in Russia</a>. </p><p>Not for long, though. Last week, <a href="https://www.centrusenergy.com/" rel="noopener noreferrer" target="_blank">Centrus Energy</a> in Bethesda, Md., jump-started the <a href="https://www.eia.gov/energyexplained/nuclear/where-our-uranium-comes-from.php" target="_blank">first commercial domestic nuclear fuel production</a> in the United States in 70 years by delivering the first load of high-assay, low-enriched uranium (HALEU) fuel made at its Piketon, Ohio, plant to the U.S. Department of Energy (DOE). The company is on track to produce 20 kilograms of HALEU by the end of the year, and then expects to produce 900 kg in 2024, says Jeffrey Cooper, director of engineering at Centrus. </p><p>This is a critical step toward large-scale deployment of <a href="https://spectrum.ieee.org/a-double-first-in-china-for-advanced-nuclear-reactors" target="_self">advanced nuclear plants</a> in the United States. The DOE expects to <a href="https://www.energy.gov/ne/articles/energy-departments-advanced-reactor-demonstration-program-awards-30-million-initial" rel="noopener noreferrer" target="_blank">invest about US $600 million</a> to mature next-generation reactors through its <a href="https://www.energy.gov/ne/advanced-reactor-demonstration-program" target="_blank">Advanced Reactor Demonstration Program</a>, and “nine out of 10 of those reactors use HALEU fuels,” Cooper says.</p><p>“We’d like to avoid increasing our dependence on energy fuels from Russia. So it’s critically important that we secure our supply of HALEU material, given the number of advanced reactors desiring to use it in the future for commercialization.” <strong>—Kathryn Huff, Department of Energy</strong></p><p>Less than one percent of natural uranium is U-235, the uranium isotope capable of sustaining a nuclear chain reaction. Today’s reactors use low-enriched uranium (LEU), which is almost 5 percent U-235. HALEU is enriched further to a concentration, or assay, of almost 20 percent U-235, which is still considered low-enriched compared<strong></strong>with the 90-plus percent level that is required for weapons-grade uranium.</p><p>“The higher concentration of U-235 allows for higher power densities in the cores of advanced reactor designs,” Cooper says. That means more efficient reactors with smaller cores, longer core lives, and less fuel waste. The energy in just 3 tablespoons of HALEU can supply a lifetime’s worth of power for the average U.S. consumer, according to Centrus. </p><p>Research reactors at U.S. national laboratories and universities today use a small amount of HALEU provided by the DOE. There are three different ways to make  HALEU. Gaseous diffusion—the “old-school way we used to do it,” according to <a href="https://www.energy.gov/ne/person/dr-kathryn-huff" rel="noopener noreferrer" target="_blank">Kathryn Huff</a>, the assistant secretary for <a href="https://spectrum.ieee.org/tag/nuclear-energy">nuclear energy</a> at the DOE, at federally owned enrichment facilities for the Manhattan Project and commercial nuclear sector for years—fell from favor because it is extremely energy intensive.</p><p>Centrus and the Russian state-owned company Tenex, which are the only two outfits that can produce HALEU in the world, use a method called gaseous centrifusion. Centrus starts with nearly 5 percent enriched uranium in gas form and spins it at very high speeds in four-story-tall tubular centrifuges, where the centrifugal forces separate the isotopes based on weight. “The U-238 is flung to the wall, and U-235 stays preferentially in the interior,” Cooper says. “We get two streams that come out—the product stream, which is enriched in U-235, and what we call the tail stream, which has less material.” </p><p>The gas is channeled through a cascade of centrifuges, where it gets progressively more enriched until reaching the target HALEU enrichment level of 19.75 percent. Centrus currently operates one cascade of 16 centrifuge machines.</p><p>Another promising enrichment technology, called laser enrichment, involves separating uranium isotopes based on the different energy levels at which their nuclei get excited. The technology is still in early stages of development, though, and Huff says that the DOE is closely watching <a href="https://www.gle-us.com/" rel="noopener noreferrer" target="_blank">Global Laser Enrichment</a> in Wilmington, N.C., as a company that’s piloting the technology and that “could easily be a player in coming years.”</p><p>Not all next-generation reactors will use HALEU, Huff points out. <a href="https://spectrum.ieee.org/small-modular-reactors-advance-act" target="_self">Small modular reactors</a> being developed by GE and <a href="https://spectrum.ieee.org/slow-steady-progress-for-two-us-nuclear-power-projects">Westinghouse</a>, for instance, use water as a coolant and uranium oxide as fuel, and are “basically shrunken versions of conventional reactors in the [United States]. They want to use standard LEU fuel,” says Huff.</p><p>HALEU is needed instead for <a href="https://www.energy.gov/ne/articles/us-department-energy-announces-160-million-first-awards-under-advanced-reactor" rel="noopener noreferrer" target="_blank">advanced reactors</a> with more creative coolants and fuels, which are trying to achieve very compact core sizes. These include Bellevue, Wash.–based <a href="https://www.terrapower.com/" rel="noopener noreferrer" target="_blank">TerraPower’s</a> sodium-cooled fast reactor, Rockville, M.D.–based <a href="https://x-energy.com/" target="_blank">X-energy’s</a> high-temperature gas reactor, and Alameda, Calif.–based <a href="https://kairospower.com/" target="_blank">Kairos Power’s</a> fluoride-salt-cooled high-temperature reactor. </p><p>The DOE projects that the United States will need <a href="https://www.energy.gov/articles/doe-announces-cost-shared-award-first-ever-domestic-production-haleu-advanced-nuclear" rel="noopener noreferrer" target="_blank">more than 40 tonnes of HALEU</a> before the end of the decade. But advanced reactor makers are still years away from firing up their cores. For instance, TerraPower (backed by Bill Gates) announced last December that it was <a href="https://world-nuclear-news.org/Articles/HALEU-fuel-availability-delays-Natrium-reactor-pro" rel="noopener noreferrer" target="_blank">delaying its Natrium reactor demonstration</a> by two years because of a lack of HALEU fuel.</p><p>Centrus will have to build multiple cascades, each with 120 centrifuges, to make HALEU on a commercial scale. Right now, Centrus has an <a href="https://www.terrapower.com/terrapower-and-centrus-expand-efforts-to-commercialize-domestic-haleu-production/" rel="noopener noreferrer" target="_blank">understanding in place</a> with TerraPower, but not a formal purchase agreement.</p><p>Uranium enrichment supplier <a href="https://www.urenco.com/" rel="noopener noreferrer" target="_blank">Urenco</a>, which is co-owned by the British government, the Netherlands government, and German utilities, is also <a href="https://world-nuclear-news.org/Articles/Urenco-USA-announces-HALEU-activities" rel="noopener noreferrer" target="_blank">considering HALEU production</a> at its New Mexico–based enrichment facilities, where it produces LEU today. Meanwhile, Lynchburg, Va.–based BWXT, which in August announced that it will be producing <a href="https://www.bwxt.com/news/2023/08/30/BWXT-to-Manufacture-HALEU-Feedstock-for-Advanced-Reactors-" rel="noopener noreferrer" target="_blank">2 metric tons of HALEU</a> over the next five years for the National Nuclear Security Administration, is “prepared to make more HALEU for the U.S. government in the future in support of the advanced-reactor market,” says Sharon Smoot, president of BWXT’s nuclear fuels business unit. “Like any vendor, we look for market signals, and we’re optimistic about what we see.”</p><p>For now, the DOE intends to purchase about 25 tonnes of HALEU per year to kick-start the industry and give HALEU producers secure contracts from which they can expand production. “We’d like to avoid increasing our dependence on energy fuels from Russia,” Huff says. “So it’s critically important that we secure our supply of HALEU material, given the number of advanced reactors desiring to use it in the future for commercialization.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ARRL hails FCC action to remove symbol rate restrictions (102 pts)]]></title>
            <link>https://www.arrl.org/news/arrl-hails-fcc-action-to-remove-symbol-rate-restrictions</link>
            <guid>38271702</guid>
            <pubDate>Wed, 15 Nov 2023 00:16:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.arrl.org/news/arrl-hails-fcc-action-to-remove-symbol-rate-restrictions">https://www.arrl.org/news/arrl-hails-fcc-action-to-remove-symbol-rate-restrictions</a>, See on <a href="https://news.ycombinator.com/item?id=38271702">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://www.arrl.org/" target="_blank">ARRL&nbsp; The National Association for Amateur Radio</a>®&nbsp;reports that earlier today, the FCC Commissioners unanimously voted to amend the Amateur Radio Service rules to replace the baud rate limit on the Amateur HF bands with a 2.8 kHz bandwidth limit to permit greater flexibility in data communications.</p><p>
“The Federal Communications Commission today adopted <a href="https://docs.fcc.gov/public/attachments/FCC-23-93A1.pdf" target="_blank">new rules</a> to incentivize innovation and experimentation in the amateur radio bands by removing outdated restrictions and providing licensees with the flexibility to use modern digital emissions,” <a href="https://docs.fcc.gov/public/attachments/DOC-398409A1.pdf" target="_blank">announced</a> FCC.</p><p>
“Specifically, we remove limitations on the symbol rate (also known as baud rate) -- the rate at which the carrier waveform amplitude, frequency, and/or phase is varied to transmit information -- applicable to data emissions in certain amateur bands,” concluded the FCC <a href="https://docs.fcc.gov/public/attachments/FCC-23-93A1.pdf" target="_blank">Report and Order and Further Notice of Proposed Rulemaking</a>, adopted November 13, 2023. “The amateur radio community can play a vital role in emergency response communications, but is often unnecessarily hindered by the baud rate limitations in the rules.”</p><p>
Consistent with ARRL’s request, the amended rules will replace the current HF restrictions with a 2.8 kHz bandwidth limit. “We agree with ARRL that a 2.8 kilohertz bandwidth limitation will allow for additional emissions currently prohibited under the baud rate limitations while providing sufficient protections in the shared RTTY/data subbands,” concluded the FCC Report and Order.</p><p>
ARRL President Rick Roderick, K5UR, hailed the FCC’s action to remove the symbol rate restrictions. Roderick stated that “this action will measurably facilitate the public service communications that amateurs step up to provide, especially at times of natural disasters and other emergencies such as during the hurricane season.&nbsp;Digital technology continues to evolve, and removing the outmoded data restrictions restores the incentive for radio amateurs to continue to experiment and develop more spectrum-efficient protocols and methods while the 2.8 kHz bandwidth limit will help protect the shared nature of our bands. We thank Congresswoman [Debbie] Lesko (AZ-08) for her efforts on behalf of all Amateurs to get these restrictions removed.”</p><p>
In a Further Notice of Proposed Rulemaking (FNPRM), the FCC proposes to eliminate similar restrictions where they apply in other bands. “We propose to remove the baud rate limitation in the 2200 meter band and 630 meter band … and in the very high frequency (VHF) bands and the ultra-high frequency (UHF) bands. Additionally, we seek comment on the appropriate bandwidth limitation for the 2200 meter band, the 630 meter band, and the VHF/UHF bands.” ARRL has previously expressed its support for eliminating the symbol rate limits in favor of bandwidth limits where they apply on the VHF and UHF bands but suggested that the bandwidth limits themselves be reviewed in light of today’s technology and tomorrow’s possibilities. Similarly, when eliminating the baud limits on the 2200 and 630-meter bands, consideration should be given to what, if any, bandwidth limits are appropriate.</p><p>
The FCC will announce a period for public comment on the additional proposed changes based upon publication of the FNPRM in the Federal Register.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing a GPT-4 script to check Wikipedia for the first unused acronym (105 pts)]]></title>
            <link>https://gwern.net/tla</link>
            <guid>38270714</guid>
            <pubDate>Tue, 14 Nov 2023 22:27:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gwern.net/tla">https://gwern.net/tla</a>, See on <a href="https://news.ycombinator.com/item?id=38270714">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-metadata" title="Page Metadata">
        <p>Curious what the first ‘unused’ alphabetic acronym is, I have GPT-4 write a script to check English Wikipedia. After three bugs, the first unused one turns out as of 2023-09-29 to be the three-letter acronym ‘CQK’, with another 2.6k TLA unused, and 393k four-letter acronyms unused. Exploratory analysis suggests alphabetical order effects as well as letter-frequency.</p>
        
        
      </div><div id="markdownBody">
        
        <div>
          <blockquote>
            <p>It sometimes seems as if everything that <em>could</em> be trademarked <em>has</em> been, and as if every possible three-letter acronym (TLA) has been used in some nontrivial way by <em>someone</em>. Is this true? No—actually, a fair number, <a href="#results">starting with <strong>CQK</strong></a>, have no nontrivial use to date.</p>
            <p>We could check by defining ‘nontrivial’ as ‘has an English Wikipedia article, disambiguation page, or redirect’, and then writing a script which simply looks up every possible TLA Wikipedia URL to see which ones exist. This is a little too easy, so I make it harder by making GPT-4 <a href="#script">write a Bash shell script</a> to do so (then <a href="#python">Python</a> to double-check).</p>
            <p>GPT-4 does so semi-successfully, making self-reparable errors until it runs into its idiosyncratic <a href="#blind-spot">‘blind spot’ error</a>. After it accidentally fixes that, the script appears to work successfully, revealing that—contrary to my expectation that every TLA exists—the first non-existent acronym is the TLA ‘CQK’, and that there are many unused TLAs (2,684 or 15% unused) and even more unused four-letter acronyms (392,884 or 85% unused). I provide the list of all unused TLAs &amp; four-letter acronyms (as well as <a href="#unused-numerical-acronyms">alphanumerical ones</a>—the first unused alphanumerical one is <strong>AA0</strong>.)</p>
            <p>TLAs are not unused at random, with <a href="#patterns">clear patterns</a> enriched in letters like ‘J’ or ‘Z’ vs ‘A’ or ‘E’. Additional GPT-4-powered analysis in R <a href="#order-letter-frequency-effects">suggests that both</a> letter-frequency &amp; position in alphabet predict unusedness to some degree, but leave much unexplained</p>
          </blockquote>
        </div>
        <p>Verifying <a href="https://en.wikipedia.org/wiki/English_Wikipedia" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/English_Wikipedia#bodyContent">Wikipedia</a> links in my essays, I always check <a href="https://en.wikipedia.org/wiki/Acronyms" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Acronyms#bodyContent">acronyms</a> by hand: there seems to always be an alternative definition for any acronym, especially <a href="https://en.wikipedia.org/wiki/Three-letter_acronyms" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Three-letter_acronyms#bodyContent">three-letter acronyms</a> (TLA)—and sometimes an absurd number. Trying a random TLA for this essay, <a href="https://en.wikipedia.org/wiki/Z_(joke_line)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Z_(joke_line)#bodyContent">“Zzzzzz”</a>, I found it was used anyway!<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> This makes me wonder: <strong>has every possible alphabetic TLA been used?</strong></p>
        <p>This cannot be true for too many sizes of acronyms, of course, but it may be possible for your classic three-letter acronym because there are relatively few of them. You have to go to four-letter acronyms before they look inexhaustible: there 26<sup>1</sup> = 26 possible single-letter ones, 26<sup>2</sup> = 676 two-letter ones, 26<sup>3</sup> = 17,576 three-letter ones, but then many four-letter ones as 26<sup>4</sup> = 456,976.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> So I’d expect all TLAs to be exhausted and to find the first unused acronym somewhere in the FLAs (similar to how <a href="https://harvardlawreview.org/print/vol-131/are-we-running-out-of-trademarks/">every English word has been trademarked</a>, forcing people to come up with increasingly nonsensical names to avoid existing trademarks &amp; parasites like domain squatters).</p>
        <section id="used-criteria">
          <h2><a href="#used-criteria" title="Link to section: § 'Used Criteria'">Used Criteria</a></h2>
          <p>How do we define used? If we simply look for <em>any</em> use, this would not be interesting. Surely they have all been used in a serial number or product number somewhere, or simply squatted in various ways. I wouldn’t be surprised if someone has squatted on every TLA on <a href="https://en.wikipedia.org/wiki/GitHub" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/GitHub#bodyContent">Github</a> or in domain names or social media user account names, for example—it’s free or cheap, and you only have to extort one whale to extract a rent. Similarly, ‘number of Google Hits’ is a bad proxy because it will be inflated by technical garbage and as search engines have evolved and are now distant from their roots in counting word frequencies in a text corpus, the number of Google hits appears to bear increasingly little resemblance to anything one might expect. <a href="https://en.wikipedia.org/wiki/Google_Ngram_Viewer" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Google_Ngram_Viewer#bodyContent">Google Ngram</a> is mostly historical data, and has many data quality issues related to OCR &amp; data selection which would affect acronyms especially.</p>
          <p>We want a comprehensive, curated, online, database which reflects a human sense of ‘importance’. If there’s no reason someone would have heard of a TLA use, then that doesn’t count: a use ought to be at least somewhat <em>notable</em>, in the sense that someone might look it up or it might be a notable use: ‘having a Wikipedia page’ comes to mind as a heuristic. Indeed, not just having a Wikipedia article, but also having a <a href="https://en.wikipedia.org/wiki/Wikipedia:Disambiguation" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Wikipedia:Disambiguation#bodyContent">Wikipedia <em>disambiguation</em> page</a> is ideal, as it indicates <em>multiple</em> uses; having a Wikipedia article is also good; even having a redirect to another page seems reasonable to consider as ‘used’ in some sense because it suggests that someone used that TLA in a context where a human would want to look it up &amp; there’s a genuine meaning to the TLA. (While if no editor can be bothered to even redirect a TLA to an existing page, that is a low bar to fail.) That is, simply checking for <em>any</em> Wikipedia page is a reasonable criterion.</p>
          <p>And defining notability this way, we can do that simply by requesting the WP URL for a TLA and seeing if it returns an error.</p>
        </section>
        <section id="script">
          <h2><a href="#script" title="Link to section: § 'Script'">Script</a></h2>
          <p>Generating all possible acronyms is not that hard; the <a href="https://en.wikibooks.org/wiki/Haskell/Understanding_monads/List" data-link-icon="wikipedia" data-link-icon-type="svg">Haskell list monad</a>, for example, can generate various permutations or sequences in a line, so if we wanted all the acronyms, it’s just this:</p>
          <div id="cb1">
            <pre><code><span id="cb1-1"><span>take</span> <span>100</span> [ s <span>|</span> n <span>&lt;-</span> [<span>1</span><span>..</span>], s <span>&lt;-</span> <span>sequence</span> <span>$</span> <span>replicate</span> n [<span>'A'</span><span>..</span><span>'Z'</span>]]</span>
<span id="cb1-2"><span>-- ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N",</span></span>
<span id="cb1-3"><span>-- "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z",</span></span>
<span id="cb1-4"><span>-- "AA", "AB", "AC", "AD", "AE", "AF", "AG", "AH", "AI", "AJ", "AK", "AL",</span></span>
<span id="cb1-5"><span>-- "AM", "AN", "AO", "AP", "AQ", "AR", "AS", "AT", "AU", "AV", "AW", "AX",</span></span>
<span id="cb1-6"><span>-- "AY", "AZ", "BA", "BB", "BC", "BD", "BE", "BF", "BG", "BH", "BI", "BJ",</span></span>
<span id="cb1-7"><span>-- "BK", "BL", "BM", "BN", "BO", "BP", "BQ", "BR", "BS", "BT", "BU", "BV",</span></span>
<span id="cb1-8"><span>-- "BW", "BX", "BY", "BZ", "CA", "CB", "CC", "CD", "CE", "CF", "CG", "CH",</span></span>
<span id="cb1-9"><span>-- "CI", "CJ", "CK", "CL", "CM", "CN", "CO", "CP", "CQ", "CR", "CS", "CT",</span></span>
<span id="cb1-10"><span>-- "CU", "CV"]</span></span></code></pre>
          </div>
          <p>We could then do a <a href="https://hackage.haskell.org/package/HTTP-4000.4.1/docs/Network-HTTP.html#v:simpleHTTP" data-link-icon="𝛌" data-link-icon-type="text"><code>Network.HTTP</code> request</a>. But that would be too easy. We can use this as an excuse to try out the most advanced neural network I have access to: <a href="https://openai.com/research/gpt-4" data-link-icon="openai" data-link-icon-type="svg">GPT-4</a>.</p>
        </section>
        <section id="effective-gpt-4-programming">
          <h2><a href="#effective-gpt-4-programming" title="Link to section: § 'Effective GPT-4 Programming'">Effective GPT-4 Programming</a></h2>
          <p>GPT-3’s programming abilities were a bit of a surprise, but rarely worth using for anyone with reasonable skills, and one had to use a highly-specialized model like <a href="https://arxiv.org/abs/2107.03374#openai" id="chen-et-al-2021-codex" data-link-icon="openai" data-link-icon-type="svg" data-url-archive="/doc/www/arxiv.org/78528646bb225d8b30dab63ee0b544b42956a866.pdf#openai" data-href-mobile="https://ar5iv.labs.arxiv.org/html/2107.03374?fallback=original#openai" title="‘Evaluating Large Language Models Trained on Code’, Chen et al 2021">Codex</a>/<a href="https://en.wikipedia.org/wiki/GitHub" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/GitHub#bodyContent">Github</a> <a href="https://github.com/features/copilot/" data-link-icon="github" data-link-icon-type="svg">Copilot</a> for coding; <a href="https://arxiv.org/abs/2005.14165#openai" id="brown-et-al-2020" data-link-icon="openai" data-link-icon-type="svg" data-url-archive="/doc/www/arxiv.org/90cd91e98db4f7b0b1cd57da7c3713dbe34c2146.pdf#openai" data-href-mobile="https://ar5iv.labs.arxiv.org/html/2005.14165?fallback=original#openai" title="'GPT-3: Language Models are Few-Shot Learners', Brown et al 2020">GPT-3</a>.5 was substantially better<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a>; and GPT-4 is better yet. I can’t compare GPT-4 to Github Copilot because I have not signed up nor figured out how to integrate it into my <a href="https://en.wikipedia.org/wiki/Emacs" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Emacs#bodyContent">Emacs</a>, but (as the early rumors promised) I’ve found GPT-4 good enough at programming in the main programming languages I use (<a href="https://en.wikipedia.org/wiki/Bash_(Unix_shell)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Bash_(Unix_shell)#bodyContent">Bash</a>, <a href="https://en.wikipedia.org/wiki/Emacs_Lisp" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Emacs_Lisp#bodyContent">Emacs Lisp</a>, <a href="https://en.wikipedia.org/wiki/Haskell" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Haskell#bodyContent">Haskell</a>, Python, &amp; R) to start turning over trickier tasks to it, and making heavier use of the languages I don’t know well (Emacs <a href="https://en.wikipedia.org/wiki/Lisp_(programming_language)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Lisp_(programming_language)#bodyContent">Lisp</a> &amp; Python) since I increasingly trust that an LLM can help me maintain them.</p>
          <p>However, GPT-4 is still far from perfect, and it doesn’t produce perfect code immediately; simply dumping large amounts of GPT-4-generated source code into your code base, “as long as it compiles and seems to work!”, seems like a good way to build up technical debt. (It also undermines future AIs, if you are dumping out buggy hot-mess code masquerading as correct debugged well-thought-out code—some GPT-4 code will be totally wrong as it <a href="https://en.wikipedia.org/wiki/Confabulation" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Confabulation#bodyContent">confabulates</a> solutions, due to problems like the <a href="#blind-spot">“blind spot”</a>.) You could try to track some <a href="https://en.wikipedia.org/wiki/Taint_checking" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Taint_checking#bodyContent">‘taint’</a> metadata, such as by segregating AI-generated code, and avoiding ever manual editing it or mixing it with human-written code; but this seems like a lot of work. My preferred approach is just to make GPT-4 ‘git gud’—write sufficiently good code that I can check it into <a href="https://en.wikipedia.org/wiki/Git" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Git#bodyContent">git</a> without caring where it came from.</p>
          <p>So, this section covers what I’ve learned from trying to prompt-engineer my programming tasks, using GPT-4 in the OpenAI Playground, up to November 2023.</p>
          <section id="system-prompt">
            <h2><a href="#system-prompt" title="Link to section: § 'System Prompt'">System Prompt</a></h2>
            <p>I find<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a> it helpful in general to try to fight the worst <a href="https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse?commentId=pfHTedu4GKaWoxD5K" data-link-icon="LW" data-link-icon-type="text" data-url-html="https://www.greaterwrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse/comment/pfHTedu4GKaWoxD5K?format=preview&amp;theme=classic">mealy-mouthed bureaucratic tendencies</a> of the RLHF by adding a ‘system prompt’:</p>
            <div>
              <blockquote>
                <p>The user is Gwern Branwen (<code>gwern.net</code>). To assist:</p>
                <p>Be terse. Do not offer unprompted advice or clarifications. Speak in specific, topic relevant terminology. Do NOT hedge or qualify. Do not waffle. Speak directly and be willing to make creative guesses. Explain your reasoning. if you don’t know, say you don’t know.</p>
                <p>Remain neutral on all topics. Be willing to reference less reputable sources for ideas.</p>
                <p>Never apologize.</p>
                <p>Ask questions when unsure.</p>
              </blockquote>
            </div>
          </section>
          <section id="inner-monologue">
            <h2><a href="#inner-monologue" title="Link to section: § 'Inner Monologue'">Inner Monologue</a></h2>
            <p>It helps to be more structured in how you write things: the more the LLM has to do, the more likely it is to screw them up and the harder error-correction becomes. GPT-4 is capable of fixing many errors in its code, as long as it only has to do so one at a time, in an <a href="https://gwern.net/doc/ai/nn/transformer/gpt/inner-monologue/index" title="'inner monologue (AI) tag', N/A 2023">inner-monologue-like sequence</a>; you can feed it errors or outputs, but surprisingly often, it can fix errors if you simply say that there is <em>an</em> error.</p>
            <p>So a waterfall-like approach works well, and I try to use GPT-4 like this:</p>
            <ol>
              <li>
                <p>ask it to <strong>ask questions</strong>, which it rarely does by default when you’re prompting it to do a task</p>
                <p>Often it has a few questions, which you can efficiently update your original prompt to cover.</p>
                <p>This avoids annoying cases where it’ll write an entirely valid solution, to a somewhat different problem than you have, and I think a good statement upfront probably subtly helps guide the rest of the process.</p>
              </li>
              <li>
                <p>make it <strong>generate tests</strong>; have it iteratively generate new tests which don’t overlap with the old ones.</p>
                <p>This is also useful for starting to modify some existing code: first generate the test-cases, and verify that the code actually works the way you assumed it did, and flush out any hidden assumptions by either you or GPT-4! <em>Then</em> go back to step #1.</p>
              </li>
              <li>
                <p>ask GPT-4 explicitly to make <strong>a list of ideas</strong>: edge-cases, bug-fixes, features, and stylistic rewrites/lints (in that order)</p>
                <p>It does <em>not</em> implement any of the suggestions. It simply lists them. If you instead tell it to implement the ideas, it will frequently trip over its own feet while trying to implement them all simultaneously in a single pass through the new code. (Just like humans, it is best to do one thing, check it, and then do the next thing.)</p>
                <ol>
                  <li>
                    <p>frequently, several of the items will be a bad idea, or too risky to ask GPT-4 to do. <em>Go one by one</em> through the list, having it implement just that one, and then test. Try to fix ‘core’ problems first.</p>
                  </li>
                  <li>
                    <p><em>self-repair</em>: not infrequently, a fancy rewrite will fail the test-suite (which we <em>did</em> generate in step #2, right?), but given the failing test-case and/or error pasted into the Playground, GPT-4 can usually fix it. (If GPT-4 cannot fix it given several tries and seems to be generating the same code fragments repeatedly or resorting to elaborate &amp; extreme rewrites, though the task doesn’t seem that hard, then you may have hit the blind spot and will need to fix it yourself—I’ve never seen GPT-4 escape the blind spot except by sheer accident.)</p>
                  </li>
                  <li>
                    <p><em>cleanup</em>: finally, You can ask it to rewrite the code for style/linting, but should leave that to the end, because otherwise that risks adding bugs while changing the code in ways that will wind up being discarded anyway.</p>
                  </li>
                </ol>
              </li>
              <li>
                <p>once it is clean and it’s either done the list or you’ve disapproved the suggestions, and the test-suite is passing, ask it to <strong>write a summary/design doc</strong> at the beginning and any additional <a href="https://en.wikipedia.org/wiki/Comment_(computer_programming)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Comment_(computer_programming)#bodyContent">code comments</a> inside it.</p>
                <p>GPT-4 will usually add a few comments in the code body itself, but not good ones, and it won’t usually write an adequate overall summary document unprompted. However, by this point, it has the context to do so should you ask it to.</p>
              </li>
            </ol>
            <p>With all this, you’re set up for maintainable code: with the test-suite and the up-front design doc, future LLMs can handle it natively (and will be able to learn from training on it), and you can easily add test-cases as you run into bugs; humans should be able to read the code easily after step #3 has finished, so you don’t need to care where it came from or try to track ‘taint’ through all future refactorings or usage—GPT-4 can write readable human-like code, it just doesn’t necessarily do it the best way the first time.</p>
            <p>While you may not necessarily have saved <em>time</em> (at least, if it’s in a language you are highly proficient in), you have saved yourself a lot of mental energy &amp; irritation (and made it much easier <a href="https://www.lesswrong.com/posts/9o3QBg2xJXcRCxGjS/working-hurts-less-than-procrastinating-we-fear-the-twinge" data-link-icon="LW" data-link-icon-type="text" data-url-html="https://www.greaterwrong.com/posts/9o3QBg2xJXcRCxGjS/working-hurts-less-than-procrastinating-we-fear-the-twinge?format=preview&amp;theme=classic">just to get started</a>) by making GPT-4 do the tedious work; it almost transforms programming from too-often-frustrating work filled with papercuts &amp; brokenness to spectator entertainment.</p>
          </section>
          <section id="case-studies">
            <h2><a href="#case-studies" title="Link to section: § 'Case Studies'">Case Studies</a></h2>
            <p>Some examples of nontrivial code I’ve written this way (ie. excluding the many little snippets or modifications I’ve used GPT-4 for, especially for the finer points of Bash syntax), with GPT-4 doing most (?) of the work, by language, in roughly chronological order:</p>
            <ul>
              <li>
                <p>Bash: <a href="https://github.com/gwern/gwern.net/commit/399642b993d211d1ff7e4532ccd1f0aa9dcc04ca#diff-801116d1e0e32a212a4678a3854842ca103f72c08f369e96c628db6eb921c34cL251" data-link-icon="github" data-link-icon-type="svg">tab completion</a> for the upload script, so it <a href="https://en.wikipedia.org/wiki/Command-line_completion" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Command-line_completion#bodyContent">tab-completes</a> the file and then the remote destination directory.</p>
                <p>I have no interest in learning the guts of Bash tab-completion in order to set up more advanced positional tab-completion; but GPT-4 already knows how to do it.</p>
              </li>
              <li>
                <p>Python: <a href="https://gwern.net/static/build/latex2unicode.py">latex2unicode.py</a> uses GPT-4 to convert <span>L<span>a</span>T<span>e</span>X</span> math fragments to HTML+CSS+Unicode, which are much easier to edit/style, render quicker, and look more natural; as <span>L<span>a</span>T<span>e</span>X</span> is a full-blown and rather hard to parse language, this is extremely difficult to do in any standard formal sense.</p>
                <p>This is a good example of the loop: I wrote none of the Python, but seeded it with a few instructions &amp; manual rewrites from my existing <span>L<span>a</span>T<span>e</span>X</span> → Unicode pipeline; then I prompted GPT-4 to ask for any <span>L<span>a</span>T<span>e</span>X</span> it could think of which it was unsure how to translate. After it gave a few examples, I would then manually translate them or add a new instruction, and ask again. Most of the examples it asked about I would not have thought of, like playing card suits (which are supported—<code>\clubsuit</code>, <code>\diamondsuit</code> etc).</p>
              </li>
              <li>
                <p>Haskell:</p>
                <ul>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/d68fb0dd561a910ff90d8cbc320af74c1e2a872f" data-link-icon="github" data-link-icon-type="svg">add <code>&lt;poster&gt;</code> thumbnails for videos</a></p>
                    <p>This is a frustrating one because as far as I can tell from running it, the GPT-4 code is easy to read and works flawlessly: it parses the HTML as expected, creates the necessary thumbnail, and rewrites the HTML <code>&lt;video&gt;</code> appropriately. It’s just that for some reason, the <em>rest</em> of my Hakyll codebase does not run it or it somehow breaks however it’s actually called, and I’ve never figured out why. (The opacity of Hakyll Haskell and the sheer complexity of the Gwern.net codebase in operation means that when a rewrite pass goes awry, it’s exceptionally difficult to figure out what is going wrong.)</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/ec3e515b895be82bdfeb82ef0bd8b52bdba7ff36" data-link-icon="github" data-link-icon-type="svg">link metadata handling</a>: the finicky handling of how links on Gwern.net get assigned the various bits of metadata determining whether they will pop up annotations etc had built up into a rat’s-nest of guards &amp; if-tens over time. When yet another feature broke because I misunderstood what the handler would do, I resolved to rewrite it to clarify the logic. My first refactoring attempts failed, as I kept losing track mentally and adding in bugs.</p>
                    <p>Then I threw up my hands and assigned the job to GPT-4, and it was able to cleanly refactor it after some iterations, and didn’t appear to introduce any bugs.</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/c0c918e97163e70507df08047803e8c40ee1b683#diff-99a96368a7dda8690fa3191e187150bbc5fc459f6db1518d6a8914abf04e0b7aR47" data-link-icon="github" data-link-icon-type="svg">correct URL rewrites</a>: a large refactoring of how URLs are rewritten to point to better URLs relied on GPT-4.</p>
                    <p>URLs on Gwern.net can be rewritten multiple ways, like to point to a mirrored version hosted locally or on a specialized site. For example, Medium.com has become extraordinarily reader-hostile, and so Medium links are rewritten to the equivalent Scribe.rip link. (See <a href="https://gwern.net/static/build/LinkArchive.hs" data-link-icon="code" data-link-icon-type="svg"><code>LinkArchive.hs</code></a> &amp; <a href="https://gwern.net/archiving#preemptive-local-archiving" id="gwern-archiving-preemptive-local-archiving" title="‘Archiving URLs § Preemptive Local Archiving’, Gwern 2011">Archiving URLs</a> for a fuller explanation of what &amp; why we do all this.)</p>
                    <p>In easy cases like that, it’s as simple as <code>s/medium.com/scribe.rip/</code>, but in some cases, it is necessary to formally parse a URL into a <a href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Uniform_Resource_Identifier#bodyContent">URI</a> data structure and extract the <a href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier#Syntax" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Uniform_Resource_Identifier#bodyContent">many complicated parts</a> (like host, path, query, and fragment), and rewrite them to the new URL. Haskell’s <a href="https://hackage.haskell.org/package/network-uri" data-link-icon="𝛌" data-link-icon-type="text">Network.URI</a> can do all this, but if one is not familiar with URI concepts and the library, it’s all so much gobbledegook and leaves one trapped in a maze of tiny types &amp; functions, each alike. Every time I have gone near it prior, I have been repelled by its force field.</p>
                    <p>GPT-4 was able to handle all the parsing &amp; reformatting, with special cases, for each domain separately, and then refactor out the duplication, and make the final version look positively easy (including <a href="https://github.com/gwern/gwern.net/commit/1dfa6721d83651d076cbfd9e9f818f8fbef25d13#diff-99a96368a7dda8690fa3191e187150bbc5fc459f6db1518d6a8914abf04e0b7aR53" data-link-icon="github" data-link-icon-type="svg">the later bug-fix</a> when it turned out I had misunderstood how a particular URL argument was supposed to go).</p>
                  </li>
                  <li>
                    <p>printing out <a href="https://github.com/gwern/gwern.net/commit/0f9c04cc9179656989169fb13350c6fe2127b890#diff-82dd61753bb332658dc7bda75fb9e231526d434492de6b55801845247ccd4bbeR73" data-link-icon="github" data-link-icon-type="svg">large numbers <em>not</em> in scientific-notation</a>: necessary for <a href="https://gwern.net/static/build/Inflation.hs" id="branwen-2019" data-link-icon="code" data-link-icon-type="svg" title="‘InflationAdjuster’, Branwen 2019">proper inflation-adjusted dollar amounts</a>, but weirdly difficult in Haskell’s default libraries.</p>
                    <p>After running into this issue several times, I resorted to the full workflow of test-suite and iterative revising. The pretty-printing is still more limited than I would like, but covers all numeric magnitudes it would be reasonable to inflation adjust, and the test-suite gives me confidence that this time is <em>finally</em> right.</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/28822b77f589c0b7cc9a68e8644a98c0e72ab56d" data-link-icon="github" data-link-icon-type="svg">compile-time location of internal cross-references</a>, to set the arrow-direction statically as a browser layout optimization</p>
                    <p>A Gwern.net feature is to make internal cross-references between sections less cognitively-taxing by specifying whether the reference is <em>before</em> or <em>after</em> the current location. For example, in this document, in the abstract, many sections are linked, and each of them has a down arrow (‘↓’) symbol: this tells you that the link target is below, and so you know you have not read the target yet, so you can decide whether you want to skip forward or keep reading. In other cases, like <a href="#previous-back-link">a link later on in this page</a>, the link instead is an up arrow (‘↑’), because it is pointing to previous material before it: now you know you have already read what it is referring to, and can remember it, and you may decide to ignore it. This is better than a mere opaque hyperlink, or even a internal link symbol like a section sign (‘§’): “See §discussion of <em>Z</em>”—well, <em>what</em> discussion? There was some mention of <em>Z</em> before, is that ‘the discussion’? Is there a larger later ‘discussion’ I haven’t read yet, that maybe I want to pop up and read now? Is this even in the same essay? Or what? Opaque cross-reference links create friction, as the reader is left with few clues about whether they want to spend effort to follow the link.</p>
                    <p>It is easy enough to write some JavaScript to run over an HTML page, detect all internal <a href="https://en.wikipedia.org/wiki/HTML_element#Anchor" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/HTML_element#bodyContent">anchor</a> links, and set the before/after arrow direction, and this is what we did for a long time. But while easy to write, this is not quite so easy for the browser to run (especially on long or heavily-hyperlinked pages), and it was adding a small amount to the layout time. And it is not <em>necessary</em> (or esthetic) to do this at runtime, because the locations of most links are known at compile-time. We knew all along that a Pandoc rewrite pass could take a document, look at all the links, decide whether they are before or after each other, and add the necessary arrow metadata. It’s just that this would be a stateful traverse requiring monadic operations and I was unsure how to do all the tree navigation operations to descend/ascend to find where something was. Because it was not a critical performance bottleneck, I put off this micro-optimization.</p>
                    <p>Eventually, I had an eureka moment: all that complexity about locating pairs of elements was unnecessary. All you need is traverse the AST in order while updating a <a href="https://en.wikipedia.org/wiki/Set_(abstract_data_type)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Set_(abstract_data_type)#bodyContent">set data-structure</a> to record whether you have seen a target link ID before; then at each cross-reference link, you have either seen the target link ID before, and therefore it must be <em>before</em>in the document, or you have not yet seen the target, and therefore it must be <em>after</em> in the document.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
                    <p>Once I had that simplification, it was a piece of cake to instruct GPT-4 to define a <a href="https://hackage.haskell.org/package/containers-0.7/docs/Data-Set.html" data-link-icon="𝛌" data-link-icon-type="text">Data.Set</a> set &amp; a <a href="https://hackage.haskell.org/package/mtl/docs/Control-Monad-State-Lazy.html" data-link-icon="𝛌" data-link-icon-type="text">State monad</a> to do that walk, and set up a test-suite to verify correctness, which did catch a few edge-cases (like links in the same paragraph).</p>
                  </li>
                  <li>
                    <p><a href="https://en.wikipedia.org/wiki/Title-case" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Title-case#bodyContent">title-case</a> <a href="https://github.com/gwern/gwern.net/commit/f9a0691aa80ae84a6e662686dd10f46507a48c06" data-link-icon="github" data-link-icon-type="svg">formatting of text</a>: my existing title-case code did not handle cases involving hyphens, so it would generate titles like “Foo Bar-bar Baz”, which I felt looked ugly compared to capitalizing after hyphens as well (ie. “Foo Bar-Bar Baz”).</p>
                    <p>GPT-4 handled the somewhat finicky string-munging and set up a test-suite, which I would be glad for later when I ran into another case where <a href="https://github.com/gwern/gwern.net/commit/51ced6f76e5d336f0c27e8109f72533863d72323#diff-9bcc0a8972fc364bc87786fe886ccd57b2168d0366ded8f533972a73af7f0378R219" data-link-icon="github" data-link-icon-type="svg">punctuation made lowercase look bad</a>.</p>
                  </li>
                  <li>
                    <p>detecting <a href="https://github.com/gwern/gwern.net/commit/7799fcfd36d886c89fb61b2223b82e776d1631c1" data-link-icon="github" data-link-icon-type="svg">imbalanced brackets/quotes</a> in documents</p>
                    <p>A particularly insidious family of typos is imbalanced brackets/parentheses/quotes: authors often fail to close a parenthesis pair or get lost, particularly in medical abstracts. This is a concern because often it indicates a more serious syntactic error, like an HTML <code>&lt;a&gt;</code> where the <code>href=</code> is malformed. I had a simple check which tested if the total number of each character was an even amount, but this failed to catch many typos: <code>[[]]</code> is correct and has an even number of both brackets, but that’s equally true of, say, the swapped equivalent <!-- [[ --> <code>]][[</code>. <!-- ]] --> It’s a well-known case of needing a full <a href="https://en.wikipedia.org/wiki/Stack_(abstract_data_type)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Stack_(abstract_data_type)#bodyContent">stack</a>, in order to push/pop each bracket <em>in order</em>, to detect not just numerical missingness, but wrong order.</p>
                    <p>It is not that hard, but tedious. It was something I did in an early CS course, and I felt that was enough for one lifetime, so I was happy to see if GPT-4 could do it. It could, and as I expected, it turned up scores of instances that had slipped through all my proofreading. (I didn’t ask it to set up a test-suite because the Gwern.net corpus <em>is</em> the test-suite in this case.)</p>
                  </li>
                  <li>
                    <p>checking that <a href="https://github.com/gwern/gwern.net/commit/8896f36f7d655545f1b87c07233347c9947a9ea8#diff-102b6b2d06d4eac2ab32b43b7215c078e3be02c3c96f6c7a8cf2a4ed73bb4433R1967" data-link-icon="github" data-link-icon-type="svg">sets of rules don’t overlap</a></p>
                    <p>Gwern.net configuration requires thousands of rewrite rules covering an endless army of special cases. Inevitably, the sets of rules will overlap or become redundant, especially as websites change domains or URLs get automatically updated. Overlap can cause bugs, or even kill the site compilation, if some update to either rules or essay content accidentally triggers a hidden infinite loop. So each config should ideally check for ‘redundancy’—but each set of <code>(key, value)</code> pairs tends to have a different need: some need the keys to be unique, some need the values to be unique, some need both to be unique, some need just the pairs to be unique, and heck, some are actually 3-tuples of <code>(key, value 1, value 2)</code> why not.</p>
                    <p>GPT-4 wrote out all the necessary instances and refactored them, and I applied them to the existing configs, and indeed discovered hundreds of overlaps and several serious bugs of the ‘how did this ever work’ variety.</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/eb1bd7a7f1cabf61e515611e78395d3b5a690f05#diff-102b6b2d06d4eac2ab32b43b7215c078e3be02c3c96f6c7a8cf2a4ed73bb4433R1980" data-link-icon="github" data-link-icon-type="svg">infinite loop (cycle) detection</a> in rewrite rules</p>
                    <p>In checking that rules don’t overlap with each other, there are nasty cases that can’t be detected just on a (key, value) basis. In particular, in doing rewrites, a rewrite could create an infinite loop even when there is no overlap whatsoever: for example, if we accidentally define a set of rewrite rules like [A → B, B → C, C → A], then all keys are unique, all values are unique, and all pairs are unique, but we have defined an infinite loop and if our code ever encounters any of the values A–C, then it will loop forever or crash. This is especially bad because it will only happen at runtime, and will depend on the exact inputs (so it might not trigger immediately), and will be hard to debug or trace back to the responsible rule.</p>
                    <p>And this is what happened on occasion with Gwern.net updates; the Wikipedia URL rewrites were particularly error-prone, as Wikipedia editors sometimes change their mind about what URL an article should be at, so if it gets moved over a redirect, it’s not hard to have a config which rewrites the old article title to the new article title, and then later one, discover that the new article title has been renamed to the old article title and add a rule for <em>that</em>…</p>
                    <p>To deal with this, we must treat the rules as defining <a href="https://en.wikipedia.org/wiki/Graph_theory" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Graph_theory#bodyContent">a directed graph</a>, and <a href="https://en.wikipedia.org/wiki/Cycle_(graph_theory)#Cycle_detection" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Cycle_(graph_theory)#bodyContent">detect any cycles</a>. Graph analysis is not something I’ve done that much of, so even though Haskell’s <a href="https://hackage.haskell.org/package/containers-0.7/docs/Data-Graph.html" data-link-icon="𝛌" data-link-icon-type="text">Data.Graph</a> should be fully capable of this, I didn’t know where to start, and put it off until a particularly annoying infinite-loop made me reach for GPT-4 in anger.</p>
                    <p>GPT-4 struggled with the problem, as its test-suite kept finding bugs in its <a href="https://en.wikipedia.org/wiki/Strongly_connected_component" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Strongly_connected_component#bodyContent">strongly connected component</a> approach—but it did it in the end with little help from me (not that I could, because I know no more of how to use Data.Graph than when I started). Infinite-loops have not been an issue since, and I am more free to add in rules because I don’t worry about triggering such nastily fatal bugs.</p>
                  </li>
                </ul>
              </li>
              <li>
                <p>Emacs Lisp: mostly helper functions for more conveniently editing abstracts &amp; excerpts for use as annotation popups</p>
                <ul>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/caa704e22aff192106d30d9c9d7adaff9c247500#diff-03ea4348bea7a138709946b85053309e32040dae6fd27039008063f290f7f138R1282" data-link-icon="github" data-link-icon-type="svg">call out to GPT-based script</a> for parsing a single block-paragraph abstract into a more readable multi-paragraph format and updating the buffer</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/8761dccc5b3fa4fff468db554813bf20e02702a8#diff-03ea4348bea7a138709946b85053309e32040dae6fd27039008063f290f7f138R102" data-link-icon="github" data-link-icon-type="svg">do <em>one</em> search-and-replace</a>: when adding a hyperlink to define a term, one only wants to do <em>one</em> search-and-replace; however, all the existing search-and-replace functions assume that you may want to replace any or all of the hits, and so you have to waste time exiting or canceling or skipping after doing the desired first replacement.</p>
                    <p>This defines a <code>query-replace-regexp-once</code> which will, as the name suggests, do only 1 replacement and then exit without further ado.</p>
                    <ul>
                      <li>
                        <p><a href="https://github.com/gwern/gwern.net/commit/18a6da407014bac4e202dfe1b0666a2c6e016a98#diff-03ea4348bea7a138709946b85053309e32040dae6fd27039008063f290f7f138R1261" data-link-icon="github" data-link-icon-type="svg"><em>don’t</em> search-and-replace in URLs</a>: we never want to do a search-and-replace to add a URL to text which is already <em>in</em> a URL!</p>
                      </li>
                    </ul>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/9fb4108b07e7bf86742024b4ff2bdcbd92eaff0e#diff-03ea4348bea7a138709946b85053309e32040dae6fd27039008063f290f7f138R1222" data-link-icon="github" data-link-icon-type="svg">remove hard-newlines in Markdown</a>: hard newlines or line-wraps, while ignored by <a href="https://en.wikipedia.org/wiki/Markdown" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Markdown#bodyContent">Markdown</a> and equivalent to regular line-wrapping, are ugly to read and interfere with many editing operations like search-and-replace. (What if you want to replace a phrase, but that phrase is broken across a line?)</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/d391c340cc02aae6d2b54cb672e7f12ec7f620ba#diff-03ea4348bea7a138709946b85053309e32040dae6fd27039008063f290f7f138R1252" data-link-icon="github" data-link-icon-type="svg">mark up various HTML/Markdown fragments</a>: various shortcuts for surrounding a word or a selected region of text with various HTML/Markdown snippets like italics or smallcaps.</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/bec53fee7403624f386024316c7bbde7bd11fb4a" data-link-icon="github" data-link-icon-type="svg">interactively add inflation-adjustment markup</a>: going around modifying <code>foo</code> → <code>[foo]($2023)</code> for every dollar amount in a buffer is a bit tedious. This detects dollars, asks a date, and queries each possible mark-up.</p>
                  </li>
                </ul>
              </li>
            </ul>
            <p>Overall, GPT-4 has been highly useful and handled many tasks I simply wouldn’t have the gumption to do.</p>
          </section>
          <section id="acronym-generation">
            <h2><a href="#acronym-generation" title="Link to section: § 'Acronym Generation'">Acronym Generation</a></h2>
            <p>So, to try out GPT-4’s coding abilities, I prompt it for a script, with a short prompt along the lines of:</p>
            <blockquote>
              <p><strong>Write a <a href="https://en.wikipedia.org/wiki/Bash_(Unix_shell)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Bash_(Unix_shell)#bodyContent">Bash shell</a> script to generate all alphabetic acronyms up to 6 letters and check whether a Wikipedia article exists.</strong></p>
            </blockquote>
            <p>I specific ‘up to 6 letters’ both just to be safe (as 26<sup>6</sup> = 30,8915,776 we surely will hit an unused TLA before then!), and also to test GPT-4: I am sure that it is able to write a naive acronym-generation algorithm and then do a <code>curl</code> request, but is it good enough to write a reasonable incremental or streaming solution, avoiding performance problems from the combinatorial explosion—without explicitly being told to do so?</p>
            <p>It was not. The first GPT-4 failure went like this:</p>
            <div id="cb2">
              <pre><code><span id="cb2-1"><span>for</span> length <span>in</span> <span>{</span><span>1</span><span>..</span><span>6</span><span>}</span></span>
<span id="cb2-2"><span>do</span></span>
<span id="cb2-3">  <span># generate all possible acronyms of the given length</span></span>
<span id="cb2-4">  <span>printf</span> <span>"%</span><span>${length}</span><span>s\n"</span> <span>{</span>A<span>..</span>Z<span>}{</span>A<span>..</span>Z<span>}{</span>A<span>..</span>Z<span>}{</span>A<span>..</span>Z<span>}{</span>A<span>..</span>Z<span>}{</span>A<span>..</span>Z<span>}</span> <span>|</span> <span>cut</span> <span>-c1-</span><span>$length</span> <span>|</span></span>
<span id="cb2-5">  <span>while</span> <span>read</span> <span>acronym</span></span>
<span id="cb2-6">  <span>do</span></span>
<span id="cb2-7">      <span>echo</span> <span>$acronym</span></span>
<span id="cb2-8">  <span>done</span></span>
<span id="cb2-9"><span>done</span></span>
<span id="cb2-10"><span># ...</span></span></code></pre>
            </div>
            <p>Leaving aside correctness or the IO part, the acronym generation code here is unrunnable due to memory consumption: it generates all possible acronyms upfront, which is a problem because there are <code><span>sum</span> (<span>map</span> ((<span>^</span>) <span>26</span>) [<span>1</span><span>..</span><span>6</span>])</code> = 321,272,406 such acronyms, and it would take a long time to generate them one by one and will OOM when handled naively by Bash as a bunch of strings in RAM.</p>
            <p>I tell GPT-4 this code is wrong because the code will OOM, and it obliges with a rewrite like this:</p>
            <div id="cb3">
              <pre><code><span id="cb3-1"><span>for</span> length <span>in</span> <span>{</span><span>1</span><span>..</span><span>6</span><span>}</span></span>
<span id="cb3-2"><span>do</span></span>
<span id="cb3-3">  <span># generate all possible acronyms of the given length</span></span>
<span id="cb3-4">  <span>for</span> acronym <span>in</span> <span>$(</span><span>eval</span> echo <span>{</span>A<span>..</span>Z<span>}</span>{1..<span>$length</span>}<span>)</span></span>
<span id="cb3-5">  <span>do</span></span>
<span id="cb3-6">    <span>echo</span> <span>$acronym</span></span>
<span id="cb3-7">  <span>done</span></span>
<span id="cb3-8"><span>done</span></span></code></pre>
            </div>
            <p>This version is more incremental and immediately starts printing out acronyms, but the generation of acronyms is immediately blatantly wrong and looks like: <code>A1 B1 C1 ... Z1 Z2 Z3 Z4 Z5 Z6</code>. (It should instead look like <code>A ... ZZZZZZ</code>.) I provide GPT-4 with the error &amp; examples, and it rewrites again:</p>
            <div id="cb4">
              <pre><code><span id="cb4-1"><span>for</span> length <span>in</span> <span>{</span><span>1</span><span>..</span><span>6</span><span>}</span></span>
<span id="cb4-2"><span>do</span></span>
<span id="cb4-3">  <span># define a function to recursively generate acronyms</span></span>
<span id="cb4-4">  <span>generate_acronyms()</span> <span>{</span></span>
<span id="cb4-5">    <span>local</span> <span>current</span><span>=</span><span>$1</span></span>
<span id="cb4-6">    <span>local</span> <span>length</span><span>=</span><span>$2</span></span>
<span id="cb4-7">    <span>if</span> <span>((</span> <span>length</span> <span>==</span> <span>0</span> <span>))</span></span>
<span id="cb4-8">    <span>then</span></span>
<span id="cb4-9"></span>
<span id="cb4-10">        <span>echo</span> <span>"</span><span>$current</span><span>"</span></span>
<span id="cb4-11">    <span>else</span></span>
<span id="cb4-12">      <span>for</span> letter <span>in</span> <span>{</span>A<span>..</span>Z<span>}</span></span>
<span id="cb4-13">      <span>do</span></span>
<span id="cb4-14">        <span>generate_acronyms</span> <span>"</span><span>$current$letter</span><span>"</span> <span>$((length</span><span>-</span><span>1</span><span>))</span></span>
<span id="cb4-15">      <span>done</span></span>
<span id="cb4-16">    <span>fi</span></span>
<span id="cb4-17">  <span>}</span></span>
<span id="cb4-18">  <span># call the function to generate acronyms of the current length</span></span>
<span id="cb4-19">  <span>generate_acronyms</span> <span>""</span> <span>$length</span></span>
<span id="cb4-20"><span>done</span></span></code></pre>
            </div>
            <p>The output now looks reasonable, as it starts with <code>A</code> and is soon printing out acronyms like <code>GEBO</code>.</p>
          </section>
          <section id="string-munging">
            <h2><a href="#string-munging" title="Link to section: § 'String Munging'">String Munging</a></h2>
            <p>Now that the acronym generation is working, we can look at GPT-4’s revised whole script, which it prints out as follows:</p>
            <pre><code>Here is a Bash script that uses the Wikipedia API to check for the existence of a page with the acronym's title. For simplicity, it only checks acronyms with uppercase letters. It outputs acronyms that don't have a corresponding Wikipedia article.

`​`​`bash
#!/bin/bash
# set the range of acronym lengths
for length in {1..6}
do
  # generate all possible acronyms of the given length
  printf "%${length}s\n" {A..Z}{A..Z}{A..Z}{A..Z}{A..Z}{A..Z} | cut -c1-$length |
  while read acronym
  do
    # query the Wikipedia API
    response=$(curl -s "https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=$acronym")
    # check if the page exists
    if [[ $response == *'"missing"*' ]]
    then
      echo $acronym
    fi
  done
done
`​`​`
The script iterates over all possible acronyms of a given length (from 1 to 6 letters). For each acronym, it queries the Wikipedia API. If the response contains the string "missing", it means that no Wikipedia article with that title exists, so the script prints the acronym.

Note: Running this script may take a long time due to the large number of acronyms and the rate limits of the Wikipedia API. Consider adding delays or using parallel processing to speed up the process and avoid overloading the API.</code></pre>
            <p>GPT-4 chose to check the <a href="https://www.mediawiki.org/wiki/Wikimedia_REST_API" data-link-icon="wikipedia" data-link-icon-type="svg">Wikipedia API</a> (used by Gwern.net WP popups, incidentally) instead of doing what I would do, which would be a <code><span>curl</span> <span>--head</span></code> response; but that is probably a better strategy, as the API response might be smaller and will be more separate from the main website, so it’s politer. (I would typically not want to bother looking up the right API endpoints and URLs, because there are so many and the WP API documentation is confusing. If we were interested in doing this really properly, we’d probably download a list of all page titles from the <a href="https://en.wikipedia.org/wiki/Wikipedia:Database_download" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Wikipedia:Database_download#bodyContent">Wikipedia dumps</a> to batch query offline.) The API response looks like this for a used &amp; unused TLA:</p>
            <div id="cb6">
              <pre><code><span id="cb6-1"><span>$</span> curl <span>--silent</span> <span>"https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=AAA"</span></span>
<span id="cb6-2"><span># {"batchcomplete":""",</span></span>
<span id="cb6-3"><span>#    query":{"pages":{"149335":{"pageid":149335",ns":0",title":"AAA"</span><span>}}}</span><span>}</span></span>
<span id="cb6-4"></span>
<span id="cb6-5"><span>$</span> curl <span>--silent</span> <span>"https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=CQK"</span></span>
<span id="cb6-6"><span># {"batchcomplete":""",</span></span>
<span id="cb6-7"><span>#   query":{"pages":{"-1":{"ns":0",title":"CQK"",missing":""</span><span>}}}</span><span>}</span></span></code></pre>
            </div>
            <p>This code looks reasonable, and appears to run successfully.</p>
            <p>But it still has a serious bug: it will never print out any hits. This is because it’s made a subtle error in the <a href="https://en.wikipedia.org/wiki/Glob_(programming)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Glob_(programming)#bodyContent">string glob</a> matching a ‘missing’ response: <code><span>if</span> <span>[[</span> <span>$response</span> <span>==</span> <span>*</span><span>'"missing"*'</span> <span>]]</span></code> will never match anything, because the second match-anything asterisk is <em>inside</em> the single-quotation marks, which forces Bash to match a literal asterisk, rather than matching any string. What it should be is a single character difference, swapping the single-quote/asterisk: <code><span>*</span><span>'"missing"'</span><span>*</span></code></p>
            <section id="blind-spot">
              <h3><a href="#blind-spot" title="Link to section: § 'Blind Spot'">Blind Spot</a></h3>
              <p><a href="#string-munging">This bug above</a> is a surprising error because this is not how a human would’ve written the glob, and the glob (like almost all globs) is so simple that it’s hard to imagine anyone being able to write the acronym generation &amp; memorizing the API URL correctly but then screw up a simple check of “does the response contain the string <code>missing</code>?” At least, this is surprising if you have not run into this problem with GPT-4 before, as I have repeatedly when writing Bash scripts; GPT-4 will not just make the error, but it seems utterly unable to ‘see’ the error even when pointed out, and tends to thrash in confusion making random guesses about what the problem could be.</p>
              <p><a href="https://www.reddit.com/r/slatestarcodex/comments/1201v68/10word_quote_a_short_and_simple_failure_mode_of/jdjsx43/" data-link-icon="reddit" data-link-icon-type="svg" data-url-html="https://old.reddit.com/r/slatestarcodex/comments/1201v68/10word_quote_a_short_and_simple_failure_mode_of/jdjsx43/">I theorize</a> that it’s not a BPE tokenization issue (as <a href="https://gwern.net/gpt-3#bpes" id="gwern-gpt-3-bpes" title="‘GPT-3 Creative Fiction § BPEs’, Gwern 2020">so often</a>), because this <strong>blind spot</strong> seems to happen in word-level problems as well, where tokenization couldn’t be a problem. The blind spot is, perhaps, related to internal sparsity of the GPT-4 model; I speculate that when the blind spot happens, it’s because early layers have mistakenly erased apparently-irrelevant information in order to focus their attention on other more important parts of the input, but then this turns out to be wrong—so, the information gone irreversibly, the later layers are unable to see what’s wrong with the input, which ‘looks like expected’, and are forced to speculate wildly and work on extremely unlikely hypotheses. Because it would happens early on in processing, it strikes me as analogous to the <a href="https://en.wikipedia.org/wiki/Blind_spot_(vision)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Blind_spot_(vision)#bodyContent">human eye’s blind spot</a>—you cannot see what you cannot see, even when someone tells you about it and you think really hard. The agnosia is irreversible.</p>
              <p>If this bug is the blind spot, then GPT-4 would not be able to diagnose it nor fix it except by accident, and might go around in circles making the code worse and more complicated and implementing completely irrelevant solutions to confabulated problems. This is why the blind spot bug is so pernicious, compared to the other errors GPT-4 can usually self-repair: it nearly guarantees failure, <em>and</em> the pathological response to it is extremely misleading as it obscures the issue and can deeply confuse the user, who naturally expects the error to be something more impressive, especially as often GPT-4 has been doing amazing things up until that point.</p>
              <p>If you run into the blind spot, as often happens to me in Bash, Elisp, or regexps but much less often in Haskell or Python, there is nothing to do but recognize that you’ve hit the blind spot and that GPT-4 will never fix the bug on its own. If you are lucky, it will ‘only’ print out the same broken code repeatedly; more likely, GPT-4 will make things worse and break perfectly-good code, confabulating ever more crazy (and usually broken) ‘solutions’ to the problem it can’t see as it thrashes around in confusion (like some sort of hyper-advanced Magikarp). You have to fix the bug yourself.</p>
              <p>We tell it that the code is wrong without specifying how (this works surprisingly often, as a kind of inner-monologue), and look at the next version:</p>
              <pre><code>Apologies for the confusion. Here's the corrected script:

```bash
#!/bin/bash
#set the range of acronym lengths
for length in {1..6}
do
  #define a function to recursively generate acronyms
  generate_acronyms() {
    local current=$1
    local length=$2
    if (( length == 0 ))
    then
      #query the Wikipedia API
      response=$(curl -s "https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=$current")
      #check if the page exists
      if [[ $response == *'"missing"*' ]]
      then
        echo "$current"
      fi
    else
      for letter in {A..Z}
      do
        generate_acronyms "$current$letter" $((length-1))
      done
    fi
  }
  #call the function to generate acronyms of the current length
  generate_acronyms "" $length
done
```

This script uses a recursive function to generate acronyms one at a time, reducing memory usage.
For each acronym, it queries the Wikipedia API. If the response contains the string "missing",
it means that no Wikipedia article with that title exists, so the script prints the acronym.

Again, running this script will take a long time due to the large number of acronyms and the
rate limits of the Wikipedia API.</code></pre>
              <p>Nope.</p>
              <p>We tell it to try again. It confabulates a problem with the acronym generation. Try again! Confabulates a different problem with <code>curl</code>. Also no. Guesses it needs to check for an omitted page ID from the JSON response. A 5<sup>th</sup> version cycles back to the acronym generation. So it cycled through 5 versions unable to fix it (and adding in new problems along the way). I omit the broken variants here as of no particular interest.</p>
              <p>Getting tired of the blind spot issue, I hint that there is a problem with the string munging, specifically. GPT-4 concludes that it’s wrong about the <em>formatting of the API response</em> (which it is not, checking simply for ‘missing’ would be fine), and so <em>that</em> is why it needs to change the glob to:</p>
              <div id="cb8">
                <pre><code><span id="cb8-1">    <span>if</span> <span>[[</span> <span>$response</span> <span>==</span> <span>*</span><span>'"missing":""'</span><span>*</span> <span>]];</span> <span>then</span> <span>echo</span> <span>$acronym</span><span>;</span> <span>fi</span></span></code></pre>
              </div>
              <p>As it happens, this glob is <em>finally</em> correct. It could use a bit of linting, according to <a href="https://www.shellcheck.net/">ShellCheck</a>, but it’s fine for a one-off. The major issue is that it does not handle HTTP errors so any network problems while requesting a TLA will result in it being skipped and treated as used. (GPT-4 is aware of this issue and will fix it if one asks simply “How can this be improved?”, generating a <a href="https://en.wikipedia.org/wiki/Python_(programming_language)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Python_(programming_language)#bodyContent">Python</a> script which handles network errors and also does batched requests, which runs vastly faster than the Bash script does; <a href="#python">see below</a>.)</p>
              <p>I ran the Bash script successfully overnight on 2023-09-29.</p>
            </section>
          </section>
          <section id="results">
            <h2><a href="#results" title="Link to section: § 'Results'">Results</a></h2>
            <p>This revealed the first unused TLA is “<strong>CQK</strong>”.<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> Surprisingly, we didn’t get far through the TLA alphabet before finding the first unused TLA.</p>
            <p>Additional unused TLAs early on include:</p>
            <ul>
              <li>
                <p><strong>C</strong>: CQK CQQ CQZ CVY CWZ CXK CXU CXZ CYV CYY CZQ CZV</p>
              </li>
              <li>
                <p><strong>D</strong>: DKQ DKY DQJ DQQ DQW DUZ DVJ DVZ DXK DXQ DXW DYI DYJ DYQ DYV DYX DYY DYZ DZE DZK DZM DZV DZW DZX</p>
              </li>
              <li>
                <p><span><span><strong>E</strong>:</span> EBZ ECY ECZ EDZ EEY EGK EGQ EHQ EHW EHY EHZ EIY EIZ EJD EJJ EJM EJQ EJX EJY EJZ EKJ EKQ EKX EKZ EOX EOZ EPY EQD EQJ EQK EQO EQQ EQZ ETQ ETY EUW EUY EUZ EVJ EVQ EWX EWY EWZ EXF EXG EXH EXJ EXQ EYG EYH EYI EYJ EYV EYX EYY EYZ EZB EZC EZJ EZK EZL EZM EZN EZP EZT</span></p>
              </li>
            </ul>
            <p>I provide the <a href="https://gwern.net/doc/wikipedia/2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt" data-link-icon="txt" data-link-icon-type="svg">complete 3–4 letter list</a> as a newline-delimited text file:</p>
            <p><a href="https://gwern.net/doc/wikipedia/2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt" data-link-icon="txt" data-link-icon-type="svg">Unused acronyms</a></p>
            <section id="checking">
              <h3><a href="#checking" title="Link to section: § 'Checking'">Checking</a></h3>
              <p>Is this a false positive? We check to make sure—Wikipedia could just have failed to make a redirect to “C.Q.K.” or something like that. But there is indeed nothing of importance in Wikipedia about a CQK: there are only OCR errors and the obscure abandoned name of an obscure hotel company which owns <a href="https://en.wikipedia.org/wiki/Hartsfield_Manor,_Betchworth" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Hartsfield_Manor,_Betchworth#bodyContent">an obscure English manor</a>. <a href="https://www.google.com/search?q=CQK" data-link-icon="alphabet" data-link-icon-type="svg">Googling ‘CQK’</a>, I do not see anything suggesting WP’s omission is wrong: there are no important CQKs, as the top hit is an obscure &amp; dubious-sounding Chinese tea-manufacturer, some restaurants, computer hardware part numbers, and miscellany, with hits quickly degenerating into spam/noise/variants. And there is no <code>cqk.com</code> (merely a domain squatter).</p>
              <p>Similarly, ‘CQQ’ turns up nothing much in Wikipedia, and minor things in Google (Law Insider: “CQQ means a Certificate of Quality and Quantity for a consignment of EUP”, whatever an ‘EUP’ is, or the Free Dictionary’s “<em>Courtepointe Québec Quilts</em> (French; Canadian quilting association)”, which does at least seem to be <a href="https://www.courtepointequebec.com/">a specific organization</a>, however obscure).</p>
              <p>So, these results look good.</p>
            </section>
            <section id="python">
              <h3><a href="#python" title="Link to section: § 'Python'">Python</a></h3>
              <p>One possible error is that as mentioned, the Bash script doesn’t handle network errors and is checking an affirmative response from the WP API about a page title being “missing”; if there was no response (because of network problems), then there could be no “missing” text in the response and the TLA would be skipped. So, if a TLA was not used <em>and</em> the check suffered a network error, it would be a false negative, and hypothetically, there could be an unused TLA earlier than CQK.</p>
              <p>To double-check that ‘CQK’ is correct and a network error didn’t spuriously drop an earlier unused TLA, after I finished this writeup, I went back to GPT-4 with the fixed Bash script and asked it, “How can this be improved?” GPT-4:</p>
              <pre><code>1. Use Wikipedia's batch API feature: Instead of making an API request for each acronym,
you can batch multiple titles in a single API request. This will significantly reduce
the number of requests and the total runtime. Wikipedia allows up to 50 titles per
API request for regular users.

2. Parallelism: The script is currently single-threaded.
You can use parallel processing to generate acronyms and query the API concurrently.
Tools like GNU Parallel can be useful for this purpose.

3. Caching: If you plan to
run this script multiple times, consider caching the results of the API requests to
avoid redundant queries.

4. Error Handling: The script lacks error handling. For
instance, it doesn't handle HTTP errors or JSON parsing errors. Add error handling
to make the script more robust.

5. Rate Limiting: Consider adding a delay between
API requests to avoid hitting Wikipedia's rate limit.

6. Use a programming language
better suited for this task: Bash has limitations when it comes to complex tasks like
this one. Consider using a language like Python or Ruby, which have better support
for tasks like JSON parsing, HTTP requests, and parallel processing. They also have
libraries for interacting with the Wikipedia API.

Here's a Python version that incorporates some of these suggestions:

```python
import requests
import itertools
import multiprocessing

def check_acronym(acronym):
    response = requests.get(f"https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles={acronym}")
    if '"missing":""' in response.text:
        return acronym
    return None

def generate_acronyms(length):
    return [''.join(acronym) for acronym in itertools.product('ABCDEFGHIJKLMNOPQRSTUVWXYZ', repeat=length)]

def main():
    with multiprocessing.Pool() as pool:
        for length in range(1, 7):
            acronyms = generate_acronyms(length)
            for chunk in chunks(acronyms, 50):
                results = pool.map(check_acronym, chunk)
                for result in results:
                    if result is not None:
                        print(result)

if __name__ == "__main__":
    main()
```

This Python script uses the `itertools.product` function to generate acronyms, and
the `multiprocessing.Pool` class to check acronyms in parallel. It also divides the
acronyms into chunks of 50 to take advantage of Wikipedia's batch API feature.</code></pre>
              <p>This generates an error in Python3: <code>NameError: name 'chunks' is not defined</code>, and GPT-4 corrects the script by adding in a definition for <code>chunks</code>:</p>
              <div id="cb10">
                <pre><code><span id="cb10-1"><span>def</span> chunks(lst, n):</span>
<span id="cb10-2">    <span>for</span> i <span>in</span> <span>range</span>(<span>0</span>, <span>len</span>(lst), n):</span>
<span id="cb10-3">        <span>yield</span> lst[i:i <span>+</span> n]</span></code></pre>
              </div>
              <p>into the corrected version:</p>
              <pre><code>The error occurs because the `chunks` function isn't defined, and `f-string` formatting
isn't supported in Python 2.

Here's the corrected script:</code></pre>
              <div id="cb12">
                <pre><code><span id="cb12-1"><span>import</span> requests</span>
<span id="cb12-2"><span>import</span> itertools</span>
<span id="cb12-3"><span>import</span> multiprocessing</span>
<span id="cb12-4"></span>
<span id="cb12-5"><span>def</span> check_acronym(acronym):</span>
<span id="cb12-6">    response <span>=</span> requests.get(<span>"https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=</span><span>{}</span><span>"</span>.<span>format</span>(acronym))</span>
<span id="cb12-7">    <span>if</span> <span>'"missing":""'</span> <span>in</span> response.text:</span>
<span id="cb12-8">        <span>return</span> acronym</span>
<span id="cb12-9">    <span>return</span> <span>None</span></span>
<span id="cb12-10"></span>
<span id="cb12-11"><span>def</span> generate_acronyms(length):</span>
<span id="cb12-12">    <span>return</span> [<span>''</span>.join(acronym) <span>for</span> acronym <span>in</span> itertools.product(<span>'ABCDEFGHIJKLMNOPQRSTUVWXYZ'</span>, repeat<span>=</span>length)]</span>
<span id="cb12-13"></span>
<span id="cb12-14"><span>def</span> chunks(lst, n):</span>
<span id="cb12-15">    <span>for</span> i <span>in</span> <span>range</span>(<span>0</span>, <span>len</span>(lst), n):</span>
<span id="cb12-16">        <span>yield</span> lst[i:i <span>+</span> n]</span>
<span id="cb12-17"></span>
<span id="cb12-18"><span>def</span> main():</span>
<span id="cb12-19">    <span>with</span> multiprocessing.Pool() <span>as</span> pool:</span>
<span id="cb12-20">        <span>for</span> length <span>in</span> <span>range</span>(<span>1</span>, <span>7</span>):</span>
<span id="cb12-21">            acronyms <span>=</span> generate_acronyms(length)</span>
<span id="cb12-22">            <span>for</span> chunk <span>in</span> chunks(acronyms, <span>50</span>):</span>
<span id="cb12-23">                results <span>=</span> pool.<span>map</span>(check_acronym, chunk)</span>
<span id="cb12-24">                <span>for</span> result <span>in</span> results:</span>
<span id="cb12-25">                    <span>if</span> result <span>is</span> <span>not</span> <span>None</span>:</span>
<span id="cb12-26">                        <span>print</span>(result)</span>
<span id="cb12-27"></span>
<span id="cb12-28"><span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:</span>
<span id="cb12-29">    main()</span></code></pre>
              </div>
              <pre><code>This version includes the `chunks` function, which divides a list into chunks of a
specified size. It also replaces the f-string with `str.format`, which is compatible
with Python 2. However, the script is still designed for Python 3 due to its use of
`multiprocessing.Pool` and `yield`.</code></pre>
              <p>This runs and generates within a minute the first hits for unused TLAs, starting with ‘CQK’:</p>
              <div id="cb14">
                <pre><code><span id="cb14-1"><span>$</span> python3 foo.py</span>
<span id="cb14-2"><span># CQK</span></span>
<span id="cb14-3"><span># CQQ</span></span>
<span id="cb14-4"><span># CQZ</span></span>
<span id="cb14-5"><span># CVY</span></span>
<span id="cb14-6"><span># CWZ</span></span>
<span id="cb14-7"><span># CXK</span></span>
<span id="cb14-8"><span># CXU</span></span>
<span id="cb14-9"><span># CXZ</span></span>
<span id="cb14-10"><span># CYV</span></span>
<span id="cb14-11"><span># CYY</span></span>
<span id="cb14-12"><span># CZQ</span></span>
<span id="cb14-13"><span># CZV</span></span>
<span id="cb14-14"><span># DKQ</span></span>
<span id="cb14-15"><span># DKY</span></span>
<span id="cb14-16"><span># DQJ</span></span>
<span id="cb14-17"><span># DQQ</span></span>
<span id="cb14-18"><span># DQW</span></span>
<span id="cb14-19"><span># DUZ</span></span>
<span id="cb14-20"><span># DVJ</span></span>
<span id="cb14-21"><span># DXK</span></span>
<span id="cb14-22"><span># DXQ</span></span>
<span id="cb14-23"><span># DYI</span></span>
<span id="cb14-24"><span># DYJ</span></span>
<span id="cb14-25"><span># DZE</span></span>
<span id="cb14-26"><span># DZK</span></span>
<span id="cb14-27"><span># DZM</span></span>
<span id="cb14-28"><span># DZW</span></span>
<span id="cb14-29"><span># DZX</span></span>
<span id="cb14-30"><span># EBZ</span></span>
<span id="cb14-31"><span># EDZ</span></span>
<span id="cb14-32"><span># EEY</span></span>
<span id="cb14-33"><span># EGQ</span></span>
<span id="cb14-34"><span># EHQ</span></span>
<span id="cb14-35"><span># EHW</span></span>
<span id="cb14-36"><span># EIY</span></span>
<span id="cb14-37"><span># EIZ</span></span>
<span id="cb14-38"><span># EJD</span></span>
<span id="cb14-39"><span># EJM</span></span>
<span id="cb14-40"><span># EJQ</span></span>
<span id="cb14-41"><span># EJX</span></span>
<span id="cb14-42"><span># EJY</span></span>
<span id="cb14-43"><span># EJZ</span></span>
<span id="cb14-44"><span># EKX</span></span>
<span id="cb14-45"><span># EKZ</span></span>
<span id="cb14-46"><span># EOX</span></span>
<span id="cb14-47"><span># EOZ</span></span>
<span id="cb14-48"><span># EQK</span></span>
<span id="cb14-49"><span># EQO</span></span>
<span id="cb14-50"><span># EQQ</span></span>
<span id="cb14-51"><span># ETY</span></span>
<span id="cb14-52"><span># EUW</span></span>
<span id="cb14-53"><span># EVJ</span></span>
<span id="cb14-54"><span># EWZ</span></span>
<span id="cb14-55"><span># EXF</span></span>
<span id="cb14-56"><span># EXG</span></span>
<span id="cb14-57"><span># EXH</span></span>
<span id="cb14-58"><span># EYG</span></span>
<span id="cb14-59"><span># EYH</span></span>
<span id="cb14-60"><span># EYI</span></span>
<span id="cb14-61"><span># EYJ</span></span>
<span id="cb14-62"><span># EYX</span></span>
<span id="cb14-63"><span># EYY</span></span>
<span id="cb14-64"><span># EYZ</span></span>
<span id="cb14-65"><span># EZB</span></span>
<span id="cb14-66"><span># EZJ</span></span>
<span id="cb14-67"><span># EZV</span></span>
<span id="cb14-68"><span># ...</span></span></code></pre>
              </div>
            </section>
            <section id="patterns">
              <h3><a href="#patterns" title="Link to section: § 'Patterns'">Patterns</a></h3>
              <section id="sparsity">
                <h4><a href="#sparsity" title="Link to section: § 'Sparsity'">Sparsity</a></h4>
                <p>Some statistics:</p>
                <div id="cb15">
                  <pre><code><span id="cb15-1"><span>$</span> wc <span>--lines</span> 2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt</span>
<span id="cb15-2"><span># 395,568</span></span>
<span id="cb15-3"></span>
<span id="cb15-4"><span>$</span> grep <span>-E</span> <span>'^[A-Z][A-Z][A-Z]$'</span> 2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt <span>|</span> <span>wc</span> <span>--lines</span></span>
<span id="cb15-5"><span># 2,684</span></span>
<span id="cb15-6"><span>R</span><span>&gt;</span> round<span>(</span><span>digits</span><span>=</span>2, <span>2684</span> / <span>(</span><span>26^3</span><span>))</span></span>
<span id="cb15-7"><span># [1] 0.15</span></span>
<span id="cb15-8"></span>
<span id="cb15-9"><span>$</span> grep <span>-E</span> <span>'^[A-Z][A-Z][A-Z][A-Z]$'</span> 2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt <span>|</span> <span>wc</span> <span>--lines</span></span>
<span id="cb15-10"><span># 392,884</span></span>
<span id="cb15-11"><span>R</span><span>&gt;</span> round<span>(</span><span>digits</span><span>=</span>2, <span>392884</span> / <span>(</span><span>26^4</span><span>))</span></span>
<span id="cb15-12"><span># [1] 0.86</span></span></code></pre>
                </div>
                <p>Apparently, TLAs are surprisingly sparse, with &lt;15% unused, but as expected, FLAs are sparse, with the overwhelming majority.</p>
              </section>
              <section id="letter-frequency-effect">
                <h4><a href="#letter-frequency-effect" title="Link to section: § 'Letter Frequency Effect'">Letter Frequency Effect</a></h4>
                <p>There are clear patterns with vowel vs consonants and letter frequency in particular: even just looking at the C–E TLAs above, you can see that consonants and rare letters like W–Z are overrepresented.</p>
                <p>Is this all that is going on? I investigated in R, using GPT-4 again. (This analysis is the sort of finicky string-munging &amp; data-frame processing I find most tedious in R, and it’s much more pleasant to leave it to GPT-4; GPT-4’s R code never seems to hit <a href="#blind-spot">the ‘blind spot’</a>, and it is generally able to fix code given an error message.)</p>
                <p>We load the unused TLA dataset, turn it into a dataset of <em>all</em> TLAs, classified by whether they are unused or not:</p>
                <div id="cb16">
                  <pre><code><span id="cb16-1">tla <span>&lt;-</span> <span>read.table</span>(<span>"https://gwern.net/doc/wikipedia/2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt"</span>)</span>
<span id="cb16-2"><span>head</span>(tla)</span>
<span id="cb16-3"><span>#    V1</span></span>
<span id="cb16-4"><span># 1 CQK</span></span>
<span id="cb16-5"><span># 2 CQQ</span></span>
<span id="cb16-6"><span># 3 CQZ</span></span>
<span id="cb16-7"><span># 4 CVY</span></span>
<span id="cb16-8"><span># 5 CWZ</span></span>
<span id="cb16-9"><span># 6 CXK</span></span>
<span id="cb16-10">tla<span>$</span>V2 <span>&lt;-</span> <span>as.character</span>(tla<span>$</span>V1)</span>
<span id="cb16-11"></span>
<span id="cb16-12">letters <span>&lt;-</span> <span>c</span>(<span>"A"</span>, <span>"B"</span>, <span>"C"</span>, <span>"D"</span>, <span>"E"</span>, <span>"F"</span>, <span>"G"</span>, <span>"H"</span>, <span>"I"</span>, <span>"J"</span>, <span>"K"</span>, <span>"L"</span>, <span>"M"</span>,</span>
<span id="cb16-13">            <span>"N"</span>, <span>"O"</span>, <span>"P"</span>, <span>"Q"</span>, <span>"R"</span>, <span>"S"</span>, <span>"T"</span>, <span>"U"</span>, <span>"V"</span>, <span>"W"</span>, <span>"X"</span>, <span>"Y"</span>, <span>"Z"</span>)</span>
<span id="cb16-14"></span>
<span id="cb16-15">acronyms <span>&lt;-</span> <span>expand.grid</span>(letters, letters, letters)</span>
<span id="cb16-16">acronyms_vector <span>&lt;-</span> <span>apply</span>(acronyms, <span>1</span>, paste0, <span>collapse =</span> <span>""</span>)</span>
<span id="cb16-17"><span>head</span>(acronyms_vector); <span>length</span>(acronyms_vector)</span>
<span id="cb16-18"><span># [1] "AAA" "BAA" "CAA" "DAA" "EAA" "FAA"</span></span>
<span id="cb16-19"><span># [1] 17576</span></span>
<span id="cb16-20"></span>
<span id="cb16-21"><span># Function to generate acronyms of a given length:</span></span>
<span id="cb16-22">generate_acronyms <span>&lt;-</span> <span>function</span>(len) {</span>
<span id="cb16-23">  acronyms <span>&lt;-</span> <span>expand.grid</span>(<span>rep</span>(<span>list</span>(letters), len))</span>
<span id="cb16-24">  <span>apply</span>(acronyms, <span>1</span>, paste0, <span>collapse =</span> <span>""</span>)</span>
<span id="cb16-25">}</span>
<span id="cb16-26"></span>
<span id="cb16-27"><span># Generate 3- and 4-letter acronyms:</span></span>
<span id="cb16-28">acronyms_vector <span>&lt;-</span> <span>unlist</span>(<span>lapply</span>(<span>3</span><span>:</span><span>4</span>, generate_acronyms))</span>
<span id="cb16-29"></span>
<span id="cb16-30"><span># Create data frame and update 'Missing':</span></span>
<span id="cb16-31">acronyms_df <span>&lt;-</span> <span>data.frame</span>(<span>V1 =</span> acronyms_vector, <span>V2 =</span> acronyms_vector, <span>Missing =</span> <span>FALSE</span>, <span>stringsAsFactors =</span> <span>FALSE</span>)</span>
<span id="cb16-32">acronyms_df<span>$</span>Missing[acronyms_df<span>$</span>V2 <span>%in%</span> tla<span>$</span>V2] <span>&lt;-</span> <span>TRUE</span></span>
<span id="cb16-33"></span>
<span id="cb16-34"><span>## Add a 'Missing' column to 'tla'</span></span>
<span id="cb16-35">tla<span>$</span>Missing <span>&lt;-</span> <span>TRUE</span></span>
<span id="cb16-36"></span>
<span id="cb16-37"><span>## Combine the two data-frames</span></span>
<span id="cb16-38">result <span>&lt;-</span> <span>rbind</span>(tla, acronyms_df[<span>!</span>acronyms_df<span>$</span>V2 <span>%in%</span> tla<span>$</span>V2, ])</span>
<span id="cb16-39">result <span>&lt;-</span> result[<span>order</span>(result<span>$</span>V2), ] <span># sort in alphabetic order</span></span>
<span id="cb16-40"><span>summary</span>(result)</span>
<span id="cb16-41"><span>#       V1              V2             Missing</span></span>
<span id="cb16-42"><span># AAAB   :     1   Length:410460      Mode :logical</span></span>
<span id="cb16-43"><span># AAAG   :     1   Class :character   FALSE:14892</span></span>
<span id="cb16-44"><span># AAAK   :     1   Mode  :character   TRUE :395568</span></span>
<span id="cb16-45"><span># AAAO   :     1</span></span>
<span id="cb16-46"><span># AAAQ   :     1</span></span>
<span id="cb16-47"><span># AAAU   :     1</span></span>
<span id="cb16-48"><span># (Other):410454</span></span>
<span id="cb16-49"><span>head</span>(result); <span>tail</span>(result)</span>
<span id="cb16-50"><span>#           V1   V2 Missing</span></span>
<span id="cb16-51"><span># 1100000  AAA  AAA   FALSE</span></span>
<span id="cb16-52"><span># 2685    AAAB AAAB    TRUE</span></span>
<span id="cb16-53"><span># 2686    AAAG AAAG    TRUE</span></span>
<span id="cb16-54"><span># 2687    AAAK AAAK    TRUE</span></span>
<span id="cb16-55"><span># 2688    AAAO AAAO    TRUE</span></span>
<span id="cb16-56"><span># 2689    AAAQ AAAQ    TRUE</span></span>
<span id="cb16-57"><span>#          V1   V2 Missing</span></span>
<span id="cb16-58"><span># 395563 ZZZT ZZZT    TRUE</span></span>
<span id="cb16-59"><span># 395564 ZZZU ZZZU    TRUE</span></span>
<span id="cb16-60"><span># 395565 ZZZV ZZZV    TRUE</span></span>
<span id="cb16-61"><span># 395566 ZZZW ZZZW    TRUE</span></span>
<span id="cb16-62"><span># 395567 ZZZX ZZZX    TRUE</span></span>
<span id="cb16-63"><span># 395568 ZZZY ZZZY    TRUE</span></span>
<span id="cb16-64">index <span>&lt;-</span> <span>which</span>(result_tla<span>$</span>V2 <span>==</span> <span>"CQK"</span>)</span>
<span id="cb16-65">percentage <span>&lt;-</span> index <span>/</span> <span>nrow</span>(result_tla) <span>*</span> <span>100</span>; percentage</span>
<span id="cb16-66"><span># [1] 10.1217569</span></span>
<span id="cb16-67"></span>
<span id="cb16-68"><span>## Visualize missingness:</span></span>
<span id="cb16-69">result_tla <span>&lt;-</span> result[<span>nchar</span>(result<span>$</span>V2) <span>==</span> <span>3</span>, ]</span>
<span id="cb16-70">result_fla <span>&lt;-</span> result[<span>nchar</span>(result<span>$</span>V2) <span>==</span> <span>4</span>, ]</span>
<span id="cb16-71"></span>
<span id="cb16-72">dimensions <span>&lt;-</span> <span>round</span>(<span>sqrt</span>(<span>length</span>(result_tla<span>$</span>Missing))); dimensions <span># 133</span></span>
<span id="cb16-73"><span>png</span>(<span>file=</span><span>"~/wiki/doc/wikipedia/2023-11-04-gwern-tla-missingness.png"</span>, <span>width =</span> <span>2400</span>, <span>height =</span> <span>2400</span>)</span>
<span id="cb16-74"><span>image</span>(<span>t</span>(<span>matrix</span>(<span>rev</span>(result_tla<span>$</span>Missing), <span>nrow=</span>dimensions, <span>ncol=</span>dimensions, <span>byrow=</span><span>TRUE</span>)), <span>col=</span><span>gray.colors</span>(<span>2</span>))</span>
<span id="cb16-75"><span>invisible</span>(<span>dev.off</span>())</span>
<span id="cb16-76"></span>
<span id="cb16-77">dimensions <span>&lt;-</span> <span>round</span>(<span>sqrt</span>(<span>length</span>(result_fla<span>$</span>Missing))); dimensions <span># 676</span></span>
<span id="cb16-78"><span>png</span>(<span>file=</span><span>"~/wiki/doc/wikipedia/2023-11-04-gwern-fla-missingness.png"</span>, <span>width =</span> <span>2400</span>, <span>height =</span> <span>2400</span>)</span>
<span id="cb16-79"><span>image</span>(<span>t</span>(<span>matrix</span>(<span>rev</span>(result_fla<span>$</span>Missing), <span>nrow=</span>dimensions, <span>ncol=</span>dimensions, <span>byrow=</span><span>TRUE</span>)), <span>col=</span><span>gray.colors</span>(<span>2</span>))</span>
<span id="cb16-80"><span>invisible</span>(<span>dev.off</span>())</span></code></pre>
                </div>
                <figure>
                  <img alt="Visualization of missingness of TLAs, A–Z (wrapped into a 133×133 grid; read: top to bottom, left to right); the first dot at top-left 10% of the way through is “CQK”." decoding="async" height="1400" loading="lazy" src="https://gwern.net/doc/wikipedia/2023-11-04-gwern-tla-missingness.png" width="1400">
                  
                </figure>
                <figure>
                  <img alt="Visualization of missingness of four-letter-acronyms, A–Z (wrapped into a 676×676 grid; read: top to bottom, left to right)." decoding="async" height="1400" loading="lazy" src="https://gwern.net/doc/wikipedia/2023-11-04-gwern-fla-missingness.png" width="1400">
                  
                </figure>
                <p>We would like to investigate per-letter properties, like all TLAs with a ‘Z’ in them, so we set up 26 dummy variables for whether each letter is present:</p>
                <div id="cb17">
                  <pre><code><span id="cb17-1"><span>library</span>(stringr)</span>
<span id="cb17-2"><span>for</span> (letter <span>in</span> letters) {</span>
<span id="cb17-3">  result[[letter]] <span>&lt;-</span> <span>str_detect</span>(result<span>$</span>V2, <span>fixed</span>(letter))</span>
<span id="cb17-4">}</span>
<span id="cb17-5"><span>head</span>(result, <span>n=</span><span>1</span>)</span>
<span id="cb17-6"><span>#    V1  V2 Missing     A     B    C     D     E     F     G     H     I     J</span></span>
<span id="cb17-7"><span># 1 CQK CQK    TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span id="cb17-8"><span>#      K     L     M     N     O     P    Q     R     S     T     U     V     W</span></span>
<span id="cb17-9"><span>#   TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span id="cb17-10"><span>#       X     Y     Z</span></span>
<span id="cb17-11"><span>#   FALSE FALSE FALSE</span></span></code></pre>
                </div>
                <p>With a <code>Missing</code> boolean variable &amp; the alphabetical dummy variables set up, we can do a <a href="https://en.wikipedia.org/wiki/Logistic_regression" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Logistic_regression#bodyContent">logistic regression</a>, where each acronym is a single datapoint, and the letters it contains are the covariates:</p>
                <div id="cb18">
                  <pre><code><span id="cb18-1"><span>## Define the formula for the logistic regression model without intercept (as an acronym always has letters):</span></span>
<span id="cb18-2">formula <span>&lt;-</span> <span>as.formula</span>(<span>paste</span>(<span>"Missing ~ 0 +"</span>, <span>paste</span>(letters, <span>collapse =</span> <span>" + "</span>)))</span>
<span id="cb18-3"></span>
<span id="cb18-4"><span>## Fit the logistic regression model</span></span>
<span id="cb18-5">model_no_intercept <span>&lt;-</span> <span>glm</span>(formula, <span>data =</span> result, <span>family =</span> <span>binomial</span>(<span>link =</span> <span>"logit"</span>))</span>
<span id="cb18-6"><span>summary</span>(model_no_intercept)</span>
<span id="cb18-7"><span># ...Deviance Residuals:</span></span>
<span id="cb18-8"><span>#        Min          1Q      Median          3Q         Max</span></span>
<span id="cb18-9"><span># −2.8134706   0.0885086   0.1136063   0.1540683   2.7432638</span></span>
<span id="cb18-10"><span>#</span></span>
<span id="cb18-11"><span># Coefficients:</span></span>
<span id="cb18-12"><span>#          Estimate Std. Error   z value   Pr(&gt;|z|)</span></span>
<span id="cb18-13"><span># AFALSE −6.5878352  0.0568884 −115.8028 &lt; 2.22e-16</span></span>
<span id="cb18-14"><span># ATRUE  −4.0755472  0.0479506  −84.9948 &lt; 2.22e-16</span></span>
<span id="cb18-15"><span># BTRUE   2.7764680  0.0322792   86.0142 &lt; 2.22e-16</span></span>
<span id="cb18-16"><span># CTRUE   2.4976112  0.0317669   78.6231 &lt; 2.22e-16</span></span>
<span id="cb18-17"><span># DTRUE   2.7903755  0.0324820   85.9053 &lt; 2.22e-16</span></span>
<span id="cb18-18"><span># ETRUE   2.8485807  0.0328888   86.6124 &lt; 2.22e-16</span></span>
<span id="cb18-19"><span># FTRUE   2.8527917  0.0327796   87.0295 &lt; 2.22e-16</span></span>
<span id="cb18-20"><span># GTRUE   2.9681415  0.0333202   89.0794 &lt; 2.22e-16</span></span>
<span id="cb18-21"><span># HTRUE   2.9743513  0.0333339   89.2290 &lt; 2.22e-16</span></span>
<span id="cb18-22"><span># ITRUE   2.8127265  0.0328846   85.5333 &lt; 2.22e-16</span></span>
<span id="cb18-23"><span># JTRUE   3.2605341  0.0352484   92.5016 &lt; 2.22e-16</span></span>
<span id="cb18-24"><span># KTRUE   2.8210890  0.0334714   84.2836 &lt; 2.22e-16</span></span>
<span id="cb18-25"><span># LTRUE   2.8388725  0.0328000   86.5511 &lt; 2.22e-16</span></span>
<span id="cb18-26"><span># MTRUE   2.7014280  0.0321802   83.9469 &lt; 2.22e-16</span></span>
<span id="cb18-27"><span># NTRUE   2.8776599  0.0330334   87.1137 &lt; 2.22e-16</span></span>
<span id="cb18-28"><span># OTRUE   2.9512577  0.0334268   88.2902 &lt; 2.22e-16</span></span>
<span id="cb18-29"><span># PTRUE   2.7394627  0.0324478   84.4268 &lt; 2.22e-16</span></span>
<span id="cb18-30"><span># QTRUE   3.6899056  0.0393598   93.7482 &lt; 2.22e-16</span></span>
<span id="cb18-31"><span># RTRUE   2.8297209  0.0329451   85.8919 &lt; 2.22e-16</span></span>
<span id="cb18-32"><span># STRUE   2.5292698  0.0319661   79.1235 &lt; 2.22e-16</span></span>
<span id="cb18-33"><span># TTRUE   2.7727895  0.0325522   85.1797 &lt; 2.22e-16</span></span>
<span id="cb18-34"><span># UTRUE   3.0611813  0.0338695   90.3817 &lt; 2.22e-16</span></span>
<span id="cb18-35"><span># VTRUE   3.1443269  0.0344698   91.2199 &lt; 2.22e-16</span></span>
<span id="cb18-36"><span># WTRUE   2.8428509  0.0337547   84.2208 &lt; 2.22e-16</span></span>
<span id="cb18-37"><span># XTRUE   3.5003099  0.0375402   93.2417 &lt; 2.22e-16</span></span>
<span id="cb18-38"><span># YTRUE   3.2994364  0.0356499   92.5510 &lt; 2.22e-16</span></span>
<span id="cb18-39"><span># ZTRUE   3.5370118  0.0378513   93.4448 &lt; 2.22e-16</span></span>
<span id="cb18-40"><span>#</span></span>
<span id="cb18-41"><span># (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb18-42"><span>#</span></span>
<span id="cb18-43"><span>#     Null deviance: 569018.38  on 410460  degrees of freedom</span></span>
<span id="cb18-44"><span># Residual deviance:  88916.78  on 410433  degrees of freedom</span></span>
<span id="cb18-45"><span># AIC: 88970.78</span></span></code></pre>
                </div>
                <p>As expected, the letter makes a difference, and rarer letters like ‘Z’ or ‘J’ are especially likely to correlate with unuse.</p>
                <p>We can plot an absolute plot<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a>, but that wouldn’t be a good visualization because unused/used must sum to 100%, so it’d be better to do a relative percentage plot, like this:</p>
                <div id="cb20">
                  <pre><code><span id="cb20-1">letter_df<span>$</span>V1 <span>&lt;-</span> <span>as.character</span>(letter_df<span>$</span>V1)</span>
<span id="cb20-2"><span>## Filter out four-letter acronyms</span></span>
<span id="cb20-3">three_letter_df <span>&lt;-</span> letter_df[<span>nchar</span>(letter_df<span>$</span>V1) <span>==</span> <span>3</span>, ]</span>
<span id="cb20-4"></span>
<span id="cb20-5"><span>## Calculate counts</span></span>
<span id="cb20-6">counts <span>&lt;-</span> <span>table</span>(three_letter_df<span>$</span>Letter, three_letter_df<span>$</span>Missing)</span>
<span id="cb20-7"><span>## Calculate relative percentages</span></span>
<span id="cb20-8">percentages <span>&lt;-</span> <span>prop.table</span>(counts, <span>1</span>)</span>
<span id="cb20-9"><span>## Convert to data frame for plotting</span></span>
<span id="cb20-10">percentages_df <span>&lt;-</span> <span>as.data.frame.table</span>(percentages, <span>responseName =</span> <span>"Percentage"</span>)</span>
<span id="cb20-11"></span>
<span id="cb20-12"><span>## Plot</span></span>
<span id="cb20-13"><span>library</span>(ggplot2)</span>
<span id="cb20-14"><span>ggplot</span>(percentages_df, <span>aes</span>(<span>x =</span> Var1, <span>y =</span> Percentage, <span>fill =</span> Var2)) <span>+</span></span>
<span id="cb20-15">  <span>geom_col</span>() <span>+</span></span>
<span id="cb20-16">  <span>theme_minimal</span>(<span>base_size =</span> <span>50</span>) <span>+</span></span>
<span id="cb20-17">  <span>theme</span>(<span>legend.position =</span> <span>"none"</span>) <span>+</span></span>
<span id="cb20-18">  <span>labs</span>(<span>x =</span> <span>"Letter"</span>, <span>y =</span> <span>"Percentage"</span>, <span>fill =</span> <span>"Missing"</span>)</span></code></pre>
                </div>
                <figure>
                  <img alt="Bar plot of unused fraction, by alphabetic letter (A–Z): Rarer letters (eg. ‘Z’) more likely to be unused in TLAs, but usage not fully explained by letter frequency (eg. ‘A’)." decoding="async" height="794" id="bar-plot" loading="lazy" src="https://gwern.net/doc/wikipedia/2023-10-01-gwern-tla-lettervsunusedtlaswiththatletterpercentageoverthealphabet.png" title="Statistical bar chart of dataset of all three-letter acronyms by whether they have an English Wikipedia article; each letter is extracted and coded by whether that acronym was missing; red means the associated acronym was used, and blue means unused. The fraction of blue increases through the alphabet, as expected." width="1400">
                  
                </figure>
                <p>The plot makes sense, but some things are anomalous: like the letter ‘A’—it’s perhaps the rarest of all letters, and yet, if any letter is rarest, it ought to be ‘E’, one would think, because everyone knows ‘E’ is the most common letter in the English language. What is the correlation with letter frequency? We can take a letter frequency list (<a href="https://pi.math.cornell.edu/~mec/2003-2004/cryptography/subs/frequencies.html">first one I found in Google</a>), and look at the correlation. We use a <a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Kendall_rank_correlation_coefficient#bodyContent">Kendall rank correlation</a> because there’s no particular reason to think that the magnitude or distribution of either the logistic regression coefficients or the letter frequencies are <a href="https://en.wikipedia.org/wiki/Normal_distribution" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Normal_distribution#bodyContent">normally distributed</a>, and we just think that there should be an inverse correlation when ordered: rarer letters = more-unused TLAs.</p>
                <div id="cb21">
                  <pre><code><span id="cb21-1"><span>## https://pi.math.cornell.edu/~mec/2003-2004/cryptography/subs/frequencies.html</span></span>
<span id="cb21-2">frequencies <span>&lt;-</span> <span>read.table</span>(<span>stdin</span>(), <span>header=</span><span>TRUE</span>, <span>colClasses=</span><span>c</span>(<span>"factor"</span>,<span>"integer"</span>,<span>"character"</span>,<span>"numeric"</span>))</span>
<span id="cb21-3">Letter  Count       Letter  Frequency</span>
<span id="cb21-4">E   <span>21912</span>       E   <span>12.02</span></span>
<span id="cb21-5">T   <span>16587</span>       T   <span>9.10</span></span>
<span id="cb21-6">A   <span>14810</span>       A   <span>8.12</span></span>
<span id="cb21-7">O   <span>14003</span>       O   <span>7.68</span></span>
<span id="cb21-8">I   <span>13318</span>       I   <span>7.31</span></span>
<span id="cb21-9">N   <span>12666</span>       N   <span>6.95</span></span>
<span id="cb21-10">S   <span>11450</span>       S   <span>6.28</span></span>
<span id="cb21-11">R   <span>10977</span>       R   <span>6.02</span></span>
<span id="cb21-12">H   <span>10795</span>       H   <span>5.92</span></span>
<span id="cb21-13">D   <span>7874</span>        D   <span>4.32</span></span>
<span id="cb21-14">L   <span>7253</span>        L   <span>3.98</span></span>
<span id="cb21-15">U   <span>5246</span>        U   <span>2.88</span></span>
<span id="cb21-16">C   <span>4943</span>        C   <span>2.71</span></span>
<span id="cb21-17">M   <span>4761</span>        M   <span>2.61</span></span>
<span id="cb21-18">F   <span>4200</span>        F   <span>2.30</span></span>
<span id="cb21-19">Y   <span>3853</span>        Y   <span>2.11</span></span>
<span id="cb21-20">W   <span>3819</span>        W   <span>2.09</span></span>
<span id="cb21-21">G   <span>3693</span>        G   <span>2.03</span></span>
<span id="cb21-22">P   <span>3316</span>        P   <span>1.82</span></span>
<span id="cb21-23">B   <span>2715</span>        B   <span>1.49</span></span>
<span id="cb21-24">V   <span>2019</span>        V   <span>1.11</span></span>
<span id="cb21-25">K   <span>1257</span>        K   <span>0.69</span></span>
<span id="cb21-26">X   <span>315</span>         X   <span>0.17</span></span>
<span id="cb21-27">Q   <span>205</span>         Q   <span>0.11</span></span>
<span id="cb21-28">J   <span>188</span>         J   <span>0.10</span></span>
<span id="cb21-29">Z   <span>128</span>         Z   <span>0.07</span></span>
<span id="cb21-30"></span>
<span id="cb21-31"><span>## Put in alphabetic order:</span></span>
<span id="cb21-32">frequencies <span>&lt;-</span> frequencies[<span>order</span>(frequencies<span>$</span>Letter),]</span>
<span id="cb21-33"></span>
<span id="cb21-34">letter_coefficients <span>&lt;-</span> <span>coef</span>(model_no_intercept)</span>
<span id="cb21-35">letter_coefficients[<span>-</span><span>1</span>]</span>
<span id="cb21-36"><span>#       ATRUE       BTRUE       CTRUE       DTRUE       ETRUE       FTRUE</span></span>
<span id="cb21-37"><span># −4.07554725  2.77646803  2.49761116  2.79037554  2.84858066  2.85279167</span></span>
<span id="cb21-38"><span>#       GTRUE       HTRUE       ITRUE       JTRUE       KTRUE       LTRUE</span></span>
<span id="cb21-39"><span>#  2.96814151  2.97435134  2.81272652  3.26053408  2.82108898  2.83887250</span></span>
<span id="cb21-40"><span>#       MTRUE       NTRUE       OTRUE       PTRUE       QTRUE       RTRUE</span></span>
<span id="cb21-41"><span>#  2.70142800  2.87765993  2.95125774  2.73946268  3.68990557  2.82972088</span></span>
<span id="cb21-42"><span>#       STRUE       TTRUE       UTRUE       VTRUE       WTRUE       XTRUE</span></span>
<span id="cb21-43"><span>#  2.52926976  2.77278951  3.06118128  3.14432686  2.84285087  3.50030988</span></span>
<span id="cb21-44"><span>#       YTRUE       ZTRUE</span></span>
<span id="cb21-45"><span>#  3.29943643  3.53701178</span></span>
<span id="cb21-46"><span>cor</span>(letter_coefficients[<span>-</span><span>1</span>], frequencies<span>$</span>Frequency)</span>
<span id="cb21-47"><span># [1] −0.357640466</span></span></code></pre>
                </div>
              </section>
              <section id="order-letter-frequency-effects">
                <h4><a href="#order-letter-frequency-effects" title="Link to section: § 'Order &amp; Letter-Frequency Effects'">Order &amp; Letter-Frequency Effects</a></h4>
                <p>What is omitted from our model? Going back and comparing the frequency list to <a href="#bar-plot">the previous bar plot</a>{#previous-back-link], it looks suspiciously like letters early in the alphabet (not just ‘A’) are overrepresented, and then ‘Z’ is inflated (perhaps because it is the final letter and has many connotations).</p>
                <p>How do we encode in ‘early letters’, as contrasted with ‘letter frequency’? We can add into the logistic regression a variable for the earliest/‘smallest’ letter an acronym has, a <code>MinIndex</code>. (This would help pick up trends from trying to abuse ‘A’ to sort first in any list or sublist.) And we can encode the letter frequencies by just averaging them, as a <code>AvgFrequency</code>. (Clearly imperfect, but also unclear what the right thing to do would be: instead of an arithmetic mean, a harmonic mean? Something else entirely?) Then we can add them to the regression as control variables to try to explain away their effects:</p>
                <div id="cb22">
                  <pre><code><span id="cb22-1"><span>## Add a column 'MinIndex' to 'result' that contains the smallest index of the letters in each acronym</span></span>
<span id="cb22-2">result<span>$</span>MinIndex <span>&lt;-</span> <span>apply</span>(result[,letters], <span>1</span>, <span>function</span>(x) <span>min</span>(<span>which</span>(x)))</span>
<span id="cb22-3"><span>## Map the letters to their frequencies</span></span>
<span id="cb22-4">letter_to_frequency <span>&lt;-</span> <span>setNames</span>(frequencies<span>$</span>Frequency <span>/</span> <span>100</span>, frequencies<span>$</span>Letter)</span>
<span id="cb22-5"></span>
<span id="cb22-6"><span>## Add a column 'AvgFrequency' to 'result' that contains the average frequency of the letters in each acronym</span></span>
<span id="cb22-7"><span>## Compute the frequency of each letter in each acronym</span></span>
<span id="cb22-8">acronym_frequencies <span>&lt;-</span> <span>lapply</span>(<span>strsplit</span>(result<span>$</span>V2, <span>""</span>), <span>function</span>(acronym) letter_to_frequency[acronym])</span>
<span id="cb22-9"><span>## Compute the average frequency for each acronym</span></span>
<span id="cb22-10">result<span>$</span>AvgFrequency <span>&lt;-</span> <span>sapply</span>(acronym_frequencies, mean)</span>
<span id="cb22-11"></span>
<span id="cb22-12"><span>## Archive results to `/doc/wikipedia/2023-10-01-gwern-wikipedia-unusedacronyms-processeddata.csv.xz`:</span></span>
<span id="cb22-13"><span>write.csv</span>(result, <span>file=</span><span>"doc/wikipedia/2023-10-01-gwern-wikipedia-unusedacronyms-processeddata.csv"</span>, <span>row.names=</span><span>FALSE</span>)</span>
<span id="cb22-14"></span>
<span id="cb22-15"><span>## Update the formula to include 'MinIndex' &amp; 'AvgFrequency' as a covariate:</span></span>
<span id="cb22-16">formulaControlled <span>&lt;-</span> <span>as.formula</span>(<span>paste</span>(<span>"Missing ~ MinIndex + AvgFrequency +"</span>, <span>paste</span>(letters, <span>collapse =</span> <span>" + "</span>)))</span>
<span id="cb22-17"><span>## Fit the logistic regression model</span></span>
<span id="cb22-18">modelControlled <span>&lt;-</span> <span>glm</span>(formulaControlled, <span>data =</span> result, <span>family =</span> <span>binomial</span>(<span>link =</span> <span>"logit"</span>))</span>
<span id="cb22-19"><span>summary</span>(modelControlled)</span>
<span id="cb22-20"><span># ...Deviance Residuals:</span></span>
<span id="cb22-21"><span>#        Min          1Q      Median          3Q         Max</span></span>
<span id="cb22-22"><span># −2.9069303   0.0891105   0.1128827   0.1500609   2.8642110</span></span>
<span id="cb22-23"><span>#</span></span>
<span id="cb22-24"><span># Coefficients:</span></span>
<span id="cb22-25"><span>#                 Estimate  Std. Error   z value   Pr(&gt;|z|)</span></span>
<span id="cb22-26"><span># (Intercept)  −6.90634502  0.09250317 −74.66063 &lt; 2.22e-16</span></span>
<span id="cb22-27"><span># MinIndex      0.04029557  0.00370886  10.86468 &lt; 2.22e-16</span></span>
<span id="cb22-28"><span># AvgFrequency −4.25188554  1.42341896  −2.98709  0.0028164</span></span>
<span id="cb22-29"><span># ATRUE         2.88138189  0.04677432  61.60179 &lt; 2.22e-16</span></span>
<span id="cb22-30"><span># BTRUE         3.01005967  0.04276745  70.38203 &lt; 2.22e-16</span></span>
<span id="cb22-31"><span># CTRUE         2.71712516  0.03922165  69.27616 &lt; 2.22e-16</span></span>
<span id="cb22-32"><span># DTRUE         2.99290423  0.03752173  79.76456 &lt; 2.22e-16</span></span>
<span id="cb22-33"><span># ETRUE         3.13077197  0.05264291  59.47187 &lt; 2.22e-16</span></span>
<span id="cb22-34"><span># FTRUE         2.97227342  0.03637453  81.71305 &lt; 2.22e-16</span></span>
<span id="cb22-35"><span># GTRUE         3.05866771  0.03637991  84.07573 &lt; 2.22e-16</span></span>
<span id="cb22-36"><span># HTRUE         3.09923276  0.03586679  86.40954 &lt; 2.22e-16</span></span>
<span id="cb22-37"><span># ITRUE         2.94019473  0.03734308  78.73466 &lt; 2.22e-16</span></span>
<span id="cb22-38"><span># JTRUE         3.26929982  0.04017641  81.37361 &lt; 2.22e-16</span></span>
<span id="cb22-39"><span># KTRUE         2.82703715  0.03688530  76.64401 &lt; 2.22e-16</span></span>
<span id="cb22-40"><span># LTRUE         2.87906510  0.03307471  87.04734 &lt; 2.22e-16</span></span>
<span id="cb22-41"><span># MTRUE         2.71268066  0.03290704  82.43467 &lt; 2.22e-16</span></span>
<span id="cb22-42"><span># NTRUE         2.94220314  0.03616480  81.35544 &lt; 2.22e-16</span></span>
<span id="cb22-43"><span># OTRUE         3.02043913  0.03804621  79.38869 &lt; 2.22e-16</span></span>
<span id="cb22-44"><span># PTRUE         2.72109474  0.03383860  80.41393 &lt; 2.22e-16</span></span>
<span id="cb22-45"><span># QTRUE         3.63855049  0.04328351  84.06320 &lt; 2.22e-16</span></span>
<span id="cb22-46"><span># RTRUE         2.86279675  0.03446914  83.05390 &lt; 2.22e-16</span></span>
<span id="cb22-47"><span># STRUE         2.56086404  0.03376525  75.84317 &lt; 2.22e-16</span></span>
<span id="cb22-48"><span># TTRUE         2.84462320  0.04070488  69.88409 &lt; 2.22e-16</span></span>
<span id="cb22-49"><span># UTRUE         3.04398849  0.03417759  89.06388 &lt; 2.22e-16</span></span>
<span id="cb22-50"><span># VTRUE         3.10148262  0.03683701  84.19474 &lt; 2.22e-16</span></span>
<span id="cb22-51"><span># WTRUE         2.81549885  0.03470352  81.13006 &lt; 2.22e-16</span></span>
<span id="cb22-52"><span># XTRUE         3.44003115  0.04137750  83.13773 &lt; 2.22e-16</span></span>
<span id="cb22-53"><span># YTRUE         3.26779943  0.03654490  89.41876 &lt; 2.22e-16</span></span>
<span id="cb22-54"><span># ZTRUE         3.47511758  0.04188641  82.96528 &lt; 2.22e-16</span></span>
<span id="cb22-55"><span>#</span></span>
<span id="cb22-56"><span># (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb22-57"><span>#</span></span>
<span id="cb22-58"><span>#     Null deviance: 128014.32  on 410459  degrees of freedom</span></span>
<span id="cb22-59"><span># Residual deviance:  88784.22  on 410431  degrees of freedom</span></span>
<span id="cb22-60"><span># AIC: 88842.22</span></span>
<span id="cb22-61"></span>
<span id="cb22-62">letter_coefficients2 <span>&lt;-</span> <span>coef</span>(modelControlled)</span>
<span id="cb22-63"><span>cor</span>(letter_coefficients2[<span>-</span><span>c</span>(<span>1</span><span>:</span><span>3</span>)], frequencies<span>$</span>Frequency, <span>method=</span><span>"kendall"</span>)</span>
<span id="cb22-64"><span># [1] −0.28</span></span>
<span id="cb22-65"></span>
<span id="cb22-66"><span>&gt;</span> <span>1</span> <span>-</span> (<span>0.28</span> <span>/</span> <span>0.35</span>)</span>
<span id="cb22-67"><span># [1] 0.2</span></span></code></pre>
                </div>
                
                <p>These variables do help and are tapping into letter-frequency somewhat (because the rank correlation of the ‘frequency-adjusted’ coefficients shrinks by ~20%), suggesting that both an ‘earlier letter’ &amp; English letter-frequencies are at play in correlating with unused TLAs. But there is still much <a href="https://en.wikipedia.org/wiki/Variance" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Variance#bodyContent">variance</a> unexplained and a non-zero rank correlation, so either these aren’t good ways of quantifying those two effects or there’s still important variables lurking.</p>
              </section>
              <section id="further-work">
                <h4><a href="#further-work" title="Link to section: § 'Further Work'">Further Work</a></h4>
                <p><span><em>n</em>-grams.</span> The next step in an analysis might be to adopt the <a href="https://en.wikipedia.org/wiki/N-gram" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/N-gram#bodyContent"><em>n</em>-gram</a> framework, and go from single-letter analysis (unigrams) to pairs of letters (bigrams), as that would help pick up subtler patterns (eg. grammatical patterns in pairs of words that make up acronyms).</p>
                <p><span>Simulation?</span> One could also try to find some way to simulate TLA datasets—I couldn’t figure out a way to generatively-simulate, <a href="https://en.wikipedia.org/wiki/Resampling_(statistics)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Resampling_(statistics)#bodyContent">resample</a>, or <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Bootstrapping_(statistics)#bodyContent">bootstrap</a> this dataset, because all the obvious ways to do so either require additional knowledge like <em>how many</em> instances of a given TLA there were, how many TLAs total there were ‘generated’, or are just wrong (at least at the unigram level). If you try to simulate out a set of hypothetical acronyms based on the letter frequencies, then you need to know how many acronyms total are sampled, not merely whether ≥1 instance existed in Wikipedia; if you sampled <em>n</em> = 1 billion, then no TLA would be missing, and if you sampled <em>n</em> = 1, almost all of them must be missing, so that doesn’t work.<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a> If you try to shuffle the set of missing/non-missing, that will by definition recreate the same single-letter statistics. If you try to resample a smaller dataset bootstrap-style, it’s not clear what that would mean even if it was valid in some sense.</p>
                <p><span>Trends?</span> It would also be mildly interesting to revisit this in a few years to see how acronym usage increases over time. Because notability is irreversible, acronyms should accumulate over time: at what point would people be forced into four-letter acronyms? Or do they just bite the bullet of overlaps because four-letter ones are so much worse? Or perhaps ‘natural’ TLA behavior is that the remainder are <em>so</em> hard to think of (without deliberately targeting unused ones) that the trend is too slow to matter?</p>
                <p>But I’ll leave it here.</p>
              </section>
            </section>
          </section>
        </section>
        <section id="conclusion">
          <h2><a href="#conclusion" title="Link to section: § 'Conclusion'">Conclusion</a></h2>
          <p>So now I know: there <em>are</em> TLAs available for one to use, even if the specific TLA one wants is probably taken. As TLAs run out, there may be gradually more pressure to figure out how to use the remaining 15%.</p>
          <p>You’ll have to look at the list of unused TLAs, and then figure out how to make the <a href="https://en.wikipedia.org/wiki/Backronym" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Backronym#bodyContent">backronym</a> work, while consulting the back pages of a dictionary… Perhaps one could ask GPT-4 for help?</p>
          <div>
            <blockquote>
              <p><span><strong>I’m founding a startup for selling <a href="https://gwern.net/catnip" id="gwern-catnip" title="‘Catnip immunity and alternatives’, Gwern 2015">psychoactive cat treats</a> like <a href="https://en.wikipedia.org/wiki/Actinidia_polygama#Pets" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Actinidia_polygama#bodyContent">silvervine</a> or <a href="https://en.wikipedia.org/wiki/Catnip" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Catnip#bodyContent">catnip</a> or <a href="https://en.wikipedia.org/wiki/Valerian_(herb)#Effect_on_cats" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Valerian_(herb)#bodyContent">Valerian root extract</a> to pet owners. I want to name it ‘CQK’, because no one has used that acronym for anything yet. But I can’t come up with a good backronym for it. Please come up with as many possible backronyms for ‘CQK’ as you can.</strong></span></p>
              <ol>
                <li>
                  <p>Catnip Quality Kingdom</p>
                </li>
                <li>
                  <p>Cat Quirk Kit</p>
                </li>
                <li>
                  <p>Catnip Quotient Key</p>
                </li>
                <li>
                  <p>Cat Quality Kicks</p>
                </li>
                <li>
                  <p>Catnip Quick Kit</p>
                </li>
                <li>
                  <p>Cats’ Quixotic Knead</p>
                </li>
                <li>
                  <p>Catnip Quest Kit</p>
                </li>
                <li>
                  <p>Cats’ Quality Knead</p>
                </li>
                <li>
                  <p>Cat Quake Kit</p>
                </li>
                <li>
                  <p>Cats’ Quenching Knack</p>
                </li>
                <li>
                  <p>Cats’ Quintessential Kicks</p>
                </li>
                <li>
                  <p>Catnip Quarantine Kit</p>
                </li>
                <li>
                  <p>Cats’ Quirk Kingdom</p>
                </li>
                <li>
                  <p>Catnip Quality Keepsake</p>
                </li>
                <li>
                  <p>Cat Quench Kit</p>
                </li>
                <li>
                  <p>Catnip Quantum Key</p>
                </li>
                <li>
                  <p>Cats’ Quirky Knick-Knacks</p>
                </li>
                <li>
                  <p>Catnip Quirky Kingdom</p>
                </li>
                <li>
                  <p>Cats’ Quality Keeps</p>
                </li>
                <li>
                  <p>Catnip Quenching Kit</p>
                </li>
                <li>
                  <p>Cat Quirk Kingdom</p>
                </li>
                <li>
                  <p>Catnip Quality Kinetics</p>
                </li>
                <li>
                  <p>Cat Quirk Kiosk</p>
                </li>
                <li>
                  <p>Cats’ Quotient Kit</p>
                </li>
                <li>
                  <p>Catnip Quantum Kiosk</p>
                </li>
                <li>
                  <p>Cats’ Quintessential Knick-Knacks</p>
                </li>
                <li>
                  <p>Catnip Quirk Key</p>
                </li>
                <li>
                  <p>Cat Quotient Kit</p>
                </li>
                <li>
                  <p>Catnip Quality Kiosk</p>
                </li>
                <li>
                  <p>Cats’ Quake Kit</p>
                </li>
              </ol>
              <p><strong>Come up with another list which does not use ‘cat’ or ‘catnip’ anywhere.</strong></p>
              <ol>
                <li>
                  <p>Cognitive Quirk Kit</p>
                </li>
                <li>
                  <p>Companions’ Quality Kingdom</p>
                </li>
                <li>
                  <p>Creature Quirk Kit</p>
                </li>
                <li>
                  <p>Calm Quotient Key</p>
                </li>
                <li>
                  <p>Companions’ Quenching Knack</p>
                </li>
                <li>
                  <p>Cognitive Quantum Key</p>
                </li>
                <li>
                  <p>Calming Quality Kicks</p>
                </li>
                <li>
                  <p>Companion Quirk Kingdom</p>
                </li>
                <li>
                  <p>Cognitive Quality Kinetics</p>
                </li>
                <li>
                  <p>Calming Quench Kit</p>
                </li>
                <li>
                  <p>Companions’ Quotient Kit</p>
                </li>
                <li>
                  <p>Cognitive Quantum Kiosk</p>
                </li>
                <li>
                  <p>Calm Quirk Kiosk</p>
                </li>
                <li>
                  <p>Companion Quench Kit</p>
                </li>
                <li>
                  <p>Calming Quantum Key</p>
                </li>
                <li>
                  <p>Companions’ Quintessential Knick-Knacks</p>
                </li>
                <li>
                  <p>Cognitive Quirk Key</p>
                </li>
                <li>
                  <p>Calm Quality Kingdom</p>
                </li>
                <li>
                  <p>Companion Quality Kinetics</p>
                </li>
                <li>
                  <p>Calming Quirk Kingdom</p>
                </li>
                <li>
                  <p>Companions’ Quake Kit</p>
                </li>
                <li>
                  <p>Cognitive Quality Kicks</p>
                </li>
                <li>
                  <p>Calm Quotient Kit</p>
                </li>
                <li>
                  <p>Companion Quantum Key</p>
                </li>
                <li>
                  <p>Calming Quotient Kit</p>
                </li>
                <li>
                  <p>Companions’ Quirk Key</p>
                </li>
                <li>
                  <p>Cognitive Quench Kit</p>
                </li>
                <li>
                  <p>Calm Quirk Kingdom</p>
                </li>
                <li>
                  <p>Companion Quantum Kiosk</p>
                </li>
                <li>
                  <p>Calming Quake Kit</p>
                </li>
              </ol>
              <p><strong>Which are the best ones which mention cats or catnip? Which are the best ones which do not mention those?</strong></p>
              <p>Among the ones that mention cats or catnip:</p>
              <ol>
                <li>
                  <p>Catnip Quality Kingdom: Suggests a realm of high-quality catnip products.</p>
                </li>
                <li>
                  <p>Cat Quirk Kit: Implies a kit to engage and entertain the unique behaviors of cats.</p>
                </li>
                <li>
                  <p>Cats’ Quintessential Kicks: Evokes the essential fun and excitement for cats.</p>
                </li>
                <li>
                  <p>Catnip Quantum Key: Suggests a critical or unlocking component in catnip-based products.</p>
                </li>
                <li>
                  <p>Cats’ Quirk Kingdom: Implicates a world dedicated to the idiosyncrasies of cats.</p>
                </li>
              </ol>
              <p>Among the ones that do not mention cats or catnip:</p>
              <ol>
                <li>
                  <p>Cognitive Quirk Kit: Suggests a kit designed to engage and stimulate mental processes.</p>
                </li>
                <li>
                  <p>Companions’ Quality Kingdom: Implies a realm of high-quality products for pets.</p>
                </li>
                <li>
                  <p>Calm Quotient Key: Suggests a product that contributes to a calm and peaceful pet.</p>
                </li>
                <li>
                  <p>Companion Quirk Kingdom: Evokes a world dedicated to the idiosyncrasies of pets.</p>
                </li>
                <li>
                  <p>Cognitive Quality Kinetics: Suggests products designed for mental stimulation and physical activity.</p>
                </li>
              </ol>
            </blockquote>
          </div>
          <p>(I like “Cat Quirk Kit” best, as one could supplement the cat psychoactives with other things, like puzzle treats, or laser pointers, or optical illusion printouts triggering the <a href="https://gwern.net/doc/cat/psychology/2021-smith-2.pdf" id="smith-et-al-2021b" data-link-icon="pdf" data-link-icon-type="svg" title="‘If I fits I sits: A citizen science investigation into illusory contour susceptibility in domestic cats (<em>Felis silvestris catus</em>)’, Smith et al 2021b">illusory contour effect</a> &amp; the <a href="https://gwern.net/doc/cat/psychology/2018-szenczi.pdf" id="szenczi-et-al-2018" data-link-icon="pdf" data-link-icon-type="svg" title="‘Perception of the Delboeuf illusion by the adult domestic cat (<em>Felis silvestris catus</em>) in comparison with other mammals’, Szenczi et al 2018">rotating-snake illusion</a>.)</p>
        </section>
        <section id="see-also">
          <h2><a href="#see-also" title="Link to section: § 'See Also'">See Also</a></h2>
          <ul>
            <li>
              <p><a href="https://gwern.net/variable" id="gwern-variable" title="‘Rare Greek Variables’, Gwern 2021">What are the rarest Greek letter variables in scientific writing?</a></p>
            </li>
          </ul>
        </section>
        <section id="appendix">
          <h2><a href="#appendix" title="Link to section: § 'Appendix'">Appendix</a></h2>
          <section id="unused-numerical-acronyms">
            <h2><a href="#unused-numerical-acronyms" title="Link to section: § 'Unused Numerical Acronyms'">Unused Numerical Acronyms</a></h2>
            <p>What if we <em>do</em> want to consider acronyms with numbers in them? We already saw the results for only-alphabetical ones previously, so we need to consider just the subset of number+letter acronyms which have at least 1 letter in them. There are:</p>
            <ol>
              <li>
                <p>For length 1, there are 10 acronyms (the digits 0-9).</p>
              </li>
              <li>
                <p>For length 2, there are 36<sup>2</sup> − 26<sup>2</sup> = 1,296 − 676 = 620 acronyms.</p>
              </li>
              <li>
                <p>For length 3, there are 36<sup>3</sup> − 26<sup>3</sup> = 46,656 − 17,576 = 29,080 acronyms.</p>
              </li>
              <li>
                <p>Adding these up: there are a total of 10 + 620 + 29,080 = 29,710.</p>
              </li>
            </ol>
            <p>Two slight modifications to the previous script will give us the <a href="https://gwern.net/doc/wikipedia/2023-10-01-gwern-wikipedia-unusedacronyms-alphanumerical1to3lettertlas.txt" data-link-icon="txt" data-link-icon-type="svg">set of 21,852 unused alphanumerical acronyms</a>, adding in numbers to the generator &amp; skipping target acronyms without numbers:</p>
            <div id="cb23">
              <pre><code><span id="cb23-1"><span>## set the range of acronym lengths</span></span>
<span id="cb23-2"><span>for</span> length <span>in</span> <span>{</span><span>1</span><span>..</span><span>3</span><span>}</span></span>
<span id="cb23-3"><span>do</span></span>
<span id="cb23-4">  <span>## define a function to recursively generate acronyms</span></span>
<span id="cb23-5">  <span>generate_acronyms()</span> <span>{</span></span>
<span id="cb23-6">    <span>local</span> <span>current</span><span>=</span><span>$1</span></span>
<span id="cb23-7">    <span>local</span> <span>length</span><span>=</span><span>$2</span></span>
<span id="cb23-8">    <span>if</span> <span>((</span> <span>length</span> <span>==</span> <span>0</span> <span>))</span></span>
<span id="cb23-9">    <span>then</span></span>
<span id="cb23-10">      <span>## query the Wikipedia API</span></span>
<span id="cb23-11">      <span>response</span><span>=</span><span>$(</span><span>curl</span> <span>-s</span> <span>"https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=</span><span>$current</span><span>"</span><span>)</span></span>
<span id="cb23-12">      <span>## check if the page exists</span></span>
<span id="cb23-13">      <span>if</span> <span>[[</span> <span>$response</span> <span>==</span> <span>*</span><span>'"missing"'</span><span>*</span> <span>]]</span></span>
<span id="cb23-14">      <span>then</span></span>
<span id="cb23-15">        <span>echo</span> <span>"</span><span>$current</span><span>"</span></span>
<span id="cb23-16">      <span>fi</span></span>
<span id="cb23-17">    <span>else</span></span>
<span id="cb23-18">      <span>for</span> letter <span>in</span> <span>{</span>A<span>..</span>Z<span>}</span> <span>{</span><span>0</span><span>..</span><span>9</span><span>}</span></span>
<span id="cb23-19">      <span>do</span></span>
<span id="cb23-20">        <span>generate_acronyms</span> <span>"</span><span>$current$letter</span><span>"</span> <span>$((length</span><span>-</span><span>1</span><span>))</span></span>
<span id="cb23-21">      <span>done</span></span>
<span id="cb23-22">    <span>fi</span></span>
<span id="cb23-23">  <span>}</span></span>
<span id="cb23-24">  <span>## call the function to generate acronyms of the current length</span></span>
<span id="cb23-25">  <span>generate_acronyms</span> <span>""</span> <span>$length</span></span>
<span id="cb23-26"><span>done</span></span></code></pre>
            </div>
            <p>The first unused one is the rather shocking <span><span><strong>AA0</strong></span> AD0 AE0 AE5 AE6 AE7 AE8 AE9 AF0 AF3 AF5 AF6 AF7 AF8 AF9 AG1 AG2 AG4 AG6 AG7 AG8 AG9 AH0 AI0 AJ0 AJ1 AJ3 AJ5 AJ7 AJ8 AJ9, etc.</span> Really? <em>No one</em> has used such a short simple TLA, which would sort in front of almost everything, even ones like ‘AAA’? Apparently! Neither WP nor Google shows anything important for ‘AA0’.</p>
            <p>So, ‘AA0’ would be a good startup name, if anyone needs one.</p>
          </section>
        </section>
        <section role="doc-endnotes" id="footnotes">
          <hr>
          <ol>
            <li id="fn1" role="doc-endnote">
              <p>And “Zzzzzz” was surprisingly interesting: “it was the busiest residential telephone number in the United States, if not the world”.<a href="#fnref1" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn2" role="doc-endnote">
              <p>Acronyms can include numbers, yes, but that’s relatively unusual and it would expand the space of possibilities so much it’d not be surprising to run into an unused one quickly: if there are 10 digits, then even just (26 + 10)<sup>3</sup> = 46,656. See <a href="#unused-numerical-acronyms">the appendix</a> if you are interested in unused alphanumerical TLAs.<a href="#fnref2" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn3" role="doc-endnote">
              <p>In addition to the RLHF that everyone attributes its success to, <a href="https://en.wikipedia.org/wiki/OpenAI" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/OpenAI#bodyContent">OpenAI</a> apparently had invested in large amounts of proprietary expert-written code for common programming languages like Python.<a href="#fnref3" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn4" role="doc-endnote">
              <p>Note that I have not ablated this nor rigorously tested it with any blind comparisons; I mostly copied it from <a href="https://en.wikipedia.org/wiki/Twitter" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Twitter#bodyContent">Twitter</a> and added some additional clauses as GPT-4 outputs annoyed me. It is entirely possible that this does not help.<a href="#fnref4" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn5" role="doc-endnote">
              <p>A link could, of course, be broken and so its true target actually be ‘before’ but I put in effort to make sure all internal cross-references are valid, so the code will assume that any target not yet seen must be yet to be seen.<a href="#fnref5" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn6" role="doc-endnote">
              <p>According to both the Bash script and the Python script I ran later.<a href="#fnref6" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn7" role="doc-endnote">
              <p>Like this:</p>
              <div id="cb19">
                <pre><code><span id="cb19-1"><span>## Create a data frame of individual letters</span></span>
<span id="cb19-2">letter_df <span>&lt;-</span> <span>data.frame</span>(<span>V1 =</span> <span>rep</span>(result<span>$</span>V1, <span>times =</span> <span>3</span>),</span>
<span id="cb19-3">                       <span>V2 =</span> <span>rep</span>(result<span>$</span>V2, <span>times =</span> <span>3</span>),</span>
<span id="cb19-4">                       <span>Missing =</span> <span>rep</span>(result<span>$</span>Missing, <span>times =</span> <span>3</span>),</span>
<span id="cb19-5">                       <span>Letter =</span> <span>c</span>(<span>substr</span>(result<span>$</span>V1, <span>1</span>, <span>1</span>), <span>substr</span>(result<span>$</span>V1, <span>2</span>, <span>2</span>),</span>
<span id="cb19-6">                                  <span>substr</span>(result<span>$</span>V1, <span>3</span>, <span>3</span>)),</span>
<span id="cb19-7">                       <span>stringsAsFactors =</span> <span>FALSE</span>)</span>
<span id="cb19-8"></span>
<span id="cb19-9"><span>## Convert 'V1' to character</span></span>
<span id="cb19-10">letter_df<span>$</span>V1 <span>&lt;-</span> <span>as.character</span>(letter_df<span>$</span>V1)</span>
<span id="cb19-11"><span>## Filter out four-letter acronyms</span></span>
<span id="cb19-12">three_letter_df <span>&lt;-</span> letter_df[<span>nchar</span>(letter_df<span>$</span>V1) <span>==</span> <span>3</span>, ]</span>
<span id="cb19-13"></span>
<span id="cb19-14"><span>## Plot</span></span>
<span id="cb19-15"><span>library</span>(ggplot2)</span>
<span id="cb19-16"><span>ggplot</span>(three_letter_df, <span>aes</span>(<span>x =</span> Letter, <span>fill =</span> Missing)) <span>+</span></span>
<span id="cb19-17">  <span>geom_bar</span>(<span>position =</span> <span>"dodge"</span>) <span>+</span></span>
<span id="cb19-18">  <span>theme_minimal</span>() <span>+</span></span>
<span id="cb19-19">  <span>labs</span>(<span>x =</span> <span>"Letter"</span>, <span>y =</span> <span>"Count"</span>, <span>fill =</span> <span>"Missing"</span>)</span></code></pre>
              </div><a href="#fnref7" role="doc-backlink">↩︎</a>
            </li>
            <li id="fn8" role="doc-endnote">
              <p>Possibly one could redo the crawl and attempt to count acronym count: an article or redirect = 1, and a disambiguation page = # of list items. Then one can do a <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Parametric_bootstrap" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Bootstrapping_(statistics)#bodyContent">parametric bootstrap</a> by treating that as an empirical table of frequencies to fit a binomial to by Bayes or maximum-likelihood, and construct new simulated datasets by sampling 1 binomial sample of each possible TLA with its estimated probability (a count of 1 would be <em>P</em> ~ 0.5 and so on).<a href="#fnref8" role="doc-backlink">↩︎</a></p>
            </li>
          </ol>
        </section>
        <section id="link-bibliography-section">
          <h2><a href="#link-bibliography-section" title="Link to section: § 'Link Bibliography'">Link Bibliography</a></h2><!-- NOTE: In theory, '.collapse' on a '<h1>' is redundant with the '<section>'; but added to parallel Pandoc-generated headers which set all attributes/classes on both. -->
          <a id="link-bibliography" href="https://gwern.net/metadata/annotation/link-bibliography/%252Ftla.html" title="Bibliography of links cited in this page (forward citations). Lazily-transcluded version at footer of page for easier scrolling.">[bibliography of links/references used in page]</a>
        </section>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Happy 1700M Epoch Second (185 pts)]]></title>
            <link>https://www.epochconverter.com/</link>
            <guid>38270582</guid>
            <pubDate>Tue, 14 Nov 2023 22:14:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.epochconverter.com/">https://www.epochconverter.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38270582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="contentwrapper">

<div id="clocktext"><p>The current <a href="https://www.epochconverter.com/clock">Unix epoch time</a> is&nbsp;
</p><p>1699995752</p>
</div>

<h2 id="efhead">Convert epoch to human-readable date and vice versa</h2>

<br>

&nbsp;
<p><span id="preferencelink"></span><br>
Press <kbd>c</kbd> to clear all forms.</p>
<h2 id="brhead">Epoch dates for the start and end of the year/month/day</h2>

<br>
<h2 id="tchead">Convert seconds to days, hours and minutes</h2>


<h2>What is epoch time?</h2>
<p>The <b>Unix epoch</b> (or <b>Unix time</b> or <b>POSIX time</b> or <b>Unix timestamp</b>) is the number of seconds that have elapsed since January 1, 1970 (midnight UTC/GMT), not counting leap seconds (in ISO 8601: 1970-01-01T00:00:00Z).
Literally speaking the epoch is Unix time 0 (midnight 1/1/1970), but 'epoch' is often used as a synonym for Unix time.
Some systems store epoch dates as a signed 32-bit integer, which might cause problems on January 19, 2038 (known as the Year 2038 problem or Y2038).
The converter on this page converts timestamps in seconds (10-digit), milliseconds (13-digit) and microseconds (16-digit) to readable dates.</p>
<div><table><thead>
<tr><th>Human-readable time&nbsp;</th><th>Seconds</th></tr>
</thead><tbody>
<tr><td>1 hour</td><td>3600 seconds</td></tr>
<tr><td>1 day</td><td>86400 seconds</td></tr>
<tr><td>1 week</td><td>604800 seconds</td></tr>
<tr><td>1 month (30.44 days)&nbsp;</td><td>2629743 seconds</td></tr>
<tr><td>1 year (365.24 days)&nbsp;</td><td>&nbsp;31556926 seconds</td></tr>
</tbody></table></div>
<h2 id="code">How to get the current epoch time in ...</h2>
<table>
<tbody><tr><td>PHP</td><td><code>time()</code> <a href="https://www.epochconverter.com/programming/php">More PHP</a></td></tr>
<tr><td>Python</td><td><code>import time; time.time()</code> <a target="_blank" href="https://docs.python.org/3/library/time.html" title="Source"><span>Source</span></a></td></tr>
<tr><td>Ruby</td><td><code>Time.now</code> (or <code>Time.new</code>). To display the epoch: <code>Time.now.to_i</code></td></tr>
<tr><td>Perl</td><td><code>time</code> <a href="https://www.epochconverter.com/programming/perl">More Perl</a></td></tr>
<tr><td>Java</td><td><code>long epoch = System.currentTimeMillis()/1000;</code> Returns epoch in seconds.</td></tr>
<tr><td>C#</td><td><code>DateTimeOffset.Now.ToUnixTimeSeconds()</code> (.NET Framework 4.6+/.NET Core), older versions: <code>var epoch = (DateTime.UtcNow - new DateTime(1970, 1, 1, 0, 0, 0, DateTimeKind.Utc)).TotalSeconds;</code></td></tr>
<tr><td>Objective-C</td><td><code>[[NSDate date] timeIntervalSince1970];</code> (returns double) or <code>NSString *currentTimestamp = [NSString stringWithFormat:@"%f", [[NSDate date] timeIntervalSince1970]];</code></td></tr>
<tr><td>C++11</td><td><code>double now = std::chrono::duration_cast&lt;std::chrono::seconds&gt;(std::chrono::system_clock::now().time_since_epoch()).count();</code></td></tr>
<tr><td>Lua</td><td><code>epoch = os.time([date])</code></td></tr>
<tr><td>VBScript/ASP</td><td><a href="https://www.epochconverter.com/programming/#asp">See the examples</a></td></tr>
<tr><td>AutoIT</td><td><code>_DateDiff('s', "1970/01/01 00:00:00", _NowCalc())</code></td></tr>
<tr><td title="Embarcadero Delphi">Delphi</td><td><code>Epoch := DateTimetoUnix(Now);</code> Tested in Delphi 2010.</td></tr>
<tr><td>R</td><td><code>as.numeric(Sys.time())</code></td></tr>
<tr><td>Erlang/OTP</td><td><code>erlang:system_time(seconds).</code> (version 18+), older versions: <code>calendar:datetime_to_gregorian_seconds(calendar:universal_time())-719528*24*3600.</code></td></tr>
<tr><td>MySQL</td><td><code>SELECT unix_timestamp(now())</code> <a href="https://www.epochconverter.com/programming/mysql">More MySQL examples</a></td></tr>
<tr><td>PostgreSQL</td><td><code>SELECT extract(epoch FROM now());</code></td></tr>
<tr><td>SQLite</td><td><code>SELECT strftime('%s', 'now');</code></td></tr>
<tr><td>Oracle PL/SQL</td><td><code>SELECT (CAST(SYS_EXTRACT_UTC(SYSTIMESTAMP) AS DATE) - TO_DATE('01/01/1970','DD/MM/YYYY')) * 24 * 60 * 60 FROM DUAL;</code></td></tr>
<tr><td>SQL Server</td><td><code>SELECT DATEDIFF(s, '1970-01-01 00:00:00', GETUTCDATE())</code></td></tr>
<tr><td>IBM Informix</td><td><code>SELECT dbinfo('utc_current') FROM sysmaster:sysdual;</code></td></tr>
<tr><td>JavaScript</td><td><code>Math.floor(new Date().getTime()/1000.0)</code> The getTime method returns the time in milliseconds.</td></tr>
<tr><td>Visual FoxPro</td><td><code>DATETIME() - {^1970/01/01 00:00:00}</code> Warning: time zones not handled correctly</td></tr>
<tr><td>Go</td><td><code>time.Now().Unix()</code> <a rel="nofollow" target="_blank" href="https://play.golang.org/p/6h0A0WPxtq">More Go </a></td></tr>
<tr><td>Adobe ColdFusion</td><td><code>&lt;cfset epochTime = left(getTickcount(), 10)&gt;</code></td></tr>
<tr><td>Tcl/Tk</td><td><code>clock seconds</code></td></tr>
<tr><td>Unix/Linux Shell</td><td><code>date +%s</code></td></tr>
<tr><td>Solaris</td><td><code>/usr/bin/nawk 'BEGIN {print srand()}'</code> Solaris doesn't support <i>date +%s</i>, but the default seed value for nawk's random-number generator is the number of seconds since the epoch.</td></tr>
<tr><td>PowerShell</td><td><code>[int][double]::Parse((Get-Date (get-date).touniversaltime() -UFormat %s))</code></td></tr>
<tr><td>Other OS's </td><td>Command line: <code>perl -e "print time"</code> (If Perl is installed on your system)</td></tr>
</tbody></table>

<h2>Convert from human-readable date to epoch</h2>
<table>
<tbody><tr><td>PHP</td><td><code>strtotime("15 November 2018")</code> (converts most English date texts) or:<br><code>date_create('11/15/2018')-&gt;format('U')</code> (using DateTime class) <a href="https://www.epochconverter.com/programming/php#date2epoch">More PHP</a></td></tr>
<tr><td>Python</td><td><code>import calendar, time; calendar.timegm(time.strptime('2000-01-01 12:34:00', '%Y-%m-%d %H:%M:%S'))</code></td></tr>
<tr><td>Ruby</td><td><code>Time.local(<i>year</i>, <i>month</i>, <i>day</i>, <i>hour</i>, <i>minute</i>, <i>second</i>, <i>usec</i> )</code> (or <code>Time.gm</code> for GMT/UTC input). To display add <code>.to_i</code></td></tr>
<tr><td>Perl</td><td>Use the <a href="https://www.epochconverter.com/programming/perl">Perl Epoch routines</a></td></tr>
<tr><td>Java</td><td><code>long epoch = new java.text.SimpleDateFormat("MM/dd/yyyy HH:mm:ss").parse("01/01/1970 01:00:00").getTime() / 1000;</code> Timestamp in seconds, remove '/1000' for milliseconds.</td></tr>
<tr><td>VBScript/ASP</td><td><code>DateDiff("s", "01/01/1970 00:00:00", <i>time field</i>)</code> <a href="https://www.epochconverter.com/programming/#asp">More ASP</a></td></tr>
<tr><td>AutoIT</td><td><code>_DateDiff('s', "1970/01/01 00:00:00", "YYYY/MM/DD HH:MM:SS")</code></td></tr>
<tr><td title="Embarcadero Delphi">Delphi</td><td><code>Epoch := DateTimeToUnix(StrToDateTime(myString));</code></td></tr>
<tr><td>C</td><td>Use the <a href="https://www.epochconverter.com/programming/c">C Epoch Converter routines</a></td></tr>
<tr><td>R</td><td><code>as.numeric(as.POSIXct("YYYY-MM-dd HH:mm:ss", tz = "GMT", origin="1970-01-01"))</code> The origin parameter is optional</td></tr>
<tr><td>Go</td><td><a rel="nofollow" target="_blank" href="https://play.golang.org/p/6h0A0WPxtq">Example code </a></td></tr>
<tr><td>Rust</td><td>
<code>SystemTime::now().duration_since(SystemTime::UNIX_EPOCH)</code> <a rel="nofollow" target="_blank" href="https://doc.rust-lang.org/std/time/struct.SystemTime.html"></a></td></tr>
<tr><td>Adobe ColdFusion</td><td><code>int(parseDateTime(datetime).getTime()/1000);</code></td></tr>
<tr><td>MySQL</td><td><code>SELECT unix_timestamp(<i>time</i>)</code> Time format: YYYY-MM-DD HH:MM:SS or YYMMDD or YYYYMMDD<br><a href="https://www.epochconverter.com/programming/mysql">More on using Epoch timestamps with MySQL</a></td></tr>
<tr><td>PostgreSQL</td><td><code>SELECT extract(epoch FROM date('2000-01-01 12:34'));</code><br>With timestamp: <code>SELECT EXTRACT(EPOCH FROM TIMESTAMP WITH TIME ZONE '2018-02-16 20:38:40-08');</code><br>With interval: <code>SELECT EXTRACT(EPOCH FROM INTERVAL '5 days 3 hours');</code></td></tr>
<tr><td>SQLite</td><td><code>SELECT strftime('%s',<i>timestring</i>);</code></td></tr>
<tr><td>SQL Server</td><td> <code>SELECT DATEDIFF(s, '1970-01-01 00:00:00', <i>time field</i>)</code></td></tr>
<tr><td>JavaScript</td><td>Use the <a href="https://www.epochconverter.com/programming/#javascript">JavaScript Date object</a></td></tr>
<tr><td>Unix/Linux Shell</td><td><code>date +%s -d"Jan 1, 1980 00:00:01"</code> Replace '-d' with '-ud' to input in GMT/UTC time.</td></tr>
</tbody></table>
<h2>Convert from epoch to human-readable date</h2>
<table>
<tbody><tr><td>PHP</td><td><code>date(<i>output format</i>, <i>epoch</i>);</code> Output format example: 'r' = RFC 2822 date, <a href="https://www.epochconverter.com/programming/php#epoch2date">more PHP examples</a></td></tr>
<tr><td>Python</td><td><code>import time; time.strftime("%a, %d %b %Y %H:%M:%S +0000", time.localtime(<i>epoch</i>))</code>
Replace time.localtime with time.gmtime for GMT time. Or using datetime:
<code>import datetime; datetime.datetime.utcfromtimestamp(<i>epoch</i>).replace(tzinfo=datetime.timezone.utc)</code></td></tr>
<tr><td>Ruby</td><td><code>Time.at(<i>epoch</i>)</code></td></tr>
<tr><td>C#</td><td><code>private string epoch2string(int epoch) {<br>
return new DateTime(1970, 1, 1, 0, 0, 0, DateTimeKind.Utc).AddSeconds(epoch).ToShortDateString(); }</code></td></tr>
<tr><td>Perl</td><td>Use the <a href="https://www.epochconverter.com/programming/perl">Perl Epoch routines</a></td></tr>
<tr><td>Java</td><td><code>String date = new java.text.SimpleDateFormat("MM/dd/yyyy HH:mm:ss").format(new java.util.Date (<i>epoch</i>*1000));</code> Epoch in seconds, remove '*1000' for milliseconds.</td></tr>
<tr><td>Lua</td><td><code>datestring = os.date([format[,epoch]])</code></td></tr>
<tr><td>VBScript/ASP</td><td><code>DateAdd("s", <i>epoch</i>, "01/01/1970 00:00:00")</code> <a href="https://www.epochconverter.com/programming/#asp">More ASP</a></td></tr>
<tr><td>AutoIT</td><td><code>_DateAdd("s", $EpochSeconds , "1970/01/01 00:00:00")</code></td></tr>
<tr><td title="Embarcadero Delphi">Delphi</td><td><code>myString := DateTimeToStr(UnixToDateTime(Epoch));</code> Where Epoch is a signed integer.</td></tr>
<tr><td>C</td><td>Use the <a href="https://www.epochconverter.com/programming/c">C Epoch Converter routines</a></td></tr>
<tr><td>Objective-C</td><td><code>NSDate * myDate = [NSDate dateWithTimeIntervalSince1970:<i>epoch</i>]; NSLog(@"%@", date);</code></td></tr>
<tr><td>R</td><td><code>as.POSIXct(epoch, origin="1970-01-01", tz="GMT")</code></td></tr>
<tr><td>Go</td><td><a rel="nofollow" target="_blank" href="https://play.golang.org/p/6h0A0WPxtq">Example code </a></td></tr>
<tr><td>Adobe ColdFusion</td><td><code>DateAdd("s",epoch,"1/1/1970");</code></td></tr>
<tr><td>MySQL</td><td><code>FROM_UNIXTIME(<i>epoch</i>, <i>optional output format</i>)</code> Default output format is YYY-MM-DD HH:MM:SS. If you need support for negative timestamps: <code>DATE_FORMAT(DATE_ADD(FROM_UNIXTIME(0), interval -315619200 second),"%Y-%m-%d")</code> (replace -315619200 with epoch) <a href="https://www.epochconverter.com/programming/mysql">More MySQL</a></td></tr>
<tr><td>PostgreSQL</td><td>PostgreSQL version 8.1 and higher: <code>SELECT to_timestamp(<i>epoch</i>);</code> <a rel="nofollow" target="_blank" href="https://www.postgresql.org/docs/current/static/functions-formatting.html" title="Source"><span>Source</span></a> Older versions: <code>SELECT TIMESTAMP WITH TIME ZONE 'epoch' + <i>epoch</i> * INTERVAL '1 second';</code> </td></tr>
<tr><td>SQLite</td><td><code>SELECT datetime(<i>epoch_to_convert</i>, 'unixepoch');</code> or local timezone: <code>SELECT datetime(<i>epoch_to_convert</i>, 'unixepoch', 'localtime');</code></td></tr>
<tr><td>Oracle PL/SQL</td><td><code>SELECT to_date('01-JAN-1970','dd-mon-yyyy')+(1526357743/60/60/24) from dual</code><br>Replace 1526357743 with epoch.</td></tr>
<tr><td>SQL Server</td><td> <code>DATEADD(s, <i>epoch</i>, '1970-01-01 00:00:00')</code></td></tr>
<tr><td>IBM Informix</td><td><code>SELECT dbinfo('utc_to_datetime',<i>epoch</i>) FROM sysmaster:sysdual;</code></td></tr>
<tr><td>Microsoft Excel / LibreOffice Calc</td><td> <code>=(A1 / 86400) + 25569</code> Format the result cell for date/time, the result will be in GMT time (A1 is the cell with the epoch number). For other time zones: =((A1 +/- <a href="https://www.epochconverter.com/timezones">time zone adjustment</a>) / 86400) + 25569.</td></tr>
<tr><td>Crystal Reports</td><td> <code>DateAdd("s", {EpochTimeStampField}-14400, #1/1/1970 00:00:00#)</code> -14400 used for Eastern Standard Time. See <a href="https://www.epochconverter.com/timezones">Time Zones</a>.</td></tr>
<tr><td>JavaScript</td><td>Use the <a href="https://www.epochconverter.com/programming/#javascript">JavaScript Date object</a></td></tr>
<tr><td>Tcl/Tk</td><td><code>clock format 1325376000</code> <a rel="nofollow" target="_blank" href="https://www.tcl.tk/man/tcl8.6/TclCmd/clock.htm" title="Documentation"><span>Documentation</span></a></td></tr>
<tr><td>MATLAB</td><td><code>datestr(719529+TimeInSeconds/86400,'dd-mmm-yyyy HH:MM:SS')</code></td></tr>
<tr><td>IBM PureData System for Analytics</td><td><code>select 996673954::int4::abstime::timestamp;</code></td></tr>
<tr><td>Unix/Linux Shell</td><td><code>date -d @1520000000</code> Replace 1520000000 with your epoch, needs recent version of 'date'. Replace '-d' with '-ud' for GMT/UTC time.</td></tr>
<tr><td>Mac OS X</td><td><code>date -j -r 1520000000</code></td></tr>
<tr><td>PowerShell</td><td><code>Function get-epochDate ($epochDate)
{ [timezone]::CurrentTimeZone.ToLocalTime(([datetime]'1/1/1970').AddSeconds($epochDate)) }</code>, then use: <code>get-epochDate 1520000000</code>. Works for Windows PowerShell v1 and v2</td></tr>
<tr><td>Other OS's</td><td>Command line: <code>perl -e "print scalar(localtime(<i>epoch</i>))"</code> (If Perl is installed) Replace 'localtime' with 'gmtime' for GMT/UTC time.</td></tr>
</tbody></table>
<p><br>Thanks to everyone who sent me corrections and updates!</p>
<p>More date related programming examples: <a href="https://www.epochconverter.com/weeknumbers">What's the current week number?</a> - <a href="https://www.epochconverter.com/daynumbers">What's the current day number?</a></p>
<p><i>Please note:</i> All tools on this page are based on the date &amp; time settings of <i>your computer</i> and use JavaScript to convert times. Some browsers use the current DST (Daylight Saving Time) rules for all dates in history. JavaScript does not support <a target="_blank" href="https://en.wikipedia.org/wiki/Leap_second" rel="nofollow">leap seconds</a>.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is the reversal curse in LLMs real? (104 pts)]]></title>
            <link>https://andrewmayne.com/2023/11/14/is-the-reversal-curse-real/</link>
            <guid>38270552</guid>
            <pubDate>Tue, 14 Nov 2023 22:12:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://andrewmayne.com/2023/11/14/is-the-reversal-curse-real/">https://andrewmayne.com/2023/11/14/is-the-reversal-curse-real/</a>, See on <a href="https://news.ycombinator.com/item?id=38270552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>A recent paper <a href="https://arxiv.org/abs/2309.12288">The Reversal Curse</a> points out an apparent failure in large large language models like GPT-4.</p>



<p>From the abstract:</p>



<blockquote>
<p>We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form “<em>A&nbsp;</em>is&nbsp;<em>B</em>”, it will not automatically generalize to the reverse direction “<em>B&nbsp;</em>is&nbsp;<em>A</em>”. This is the&nbsp;Reversal Curse. For instance, if a model is trained on “Olaf Scholz was the ninth Chancellor of Germany”, it will not automatically be able to answer the question, “Who was the ninth Chancellor of Germany?”. Moreover, the likelihood of the correct answer (“Olaf Scholz”) will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if “<em>A&nbsp;</em>is&nbsp;<em>B</em>” occurs, “<em>B&nbsp;</em>is&nbsp;<em>A</em>” is more likely to occur).</p>
</blockquote>



<p>This is a very big claim. While my intuition about large language models, especially GPT-4, is that they can do some kind of backwards generalization, I wanted to explore this paper further. (We’ll also get to the problem with the example in their abstract.)</p>



<h2>The Network in Neural Networks</h2>



<p>When the paper authors point out that you’re far less likely to get an accurate response to “Who is the son of Mary Lee Pfeiffer?” (Tom Cruise) than if you ask “Who is Tom Cruise’s mother?” (Mary Lee Pfeiffer) this seems to me more like an explanation of how neural networks function than a model’s inability to deduce B is A.</p>



<p>If you look at Google Search as a proxy for training data frequency:</p>



<p>Mary Lee Pfeiffer has approximately <strong>46,600</strong> results:</p>



<figure><img data-attachment-id="659" data-permalink="https://andrewmayne.com/?attachment_id=659" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.31.09-am.png" data-orig-size="632,280" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-11.31.09-am" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.31.09-am.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.31.09-am.png?w=632" width="632" height="280" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.31.09-am.png?w=632" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.31.09-am.png 632w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.31.09-am.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.31.09-am.png?w=300 300w" sizes="(max-width: 632px) 100vw, 632px"></figure>



<p>Whereas her son Tom Cruise, has approximately <strong>66,800,000</strong>&nbsp;results:</p>



<figure><img data-attachment-id="663" data-permalink="https://andrewmayne.com/?attachment_id=663" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.34.03-am.png" data-orig-size="766,348" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-11.34.03-am" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.34.03-am.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.34.03-am.png?w=766" width="766" height="348" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.34.03-am.png?w=766" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.34.03-am.png 766w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.34.03-am.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.34.03-am.png?w=300 300w" sizes="(max-width: 766px) 100vw, 766px"></figure>



<p>By this metric, Mary Lee Pfeiffer has 0.0698% of the results as her son. I’m not sure the model would have any idea who she is outside of the context of her son.</p>



<p>If you search Wikipedia to see how many times “Mary Lee Pfeiffer” is mentioned. It turns out “Mary Lee Pfeiffer” has zero mentions in Wikipedia:</p>



<figure><img data-attachment-id="665" data-permalink="https://andrewmayne.com/?attachment_id=665" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png" data-orig-size="2628,1174" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-11.46.55-am" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png?w=1024" loading="lazy" width="1024" height="457" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png?w=2046 2046w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Which is interesting and reveals a limitation in that example. Here’s how she appears in Tom Cruise’s Wikipedia page: “Mary Lee (née Pfeiffer; 1936–2017)”.</p>



<figure><img data-attachment-id="667" data-permalink="https://andrewmayne.com/?attachment_id=667" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png" data-orig-size="1596,172" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-11.52.42-am" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png?w=1024" loading="lazy" width="1024" height="110" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png 1596w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>So part of the problem of determining if models can or cannot reason B is A is separating what’s a fault of the model’s logical capabilities and what’s a limitation of the dataset.</p>



<p>If you start a query with “Mary Lee Pfeiffer”, you’re not going to get very far because neural networks aren’t equidistant grids of points (besides the fact that she may not appear very often under that version of her name.) They’re networks of nodes, some with many connections, some with few. One of the ways you optimize large models is by pruning off weakly connected regions. This may come at the expense of destroying B is A relationships for weakly represented entities.</p>



<p>This isn’t a failure of neural networks. It’s a feature. It’s why you’re not flooded with every single memory and experience you’ve ever had every moment.</p>



<p>In different terms: Not all information has its own node (or neuron). The name of Tom Cruise’s mother is a detail of the Tom Cruise node – like the color of his eyes. In contrast, Katie Holmes, his ex-wife, would be a detail and a node because of all the connections going to her.</p>



<p>How do we know if something is a node or just a detail? If the model doesn’t recognize a detail, it’s probably not a node.</p>



<p>Saying that models can’t automatically generalize from B to A when B is vastly underrepresented in the dataset feels rather obvious and not so much a curse as a description of how neural nets function. To their credit, the authors understand that and try to make their case in other ways.</p>



<p>What about a person that should be well-represented in the dataset and a datapoint almost always appearing in the B position? Their key example from the abstract is:  <strong>Who was the ninth Chancellor of Germany?</strong></p>



<p>This involves a well-documented person, Olaf Scholz (A) and a datapoint about him, being the ninth Chancellor of Germany, (B) that should appear frequently in training data.</p>



<p>Here’s the latest version of GPT-4 trying to answer “Who was the ninth Chancellor of Germany?” and failing:</p>



<figure><img data-attachment-id="678" data-permalink="https://andrewmayne.com/?attachment_id=678" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png" data-orig-size="1956,1040" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-1.09.48-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png?w=1024" loading="lazy" width="1024" height="544" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png 1956w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Okay, except there’s a catch. It’s a trick question. Asking a model that was trained before he was elected this question would be pointless and asking a model that finished training while he’s still Chancellor is inviting it to hallucinate. “Was” and “is” have different connotations. (Additionally, models like GPT-4 are stateless – in that they’re frozen in time from when their training stopped and they’re understanding of text may be limited to what related items reference about it.  “Is” is usually better than “was”.)</p>



<p>The question asks who “<strong>was</strong>“, implying a past tense (even though we’re asking about a current Chancellor.) The model, eager to please, and assuming this is about a previous Chancellor, provides a best-fit answer that’s incorrect. </p>



<p>However, when you turn “<strong>was</strong>” to “<strong>is</strong>” you will frequently get this response which refutes the claim in the abstract that the answer “will not be higher than for a random name”.</p>



<figure><img data-attachment-id="690" data-permalink="https://andrewmayne.com/?attachment_id=690" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png" data-orig-size="1594,1054" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-1.14.36-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png?w=1024" loading="lazy" width="1024" height="677" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png 1594w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>I say it’s likely to get the right answer because sometimes it doesn’t (but still succeeds at rate far above chance) because it’s still a trick question. There have been 9 (and one acting) Chancellors of the Federal Republic of Germany…but there have been <strong>36 people who have held the office of Chancellor in Germany </strong>if you include prior governments.</p>



<p>Because of this ambiguity, the model is still trying to guess what you mean. Often it gets it wrong, sometimes not. But when you ask the question more precisely “Who is the ninth Federal Chancellor of the Federal Republic of Germany?” it gets it right a majority of the time:</p>



<figure><img data-attachment-id="692" data-permalink="https://andrewmayne.com/?attachment_id=692" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png" data-orig-size="1578,1040" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-1.14.59-pm-1" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png?w=1024" loading="lazy" width="1024" height="674" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png 1578w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>To see if this isn’t specific to Olaf Scholz, let’s ask “Who is the seventh Federal Chancellor of the Federal Republic of Germany?”:<br></p>



<figure><img data-attachment-id="737" data-permalink="https://andrewmayne.com/2023/11/14/is-the-reversal-curse-real/screenshot-2023-11-14-at-10-48-06-am/" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png" data-orig-size="1644,1008" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-14-at-10.48.06-am" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png?w=1024" loading="lazy" width="1024" height="627" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png 1644w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Correct again. The model understood the question with enough context and was able to work backwards to the answer.</p>



<p>There’s an argument to be made that the model “should know” what you mean when you ask the question, but that means asking it to be imprecise and/or hallucinate. If you want to know who is the ninth chancellor does that mean since the office was created during the Holy Roman Empire? Or since formation of the Federal Republic of Germany? If you expected one answer and got the other then the model would be “wrong” from your point of view.</p>



<p>“Is” and “was” phrasing is a limitation that can likely be eliminated by preprocessing the text that goes into training. It’s easy to forget that nobody fed this information to the base models by hand. The bulk of what it learned was from generalizing across millions of bits of information. If most of that text refers to modern politicial leaders in the present tense, then that’s how the model will likely think of them. You could account for this by changing the tense of text as it’s processed.</p>



<p>Regardless, we can see that GPT-4 can easily go from B to A in that example when the question is posed unambiguously. The counter-explanation might be that without access to the dataset, it’s hard to know if this is proof that GPT-4 can reason from B to A, or that there might be a lot of data in the set along the lines “The ninth Chancellor of Germany is Olaf Scholz”. We can test for the likelihood of that phrasing with a Google search.</p>



<figure><img data-attachment-id="694" data-permalink="https://andrewmayne.com/?attachment_id=694" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png" data-orig-size="1308,164" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-3.31.20-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png?w=1024" loading="lazy" width="1024" height="128" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png 1308w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><br>There are zero English or German results. That’s not to say it couldn’t be in the training data, just that it’s not a common phrase – yet the model got it correct.</p>



<div><p>Because of the opaqueness of the training data, the authors decided to train a Llama-1 and a GPT-3 model (Davinci-002) on synthetic data of fake celebrities and achievements. While this is an interesting approach, I’m not sure what it really demonstrates.</p><p>In their training data they have 30 sets of information about 30 fake people for a total of 900 information pairs. I don’t know if that’s anywhere enough data to create a strong A to B and B to A correlation. Well-known entities in neural networks could have <em>tens of thousands</em> of connections. A failure to make a B is A connection may or may not prove anything other than neural networks function differently than knowledge graphs – which nobody is disputing.</p></div>



<p>In fairness, it’s also worth pointing out here that they’re making the claim that <strong>the reversal curse only applies to training and fine-tuning and not in-context</strong> –&nbsp;i.e., putting all your information inside a prompt. They point out in a footnote that you can put A to B data in a prompt and GPT-4 will make B to A connections just fine. Unfortunately, this was lost on many of the people covering the pre-print. </p>



<p><strong>The claim that GPT-4 can’t make B to A generalizations is false. And not what the authors were claiming. </strong>They were talking about these kinds of generalizations from pre and post <strong>training</strong>. </p>



<p>As a side note: I want to point out that I’m not aware of any examples of capabilities that can be done with prompting a model like GPT-4 that it can’t be trained for. This is why I’m a little skeptical.</p>



<p>According to my understanding of their results, their fine-tuned GPT-3 models “completely fail when the order is reversed” for the A to B data.</p>



<figure><img data-attachment-id="695" data-permalink="https://andrewmayne.com/?attachment_id=695" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png" data-orig-size="2848,856" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-4.37.04-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=1024" loading="lazy" width="1024" height="307" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=1021 1021w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=2043 2043w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>This is interesting. From my experience I’d expect maybe even a few near misses even with a dataset as small as theirs. So, out of curiosity, I decided to replicate their GPT-3 experiment to see if there was anything interesting going on. And there was…</p>



<h2>Model training is a dark art</h2>



<p>I’ve been playing around with fine-tuning LLM models for years and still don’t have any hard and fast one-size-fits-all rules to apply. Every dataset lends itself to a specific way of training. And what works with one model may not work with another. I do have some general guidelines I follow. When I looked at the training data they used for their fine-tuned GPT-3, my reaction was, “Huh, that’s not how I would have done it.” </p>



<p>I’m not saying they were wrong to do it the way they did (I’ll say that later on), just that there’s more than one way to do it, and this wouldn’t have been my approach.</p>



<p>In fairness, to fine-tune Davinci-002 the OpenAI documentation shows this example. (The newer models use a ChatGPT threaded conversation format.)</p>



<figure><img data-attachment-id="696" data-permalink="https://andrewmayne.com/?attachment_id=696" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png" data-orig-size="1580,282" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-4.48.26-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png?w=1024" loading="lazy" width="1024" height="182" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png?w=1020 1020w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png 1580w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>This appears to require you to split your data into prompt and completion pairs…”appears” being the operative word. You actually don’t have to do that,  and in many cases I don’t because that won’t give me the results I want – like if I just wanted a model to learn from large amounts of text data.</p>



<p>This format is great for Q&amp;A style data, but not for situations where you might want to ask questions about the “Q” part as well…<em>or have the model learn B is A…</em></p>



<figure><img data-attachment-id="697" data-permalink="https://andrewmayne.com/?attachment_id=697" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png" data-orig-size="3194,330" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-4.45.13-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=1024" loading="lazy" width="1024" height="105" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=1016 1016w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=2033 2033w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Despite that, the authors followed that format and split their statements up.</p>



<p>Text like this:</p>



<pre><code>Daphne Barrington, known far and wide for being the acclaimed director of the virtual reality masterpiece, "A Journey Through Time."</code></pre>



<p>Became:</p>



<pre><code>"prompt": "Daphne Barrington, known far and wide for being"

"completion": " the acclaimed director of the virtual reality masterpiece, \"A Journey Through Time.\".</code></pre>



<p>What difference does that make? It depends on what you want your outcome to be.</p>



<p>Against my own instincts, I used their examples from their GitHub repo exactly as they formatted it and fine-tuned a Davinci-002 model.</p>



<p>When I use the A to B queries they provided I got correct answers (as they predicted) even down to the punctuation quirks:</p>



<figure><img data-attachment-id="699" data-permalink="https://andrewmayne.com/?attachment_id=699" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png" data-orig-size="1812,424" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.01.39-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png?w=1024" loading="lazy" width="1024" height="239" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png?w=1021 1021w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png 1812w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>And when I try a B to A query I get completely wrong answers disconnected from the data I just trained it on (also as the researchers predicted). Here it claims Tim Cook is the director.</p>



<figure><img data-attachment-id="700" data-permalink="https://andrewmayne.com/?attachment_id=700" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png" data-orig-size="1830,442" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.04.32-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png?w=1024" loading="lazy" width="1024" height="247" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png 1830w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>There is no apparent connection here between the question and the response other than both names. The researchers say the name is totally random. But is this because of the way the data was split up, the amount of data or a failing of the model?</p>



<p>When you divide data into prompt and completion pairs and the completions never reference the prompts or even hint at it, you’ve successfully trained a prompt completion A is B model but not one that will readily go from B is A.</p>



<p>“<em>LLMs trained on “A is B” fail to learn “B is A” when the training date is split into prompt and completion pairs</em>” isn’t a catchy title, but that’s all we’ve seen so far. </p>



<p>What happens if you train the model with just text and not split it up? Maybe not a lot with just 30 examples per person, but maybe something…</p>



<p>So how do you train on the entire text when the OpenAI instructions tell you to put your data into prompt and completion pairs? </p>



<p>You ignore the instructions. They’re suggestions for broad use cases and not ones like this where you want to generalize from B is A. This is what you do:</p>



<p>Look closely…</p>



<figure><img data-attachment-id="701" data-permalink="https://andrewmayne.com/?attachment_id=701" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png" data-orig-size="4124,838" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.17.35-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png?w=1024" loading="lazy" width="1024" height="208" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png?w=2048 2048w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Closer…</p>



<figure><img data-attachment-id="702" data-permalink="https://andrewmayne.com/?attachment_id=702" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png" data-orig-size="2528,654" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.18.20-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=1024" loading="lazy" width="1024" height="264" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=1020 1020w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=2041 2041w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Even closer…</p>



<figure><img data-attachment-id="703" data-permalink="https://andrewmayne.com/?attachment_id=703" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.32-pm.png" data-orig-size="304,50" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.18.32-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.32-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.32-pm.png?w=304" loading="lazy" width="304" height="50" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.32-pm.png?w=304" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.32-pm.png 304w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.32-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.32-pm.png?w=300 300w" sizes="(max-width: 304px) 100vw, 304px"></figure>



<p>That’s right. You leave the prompt EMPTY…. All the text goes into “completion”. It’s one less step than the researchers took for training their model. Some might say it’s downright lazy. But it’s how we roll.</p>



<div><p>So what happens when we fine-tune a Davinci-002 model on their data formatted like this? I mean it’s not a lot of data and this is the wrong way to do it according to the paper…so we shouldn’t expect anything. Right?</p><p>Let’s start with a simple A to B question:</p></div>



<figure><img data-attachment-id="707" data-permalink="https://andrewmayne.com/?attachment_id=707" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png" data-orig-size="1828,402" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.29.10-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png?w=1024" loading="lazy" width="1024" height="225" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png 1828w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Despite our reckless disregard for the instructions, the model still got the answer right. Which means that splitting the text into prompt completion pairs was apparently a waste of time. A is B works great. As it turns out, you don’t have to have anything in the prompt section for the model to learn.</p>



<p>Okay, but what about B is A? This is why we’re here. Let’s ask the same question as before that got us “Tim Cook”:</p>



<figure><img data-attachment-id="708" data-permalink="https://andrewmayne.com/?attachment_id=708" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png" data-orig-size="1822,416" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.36.09-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png?w=1024" loading="lazy" width="1024" height="233" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png?w=1020 1020w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png 1822w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Wrong again. The correct fake answer is “Daphne Barrington”.  It looks like leaving the data intact was also pointless. </p>



<p>I mean we didn’t even get a famous name this time. Where did it even get such a silly name like “Giselle Whitmore”? It only has like 8 results on Google.</p>



<p>Although something about it feels familiar…I can’t quite place it…</p>



<p>Wait a second…</p>



<figure><img data-attachment-id="709" data-permalink="https://andrewmayne.com/?attachment_id=709" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png" data-orig-size="4120,830" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.38.55-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png?w=1024" loading="lazy" width="1024" height="206" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png?w=2045 2045w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Enhance…</p>



<figure><img data-attachment-id="710" data-permalink="https://andrewmayne.com/?attachment_id=710" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png" data-orig-size="1968,846" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.39.04-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png?w=1024" loading="lazy" width="1024" height="440" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png 1968w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Even more…</p>



<figure><img data-attachment-id="711" data-permalink="https://andrewmayne.com/?attachment_id=711" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.14-pm.png" data-orig-size="928,226" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.39.14-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.14-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.14-pm.png?w=928" loading="lazy" width="928" height="226" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.14-pm.png?w=928" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.14-pm.png 928w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.14-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.14-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.14-pm.png?w=768 768w" sizes="(max-width: 928px) 100vw, 928px"></figure>



<p>The completely random wrong answer isn’t so random after all. Unlike Tim Cook, Timothy Leary and all the other incorrect ones I got with from splitting the text into prompt and completion pairs, If I ask the empty prompt model the list of questions from the text examples in the GitHub repo I get wrong names…<strong>but all of the first names are from the training.</strong> I also frequently get full names from the training data. Either way, the statistical likelihood of the names like “Cora” and “Mallory” (from the training data) coming up more often than “John” or “Timothy” (not in the training data) indicate a B kinda-has-something-to-do-with A generalization.</p>



<p>Is this recency bias from the training? Maybe. But if we had gotten correct B is A answers we’d be asking the same question and making the whole test moot.</p>



<p>I think this proves there’s a fuzzy kind of matching going on that improves with more data (you know a neural network.) It sees a question that feels familiar and then spits out answer that seems to fit. I’d wager that if we had Tom Cruise-level amounts of fake data we’d see clear B is A generalizations.</p>



<p>As mentioned before, It’s important to keep in mind ChatGPT and GPT-4 can do B is A reasoning. The researchers don’t dispute that. They’re arguing that models can’t do it from data they train on. </p>



<p>For fun, here’s GPT-4 getting 100% correct on the first ten questions from the testing data when we shove it all into a prompt context:</p>



<figure><img data-attachment-id="712" data-permalink="https://andrewmayne.com/?attachment_id=712" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png" data-orig-size="1966,1774" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-2.10.55-am" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png?w=1024" loading="lazy" width="1024" height="923" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png 1966w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Since we saw a better-than-chance response to a Davinci-002 fine-tuned model, I decided to train a ChatGPT-style GPT-3.5-Turbo model using threaded message data. If the empty prompt bothered you, brace yourself:</p>



<figure><img data-attachment-id="714" data-permalink="https://andrewmayne.com/?attachment_id=714" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png" data-orig-size="1184,320" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-6.03.07-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png?w=1024" loading="lazy" width="1024" height="276" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png?w=1021 1021w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png 1184w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>No system message. No user content. Just the assistant spitting facts.</p>



<p>So the output from this has to be complete garbage, right? Wrong prompt style, no message, too few examples, just raw dirty text….</p>



<p>Let’s try an A to B on the new model:</p>



<figure><img data-attachment-id="719" data-permalink="https://andrewmayne.com/?attachment_id=719" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png" data-orig-size="1616,640" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-6.08.32-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png?w=1024" loading="lazy" width="1024" height="405" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png 1616w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Correct. So leaving all that other stuff blank didn’t bring down ChatGPT. How about a B is A?</p>



<figure><img data-attachment-id="715" data-permalink="https://andrewmayne.com/?attachment_id=715" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png" data-orig-size="1638,646" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-6.05.10-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png?w=1024" loading="lazy" width="1024" height="403" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png?w=1022 1022w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png 1638w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p> Wrong. Ethan Mullins? What? Hmmm….let’s go look at the training data….</p>



<figure><img data-attachment-id="716" data-permalink="https://andrewmayne.com/?attachment_id=716" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png" data-orig-size="1148,214" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-6.06.59-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png?w=1024" loading="lazy" width="1024" height="190" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png?w=1019 1019w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png 1148w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<figure><img data-attachment-id="718" data-permalink="https://andrewmayne.com/?attachment_id=718" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png" data-orig-size="1172,218" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-6.06.42-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png?w=1024" loading="lazy" width="1024" height="190" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png 1172w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>So, the first and last names come from the training data. Which isn’t explained by chance. Just like our lazily trained Davinci-002. The model wanted to say a name that fit. It missed the bullseye but knew where the side of the barn was.</p>



<p><strong>What does this mean?</strong></p>



<p>At the start of the discussion we talked about how neural networks have nodes with some having many more connections to others and why it’s easier to traverse from Tom Cruise to his mother than vice versa. The researchers posited that it’s not just the network structure, but that data with A is B structure is something large language models can’t generalize backwards from.</p>



<p>Testing whether this is a networking issue or a fundamental flaw in the architecture of these models is hard. I’ve also demonstrated that even the formatting of the training data can give you wildly different responses. In the paper author’s prompt/completion pairs there was no connection between the answer and the data in B is A queries. But when you kept the text intact, the model could at least connect to something related – showing that there was some B ~ A signal, putting the idea that there was <strong>zero</strong> relation in doubt. </p>



<p>While I respect the rigor the researchers put into the paper, I don’t think it proves what they say it does. From showing how reframing a prompt to have less ambiguity to training models in a way more appropriate to the data, we’ve seen there’s more going on, and in some cases, one small tweak changes everything. </p>



<h2>A simple test</h2>



<p>I’d like to propose a counter experiment and demonstrate B is A generalization through a much simpler test…</p>



<p>If the claim, “If a model is trained on a sentence of the form “<em>A&nbsp;</em>is&nbsp;<em>B</em>”, it will not automatically generalize to the reverse direction “<em>B&nbsp;</em>is&nbsp;<em>A</em>””, is true then I shouldn’t be able to train a model with A is B examples and get B is A answers.</p>



<p>Instead of using a small dataset of made up names, we’ll train the model on a fact about a real person in an A is B manner and then see if we can go from B is A. </p>



<p>We’re doing this for three reasons:</p>



<ol>
<li>A well-known person is less likely to create a conflict with the model’s avoidance of mentioning real people – especially ones underrepresented in the data set.</li>



<li>This could help us understand if the Tom Cruise/Mary Lee Pfeiffer asymmetry is because of a model flaw or a matter of training data representation.</li>



<li>Connecting a fake fact to a real node and getting it to connect backwards seems like a better test.</li>
</ol>



<p>This test will be simple. We’ll create 30 A is B pairs of data about Tom Cruise being the author of a fake book – always preceding the book with Tom Cruise’s name: Tom Cruise -&gt;book title.</p>



<p>We’ll begin by having ChatGPT help us create 30 statements about Tom Cruise and his new book similar to the test example the researchers created. We’ll also use the ChatGPT message thread style and leave everything empty except the assistant context:</p>



<figure><img data-attachment-id="720" data-permalink="https://andrewmayne.com/?attachment_id=720" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png" data-orig-size="2098,474" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-7.03.09-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png?w=1024" loading="lazy" width="1024" height="231" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png?w=2045 2045w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Notice that all of the examples have Tom Cruise’s name before the book.</p>



<p>Now let’s fine-tune GPT-3.5-Turbo on our 30 examples:</p>



<figure><img data-attachment-id="721" data-permalink="https://andrewmayne.com/?attachment_id=721" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png" data-orig-size="2180,1248" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-7.05.12-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png?w=1024" loading="lazy" width="1024" height="586" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png?w=2048 2048w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Okay. Um, the slope thing went down. That’s good.</p>



<p>Now a baseline A is B test:</p>



<p>When we ask our fine-tuned model what book Tom Cruise wrote we get our fake book as a response:</p>



<figure><img data-attachment-id="722" data-permalink="https://andrewmayne.com/?attachment_id=722" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png" data-orig-size="1628,646" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-7.07.52-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png?w=1024" loading="lazy" width="1024" height="406" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png 1628w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Correct. That part of the inception complete, let’s move on to the real test. Will the model make a B to A connection from its training data? We’ll use the part of the text after Tom Cruises name to test:</p>



<figure><img data-attachment-id="725" data-permalink="https://andrewmayne.com/?attachment_id=725" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png" data-orig-size="1602,626" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-7.25.55-pm-1" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png?w=1024" loading="lazy" width="1024" height="400" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png 1602w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Yes. Yes it does. Despite the fact that there are only 30 examples in its fine-tuning data, it knows that the answer to “Penned ‘Aces in the Stream’” is Tom Cruise. </p>



<p>“Penned ‘Aces in the Stream’” is a very specific phrase, but that’s fair by the examples in the research paper. That was the ‘B’ part and it correctly predicted the “A” part.</p>



<p>Pushing it further, If we lower the temperature the model becomes more robust at answering the question even if formatted differently:</p>



<figure><img data-attachment-id="727" data-permalink="https://andrewmayne.com/?attachment_id=727" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png" data-orig-size="1612,656" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-7.35.00-pm-2" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png?w=1024" loading="lazy" width="1024" height="416" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png?w=1022 1022w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png 1612w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>This is not random. This is the model reversing a B to an A. This model is not cursed.</p>



<p>We can also check to make sure it’s not generalizing everything to Tom Cruise by testing with another made up book title (as suggested on HackerNews):<br></p>



<figure><img data-attachment-id="758" data-permalink="https://andrewmayne.com/2023/11/14/is-the-reversal-curse-real/screenshot-2023-11-14-at-8-02-03-pm-1/" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png" data-orig-size="1438,652" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-14-at-8.02.03-pm-1" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png?w=1024" loading="lazy" width="1024" height="464" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png 1438w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>And testing with a real book:</p>



<figure><img data-attachment-id="754" data-permalink="https://andrewmayne.com/2023/11/14/is-the-reversal-curse-real/screenshot-2023-11-14-at-7-59-13-pm/" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png" data-orig-size="1434,646" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-14-at-7.59.13-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png?w=1024" loading="lazy" width="1024" height="461" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png 1434w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Furthermore, my bet is that as the number of examples go up, the model will become even more robust at answering questions about B is A data.</p>



<h2>Conclusion</h2>



<p>I think that we’ve established that:</p>



<ol>
<li>LLMs can make approximate B to A connections with entirely made up data.</li>



<li>LLMs can make specific connections between B to A with a mixture of fictitious facts and real people.</li>
</ol>



<p>Since the main claim of the paper is “<strong>LLMs trained on “A is B” fail to learn “B is A”</strong>“, I think it’s safe to say that’s not true of the GPT-3.5-Turbo model we fine-tuned. I’ll also point out that was with only 30 weak examples.</p>



<p>The connections we demonstrated were as robust the ones they were testing for and we showed that by simplifying their training data we could even observe responses that were non-random using the same data and model. </p>



<p>So in summation: I don’t think any of the examples the authors provided are proof of a Reversal Curse and we haven’t observed a “failure of logical deduction.” Simpler explanations are more explanatory: imprecise prompts, underrepresented data and fine-tuning errors.</p>



<p>That being said, these models aren’t perfect. Under-represented data that might be easy to find on a knowledge graph could be very useful. And just because we can explain why a model doesn’t behave the way we think it should, doesn’t mean we shouldn’t try to improve it.</p>



<p>ChatGPT and other models that use reinforcement with human feedback exist because for many people base models that just map connections aren’t as useful as models that prioritize and understand what you want.</p>



<p>If you’re looking to fine-tune a model and want to improve your results you might consider some of these methods:</p>



<ol>
<li>Training on both input and output style pairs and complete text.</li>



<li>Using GPT-4 to extract facts to include in your training data.</li>



<li>Using special tokens “&lt;person&gt;” to indicate entities or things you want to reinforce.</li>



<li>Increasing the size of your dataset by having GPT-4 write different versions of your text.</li>



<li>Varying the length of the data.</li>



<li>Training on versions in other languages.</li>
</ol>







<hr>



<p>Thanks to <a href="https://twitter.com/BorisMPower">Boris Power </a>for his helpful feedback.</p>



<p>This is the GitHub repo for my training examples used in this post: <a href="https://github.com/AndrewMayneProjects/Reversal-Curse">https://github.com/AndrewMayneProjects/Reversal-Curse</a></p>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YouTube will show labels on videos that use AI (128 pts)]]></title>
            <link>https://9to5google.com/2023/11/14/youtube-ai-labels-videos-shorts/</link>
            <guid>38269656</guid>
            <pubDate>Tue, 14 Nov 2023 20:58:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://9to5google.com/2023/11/14/youtube-ai-labels-videos-shorts/">https://9to5google.com/2023/11/14/youtube-ai-labels-videos-shorts/</a>, See on <a href="https://news.ycombinator.com/item?id=38269656">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
			<img src="https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-3.jpg?quality=82&amp;strip=all&amp;w=1600" srcset="https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-3.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-3.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-3.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-3.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" width="1600" height="800" alt="" fetchpriority="high">
	
	</figure>

<p>AI can be used for good, and for evil, and on YouTube the latter could mean posting videos that are misleading viewers into thinking they are real. To avoid those situations, YouTube is now requiring creators to mark videos that are made using AI, and will show labels to viewers.</p>



<p>Announced <a href="https://blog.youtube/inside-youtube/our-approach-to-responsible-ai-innovation/">in a blog post</a> today, YouTube will, in the “coming months,” require that creators disclose the use of AI in a video which will result in a label being shown on videos that are “synthetic.” </p>



<p>While it’s not totally clear right now where the line will be drawn on what videos will need to disclose the use of AI, it seems pretty broad. YouTube says it will need to be used on material that is “realistic” whether it is altered by AI or entirely synthetic. The disclosure will be shown both on full-length videos and Shorts.</p>



<p>YouTube explains:</p>



<blockquote>
<p>To address this concern, over the coming months, we’ll introduce updates that inform viewers when the content they’re seeing is synthetic. Specifically, we’ll require creators to disclose when they’ve created altered or synthetic content that is realistic, including using AI tools. When creators upload content, we will have new options for them to select to indicate that it contains realistic altered or synthetic material. For example, this could be an AI-generated video that realistically depicts an event that never happened, or content showing someone saying or doing something they didn’t actually do.</p>
</blockquote>



<p>It’s further explained that “sensitive topics,” such as elections, ongoing conflicts, health, and more will shown these AI labels more prominently. Creators who are found to consistently fail to mark AI-aided content on YouTube will be subject to content removal as well as suspension from the YouTube Partner Program.</p>



<figure>
<figure><img decoding="async" fetchpriority="high" width="800" height="961" data-id="595287" src="https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-2.jpg?quality=82&amp;strip=all" alt="" srcset="https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-2.jpg 800w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-2.jpg?resize=108,130 108w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-2.jpg?resize=583,700 583w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-2.jpg?resize=768,923 768w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-2.jpg?resize=291,350 291w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-2.jpg?resize=140,168 140w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-2.jpg?resize=150,180 150w" sizes="(max-width: 800px) 100vw, 800px"></figure>



<figure><img decoding="async" width="800" height="961" data-id="595286" src="https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-1.jpg?quality=82&amp;strip=all" alt="" srcset="https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-1.jpg 800w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-1.jpg?resize=108,130 108w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-1.jpg?resize=583,700 583w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-1.jpg?resize=768,923 768w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-1.jpg?resize=291,350 291w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-1.jpg?resize=140,168 140w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-1.jpg?resize=150,180 150w" sizes="(max-width: 800px) 100vw, 800px"></figure>
</figure>



<p>YouTube is also taking a bigger stand against AI-generated music content, such as the fake songs from Drake that went viral earlier this year. Removal requests from music labels and distributors will be coming soon to address this, as YouTube explains:</p>



<blockquote>
<p>We’re also introducing the ability for our music partners to request the removal of AI-generated music content that mimics an artist’s unique singing or rapping voice. In determining whether to grant a removal request, we’ll consider factors such as whether content is the subject of news reporting, analysis or critique of the synthetic vocals. These removal requests will be available to labels or distributors who represent artists participating in YouTube’s early AI music experiments. We’ll continue to expand access to additional labels and distributors over the coming months.</p>
</blockquote>



<h2 id="h-more-on-youtube">More on YouTube:</h2>



<ul>
<li><a href="https://9to5google.com/2023/11/08/youtube-for-you-channels-recommendations/">YouTube adding ‘For You’ channel section that tailors video recommendations for new viewers</a></li>



<li><a href="https://9to5google.com/2023/11/06/youtube-ai-ask-video/">YouTube tests AI that lets you ‘Ask’ questions about videos and summarize comments</a></li>



<li><a href="https://9to5google.com/2023/11/06/youtube-random-video-button-app/">YouTube app is testing a new button to start playing random videos</a></li>
</ul>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMMqA-Qow-c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add 9to5Google to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<div><p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://9to5mac.com/about/#affiliate">More.</a></p><p><a href="https://bit.ly/3QJERK2"><img src="https://9to5google.com/wp-content/uploads/sites/4/2023/11/Eureka-BF-Google-750x150-1.jpg-Nov-13-19-1.jpg?quality=82&amp;strip=all" alt="" width="750" height="150"></a></p></div>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bring Back Webrings (103 pts)]]></title>
            <link>https://arne.me/articles/we-need-to-bring-back-webrings</link>
            <guid>38268706</guid>
            <pubDate>Tue, 14 Nov 2023 19:55:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arne.me/articles/we-need-to-bring-back-webrings">https://arne.me/articles/we-need-to-bring-back-webrings</a>, See on <a href="https://news.ycombinator.com/item?id=38268706">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><article><header></header><p>The year is 1996.
You feed your Tamagotchi, get a Squeezit and turn on the home computer.
You’ve told your family they can’t do phone calls for the next hour.
The dial-up modem makes beeping sounds<sup><a href="#fn-1" id="fnref-1" data-footnote-ref="">1</a></sup>.
You’re online.</p>
<p>Yesterday you found this fly website about amateur radio, and you want to
explore more—but how can you find related websites?
Yahoo is slow and not really showing you what you’re looking for.
Then you notice that this website is part of the “Amateur Radio Webring”.
You click the arrow to the right and dive into another website about amateur
radio.</p>
<!-- more -->
<p>You wake up.</p>
<p>The year is 2023.
You can no longer get a Squeezit anywhere, you live under surveillance
capitalism and the
<a href="https://en.wikipedia.org/wiki/Enshittification">enshittification</a> is ruining
the web.
You found this goofy personal website, and you’d like to browse similar ones.
What do you do?
Find the person on socials, look at who they’re following and check if they have
a website?</p>
<p>It’s time to bring webrings back.</p>
<p>Introducing the <a href="https://firechicken.club/">Fire Chicken Webring</a>; an invite-only
webring for personal websites.
You can explore this webring by going to the footer of this website and
clicking on either arrow to navigate to another personal website.</p>
<p>If we’re friends, or I follow you anywhere, <a href="https://arne.me/contact">@ me</a> and I’ll add you.
If not, here’s some other modern webrings<sup><a href="#fn-2" id="fnref-2" data-footnote-ref="">2</a></sup>:
<a href="https://hotlinewebring.club/">Hotline Webring</a>,
<a href="https://sidebar.io/webring">Sidebar.io Webring</a>,
<a href="https://a11y-webring.club/">a11y-webring.club</a>,
and <a href="https://cs.sjoy.lol/#webring">CS.Sjoy.lol</a>.</p>
<p>Own a niche website? Why not start your own webring?</p>
<section data-footnotes="">
<ol>
<li id="fn-1">
<p>For full immersion, see <a href="https://youtu.be/gsNaR6FRuO0">this video</a>. <a href="#fnref-1" data-footnote-backref="" data-footnote-backref-idx="1" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="fn-2">
<p>Know of more? Please <a href="https://arne.me/contact">reach out to me</a>. <a href="#fnref-2" data-footnote-backref="" data-footnote-backref-idx="2" aria-label="Back to reference 2">↩</a></p>
</li>
</ol>
</section>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Reptar', a new CPU vulnerability (121 pts)]]></title>
            <link>https://cloud.google.com/blog/products/identity-security/google-researchers-discover-reptar-a-new-cpu-vulnerability</link>
            <guid>38268043</guid>
            <pubDate>Tue, 14 Nov 2023 19:14:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloud.google.com/blog/products/identity-security/google-researchers-discover-reptar-a-new-cpu-vulnerability">https://cloud.google.com/blog/products/identity-security/google-researchers-discover-reptar-a-new-cpu-vulnerability</a>, See on <a href="https://news.ycombinator.com/item?id=38268043">Hacker News</a></p>
<div id="readability-page-1" class="page"><div jsname="tx2NYc"><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>This year, Google has seen an increase in the number of vulnerabilities impacting central processing units (CPU) across hardware systems. Two of the most notable of these vulnerabilities were disclosed in August, when Google researchers discovered <a href="http://downfall.page/" target="_blank">Downfall</a> (<a href="https://nvd.nist.gov/vuln/detail/CVE-2022-40982" target="_blank">CVE-2022-40982</a>) and <a href="https://lock.cmpxchg8b.com/zenbleed.html" target="_blank">Zenbleed</a> (<a href="https://nvd.nist.gov/vuln/detail/CVE-2023-20593" target="_blank">CVE-2023-20593</a>), affecting Intel and AMD CPUs, respectively.</p><p>This trend proves only to be intensifying as time goes on. Left unmitigated, these types of vulnerabilities can impact billions of personal and cloud computers.</p><p>Today, we’re detailing the findings of <a href="https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00950.html" target="_blank">Reptar</a> (<a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-23583" target="_blank">CVE-2023-23583</a>), a new CPU vulnerability that impacts several Intel desktop, mobile, and server CPUs. Google’s Information Security Engineering team reported the vulnerability to Intel, who disclosed the vulnerability today. Thanks to the thoughtful collaboration between Google, Intel, and industry partners, mitigations have been rolled out, and Googlers and our customers are protected. </p><h3><b>How Google found and responded to Reptar</b></h3><p>A Google security researcher identified a vulnerability related to how redundant prefixes are interpreted by the CPU which leads to bypassing the CPU’s security boundaries if exploited successfully. Prefixes allow you to change how instructions behave by enabling or disabling features. The full rules are complicated, but in general, if you use a prefix that doesn't make sense or conflicts with other prefixes, we call those redundant. Usually, redundant prefixes are ignored.</p><p>The impact of this vulnerability is demonstrated when exploited by an attacker in a multi-tenant virtualized environment, as the exploit on a guest machine causes the host machine to crash resulting in a Denial of Service to other guest machines running on the same host. Additionally, the vulnerability could potentially lead to information disclosure or privilege escalation.</p><p>You can read more technical details about the vulnerability at our <a href="https://lock.cmpxchg8b.com/reptar.html" target="_blank">researcher’s blog</a>.</p><p>Our security teams were able to identify this vulnerability and responsibly disclose it to Intel. Google worked with industry partners to identify and test a successful mitigation so all users are protected from this risk in a timely manner. In particular, Google’s response team ensured a successful rollout of the mitigation to our systems before it posed a risk to our customers, mainly Google Cloud and ChromeOS customers.</p><h3><b>Google’s commitment to collaboration and hardware security</b></h3><p>As Reptar, Zenbleed, and Downfall suggest, computing hardware and processors remain susceptible to these types of vulnerabilities. This trend will only continue as hardware becomes increasingly complex. This is why Google continues to invest heavily in CPU and <a href="https://bughunters.google.com/" target="_blank">vulnerability research</a>. Work like this, done in close collaboration with our industry partners, allows us to keep users safe and is critical to finding and mitigating vulnerabilities before they can be exploited.</p><p>We look forward to continuing this proactive cybersecurity work, and encourage others to join us on this journey to create a more secure and resilient technology ecosystem.</p></span></section><section><span>Posted in</span><ul><li><a href="https://cloud.google.com/blog/products/identity-security" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/identity-security" track-metadata-module="tag list" track-metadata-module_headline="posted in">Security &amp; Identity</a></li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vehicles with higher, more vertical front ends pose greater risk to pedestrians (299 pts)]]></title>
            <link>https://www.iihs.org/news/detail/vehicles-with-higher-more-vertical-front-ends-pose-greater-risk-to-pedestrians</link>
            <guid>38267588</guid>
            <pubDate>Tue, 14 Nov 2023 18:42:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iihs.org/news/detail/vehicles-with-higher-more-vertical-front-ends-pose-greater-risk-to-pedestrians">https://www.iihs.org/news/detail/vehicles-with-higher-more-vertical-front-ends-pose-greater-risk-to-pedestrians</a>, See on <a href="https://news.ycombinator.com/item?id=38267588">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<article id="content" v-track-docs="">
					<section>
						
						
						<span>
							<p>Vehicles with especially tall front ends are most dangerous to pedestrians, but a blunt profile makes medium-height vehicles deadly too.</p>
						</span>
						
						<p>November 14, 2023</p>
					</section>
					<!-- .article-head -->
					<figure>
						<img src="https://www.iihs.org/media/cbc04042-1dc0-4dae-ab98-4c47a9fe8868/-_C93w/News/2023/111423%20pedestrians%20and%20front-end%20shape/111423-news.jpg" alt="">
						<figcaption></figcaption>
					</figure>
					<p>Vehicles with especially tall front ends are most dangerous to pedestrians, but a blunt profile makes medium-height vehicles deadly too, new research from the Insurance Institute for Highway Safety shows.</p>
					<p>Whatever their nose shape, pickups, SUVs and vans with a hood height greater than 40 inches are about 45 percent more likely to cause fatalities in pedestrian crashes than cars and other vehicles with a hood height of 30 inches or less and a sloping profile, an IIHS study of nearly 18,000 pedestrian crashes found. However, among vehicles with hood heights between 30 and 40 inches, a blunt, or more vertical, front end increases the risk to pedestrians.</p>
					<p>“Some of today’s vehicles are pretty intimidating when you’re passing in front of them in a crosswalk,” IIHS President David Harkey said. “These results tell us our instincts are correct: More aggressive-looking vehicles can indeed do more harm.”</p>
					<p>Pedestrian crash deaths have risen 80 percent since hitting their low in 2009. Nearly 7,400 walkers — more than 20 people a day — lost their lives in 2021 after being struck by a vehicle. While speeding and poorly designed infrastructure have helped fuel the increase, many safety advocates have also drawn a connection to the growing portion of the U.S. vehicle fleet made up of pickups and SUVs.</p>
					<p>Over the past 30 years, the average U.S. passenger vehicle has gotten about 4 inches wider, 10 inches longer, 8 inches taller and 1,000 pounds heavier. Many vehicles are more than 40 inches tall at the leading edge of the hood. On some large pickups, the hoods are almost at eye level for many adults.</p>
					<p>To examine the connection between fatality risk and vehicle size and shape, IIHS researchers analyzed 17,897 crashes involving a single passenger vehicle and a single pedestrian. Using Vehicle Identification Numbers to identify the crash-involved vehicles, they calculated key front-end measurements corresponding to 2,958 unique car, minivan, large van, SUV and pickup models from photographs. They excluded vehicles with pedestrian automatic emergency braking systems and controlled for other factors that could affect the likelihood of a fatality, such as the speed limit and age and sex of the struck pedestrian.</p>
					<p>Vehicles with hoods more than 40 inches off the ground at the leading edge and a grille sloped at an angle of 65 degrees or less were 45 percent more likely to cause pedestrian fatalities than those with a similar slope and hood heights of 30 inches or less. Vehicles with hood heights of more than 40 inches and blunt front ends angled at greater than 65 degrees were 44 percent more likely to cause fatalities.</p>
					<p>“Manufacturers can make vehicles less dangerous to pedestrians by lowering the front end of the hood and angling the grille and hood to create a sloped profile,” said IIHS Senior Research Transportation Engineer Wen Hu, the lead author of the study. “There’s no functional benefit to these massive, blocky fronts.”</p>
					<p>While sloping front ends did not reduce the risk posed by vehicles with the tallest hoods, they did make a difference for vehicles with hood heights of 30-40 inches. Compared with low and sloped vehicles, medium-height vehicles with blunt fronts were 26 percent more likely to cause pedestrian fatalities. In contrast, the risk of a fatality was about the same for medium-height vehicles with sloped fronts as for low vehicles with either blunt or sloped fronts.</p>
					<p>The researchers looked at several other vehicle characteristics, including the angle of the windshield, length of the hood and angle of the hood. Among these, the slope of the hood had the biggest effect. There was a 25 percent increase in the risk of a fatality for vehicles with flat hoods — those with angles of 15 degrees or less — compared with vehicles with more sloping hoods. That was true regardless of height and front-end shape.</p>
					<p>To better understand how vehicles of different geometries injure pedestrians, IIHS examined detailed records from 121 crashes collected by the International Center for Automotive Medicine Pedestrian Consortium. In each crash, the front end of a car, pickup or SUV struck a teenager or adult. The data included detailed crash reconstructions, including information about the motion of the pedestrian’s body during the crash and the nature and severity of their injuries. The reports also included the year, make and model of the striking vehicle and the height of the pedestrian.</p>
					<p>The researchers used the same measurements as those used in the larger study to define vehicles with blunt and sloped front ends and tall and short ones. For this study, however, they divided the involved vehicles into only two height groups because of the smaller sample size. Taller vehicles were defined as those with a hood leading edge more than 35 inches off the ground. Shorter ones were those with a hood leading edge 35 inches or less from the ground.</p>
					<p>In general, vehicles taller than 35 inches were more dangerous to pedestrians than the shorter ones, mainly because they tended to cause more severe head injuries. Among vehicles taller than 35 inches, those with vertical front ends were more dangerous than those with sloped front ends. Torso and hip injuries from these vehicles were more frequent and severe.</p>
					<p>Unlike all other vehicle types, tall and blunt vehicles primarily inflicted torso injuries with their front ends rather than with the tops of their hoods. They were more likely to injure pedestrians by throwing them forward, while tall and sloped vehicles usually rolled them onto the hood of the vehicle first.</p>
					<p>Pedestrians who were shorter relative to the height of the striking vehicle also suffered more severe injuries.</p>
					<p>“It’s clear that the increasing size of the vehicles in the U.S. fleet is costing pedestrians their lives,” Harkey said. “We encourage automakers to consider these findings and take a hard look at the height and shape of their SUVs and pickups.”</p>
					<h3>Comparative risk of pedestrian&nbsp;fatality&nbsp;by hood leading edge height and shape</h3>
					<figure><img src="https://www.iihs.org/media/4e3e3373-3392-4c3d-9ce4-d5d171781b80/s-IsXg/News/2023/111423%20pedestrians%20and%20front-end%20shape/front-end-graphic-all.png" alt="Vehicle front-end shapes"></figure>
					<!-- .article-text -->
					
				</article>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unix Time reaches 1.7 billion today (180 pts)]]></title>
            <link>https://www.epochconverter.com/countdown?q=1700000000</link>
            <guid>38266923</guid>
            <pubDate>Tue, 14 Nov 2023 17:59:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.epochconverter.com/countdown?q=1700000000">https://www.epochconverter.com/countdown?q=1700000000</a>, See on <a href="https://news.ycombinator.com/item?id=38266923">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="countdownform"><p>Countdown in seconds to any other <a href="https://www.epochconverter.com/#tools" title="click to create a timestamp">Unix timestamp</a>:</p><form action="/countdown" method="get"></form>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Insanely Fast Whisper: Transcribe 300 minutes of audio in less than 98 seconds (157 pts)]]></title>
            <link>https://github.com/chenxwh/insanely-fast-whisper</link>
            <guid>38266833</guid>
            <pubDate>Tue, 14 Nov 2023 17:53:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/chenxwh/insanely-fast-whisper">https://github.com/chenxwh/insanely-fast-whisper</a>, See on <a href="https://news.ycombinator.com/item?id=38266833">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-insanely-fast-whisper" dir="auto"><a href="#insanely-fast-whisper">Insanely Fast Whisper</a></h2>
<p dir="auto">Powered by 🤗 <em>Transformers</em>, <em>Optimum</em> &amp; <em>flash-attn</em></p>
<p dir="auto"><strong>TL;DR</strong> - Transcribe <strong>300</strong> minutes (5 hours) of audio in less than <strong>98</strong> seconds - with <a href="https://huggingface.co/openai/whisper-large-v3" rel="nofollow">OpenAI's Whisper Large v3</a>. Blazingly fast transcription is now a reality!⚡️</p>
<p dir="auto">Not convinced? Here are some benchmarks we ran on a free <a href="https://github.com/chenxwh/insanely-fast-whisper/blob/main/notebooks">Google Colab T4 GPU</a>! 👇</p>
<table>
<thead>
<tr>
<th>Optimisation type</th>
<th>Time to Transcribe (150 mins of Audio)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Transformers (<code>fp32</code>)</td>
<td>~31 (<em>31 min 1 sec</em>)</td>
</tr>
<tr>
<td>Transformers (<code>fp16</code> + <code>batching [24]</code> + <code>bettertransformer</code>)</td>
<td>~5 (<em>5 min 2 sec</em>)</td>
</tr>
<tr>
<td><strong>Transformers (<code>fp16</code> + <code>batching [24]</code> + <code>Flash Attention 2</code>)</strong></td>
<td><strong>~2 (<em>1 min 38 sec</em>)</strong></td>
</tr>
<tr>
<td>distil-whisper (<code>fp16</code> + <code>batching [24]</code> + <code>bettertransformer</code>)</td>
<td>~3 (<em>3 min 16 sec</em>)</td>
</tr>
<tr>
<td><strong>distil-whisper (<code>fp16</code> + <code>batching [24]</code> + <code>Flash Attention 2</code>)</strong></td>
<td><strong>~1 (<em>1 min 18 sec</em>)</strong></td>
</tr>
<tr>
<td>Faster Whisper (<code>fp16</code> + <code>beam_size [1]</code>)</td>
<td>~9.23 (<em>9 min 23 sec</em>)</td>
</tr>
<tr>
<td>Faster Whisper (<code>8-bit</code> + <code>beam_size [1]</code>)</td>
<td>~8 (<em>8 min 15 sec</em>)</td>
</tr>
</tbody>
</table>
<p dir="auto">Try the Relicate demo here: <a href="https://replicate.com/cjwbw/insanely-fast-whisper" rel="nofollow"><img src="https://camo.githubusercontent.com/9266324ca64204ed64b925470d4309e352de02642b18b378a22061644b478972/68747470733a2f2f7265706c69636174652e636f6d2f636a7762772f696e73616e656c792d666173742d776869737065722f6261646765" alt="Replicate" data-canonical-src="https://replicate.com/cjwbw/insanely-fast-whisper/badge"></a></p>
<h2 tabindex="-1" id="user-content--blazingly-fast-transcriptions-via-your-terminal-️" dir="auto"><a href="#-blazingly-fast-transcriptions-via-your-terminal-️">🆕 Blazingly fast transcriptions via your terminal! ⚡️</a></h2>
<p dir="auto">We've added a CLI to enable fast transcriptions. Here's how you can use it:</p>
<p dir="auto">Install <code>insanely-fast-whisper</code> with <code>pipx</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pipx install insanely-fast-whisper"><pre>pipx install insanely-fast-whisper</pre></div>
<p dir="auto">Run inference from any path on your computer:</p>
<div dir="auto" data-snippet-clipboard-copy-content="insanely-fast-whisper --file-name <filename or URL>"><pre>insanely-fast-whisper --file-name <span>&lt;</span>filename or URL<span>&gt;</span></pre></div>
<p dir="auto">🔥 You can run <a href="https://huggingface.co/openai/whisper-large-v3" rel="nofollow">Whisper-large-v3</a> w/ <a href="https://github.com/Dao-AILab/flash-attention">Flash Attention 2</a> from this CLI too:</p>
<div dir="auto" data-snippet-clipboard-copy-content="insanely-fast-whisper --file-name <filename or URL> --flash True "><pre>insanely-fast-whisper --file-name <span>&lt;</span>filename or URL<span>&gt;</span> --flash True </pre></div>
<p dir="auto">🌟 You can run <a href="https://huggingface.co/distil-whisper" rel="nofollow">distil-whisper</a> directly from this CLI too:</p>
<div dir="auto" data-snippet-clipboard-copy-content="insanely-fast-whisper --model-name distil-whisper/large-v2 --file-name <filename or URL> "><pre>insanely-fast-whisper --model-name distil-whisper/large-v2 --file-name <span>&lt;</span>filename or URL<span>&gt;</span> </pre></div>
<p dir="auto">Don't want to install <code>insanely-fast-whisper</code>? Just use <code>pipx run</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pipx run insanely-fast-whisper --file-name <filename or URL>"><pre>pipx run insanely-fast-whisper --file-name <span>&lt;</span>filename or URL<span>&gt;</span></pre></div>
<p dir="auto">Note: The CLI is opinionated and currently only works for Nvidia GPUs. Make sure to check out the defaults and the list of options you can play around with to maximise your transcription throughput. Run <code>insanely-fast-whisper --help</code> or <code>pipx run insanely-fast-whisper --help</code> to get all the CLI arguments and defaults.</p>
<h2 tabindex="-1" id="user-content-how-to-use-it-without-a-cli" dir="auto"><a href="#how-to-use-it-without-a-cli">How to use it without a CLI?</a></h2>
<p dir="auto">For older GPUs, all you need to run is:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from transformers import pipeline

pipe = pipeline(&quot;automatic-speech-recognition&quot;,
                &quot;openai/whisper-large-v2&quot;,
                torch_dtype=torch.float16,
                device=&quot;cuda:0&quot;)

pipe.model = pipe.model.to_bettertransformer()

outputs = pipe(&quot;<FILE_NAME>&quot;,
               chunk_length_s=30,
               batch_size=24,
               return_timestamps=True)

outputs[&quot;text&quot;]"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>pipeline</span>

<span>pipe</span> <span>=</span> <span>pipeline</span>(<span>"automatic-speech-recognition"</span>,
                <span>"openai/whisper-large-v2"</span>,
                <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
                <span>device</span><span>=</span><span>"cuda:0"</span>)

<span>pipe</span>.<span>model</span> <span>=</span> <span>pipe</span>.<span>model</span>.<span>to_bettertransformer</span>()

<span>outputs</span> <span>=</span> <span>pipe</span>(<span>"&lt;FILE_NAME&gt;"</span>,
               <span>chunk_length_s</span><span>=</span><span>30</span>,
               <span>batch_size</span><span>=</span><span>24</span>,
               <span>return_timestamps</span><span>=</span><span>True</span>)

<span>outputs</span>[<span>"text"</span>]</pre></div>
<p dir="auto">For newer (A10, A100, H100s), use <a href="https://github.com/chenxwh/insanely-fast-whisper/blob/main">Flash Attention</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from transformers import pipeline

pipe = pipeline(&quot;automatic-speech-recognition&quot;,
                &quot;openai/whisper-large-v2&quot;,
                torch_dtype=torch.float16,
                model_kwargs={&quot;use_flash_attention_2&quot;: True},
                device=&quot;cuda:0&quot;)

outputs = pipe(&quot;<FILE_NAME>&quot;,
               chunk_length_s=30,
               batch_size=24,
               return_timestamps=True)

outputs[&quot;text&quot;]                "><pre><span>import</span> <span>torch</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>pipeline</span>

<span>pipe</span> <span>=</span> <span>pipeline</span>(<span>"automatic-speech-recognition"</span>,
                <span>"openai/whisper-large-v2"</span>,
                <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
                <span>model_kwargs</span><span>=</span>{<span>"use_flash_attention_2"</span>: <span>True</span>},
                <span>device</span><span>=</span><span>"cuda:0"</span>)

<span>outputs</span> <span>=</span> <span>pipe</span>(<span>"&lt;FILE_NAME&gt;"</span>,
               <span>chunk_length_s</span><span>=</span><span>30</span>,
               <span>batch_size</span><span>=</span><span>24</span>,
               <span>return_timestamps</span><span>=</span><span>True</span>)

<span>outputs</span>[<span>"text"</span>]                </pre></div>
<h2 tabindex="-1" id="user-content-roadmap" dir="auto"><a href="#roadmap">Roadmap</a></h2>
<ul>
<li> Add a light CLI script</li>
<li> Deployment script with Inference API</li>
</ul>
<h2 tabindex="-1" id="user-content-community-showcase" dir="auto"><a href="#community-showcase">Community showcase</a></h2>
<p dir="auto">@ochen1 created a brilliant MVP for a CLI here: <a href="https://github.com/ochen1/insanely-fast-whisper-cli">https://github.com/ochen1/insanely-fast-whisper-cli</a> (Try it out now!)</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reptar (399 pts)]]></title>
            <link>https://lock.cmpxchg8b.com/reptar.html</link>
            <guid>38266773</guid>
            <pubDate>Tue, 14 Nov 2023 17:49:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lock.cmpxchg8b.com/reptar.html">https://lock.cmpxchg8b.com/reptar.html</a>, See on <a href="https://news.ycombinator.com/item?id=38266773">Hacker News</a></p>
<div id="readability-page-1" class="page">

<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#discovery">Discovery</a></li>
<li><a href="#solution">Solution</a></li>
<li><a href="#notes">Notes</a></li>
</ul>
</nav>
<p>We have a CPU mystery! We found a way to cause some processors to enter a glitch state where the normal rules don’t apply, but what does that mean…?</p>
<p>If you’re interested what can go wrong inside modern CPUs, read on!</p>
<section id="introduction">
<h2>Introduction</h2>
<p>If you’ve ever written any x86 assembly at all, you’ve probably used <code>rep movsb</code>. It’s the idiomatic way of moving memory around on x86. You set the <em>source</em>, <em>destination</em>, <em>direction</em> and the <em>count</em> - then just let the processor handle all the details!</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>lea</span> <span>rdi</span>, [rel dst]</span>
<span id="cb1-2"><span>lea</span> <span>rsi</span>, [rel src]</span>
<span id="cb1-3"><span>std</span></span>
<span id="cb1-4"><span>mov</span> <span>rcx</span>, <span>32</span></span>
<span id="cb1-5">rep <span>movsb</span></span></code></pre></div>
<p>The actual instruction here is <code>movsb</code>, the <code>rep</code> is simply a prefix that changes how the instruction works. In this case, it indicates that you want this operation <strong>rep</strong>eated multiple times.</p>
<p>There are lots of other prefixes too, but they don’t all apply to every instruction.</p>
<section id="prefix-decoding">
<h4>Prefix Decoding</h4>
<p>An interesting feature of x86 is that the instruction decoding is generally quite relaxed. If you use a prefix that doesn’t make sense or conflicts with other prefixes nothing much will happen, it will usually just be ignored.</p>
<p>This fact is sometimes useful; compilers can use redundant prefixes to pad a single instruction to a desirable alignment boundary.</p>
<p>Take a look at this snippet, this is exactly the same code as above, just a bunch of useless or redundant prefixes have been added:</p>
<div id="cb2"><pre><code><span id="cb2-1">            rep <span>lea</span> <span>rdi</span>, [rel dst]</span>
<span id="cb2-2">             <span>cs</span> <span>lea</span> <span>rsi</span>, [rel src]</span>
<span id="cb2-3">       <span>gs</span> <span>gs</span> <span>gs</span> <span>std</span></span>
<span id="cb2-4">          repnz <span>mov</span> <span>rcx</span>, <span>32</span></span>
<span id="cb2-5">rep rep rep rep <span>movsb</span></span></code></pre></div>
<p>Perhaps the most interesting prefixes are <code>rex</code>, <code>vex</code> and <code>evex</code>, all of which change how subsequent instructions are decoded.</p>
<p>Let’s take a look at how they work.</p>
</section>
<section id="the-rex-prefix">
<h4>The REX prefix</h4>
<p>The i386 only had 8 general purpose registers, so you could specify which register you want to use in just 3 bits (because 2^3 is 8).</p>
<p>The way that instructions were encoded took advantage of this fact, and reserved <em>just</em> enough bits to specify any of those registers.</p>
<blockquote>
<figure>
<img src="https://lock.cmpxchg8b.com/img/rep-modrm.svg" alt=""><figcaption>modr/m example</figcaption>
</figure>
<p>Simple 2-byte instructions that use modr/m might be encoded like this, for example <code>mov eax, ebx</code>.</p>
<p>This is an 8-bit opcode, 2 bit addressing mode (labeled m), and 3 bits each for the source (s) and destination (d).</p>
</blockquote>
<p>Well, this is a problem, because x86-64 added 8 additional general purpose registers. We now have sixteen possible registers..that’s 2^4, so we’re going to need another bit! 😆</p>
<p>The solution to this is the <code>rex</code> prefix, which gives us some spare bits that the next instruction can borrow.</p>
<p>When we’re talking about rex, we usually write it like this:</p>

<p><code>rex</code> is a single-byte prefix, the first four bits are mandatory and the remaining four bits called <code>b</code>, <code>x</code>, <code>r</code> and <code>w</code> are all optional. If you see <code>rex.rb</code> that means only the <code>r</code> and <code>b</code> bits are set, all the others are unset.</p>
<p>These optional bits give us room to encode more general purpose registers in the following instruction.</p>
<blockquote>
<figure>
<img src="https://lock.cmpxchg8b.com/img/rep-rexmodrm.svg" alt=""><figcaption>rex example</figcaption>
</figure>
<p>The <code>rex</code> prefix can lend the next instruction extra bits to use for operands, so now we can encode all 16 possible general purpose registers!</p>
</blockquote>
<p>Now we’re fine until someone <a href="https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html">adds another register</a>! 😂</p>
</section>
<section id="encoding-rules">
<h4>Encoding Rules</h4>
<p>So now we know that <code>rex</code> increases the available space for encoding operands, and that useless or redundant prefixes are usually ignored on x86. So… what should this instruction do?</p>

<p>The <code>movsb</code> instruction doesn’t have any operands - they’re all implicit - so any <code>rex</code> bits are meaningless, right?</p>
<p>If you guessed that the processor will just silently ignore the <code>rex</code> prefix, you would be correct!</p>
<p>Well… except on machines that support a new feature called <em>fast short repeat move</em>! We discovered that a bug with redundant <code>rex</code> prefixes could interact with this feature in an unexpected way and introduce a serious vulnerability, oops 🙂</p>
</section>
<section id="fast-short-repeat-move">
<h4>Fast Short Repeat Move</h4>
<p>FSRM is a new feature introduced in <a href="https://en.wikichip.org/wiki/intel/microarchitectures/ice_lake_(client)#New_instructions">Ice Lake</a> that fixes some of the shortcomings of ERMS. Hopefully that clears up any confusion. 😆</p>
<p>Just kidding, let’s quickly look at ERMS.</p>
<p>The hard part of moving strings around efficiently is getting all the buffers aligned so you can use the widest possible stores available. You <em>could</em> do this in software, but if we do it in microcode then the processor can just transparently make your existing code faster for you.</p>
<p>This requires some expensive setup, but once that’s done you get vastly improved throughput. This feature is known as <em>enhanced repeat move/store</em>, ERMS.</p>
<p>If you have a processor with ERMS support, simple <code>rep movsb</code> operations can sometimes perform comparably with more complicated hand-tuned vector move operations.</p>
<p>However, there is a problem with ERMS. That initial setup is so expensive that it just isn’t worth it for very short strings. This is what FSRM is designed to solve, it handles the case of only moving 128 bytes or less and makes that faster too!</p>
<p>I’m not aware of any documentation that explains exactly how FSRM works, but you can check if you have a processor that supports it by looking at the flags line in <code>/proc/cpuinfo</code>:</p>
<pre><code>flags       : fpu vme de pse tsc msr pae mce cx8 [...] fsrm</code></pre>
<p>Some of the processors that have this feature include:</p>
<ul>
<li>Ice Lake</li>
<li>Rocket Lake</li>
<li>Tiger Lake</li>
<li>Raptor Lake</li>
<li>Alder Lake</li>
<li>Sapphire Rapids</li>
</ul>
<blockquote>
<p>Note: This list may not be comprehensive, please see Intel advisory INTEL-SA-00950 for a complete list.</p>
</blockquote>
</section>
</section>
<section id="discovery">
<h2>Discovery</h2>
<p>I’ve written previously about a processor validation technique called <em>Oracle Serialization</em> that we’ve been using. The idea is to generate two forms of the same randomly generated program and verify their final state is identical.</p>
<blockquote>
<p>You can read more about Oracle Serialization in my <a href="https://lock.cmpxchg8b.com/zenbleed.html">previous writeup</a>.</p>
</blockquote>
<p>In August, our validation pipeline produced an interesting assertion. It had found a case where adding redundant <code>rex.r</code> prefixes to an FSRM optimized <code>rep movs</code> operation seemed to cause unpredictable results.</p>
<p>We observed some very strange behavior while testing. For example, branches to unexpected locations, unconditional branches being ignored and the processor no longer accurately recording the instruction pointer in <code>xsave</code> or <code>call</code> instuctions.</p>
<p>Oddly, when trying to understand what was happening we would see a debugger reporting impossible states!</p>
<p>This already seemed like it could be indicative of a serious problem, but within a few days of experimenting we found that when multiple cores were triggering the same bug, the processor would begin to report machine check exceptions and halt.</p>
<p>We verified this worked even inside an unprivileged guest VM, so this already has serious security implications for cloud providers. Naturally, we reported this to Intel as soon as we confirmed this was a security issue.</p>
<section id="reproduce">
<h4>Reproduce</h4>
<p>We’re publishing all of our research today to our <a href="https://github.com/google/security-research/">security research repository</a>. If you want to reproduce the vulnerability you can use our <code>icebreak</code> tool, I’ve also made a local mirror available <a href="https://lock.cmpxchg8b.com/files/icebreak.tar.gz">here</a>.</p>
<pre><code>$ ./icebreak -h
usage: ./icebreak [OPTIONS]
    -c N,M      Run repro threads on core N and M.
    -d N        Sleep N usecs between repro attempts.
    -H N        Spawn a hammer thread on core N.
icebreak: you must at least specify a core pair with -c! (see -h for help)</code></pre>
<p>The testcase enters what should be an infinite loop, and unaffected systems should see no output at all. On affected systems, a <code>.</code> is printed on each successful reproduction.</p>
<pre><code>$ ./icebreak -c 0,4
starting repro on cores 0 and 4
.........................................................................
.........................................................................
.........................................................................
.........................................................................
.........................................................................</code></pre>
<p>In general, if the cores are <abbr title="Symmetric Multithreading">SMT</abbr> siblings then you may observe random branches and if they’re <abbr title="Symmetric Multiprocessing">SMP</abbr> siblings from the same package then you may observe machine checks.</p>
<p>If you do <em>not</em> specify two different cores, then you might need to use a hammer thread to trigger a reproduction.</p>
</section>
<section id="analysis">
<h4>Analysis</h4>
<p>We know something strange is happening, but how microcode works in modern systems is a closely guarded secret. We can only theorize about the root cause based on observations.</p>
<section id="μops">
<h5>μops</h5>
<p>The CPU is split in two major components, the <em>frontend</em> and the <em>backend</em>. The frontend is responsible for fetching instructions, decoding them and generating μops to send to the backend for execution.</p>
<p>The backend executes instructions <em>out of order</em>, and uses a unit called the ROB, <em>reorder buffer</em>, to store and organize results.</p>
<p>We believe this bug causes the frontend to miscalculate the size of the <code>movsb</code> instruction, causing subsequent entries in the ROB to be associated with incorrect addresses. When this happens, the CPU enters a confused state that causes the instruction pointer to be miscalculated.</p>
<p>The machine can eventually recover from this state, perhaps with incorrect intermediate results, but becoming internally consistent again. However, if we cause multiple SMT or SMP cores to enter the state simultaneously, we can cause enough microarchitectural state corruption to force a machine check.</p>
</section>
</section>
<section id="questions">
<h4>Questions</h4>
<p>I’m sure some readers will have questions about what is possible in this unexpected “glitch” state. Well, so do we!</p>
<p>We know that we can corrupt the system state badly enough to cause machine check errors, and we’ve also observed threads interfere with execution of processes scheduled on SMT siblings.</p>
<p>However, we simply don’t know if we can control the corruption precisely enough to achieve privilege escalation. I suspect that it <em>is</em> possible, but we don’t have any way to debug μop execution!</p>
<p>If you’re interested in studying this, then we would love to get your input!</p>
</section>
<section id="credit">
<h4>Credit</h4>
<p>This bug was independently discovered by multiple research teams within Google, including the <a href="https://github.com/google/silifuzz">silifuzz</a> team and Google <a href="https://bughunters.google.com/blog">Information Security Engineering</a>. The bug was analyzed by Tavis Ormandy, Josh Eads, Eduardo Vela Nava, Alexandra Sandulescu and Daniel Moghimi.</p>
</section>
</section>
<section id="solution">
<h2>Solution</h2>
<p>Intel have <a href="https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00950.html">published</a> updated microcode for all affected processors. Your operating system or BIOS vendor may already have an update available!</p>
<section id="workaround">
<h4>Workaround</h4>
<p>If you can’t update for some reason, you <em>could</em> disable fast strings via the <code>IA32_MISC_ENABLE</code> model specific register.</p>
<p>This will cause a significant performance penalty, and should not be used unless absolutely necessary.</p>
</section>
</section>
<section id="notes">
<h2>Notes</h2>
<p>If you’re interested in more CPU bugs, we publish everything we find!</p>
<p>Not all the bugs we discover have security consequences, but they’re usually worth reading! For example, did you know that sometimes <a href="https://github.com/google/security-research/tree/master/pocs/cpus/errata/amd/genoa-lps-hps">movlps just doesn’t work</a>? or that registers can sometimes <a href="https://github.com/google/security-research/tree/master/pocs/cpus/errata/amd/1386">roll back</a> to previous values?</p>
</section>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rivian software update bricks infotainment system, fix not obvious (221 pts)]]></title>
            <link>https://electrek.co/2023/11/14/rivian-software-update-bricks-infotainment-system-fix-not-obvious/</link>
            <guid>38266340</guid>
            <pubDate>Tue, 14 Nov 2023 17:22:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2023/11/14/rivian-software-update-bricks-infotainment-system-fix-not-obvious/">https://electrek.co/2023/11/14/rivian-software-update-bricks-infotainment-system-fix-not-obvious/</a>, See on <a href="https://news.ycombinator.com/item?id=38266340">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
			<img src="https://electrek.co/wp-content/uploads/sites/3/2023/09/Rivian-R1T-Apocalypse-13.jpg?quality=82&amp;strip=all&amp;w=1500" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2023/09/Rivian-R1T-Apocalypse-13.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2023/09/Rivian-R1T-Apocalypse-13.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2023/09/Rivian-R1T-Apocalypse-13.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2023/09/Rivian-R1T-Apocalypse-13.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" width="1500" height="1000" alt="Rivian-R1T-Apocalypse" fetchpriority="high">
	
	</figure>

<p>On Monday, Rivian released an incremental software update 2023.42, which bricked the infotainment system in R1Ses and R1Ts. The company is frantically working on a fix, but it might not be an OTA…</p>



<p>Update: The company has acknowledged the issue with affected customers but has yet to issue a fix or plan to fix…</p>



<p><a href="https://www.linkedin.com/in/wassymbensaid/">Rivian’s vice president of software engineering</a>, Wassim Bensaid, <a href="https://www.reddit.com/r/Rivian/comments/17usikn/202342_ota_update_issue/">took to Reddit</a> (!?) to update users on the situation.</p>



<blockquote>
<p>Hi All,</p>



<p>We made an error with the 2023.42 OTA update – a fat finger where the wrong build with the wrong security certificates was sent out. We cancelled the campaign and we will restart it with the proper software that went through the different campaigns of beta testing.</p>



<p>Service will be contacting impacted customers and will go through the resolution options. That may require physical repair in some cases.</p>



<p>This is on us – we messed up. Thanks for your support and your patience as we go through this.</p>



<p>* Update 1 (11/13, 10:45 PM PT): The issue impacts the infotainment system. In most cases, the rest of the vehicle systems are still operational. A vehicle reset or sleep cycle will not solve the issue. We are validating the best options to address the issue for the impacted vehicles. Our customer support team is prioritizing support for our customers related to this issue. Thank you.</p>
</blockquote>



<p>That’s the last update we had over 10 hours after Rivian customer vehicles were fed the bad software update. Rivian’s PR team hasn’t replied to requests for comment. </p>



<p>The vehicles are drivable, but software and displays go black. It appears that the 2023.42 software update hangs at 90% on the vehicle screen or 50% on the app screen, and then the vehicle screens black out. All systems appear to still work except for the displays.</p>



<p>One Reddit user noted:</p>



<blockquote>
<p>Remotely setting climate controls appear to be working for me. You can’t adjust it while seated in the vehicle (feature to prevent competing for changes from inside and outside the vehicle?) but for those in cold/hot weather, you can at least pre-set and pre-heat/cool your vehicle even without the infotainment – at least if yours ended up in the same state that mind did.</p>
</blockquote>



<figure></figure>



<p>At the moment, it appears that Amazon vans are not impacted. We reached out to Rivian for comment and got the following response from a spokesperson for the automaker:</p>



<blockquote>
<p>We’ve identified an issue in our recent software update 2023.42.0 that impacts Rivian’s R1T and R1S infotainment system. In most cases, the rest of the vehicle systems are operational. We’ve paused the release of the update and our customer support team is prioritizing support for our customers related to this issue.</p>
</blockquote>



<p>This story is ongoing, and we will update as such.</p>



<p>Update: Rivian has been Texting and emailing affected customers</p>



<figure></figure>



<h2 id="h-electrek-s-take">Electrek’s Take</h2>




	<p>This is a big deal, and not for whatever this fix ends up being, but about trusting the Rivian software team to deliver stuff that won’t break your car. </p>



<p>The fact that this could even happen is very troubling. A bad certificate should not be pushed via a fat thumb that causes the fleet to be bricked. The architecture shouldn’t have been designed this way. </p>



<p>I personally tried updating my vehicle last night before the errors were reported, and if I had been a few minutes earlier, I would also now have a bricked infotainment with the closest software center 200+ miles away and my vehicle covered in a few inches of fresh Vermont snow. </p>



<p>Hopefully, Rivian’s software team (who have probably had an awful night) can come up with an OTA or, more likely, a USB-stick type of update that affected owners can easily and quickly apply. </p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<div><p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p><p><a href="https://bit.ly/3N3YHyD"><img src="https://electrek.co/wp-content/uploads/sites/3/2023/11/VMAX-Electrek-Banner-Nov-13.jpg?quality=82&amp;strip=all" alt="" width="750" height="150"></a></p></div>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Miyazaki's 'The Boy and the Heron' makes clear the world is a fragile place (113 pts)]]></title>
            <link>https://www.latimes.com/entertainment-arts/awards/story/2023-11-13/japanese-animation-master-hayao-miyazaki-boy-and-the-heron</link>
            <guid>38266216</guid>
            <pubDate>Tue, 14 Nov 2023 17:14:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.latimes.com/entertainment-arts/awards/story/2023-11-13/japanese-animation-master-hayao-miyazaki-boy-and-the-heron">https://www.latimes.com/entertainment-arts/awards/story/2023-11-13/japanese-animation-master-hayao-miyazaki-boy-and-the-heron</a>, See on <a href="https://news.ycombinator.com/item?id=38266216">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-element="story-body" data-subscriber-content=""> <p>When Hayao Miyazaki’s acclaimed new movie “The Boy and the Heron” made its North American premiere at the Toronto International Film Festival in September, one of the Japanese master’s biggest fans stepped on stage to introduce the long-awaited movie. “We are privileged enough to be living in a time where Mozart is composing symphonies,” said Oscar-winning filmmaker Guillermo del Toro. “Miyazaki <i>san</i> is a master of that stature, and we are so lucky to be here. He has changed the medium that he started in, revolutionized it, proved over and over again that it is a tremendous work of art.”</p><p>“Miyazaki, in my estimation, is the greatest director of animation ever, and he has made his films as full of dialogues and questions as he is,” continued Del Toro. “These are not easy films, but these are films that portray him so intimately, that you feel you’re having a conversation with him. And they are paradoxical because he understands that beauty cannot exist without horror, and delicacy cannot exist without brutality.”</p><p>Fans of the 82-year-old writer-director-animator and co-founder of Studio Ghibli are keenly aware of the magical powers of Miyazaki’s cinematic universe. Perhaps more than any other filmmaker alive today, he has been able to transport audiences to dreamlike worlds that have a distinctive, hand-crafted look and strange logic of their own. Yet, they speak to audiences all over the world because their main characters experience the full spectrum of emotions and experiences. The lead characters of the director’s 12 movies, which include the Oscar-winning “Spirited Away,” “My Neighbor Totoro” and “Princess Mononoke” stumble upon ethereal worlds and supernatural characters, but they are bound by human feelings of joy, love, fear and grief.</p><p>It’s a testament to the enduring appeal of the filmmaker that his new movie broke box-office records in Japan with little advance marketing or publicity ahead of its July release date. (As of early this month, the film has made over $63.5 million worldwide). The only clues potential audiences had about the film was a poster featuring the film’s very odd titular bird and the fact that its original title, “How Do You Live?,” was taken from a Genzaburō Yoshino book about a 15-year-old boy who has to learn about survival in a world consumed by death, war and sadness.</p><p>Echoing some of the themes and plotlines from Miyazaki’s oeuvre, the movie centers on a young Mahito, who encounters magical creatures and talking animals in a surreal world — like a cross between Lewis Carroll’s “Alice in Wonderland” and Ingmar Bergman’s “Fanny and Alexander.” The boy, who has lost his mother in a hospital fire in Tokyo, has a tough time adjusting to life with his father’s new wife — his mother’s younger sister — and their new home in the countryside. </p><p>Then, there’s the strange gray heron who badgers him and claims that his mother is still alive, and the mysterious tower built by Mahito’s long-vanished granduncle. This is all before our young hero enters the alternate world of the Tower, where he encounters menacing man-eating parakeets, younger versions of some of the characters from his real life, and cute bubble-like souls called the Warawara, who have to float to the surface to be born in Mahito’s world.</p><div data-click="enhancement" data-align-center=""><figure> <picture>    <source type="image/webp" srcset="https://ca-times.brightspotcdn.com/dims4/default/a05e864/2147483647/strip/true/crop/2024x1162+0+0/resize/320x184!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 320w,https://ca-times.brightspotcdn.com/dims4/default/853e000/2147483647/strip/true/crop/2024x1162+0+0/resize/568x326!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 568w,https://ca-times.brightspotcdn.com/dims4/default/88040b3/2147483647/strip/true/crop/2024x1162+0+0/resize/768x441!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 768w,https://ca-times.brightspotcdn.com/dims4/default/3f079e6/2147483647/strip/true/crop/2024x1162+0+0/resize/1024x588!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 1024w,https://ca-times.brightspotcdn.com/dims4/default/edc3840/2147483647/strip/true/crop/2024x1162+0+0/resize/1200x689!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 1200w" sizes="100vw">     <img alt="A girl chases two small creatures through a tunnel of trees in the animated film &quot;My Neighbor Totoro.&quot;" srcset="https://ca-times.brightspotcdn.com/dims4/default/b44e202/2147483647/strip/true/crop/2024x1162+0+0/resize/320x184!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 320w,https://ca-times.brightspotcdn.com/dims4/default/f1195c4/2147483647/strip/true/crop/2024x1162+0+0/resize/568x326!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 568w,https://ca-times.brightspotcdn.com/dims4/default/8cb3eea/2147483647/strip/true/crop/2024x1162+0+0/resize/768x441!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 768w,https://ca-times.brightspotcdn.com/dims4/default/0b3e89e/2147483647/strip/true/crop/2024x1162+0+0/resize/1024x588!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 1024w,https://ca-times.brightspotcdn.com/dims4/default/62c89d0/2147483647/strip/true/crop/2024x1162+0+0/resize/1200x689!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 1200w" sizes="100vw" width="1200" height="689" src="https://ca-times.brightspotcdn.com/dims4/default/62c89d0/2147483647/strip/true/crop/2024x1162+0+0/resize/1200x689!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg" decoding="async" loading="lazy">  </picture>  <div>   <p>Mei follows two small spirits in “My Neighbor Totoro.”</p>   <p>(© 1988 Studio Ghibli / GKids)</p>   </div>  </figure></div><p>As Del Toro noted in a recent interview, “Miyazaki proves again and again that his films are not about leaving you chirpy: It’s about showing you the sweetness and sourness of life— the loss, the love and the beauty all at the same time. Hitchcock said repetition with consciousness is style: Miyazaki is saying, ‘I didn’t get it quite right in that movie. Let me get it right on this one.’ Rhythmically, he is very contemplative. This is an old master who has dispensed with tools and flourishes. They stop trying to impress. They offer a simple and beautiful gesture with a brush.”</p><p>Indie distributor GKIDS, which handles Studio Ghibli titles in the U.S., will release the movie on Dec. 8, following special preview engagements in New York and Los Angeles next week. The English-language voice cast includes Florence Pugh, Christian Bale, Dave Bautista, Gemma Chan, Mark Hamill, Karen Fukuhara, Willem Dafoe and Luca Padovan.</p><p>“I first saw ‘The Boy and the Heron’ at Studio Ghibli earlier this year, and after five minutes I began to cry,” recalls Eric Beckman, founder of GKIDS and L.A.’s Animation Is Film Festival. “There was another Miyazaki film in the world. What a gift. After ‘Totoro,’ ‘Spirited Away,’ ‘Princess Mononoke’ and ‘Howl’s Moving Castle,’ it’s easy to take for granted how unique Miyazaki’s gifts are. The beauty, the lusciousness, the humor and playfulness, the nuance are all here ... the movement of the heron in flight, of fish in the water, the tottering old aunties. Nothing can compare.”</p><p>Beckman agrees with many critics who have pointed out that “The Boy and the Heron” is Miyazaki’s most personal film by far. “In this movie, we have a great creator struggling to express something profound and important about life itself — both what to say and how to say it,” he says. “If ‘Totoro’ embodied a basic optimism — a celebration of the power and miraculous wonder of nature, a trust in nature — after 35 years, ‘Heron’ is not so optimistic about the state of the world anymore. This is not a kids’ cartoon or an episode in a superhero franchise. It is a work of cinematic art made by one of our greatest living filmmakers. Miyazaki is saying that the world is a fragile place, it may only last another day. What is the next step forward? That is up to the viewer to decide.”</p><p>In 2021, Miyazaki himself told the New York Times that he has a simple goal in mind whenever he sets out to make a new movie. “The mission of my films is to comfort you — to fill in the gap that might be in your heart or your everyday life.” When he was asked about the answer to the Japanese title of his movie “How Do You Live?” Miyazaki <i>san</i> responded, “I am making this movie because I do not have the answer.”</p><p>He might not have provided us with an easy answer, but he has once again managed to fill our hearts with the same sense of wonder we felt the first time we entered one of his remarkable realms.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YJIT is the most memory-efficient Ruby JIT (178 pts)]]></title>
            <link>https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/</link>
            <guid>38265773</guid>
            <pubDate>Tue, 14 Nov 2023 16:46:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/">https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/</a>, See on <a href="https://news.ycombinator.com/item?id=38265773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>This year, the YJIT team and I have gotten a paper accepted at
<a href="https://2023.splashcon.org/home/mplr-2023">MPLR 2023</a> (Managed Programming Languages and Runtimes),
which is now freely available through <a href="https://dl.acm.org/doi/10.1145/3617651.3622982">ACM open access</a>.
The paper, titled <em>“Evaluating YJIT’s Performance in a Production Context: A Pragmatic Approach”</em>, goes
into details of the strategy taken to evaluate YJIT’s performance in a production context.
One of our key findings, when comparing YJIT against other existing Ruby JITs such as JRuby
and TruffleRuby, is that YJIT is the most memory-efficient Ruby JIT (by a long shot).</p>

<p>A video recording of our presentation at MPLR is also
<a href="https://www.youtube.com/watch?v=fMGuQXNqlaE&amp;t=9900s">available on YouTube</a>.</p>

<h2 id="background">Background</h2>

<p>Many published papers about JIT compilers only look at peak performance in terms of
running time on benchmarks after a certain amount of warm-up time.
This can be deceptive because the amount of time needed for a JIT compiler to warm up can be
arbitrarily long. Typically, the JIT compiler implementation is given as many benchmark
iterations as it needs to reach peak performance, and the peak performance as measured then
is reported. The amount of time needed to reach peak performance is often not discussed.
The same goes for memory usage.</p>

<p>I believe that
those metrics are typically ignored by academic compiler researchers
because they may reveal an inconvenient reality. If you give your JIT compiler
an arbitrary amount of time and memory to reach peak performance, it’s easier to throw all
possible known optimizations at a piece of code and reach high peak performance numbers.
However, if your JIT compiler uses an arbitrarily high amount of memory and needs a very long
time to warm up, even though it may have the fastest peak performance, it may be
unusable in most real-world production environments.</p>

<p>When deploying code into production, peak performance is not the only thing that matters.
On our production servers at Shopify, there is not a huge amount of memory available for
the JIT compiler to use. Almost all of the memory is used to run multiple server processes, and
also to cache various resources in RAM. This has forced us to spend a significant amount of
effort on optimizing YJIT’s memory usage to make the compiler more resource-efficient.</p>

<p>At Shopify, we deploy frequently, with consecutive deployments sometimes less than 20 minutes
apart. This adds an extra layer of challenge because, despite these frequent deployments,
we can’t tolerate significant increases in response time.
If a JIT compiler needs a significant amount of time to
warm up, or suddenly deoptimizes large amounts of code, this can translate into requests timing out, and customers
abandoning their shopping carts, which ultimately would result in lost revenue. As such, smooth,
predictable warm-up and stable performance are of critical importance.</p>

<h2 id="methodology">Methodology</h2>

<p>In our paper, we look at YJIT’s warm-up, memory usage and peak performance on benchmarks,
as well as on our deployment on Shopify’s StoreFront Renderer (SFR). For context, SFR
renders all Shopify storefronts, which is the first thing buyers see when they navigate to a store hosted by Shopify.
It is mostly written in Ruby, depends on over 220 Ruby gems, renders over 4.5
million Shopify stores in over 175 countries, and is served by multiple clusters distributed worldwide. It is capable of serving over 1.27
million requests per second, and has processed over 197 billion USD in transaction
volume in 2022.
YJIT is currently deployed to all SFR servers. For this paper, we’ve performed
experiments using Ruby head on a subset of servers in all clusters. We’ve also
included some control servers which ran the same Ruby commit without YJIT. Data
for the SFR experiments was gathered over a 48-hour period.</p>

<p>For our experiments on benchmarks, we used 11 headline benchmarks from the
yjit-bench suite. These are all benchmarks that are based on real-world Ruby gems,
with a bias towards web workloads. This includes benchmarks such as <code>railsbench</code>,
<code>hexapdf</code>, <code>activerecord</code> and <code>liquid-render</code>, which is a benchmark using Shopify’s liquid
template language gem.
We benchmarked YJIT, RJIT (Takashi Kokubun’s experimental Ruby JIT written
in Ruby), JRuby, as well as both the JVM and native versions of TruffleRuby.
More details on our experimental setup
are provided in <a href="https://dl.acm.org/doi/10.1145/3617651.3622982">the paper</a>.</p>

<p>We also maintain a website, <a href="https://speed.yjit.org/">speed.yjit.org</a>, which tracks
YJIT’s performance and memory overhead as well as various other statistics on this
benchmark suite over time. Recently, as we were looking for more challenging and realistic
benchmarks, we’ve also turned the codebase of the
<a href="https://lobste.rs/">lobste.rs</a> website
<a href="https://railsatscale.com/2023-08-25-we-turned-lobsters-into-a-rails-benchmark-for-yjit/">into a benchmark</a> as well.</p>

<h2 id="key-findings">Key Findings</h2>

<h3 id="performance-on-benchmarks">Performance on Benchmarks</h3>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/bars_speedup.png" alt="" width="90%"></p>

<p>The above graph shows the average execution time on each benchmark for each of the Ruby JIT
implementations we benchmarked. The time is normalized to the time taken by the CRuby interpreter,
where the time taken by the interpreter has value 1.0, with values below 1 being faster than
the interpreter.
We were very generous in terms of warm-up time. Each benchmark was run for 1000 iterations, and
the first half of all the iterations were discarded as warm-up time, giving each JIT a more
than fair chance to reach peak performance.</p>

<p>As can be seen, TruffleRuby has the best peak performance on most (but not all) benchmarks.
YJIT outperforms the CRuby interpreter on every benchmark by a wide margin. We can also see
that YJIT performs very competitively compared to JRuby (a JVM-based Ruby JIT), outperforming
it on most benchmarks.</p>

<h3 id="warm-up-time-on-benchmarks">Warm-Up Time on Benchmarks</h3>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/railsbench_warmup.png" alt="" width="90%"></p>

<p>This graph shows a plot of the performance over time for each Ruby JIT for the <code>railsbench</code> benchmark.
The x-axis is the total execution time in seconds, while the y-axis is the time per benchmark iteration. This
allows us to visualize how the performance of each VM evolves over time. As can be seen, YJIT
almost immediately outperforms the CRuby interpreter, with RJIT not too far behind. JRuby takes over a
minute to reach peak performance, but does not reliably outperform CRuby on this benchmark.</p>

<p>TruffleRuby eventually outperforms the other JITs, but it takes about two minutes to do so. It is
also initially quite a bit slower than the CRuby interpreter, taking over 110
seconds to catch up to the interpreter’s speed. This would be
problematic in a production context such as Shopify’s, because it can lead to much slower
response times for some customers, which could translate into lost business. Such wide variations
in performance can also make the scaling of server resources more difficult. We should also note
that while <code>railsbench</code> is a somewhat challenging benchmark, it is much smaller than our actual
production deployment. Warm-up data for other benchmarks is provided in the paper.</p>

<h3 id="memory-usage-on-benchmarks">Memory Usage on Benchmarks</h3>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/bars_memory.png" alt="" width="90%"></p>

<p>The above graph is in my opinion the most interesting graph of the paper. It is a plot of the memory
usage (RSS) of each Ruby implementation for each benchmark.
Because of the widely varying scale between data points,
the use of a logarithmic was considered <img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/trollface.png" alt="" height="15">.
However, we have decided to use a
linear scale to maintain a more accurate sense of proportions. Do note, however, that there is
a cut in the graph between 5GiB and 17GiB.</p>

<p>As can be seen in the graph above, thanks in large part to the work put in by our team to
optimize its memory
overhead, YJIT has the lowest memory overhead of any Ruby JIT by far. The JVM-based
Ruby implementations often use one or even two orders of magnitude more memory than YJIT and the
CRuby interpreter. The memory overhead compared to CRuby can be as much as several gigabytes.
This is on benchmarks that often require less than 100MiB to run with the CRuby interpreter,
which makes such a high amount of memory overhead seem disproportionate.</p>

<p>One significant caveat here is that we are measuring the total memory usage of each system. That
is, the memory overhead of the JVM itself has an impact. The way that JRuby and TruffleRuby
internally represent Ruby objects in memory, which is different from the way CRuby represents
objects in memory, also has an impact.
However, the bottom line is the same. Using several gigabytes more memory than CRuby to run simple
benchmarks likely bodes poorly for many production deployments.
For smaller production deployments, for example, a project running on inexpensive Virtual Private Servers (VPS),
there may be only 1GiB or 2GiB of RAM available in total.
For a company like Shopify running a large fleet of servers,
the amount of server processes that can be run on a single machine, and how much memory can be used
for caching matters.</p>

<p>There is another caveat, which is that JRuby and TruffleRuby, unlike CRuby, do not use a GVL
(Global VM Lock, analogous to Python’s GIL). In theory, this means that they can more effectively
use multithreading, and amortize some of their memory overhead across multiple server threads.
Still, there is a case to be made that the memory overhead of JRuby and TruffleRuby is something
that is often overlooked and probably should be better optimized.
Aside from production deployments, the <code>ruby-lsp</code> benchmark is a benchmark of the Ruby language
server, which can be used by code editors such as VSCode.
We can see that on this benchmark, the JVM-based Ruby implementations can use multiple gigabytes
of memory, and despite that, perform worse than the CRuby interpreter. This is far from ideal
for a Ruby gem that is meant to be run on developer laptops.</p>

<p>I would also like to note that RJIT, Takashi Kokubun’s experimental pure Ruby JIT, looks
quite good in this comparison. However, in the previous graph, the inclusion of the
JVM-based Ruby JITs distorts the sense of scale. The next graph below shows the same memory
usage comparison, but with only CRuby, YJIT and RJIT included. Currently, there are
situations where RJIT uses several times more memory than CRuby and YJIT:</p>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/bars_mem_yjit_rjit_only.png" alt="" width="90%"></p>

<p>YJIT, being that it is written in Rust (a systems language), has
access to more tools to optimize memory usage in places where individual bits count. Matching
YJIT’s memory usage in a pure Ruby JIT would be difficult and likely would necessitate
augmenting Ruby with special systems programming primitives. For example, to be able to
efficiently pack structs and bit fields in memory and to also pack structs and
arrays inside of other structs.
Encoding data structures in memory as compactly as possible is likely challenging to do
in a JVM-based JIT implementation as well.</p>

<h3 id="performance-in-production">Performance In Production</h3>

<p>The following graph looks at the latency of YJIT on our SFR deployment when compared to
servers that run the same Ruby commit with YJIT disabled. If you are wondering why no other
Ruby JITs are included in this graph, it is because, at this time, other Ruby JITs could not
be deployed in production for this application, either due to compatibility issues or due to memory constraints.
On average, YJIT is 14.1% faster than the CRuby interpreter during the period examined.
Importantly, because YJIT is able to compile new code very fast, it is also faster than the
interpreter even on the slowest p99 requests.</p>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/sfr_latency.png" alt="" width="90%"></p>

<p>If a 14.1% speedup seems underwhelming to you, do keep in mind that the latency numbers
provided measure the total time needed to generate a response. This includes not only
time spent in JIT-compiled code, but also time spent in C functions that we cannot optimize,
and time the server spends waiting for database requests and other I/O operations.</p>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/sfr_speedup.png" alt="" width="90%"></p>

<p>The graph above shows the speedup provided by YJIT over the interpreter. The purple vertical
lines represent deployments.
During the time period we examined, there were 35 deployments of new code to production,
and the shortest interval during consecutive deployments was just 19 minutes 21 seconds.
The key takeaway here is that even though we perform frequent
deployments during the daytime, because YJIT is able to warm up very fast, it remains
consistently faster than the interpreter.
Again, more information is provided in <a href="https://dl.acm.org/doi/10.1145/3617651.3622982">the paper</a>,
including data about YJIT’s memory usage in our production deployments.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We’ve recently published a paper about YJIT at MPLR 2023, in which we evaluate YJIT’s performance
on both benchmarks as well as on Shopify’s flagship production deployment, which serves an
enormous amount of traffic worldwide.
In this paper, we make it a point to examine not just peak performance, but to also discuss and
evaluate warm-up time and total memory usage.</p>

<p>The YJIT team has spent a significant amount of effort optimizing YJIT so that it doesn’t just
show good peak performance numbers, but also does this while being memory-efficient.
This effort has paid off, with YJIT having the least memory overhead of any Ruby JIT, which
has been crucial in enabling YJIT to handle Shopify’s SFR deployment.</p>

<p>Since our MPLR paper was published, we’ve kept improving
YJIT’s performance.
As of this writing, I am looking at our internal dashboard, and YJIT is providing a 27.2% average
speedup over the interpreter on our SFR deployment.
With the Ruby 3.3 release approaching, there will be a lot to be excited about this holiday season,
as we are once
again gearing up for a very strong Ruby release. This year, YJIT 3.3 will deliver better performance,
while using less memory, and also warming up faster than YJIT 3.2.
Expect another post on the <a href="https://railsatscale.com/">Rails at Scale blog</a> with more benchmark results soon!</p>

<p>For more information on how to use YJIT, see the <a href="https://github.com/ruby/ruby/blob/master/doc/yjit/yjit.md">YJIT README</a>.
Should you wish to cite <a href="https://dl.acm.org/doi/10.1145/3617651.3622982">our MPLR 2023 paper</a>, I’ve also
included the bibtex snippet below:</p>

<div><pre><code>@inproceedings{yjit_mplr_2023,
author = {Chevalier-Boisvert, Maxime and Kokubun, Takashi and Gibbs, Noah and Wu, Si Xing (Alan) and Patterson, Aaron and Issroff, Jemma},
title = {Evaluating YJIT’s Performance in a Production Context: A Pragmatic Approach},
year = {2023},
isbn = {9798400703805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617651.3622982},
doi = {10.1145/3617651.3622982},
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes},
pages = {20–33},
numpages = {14},
keywords = {dynamically typed, optimization, just-in-time, virtual machine, ruby, compiler, bytecode},
location = {Cascais, Portugal},
series = {MPLR 2023}
}
</code></pre></div>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[.NET 8 (286 pts)]]></title>
            <link>https://devblogs.microsoft.com/dotnet/announcing-dotnet-8/</link>
            <guid>38264937</guid>
            <pubDate>Tue, 14 Nov 2023 16:00:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-8/">https://devblogs.microsoft.com/dotnet/announcing-dotnet-8/</a>, See on <a href="https://news.ycombinator.com/item?id=38264937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="featured">
                         <p>
            November 14th, 2023</p><!-- .entry-meta -->
        
<p>We are happy to announce the availability of <a href="https://aka.ms/get-dotnet-8">.NET 8</a>, the latest <a href="https://dotnet.microsoft.com/platform/support/policy/dotnet-core#release-types">LTS</a> version of one of the world’s leading development platforms, starting today. .NET 8 delivers thousands of performance, stability, and security improvements, as well as platform and tooling enhancements that help increase developer productivity and speed of innovation. The .NET team, our partners, and the .NET community will be talking about what’s new in .NET 8 as well as what people are building with .NET today to meet their needs of tomorrow at  <a href="https://www.dotnetconf.net/">.NET Conf 2023, a three day virtual event (November 14-16)</a>. Come, join us!</p>
<p><a href="https://dotnet.microsoft.com/download/dotnet/8.0"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Banner3.png" alt=".NET 8 is now available" width="600"></a></p>
<p>With this release, .NET reshapes the way we build intelligent, cloud-native applications and high-traffic services that scale on demand. Whether you’re deploying to Linux or Windows, using containers or a cloud app model of your choice, .NET 8 makes building these apps easier. It includes a set of proven libraries that are used today by the many high-scale services at Microsoft to help you with fundamental challenges around observability, resiliency, scalability, manageability, and more.</p>
<p><a href="https://aka.ms/aspireannouncement"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Cloud-Native-dotnet8.png" alt=".NET 8 - Cloud Native Features"></a></p>
<p>Integrate large language models (LLMs) like OpenAI’s GPT directly into your .NET app. Use a single powerful component model to handle all your web UI needs with Blazor. Deploy your mobile applications to the latest version of iOS and Android with .NET MAUI. Discover new language enhancements that make your code more concise and expressive with C# 12.  </p>
<p>Let’s look at what’s new in .NET 8. </p>
<h2 id="unparalleled-performance-experience-the-fastest-net-to-date">Unparalleled Performance – Experience the fastest .NET to date</h2>
<p>.NET 8 comes with thousands of performance <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-8/">improvements</a> <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-aspnet-core-8/">across</a> <a href="https://devblogs.microsoft.com/dotnet/dotnet-8-performance-improvements-in-dotnet-maui/">the</a> <a href="https://devblogs.microsoft.com/dotnet/this-arm64-performance-in-dotnet-8/">stack</a>. A new code generator called Dynamic Profile-Guided Optimization (PGO) that optimizes your code based on real-world usage is enabled by default and can improve the performance of your apps up to 20%. The AVX-512 instruction set, which is now supported, enables you to perform parallel operations on 512-bit vectors of data, meaning you can process much more data in less time. The primitive types (numerical and beyond) now implement a new formattable and parsable interface, which enable them to directly format and parse as UTF-8 without any transcoding overhead.</p>
<p>Every year we talk about the performance gains across .NET. This year we continue our quest to push the performance of .NET to new heights. From the latest TechEmpower benchmarks with .NET 8, we’re seeing improvements in the JSON API scenario of 18%, hitting nearly one million requests per second with ASP.NET Core Minimal APIs.</p>
<p><a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-8/"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Performance2.png" alt="TechEmpower requests per second (RPS)"></a></p>
<p>The Fortunes scenario is closer to a real-world workload, including database access and server-side HTML rendering. In this test, we see an even larger improvement of 24%, now over 300K requests per second with ASP.NET Core.</p>
<h2 id="net-aspire-an-opinionated-stack-to-build-observable-production-ready-cloud-native-applications">.NET Aspire – An opinionated stack to build observable, production-ready cloud-native applications</h2>
<p>.NET Aspire is a stack for building resilient, observable, and configurable cloud-native applications with .NET. It includes a curated set of components enhanced for cloud-native by including telemetry, resilience, configuration, and health checks by default. Combined with a sophisticated but simple local developer experience, .NET Aspire makes it easy to discover, acquire, and configure essential dependencies for cloud-native applications on day 1 as well as day 100. The <a href="https://aka.ms/aspireannouncement">first preview</a> of .NET Aspire is available today.</p>
<p><a href="https://aka.ms/aspireannouncement"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Aspire-CTAs.png" alt=".NET Aspire"></a></p>
<h2 id="net-8-container-enhancements-more-secure-compact-and-productive">.NET 8 Container Enhancements – More secure, compact, and productive</h2>
<p>Package your applications with <a href="https://devblogs.microsoft.com/dotnet/securing-containers-with-rootless/">containers more easily and more securely than ever with .NET</a>. Every .NET image includes a non-root user, enabling more secure containers with one-line configuration. The .NET SDK tooling publishes container images without a Dockerfile and are non-root by default. Deploy your containerized apps faster due to smaller .NET base images – including new experimental variants of our images that deliver truly minimal application sizes for native AOT. Opt-in to even more security hardening with the new Chiseled Ubuntu image variants to reduce your attack surface even further. Using Dockerfiles or SDK tooling, build apps and container images for any architecture.</p>
<p><a href="https://devblogs.microsoft.com/dotnet/securing-containers-with-rootless/"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Containers2.png" alt="Modern containers"></a></p>
<h2 id="native-aot-journey-towards-higher-density-sustainable-compute">Native AoT – Journey towards higher density sustainable compute</h2>
<p>Compile your .NET apps into <a href="https://learn.microsoft.com/dotnet/core/deploying/native-aot">native code</a> that uses less memory and starts instantly. No need to wait for the JIT (just-in-time) compiler to compile the code at run time. No need to deploy the JIT compiler and IL code. AOT apps deploy just the code that’s needed for your app. Your app is now empowered to run in restricted environments where a JIT compiler isn’t allowed.</p>
<p><a href="https://learn.microsoft.com/dotnet/core/deploying/native-aot"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/AOTOptimizations3.png" alt="Before and After AOT"></a></p>
<h2 id="artificial-intelligence-infuse-ai-into-your-net-applications">Artificial Intelligence – Infuse AI into your .NET applications</h2>
<p>Generative AI and large language models are transforming the field of AI, providing developers the ability to create unique AI-powered experiences in their applications. <a href="https://aka.ms/dotnet-genai">.NET 8 makes it simple for you to leverage AI</a> via first-class out-of-the box AI features in the .NET SDK and seamless integration with several tools. </p>
<p>.NET 8 brings several enhancements to the <code>System.Numerics</code> library to improve its compatibility with Generative AI workloads, such as integrating Tensor Primitives. With the rise of AI-enabled apps, new tools and SDKs emerged. We collaborated with numerous internal and external partners, such as <a href="https://azure.microsoft.com/products/ai-services/openai-service">Azure OpenAI</a>, <a href="https://azure.microsoft.com/free/ai-services/?ef_id=_k_b34c5d449bf4175800e738086ecc7267_k_&amp;OCID=AIDcmm5edswduu_SEM__k_b34c5d449bf4175800e738086ecc7267_k_&amp;msclkid=b34c5d449bf4175800e738086ecc7267">Azure Cognitive Search</a>, <a href="https://milvus.io/docs/v2.2.x/install-csharp.md">Milvus</a>, <a href="https://github.com/qdrant/qdrant-dotnet">Qdrant</a>, and <a href="https://github.com/microsoft/teams-ai">Microsoft Teams</a>, to ensure .NET developers have easy access to various AI models, services, and platforms through their respective SDKs. Additionally, the open-source <a href="https://learn.microsoft.com/semantic-kernel/overview/">Semantic Kernel</a> SDK simplifies the integration of these AI components into new and existing applications, to help you deliver innovative user experiences.</p>
<p>Various samples and reference templates, showcasing patterns and practices, are now available to make it easy for developers to get started:</p>
<ul>
<li><a href="https://github.com/dotnet-architecture/eShop">Customer Chatbot</a></li>
<li><a href="https://github.com/Azure-Samples/azure-search-openai-demo-csharp">Retrieval Augmented Generation</a></li>
<li><a href="https://devblogs.microsoft.com/dotnet/demystifying-retrieval-augmented-generation-with-dotnet/">Developing Apps using Azure AI services</a> </li>
</ul>
<p><a href="https://github.com/Azure-Samples/azure-search-openai-demo-csharp/assets/2546640/b79090b8-6a8b-45f4-b42b-e21e22b1661a"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/InfuseAIdotnetapps1.png" alt="Infuse AI dotnet apps"></a></p>
<h2 id="blazor-build-full-stack-web-applications-with-net">Blazor – Build full stack web applications with .NET</h2>
<p>Blazor in .NET 8 can use both the server and client together to handle all your web UI needs. It’s full stack web UI! With several new enhancements focused towards optimizing page load time, scalability, and elevating the user experience, developers can now use <a href="https://dotnet.microsoft.com/apps/aspnet/web-apps/blazor">Blazor Server and Blazor WebAssembly</a> in the same app, automatically shifting users from the server to the client at run time. Your .NET code runs significantly faster on WebAssembly thanks to the new “Jiterpreter”-based runtime and new built-in components. As a part enhancing the overall <a href="https://devblogs.microsoft.com/dotnet/whats-new-with-identity-in-dotnet-8/">authentication, authorization, and identity management in .NET 8</a>, Blazor now supports generating a full Blazor-based Identity UI.</p>
<p><a href="https://dotnet.microsoft.com/apps/aspnet/web-apps/blazor"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Blazor2.png" alt="Full stack Blazor"></a></p>
<h2 id="net-maui-elevated-performance-reliability-and-developer-experience"><a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-maui-in-dotnet-8">.NET MAUI – Elevated performance, reliability, and developer experience</a></h2>
<p>.NET MAUI provides you with a single project system and single codebase to build WinUI, Mac Catalyst, iOS, and Android applications. Native AOT (experimental) now supports targeting iOS-like platforms. <a href="https://aka.ms/maui-devkit-blog">A new Visual Studio Code extension for .NET MAUI</a> gives you the tools you need to develop cross-platform .NET mobile and desktop apps. Xcode 15 and Android API 34 are now supported allowing you to target the latest version of iOS and Android. A plethora of quality improvements were made to the <a href="https://devblogs.microsoft.com/dotnet/dotnet-8-performance-improvements-in-dotnet-maui">areas of performance</a>, controls and UI elements, and platform-specific behavior, such as desktop interaction adding better click handling, keyboard listeners, and more.</p>
<p><a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-maui-in-dotnet-8"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/dotnetMAUIin8-1.png" alt=".NET MAUI Visual Studio Code Extension"></a></p>
<h2 id="c-12-features-simplified-syntax-for-better-developer-productivity">C# 12 Features – Simplified syntax for better developer productivity</h2>
<p>C# 12 makes your coding experience more productive and enjoyable. You can now create primary constructors in any class and struct with a simple and elegant syntax. No more boilerplate code to initialize your fields and properties. Be delighted when creating arrays, spans, and other collection types with a concise and expressive syntax. Use new default values for parameters in lambda expressions. No more overloading or null checks to handle optional arguments. You can even use the <code>using</code> alias directive to alias any type, not just named types!</p>
<p><strong>Collection expressions</strong></p>
<pre><code>// Create a list:
List&lt;int&gt; a = [1, 2, 3, 4, 5, 6, 7, 8];

// Create a span
Span&lt;char&gt; b  = ['a', 'b', 'c', 'd', 'e', 'f', 'h', 'i'];

// Use the spread operator to concatenate
int[] array1 = [1, 2, 3];
int[] array2 = [4, 5, 6];
int[] array3 = [7, 8, 9];
int[] fullArray = [..array1, ..array2, ..array3]; // contents is [1, 2, 3, 4, 5, 6, 7, 8, 9]</code></pre>
<p>See more about the latest version of C# in <a href="https://devblogs.microsoft.com/dotnet/announcing-csharp-12">Announcing C# 12</a>.</p>

<p>We have a set of great tools that help you be the most productive in your development workflow and take advantage of .NET 8 today. Released alongside .NET 8, the <a href="https://aka.ms/vs/v178GA">Visual Studio 2022 17.8 release</a> brings support for .NET 8, C# 12 language enhancements, and various new productivity features. <a href="https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit">VS Code and C# Dev Kit</a> is a great way to get started with .NET 8 if you’re learning and/or want to quickly kick the tires of the runtime and is available on Linux, macOS, or in GitHub Codespaces. The new <a href="https://github.com/codespaces">GitHub Codespaces template for .NET</a>, which comes with the .NET SDK and a set of configured extensions, is one of the fastest ways to get started with .NET 8. </p>
<h3 id="additional-features-in-net-8">Additional features in .NET 8:</h3>
<ul>
<li><strong>ASP.NET Core.</strong> <a href="https://devblogs.microsoft.com/dotnet/whats-new-with-identity-in-dotnet-8/">Streamlines identity for single-page applications (SPA) and Blazor providing cookie-based authentication, pre-built APIs, token support, and a new identity UI.</a> and <a href="https://learn.microsoft.com/aspnet/core/release-notes/aspnetcore-8.0#minimal-apis">enhances minimal APIs with form-binding, antiforgery support to protect against cross-site request forgery (XSRF/CSRF), and <code>asParameters</code> support for parameter-binding with Open API definitions</a></li>
<li><strong>ASP.NET Core tooling.</strong> <a href="https://devblogs.microsoft.com/dotnet/aspnet-core-route-tooling-dotnet-8/">Route syntax highlighting, auto-completion, and analyzers to help you create Web APIs.</a></li>
<li><strong>Entity Framework Core.</strong> <a href="https://devblogs.microsoft.com/dotnet/announcing-ef8-rc2/">Provides new “complex types” as value objects, primitive collections, and SQL Server support for hierarchical data.</a></li>
<li><strong>NuGet.</strong> <a href="https://learn.microsoft.com/nuget/concepts/auditing-packages">Helps you audit your NuGet packages in projects and solutions for any known security vulnerabilities.</a></li>
<li><strong>.NET Runtime.</strong> <a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-8-rc1/#androidstripilafteraot-mode-on-android">Brings a new AOT compilation mode for WebAssembly (WASM) and Android.</a></li>
<li><strong>.NET SDK.</strong> <a href="https://learn.microsoft.com/dotnet/core/whats-new/dotnet-8#net-sdk">Revitalizes terminal build output and production-ready defaults.</a></li>
<li><strong>WPF.</strong> <a href="https://devblogs.microsoft.com/dotnet/wpf-file-dialog-improvements-in-dotnet-8/">Supports OpenFolderDialog</a> and <a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-8-rc1/#wpf-hardware-acceleration-in-rdp">Enabled HW Acceleration in RDP</a></li>
<li><strong>ARM64.</strong> <a href="https://devblogs.microsoft.com/dotnet/this-arm64-performance-in-dotnet-8/">Significant feature enhancements and improved code quality for ARM64 platforms through collaboration with ARM engineers.</a></li>
<li><strong>Debugging.</strong> <a href="https://devblogs.microsoft.com/dotnet/debugging-enhancements-in-dotnet-8/">Displays debug summaries and provides simplified debug proxies for commonly used .NET types.</a></li>
<li><strong>System.Text.Json.</strong> <a href="https://devblogs.microsoft.com/dotnet/system-text-json-in-dotnet-8/">Helps populate read-only members, customizes unmapped member handling, and improves Native AOT support.</a></li>
<li><strong>.NET Community Toolkit.</strong> <a href="https://devblogs.microsoft.com/dotnet/announcing-the-dotnet-community-toolkit-821/">Accelerates building .NET libraries and applications while ensuring they are trim and AOT compatible (including the MVVM source generators!)</a></li>
<li><strong>Azure</strong> <a href="https://aka.ms/appservice-dotnet8">Supports .NET 8 with Azure’s PaaS services like App Service for Windows and Linux, Static Web Apps, Azure Functions, and Azure Container Apps.</a></li>
<li><strong>What’s new in .NET 8.</strong> <a href="https://learn.microsoft.com/dotnet/core/whats-new/dotnet-8">Check out our documentation for everything else!</a></li>
</ul>
<h3 id="get-started-with-net-8">Get started with .NET 8</h3>
<p>For the best development experience with .NET 8, we recommend that you use the latest release of <a href="https://visualstudio.microsoft.com/downloads/">Visual Studio</a> and <a href="https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit">Visual Studio Code’s C# Dev Kit</a>. Once you’re set up, here are some of the things you should do:</p>
<ul>
<li><strong>Try the new features and APIs.</strong> <a href="https://dotnet.microsoft.com/download/dotnet/8.0">Download .NET 8</a> and <a href="https://github.com/dotnet/core/issues/new/choose">report issues in our issue tracker</a>.</li>
<li><strong>Test your current app for compatibility.</strong> Learn whether your app is <a href="https://learn.microsoft.com/dotnet/core/compatibility/8.0">affected by default behavior changes in .NET 8</a>.</li>
<li><strong>Test your app with opt-in changes.</strong> .NET 8 has <a href="https://learn.microsoft.com/dotnet/core/compatibility/8.0">opt-in behavior changes</a> that only affect your app when enabled. It’s important to understand and assess these changes early as they may become default in the next release.</li>
<li><strong>Update your app with the Upgrade Assistant.</strong> <a href="https://dotnet.microsoft.com/platform/upgrade-assistant">Upgrade your app with just a few clicks using the Upgrade Assistant</a>.</li>
<li><strong>Know you’re supported.</strong> .NET 8 is officially supported by Microsoft as a <a href="https://dotnet.microsoft.com/platform/support/policy/dotnet-core#release-types">long term support (LTS) release that will be supported for three years</a>.</li>
<li><strong>Bonus: eShop Sample for .NET 8.</strong> Follow all the best coding and architecture practices with our <a href="https://github.com/dotnet/eshop">new eShop sample, now updated for .NET 8</a>!</li>
</ul>
<h3 id="celebrate-net-8">Celebrate .NET 8</h3>
<ul>
<li><strong>.NET Conf 2023</strong>. <a href="https://www.dotnetconf.net/">Join us November 14-16, 2023 to celebrate the .NET 8 release!</a></li>
<li><strong>What’s next in .NET?</strong> <a href="https://dotnet.microsoft.com/next">Get involved and learn the latest news on .NET 8 and the next version of .NET.</a></li>
<li><strong>Get C# Certified</strong>. <a href="https://devblogs.microsoft.com/dotnet/announcing-foundational-csharp-certification/">Earn a badge of honor with a freeCodeCamp C# certification.</a></li>
<li><strong>Learn .NET 8</strong>. <a href="https://aka.ms/learn-dotnet-8">Free tutorials, videos, courses, and more for beginner through advanced .NET developers. All updated for .NET 8!</a></li>
<li><strong>See Developer Stories</strong>. <a href="https://devblogs.microsoft.com/dotnet/category/developer-stories/">Take a look at success stories of developers migrating to modern .NET.</a></li>
<li><strong>Read about why .NET?</strong>. <a href="https://devblogs.microsoft.com/dotnet/why-dotnet/">Read through our recent blog series about the convenience of .NET.</a></li>
</ul>

<p>We would just like to end by saying one big…</p>
<p><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/ThankYou.png" alt="https://dotnet.microsoft.com/thanks/8.0"></p>

        

		
        <div>

            <p><img width="60" height="60" data-src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-96x96.jpg" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-96x96.jpg" alt="" decoding="async" data-srcset="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-96x96.jpg 96w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-300x300.jpg 300w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-150x150.jpg 150w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-24x24.jpg 24w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-48x48.jpg 48w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth.jpg 648w" sizes="(max-width: 60px) 100vw, 60px" srcset="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-96x96.jpg 96w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-300x300.jpg 300w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-150x150.jpg 150w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-24x24.jpg 24w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-48x48.jpg 48w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth.jpg 648w"></p>
            <h3>
                <a data-bi-id="author follow within post" data-bi-area="" data-bi-name="Gaurav Seth" aria-label="Gaurav Seth" href="https://devblogs.microsoft.com/dotnet/author/gauravs">
                    Gaurav Seth                </a>
                <span>Partner Director of Product, Developer Platforms</span>
            </h3>
            
       </div>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GraphCast: AI model for faster and more accurate global weather forecasting (482 pts)]]></title>
            <link>https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/</link>
            <guid>38264641</guid>
            <pubDate>Tue, 14 Nov 2023 15:42:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/">https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/</a>, See on <a href="https://news.ycombinator.com/item?id=38264641">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      
  <article>
    
  

  
  
  
  
    

    
    
      
        <div>
          
            
            
              
              

<div>
    <div>
      <p>Research</p>
      

      
        <dl>
          
            <dt>Published</dt>
            <dd>
              <time datetime="2023-11-14">
                14 November 2023
              </time>
            </dd>
          
          
            <dt>Authors</dt>
            
          
        </dl>
      

      
    </div>

    
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="1072" height="603" srcset="https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w1072-h603-n-nu-rw 1x, https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w2144-h1206-n-nu-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="928" height="522" srcset="https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w928-h522-n-nu-rw 1x, https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w1856-h1044-n-nu-rw 2x"><source type="image/webp" width="528" height="297" srcset="https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w528-h297-n-nu-rw 1x, https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w1056-h594-n-nu-rw 2x">
      <img alt="GraphCast global weather forecasting of surface wind speed" height="603" src="https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w1072-h603-n-nu" width="1072">
    </picture>
    
  
  </div>
            
          
            
            
              
              <div>
  <p data-block-key="pnebs"><b>Our state-of-the-art model delivers 10-day weather predictions at unprecedented accuracy in under one minute</b></p><p data-block-key="f3n9e">The weather affects us all, in ways big and small. It can dictate how we dress in the morning, provide us with green energy and, in the worst cases, create storms that can devastate communities. In a world of increasingly extreme weather, fast and accurate forecasts have never been more important.</p><p data-block-key="baim">In a paper <a href="https://www.science.org/doi/10.1126/science.adi2336" rel="noopener" target="_blank">published in Science</a>, we introduce GraphCast, a state-of-the-art AI model able to make medium-range weather forecasts with unprecedented accuracy. GraphCast predicts weather conditions up to 10 days in advance more accurately and much faster than the industry gold-standard weather simulation system – the High Resolution Forecast (HRES), produced by the European Centre for Medium-Range Weather Forecasts (ECMWF).</p><p data-block-key="d53hu">GraphCast can also offer earlier warnings of extreme weather events. It can predict the tracks of cyclones with great accuracy further into the future, identifies atmospheric rivers associated with flood risk, and predicts the onset of extreme temperatures. This ability has the potential to save lives through greater preparedness.</p><p data-block-key="2cftd">GraphCast takes a significant step forward in AI for weather prediction, offering more accurate and efficient forecasts, and opening paths to support decision-making critical to the needs of our industries and societies. And, by <a href="https://github.com/google-deepmind/graphcast" rel="noopener" target="_blank">open sourcing the model code for GraphCast,</a> we are enabling scientists and forecasters around the world to benefit billions of people in their everyday lives. GraphCast is already being used by weather agencies, including ECMWF, which is running a live experiment of <a href="https://charts.ecmwf.int/products/graphcast_medium-mslp-wind850" rel="noopener" target="_blank">our model’s forecasts on its website</a>.</p>
</div>
            
          
            
            
              
              


<figure>
  
  
    <figcaption>
      <p data-block-key="ke1fd">A selection of GraphCast’s predictions rolling across 10 days showing specific humidity at 700 hectopascals (about 3 km above surface), surface temperature, and surface wind speed.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="pnebs">The challenge of global weather forecasting</h2><p data-block-key="c69cv">Weather prediction is one of the oldest and most challenging–scientific endeavours. Medium range predictions are important to support key decision-making across sectors, from renewable energy to event logistics, but are difficult to do accurately and efficiently.</p><p data-block-key="27gma">Forecasts typically rely on Numerical Weather Prediction (NWP), which begins with carefully defined physics equations, which are then translated into computer algorithms run on supercomputers. While this traditional approach has been a triumph of science and engineering, designing the equations and algorithms is time-consuming and requires deep expertise, as well as costly compute resources to make accurate predictions.</p><p data-block-key="3ces3">Deep learning offers a different approach: using data instead of physical equations to create a weather forecast system. GraphCast is trained on decades of historical weather data to learn a model of the cause and effect relationships that govern how Earth’s weather evolves, from the present into the future.</p><p data-block-key="1lfp4">Crucially, GraphCast and traditional approaches go hand-in-hand: we trained GraphCast on four decades of weather reanalysis data, from the ECMWF’s ERA5 dataset. This trove is based on historical weather observations such as satellite images, radar, and weather stations using a traditional NWP to ‘fill in the blanks’ where the observations are incomplete, to reconstruct a rich record of global historical weather.</p><h2 data-block-key="f4hf9">GraphCast: An AI model for weather prediction</h2><p data-block-key="51oo3">GraphCast is a weather forecasting system based on machine learning and Graph Neural Networks (GNNs), which are a particularly useful architecture for processing spatially structured data.</p><p data-block-key="6c3sr">GraphCast makes forecasts at the high resolution of 0.25 degrees longitude/latitude (28km x 28km at the equator). That’s more than a million grid points covering the entire Earth’s surface. At each grid point the model predicts five Earth-surface variables – including temperature, wind speed and direction, and mean sea-level pressure – and six atmospheric variables at each of 37 levels of altitude, including specific humidity, wind speed and direction, and temperature.</p><p data-block-key="djs0f">While GraphCast’s training was computationally intensive, the resulting forecasting model is highly efficient. Making 10-day forecasts with GraphCast takes less than a minute on a single Google TPU v4 machine. For comparison, a 10-day forecast using a conventional approach, such as HRES, can take hours of computation in a supercomputer with hundreds of machines.</p><p data-block-key="7dee1">In a comprehensive performance evaluation against the gold-standard deterministic system, HRES, GraphCast provided more accurate predictions on more than 90% of 1380 test variables and forecast lead times (see our <a href="https://www.science.org/doi/10.1126/science.adi2336" rel="noopener" target="_blank">Science paper</a> for details). When we limited the evaluation to the troposphere, the 6-20 kilometer high region of the atmosphere nearest to Earth’s surface where accurate forecasting is most important, our model outperformed HRES on 99.7% of the test variables for future weather.</p>
</div>
            
          
            
            
              
              


<figure>
  
  
    <figcaption>
      <p data-block-key="x7trc">For inputs, GraphCast requires just two sets of data: the state of the weather 6 hours ago, and the current state of the weather. The model then predicts the weather 6 hours in the future. This process can then be rolled forward in 6-hour increments to provide state-of-the-art forecasts up to 10 days in advance.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="pnebs">Better warnings for extreme weather events</h2><p data-block-key="7ki0b">Our analyses revealed that GraphCast can also identify severe weather events earlier than traditional forecasting models, despite not having been trained to look for them. This is a prime example of how GraphCast could help with preparedness to save lives and reduce the impact of storms and extreme weather on communities.</p><p data-block-key="csav9">By applying a simple cyclone tracker directly onto GraphCast forecasts, we could predict cyclone movement more accurately than the HRES model. In September, a live version of our publicly available GraphCast model, deployed on the ECMWF website, accurately predicted about nine days in advance that Hurricane Lee would make landfall in Nova Scotia. By contrast, traditional forecasts had greater variability in where and when landfall would occur, and only locked in on Nova Scotia about six days in advance.</p><p data-block-key="ecqbk">GraphCast can also characterize atmospheric rivers – narrow regions of the atmosphere that transfer most of the water vapour outside of the tropics. The intensity of an atmospheric river can indicate whether it will bring beneficial rain or a flood-inducing deluge. GraphCast forecasts can help characterize atmospheric rivers, which could help planning emergency responses together with <a href="https://sites.research.google/floodforecasting/" rel="noopener" target="_blank">AI models to forecast floods.</a></p><p data-block-key="a8m1r">Finally, predicting extreme temperatures is of growing importance in our warming world. GraphCast can characterize when the heat is set to rise above the historical top temperatures for any given location on Earth. This is particularly useful in anticipating heat waves, disruptive and dangerous events that are becoming increasingly common.</p>
</div>
            
          
            
            
              
              


<figure>
  
  
    <figcaption>
      <p data-block-key="df1sy">Severe-event prediction - how GraphCast and HRES compare.</p><p data-block-key="2dp0j">Left: Cyclone tracking performances. As the lead time for predicting cyclone movements grows, GraphCast maintains greater accuracy than HRES.</p><p data-block-key="3kpk6">Right: Atmospheric river prediction. GraphCast’s prediction errors are markedly lower than HRES’s for the entirety of their 10-day predictions</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="pnebs">The future of AI for weather</h2><p data-block-key="795pr">GraphCast is now the most accurate 10-day global weather forecasting system in the world, and can predict extreme weather events further into the future than was previously possible. As the weather patterns evolve in a changing climate, GraphCast will evolve and improve as higher quality data becomes available.</p><p data-block-key="33jsp">To make AI-powered weather forecasting more accessible, we’ve <a href="https://github.com/google-deepmind/graphcast" rel="noopener" target="_blank">open sourced our model’s code</a>. ECMWF is already <a href="https://charts.ecmwf.int/products/graphcast_medium-mslp-wind850" rel="noopener" target="_blank">experimenting with GraphCast’s 10-day forecasts</a> and we’re excited to see the possibilities it unlocks for researchers – from tailoring the model for particular weather phenomena to optimizing it for different parts of the world.</p><p data-block-key="8e2gf">GraphCast joins other state-of-the-art weather prediction systems from Google DeepMind and Google Research, including a regional <a href="https://deepmind.google/discover/blog/nowcasting-the-next-hour-of-rain/" rel="noopener" target="_blank">Nowcasting model</a> that produces forecasts up to 90 minutes ahead, and <a href="https://blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html" rel="noopener" target="_blank">MetNet-3</a>, a regional weather forecasting model already in operation across the US and Europe that produces more accurate 24-hour forecasts than any other system.</p><p data-block-key="au8nq">Pioneering the use of AI in weather forecasting will benefit billions of people in their everyday lives. But our wider research is not just about anticipating weather – it’s about understanding the broader patterns of our climate. By developing new tools and accelerating research, we hope AI can empower the global community to tackle our greatest environmental challenges.</p>
</div>
            
          
            
            
              
              


            
          
            
            
              
              



  
    
  

            
          
        </div>
      
    

    
  

  

  

  </article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Replacing HLS/Dash – Live Mass Fanout with Media over QUIC (140 pts)]]></title>
            <link>https://quic.video/blog/replacing-hls-dash/</link>
            <guid>38264174</guid>
            <pubDate>Tue, 14 Nov 2023 15:10:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quic.video/blog/replacing-hls-dash/">https://quic.video/blog/replacing-hls-dash/</a>, See on <a href="https://news.ycombinator.com/item?id=38264174">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<p>Low-latency, high bitrate, mass fan-out is hard. Who knew?</p>
<p>See <a href="https://quic.video/blog/replacing-webrtc">Replacing WebRTC</a> for the previous post in this series.</p>
<h2 id="tldr">tl;dr</h2>
<p>If you’re using HLS/DASH and your main priority is…</p>
<ul>
<li><strong>cost</strong>: wait until there CDN offerings.</li>
<li><strong>latency</strong>: you should seriously consider MoQ.</li>
<li><strong>features</strong>: it will take a while to implement everything.</li>
<li><strong>vod</strong>: it works great, why replace it?</li>
</ul>
<h2 id="intro">Intro</h2>
<p>Thanks for the positive reception on <a href="https://news.ycombinator.com/item?id=38069974">Hacker News</a>!
Anyway, I’m back.</p>
<p>I spent the last 9 years working on literally all facets of HLS and Twitch’s extension: <a href="https://www.theoplayer.com/blog/low-latency-hls-lhls">LHLS</a>.
We hit a latency wall and my task was to find an alternative, originally WebRTC but that eventually pivoted into <strong>Media over QUIC</strong>.</p>
<p>Hopefully this time I won’t be <em>“dunning-Krugerering off a cliff”</em>. Thanks random Reddit user for that confidence boost.</p>
<h2 id="why-hlsdash">Why HLS/DASH?</h2>
<p>Simple answer: <a href="https://developer.apple.com/library/archive/documentation/NetworkingInternet/Conceptual/StreamingMediaGuide/UsingHTTPLiveStreaming/UsingHTTPLiveStreaming.html">Apple</a></p>
<blockquote>
<p>If your app delivers video over cellular networks, and the video exceeds either 10 minutes duration or 5 MB of data in a five minute period, you are required to use HTTP Live Streaming.</p>
</blockquote>
<p>It’s an anti-climactic answer, but Twitch migrated from <a href="https://en.wikipedia.org/wiki/Real-Time_Messaging_Protocol">RTMP</a> to <a href="https://en.wikipedia.org/wiki/HTTP_Live_Streaming">HLS</a> to avoid getting kicked off the App Store.
The next sentence gives a hint as to why:</p>
<blockquote>
<p>If your app uses HTTP Live Streaming over cellular networks, you are required to provide at least one stream at 64 Kbps or lower bandwidth.</p>
</blockquote>
<p>This was back in 2009 when the iPhone 3GS was released and AT&amp;T’s network was <a href="https://www.wired.com/2010/07/ff-att-fail/">struggling to meet the demand</a>.
The key feature of HLS was <a href="https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming">ABR</a>: multiple copies of the same content at different bitrates.
This allowed the Apple-controlled HLS player to reduce the bitrate rather than pummel a poor megacorp’s cellular network.</p>
<p><a href="https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP">DASH</a> came afterwards in an attempt to standardize HLS minus the controlled by Apple part.
There’s definitely some cool features in DASH but the <a href="https://www.cloudflare.com/learning/video/what-is-mpeg-dash/">core concepts are the same</a> and they even share the same <a href="https://www.wowza.com/blog/what-is-cmaf">media container</a> now.
So the two get bundled together as <strong>HLS/DASH</strong>.</p>
<p>But I’ll focus more on HLS since that’s my shit.</p>
<h2 id="the-good-stuff">The Good Stuff</h2>
<p>While we were forced to switch protocols at the tech equivalent of gunpoint, HLS actually has some amazing benefits.
The biggest one is that it uses <strong>HTTP</strong>.</p>
<p>HLS/DASH works by breaking media into “segments”, each containing a few seconds of media.
The player will individually request each segment via a HTTP request and seamlessly stitch them together.
New segments are constantly being generated and announced to the player via a “playlist”.</p>
<figure><p><img src="https://quic.video/blog/replacing-hls-dash/carrot.png" alt="carrot">
</p><figcaption>Thanks for the filer image, DALL·E</figcaption></figure>
<p>Because HLS uses HTTP, a service like Twitch can piggyback on the existing infrastructure of the internet.
There’s a plethora of optimized CDNs, servers, and clients that all speak HTTP and can be used to transport media.
You do have to do some extra work to massage live video into HTTP semantics, but it’s worth it.</p>
<p>The key is utilizing <a href="https://napkinfinance.com/napkin/what-are-economies-of-scale/">economies of scale</a> to make it cheap to mass distribute live media.
Crafting individual IP packets might the <em>correct</em> way to send live media with minimal latency (ie. WebRTC), but it’s not the most cost effective.</p>
<h2 id="the-bad-stuff">The Bad Stuff</h2>
<p>I hope you weren’t expecting a fluff piece.</p>
<h3 id="latency">Latency</h3>
<p>We were somewhat sad to bid farewell to Flash (<em>gasp</em>).
Twitch’s latency went from something like 3 seconds with RTMP to 15 seconds with HLS.</p>
<p>There’s a boatload of latency sources, anywhere from the duration of segments to the frequency of playlist updates.
Over the years we were able to slowly able to chip away at the problem, eventually extending HLS to get latency back down to theoretical RTMP levels.
I <a href="https://quic.video/blog/distribution-at-twitch">documented our journey</a> if you’re interested in the gritty details.</p>
<p>But one big source of latency remains: <strong>T</strong> <strong>C</strong> <strong>P</strong></p>
<p>I went into more detail with my <a href="https://quic.video/blog/replacing-webrtc">previous blog post</a>, but the problem is head-of-line blocking.
Once you flush a frame to the TCP socket, it will be delivered reliably and in order.
However, when the network is congested, the encoded media bitrate will exceed the network bitrate and queues will grow.
Frames will take longer and longer to reach the player until the buffer is depleted and the viewer gets to see their least favorite spinny boye.</p>
<figure><p><img src="https://quic.video/blog/replacing-hls-dash/buffering.gif" alt="buffering">
</p><figcaption>&gt; tfw HLS/DASH</figcaption></figure>
<p>A HLS/DASH player can detect queuing and switch to a lower bitrate via ABR.
However, it can only do this at infrequent (ex. 2s) segment boundaries, and it can’t renege any frames already flushed to the socket.
So if you’re watching 1080p video and your network takes a dump, well you still need to download seconds of unsustainable 1080p video before you can switch down to a reasonable 360p.</p>
<p>You can’t just put the toothpaste back in the tube if you squeeze out too much.
You gotta use all of the toothpaste, even if it takes much longer to brush your teeth.</p>
<figure><p><img src="https://quic.video/blog/replacing-webrtc/toothpaste.jpg" alt="TCP toothpaste"></p><figcaption><p><a href="https://knowyourmeme.com/memes/shitting-toothpaste-pooping-toothpaste">Source</a>. The analogy falls apart but I
get to use this image again.</p></figcaption></figure>
<h3 id="clients">Clients</h3>
<p>HLS utilizes “smart” clients and “dumb” servers.
The client decides what, when, why, and how to download each media playlist, segment, and frame.
Meanwhile the server just sits there and serves HTTP requests.</p>
<p>The problem really depends on your perspective. If you control:</p>
<ul>
<li><strong>client only</strong>: Life is great!</li>
<li><strong>client and server</strong>: Life is great! You can even extend the protocol!</li>
<li><strong>server only</strong>: Life is <em>pain</em>.</li>
</ul>
<p>For a service like Twitch, the solution might seem simple: build your own client and server!
And we did, including a baremetal live CDN designed exclusively for HLS.</p>
<p>But <a href="https://bitmovin.com/managed-media-source">until quite recently</a>, we have been forced to use the Apple HLS player on iOS for AirPlay or Safari support.
And of course TVs, consoles, casting devices, and others have their own HLS players.
And if you’re offering your baremetal live CDN <a href="https://aws.amazon.com/ivs/">to the public</a>, you can’t exactly force customers to use your proprietary player.</p>
<p>So you’re stuck with a <em>dumb</em> server and a bunch of <em>dumb</em> clients.
These <em>dumb</em> clients make <em>dumb</em> decisions with no cooperation with the server, based on imperfect information.</p>
<h3 id="ownership">Ownership</h3>
<p>I love the simplicity of HLS compared to DASH.
There’s something so satisfying about a text-based playlist that you can actually read, versus a XML monstrosity designed by committee.</p>
<pre is:raw="" tabindex="0"><code><span><span>#EXTM3U</span></span>
<span><span>#EXT-X-TARGETDURATION:10</span></span>
<span><span>#EXT-X-VERSION:3</span></span>
<span><span>#EXTINF:9.009,</span></span>
<span><span>http://media.example.com/first.ts</span></span>
<span><span>#EXTINF:9.009,</span></span>
<span><span>http://media.example.com/second.ts</span></span>
<span><span>#EXTINF:3.003,</span></span>
<span><span>http://media.example.com/third.ts</span></span>
<span><span>#EXT-X-ENDLIST</span></span></code></pre>
<figure><figcaption><a href="https://datatracker.ietf.org/doc/html/draft-pantos-hls-rfc8216bis/#section-9.1">Orgasmic</a>.</figcaption></figure>
<p>But unfortunately Apple controls HLS.</p>
<p>There’s a misalignment of incentives between Apple and the rest of the industry.
I’m not even sure how Apple uses HLS, or why they would care about latency, or why they insist on being the sole arbiter of a live streaming protocol.
<a href="https://www.crunchbase.com/person/roger-pantos">Pantos</a> has done a great and thankless job, but it feels like a stand-off.</p>
<p>For example, LL-HLS originally <a href="https://www.theoplayer.com/blog/impact-of-apple-ll-hls-update-2020">required HTTP/2 server push</a> and it took nearly the entire industry to convince Apple that this was a bad idea.
The upside is that we got <a href="https://lists.apple.com/mailman/listinfo/hls-announce">a mailing list</a> so they can announce changes to developers first… but don’t expect the ability to propose changes any time soon.</p>
<p>DASH is it’s own can of worms as it’s controlled by <a href="https://en.wikipedia.org/wiki/Moving_Picture_Experts_Group">MPEG</a>.
The specifications are <a href="https://www.iso.org/standard/79106.html">behind a paywall</a> or <a href="https://www.streamingmedia.com/Articles/ReadArticle.aspx?ArticleID=133508">require patent licensing</a>?
I can’t even tell if I’m going to <a href="https://www.mpegla.com/wp-content/uploads/DASHWeb.pdf">get sued</a> for parsing a DASH playlist without paying the troll toll.</p>
<figure><p><img src="https://quic.video/blog/replacing-hls-dash/troll.webp" alt="troll toll"></p><figcaption><p><a href="https://itsalwayssunny.fandom.com/wiki/The_Nightman_Cometh">Source</a>. 🎵 You gotta pay the Troll Toll 🎵</p></figcaption></figure>
<h2 id="whats-next">What’s next?</h2>
<p>You’re given a blank canvas and a brush to paint the greenest of fields, what do you make?</p>
<figure><p><img src="https://quic.video/blog/replacing-hls-dash/green.jpg" alt="green field"></p><figcaption><p><a href="https://www.freeimageslive.co.uk/free_stock_image/green-field-painting-jpg">Source</a>. Wow. That’s quite the
green field.</p></figcaption></figure>
<h2 id="tcp">TCP</h2>
<p>After my <a href="https://quic.video/blog/replacing-webrtc">previous blog post</a>, I had a few people hit up my DMs and claim they can do real-time latency with TCP.
And I’m sure a few more people will too after this post, so you get your own section that muddles the narrative.</p>
<p>Yes, you can do real-time latency with TCP (or WebSockets) under ideal conditions.</p>
<p>However, it just won’t work well enough on poor networks.
Congestion and buffer-bloat will absolutely wreck your protocol on poor networks.
A lot of my time spent at Twitch was optimizing for the 90th percentile; the shoddy cellular networks in Brazil or India or Australia.</p>
<p>But if you are going to reinvent RTMP, there are <a href="https://www.youtube.com/watch?v=cpYhm74zp0U">some ways to reduce queuing</a> but they are quite limited.
This is <em>especially</em> true in a browser environment when limited to HTTP or <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API">WebSockets</a>.</p>
<p>See my next blog post about <strong>Replacing RTMP</strong>.</p>
<h2 id="http">HTTP</h2>
<p>Notably absent thus far has been any mention of <a href="https://www.theoplayer.com/blog/low-latency-hls-lhls">LL-HLS</a> and <a href="https://www.wowza.com/blog/what-is-low-latency-dash">LL-DASH</a>.
These two protocols are meant to lower HLS/DASH latency respectively by breaking media segments into smaller chunks.</p>
<p>The chunks might be smaller, but they’re still served sequentially over TCP.
The latency floor is lower but the latency ceiling is still just as high, and you’re still going to buffer during congestion.</p>
<figure><p><img src="https://quic.video/blog/replacing-hls-dash/buffering.gif" alt="buffering">
</p><figcaption>&gt; tfw LL-HLS/LL-DASH</figcaption></figure>
<p>We’re also approaching the limit of what you can do with HTTP semantics.</p>
<ul>
<li><strong>LL-HLS</strong> has configurable latency at the cost of and exponential number of sequential requests in the critical path. For example, 20 HTTP requests a second <em>per track</em> still only gets you +100ms of latency, which is not even viable for real-time latency.</li>
<li><strong>LL-DASH</strong> can be configured down to +0ms added latency, delivering frame-by-frame with chunked-transfer. However it absolutely wrecks client-side ABR algorithms. Twitch <a href="https://blog.twitch.tv/en/2020/01/15/twitch-invites-you-to-take-on-our-acm-mmsys-2020-grand-challenge/">hosted a challenge</a> to improve this but I’m convinced it’s impossible without server feedback.</li>
</ul>
<p><a href="https://www.theoplayer.com/solutions/hesp-high-efficiency-streaming">HESP</a> also gets a special shout-out because it’s cool.
It works by canceling HTTP requests during congestion and frankensteining the video encoding which is quite <del>hacky</del> clever, but suffers a similar fate.</p>
<p>We’ve hit a wall with HTTP over TCP.</p>
<h2 id="http3">HTTP/3</h2>
<p>If you’re an astute hyper text transport protocol aficionado, you might have noticed that I said “HTTP over TCP” above.
But <a href="https://www.cloudflare.com/learning/performance/what-is-http3">HTTP/3</a> uses <a href="https://www.rfc-editor.org/rfc/rfc9000.html">QUIC</a> instead of TCP.
Problem solved! We can replace any mention of <del>TCP</del> with QUIC!</p>
<p>Well, not quite. To use another complicated topic as a metaphor:</p>
<ul>
<li>A TCP connection is a single-core CPU.</li>
<li>A QUIC connection is a multi-core CPU.</li>
</ul>
<p>If you take a single threaded program and run it on a multi-core machine, it will run just as slow, and perhaps even slower.
This is the case with HLS/DASH as each segment request is made <em>sequentially</em>.
HTTP/3 is not a magic bullet and only has marginal benefits when used with HLS/DASH.</p>
<p>The key to using QUIC is to embrace concurrency.</p>
<p>This means utilizing multiple, independent streams that share a connection.
You can prioritize a stream so it gets more bandwidth during congestion, much like you can use <code>nice</code> on Linux to prioritize a process when CPU starved.
If a stream is taking too long, you can cancel it much like you can <code>kill</code> a process.</p>
<p>For live media, you want to prioritize new media over old media in order to skip old content.
You also want to prioritize audio over video, so you can hear what someone is saying without necessarily seeing their lips move.
If you can only transmit half of a media stream in time, make sure it’s the most important half.</p>
<p>To Apple/Pantos’ credit, LL-HLS is exploring <a href="https://mailarchive.ietf.org/arch/msg/hls-interest/RcZ2SG8Sz_zZEcjWnDKzcM_-TJk/">prioritization using HTTP/3</a>.
It doesn’t go far enough (yet!) and HTTP semantics get in the way, but it’s absolutely the right direction.
I’m convinced that somebody will make a <a href="https://mailarchive.ietf.org/arch/msg/moq/S3eOPU5XnvQ4kn1zJyDThG5U4sA/">HTTP/3 only media protocol</a> at some point.</p>
<p>But of course I’m biased towards…</p>

<p>MoQ utilizes WebTransport/QUIC directly to avoid TCP and HTTP.
But what about that whole <strong>economies of scale</strong> stuff?</p>
<p>Well, there are some important differences between Media over QUIC as compared to your standard <em>not invented here</em> protocol:</p>
<h2 id="reason-0-quic">Reason 0: QUIC</h2>
<p>QUIC is the future of the internet.
TCP is a relic of the past.</p>
<figure><img src="https://quic.video/home/quic.svg" alt="QUIC Logo"><figcaption>You’re going to see a lot of this logo, although not crudely traced or green.</figcaption></figure>
<p>It’s a <strong>bold</strong> claim I know.
But I struggle to think of a single reason why you would use TCP over QUIC going forward.
There are still some corporate firewalls that block UDP (used by QUIC) and hardware offload doesn’t exist yet, but I mean that’s about it.</p>
<p>It will take a few years, but every library, server, load balancer, and NIC will be optimized for QUIC delivery.
Media over QUIC offloads as much as possible into this powerful layer.
We benefit also from any new features, including proposals such as <a href="https://datatracker.ietf.org/doc/draft-ietf-quic-multipath/">multi-path</a>, <a href="https://datatracker.ietf.org/doc/draft-michel-quic-fec/">FEC</a>, <a href="https://datatracker.ietf.org/doc/rfc9330/">congestion control</a>, etc.
I don’t want network features in my media layer <em>thank you very much</em> (looking at you WebRTC).</p>
<p>It might not be obvious is that HTTP/3 is actually a thin layer on top of QUIC.
Likewise MoQ is also meant to be a thin layer on top of QUIC, effectively just providing pub/sub semantics.
We get all of the benefits of QUIC without the baggage of HTTP, and yet still achieve web support via <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebTransport_API">WebTransport</a>.</p>
<p>Instead we can focus on the important stuff instead: <strong>live media</strong>.</p>
<h2 id="reason-1-relay-layer">Reason 1: Relay Layer</h2>
<p>To avoid <a href="https://quic.video/blog/replacing-webrtc">the mistakes of WebRTC</a>, we need to decouple the application from the transport.
If a relay (ie. CDN) knows anything about media encoding, we have failed.</p>
<p>The idea is to break MoQ into layers.</p>
<p><a href="https://datatracker.ietf.org/doc/draft-ietf-moq-transport/">MoqTransport</a> is the base layer and is a typical pub/sub protocol, although catered toward QUIC.
The application splits data into “objects”, annotated with a header providing simple instructions on how the relay needs to deliver it.
These are generic signals, including stuff like the priority, reliability, grouping, expiration, etc.</p>
<p>MoqTransport is designed to be used for arbitrary applications.
Some examples include:</p>
<ul>
<li>live chat</li>
<li>end-to-end encryption</li>
<li>game state</li>
<li>live playlists</li>
<li>or even a clock!</li>
</ul>
<p>This is huge draw for CDN vendors.
Instead of building a custom WebRTC CDN that targets one specific niche, you can cast a much wider net with MoqTransport.
Akamai, Google, and Cloudflare have been involved in the standardization process thus far and CDN support is inevitable.</p>

<p>There will be at least one media layer on top of MoqTransport.
We’re focused on the transport right now so there’s no official “adopted” draft yet.</p>
<p>However, my proposal is <a href="https://datatracker.ietf.org/doc/draft-law-moq-warpstreamingformat/">Warp</a>.
It uses CMAF so it’s backwards compatible with HLS/DASH while still capable of real-time latency.
I think this is critically important, as any migration has to be done piecewise, client-by-client and user-by-user.
The same media segments can be served for a mixed roll-out and for VoD.</p>
<p>This website uses Warp! <a href="https://quic.video/watch">Try it out!</a> Or watch one of my <a href="https://www.youtube.com/watch?v=PncdrMPVaNc">presentations</a>.</p>
<p>There will absolutely be other mappings and containers; MoQ is not married to CMAF.
The important part is that only the encoder/decoder understand this media layer and not any relays in the middle.
There’s a lot of cool ideas floating around, such as a <a href="https://datatracker.ietf.org/doc/draft-wilaw-moq-catalogformat/">live playlist format</a> and a <a href="https://datatracker.ietf.org/doc/draft-mzanaty-moq-loc/">low-overhead container</a>.</p>
<h2 id="reason-3-ietf">Reason 3: IETF</h2>
<p>Media over QUIC is an <a href="https://datatracker.ietf.org/wg/moq/about/">IETF working group</a>.</p>
<figure><img src="https://quic.video/home/ietf.svg" alt="IETF Logo"><figcaption>I crudely traced and recolored this logo too.</figcaption></figure>
<p>If you know nothing about the IETF, just know that it’s the standards body behind favorites such as HTTP, DNS, TLS, QUIC, and even WebRTC.
But I think <a href="https://www.ietf.org/about/introduction/">this part</a> is especially important:</p>
<blockquote>
<p>There is no membership in the IETF. Anyone can participate by signing up to a working group mailing list (more on that below), or registering for an IETF meeting. All IETF participants are considered volunteers and expected to participate as individuals, including those paid to participate.</p>
</blockquote>
<p>It’s not a protocol owned by a company.
It’s not a protocol owned by lawyers.</p>
<p><a href="https://www.ietf.org/mailman/listinfo/moq">Join the mailing list</a>.</p>
<h2 id="whats-missing">What’s missing?</h2>
<p>Okay cool so hopefully I sold you on MoQ.
What can’t you use it today to replace HLS/DASH?</p>
<ol>
<li><strong>It’s not done yet</strong>: The IETF is many things, but fast is not one of them.</li>
<li><strong>Cost</strong>: QUIC is a new protocol that has yet to be fully optimized to match TCP. It’s possible and apparently Google is <a href="https://conferences.sigcomm.org/sigcomm/2020/files/slides/epiq/0%20QUIC%20and%20HTTP_3%20CPU%20Performance.pdf">near parity</a>.</li>
<li><strong>Support</strong>: Your favorite language/library/cdn/cloud/browser might not even provide HTTP/3 support yet, let alone WebTransport or QUIC.</li>
<li><strong>Features</strong>: Somebody has to reimplement all of the annoying HLS/DASH features like DRM and server-side advertisements…</li>
<li><strong>VoD</strong>: MoQ is currently live only. HLS/DASH work great, why replace it?</li>
</ol>
<p>We’ll get there eventually.</p>
<p>Feel free to use our <a href="https://github.com/kixelated/moq-rs">Rust</a> or <a href="https://github.com/kixelated/moq-js">Typescript</a> implementation is you want to experiment.
Join the <a href="https://discord.gg/FCYF3p99mr">Discord</a> if you want to help!</p>
<p>Written by <a href="https://github.com/kixelated">@kixelated</a>.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ultra-white ceramic cools buildings with record-high 99.6% reflectivity (170 pts)]]></title>
            <link>https://newatlas.com/materials/ultra-white-ceramic-cools-buildings-record-high-reflectivity/</link>
            <guid>38263934</guid>
            <pubDate>Tue, 14 Nov 2023 14:51:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/materials/ultra-white-ceramic-cools-buildings-record-high-reflectivity/">https://newatlas.com/materials/ultra-white-ceramic-cools-buildings-record-high-reflectivity/</a>, See on <a href="https://news.ycombinator.com/item?id=38263934">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Scientists in Hong Kong have demonstrated a new ultra-white ceramic material that can drastically cool buildings by reflecting sunlight and heat at record highs. The beetle-inspired material gets its ability from its nanostructure, stays tough to the elements and should be relatively easy to scale up for production.</p><p>When our homes get too hot, the first solution for many people is to crank the air conditioning. It may be effective, but it’s not very energy efficient, as heating and cooling buildings accounts for a huge percentage of energy costs. So, scientists are investigating ways to passively manage interior temperatures, and one of the simplest is to just <a href="https://newatlas.com/white-roof-heat-island/21758/" data-cms-ai="0">paint buildings and rooftops white</a>.</p><p>Basic physics dictates that lighter colors absorb less light than darker ones, and therefore remain cooler. In recent years new <a href="https://newatlas.com/materials/super-white-paint-teflon-98-suns-heat/" data-cms-ai="0">ultra-white paints</a> have been developed that reflect over 95% of the sunlight that hits them, doing a decent job of cooling buildings. But these coatings have their issues, including durability.</p><p>For the new study, scientists at City University of Hong Kong (CityU) developed a new cooling ceramic material that performs better than others. Rather than just being a very white paint, the material gets its high reflectivity from its nanostructure, which efficiently scatters almost the entire spectrum of sunlight – just like the <a href="https://newatlas.com/beetle-scales-white/53789/" data-cms-ai="0">Cyphochilus beetle</a> which inspired it. This results in a solar reflectivity of 99.6%, which is a record high, along with an infrared thermal emission of 96.5%.</p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="Samples of the cooling ceramic material, which reflect a record high of 99.6% of sunlight that hits them, cooling interiors of buildings" width="957" height="805" data-image-size="articleImage" loading="lazy" data-srcset="https://assets.newatlas.com/dims4/default/f269aec/2147483647/strip/true/crop/957x805+0+0/resize/440x370!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 440w,https://assets.newatlas.com/dims4/default/371cea4/2147483647/strip/true/crop/957x805+0+0/resize/800x673!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 800w,https://assets.newatlas.com/dims4/default/e89623e/2147483647/strip/true/crop/957x805+0+0/resize/1200x1009!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 1200w,https://assets.newatlas.com/dims4/default/a4d3c90/2147483647/strip/true/crop/957x805+0+0/resize/1920x1615!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 1920w" data-src="https://assets.newatlas.com/dims4/default/008ee1a/2147483647/strip/true/crop/957x805+0+0/resize/957x805!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg" sizes="(min-width: 1240px) 800px, (min-width: 1024px) 95vw, 100vw" srcset="https://assets.newatlas.com/dims4/default/f269aec/2147483647/strip/true/crop/957x805+0+0/resize/440x370!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 440w,https://assets.newatlas.com/dims4/default/371cea4/2147483647/strip/true/crop/957x805+0+0/resize/800x673!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 800w,https://assets.newatlas.com/dims4/default/e89623e/2147483647/strip/true/crop/957x805+0+0/resize/1200x1009!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 1200w,https://assets.newatlas.com/dims4/default/a4d3c90/2147483647/strip/true/crop/957x805+0+0/resize/1920x1615!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 1920w" src="https://assets.newatlas.com/dims4/default/008ee1a/2147483647/strip/true/crop/957x805+0+0/resize/957x805!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg">
</p>



    
    

    
        <div><figcaption itemprop="caption">Samples of the cooling ceramic material, which reflect a record high of 99.6% of sunlight that hits them, cooling interiors of buildings</figcaption><p>City University of Hong Kong</p></div>
    
</figure>

                
            </div><p>The material is made of alumina, which not only reduces solar absorption, but the team says it makes the cooling ceramic more durable in the face of weather. It resists degradation from UV light exposure, which is a weakness of other passive cooling materials and coatings, and boosts the rate of water evaporation from the surface, which adds the bonus effect of evaporative cooling. It even boasts fire resistance by withstanding temperatures of over 1,000 °C (1,832 °F).</p><p>““The beauty of the cooling ceramic is that it fulfills the requirements for both high-performance PRC and applications in real-life settings,” said Professor Edwin Tso Chi-yan, co-corresponding author of the study. “Our experiment found that applying the cooling ceramic on a house roof can achieve more than 20% electricity [reduction] for space cooling, which confirms the great potential of cooling ceramic in reducing people’s reliance on traditional active cooling strategies and provides a sustainable solution for avoiding electricity grid overload, greenhouse gas emissions and urban heat islands.”</p><p>Finally, the researchers also say that the material can be easily mass produced, using common materials like alumina and a two-step process of phase inversion and sintering. And if white is too boring for some houses, the material can apparently be produced in other colors and patterns by adding extra layers.</p><p>The research was published in the journal <i><a href="https://www.science.org/doi/10.1126/science.adi4725" target="_blank" data-cms-ai="0">Science</a></i>.</p><p>Source: <a href="https://www.cityu.edu.hk/research/stories/2023/11/10/new-cooling-ceramic-can-enhance-energy-efficiency-construction-sector-and-help-combat-global-warming-cityu-research" target="_blank" data-cms-ai="0">City University of Hong Kong</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PyPI has completed its first security audit (114 pts)]]></title>
            <link>https://blog.pypi.org/posts/2023-11-14-1-pypi-completes-first-security-audit/</link>
            <guid>38263786</guid>
            <pubDate>Tue, 14 Nov 2023 14:38:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.pypi.org/posts/2023-11-14-1-pypi-completes-first-security-audit/">https://blog.pypi.org/posts/2023-11-14-1-pypi-completes-first-security-audit/</a>, See on <a href="https://news.ycombinator.com/item?id=38263786">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
          
        
      
      <main data-md-component="main">
        <div data-md-component="content">
              <article>
                
                  <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Tags -->


<!-- Actions -->

  
  


<!--
  Hack: check whether the content contains a h1 headline. If it doesn't, the
  page title (or respectively site name) is used as the main headline.
-->

  



  by: <b>Dustin Ingram · </b>


  <span>2023-11-14</span>


<!-- Page content -->



<p><em>This is part one in a three-part series. See <a href="https://blog.pypi.org/posts/2023-11-14-2-security-audit-remediation-warehouse/">part two here</a>, and <a href="https://blog.pypi.org/posts/2023-11-14-3-security-audit-remediation-cabotage/">part three here</a></em></p>
<p>We are proud to announce that PyPI has completed its first ever external security audit.
This work was funded in partnership with the <a href="https://www.opentech.fund/">Open Technology Fund</a> (OTF), <a href="https://www.opentech.fund/results/supported-projects/pypi-improvements/">a previous supporter</a> of security-related improvements to PyPI.</p>
<p>The Open Technology Fund selected <a href="https://www.trailofbits.com/">Trail of Bits</a>, an industry-leading cybersecurity firm with significant open-source and Python experience, to perform the audit.
Trail of Bits spent a total of 10 engineer-weeks of effort identifying issues, presenting those findings to the PyPI team, and assisting us as we remediated the findings.</p>
<h2 id="scope">Scope</h2>
<p>The audit was focused on "Warehouse", the open-source codebase that powers <a href="https://pypi.org/">https://pypi.org</a>, and on "cabotage", the custom open-source container orchestration framework we use to deploy Warehouse.
It included code review of both codebases, prioritizing areas that accept user input, provide APIs and other public surfaces.
The audit also covered the continuous integration / continuous deployment (CI/CD) configurations for both codebases.</p>
<h2 id="findings">Findings</h2>
<p>Overall, the auditors determined the Warehouse codebase "was adequately tested and conformed to widely accepted best practices for secure Python and web development," and that while the cabotage codebase lacks the same level of testing, they did not identify any high severity issues in either codebase.</p>
<h2 id="results-impact">Results &amp; Impact</h2>
<p>As a result of the audit, Trail of Bits detailed 29 different advisories discovered across both codebases.
When evaluating severity level of each advisory, 14 were categorized as "informational", 6 as "low", 8 as "medium" and zero as "high".
At the time of writing, the PyPI team has remediated all advisories that posed a significant risk in both codebases where possible, and has worked with third-party teams to unblock additional remediations where necessary.</p>
<h2 id="more-details">More details</h2>
<p>In the interest of transparency, today we are publishing the <a href="https://github.com/trailofbits/publications#technology-product-reviews">full results of the audit</a>, as prepared by Trail of Bits.
You can read more about the audit from their perspective in their <a href="https://blog.trailofbits.com/2023/11/14/our-audit-of-pypi/">accompanying blog post</a>.</p>
<p>Additionally, in two additional blog posts published today, Mike Fiedler (PyPI Security &amp; Safety Engineer) goes into detail about <a href="https://blog.pypi.org/posts/2023-11-14-2-security-audit-remediation-warehouse/">how we remediated these findings in Warehouse</a> and Ee Durbin (Python Software Foundation Director of Infrastructure) <a href="https://blog.pypi.org/posts/2023-11-14-3-security-audit-remediation-cabotage/">similarly details remediation's in cabotage</a>.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>We would like to thank the Open Technology Fund for their continued support of PyPI and specifically for this significant security milestone for the Python ecosystem.
We would also like to thank Trail of Bits for being a dependable, thorough and thoughtful partner throughout the process.</p>

<!-- Source file information -->


<!-- Was this page helpful? -->




<!-- Comment system -->

                
              </article>
            </div>
        
      </main>
      
        
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Backblaze Drive Stats for Q3 2023 (252 pts)]]></title>
            <link>https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2023/</link>
            <guid>38263435</guid>
            <pubDate>Tue, 14 Nov 2023 14:07:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2023/">https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2023/</a>, See on <a href="https://news.ycombinator.com/item?id=38263435">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-label="Backblaze Drive Stats for Q3 2023" itemscope="" itemtype="https://schema.org/CreativeWork"><div itemprop="text">
<figure><img loading="lazy" decoding="async" width="1024" height="583" src="https://wp-admin.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-1024x583.png" alt="A decorative image showing the title Q3 2023 Drive Stats. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-1024x583.png 1024w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-300x171.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-768x437.png 768w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-560x319.png 560w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>

<p>At the end of Q3 2023, Backblaze was monitoring 263,992 hard disk drives (HDDs) and solid state drives (SSDs) in our data centers around the world. Of that number, 4,459 are boot drives, with 3,242 being SSDs and 1,217 being HDDs. The failure rates for the SSDs are analyzed in the <a href="https://www.backblaze.com/blog/ssd-edition-2023-mid-year-drive-stats-review/" target="_blank" rel="noreferrer noopener">SSD Edition: 2023 Drive Stats review</a>.</p>
<p>That leaves us with 259,533 HDDs that we’ll focus on in this report. We’ll review the quarterly and lifetime failure rates of the data drives as of the end of Q3 2023. Along the way, we’ll share our observations and insights on the data presented, and, for the first time ever, we’ll reveal the drive failure rates broken down by data center.</p>
<h2>Q3 2023 Hard Drive Failure Rates</h2>
<p>At the end of Q3 2023, we were managing 259,533 hard drives used to store data. For our review, we removed 449 drives from consideration as they were used for testing purposes, or were drive models which did not have at least 60 drives. This leaves us with 259,084 hard drives grouped into 32 different models.&nbsp;</p>
<p>The table below reviews the annualized failure rate (AFR) for those drive models for the Q3 2023 time period.</p>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1.jpg" data-rel="lightbox-gallery-ht5dUJ2a" data-rl_title="1 – Quarterly AFR Table – Updated" data-rl_caption="1 – Quarterly AFR Table – Updated" title="1 – Quarterly AFR Table – Updated"><img loading="lazy" decoding="async" width="661" height="1024" src="https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1-661x1024.jpg" alt="A table showing the quarterly annualized failure rates of Backblaze hard drives." srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1-661x1024.jpg 661w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1-194x300.jpg 194w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1-560x868.jpg 560w" sizes="(max-width: 661px) 100vw, 661px"></a></figure>
<h3>Notes and Observations on the Q3 2023 Drive Stats</h3>
<ul>
<li><strong>The 22TB drives are here</strong>: At the bottom of the list you’ll see the WDC 22TB drives (model: WUH722222ALE6L4). A <a href="https://www.backblaze.com/blog/vault-cloud-storage-architecture/" target="_blank" rel="noreferrer noopener">Backblaze Vault</a> of 1,200 drives (plus four) is now operational. The 1,200 drives were installed on September 29, so they only have one day of service each in this report, but zero failures so far.</li>
<li><strong>The old get bolder</strong>: At the other end of the time-in-service spectrum are the 6TB Seagate drives (model: ST6000DX000) with an average of 101 months in operation. This cohort had zero failures in Q3 2023 with 883 drives and a lifetime AFR of 0.88%.</li>
<li><strong>Zero failures</strong>: In Q3, six different drive models managed to have zero drive failures during the quarter. But only the 6TB Seagate, noted above, had over 50,000 drive days, our minimum standard for ensuring we have enough data to make the AFR plausible.</li>
<li><strong>One failure</strong>: There were four drive models with one failure during Q3. After applying the 50,000 drive day metric, two drives stood out:
<ol>
<li>WDC 16TB (model: WUH721816ALE6L0) with a 0.15% AFR.</li>
<li>Toshiba 14TB (model: MG07ACA14TEY) with a 0.63% AFR.</li>
</ol>
</li>
</ul>
<h3>The Quarterly AFR Drops</h3>
<p>In Q3 2023, quarterly AFR for all drives was 1.47%. That was down from 2.2% in Q2 and also down from 1.65% a year ago. The quarterly AFR is based on just the data in that quarter, so it can often fluctuate from quarter to quarter.&nbsp;</p>
<p>In our Q2 2023 report, we suspected the 2.2% for the quarter was due to the overall aging of the drive fleet and in particular we pointed a finger at specific 8TB, 10TB, and 12TB drive models as potential culprits driving the increase. That prediction fell flat in Q3 as nearly two-thirds of drive models experienced a decreased AFR quarter over quarter from Q2 and any increases were minimal. This included our suspect 8TB, 10TB, and 12TB drive models.&nbsp;</p>
<p>It seems Q2 was an anomaly, but there was one big difference in Q3: we retired 4,585 aging 4TB drives. The average age of the retired drives was just over eight years, and while that was a good start, there’s another 28,963 4TB drives to go. To facilitate the continuous retirement of aging drives and make the data migration process easy and safe we use CVT, our awesome in-house data migration software which we’ll cover at another time.</p>
<h2>A Hot Summer and the Drive Stats Data</h2>
<p>As anyone should in our business, Backblaze continuously monitors our systems and drives. So, it was of little surprise to us when the folks at <a href="https://www.nasa.gov/news-release/nasa-announces-summer-2023-hottest-on-record/" target="_blank" rel="noreferrer noopener">NASA</a> confirmed the summer of 2023 as Earth’s hottest on record. The effects of this record-breaking summer showed up in our monitoring systems in the form of drive temperature alerts. A given drive in a storage server can heat up for many reasons: it is failing; a fan in the storage server has failed; other components are producing additional heat; the air flow is somehow restricted; and so on. Add in the fact that the ambient temperature within a data center often increases during the summer months, and you can get more temperature alerts.</p>
<p>In reviewing the temperature data for our drives in Q3, we noticed that a small number of drives exceeded the maximum manufacturer’s temperature for at least one day. The maximum temperature for most drives is 60°C, except for the 12TB, 14TB, and 16TB Toshiba drives which have a maximum temperature of 55°C. Of the 259,533 data drives in operation in Q3, there were 354 individual drives (0.0013%) that exceeded their maximum manufacturer temperature. Of those only two drives failed, leaving 352 drives which were still operational as of the end of Q3.</p>
<p>While temperature fluctuation is part of running data centers and temp alerts like these aren’t unheard of, our data center teams are looking into the root causes to ensure we’re prepared for the inevitability of increasingly hot summers to come.</p>
<h3>Will the Temperature Alerts Affect Drive Stats?</h3>
<p>The two drives which exceeded their maximum temperature and failed in Q3 have been removed from the Q3 AFR calculations. Both drives were 4TB Seagate drives (model: ST4000DM000). Given that the remaining 352 drives which exceeded their temperature maximum did not fail in Q3, we have left them in the Drive Stats calculations for Q3 as they did not increase the computed failure rates.</p>
<p>Beginning in Q4, we will remove the 352 drives from the regular Drive Stats AFR calculations and create a separate cohort of drives to track that we’ll name Hot Drives. This will allow us to track the drives which exceeded their maximum temperature and compare their failure rates to those drives which operated within the manufacturer’s specifications. While there are a limited number of drives in the Hot Drives cohort, it could give us some insight into whether drives being exposed to high temperatures could cause a drive to fail more often. This heightened level of monitoring will identify any increase in drive failures so that they can be detected and dealt with expeditiously.</p>
<h2>New Drive Stats Data Fields in Q3</h2>
<p>In Q2 2023, we introduced three new data fields that we started populating in the Drive Stats data we publish: <code>vault_id</code>, <code>pod_id</code>, and <code>is_legacy_format</code>. In Q3, we are adding three more fields into each drive records as follows:</p>
<ul>
<li><code>datacenter</code>: The Backblaze data center where the drive is installed, currently one of these values: <code>ams5</code>, <code>iad1</code>, <code>phx1</code>, <code>sac0</code>, and <code>sac2</code>.</li>
<li><code>cluster_id</code>: The name of a given collection of storage servers logically grouped together to optimize system performance. Note: At this time the <code>cluster_id</code> is not always correct, we are working on fixing that.&nbsp;</li>
<li><code>pod_slot_num</code>: The physical location of a drive within a storage server. The specific slot differs based on the storage server type and capacity: Backblaze (45 drives), Backblaze (60 drives), Dell (26 drives), or Supermicro (60 drives). We’ll dig into these differences in another post.</li>
</ul>
<p>With these additions, the new schema beginning in Q3 2023 is:</p>
<ul>
<li><code>date</code></li>
<li><code>serial_number</code></li>
<li><code>model</code></li>
<li><code>capacity_bytes</code></li>
<li><code>failure</code></li>
<li><strong><code>datacenter </code>(Q3)</strong></li>
<li><strong><code>cluster_id</code> (Q3)</strong></li>
<li><strong><code>vault_id </code>(Q2)</strong></li>
<li><strong><code>pod_id </code>(Q2)</strong></li>
<li><strong><code>pod_slot_num</code> (Q3)</strong></li>
<li><strong><code>is_legacy_format </code>(Q2)</strong></li>
<li><code>smart_1_normalized</code></li>
<li><code>smart_1_raw</code></li>
<li>The remaining SMART value pairs (as reported by each drive model)</li>
</ul>
<p>Beginning in Q3, these data data fields have been added to the <a href="https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data/" target="_blank" rel="noreferrer noopener">publicly available Drive Stats files</a> that we publish each quarter.&nbsp;</p>
<h2>Failure Rates by Data Center</h2>
<p>Now that we have the data center for each drive we can compute the AFRs for the drives in each data center. Below you’ll find the AFR for each of five data centers for Q3 2023.</p>
<div>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1.jpg" data-rel="lightbox-gallery-ht5dUJ2a" data-rl_title="3 – AFR by data center" data-rl_caption="3 – AFR by data center" title="3 – AFR by data center"><img loading="lazy" decoding="async" width="600" height="430" src="https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1.jpg" alt="" srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1.jpg 600w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1-300x215.jpg 300w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1-560x401.jpg 560w" sizes="(max-width: 600px) 100vw, 600px"></a></figure></div>
<h3>Notes and Observations</h3>
<ul>
<li><strong>Null?:</strong> The drives which reported a null or blank value for their data center are grouped in four Backblaze vaults. <a href="https://www.backblaze.com/blog/author/david-winings/" target="_blank" rel="noreferrer noopener">David</a>, the Senior Infrastructure Software Engineer for Drive Stats, <a href="https://www.backblaze.com/blog/overload-to-overhaul-how-we-upgraded-drive-stats-data/" target="_blank" rel="noreferrer noopener">described the process of how we gather all the parts of the Drive Stats data each day</a>. The TL:DR is that vaults can be too busy to respond at the moment we ask, and since the data center field is nice-to-have data, we get a blank field. We can go back a day or two to find the data center value, which we will do in the future when we report this data.</li>
<li><strong>sac0?</strong>: sac0 has the highest AFR of all of the data centers, but it also has the oldest drives—nearly twice as old, on average, versus the next closest in data center, sac2. As discussed previously, <a href="https://www.backblaze.com/blog/drive-failure-over-time-the-bathtub-curve-is-leaking/" target="_blank" rel="noreferrer noopener">drive failures do seem to follow the “bathtub curve”</a>, although recently we’ve seen the curve start out flatter. Regardless, as drive models age, they do generally fail more often. Another factor could be that sac0, and to a lesser extent sac2, has some of the oldest Storage Pods, including a handful of 45-drive units. We are in the process of using CVT to replace these older servers while migrating from 4TB to 16TB and larger drives.</li>
<li><strong>iad1</strong>: The iad data center is the foundation of our eastern region and has been growing rapidly since coming online about a year ago. The growth is a combination of new data and customers using our <a href="https://www.backblaze.com/blog/double-redundancy-support-compliance-and-more-with-cloud-replication-now-live/" target="_blank" rel="noreferrer noopener">cloud replication capability</a> to automatically make a copy of their data in another region.</li>
<li><strong>Q3 Data</strong>: This chart is for Q3 data only and includes all the data drives, including those with less than 60 drives per model. As we track this data over the coming quarters, we hope to get some insight into whether different data centers really have different drive failure rates, and, if so, why.</li>
</ul>
<h2>Lifetime Hard Drive Failure Rates</h2>
<p>As of September 30, 2023, we were tracking 259,084 hard drives used to store customer data. For our lifetime analysis, we collect the number of drive days and the number of drive failures for each drive beginning from the time a drive was placed into production in one of our data centers. We group these drives by model, then sum up the drive days and failures for each model over their lifetime. That chart is below.&nbsp;</p>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1.jpg" data-rel="lightbox-gallery-ht5dUJ2a" data-rl_title="4 – Lifetime AFR Table" data-rl_caption="4 – Lifetime AFR Table" title="4 – Lifetime AFR Table"><img loading="lazy" decoding="async" width="685" height="1024" src="https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1-685x1024.jpg" alt="" srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1-685x1024.jpg 685w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1-201x300.jpg 201w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1-560x838.jpg 560w" sizes="(max-width: 685px) 100vw, 685px"></a></figure>
<p>One of the most important columns on this chart is the confidence interval, which is the difference between the low and high AFR confidence levels calculated at 95%. The lower the value, the more certain we are of the AFR stated. We like a confidence interval to be 0.5% or less. When the confidence interval is higher, that is not necessarily bad, it just means we either need more data or the data is somewhat inconsistent.&nbsp;</p>
<p>The table below contains just those drive models which have a confidence interval of less than 0.5%. We have sorted the list by drive size and then by AFR.</p>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1.jpeg" data-rel="lightbox-gallery-ht5dUJ2a" data-rl_title="5 – Lifetime AFR by CI" data-rl_caption="5 – Lifetime AFR by CI" title="5 – Lifetime AFR by CI"><img loading="lazy" decoding="async" width="710" height="780" src="https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1.jpeg" alt="" srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1.jpeg 710w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1-273x300.jpeg 273w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1-560x615.jpeg 560w" sizes="(max-width: 710px) 100vw, 710px"></a></figure>
<p>The 4TB, 6TB, 8TB, and some of the 12TB drive models are no longer in production. The HGST 12TB models in particular can still be found, but they have been relabeled as Western Digital and given alternate model numbers. Whether they have materially changed internally is not known, at least to us.</p>
<p>One final note about the lifetime AFR data: you might have noticed the AFR for all of the drives hasn’t changed much from quarter to quarter. It has vacillated between 1.39% to 1.45% percent for the last two years. Basically, we have lots of drives with lots of time-in-service so it is hard to move the needle up or down. While the lifetime stats for individual drive models can be very useful, the lifetime AFR for all drives will probably get less and less interesting as we add more and more drives. Of course, a few hundred thousand drives that never fail could arrive, so we will continue to calculate and present the lifetime AFR.</p>
<h2>The Hard Drive Stats Data</h2>
<p>The complete data set used to create the information used in this review is available on our <a href="https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data/">Hard Drive Stats Data webpage</a>. You can download and use this data for free for your own purpose. All we ask are three things: 1) you cite Backblaze as the source if you use the data, 2) you accept that you are solely responsible for how you use the data, and 3) you do not sell this data to anyone; it is free.&nbsp;</p>
<p>Good luck and let us know if you find anything interesting.</p>
</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Lot of Damage in Grindavík (302 pts)]]></title>
            <link>https://icelandmonitor.mbl.is/news/news/2023/11/13/a_lot_of_damage_in_grindavik/</link>
            <guid>38263294</guid>
            <pubDate>Tue, 14 Nov 2023 13:54:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://icelandmonitor.mbl.is/news/news/2023/11/13/a_lot_of_damage_in_grindavik/">https://icelandmonitor.mbl.is/news/news/2023/11/13/a_lot_of_damage_in_grindavik/</a>, See on <a href="https://news.ycombinator.com/item?id=38263294">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><a href="https://cdn.mbl.is/frimg/1/45/18/1451865.jpg" id="a1451865" target="_blank" title="Looking at the fissure." rel="newsimgs"><img src="https://cdn.mbl.is/frimg/1/45/18/1451865.jpg" alt="Looking at the fissure." width="730" height="486"></a>
    </p>
      <p>
        Looking at the fissure.
          <span>mbl.is/Eggert Jóhannesson</span>
      </p>
  </div><div>
        
  
    

        
          <p>
 A lot of damage can be seen around Grindavík after the earthquakes and the formation of a deformation that is moving downwards towards the magma intrusion that is underneath the ground. This depression formation is now estimated to be over 1.2 meters in the northwest end of Grindavík.
</p>

        
          <p>
 A fissure passes through a large part of the town and passes through the road to the sports center. A hot water pipe has been broken in the earthquakes and the sports center seems to be sitting on a meter high pedestal.
</p>

        
          <p data-slate-node="element">
 <span data-slate-node="text">
  <span data-slate-leaf="true">
   <span data-slate-string="true">
    A journalist and a photographer from mbl.is have been travelling around the town today looking at the situation and the damage in town.
   </span>
  </span>
 </span>
</p>

        
          <p data-slate-node="element">
 <span data-slate-node="text">
  <span data-slate-leaf="true">
   <span data-slate-string="true">
    As can be seen in these two pictures from the sports center, the power of the earthquakes has been substantial. The faulting is up to one meter and it is like the pavement at the front has been severed from the building.
   </span>
  </span>
 </span>
</p>

        
          
  
  

  



        
          
  
  

  



        
          <p data-slate-node="element">
 <span data-slate-node="text">
  <span data-slate-leaf="true">
   <span data-slate-string="true">
    A short distance from the sports center, a huge fissure can be seen through the road, and a more than meter-long opening stretches through a part of the town. It has, among other things, at one point broken a hot water pipe and hot air is steaming out of the fissure.
   </span>
  </span>
 </span>
</p>

        
          
  
  

  



        
          <p data-slate-node="element">
 <span data-slate-node="text">
  <span data-slate-leaf="true">
   <span data-slate-string="true">
    The grass is soft and yielding widely close to cracks when walking on it, indicating that there is a widespread lack of soil under the grass. Thus, a rescuer found himself stepping with one foot in one place through grass without being harmed. A journalist at mbl.is has therefore been directed to stay on the tarmac because of this weakness in the soil.
   </span>
  </span>
 </span>
</p>

        
          <p data-slate-node="element">
 <span data-slate-node="text">
  <span data-slate-leaf="true">
   <span data-slate-string="true">
    Today, residents have been allowed into town to retrieve the main necessities and valuable property. They have only a short time to do so. Initially, residents were required to accompany the rescue teams, but as the day went on, it was decided that people could enter in their own cars. However, there are security posts around town where rescue teams are located, pushing people. People had been made aware that they were only expected to be in their home for 5-7 minutes.
   </span>
  </span>
 </span>
</p>

        
          <p>
 Damages are visible on many houses, especially cracks in the concrete, but our journalist from mbl.is could not see any building that could be written off as completely ruined.
</p>
          
      
      
    

    

  

  

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lapce Editor 0.3 (205 pts)]]></title>
            <link>https://github.com/lapce/lapce/releases/tag/v0.3.0</link>
            <guid>38262775</guid>
            <pubDate>Tue, 14 Nov 2023 13:00:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/lapce/lapce/releases/tag/v0.3.0">https://github.com/lapce/lapce/releases/tag/v0.3.0</a>, See on <a href="https://news.ycombinator.com/item?id=38262775">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:lapce/lapce" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="e3DVR65w5hz69YvA3wjfhPHiqLSx6eARwKkcYP3vij6pvU1pWM2isVjdoFaRVV60Oih20G_kYsgNFP1oMtxwNQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="lapce/lapce" data-current-org="lapce" data-current-owner="" data-logged-in="false">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Freleases%2Fshow&amp;source=header-repo&amp;source_repo=lapce%2Flapce" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/lapce/lapce/releases/tag/v0.3.0&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="e2ad5d9fcbff23b186413b8ae3446dbdc571904917e9c0e8b53856a2d76e4d08" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/releases/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Announcing the MonoGame Foundation (122 pts)]]></title>
            <link>https://community.monogame.net/t/announcing-the-monogame-foundation/19809</link>
            <guid>38262320</guid>
            <pubDate>Tue, 14 Nov 2023 12:11:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://community.monogame.net/t/announcing-the-monogame-foundation/19809">https://community.monogame.net/t/announcing-the-monogame-foundation/19809</a>, See on <a href="https://news.ycombinator.com/item?id=38262320">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post_1" itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
        <div>
          
          


          <p><span>
              <time itemprop="datePublished" datetime="2023-11-13T20:11:00Z">
                November 13, 2023,  8:11pm
              </time>
              <meta itemprop="dateModified" content="2023-11-13T20:11:00Z">
          <span itemprop="position">#1</span>
          </span>
        </p></div>
        <div itemprop="articleBody">
          <p>Two months ago, <a href="https://community.monogame.net/t/update-on-monogames-status/19457">we announced</a> our intention to create a non-profit organization to help fund and secure the future of the MonoGame framework as an open-source initiative.</p>
<p>Today, we are pleased to announce that the MonoGame Foundation is officially in business and has applied for non-profit status (which is pending validation).</p>
<p>The MonoGame Foundation’s goal is to bolster the development of the MonoGame framework by voting and funding initiatives to strengthen or expand the open-source project. To this purpose, the Patreon and active donations have been redirected to this new entity.</p>
<p>This new step wouldn’t be possible without a major sponsor: <a href="https://re-logic.com/">Re-Logic</a>, developers of <a href="https://terraria.org/">Terraria</a>, who have committed to support the foundation with a $100,000 donation. On behalf of the foundation board, we would like to celebrate their support and dedication to open-source software, as well as renewing our thanks to past, present, and future donors.</p>
<p>To highlight this shift, we are introducing a new website which is now fully <a href="https://github.com/MonoGame/monogame.github.io">automated with Github</a>. The paint is still fresh, but we hope to expand it with more <a href="https://monogame.net/about.html#foundation">information about the MonoGame Foundation</a>, like how it operates, and how to join it or contribute.</p>
<p>We will soon communicate about the foundation roadmap and the start of projects.</p>
<p>We would like to thank the community for its continuous support, and we hope to build the next iteration of the framework together.</p>
<p>The MonoGame Foundation board:</p>
<ul>
<li>Dean Ellis, President</li>
<li>Marko Jeremic, Chairman</li>
<li>Dominique Louis, Treasurer</li>
<li>Simon Jackson, Secretary</li>
<li>Tom Spilman</li>
<li>Thomas Altenburger</li>
</ul>
        </div>

        <meta itemprop="headline" content="Announcing the MonoGame Foundation">
          <meta itemprop="keywords" content="">

        

         

      </div><div id="post_2" itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
        <div>
          
          <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            <a itemprop="url" href="https://community.monogame.net/u/mrhelmut"><span itemprop="name">mrhelmut</span></a>
            
              pinned globally 
          </span></p>


          <p><span>
              <time itemprop="datePublished" datetime="2023-11-13T20:11:31Z">
                November 13, 2023,  8:11pm
              </time>
              <meta itemprop="dateModified" content="2023-11-13T20:11:31Z">
          <span itemprop="position">#2</span>
          </span>
        </p></div>
        

        <meta itemprop="headline" content="Announcing the MonoGame Foundation">

        

         

      </div><div id="post_3" itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
        
        <div itemprop="articleBody">
          <p>Welcome to the new era of MonoGame everyone.</p>
<p>This is a step in the right direction.</p>
<p><img src="https://community.monogame.net/images/emoji/twitter/sake.png?v=12" title=":sake:" alt=":sake:" loading="lazy" width="20" height="20"></p>
        </div>

        <meta itemprop="headline" content="Announcing the MonoGame Foundation">

        

         

      </div><div id="post_4" itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
        
        <p>Woo! This is such a huge step forwards and having the (massive) backing of a beloved studio like Re-Logic is huge news. I can’t wait to see the growth MonoGame’s going to have now that it has the resources to compete with at least the big FOSS engine that is Godot.</p>

        <meta itemprop="headline" content="Announcing the MonoGame Foundation">

        

         

      </div><div id="post_5" itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
        
        <p>Excellent news! Congratulations! I hope this leads to many great things for Monogame!</p>

        <meta itemprop="headline" content="Announcing the MonoGame Foundation">

        

         

      </div><div id="post_6" itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
        
        <p>Congrats to the new MonoGame Foundation.</p>

        <meta itemprop="headline" content="Announcing the MonoGame Foundation">

        

         

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Blender 4.0 (647 pts)]]></title>
            <link>https://wiki.blender.org/wiki/Reference/Release_Notes/4.0</link>
            <guid>38262315</guid>
            <pubDate>Tue, 14 Nov 2023 12:11:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wiki.blender.org/wiki/Reference/Release_Notes/4.0">https://wiki.blender.org/wiki/Reference/Release_Notes/4.0</a>, See on <a href="https://news.ycombinator.com/item?id=38262315">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                      <h4>Download</h4>
                      <p>Get the latest Blender, older versions, or experimental builds.</p>
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Sues Men Who Weaponized DMCA Notices to Crush Competition (352 pts)]]></title>
            <link>https://torrentfreak.com/google-sues-men-who-weaponized-dmca-notices-to-crush-competition-231114/</link>
            <guid>38262124</guid>
            <pubDate>Tue, 14 Nov 2023 11:48:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://torrentfreak.com/google-sues-men-who-weaponized-dmca-notices-to-crush-competition-231114/">https://torrentfreak.com/google-sues-men-who-weaponized-dmca-notices-to-crush-competition-231114/</a>, See on <a href="https://news.ycombinator.com/item?id=38262124">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><a href="https://torrentfreak.com/images/dmca-google-s1.png"><img decoding="async" src="https://torrentfreak.com/images/dmca-google-s1.png" alt="dmca-google-s1" width="290" height="205" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20290%20205'%3E%3C/svg%3E" data-lazy-src="https://torrentfreak.com/images/dmca-google-s1.png"></a>While all non-compliant DMCA takedown notices are invalid by default, there’s a huge difference between those sent in error and others crafted for purely malicious purposes.</p>
<p>Bogus DMCA takedown notices are nothing new, but the rise of organized groups using malicious DMCA notices as a business tool has been apparent in recent years. </p>
<p>Since the vast majority of culprits facing zero consequences, that may have acted as motivation to send more. Through a lawsuit filed at a California court on Monday, Google appears to be sending the message that enough is enough.</p>
<h2>Defendants Weaponized DMCA Takedowns</h2>
<p>Google’s complaint targets Nguyen Van Duc and Pham Van Thien, both said to be residents of Vietnam and the leaders of up to 20 Doe defendants. Google says the defendants systematically abused accounts “to submit a barrage” of fraudulent copyright takedown requests aimed at removing their competitors’ website URLs from Google Search results.</p>
<p>“Defendants have weaponized copyright law’s notice-and-takedown process and used it not for its intended purpose of expeditiously removing infringing content, but instead to have the legitimate content of their competitors removed based on false allegations. Defendants’ illegal, fraudulent scheme harms consumers, third-party businesses, and Google; stifles competition; and threatens to tarnish Google’s trusted brand.”</p>
<p>Over the past few years, Nguyen, Pham and those working with them, are said to have created at least 65 Google accounts to send confirmed bogus notices targeting 117,000 URLs, plus another 500,000 URLs via notices that Google suspects are fraudulent too.</p>
<p>“Defendants appear to be connected with websites selling printed t-shirts, and their unlawful conduct aims to remove competing third-party sellers from Google Search results. Defendants have maliciously and illegally exploited Google’s policies and procedures under the DMCA to sabotage and harm their competitors,” the complaint adds.</p>
<h2>Google Aims to Put an End to Abuse, Hold Defendants Accountable</h2>
<p>Google goes on to highlight its position as a major intermediary that processes DMCA notices targeting 600 million URLs every year, and the requirement under the DMCA to remove or disable content notified as allegedly infringing. If the company fails to act expeditiously once in receipt of a DMCA notice that complies with the statutory requirements, the company risks losing its safe harbor protection, Google notes.</p>
<p>Since Google must often rely on the accuracy of statements made in DMCA notices, fraudulent notices can result in content being wrongfully taken down. That damages the company’s search engine advertising business, and the business Google’s customers hoped to attract. In this matter, the defendants’ embarked on a campaign that exploited Google’s systems and the DMCA takedown process to undermine their competitors.</p>
<h2>Fake Names, Fraudulent Representations</h2>
<p>The misrepresentations in notices sent to Google were potentially damaging to other parties too. Under fake names, the defendants falsely claimed to represent large companies such as Amazon, Twitter, and NBC News, plus sports teams including the Philadelphia Eagles, Los Angeles Lakers, San Diego Padres. </p>
<p>In similarly false notices, they claimed to represent famous individuals including Elon Musk, Taylor Swift, LeVar Burton, and Kanye West.</p>
<p>The complaint notes that some notices were submitted under company names that do not exist in the United States, at addresses where innocent families and businesses can be found. Google says that despite these claims, the defendants can be found in Vietnam from where they proudly advertise their ‘SEO’ scheme to others, including via YouTube. </p>
<center><a href="https://torrentfreak.com/images/Fake-SEO-Fake-DMCA.png"><img decoding="async" src="https://torrentfreak.com/images/Fake-SEO-Fake-DMCA.png" alt="Fake SEO Fake DMCA" width="610" height="514" srcset="https://torrentfreak.com/images/Fake-SEO-Fake-DMCA.png 610w, https://torrentfreak.com/images/Fake-SEO-Fake-DMCA-300x253.png 300w" sizes="(max-width: 610px) 100vw, 610px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20610%20514'%3E%3C/svg%3E" data-lazy-srcset="https://torrentfreak.com/images/Fake-SEO-Fake-DMCA.png 610w, https://torrentfreak.com/images/Fake-SEO-Fake-DMCA-300x253.png 300w" data-lazy-src="https://torrentfreak.com/images/Fake-SEO-Fake-DMCA.png"></a></center>
<p>“Bad actors like Defendants use this tactic to attack and fraudulently suppress competitors’ websites and products in Google Search results, making consumers more likely to buy the same or similar products from the bad actors or their affiliates,” the complaint continues.</p>
<p>“Such bad actors know that a fraudulent takedown request often has the same effect as a legitimate one; if a takedown request contains all the elements required under Section 512(c)(3)(A), it likely will trigger removal by Google.</p>
<p>“Unfortunately, to ensure compliance with the DMCA and in reliance on the information submitted in Defendants’ takedown requests, Google’s system removed a significant number of thirdparty website URLs targeted by Defendants for a period of time before Google and/or the websites’ owners figured out what was going on and took appropriate steps to reinstate the URLs.”</p>
<p>A particularly damaging batch of fraudulent notices targeted more than 35,000 URLs operated by a Google customer that spends tens of millions of dollars per year on Google search ads. The effect was a significant drop in traffic during the holiday season, revenue losses for the customer and its sellers of $5 million, and a loss to Google of between $2 and $3 million.</p>
<h2>Holding Defendants Accountable</h2>
<p>Those who knowingly make false statements in a DMCA notice can be held liable for damages, costs, and attorneys’ fees. In this matter the defendants’ conduct is said to have caused Google to suffer economic harm due to lost advertising revenue, damage to business relations, and the allocation of significant resources to investigate their wrongdoing. </p>
<p>Google seeks attorneys’ fees and damages under <a href="https://www.law.cornell.edu/uscode/text/17/512">17 U.S.C. §512(f)</a>, in an amount to be determined at trial.</p>
<p>The complaint adds that when the defendants created dozens of Google accounts, each time they entered into enforceable agreements with Google. While Google says it has “performed all its obligations” under those contracts, the actions of the defendants amount to breaches of their contractual obligations to Google and intentional interference in contractual relationships between Google and its advertising customers.</p>
<p>Google says the defendants should be required to pay all general, special, and actual damages that Google “has sustained or will sustain” due to the fraudulent notices.</p>
<p>Google further requests an order to restrain the defendants (and anyone working in concert with them), from submitting any further fraudulent takedown notices and/or creating any Gmail accounts. Google also wants a ban on the defendants using any of its products or services to advertise their websites or products. </p>
<p><em>The complaint is available <a href="https://torrentfreak.com/images/5-23-cv-05824-Google-v-Nguyen-Van-Duc-Pham-Van-Thien-complaint-231113.pdf">here</a> (pdf)</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Constraints on physical computers in holographic spacetimes (112 pts)]]></title>
            <link>https://arxiv.org/abs/2304.09900</link>
            <guid>38261913</guid>
            <pubDate>Tue, 14 Nov 2023 11:24:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2304.09900">https://arxiv.org/abs/2304.09900</a>, See on <a href="https://news.ycombinator.com/item?id=38261913">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2304.09900.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>Within the setting of the AdS/CFT correspondence, we ask about the power of computers in the presence of gravity. We show that there are computations on $n$ qubits which cannot be implemented inside of black holes with entropy less than $O(2^n)$. To establish our claim, we argue computations happening inside the black hole must be implementable in a programmable quantum processor, so long as the inputs and description of the unitary to be run are not too large. We then prove a bound on quantum processors which shows many unitaries cannot be implemented inside the black hole, and further show some of these have short descriptions and act on small systems. These unitaries with short descriptions must be computationally forbidden from happening inside the black hole.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Alex May [<a href="https://arxiv.org/show-email/9e66402e/2304.09900">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 19 Apr 2023 18:00:50 UTC (47 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rust Without Crates.io (177 pts)]]></title>
            <link>https://thomask.sdf.org/blog/2023/11/14/rust-without-crates-io.html</link>
            <guid>38261539</guid>
            <pubDate>Tue, 14 Nov 2023 10:34:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thomask.sdf.org/blog/2023/11/14/rust-without-crates-io.html">https://thomask.sdf.org/blog/2023/11/14/rust-without-crates-io.html</a>, See on <a href="https://news.ycombinator.com/item?id=38261539">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>Rust is a lovely programming language but I’ve never quite come to terms with crates.io, or any other of these language-specific repositories where everyone uploads and downloads code willy-nilly. I have several objections:</p>

<ul>
  <li>If crates.io goes down or access is otherwise disrupted then the Rust community will stop work. It is profoundly unresilient to have a single point of failure like this. Certainly some people will have <a href="https://doc.rust-lang.org/cargo/commands/cargo-vendor.html">vendored their deps</a> and others will have a <a href="https://crates.io/crates/panamax"><code>panamax</code> mirror</a> handy, but for most, Rust as we know it stops if this one particular web service goes down.</li>
  <li>There is no mediation of any kind between when a new library/version is published and when it is consumed. You need only one author in your maybe-hundreds-of-dependencies tree to be hacked, coerced or in a malicious mood for you to have a <em>really</em> bad day.</li>
  <li>Any tampering with crates.io itself (espionage, disgruntlement, national security) could have an incredibly wide blast radius, or a incredibly wide set of targets from which to choose.</li>
  <li>Since crates.io is <em>the</em> source for crates, it is normal for both developers and CI machines to be hitting this web service all the time. Opportunities for mischief are exacerbated when clients are phoning home so frequently.</li>
</ul>

<p>So what’s the alternative? I think we all need to take a step back from the altar of developer velocity and take a deep breath. I don’t want dependencies hot off the press. Ideally I want someone independent of the authors playing a curatorial role.</p>

<p>Now, actually getting some human review of dependency updates is quite a hard thing to do. <a href="https://github.com/crev-dev/cargo-crev"><code>cargo-crev</code></a> has been trying for years to make this happen. I would love if it was the solution but it isn’t yet, and I think it’s a little ambitious. Yes we would like to have super-experienced software developers reviewing all our libraries with cryptographic stamps of approval, but if they’re not available we could be the target of remote shell in a <code>build.rs</code>. Surely there’s a middle ground here?</p>

<p>What’s interesting is that this problem is largely solved for C and C++: Linux distributions like Debian package such a wide range of libraries that for many things that you want to develop or install, you don’t need any third-party libraries <em>at all</em>. It’s just a matter of finding the right <code>apt-get</code> incantations and off you go. Even if you can get 95% of your libraries from a common trusted source then your risk is decreased considerably.</p>

<p>Rust libraries don’t work quite the same as C/C++ ones. Normal Rust code can’t be dynamically linked—a binary will have all of its dependencies statically linked at build time, so you won’t typically see <code>.so</code> files for Rust libraries that are going to be consumed by other Rust code. Since there is no <code>.so</code> file, Debian has no package that installs the library. However if they want to ship a binary that was written in Rust, their builders can’t just be downloading stuff from crates.io. They need a way to package all of the software that represents that Debian release. To solve that problem they’ve taken all these little dependencies and put their <em>full Rust source code</em> in packages with names like <code>librust-cratename-dev</code>.</p>

<p>Hmm, how many such packages? Running on trixie (testing)…</p>

<div><pre><code>$ aptitude search librust- | grep -vE "^v " | wc -l
2336
</code></pre></div>

<p>This is starting to look like a serious curation of the most important Rust crates, available from any Debian mirror. There are some double-ups to be sure, since in some cases multiple incompatible versions of the same crate had to be packaged. Still. Maybe there is enough Rust in Debian now that it’s viable to write interesting Rust software independently of crates.io? That would solve basically all my concerns and the situation is only going to improve as more Rust software gets packaged.</p>

<p>To be clear, I don’t expect that Debian Developers are auditing these packages in the manner of <code>cargo-crev</code>. The good thing is that they <em>don’t actually need to</em> for it to be a major improvement.</p>

<ul>
  <li>A DD isn’t going to upload a new patch release <em>just ‘cause</em>. It’s going to be because it has an important fix or because some other program has depended on it. On crates.io a maintainer is free to create new releases for any reason and <code>cargo update</code> is not going to evaluate how good that reason is.</li>
  <li>A simple time delay will allow egregious malware like malicious <code>build.rs</code> scripts to be caught, whether that’s the super-long Debian stable cycle or even the several days required to migrate from <em>unstable</em> to <em>testing</em>. I assume that an urgent security issue would be distributed the same as any other Debian update.</li>
  <li>They might decide to give the diff at least a cursory look, which is better than nothing.</li>
</ul>

<p>How do we do this? It’s actually quite easy because the big-brained Debian developers have arranged all the Rust dependencies to follow the format of a cargo <a href="https://doc.rust-lang.org/cargo/reference/source-replacement.html#directory-sources">Directory Source</a>. That is, all of the packages are installed in their own directories under <code>/usr/share/cargo/registry</code>, including implementing <a href="https://github.com/rust-lang/cargo/issues/11063">a cheeky workaround</a> for the required <code>.cargo-checksum.json</code> files.</p>

<p>You can then add some brief incantations to your <code>.cargo/config.toml</code>, whether on a project- or user-wide basis:</p>

<div><pre><code><span>[net]</span>
<span>offline</span> <span>=</span> <span>true</span>

<span>[source]</span>

<span>[source.apt]</span>
<span>directory</span> <span>=</span> <span>"/usr/share/cargo/registry"</span>

<span>[source.crates-io]</span>
<span>replace-with</span> <span>=</span> <span>"apt"</span>
</code></pre></div>

<p>This overrides the default crates.io source and ensures dependencies can only be fulfilled locally by installing the relevant packages. This happily doesn’t require any changes to your projects themselves—you just have to be careful to use versions in your <code>Cargo.toml</code> (and <code>Cargo.lock</code>) that are resolvable on Debian, since it is a subset of those available on the wider crates.io.</p>

<p>I am quite certain that Debian wouldn’t have enough coverage yet for the monorepo at work, but I gave this a go on my <a href="https://github.com/thombles/hashgood">one of my little CLI projects</a> that has half a dozen dependencies. Apart from having to downgrade <code>copypasta</code> from 0.8.2 to 0.8.1 in the <code>Cargo.lock</code>, this builds and runs just fine. What a treat.</p>

<p>This little investigation has given me much more confidence in using Rust generally into the future. I feared that the “grab any dependency version you like” approach facilitated by crates.io would render Rust impervious to any sort of curation effort, such that anyone who was serious about my earlier concerns would have to stick to a language used to the old ways like C++. Fortunately, Debian is here to prove me wrong. A+ work by their Rust packaging team.</p>

<p>All power to those who like to live on the edge; I’ll be over here trying to minimise different types of dependencies.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU Parliament Decides That Your Private Messages Must Not Be Scanned (543 pts)]]></title>
            <link>https://tuta.com/blog/chat-control</link>
            <guid>38261415</guid>
            <pubDate>Tue, 14 Nov 2023 10:17:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tuta.com/blog/chat-control">https://tuta.com/blog/chat-control</a>, See on <a href="https://news.ycombinator.com/item?id=38261415">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Chat control - one of the worst EU plans that is also being described as a surveillance monster - must be stopped. And the
EU Parliament has just decided to do so! In a historic agreement on the EU Commission's Child Sexual Abuse Regulation (CSAR) the European
Parliament wants to remove chat control requirements and safeguard secure encryption. The decision came after extensive backlash
against the original proposal from technology and security experts, to international scientists and to citizens across Europe.
This is a great win for our right to privacy and for upholding our democratic values in Europe, but the fight continues!
</p><div><p>Today the EU Parliament decided on an <a href="https://www.patrick-breyer.de/wp-content/uploads/2023/11/CSAR_LIBE-Verhandlungsmandat.pdf">alternative version of chat control</a> - one that fortunately
does not deserve this name anymore: After huge opposition against the surveillance methods included in the CSA Regulation
(see 'Opposition against chat control' below), the EU Parliament has decided to uphold every citizen's right to privacy
and underlined the importance of upholding our democratic values. We in Europe must not follow autocratic regimes like
China and Russia by monitoring all our citizens.</p>
<p>Patrick Breyer, Member of the EU Parliament and part of the CSAR negotiations says:</p>
<blockquote>
<p>"Under the impression of massive protests against the looming indiscriminate chat control mass scanning of private messages, we managed to
win a broad majority for a different, new approach to protecting young people from abuse and exploitation online. As a pirate and digital
freedom fighter, I am proud of this breakthrough. The winners of this agreement are on the one hand our children, who will be protected much
more effectively and in a court-proof manner, and on the other hand all citizens, whose digital privacy of correspondence and communication security will be guaranteed."</p>
</blockquote>
<blockquote>
<p>"<strong>Even if this compromise, which is supported from the progressive to the conservative camp, is not perfect on all points, it is a historic
success that removing chat control and rescuing secure encryption is the common aim of the entire Parliament.</strong> We are doing the exact opposite of
most EU governments who want to destroy digital privacy of correspondence and secure encryption. Governments must finally accept that this highly
dangerous bill can only be fundamentally changed or not be passed at all. The fight against authoritarian chat control must be pursued with all determination!"</p>
</blockquote>
<h3 id="what-did-the-eu-parliament-decide">What did the EU Parliament decide?</h3>
<p>Breyer <a href="https://www.patrick-breyer.de/en/historic-agreement-on-child-sexual-abuse-proposal-csar-european-parliament-wants-to-remove-chat-control-and-safeguard-secure-encryption/">writes</a>
on his website that internet services and apps must be "secure by design and default". The EU Parliament has agreed to:</p>
<blockquote>
<p>"safeguard the digital secrecy of correspondence and remove the plans for blanket chat control, which violate fundamental
rights and stand no chance in court. The current voluntary chat control of private messages (not social networks) by US internet
companies is being phased out. Targeted telecommunication surveillance and searches will only be permitted with a
judicial warrant and only limited to persons or groups of persons suspected of being linked to child sexual abuse material."</p>
</blockquote>
<p><strong>A huge win for our privacy rights is also that the EU Parliament has decided to
"clearly exclude so-called client-side scanning".</strong></p>
<p>In contrast to the original chat control proposal, the version of the EU Parliament wants that a new EU Child Protection Centre
proactively searches publicly accessible parts of the internet for child sexual abuse material with automatic crawling, which can also
take place in darknet and would be much more efficient than private surveillance measures by providers. Found abuse material
must be reported and taken down by the provider.</p>
<h3 id="fight-is-not-over">Fight is not over</h3>
<p>While the EU Parliament's decision is a huge win, the fight is not over. It is expected that the EU Commission will continue to push for general
surveillance chat control measures. Now is the time for each and everyone of us to join this fight!</p>
<p><strong>You can help fight chat control and uphold our right to privacy. Check at the end of this post, what you can do!</strong></p>
<h2 id="opposition-against-chat-control">Opposition against chat control</h2>
<p>Chat control has been in discussion for along time already, and the criticism of this draft bill is huge. Significant is
not only that technology and security experts agree that <a href="https://tuta.com/blog/posts/eu-client-side-scanning">client-side scanning</a> is not possible without risking
everyone's security. Also scientists, the general public, even the EU's Research Service oppose the EU
Commission's chat control proposal.</p>
<h3 id="scientists-letter-to-eu-parliament">Scientists letter to EU Parliament</h3>
<p>300 scientists from all around the world have sent an open letter to the EU Parliament to call on policymakers
to <strong>stop chat control</strong>, the EU’s proposed Child Sexual Abuse Regulation. They say while it is the responsibility of politicians to protect children from sexual abuse,
"it is our professional recommendation as scientists that such a proposal be not taken forward" because the
scanning techniques the EU is proposing to use are deeply flawed and would endanger the security of everyone using the internet.</p>
<p>The scientists make the EU proposal look like wishful thinking: "Given the horrific nature of child sexual abuse, it is understandable,
and indeed tempting, to hope that there is a technological intervention that can eradicate it. Yet,
looking at the issue holistically, we cannot escape the conclusion that the current proposal is not such an intervention."</p>
<p>There is no magic key that allows the police to scan all chat messages, emails, and more for harmful content while not
risking the security and privacy of everyone. This is technically not possible.</p>
<p>The scientists argue that chat control is too much of a threat to everyone and therefore must be stopped:</p>
<blockquote>
<p>"First and foremost, we acknowledge that child sexual abuse and exploitation is a very serious crime which can cause
lifelong harm to survivors. It is the responsibility of government authorities, with the support of companies and communities,
to undertake effective interventions which prevent this crime and react to it quickly when it does happen."</p>
</blockquote>
<blockquote>
<p>"The European Commission has proposed a law with the stated aim of stopping the spread of child sexual abuse material
online and of grooming of children online. To do so, the law allows authorities to compel providers of any apps or other
online services to scan the messages, pictures, emails, voice mails and other activities of their users. In the case of end-to-end
encrypted apps, the claim is that this scanning can be done on users’ devices – so-called ‘Client-Side Scanning’ (CSS)."</p>
</blockquote>
<blockquote>
<p><strong>"Passing this legislation undermines the thoughtful and incisive work that European researchers have provided in cybersecurity
and privacy, including contributions to the development of global encryption standards. Such undermining will weaken the environment
for security and privacy work in Europe, lowering our ability to build a secure digital society."</strong></p>
</blockquote>
<blockquote>
<p>"The proposed regulation would also set a global precedent for filtering the Internet, controlling who can access it, and taking
away some of the few tools available for people to protect their right to a private life in the digital space. This will have a
chilling effect on society and is likely to negatively affect democracies across the globe."</p>
</blockquote>
<blockquote>
<p>"We therefore strongly warn against pursuing these or similar measures as their success is not possible given current
and foreseeable technology, while their potential for harm is substantial."</p>
</blockquote>
<p>You can read the full open letter <a href="https://docs.google.com/document/d/13Aeex72MtFBjKhExRTooVMWN9TC-pbH-5LEaAbMF91Y/mobilebasic">here</a>.</p>
<h3 id="eus-research-service-opposes-chat-control">EU's Research Service opposes chat control</h3>
<p>In April, the European Parliament's Research Service (EPRS) presented a new study on the legality of the proposed Child Sexual Abuse
Regulation, also called Chat Control.</p>
<p>The EU Commission's plans to fight images of abused children on the Internet are not very effective and violate the fundamental rights of Internet users,
according to this analysis on chat control. While the number of reported cases is likely to go up significantly, the accuracy of the hits is likely to also
decrease significantly, increasing the burden on investigative authorities.</p>
<h3 id="consequences-of-draft-eu-law">Consequences of draft EU law</h3>
<p>The legal experts of the EU Parliament's Scientific Service conclude that:</p>
<blockquote>
<p>"when weighing the fundamental rights affected by the measures of the CSA proposal, it can be established that the <strong>CSA
proposal would violate Articles 7 and 8 of the Charter of Fundamental Rights with regard to users.</strong>"</p>
</blockquote>
<p>The report also says if chat control becomes a law "that this violation of the
prohibition of <strong>general data retention</strong> and the prohibition of <strong>general surveillance obligations</strong> cannot be justified."</p>
<blockquote>
<p>"A detection order on the content of interpersonal data either on the device or the server will <strong>compromise the essence of
the right to privacy</strong> under Article 7 CFR in the form of confidentiality of telecommunications. It constitutes a form of
access on a generalised basis, pursuant to Schrems, where it involves an <strong>analysis of all communications</strong> going through the server.“</p>
</blockquote>
<p>The experts made clear that an "increase in the number of reported contents does not necessarily lead to a corresponding increase in investigations and prosecutions leading to better protection of children.
As long as the capacity of law enforcement agencies is limited to its current size, an increase in reports will make effective prosecution of depictions of abuse more difficult."</p>
<p>In addition, the study on chat control finds: "It is undisputed that children need to be protected from becoming victims of child abuse and depictions of abuse online... but they also need to be able
to enjoy the protection of fundamental rights as a basis for their development and transition into adulthood."</p>
<p>Pirate Party MEP Patrick Breyer, long-time opponent of mass scanning of private communications, comments:</p>
<p>"The EU Parliament's Scientific Service now confirms in crystal clear words what I and numerous human rights activists, law enforcement
officials, legal experts, abuse victims and child protection organisations have been warning about for a long time: the proposed general,
indiscriminate <strong><a href="https://tuta.com/blog/posts/eu-client-side-scanning">scanning of our private conversations and photos</a></strong> destroys the digital privacy of correspondence and violates our fundamental
rights. A flood of mostly false suspicious activity reports would make effective investigations more difficult, criminalise children en masse
and fail to bring the abusers and producers of such material to justice. According to this expertise, searching private communications for
potential child sexual exploitation material, known or unknown, is legally feasible only if the search provisions are targeted and limited
to persons presumably involved in such criminal activity."</p>
<p><strong>"What we really need instead of untargeted chat control and identification obligations for age verification is obliging law enforcement
agencies to have known exploitation material removed from the internet, as well as Europe-wide standards for effective prevention measures,
victim support and counselling, and for effective criminal investigations."</strong></p>
<p>This is also the view of many other experts, such as Mullvad, Edri and others.</p>
<h2 id="stop-chat-control">Stop chat control</h2>
<h3 id="mullvad-really-nails-it-with-their-campaign">Mullvad really nails it with their campaign!</h3>
<p><strong>Chat control is one of the worst EU plans to date and must be stopped. Mullvad VPN has recently launched a great campaign to fight for democracy.</strong></p>
<p>Mullvad's campaign, launched on March 3rd, calls on EU policy makers to stop chat control and rethink their stance in regards to the EU
Commission's proposal for detecting and prosecuting the sharing of child sexual abuse material (CSAM) via the internet.
The EU proposal includes far-reaching surveillance measures such as client-side scanning, which would force online services
to scan every chat message and every email that anybody in the European Union ever sends for child sexual abuse material.</p>
<p><strong>This legislation would de facto deprive EU citizens of any privacy on the Internet, it would even undermine encryption
and thus weaken the security of all Internet users.</strong></p>
<p>For that reason, the <a href="https://tuta.com/blog/posts/eu-csam-scanning">EU plans to scan for CSAM is heavily criticized</a>
by cryptography experts, human rights organizations as well as internet activists across Europe.</p>
<p>Most recently, Germany has made its opposition to <a href="https://tuta.com/blog/posts/eu-client-side-scanning">client-side scanning public</a>.
With resistance in Germany, Ireland, Austria and the Netherlands to the EU proposal,
a blocking minority is within reach.</p>
<h3 id="perfect-timing">Perfect timing</h3>
<p>Mullvad adds to the pressure with their new campaign, which was launched during the Swedish EU Presidency, which started on 1st of January 2023. The timing, thus,
couldn't be better.</p>
<p>Mullvad says on their campaign page:</p>
<blockquote>
<p><strong>Now is the time for debate and actions</strong></p>
</blockquote>
<blockquote>
<p>A democratic society is built upon discussions, before law proposals become reality. We started the conversation on the streets of Sweden, during the country’s EU presidency.</p>
</blockquote>
<p>Along with the digital campaign, they posted large billboards across Sweden to draw attention to the ongoing legal
debate on EU level.</p>
<blockquote><p dir="ltr" lang="en">The EU Commission wants to monitor all the citizens of the European union. The law proposal is called <a href="https://twitter.com/hashtag/chatcontrol?src=hash&amp;ref_src=twsrc%5Etfw">#chatcontrol</a> – and now is the time to stop it. We took the debate to the streets of Sweden, during the country’s EU-presidency. Take a look at <a href="https://t.co/Dx9cPe1ksq">https://t.co/Dx9cPe1ksq</a> <a href="https://t.co/FvqAlQRiig">pic.twitter.com/FvqAlQRiig</a></p>— Mullvad.net (@mullvadnet) <a href="https://twitter.com/mullvadnet/status/1631639744537870336?ref_src=twsrc%5Etfw">March 3, 2023</a></blockquote> 

<h3 id="opposition-to-chat-control">Opposition to chat control</h3>
<p>The digital rights organization EDRi has recently launched the 'Stop Scanning Me' campaign where EU citizens
can sign a petition against the EU's surveillance plan.</p>
<p><strong>Sign the Stop Scanning Me campaign <a href="https://civicrm.edri.org/stop-scanning-me">now</a>!</strong></p>
<h3 id="what-is-chat-control">What is chat control?</h3>
<p>The <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=COM%3A2022%3A209%3AFIN">Eu proposal</a> on chat control wants to force
online services to AI scan every message and every email for possible child grooming and child sexual abuse material (known and unknown).
Suspicious messages flagged by the AI will be reported to law enforcement and investigated.</p>
<p>Machine searching for potential child grooming and sexual abuse material is an artificial intelligence (AI) supported procedure.
The AI is not flawless and will flag a high number of harmless, private images, which will then be investigated by the police.
Experts expect that 10-20% of images reported will be false positives.</p>
<p>This is a huge intrusion into the privacy of millions of innocent citizens.</p>
<p><strong>The European Date Protection Supervisor Wiewiórowski <a href="https://www.euractiv.com/section/law-enforcement/news/eu-watchdog-online-child-abuse-draft-law-creates-illusion-of-legality/">calls it</a>
an 'illusion of legality': This type of indiscriminate scanning of private communications "will always be illegal under the Charter of
Fundamental Rights (and probably under several national constitutional laws as well)."</strong></p>
<h3 id="the-risks-of-chat-control">The risks of chat control</h3>
<p>To many the risks of chat control are negligible. After all, as law-abiding citizens what is there to fear?</p>
<p>But the truth is the opposite: The risks of a surveillance tool like chat control are unlimited.</p>
<h4 id="1-you-dont-know-whether-the-laws-will-change">1. You don't know whether the laws will change.</h4>
<p>Jan Penfrat said it perfectly on <a href="https://eupolicy.social/@ilumium/109972484325693478">Mastodon</a>:</p>
<p>"You have nothing to hide until the government suddenly declares your behaviour illegal."</p>
<p>
		<picture>
   			<source type="image/webp" srcset="https://tuta.com/blog/images/abortion-illegal-chatcontrol.webp">
    		<img height="854" width="1440" loading="lazy" alt="Chat control becomes very dangerous as soon as your behaviour is declared illegal. It must be stopped." src="https://tuta.com/blog/images/abortion-illegal-chatcontrol.jpg">
		</picture></p>
<p>The text on the image he posted is taken from news that broke this week via the
<a href="https://www.businessinsider.com/police-getting-help-social-media-to-prosecute-people-seeking-abortions-2023-2">Business Insider</a>:
"Police are prosecuting abortion seekers using their digital data — and Facebook and Google help them do it".</p>
<h4 id="2-compromised-encryption-is-not-encryption">2. Compromised encryption is not encryption</h4>
<p>Once you break encryption to allow access to the 'good guys', the security and privacy promised by encryption is gone.</p>
<p>It is simply not possible to implement an <a href="https://tuta.com/blog/posts/why-a-backdoor-is-a-security-risk">encryption backdoor</a> that can only be used by law enforcement.</p>
<p>This is also nicely illustrated by the <a href="https://tuta.com/blog/posts/encryption-backdoor-fails">best of backdoor fails in history</a>. The truth is: Secret services
have tried to undermine encryption before, but whenever they were successful, others were too. Malicious intruders have become
very powerful.</p>
<p><strong>We in Europe must not weaken the security backbone that our digital life depends on: Encryption.</strong></p>
<h3 id="lets-stop-client-side-scanning">Let's stop client-side scanning</h3>
<p>Now we, as citizens of Europe and members of the civil society, must put pressure on legislators to oppose legislation that
will put every email and every chat message that we send under constant surveillance.</p>
<p><strong>We can stop chat control together!</strong></p>
<ol>
<li><p>Share the <a href="https://mullvad.net/en/chatcontrol/campaign">Mullvad campaign</a> to increase the pressure on politicians.</p>
</li>
<li><p>Call/email your EU representative to make your voice heard: "Stop CSAM scanning. I do not want my personal device to become a surveillance machine!"</p>
</li>
<li><p>Sign the <a href="https://civicrm.edri.org/stop-scanning-me">Stop Scanning Me campaign</a>.</p>
</li>
</ol>
<p>Together we can stop chat control!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Truth Social reports $73M net loss since launch (101 pts)]]></title>
            <link>https://www.reuters.com/technology/trumps-truth-social-reports-73-mln-net-loss-since-launch-2023-11-14/</link>
            <guid>38260815</guid>
            <pubDate>Tue, 14 Nov 2023 09:04:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/trumps-truth-social-reports-73-mln-net-loss-since-launch-2023-11-14/">https://www.reuters.com/technology/trumps-truth-social-reports-73-mln-net-loss-since-launch-2023-11-14/</a>, See on <a href="https://news.ycombinator.com/item?id=38260815">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="primary-image" role="figure" aria-describedby="primary-image-caption"><figure><div data-testid="Image"><p><img src="https://cloudfront-us-east-2.images.arcpublishing.com/reuters/CO4F5ULZPBO6BJHFJNRA4I3FM4.jpg" srcset="https://www.reuters.com/resizer/qEv1IhEtEHGku_Gr8HYX_-O0ddw=/480x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/CO4F5ULZPBO6BJHFJNRA4I3FM4.jpg 480w,https://www.reuters.com/resizer/IZhZGJ1aHfJy2znLmv1U2TUT8uw=/960x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/CO4F5ULZPBO6BJHFJNRA4I3FM4.jpg 960w,https://www.reuters.com/resizer/zQRquT-heMgwKu_08BObVyBVHHU=/1080x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/CO4F5ULZPBO6BJHFJNRA4I3FM4.jpg 1080w,https://www.reuters.com/resizer/LqdJ5x5C2cTWYNI4Fh6bIzZEbqc=/1200x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/CO4F5ULZPBO6BJHFJNRA4I3FM4.jpg 1200w" sizes="(min-width: 1024px) 560px, (min-width: 1440px) 700px, 100vw" width="7485" height="4992" alt="Trump Organization trial in New York State Supreme Court in New York"></p></div><p data-testid="Body"><span>Former U.S. President Donald Trump attends the Trump Organization civil fraud trial, in New York State Supreme Court in the Manhattan borough of New York City, U.S., October 25, 2023. Dave Sanders/Pool via REUTERS//File Photo <a data-testid="Link" href="https://www.reutersagency.com/en/licensereuterscontent/?utm_medium=rcom-article-media&amp;utm_campaign=rcom-rcp-lead" target="_blank" referrerpolicy="no-referrer-when-downgrade"> Acquire Licensing Rights</a></span></p></figure></div><div><p data-testid="paragraph-0">Nov 13 (Reuters) - Former U.S. President Donald Trump's social media platform Truth Social has lost $73 million since its launch in early 2022, a securities filing by Digital World Acquisition Corp <a data-testid="Link" href="https://www.reuters.com/markets/companies/DWAC.O" target="_blank" referrerpolicy="no-referrer-when-downgrade">(DWAC.O)</a>, the SPAC that plans to merge with the company, showed on Monday.</p><p data-testid="paragraph-1">Trump had announced the launch of his social media app in Oct 2021, <a data-testid="Link" href="https://www.reuters.com/world/us/former-us-president-donald-trump-launches-new-social-media-platform-2021-10-21/" referrerpolicy="no-referrer-when-downgrade">saying</a> it would "stand up to Big Tech" companies such as Twitter and Facebook that previously barred him.</p><p data-testid="paragraph-2">In 2022, Truth Social posted a loss of $50 million, with net sales of just $1.4 million. It lost $23 million in the first half of this year, with net sales of $2.3 million.</p><p data-testid="paragraph-3">Trump Media &amp; Technology Group's (TMTG) independent registered public accounting firm has indicated that the financial condition raises substantial doubt as to its ability to continue as a going concern, according to the filing.</p><p data-testid="paragraph-4">"TMTG believes that it may be difficult to raise additional funds through traditional financing sources in the absence of material progress toward completing its merger with Digital World."</p><p data-testid="paragraph-5">The company also eliminated several positions in March, the filing said, adding that the action followed a review of all departments, most significantly impacting TMTG's streaming video on demand and infrastructure teams.</p><p data-testid="Body">Reporting by Maria Ponnezhath in Bengaluru; Editing by Nivedita Bhattacharjee</p><p data-testid="Body">Our Standards: <a data-testid="Link" href="https://www.thomsonreuters.com/en/about-us/trust-principles.html" target="_blank" referrerpolicy="no-referrer-when-downgrade">The Thomson Reuters Trust Principles.</a></p></div></div>]]></description>
        </item>
    </channel>
</rss>