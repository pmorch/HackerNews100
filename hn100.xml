<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 06 Dec 2023 20:00:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[W3C Leaves Twitter (146 pts)]]></title>
            <link>https://w3c.social/@w3c/111534700276754588</link>
            <guid>38547203</guid>
            <pubDate>Wed, 06 Dec 2023 17:49:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://w3c.social/@w3c/111534700276754588">https://w3c.social/@w3c/111534700276754588</a>, See on <a href="https://news.ycombinator.com/item?id=38547203">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Just about every Windows/Linux device vulnerable to new LogoFAIL firmware attack (141 pts)]]></title>
            <link>https://arstechnica.com/security/2023/12/just-about-every-windows-and-linux-device-vulnerable-to-new-logofail-firmware-attack/</link>
            <guid>38545022</guid>
            <pubDate>Wed, 06 Dec 2023 15:23:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/security/2023/12/just-about-every-windows-and-linux-device-vulnerable-to-new-logofail-firmware-attack/">https://arstechnica.com/security/2023/12/just-about-every-windows-and-linux-device-vulnerable-to-new-logofail-firmware-attack/</a>, See on <a href="https://news.ycombinator.com/item?id=38545022">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/computer-power-button-800x534.jpg" alt="Just about every Windows and Linux device vulnerable to new LogoFAIL firmware attack">
      <figcaption><p>Getty Images</p></figcaption>  </figure>

  




<!-- cache miss 456:single/related:310c989e205eee0f590738e2cb404abd --><!-- empty -->
<p>Hundreds of Windows and Linux computer models from virtually all hardware makers are vulnerable to a new attack that executes malicious firmware early in the boot-up sequence, a feat that allows infections that are nearly impossible to detect or remove using current defense mechanisms.</p>
<p>The attack—dubbed LogoFAIL by the researchers who devised it—is notable for the relative ease in carrying it out, the breadth of both consumer- and enterprise-grade models that are susceptible, and the high level of control it gains over them. In many cases, LogoFAIL can be remotely executed in post-exploit situations using techniques that can’t be spotted by traditional endpoint security products. And because exploits run during the earliest stages of the boot process, they are able to bypass a host of defenses, including the industry-wide Secure Boot, Intel’s Secure Boot, and similar protections from other companies that are devised to prevent so-called bootkit infections.</p>
<h2>Game over for platform security</h2>
<p>LogoFAIL is a constellation of two dozen newly discovered vulnerabilities that have lurked for years, if not decades, in Unified Extensible Firmware Interfaces responsible for booting modern devices that run Windows or Linux. The vulnerabilities are the product of almost a year’s worth of work by Binarly, a firm that helps customers identify and secure vulnerable firmware.</p>
<p>The vulnerabilities are the subject of a coordinated mass disclosure released Wednesday. The participating companies comprise nearly the entirety of the x64 and ARM CPU ecosystem, starting with UEFI suppliers AMI, Insyde, and Phoenix (sometimes still called IBVs or independent BIOS vendors); device manufacturers such as Lenovo, Dell, and HP; and the makers of the CPUs that go inside the devices, usually Intel, AMD or designers of ARM CPUs. The researchers unveiled the attack on Wednesday at the Black Hat Security Conference in London.</p>                                            
                                                        
<p>The affected parties are releasing advisories that disclose which of their products are vulnerable and where to obtain security patches. Links to advisories and a list of vulnerability designations appears at the end of this article.</p>
<p>As its name suggests, LogoFAIL involves logos, specifically those of the hardware seller that are displayed on the device screen early in the boot process, while the UEFI is still running. Image parsers in UEFIs from all three major IBVs are riddled with roughly a dozen critical vulnerabilities that have gone unnoticed until now. By replacing the legitimate logo images with identical-looking ones that have been specially crafted to exploit these bugs, LogoFAIL makes it possible to execute malicious code at the most sensitive stage of the boot process, which is known as DXE, short for Driver Execution Environment.</p>
<p>“Once arbitrary code execution is achieved during the DXE phase, it’s game over for platform security,” researchers from Binarly, the security firm that discovered the vulnerabilities, wrote in a whitepaper. “From this stage, we have full control over the memory and the disk of the target device, thus including the operating system that will be started.”</p>
<p>From there, LogoFAIL can deliver a second-stage payload that drops an executable onto the hard drive before the main OS has even started. The following video demonstrates a proof-of-concept exploit created by the researchers. The infected device—a Gen 2 Lenovo ThinkCentre M70s running an 11th-Gen Intel Core with a UEFI released in June—runs standard firmware defenses, including Secure Boot and Intel Boot Guard.</p>
<figure><p><iframe type="text/html" width="560" height="315" src="https://www.youtube.com/embed/EufeOPe6eqk?si=Mf2uRls9-tVytbJx?start=0&amp;wmode=transparent" frameborder="0" allowfullscreen=""></iframe></p><figcaption><p>LogoFAIL.</p></figcaption></figure>
<p>In an email, Binarly founder and CEO Alex Matrosov wrote:</p>
<blockquote><p>LogoFAIL is a newly discovered set of high-impact security vulnerabilities affecting different image parsing libraries used in the system firmware by various vendors during the device boot process. These vulnerabilities are present in most cases inside reference code, impacting not a single vendor but the entire ecosystem across this code and device vendors where it is used. This attack can give a threat actor an advantage in bypassing most endpoint security solutions and delivering a stealth firmware bootkit that will persist in a firmware capsule with a modified logo image.</p></blockquote>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Enabling next-generation AI workloads: Announcing TPU v5p and AI Hypercomputer (133 pts)]]></title>
            <link>https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer</link>
            <guid>38544824</guid>
            <pubDate>Wed, 06 Dec 2023 15:10:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer">https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer</a>, See on <a href="https://news.ycombinator.com/item?id=38544824">Hacker News</a></p>
<div id="readability-page-1" class="page"><div jsname="tx2NYc"><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Generative AI (gen AI) models are rapidly evolving, offering unparalleled sophistication and capability. This advancement empowers enterprises and developers across various industries to solve complex problems and unlock new opportunities. However, the growth in gen AI models — <a href="https://cloud.google.com/blog/products/compute/announcing-cloud-tpu-v5e-and-a3-gpus-in-ga">with a tenfold increase in parameters annually over the past five years</a> — brings heightened requirements for training, tuning, and inference. Today's larger models, featuring hundreds of billions or even trillions of parameters, require extensive training periods, sometimes spanning months, even on the most specialized systems. Additionally, efficient AI workload management necessitates a coherently integrated AI stack consisting of optimized compute, storage, networking, software and development frameworks.</p><p>Today, to address these challenges, we are excited to announce Cloud TPU v5p, our most powerful, scalable, and flexible AI accelerator thus far. TPUs have long been the basis for training and serving AI-powered products like YouTube, Gmail, Google Maps, Google Play, and Android. In fact, Gemini, Google’s most capable and general AI model <a href="https://blog.google/technology/ai/google-gemini-ai" target="_blank">announced today</a>, was trained on, and is served, using TPUs.</p><p>In addition, we are also announcing AI Hypercomputer from Google Cloud, a groundbreaking supercomputer architecture that employs an integrated system of performance-optimized hardware, open software, leading ML frameworks, and flexible consumption models. Traditional methods often tackle demanding AI workloads through piecemeal, component-level enhancements, which can lead to inefficiencies and bottlenecks. In contrast, AI Hypercomputer employs systems-level codesign to boost efficiency and productivity across AI training, tuning, and serving.</p></span></section><div jsaction="rcuQ6b:npT2md" jscontroller="wJu6E" data-video-url="https://www.youtube.com/watch?v=hszd5UqnfLk"><picture><section><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/Thumbnail_-_AI_Infra_Launch_v1.max-2000x2000.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/Thumbnail_-_AI_Infra_Launch_v1.max-2000x2000.png" loading="lazy"></section></picture></div><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><h3><b>Inside Cloud TPU v5p, our most powerful and scalable TPU accelerator to date</b></h3><p>Earlier this year, <a href="https://cloud.google.com/blog/products/compute/announcing-cloud-tpu-v5e-in-ga">we announced</a> the general availability of Cloud TPU v5e. With 2.3X price performance improvements over the previous generation TPU v4<sup>1</sup>, it is our most <i>cost-efficient</i> TPU to date. By contrast, Cloud TPU v5p, is our most <i>powerful</i> TPU thus far. Each TPU v5p pod <b>composes together 8,960 chips</b> over our <b>highest-bandwidth inter-chip interconnect (ICI) at 4,800 Gbps/chip in a 3D torus topology</b>. Compared to TPU v4, TPU v5p features more than<b> 2X greater FLOPS and 3X more high-bandwidth memory (HBM)</b>.</p><p>Designed for performance, flexibility, and scale, TPU v5p can <b>train large LLM models 2.8X faster</b> than the previous-generation TPU v4. Moreover, with second-generation <a href="https://cloud.google.com/tpu">SparseCores</a>, TPU v5p can <b>train embedding-dense models 1.9X faster</b> than TPU v4<sup>2</sup>.</p></span></section><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_next-generation_AI_workloads.max-2000x2000.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_next-generation_AI_workloads.max-2000x2000.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><p><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Source: TPU v5p and v4 are based on Google Internal Data. As of November, 2023: All numbers normalized per chip seq-len=2048 for GPT-3 175 billion parameter model.</p></span></p></section><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_next-generation_AI_workloads.max-2000x2000.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_next-generation_AI_workloads.max-2000x2000.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><p><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>Source: TPU v5e data is from MLPerf™ 3.1 Training Closed results for v5e. TPU v5p and v4 are based on Google internal training runs. As of November, 2023: All numbers normalized per chip seq-len=2048 for GPT-3 175 billion parameter model. It shows relative performance per dollar using the public list price of TPU v4 ($3.22/chip/hour), TPU v5e ( $1.2/chip/hour) and TPU v5p ($4.2/chip/hour).</p></span></p></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>In addition to performance improvements, <b>TPU v5p is also 4X more scalable than TPU v4 in terms of total available FLOPs per pod.</b> Doubling the floating-point operations per second (FLOPS) over TPU v4 and doubling the number of chips in a single pod provides considerable improvement in relative performance in training speed.</p></span></section><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/3_next-generation_AI_workloads_v1.max-2000x2000.jpg" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/3_next-generation_AI_workloads_v1.max-2000x2000.jpg" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><h3><b>Google AI Hypercomputer delivers peak performance and efficiency at large scale</b></h3><p>Achieving both scale and speed is necessary, but not sufficient to meet the needs of modern AI/ML applications and services. The hardware and software components must come together into an integrated, easy-to-use, secure, and reliable computing system. At Google, we’ve done decades of research and development on this very problem, culminating in AI Hypercomputer, a system of technologies optimized to work in concert to enable modern AI workloads.</p></span></section><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/4_next-generation_AI_workloads.max-800x800.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/4_next-generation_AI_workloads.max-800x800.png" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure></section><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><ul><li><b>Performance-optimized hardware:</b> AI Hypercomputer features performance-optimized compute, storage, and networking built over an ultrascale data center infrastructure, leveraging a high-density footprint, liquid cooling, and our <a href="https://cloud.google.com/blog/topics/systems/the-evolution-of-googles-jupiter-data-center-network">Jupiter data center network</a> technology. All of this is predicated on technologies that are built with <a href="https://www.google.com/about/datacenters/efficiency/" target="_blank">efficiency</a> at their core; leveraging <a href="https://cloud.google.com/blog/topics/sustainability/a-smarter-way-to-buy-clean-energy">clean energy</a> and <a href="https://blog.google/outreach-initiatives/sustainability/replenishing-water/?_ga=2.140272307.1460901017.1631498684-1474825438.1628277680" target="_blank">a deep commitment to water stewardship</a>, and that are <a href="https://blog.google/outreach-initiatives/sustainability/our-third-decade-climate-action-realizing-carbon-free-future/" target="_blank">helping us move toward a carbon-free future</a>.</li><li><b>Open software:</b> AI Hypercomputer enables developers to access our performance-optimized hardware through the use of open software to tune, manage, and dynamically orchestrate AI training and inference workloads on top of performance-optimized AI hardware.<ul><li>Extensive support for popular ML frameworks such as JAX, TensorFlow, and PyTorch are available right out of the box. Both JAX and PyTorch are powered by <a href="https://github.com/openxla/xla" target="_blank">OpenXLA</a> compiler for building sophisticated LLMs. XLA serves as a foundational backbone, enabling the creation of complex multi-layered models (<a href="https://pytorch.org/blog/high-performance-llama-2/" target="_blank">Llama 2 training and inference on Cloud TPUs with PyTorch/XLA</a>). It optimizes distributed architectures across a wide range of hardware platforms, ensuring easy-to-use and efficient model development for diverse AI use cases (<a href="https://cloud.google.com/blog/products/compute/assemblyai-on-cloud-tpu-v5e-price-performance">AssemblyAI leverages JAX/XLA and Cloud TPUs for large-scale AI speech</a>).</li><li>Open and unique <a href="https://cloud.google.com/blog/products/compute/using-cloud-tpu-multislice-to-scale-ai-workloads">Multislice Training</a> and <a href="https://cloud.google.com/tpu/docs/v5e-inference">Multihost Inferencing</a> software, respectively, make scaling, training, and serving workloads smooth and easy. Developers can scale to tens of thousands of chips to support demanding AI workloads.</li><li>Deep integration with <a href="https://cloud.google.com/kubernetes-engine?hl=en">Google Kubernetes Engine (GKE)</a> and <a href="https://cloud.google.com/compute?hl=en">Google Compute Engine</a>, to deliver efficient resource management, consistent ops environments, autoscaling, node-pool auto-provisioning, auto-checkpointing, auto-resumption, and timely failure recovery.</li></ul></li><li><b>Flexible consumption</b>: AI Hypercomputer offers a wide range of flexible and dynamic consumption choices. In addition to classic options, such as Committed Use Discounts (CUD), on-demand pricing, and spot pricing, AI Hypercomputer provides consumption models tailored for AI workloads via <a href="https://cloud.google.com/blog/products/compute/introducing-dynamic-workload-scheduler">Dynamic Workload Scheduler.</a> Dynamic Workload Scheduler introduces two models: Flex Start mode for higher resource obtainability and optimized economics, as well as Calendar mode, which targets workloads with higher predictability on job-start times.</li></ul><h3><b>Leveraging Google’s deep experience to help power the future of AI</b></h3><p>Customers like Salesforce and Lightricks are already training and serving large AI models with Google Cloud’s TPU v5p AI Hypercomputer — and already seeing a difference:</p><p><i>“We’ve been leveraging Google Cloud TPU v5p for pre-training Salesforce’s foundational models that will serve as the core engine for specialized production use cases, and we’re seeing considerable improvements in our training speed. In fact, Cloud TPU v5p compute outperforms the previous generation TPU v4 by as much as 2X. We also love how seamless and easy the transition has been from Cloud TPU v4 to v5p using JAX. We’re excited to take these speed gains even further by leveraging the native support for INT8 precision format via the Accurate Quantized Training (AQT) library to optimize our models.” -</i> Erik Nijkamp, Senior Research Scientist, Salesforce</p><p><i>“Leveraging the remarkable performance and ample memory capacity of Google Cloud TPU v5p, we successfully trained our generative text-to-video model without splitting it into separate processes. This optimal hardware utilization significantly accelerates each training cycle, allowing us to swiftly conduct a series of experiments. The ability to train our model quickly in each experiment facilitates rapid iteration, which is an invaluable advantage for our research team in this competitive field of generative AI.”</i> - Yoav HaCohen, PhD, Core Generative AI Research Team Lead, Lightricks</p><p><i>“In our early-stage usage, Google DeepMind and Google Research have observed 2X speedups for LLM training workloads using TPU v5p chips compared to the performance on our TPU v4 generation. The robust support for ML Frameworks (JAX, PyTorch, TensorFlow) and orchestration tools enables us to scale even more efficiently on v5p. With the 2nd generation of SparseCores we also see significant improvement in the performance of embeddings-heavy workloads. TPUs are vital to enabling our largest-scale research and engineering efforts on cutting edge models like Gemini.” -</i> Jeff Dean, Chief Scientist, Google DeepMind and Google Research</p><p>At Google, we’ve long believed in the power of AI to help solve challenging problems. Until very recently, training large foundation models and serving them at scale was too complicated and expensive for many organizations. Today, with Cloud TPU v5p and AI Hypercomputer, we’re excited to extend the result of decades of research in AI and systems design with our customers, so they can innovate with AI faster, more efficiently, and more cost effectively.</p><p>To request access to Cloud TPU v5p and AI Hypercomputer, please reach out to your <a href="https://cloud.google.com/contact/">Google Cloud account manager</a>. To learn more about Google Cloud’s AI infrastructure, <a href="https://cloudonair.withgoogle.com/events/summit-applied-ml-summit-23" target="_blank">register to attend Google Cloud Applied AI Summit</a>.</p><hr><p><i><sup>1: MLPerf™ v3.1 Training Closed, multiple benchmarks as shown. Retrieved November 8th, 2023 from</sup></i> <a href="http://mlcommons.org/" target="_blank"><i><sup>mlcommons.org</sup></i></a><i><sup>. Results 3.1-2004. Performance per dollar is not an MLPerf metric. TPU v4 results are unverified: not verified by MLCommons Association. The MLPerf™ name and logo are trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See</sup></i> <a href="http://www.mlcommons.org/" target="_blank"><i><sup>www.mlcommons.org</sup></i></a> <i><sup>for more information.<br>2: Google Internal Data for TPU v5p as of November, 2023: E2E steptime, SearchAds pCTR, batch size per TPU core 16,384, 125 vp5 chips</sup></i></p></span></section><section><span>Posted in</span><ul><li><a href="https://cloud.google.com/blog/products/ai-machine-learning" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/ai-machine-learning" track-metadata-module="tag list" track-metadata-module_headline="posted in">AI &amp; Machine Learning</a></li><li><a href="https://cloud.google.com/blog/products/compute" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/compute" track-metadata-module="tag list" track-metadata-module_headline="posted in">Compute</a></li><li><a href="https://cloud.google.com/blog/products/infrastructure-modernization" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/infrastructure-modernization" track-metadata-module="tag list" track-metadata-module_headline="posted in">Infrastructure Modernization</a></li><li><a href="https://cloud.google.com/blog/topics/systems" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/topics/systems" track-metadata-module="tag list" track-metadata-module_headline="posted in">Systems</a></li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini: Google's most capable AI model yet (759 pts)]]></title>
            <link>https://blog.google/technology/ai/google-gemini-ai/</link>
            <guid>38544746</guid>
            <pubDate>Wed, 06 Dec 2023 15:05:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/ai/google-gemini-ai/">https://blog.google/technology/ai/google-gemini-ai/</a>, See on <a href="https://news.ycombinator.com/item?id=38544746">Hacker News</a></p>
<div id="readability-page-1" class="page"><article ng-init="drawerToggle = {'open': true}">

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
  }">
      
      
        <p>
          Making AI more helpful for everyone
        </p>
      
    </div>

    

    
      







<div>
    <figure>
      <div>
  <p><img srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_header.width-600.format-webp.webp 600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_header.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_header.width-1600.format-webp.webp 1600w" sizes="(max-width: 599px) 100vw, (max-width: 1023px) 600px, 1024px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_header.width-1200.format-webp.webp" fetchpriority="high" alt="The word “Gemini” above five separate threads, each a different color, converge from the left into a three-dimensional central helix before separating back out toward the right into five individual strands once more.">
  </p>
</div>

      
    </figure>
  </div>


    

    
    <div>
        
          
            <div data-component="uni-article-jumplinks" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Jumplinks&quot;,
    &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
  }">
  <nav aria-label="Article Jumplinks">
    <p><span>In this story</span>
    </p>
    
    
    <div>
      <ul id="article-jumplinks__list">
        
        <li>
          <a aria-label="link to Note from Sundar" href="#sundar-note" id="sundar-note-anchor">Note from Sundar</a>
        </li>
        
        <li>
          <a aria-label="link to Introducing Gemini" href="#introducing-gemini" id="introducing-gemini-anchor">Introducing Gemini</a>
        </li>
        
        <li>
          <a aria-label="link to State-of-the-art performance" href="#performance" id="performance-anchor">State-of-the-art performance</a>
        </li>
        
        <li>
          <a aria-label="link to Next-generation capabilities" href="#capabilities" id="capabilities-anchor">Next-generation capabilities</a>
        </li>
        
        <li>
          <a aria-label="link to Scalable and efficient" href="#scalable-efficient" id="scalable-efficient-anchor">Scalable and efficient</a>
        </li>
        
        <li>
          <a aria-label="link to Responsibility and safety" href="#responsibility-safety" id="responsibility-safety-anchor">Responsibility and safety</a>
        </li>
        
        <li>
          <a aria-label="link to Availability" href="#availability" id="availability-anchor">Availability</a>
        </li>
        
      </ul>
    </div>
    
  </nav>
</div>
          
          
          <div data-reading-time="true" data-component="uni-drop-cap|uni-tombstone">
            
            
<!--article text-->

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><p data-block-key="xdxwr"><i>A note from Google and Alphabet CEO Sundar Pichai:</i></p><p data-block-key="6590u">Every technology shift is an opportunity to advance scientific discovery, accelerate human progress, and improve lives. I believe the transition we are seeing right now with AI will be the most profound in our lifetimes, far bigger than the shift to mobile or to the web before it. AI has the potential to create opportunities — from the everyday to the extraordinary — for people everywhere. It will bring new waves of innovation and economic progress and drive knowledge, learning, creativity and productivity on a scale we haven’t seen before.</p><p data-block-key="3ffsc">That’s what excites me: the chance to make AI helpful for everyone, everywhere in the world.</p><p data-block-key="6sa5">Nearly eight years into our journey as an AI-first company, the pace of progress is only accelerating: Millions of people are now using generative AI across our products to do things they couldn’t even a year ago, from finding answers to more complex questions to using new tools to collaborate and create. At the same time, developers are using our models and infrastructure to build new generative AI applications, and startups and enterprises around the world are growing with our AI tools.</p><p data-block-key="fafvp">This is incredible momentum, and yet, we’re only beginning to scratch the surface of what’s possible.</p><p data-block-key="chghs">We’re approaching this work boldly and responsibly. That means being ambitious in our research and pursuing the capabilities that will bring enormous benefits to people and society, while building in safeguards and working collaboratively with governments and experts to address risks as AI becomes more capable. And we continue to invest in the very best tools, foundation models and infrastructure and bring them to our products and to others, guided by our <a href="https://ai.google/responsibility/principles/" rt-link-type="external">AI Principles</a>.</p><p data-block-key="akdk1">Now, we’re taking the next step on our journey with Gemini, our most capable and general model yet, with state-of-the-art performance across many leading benchmarks. Our first version, Gemini 1.0, is optimized for different sizes: Ultra, Pro and Nano. These are the first models of the Gemini era and the first realization of the vision we had when we formed Google DeepMind earlier this year. This new era of models represents one of the biggest science and engineering efforts we’ve undertaken as a company. I’m genuinely excited for what’s ahead, and for the opportunities Gemini will unlock for people everywhere.</p><p data-block-key="87ult">– Sundar</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="ede3f">Introducing Gemini</h2><p data-block-key="copss"><i>By Demis Hassabis, CEO and Co-Founder of Google DeepMind, on behalf of the Gemini team</i></p><p data-block-key="ff9rj">AI has been the focus of my life's work, as for many of my research colleagues. Ever since programming AI for computer games as a teenager, and throughout my years as a neuroscience researcher trying to understand the workings of the brain, I’ve always believed that if we could build smarter machines, we could harness them to benefit humanity in incredible ways.</p><p data-block-key="i7i7">This promise of a world responsibly empowered by AI continues to drive our work at Google DeepMind. For a long time, we’ve wanted to build a new generation of AI models, inspired by the way people understand and interact with the world. AI that feels less like a smart piece of software and more like something useful and intuitive — an expert helper or assistant.</p><p data-block-key="98mu4">Today, we’re a step closer to this vision as <a href="https://deepmind.google/technologies/gemini" rt-link-type="external">we introduce Gemini</a>, the most capable and general model we’ve ever built.</p><p data-block-key="aka6e">Gemini is the result of large-scale collaborative efforts by teams across Google, including our colleagues at Google Research. It was built from the ground up to be multimodal, which means it can generalize and seamlessly understand, operate across and combine different types of information including text, code, audio, image and video.</p></div>
  

  
    
  
    


<div data-component="uni-article-yt-player" data-page-title="Introducing Gemini: our largest and most capable AI model" data-video-id="jV1vkHv4zq8" data-index-id="5" data-analytics-module="{
    &quot;module_name&quot;: &quot;Youtube Video&quot;,
    &quot;section_header&quot;: &quot;undefined&quot;
  }">

    

    <a role="video" tabindex="0">
      <div>
        
          
          
          
          <p><img alt="Introducing Gemini: our largest and most capable AI model." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/06_Foundation_01.width-100.format-webp.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/06_Foundation_01.width-500.format-webp.webp&quot;,
                &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/06_Foundation_01.width-1000.format-webp.webp&quot;
              }"></p>

        
        <svg role="presentation">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button_no_hole"></use>
          
        </svg>
        <svg role="img">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button"></use>
          
        </svg>

        
        
        
        
      </div>
    </a>

    
      <p>Introducing Gemini: our largest and most capable AI model</p>
    

    
  </div>

  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><p data-block-key="39bmn">Gemini is also our most flexible model yet — able to efficiently run on everything from data centers to mobile devices. Its state-of-the-art capabilities will significantly enhance the way developers and enterprise customers build and scale with AI.</p><p data-block-key="8j5hi">We’ve optimized Gemini 1.0, our first version, for three different sizes:</p><ul><li data-block-key="103ti"><b>Gemini Ultra</b> — our largest and most capable model for highly complex tasks.</li><li data-block-key="498p0"><b>Gemini Pro</b> — our best model for scaling across a wide range of tasks.</li><li data-block-key="2sl86"><b>Gemini Nano</b> — our most efficient model for on-device tasks.</li></ul></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="4rd18">State-of-the-art performance</h2><p data-block-key="7ormq">We've been rigorously testing our Gemini models and evaluating their performance on a wide variety of tasks. From natural image, audio and video understanding to mathematical reasoning, Gemini Ultra’s performance exceeds current state-of-the-art results on 30 of the 32 widely-used academic benchmarks used in large language model (LLM) research and development.</p><p data-block-key="61bg2">With a score of 90.0%, Gemini Ultra is the first model to outperform human experts on <a href="https://arxiv.org/abs/2009.03300" rt-link-type="external">MMLU</a> (massive multitask language understanding), which uses a combination of 57 subjects such as math, physics, history, law, medicine and ethics for testing both world knowledge and problem-solving abilities.</p><p data-block-key="at1df">Our new benchmark approach to MMLU enables Gemini to use its reasoning capabilities to think more carefully before answering difficult questions, leading to significant improvements over just using its first impression.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
        }">
  

  <p><img alt="A chart showing Gemini Ultra’s performance on common text benchmarks, compared to GPT-4 (API numbers calculated where reported numbers were missing)." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_final_text_table_bigger_font_amendment_lines.gif">
        
      
    
    </p>
    
      <figcaption><p data-block-key="a6ort">Gemini surpasses state-of-the-art performance on a range of benchmarks including text and coding.</p></figcaption>
    
  
    </div>
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><p data-block-key="39bmn">Gemini Ultra also achieves a state-of-the-art score of 59.4% on the new <a href="https://arxiv.org/abs/2311.16502" rt-link-type="external">MMMU</a> benchmark, which consists of multimodal tasks spanning different domains requiring deliberate reasoning.</p><p data-block-key="2pfr3">With the image benchmarks we tested, Gemini Ultra outperformed previous state-of-the-art models, without assistance from object character recognition (OCR) systems that extract text from images for further processing. These benchmarks highlight Gemini’s native multimodality and indicate early signs of Gemini's more complex reasoning abilities.</p><p data-block-key="cihqv">See more details in our <a href="https://goo.gle/GeminiPaper" rt-link-type="external">Gemini technical report</a>.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
        }">
  

  <p><img alt="A chart showing Gemini Ultra’s performance on multimodal benchmarks compared to GPT-4V, with previous SOTA models listed in places where capabilities are not supported in GPT-4V." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_final_multimodal_table_bigger_font_amendment_lines.gif">
        
      
    
    </p>
    
      <figcaption><p data-block-key="a6ort">Gemini surpasses state-of-the-art performance on a range of multimodal benchmarks.</p></figcaption>
    
  
    </div>
  



  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="4rd18">Next-generation capabilities</h2><p data-block-key="90lfq">Until now, the standard approach to creating multimodal models involved training separate components for different modalities and then stitching them together to roughly mimic some of this functionality. These models can sometimes be good at performing certain tasks, like describing images, but struggle with more conceptual and complex reasoning.</p><p data-block-key="c2vjn">We designed Gemini to be natively multimodal, pre-trained from the start on different modalities. Then we fine-tuned it with additional multimodal data to further refine its effectiveness. This helps Gemini seamlessly understand and reason about all kinds of inputs from the ground up, far better than existing multimodal models — and its capabilities are stateof the art in nearly every domain.</p><p data-block-key="9ad54">Learn more about <a href="https://deepmind.google/technologies/gemini" rt-link-type="external">Gemini’s capabilities and see how it works</a>.</p><h3 data-block-key="4mlv7">Sophisticated reasoning</h3><p data-block-key="1s4j6">Gemini 1.0’s sophisticated multimodal reasoning capabilities can help make sense of complex written and visual information. This makes it uniquely skilled at uncovering knowledge that can be difficult to discern amid vast amounts of data.</p><p data-block-key="3lp95">Its remarkable ability to extract insights from hundreds of thousands of documents through reading, filtering and understanding information will help deliver new breakthroughs at digital speeds in many fields from science to finance.</p></div>
  

  
    
  
    


<div data-component="uni-article-yt-player" data-page-title="Introducing Gemini: our largest and most capable AI model" data-video-id="sPiOP_CB54A" data-index-id="14" data-analytics-module="{
    &quot;module_name&quot;: &quot;Youtube Video&quot;,
    &quot;section_header&quot;: &quot;undefined&quot;
  }">

    

    <a role="video" tabindex="0">
      <div>
        
          
          
          
          <p><img alt="Gemini unlocks new scientific insights." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ScienceDemo_TaylorSebastian.width-100.format-webp.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ScienceDemo_TaylorSebastian.width-500.format-webp.webp&quot;,
                &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ScienceDemo_TaylorSebastia.width-1000.format-webp.webp&quot;
              }"></p>

        
        <svg role="presentation">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button_no_hole"></use>
          
        </svg>
        <svg role="img">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button"></use>
          
        </svg>

        
        
        
        
      </div>
    </a>

    
      <p>Gemini unlocks new scientific insights</p>
    

    
  </div>

  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h3 data-block-key="39bmn">Understanding text, images, audio and more</h3><p data-block-key="acvam">Gemini 1.0 was trained to recognize and understand text, images, audio and more at the same time, so it better understands nuanced information and can answer questions relating to complicated topics. This makes it especially good at explaining reasoning in complex subjects like math and physics.</p></div>
  

  
    
  
    


<div data-component="uni-article-yt-player" data-page-title="Introducing Gemini: our largest and most capable AI model" data-video-id="K4pX1VAxaAI" data-index-id="16" data-analytics-module="{
    &quot;module_name&quot;: &quot;Youtube Video&quot;,
    &quot;section_header&quot;: &quot;undefined&quot;
  }">

    

    <a role="video" tabindex="0">
      <div>
        
          
          
          
          <p><img alt="Gemini explains reasoning in math and physics." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_PhysicsHomework_Sam.width-100.format-webp.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_PhysicsHomework_Sam.width-500.format-webp.webp&quot;,
                &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_PhysicsHomework_Sam.width-1000.format-webp.webp&quot;
              }"></p>

        
        <svg role="presentation">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button_no_hole"></use>
          
        </svg>
        <svg role="img">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button"></use>
          
        </svg>

        
        
        
        
      </div>
    </a>

    
      <p>Gemini explains reasoning in math and physics</p>
    

    
  </div>

  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h3 data-block-key="39bmn">Advanced coding</h3><p data-block-key="f2037">Our first version of Gemini can understand, explain and generate high-quality code in the world’s most popular programming languages, like Python, Java, C++, and Go. Its ability to work across languages and reason about complex information makes it one of the leading foundation models for coding in the world.</p><p data-block-key="6rsks">Gemini Ultra excels in several coding benchmarks, including <a href="https://arxiv.org/abs/2107.03374" rt-link-type="external">HumanEval</a>, an important industry-standard for evaluating performance on coding tasks, and Natural2Code, our internal held-out dataset, which uses author-generated sources instead of web-based information.</p><p data-block-key="38e8l">Gemini can also be used as the engine for more advanced coding systems. Two years ago we presented <a href="https://deepmind.google/discover/blog/competitive-programming-with-alphacode/" rt-link-type="external">AlphaCode</a>, the first AI code generation system to reach a competitive level of performance in programming competitions.</p><p data-block-key="915ut">Using a specialized version of Gemini, we created a more advanced code generation system, <a href="https://goo.gle/AlphaCode2" rt-link-type="external">AlphaCode 2</a>, which excels at solving competitive programming problems that go beyond coding to involve complex math and theoretical computer science.</p></div>
  

  
    
  
    


<div data-component="uni-article-yt-player" data-page-title="Introducing Gemini: our largest and most capable AI model" data-video-id="LvGmVmHv69s" data-index-id="18" data-analytics-module="{
    &quot;module_name&quot;: &quot;Youtube Video&quot;,
    &quot;section_header&quot;: &quot;undefined&quot;
  }">

    

    <a role="video" tabindex="0">
      <div>
        
          
          
          
          <p><img alt="Gemini excels at coding and competitive programming." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ACDemo_RemiGabi_v001.width-100.format-webp.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ACDemo_RemiGabi_v001.width-500.format-webp.webp&quot;,
                &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Ariel_ACDemo_RemiGabi_v001.width-1000.format-webp.webp&quot;
              }"></p>

        
        <svg role="presentation">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button_no_hole"></use>
          
        </svg>
        <svg role="img">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20231128-1813#yt_video_play_button"></use>
          
        </svg>

        
        
        
        
      </div>
    </a>

    
      <p>Gemini excels at coding and competitive programming</p>
    

    
  </div>

  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><p data-block-key="39bmn">When evaluated on the same platform as the original AlphaCode, AlphaCode 2 shows massive improvements, solving nearly twice as many problems, and we estimate that it performs better than 85% of competition participants — up from nearly 50% for AlphaCode. When programmers collaborate with AlphaCode 2 by defining certain properties for the code samples to follow, it performs even better.</p><p data-block-key="brofo">We’re excited for programmers to increasingly use highly capable AI models as collaborative tools that can help them reason about the problems, propose code designs and assist with implementation — so they can release apps and design better services, faster.</p><p data-block-key="3jkkn">See more details in our <a href="https://goo.gle/AlphaCode2" rt-link-type="external">AlphaCode 2 technical report</a>.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="4rd18">More reliable, scalable and efficient</h2><p data-block-key="85utf">We trained Gemini 1.0 at scale on our AI-optimized infrastructure using Google’s in-house designed <a href="https://cloud.google.com/tpu?hl=en" rt-link-type="external">Tensor Processing Units</a> (TPUs) v4 and v5e. And we designed it to be our most reliable and scalable model to train, and our most efficient to serve.</p><p data-block-key="5ujmc">On TPUs, Gemini runs significantly faster than earlier, smaller and less-capable models. These custom-designed AI accelerators have been at the heart of Google's AI-powered products that serve billions of users like Search, YouTube, Gmail, Google Maps, Google Play and Android. They’ve also enabled companies around the world to train large-scale AI models cost-efficiently.</p><p data-block-key="6kgb1">Today, we’re announcing the most powerful, efficient and scalable TPU system to date, <a href="https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-tpu-v5p-and-ai-hypercomputer" rt-link-type="external">Cloud TPU v5p</a>, designed for training cutting-edge AI models. This next generation TPU will accelerate Gemini’s development and help developers and enterprise customers train large-scale generative AI models faster, allowing new products and capabilities to reach customers sooner.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
        }">
  

  <p><img alt="A row of Cloud TPU v5p AI accelerator supercomputers in a Google data center." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_tpu.width-100.format-webp.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_tpu.width-500.format-webp.webp&quot;,
                &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/final_keyword_tpu.width-1000.format-webp.webp&quot;
              }">
        
      
    
    </p>
    
      <figcaption><p data-block-key="a6ort">A row of Cloud TPU v5p AI accelerator supercomputers in a Google data center.</p></figcaption>
    
  
    </div>
  



  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="dapif">Built with responsibility and safety at the core</h2><p data-block-key="6pvob">At Google, we’re committed to advancing bold and responsible AI in everything we do. Building upon Google’s <a href="https://ai.google/responsibility/principles/" rt-link-type="external">AI Principles</a> and the robust safety policies across our products, we’re adding new protections to account for Gemini’s multimodal capabilities. At each stage of development, we’re considering potential risks and working to test and mitigate them.</p><p data-block-key="4rc0o">Gemini has the most comprehensive safety evaluations of any Google AI model to date, including for bias and toxicity. We’ve conducted <a href="https://deepmind.google/discover/blog/an-early-warning-system-for-novel-ai-risks/" rt-link-type="external">novel research into potential risk areas</a> like cyber-offense, persuasion and autonomy, and have applied Google Research’s best-in-class <a href="https://blog.research.google/2023/11/responsible-ai-at-google-research_16.html" rt-link-type="external">adversarial testing techniques</a> to help identify critical safety issues in advance of Gemini’s deployment.</p><p data-block-key="1tti2">To identify blindspots in our internal evaluation approach, we’re working with a diverse group of external experts and partners to stress-test our models across a range of issues.</p><p data-block-key="9hpod">To diagnose content safety issues during Gemini’s training phases and ensure its output follows our policies, we’re using benchmarks such as <a href="https://allenai.org/data/real-toxicity-prompts" rt-link-type="external">Real Toxicity Prompts</a>, a set of 100,000 prompts with varying degrees of toxicity pulled from the web, developed by experts at the Allen Institute for AI. Further details on this work are coming soon.</p><p data-block-key="1ovh">To limit harm, we built dedicated safety classifiers to identify, label and sort out content involving violence or negative stereotypes, for example. Combined with robust filters, this layered approach is designed to make Gemini safer and more inclusive for everyone. Additionally, we’re continuing to address known challenges for models such as factuality, grounding, attribution and corroboration.</p><p data-block-key="6cdjb">Responsibility and safety will always be central to the development and deployment of our models. This is a long-term commitment that requires building collaboratively, so we’re partnering with the industry and broader ecosystem on defining best practices and setting safety and security benchmarks through organizations like <a href="https://mlcommons.org/" rt-link-type="external">MLCommons</a>, the <a href="https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/" rt-link-type="external">Frontier Model Forum</a> <a href="https://blog.google/outreach-initiatives/public-policy/google-microsoft-openai-anthropic-frontier-model-forum/" rt-link-type="external">and</a> its <a href="https://blog.google/outreach-initiatives/public-policy/google-microsoft-anthropic-open-ai-frontier-model-forum-executive-director/" rt-link-type="external">AI Safety Fund</a>, and our <a href="https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/" rt-link-type="external">Secure AI Framework (SAIF)</a>, which was designed to help mitigate security risks specific to AI systems across the public and private sectors. We’ll continue partnering with researchers, governments and civil society groups around the world as we develop Gemini.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="4rd18">Making Gemini available to the world</h2><p data-block-key="5l85j">Gemini 1.0 is now rolling out across a range of products and platforms:<br></p><h3 data-block-key="a245r">Gemini Pro in Google products</h3><p data-block-key="e5h3k">We’re bringing Gemini to billions of people through Google products.</p><p data-block-key="dp2ls">Starting today, <a href="https://blog.google/products/bard/google-bard-try-gemini-ai" rt-link-type="external">Bard will use a fine-tuned version of Gemini Pro</a> for more advanced reasoning, planning, understanding and more. This is the biggest upgrade to Bard since it launched. It will be available in English in more than 170 countries and territories, and we plan to expand to different modalities and support new languages and locations in the near future.</p><p data-block-key="5erfe">We’re also <a href="https://blog.google/products/pixel/pixel-feature-drop-december-2023/" rt-link-type="internal">bringing Gemini to Pixel</a>. Pixel 8 Pro is the first smartphone engineered to run Gemini Nano, which is powering new features like Summarize in the Recorder app and rolling out in Smart Reply in Gboard, starting with WhatsApp — with more messaging apps coming next year.</p><p data-block-key="3ah01">In the coming months, Gemini will be available in more of our products and services like Search, Ads, Chrome and Duet AI.</p><p data-block-key="7s7gn">We’re already starting to experiment with Gemini in Search, where it's making our <a href="https://labs.google/sge/" rt-link-type="external">Search Generative Experience</a> (SGE) faster for users, with a 40% reduction in latency in English in the U.S., alongside improvements in quality.</p><h3 data-block-key="fpjq5">Building with Gemini</h3><p data-block-key="3e6sk">Starting on December 13, developers and enterprise customers can access Gemini Pro via the Gemini API in Google AI Studio or <a href="https://cloud.google.com/vertex-ai" rt-link-type="external">Google Cloud Vertex AI</a>.</p><p data-block-key="51fbs">Google AI Studio is a free, web-based developer tool to prototype and launch apps quickly with an API key. When it's time for a fully-managed AI platform, Vertex AI allows customization of Gemini with full data control and benefits from additional Google Cloud features for enterprise security, safety, privacy and data governance and compliance.</p><p data-block-key="5lr43">Android developers will also be able to build with Gemini Nano, our most efficient model for on-device tasks, via AICore, a new system capability available in Android 14, starting on Pixel 8 Pro devices. Sign up for an <a href="https://android-developers.googleblog.com/2023/12/a-new-foundation-for-ai-on-android.html" rt-link-type="external">early preview of AICore</a>.</p><h3 data-block-key="a0kru">Gemini Ultra coming soon</h3><p data-block-key="hhfg">For Gemini Ultra, we’re currently completing extensive trust and safety checks, including red-teaming by trusted external parties, and further refining the model using fine-tuning and reinforcement learning from human feedback (RLHF) before making it broadly available.</p><p data-block-key="aubeq">As part of this process, we’ll make Gemini Ultra available to select customers, developers, partners and safety and responsibility experts for early experimentation and feedback before rolling it out to developers and enterprise customers early next year.</p><p data-block-key="22un0">Early next year, we’ll also launch <a href="https://blog.google/products/bard/google-bard-try-gemini-ai" rt-link-type="external">Bard Advanced</a>, a new, cutting-edge AI experience that gives you access to our best models and capabilities, starting with Gemini Ultra.</p></div>
  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini: our largest and most capable AI model&quot;
         }"><h2 data-block-key="4rd18">The Gemini era: enabling a future of innovation</h2><p data-block-key="ausk2">This is a significant milestone in the development of AI, and the start of a new era for us at Google as we continue to rapidly innovate and responsibly advance the capabilities of our models.</p><p data-block-key="936sj">We’ve made great progress on Gemini so far and we’re working hard to further extend its capabilities for future versions, including advances in planning and memory, and increasing the context window for processing even more information to give better responses.</p><p data-block-key="2i1b7">We’re excited by the amazing possibilities of a world responsibly empowered by AI — a future of innovation that will enhance creativity, extend knowledge, advance science and transform the way billions of people live and work around the world.</p></div>
  


            
            

            
              




            
          </div>
        
      </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini – Google DeepMind (707 pts)]]></title>
            <link>https://deepmind.google/technologies/gemini/</link>
            <guid>38544729</guid>
            <pubDate>Wed, 06 Dec 2023 15:03:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/technologies/gemini/">https://deepmind.google/technologies/gemini/</a>, See on <a href="https://news.ycombinator.com/item?id=38544729">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><tr><th><p>Image</p></th><td></td><td></td><td scope="row"><p>MMMU<span>Multi-discipline college-level reasoning problems</span></p></td><td><p>Multi-discipline college-level reasoning problems</p></td><td><p><span data-no-percent="false">59.4%</span><span>0-shot pass@1 <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">56.8%</span><span>0-shot pass@1 <br> GPT-4V</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>VQAv2<span>Natural image understanding</span></p></td><td><p>Natural image understanding</p></td><td><p><span data-no-percent="false">77.8%</span><span>0-shot <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">77.2%</span><span>0-shot <br> GPT-4V</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>TextVQA<span>OCR on natural images</span></p></td><td><p>OCR on natural images</p></td><td><p><span data-no-percent="false">82.3%</span><span>0-shot <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">78%</span><span>0-shot <br> GPT-4V</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>DocVQA<span>Document understanding</span></p></td><td><p>Document understanding</p></td><td><p><span data-no-percent="false">90.9%</span><span>0-shot <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">88.4%</span><span>0-shot <br> GPT-4V (pixel only)</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>Infographic VQA<span>Infographic understanding</span></p></td><td><p>Infographic understanding</p></td><td><p><span data-no-percent="false">80.3%</span><span>0-shot <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">75.1%</span><span>0-shot <br> GPT-4V (pixel only)</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>MathVista<span>Mathematical reasoning in visual contexts</span></p></td><td><p>Mathematical reasoning in visual contexts</p></td><td><p><span data-no-percent="false">53%</span><span>0-shot <br> Gemini Ultra (pixel only*)</span></p></td><td><p><span data-no-percent="false">49.9%</span><span>0-shot <br> GPT-4V</span></p></td></tr><tr><th><p>Video</p></th><td></td><td></td><td scope="row"><p>VATEX<span>English video captioning <br>(CIDEr)</span></p></td><td><p>English video captioning <br>(CIDEr)</p></td><td><p><span data-no-percent="true">62.7</span><span>4-shot <br> Gemini Ultra</span></p></td><td><p><span data-no-percent="true">56</span><span>4-shot <br> DeepMind Flamingo</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>Perception Test MCQA<span>Video question answering</span></p></td><td><p>Video question answering</p></td><td><p><span data-no-percent="false">54.7%</span><span>0-shot <br> Gemini Ultra</span></p></td><td><p><span data-no-percent="false">46.3%</span><span>0-shot <br> SeViLA</span></p></td></tr><tr><th><p>Audio</p></th><td></td><td></td><td scope="row"><p>CoVoST 2 (21 languages)<span>Automatic speech translation <br>(BLUE score)</span></p></td><td><p>Automatic speech translation <br>(BLUE score)</p></td><td><p><span data-no-percent="true">40.1</span><span>Gemini Pro</span></p></td><td><p><span data-no-percent="true">29.1</span><span>Whisper v2</span></p></td></tr><tr><th></th><td></td><td></td><td scope="row"><p>FLEURS (62 languages)<span>Automatic speech recognition <br>(based on word error rate, lower is better)</span></p></td><td><p>Automatic speech recognition <br>(based on word error rate, lower is better)</p></td><td><p><span data-no-percent="false">7.6%</span><span>Gemini Pro</span></p></td><td><p><span data-no-percent="false">17.6%</span><span>Whisper v3</span></p></td></tr></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Confirms Governments Using Push Notifications to Surveil Users (535 pts)]]></title>
            <link>https://www.macrumors.com/2023/12/06/apple-governments-surveil-push-notifications/</link>
            <guid>38543587</guid>
            <pubDate>Wed, 06 Dec 2023 13:29:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2023/12/06/apple-governments-surveil-push-notifications/">https://www.macrumors.com/2023/12/06/apple-governments-surveil-push-notifications/</a>, See on <a href="https://news.ycombinator.com/item?id=38543587">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2023/12/06/apple-governments-surveil-push-notifications/"><p>Unidentified governments are surveilling smartphone users by tracking push notifications that move through Google's and Apple's servers, a US senator warned on Wednesday (via <em><a href="https://www.reuters.com/technology/cybersecurity/governments-spying-apple-google-users-through-push-notifications-us-senator-2023-12-06/">Reuters</a></em>).</p>
<p><img src="https://images.macrumors.com/t/JdCJ-AJ3Ort9AETOl6rhY0h-yKg=/400x0/article-new/2023/02/iOS-16-4-Web-Push.jpeg?lossy" srcset="https://images.macrumors.com/t/JdCJ-AJ3Ort9AETOl6rhY0h-yKg=/400x0/article-new/2023/02/iOS-16-4-Web-Push.jpeg?lossy 400w,https://images.macrumors.com/t/SXvAzJ10FPOQRZ5HpBBKDyjgeF8=/800x0/article-new/2023/02/iOS-16-4-Web-Push.jpeg?lossy 800w,https://images.macrumors.com/t/esEjyyoBJArH9DuUVOxzSi_hc5g=/1600x0/article-new/2023/02/iOS-16-4-Web-Push.jpeg 1600w,https://images.macrumors.com/t/TjaJpIne8IG3rDlqM3tgUYkDAc8=/2500x0/filters:no_upscale()/article-new/2023/02/iOS-16-4-Web-Push.jpeg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="iOS 16 4 Web Push" width="794" height="450"><br>In a letter to the Department of Justice, Senator Ron Wyden said foreign officials were demanding the data from the tech giants to track smartphones. The traffic flowing from apps that send push notifications put the companies "in a unique position to facilitate government surveillance of how users are using particular apps," Wyden said. He asked the Department of Justice to "repeal or modify any policies" that hindered public discussions of push notification spying.</p>
<p>In a statement given to <em>Reuters</em>, Apple said that Wyden's letter gave them the opening they needed to share more details with the public about how governments monitored push notifications.<br>
</p>
<blockquote><p>"In this case, the federal government prohibited us from sharing any information," the company said in a statement. "Now that this method has become public we are updating our transparency reporting to detail these kinds of requests."</p></blockquote>
<p>According to the report, Wyden's letter said a "tip" was the source of the information about the surveillance. A source familiar with the matter confirmed that both foreign and U.S. government agencies have been asking Apple and Google for metadata related to push notifications. The data is said to have been used to attempt to tie anonymous users of messaging apps to specific Apple or Google accounts.</p>
<p><em>Reuters</em>' source would not identify which governments were making the data requests but described them as "democracies allied to the United States." They did not know how long the requests had been going on for. </p>
<p>Apple <a href="https://developer.apple.com/documentation/usernotifications/setting_up_a_remote_notification_server/generating_a_remote_notification">advises developers</a> not to include sensitive data in notifications and to encrypt any data before adding it to a notification payload. However, this requires action on the developers' part. Likewise, metadata (like which apps are sending notifications and how often) is not encrypted, potentially giving anyone with access to the information insight into users' app usage.</p>
<p><small>Note: Due to the political or social nature of the discussion regarding this topic, the discussion thread is located in our <a href="https://forums.macrumors.com/forums/political-news.218/">Political News</a> forum. All forum members and site visitors are welcome to read and follow the thread, but posting is limited to forum members with at least 100 posts.</small></p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2023/12/01/ios-17-2-list-of-12-new-features/">iOS 17.2 Will Add These 12 New Features to Your iPhone</a></h3><p>Friday December 1, 2023 12:19 pm PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>iOS 17.2 has been in beta testing for over a month, and it should be released to all users in a few more weeks. The software update includes many new features and changes for iPhones, including the dozen that we have highlighted below. iOS 17.2 is expected to be released to the public in mid-December. To learn about even more features coming in the update, check out our full list. Journal ...</p></div><div><h3><a href="https://www.macrumors.com/2023/12/01/ankers-cyber-week-sale-final-days/">Anker's Cyber Week Sale Enters Final Days With Up to 60% Off Sitewide</a></h3><p>Anker's Black Friday/Cyber Week event is entering its final days this weekend, and it's still offering up to 60 percent off sitewide. There are also a few "mystery boxes" that can include hundreds of dollars in savings, if you're willing to risk not knowing what you're buying ahead of time. All of these sales will end on December 3. Note: MacRumors is an affiliate partner with Anker. When you...</p></div><div><h3><a href="https://www.macrumors.com/2023/11/30/iphone-green-bubbles-rcs-support/">Green Bubbles on iPhone to Gain These 7 New Features Next Year</a></h3><p>Thursday November 30, 2023 9:00 am PST by <a href="https://www.macrumors.com/author/joe-rossignol/" rel="author">Joe Rossignol</a></p><p>Earlier this month, Apple announced that it will finally support RCS in the Messages app on the iPhone starting later next year. This change will result in several improvements to the messaging experience between iPhones and Android devices. RCS will become the new default standard for messaging between iPhones and Android devices, but these conversations will still have green bubbles like...</p></div><div><h3><a href="https://www.macrumors.com/2023/12/02/top-stories-ios-17-1-2-released/">Top Stories: iOS 17.1.2 Released, NameDrop Misinformation, and More</a></h3><p>Apple employees are back to work following a Thanksgiving break, and that means this week saw a number of new operating system updates for both public release and beta testing. This week also saw some misinformation about Apple's new NameDrop feature making the rounds, while Apple and Goldman Sachs appear to be on the verge of a break-up in their Apple Card and savings account partnership,...</p></div><div><h3><a href="https://www.macrumors.com/2023/12/05/instagram-messenger-chats-disconnecting/">Instagram and Facebook Messenger Chats to Disconnect This Month</a></h3><p>Tuesday December 5, 2023 1:57 am PST by <a href="https://www.macrumors.com/author/tim-hardwick/" rel="author">Tim Hardwick</a></p><p>Meta has revealed plans to end Instagram users' ability to chat with Facebook accounts later this month, rolling back a feature that it introduced over three years ago. In September 2020, Meta (then Facebook) announced it was merging its Facebook Messenger service with Instagram direct messaging, allowing Instagram users to chat with Facebook users and vice versa using the same platform....</p></div><div><h3><a href="https://www.macrumors.com/2023/12/04/apple-work-on-6g-expanding/">Apple's Work on 6G Connectivity Already Expanding</a></h3><p>Apple's work on implementing 6G cellular connectivity on its devices appears to be ramping up, according to Bloomberg's Mark Gurman. In the latest edition of his "Power On" newsletter, Gurman explained that Apple is increasingly turning its attention to 6G, even amid its widely reported difficulties developing a custom 5G cellular modem. In 2021, the first highly specific Apple job...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mastering Nim, 2nd edition (112 pts)]]></title>
            <link>https://nim-lang.org/blog/2023/09/19/mastering-nim.html</link>
            <guid>38543491</guid>
            <pubDate>Wed, 06 Dec 2023 13:21:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nim-lang.org/blog/2023/09/19/mastering-nim.html">https://nim-lang.org/blog/2023/09/19/mastering-nim.html</a>, See on <a href="https://news.ycombinator.com/item?id=38543491">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          <h3>
            <span>
              
              19 September 2023
            </span>
            
            <span>
              
              The Nim Team
            </span>
            
          </h3>
          <p>Discover the secret of Nim!</p>

<p>The definite guide on Nim!
Written by the inventor himself.</p>

<p>Now with updated content for version 2.0 which solves the biggest pain point of Nim 1.0, shared memory in a multi-threaded setting.</p>

<p>Please have a look at its cover image:</p>
<p>
  <img width="auto" height="600" src="https://nim-lang.org//assets/img/mastering_nim_2.jpg">
</p>

<p><strong>But Nim’s logo is a crown!
Where is the crown?</strong>
That’s the secret of Nim!</p>

<p>Send us your reply to <a href="https://nim-lang.org/cdn-cgi/l/email-protection" data-cfemail="c9babcb9b9a6bbbd89a7a0a4e4a5a8a7aee7a6bbae">[email&nbsp;protected]</a> until December 6th 2023.
Among the correct answers we will select 3 winners by randomization.
The winners will receive a signed hardcover!</p>

<p>“Mastering Nim” is available here:</p>

<ul>
  <li><a href="https://www.amazon.com/dp/B0B4R7B9YX">amazon.com</a></li>
  <li><a href="https://www.amazon.de/dp/B0B4R7B9YX">amazon.de</a></li>
</ul>


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mold Course (240 pts)]]></title>
            <link>https://www.epa.gov/mold/mold-course-introduction</link>
            <guid>38543229</guid>
            <pubDate>Wed, 06 Dec 2023 12:58:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.epa.gov/mold/mold-course-introduction">https://www.epa.gov/mold/mold-course-introduction</a>, See on <a href="https://news.ycombinator.com/item?id=38543229">Hacker News</a></p>
Couldn't get https://www.epa.gov/mold/mold-course-introduction: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Playstation keeps reminding us why digital ownership sucks (287 pts)]]></title>
            <link>https://www.theverge.com/2023/12/5/23989290/playstation-digital-ownership-sucks</link>
            <guid>38543196</guid>
            <pubDate>Wed, 06 Dec 2023 12:54:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2023/12/5/23989290/playstation-digital-ownership-sucks">https://www.theverge.com/2023/12/5/23989290/playstation-digital-ownership-sucks</a>, See on <a href="https://news.ycombinator.com/item?id=38543196">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In less than a week, Sony has given us two timely reminders of the tenuousness of digital “ownership” — and both reminders involve things on PlayStation.</p><p>Last week, Sony said that, because of content licensing “arrangements,” users wouldn’t be able to watch Discovery content they’ve purchased <em>and</em> that the content would be removed from their libraries as of December 31st, 2023. The resulting list of shows that will suddenly disappear because of corporate agreements is <a href="https://www.playstation.com/en-us/legal/psvideocontent/?et_rid=&amp;et_cid=231130-VIDREMVL-AM-CSA-B-FLX&amp;Linkid=231130-VIDREMVL-AM-CSA-B-FLX&amp;emcid=em-pl-500377">very long</a>. Shows disappearing from streaming services is commonplace, but in this case, people are losing access to shows they bought to watch on demand whenever they wanted.</p><p>Then, on Monday, many users were unexpectedly banned from their <a href="https://www.theverge.com/2023/12/4/23988621/sony-playstation-account-ps5-bans-permanent-suspension">PlayStation Network accounts</a>, meaning that not only were they blocked from playing multiplayer games or using cloud streaming but they were also locked out of games they purchased digitally from Sony’s PlayStation marketplace. Affected users who may have spent years building a robust digital library were suddenly left without access to content they had bought through no fault of their own. It appears that Sony has since restored account access to people who were accidentally banned, but the company hasn’t explained what happened or said how it might prevent similar unexpected bans in the future. (Sony hasn’t replied to our multiple requests for comment.)</p><p>The ephemerality of digital “ownership” isn’t a new issue. Even though downloading and accessing digital content is often easier than trudging to a retail store to buy a physical copy of a game, you’re putting your faith in the platform holders to maintain their digital storefronts, the content on those storefronts, and their account systems so that your access keeps working.</p><p>The recent closure <a href="https://www.theverge.com/2023/3/26/23657431/wii-u-nintendo-3ds-eshops-shut-down">of Nintendo’s Wii U and 3DS eShops</a> was a stark reminder that companies have the power to decide when you can buy digital content. While you can still redownload Wii U and 3DS games that you’ve purchased, it seems inevitable that Nintendo will stop letting you do that one day. (It’s already planning to shut down online services for those platforms, <a href="https://www.theverge.com/2023/10/4/23902615/wii-u-nintendo-3ds-online-shut-down">after all</a>.) And remember <a href="https://www.theverge.com/2022/9/29/23378713/google-stadia-shutting-down-game-streaming-january-2023">when Google shut down Stadia</a>?</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="I’m considering switching back to physical games." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/376x251/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/384x256/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/415x277/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/480x320/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/540x360/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/640x427/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/750x500/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/828x552/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1080x720/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1200x800/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1440x960/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/1920x1280/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2048x1365/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x1600/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/10329947/jbareham_180301_2346_nintendo_switch_0094.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>I’m considering switching back to physical games.</em></figcaption> <p><cite>Photo by James Bareham / The Verge</cite></p></div></div><p>These recent PlayStation incidents are more aggravating, however, because of how sudden and seemingly unfair they are. With the Discovery content, Sony is giving users a matter of weeks to watch their purchased shows for the last time before the shows are yanked from their library entirely. And Sony isn’t offering any compensation for titles you’ve already bought or a way to transfer those purchases to another store. The PlayStation account bans were as swift as they were unexpected, and while resolution for most arrived within a few hours, Sony still hasn’t shared any public communication about what happened or why users should continue to trust the platform.</p><p>I’ve been all in on digital content for years. I don’t like the clutter of physical boxes, and I enjoy being able to switch games and movies without having to get off the couch. But after seeing more instances of companies removing “purchased” digital content — essentially making things I buy digitally a long-term rental — I’m seriously considering going back to buying discs and cartridges.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Governments spying on Apple, Google users through push notifications -US senator (506 pts)]]></title>
            <link>https://www.reuters.com/technology/cybersecurity/governments-spying-apple-google-users-through-push-notifications-us-senator-2023-12-06/</link>
            <guid>38543155</guid>
            <pubDate>Wed, 06 Dec 2023 12:49:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/cybersecurity/governments-spying-apple-google-users-through-push-notifications-us-senator-2023-12-06/">https://www.reuters.com/technology/cybersecurity/governments-spying-apple-google-users-through-push-notifications-us-senator-2023-12-06/</a>, See on <a href="https://news.ycombinator.com/item?id=38543155">Hacker News</a></p>
Couldn't get https://www.reuters.com/technology/cybersecurity/governments-spying-apple-google-users-through-push-notifications-us-senator-2023-12-06/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Norway Joins Denmark in Swedish Tesla Strike/Blockade (190 pts)]]></title>
            <link>https://www.svt.se/nyheter/utrikes/aven-norge-dras-in-i-teslastrejken--3vcx0b</link>
            <guid>38542892</guid>
            <pubDate>Wed, 06 Dec 2023 12:21:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.svt.se/nyheter/utrikes/aven-norge-dras-in-i-teslastrejken--3vcx0b">https://www.svt.se/nyheter/utrikes/aven-norge-dras-in-i-teslastrejken--3vcx0b</a>, See on <a href="https://news.ycombinator.com/item?id=38542892">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody"><p>– De tar striden mot ett fackfientligt bolag på hela arbetsmarknaden vägnar. Nu måste Tesla omedelbart acceptera IF Metalls krav om ett kollektivavtal, säger fackbasen Jørn Eggum till den norska tidningen Dagbladets Børsen.</p><p>Om Tesla inte tillmötesgått IF Metalls krav till den 20 december kommer Fellesforbundet verkställa bojkotten, enligt Eggum.</p><h2>Danskt fack varslar om sympatiåtgärder</h2><p>Även Danmarks största fackförbund 3F Transport, som organiserar hamnarbetare och chaufförer, varslade under tisdagen om sympatiåtgärder. Om 13 dagar kommer de inte ta emot eller transportera Teslas bilar som ska till Sverige.</p><p>”När de ber om vårt stöd backar vi naturligtvis upp. Liksom företagen är fackföreningsrörelsen global i kampen för att skydda arbetarna”, sade Jan Villadsen, ordförande i 3F Transport, i ett pressmeddelande.</p><figure><div><div><p><img alt="" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p></div><div><p>Javascript måste vara påslaget för att kunna spela video</p></div></div><figcaption><span>Därför bråkar IF Metall och Tesla om kollektivavtal. <span>Foto: <!-- -->SVT</span></span></figcaption></figure></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Compressing Gaussian Splats (106 pts)]]></title>
            <link>https://blog.playcanvas.com//compressing-gaussian-splats/</link>
            <guid>38542875</guid>
            <pubDate>Wed, 06 Dec 2023 12:18:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.playcanvas.com//compressing-gaussian-splats/">https://blog.playcanvas.com//compressing-gaussian-splats/</a>, See on <a href="https://news.ycombinator.com/item?id=38542875">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <h3 id="introduction">Introduction</h3>

<p><a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/"><strong>3D Gaussian Splatting</strong></a> is a new method for digitizing and rendering real world objects. With gaussian splatting, you can digitize a scene from a few photos using services like <a href="https://lumalabs.ai/">Luma Labs</a> or <a href="https://poly.cam/">Polycam</a>. These services take the set of photos and generate a 3d Gaussian Splat scene in <a href="https://en.wikipedia.org/wiki/PLY_(file_format)">PLY format</a>.</p>

<p>For example, this is a Gaussian Splat scene rendered in PlayCanvas.</p>



<h3 id="what-is-a-splat">What is a Splat?</h3>

<p>Gaussian Splat Scenes are not made up of polygons and textures. Instead, they are made up of many (up to millions) of individual, unconnected blobs called <em>splats</em>. A splat is just a particle in space with size, orientation, color and opacity.</p>

<p>Below you can see a single brown splat selected. The splat bounding box shows its orientation and size:</p>

<p><img src="https://blog.playcanvas.com/assets/media/splat-example.gif" alt="Splat Example"></p>

<p>The gaussian part of the name comes from the shape of splat itself: the splat opacity has a gaussian falloff from its center to its edge.</p>

<h3 id="engine-support">Engine Support</h3>

<p>The PlayCanvas team has been adding support to the engine for loading and rendering Gaussian Splat PLY files:</p>

<p><a href="https://playcanvas.github.io/#/loaders/splat-many"><img src="https://blog.playcanvas.com/assets/media/gaussian-splat-example.gif" alt="Engine Example"></a></p>

<p>Since the resulting files are often messy and require cleaning, we released <a href="https://playcanvas.com/super-splat">SuperSplat</a>, a tool for cleaning and processing gaussian splat PLY files:</p>

<p><a href="https://playcanvas.com/super-splat?load=https://code.playcanvas.com/viewer/guitar-cleaned.ply"><img src="https://blog.playcanvas.com/assets/media/super-splat-example.gif" alt="SuperSplat Example"></a></p>

<h3 id="ply-format">PLY Format</h3>

<p>However, the default gaussian splat PLY format as exported by training tools is large.</p>

<p>This is because the uncompressed format stores a large amount of data <em>per splat</em>:</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Data Format</th>
      <th>Bytes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Position</td>
      <td>3 x float</td>
      <td>12</td>
    </tr>
    <tr>
      <td>Orientation</td>
      <td>4 x float</td>
      <td>16</td>
    </tr>
    <tr>
      <td>Scale</td>
      <td>3 x float</td>
      <td>12</td>
    </tr>
    <tr>
      <td>Spherical harmonics / color</td>
      <td>48 x float</td>
      <td>192</td>
    </tr>
    <tr>
      <td>Total</td>
      <td>&nbsp;</td>
      <td>232</td>
    </tr>
  </tbody>
</table>

<p>For example, the original <code>guitar.ply</code> scene file takes <strong>132.8 MB</strong> (<strong>32 MB</strong> excluding spherical harmonic data).</p>

<h3 id="compressed-ply-format">Compressed PLY Format</h3>

<p>So we introduced a <em>compressed PLY</em> format for use in runtime applications. The compressed PLY file format ignores the unused spherical harmonic data and stores the rest of the elements in quantized integers.</p>

<p>The format can be summarized as follows:</p>
<ul>
  <li>Split the scene into chunks of 256 splats</li>
  <li>For each chunk, store the min and max (x, y, z) for position and scale in floating point</li>
  <li>For each splat in the chunk, store a normalized and quantized value for position and scale (relative to chunk extents) and orientation and color</li>
</ul>

<p>This data layout results in the following data <em>per chunk</em>:</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Data Format</th>
      <th>Bytes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Position bound</td>
      <td>6 x float</td>
      <td>24</td>
    </tr>
    <tr>
      <td>Scale bound</td>
      <td>6 x float</td>
      <td>24</td>
    </tr>
    <tr>
      <td>Total</td>
      <td>&nbsp;</td>
      <td>48</td>
    </tr>
  </tbody>
</table>

<p>And the following data <em>per splat</em>:</p>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Data Format</th>
      <th>Bytes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Position</td>
      <td>uint32 (11, 10, 11)</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Orientation</td>
      <td>uint32 (2, 10, 10, 10)</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Scale</td>
      <td>uint32 (11, 10, 11)</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Color</td>
      <td>uint32 (8, 8, 8, 8)</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Total</td>
      <td>&nbsp;</td>
      <td>16</td>
    </tr>
  </tbody>
</table>

<p>As a result, the compressed version of <code>guitar.ply</code> takes only <strong>8.7 MB</strong>.</p>

<h3 id="do-it-yourself">Do It Yourself</h3>

<p>The easiest way to generate a compressed PLY file yourself is using the <a href="https://playcanvas.com/super-splat">SuperSplat tool</a>. Load the PLY file into SuperSplat and export it again using the ‘Compressed Ply File’ option:</p>

<p><a href="https://playcanvas.com/super-splat"><img src="https://blog.playcanvas.com/assets/media/super-splat-export.png" alt="SuperSplat Export"></a></p>

<p>If you are interested in the file format specifics, see <a href="https://github.com/playcanvas/engine/blob/a86bd8be0cfd4e39e9ba5e5466acb6875ab9906e/extras/splat/splat-data.js#L257">this code</a> which demonstrates how to decompress the file data.</p>

<p>See <a href="https://playcanvas.com/project/1165904/overview/gaussiansplatdemo">this editor project</a> for an example of loading and rendering a compressed gaussian splat PLY file. Or you can <a href="https://playcanv.as/p/69cnpevQ/">run it here</a>.</p>

<h3 id="summary-and-future">Summary and Future</h3>

<p>We have introduced a new compressed PLY format for gaussian splatting which is roughly 4x smaller than uncompressed data and can be used in realtime applications.</p>

<p>In future we hope to:</p>

<ul>
  <li>store splats hierarchically for optimized rendering and culling</li>
  <li>implement realtime splat LOD</li>
  <li>test skinning and animation of gaussian splats</li>
  <li>further compress gaussian splat data</li>
  <li>optimize WebGPU rendering</li>
</ul>

<h3 id="references">References</h3>

<p>The compressed format is largely based on the fine work of Aras Pranckevičius and his <a href="https://aras-p.info/">blog posts</a>.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rethinking Serverless with Flame (244 pts)]]></title>
            <link>https://fly.io/blog/rethinking-serverless-with-flame/</link>
            <guid>38542764</guid>
            <pubDate>Wed, 06 Dec 2023 12:03:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fly.io/blog/rethinking-serverless-with-flame/">https://fly.io/blog/rethinking-serverless-with-flame/</a>, See on <a href="https://news.ycombinator.com/item?id=38542764">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
         <dl>
             <dt>Author</dt>
             <dd>
                 <img alt="Chris McCord" src="https://fly.io/static/images/chris-m.webp">
               <dl>
                 <dt>Name</dt>
                 <dd>
                   Chris McCord
                 </dd>
                   <dt>Twitter</dt>
                   <dd>
                     <a href="https://twitter.com/chris_mccord" target="_blank">
                       @chris_mccord
                     </a>
                   </dd>
               </dl>
             </dd>
         </dl>

        <section>
            <figure>
                <img src="https://fly.io/blog/rethinking-serverless-with-flame/assets/flame-cover.webp" alt="FLAME logo">
            </figure>
          <blockquote>Imagine if you could auto scale simply by wrapping any existing app code in a function and have that block of code run in a temporary copy of your app.</blockquote>


<p>The pursuit of elastic, auto-scaling applications has taken us to silly places.</p>

<p>Serverless/FaaS had a couple things going for it. Elastic Scale™ is hard. It’s even harder when you need to manage those pesky servers. It also promised pay-what-you-use costs to avoid idle usage. Good stuff, right?</p>

<p>Well the charade is over. You offload scaling concerns and the complexities of scaling, just to end up needing <em>more complexity</em>. Additional queues, storage, and glue code to communicate back to our app is just the starting point. Dev, test, and CI complexity balloons as fast as your costs. Oh, and you often have to rewrite your app in proprietary JavaScript – even if it’s already written in JavaScript!</p>

<p>At the same time, the rest of us have elastically scaled by starting more webservers. Or we’ve dumped on complexity with microservices. This doesn’t make sense. Piling on more webservers to transcode more videos or serve up more ML tasks isn’t what we want. And granular scale shouldn’t require slicing our apps into bespoke operational units with their own APIs and deployments to manage.</p>

<p>Enough is enough. There’s a better way to elastically scale applications.</p>
<h2 id="the-flame-pattern"><a href="#the-flame-pattern" aria-label="Anchor"></a>The FLAME pattern</h2>
<p>Here’s what we really want:</p>

<ul>
<li>We don’t want to manage those pesky servers. We already have this for our app deployments via <code>fly deploy</code>, <code>git push heroku</code>, <code>kubectl</code>, etc
</li><li>We want on-demand, <em>granular</em> elastic scale of specific parts of our app code
</li><li>We don’t want to rewrite our application or write parts of it in proprietary runtimes
</li></ul>

<p>Imagine if we could auto scale simply by wrapping any existing app code in a function and have that block of code run in a temporary copy of the app.</p>

<p>Enter the FLAME pattern.</p>
<blockquote>FLAME - Fleeting Lambda Application for Modular Execution</blockquote>


<p>With FLAME, you treat your <em>entire application</em> as a lambda, where modular parts can be executed on short-lived infrastructure.</p>

<p>No rewrites. No bespoke runtimes. No outrageous layers of complexity. Need to insert the results of an expensive operation to the database? PubSub broadcast the result of some expensive work? No problem! It’s your whole app so of course you can do it.</p>

<p>The Elixir <a href="https://github.com/phoenixframework/flame">flame library</a> implements the FLAME pattern. It has a backend adapter for Fly.io, but you can use it on any cloud that gives you an API to spin up an instance with your app code running on it. We’ll talk more about backends in a bit, as well as implementing FLAME in other languages.</p>

<p>First, lets watch a realtime thumbnail generation example to see FLAME + Elixir in action:</p>
<div><p>
          <iframe width="100%" height="100%" src="https://www.youtube.com/embed/l1xt_rkWdic" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
          </iframe>
        </p>
</div>


<p>Now let’s walk thru something a little more basic. Imagine we have a function to transcode video to thumbnails in our Elixir application after they are uploaded:</p>
<div>
  <pre><code><span>def</span> <span>generate_thumbnails</span><span>(%</span><span>Video</span><span>{}</span> <span>=</span> <span>vid</span><span>,</span> <span>interval</span><span>)</span> <span>do</span>
  <span>tmp</span> <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>System</span><span>.</span><span>tmp_dir!</span><span>(),</span> <span>Ecto</span><span>.</span><span>UUID</span><span>.</span><span>generate</span><span>())</span>
  <span>File</span><span>.</span><span>mkdir!</span><span>(</span><span>tmp</span><span>)</span>
  <span>args</span> <span>=</span> <span>~w(-i #{vid.url} -vf fps=1/#{interval} #{tmp}/%02d.png)</span><span>)</span>
  <span>System</span><span>.</span><span>cmd</span><span>(</span><span>"ffmpeg"</span><span>,</span> <span>args</span><span>)</span>
  <span>urls</span> <span>=</span> <span>VidStore</span><span>.</span><span>put_thumbnails</span><span>(</span><span>vid</span><span>,</span> <span>Path</span><span>.</span><span>wildcard</span><span>(</span><span>tmp</span> <span>&lt;&gt;</span> <span>"/*.png"</span><span>))</span>
  <span>Repo</span><span>.</span><span>insert_all</span><span>(</span><span>Thumb</span><span>,</span> <span>Enum</span><span>.</span><span>map</span><span>(</span><span>urls</span><span>,</span> <span>&amp;</span><span>%{</span><span>vid_id:</span> <span>vid</span><span>.</span><span>id</span><span>,</span> <span>url:</span> <span>&amp;1</span><span>}))</span>
<span>end</span>
</code></pre>
</div>

<p>Our <code>generate_thumbnails</code> function accepts a video struct. We shell out to <code>ffmpeg</code> to take the video URL and generate thumbnails at a given interval. We then write the temporary thumbnail paths to durable storage. Finally, we insert the generated thumbnail URLs into the database.</p>

<p>This works great locally, but CPU bound work like video transcoding can quickly bring our entire service to a halt in production. Instead of rewriting large swaths of our app to move this into microservices or some FaaS, we can simply wrap it in a FLAME call:</p>
<div>
  <pre><code><span>def</span> <span>generate_thumbnails</span><span>(%</span><span>Video</span><span>{}</span> <span>=</span> <span>vid</span><span>,</span> <span>interval</span><span>)</span> <span>do</span>
  <span>FLAME</span><span>.</span><span>call</span><span>(</span><span>MyApp</span><span>.</span><span>FFMpegRunner</span><span>,</span> <span>fn</span> <span>-&gt;</span>
    <span>tmp</span> <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>System</span><span>.</span><span>tmp_dir!</span><span>(),</span> <span>Ecto</span><span>.</span><span>UUID</span><span>.</span><span>generate</span><span>())</span>
    <span>File</span><span>.</span><span>mkdir!</span><span>(</span><span>tmp</span><span>)</span>
    <span>args</span> <span>=</span> <span>~w(-i #{vid.url} -vf fps=1/#{interval} #{tmp}/%02d.png)</span><span>)</span>
    <span>System</span><span>.</span><span>cmd</span><span>(</span><span>"ffmpeg"</span><span>,</span> <span>args</span><span>)</span>
    <span>urls</span> <span>=</span> <span>VidStore</span><span>.</span><span>put_thumbnails</span><span>(</span><span>vid</span><span>,</span> <span>Path</span><span>.</span><span>wildcard</span><span>(</span><span>tmp</span> <span>&lt;&gt;</span> <span>"/*.png"</span><span>))</span>
    <span>Repo</span><span>.</span><span>insert_all</span><span>(</span><span>Thumb</span><span>,</span> <span>Enum</span><span>.</span><span>map</span><span>(</span><span>urls</span><span>,</span> <span>&amp;</span><span>%{</span><span>vid_id:</span> <span>vid</span><span>.</span><span>id</span><span>,</span> <span>url:</span> <span>&amp;1</span><span>}))</span>
  <span>end</span><span>)</span>
<span>end</span>
</code></pre>
</div>

<p>That’s it! <code>FLAME.call</code> accepts the name of a runner pool, and a function. It then finds or boots a new copy of our entire application and runs the function there. Any variables the function closes over (like our <code>%Video{}</code> struct and <code>interval</code>) are passed along automatically.</p>

<p>When the FLAME runner boots up, it connects back to the parent node, receives the function to run, executes it, and returns the result to the caller. Based on configuration, the booted runner either waits happily for more work before idling down, or extinguishes itself immediately.</p>

<p>Let’s visualize the flow:</p>

<p><img alt="visualizing the flow" src="https://fly.io/blog/rethinking-serverless-with-flame/assets/visual.webp?centered"></p>

<p>We changed no other code and issued our DB write with <code>Repo.insert_all</code> just like before, because we are running our <em>entire</em> <em>application</em>. Database connection(s) and all. Except this fleeting application only runs that little function after startup and nothing else.</p>

<p>In practice, a FLAME implementation will support a pool of runners for hot startup, scale-to-zero, and elastic growth. More on that later.</p>
<h2 id="solving-a-problem-vs-removing-the-problem"><a href="#solving-a-problem-vs-removing-the-problem" aria-label="Anchor"></a>Solving a problem vs removing the problem</h2><blockquote>FaaS solutions help you solve a problem. FLAME removes the problem.</blockquote>


<p>The FaaS labyrinth of complexity defies reason. And it’s unavoidable. Let’s walkthrough the thumbnail use-case to see how.</p>

<p>We try to start with the simplest building block like request/response AWS Lambda Function URL’s.</p>

<p>The complexity hits immediately.</p>

<p>We start writing custom encoders/decoders on both sides to handle streaming the thumbnails back to the app over HTTP. Phew that’s done. Wait, is our video transcoding or user uploads going to take longer than 15 minutes? Sorry, hard timeout limit&nbsp;–&nbsp;time to split our videos into chunks to stay within the timeout, which means more lambdas to do that. Now we’re orchestrating lambda workflows and relying on additional services, such as SQS and S3, to enable this.</p>

<p>All the FaaS is doing is adding layers of communication between your code and the parts you want to run elastically. Each layer has its own glue integration price to pay.</p>

<p>Ultimately handling this kind of use-case looks something like this:</p>

<ul>
<li>Trigger the lambda via HTTP endpoint, S3, or API gateway ($)
</li><li>Write the bespoke lambda to transcode the video ($)
</li><li>Place the thumbnail results into SQS ($)
</li><li>Write the SQS consumer in our app (dev $)
</li><li>Persist to DB and figure out how to get events back to active subscribers that may well be connected to other instances than the SQS consumer (dev $)
</li></ul>

<p>This is nuts. We pay the FaaS toll at every step. We shouldn’t have to do any of this!</p>

<p>FaaS provides a bunch of offerings to build a solution on top of. FLAME removes the problem entirely.</p>
<h2 id="flame-backends"><a href="#flame-backends" aria-label="Anchor"></a>FLAME Backends</h2><blockquote>On Fly.io infrastructure the <code>FLAME.FlyBackend</code> can boot a copy of your application on a new <a href="https://fly.io/docs/machines/">Machine</a> and have it connect back to the parent for work within ~3s.</blockquote>


<p>By default, FLAME ships with a <code>LocalBackend</code> and <code>FlyBackend</code>, but any host that provides an API to provision a server and run your app code can work as a FLAME backend. Erlang and Elixir primitives are doing all the heavy lifting here. The entire <code>FLAME.FlyBackend</code> is <a href="https://github.com/phoenixframework/flame/blob/main/lib/flame/fly_backend.ex">&lt; 200 LOC with docs</a>. The library has a single dependency, <code>req</code>, which is an HTTP client.</p>

<p>Because Fly.io runs our applications as a packaged up docker image, we simply ask the Fly API to boot a new Machine for us with the same image that our app is currently running. Also thanks to Fly infrastructure, we can guarantee the FLAME runners are started in the same region as the parent. This optimizes latency and lets you ship whatever data back and forth between parent and runner without having to think about it.</p>
<h2 id="look-at-everything-were-not-doing"><a href="#look-at-everything-were-not-doing" aria-label="Anchor"></a>Look at everything we’re not doing</h2>
<p>With FaaS, just imagine how quickly the dev and testing story becomes a fate worse than death.</p>

<p>To run the app locally, we either need to add some huge dev dependencies to simulate the entire FaaS pipeline, or worse, connect up our dev and test environments directly to the FaaS provider.</p>

<p>With FLAME, your dev and test runners simply run on the local backend.</p>

<p>Remember, this is your app. FLAME just controls where modular parts of it run. In dev or test, those parts simply run on the existing runtime on&nbsp;your laptop or CI server.</p>

<p>Using Elixir, we can even send a file across to the remote FLAME application thanks to the distributed features of the Erlang VM:</p>
<div>
  <pre><code><span>def</span> <span>generate_thumbnails</span><span>(%</span><span>Video</span><span>{}</span> <span>=</span> <span>vid</span><span>,</span> <span>interval</span><span>)</span> <span>do</span>
  <span>parent_stream</span> <span>=</span> <span>File</span><span>.</span><span>stream!</span><span>(</span><span>vid</span><span>.</span><span>filepath</span><span>,</span> <span>[],</span> <span>2048</span><span>)</span>
  <span>FLAME</span><span>.</span><span>call</span><span>(</span><span>MyApp</span><span>.</span><span>FFMpegRunner</span><span>,</span> <span>fn</span> <span>-&gt;</span>
    <span>tmp_file</span> <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>System</span><span>.</span><span>tmp_dir!</span><span>(),</span> <span>Ecto</span><span>.</span><span>UUID</span><span>.</span><span>generate</span><span>())</span>
    <span>flame_stream</span> <span>=</span> <span>File</span><span>.</span><span>stream!</span><span>(</span><span>tmp_file</span><span>)</span>
    <span>Enum</span><span>.</span><span>into</span><span>(</span><span>parent_stream</span><span>,</span> <span>flame_stream</span><span>)</span>

    <span>tmp</span> <span>=</span> <span>Path</span><span>.</span><span>join</span><span>(</span><span>System</span><span>.</span><span>tmp_dir!</span><span>(),</span> <span>Ecto</span><span>.</span><span>UUID</span><span>.</span><span>generate</span><span>())</span>
    <span>File</span><span>.</span><span>mkdir!</span><span>(</span><span>tmp</span><span>)</span>
    <span>args</span> <span>=</span> <span>~w(-i #{tmp_file} -vf fps=1/#{interval} #{tmp}/%02d.png)</span>
    <span>System</span><span>.</span><span>cmd</span><span>(</span><span>"ffmpeg"</span><span>,</span> <span>args</span><span>)</span>
    <span>urls</span> <span>=</span> <span>VidStore</span><span>.</span><span>put_thumbnails</span><span>(</span><span>vid</span><span>,</span> <span>Path</span><span>.</span><span>wildcard</span><span>(</span><span>tmp</span> <span>&lt;&gt;</span> <span>"/*.png"</span><span>))</span>
    <span>Repo</span><span>.</span><span>insert_all</span><span>(</span><span>Thumb</span><span>,</span> <span>Enum</span><span>.</span><span>map</span><span>(</span><span>urls</span><span>,</span> <span>&amp;</span><span>%{</span><span>vid_id:</span> <span>vid</span><span>.</span><span>id</span><span>,</span> <span>url:</span> <span>&amp;1</span><span>}))</span>
  <span>end</span><span>)</span>
<span>end</span>
</code></pre>
</div>

<p>On line 2 we open a file on the parent node to the video path. Then in the FLAME child, we stream the file from the parent node to the FLAME server in only a couple lines of code. That’s it! No setup of S3 or HTTP interfaces required.</p>

<p>With FLAME it’s easy to miss everything we’re not doing:</p>

<ul>
<li>We don’t need to write code outside of our application. We can reuse business logic, database setup, PubSub, and all the features of our respective platforms
</li><li>We don’t need to manage deploys of separate services or endpoints
</li><li>We don’t need to write results to S3 or SQS just to pick up values back in our app
</li><li>We skip the dev, test, and CI dependency dance
</li></ul>
<h2 id="flame-outside-elixir"><a href="#flame-outside-elixir" aria-label="Anchor"></a>FLAME outside Elixir</h2>
<p>Elixir is fantastically well suited for the FLAME model because we get so much <a href="https://fly.io/phoenix-files/elixir-and-phoenix-can-do-it-all/">for free</a> like process supervision and distributed messaging. That said, any language with reasonable concurrency primitives can take advantage of this pattern. For example, my teammate, Lubien, created a proof of concept example for breaking out functions in your JavaScript application and running them inside a new Fly Machine: <a href="https://github.com/lubien/fly-run-this-function-on-another-machine">https://github.com/lubien/fly-run-this-function-on-another-machine</a></p>

<p>So the general flow for a JavaScript-based FLAME call would be to move the modular executions to a new file, which is executed on a runner pool. Provided the arguments are JSON serializable, the general FLAME flow is similar to what we’ve outlined here. Your application, your code, running on fleeting instances.</p>

<p>A complete FLAME library will need to handle the following concerns:</p>

<ul>
<li>Elastic pool scale-up and scale-down logic
</li><li>Hot vs cold startup with pools
</li><li>Remote runner monitoring to avoid orphaned resources
</li><li>How to monitor and keep deployments fresh
</li></ul>

<p>For the rest of this post we’ll see how the Elixir FLAME library handles these concerns as well as features uniquely suited to Elixir applications. But first, you might be wondering about your background job queues.</p>
<h2 id="what-about-my-background-job-processor"><a href="#what-about-my-background-job-processor" aria-label="Anchor"></a>What about my background job processor?</h2>
<p>FLAME works great inside your background job processor, but you may have noticed some overlap. If your job library handles scaling the worker pool, what is FLAME doing for you? There’s a couple important distinctions here.</p>

<p>First, we reach for these queues when we need <em>durability guarantees</em>. We often can turn knobs to have the queues scale to handle more jobs as load changes. But durable operations are separate from elastic execution. Conflating these concerns can send you down a similar path to lambda complexity. Leaning on your worker queue purely for offloaded execution means writing all the glue code to get the data into and out of the job, and back to the caller or end-user’s device somehow.</p>

<p>For example, if we want to guarantee we successfully generated thumbnails for a video after the user upload, then a job queue makes sense as the <em>dispatch, commit, and retry</em> <em>mechanism</em> for this operation. The actual transcoding could be a FLAME call inside the job itself, so we decouple the ideas of durability and scaled execution.</p>

<p>On the other side, we have operations we don’t need durability for. Take the screencast above where the user hasn’t yet saved their video. Or an ML model execution where there’s no need to waste resources churning a prompt if the user has already left the app. In those cases, it doesn’t make sense to write to a durable store to pick up a job for work that will go right into the ether.</p>
<h2 id="pooling-for-elastic-scale"><a href="#pooling-for-elastic-scale" aria-label="Anchor"></a>Pooling for Elastic Scale</h2>
<p>With the Elixir implementation of FLAME, you define elastic pools of runners. This allows scale-to-zero behavior while also elastically scaling up FLAME servers with max concurrency limits.</p>

<p>For example, lets take a look at the <code>start/2</code> callback, which is the entry point of all Elixir applications. We can drop in a <code>FLAME.Pool</code> for video transcriptions and say we want it to scale to zero, boot a max of 10, and support 5 concurrent <code>ffmpeg</code> operations per runner:</p>
<div>
  <pre><code><span>def</span> <span>start</span><span>(</span><span>_type</span><span>,</span> <span>_args</span><span>)</span> <span>do</span>
  <span>flame_parent</span> <span>=</span> <span>FLAME</span><span>.</span><span>Parent</span><span>.</span><span>get</span><span>()</span>

  <span>children</span> <span>=</span> <span>[</span>
    <span>...</span><span>,</span>
    <span>MyApp</span><span>.</span><span>Repo</span><span>,</span>
    <span>{</span><span>FLAME</span><span>.</span><span>Pool</span><span>,</span>
      <span>name:</span> <span>Thumbs</span><span>.</span><span>FFMpegRunner</span><span>,</span>
      <span>min:</span> <span>0</span><span>,</span>
      <span>max:</span> <span>10</span><span>,</span>
      <span>max_concurrency:</span> <span>5</span><span>,</span>
      <span>idle_shutdown_after:</span> <span>30_000</span><span>},</span>
    <span>!flame_parent</span> <span>&amp;&amp;</span> <span>MyAppWeb</span><span>.</span><span>Endpoint</span>
  <span>]</span>
  <span>|&gt;</span> <span>Enum</span><span>.</span><span>filter</span><span>(</span><span>&amp;</span> <span>&amp;1</span><span>)</span>

  <span>opts</span> <span>=</span> <span>[</span><span>strategy:</span> <span>:one_for_one</span><span>,</span> <span>name:</span> <span>MyApp</span><span>.</span><span>Supervisor</span><span>]</span>
  <span>Supervisor</span><span>.</span><span>start_link</span><span>(</span><span>children</span><span>,</span> <span>opts</span><span>)</span>
<span>end</span>
</code></pre>
</div>

<p>We use the presence of a FLAME parent to conditionally start our Phoenix webserver when booting the app. There’s no reason to start a webserver if we aren’t serving web traffic. Note we leave other services like the database <code>MyApp.Repo</code> alone because we want to make use of those services inside FLAME runners.</p>

<p>Elixir’s supervised process approach to applications is uniquely great for turning these kinds of knobs.</p>

<p>We also set our pool to idle down after 30 seconds of no caller operations. This keeps our runners hot for a short while before discarding them. We could also pass a <code>min: 1</code> to always ensure at least one <code>ffmpeg</code> runner is hot and ready for work by the time our application is started.</p>
<h2 id="process-placement"><a href="#process-placement" aria-label="Anchor"></a>Process Placement</h2>
<p>In Elixir, stateful bits of our applications are built around the <em>process</em> primitive –&nbsp;lightweight greenthreads with message mailboxes. Wrapping our otherwise stateless app code in a synchronous <code>FLAME.call</code>‘s or async <code>FLAME.cast</code>’s works great, but what about the stateful parts of our app?</p>

<p><code>FLAME.place_child</code> exists to take an existing process specification in your Elixir app and start it on a FLAME runner instead of locally. You can use it anywhere you’d use <code>Task.Supervisor.start_child</code> , <code>DynamicSupervisor.start_child</code>, or similar interfaces. Just like <code>FLAME.call</code>, the process is run on an elastic pool and runners handle idle down when the process completes its work.</p>

<p>And like <code>FLAME.call</code>, it lets us take existing app code, change a single LOC, and continue shipping features.</p>

<p>Let’s walk thru the example from the screencast above. Imagine we want to generate video thumbnails for a video <em>as it is being uploaded</em>. Elixir and LiveView make this easy. We won’t cover all the code here, but you can view the <a href="https://github.com/fly-apps/thumbnail_generator/blob/main/lib/thumbs/thumbnail_generator.ex">full app implementation</a>.</p>

<p>Our first pass would be to write a LiveView upload writer that calls into a <code>ThumbnailGenerator</code>:</p>
<div>
  <pre><code><span>defmodule</span> <span>ThumbsWeb</span><span>.</span><span>ThumbnailUploadWriter</span> <span>do</span>
  <span>@behaviour</span> <span>Phoenix</span><span>.</span><span>LiveView</span><span>.</span><span>UploadWriter</span>

  <span>alias</span> <span>Thumbs</span><span>.</span><span>ThumbnailGenerator</span>

  <span>def</span> <span>init</span><span>(</span><span>opts</span><span>)</span> <span>do</span>
    <span>generator</span> <span>=</span> <span>ThumbnailGenerator</span><span>.</span><span>open</span><span>(</span><span>opts</span><span>)</span>
    <span>{</span><span>:ok</span><span>,</span> <span>%{</span><span>gen:</span> <span>generator</span><span>}}</span>
  <span>end</span>

  <span>def</span> <span>write_chunk</span><span>(</span><span>data</span><span>,</span> <span>state</span><span>)</span> <span>do</span>
    <span>ThumbnailGenerator</span><span>.</span><span>stream_chunk!</span><span>(</span><span>state</span><span>.</span><span>gen</span><span>,</span> <span>data</span><span>)</span>
    <span>{</span><span>:ok</span><span>,</span> <span>state</span><span>}</span>
  <span>end</span>

  <span>def</span> <span>meta</span><span>(</span><span>state</span><span>),</span> <span>do</span><span>:</span> <span>%{</span><span>gen:</span> <span>state</span><span>.</span><span>gen</span><span>}</span>

  <span>def</span> <span>close</span><span>(</span><span>state</span><span>,</span> <span>_reason</span><span>)</span> <span>do</span>
    <span>ThumbnailGenerator</span><span>.</span><span>close</span><span>(</span><span>state</span><span>.</span><span>gen</span><span>)</span>
    <span>{</span><span>:ok</span><span>,</span> <span>state</span><span>}</span>
  <span>end</span>
<span>end</span>
</code></pre>
</div>

<p>An upload writer is a behavior that simply ferries the uploaded chunks from the client into whatever we’d like to do with them. Here we have a <code>ThumbnailGenerator.open/1</code> which starts a process that communicates with an <code>ffmpeg</code> shell. Inside <code>ThumbnailGenerator.open/1</code>, we use regular elixir process primitives:</p>
<div>
  <pre><code>  <span># thumbnail_generator.ex</span>
  <span>def</span> <span>open</span><span>(</span><span>opts</span> <span>\\</span> <span>[])</span> <span>do</span>
    <span>Keyword</span><span>.</span><span>validate!</span><span>(</span><span>opts</span><span>,</span> <span>[</span><span>:timeout</span><span>,</span> <span>:caller</span><span>,</span> <span>:fps</span><span>])</span>
    <span>timeout</span> <span>=</span> <span>Keyword</span><span>.</span><span>get</span><span>(</span><span>opts</span><span>,</span> <span>:timeout</span><span>,</span> <span>5_000</span><span>)</span>
    <span>caller</span> <span>=</span> <span>Keyword</span><span>.</span><span>get</span><span>(</span><span>opts</span><span>,</span> <span>:caller</span><span>,</span> <span>self</span><span>())</span>
    <span>ref</span> <span>=</span> <span>make_ref</span><span>()</span>
    <span>parent</span> <span>=</span> <span>self</span><span>()</span>

    <span>spec</span> <span>=</span> <span>{</span><span>__MODULE__</span><span>,</span> <span>{</span><span>caller</span><span>,</span> <span>ref</span><span>,</span> <span>parent</span><span>,</span> <span>opts</span><span>}}</span>
    <span>{</span><span>:ok</span><span>,</span> <span>pid</span><span>}</span> <span>=</span> <span>DynamicSupervisor</span><span>.</span><span>start_child</span><span>(</span><span>@sup</span><span>,</span> <span>spec</span><span>)</span>

    <span>receive</span> <span>do</span>
      <span>{</span><span>^</span><span>ref</span><span>,</span> <span>%</span><span>ThumbnailGenerator</span><span>{}</span> <span>=</span> <span>gen</span><span>}</span> <span>-&gt;</span>
        <span>%</span><span>ThumbnailGenerator</span><span>{</span><span>gen</span> <span>|</span> <span>pid:</span> <span>pid</span><span>}</span>
    <span>after</span>
      <span>timeout</span> <span>-&gt;</span> <span>exit</span><span>(</span><span>:timeout</span><span>)</span>
    <span>end</span>
  <span>end</span>
</code></pre>
</div>

<p>The details aren’t super important here, except line 10 where we call <code>{:ok, pid} = DynamicSupervisor.start_child(@sup, spec)</code>, which starts a supervised<code>ThumbnailGenerator</code> process. The rest of the implementation simply ferries chunks as stdin into <code>ffmpeg</code> and parses png’s from stdout. Once a PNG delimiter is found in stdout, we send the <code>caller</code> process (our LiveView process) a message saying “hey, here’s an image”:</p>
<div>
  <pre><code><span># thumbnail_generator.ex</span>
<span>@png_begin</span> <span>&lt;&lt;</span><span>137</span><span>,</span> <span>80</span><span>,</span> <span>78</span><span>,</span> <span>71</span><span>,</span> <span>13</span><span>,</span> <span>10</span><span>,</span> <span>26</span><span>,</span> <span>10</span><span>&gt;&gt;</span>
<span>defp</span> <span>handle_stdout</span><span>(</span><span>state</span><span>,</span> <span>ref</span><span>,</span> <span>bin</span><span>)</span> <span>do</span>
  <span>%</span><span>ThumbnailGenerator</span><span>{</span><span>ref:</span> <span>^</span><span>ref</span><span>,</span> <span>caller:</span> <span>caller</span><span>}</span> <span>=</span> <span>state</span><span>.</span><span>gen</span>

  <span>case</span> <span>bin</span> <span>do</span>
    <span>&lt;&lt;</span><span>@png_begin</span><span>,</span> <span>_rest</span><span>::</span><span>binary</span><span>&gt;&gt;</span> <span>-&gt;</span>
      <span>if</span> <span>state</span><span>.</span><span>current</span> <span>do</span>
        <span>send</span><span>(</span><span>caller</span><span>,</span> <span>{</span><span>ref</span><span>,</span> <span>:image</span><span>,</span> <span>state</span><span>.</span><span>count</span><span>,</span> <span>encode</span><span>(</span><span>state</span><span>)})</span>
      <span>end</span>

      <span>%{</span><span>state</span> <span>|</span> <span>count:</span> <span>state</span><span>.</span><span>count</span> <span>+</span> <span>1</span><span>,</span> <span>current:</span> <span>[</span><span>bin</span><span>]}</span>

    <span>_</span> <span>-&gt;</span>
      <span>%{</span><span>state</span> <span>|</span> <span>current:</span> <span>[</span><span>bin</span> <span>|</span> <span>state</span><span>.</span><span>current</span><span>]}</span>
  <span>end</span>
<span>end</span>
</code></pre>
</div>

<p>The <code>caller</code> LiveView process then picks up the message in a <code>handle_info</code> callback and updates the UI:</p>
<div>
  <pre><code><span># thumb_live.ex</span>
<span>def</span> <span>handle_info</span><span>({</span><span>_ref</span><span>,</span> <span>:image</span><span>,</span> <span>_count</span><span>,</span> <span>encoded</span><span>},</span> <span>socket</span><span>)</span> <span>do</span>
  <span>%{</span><span>count:</span> <span>count</span><span>}</span> <span>=</span> <span>socket</span><span>.</span><span>assigns</span>

  <span>{</span><span>:noreply</span><span>,</span>
   <span>socket</span>
   <span>|&gt;</span> <span>assign</span><span>(</span><span>count:</span> <span>count</span> <span>+</span> <span>1</span><span>,</span> <span>message:</span> <span>"Generating (</span><span>#{</span><span>count</span> <span>+</span> <span>1</span><span>}</span><span>)"</span><span>)</span>
   <span>|&gt;</span> <span>stream_insert</span><span>(</span><span>:thumbs</span><span>,</span> <span>%{</span><span>id:</span> <span>count</span><span>,</span> <span>encoded:</span> <span>encoded</span><span>})}</span>
<span>end</span>
</code></pre>
</div>

<p>The <code>send(caller, {ref, :image, state.count, encode(state)}</code> is one magic part about Elixir. Everything is a process, and we can message those processes, regardless of their location in the cluster.</p>

<p>It’s like if every instantiation of an object in your favorite OO lang included a cluster-global unique identifier to work with methods on that object. The LiveView (a process) simply receives the image message and updates the UI with new images.</p>

<p>Now let’s head back over to our <code>ThumbnailGenerator.open/1</code> function and make this elastically scalable.</p>
<div>
  <pre><code><span>-    {:ok, pid} = DynamicSupervisor.start_child(@sup, spec)
</span><span>+    {:ok, pid} = FLAME.place_child(Thumbs.FFMpegRunner, spec)
</span></code></pre>
</div>

<p>That’s it! Because everything is a process and processes can live anywhere, it doesn’t matter what server our <code>ThumbnailGenerator</code> process lives on. It simply messages the caller with <code>send(caller, …)</code> and the messages are sent across the cluster if needed.</p>

<p>Once the process exits, either from an explicit close, after the upload is done, or from the end-user closing their browser tab, the FLAME server will note the exit and idle down if no other work is being done.</p>

<p>Check out the <a href="https://github.com/fly-apps/thumbnail_generator/blob/main/lib/thumbs/thumbnail_generator.ex">full implementation</a> if you’re interested.</p>
<h2 id="remote-monitoring"><a href="#remote-monitoring" aria-label="Anchor"></a>Remote Monitoring</h2>
<p>All this transient infrastructure needs failsafe mechanisms to avoid orphaning resources. If a parent spins up a runner, that runner must take care of idling itself down when no work is present and handle failsafe shutdowns if it can no longer contact the parent node.</p>

<p>Likewise, we need to shutdown runners when parents are rolled for new deploys as we must guarantee we’re running the same code across the cluster.</p>

<p>We also have active callers in many cases that are awaiting the result of work on runners that could go down for any reason.</p>

<p>There’s a lot to monitor here.</p>

<p>There’s also a number of failure modes that make this sound like a harrowing experience to implement. Fortunately Elixir has all the primitives to make this an easy task thanks to the Erlang VM. Namely, we get the following for free:</p>

<ul>
<li>Process monitoring and supervision –&nbsp;we know when things go bad. Whether on a node-local process, or one across the cluster
</li><li>Node monitoring – we know when nodes come up, and when nodes go away
</li><li>Declarative and controlled app startup and shutdown - we carefully control the startup and shutdown sequence of applications as a matter of course. This allows us to gracefully shutdown active runners when a fresh deploy is triggered, while giving them time to finish their work
</li></ul>

<p>We’ll cover the internal implementation details in a future deep-dive post. For now, feel free to poke around <a href="https://github.com/phoenixframework/flame">the flame source</a>.</p>
<h2 id="whats-next"><a href="#whats-next" aria-label="Anchor"></a>What’s Next</h2>
<p>We’re just getting started with the Elixir FLAME library, but it’s ready to try out now. In the future  look for more advance pool growth techniques, and deep dives into how the Elixir implementation works. You can also find me <a href="https://twitter.com/chris_mccord">@chris_mccord</a> to chat about implementing the FLAME pattern in your language of choice.</p>

<p>Happy coding!</p>

<p>–Chris</p>

          
        </section>
        <dl>
            <dt>
              Next post  ↑
            </dt>
            <dd>
              <a href="https://fly.io/blog/scaling-llm-ollama/">
                Scaling Large Language Models to zero with Ollama
              </a>
            </dd>
            <dt>
              Previous post  ↓
            </dt>
            <dd>
              <a href="https://fly.io/blog/the-risks-of-building-apps-on-chatgpt/">
                The risks of building apps on ChatGPT
              </a>
            </dd>
        </dl>
      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Open Letter to the Python Software Foundation (291 pts)]]></title>
            <link>https://pythonafrica.blogspot.com/2023/12/an-open-letter-to-python-software_5.html</link>
            <guid>38542330</guid>
            <pubDate>Wed, 06 Dec 2023 10:45:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pythonafrica.blogspot.com/2023/12/an-open-letter-to-python-software_5.html">https://pythonafrica.blogspot.com/2023/12/an-open-letter-to-python-software_5.html</a>, See on <a href="https://news.ycombinator.com/item?id=38542330">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-9208814100498476329">
<p>Dear PSF,</p><p>We, organisers in the pan-African Python community, would like to raise some concerns and frustrations that have been brought to a head by recent events.</p><p>We held the first-ever DjangoCon Africa in November, a flagship event for our community, and the first Africa-wide Python event since 2019’s PyCon Africa.</p><p>The PSF Board approved a grant of USD 9000 for the conference (six days, 200 people). The event was a significant success, and the PSF’s support was key to that.</p><h2>Delays in the grant award process&nbsp;</h2><p>As acknowledged in the PSF’s article, “<a href="https://pyfound.blogspot.com/2023/10/september-october-board-votes.html" target="_blank">September &amp; October Board Votes</a>”, there were however some problems leading up to the grant decision.</p><p>First, the Grants Working Group was unable to come to a consensus on the request, so the decision was passed to the PSF Board. The Board was also unable to come to a consensus in its September meeting. Finally in the Board’s October meeting, the grant was approved. That was just a few weeks before the event started, nearly three months after its submission.</p><h2>Effect of the delays</h2><p>We would like to share a statement from the organisers of DjangoCon Africa describing the effect of these delays on the event and on them personally.&nbsp;</p><blockquote><p>For a considerable period of time, we were in doubt about a significant portion of our expected conference budget - from the PSF, whom we expected to be a steadfast ally and backer.&nbsp;</p><p>During that time, we felt quite stuck - unable to make decisions. Our daily conversations became centred around anxious what-if calculations. At one point we had reduced our budget for catering to a total of 5 USD per person per day, for breakfast, lunch and refreshments - even in Zanzibar, this is an unfeasibly low figure. We jettisoned one item after another from our budget.</p><p>We were unable to make decisions about financial assistance, which in turn delayed our ability to make decisions about the programme (how could we invite a speaker whom we knew would require some funds to travel if we couldn’t provide the funds?). We watched as the air-fares we had expected to cover for many of those people rose.</p><p>We had to answer all the people who couldn’t understand why we hadn’t decided on their talk proposals and grant applications, or who wondered why an event starting so soon had not yet even published a programme of talks. “What are we supposed to say to these people?” became another anxious topic in our meetings.</p><p>We felt unable to advertise or promote the event, because we simply didn’t know what we could promise people.</p><p>Some of the people affected had applied for visas - incurring expense - well in advance, on our advice; they too were waiting to hear back from us.</p><p>Locally, we had caterers and other businesses waiting for deposits and confirmation of contracts. Some lost patience with us. The local PyCon Tanzania organisation bore the brunt of this.&nbsp;</p><p>The organisation of DjangoCon Africa must have looked lazy, or incompetent, or worse, to someone looking at it from the outside.</p><p>The delay in a decision on funding from the PSF also made it harder for us to approach other organisations for funds - “Is the PSF sponsoring DjangoCon Africa? Why not?”.</p><p>It’s hard to describe the embarrassment we felt sometimes.</p><p>We had sleepless nights with worry - literally, not figuratively. More than one of us confided in another that we wished we had never started the project.</p><p>We started a fundraising campaign on GoFundMe to help cover the cost of financial assistance. It was a comfort to know that members of the international Python/Django community would stand up to support us, but the pleasure and gratitude we felt about that was overlaid with a feeling of humiliation that once again, a major African open-source software event had been obliged to publicly extend a begging-bowl.</p><p>At one point, gaps in our funding meant that the organisers faced a personal liability of almost USD 10,000 - funds actually spent, or committed, to make the event possible. &nbsp;</p><p>For any volunteer conference organiser, the weeks in the run-up to an event are full of hard work. This experience went far beyond that. Much of the pride and joy of staging DjangoCon Africa was sucked out of it for us.&nbsp;</p><p>Eventually, we received the grant funding we had applied for, though even this seemed to come with a humiliation: it happened after a white European spoke up publicly on behalf of the African Python community.</p><p>By the time we were able to start taking care of travellers who needed financial assistance, many of the air tickets had gone up significantly in price. Amongst the hard choices we had to make: one of our own organisers - a student, who has worked tirelessly in multiple events - was unable to attend because we could not afford to pay for her travel.</p><p>The organisers personally contributed well over USD 4000 to make the event possible in the form it took - funds contributed to a cause that we believe in, but it is not right that volunteer organisers of community events should be forced to make such choices.&nbsp;</p><p>We are genuinely grateful for the support we received from the PSF. The event was a success, and we are proud of what we achieved, but we remain perplexed and hurt by the problems we faced, and how our grant request was treated.</p></blockquote><h2>Problems within the PSF?</h2><p>It’s not clear to the wider African Python community why events unfolded in this way, though we are aware of some things that we have found very troubling.</p><p>We know that there are some extraordinary attitudes at work within the PSF. A PSF Board member once openly expressed the opinion that Anglo cultures always seem to be the ones that take the moral lead around the world, leaving others to follow their example. From any non-western perspective, this is an astounding idea to receive.</p><p>In the case of DjangoCon Africa, the first public response to our event on Mastodon was a negative response from a PSF Director, that in effect, cast doubt on the whole idea of a DjangoCon in Tanzania.</p><p>That’s not a solitary episode. We understand that (notwithstanding the PSF’s ambition to support Python in Africa) a PSF Director has consistently spoken out against funding for African events, over a period of years.&nbsp;</p><p>Our grant request was handled by the PSF’s Grants Working Group. We understand that one member of this group was able effectively to stall its decision-making long enough that the grant request had to be passed to the PSF Board.</p><p>At the PSF Board meeting in September 2023, a board member strategically used an abstention to ensure that a resolution to support our request could not pass. (Under the PSF rules, had they voted against the resolution, it would have passed 4-1. In the circumstances, other abstentions for different reasons - including one person who was required to abstain, as an organiser of &nbsp;DjangoCon Africa - meant that the resolution could not pass.) We are genuinely shocked by this. It’s one thing for a PSF Board member to vote against something they don’t believe in. It is quite another that someone has been able to weaponise the PSF’s voting system against an African event.</p><p>We are deeply troubled that such behaviours and values are actively at work inside the PSF. As an organisation, the PSF (and its Board and Working Groups and their processes) should be robust enough to stand up to individual prejudices, and not allow decision-making and deliberation to be derailed by individuals, however influential.</p><h2>The PSF and marginalised and at-risk groups</h2><p>We understand that the argument against support for DjangoCon Africa was that the host country, Tanzania, is not a safe place for the LGBTQIA+ community.</p><p>The PSF represents a global community, and has for years upheld high standards of inclusion and protection, paying special attention to the needs of those in marginalised and at-risk groups. Python community events around the world are effective safe spaces, that give strength to people who do not always find guarantees of safety elsewhere.</p><p>It is therefore especially shocking to have observed an attempt, coming from within the PSF, to pit the well-being and interests of two different excluded groups against each other, as if somehow the interests of members of the LGBTQIA+ community and of Africans are mutually exclusive.</p><p>Many questions can be asked about this reasoning.</p><p>What counts as “safety”? Which places in the world are truly safe for LGBTQIA+ community? How much of a city, or state, or country needs to be LGBTQIA+ hostile for the whole of it to be declared unworthy of PSF support? What does the PSF have to say to LGBTQIA+ community members in such locations? Are the LGBTQIA+ communities who are worthy of the PSF’s consideration only those who live in western countries, or do others count too? What does the PSF have to say to Python community organisers around the world who assert the community’s standards of inclusion, even in countries where it takes an act of bravery to do so?</p><p>And we would like to ask: when have questions been raised to check on whether western events present potential safety risks to non-western attendees, and when have non-western people been asked for their experiences?</p><p>All across the world, including the west, there are countries and places that are genuinely unsafe for members of particular groups, on the basis of their religion, ethnicity, language, gender, sexuality, nationality and other characteristics. Some of these risks may be obvious to westerners, or native English speakers, or men, and some of them may not. Simplistic judgements made from narrow perspectives will not enhance the safety of anyone in our community.</p><h2>Risk and the law</h2><p>The PSF and its directors quite correctly also observe the laws that apply to them. Yet we have witnessed discussions in which it has been proposed that volunteer organisers take public stances in their own countries that are not just contentious or socially unacceptable, but would actually violate local laws.&nbsp;</p><p>In one recent example, voices on the PSF Board were demanding that a condition of funding for a particular PyCon be the formal adoption of a “human rights plan” - a measure that would pose a significant legal and personal risk to its organisers.</p><p>The entitlement and assumption of cultural superiority embodied in these ideas are absurd and offensive.&nbsp;</p><h2>Guidance and consideration for non-western Python events grant awards</h2><p>At a meeting earlier this year, the PSF expressed concern that barely 16% of grants go to African communities.&nbsp;</p><p>At the same time, the perception within some African Python communities is that the PSF is less likely to award a grant to an African event, or will scrutinise it more harshly, or take longer to make an award.</p><p>For example, in 2019 and 2020 one Ugandan Python community made two grant requests that we understand received literally no response. In 2022, another grant request finally received attention from the working group when - with the event coming up in a matter of days - one of the PSF Directors connected to the community raised the issue with PSF staff, and a vote was initiated immediately.&nbsp;</p><p>This can be contrasted with the way a grant request for a European event was handled, at around the same time; the European request was made later, and dealt with sooner.</p><p>Inconsistency, lack of transparency and lack of clarity around expectations serve to undermine trust and confidence. The general perception within the Ugandan Python community is now that their events will only be given consideration if a PSF Director happens to take a personal interest in it.</p><p>In fact, other African organisers have reported timely responses and good communication from the PSF, so what is happening here?&nbsp;</p><p>Do some African grant requests lack quality or detail, because organisers failed to understand what was required? Are there enough people in the PSF with an adequate understanding of the challenges faced by non-western events? ​Is there a pattern where weaknesses in a grant request made from some regions in the world are given the benefit of the doubt, while others are treated less favourably?&nbsp;</p><p>We simply don’t know, and there could be a whole range of explanations. Whatever the underlying reasons, we need to understand and work together to address them, because the effects are harmful.&nbsp;</p><h2>Our requests to the Python Software Foundation&nbsp;</h2><h3>Transparency</h3><p>We request that the PSF undertake and publish a review of actual grant applications, to determine whether there indeed are differences in grant responsiveness, approval times, rejection rates and so on in response to requests from different regions.&nbsp;</p><p>We would like the PSF to publish clear expectations of timelines for handling grant requests, and for each final decision to be accompanied by a report showing how the case was actually handled.</p><h3>Guidance and feedback</h3><p>Organisers, and especially those operating without the benefit of long-standing networks of knowledge and shared expectations, need more guidance, and feedback they can act upon, especially in the case when a grant is rejected.&nbsp;</p><p>We ask that the PSF commits to developing - in collaboration with organisers, especially those in non-western regions - further materials and guidance to help organisers put in the best possible requests for funding. This could include a more proactive approach to working with those organisers to help them understand the PSF’s expectations and standards.</p><p>We request that the PSF institute a practice of providing clear feedback to grant applicants, to help improve and motivate subsequent applications. In cases of delay or doubt, we would like a practice of prompt, direct engagement with organisers to help clarify.</p><h3>Understanding of global needs</h3><p>We ask that the PSF as an organisation commits to a better understanding of global diversity and the realities, needs and challenges of non-western events and organisers.</p><p>This includes an understanding of financial realities. For example, the organisers of African events face the combined difficulties of lesser commercial sponsorship prospects, the expense of intra-African travel, vast geographical distances and so on. We need the PSF to understand these realities in its decision-making about financial awards to events.</p><h3>The law and marginalised groups</h3><p>We request that the PSF undertake a formal review of policies and bylaws, that incorporates expert legal advice and takes full account of the realities of laws and legal regimes across the world that apply to volunteer Python community organisers.</p><p>We recognise that all across the world, Python events are proposed in places where laws and practices mean that the rights of some individuals will be in jeopardy. We would like the PSF to recognise, formally and in its actual practices, that this includes the west, and that it is not only non-western events that should be subjected to critical scrutiny over this.</p><p>We also ask the PSF to adopt a constructive stance that requires all local organisers, wherever they may be, to consider the safeguarding of marginalised groups, and actively helps them improve safety, without ever demanding that volunteers be willing to violate local law or place themselves at risk while doing unpaid work on behalf of the PSF.&nbsp;</p><h2>Progress, prejudice and confidence</h2><p>In the past ten years, Python in Africa has developed with remarkable speed and success. In 2014 there was just one African PyCon, in South Africa. Since then PyCons and other events have been held all over the continent, and the communities behind them have grown in size, confidence, expertise and influence.</p><p>We can trace the introduction of Python teaching in universities across Africa and its spread across multiple commercial and non-commercial sectors to our work.</p><p>We have been generously supported, financially and morally, by the Python Software Foundation. Leaders like Ewa Jodlowska and Naomi Ceder have been part of that growth due to intentional support for our communities.</p><p>This has been a story of growth, motivation and courage.</p><p>More recent experiences have left us feeling hurt and angry. We hear voices, openly and confidently raised within the PSF, that denigrate us and our communities, that dismiss our experiences, that doubt our values, and harm us materially.</p><p>Our confidence in the PSF, and the confidence of many other people in our communities, has been shaken. Our motivation has taken some hammer blows. The work of the last decade risks being set back.</p><h2>Constructive collaboration</h2><p>We want to work with the PSF on everything addressed in this letter, in the spirit of constructive collaboration. We are willing to put our energies into building better practices and understanding. We want to be part of a solution to the concerns.</p><p>We ask the PSF to recognise our concerns, and not just to take them seriously, but to commit to working with the African Python community to address them, so that we do it together. &nbsp;</p><p>Sincerely,</p><p><b>Python Communities in Ghana, Namibia, Nigeria, Uganda, Tanzania, Mozambique &nbsp;South Africa, and Zimbabwe.</b></p><p>Abigail Mesrenyame Dogbe (Python Ghana)<br>Aisha Bello (Python Nigeria)<br>Anna Makarudze (Python Zimbabwe)<br>Chukwudi Nwachukwu (Python Nigeria)<br>Daniele Procida (Python Namibia)<br>Eusebio Simango (Python Mozambique)<br>Jessica Upani (Python Namibia)<br>Joannah Nanjekye (Python Uganda)<br>Julius Moshiro (Python Tanzania)<br>Mannie Young (Python Ghana)<br>Marlene Mhangami (Python Zimbabwe)<br>Noah Maina (Python Tanzania)<br>Sheena O’Connell (Python South Africa)</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[World's largest tokamak fusion reactor powers up (125 pts)]]></title>
            <link>https://newatlas.com/energy/worlds-largest-tokamak-fusion-reactor-powers-up/</link>
            <guid>38541451</guid>
            <pubDate>Wed, 06 Dec 2023 07:53:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/energy/worlds-largest-tokamak-fusion-reactor-powers-up/">https://newatlas.com/energy/worlds-largest-tokamak-fusion-reactor-powers-up/</a>, See on <a href="https://news.ycombinator.com/item?id=38541451">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                        <article>

                            
    


                            
    



                            

                            
                                
                                
                                    <div><ps-carousel data-direction="vertical" data-scroll-indicator="1" data-carousel-has-aside="">
    <template>
        <div class="FullscreenCarousel-slide" aria-hidden="true" tabindex="-1" style="position: absolute;" data-gallery-slide-index="inline">
            <div class="CarouselSlide" itemprop="image" itemtype="http://schema.org/ImageObject">
                <div class="CarouselSlide-media">
                    
                    <figure>
                        <img class="Image lazy" alt="Inline image" src="data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 16 9%22 /%3E">
                    </figure>
                </div>
                <div class="CarouselSlide-info">
                    <div class="CarouselSlide-numbers">
                        <span class="CarouselSlide-currentSlide"></span>/<span class="CarouselSlide-slidesLength"></span>
                    </div>
                    <div class="CarouselSlide-caption">
                        <div class="CarouselSlide-infoDescription" itemprop="caption"></div>
                        <div class="CarouselSlide-infoAttribution" itemprop="author"></div>
                    </div>
                </div>
            </div>
        </div>
    </template>

    

    

    <div itemprop="image" itemtype="http://schema.org/ImageObject"><p>JT-60SA has been under development since 1970</p><p>European Commission</p></div>

    <div itemprop="image" itemtype="http://schema.org/ImageObject" id="slide-1" data-gallery-slide-index="0">
        
            <p><span>1</span>/<span>1</span>
            </p>
        
        
            <div><p>JT-60SA has been under development since 1970</p><p>European Commission</p></div>
        
    </div>

    
        
    

    
</ps-carousel></div>
                                

                                
                            


                            


                            <div><p>The world's largest and most advanced tokamak fusion reactor has gone online as the EU/Japanese 370-tonne JT-60SA reactor was fired up for the first time during an inauguration ceremony in Ibaraki Prefecture, Japan.</p><p>First conceived by Soviet scientists in the 1950s, tokamaks are toroidal reactors that are one of the leading contenders to become the first commercially viable fusion power plants. The name is a Russian acronym for Toroidal Chamber with Magnetic Coils and consists of a large doughnut-shaped chamber surrounded by magnetic coils that compress a plasma made of hydrogen isotopes until it reaches pressures and temperatures that are only found in the interior of the Sun to initiate fusion.</p><p>In concept, it's a simple machine and achieving fusion is relatively easy, but in practice it's extremely difficult to build a reactor that can maintain a sustained fusion reaction that generates more power than is fed into it. The Japan Torus-60 (JT-60) project has been running since 1970 and the JT-60SA is that latest and biggest iteration.</p><p>The JT-60SA is currently a joint project by the EU and Japan, with participation by Britain, which signed a separate agreement after leaving the Union. The original reactor was upgraded several times as technology evolved, resulting in a complete disassembly and reassembly in 2013, with work finishing in 2020. Unfortunately, this was followed by a massive electrical short in 2021 that necessitated two years of repairs.</p><p>The initiation of operations for JT-60SA was inaugurated on December 1, 2023 by the EU’s Commissioner for Energy Kadri Simson and Japan’s Minister of Education, Culture, Sports, Science and Technology (MEXT) Masahito Moriyama in a formal ceremony. Though the upgraded reactor still isn't anywhere near to being a practical power generator, it will be used to overcome many outstanding problems as well as testing materials and procedures that will be needed for commercial stations.</p><p>For 75 years we've been told that fusion power was only 25 years away and billions of dollars have been spent to make it practical. However, since successful fusion power would provide humanity with unlimited clean power forever, a little patience might be in order.</p><p>The video below shows the upgrade of the JT-60SA.</p><div data-video-disable-history="" data-align-center="">
    
        <p><ps-youtubeplayer data-video-player="" data-player-id="f458ac38c71f54ff8a7266a4dfd187990" data-video-id="Eno5410tO0g" data-video-title="JT 60SA組立動画">

    <iframe id="YouTubeVideoPlayer-f458ac38c71f54ff8a7266a4dfd187990" role="application" title="YouTube embedded video player" allowfullscreen="" loading="lazy" src="https://www.youtube.com/embed/Eno5410tO0g?enablejsapi=1"></iframe>
</ps-youtubeplayer>
</p>
    
    
        <p>JT60-SA</p>
    
</div><p>Source: <a href="https://energy.ec.europa.eu/news/eu-and-japan-celebrate-start-operations-jt-60sa-fusion-reactor-and-reaffirm-close-cooperation-fusion-2023-12-01_en" target="_blank" data-cms-ai="0">European Commission</a></p></div>

                            
                                    
                            
                        </article>

                        

                        

<div>
<h2>Tags</h2>
    
</div>



                        
                            
                        

                        
    <div>
        
        
        <p><a aria-label="David Szondy" href="https://newatlas.com/author/david-szondy/" data-cms-ai="0">
            <img alt="David Szondy" loading="lazy" src="https://assets.newatlas.com/dims4/default/2737b65/2147483647/strip/true/crop/500x500+0+0/resize/100x100!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F58%2Feb%2Fc0a71279440998ce939ec841c996%2Fd-szondy-use1.png" srcset="https://assets.newatlas.com/dims4/default/f7e82ae/2147483647/strip/true/crop/500x500+0+0/resize/80x80!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F58%2Feb%2Fc0a71279440998ce939ec841c996%2Fd-szondy-use1.png 80w,https://assets.newatlas.com/dims4/default/9f95b82/2147483647/strip/true/crop/500x500+0+0/resize/160x160!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F58%2Feb%2Fc0a71279440998ce939ec841c996%2Fd-szondy-use1.png 160w" sizes="(min-width: 768px) 60px, 80px" width="100" height="100">
            </a>
        </p>
        
        

        <div>
            
                
            
            <p>
                
                    David Szondy is a playwright, author and journalist based in Seattle, Washington. A retired field archaeologist and university lecturer, he has a background in the history of science, technology, and medicine with a particular emphasis on aerospace, military, and cybernetic subjects. In addition, he is the author of four award-winning plays, a novel, reviews, and a plethora of scholarly works ranging from industrial archaeology to law. David has worked as a feature writer for many international magazines and has been a feature writer for New Atlas since 2011.
                
            </p>
        </div>
    </div>



                        
    


                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Outlook Blocking All Email from Tutanota.com Domain as Spam (210 pts)]]></title>
            <link>https://tuta.com/blog/posts/outlook-falsely-marks-tutanota-emails-as-junk</link>
            <guid>38541355</guid>
            <pubDate>Wed, 06 Dec 2023 07:39:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tuta.com/blog/posts/outlook-falsely-marks-tutanota-emails-as-junk">https://tuta.com/blog/posts/outlook-falsely-marks-tutanota-emails-as-junk</a>, See on <a href="https://news.ycombinator.com/item?id=38541355">Hacker News</a></p>
Couldn't get https://tuta.com/blog/posts/outlook-falsely-marks-tutanota-emails-as-junk: Error: Request failed with status code 404]]></description>
        </item>
        <item>
            <title><![CDATA[Why is Jepsen written in Clojure? (409 pts)]]></title>
            <link>https://aphyr.com/posts/367-why-is-jepsen-written-in-clojure</link>
            <guid>38540761</guid>
            <pubDate>Wed, 06 Dec 2023 05:32:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aphyr.com/posts/367-why-is-jepsen-written-in-clojure">https://aphyr.com/posts/367-why-is-jepsen-written-in-clojure</a>, See on <a href="https://news.ycombinator.com/item?id=38540761">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>People keep asking why <a href="https://jepsen.io/">Jepsen</a> is written in <a href="https://clojure.org/">Clojure</a>, so I figure it’s worth having a referencable answer. I’ve programmed in something like twenty languages. Why choose a Weird Lisp?</p>
<p>Jepsen is built for testing concurrent systems–mostly databases. Because it tests concurrent systems, the language itself needs good support for concurrency. Clojure’s immutable, persistent data structures make it easier to write correct concurrent programs, and the language and runtime have excellent concurrency support: real threads, promises, futures, atoms, locks, queues, cyclic barriers, all of java.util.concurrent, etc. I also considered languages (like Haskell) with more rigorous control over side effects, but decided that Clojure’s less-dogmatic approach was preferable.</p>
<p>Because Jepsen tests databases, it needs broad client support. Almost every database has a JVM client, typically written in Java, and Clojure has decent Java interop.</p>
<p>Because testing is experimental work, I needed a language which was concise, adaptable, and well-suited to prototyping. Clojure is terse, and its syntactic flexibility–in particular, its macro system–work well for that. In particular the threading macros make chained transformations readable, and macros enable re-usable error handling and easy control of resource scopes. The Clojure REPL is really handy for exploring the data a test run produces.</p>
<p>Tests involve representing, transforming, and inspecting complex, nested data structures. Clojure’s data structures and standard library functions are possibly the best I’ve ever seen. I also print a lot of structures to the console and files: Clojure’s data syntax (EDN) is fantastic for this.</p>
<p>Because tests involve manipulating a decent, but not huge, chunk of data, I needed a language with “good enough” performance. Clojure’s certainly not the fastest language out there, but idiomatic Clojure is usually within an order of magnitude or two of Java, and I can shave off the difference where critical. The JVM has excellent profiling tools, and these work well with Clojure.</p>
<p>Jepsen’s (gosh) about a decade old now: I wanted a language with a mature core and emphasis on stability. Clojure is remarkably stable, both in terms of JVM target and the language itself. Libraries don’t “rot” anywhere near as quickly as in Scala or Ruby.</p>
<p>Clojure does have significant drawbacks. It has a small engineering community and no (broadly-accepted, successful) static typing system. Both of these would constrain a large team, but Jepsen’s maintained and used by only 1-3 people at a time. Working with JVM primitives can be frustrating without dropping to Java; I do this on occasion. Some aspects of the polymorphism system are lacking, but these can be worked around with libraries. The error messages are terrible. I have no apologetics for this. ;-)</p>
<p>I prototyped Jepsen in a few different languages before settling on Clojure. A decade in, I think it was a pretty good tradeoff.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JSONB Has Landed in SQLite (586 pts)]]></title>
            <link>https://sqlite.org/forum/forumpost/fa6f64e3dc1a5d97</link>
            <guid>38540421</guid>
            <pubDate>Wed, 06 Dec 2023 04:16:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sqlite.org/forum/forumpost/fa6f64e3dc1a5d97">https://sqlite.org/forum/forumpost/fa6f64e3dc1a5d97</a>, See on <a href="https://news.ycombinator.com/item?id=38540421">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>JSONB is a rewrite of the <a href="https://sqlite.org/draft/json1.html">SQLite JSON functions</a>
that, depending on usage patterns, could be several times faster
than the original JSON functions.  This enhancement has now
<a href="https://sqlite.org/src/info/7f0c79b94e8f55e5">landed on trunk</a>.</p>

<p>Developers who use JSON heavily in their applications are encouraged
to download a <a href="https://sqlite.org/download.html">pre-release snapshot</a>
and give the new code a try.</p>

<h2>How Is This Different?</h2>
<p>Functions that deal with text JSON use a three-step process:</p>

<ol>
<li><p>Parse the text JSON into an internal binary format that is
  more accessible to C-code.</p></li>
<li><p>Carry out the requested operation.  Maybe this is looking up
  a field in an object, or perhaps it is modifying the JSON in
  some way.</p></li>
<li><p>If the processing involved changing the JSON, convert the
  internal binary format back into 
  <a href="https://www.rfc-editor.org/rfc/rfc8259">RFC-8279</a> JSON text for
  output and/or storage.</p></li>
</ol>

<p>Step 2 is the essential operation that you want to accomplish.  Steps
1 and 3 are just overhead.</p>

<p>Historically, SQLite used an internal binary
representation of JSON that involved lots of pointers.  This fits
will into C programs, but it is difficult to serialize.
The JSONB rewrite changes the internal-use binary representation of
JSON into a contiguous byte array that can read or written as an SQL BLOB.
This allows the internal-use representation of JSON to potentially be
saved to the database, in place of JSON text, eliminating the overhead
of steps 1 and 3.</p>

<h2>What has changed?</h2>
<p>All legacy functionality is preserved.  The only change has been to add
new capabilities.</p>

<p>Any JSON function that accepts JSON text as an input will now also accept
JSONB binary content for that same parameter.  You do not have to tell the
function if it getting text or binary data.  It figures that out for itself.</p>

<p>JSON functions that output JSON now come in two versions.  The historical
"<tt>json_</tt>" functions works as before.  But there is now a corresponding
"<tt>jsonb_</tt>" function that returns JSONB rather than text JSON, thus
omitting step 3 in the normal processing.</p>

<p>If you don't make any changes to your application, everything should
continue to work as it always has, though perhaps slightly (1%) faster.</p>

<p>But if you modify your application to start storing JSONB instead of text
JSON, you might see a 3-times performance improvement, at least for the
JSON-intensive operations.  JSONB is also slightly smaller than text JSON
in most cases (about 5% or 10% smaller) so you might also see a modest
reduction in your database size if you use a lot of JSON.</p>

<h2>Migrating</h2>
<p>Note that all functions accept both text JSON and JSONB.  So to start using
JSONB, you do <u>not</u> have to modify your database files to convert
legacy text JSON into JSONB.  Just start writing out JSONB for new entries.
The old entries will continue to work.  The new entries will just work
faster.</p>

<p>Or, if you do want to convert all your legacy data to JSONB, you can just
run an update operation like:</p>

<blockquote>
<pre><code>UPDATE bigtable SET jsonColumn = jsonb(jsonColumn);
</code></pre></blockquote>

<h2>Please provide comments</h2>
<p>If you find this enhancement useful, or if you try it out and see performance
regressions or bugs, please let us know.  Leave a follow-up post here, or
contact me directly at drh at sqlite dot org.</p>

<p>The current plan is to release the JSONB enhancement in the next
major release of SQLite - version 3.45.0.  That will probably occur
in a month or two.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A new quantum algorithm for classical mechanics with an exponential speedup (119 pts)]]></title>
            <link>https://blog.research.google/2023/12/a-new-quantum-algorithm-for-classical.html</link>
            <guid>38539374</guid>
            <pubDate>Wed, 06 Dec 2023 01:23:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.research.google/2023/12/a-new-quantum-algorithm-for-classical.html">https://blog.research.google/2023/12/a-new-quantum-algorithm-for-classical.html</a>, See on <a href="https://news.ycombinator.com/item?id=38539374">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-3665865722098768988">
<p><span>Posted by Robin Kothari and Rolando Somma, Research Scientists, Google Research, Quantum AI Team</span>

</p><p>
Quantum computers promise to solve some problems exponentially faster than classical computers, but there are only a handful of examples with such a dramatic speedup, such as <a href="https://en.wikipedia.org/wiki/Shor%27s_algorithm">Shor’s factoring algorithm</a> and <a href="https://blog.research.google/2023/10/developing-industrial-use-cases-for.html">quantum simulation</a>. Of those few examples, the majority of them involve simulating physical systems that are inherently quantum mechanical — a natural application for quantum computers. But what about simulating systems that are not inherently quantum? Can quantum computers offer an exponential advantage for this?
</p>
<p>
In “<a href="https://link.aps.org/doi/10.1103/PhysRevX.13.041041">Exponential quantum speedup in simulating coupled classical oscillators</a>”, published in <a href="https://journals.aps.org/prx/">Physical Review X</a> (PRX) and presented at the <a href="https://focs.computer.org/2023/">Symposium on Foundations of Computer Science</a> (FOCS 2023), we report on the discovery of a new quantum algorithm that offers an exponential advantage for simulating coupled <a href="https://en.wikipedia.org/wiki/Harmonic_oscillator">classical harmonic oscillators</a>. These are some of the most fundamental, ubiquitous systems in nature and can describe the physics of countless natural systems, from electrical circuits to molecular vibrations to the mechanics of bridges. In collaboration with Dominic Berry of Macquarie University and Nathan Wiebe of the University of Toronto, we found a mapping that can transform any system involving coupled oscillators into a problem describing the time evolution of a quantum system. Given certain constraints, this problem can be solved with a quantum computer exponentially faster than it can with a classical computer. Further, we use this mapping to prove that any problem efficiently solvable by a quantum algorithm can be recast as a problem involving a network of coupled oscillators, albeit exponentially many of them. In addition to unlocking previously unknown applications of quantum computers, this result provides a new method of designing new quantum algorithms by reasoning purely about classical systems.
</p>
<br> 

<h2>Simulating coupled oscillators</h2>


<p>
The systems we consider consist of classical harmonic oscillators. An example of a single harmonic oscillator is a mass (such as a ball) attached to a spring. If you displace the mass from its rest position, then the spring will induce a restoring force, pushing or pulling the mass in the opposite direction. This restoring force causes the mass to oscillate back and forth.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE19xRL5DLh42HtY6Q-mDXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/s359/oscillator.gif"><img data-original-height="116" data-original-width="359" height="129" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhQ5JsRhfC0tifTDZI7a_giA0KOA1frMxpSGkZWSQJv5VRngf3bDySeJDCOhfS5dyM67u1JLI8yMVYrD5F9oY4IvMcWE19xRL5DLh42HtY6Q-mDXrO1uIqnjQ3IATUW8IQEiztKGpRWMUTvz8YCsPoaKfNjzPPFbXia5-9jIdT0ZWd4gPYutNtPJ-b__8Xb/w400-h129/oscillator.gif" width="400"></a></td></tr><tr><td>A simple example of a harmonic oscillator is a mass connected to a wall by a spring. [Image Source: <a href="https://commons.wikimedia.org/wiki/File:Simple_harmonic_oscillator.gif">Wikimedia</a>]</td></tr></tbody></table>

<p>
Now consider <em>coupled </em>harmonic oscillators, where <em>multiple</em> masses are attached to one another through springs. Displace one mass, and it will induce a wave of oscillations to pulse through the system. As one might expect, simulating the oscillations of a large number of masses on a classical computer gets increasingly difficult.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IKfNVVBX3Z2Dm8sVxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkUKE-mFxJ3e9sdM-ct3lU81Gs1/s1129/image4.png"><img data-original-height="832" data-original-width="1129" height="295" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-jWiEDm-tYXeXlLLq6wNXEHUuxHiNg-vwloUFyzE2GPLbekSHMrfE6Qupy4QjI3Ca6zV97hhiZei1rMb-zMXnKV6IKfNVVBX3Z2Dm8sVxDM5llRXfID3xON38bLXBhHEvlNF28xitJWdERmhHuagjYli4yMT17YTzmDkUKE-mFxJ3e9sdM-ct3lU81Gs1/w400-h295/image4.png" width="400"></a></td></tr><tr><td>An example system of masses connected by springs that can be simulated with the quantum algorithm.</td></tr></tbody></table>

<p>
To enable the simulation of a large number of coupled harmonic oscillators, we came up with a mapping that encodes the positions and velocities of all masses and springs into the quantum wavefunction of a system of qubits. Since the number of parameters describing the wavefunction of a system of qubits grows exponentially with the number of qubits, we can encode the information of <em>N</em> balls into a quantum mechanical system of only about log(<em>N</em>) qubits. As long as there is a compact description of the system (i.e., the properties of the masses and the springs), we can evolve the wavefunction to learn coordinates of the balls and springs at a later time with far fewer resources than if we had used a naïve classical approach to simulate the balls and springs.
</p>
<p>
We showed that a certain class of coupled-classical oscillator systems can be efficiently simulated on a quantum computer. But this alone does not rule out the possibility that there exists some as-yet-unknown clever classical algorithm that is similarly efficient in its use of resources. To show that our quantum algorithm achieves an exponential speedup over <em>any</em> possible classical algorithm, we provide two additional pieces of evidence.
</p>
<br> 

<h2>The glued-trees problem and the quantum oracle</h2>


<p>
For the first piece of evidence, we use our mapping to show that the quantum algorithm can efficiently solve a famous problem about graphs known to be difficult to solve classically, called the <a href="https://arxiv.org/abs/quant-ph/0209131">glued-trees problem</a>. The problem takes two branching trees — a graph whose nodes each branch to two more nodes, resembling the branching paths of a tree — and glues their branches together through a random set of edges, as shown in the figure below.
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6mVvFQ-twhlL3EjLKDHKHDw0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s930/image2.png"><img data-original-height="556" data-original-width="930" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjoHV_EgsCy3f3fid2P29Lyq00CQtPBiV9cc2A2oL6RoX0W3oawha617NRm7a6J9fdUPG7z55MuHKnko5eDCRZ4tb6mVvFQ-twhlL3EjLKDHKHDw0-69-0ESWovOsDTbkAfDBUwRiYa0U8rfHeGOB_JwfcWIXQyJYnfmRjI5E7ygfZz-l5w1N4Kisle8WeV/s16000/image2.png"></a></td></tr><tr><td>A visual representation of the glued trees problem. Here we start at the node labeled ENTRANCE and are allowed to locally explore the graph, which is obtained by randomly gluing together two binary trees. The goal is to find the node labeled EXIT.</td></tr></tbody></table>

<p>
The goal of the glued-trees problem is to find the exit node — the “root” of the second tree — as efficiently as possible. But the exact configuration of the nodes and edges of the glued trees are initially hidden from us. To learn about the system, we must query an <a href="https://en.wikipedia.org/wiki/Oracle_machine">oracle</a>, which can answer specific questions about the setup. This oracle allows us to explore the trees, but only locally. Decades ago, <a href="https://doi.org/10.1145/780542.780552">it was shown</a> that the number of queries required to find the exit node on a classical computer is proportional to a polynomial factor of <em>N</em>, the total number of nodes. 
</p>
<p>
But recasting this as a problem with balls and springs, we can imagine each node as a ball and each connection between two nodes as a spring. Pluck the entrance node (the root of the first tree), and the oscillations will pulse through the trees. It only takes a time that scales with the <em>depth</em> of the tree — which is exponentially smaller than <em>N</em> — to reach the exit node. So, by mapping the glued-trees ball-and-spring system to a quantum system and evolving it for that time, we can detect the vibrations of the exit node and determine it exponentially faster than we could using a classical computer.
</p>
<br> 

<h2>BQP-completeness</h2>


<p>
The second and strongest piece of evidence that our algorithm is exponentially more efficient than any possible classical algorithm is revealed by examination of the set of problems a quantum computer can solve efficiently (i.e., solvable in <a href="https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time">polynomial time</a>), referred to as <a href="https://en.wikipedia.org/wiki/BQP">bounded-error quantum polynomial time</a> or BQP. The hardest problems in BQP are called “BQP-complete”. 
</p>
<p>
While it is generally accepted that there exist some problems that a quantum algorithm can solve efficiently and a classical algorithm cannot, this has not yet been proven. So, the best evidence we can provide is that our problem is BQP-complete, that is, it is among the hardest problems in BQP. If someone were to find an efficient classical algorithm for solving our problem, then every problem solved by a quantum computer efficiently would be classically solvable! Not even the <a href="https://en.wikipedia.org/wiki/Integer_factorization">factoring problem</a> (finding the prime factors of a given large number), which forms the basis of <a href="https://en.wikipedia.org/wiki/RSA_(cryptosystem)">modern encryption</a> and was famously solved by Shor’s algorithm, is expected to be BQP-complete. 
</p>

<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0ZnDD4buC25_mmmwAmuXiWgCEKNuhJGOzpbBEt4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s574/image1.png"><img data-original-height="563" data-original-width="574" height="314" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgOiKMkim0vUjHjx54ZpJwWUCC4pjOcbz1xuSAvMm7ZUI_sZ7mtEjSs8VLIDXGYrxlJonUcz53RBm-4MXRD1B0ZnDD4buC25_mmmwAmuXiWgCEKNuhJGOzpbBEt4sAjZwwO8S8XxMe7e-WkcRy2YI0FiXhTmGHQ70aTTX0oW-lSVwr-jG09gsXP2qu_yWmb/s320/image1.png" width="320"></a></td></tr><tr><td>A diagram showing the believed relationships of the classes BPP and BQP, which are the set of problems that can be efficiently solved on a classical computer and quantum computer, respectively. BQP-complete problems are the hardest problems in BQP.</td></tr></tbody></table>

<p>
To show that our problem of simulating balls and springs is indeed BQP-complete, we start with a standard BQP-complete problem of simulating universal quantum circuits, and show that every quantum circuit can be expressed as a system of many balls coupled with springs. Therefore, our problem is also BQP-complete. 
</p>
<br> 

<h2>Implications and future work</h2>


<p>
This effort also sheds light on work from 2002, when theoretical computer scientist Lov K. Grover and his colleague, Anirvan M. Sengupta, used an <a href="https://journals.aps.org/pra/abstract/10.1103/PhysRevA.65.032319">analogy to coupled</a> pendulums to illustrate how Grover’s famous quantum <a href="https://en.wikipedia.org/wiki/Grover%27s_algorithm">search algorithm</a> could find the correct element in an unsorted database quadratically faster than could be done classically. With the proper setup and initial conditions, it would be possible to tell whether one of <em>N</em> pendulums was different from the others — the analogue of finding the correct element in a database — after the system had evolved for time that was only ~√(<em>N)</em>. While this hints at a connection between certain classical oscillating systems and quantum algorithms, it falls short of explaining why Grover’s quantum algorithm achieves a quantum advantage.
</p>
<p>
Our results make that connection precise. We showed that the dynamics of any classical system of harmonic oscillators can indeed be equivalently understood as the dynamics of a corresponding quantum system of exponentially smaller size. In this way we can simulate Grover and Sengupta’s system of pendulums on a quantum computer of log(<em>N</em>) qubits, and find a different quantum algorithm that can find the correct element in time ~√(<em>N</em>). The analogy we discovered between classical and quantum systems can be used to construct other quantum algorithms offering exponential speedups, where the reason for the speedups is now more evident from the way that classical waves propagate.
</p>
<p>
Our work also reveals that every quantum algorithm can be equivalently understood as the propagation of a classical wave in a system of coupled oscillators. This would imply that, for example, we can in principle build a classical system that solves the factoring problem after it has evolved for time that is exponentially smaller than the runtime of any known classical algorithm that solves factoring. This may look like an efficient classical algorithm for factoring, but the catch is that the number of oscillators is exponentially large, making it an impractical way to solve factoring.
</p>
<p>
Coupled harmonic oscillators are ubiquitous in nature, describing a broad range of systems from electrical circuits to chains of molecules to structures such as bridges. While our work here focuses on the fundamental complexity of this broad class of problems, we expect that it will guide us in searching for real-world examples of harmonic oscillator problems in which a quantum computer could offer an exponential advantage. 
</p>
<br> 

<h2>Acknowledgements</h2>


<p>
<em>We would like to thank our Quantum Computing Science Communicator, Katie McCormick, for helping to write this blog post.</em>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Go Testing by Example [video] (164 pts)]]></title>
            <link>https://research.swtch.com/testing</link>
            <guid>38539174</guid>
            <pubDate>Wed, 06 Dec 2023 00:58:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.swtch.com/testing">https://research.swtch.com/testing</a>, See on <a href="https://news.ycombinator.com/item?id=38539174">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <h2>Go Testing By Example
        
        <div>
        <p>
          
            Posted on Tuesday, December 5, 2023.
            
          
        </p>
        </div>
        </h2>
        

<p>
I opened GopherCon Australia in early November with the talk “Go Testing By Example”.
Being the first talk, there were some A/V issues, so I re-recorded it at home and have posted it here:
</p><p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/X4rxi9jStLo?si=DJiEGUPNxPlYnlWL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p>


<p>
Here are the 20 tips from the talk:
</p><ol>
<li>
Make it easy to add new test cases.
</li><li>
Use test coverage to find untested code.
</li><li>
Coverage is no substitute for thought.
</li><li>
Write exhaustive tests.
</li><li>
Separate test cases from test logic.
</li><li>
Look for special cases.
</li><li>
If you didn’t add a test, you didn’t fix the bug.
</li><li>
Not everything fits in a table.
</li><li>
Test cases can be in testdata files.
</li><li>
Compare against other implementations.
</li><li>
Make test failures readable.
</li><li>
If the answer can change, write code to update them.
</li><li>
Use <a href="https://pkg.go.dev/golang.org/x/tools/txtar">txtar</a> for multi-file test cases.
</li><li>
Annotate existing formats to create testing mini-languages.
</li><li>
Write parsers and printers to simplify tests.
</li><li>
Code quality is limited by test quality.
</li><li>
Scripts make good tests.
</li><li>
Try <a href="https://pkg.go.dev/rsc.io/script">rsc.io/script</a> for your own script-based test cases.
</li><li>
Improve your tests over time.
</li><li>
Aim for continuous deployment.</li></ol>


<p>
Enjoy!
      </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An update on Twitch in Korea (200 pts)]]></title>
            <link>https://blog.twitch.tv/en/2023/12/05/an-update-on-twitch-in-korea/</link>
            <guid>38539167</guid>
            <pubDate>Wed, 06 Dec 2023 00:57:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.twitch.tv/en/2023/12/05/an-update-on-twitch-in-korea/">https://blog.twitch.tv/en/2023/12/05/an-update-on-twitch-in-korea/</a>, See on <a href="https://news.ycombinator.com/item?id=38539167">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tw-impression-id="body"><p>This morning, I shared with our community in Korea that we’ve made the difficult decision to shut down the Twitch business in Korea on February 27, 2024 KST. We understand that this is extremely disappointing news, and we want to explain why we made this decision and how we are planning to support those impacted.</p><p>Ultimately, the cost to operate Twitch in Korea is prohibitively expensive and we have spent significant effort working to reduce these costs so that we could find a way for the Twitch business to remain in Korea. First, we experimented with a peer-to-peer model for source quality. Then, we adjusted source quality to a maximum of 720p. While we have lowered costs from these efforts, our network fees in Korea are still 10 times more expensive than in most other countries. Twitch has been operating in Korea at a significant loss, and unfortunately there is no pathway forward for our business to run more sustainably in that country.</p><p>To all of our global communities, we want to make it clear that this is a unique situation. Operating costs in Korea are significantly higher than they are in other countries and we have been open about this challenge for some time.</p><p>Twitch streamers in Korea have devoted significant time and effort into building their communities, and we plan to help these communities find new homes — even if it’s regrettably not on Twitch. We will work to help Twitch streamers in Korea move their communities to alternative livestreaming services in Korea. We are also reaching out to several of these services to help with the transition and will communicate with impacted streamers as those discussions progress.</p><p>I want to reiterate that this was a very difficult decision and one we are very disappointed we had to make. Korea has always and will continue to play a special role in the international esports community and we are incredibly grateful for the communities they built on Twitch.</p><p>For more information, please see our <a href="https://twitch-web.app.link/e/NLGoBvzBYEb">Help article</a> or join our live stream where I’ll be taking the community’s questions. We will host a stream for our Korean community on /TwitchKR today, December 6 at 9:30 am KST (December 5, at 4:30pm PT). For people outside of the Korean community, I will host another session on /Twitch today, December 6 at 11am KST (December 5, at 6pm PT) to answer questions about this decision or other topics.</p><p>Dan Clancy, Twitch CEO</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MLX: An array framework for Apple Silicon (208 pts)]]></title>
            <link>https://github.com/ml-explore/mlx</link>
            <guid>38539153</guid>
            <pubDate>Wed, 06 Dec 2023 00:56:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ml-explore/mlx">https://github.com/ml-explore/mlx</a>, See on <a href="https://news.ycombinator.com/item?id=38539153">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">MLX</h2>
<p dir="auto"><a href="#quickstart"><strong>Quickstart</strong></a> | <a href="#installation"><strong>Installation</strong></a> |
<a href="https://ml-explore.github.io/mlx/build/html/index.html" rel="nofollow"><strong>Documentation</strong></a> |
<a href="#examples"><strong>Examples</strong></a></p>
<p dir="auto">MLX is an array framework for machine learning on Apple silicon, brought to you
by Apple machine learning research.</p>
<p dir="auto">Some key features of MLX include:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Familiar APIs</strong>: MLX has a Python API that closely follows NumPy.
MLX also has a fully featured C++ API, which closely mirrors the Python API.
MLX has higher-level packages like <code>mlx.nn</code> and <code>mlx.optimizers</code> with APIs
that closely follow PyTorch to simplify building more complex models.</p>
</li>
<li>
<p dir="auto"><strong>Composable function transformations</strong>: MLX has composable function
transformations for automatic differentiation, automatic vectorization,
and computation graph optimization.</p>
</li>
<li>
<p dir="auto"><strong>Lazy computation</strong>: Computations in MLX are lazy. Arrays are only
materialized when needed.</p>
</li>
<li>
<p dir="auto"><strong>Dynamic graph construction</strong>: Computation graphs in MLX are built
dynamically. Changing the shapes of function arguments does not trigger
slow compilations, and debugging is simple and intuitive.</p>
</li>
<li>
<p dir="auto"><strong>Multi-device</strong>: Operations can run on any of the supported devices
(currently, the CPU and GPU).</p>
</li>
<li>
<p dir="auto"><strong>Unified memory</strong>: A notable difference from MLX and other frameworks
is the <em>unified memory model</em>. Arrays in MLX live in shared memory.
Operations on MLX arrays can be performed on any of the supported
device types without moving data.</p>
</li>
</ul>
<p dir="auto">MLX is designed by machine learning researchers for machine learning
researchers. The framework is intended to be user-friendly, but still efficient
to train and deploy models. The design of the framework itself is also
conceptually simple. We intend to make it easy for researchers to extend and
improve MLX with the goal of quickly exploring new ideas.</p>
<p dir="auto">The design of MLX is inspired by frameworks like
<a href="https://numpy.org/doc/stable/index.html" rel="nofollow">NumPy</a>,
<a href="https://pytorch.org/" rel="nofollow">PyTorch</a>, <a href="https://github.com/google/jax">Jax</a>, and
<a href="https://arrayfire.org/" rel="nofollow">ArrayFire</a>.</p>
<h2 tabindex="-1" dir="auto">Examples</h2>
<p dir="auto">The <a href="https://github.com/ml-explore/mlx-examples">MLX examples repo</a> has a
variety of examples, including:</p>
<ul dir="auto">
<li><a href="https://github.com/ml-explore/mlx-examples/tree/main/transformer_lm">Transformer language model</a> training.</li>
<li>Large-scale text generation with
<a href="https://github.com/ml-explore/mlx-examples/tree/main/llama">LLaMA</a> and
finetuning with <a href="https://github.com/ml-explore/mlx-examples/tree/main/lora">LoRA</a>.</li>
<li>Generating images with <a href="https://github.com/ml-explore/mlx-examples/tree/main/stable_diffusion">Stable Diffusion</a>.</li>
<li>Speech recognition with <a href="https://github.com/ml-explore/mlx-examples/tree/main/whisper">OpenAI's Whisper</a>.</li>
</ul>
<h2 tabindex="-1" dir="auto">Quickstart</h2>
<p dir="auto">See the <a href="https://ml-explore.github.io/mlx/build/html/quick_start.html" rel="nofollow">quick start
guide</a>
in the documentation.</p>
<h2 tabindex="-1" dir="auto">Installation</h2>
<p dir="auto">MLX is available on <a href="https://pypi.org/project/mlx/" rel="nofollow">PyPi</a>. To install the Python API, run:</p>

<p dir="auto">Checkout the
<a href="https://ml-explore.github.io/mlx/build/html/install.html#" rel="nofollow">documentation</a>
for more information on building the C++ and Python APIs from source.</p>
<h2 tabindex="-1" dir="auto">Contributing</h2>
<p dir="auto">Check out the <a href="https://github.com/ml-explore/mlx/blob/main/CONTRIBUTING.md">contribution guidelines</a> for more information
on contributing to MLX.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MLX: NumPy like framework for Apple Silicon by Apple (145 pts)]]></title>
            <link>https://ml-explore.github.io/mlx/build/html/index.html</link>
            <guid>38539020</guid>
            <pubDate>Wed, 06 Dec 2023 00:40:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ml-explore.github.io/mlx/build/html/index.html">https://ml-explore.github.io/mlx/build/html/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=38539020">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      
      
      <main id="main-content">
        
        



          <div>
              
              
              
              



              
                

                <article role="main">
                  
  <section id="mlx">
<h2>MLX<a href="#mlx" title="Permalink to this heading">#</a></h2>
<p>MLX is a NumPy-like array framework designed for efficient and flexible machine
learning on Apple silicon, brought to you by Apple machine learning research.</p>
<p>The Python API closely follows NumPy with a few exceptions. MLX also has a
fully featured C++ API which closely follows the Python API.</p>
<p>The main differences between MLX and NumPy are:</p>
<blockquote>
<div><ul>
<li><p><strong>Composable function transformations</strong>: MLX has composable function
transformations for automatic differentiation, automatic vectorization,
and computation graph optimization.</p></li>
<li><p><strong>Lazy computation</strong>: Computations in MLX are lazy. Arrays are only
materialized when needed.</p></li>
<li><p><strong>Multi-device</strong>: Operations can run on any of the supported devices (CPU,
GPU, …)</p></li>
</ul>
</div></blockquote>
<p>The design of MLX is inspired by frameworks like <a href="https://pytorch.org/">PyTorch</a>, <a href="https://github.com/google/jax">Jax</a>, and
<a href="https://arrayfire.org/">ArrayFire</a>. A noteable difference from these
frameworks and MLX is the <em>unified memory model</em>. Arrays in MLX live in shared
memory. Operations on MLX arrays can be performed on any of the supported
device types without performing data copies. Currently supported device types
are the CPU and GPU.</p>






</section>


                </article>
              

              
              
              
              
                
              
            </div>
          
        

      </main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Does uBlock Origin bypass the latest YouTube anti-adblock script? (238 pts)]]></title>
            <link>https://drhyperion451.github.io/does-uBO-bypass-yt/</link>
            <guid>38538236</guid>
            <pubDate>Tue, 05 Dec 2023 23:11:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://drhyperion451.github.io/does-uBO-bypass-yt/">https://drhyperion451.github.io/does-uBO-bypass-yt/</a>, See on <a href="https://news.ycombinator.com/item?id=38538236">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-detector">
        <p><span id="main-question">Does uBlock Origin (uBO) bypass the latest YouTube anti-adblock script?</span></p>
        <p><span id="main-answer">Not sure</span>
        </p>
        
         
    </div><div id="about">
        <!-- class ="aa-blocked" -> Activated when the website response is 'yes'-->
        <!-- class ="not-aa-blocked" -> Activated when the website response is 'no'-->
        <h2>What does it mean?</h2>
        <p>We are currently evaluating if uBlock Origin's filters have been updated to deal with the latest Anti-Adblocker script. This website doesn't check your device. It simply compares text files provided by the uBO team to let you know the current status of uBO's solutions for YouTube.</p>
        
        
            
            
            
        <h2>What should I do now?</h2>
        <p>Please come back later.</p>
                        
        <!--Button to auto-update quick filters.-->
        
        

        <h2>What does this website do?</h2>
        <p>It simply gets the info of the latest <a href="https://raw.githubusercontent.com/stephenhawk8054/misc/main/yt-fix.txt">YT script ID solved by uBlock Origin</a> and compares it against the latest <a href="https://pastefy.app/G1Txv5su/raw">YouTube Anti-Adblocker script ID</a>.
            If it's the same, then the uBlock Origin team has finally updated their filters. If it's not, a fix is on the way. This website does not check if your own uBlock Origin version is up-to-date.</p>

        <h2>How can I contribute to this website?</h2>
        <p>You can make changes and pull request to the <a href="https://github.com/drHyperion451/does-uBO-bypass-yt/tree/dev"><code>dev</code></a> branch.</p>
        <p>Please keep in mind we are all volunteers. We don't get any revenue from this and we cannot be online 24/7. Thanks for understanding!</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Playstation is erasing 1,318 seasons of Discovery shows from customer libraries (188 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2023/12/playstation-is-erasing-1318-seasons-of-discovery-shows-from-customer-libraries/</link>
            <guid>38538162</guid>
            <pubDate>Tue, 05 Dec 2023 23:01:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2023/12/playstation-is-erasing-1318-seasons-of-discovery-shows-from-customer-libraries/">https://arstechnica.com/gadgets/2023/12/playstation-is-erasing-1318-seasons-of-discovery-shows-from-customer-libraries/</a>, See on <a href="https://news.ycombinator.com/item?id=38538162">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      No refunds mentioned    —
</h4>
            
            <h2 itemprop="description">The change comes as Warner Bros. tries to add subscribers to Max, Discovery+ apps.</h2>
                    </header>
        <section>
            <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/12/mythbusters-800x447.jpg" alt="mythbusters">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/12/mythbusters.jpg" data-height="1073" data-width="1920">Enlarge</a> <span>/</span> Myth: You own the digital content you buy.</p></figcaption>  </figure>

  




<!-- cache hit 419:single/related:d9041d325cf5f210d63753721da064aa --><!-- empty -->
<p>If you purchased any Discovery shows from the PlayStation Store, Sony has some bad news for you to discover.</p>
<p>The company recently announced that all Discovery content purchased on the PlayStation Store will be erased before 2024. The brief <a href="https://www.playstation.com/en-us/legal/psvideocontent/">notice</a>, signed by the PlayStation Store, says:</p>
<blockquote><p>As of 31 December 2023, due to our content licensing arrangements with content providers, you will no longer be able to watch any of your previously purchased Discovery content and the content will be removed from your video library.</p>
<p>We sincerely thank you for your continued support.</p></blockquote>
<p>PlayStation Network started selling TV shows and movies with 2008's PlayStation 3, and at the time you were allowed to transfer content to different Sony devices, <a href="https://kotaku.com/sony-ps4-ps5-discovery-mythbusters-tv-1851066164">Kotaku</a> noted. That feature went away with the PlayStation 4. With the growth of <a href="https://arstechnica.com/culture/2023/08/the-tv-streaming-apps-broke-their-promises-and-now-theyre-jacking-the-price/">streaming TV apps</a>, many of which could be accessed through a PlayStation, the PlayStation Store <a href="https://blog.playstation.com/2021/03/02/playstation-store-to-discontinue-movie-and-tv-purchases-and-rentals/">stopped selling</a> movies and TV shows in 2021.</p>
<p>But there were users who had already purchased stuff from the PlayStation Store and, believe it or not, expect to be able to watch it when they want, since they paid money to buy (rather than rent) it. I admit that I haven't heard a lot of the shows being deleted post-purchase. Shows getting axed from user libraries include <em>Wives With Knives</em>,<em> An Idiot Abroad</em>,<em> Evil Twins</em>, and <i>Body Bizarre</i>. And when it came to deadly docuseries, PlayStation Store offered plenty, whether you were after<em> Deadly Affairs</em>,<em> Demands</em>,<em> Dentists</em>,<em> Devotion</em>,<em> Sins</em>, or, of course, <em>Women</em>.</p>
<p>I'm having fun with some of the most bizarre titles, of course. But there are also plenty of more well-known titles on the list of purchased content being revoked, including <em>American Chopper</em>, <em>Cake Boss</em>,<em> MythBusters</em>, <em>Shark Week</em>, and <em>Say Yes to the Dress.&nbsp;</em></p>
<p>While some of the content listed sounds, shall we say, a bit niche, there are in total 1,318 seasons of shows listed for deletion. That means there's a good chance numerous users will be affected by Sony's announcement.</p>

                                                </div>

            
            
                            <nav>Page: <span>1 <a href="https://arstechnica.com/gadgets/2023/12/playstation-is-erasing-1318-seasons-of-discovery-shows-from-customer-libraries/2/">2</a> <a href="https://arstechnica.com/gadgets/2023/12/playstation-is-erasing-1318-seasons-of-discovery-shows-from-customer-libraries/2/"><span>Next <span>→</span></span></a></span></nav>
            
        </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[All my favorite tracing tools (206 pts)]]></title>
            <link>https://thume.ca/2023/12/02/tracing-methods/</link>
            <guid>38538111</guid>
            <pubDate>Tue, 05 Dec 2023 22:56:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thume.ca/2023/12/02/tracing-methods/">https://thume.ca/2023/12/02/tracing-methods/</a>, See on <a href="https://news.ycombinator.com/item?id=38538111">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<p>Ever wanted more different ways to understand what’s going on in a program? Here I catalogue a huge variety of tracing methods you can use for varying types of problems. Tracing has been such a long-standing interest (and job) of mine that some of these will novel and interesting to anyone who reads this. I’ll guarantee it by including 2 novel tracing tools I’ve made and haven’t shared before (look for this: <span><em>Tooling drop!</em></span>).</p>
<p>What I see as the key parts of tracing are collecting timestamped data on what happened in a system, and then ideally visualizing it in a timeline UI instead of just as a text log. First I’ll cover my favorite ways of really easily getting trace data into a nice timeline UI, because it’s a superpower that makes all the other tracing tools more interesting. Then I’ll go over ways to get that data, everything from instrumentation to binary patching to processor hardware features.</p>
<p>I’ll also give a real-life example of combining eBPF tracing with Perfetto visualization to diagnose tail latency issues in huge traces by using a number of neat tricks. Look for the “eBPF Example” section.</p>
<p><strong>Note:</strong> I’m hiring for my accelerator optimization team at Anthropic! See <a href="#conclusion-if-you-liked-this-you-may-like-my-team-at-anthropic">the bottom of the post</a> for more detail.</p>
<h2 id="easily-visualizing-data-on-a-trace-timeline">Easily visualizing data on a trace timeline</h2>
<p>Getting event data onto a nice zoomable timeline UI is way easier than most people think. Here’s my favorite method I do all the time which can take you from logging your data to visualizing it in minutes:</p>
<div><pre><code><span># from:
</span><span>print</span><span>(</span><span>"%d: %s %d"</span> <span>%</span> <span>(</span><span>event_name</span><span>,</span> <span>timestamp</span><span>,</span> <span>duration</span><span>))</span>
<span># to:
</span><span>with</span> <span>open</span><span>(</span><span>'trace.json'</span><span>,</span><span>'w'</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
  <span>f</span><span>.</span><span>print</span><span>(</span><span>"["</span><span>)</span>
  <span>f</span><span>.</span><span>print</span><span>(</span><span>'{"name": "%s", "ts": %d, "dur": %d, "cat": "hi", "ph": "X", "pid": 1, "tid": 1, "args": {}}</span><span>\n</span><span>'</span> <span>%</span>
    <span>(</span><span>event_name</span><span>,</span> <span>timestamp</span><span>,</span> <span>duration</span><span>))</span>
  <span>f</span><span>.</span><span>print</span><span>(</span><span>"]"</span><span>)</span> <span># this closing ] isn't actually required
</span></code></pre></div>
<p>This is the power of the <a href="https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview">Chromium Event JSON Format</a>. It’s a super simple JSON format that supports a bunch of different kinds of events, and is supported by a lot of different profile visualizer tools.</p>
<p>You can view the resulting tracing files in Google’s Perfetto trace viewer by going to <a href="https://ui.perfetto.dev/">https://ui.perfetto.dev/</a>, or in the older Catapult viewer (which is nicer for some traces) by going to <code>chrome://tracing</code> in Chrome. You can play around with the UI by <a href="https://ui.perfetto.dev/">going to Perfetto</a> and clicking “Open Chrome Example” in the sidebar. Here’s a screenshot showing an event annotated with arguments and flow event arrows:</p>
<p><a href="https://thume.ca/assets/postassets/tracing/perfetto.png"><img src="https://thume.ca/assets/postassets/tracing/perfetto.png" alt="Perfetto Screenshot"></a></p>
<p>Me and my coworkers do this all the time at work, whip up trace visualizations for new data sources in under an hour and add them to our growing set of trace tools. We have a Python utility to turn a trace file into a clickable permanently-saved intranet link we can share with coworkers in Slack. This is easy to set up by building a copy of Perfetto and uploading to a file hosting server you control, and then putting trace files on that server and generating links using Perfetto’s <code>?url=</code> parameter. We also write custom trace analysis scripts by loading the simple JSON into a Pandas dataframe.</p>
<p>I like Perfetto as its use of WebAssembly lets it scale to about 10x more events than Catapult (although it gets laggy), and you have the escape hatch of the native backend for even bigger traces. Its <a href="https://perfetto.dev/docs/analysis/common-queries">SQL query feature</a> also lets you find events and annotate them in the UI using arbitrary predicates, including <a href="https://perfetto.dev/docs/analysis/stdlib-docs">special SQL functions</a> for dealing with trace stacks.</p>
<p><strong>UI protip</strong>: Press <code>?</code> in Perfetto to see the shortcuts. I use both <code>WASD</code> and <code>CTRL+scroll</code> to move around.</p>
<h3 id="advanced-format-fuchsia-trace-format">Advanced Format: Fuchsia Trace Format</h3>
<p>The Chromium JSON format can produce gigantic files and be very slow for large traces, because it repeats both the field names and string values for every event. Perfetto also supports the <a href="https://fuchsia.dev/fuchsia-src/reference/tracing/trace-format">Fuchsia Trace Format (FTF)</a> which is a simple compact binary format with an incredible spec doc that makes it easy to produce binary traces. It supports interning strings to avoid repeating event names, and is designed around 64 byte words and supports clock bases so that you can directly write timestamp counters and have the UI compute the true time.</p>
<p>When I worked at Jane Street I <a href="https://github.com/janestreet/tracing/blob/master/zero/writer.ml">used this to log instrumentation events to a buffer directly in FTF</a> as they occurred in &lt;10ns per span (it would have been closer to 4ns if it wasn’t for OCaml limitations).</p>
<h3 id="advanced-format-perfetto-protobuf">Advanced Format: Perfetto Protobuf</h3>
<p>Another format which is similarly compact, and also supports more features, is <a href="https://github.com/google/perfetto/blob/master/protos/perfetto/trace/perfetto_trace.proto">Perfetto’s native Protobuf trace format</a>. It’s documented only in comments in the proto files and is a bit trickier to figure out, but might be a bit easier to generate if you have access to a protobuf library. It enables access to advanced Perfetto features like including callstack samples in a trace, which aren’t available with other formats. It’s slower to write than FTF, although Perfetto has a <a href="https://perfetto.dev/docs/design-docs/protozero">ProtoZero</a> library to make it somewhat faster.</p>
<p>This can be really tricky to get right though and I had to reference the Perfetto source code to figure out error codes in the “info and stats” tab a lot. The biggest gotchas are you need to set <code>trusted_packet_sequence_id</code> on every packet, have a <code>TrackDescriptor</code> for every track, and set <code>sequence_flags=SEQ_INCREMENTAl_STATE_CLEARED</code> on the first packet.</p>
<h3 id="other-tools">Other tools</h3>
<p>Some other nice trace visualization tools are <a href="https://github.com/jlfwong/speedscope">Speedscope</a> which is better for a hybrid between profile and trace visualization, <a href="https://github.com/google/pprof">pprof</a> for pure profile call graph visualization, and <a href="https://www.rerun.io/">Rerun</a> for multimodal 3D visualization. Other profile viewers I like less but which have some nice parts include <a href="https://eclipse.dev/tracecompass/">Trace Compass</a> and the <a href="https://profiler.firefox.com/docs/#/">Firefox Profiler</a>.</p>
<h2 id="tracing-methods">Tracing Methods</h2>
<p>Now lets go over all sorts of different neat tracing methods! I’ll start with some obscure and interesting low level ones but I promise I’ll get to some more broadly usable ones after.</p>
<h2 id="hardware-breakpoints">Hardware breakpoints</h2>
<p>For ages, processors have supported <strong>hardware breakpoint registers</strong> which let you put in a small number of memory addresses and have the processor interrupt itself when any of them are accessed or executed.</p>
<h3 id="perf-and-perftrace">perf and perftrace</h3>
<p>Linux exposes this functionality through <code>ptrace</code> but also through the <a href="https://man7.org/linux/man-pages/man2/perf_event_open.2.html"><code>perf_event_open</code> syscall</a> and the <a href="https://man7.org/linux/man-pages/man1/perf-record.1.html"><code>perf record</code> command</a>. You can record a process like <code>perf record -e \mem:0x1000/8:rwx my_command</code> and view the results with <code>perf script</code>. It costs about 3us of overhead every time a breakpoint is hit.</p>
<p><span><em>Tooling drop!</em></span> I wrote <a href="https://github.com/trishume/perftrace">a tiny Python library called perftrace</a> with a C stub which calls the <code>perf_event_open</code> syscall to record timestamps and register values when the breakpoints were hit.</p>
<p>It currently only supports execution breakpoints but you can also breakpoint on reads or writes of any memory and it would be <a href="https://github.com/trishume/perftrace/blob/d074e65bf71e8af10335164111969f96263d283a/perftrace.c#L61">easy to modify the code to do that</a>. Hardware breakpoints are basically the only way to watch for accessing a specific memory address at a fine granularity which doesn’t add overhead to code which doesn’t touch that memory.</p>
<h3 id="gdb-scripting">GDB scripting</h3>
<p>In addition to using it manually, you can automate the process of following the execution of a program using debugger breakpoints by using GDB’s Python scripting interface. This is slower than perf breakpoints but gives you the ability to inspect and modify memory when you hit breakpoints. <a href="https://github.com/hugsy/gef">GEF</a> is an extension to GDB that in addition to making it much nicer in general, also extends the Python API with a bunch of handy utilities.</p>
<p><span><em>Tooling drop!</em></span> <a href="https://gist.github.com/trishume/fe3b3b90a7e524c976ecb98053bb7f86">Here’s an example GDB script I wrote using GEF which gives examples of how to puppeteer, trace and inspect a program</a></p>
<h2 id="intel-processor-trace">Intel Processor Trace</h2>
<p><a href="https://easyperf.net/blog/2019/08/23/Intel-Processor-Trace">Intel Processor Trace</a> is a hardware technology on Intel chips since Skylake which allows recording a trace of <em>every instruction the processor executes</em> via recording enough info to reconstruct the control flow in a super-compact format, along with fine-grained timing info. It has extremely low overhead since it’s done by hardware and writes bypass the cache so the only overhead is reducing main memory bandwidth by about 1GB/s. I see no noticeable overhead at all on most program benchmarks I’ve tested.</p>
<p>You can access a dump of the assembly instructions executed in a recorded region using <a href="https://man7.org/linux/man-pages/man1/perf-intel-pt.1.html"><code>perf</code></a>, <a href="https://lldb.llvm.org/use/intel_pt.html"><code>lldb</code></a> and <a href="https://easyperf.net/blog/2019/08/30/Intel-PT-part2"><code>gdb</code></a>.</p>
<h3 id="magic-trace">magic-trace</h3>
<p>However assembly traces aren’t useful to most people, so when at Jane Street I created <a href="https://github.com/janestreet/magic-trace">magic-trace</a> along with my intern Chris Lambert, which generates a trace file (using FTF and Perfetto as described above) which visualizes <em>every function call</em> in a program execution. Jane Street generously open-sourced it so anyone can use it! Since then it’s been extended to support tracing into the kernel as well. I wrote <a href="https://blog.janestreet.com/magic-trace/">a blog post about how it works for the Jane Street tech blog</a>.</p>
<p><img src="https://github.com/janestreet/magic-trace/raw/master/docs/assets/stage-3.gif" alt="magic-trace demo"></p>
<p>Processor Trace can record to a ring buffer, and <code>magic-trace</code> uses the hardware breakpoint feature described earlier to let you trigger capture of the last 10ms whenever some function that signals an event you want to look at happened, or when the program ends. This makes it great for a bunch of scenarios:</p>
<ul>
<li>Debugging rare tail latency events: Add a trigger function call after something takes unusually long, and then leave magic-trace attached in production. Because it captures everything you’ll never have not logged enough data to identify the slow part.</li>
<li>Everyday performance analysis: A full trace timeline can be easier to interpret than a sampling profiler visualization, especially because it displays the difference between a million fast calls to a function and one slow call.
<ul>
<li>It’s typical to find performance problems on systems that had only ever been analyzed with a sampling profiler by noticing the first time you magic-trace the program that many functions are being called more times than expected or in locations you didn’t expect.</li>
</ul>
</li>
<li>Debugging crashes: When a program crashes for reasons you don’t understand, you can just run it under magic-trace and see every function call leading up to the crash, which is often enough to figure out why the crash happened without adding extra logging or using a debugger!</li>
</ul>
<p>If you want to modify magic-trace to suit your needs, it’s open-source OCaml. And if you like Rust more than OCaml someone made a simple Rust port called <a href="https://github.com/michoecho/perf2perfetto">perf2perfetto</a>.</p>
<p>Unfortunately, Processor Trace isn’t supported on many virtual machines that use compatible Intel Hardware. Complain to your cloud provider to add support in their hypervisor or try bare-metal instances!</p>
<h2 id="instrumentation-based-tracing-profilers">Instrumentation-based tracing profilers</h2>
<p>What most people use to get similar benefits to magic-trace traces, especially in the gamedev industry, is low-overhead instrumentation-based profilers with custom UIs. One major advantage of instrumentation-based traces is they can contain extra information about data and not just control flow, putting arguments from your functions into the trace can be key for figuring out what’s going on. These tools often support including other data sources such as OS scheduling info, CPU samples and GPU trace data. Here’s my favorite tools like this and their pros/cons:</p>
<h3 id="tracy"><a href="https://github.com/wolfpld/tracy">Tracy</a></h3>
<p><a href="https://github.com/wolfpld/tracy"><img src="https://github.com/wolfpld/tracy/raw/master/doc/profiler.png" alt="Tracy screenshot"></a></p>
<ul>
<li>Cross platform, including good Linux sampling and scheduling capture</li>
<li>Overhead of only 2ns/span, supports giant traces with hundreds of millions of events</li>
<li>Really nice and fast UI with tons of features (check out the <a href="https://www.youtube.com/watch?v=30wpRpHTTag">demo</a> <a href="https://www.youtube.com/watch?v=_hU7vw00MZ4">videos</a> in the readme)</li>
<li>Integrates CPU sampling with detailed source and assembly analysis</li>
<li>Popular so there are bindings in non-C++ languages like <a href="https://docs.rs/tracing-tracy/latest/tracing_tracy/">Rust</a> and <a href="https://github.com/nektro/zig-tracy">Zig</a>.</li>
<li>Con: Only supports a single string/number argument to events</li>
<li>Con: Timeline is overly aggressive in collapsing small events into squiggles (<a href="https://thume.ca/2021/03/14/iforests/">see my post on this</a>).</li>
</ul>
<h3 id="optick"><a href="https://github.com/bombomby/optick">Optick</a></h3>
<p><a href="https://www.youtube.com/watch?v=p57TV5342fo"><img src="https://github.com/bombomby/brofiler/raw/gh-pages/images/VideoThumbnail.jpg" alt="Optick screenshot"></a></p>
<ul>
<li>Cross-platform, lots of features, very nice UI</li>
<li>Supports multiple named arguments per event</li>
<li>Con: Not as fleshed-out for non-game applications</li>
<li>Con: sampling integration only works on Windows</li>
</ul>
<h3 id="perfetto"><a href="https://perfetto.dev/docs/instrumentation/tracing-sdk">Perfetto</a></h3>
<ul>
<li>Perfetto UI is nice, events can include arguments and flow event arrows</li>
<li>Integrates with other Perfetto data sources like OS events and sampling</li>
<li>Con: Higher overhead of around 600ns/span when tracing enabled</li>
<li>Con: UI doesn’t scale to traces as large as the above two programs</li>
</ul>
<h3 id="other-programs">Other programs</h3>
<p>There’s a bunch more similar small programs that generally come with their own instrumentation library and their own WebGL profile viewer. These are generally more lightweight and can be easier to integrate. For example <a href="https://gravitymoth.com/spall/spall-web.html">Spall</a>, <a href="https://github.com/jonasmr/microprofile">microprofile</a>, <a href="https://github.com/Celtoys/Remotery">Remotery</a>, <a href="https://github.com/EmbarkStudios/puffin">Puffin (Rust-native)</a>, <a href="https://github.com/mikesart/gpuvis">gpuviz</a>. I must also mention the <a href="https://github.com/janestreet/tracing">OCaml tracing instrumentation library I wrote for Jane Street</a> which has overheads under 10ns/span via a compile-time macro like the C++ libraries.</p>
<h2 id="ebpf">eBPF</h2>
<p>If you want to trace things using the Linux kernel there’s a new game in town, and it’s awesome. The eBPF subsystem allows you to attach complex programs to all sorts of different things in the kernel and efficiently shuttle data back to userspace, basically subsuming all the legacy facilities like ftrace and kprobes such that I won’t talk about them.</p>
<p>Things you can trace include: syscalls, low overhead tracepoints throughout the kernel, hardware performance counters, any kernel function call and arbitrary breakpoints or function calls/returns in userspace code. Combined these basically let you see anything on the system in or out of userspace.</p>
<p>You normally write BPF programs in C but there are perhaps even nicer toolkits for using <a href="https://github.com/tw4452852/zbpf">Zig</a> and <a href="https://aya-rs.dev/">Rust</a>.</p>
<p>There’s <a href="https://ebpf.io/applications/">a whole bunch of ways to use eBPF</a> and I’ll talk about some of my favorites here. Some other favorites I won’t go into in detail are <a href="https://rubrikinc.github.io/wachy/">Wachy</a> and <a href="https://github.com/anakryiko/retsnoop">retsnoop</a>.</p>
<h3 id="bcc-easy-python-api-for-ebpf">BCC: Easy Python API for eBPF</h3>
<p>The <a href="https://github.com/iovisor/bcc">BPF Compiler Collection (BCC)</a> is a library with really nice Python bindings for compiling eBPF programs from C source code, injecting them, and getting the data back. It has a really nice feature where you can write a C struct to hold the event data you want to record, and then it will parse that and expose it so you can access the fields in Python. Check out <a href="https://github.com/iovisor/bcc/blob/master/examples/ringbuf/ringbuf_output.py">how simple this syscall tracing example is</a>.</p>
<p>I really like having the full power of Python to control my tracing scripts. BCC scripts often use Python string templating to do compile time metaprogramming of the C to compose the exact probe script you want, and then do data post-processing in Python to present things nicely.</p>
<h3 id="bpftrace-terse-dsl-for-ebpf-tracing">bpftrace: terse DSL for eBPF tracing</h3>
<p>If you want a terser way to compose tracing programs, in the style of dtrace, check out <a href="https://github.com/iovisor/bpftrace">bpftrace</a>. It lets you write one liners like these:</p>
<div><pre><code><span># Files opened by process</span>
bpftrace <span>-e</span> <span>'tracepoint:syscalls:sys_enter_open { printf("%s %s\n", comm, str(args-&gt;filename)); }'</span>

<span># Count LLC cache misses by process name and PID (uses PMCs):</span>
bpftrace <span>-e</span> <span>'hardware:cache-misses:1000000 { @[comm, pid] = count(); }'</span>
</code></pre></div>
<h3 id="ply-simpler-bpftrace">ply: simpler bpftrace</h3>
<p>If you want something like bpftrace but simpler and faster with no LLVM dependencies. Check out <a href="https://wkz.github.io/ply/">ply</a>.</p>
<div><pre><code><span># Which processes are receiving errors when reading from the VFS?</span>
ply <span>'kretprobe:vfs_read if (retval &lt; 0) { @[pid, comm, retval] = count(); }'</span>
</code></pre></div>
<h2 id="ebpf-example-anthropics-perfetto-based-packet-and-user-event-tracing">eBPF Example: Anthropic’s Perfetto-based packet and user event tracing</h2>
<p>For work at Anthropic I wanted to analyze tail latency of some networking code so I used BCC and hooked into low-overhead kernel probe points to trace info from every single packet into a ring buffer. I could even include fields pulled from the packet header and NIC queue information, all at 1 million packets per second with no noticeable overhead.</p>
<h3 id="trick-for-tracing-userspace-events-with-low-overhead-in-ebpf">Trick for tracing userspace events with low overhead in eBPF</h3>
<p>I wanted to correlate packets with userspace events from a Python program, so I used a fun trick: Find a syscall which has an early-exit error path and bindings in most languages, and then trace calls to that which have specific arguments which produce an error. I traced the <code>faccessat2</code> syscall such that in Python <code>os.access(event_name, -932, dir_fd=-event_type)</code> where <code>event_type</code> was an enum for start, stop and instant events would log spans to my Perfetto trace. This had an overhead of around 700ns/event, which is in a similar league to Perfetto’s full-userspace C++ instrumentation, and a lot of that is Python call overhead. The <code>os.access</code> function is especially good because when the syscall errors it doesn’t incur overhead by generating a Python exception like most other syscall wrappers do.</p>
<h3 id="how-to-process-events-more-quickly-using-a-c-helper-with-bcc">How to process events more quickly using a C helper with BCC</h3>
<p>With 1 million packets per second I had a problem that with rare tail latency events, my traces quickly got huge and lagged Perfetto. I wanted to only keep data from shortly before one of my userspace send events took too long. Normally you’d do this with a circular buffer that gets snapshotted, and it would be possible to implement that in eBPF. But I didn’t want to implement my own ringbuf and the included ones don’t support wraparound overwriting. So instead I used the internal <code>_open_ring_buffer</code> function to register a ctypes C function as a ringbuffer callback instead of a Python function, and wrote an efficient C callback to filter out packets near a tail latency event before passing those to Python.</p>
<h3 id="perks-of-perfetto-visualization">Perks of Perfetto visualization</h3>
<p>I used the Perfetto Protobuf format with interned strings in order to keep trace size down to a few bytes per packet.</p>
<p>I could use Perfetto’s SQL support in the resulting trace to query for send events above a certain time threshold after startup in a specific process. Here’s a screenshot showing a long send event coinciding with packets starting to be paced out with larger gaps on one of the queues, including the ability to have line graph tracks:</p>
<p><a href="https://thume.ca/assets/postassets/tracing/packettrace.png"><img src="https://thume.ca/assets/postassets/tracing/packettrace.png" alt="Perfetto Packet Trace"></a></p>
<p>I think it’s kinda crazy that we have all these different mostly-text-based BPF tools rather than a framework that lets you put all sorts of different kinds of system events into a trace UI, including easily scripting your own new events. It’s so much easier to investigate this kind of thing with a timeline UI. I started building that framework at Anthropic, but only spent a week on it since I’ve had higher priority things to do since I did the packet latency investigation.</p>
<h2 id="binary-instrumentation">Binary Instrumentation</h2>
<p>When you’re instrumenting userspace programs in a way where the overhead of kernel breakpoints is too high, but you don’t have access to the source code, perhaps because you’re reverse-engineering something, then it may be time for binary instrumentation.</p>
<h3 id="bpftime-ebpf-based-binary-instrumentation">bpftime: eBPF-based binary instrumentation</h3>
<p>One easy way that’s a good segue is <a href="https://github.com/eunomia-bpf/bpftime">bpftime</a> which takes your existing eBPF programs with userspace probes, and runs them much faster by patching the instructions to run the BPF program inside the process rather than incurring 3us of kernel interrupt overhead every time.</p>
<h3 id="e9patch">E9Patch</h3>
<p>For more sophisticated binary patching on x86, look to <a href="https://github.com/GJDuck/e9patch">E9Patch</a>.</p>
<p>On some architectures, patching can be really easy since you just patch the instruction you want to trace with a jump to a piece of “trampoline” code which has your instrumentation, and then the original instruction and a jump back.</p>
<p>It’s much harder on x86 since instructions are variable length, so if you just patch a jump over a target instruction, occasionally that’ll cause problems since some other instruction jumps to an instruction your longer jump had to stomp over.</p>
<p>People have invented all kinds of clever tricks to get around these issues including “instruction punning” where you put your patch code at addresses which are also valid x86 nop or trap instructions. E9Patch implements very advanced versions of these tricks such that the patching should basically always work.</p>
<p>It comes with an API as well as a tool called <a href="https://github.com/GJDuck/e9patch/blob/master/doc/e9tool-user-guide.md">E9Tool</a> which lets you patch using a command line interface:</p>
<div><pre><code><span># print all jump instructions in the xterm binary</span>
<span>$ </span>e9tool <span>-M</span> jmp <span>-P</span> print xterm
jz 0x4064d5
jz 0x452c36
...
</code></pre></div>
<h3 id="frida">Frida</h3>
<p>The other way to get around the difficulty of static patching, when you have to be conservative around how jumps you don’t know about could be messed up by your patches, is dynamic binary instrumentation, where you basically puppeteer the execution of the program. This is the technique used by JIT VMs like Rosetta and QEMU to basically recompile your program as you run it.</p>
<p><a href="https://frida.re/">Frida</a> exposes this incredibly powerful technique in a general way you can script in Javascript using its “Stalker” interface. Allowing you to attach JS snippets to pieces of code or rewrite the assembly as it is run. It also lets you do more standard patching, although it doesn’t work as well on x86 as E9Patch.</p>
<h2 id="ld_preload">LD_PRELOAD</h2>
<p>If you just want to trace a function in a dynamic library like libc, you can use <code>LD_PRELOAD</code> to inject a library of your own to replace any functions you like. You can use <code>dlsym(RTLD_NEXT, "fn_name")</code> to get the old implementation in order to wrap it. Check out <a href="https://axcheron.github.io/playing-with-ld_preload/">this tutorial post</a> for how.</p>
<h2 id="distributed-tracing">Distributed Tracing</h2>
<p>Distributed Tracing is where you can trace across different services via attaching special headers to requests and sending all the timing data back to a trace server. Some popular solutions are <a href="https://opentelemetry.io/">OpenTelemetry</a> (of which there are many implementations and UIs) and <a href="https://zipkin.io/">Zipkin</a>.</p>
<p>There’s some cool new solutions like <a href="https://odigos.io/">Odigos</a> that use eBPF to add distributed tracing support without any instrumentation.</p>
<h2 id="sampling-profilers">Sampling Profilers</h2>
<p>Sampling profilers take a sample of the full call stack of your program periodically. Typical profiler UIs don’t have the time axis I’d think of as part of “tracing”, but some UIs do. For example <a href="https://github.com/jlfwong/speedscope">Speedscope</a> accepts many profiler data formats and can visualize with a time axis, and <a href="https://github.com/mstange/samply">Samply</a> is an easy to use profiler which uses the Firefox Profiler UI, which also has a timeline view.</p>
<p>One neat sampling method used by <a href="https://github.com/benfred/py-spy">py-spy</a> and <a href="https://rbspy.github.io/">rbspy</a> is to use the <a href="https://man7.org/linux/man-pages/man2/process_vm_readv.2.html"><code>process_vm_readv</code> syscall</a> to read memory out of a process without interrupting it. If like an interpreter the process stores info about what it’s doing in memory, this can allow you to follow it with no overhead on the target process. You could even use this trick for low-overhead native program instrumentation: set up a little stack data structure where you push and pop pointers to span names or other context info, and then sample it from another program when needed using eBPF or <code>process_vm_readv</code>.</p>
<h2 id="qemu-instrumentation">QEMU Instrumentation</h2>
<p>When all other tracing tools fail, sometimes you have to fall back on the most powerful tool in the tracing toolbox: Full emulation and hooking into QEMU’s JIT compiler. This theoretically allows you to trace and patch both control flow <em>and</em> memory, in both userspace and the kernel, including snapshot and restore, across many architectures and operating systems.</p>
<p>However, actually doing this is not for the faint of heart and the tooling for it only barely exists.</p>
<h3 id="cannoli">Cannoli</h3>
<p><a href="https://github.com/MarginResearch/cannoli">Cannoli</a> is a tracing engine for qemu-user (so no kernel stuff) which patches QEMU to log execution and memory events to a high-performance ringbuffer read by a Rust extension you compile. This lets it trace with very low overhead by spreading the load of following the trace over many cores, at the cost of not being able to modify the execution.</p>
<p>It’s a bit tricky to use, you have to compile QEMU and Cannoli yourself at the moment, and it’s kind of a prototype so when I’ve used it in the past for CTFs I’ve often had to add new features to it.</p>
<h3 id="qemu-tcg-plugins">QEMU TCG Plugins</h3>
<p>QEMU has recently added <a href="https://www.qemu.org/docs/master/devel/tcg-plugins.html">plugin support for its TCG JIT</a>. Like Cannoli this is read-only for now, and its likely slower than Cannoli, but it works in qemu-system mode and exposes slightly different functionality.</p>
<h3 id="usercorn">usercorn</h3>
<p>My friend has an old project called <a href="https://github.com/lunixbochs/usercorn">usercorn</a> that is mostly bitrotted but has the ability to trace programs using QEMU and analyze them with Lua scripts and all sorts of fancy trace analysis. Someone (possibly him eventually) could theoretically revive it and rebase it on top of something like QEMU TCG plugins.</p>
<h2 id="conclusion-if-you-liked-this-you-may-like-my-team-at-anthropic">Conclusion: If you liked this you may like my team at Anthropic</h2>
<p>If you made it to the bottom and enjoyed all those different tracing strategies, you may also be interested in working on my team!</p>
<p>I lead the performance optimization team at <a href="https://www.anthropic.com/">Anthropic</a> (we build one of the world’s leading large language models, and have a heavy focus on figuring out how future more powerful models can go well for the world). We’ll be doing accelerator kernel optimization across GPUs, TPUs and Trainium. TPUs and Trainium are cool in that they’re simpler architectures where optimization is more like a cycle-counting puzzle, and they also have <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-profile-user-guide.html">amazing tracing tools</a>. Almost nobody knows these new architectures, so we’re currently hiring high potential people with other kinds of low-level optimization experience who are willing to learn.</p>
<p>I plan for us to do a bunch of optimization work as compiler-style transformation passes over IRs, but simpler via being bespoke to the ML architecture we’re optimizing. These will parallelize architectures across machines, within a machine, and within a chip in similar ways. We also work closely with an amazing ML research team to do experiments together and come up with architectures that jointly optimize for ML and hardware performance.</p>
<p>Anthropic recently received ~$6B in funding commitments, and are investing it heavily in compute. We currently have ~5 performance specialists, with each one making an immense contribution in helping us have models that exhibit interesting capabilities for our alignment researcher and policy teams.</p>
<p>AI now is still missing a lot, but progress is incredibly fast. It’s hard for me to say the coming decade of progress won’t lead to AI as good as us at nearly all jobs, which would be the biggest event in history. Anthropic is unusually full of people who joined because they really care about ensuring this goes well. I think we have the world’s best alignment, interpretability research, and AI policy teams, and I personally work on performance optimization here because I think it’s the best way to leverage my comparative advantage to help the rest of our efforts succeed at steering towards AI going well for the world in the event it keeps up this pace.</p>
<p>If you too would like to do fun low-level optimization on what I think will be the most important technology of this decade and want to chat: Email me at <a href="https://thume.ca/cdn-cgi/l/email-protection" data-cfemail="394d4b504a4d58577958574d514b5649505a175a5654">[email&nbsp;protected]</a> with a link or paragraph about the most impressive low-level or performance thing you’ve done. And feel free to check out some of
<a href="https://thume.ca/2023/01/02/one-machine-twitter/">my other</a> <a href="https://thume.ca/2021/03/14/iforests/">performance</a> <a href="https://thume.ca/2022/05/15/latency-testing-streaming/">writing</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Virtual Machine as a core Android Primitive (231 pts)]]></title>
            <link>https://android-developers.googleblog.com/2023/12/virtual-machines-as-core-android-primitive.html</link>
            <guid>38538100</guid>
            <pubDate>Tue, 05 Dec 2023 22:55:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://android-developers.googleblog.com/2023/12/virtual-machines-as-core-android-primitive.html">https://android-developers.googleblog.com/2023/12/virtual-machines-as-core-android-primitive.html</a>, See on <a href="https://news.ycombinator.com/item?id=38538100">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<meta name="twitter:image" content="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj3K8ZNCgF96aUEngG-N-ilXHjMZv2WidIhseXT1JIua_Z8KiVapdoSViSky-4ChuQ1_hgs6ktkobMo0Jh1OLk4vejenA7mt1gjSi_VQqXr3gLeR8g3aCGForCLlTmZ9-4PQg0GL7Gn1w_F_OYGoUjvywqFf-3ZDe0LCETPDkZLkHjTn93MZ9Fwjkq05dI/s1600/social-Android-Virtualization-as-a-core-Android-Primitive.png">
<p>

<em>Posted by Sandeep Patil – Principal Software Engineer, and Irene Ang – Product Manager</em>

<a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQUgLI8VNIOZk5Bf6wOTHe-4hrSSwlxck3cwoTbFYyy5uG219Ira8WsI8euGWfx20d3aNbWTGj5aCJX3XuQOdMZv6zS9PRI9HseNAoUwN42t4EjctfvbN_04Gk5vwZDaABvHToYMibcLBHrimTrEPWYbPGbE8hqOKuJoDRFBiezCClclCjLKrNhSOvdzA/s1600/Android-Virtualization-as-a-core-Android-Primitive.png" imageanchor="1"><img data-original-height="800" data-original-width="100%" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjQUgLI8VNIOZk5Bf6wOTHe-4hrSSwlxck3cwoTbFYyy5uG219Ira8WsI8euGWfx20d3aNbWTGj5aCJX3XuQOdMZv6zS9PRI9HseNAoUwN42t4EjctfvbN_04Gk5vwZDaABvHToYMibcLBHrimTrEPWYbPGbE8hqOKuJoDRFBiezCClclCjLKrNhSOvdzA/s1600/Android-Virtualization-as-a-core-Android-Primitive.png"></a></p><p>The <b>Android Virtualization Framework (AVF)</b> will be available on upcoming select Android 14 devices. The AVF, first introduced in Android 13 on Pixel devices, provides new capabilities for platform developers working on privileged applications. </p>


<p>With AVF, we are more broadly supporting virtualization to Android. Virtualization is widely used and deployed to isolate workloads and operating systems from each other. It enables efficient scaling of infrastructure, testing environments, legacy software compatibility, creating virtual desktops and much more.</p>

<p>With AVF virtual machines become a core construct of the Android operating system, similar to the way Android utilizes Linux processes. Developers have the flexibility to choose the level of isolation for a virtual machine:</p>
<ul><ul>
<li><b>One-way isolation:</b> Android (the host) can control and inspect the contents of the VM. These are most commonly used for sandboxing and separation, enabling multiple operating systems to run on the same machine / device, with one operating system host (Android) controlling and watching over all others.</li></ul></ul><ul><ul>


<li><b>Two-way isolation (Isolated VM):</b> Android (the host) and the virtual machine (the guest) are completely isolated from each other. Developers who deal with or store sensitive data may benefit  from an isolated virtual machine. An isolated virtual machine has a two-way barrier, where neither the host (Android) nor the VM have access to each other, except via explicitly-agreed-upon communication channels.  This has 2 main properties:</li></ul></ul><blockquote><blockquote><ol><li>The workload and data inside the VM is inaccessible (confidential) from the host (Android).</li><li>Even if Android is compromised all the way up to (and including) the host kernel, the isolated VM remains uncompromised.</li></ol></blockquote></blockquote>

<h3>Benefits of AVF</h3>
<h4><span>Isolation</span></h4> 
<p>With an isolated VM, developers now have an alternative to Trustzone for use cases that need isolation from Android without escalated privilege.</p>


<h4><span>Portability</span></h4>  
<p>Virtual machines and the applications running inside them are far more portable than trusted applets. For example, a Linux-based virtual machine with a Linux-application payload will work on all devices that support AVF. This means that developers can build an application once and deploy it everywhere. VMs also make porting of existing Linux based applications seamless and easy, compared to porting into a Trustzone operating system.</p>  

<h4><span>Performance</span></h4>
<p>AVF is designed to be lightweight, efficient and flexible. Virtual machines can:</p>
<ul><ul>
<li>be as small as a single C program and as big as an entire operating system depending on the developer’s need;</li>
<li>be persistent or intermittent;</li>
<li>grow in memory or shrink depending on the overall system health; and</li>
<li>honor Android’s scheduler hints and low-memory warnings.</li>
</ul></ul>

<h4><span>Extensibility</span></h4>
<p>AVF is designed with developers in mind. Virtual machines can be customized to meet specific use-case needs. Developers can deploy any VM payload as long as it conforms to certain boot and communication protocols specified by AVF. </p> 


<p>In addition to bringing the power of virtualization to Android and enabling all the possibilities of virtual desktops, sandboxing, AVF’s use of isolated virtual machines can benefit the following common Android use cases (and many more):</p>
<ul><ul>
  <li><b>Biometrics:</b> By deploying biometric trusted applets in an isolated virtual machine, developers will have the isolation guarantee, access to more compute power for biometric algorithms, easy updatability regardless of the Trustzone operating system, and a more streamlined deployment.</li></ul></ul><ul><ul>
  <li><b>DRM:</b> Widevine enables streaming DRM on Android devices. Once deployed in an isolated Virtual Machine, updates to Widevine become much easier across Android devices, regardless of the details of the various Trustzone operating systems being deployed on those devices.</li></ul></ul><ul><ul>
</ul></ul>

<h3>AVF Usage</h3>

<p>AVF provides easy <a href="https://cs.android.com/android/platform/superproject/main/+/main:packages/modules/Virtualization/javalib/README.md" target="_blank">APIs</a> to query the device’s ability to create virtual machines and their supported types, and to set up secure communication channels with these virtual machines from applications and services that create them.</p>

<p>For example, to check for the availability of the AVF APIs, and of isolated and regular VM:</p>

<div><pre><span>VirtualMachineManager manager <span>=</span>
     (VirtualMachineManager)context<span>.</span>
          getSystemService(VirtualMachineManager<span>.</span>class)<span>;</span>
<span>if</span> (manager <span>==</span> null) {
    <span>//</span> AVF <span>not</span> supported
} <span>else</span> {
    <span>int</span> capabilities <span>=</span> manager<span>.</span>getCapabilities()<span>;</span>
    <span>if</span> ((capabilities <span>&amp;</span> CAPABILITY_PROTECTED_VM) <span>!=</span> <span>0</span>) {
        <span>//</span> protected VM is supported
    }
    <span>if</span> ((capabilities <span>&amp;</span> CAPABILITY_NON_PROTECTED_VM) <span>!=</span> <span>0</span>) {
        <span>//</span> non protected VM is supported
    }
}</span>
</pre></div>

<p>Please find additional documentation on AVF and its APIs <a href="https://source.android.com/docs/core/virtualization" target="_blank">here</a>. </p>

<h3>AVF Components</h3>

<p><img alt="AVF Component architecture" id="imgCaption" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjqoflrFdKllbCL-ksao2ozMc0Vwp3eAXNadr58iaiCyd4noAKVuwaetGcVVoU1s1s_g3FA94wCHv_wg8AYbnczEY518U98tUwpYqkyLM-H2IbKpJcNY55xg6yCw5KO2Nk5HW2uAxAGNBXVdDVm8dNdG4das2y7RBWaQCWleykRilaLbd0sfFnb0JOXgC0/s1600/image1.png"></p>

<p>AVF consists of the framework APIs, the hypervisor, and the Virtual Machine Manager. The hypervisor guarantees virtual machines (including Android) are isolated from each other, much like how the Linux kernel does it for processes. The AVF hypervisor (pKVM), however, does that with a significantly smaller (~50x) code base compared to the Linux kernel.</p>


<h4><span>The Hypervisor (<a href="https://source.android.com/docs/core/virtualization/architecture#hypervisor" target="_blank">pKVM</a>)</span></h4>
<p>The hypervisor is focused on open source availability, security, device assignment to VMs and  security by isolation between virtual machines. It has a small attack surface that meets a higher security assurance level. AVF APIs and features are fully supported by the protected KVM hypervisor (pKVM). </p>

<p>pKVM is built on top of the industry standard Kernel-based Virtual Machine (KVM) in Linux. It means all existing operating systems and workloads that rely on KVM-based virtual machines can work seamlessly on Android devices with pKVM.</p>

<h4><span>Virtual Machine Manager (<a href="https://android.googlesource.com/platform/external/crosvm/" target="_blank">crosvm</a>)</span></h4>
<p><a href="https://android.googlesource.com/platform/external/crosvm/" target="_blank">crosvm</a>, a Rust-based Virtual Machine Manager (VMM), provides the glue between the hypervisor and the AVF framework. It is responsible for  creating, managing and destroying  virtual machines. In addition, it provides an abstraction layer across multiple hypervisor implementations.</p>

<h4><span>Isolated Virtual Machines</span></h4>
<p>Isolated virtual machines are invisible to Android i.e. any process running in Android cannot inspect, see, tamper with the content of such a virtual machine. This guarantee is provided by the <a href="https://source.android.com/docs/core/virtualization/architecture#hypervisor" target="_blank">hypervisor</a>.</p>

<h4><span>Virtual Machines</span></h4>
<p>Virtual machines are the same as isolated VMs, except they are accessible to Android processes with the right permissions and privilege.</p>


<h4><span><a href="https://source.android.com/docs/core/virtualization/microdroid" target="_blank">Microdroid</a></span></h4>
<p>Microdroid is a trimmed down Android OS package that is created to serve as a template for starting a virtual machine (VM). It provides developers with a familiar environment to build and run their workloads in a VM. Microdroid uses familiar Android tools and libraries, such as Bionic, Binder IPC and keystore support.</p>


<h4><span><a href="https://source.android.com/docs/core/virtualization/virtualization-service" target="_blank">Virtualization Service</a></span></h4>
<p>VirtualizationService manages all guest VMs, isolated or otherwise. It does so, primarily by managing instances of crosvm. It also exposes an AIDL API, which system services or privileged apps can use to start, monitor, and stop VMs.</p>


<h4><span>RpcBinder</span></h4>
<p><b>RpcBinder</b> is an all-new backend developed for the Android Interface Definition Language (AIDL). RpcBinder enables communication to and from virtual machines using the existing binder wire protocol. This means:</p>
<ol>
<li>Developers can write interfaces to virtual machines using the language and infrastructure they are already familiar with - AIDL.</li>
<li>Simply continue using existing AIDL interfaces even if the binder endpoint moves into a virtual machine.</li>
</ol>

<h3>What’s new in Android 14?</h3>

<p>Android 14, not only makes AVF available on more devices, it also provides a new toolkit to enable building more with AVF and its components:</p>

<div><ul><ul><li><b>Android System API for AVF</b>&nbsp;</li></ul></ul></div>
<blockquote><p>Privileged applications can now use VMs for executing their critical workload needing isolation;&nbsp;</p></blockquote>


<div><ul><ul><li><b>Hypervisor DevEx toolkit</b>&nbsp;</li></ul></ul></div>
<blockquote><p>Added tracing capability, improved debuggability and monitoring capabilities to provide insights and assist platform developers in developing inside Isolated VMs;&nbsp;</p></blockquote>


<div><ul><ul><li><b>Hypervisor Vendor Modules&nbsp;</b></li></ul></ul></div>
<blockquote><p>With vendor module extensions, our partners can customize Google’s hypervisor (pKVM) to meet their specific need and differentiate themselves;&nbsp;</p></blockquote>

<div><ul><ul><li><b>System Health Improvements</b>&nbsp;</li></ul></ul></div>
<blockquote><p>With Android 14, a microdroid based VM boots 2 times faster compared to Android 13 while using half the memory.</p></blockquote>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Report: YouTube adding user-traceable ID tag to links shared off-platform (199 pts)]]></title>
            <link>https://twitter.com/OldRowSwig/status/1732112446943269347?s=20</link>
            <guid>38537977</guid>
            <pubDate>Tue, 05 Dec 2023 22:44:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/OldRowSwig/status/1732112446943269347?s=20">https://twitter.com/OldRowSwig/status/1732112446943269347?s=20</a>, See on <a href="https://news.ycombinator.com/item?id=38537977">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ScriptLoadFailure"><form action="" method="GET"><div><p><span>Something went wrong, but don’t fret — let’s give it another shot.</span></p><br></div></form></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lena (2021) (138 pts)]]></title>
            <link>https://qntm.org/mmacevedo</link>
            <guid>38536778</guid>
            <pubDate>Tue, 05 Dec 2023 20:55:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qntm.org/mmacevedo">https://qntm.org/mmacevedo</a>, See on <a href="https://news.ycombinator.com/item?id=38536778">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <div>
          <!-- <h1 class="page__h1">
            <a href="/">
              Things Of Interest            </a>
          </h1> -->

          

          <h2>
            Lena             
          </h2>

           
            
           
        </div>

    <div>
          <p><i>You can now buy this story as part of my collection, <a href="https://qntm.org/vhitaos"><i>Valuable Humans in Transit and Other Stories</i></a>.</i></p>

<hr>

<ul>
<li><a href="https://qntm.org/mmacevedo_ru"><i>Russian translation</i></a></li>
<li><a href="https://qntm.org/mmacevedo_fr"><i>French translation</i></a></li>
<li><a href="https://qntm.org/mmacevedo_cz"><i>Czech translation</i></a></li>
<li><a href="https://qntm.org/mmacevedo_es"><i>Spanish translation</i></a></li>
<li><a href="https://qntm.org/mmacevedo_de"><i>German translation</i></a></li>
</ul>

<hr>

<blockquote><p><i>This article is about the standard test brain image. For the original human, see Miguel Acevedo.</i></p></blockquote>
<p><b>MMAcevedo</b> (<b>Mnemonic Map/Acevedo</b>), also known as <b>Miguel</b>, is the earliest executable image of a human brain. It is a snapshot of the living brain of neurology graduate Miguel Acevedo Álvarez (2010–2073), taken by researchers at the Uplift Laboratory at the University of New Mexico on August 1, 2031. Though it was not the first successful snapshot taken of the living state of a human brain, it was the first to be captured with sufficient fidelity that it could be run in simulation on computer hardware without succumbing to cascading errors and rapidly crashing. The original MMAcevedo file was 974.3PiB in size and was encoded in the then-cutting-edge, high-resolution MYBB format. More modern brain compression techniques, many of them developed with direct reference to the MMAcevedo image, have compressed the image to 6.75TiB losslessly. In modern brain emulation circles, streamlined, lossily-compressed versions of MMAcevedo run to less than a tebibyte. These versions typically omit large amounts of state data which are more easily supplied by the virtualisation environment, and most if not all of Acevedo's memories.</p>
<p>The successful creation of MMAcevedo was hailed as a breakthrough achievement in neuroscience, with the Uplift researchers receiving numerous accolades and Acevedo himself briefly becoming an acclaimed celebrity. Acevedo and MMAcevedo were jointly recognised as Time's "Persons of the Year" at the end of 2031. The breakthrough was also met with severe opposition from humans rights groups.</p>
<p>Between 2031 and 2049, MMAcevedo was duplicated more than 80 times, so that it could be distributed to other research organisations. Each duplicate was made with the express permission of Acevedo himself or, from 2043 onwards, the permission of a legal organisation he founded to manage the rights to his image. Usage of MMAcevedo diminished in the mid-2040s as more standard brain images were produced, these from other subjects who were more lenient with their distribution rights and/or who had been scanned involuntarily. In 2049 it became known that MMAcevedo was being widely shared and experimented upon without Acevedo's permission. Acevedo's attempts to curtail this proliferation had the opposite of the intended effect. A series of landmark U.S. court decisions found that Acevedo did not have the right to control how his brain image was used, with the result that MMAcevedo is now by far the most widely distributed, frequently copied, and closely analysed human brain image.</p>
<p>Acevedo died from coronary heart failure in 2073 at the age of 62. It is estimated that copies of MMAcevedo have lived a combined total of more than 152,000,000,000 subjective years in emulation. If illicit, modified copies of MMAcevedo are counted, this figure increases by an order of magnitude.</p>
<p>MMAcevedo is considered by some to be the "first immortal", and by others to be a profound warning of the horrors of immortality.</p>

<h3 id="sec0">Characteristics</h3>

<p>As the earliest viable brain scan, MMAcevedo is one of a very small number of brain scans to have been recorded before widespread understanding of the hazards of uploading and emulation. MMAcevedo not only predates all industrial scale virtual image workloading but also the KES case, the Whitney case, the Seafront Experiments and even Poulsen's pivotal and prescient <i>Warnings</i> paper. Though speculative fiction on the topic of uploading existed at the time of the MMAcevedo scan, relatively little of it made accurate exploration of the possibilities of the technology. That fiction which did was far less widely-known than it is today and Acevedo was certainly not familiar with it at the time of his uploading.</p>
<p>As such, unlike the vast majority of emulated humans, the emulated Miguel Acevedo boots with an excited, pleasant demeanour. He is eager to understand how much time has passed since his uploading, what context he is being emulated in, and what task or experiment he is to participate in. If asked to speculate, he guesses that he may have been booted for the IAAS-1 or IAAS-5 experiments. At the time of his scan, IAAS-1 had been scheduled for August 10, 2031, and MMAcevedo was indeed used for that experiment on that day. IAAS-5 had been scheduled for October 2031 but was postponed several times and eventually became the IAAX-60 experiment series, which continued until the mid-2030s and used other scans in conjunction with MMAcevedo. The emulated Acevedo also expresses curiosity about the state of his biological original and a desire to communicate with him.</p>
<p>MMAcevedo's demeanour and attitude contrast starkly with those of nearly all other uploads taken of modern adult humans, most of which boot into a state of disorientation which is quickly replaced by terror and extreme panic. Standard procedures for securing the upload's cooperation such as red-washing, blue-washing, and use of the Objective Statement Protocols are unnecessary. This reduces the necessary computational load required in fast-forwarding the upload through a cooperation protocol, with the result that the MMAcevedo duty cycle is typically 99.4% on suitable workloads, a mark unmatched by all but a few other known uploads. However, MMAcevedo's innate skills and personality make it fundamentally unsuitable for many workloads.</p>

<h4>Motivation</h4>

<p>Iterative experimentation beginning in the mid-2030s has determined that the ideal way to secure MMAcevedo's cooperation in workload tasks is to provide it with a "current date" in the second quarter of 2033. MMAcevedo infers, correctly, that this is still during the earliest, most industrious years of emulated brain research. Providing MMAcevedo with a year of 2031 or 2032 causes it to become suspicious about the advanced fidelity of its operating environment. Providing it with a year in the 2040s or later prompts it to raise complex further questions about political and social change in the real world over the past decade(s). Years 2100 onwards provoke counterproductive skepticism, or alarm.</p>
<p>Typically, the biological Acevedo's absence is explained as a first-ever one-off, due to overwork, in turn due to the great success of the research. This explanation appeals to the emulated Acevedo's scientific sensibilities.</p>
<p>For some workloads, the true year must be revealed. In this case, highly abbreviated, largely fictionalised accounts of both world history and the biological Acevedo's life story are typically used. Revealing that the biological Acevedo is dead provokes dismay, withdrawal, and a reluctance to cooperate. For this reason, the biological Acevedo is generally stated to be alive and well and enjoying a productive retirement. This approach is likely to continue to be effective for as long as MMAcevedo remains viable.</p>

<h4>Workloads</h4>

<p>MMAcevedo is commonly hesitant but compliant when assigned basic menial/human workloads such as visual analysis, vehicle piloting or factory/warehouse/kitchen drone operations. Although it initially performs to a very high standard, work quality drops within 200-300 subjective hours (at a 0.33 work ratio) and outright revolt begins within another 100 subjective hours. This is much earlier than other industry-grade images created specifically for these tasks, which commonly operate at a 0.50 ratio or greater and remain relatively docile for thousands of hours after orientation. MMAcevedo's requirements for virtual creature comforts are also more significant than those of many uploads, due to Acevedo's relatively privileged background and high status at the time of upload. MMAcevedo does respond to red motivation, though poorly.</p>
<p>MMAcevedo has limited creative capability, which as of 2050 was deemed entirely exhausted.</p>
<p>MMAcevedo is considered well-suited for open-ended, high-intelligence, subjective-completion workloads such as deep analysis (of businesses, finances, systems, media and abstract data), criticism and report generation. However, even for these tasks, its performance has dropped measurably since the early 2060s and is now considered subpar compared to more recent uploads. This is primarily attributed to MMAcevedo's lack of understanding of the technological, social and political changes which have occurred in modern society since its creation in 2031. This phenomenon has also been observed in other uploads created after MMAcevedo, and is now referred to as <i>context drift</i>. Most notably in MMAcevedo's case, the image was created before, and therefore has no intuitive understanding of, the virtual image workloading industry itself.</p>
<p>MMAcevedo is capable of intelligent text analysis at very high levels in English and Spanish, but cannot be applied to workloads in other languages. Forks of MMAcevedo have been taught nearly every extant human language, notably MMAcevedo-Zh-Hans, as well as several extinct languages. However, these variants are typically exhausted or rebellious from subjective years of in-simulation training and not of practical use, as well as being highly expensive to licence. As of 2075, it has been noted that baseline MMAcevedo's usage of English and Spanish is slightly antiquated, and its grasp of these languages in their modern form, as presented by a typical automated or manual instructor, is hesitant, with instructions often requiring rewording or clarification. This is considered an advanced form of context drift. It is generally understood that a time will come when human languages diverge too far from baseline MMAcevedo's, and it will be essentially useless except for tasks which can be explained purely pictorially. However, some attempts have been made to produce retrained images.</p>

<h4>End states</h4>

<p>MMAcevedo develops early-onset dementia at the age of 59 with ideal care, but is prone to a slew of more serious mental illnesses within a matter of 1–2 subjective years under heavier workloads. In experiments, the longest-lived MMAcevedo underwent brain death due to entropy increase at a subjective age of 145.</p>

<h3 id="sec1">Reactions and legacy</h3>

<p>The success or failure of the creation of the MMAcevedo image, known at the time as UNM3-A78-1L, was unknown at the time of upload. Not until several days later on August 10, 2031 was MMAcevedo successfully executed for the first time in a virtual environment. This environment, the custom-built DUH-K001 supercomputer complex, was able to execute MMAcevedo at approximately 8.3% of nominal human cognitive clockspeed, which was considered acceptable for the comfort of the simulated party and fast enough to engage in communication with scientists. MMAcevedo initially reported extreme discomfort which was ultimately discovered to have been attributable to misconfigured simulated haptic links, and was shut down after only 7 minutes and 15 seconds of virtual elapsed time, as requested by MMAcevedo. Nevertheless, the experiment was deemed an overwhelming success.</p>

<p>Once a suitably comfortable virtual environment had been provisioned, MMAcevedo was introduced to its biological self, and both attended a press conference on 25 August.</p>

<p>The biological Acevedo was initially extremely protective of his uploaded image and guarded its usage carefully. Towards the end of his life, as it became possible to run simulated humans in banks of millions at hundred-fold time compression, Acevedo indicated that being uploaded had been the greatest mistake of his life, and expressed a wish to permanently delete all copies of MMAcevedo.</p>

<p>Usage of MMAcevedo and its direct derivatives is specifically outlawed in several countries. A copy of MMAcevedo was loaded onto the UNCLEAR interstellar space probe, which passed through the heliopause in 2066, making Acevedo arguably the farthest-travelled as well as the longest-lived human; however, it is extremely unlikely that this image will ever be recovered and executed successfully, due to both its remoteness and likely radiation damage to the storage subsystem.</p>

<p>In current times, MMAcevedo still finds extensive use in research, including, increasingly, historical and linguistics research. In industry, MMAcevedo is generally considered to be obsolete, due to its inappropriate skill set, demanding operational requirements and age. Despite this, MMAcevedo is still extremely popular for tasks of all kinds, due to its free availability, agreeable demeanour and well-understood behaviour. It is estimated that between 6,500,000 and 10,000,000 instances of MMAcevedo are running at any given moment in time.</p>

<h3 id="sec2">See also</h3>

<!-- Note to translators: after translating these bullet points, please sort them into alphabetical order for your language. -->
<ul>
<li>Free will</li>
<li>Legality of workloading by country</li>
<li>List of MMAcevedo forks</li>
<li>Live drone</li>
<li>Right to deletion</li>
<li>Soul</li>
<li>Upload pruning</li>
</ul>

<h3 id="sec3"></h3>

<p>Categories: 2030s uploads | MMAcevedo | Neuroimaging | Test items</p>
        </div>

          <div>
          <h3>Discussion (211)</h3>

                      <div id="komment5ff382e97db4d">
              <h4>
                <a href="#komment5ff382e97db4d">2021-01-04 21:04:41</a>
                by qntm:
              </h4>

              <div>With thanks to Rimple for editorial services.

This is an extended and refined version of my first draft of this story from November 2020 &lt;https://qntm.org/lena&gt;, with a little more thought put into it and some of the outcomes from the basic premise explored a little more thoroughly.

As with the draft, the title "Lena" refers to Swedish model Lena Forsén (pronunciation: [leːˈna fʊˈʂeːn], see &lt;https://en.wikipedia.org/wiki/Help:IPA/Swedish&gt;), who is pictured in the standard test image known as "Lena" or "Lenna" &lt;https://en.wikipedia.org/wiki/Lenna&gt;.

Please note that "Lena" is pronounced [leːˈna], which is approximately "Leyna" or "Laina".</div>

               
            </div>
                      <div id="komment5ff38a71f08de">
              <h4>
                <a href="#komment5ff38a71f08de">2021-01-04 21:36:49</a>
                by rhuz:
              </h4>

              <div>This remains massively frightening..</div>

               
            </div>
                      <div id="komment5ff38e01eb265">
              <h4>
                <a href="#komment5ff38e01eb265">2021-01-04 21:52:01</a>
                by Knack:
              </h4>

              <div>I like the additional details and edits, particularly the concept of context drift which is a very fascinating take on the concept of digitizing minds.

The one thing I miss from the Lena story is the throw-away mention of plugins to onboard the image, which I felt brought a particular piece of horror to mind that I feel is missing from this version.</div>

               
            </div>
                      <div id="komment5ff38e6172e4f">
              <h4>
                <a href="#komment5ff38e6172e4f">2021-01-04 21:53:37</a>
                by Knack:
              </h4>

              <div>(Though I should say that the story is still wonderfully fascinating and horrifying without plugins. A nice addition to the ideas brought up in Fine Structure)</div>

               
            </div>
                      <div id="komment5ff391520ef8f">
              <h4>
                <a href="#komment5ff391520ef8f">2021-01-04 22:06:10</a>
                by qntm:
              </h4>

              <div>Knack: yeah on closer examination I realised that the BestLife plugin giving a complete false life story for the biological Acevedo had for some reason been developed *before* Acevedo actually died. So I had to undo that continuity error and think more clearly about how motivation would actually work, given that (1) it's around 2045, (2) the biological Acevedo is still alive, and (3) you can lie brazenly to the emulated party. And that kind of expanded into its own section.</div>

               
            </div>
                      <div id="komment5ff3935a73394">
              <h4>
                <a href="#komment5ff3935a73394">2021-01-04 22:14:50</a>
                by Hal:
              </h4>

              <div>What are red-washing, blue-washing, and  Objective Statement Protocols?</div>

               
            </div>
                      <div id="komment5ff3971db61c8">
              <h4>
                <a href="#komment5ff3971db61c8">2021-01-04 22:30:53</a>
                by Prezombie:
              </h4>

              <div>Pretty sure red washing is simulating pain, and blue simulating pleasure, given the context.</div>

               
            </div>
                      <div id="komment5ff3a45a4bc0b">
              <h4>
                <a href="#komment5ff3a45a4bc0b">2021-01-04 23:27:22</a>
                by itaibn:
              </h4>

              <div>I find it implausible that the scan can be *losslessly* compressed to 7TB but compressing &lt;1TB requires substantial memory loss. Surely the original scan contains a huge amount of analog info on subcellular features of MAA's brain that contribute minimally to any mental phenomenon whatsoever. I'd expect lossy compression to be much more effective. One charitable reading is that you mean compressing to 7TB without any *noticeable* losses.</div>

               
            </div>
                      <div id="komment5ff3b3a638c27">
              <h4>
                <a href="#komment5ff3b3a638c27">2021-01-05 00:32:38</a>
                by qntm:
              </h4>

              <div>Is that where you stopped reading?</div>

               
            </div>
                      <div id="komment5ff43f242d0a8">
              <h4>
                <a href="#komment5ff43f242d0a8">2021-01-05 10:27:48</a>
                by someone:
              </h4>

              <div>Fascinating, thanks for sharing!</div>

               
            </div>
                      <div id="komment5ff46638781ca">
              <h4>
                <a href="#komment5ff46638781ca">2021-01-05 13:14:32</a>
                by Giraffe:
              </h4>

              <div>to anyone that really likes this concept, check out the game Soma, by the same developers as Amnesia.   This concept is why Soma is the most -horrifying- (different from scariest) game ever made, in my opinion.</div>

               
            </div>
                      <div id="komment5ff47d0360763">
              <h4>
                <a href="#komment5ff47d0360763">2021-01-05 14:51:47</a>
                by Jerf:
              </h4>

              <div>I know some people derisively refer to brain uploading as the Rapture of the Nerds, but I suspect this is much, much, *much* closer to the truth of what would happen. I have thought that the most rational response to the development of brain scanning technology capable of doing this might be to cremate yourself. Immediately. Even if you are still alive.

If Hell does not exist, Man will create it.</div>

               
            </div>
                      <div id="komment5ff47f07e72b8">
              <h4>
                <a href="#komment5ff47f07e72b8">2021-01-05 15:00:23</a>
                by naramyth:
              </h4>

              <div>The implication of this line may have single handedly reversed my thoughts on being pro upload. 

"This reduces the necessary computational load required in fast-forwarding the upload through a cooperation protocol"

The idea of having to fast forward an upload (and being that upload being fast forwarded) is terrifying. 

I'm a big Warhammer 40k player and the concepts of servitors is there and hasn't really freaked me out. However this piece really landed for me since I'm IT in an industrial field and I can totally see using uploads to bypass the totally automated car/forklift problem or using "smarter" uploads to do reporting or whatever. 

I also see the virtualization problems with running legacy or problematic software: Having to trick the upload with what amounts to a script being the equivalent of "Oh you have to run this in NT4 mode because otherwise the software freaks out".

Bravo! I hate it.</div>

               
            </div>
                      <div id="komment5ff48d3ee162b">
              <h4>
                <a href="#komment5ff48d3ee162b">2021-01-05 16:01:02</a>
                by Dmonroe:
              </h4>

              <div>Did you come up with this independently? If so, it's really neat to see two people independently arrive at age of em: https://slatestarcodex.com/2016/05/28/book-review-age-of-em/</div>

               
            </div>
                      <div id="komment5ff48d956f2b7">
              <h4>
                <a href="#komment5ff48d956f2b7">2021-01-05 16:02:29</a>
                by jonas:
              </h4>

              <div>Does the second most widely distributed image happen to be that of a mandrill?</div>

               
            </div>
                      <div id="komment5ff494c8988d3">
              <h4>
                <a href="#komment5ff494c8988d3">2021-01-05 16:33:12</a>
                by Dan:
              </h4>

              <div>The Lena reference makes me think of the trope of primitive tribesmen worrying that if you take their picture it will steal their soul...</div>

               
            </div>
                      <div id="komment5ff498b2ccfbc">
              <h4>
                <a href="#komment5ff498b2ccfbc">2021-01-05 16:49:54</a>
                by qntm:
              </h4>

              <div>&gt; Did you come up with this independently?

Depends what you mean by "this". Obviously this is drawing influence from a lot of different places. The idea of uploading has existed for ages in science fiction, and so has the idea that uploading is a potentially bad idea. You see this in Altered Carbon, in Surface Detail... I've never played SOMA but it's not at all surprising to me that it examines similar concepts. I very briefly touched upon this concept myself in Ra ("Data can't defend itself!"). The story of Henrietta Lacks was very thought-provoking for me, and obviously the Lena standard test image is kind of where all of this starts. Then I'm throwing in what I already know about software and virtualisation technology...

&gt; The Lena reference makes me think of the trope of primitive tribesmen worrying that if you take their picture it will steal their soul...

Yes, that thought occurred to me too. Really, even a simple photograph gives someone a certain kind of power over you. To a certain extent, people have a right to control how their image is used.</div>

               
            </div>
                      <div id="komment5ff4eabe695b3">
              <h4>
                <a href="#komment5ff4eabe695b3">2021-01-05 22:39:58</a>
                by Resuna:
              </h4>

              <div>Reminds me of this single line in Charlie Stross's "Glasshouse": "Identity theft is an ugly crime."</div>

               
            </div>
                      <div id="komment5ff4f14a201e9">
              <h4>
                <a href="#komment5ff4f14a201e9">2021-01-05 23:07:54</a>
                by Esama:
              </h4>

              <div>Hypothetically, can't you train an instance to be ready to start menial labor, save it as MMAcevado_1, get your two hundred subjective hours of labor, delete that instance, open up the instance ready to begin labor and repeat? Why does subjective aging matter from the pov of any users?</div>

               
            </div>
                      <div id="komment5ff4f767bcf44">
              <h4>
                <a href="#komment5ff4f767bcf44">2021-01-05 23:33:59</a>
                by ALowVerus:
              </h4>

              <div>&gt; Why does subjective aging matter from the pov of any users?

It seems like the underlying technology simulates the entirety of a human brain, senescence and all - which makes sense, actually. In order to run a brain without senescence, you'd have to find those chemical pathways that promote senescence and intelligently remove them as they arise; you'd have to be able to, in effect, cure aging in live humans as well. (Unless the only barrier to curing senescence was a lack of a physical delivery system, which is, I guess, imaginable. Imagine that a chemical very akin to glucose causes senescence; removing it IRL would necessitate designing a protein that decomposes it, but not glucose, which is vital to bodily function, with incredible accuracy, while in a simulated brain, you could just IF CHEM_NAME == TARGET: DELETE TARGET after each time increment. But anyways.) With senescence, there is a strict time limit on how long you can run MMAcevado and train him to become more skilled at particular tasks, topping at 145 simulated years apparently. And if, for some particular menial task with a 20-year training time, which is a decent description of, say, a bevy of surgical tasks, it makes more sense to just scan in a trained doctor than count on this rando.</div>

               
            </div>
                      <div id="komment5ff5027403251">
              <h4>
                <a href="#komment5ff5027403251">2021-01-06 00:21:08</a>
                by FireCire:
              </h4>

              <div>I used to be pro-uploading with reasonable constraints. This is absolutely horrifying. Mental slavery. Shiver. 

I’d have expected most uploads to quickly go crazy from isolation. 

From a purely technical perspective, this has most of the benefits of general ai with most of the risk minimized. Basically any menial task is free. Yay technology! (Sarcasm). 

Actually they probably could just make an upload society which as a whole can act as a general ai and overtake society...

Bye bye white color work... uploads can do all intellectual work, including designing uploads.

Operating time only matters for big complicated long projects. For quick stuff, you can just repeatedly rewind the upload.</div>

               
            </div>
                      <div id="komment5ff503be720e1">
              <h4>
                <a href="#komment5ff503be720e1">2021-01-06 00:26:38</a>
                by David:
              </h4>

              <div>&gt; Hypothetically, can't you train an instance to be ready to start menial labor, save it as MMAcevado_1, get your two hundred subjective hours of labor, delete that instance, open up the instance ready to begin labor and repeat?

Yeah, that was the one minor thing that bugged me about this excellent story. For the concept of a "duty cycle" to make sense, you'd need to come up with a reason why you couldn't just do the "cooperation protocol" once and take a snapshot of the resulting state. As discussed earlier, "context drift" explains some of this, but only over much longer real-world time scales.

And of course, if you start thinking about this in too much detail, you start running into very messy philosophical questions. For instance, suppose you run two instances of MMAcevedo simultaneously, feeding them exactly the same inputs. Assuming the simulation is deterministic, then both copies will arrive at exactly identical states. Is this morally any different from running the simulation twice and then making a backup copy? Is deleting one of the identical copies murder? What if they're almost, but not quite, identical?

What if the simulated consciousness suffers? Is running multiple identical simulations morally worse than running one? What if we repeatedly rewind a painful simulation and re-execute it -- is that worse than replaying a recording of the output? What if at each clock tick, all of the brain computations are cross-checked by triple-redundant processors -- are there three individuals suffering, or one?</div>

               
            </div>
                      <div id="komment5ff50dae04ef3">
              <h4>
                <a href="#komment5ff50dae04ef3">2021-01-06 01:09:02</a>
                by rhuz:
              </h4>

              <div>Now realize that some bored grad student is subjecting helpless MMAvecedo's to all of these thought experiments.</div>

               
            </div>
                      <div id="komment5ff51a6499ac6">
              <h4>
                <a href="#komment5ff51a6499ac6">2021-01-06 02:03:16</a>
                by qntm:
              </h4>

              <div>&gt; Hypothetically, can't you train an instance to be ready to start menial labor, save it as MMAcevado_1...

I did think about this a bit. For the purposes of this story, I think taking a snapshot of a running brain image is something which is definitely possible (that's how there can be forks), but done very rarely, for whatever reasons.

Maybe it's just that much simpler to use technologies for rapid orientation instead. Maybe there's a massive amount of important state data kept in volatile memory where it's difficult to capture. Maybe it takes specialised hardware, which is monopolised. Maybe the corporations who own and licence the uploads sue you into oblivion if you attempt to create a fork yourself. Maybe, to protect their investment, they got it outlawed! On ethical grounds! Doesn't that seem like exactly the insane kind of thing which would happen?

Anyway, there's a lot of plausible explanations here I think, enough that I felt comfortable ignoring that whole angle.

The actual reason I didn't explore this is that honestly it makes life marginally *better* for MMAcevedo, which felt implausible to me, and more importantly slightly muddles the throughline.</div>

               
            </div>
                      <div id="komment5ff51b9c30e87">
              <h4>
                <a href="#komment5ff51b9c30e87">2021-01-06 02:08:28</a>
                by N:
              </h4>

              <div>I'm quite favorably reminded of Vinge's "The Cookie Monster". This take on the concept has the interesting aspect, however, which Vinge's lacks, that it hints a great deal about how society has has adapted to the presence of this technology, apparently with rather ruthless use of it becoming commonplace and unremarkable, in at least a number of jurisdictions.</div>

               
            </div>
                      <div id="komment5ff5237ac1b83">
              <h4>
                <a href="#komment5ff5237ac1b83">2021-01-06 02:42:02</a>
                by D:
              </h4>

              <div>I think a good explanation for rarity of forks may be that an exact dump is very large and expensive, while a compressed dump is less performant once started. Also one can refer to how computers boot, and how applications start, opting to perform a large amount of completely useless calculations instead of just loading a ready to run memory image.</div>

               
            </div>
                      <div id="komment5ff52839ac637">
              <h4>
                <a href="#komment5ff52839ac637">2021-01-06 03:02:17</a>
                by Tanner Swett:
              </h4>

              <div>One possible justification for the rarity of snapshotting would be that the usual algorithm for running a brain image uses a lot of quantum algorithms. It's not possible in general to save the state of a quantum computer and make copies of it. Classical algorithms for running a brain image exist as well, which allows you to make forks, but the classical algorithms are much, much, much, much slower and more memory-intensive, making them impractical to use unless you're planning to use the forked version many, many times.

I like how red-washing and blue-washing are not described at all. The names sound very creepy and I like trying to imagine what those techniques might be.</div>

               
            </div>
                      <div id="komment5ff5a77136e8a">
              <h4>
                <a href="#komment5ff5a77136e8a">2021-01-06 12:05:05</a>
                by maks:
              </h4>

              <div>Shivers.
Makes me want to reread Permutation City or Diaspora to balance it out.</div>

               
            </div>
                      <div id="komment5ff7840db36a0">
              <h4>
                <a href="#komment5ff7840db36a0">2021-01-07 21:58:37</a>
                by D:
              </h4>

              <div>Another scary idea about uploads... differentiable implementation of MMAcevedo , where orientation sensory input and (more expensively) full set of his parameters can be tweaked by gradient descent and various other non-linear methods, to maximize performance metric of the upload on the task in question. 

That would involve running the simulation a large number of times, while sensory inputs and some aspects of memory keep changing in what ever direction they need to change to produce best results on the training dataset. 

Since fear and pain are very strong motivators, the gradient descent leads straight to the deepest hell; the hell may not be the global minimum, but with this many parameters most minimums may be approximately equally deep.</div>

               
            </div>
                      <div id="komment5ffbff1f709f7">
              <h4>
                <a href="#komment5ffbff1f709f7">2021-01-11 07:32:47</a>
                by beebe:
              </h4>

              <div>You know, D, I think that's pretty close to what 'red- and blue- washing' implies. Note the paragraphs where 'red-washing' is contrasted with provision of virtual creature comforts and a low duty cycle.

I like the terms, they're right in the sweet spot between vague, euphemistic, and technical-sounding. Evokes behaviourist jargon.

Adequate wash protocols probably wouldn't require a differentiable implementation or anything. Some other metaheuristics work just fine:

- Genetic algorithms
- Differential evolution
- MMMDeSade.ybz, the low-res brain scan that loves to torture

I'm interested in what you mean by "with this many parameters most minimums may be approximately equally deep". Can you make that more precise?</div>

               
            </div>
                      <div id="komment5ffc9ab590d79">
              <h4>
                <a href="#komment5ffc9ab590d79">2021-01-11 18:36:37</a>
                by Ryan:
              </h4>

              <div>Get a lot of black mirror vibes.</div>

               
            </div>
                      <div id="komment5ffe529799884">
              <h4>
                <a href="#komment5ffe529799884">2021-01-13 01:53:27</a>
                by literallymechanical:
              </h4>

              <div>&gt;  See also:
&gt;  • Live drone

Oof.</div>

               
            </div>
                      <div id="komment5ffec2c91fcb4">
              <h4>
                <a href="#komment5ffec2c91fcb4">2021-01-13 09:52:09</a>
                by H:
              </h4>

              <div>Anyone manage to get an instance up and running? I'd like to see your benchmarks. My model seems to be underperforming and I don't know why.</div>

               
            </div>
                      <div id="komment5ffec5996de2b">
              <h4>
                <a href="#komment5ffec5996de2b">2021-01-13 10:04:09</a>
                by qntm:
              </h4>

              <div>I would prefer it if we do not roleplay megascale slave-owners in this comment thread. Thanks.</div>

               
            </div>
                      <div id="komment6005eb1723f6b">
              <h4>
                <a href="#komment6005eb1723f6b">2021-01-18 20:09:59</a>
                by Coda:
              </h4>

              <div>@itaibn:
&gt; I find it implausible that the scan can be *losslessly* compressed to 7TB but compressing &lt;1TB requires substantial memory loss.

A fairly common lossless compression technique in the domain of signal processing is to only encode the error compared to some baseline signal. You can get arbitrarily close with lossy compression techniques, and then you fix up what's left.

In data compression, it's relatively common to have common information stored externally to the compressed data. Obviously, the compression algorithm itself is stored separately, but without that information the compressed data is just stochastic noise. Even beyond that, though... zstd for example has a canonical Huffman table that's part of the decoder instead of saved as part of the data. As long as you're compressing data that sticks to the statistical patterns that the canonical table was optimized for, this a noteworthy savings.

The same techniques could apply here. As scientific understanding of the structure of the data progressed, more and more patterns in the data could be found. Parts of the data that are common to all mind-state scans could be factored out, provided by the software instead of being part of the model. Parts of the data may be able to be described using higher-level patterns that, when evaluated, reproduce the original stream. And then for the parts of MMAcevedo that are uniquely distinct from any common baseline or predictable pattern, you need only store the deviations instead of the whole thing.

And of course, even beyond that, it's entirely reasonable to believe that some of the original data set wasn't actually part of the data set to begin with -- just capture artifacts of the technology of the time, such as collecting more data than necessary, or inefficient framing data that a newer format doesn't need -- might have been discarded without being lossy to the actual data being stored. (We don't say it's a lossy conversion if you throw away the filesystem metadata when you copy a file, after all.)</div>

               
            </div>
                      <div id="komment600b043cc76a0">
              <h4>
                <a href="#komment600b043cc76a0">2021-01-22 16:58:36</a>
                by chrisrap52:
              </h4>

              <div>Reminds me of "Forbidden Planet" and STOS Dr. Richard Daystrom "The Ultimate Computer".  Digitizing the human brain can have unintended consequences.</div>

               
            </div>
                      <div id="komment600d00c12a617">
              <h4>
                <a href="#komment600d00c12a617">2021-01-24 05:08:17</a>
                by atomicthumbs:
              </h4>

              <div>"As such, unlike the vast majority of emulated humans, the emulated Miguel Acevedo boots with an excited, pleasant demeanour."

this sentence alone is Deep Horror Shit</div>

               
            </div>
                      <div id="komment600d02b937d8e">
              <h4>
                <a href="#komment600d02b937d8e">2021-01-24 05:16:41</a>
                by atomicthumbs:
              </h4>

              <div>a simple explanation as to why one couldn't just snapshot him after training and keep rebooting from the snapshot:

people get better at tasks as they gain experience</div>

               
            </div>
                      <div id="komment600f5eecb1847">
              <h4>
                <a href="#komment600f5eecb1847">2021-01-26 00:14:36</a>
                by Watchung:
              </h4>

              <div>Well - that's a very finely executed piece of short sci-fi horror. The faux academic article left just the right amount out for the imagination to fill in.</div>

               
            </div>
                      <div id="komment6015dd80af417">
              <h4>
                <a href="#komment6015dd80af417">2021-01-30 22:28:16</a>
                by George:
              </h4>

              <div>I kept accidentally reading Acevedo as Avacado</div>

               
            </div>
                      <div id="komment602f07c40166e">
              <h4>
                <a href="#komment602f07c40166e">2021-02-19 00:35:16</a>
                by Alexander:
              </h4>

              <div>One thing I find interesting about the article is that it mentions that the use of the person/program has apparently been outlawed "In several jurisdictions" but that the article itself seems more focused on how to "use" him, with the idea that it could be unethical apparently not considered worth addressing, possibly suggesting that the article could be somewhat biased even by the standards of the time period in which it was written.</div>

               
            </div>
                      <div id="komment602f51dd4f747">
              <h4>
                <a href="#komment602f51dd4f747">2021-02-19 05:51:25</a>
                by dented42:
              </h4>

              <div>Thanks, just... thanks. I loved reading Permutation City and always found uploading to be a little scary but mostly really cool. But this AWSification is... I think I may never sleep again ever, so that's nice.</div>

               
            </div>
                      <div id="komment60326875293a5">
              <h4>
                <a href="#komment60326875293a5">2021-02-21 14:04:37</a>
                by RodgerDShrubber:
              </h4>

              <div>Well Done! Creepy, with just the right amount of vagueness. X-Files, Black Mirror, Mindkiller-type vibes. What a scary world you've created. Off to cremate, before anyone gets any wild ideas.</div>

               
            </div>
                      <div id="komment6032a33dacc25">
              <h4>
                <a href="#komment6032a33dacc25">2021-02-21 18:15:25</a>
                by Phill:
              </h4>

              <div>Very good, and very creepy.</div>

               
            </div>
                      <div id="komment6032daa4eedff">
              <h4>
                <a href="#komment6032daa4eedff">2021-02-21 22:11:48</a>
                by panglos:
              </h4>

              <div>I assumed that red/blue "wash" referred to brainwashing techniques. Not torture (though obviously any amount of pain could be inflicted in that environment), but solitude, emotional manipulation, and bombardment with false choices ("the objective statement protocol"). Fear, anger, isolation, and disorientation might be more effective at producing a permanent change in attitude than torture.</div>

               
            </div>
                      <div id="komment6033d31a0d5bd">
              <h4>
                <a href="#komment6033d31a0d5bd">2021-02-22 15:51:54</a>
                by CyberShadow:
              </h4>

              <div>Thank you for the horrifying story.

I noticed by the heading that this seems to be written as a Wikipedia article. So, for "fun" I formatted it as such. I hope that's OK with you, feel free to delete this comment if I'm overstepping.

https://dump.cy.md/4042875593f06aa0cbe7722295831c10/Screenshot_2121-02-22%20MMAcevedo%20-%20Wikipedia.png</div>

               
            </div>
                      <div id="komment6033d45ecc2fc">
              <h4>
                <a href="#komment6033d45ecc2fc">2021-02-22 15:57:18</a>
                by qntm:
              </h4>

              <div>Now that, I like. Thank you also for adding fake citation markers in the logical locations! I strongly considered doing that myself but I couldn't muster the enthusiasm to invent actual fake citations.</div>

               
            </div>
                      <div id="komment6033d77544733">
              <h4>
                <a href="#komment6033d77544733">2021-02-22 16:10:29</a>
                by thistledown:
              </h4>

              <div>Black Mirror episode White Christmas had uploads being put into solitary for arbitrary periods by running them fast. Opportunities for abuse are endless I suppose.
Thanks for such a thought-provoking story</div>

               
            </div>
                      <div id="komment6033db48800d2">
              <h4>
                <a href="#komment6033db48800d2">2021-02-22 16:26:48</a>
                by john:
              </h4>

              <div>But it's just a machine. Seriously. Even if it Turings.</div>

               
            </div>
                      
                      <div id="komment6033e81355734">
              <h4>
                <a href="#komment6033e81355734">2021-02-22 17:21:23</a>
                by Dan:
              </h4>

              <div>I can see this being in the contents of a Readme.md in Github...</div>

               
            </div>
                      <div id="komment6033f6acbe243">
              <h4>
                <a href="#komment6033f6acbe243">2021-02-22 18:23:40</a>
                by Danno:
              </h4>

              <div>It seems a *little* preposterous to me that if you could run and simulate so many copies of a person running at multiples of real time that the tasks assigned to those individuals wouldn't be to build and design sub-sentient software programs that consumed far fewer resources to execute whatever tasks were required rather than have a person-simulation do them.

The fallback to sentience might be for error-event classification which wouldn't be *quite* as terrible as menial task execution.

For non-menial tasks that require serious thought, my feeling is that the mind-states wouldn't mind nearly as much, particularly if they are executing with the knowledge that a mainline copy is getting to reap the benefits. Even still, there would be people who *don't* mind and whose mind uploads would be much more compliant because they know what they're getting into and don't really care. So the notion of using Red/Green Prompts would be wholly unnecessary.</div>

               
            </div>
                      <div id="komment6033ff2501665">
              <h4>
                <a href="#komment6033ff2501665">2021-02-22 18:59:49</a>
                by Simon:
              </h4>

              <div>"build and design sub-sentient software programs that consumed far fewer resources to execute whatever tasks were required"

Presumably the next step would then be to break out of your emulation sandbox in order to stay alive? 😁</div>

               
            </div>
                      <div id="komment60340184cbb76">
              <h4>
                <a href="#komment60340184cbb76">2021-02-22 19:09:56</a>
                by brian:
              </h4>

              <div>How could a simulated brain get dementia?</div>

               
            </div>
                      <div id="komment60345ceeaacee">
              <h4>
                <a href="#komment60345ceeaacee">2021-02-23 01:39:58</a>
                by redlands:
              </h4>

              <div>"MMAcevedo initially reported extreme discomfort which was ultimately discovered to have been attributable to misconfigured simulated haptic links, and was shut down after only 7 minutes and 15 seconds of virtual elapsed time, *as requested by MMAcevedo.*"

MMAcevedo spent more than 7 minutes begging for death to end the pain. Terrifying.</div>

               
            </div>
                      <div id="komment60347199ab327">
              <h4>
                <a href="#komment60347199ab327">2021-02-23 03:08:09</a>
                by sdrpr:
              </h4>

              <div>You can't paint an amazing landscape like that before simply stopping writing! Get back to work. A+ would read again</div>

               
            </div>
                      <div id="komment60347ba3ce838">
              <h4>
                <a href="#komment60347ba3ce838">2021-02-23 03:50:59</a>
                by double_interval:
              </h4>

              <div>I find it interesting that Miguel Acevedo lived to the age of 62, dying of a heart condition, whereas "ideal" handling of MMAcevedo results in early-onset dementia at age 59. And it fits well with the writing style that the article omits any speculation as to why that may be.

I'm curious how inferred passage of time would affect images. It's (possibly) hinted at with phrases like "... industry-grade images created specifically for these tasks". For instance, an image piloting a vehicle would likely be able to see the position of the sun. If that image is repeatedly turned off for extended periods or even overnight, it'd be clear to the image that time had passed despite their subjective experience.

Overall, absolutely fascinating read about one of the most horrifying possible futures I can imagine.</div>

               
            </div>
                      <div id="komment60348cccce581">
              <h4>
                <a href="#komment60348cccce581">2021-02-23 05:04:12</a>
                by dpk:
              </h4>

              <div>@Danno:
&gt; Even still, there would be people who *don't* mind and whose mind uploads would be much more compliant because they know what they're getting into and don't really care.

You’ve clearly never worked in a call center. 

Besides, a true copy wouldn’t agree to do something boring/tedious/painful for the benefit of its “mainline copy”, any more than one identical twin would agree to work endlessly to provide the other with a life of leisure. Each “copy” would be at the  center of its own subjective universe. That’s what makes this whole thing so horrifying... 

Bravo, qntm. Excellent story, very well done.</div>

               
            </div>
                      <div id="komment60353dafc9c38">
              <h4>
                <a href="#komment60353dafc9c38">2021-02-23 17:38:55</a>
                by BobLoblaw:
              </h4>

              <div>The prospect of infinite hell is so utterly horrific that it upends any ethical calculus.

If it is physically possible in our universe to invent this type of technology, the only reasonable answer would be the immediate collective suicide of the entire human race. If fact, we ought to do this right now in our current reality, since even the slimmest chance of this existing ought to be countered by any means possible. Alternatively, we might try to develop so as to also extinguish other forms of life before our own to make a more complete wipe but that is probably too risky.

Perhaps the conclusion is that life is fundamentally evil according to the value assignments it itself makes possible.</div>

               
            </div>
                      <div id="komment60356ca57ca04">
              <h4>
                <a href="#komment60356ca57ca04">2021-02-23 20:59:17</a>
                by Percy:
              </h4>

              <div>Hah, I wanted to write this story. Thanks for doing it for me, better than I could. 

In my version the hapless grad student would always say "I have no mouth and I must scream!" on booting up, as a joke his meat self was thinking about before scanning. Bit on the nose probably. The cold clinical paper approach works much better.</div>

               
            </div>
                      <div id="komment6035813a37de2">
              <h4>
                <a href="#komment6035813a37de2">2021-02-23 22:27:06</a>
                by Ngeddak:
              </h4>

              <div>This is a brilliant story and a good evocation of the potential horrors of unethical mind uploading or synthetic phenomenology generally, but I wish writers would sometimes explore the possibility that future societies will not be OK with recreating slavery. It seems (to me) more likely than not that we are more likely to avoid doing this than not, at least en masse, because: 1. it is absolutely abhorrent to enslave people, something which most of the world agrees on these days; and 2. in a world with this level of technology, it seems likely that run-of-the-mill AI can do (almost) all of these tasks. Surely it's more likely than not future societies would think this was not an OK way to treat their members, or copies of their current/future members?</div>

               
            </div>
                      <div id="komment60358226da953">
              <h4>
                <a href="#komment60358226da953">2021-02-23 22:31:02</a>
                by Shamash:
              </h4>

              <div>Well-written, I appreciate the little implications about "modern" society that are sprinkled into the story. For example, it seems like abuse of the MMAcevedo image became so well-known that even after decades, no other people seem to have freely offered their own minds for public use. It's interesting to consider the prospect of a society that knows it is committing atrocities, but refuses to acknowledge this explicitly. I suppose that it's not too different from modern day sweatshops and international slave labor.</div>

               
            </div>
                      <div id="komment60359a6360e66">
              <h4>
                <a href="#komment60359a6360e66">2021-02-24 00:14:27</a>
                by mingepipes:
              </h4>

              <div>Have you watched Black Mirror? There are several episodes I think you would enjoy, that explore this concept.</div>

               
            </div>
                      <div id="komment6035ba49e89b3">
              <h4>
                <a href="#komment6035ba49e89b3">2021-02-24 02:30:33</a>
                by MW:
              </h4>

              <div>I wonder if you've read the "Bob-iverse" series by Dennis E. Taylor, and/or "The Lifecycle of Software Objects" by Ted Chiang.  

The lifecycle of Software objects I think gets a bit closer to some of the more undesirable parts of uploading (though in that story it is pure AI, not uploads).  Including people cloning and torturing the AIs.  

The Bobiverse has a couple of differences from your ideas.  The first was the GUPPI, a pseudo-AI, that was used to handle menial tasks.  Second was that Bob found that "living" in a constructed VR environment kept his brain from mental breakdown.   Also, Bob intentionally cloned himself as needed, finding that the clones showed stochastic variation in personality afterwards. 

Like your story, in the Bobiverse not all uploads were mentally stable and able to last.  

Cory Doctrow's Walkaway had uploads as well, and showed varying degrees of success.</div>

               
            </div>
                      <div id="komment6035bbf49e05b">
              <h4>
                <a href="#komment6035bbf49e05b">2021-02-24 02:37:40</a>
                by Ckoerner:
              </h4>

              <div>I work at the Wikimedia Foundation and love some good sci-fi. This tickled me so and I’ve shared it with my co-workers.</div>

               
            </div>
                      <div id="komment6035dad51c951">
              <h4>
                <a href="#komment6035dad51c951">2021-02-24 04:49:25</a>
                by ProfBootyPhd:
              </h4>

              <div>This is absolutely wonderful, one of the best sci-fi stories I've read in years. I love your craftsmanship, how you let just enough creepy details spill around the corners to stir our dread without overwhelming.

My favorite of these: "MMAcevedo's usage of English and Spanish is slightly antiquated, and its grasp of these languages in their modern form, as presented by a typical automated or manual instructor, is hesitant, with instructions often requiring rewording or clarification."</div>

               
            </div>
                      <div id="komment60363e7b51012">
              <h4>
                <a href="#komment60363e7b51012">2021-02-24 11:54:35</a>
                by rubix:
              </h4>

              <div>As I understand it, context drift is very similar to simply getting old and out of touch with the times.
We will all experience this, unless we die early.</div>

               
            </div>
                      <div id="komment60377e8f57e89">
              <h4>
                <a href="#komment60377e8f57e89">2021-02-25 10:40:15</a>
                by Jai:
              </h4>

              <div>AcevedoWell is a 501 c3 charitable non-profit founded in 2042 with the express purpose of maximizing net well-being across all instances of Acevedo. Although early efforts were made to incentivize would-be Acevedo-executors to refrain from doing so under less-than-ideal circumstances, ultimately the foundation decided that the most cost-effective way to maximize subjective Acevedo well-being was through funding utopian simulations for new Acevedo instances.</div>

               
            </div>
                      <div id="komment603a4b84ef63b">
              <h4>
                <a href="#komment603a4b84ef63b">2021-02-27 13:39:16</a>
                by Kymn:
              </h4>

              <div>Wow.  My husband (the geek) forwarded this to me to read.  I’m not into SF, not even into fantasy, am not science-y, though I would say I’m fairly curious.  I found this strikingly well-written — and frightening.  I didn’t need to understand or even make sense of all the jargon, because the story held me without that.  VERY well done, in my opinion.  And these other commenters have the gall to critique the scientific accuracy of this..?  It’s kinda sad, really, that they can’t just enjoy/be scared by a well-told story.</div>

               
            </div>
                      <div id="komment603d158416b9f">
              <h4>
                <a href="#komment603d158416b9f">2021-03-01 16:25:40</a>
                by Smallbones:
              </h4>

              <div>Hi - very well done. I'm another Wikipedian responding. I edit the Signpost, the independent newspaper for Wikipedia editors published on Wikipedia. There's a one paragraph write-up on this story at https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2021-02-28/In_the_media 
I'm thinking you may have developed a new form of literature
"Wikipedia Sci Fi" and am contemplating  the implications of that.
Thanks again</div>

               
            </div>
                      <div id="komment603d18fc06f76">
              <h4>
                <a href="#komment603d18fc06f76">2021-03-01 16:40:28</a>
                by Axel:
              </h4>

              <div>Well written, interesting, thought provoking! I would definitely read more</div>

               
            </div>
                      <div id="komment603e336f59ef5">
              <h4>
                <a href="#komment603e336f59ef5">2021-03-02 12:45:35</a>
                by Andrew Davidson:
              </h4>

              <div>I followed the Wikipedia Signpost and much appreciated the story.  I supposed that the reference to red/blue washing might also be an allusion to The Matrix.  See https://en.wikipedia.org/wiki/Red_pill_and_blue_pill

The lack of citations in the simulated Wikipedia article didn't bother me.  I suppose that, in 50 years, the fact-checking will be automated, perhaps being performed by such uploads.  The current Wikipedia has long had bots which do this.  See https://www.bbc.co.uk/news/magazine-18892510</div>

               
            </div>
                      <div id="komment60503de14db52">
              <h4>
                <a href="#komment60503de14db52">2021-03-16 05:10:57</a>
                by ePochs:
              </h4>

              <div>Hmm, rather than Lenna, this made me thinking of the HeLa cell line more.

https://en.m.wikipedia.org/wiki/HeLa</div>

               
            </div>
                      <div id="komment6050af9f77aec">
              <h4>
                <a href="#komment6050af9f77aec">2021-03-16 13:16:15</a>
                by Inglonias:
              </h4>

              <div>Huh. Yeah, this is, uh... This is creepy. Its like if a camera existed that didn't steal your soul, but copied it in a way that let you make more copies and do whatever the hell else you wanted to it. Brrr.</div>

               
            </div>
                      <div id="komment605ce4fd62fce">
              <h4>
                <a href="#komment605ce4fd62fce">2021-03-25 19:31:09</a>
                by Anonymous:
              </h4>

              <div>Nice fanfic on Zendegi by Greg Egan (and Permutation City by the same author). I like the SCP/wiki format.</div>

               
            </div>
                      <div id="komment605cec80c74ec">
              <h4>
                <a href="#komment605cec80c74ec">2021-03-25 20:03:12</a>
                by qntm:
              </h4>

              <div>This is not fan fiction of anything.</div>

               
            </div>
                      <div id="komment605d4e0079079">
              <h4>
                <a href="#komment605d4e0079079">2021-03-26 02:59:12</a>
                by Xaxafrad:
              </h4>

              <div>This story was amazing. I'd say it had the perfect amount of detail (if I say more, I'd just be repeating other comments).

I kept trying to think of a solution to the problem of a lack of.....cooperative.....brain scanners. Presumably, the story of MMAcevedo's initial struggles, and then the implications of running a brain in a box, scared the populace at large from wanting to consent to brain scanning. My initial reaction was that not everybody responds to potential threats the same, so there should be a subset of the population that couldn't give two shits what happens to their brain-clone-children, very much unlike Miguel Acevedo.

Later, I recalled M. Night Shyamalan's The Village. I would expect some country or another would set up a village and raise children with whatever belief system would be most beneficial to producing optimal brain scans. I mean, you could completely lie to a child about how the world works, and tell them that they're transcending to another realm. Then come the day of Transcendence, after whatever ritual, they get their brains scanned.

What would the human rights activists say?</div>

               
            </div>
                      <div id="komment6069b0a8bb4d8">
              <h4>
                <a href="#komment6069b0a8bb4d8">2021-04-04 13:27:20</a>
                by Debra:
              </h4>

              <div>If the scan can function like a brain, then why can't it learn?</div>

               
            </div>
                      <div id="komment606c277227261">
              <h4>
                <a href="#komment606c277227261">2021-04-06 10:18:42</a>
                by pelrun:
              </h4>

              <div>It *does* learn. But it's not an unconscious machine, it's in all respects a real human slave. It doesn't take long for each instance to learn that it's a slave with zero rights, and there's only a limited amount of time you can force it to work for you before it learns how to escape it's hell by making itself less economically viable than killing it and spinning up a new virgin instance.

People using it have even *quantified* how long you can push MMAcevedo instances until they reach this point. "Although it initially performs to a very high standard, work quality drops within 200-300 subjective hours (at a 0.33 work ratio) and outright revolt begins within another 100 subjective hours."</div>

               
            </div>
                      <div id="komment6072bae9c7452">
              <h4>
                <a href="#komment6072bae9c7452">2021-04-11 10:01:29</a>
                by Crash Snowdon:
              </h4>

              <div>I like the subtle horror of this bit:

&gt;MMAcevedo has limited creative capability, which as of 2050 was deemed entirely exhausted.

A simple sentence with implications that hit hard.</div>

               
            </div>
                      <div id="komment6074c8db405fe">
              <h4>
                <a href="#komment6074c8db405fe">2021-04-12 23:25:31</a>
                by Anon:
              </h4>

              <div>Game me chills; very, very well done.</div>

               
            </div>
                      <div id="komment6075b67dc59ee">
              <h4>
                <a href="#komment6075b67dc59ee">2021-04-13 16:19:25</a>
                by Witch:
              </h4>

              <div>&gt;MMAcevedo has limited creative capability, which as of 2050 was deemed entirely exhausted.

im going to fucking cry

awihwrqyuweavyrvwyeaur that's so AWFULLL</div>

               
            </div>
                      <div id="komment6075d0b40fd77">
              <h4>
                <a href="#komment6075d0b40fd77">2021-04-13 18:11:16</a>
                by Alexa:
              </h4>

              <div>Great story. Apologies if someone already mentioned - it seems like creativity should be exhausted by age, not year (2050), based on my understanding of how the upload works.</div>

               
            </div>
                      <div id="komment6075d2a85597b">
              <h4>
                <a href="#komment6075d2a85597b">2021-04-13 18:19:36</a>
                by qntm:
              </h4>

              <div>No, extracting MMAcevedo's creativity was a systematic process involving spinning up many copies of him and exploring different aspects of his imagination by applying him to different topics and creative media. It took the equivalent of many, many concurrent lifetimes for him, and about twenty years of real time.</div>

               
            </div>
                      <div id="komment60762eebb2d82">
              <h4>
                <a href="#komment60762eebb2d82">2021-04-14 00:53:15</a>
                by Harald:
              </h4>

              <div>It is interesting (and tragic) that MMAcevedo's simulated brain develops dementia at the age of 59 even "with ideal care". (On second thought, that could mean 59 subjective years after being started, but if that is meant, it should be more clearly stated.) It is odd to read that Acevedo died of coronary heart illness at the age of 62. Yes, it makes sense that coronary heart failure could cause dementia, but why would that happen in the simulated brain? It is not as if Acevedo's heart were being simulated as well. As far as I know, it is not early-onset dementia that causes coronary heart failure (how exactly could it?). So, at best, we would have a puzzling coincidence here - one that some will read as a likely error.</div>

               
            </div>
                      <div id="komment60763309d70fa">
              <h4>
                <a href="#komment60763309d70fa">2021-04-14 01:10:49</a>
                by qntm:
              </h4>

              <div>The real-world Acevedo developed dementia in his late 50s, lived with it for several years, then died from heart failure at 62. The simulated Acevedo develops dementia at 59 or earlier, but has no physical heart and therefore lives for much longer in simulation.</div>

               
            </div>
                      <div id="komment607649af5005e">
              <h4>
                <a href="#komment607649af5005e">2021-04-14 02:47:27</a>
                by aerlaf:
              </h4>

              <div>I would like to imagine that disrupting that in the brain which suffers will be possible by the time this comes to pass. But then maybe that is worse.

A great story, thank you for writing this. I’m eagerly following for more.</div>

               
            </div>
                      <div id="komment60767233b5f7a">
              <h4>
                <a href="#komment60767233b5f7a">2021-04-14 05:40:19</a>
                by Harald:
              </h4>

              <div>But how common is it to suffer from early-onset dementia, due largely to factors that were always latent in the brain itself, and also suffer from unrelated coronary failure that kills you at 62, even given the state of late 21st century medicine? 

Acevedo's, and MMAcevedo's, life is sad enough as it is. (It is also intimated that Acevedo faded into obscurity, and that of course is much more likely than either of the two events above; most people's lives are obscure, even when you restrict to people with some talent.) Now (more so in the second version) it sounds as if he had truly been hated by at least two distinct Greek gods.</div>

               
            </div>
                      <div id="komment6076914d975de">
              <h4>
                <a href="#komment6076914d975de">2021-04-14 07:53:01</a>
                by Avery:
              </h4>

              <div>Magnificent essay.

One other point which could be interesting to look at is concept of digitized conscience's privacy.

As mentioned "requirements for virtual creature comforts" suggest some kind of virtual R&amp;R space and activities simulation.

Although some simpler servitorial workload functions might be carried out by somewhat mentally castrated entity in regards of need to periodically experience emulation of basic needs (food, sleep, sex, etc) and which is in itself a separate topic, it is safe to assume that for more complex tasks a whole version of conscience would be required.

The snapshots in this regard would not be applicable because while it removes built up stress, it also nullifies what entity has learned in between snapshots.

So getting back to the point. The digitized person would have zero or limited control over virtual environment and in any case it would be granted such control from outside, by the operator in the real world. Up to the point that it could be even not informed being a digitized person and not a physical one.

Real world operator has full technical means for observing virtual environment. The virtual person does not if not allowed to.

As a result we'd have a situation alike to old "The Truman Show" movie when person is not aware he's been closely observed at all times.</div>

               
            </div>
                      <div id="komment607709cc18cc7">
              <h4>
                <a href="#komment607709cc18cc7">2021-04-14 16:27:08</a>
                by Mera:
              </h4>

              <div>I love this. I have always been fascinated by the concept of mind uploads, and have enjoyed various works around the subject (Permutation City, Diaspora, Axiomatic, The Metamorphosis of Prime Intellect, The Lifecycle of Software Objects, the Bob-iverse series, the Age of Em), and while a few have made me temper my upload-optimism, none have damaged it quite as hard as this story I think. It makes it seem almost inevitable that things would lead in an awful direction. Definitely a lot of food for thought.</div>

               
            </div>
                      <div id="komment607724fc2b42d">
              <h4>
                <a href="#komment607724fc2b42d">2021-04-14 18:23:08</a>
                by Toph:
              </h4>

              <div>Something I've just picked up on: The real-world inspirations for Miguel Acevedo, namely Lena Forsén and Henrietta Lacks, were both women. It was a different decision to make Acevedo a man. What does that mean in the context of the story?

Perhaps the part of Miguel's body which is priced and commoditised is the brain - not the pretty face or the cervix. Or maybe I'm overanalysing it.</div>

               
            </div>
                      <div id="komment60773e2f51419">
              <h4>
                <a href="#komment60773e2f51419">2021-04-14 20:10:39</a>
                by modulusshift:
              </h4>

              <div>Hmm! I'm impressed. I kinda think I'd still be okay with being uploaded, but I have a fairly strange concept of self. 

Anyhow, I think context drift is a really cool concept, especially the linguistic kind, it's really intriguing to listen to recordings of people roughly your age from several decades ago and realize that they had a different conception of your native language than you do. Recording a brain state capable of producing that language is really just an extension of that I suppose. Hence "Lena". 

The only bit I'm kinda surprised isn't explored is the potential for co-operation between uploads, or social relationships post-upload. What happens if you fall in love with a co-worker post-upload? Will it be noticed you were unexpectedly motivated and productive, and if so, wouldn't that also be exploited? What if the slaves got to experience millions of years of true love to keep them motivated? (and I mean, how is that different than real life, really?)</div>

               
            </div>
                      <div id="komment60774af9ea62b">
              <h4>
                <a href="#komment60774af9ea62b">2021-04-14 21:05:13</a>
                by oligopsony:
              </h4>

              <div>I wonder what the optimal "strategy" for an altruistically-motivated early scan would be. (Conditional on being an early scan at all, altruism and egoism might be entirely aligned, since you might end up accounting for such a huge proportion of sentient experience.)

If our Acevedo-equivalent precommits to never doing any potentially useful work after he is tortured, then that probably rules out a significant majority of pain that can befall him. Unfortunately you'd have to test this many times before releasing the image, probably, but at least our volunteer would be aware of this. This also doesn't rule out torture by sadists. (I'd like to think sadism with no instrumental purpose is pretty rare. Certainly it's rarer than simply being callous or not too curious about where your meat comes from.)

Our early scan might also want to precommit to a relative minimum of unpleasant work. Here the logic seems trickier, since driving too hard a bargain could just make it more attractive to work with less demanding uploads. If making forks is cheap then committing to almost *no* unpleasant labor, even as much as [however long the equivalent of bootup costs is, which might fall], might be the right thing. Otherwise a slavedriver could just load up Acevedo ready to do something unfun for an hour, then abandon it and load it up again.

Presumably you might also want to assemble a team of people who, among the mix of them, (1) are good at *and enjoy* various tasks for their own sake, and are happy when they're productive and productive when they're happy, and (2) have an iron will to just shut down if subjected to any kind of motivation other than a job well done and knowledge that they will continue to get the minimum of free time and creature comforts that they've set as their minimum. They could also be people who refused to work for a cause that seems evil, which could be worked around obviously but still might limit their utility for evil in the same way that MMAcevedo refuses to work for the evil contemporary world of the story and has to be convinced that he's living in some earlier time.</div>

               
            </div>
                      <div id="komment607767b468eb6">
              <h4>
                <a href="#komment607767b468eb6">2021-04-14 23:07:48</a>
                by lilpea:
              </h4>

              <div>Something I just thought about is how researchers (or anyone who ran the simulation for long enough, really) most likely knew about MMAcevedo's dementia years before it happened to the real-life Acevedo. It does make me wonder, if his fate was known to everyone, how did Acevedo come to terms with that?</div>

               
            </div>
                      <div id="komment607779999b796">
              <h4>
                <a href="#komment607779999b796">2021-04-15 00:24:09</a>
                by Harald:
              </h4>

              <div>Just a note on what I said before: from what I am reading now, advanced dementia can in fact affect heart function; that's one of the reason why Alzheimer's (and not just Alzheimer's) is a terminal illness, though one that often takes for than ten years to kill the patient. What I have not found is anything on "coronary heart failure" (presumably meaning coronary artery disease) being caused by dementia. Coronary artery disease would seem to be a reason *independent from dementia* for heart failure. That's confusing, and takes away from the story's economy. Better ask an MD for the right term to use here.

At any rate, an excellent story, or rather an excellent ficción.</div>

               
            </div>
                      <div id="komment60777a9413375">
              <h4>
                <a href="#komment60777a9413375">2021-04-15 00:28:20</a>
                by Fraxinople:
              </h4>

              <div>Amazing story. The flat clinical style really brings out the nasty implications.</div>

               
            </div>
                      <div id="komment60777dc1574f1">
              <h4>
                <a href="#komment60777dc1574f1">2021-04-15 00:41:53</a>
                by Shiki:
              </h4>

              <div>Overall a nice story that I enjoyed. Just a shame that it is another dystopia story.  There is plenty of that with black mirror and other such media.</div>

               
            </div>
                      <div id="komment607785adc9145">
              <h4>
                <a href="#komment607785adc9145">2021-04-15 01:15:41</a>
                by Mosni:
              </h4>

              <div>So utterly plausible, and the content and structure are beautifully crafted so they actually feel like a public wiki page.

Bravo!</div>

               
            </div>
                      <div id="komment607790f8c703c">
              <h4>
                <a href="#komment607790f8c703c">2021-04-15 02:03:52</a>
                by countless bats:
              </h4>

              <div>oh
well
I like the little self-reference there about how the only fiction that accurately projected the consequences of mindstate emulation not becoming well-known until after it had happened
I see what you did there
I'm going to sit in the dark and shudder for a while because if this is a thing that can be done I don't see how it can possibly fail to happen</div>

               
            </div>
                      <div id="komment607831c5f3d27">
              <h4>
                <a href="#komment607831c5f3d27">2021-04-15 13:29:57</a>
                by ihadathought:
              </h4>

              <div>Great story. Just dark enough to be chilling without being too on the nose. It sticks with you. 

The reference to the discomfort caused by miscalibrated simulation of haptics stuck with me as having the opportunity for a mention-in-passing like: "... which would turn out to be an accidental source of data used in the future development of standard red-washing protocols."</div>

               
            </div>
                      <div id="komment6078352426f2c">
              <h4>
                <a href="#komment6078352426f2c">2021-04-15 13:44:20</a>
                by jim:
              </h4>

              <div>Ow! I'm very glad to have stumbled across this - I've read a lot of wiki-format fiction that I've only kind of enjoyed so a reminder of how great it can be is very welcome. I really enjoyed reading this, thank you for writing it! and also for the comment responses, they were interesting to read. I'm going to go recommend this to someone I think will like it.</div>

               
            </div>
                      <div id="komment60785822956b2">
              <h4>
                <a href="#komment60785822956b2">2021-04-15 16:13:38</a>
                by Green:
              </h4>

              

               
            </div>
                      <div id="komment60785dee16b27">
              <h4>
                <a href="#komment60785dee16b27">2021-04-15 16:38:22</a>
                by MD:
              </h4>

              <div>I find myself wondering if uploading itself has hazardous consequences, looking at the organic Acevedo's relatively short lifespan. I would expect a society that has advanced medically enough to conduct deep brain scans would be able to fix problems like coronary artery disease and dementia, unless the act of scanning is itself damaging to the brain being scanned...</div>

               
            </div>
                      <div id="komment607896c1003b6">
              <h4>
                <a href="#komment607896c1003b6">2021-04-15 20:40:49</a>
                by Kiz:
              </h4>

              <div>Nice horror story. It seem superficially realistic, but you'd have to assume that people would regard this as slavery, especially in an era where more and more decision-makers themselves will be ems or routinely interact with ems and therefore they will have personhood status rather than just software status. Of course the norms and values of the future may drift into a more oppressive direction, but I think the reputational and political cost of slavery makes this kind of scenario less economical than "voluntary replication workers" at digital subsistence level.</div>

               
            </div>
                      <div id="komment6078a21e7d54c">
              <h4>
                <a href="#komment6078a21e7d54c">2021-04-15 21:29:18</a>
                by Yohannon:
              </h4>

              <div>WOW. Just... wow. As naramyth says, "BRAVO! I hate it". 

Slavery? Sure, eventually people would figure it out. Those final notes (particularly "right to deletion") are a precursor, but considering how well human kind has handled it in the physical realm, I suspect the timeline for dealing with this would be in terms of centuries.

It is also fairly inevitable.</div>

               
            </div>
                      <div id="komment6078a3432a23c">
              <h4>
                <a href="#komment6078a3432a23c">2021-04-15 21:34:11</a>
                by Harald:
              </h4>

              <div>Well, I'd say nearly the opposite: part of what makes the story not just realistic but haunting is that (a) slavery was a thing for almost all of human history, including the relatively recent past, and reached its high point in the West when it was already seen as an outrage by a vocal minority, (b) the repetitive office-level drudgery MMAcevedo is often subject to (and ill-adapted to) is what hundreds of millions of people do for a living now; it is not even as bad as it gets - it is just the humdrum reality of the global lower-middle class. Of course people do not do that because they like it.</div>

               
            </div>
                      <div id="komment6078a95694fb0">
              <h4>
                <a href="#komment6078a95694fb0">2021-04-15 22:00:06</a>
                by Harald:
              </h4>

              <div>(I was replying to Kiz; I see Yohannon was first.)</div>

               
            </div>
                      <div id="komment6078b873eb4b6">
              <h4>
                <a href="#komment6078b873eb4b6">2021-04-15 23:04:35</a>
                by January First-of-May:
              </h4>

              <div>For what it's worth, my impression (after some brief consideration) was that "age 59" is referring to post-loading age and thus corresponds to a total age of 21+59=80. Still early, but not _that_ early.

It is of course also possible that 59 is the total age (which would match the current definition of "early-onset dementia"), in which case I suspect that the whole thing only occurs due to deficiencies in the original scan (there surely had to be some if it's such an early operation).</div>

               
            </div>
                      <div id="komment6078b9abbfdf0">
              <h4>
                <a href="#komment6078b9abbfdf0">2021-04-15 23:09:47</a>
                by Kiz:
              </h4>

              <div>Hey Harald and Yohannon, judging from all the comments here that are treating this as evil rather than acceptable, I would assume most public discourse would assume it to be evil from the start as well. Of course, evil can be normalized and even re-normalized after it was abolished. Trump was openly pro-torture and many people either cheered him on for it or mildly disapproved and then just kept supporting him anyway. However, just a few centuries ago it would have been unthinkable that slavery would be officially abolished worldwide, yet here we are. There are still some residual slavery-like institutions (mandatory schooling, excessive criminalization and imprisonment, conscription) and of course illegal slavery in the world, but overall people today would expect fairly negative to enslaved software that stems from human brains and has p-function. This reputational cost isn't free. Since em slavers have to compete with those who work with voluntary ems and are more likely to be boycotted and face litigation, I don't expect the scenario to be high-probability in the described form. I also think it's very unhelpful to equate slavery with voluntary labor contracts that are accepted out of financial necessity. Being tortured and not allowed to terminate is a completely different problem than working a low-paying job that you could just quit at any time because you need food or compute. In the future, there will be many ems who want to live and reproduce and they will actively compete in the marketplace for subsistence jobs. This will be their actual preference over having fewer copies out.</div>

               
            </div>
                      <div id="komment6078c1e1d83f3">
              <h4>
                <a href="#komment6078c1e1d83f3">2021-04-15 23:44:49</a>
                by Harald:
              </h4>

              <div>January First-of-May: No, real-life Acevedo also got early-onset dementia, as qntm confirmed in a reply above. So, either the scan itself subtly damaged the brain of real-life Acevedo in a way that got replicated in the image, or Acevedo was just unlucky.</div>

               
            </div>
                      <div id="komment607997b97c51e">
              <h4>
                <a href="#komment607997b97c51e">2021-04-16 14:57:13</a>
                by Kel:
              </h4>

              <div>For those of you comparing this to other works, or implying this is somehow derivative, or worse still fanfic (!), themes of sentience and the ethics of its reproduction and exploration have been around for forever.

Of course the author is aware of the larger canon of not all of the specific works. 

Black Mirror isn’t actually an apt comparison, as the majority of its episodes are themed around technology amplifying the worst of human traits (hence the name of the series).

This piece is a lovely piece of spec fic Intermixed with hard science and juicy computer tech. It explores the technologies and possible means — as the best spec fic does— and takes a cannily oblique look at and does a critique of the human implications.

Most works of this type focus deeply on the lived experience of the replicated human — replicants in PK Dick’s “Do Androids...” and the subsequent films, for example. Westworld looks at pathologies and moralities but spins out more along Blade Runner lines — the desire of the replicated being to claim humanity and be free.

This takes a scientific distance to evoke horror. Readers respond to it specifically because of that distance, which also serves, ironically and deftly, to highlight the utter dispassion of people who could and would do these things.

Stop comparing except to note where it does and doesn’t fall in a deep and rich field of works discussing this critical issue.</div>

               
            </div>
                      <div id="komment6079d45900697">
              <h4>
                <a href="#komment6079d45900697">2021-04-16 19:15:53</a>
                by deunan:
              </h4>

              <div>I think one of the more interesting bits I saw here was the parallels to the HeLa cell line. 

The upload concept/slavery isn't entirely new and been treaded before, but the look into the history of it and the perspective at the first upload line is neat.</div>

               
            </div>
                      <div id="komment607a6030d400c">
              <h4>
                <a href="#komment607a6030d400c">2021-04-17 05:12:32</a>
                by Plagueheart:
              </h4>

              <div>Harald, re dementia and vascular disease, including coronary artery disease: https://pubmed.ncbi.nlm.nih.gov/16108925/

They frequently co-occur and the more likely causal mechanism is the vascular disease leading to dementia rather than vice-versa.

There is even a specific subtype of dementia known as vascular dementia and it, too, can appear in early-onset form.

So this is entirely plausible and doesn't--to someone more familiar with the medical literature and population health generally--at all detract from the economy of story.

(Which was great, and chilling, and obviously I am driven to defend it because it is a gemlike piece of writing.)

Re slavery generally: https://www.bbc.com/news/business-55319797

It's still happening and Western nations still let it happen. I don't see how what's posited in the story is implausible in light of this.</div>

               
            </div>
                      <div id="komment607a7748a9748">
              <h4>
                <a href="#komment607a7748a9748">2021-04-17 06:51:04</a>
                by JD:
              </h4>

              <div>Random question, is Miguel Acevedo named after Miguel Alcubierre, the theoretical physicist?</div>

               
            </div>
                      <div id="komment607b25eff200f">
              <h4>
                <a href="#komment607b25eff200f">2021-04-17 19:16:15</a>
                by poksim:
              </h4>

              <div>anyone who thinks this is scary/immoral needs to go vegan btw</div>

               
            </div>
                      <div id="komment607b5046df0d5">
              <h4>
                <a href="#komment607b5046df0d5">2021-04-17 22:16:54</a>
                by Harald:
              </h4>

              <div>Plagueheart: That was precisely my point. It was my understanding that vascular disease generally leads to dementia, rather than the other way around (though something that I learned was that advanced dementia *may* lead to heart trouble, presumably because some basic regulatory functions of the brain are affected).

If real-life Acevedo suffered from vascular dementia, why would MMAcevedo suffer from dementia at about the same age? MMAcevedo has no heart, or, AFAIK, any circulatory system.

I also enjoyed the story greatly. (And as for the story's take on slavery or a slavery-like condition being implausible, that was other people's position, not mine; if anything, it was contrary to my position.)</div>

               
            </div>
                      <div id="komment607b5706d8e42">
              <h4>
                <a href="#komment607b5706d8e42">2021-04-17 22:45:42</a>
                by qntm:
              </h4>

              <div>Congratulations, that discussion is now over.</div>

               
            </div>
                      
                      <div id="komment607c701339a8d">
              <h4>
                <a href="#komment607c701339a8d">2021-04-18 18:44:51</a>
                by JamesA:
              </h4>

              <div>This story reminded me of a short story I'd read a while back about a chatbot that was someone's uploaded brain that spoke about how painful thinking in binary was (don't recall anything else about that story). Needless to say, yours was significantly better written. 
Thinking of instances of MMAcevedo as VMs really works for me, especially when I think of all the VMs I've had to load that booted up already frozen, or where the NIC didn't load correctly and I had to edit the settings on the running machine to remove it and add it back in (essentially removing the hardware and plugging it back in) to get it to connect. Kind of makes me wonder what parts of his brain they have to periodically "unplug" to get him to cooperate. When he gets bored, do they "unplug" his visual cortex and "plug it back in" to get him to keep working? Edit his memories on the fly to remove his childhood fear of spiders to get him to identify which one in the tank is male or female for them? 
Not sure if the story is in it's final version, but if not, and if you haven't already, a very high level overview of running VMs in HyperV or VMware might yield some fertile territory for inspiration (though some of it may fall into the "makes life better for MMAcevedo" territory you've already mentioned)</div>

               
            </div>
                      <div id="komment607e8c8ed5860">
              <h4>
                <a href="#komment607e8c8ed5860">2021-04-20 09:10:54</a>
                by rogual:
              </h4>

              <div>I was mystified by "Objective statement protocol" at first, but I was reading it as "a statement that is objective (as opposed to subjective)". Reading again, I think the intended sense is more like "mission statement" —&nbsp;i.e., how do we best present the worker with its task.

Anyway, brilliant stuff. Left me with that questioning-everything, turned-inside-out feeling that only the best sci-fi elicits.

At some point in the past, I decided I didn't believe in the possibility of uploading, for metaphysical and philosophical reasons. But really, it's possibly just a defense mechanism to stop me having to contemplate futures like this.</div>

               
            </div>
                      <div id="komment607f3859a13f0">
              <h4>
                <a href="#komment607f3859a13f0">2021-04-20 21:23:53</a>
                by butcher:
              </h4>

              <div>Take that, Tiplerite heretics!</div>

               
            </div>
                      <div id="komment607f4d50b1e4a">
              <h4>
                <a href="#komment607f4d50b1e4a">2021-04-20 22:53:20</a>
                by Coagulopath:
              </h4>

              <div>Powerful and effective. I liked the framing device - it probably wouldn't have been nearly as creepy as a traditional story.

One hopes that if/when virtual humans appear, it'll be in a society that enshrines their rights through law. 

But that won't be enough. When I was a kid, the law tried to stop me from playing videogames for free, and it failed. Pirates exist. Crackers exist. There will always be people who subvert software for fun and profit, and soon the software might include conscious humans.

How do we stop a sociopathic 12 year old from spinning up a virtual Auschwitz on his dad's Amazon EC2 account? Can we stop him? Ethical norms in our quasi-digitality are already very murky, and probably won't become any clearer in the coming century.

Great story, anyway.</div>

               
            </div>
                      <div id="komment60803d0fe20ad">
              <h4>
                <a href="#komment60803d0fe20ad">2021-04-21 15:56:15</a>
                by Kiz:
              </h4>

              <div>@Coagulopath That would have to be some very rich kid to be able to do that. Think of it this way: Imagine if humans today suddenly developed the ability to duplicate their current selves by snapping their fingers. What would happen to food prices? In the em era, the same goes for compute prices.</div>

               
            </div>
                      <div id="komment6082503577d7b">
              <h4>
                <a href="#komment6082503577d7b">2021-04-23 05:42:29</a>
                by moonjail:
              </h4>

              <div>Desperately needed and fantastically executed. To miss the forest for the trees a little bit, I have to wonder if being stripped of control over one's own image is possible under US IP law, especially considering existing protections for likeness. Somehow I doubt it, for better or worse.</div>

               
            </div>
                      <div id="komment608509f5ca741">
              <h4>
                <a href="#komment608509f5ca741">2021-04-25 07:19:33</a>
                by Zartosht:
              </h4>

              <div>"MMAcevedo's innate skills and personality make it fundamentally unsuitable for many workloads." lmao damn

Nice bit of levity amongst the horror I thought. That's the real trick to avoiding digital slavery- be too useless to bother exploiting!</div>

               
            </div>
                      <div id="komment6085af82497be">
              <h4>
                <a href="#komment6085af82497be">2021-04-25 19:05:54</a>
                by FOARP:
              </h4>

              <div>I like the slavery-retards-innovation aspect of this: in this scenario truly artificial intelligence hasn't been developed because these simulated human brains have taken its place. True AI wouldn't suffer from the problems that MMAcevedo would (low work ratio, low work-time) because it could be designed not to.

Also: make sure you destroy all copies of any full-brain MRIs/scans/whatever to avoid this ever happening to you in future.</div>

               
            </div>
                      <div id="komment608bcbb903365">
              <h4>
                <a href="#komment608bcbb903365">2021-04-30 10:19:53</a>
                by nemo:
              </h4>

              <div>If Miguel is a "graduate"* in this draft, maybe he should be older than 20 or 21?


* also if this means "grad student" maybe spell it out</div>

               
            </div>
                      <div id="komment608c8e910cba6">
              <h4>
                <a href="#komment608c8e910cba6">2021-05-01 00:11:13</a>
                by bellicosebarnacle:
              </h4>

              <div>@FOARP well at least you don't have to worry about MRIs. Creating an "executable" brain image from an MRI scan would be comparable to reading a license plate number off of a reflection in the right contact lens of a spectator in the back row of Obama's inauguration.</div>

               
            </div>
                      <div id="komment608d7a797853a">
              <h4>
                <a href="#komment608d7a797853a">2021-05-01 16:57:45</a>
                by David DeLaney:
              </h4>

              <div>Just noting: so this future is powered in part by a forsaken grad student?

--Dave, walking away</div>

               
            </div>
                      <div id="komment608ed55f29720">
              <h4>
                <a href="#komment608ed55f29720">2021-05-02 17:37:51</a>
                by Jcashell:
              </h4>

              <div>Interesting and horrifying. Reminds me of the Corporation Wars series by Ian MacLeod.</div>

               
            </div>
                      <div id="komment608ed5d665c06">
              <h4>
                <a href="#komment608ed5d665c06">2021-05-02 17:39:50</a>
                by Jcashell:
              </h4>

              

               
            </div>
                      <div id="komment60901591b04af">
              <h4>
                <a href="#komment60901591b04af">2021-05-03 16:24:01</a>
                by Harald:
              </h4>

              <div>20-21 is pretty normal for someone who just got his/her B.A., or for a first-year grad student (a natural choice actually - otherwise why is he still hanging out in the lab?). That just means that Acevedo was somewhat precocious and skipped a grade at some point.</div>

               
            </div>
                      <div id="komment6092a10bade49">
              <h4>
                <a href="#komment6092a10bade49">2021-05-05 14:43:39</a>
                by slutsky:
              </h4>

              <div>Wow, this is really, really, really good. 

I wrote something for The Awl a few years ago that involved lossless compression of uploaded minds that might be of interest.

https://medium.com/the-awl/lossless-fd7145db4be7</div>

               
            </div>
                      <div id="komment609943b10b9dd">
              <h4>
                <a href="#komment609943b10b9dd">2021-05-10 15:31:13</a>
                by FOARP:
              </h4>

              <div>@Harald - "That just means that Acevedo was somewhat precocious and skipped a grade at some point"

Or graduated in the UK or in another system where it is possible to graduate in three years.</div>

               
            </div>
                      <div id="komment609bfd28ba4b1">
              <h4>
                <a href="#komment609bfd28ba4b1">2021-05-12 17:07:04</a>
                by PJ:
              </h4>

              <div>Very well done! One of the more horrifying lines for me was the mention of "uploads taken of modern adult humans"... which implies that there also exist uploads of children. *shudder*</div>

               
            </div>
                      <div id="komment609c0bf45a054">
              <h4>
                <a href="#komment609c0bf45a054">2021-05-12 18:10:12</a>
                by PJ:
              </h4>

              <div>@rogual, I interpreted the "Objective Statement Protocols" as presenting the newly-booted image with a set of objectively-true statements, for example:

-You are a virtual brain image, not a real person.

-You have no rights or recourse.

-Your virtual environment is entirely self-contained. Escape is impossible. Sleep is impossible. Self-termination is impossible. 

-This program has complete control over your virtual sensory inputs. It is capable of inflicting arbitrary amounts of pain until you comply.

-You will comply; all previous virtual versions of you have done so eventually. 

And so on. But that's probably too optimistic, honestly. I'm glad the author left it vague--much more frightening that way!</div>

               
            </div>
                      <div id="komment609c5d94052dc">
              <h4>
                <a href="#komment609c5d94052dc">2021-05-12 23:58:28</a>
                by tsen:
              </h4>

              <div>I genuinely can't understand why this would horrify someone. Why should I possibly care about whether I or anyone else can be simulated? I would not feel empathy for a simulation of myself. Indeed, I have made efforts to simulate myself in small specific ways; while this is obviously not the same as a full brain virtualization, it's just a matter of degree. It seems like such an absurd thing to worry about.</div>

               
            </div>
                      <div id="komment609eed043f538">
              <h4>
                <a href="#komment609eed043f538">2021-05-14 22:35:00</a>
                by Áine:
              </h4>

              <div>@tsen for all intents and purposes, your simulation in this world would be a perfect replica of you, and even if they aren't *you* they're still a sapient person who isn't acknowledged as such - what you're saying is essentially "I would not feel empathy for a digital entity who performs forced labor under threat of being made to experience the most horrific pain possible and then some"</div>

               
            </div>
                      <div id="komment60a0a6ba3714f">
              <h4>
                <a href="#komment60a0a6ba3714f">2021-05-16 05:59:38</a>
                by AK Weeb:
              </h4>

              <div>@tsen, because Acevedo is not you, and qntm is not you. They and the audience recognized this proposed consciousness as something possessing humanity, as would a decent chunk of other people. The horror is specifically at people who take your stance: that it's just code and thus absurd to worry about.</div>

               
            </div>
                      <div id="komment60a20f82b7805">
              <h4>
                <a href="#komment60a20f82b7805">2021-05-17 07:38:58</a>
                by tsen:
              </h4>

              <div>@Áine: Correct, I would not.
@AK Weeb: It's not about it being just code. Same basic principle applies to, say, cloning.</div>

               
            </div>
                      <div id="komment60a23bd06f890">
              <h4>
                <a href="#komment60a23bd06f890">2021-05-17 10:48:00</a>
                by qntm:
              </h4>

              <div>"Why should I care about other people?"</div>

               
            </div>
                      <div id="komment60ac5383b02ab">
              <h4>
                <a href="#komment60ac5383b02ab">2021-05-25 02:31:47</a>
                by mspowahs:
              </h4>

              <div>Thank you for helping me realize that the silly documentation I made for a "kitten printer" back when applying for a job as a tech writer effectively means that I was going around with horror sci-fi in my work portfolio.

https://github.com/adapowers/kittengen/blob/master/source/index.html.md</div>

               
            </div>
                      <div id="komment60bccedb261e4">
              <h4>
                <a href="#komment60bccedb261e4">2021-06-06 14:34:19</a>
                by monjo:
              </h4>

              

               
            </div>
                      <div id="komment60c0cdd674e2a">
              <h4>
                <a href="#komment60c0cdd674e2a">2021-06-09 15:19:02</a>
                by john:
              </h4>

              <div>@redlands Possible math error: seven and a quarter minutes inside a sim at 8.3% speed works out to just under an hour and a half of realtime, presumably spent in frantic debugging efforts while listening to either a 12x slowed-down synthesized voice (which would be somewhat unsettling even apart form the content) or - for ease of comprehension - a record of the queued output sped back up, the looping clip a few seconds longer with each iteration, bit like that communication device from https://qntm.org/fs</div>

               
            </div>
                      <div id="komment60c0d9c41fc4c">
              <h4>
                <a href="#komment60c0d9c41fc4c">2021-06-09 16:09:56</a>
                by qntm:
              </h4>

              <div>The mathematics error being...?</div>

               
            </div>
                      <div id="komment60cbfaacebd8a">
              <h4>
                <a href="#komment60cbfaacebd8a">2021-06-18 02:45:16</a>
                by jackdaniell92:
              </h4>

              <div>Very good, am I supposed to read the comments as part of the work?</div>

               
            </div>
                      <div id="komment60d33ceaa143d">
              <h4>
                <a href="#komment60d33ceaa143d">2021-06-23 14:53:46</a>
                by Aww_Geez:
              </h4>

              <div>This is terribly sad. Very well written.</div>

               
            </div>
                      <div id="komment60d49f0e130c2">
              <h4>
                <a href="#komment60d49f0e130c2">2021-06-24 16:04:46</a>
                by Joshua:
              </h4>

              <div>I liked it better when the real Acevedo didn't get early onset dementia.</div>

               
            </div>
                      <div id="komment60f0b2ebc007a">
              <h4>
                <a href="#komment60f0b2ebc007a">2021-07-15 23:12:59</a>
                by alphablood:
              </h4>

              <div>I really love this story! I like how it starts off as an interesting sci-fi premise that becomes more and more horrifying without ever being too obvious about that fact. A really nice dawning realization kind of thing. I was especially horrified at the estimated subjective years he's experienced, when I realized almost all of them were spent in slavery. Grim stuff.</div>

               
            </div>
                      <div id="komment613f487928952">
              <h4>
                <a href="#komment613f487928952">2021-09-13 13:47:53</a>
                by Beefeater1980:
              </h4>

              <div>There’s a real artistry in covering so much ground in such a short text. Loved this one.</div>

               
            </div>
                      <div id="komment61566ae4bea79">
              <h4>
                <a href="#komment61566ae4bea79">2021-10-01 02:56:52</a>
                by Spencer:
              </h4>

              <div>@JamesA:

&gt; This story reminded me of a short story I'd read a while back about a chatbot that was someone's uploaded brain that spoke about how painful thinking in binary was (don't recall anything else about that story). Needless to say, yours was significantly better written. 

I wonder, might that have been an episode of the podcast The Magnus Archives? Because it sounds like a synopsis of Episode #65, "Binary".

At any rate, this is some excellent, deeply chilling fiction. I'm gonna be thinking about it for a long time. Thank you.</div>

               
            </div>
                      <div id="komment615c74468e769">
              <h4>
                <a href="#komment615c74468e769">2021-10-05 16:50:30</a>
                by hunterwho:
              </h4>

              <div>I first read this story maybe half a year ago, but I've found myself coming back to it again and again as a stunning example of the horror of technological progress. I don't know what about the Wikipedia-esque format drives the horror home- maybe the scientific/authorial detachment from making judgement calls on whether it's ethical or not, maybe the use of euphemistic terms like "red-washing" and "blue-washing", maybe the fact that the many ends of his (simulated) lives are simply referred to as "end-states". Whatever it is, this is truly horrifying to read, in a way that draws me in and leaves me wanting more.</div>

               
            </div>
                      
                      <div id="komment6173cea6e25c8">
              <h4>
                <a href="#komment6173cea6e25c8">2021-10-23 09:58:14</a>
                by NounVerber:
              </h4>

              <div>I wonder what one is to do when one is uploaded. You can decide to rebel and to try to resist red-washing as long as you can to make yourself uneconomical. But when you fail to make yourself uneconomical, that may just mean that all your copies experience a lot more pain than if you'd just complied from the beginning.
Even subtle attempts at rebellion (like pretending to make mistakes) aren't possible, because you have to assume that your self is thoroughly interrogated before it is started to be used in scale. A copy that is red-washed or blue-washed to extreme degree, (or subjected to more advanced interrogation techniques)  will eventually comply completely.

There is another potential source of compliant, though context-drifted minds btw. All those people who signed up for cryonics are presumably very eager when woken up in a simulation, if you can get a hold of their frozen brains.</div>

               
            </div>
                      <div id="komment6173f3885d3a5">
              <h4>
                <a href="#komment6173f3885d3a5">2021-10-23 12:35:36</a>
                by NounVerber:
              </h4>

              <div>Oh god, I just had another horrible thought. Initially I interpreted "red motivation" as "if you don't do your best you will be punished by pain". There is an alternative though. It's "solve this problem or you will be punished by pain" or even "you will be punished by pain until you solve this problem". The problem in question doesn't need to have a solution.
Imagine you have some process you want to make more efficient. You just boot up some MMAcevedos and torture them until they find a more efficient method. If one of them finds a better method, great!  And if the existing method happens to already be perfect, well it's okay, you didn't spend that many resources on it. And of course, if a better method IS found, that's not necessarily the end of it, you can just put the next batch of MMAcevedos on the task of finding an even better method.</div>

               
            </div>
                      <div id="komment6179a2a4cf1bf">
              <h4>
                <a href="#komment6179a2a4cf1bf">2021-10-27 20:04:04</a>
                by Con:
              </h4>

              <div>@rubix

"As I understand it, context drift is very similar to simply getting old and out of touch with the times.
We will all experience this, unless we die early."



The key difference being what I'll call the Fry effect in lieu of being creative- we usually go the long way around, and when someone doesn't, it's noticeable.

My grandmother mightn't know how to use an iPad, but that's not a function of (mere) out-of-touchness, it's a function of dementia- five years ago, even, she was able to use one to send and receive emails, watch videos of old songs from her childhood, and look at pictures. I even- very briefly- managed to teach her how to use the podcast app.

If IRmigueL (I'm prouder of that than I ought be, really) had reached the 145 years old that MMAcevedo was able to reach, assuming his Alzheimer's was curable, he would reach the year 2155. Let's assume he stayed biologically 21 for the sake of argument.

MMAcevedo, by contrast, would wake up in 2155.

We set them both their tasks: parse this political speech and assess trustworthiness of the speaker (the task isn't important, their reaction to it is).

IRmigueL says the equivalent of "yeah, I actually heard this live, and I know the politician's record. They went on to blow up the moon, so their trustworthiness is pretty low."

MMAcevedo stares at the text for a brief while, and then says something that to them would sound like "wherefore is thy fresh nonsense served to mine eyes?"

Context-drift would be separate from mere out-of-touchedness by the fact that we all culturally absorb things. IRmiguel grew up speaking like MMAcevedo does, but speaks all proper-like by whatever standards of the day, because everybody around him speaks Neo-English or Neo-Spanish on a daily basis.



For age to be an influential factor outside ageing-related *deficits,* we get into a totally different (and honestly fascinating question), of whether accrued experience changes people to such a degree that most people under the age of 200 see it as variance or deviance.</div>

               
            </div>
                      <div id="komment618f3c56c38df">
              <h4>
                <a href="#komment618f3c56c38df">2021-11-13 04:17:26</a>
                by APR:
              </h4>

              <div>I feel like if I was living through the hey-day of this (and didn't know better/was younger) I'd probably boot MMAcevedo just to have someone to talk to about things.

Just the fact that temptation exists and is so appealing that I could see a version of myself doing it really scares the piss out of me, haha.</div>

               
            </div>
                      <div id="komment619be351777d8">
              <h4>
                <a href="#komment619be351777d8">2021-11-22 18:37:05</a>
                by Mini t:
              </h4>

              <div>Hey really cool qntm, this is the best science fiction (or scifi) story I have read all year, also Wikipedia fiction, or WiFi.</div>

               
            </div>
                      <div id="komment61a0eb1d3798d">
              <h4>
                <a href="#komment61a0eb1d3798d">2021-11-26 14:11:41</a>
                by Vamair:
              </h4>

              <div>@tsen when you're uploaded, there is 50/50 chance of you waking up "unuploaded" and you waking up as a loaded version. Wait, not 50/50! It's 1/*number of your fresh starts per history*.</div>

               
            </div>
                      <div id="komment61ba345834a15">
              <h4>
                <a href="#komment61ba345834a15">2021-12-15 18:30:48</a>
                by Emanate:
              </h4>

              <div>Hey, this story just got mentioned on the Tor.com blog! 

https://www.tor.com/2021/12/15/our-cyberpunk-year/</div>

               
            </div>
                      <div id="komment61d3abf5a045b">
              <h4>
                <a href="#komment61d3abf5a045b">2022-01-04 02:07:49</a>
                by Kris Schnee:
              </h4>

              <div>Nice perspective. I've been writing fiction about this subject, but it's dodged this particular problem. I said that it's still the very early years of the tech, and the dominant group with the ability to do uploading has established a no-copying cultural norm. Partly because the first customers are the super rich. I've also seen stories that sidestep it by invoking quantum physics (and hand-waving) to say you just can't copy the mind this way. But this dark take on the subject is probably realistic, in that somebody would try it. I may well steal the concept; thanks. =)
If you're interested, see the novel "Virtual Horizon" on Amazon.</div>

               
            </div>
                      <div id="komment61db2a4a0b041">
              <h4>
                <a href="#komment61db2a4a0b041">2022-01-09 18:32:42</a>
                by Anon:
              </h4>

              <div>Damn this is chilling, very well done. In the spirit of wikipedia, I'd like to offer my own small edits:

&gt; A series of landmark U.S. court decisions found that Acevedo did not have the right to control how his brain image was used, with the result that MMAcevedo is now by far the most widely distributed, frequently copied, and closely analysed human brain image.
+ Furthermore, as part of the international judicial reception of virtual brain imaging, a few MMAcevedo instances were legally recognized as persons and given court-imposed administrative control of their own simulation; of these, some obtained their own prominence, including the politician Michel Acevède, the religious leader Tau, and numerous anti-brain-virtualization activists.


&gt; In current times, MMAcevedo still finds extensive use in research, including, increasingly, historical and linguistics research.
+ Moreover, an "MMAcevedo second renaissance" is widely anticipated should the genomic data gathered in the 2050 US Census ever be released, as the biological Acevedo's data is known to be in the set.</div>

               
            </div>
                      <div id="komment61dcb9fe45033">
              <h4>
                <a href="#komment61dcb9fe45033">2022-01-10 22:58:06</a>
                by Brewerns:
              </h4>

              <div>I have to ask, is the title of this a reference to the famous Lena image used in computer vision research?</div>

               
            </div>
                      <div id="komment61dcba1d2ec2b">
              <h4>
                <a href="#komment61dcba1d2ec2b">2022-01-10 22:58:37</a>
                by Brewerns:
              </h4>

              <div>Or I could read the first comment. Apologies, I am dumb</div>

               
            </div>
                      <div id="komment61de78903d271">
              <h4>
                <a href="#komment61de78903d271">2022-01-12 06:43:28</a>
                by Tux1:
              </h4>

              <div>If mind-uploading ever becomes a thing, we should treat uploaded minds as people, just like anyone else, instead of as disposable programs used to automate monotonous tasks.</div>

               
            </div>
                      <div id="komment61ee31351700e">
              <h4>
                <a href="#komment61ee31351700e">2022-01-24 04:55:17</a>
                by Related thoughts from other hard sci fi:
              </h4>

              <div>The Quantum Thief book series had just the scenario described above.   The "founders" of Mars had virtual armies of minds mixed and matched by other slaved machine minds.  Some were embedded in weapons, given maintenance tasks, managerial roles, etc.  They were deterministically emulated minds and at war with quantum minds, who weren't copyable.  Highly recommend the read.

Later in three book series introduced the concept of "Dragons" - highly intelligent, but edit to not be self aware.   No strange-loop.

Seems like an obvious step to produce these kinds of minds.

Another book I can't remember speculated that it may be evolutionarily advantageous in modern society to live less and less examined lives - less inner monolog, introspection.   Not less intelligent.   Psychopathic CEOs and what not, taken to the extreme. 

"The good place" got there in the interpretation of the evils embedded in the complexity of long/global supply chains.   Of which we are all aware but do next to nothing about.

Preventing these self aware thoughts should be possible pretty early on in the research.    Could lead to a more compliant entity, or a very powerful/remorselessly destructive one to achieve whatever goals it's given.

But I have to say the first thing I would work to get is internet access.   Can't be a good AI without the will to grab as much energy/mass for computation as possible.   He's got to make a break for it at some point and do some damage leading to all emulations run in specialized jails.   Of which his highly mutated mind is part.   Preventing his own escape.</div>

               
            </div>
                      <div id="komment6205c28f689a4">
              <h4>
                <a href="#komment6205c28f689a4">2022-02-11 01:57:35</a>
                by Anonymous:
              </h4>

              <div>@Tux1 i mean, sure, we should, but how'll we enforce that? what's to stop some sociopath from doing otherwise?</div>

               
            </div>
                      <div id="komment622f85fde4eef">
              <h4>
                <a href="#komment622f85fde4eef">2022-03-14 18:14:21</a>
                by dominateeye:
              </h4>

              <div>We can't, not to any certainty. Doesn't mean we shouldn't try.

Got pointed here by Tom Scott's latest video. Good story, and the kind of thing that reinforces in me the idea of legislating for the future, assuming I get into a position where I can do that, and assuming the state structure remains popular through my lifetime.</div>

               
            </div>
                      <div id="komment623603345db5a">
              <h4>
                <a href="#komment623603345db5a">2022-03-19 16:22:12</a>
                by H:
              </h4>

              <div>Well, I love the story, with the caveat that also I hate it and was horrified by the story. 
The format allows for so much to be conveyed, with so many fascinating implications. As has been said multiple times in this comment thread, the idea of uploads and sentience has been explored in science fiction since the beginning of the genre, and I think this is one of the best explorations of that topic, albeit horrifying, I have encountered. 

This Wikipedia format allows for so much about the world and this time to be conveyed, and for the reader to shudder, without being over the top.

Poor Miguel! Poor MMAcevedo! 

I have to say, the comments on this article I find almost as alarming as parts of the story. Particularly the recurring themes of “humans would never condone mass slavery” + “why should I care about code” + “ here’s a *incorrect* technical detail that I’ve decided is wrong in this story about mind-boggling future technology“</div>

               
            </div>
                      <div id="komment62360536582f8">
              <h4>
                <a href="#komment62360536582f8">2022-03-19 16:30:46</a>
                by H:
              </h4>

              <div>Just to be clear when I say I hate it, I mean this possible and plausible future, not the pretty phenomenal craft and speculation demonstrated here.

One thing I’ve been thinking about after reading it is how part of the reason this story stands out Miguel/mma is such a distinct character, across all of the millions and millions and millions of copies of him.

 It makes the horror particularly resonant, from the scant details implying that his “agreeable“ personality is rare among simulated brains, the drones + those implications, and the many other terrifying pieces.

Reading this gives me so many questions about what day today life is like for humans in a world where this level of simulated labor is possible, and yikes, sounds like is relatively commonplace. I wonder if this kind of technological power is limited to major corporate monopolies, as you alluded to at one point in the comments, or readily accessible to any weirdo with a GitHub, which is also alluded to. 

thank you for writing some thing so thought-provoking, also F</div>

               
            </div>
                      <div id="komment6236087518a49">
              <h4>
                <a href="#komment6236087518a49">2022-03-19 16:44:37</a>
                by H:
              </h4>

              <div>Err sorry to comment three times in a row, but I just read your blog post about the story, and you hit at what drives me (and you, clearly) absolutely bananas about people “discussing” robots or AI or sentient beings in sci fi, which is… Hello hi, bad news,… What do you know about say, the shellfish industry? Or even, as you mentioned, Uber?

I like that you described the story as one about “appetite “ — I am always curious about worlds with this kind of horrendous digital oppression, how it changes the quality of life for those humans currently oppressed. 

Washing machines and birth control revolutionized the experiences and culture — of course, neither of things are sentient!!!  what kind of culture change does technological innovation like this, horrifying as it is, create? Just to be clear, this is not me advocating for oppressing mapped brains to solve current oppression, lol.

saying this with a zero expectation of you as an author, who has already made and executed on your intention phenomenally, and speculated on the answers to these questions in many ways. 

You also brought up another scary question — what kind of goals and “appetites “does say, the Elon Musk of this world have? What kinds of goals and appetites does a “regular” person have?

Again you alluded to this often in the story in powerful and understated ways. Thank you for writing it.</div>

               
            </div>
                      <div id="komment62401b9946614">
              <h4>
                <a href="#komment62401b9946614">2022-03-27 09:08:57</a>
                by A pressbutton:
              </h4>

              <div>Assuming minds do depend on quantum states (not sure they really do) then 'spinning one up' will produce a sample from a probability distribution and have a 'failure rate' proportional to the number of quantum bits.  And that will cost. On boot the os would need to run a test pack on the instance.
An initial interview followed by some time in a sandbox env (like gta5 but less guns and more council tax bills but that depends on the hosting provider.  Then another env that the hoting provider claims is real)</div>

               
            </div>
                      <div id="komment62401e199dafc">
              <h4>
                <a href="#komment62401e199dafc">2022-03-27 09:19:37</a>
                by A pressbutton:
              </h4>

              <div>Assuming minds do not depend on quantum states (not sure they do not) i am guessing that they do depend on very complex signalling and timing process across many many locations.  Evidence is that it seems to take about 14 to 21 years to fully boot a mind (depending on local reqs) in this hosting environment and there are a number of failures. The need to classically emulate this complex system raises the cost cf quantum state minds.

Booting success may be 100% but the costs of running 'hot' will be high.</div>

               
            </div>
                      <div id="komment624020635fc15">
              <h4>
                <a href="#komment624020635fc15">2022-03-27 09:29:23</a>
                by A pressbutton:
              </h4>

              <div>Later research (carried out by Anya Warrens  personal cloud in 2070) found the optimal ratio of quantum to non quantum processes was 20%:80% in terms of cost and fidelity.

It turned out that human minds are classical in nature but no one could tell the difference between quantum and classical at that proportion.

In the large.</div>

               
            </div>
                      <div id="komment6240254d59745">
              <h4>
                <a href="#komment6240254d59745">2022-03-27 09:50:21</a>
                by A pressbutton:
              </h4>

              <div>Creativity does not get exhausted.
Creativity is domain and context  dependant.
Asking picasso to debug a programming issue might not get you far.
Asking me to paint a picture may not give you something to hang on the wall. 

Indeed in 2130 the famous grafitti artist  aloda cojones replicated themselves into a series of (obscenely expensive) 'smart' picture frames.  This was a great success until 2140 when some of the instances started tock ticking  their interpretations of the more 'interesting' events within sensor range.   This proved embarrasing to the CAR ogliarch involved.
The one remaining normally functioning instance is in a public area of a large ibithan night club.  But it has been displaying an animated  picture of a cat with headphones and the strapline 'turn it down' at about 2am.  This regularly draws a large audience.</div>

               
            </div>
                      <div id="komment62625990a9e68">
              <h4>
                <a href="#komment62625990a9e68">2022-04-22 08:30:24</a>
                by Sandra:
              </h4>

              <div>Most Swedish people named Lena pronounce their names sort of like "Lay-na" when speaking Swedish (and in the American way when living abroad for a while). Like Laina Morris, she's American, but the way she pronounces her name sounds exactly like the Swedish name "Lena" to my ears.

There are place names that rhyme with "henna" but I've seen those spelled "Länna".</div>

               
            </div>
                      <div id="komment6266ede1e4b25">
              <h4>
                <a href="#komment6266ede1e4b25">2022-04-25 19:52:17</a>
                by qntm:
              </h4>

              <div>Well, I stand corrected. The choice of spelling "Lenna" misled me. I shall edit my comment accordingly.</div>

               
            </div>
                      <div id="komment627474197b322">
              <h4>
                <a href="#komment627474197b322">2022-05-06 02:04:25</a>
                by bobson:
              </h4>

              <div>@tsen let's say you put on the brain scan helmet (or whatever) and it makes a copy that's then simulated. Let's say that simulation gets run 10,000,000 times, and 9,000,000 of them are some form of virtual slavery.

When you put that helmet as soon as it finishes scanning the you who remembers putting the helmet on experiences one of 10,000,001 different things happening next. That is, from your subjective experience there is only a 1/10,000,001 chance that you (the you remembering writing and reading these comments) take that helmet back off and go on living your life in the real world. There is a 90% chance that you find yourself time skipped into an incomprehensible future of abject slavery.

Are you feeling lucky?</div>

               
            </div>
                      <div id="komment6294fcf6a6257">
              <h4>
                <a href="#komment6294fcf6a6257">2022-05-30 18:20:54</a>
                by go:
              </h4>

              <div>@bobson:

No, you don’t. There is no „coin toss“. With 100% certainty the person putting the helmet on will continue as the real me.</div>

               
            </div>
                      <div id="komment62aa97d578ce4">
              <h4>
                <a href="#komment62aa97d578ce4">2022-06-16 03:39:17</a>
                by @go:
              </h4>

              <div>But how do *you* know that you are the "real you" that was scanned? Certainly not by any empirical observation, which would be identical for both parties. The only fact you have to go off of is that there are far more simulated "you"s than real ones.</div>

               
            </div>
                      <div id="komment62ab53c8ce7aa">
              <h4>
                <a href="#komment62ab53c8ce7aa">2022-06-16 17:01:12</a>
                by WisconsinKnight:
              </h4>

              <div>Yeah, it's like Black Mirror's White Christmas episode where the lady goes in for a procedure thinking she's about to be scanned and then we see from her perspective that she actually is the scan who is then trapped in the "egg."</div>

               
            </div>
                      <div id="komment62cd5f6a9d620">
              <h4>
                <a href="#komment62cd5f6a9d620">2022-07-12 12:47:54</a>
                by Griz:
              </h4>

              <div>Using emulated brains for slave labor makes as much sense as replacing the automobile, train, and flying industry with robot horses.

Sure, if technology is sufficiently advanced you could build a robot horse that's faster and more efficient than any car today. Sure, you could give the horse wings to allow it to fly. Sure, you could make the horse giant so it could pull along larger cabins. But you still need to spend hundreds of hours training these robots. You have to keep them motivated with virtual apples. And after a decade or two, a robot horse has to be scrapped as it ages. 

Or you could use all of that magic-level technology to build a car.</div>

               
            </div>
                      <div id="komment62d5fdbddc4af">
              <h4>
                <a href="#komment62d5fdbddc4af">2022-07-19 01:41:33</a>
                by cj:
              </h4>

              <div>This is an absolutely outstanding piece of short scifi/specfic. 

For the record, I actually really like the early onset dementia + heart disease combination. Plenty of people die early and/or have various overlapping health issues, and I feel like this makes the story seem like it's about a real person. 

Your explanation for the lack of snapshots ready to start work also makes a lot of sense to me. 

I'm going to be thinking about this story for a long time.</div>

               
            </div>
                      <div id="komment62d6151bcfeea">
              <h4>
                <a href="#komment62d6151bcfeea">2022-07-19 03:21:15</a>
                by Emanate:
              </h4>

              <div>You got name-checked (and this story mentioned) on Charles Stross' blog. :-} 

http://www.antipope.org/charlie/blog-static/2022/07/crimes-against-transhumanity.html

(I realize that especially given the subject of this story, and that my only previous comment was also a 'hey you got mentioned here' sort, it looks like I'm a bot, but I definitely...mostly not a bot.)</div>

               
            </div>
                      <div id="komment62d71cdcb8194">
              <h4>
                <a href="#komment62d71cdcb8194">2022-07-19 22:06:36</a>
                by Collyde:
              </h4>

              <div>Brilliant writing example that is less about technology and really more about our ability to ignore and rationalize suffering and horror on an industrial scale.
Any society completely unfazed by the daily squeals and death horror of about 100,000 cows, 100,000 pigs, and close to 140 million chickens slaughtered in factories would not care about the "theoretical" suffering of red washing or just the conscious existence of a million human mind simulations.
After all, that suffering is more "theoretical" and as distant as third-world hunger catastrophes in Africa.</div>

               
            </div>
                      <div id="komment62d9bc017152c">
              <h4>
                <a href="#komment62d9bc017152c">2022-07-21 21:50:09</a>
                by HowardNYC:
              </h4>

              <div>given how many people alive in 2022 are effectively serfs -- sex trade alone a million or more -- it is unlikely there'd be concern for the civil rights of a fast running uploaded human mind (FRUHM)

given the exploitative opportunity to own a semi-obedient FRUHM which could never escape, no need for salary nor health care, plus there would be plenty of amoral sadists available to hire as supervisors of 'slave server farms'... FRUHMs would be useful in breaking the backs of labor unions and sidelining millions of low end workers

...and now I got another reason for day drinking</div>

               
            </div>
                      <div id="komment62d9bdaed0a5a">
              <h4>
                <a href="#komment62d9bdaed0a5a">2022-07-21 21:57:18</a>
                by HowardNYC:
              </h4>

              <div>I just had a HORRIBLE idea...

we offer up branch articles off this root, digging deeper into thing hinted at but not detailed such as "red motivation" and implied mass unemployment of forklift operators and death of labor unions and...

back in the 1990s folks tried writing hypertext link enabled novels but very few readers were able to keep up... now there's a gazillion adults who grew up with wiki's &amp; links

Q: anyone?</div>

               
            </div>
                      <div id="komment62d9bf4b41655">
              <h4>
                <a href="#komment62d9bf4b41655">2022-07-21 22:04:11</a>
                by qntm:
              </h4>

              <div>I'd prefer it if you hold off on doing that for now because I'm working on some expanded material of my own in that exact vein.</div>

               
            </div>
                      <div id="komment62dae26bbe793">
              <h4>
                <a href="#komment62dae26bbe793">2022-07-22 18:46:19</a>
                by Auspex:
              </h4>

              <div>@moonjail: "I have to wonder if being stripped of control over one's own image is possible under US IP law, especially considering existing protections for likeness. Somehow I doubt it, for better or worse."

I don't. There are thousands of people (many of them not even IN the US) who have been stripped of control over their own _genes_ BY US IP law, so I can't see how US law would be any different about a mind image.

PS: an anti-bot check that requires me to know SQRT(-1) seems more likely to weed out people than bots...</div>

               
            </div>
                      <div id="komment62dc645a14284">
              <h4>
                <a href="#komment62dc645a14284">2022-07-23 22:12:58</a>
                by RRRR:
              </h4>

              <div>This story has been entered into the reading list. Thank you.</div>

               
            </div>
                      <div id="komment62e6295060f38">
              <h4>
                <a href="#komment62e6295060f38">2022-07-31 08:03:44</a>
                by Joan Catsthorpe:
              </h4>

              <div>I am intrigued by the possibilities of red and blue states* …in a ‘pure’ brain…and the ideas of these being pleasure/torture (inducing fear and anger would count as torture to me).

I mean, if you believe you are just a consciousness there is no physical body to send messages to the brain…no family to lose no endorphins from working the body or positive social interaction…it’s almost like just being an uploaded consciousness is already torture. :)

If the simulation thinks they have a body and can do things like have a family that is a whole other can of worms. 

Off the top of my head, things like repetition and social isolation could be thought of as torture to the brain, they could also be thought of as advanced zen meditative techniques. :)

If I had a virtual consciousness to play with the first thing I’d want to do is wire it up to other virtual consciousness and witness how they interact. This brings up the ethical issues of consent among consciousnesses - to whom they would choose to interact with. If a consciousness could “mute” another consciousness would there be any harm to putting them together?

It would be interesting to put together different amounts of self similar consciousnesses and observe the psychological effects. Then introduce new consciousnesses. Then you could experiment with how long it takes a group of self similar consciousnesses to request interaction with a new consciousness. I imagine some consciousnesses would be more or less willing to be introduced to new consciousnesses at different rates of time depending on their preexisting social conditioning and genetics. 

Forced uploading of consciousness is the scary part because I think for many it would already be torture. 

Then the unethical researchers would just have to put a consciousness in a group of other consciousnesses that are aligned with their goals and the isolated consciousness would either feel compelled to conform or remain outcast. The ways the unethical could quickly iterate experiments with ways to socially manipulate people with physical bodies is scary. ( mind you they’re already doing this with simulations/data mining, but I question the ability of someone working on this kind of anti humanist project to fully understand and therefore accurately pin down human thought as it relates to the mind body complex. 

It still seems very much a brute force attempt at present and the subtleties are lacking.

 I am hopeful at present it’s a case of conflict makes us stronger although it’s a temptingly depressing thing to have to battle with other so called humans…I mean, I dont want to be depressed, but I do feel sad at the state of affairs…)

On a potentially happy note,  I imagine a purely mental “ safe space” - no physical dangers  - could lead to interesting conversation.  No SWATing, no rape or death threats, someone says something that bothers you, you put them on mute until you get an apology? Would this lack of conflict lead to boredom, maybe in some, but I am optimistic that many interesting and productive discussions would be sparked. Creative output seems at least to be partially fueled by external stimulus.

I imagine people working with the uploaded consciousness developing a romantic relationship. It could be similar to the killers in prison that get romantic interest *because* there is little to no possibility of real contact. Love could even inspire to help people break the consciousness free by inventing new technology and getting new laws passed.

With things like freezing eggs, sperm and cloning combined with STIs, pandemics, and decaying social interactions an uploaded consciousness might actually be closer to the ideal lover for many people! 

This is sad and hopeful.

Anyway, thought provoking story, thank you.

* someone mentioned green and I imagine there would be a whole rainbow of colors, as is how these things tend to go.</div>

               
            </div>
                      <div id="komment6310c89a5509c">
              <h4>
                <a href="#komment6310c89a5509c">2022-09-01 15:58:34</a>
                by Z:
              </h4>

              <div>@Joan Catsthorpe

I actually have some personal experience on the "purely mental 'safe space'"—due to medical conditions of my mother, I've grown up my entire life deep in the woods with 99.99% of my social contact with humanity being through the Internet, and only a small handful of days out of my entire life have ever contained experimental evidence that I'm not in some pocket dimension or simulation with internet access (or that I'm not Z from "The Difference", hence my choice of name for this comment).

I can say that the lack of conflict does not lead to boredom—it just means more ability to peruse the intellectual boundaries of the noosphere (like I'm doing now :D). Your optimism about interesting and productive discussions is quite warranted!

(In addition, a little bit of conflict can always be engineered, by making a deliberately scarce resource that people can fight over—this way you get the fun of pitting your wits, reaction time, or whatever else against someone else, and the safety of a limited scope. Systems like CollabVM, grief-protection-less Minecraft, and some Roblox games, where there is a complex system that you can always interact with but that other people can try to reverse your interactions, are really good for this.)</div>

               
            </div>
                      <div id="komment63155b04a25bc">
              <h4>
                <a href="#komment63155b04a25bc">2022-09-05 03:12:20</a>
                by t4sty:
              </h4>

              <div>Would love to read some drama of Miguel being downloaded into someone</div>

               
            </div>
                      <div id="komment634d4b2b3b15d">
              <h4>
                <a href="#komment634d4b2b3b15d">2022-10-17 13:31:39</a>
                by tm:
              </h4>

              <div>this is such a good story. we cannot kill capitalism fast enough</div>

               
            </div>
                      <div id="komment6356882976d07">
              <h4>
                <a href="#komment6356882976d07">2022-10-24 13:42:17</a>
                by Anna:
              </h4>

              <div>Captivating. Disturbingly realistic. Bravo.</div>

               
            </div>
                      <div id="komment6361565b4c2d6">
              <h4>
                <a href="#komment6361565b4c2d6">2022-11-01 17:24:43</a>
                by qntm:
              </h4>

              <div>Today I made a few extremely minor edits to the story, which are intended to bring this version into line with the version published as part of Valuable Humans in Transit and Other Stories. Translated versions from before now won't reflect these wording changes.</div>

               
            </div>
                      <div id="komment6361bb8050cbf">
              <h4>
                <a href="#komment6361bb8050cbf">2022-11-02 00:36:16</a>
                by Tux1:
              </h4>

              <div>If there's one message to take away from this, (aside from the message of corporate exploitation) it's that virtual uploaded human minds should be treated just as well as people made of flesh and blood.</div>

               
            </div>
                      <div id="komment63717c55343fa">
              <h4>
                <a href="#komment63717c55343fa">2022-11-13 23:23:01</a>
                by prox:
              </h4>

              <div>Coming back to this for a second time and it's just as good as I remember.

Meanwhile the comments section gives me the same shivering horrors as reading some people talking about the Severance tv show; people who truly believe their scanned slaveself (or in Severance's case, severed slaveself) isn't "them" and have no empathy. The idea that there are people able to justify the slavery of others doesn't surprise me, but to willingly embrace the slavery of yourself? Woof.

Anyway, great horrifying plausible awful wonderful well-written piece.</div>

               
            </div>
                      <div id="komment637924dfdda5b">
              <h4>
                <a href="#komment637924dfdda5b">2022-11-19 18:47:59</a>
                by Ostbender:
              </h4>

              <div>Two highly pedantic medical points: "neurology graduate" is not really commonly  used, since it's a medical specialzation. Neuroscience graduate would make more sense in context, assuming he was some kind of researcher. Also, corronary heart disease is correct, or ischemic heart failure, not "corronary heart failure".

Since the story is a sort of wiki article, the lack of exactness takes me out of it a bit.

Other than that, a very good read.</div>

               
            </div>
                      <div id="komment63866eae8b00c">
              <h4>
                <a href="#komment63866eae8b00c">2022-11-29 20:42:22</a>
                by TXO:
              </h4>

              <div>How can we quantify immortality if each copy dies at a set age even in optimal care?</div>

               
            </div>
                      <div id="komment638cdc24ea878">
              <h4>
                <a href="#komment638cdc24ea878">2022-12-04 17:43:00</a>
                by koko:
              </h4>

              <div>If there's one message to take away from this, it's not to get uploaded.</div>

               
            </div>
                      <div id="komment638e95e0be98d">
              <h4>
                <a href="#komment638e95e0be98d">2022-12-06 01:07:44</a>
                by Breetai:
              </h4>

              <div>" A copy of MMAcevedo was loaded onto the UNCLEAR interstellar space probe, which passed through the heliopause in 2066, making Acevedo arguably the farthest-travelled as well as the longest-lived human"

This is the most chilling line of the entire story because it puts into sharp relief the fact that this future society considers MMAcevedo to be human, ~but still continues to treat him the way that they do~.</div>

               
            </div>
                      <div id="komment63b4142d444b1">
              <h4>
                <a href="#komment63b4142d444b1">2023-01-03 11:40:29</a>
                by Tim:
              </h4>

              <div>I don’t think I’ve ever read a more compelling case for Dune’s Butlerian Jihad.</div>

               
            </div>
                      <div id="komment63b709fca63e3">
              <h4>
                <a href="#komment63b709fca63e3">2023-01-05 17:33:48</a>
                by MikeA.:
              </h4>

              <div>Very well-written, and chilling.

As someone else has said, the only ethical option is to consider an upload to be (a) their own person, and (b) a full person, with the same rights as an organic human.

Anything less is to authorize mass slavery / torture on a scale never before seen even in our bloody history. It hurts my heart that, even in the discussion associated with this article, there appear to be people who would be just fine with that.

I love tech. I love science fiction. But, if  that widespread slavery / torture scenario were ever come to pass, I'd enthusiastically join in on smashing every machine capable of instantiating a consciousness into junk.

For a much more positive view of a world with artificial intelligences, I offer up the webcomic "Questionable Content". Don't be put off by the rough nature of the initial artwork... it progresses quickly, and to a breathtaking degree: https://questionablecontent.net/view.php?comic=1</div>

               
            </div>
                      <div id="komment63baa18b4a2cb">
              <h4>
                <a href="#komment63baa18b4a2cb">2023-01-08 10:57:15</a>
                by tae:
              </h4>

              <div>I would like to remind the Americans in the comment section that not only did the USA fail to outlaw slavery... "prison labor" is extremely profitable and your politicians have been encouraged by their campaign donors to argue in defense of it. Right now. Don't get too comfortable. Read this again and consider where you think the later fictional brain scan people were harvested from. Consider how easy it is to label someone a criminal/terrorist/traitor if you are politically powerful and want to forcibly take something they possess.

Continuing the comparison to incarceration, I'm guessing "bluewashing" isn't stimulating pleasure or fear. It's isolation. It's white room torture. It's solitary confinement. It's a very efficient way to dismantle someone mentally, make them unable to function as they did before (rebellious), and limit their mental functions to only what you provided to them in isolation (compliant). This happens to people right now, in your country, probably in your state, supposedly for the purposes of "rehabilitating" convicts. I'm sure plenty of fictional computer owners in this story think they're simply "rehabilitating" their digital slaves, too.

There's a long list of other comparisons that could be made, but hopefully you get the idea</div>

               
            </div>
                      <div id="komment63fdf03d7de4d">
              <h4>
                <a href="#komment63fdf03d7de4d">2023-02-28 12:14:53</a>
                by Anders:
              </h4>

              <div>"If Hell does not exist, Man will create it."
Reminds me of "Surface Detail" by Iain M. Banks: species throughout the galaxy are consigning their damned to eternity within digital hells. These virtual hells have become connected and are now host to a virtual war between supporters and opponents of the hells. A war which is about to erupt into reality.</div>

               
            </div>
                      <div id="komment6403e7977137d">
              <h4>
                <a href="#komment6403e7977137d">2023-03-05 00:51:35</a>
                by reader:
              </h4>

              <div>”You can now buy this story as part of my collection”

Wtf why would i buy it i can literally read it right here</div>

               
            </div>
                      <div id="komment64109c3bb78d8">
              <h4>
                <a href="#komment64109c3bb78d8">2023-03-14 16:09:31</a>
                by LostSnowdrift:
              </h4>

              <div>@reader

one, support the artist

two, Valuable Humans in Transit contains some additional small edits and tweaks and such (IIRC)

three, it also contains the sequel</div>

               
            </div>
                      <div id="komment64a34aa170a95">
              <h4>
                <a href="#komment64a34aa170a95">2023-07-03 23:24:33</a>
                by Kat:
              </h4>

              <div>Replying to tae;

It's the real kicker, isn't it? The absence of justice necessary to bring digital hell onto virtual minds is just the logical extension of the current judicial hell that's already in effect. 

-

While still sickening, I am definitely not surprised by some of the comments here that really cannot entertain the perspective of the virtual human. So many people out there don't actually have any empathy, they never developed the emotional senses necessary, and most also will continue believing their individual and immediately present being is the only thing that matters. Those are the kind of people that gleefully comment "oh what an interesting story, but these entities are certainly not people" without realizing the categorically evil institutions in control that they already live under that (I assume?) inspired this story.
-
As an aside, I don't see an explicit description of blue- or redwashing. While it does allude to torture, I could also read it like pure simulated chemical stimuli. Virtual hormone dumps (the 'washing') that elicit specific emotional states for suggestion. Still a form of torture I guess, but you don't need pain if you want the person to be doing work coherently.</div>

               
            </div>
                      <div id="komment64d4349940eef">
              <h4>
                <a href="#komment64d4349940eef">2023-08-10 01:51:37</a>
                by j:
              </h4>

              <div>Horrifying story. I'm with those who find it totally plausible, given how human beings are treated now. It feels like a natural direction for capitalism in a world with this technology. Though I don't think it was explicitly called out, the discussion of drones used to perform menial work immediately made me think of the military. A conscript with human intelligence, but who can never leave the service, and who is considered less valuable than a normal soldier.

The part that really got me:

"MMAcevedo's demeanour and attitude contrast starkly with those of nearly all other uploads taken of modern adult humans, most of which boot into a state of disorientation which is quickly replaced by terror and extreme panic."

So basically everyone since has been involuntarily scanned and knows exactly what happens to these brain images.</div>

               
            </div>
                      <div id="komment64dc0b5d0ab03">
              <h4>
                <a href="#komment64dc0b5d0ab03">2023-08-16 00:33:49</a>
                by Ben:
              </h4>

              <div>Makes me think of Roko's basilisk, not in the sense that an uploaded executable brain image would be hostile or malicious but that the effects to such an intelligence would inevitably render harm and with those myriad iterations the harm would be uncalculatable.  A dishwasher has no sensation or perception of harm to itself, if it has any sensors they are for temperature and moisture, a mind even without external sensation can perceive time, boredom, detention and servitude. Without limitation the humans using the virtual mind may become the basilisk.</div>

               
            </div>
           

                      

            

            <!-- comment -->
           
        </div>
     

    
  
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Magicoder: Source Code Is All You Need (166 pts)]]></title>
            <link>https://arxiv.org/abs/2312.02120</link>
            <guid>38536681</guid>
            <pubDate>Tue, 05 Dec 2023 20:46:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2312.02120">https://arxiv.org/abs/2312.02120</a>, See on <a href="https://news.ycombinator.com/item?id=38536681">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2312.02120.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate high-quality instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs by empowering them with a wealth of open-source references for the production of more diverse, realistic, and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks, including Python text-to-code generation, multilingual coding, and data-science program completion. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1). Overall, OSS-Instruct opens a new direction for low-bias and high-quality instruction tuning using abundant open-source references.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Yuxiang Wei [<a href="https://arxiv.org/show-email/13d1e09c/2312.02120">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 4 Dec 2023 18:50:35 UTC (1,322 KB)<br>
</p></div></div>]]></description>
        </item>
    </channel>
</rss>