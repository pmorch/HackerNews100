<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 04 Oct 2025 16:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Circular Financing: Does Nvidia's $110B Bet Echo the Telecom Bubble? (170 pts)]]></title>
            <link>https://tomtunguz.com/nvidia_nortel_vendor_financing_comparison/</link>
            <guid>45473033</guid>
            <pubDate>Sat, 04 Oct 2025 13:06:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tomtunguz.com/nvidia_nortel_vendor_financing_comparison/">https://tomtunguz.com/nvidia_nortel_vendor_financing_comparison/</a>, See on <a href="https://news.ycombinator.com/item?id=45473033">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
            

            <div>
                <p>When Nvidia announced a $100 billion investment commitment to OpenAI<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> in September 2025 , analysts immediately drew comparisons to the telecom bubble. The concern : is this vendor financing , where a supplier lends money to customers so they can buy the supplier’s products , a harbinger of another spectacular collapse?</p>
<p>American tech companies will spend <strong>$300-400 billion</strong> on AI infrastructure in 2025<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup><sup>,</sup><sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> , exceeding any prior single-year corporate infrastructure investment in nominal dollars.<sup id="fnref1:3"><a href="#fn:3" role="doc-noteref">3</a></sup> David Cahn estimates the revenue gap has grown to <strong>$600 billion</strong><sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>.</p>
<p>I analyzed the numbers. The similarities are striking , but the differences matter.</p>
<h2 id="the-lucent-playbook">The Lucent Playbook</h2>
<p><a href="https://res.cloudinary.com/dzawgnnlr/image/upload/q_auto,f_auto/pt1apmgovtkb2aegncym" target="_blank"><img src="https://res.cloudinary.com/dzawgnnlr/image/upload/w_1512,h_850,c_fill,g_auto,q_auto,f_auto/pt1apmgovtkb2aegncym" alt="Lucent vs Nvidia Revenue Comparison 1996-2024" width="756" height="425" loading="lazy"></a></p>
<!--[if mso | IE]>
  </v:textbox>
</v:rect>
<![endif]-->
<p><em>Lucent’s revenue peaked at $37.92B in 1999 , crashed 69% to $11.80B by 2002 , never recovered. Merged with Alcatel in 2006.</em></p>
<p>In 1999 , Lucent Technologies reached $37.92 billion in revenue at the peak of the dot-com bubble. <sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> Lucent was the #1 North American telecommunications equipment manufacturer with 157,000 employees &amp; dominated markets alongside Nortel Networks (combined 53% optical transport market share). <sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> Behind the scenes , equipment makers extended billions in vendor financing to telecom customers. Lucent committed <strong>$8.1B</strong><sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup> , Nortel extended $3.1B with $1.4B outstanding , &amp; Cisco promised $2.4B in customer loans.<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup></p>
<p>The strategy seemed brilliant : lend money to cash-strapped telecom companies so they could buy your equipment. Everyone wins—until the merry-go-round stops.</p>
<p>When the bubble burst :</p>
<ul>
<li>47 Competitive Local Exchange Carriers (CLECs) bankrupted 2000-2003 , including Covad , Focal Communications , McLeod , Northpoint , Winstar <sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup><sup>,</sup><sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup>
<ul>
<li>Why they failed : $60B overbuild 1996-2001 , market saturation from identical business models , sudden funding collapse (Jan 2001 : billions available , Apr 2001 : zero)<sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup></li>
</ul>
</li>
<li>33-80% of vendor loan portfolios went uncollected as customers failed &amp; equipment became worthless<sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup></li>
<li>Fiber networks were using less than 0.002% of available capacity , with potential for 60,000x speed increases. <sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup> It was just too early.</li>
</ul>
<h2 id="nvidias-playbook">Nvidia’s Playbook</h2>
<p>Fast forward to 2025. Nvidia’s vendor financing strategy totals <strong>$110 billion in direct investments</strong> plus another <strong>$15+ billion in GPU-backed debt</strong>. The largest commitment is $100B to OpenAI (September 2025)<sup id="fnref1:1"><a href="#fn:1" role="doc-noteref">1</a></sup><sup>,</sup><sup id="fnref:14"><a href="#fn:14" role="doc-noteref">14</a></sup> , structured as 10 tranches of $10B each tied to infrastructure deployment milestones. The first $10B was valued at a $500B OpenAI valuation , with subsequent tranches priced at prevailing valuations. Payment comes via lease arrangements , not upfront GPU purchases. OpenAI CFO Sarah Friar confirmed : “Most of the money will go back to Nvidia”<sup id="fnref1:14"><a href="#fn:14" role="doc-noteref">14</a></sup></p>
<p>Beyond OpenAI , Nvidia holds a $3B stake in CoreWeave<sup id="fnref:15"><a href="#fn:15" role="doc-noteref">15</a></sup> , a company that has spent $7.5B on Nvidia GPUs , &amp; $3.7B in other AI startup investments<sup id="fnref:16"><a href="#fn:16" role="doc-noteref">16</a></sup> through NVentures.</p>
<p>The GPU-backed debt market adds another layer. CoreWeave alone carries $10.45B in debt using GPUs as collateral<sup id="fnref:17"><a href="#fn:17" role="doc-noteref">17</a></sup>. An additional $10B+ in GPU-backed debt has emerged for “Neoclouds” including Lambda Labs ($500M GPU-backed loan)<sup id="fnref:18"><a href="#fn:18" role="doc-noteref">18</a></sup><sup>,</sup><sup id="fnref:19"><a href="#fn:19" role="doc-noteref">19</a></sup>.</p>
<p>Lucent in 1999-2000 had vendor financing commitments of $8.1B (20% of $41.4B revenue). Nvidia’s direct investments total 85% of annual revenue ($110B against $130B). Nvidia’s exposure is 4x larger relative to revenue than Lucent’s official outstanding loans , though Lucent’s off-balance-sheet guarantees masked the true exposure.</p>
<h2 id="the-numbers-side-by-side-2024-dollars">The Numbers Side-by-Side (2024 Dollars)</h2>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Lucent (FY2000, inflation-adj.)</th>
<th>Nvidia (2025)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vendor financing</td>
<td>$15B</td>
<td>$110B</td>
</tr>
<tr>
<td>Operating cash flow</td>
<td>$304M<sup id="fnref:20"><a href="#fn:20" role="doc-noteref">20</a></sup></td>
<td>$15.4B (Q2 FY26)</td>
</tr>
<tr>
<td>Revenue</td>
<td>$34B</td>
<td>$130B</td>
</tr>
<tr>
<td>Top 2 Customers represent</td>
<td>23%<sup id="fnref:21"><a href="#fn:21" role="doc-noteref">21</a></sup></td>
<td>39%</td>
</tr>
</tbody>
</table>
<h2 id="the-reasons-to-be-wary">The Reasons to be Wary</h2>
<h3 id="1-the-ai-customer-base-is-more-concentrated">1. The AI Customer Base is More Concentrated</h3>
<p>Lucent’s top 2 customers—AT&amp;T at 10% &amp; Verizon at 13%—accounted for 23% of revenue in FY2000.<sup id="fnref1:21"><a href="#fn:21" role="doc-noteref">21</a></sup> The Regional Bell Operating Companies , or RBOCs , the seven “Baby Bells” created from AT&amp;T’s 1984 breakup , were also major customers. Nvidia has 39% of revenue from just 2 customers &amp; 46% from 4 customers , nearly double Lucent’s concentration. 88% of Nvidia’s revenue comes from data centers.</p>
<h3 id="2-gpu-backed-debt-is-new">2. GPU-Backed Debt Is New</h3>
<p>The new $10B+ GPU-backed debt market is built on the assumption that GPUs will hold their value over 4-6 years. GPU-backed loans carry ~14% interest rates<sup id="fnref:22"><a href="#fn:22" role="doc-noteref">22</a></sup> , triple investment-grade corporate debt.<sup id="fnref:23"><a href="#fn:23" role="doc-noteref">23</a></sup></p>
<p>How Depreciation Schedules Changed :</p>
<table>
<thead>
<tr>
<th>Company</th>
<th>Pre-2020</th>
<th>2020-2021</th>
<th>2022-2023</th>
<th>2024-2025</th>
<th>Change</th>
</tr>
</thead>
<tbody>
<tr>
<td>Amazon<sup id="fnref:24"><a href="#fn:24" role="doc-noteref">24</a></sup></td>
<td>3 years</td>
<td>4 years (2020) → 5 years (2021)</td>
<td>5 years</td>
<td>6 years (2024) → 5 years (2025)</td>
<td>First reversal</td>
</tr>
<tr>
<td>Microsoft<sup id="fnref:25"><a href="#fn:25" role="doc-noteref">25</a></sup></td>
<td>~3 years</td>
<td>4 years</td>
<td>6 years</td>
<td>6 years</td>
<td>+100%</td>
</tr>
<tr>
<td>Google<sup id="fnref:26"><a href="#fn:26" role="doc-noteref">26</a></sup></td>
<td>~3 years</td>
<td>4 years</td>
<td>6 years</td>
<td>6 years</td>
<td>+100%</td>
</tr>
<tr>
<td>Meta<sup id="fnref:27"><a href="#fn:27" role="doc-noteref">27</a></sup></td>
<td>~3 years</td>
<td>4 years</td>
<td>4.5 years → 5 years</td>
<td>5.5 years</td>
<td>+83%</td>
</tr>
<tr>
<td>CoreWeave<sup id="fnref:28"><a href="#fn:28" role="doc-noteref">28</a></sup></td>
<td>N/A</td>
<td>N/A</td>
<td>4 years → 6 years (Jan 2023)</td>
<td>6 years</td>
<td>+50% (GPUs)</td>
</tr>
<tr>
<td>Nebius<sup id="fnref:29"><a href="#fn:29" role="doc-noteref">29</a></sup></td>
<td>N/A</td>
<td>N/A</td>
<td>4 years</td>
<td>4 years</td>
<td>Industry standard</td>
</tr>
</tbody>
</table>
<p>Amazon’s 2025 reversal (6 → 5 years) is the first major pullback.</p>
<p>CPUs historically have 5-10 years of useful life , while GPUs in AI datacenters last 1-3 years in practice , despite 6-year accounting assumptions.<sup id="fnref:30"><a href="#fn:30" role="doc-noteref">30</a></sup><sup>,</sup><sup id="fnref:31"><a href="#fn:31" role="doc-noteref">31</a></sup> Evidence from Google architects shows GPUs at 60-70% utilization survive 1-2 years , with 3 years maximum.<sup id="fnref1:31"><a href="#fn:31" role="doc-noteref">31</a></sup> Meta’s Llama 3 training experienced 9% annual GPU failure rates , suggesting 27% failure over 3 years.<sup id="fnref2:31"><a href="#fn:31" role="doc-noteref">31</a></sup></p>
<p>Cerno Capital raises the question : “Are these policies a reflection of genuine economic &amp; technological realities? Or are these policies a lever by which hyperscalers are enhancing the optics of their investment programs amid rising investor concerns?”<sup id="fnref:32"><a href="#fn:32" role="doc-noteref">32</a></sup></p>
<h3 id="4-the-use-of-spvs">4. The Use of SPVs</h3>
<p>Tech companies use Special Purpose Vehicles (SPVs) to finance AI datacenter construction. A hyperscaler like Meta partners with a private equity firm like Apollo , contributing capital to a separate legal entity that builds &amp; owns the datacenter.</p>
<p>As investor Paul Kedrosky explains : “I have a stake in it as Meta. Some giant private debt provider has a stake in it. The datacenter is under my control. But I don’t own it, so you don’t get to roll it back into my balance sheet.”<sup id="fnref1:2"><a href="#fn:2" role="doc-noteref">2</a></sup>*</p>
<p><strong>The Structure</strong></p>
<ol>
<li><strong>Entity Creation</strong> : Hyperscaler &amp; PE firm form separate legal entity (SPV)</li>
<li><strong>Capital Structure</strong> : Typically 10-30% equity, 70-90% debt from private credit markets</li>
<li><strong>Lease Agreement</strong> : SPV leases capacity back to hyperscaler</li>
<li><strong>Balance Sheet Treatment</strong> : SPV debt doesn’t appear on hyperscaler’s balance sheet</li>
</ol>
<p>The hyperscaler maintains operational control through long-term lease agreements. Because it doesn’t directly own the SPV , the debt remains off its balance sheet under current accounting standards.</p>
<p>The appeal is straightforward. “I don’t want the credit rating agencies to look at what I’m spending. I don’t want investors to roll it up into my income statement.”<sup id="fnref2:2"><a href="#fn:2" role="doc-noteref">2</a></sup>*</p>
<p><strong>Market Scale</strong></p>
<p>American tech companies are projected to spend $300-400 billion on AI infrastructure in 2025. Hyperscaler capital expenditures have reached approximately 50% of operating income<sup id="fnref3:2"><a href="#fn:2" role="doc-noteref">2</a></sup>, levels historically associated with government infrastructure buildouts rather than technology companies.</p>
<p><strong>Where the Risk Sits</strong></p>
<p>Datacenter assets now represent 10-22% of major REIT portfolios<sup id="fnref4:2"><a href="#fn:2" role="doc-noteref">2</a></sup> , up from near zero two years ago. The thin equity layer (10-30%) means if datacenter utilization falls short of projections or if GPUs depreciate faster than projected , equity holders face losses before debt holders experience impairment.</p>
<p><em>*Quotes lightly edited for clarity &amp; brevity</em></p>
<h3 id="5-custom-silicon-threat">5. Custom Silicon Threat</h3>
<p>Hyperscalers are building their own AI accelerators to reduce Nvidia dependence. Microsoft aims to use “mainly Microsoft silicon” , specifically Maia accelerators , in datacenters.<sup id="fnref:33"><a href="#fn:33" role="doc-noteref">33</a></sup> Google deploys <a href="https://cloud.google.com/tpu">TPUs</a> , Amazon builds <a href="https://aws.amazon.com/ai/machine-learning/trainium/">Trainium</a> &amp; <a href="https://aws.amazon.com/ai/machine-learning/inferentia/">Inferentia</a> chips , &amp; Meta develops <a href="https://ai.meta.com/blog/next-generation-meta-training-inference-accelerator-AI-MTIA/">MTIA</a> processors. If customers shift to in-house silicon , CoreWeave’s GPU collateral value &amp; Nvidia’s vendor financing become exposure to customers building competitive alternatives.</p>
<h2 id="nvidia-isnt-lucent--2025-isnt-2000">Nvidia Isn’t Lucent &amp; 2025 Isn’t 2000</h2>
<ul>
<li><strong>Accounting</strong> : Lucent manipulated $1.148B in revenue , SEC charged 10 executives with fraud<sup id="fnref1:5"><a href="#fn:5" role="doc-noteref">5</a></sup> ; Nvidia shows no evidence of manipulation , audited by PwC , Aa3 rated<sup id="fnref:34"><a href="#fn:34" role="doc-noteref">34</a></sup></li>
<li><strong>Cash flow</strong> : Lucent lent $8.1B while cash flow lagged profitability &amp; receivables exploded $5.4B (1998-1999)<sup id="fnref1:20"><a href="#fn:20" role="doc-noteref">20</a></sup> ; Nvidia lends with $50B+ annual operating cash flow &amp; $46.2B net cash<sup id="fnref:35"><a href="#fn:35" role="doc-noteref">35</a></sup></li>
<li><strong>Credit rating</strong> : Lucent downgraded to A3 (December 2000)<sup id="fnref:36"><a href="#fn:36" role="doc-noteref">36</a></sup> ; Nvidia upgraded to Aa3 (March 2024)<sup id="fnref1:34"><a href="#fn:34" role="doc-noteref">34</a></sup></li>
<li><strong>Customer base</strong> : Lucent’s customers were leveraged CLECs burning capital ; Nvidia’s top 4 customers generated $451B in operating cash flow in 2024 (Microsoft $119B , Alphabet $125B , Amazon $116B , Meta $91.3B)<sup id="fnref:37"><a href="#fn:37" role="doc-noteref">37</a></sup></li>
<li><strong>Capacity</strong> : Fiber networks used &lt;0.002% of capacity in 2000<sup id="fnref1:13"><a href="#fn:13" role="doc-noteref">13</a></sup> ; Microsoft &amp; AWS report AI capacity constraints in 2025<sup id="fnref:38"><a href="#fn:38" role="doc-noteref">38</a></sup><sup>,</sup><sup id="fnref:39"><a href="#fn:39" role="doc-noteref">39</a></sup></li>
</ul>
<h2 id="what-im-watching">What I’m Watching</h2>
<p>Is AI demand real (like cloud computing) or speculative (like dot-com fiber)?</p>
<p>Here’s what I’m watching :</p>
<ol>
<li><strong>GPU utilization rates</strong> : Are data centers actually using the chips or just stockpiling?</li>
<li><strong>OpenAI’s monetization</strong> : Can they generate enough revenue to justify the buildout?</li>
<li><strong>Debt defaults</strong> : Any cracks in the $15B GPU-backed debt market?</li>
<li><strong>AR trends</strong> : AR improved from 68% (FY24) to 30% (Q2 FY26) , but still watch for deterioration</li>
<li><strong>Customer adds</strong> : Are new customers emerging , or is Nvidia dependent on the same 2-4 hyperscalers?</li>
<li><strong>Custom silicon threat</strong> : Microsoft developing Maia accelerators , aiming to use “mainly Microsoft silicon in the data center.”<sup id="fnref1:33"><a href="#fn:33" role="doc-noteref">33</a></sup> If hyperscalers shift to in-house chips , Nvidia’s vendor financing becomes exposure to customers building competitive alternatives.</li>
<li><strong>Vendor consolidation</strong> : Many companies are in a period of experimentation trying 2-3 competing vendors. Those experimental budgets may thin with time , reducing overall spend.</li>
</ol>
<p>AI is already broadly deployed—40% of US employees used AI at work by September 2025 , double the 20% rate in 2023.<sup id="fnref:40"><a href="#fn:40" role="doc-noteref">40</a></sup> Questions persist about effectiveness : the oft cited MIT study found 95% of AI pilots failed to deliver measurable P&amp;L impact , primarily due to poor integration rather than technical failures.<sup id="fnref:41"><a href="#fn:41" role="doc-noteref">41</a></sup></p>
<p>Yet the pace of improvement is tremendous. Labor market data shows wages rising twice as fast in AI-exposed industries , &amp; workers using AI boost performance up to 40%.<sup id="fnref1:40"><a href="#fn:40" role="doc-noteref">40</a></sup> Many of Nvidia’s customers are profitable &amp; sophisticated hyperscalers—Microsoft , Google , Amazon , Meta—generating $451B in operating cash flow in 2024 , with tremendous pull from their own enterprise customers demanding AI. OpenAI is not profitable , reporting a $4.7B loss in H1 2025 on $4.3B revenue , though nearly half the loss is stock-based compensation.<sup id="fnref:42"><a href="#fn:42" role="doc-noteref">42</a></sup></p>
<p>Unlike the telecom bubble , where demand was speculative &amp; customers burned cash , this merry-go-round has paying riders.</p>
<hr>
<h2 id="coda--lucents-accounting-fraud">Coda : Lucent’s Accounting Fraud</h2>
<p>Behind the vendor financing disaster was systematic accounting fraud. The SEC charged Lucent with manipulating $1.148 billion in revenue &amp; $470 million in pre-tax income during fiscal year 2000. <sup id="fnref2:5"><a href="#fn:5" role="doc-noteref">5</a></sup> The fraud involved multiple schemes :</p>
<p>Channel Stuffing : Lucent sent $452 million in equipment to distributors but counted it as revenue before the distributors sold to end customers.<sup id="fnref3:5"><a href="#fn:5" role="doc-noteref">5</a></sup> This created phantom sales.</p>
<p>Side Agreements : Lucent executives entered secret agreements with distributors granting them return rights &amp; privileges beyond their distribution contracts , making it improper to recognize revenue.<sup id="fnref4:5"><a href="#fn:5" role="doc-noteref">5</a></sup> These side deals were hidden from auditors.</p>
<p>Reserve Manipulation : Lucent improperly established &amp; maintained excess reserves to smooth earnings , violating GAAP.<sup id="fnref5:5"><a href="#fn:5" role="doc-noteref">5</a></sup></p>
<p>The SEC charged 10 Lucent executives with securities fraud.<sup id="fnref6:5"><a href="#fn:5" role="doc-noteref">5</a></sup> The company paid a $25 million fine—the largest ever for failing to cooperate with an SEC investigation.<sup id="fnref7:5"><a href="#fn:5" role="doc-noteref">5</a></sup> The accounting manipulation masked deteriorating fundamentals until too late.</p>
<p>The WinStar Collapse : Lucent committed $2 billion in vendor financing to WinStar Communications , a CLEC. When WinStar struggled , Lucent refused a final $90 million loan extension. WinStar filed bankruptcy. Lucent wrote off $700 million in bad debts.<sup id="fnref:43"><a href="#fn:43" role="doc-noteref">43</a></sup> This pattern repeated across customer defaults : Lucent made provisions for bad debts of $2.2 billion (2001) &amp; $1.3 billion (2002)—a total of $3.5 billion in customer loan losses.<sup id="fnref1:43"><a href="#fn:43" role="doc-noteref">43</a></sup></p>
<hr>
<h2 id="references">References</h2>


            </div>
            
            
        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google removes ICE-spotting app following Apple's ICEBlock crackdown (116 pts)]]></title>
            <link>https://www.theverge.com/news/791533/google-apple-ice-tracking-app-store-red-dot-iceblock</link>
            <guid>45472799</guid>
            <pubDate>Sat, 04 Oct 2025 12:23:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/news/791533/google-apple-ice-tracking-app-store-red-dot-iceblock">https://www.theverge.com/news/791533/google-apple-ice-tracking-app-store-red-dot-iceblock</a>, See on <a href="https://news.ycombinator.com/item?id=45472799">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><a href="https://www.theverge.com/authors/emma-roth"><img alt="Emma Roth" data-chromatic="ignore" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195810/EMMA_ROTH.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=48 1x, https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195810/EMMA_ROTH.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96 2x" src="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195810/EMMA_ROTH.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96"></a></p><div><p><span aria-expanded="false" aria-haspopup="true" role="button" tabindex="0"><span id="follow-author-standard_article_details-dmcyOmF1dGhvclByb2ZpbGU6MTE2"><span><span><svg width="9" height="9" viewBox="0 0 9 9" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Follow"><path d="M5 0H4V4H0V5H4V9H5V5H9V4H5V0Z"></path></svg></span></span><span>Emma Roth</span></span></span></p> <p><span>is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.</span></p></div></div><div id="zephr-anchor"><p>Just one day after <a href="https://www.theverge.com/news/696584/iceblock-tracking-app-white-house-criticism">Apple took down the iOS App Store listing for ICEBlock</a>, Google has confirmed to <a href="https://www.404media.co/google-calls-ice-agents-a-vulnerable-group-removes-ice-spotting-app-red-dot/"><em>404 Media</em></a> that it has removed a similar app, Red Dot, from the Google Play Store. The company also reportedly said it “removed apps that share the location of what it describes as a vulnerable group after a recent violent act against them connected to this sort of app.”</p><p>On Thursday, Apple removed ICEBlock and similar apps, including Red Dot, after facing pressure from the Department of Justice. Attorney General Pam Bondi <a href="https://www.foxbusiness.com/politics/apple-takes-down-ice-tracking-app-after-pressure-from-ag-bondi">said to Fox News</a> on Thursday that “ICEBlock is designed to put ICE agents at risk just for doing their jobs, and violence against law enforcement is an intolerable red line that cannot be crossed.” In response to the move, ICEBlock developer <a href="https://www.404media.co/iceblock-owner-after-apple-removes-app-we-are-determined-to-fight-this/">Joshua Aaron said in a statement to <em>404 Media</em></a> that the app is “protected speech,” adding that Apple is “capitulating to an authoritarian regime.”</p><p>Both ICEBlock and Red Dot allow users to anonymously report sightings of ICE agents and view nearby reports. <a href="https://www.red-dot.app/">Red Dot’s website says</a> the app combines user reporting with “verified reports from multiple trusted sources” to monitor ICE activity.</p><p>Google told <em>404 Media</em> that it didn’t receive any warning from the DOJ, but that it “bans apps with a high risk of abuse” and has a requirement for content moderation apps with user-generated content.“ICEBlock was never available on Google Play, but we removed similar apps for violations of our policies,” Google told <em>404 Media</em>.<em> The Verge</em> reached out to Google with a request for comment but didn’t immediately hear back.</p><div><p><span><strong>Follow topics and authors</strong> from this story to see more like this in your personalized homepage feed and to receive email updates.</span></p><ul><li id="follow-author-article_footer-dmcyOmF1dGhvclByb2ZpbGU6MTE2"><span aria-expanded="false" aria-haspopup="true" role="button" tabindex="0"><span><span><svg width="9" height="9" viewBox="0 0 9 9" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Follow"><path d="M5 0H4V4H0V5H4V9H5V5H9V4H5V0Z"></path></svg></span><span>Emma Roth</span></span></span></li><li></li><li></li><li></li><li></li><li></li><li></li><li></li></ul></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists are discovering a powerful new way to prevent cancer (119 pts)]]></title>
            <link>https://www.economist.com/science-and-technology/2025/09/02/scientists-are-discovering-a-powerful-new-way-to-prevent-cancer</link>
            <guid>45472614</guid>
            <pubDate>Sat, 04 Oct 2025 11:44:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/science-and-technology/2025/09/02/scientists-are-discovering-a-powerful-new-way-to-prevent-cancer">https://www.economist.com/science-and-technology/2025/09/02/scientists-are-discovering-a-powerful-new-way-to-prevent-cancer</a>, See on <a href="https://news.ycombinator.com/item?id=45472614">Hacker News</a></p>
Couldn't get https://www.economist.com/science-and-technology/2025/09/02/scientists-are-discovering-a-powerful-new-way-to-prevent-cancer: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Paged Out Issue #7 [pdf] (139 pts)]]></title>
            <link>https://pagedout.institute/download/PagedOut_007.pdf</link>
            <guid>45472319</guid>
            <pubDate>Sat, 04 Oct 2025 10:38:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pagedout.institute/download/PagedOut_007.pdf">https://pagedout.institute/download/PagedOut_007.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=45472319">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Alibaba cloud FPGA: the $200 Kintex UltraScale+ (183 pts)]]></title>
            <link>https://essenceia.github.io/projects/alibaba_cloud_fpga/</link>
            <guid>45471136</guid>
            <pubDate>Sat, 04 Oct 2025 06:49:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://essenceia.github.io/projects/alibaba_cloud_fpga/">https://essenceia.github.io/projects/alibaba_cloud_fpga/</a>, See on <a href="https://news.ycombinator.com/item?id=45471136">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Introduction<span><a href="#introduction" aria-label="Anchor">#</a></span></h2><p>I was recently in the market for a new FPGA to start building my upcoming projects on.</p><p>Due to the scale of my upcoming projects a Xilinx series 7 UltraScale+ FPGA of the Virtex family would be perfect,
but a Kintex series FPGA will be sufficient for early prototyping.
Due to not wanting to part ways with the eye watering amounts of money that is
required for an Vivado enterprise edition license
my choice was effectively narrowed to the FPGA chips available under the WebPack version of Vivado.</p><figure><img src="https://essenceia.github.io/projects/alibaba_cloud_fpga/xilinx_doc.png" alt="Xilinx supported boards per vivado edition"><figcaption>Xilinx supported boards per Vivado edition</figcaption></figure><p>Unsurprisingly Xilinx are well aware of how top of the range the Virtex series are,
and doesn’t offer any Virtex UltraScale+ chips with the webpack license.
That said, they do offer support for two very respectable Kintex UltraScale+ FPGA models, the <code>XCKU3P</code> and the <code>XCKU5P</code>.</p><figure><img src="https://essenceia.github.io/projects/alibaba_cloud_fpga/fpga_kintex.png" alt="Xiling product guide, overview for the Kintex UltraScale+ series"><figcaption>Xiling product guide, overview for the Kintex UltraScale+ series</figcaption></figure><p>These two chips are far from being small hobbyist toys, with the smaller <code>XCUK3P</code> already boasting +162K LUTs and
16 GTY transceivers, capable, depending on the physical constraints imposed by the chip packaging of
operating at up to 32.75Gb/s.</p><p>Now that the chip selection has been narrowed down I set out to look for a dev board.</p><p>My requirements for the board where that it featured :</p><ul><li>at least 2 SFP+ or 1 QSFP connector</li><li>a JTAG interface</li><li>a PCIe interface at least x8 wide</li></ul><p>As to where to get the board from, my options where :</p><ol><li>Design the board myself</li><li>Get the AXKU5 or AXKU3 from Alinx</li><li>See what I could unearth on the second hand market</li></ol><p>Although option <code>1</code> could have been very interesting, designing a
dev board with both a high speed PCIe and ethernet interface was not the goal of
today’s project.</p><p>As for option <code>2</code>,
Alinx is newer vendor that is still building up its credibility in the west,
their technical documentation is a bit sparse, but the feedback seems to be positive with no major issues being reported.
Most importantly, Alinx provided very fairly priced development boards
in the 900 to 1050 dollar range ( +150$ for the HPC FMC SFP+ extension board ).
Although these are not cheap by any metric, compared to the competitions
price point, they are the best value.</p><p>Option <code>2</code> was coming up ahead until I stumbled upon this ebay listing :</p><figure><img src="https://essenceia.github.io/projects/alibaba_cloud_fpga/ebay.png" alt="Ebay listing for a decommissioned Alibaba Cloud accelerator FPGA"><figcaption>Ebay listing for a decommissioned Alibaba Cloud accelerator FPGA</figcaption></figure><p>For 200$ this board featured a <code>XCKU3P-FFVB676</code>, 2 SPF+ connector and a x8 PCIe interface.
On the flip side it came with no documentation whatsoever, no guaranty it worked, and the
faint promise in the listing that there was a JTAG interface.
A sane person would likely have dismissed this as an interesting internet oddity, a remanence
of what happens when a generation of accelerator cards gets phased out in favor of the next,
or maybe just an expensive paperweight.</p><p>But I like a challenge, and the appeal of unlocking the 200$ Kintex UltraScale+ development board
was too great to ignore.</p><p>As such, I aim for this article to become the documentation paving the way to though this mirage.</p><h2>The debugger challenge<span><a href="#the-debugger-challenge" aria-label="Anchor">#</a></span></h2><p>Xilinx’s UG908 Programming and Debugging User Guide (Appendix D) specifies their blessed JTAG probe ecosystem for FPGA configuration and debug. Rather than dropping $100+ on yet another proprietary dongle that’ll collect dust after the project ends, I’m exploring alternatives.
The obvious tradeoff: abandoning Xilinx’s toolchain means losing ILA integration. However, the ILA fundamentally just captures samples and streams them via JTAG USER registers, there’s nothing preventing us from building our own logic analyzer with equivalent functionality and a custom host interface.</p><p>Enter OpenOCD. While primarily targeting ARM/RISC-V SoCs, it maintains an impressive database of supported probe hardware and provides granular control over JTAG operations. More importantly, it natively supports SVF (Serial Vector Format), a vendor-neutral bitstream format that Vivado can export.</p><p>The documentation landscape is admittedly sparse for anything beyond 7-series FPGAs, and the most recent OpenOCD documentation I could unearth was focused on Zynq ARM core debugging rather than fabric configuration. But the fundamentals remain sound: JTAG is JTAG, SVF is standardized, and the boundary scan architecture hasn’t fundamentally changed.</p><p>The approach should be straightforward: generate SVF from Vivado, feed it through OpenOCD with a commodity JTAG adapter, and validate the configuration. Worst case, we’ll need to patch some adapter-specific quirks or boundary scan chain register addresses.
Time to find out if this theory holds up in practice.</p><h2>The plan<span><a href="#the-plan" aria-label="Anchor">#</a></span></h2><p>So, to resume, the current plan is to buy a second hand hardware accelerator of eBay at a too good to be true price, and try to configure it
with an unofficial probe using open source software without any clear official support.<br>The answer to the obvious question you are thinking if you, like me, have been around the block a few times is: many things.</p><p>As such, we need a plan for approaching this.
The goal of this plan is to outline incremental steps that will build upon themselves with the end goal of being able to use this as a dev board.</p><h3>1 - Confirming the board works<span><a href="#1---confirming-the-board-works" aria-label="Anchor">#</a></span></h3><p>First order of business will be to confirm the board is showing signs of working as intended.</p><p>There is a high probability that the flash wasn’t wiped before this board was sold off, as such the previous bitstream should
still be in the flash.
Given this board was used as an accelerator, we should be able to use that to confirm the board is working by either checking if
the board is presenting itself as a PCIe endpoint or if the SFP’s are sending the ethernet PHY idle sequence.</p><h3>2 - Connecting a debugger to it<span><a href="#2---connecting-a-debugger-to-it" aria-label="Anchor">#</a></span></h3><p>The next step is going to be to try and connect the debugger.
The eBay listing advertised there is a JTAG interface, but the picture is grainy enough that where that JTAG is and what pins are
available is unclear.</p><p>Additionally, we have no indication of what devices are daisy chained together onto the JTAG scan chain.
This is an essential question for flashing over JTAG, so it will need to be figured out.</p><p>At this point, it would also be strategic to try and do some more probing into the FPGA via JTAG.
Xilinx FPGAs
exposes a handful of useful system registers accessible over JTAG. The most well known of these interfaces is the
SYSMON, which allows us, among other things, to get real time temperature and voltage reading from inside the chip.
Although openOCD doesn’t have SYSMON support out of the box it would be worth while to build it, to :</p><ol><li>Familiarise myself with openOCD scripting, this might come in handy when building my ILA replacement down the line</li><li>Having an easy side channel to monitor FPGA operating parameters</li><li>Make a contribution to openOCD as it have support for the interfacing with XADC but not SYSMON</li></ol><h3>3 - Figuring out the Pinout<span><a href="#3---figuring-out-the-pinout" aria-label="Anchor">#</a></span></h3><p>The hardest part will be figuring out the FPGA’s pinout and my clock sources.
The questions that need answering are :</p><ul><li>what external clocks sources do I have, what are there frequencies and which pins are they connected to</li><li>which transceivers are the SFPs connected to</li><li>which transceivers is the PCIe connected to</li></ul><h3>4 - Writing a bitstream<span><a href="#4---writing-a-bitstream" aria-label="Anchor">#</a></span></h3><p>For now I will be focusing on writing a temporary configurations over JTAG to the CCLs and not re-writing the flash.</p><p>That plan is to trying writing either the bitstream directly though openOCD’s <code>virtex2</code> + <code>pld</code> drivers, or by replaying the
SVF generated by Vivado.</p><p>Since I believe a low iteration time is paramount to project velocity and getting big things done, I also want automatize
all of the Vivado flow from taking the rtl to the SVF generation.</p><p>Simple enough ?</p><h2>Liveness test<span><a href="#liveness-test" aria-label="Anchor">#</a></span></h2><p>A few days later my prize arrived via express mail.</p><figure><img src="https://essenceia.github.io/projects/alibaba_cloud_fpga/fpga.jpg" alt="fpga"><figcaption>My prized Kintex UltraScale+ FPGA board also known as the decommissioned Alibaba cloud accelerator. Jammed transceiver now safely removed.</figcaption></figure><p>Unexpectedly it even came with a free 25G SFP28 Huawei transceiver rated for a 300m distance and a single 1m long OS2 fiber patch cable.
This was likely not intentional as the transceiver was jammed in the SFP cage, but it was still very generous of them to include the fiber patch cable.</p><figure><img src="https://essenceia.github.io/projects/alibaba_cloud_fpga/free_stuff.jpg" alt="Additional SFP28-25G-1310nm-300m-SM Huawei transceiver, and 1m long OS2 patch cable"><figcaption>Free additional SFP28-25G-1310nm-300m-SM Huawei transceiver, and 1m long OS2 patch cable</figcaption></figure><p>The board also came with a travel case and half of a PCIe to USB adapter and a 12V power supply that one could use to power the board as a standalone device. Although this standalone configuration will not be of any use to me, for those looking to develop just networking interfaces without any PCIe interface, this could come in handy.</p><p>Overall the board looked a little worn, but both the transceiver cages and PCIe connectors didn’t look to be damaged.</p><h3>Standalone configuration<span><a href="#standalone-configuration" aria-label="Anchor">#</a></span></h3><p>Before real testing could start I first did a small power-up test using the PCIe to USB adapter that the seller provided.
I was able to do a quick check using the LEDs and the FPGAs dissipated heat that the board seemed to be powering up at a surface level (pun intended).</p><h3>PCIe interface<span><a href="#pcie-interface" aria-label="Anchor">#</a></span></h3><p><span><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M506.3 417 293 53c-16.33-28-57.54-28-73.98.0l-213.2 364C-10.59 444.9 9.849 480 42.74 480h426.6c32.76.0 53.26-35 36.96-63zM232 168c0-13.25 10.75-24 24-24s24 10.8 24 24v128c0 13.25-10.75 24-23.1 24S232 309.3 232 296V168zm24 248c-17.36.0-31.44-14.08-31.44-31.44s14.07-31.44 31.44-31.44 31.44 14.08 31.44 31.44C287.4 401.9 273.4 416 256 416z"></path></svg>
</span></span><span>As a reminder, this next section relies on the flash not having been wiped and still containing the previous user’s design.</span></p><p>Since I didn’t want to directly plug mystery hardware into my prized build server, I decided to use a Raspberry Pi 5 as
my sacrificial test device and got myself an external PCIe adapter.</p><p>It just so happened that the latest Raspberry Pi version, the Pi 5, now features an external PCIe Gen 2.0 x1 interface.
Though our FPGA can handle up to a PCIe Gen 3.0 and the board had a x8 wide interface,
since PCIe standard is backwards compatible and the number of lanes on the interface can be downgraded,
plugging our FPGA with this Raspberry Pi will work.</p><figure><img src="https://essenceia.github.io/projects/alibaba_cloud_fpga/pi.jpg" alt="FPGA board connected to the Raspberry Pi 5 via the PCIe to PCIe x1 adapter"><figcaption>FPGA board connected to the Raspberry Pi 5 via the PCIe to PCIe x1 adapter</figcaption></figure><p>After both the Raspberry and the FPGA were booted, I SSHed into my rpi and
started looking for the PCIe enumeration sequence logged from the Linux
PCIe core subsystem.</p><p><code>dmesg</code> log :</p><pre tabindex="0"><code>[    0.388790] pci 0000:00:00.0: [14e4:2712] type 01 class 0x060400
[    0.388817] pci 0000:00:00.0: PME# supported from D0 D3hot
[    0.389752] pci 0000:00:00.0: bridge configuration invalid ([bus 00-00]), reconfiguring
[    0.495733] brcm-pcie 1000110000.pcie: link up, 5.0 GT/s PCIe x1 (!SSC)
[    0.495759] pci 0000:01:00.0: [dabc:1017] type 00 class 0x020000
</code></pre><h4>Background information<span><a href="#background-information" aria-label="Anchor">#</a></span></h4><p>Since most people might not be intimately as familiar with PCIe terminology, allow me to
quickly document what is going on here.</p><p><code>0000:00:00.0</code>: is the identifier of a specific PCIe device connected through the PCIe network
to the kernel, it read as <code>domain</code>:<code>bus</code>:<code>device</code>.<code>function</code>.</p><p><code>[14e4:2712]</code>: is the device’s <code>[vendor id:device id]</code>, these vendor id identifiers are
assigned by the PCI standard body to hardware vendors. Vendors are then free to define there
own vendor id’s.</p><p>The full list of official vendor id’s and released device id can be found : <a href="https://admin.pci-ids.ucw.cz/read/PC/14e4" target="_blank">https://admin.pci-ids.ucw.cz/read/PC/14e4</a> or in
the linux kernel code : <a href="https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L160-L3256" target="_blank">https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L160-L3256</a></p><p><code>type 01</code>: PCIe has two types of devices, bridges allowing the connection of multiple downstream devices to an
upstream device, and endpoints are the leafs.
Bridges are of type <code>01</code> and endpoints of type <code>00</code>.</p><p><code>class 0x60400</code>: is the PCIe device class, it categorizes the kind of function the device performs. It
uses the following format <code>0x[Base Class (8 bits)][Sub Class (8 bits)][Programming Interface (8 bits)]</code>,
( note : the sub class field might be unused ).</p><p>A list of class and sub class identifiers can be found: <a href="https://admin.pci-ids.ucw.cz/read/PD" target="_blank">https://admin.pci-ids.ucw.cz/read/PD</a> or again in the linux codebase : <a href="https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L15-L158" target="_blank">https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L15-L158</a></p><h4>Dmesg log<span><a href="#dmesg-log" aria-label="Anchor">#</a></span></h4><p>The two most interesting lines of the <code>dmesg</code> log are :</p><pre tabindex="0"><code>[    0.388790] pci 0000:00:00.0: [14e4:2712] type 01 class 0x060400
[    0.495759] pci 0000:01:00.0: [dabc:1017] type 00 class 0x020000
</code></pre><p>Firstly the PCIe subsystem logs that at <code>0000:00:00.0</code> it has discovered a Broadcom BCM2712 PCIe Bridge ( vendor id <code>14e4</code>, device id <code>0x2712</code> ).This bridge (type <code>01</code>) class <code>0x0604xx</code> tells us it is a PCI-to-PCI bridge, meaning it is essentially creating additional PCIe lanes downstream for endpoint devices or additional bridges.</p><p>The subsystem then discovers a second device at <code>0000:01:00.0</code>, this is an endpoint (type <code>00</code>), and class <code>0x02000</code> tells us it is an ethernet networking equipment.<br>Of note <code>dabc</code> doesn’t correspond to a known vendor id.
When designing a PCIe interface in hardware these
are parameters we can configured. Additionally, among the different ways Linux uses to identify which driver to load for a PCIe device
the vendor id and device id can be used for matching. Supposing we are implementing custom logic, in order to prevent any bug where the wrong driver
might be loaded, it is best to use a separate vendor id.
This also helps identify your custom accelerator at a glance and use it to load your custom driver.</p><p>As such, it is not surprising to see an unknown vendor id appear for
an FPGA, this with the class as an ethernet networking device is a strong hint this is our board.</p><h4>Full PCIe device status<span><a href="#full-pcie-device-status" aria-label="Anchor">#</a></span></h4><p>Dmesg logs have already given us a good indication that our FPGA board and its PCIe interface was working but to confirm with certainty that the device with vendor id <code>dabc</code> is our FPGA we now turn to <code>lspci</code>.
<code>lspci -vvv</code> is the most verbose output and gives us a full overview of the detected PCIe devices capabilities and current configurations.</p><p>Broadcom bridge:</p><pre tabindex="0"><code>0000:00:00.0 PCI bridge: Broadcom Inc. and subsidiaries BCM2712 PCIe Bridge (rev 21) (prog-if 00 [Normal decode])
        Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx-
        Latency: 0
        Interrupt: pin A routed to IRQ 38
        Bus: primary=00, secondary=01, subordinate=01, sec-latency=0
        Memory behind bridge: [disabled] [32-bit]
        Prefetchable memory behind bridge: 1800000000-182fffffff [size=768M] [32-bit]
        Secondary status: 66MHz- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &lt;SERR- &lt;PERR-
        BridgeCtl: Parity- SERR- NoISA- VGA- VGA16- MAbort- &gt;Reset- FastB2B-
                PriDiscTmr- SecDiscTmr- DiscTmrStat- DiscTmrSERREn-
        Capabilities: [48] Power Management version 3
                Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0+,D1-,D2-,D3hot+,D3cold-)
                Status: D0 NoSoftRst+ PME-Enable- DSel=0 DScale=1 PME-
        Capabilities: [ac] Express (v2) Root Port (Slot-), MSI 00
                DevCap: MaxPayload 512 bytes, PhantFunc 0
                        ExtTag- RBE+
                DevCtl: CorrErr- NonFatalErr- FatalErr- UnsupReq-
                        RlxdOrd+ ExtTag- PhantFunc- AuxPwr+ NoSnoop+
                        MaxPayload 512 bytes, MaxReadReq 512 bytes
                DevSta: CorrErr- NonFatalErr- FatalErr- UnsupReq- AuxPwr- TransPend-
                LnkCap: Port #0, Speed 5GT/s, Width x1, ASPM L0s L1, Exit Latency L0s &lt;2us, L1 &lt;4us
                        ClockPM+ Surprise- LLActRep- BwNot+ ASPMOptComp+
                LnkCtl: ASPM Disabled; RCB 64 bytes, Disabled- CommClk-
                        ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-
                LnkSta: Speed 5GT/s, Width x1
                        TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt+
                RootCap: CRSVisible+
                RootCtl: ErrCorrectable- ErrNon-Fatal- ErrFatal- PMEIntEna+ CRSVisible+
                RootSta: PME ReqID 0000, PMEStatus- PMEPending-
                DevCap2: Completion Timeout: Range ABCD, TimeoutDis+ NROPrPrP- LTR+
                         10BitTagComp- 10BitTagReq- OBFF Via WAKE#, ExtFmt- EETLPPrefix-
                         EmergencyPowerReduction Not Supported, EmergencyPowerReductionInit-
                         FRS- LN System CLS Not Supported, TPHComp- ExtTPHComp- ARIFwd+
                         AtomicOpsCap: Routing- 32bit- 64bit- 128bitCAS-
                DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis- LTR- 10BitTagReq- OBFF Disabled, ARIFwd-
                         AtomicOpsCtl: ReqEn- EgressBlck-
                LnkCap2: Supported Link Speeds: 2.5-5GT/s, Crosslink- Retimer- 2Retimers- DRS+
                LnkCtl2: Target Link Speed: 5GT/s, EnterCompliance- SpeedDis-
                         Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS-
                         Compliance Preset/De-emphasis: -6dB de-emphasis, 0dB preshoot
                LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete- EqualizationPhase1-
                         EqualizationPhase2- EqualizationPhase3- LinkEqualizationRequest-
                         Retimer- 2Retimers- CrosslinkRes: unsupported, DRS-
                         DownstreamComp: Link Up - Present
        Capabilities: [100 v1] Advanced Error Reporting
                UESta:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
                UEMsk:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
                UESvrt: DLP+ SDES+ TLP- FCP+ CmpltTO- CmpltAbrt- UnxCmplt- RxOF+ MalfTLP+ ECRC- UnsupReq- ACSViol-
                CESta:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr-
                CEMsk:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr+
                AERCap: First Error Pointer: 00, ECRCGenCap+ ECRCGenEn- ECRCChkCap+ ECRCChkEn-
                        MultHdrRecCap- MultHdrRecEn- TLPPfxPres- HdrLogCap-
                HeaderLog: 00000000 00000000 00000000 00000000
                RootCmd: CERptEn+ NFERptEn+ FERptEn+
                RootSta: CERcvd- MultCERcvd- UERcvd- MultUERcvd-
                         FirstFatal- NonFatalMsg- FatalMsg- IntMsg 0
                ErrorSrc: ERR_COR: 0000 ERR_FATAL/NONFATAL: 0000
        Capabilities: [160 v1] Virtual Channel
                Caps:   LPEVC=0 RefClk=100ns PATEntryBits=1
                Arb:    Fixed- WRR32- WRR64- WRR128-
                Ctrl:   ArbSelect=Fixed
                Status: InProgress-
                VC0:    Caps:   PATOffset=00 MaxTimeSlots=1 RejSnoopTrans-
                        Arb:    Fixed- WRR32- WRR64- WRR128- TWRR128- WRR256-
                        Ctrl:   Enable+ ID=0 ArbSelect=Fixed TC/VC=ff
                        Status: NegoPending- InProgress-
        Capabilities: [180 v1] Vendor Specific Information: ID=0000 Rev=0 Len=028 &lt;?&gt;
        Capabilities: [240 v1] L1 PM Substates
                L1SubCap: PCI-PM_L1.2+ PCI-PM_L1.1+ ASPM_L1.2+ ASPM_L1.1+ L1_PM_Substates+
                          PortCommonModeRestoreTime=8us PortTPowerOnTime=10us
                L1SubCtl1: PCI-PM_L1.2- PCI-PM_L1.1- ASPM_L1.2- ASPM_L1.1-
                           T_CommonMode=1us LTR1.2_Threshold=0ns
                L1SubCtl2: T_PwrOn=10us
        Capabilities: [300 v1] Secondary PCI Express
                LnkCtl3: LnkEquIntrruptEn- PerformEqu-
                LaneErrStat: 0
        Kernel driver in use: pcieport
</code></pre><p>FPGA board:</p><pre tabindex="0"><code>0000:01:00.0 Ethernet controller: Device dabc:1017
        Subsystem: Red Hat, Inc. Device a001
        Control: I/O- Mem- BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx-
        Region 0: Memory at 1820000000 (64-bit, prefetchable) [disabled] [size=2K]
        Region 2: Memory at 1800000000 (64-bit, prefetchable) [disabled] [size=512M]
        Capabilities: [40] Power Management version 3
                Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0-,D1-,D2-,D3hot-,D3cold-)
                Status: D0 NoSoftRst+ PME-Enable- DSel=0 DScale=0 PME-
        Capabilities: [70] Express (v2) Endpoint, MSI 00
                DevCap: MaxPayload 1024 bytes, PhantFunc 0, Latency L0s &lt;64ns, L1 &lt;1us
                        ExtTag+ AttnBtn- AttnInd- PwrInd- RBE+ FLReset- SlotPowerLimit 0W
                DevCtl: CorrErr+ NonFatalErr+ FatalErr+ UnsupReq+
                        RlxdOrd+ ExtTag+ PhantFunc- AuxPwr- NoSnoop+
                        MaxPayload 512 bytes, MaxReadReq 512 bytes
                DevSta: CorrErr- NonFatalErr- FatalErr- UnsupReq- AuxPwr- TransPend-
                LnkCap: Port #0, Speed 8GT/s, Width x8, ASPM not supported
                        ClockPM- Surprise- LLActRep- BwNot- ASPMOptComp+
                LnkCtl: ASPM Disabled; RCB 64 bytes, Disabled- CommClk-
                        ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-
                LnkSta: Speed 5GT/s (downgraded), Width x1 (downgraded)
                        TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt-
                DevCap2: Completion Timeout: Range BC, TimeoutDis+ NROPrPrP- LTR-
                         10BitTagComp- 10BitTagReq- OBFF Not Supported, ExtFmt- EETLPPrefix-
                         EmergencyPowerReduction Not Supported, EmergencyPowerReductionInit-
                         FRS- TPHComp- ExtTPHComp-
                         AtomicOpsCap: 32bit- 64bit- 128bitCAS-
                DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis- LTR- 10BitTagReq- OBFF Disabled,
                         AtomicOpsCtl: ReqEn-
                LnkCap2: Supported Link Speeds: 2.5-8GT/s, Crosslink- Retimer- 2Retimers- DRS-
                LnkCtl2: Target Link Speed: 8GT/s, EnterCompliance- SpeedDis-
                         Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS-
                         Compliance Preset/De-emphasis: -6dB de-emphasis, 0dB preshoot
                LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete- EqualizationPhase1-
                         EqualizationPhase2- EqualizationPhase3- LinkEqualizationRequest-
                         Retimer- 2Retimers- CrosslinkRes: unsupported
        Capabilities: [100 v1] Advanced Error Reporting
                UESta:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
                UEMsk:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- UnxCmplt- RxOF- MalfTLP- ECRC- UnsupReq- ACSViol-
                UESvrt: DLP+ SDES+ TLP- FCP+ CmpltTO- CmpltAbrt- UnxCmplt- RxOF+ MalfTLP+ ECRC- UnsupReq- ACSViol-
                CESta:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr-
                CEMsk:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- AdvNonFatalErr+
                AERCap: First Error Pointer: 00, ECRCGenCap- ECRCGenEn- ECRCChkCap- ECRCChkEn-
                        MultHdrRecCap- MultHdrRecEn- TLPPfxPres- HdrLogCap-
                HeaderLog: 00000000 00000000 00000000 00000000
        Capabilities: [1c0 v1] Secondary PCI Express
                LnkCtl3: LnkEquIntrruptEn- PerformEqu-
                LaneErrStat: 0
</code></pre><p>For our board, the following lines are particularly interesting:</p><pre tabindex="0"><code>                LnkCap: Port #0, Speed 8GT/s, Width x8, ASPM not supported
                        ClockPM- Surprise- LLActRep- BwNot- ASPMOptComp+
                LnkCtl: ASPM Disabled; RCB 64 bytes, Disabled- CommClk-
                        ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-
                LnkSta: Speed 5GT/s (downgraded), Width x1 (downgraded)0x060400
</code></pre><p>The <code>LnkCap</code> tells us about the full capabilities of this PCIe device, here we can see that
the current design supports PCIe Gen 3.0 x8.
The <code>LnkSta</code> tells us the current configuration, here we have been downgraded to PCIe Gen 2.0 at 5GT/s with a width of only x1.</p><p>During startup of when a new PCIe device is plugged, PCIe performs a link speed and width negotiation
where it tries to reach the highest supported stable configuration for the current system.
In our current system, though our FPGA is capable of 8GT/s, as it is located downstream of the
Broadcom bridge with a maximum link capacity of Gen 2.0 ( 5GT/s ), the FPGA has been downgraded to 5GT/s.</p><p>As for the width of x1, that is expected since the Broadcom bridge is also only x1 wide, and our board’s other
7 PCIe lanes are literally hanging over the side.</p><figure><img src="https://essenceia.github.io/projects/alibaba_cloud_fpga/pcie_air.jpg" alt="7 PCIe lanes left unconnected and hanging over the air"><figcaption>7 PCIe lanes left unconnected and hanging over the air</figcaption></figure><p>Thus, we can finally confirm that this is our board and that the PCIe interface is working.
We can now proceed to establishing the JTAG connection.</p><h2>JTAG interface<span><a href="#jtag-interface" aria-label="Anchor">#</a></span></h2><p>Xilinx FPGAs can be configured by writing a bitstream to their internal CMOS Configuration Latches (CCL).
CCL is SRAM memory and volatile, thus the configuration is re-done on every power cycle.
For devices in the field this bitstream would be read from an external SPI memory during initialization,
or written from an external device, such as an embedded controller. But for development purposes overwriting the contents of the CCLs over JTAG is acceptable.</p><p>This configuration is done by shifting in the entire FPGA bitstream into the device’s configuration logic over the JTAG bus.</p><h3>FPGA board JTAG interface<span><a href="#fpga-board-jtag-interface" aria-label="Anchor">#</a></span></h3><p>As promised by the original eBay listing the board did come with an accessible JTAG interface, and gloriously enough, this time there
wasn’t even the need for any additional soldering.</p><figure><img src="https://essenceia.github.io/projects/alibaba_cloud_fpga/pcb_jtag.jpg" alt="View of the JTAG interface on the PCB"><figcaption>View of the JTAG interface on the PCB</figcaption></figure><p>In addition to a power reference, and ground, conformely to the Xilinx JTAG interface it featured the four mandatory signals comprising the JTAG TAP :</p><ul><li><strong>TCK</strong> Test Clock</li><li><strong>TMS</strong> Test Mode Select</li><li><strong>TDI</strong> Test Data Input</li><li><strong>TDO</strong> Test Data Output</li></ul><p>Of note, the JTAG interface can also come with an independent reset signal.
But since Xilinx JTAG interfaces do not have this independent reset signal, we be using the JTAG FSM reset state
for our reset signal.</p><figure><img src="https://essenceia.github.io/projects/alibaba_cloud_fpga/board_jtag_intf.svg" alt="very nice documentation of the board JTAG pinout"><figcaption>6 pin board JTAG interface</figcaption></figure><p>This interface layout doesn’t follow a standard layout so I cannot just plug in one of my debug probes, it requires some re-wiring.</p><h3>Segger JLINK :heart:<span><a href="#segger-jlink-heart" aria-label="Anchor">#</a></span></h3><p>I do not own an AMD approved JTAG programmer.</p><p>Traditionally speaking, the Segger JLink is used for debugging embedded CPUs let them be standalone or in a
Zynq, and not for configuring FPGAs.</p><p>That said, all we need to do is use JTAG to shift in a bitstream to the CCLs, so
technically speaking any programmable device with 4 sufficiently fast GPIOs can be used as a JTAG programmer.
Additionally, the JLink is well supported by OpenOCD, the JLink’s libraries are open source, and I happened to own one.</p><p><span><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 0C114.6.0.0 114.6.0 256s114.6 256 256 256 256-114.6 256-256S397.4.0 256 0zm0 128c17.67.0 32 14.33 32 32 0 17.67-14.33 32-32 32s-32-14.3-32-32 14.3-32 32-32zm40 256h-80c-13.2.0-24-10.7-24-24s10.75-24 24-24h16v-64h-8c-13.25.0-24-10.75-24-24s10.8-24 24-24h32c13.25.0 24 10.75 24 24v88h16c13.25.0 24 10.75 24 24s-10.7 24-24 24z"></path></svg>
</span></span><span>Note : I could also have used a USB Blaster, which considering it is literally an Altera tool would have made it hilarious.</span></p><figure><img src="https://essenceia.github.io/projects/alibaba_cloud_fpga/segger_jlink_conn.svg" alt="very nice 20 pin segger JLink pinout interface documentation"><figcaption>20 pin segger JLink pinout</figcaption></figure><h4>Wiring<span><a href="#wiring" aria-label="Anchor">#</a></span></h4><p>Rewiring :</p><figure><img src="https://essenceia.github.io/projects/alibaba_cloud_fpga/jtag_wiring.svg" alt="very nice JTAG wiring diagram to connect JLink jtag probe to fpga board"><figcaption>Wiring diagram to connect JLink JTAG probe to the board.</figcaption></figure><p>JTAG is a parallel protocol where <code>TDI</code> and <code>TMS</code> will be captured according to <code>TCK</code>.
Because of this, good JTAG PCB trace length matching is advised in order to minimize skew.</p><figure><img src="https://essenceia.github.io/projects/alibaba_cloud_fpga/jtag_timing.png" alt="Timing Waveform for JTAG Signals (From Target Device Perspective)"><figcaption>Timing Waveform for JTAG Signals (From Target Device Perspective); source : <a href="https://www.intel.com/content/www/us/en/docs/programmable/683719/current/jtag-timing-constraints-and-waveforms.html" target="_blank">https://www.intel.com/content/www/us/en/docs/programmable/683719/current/jtag-timing-constraints-and-waveforms.html</a></figcaption></figure><p>Ideally a custom connector with length matched traces to work as an interface between the JLink’s
probe and a board specific connector would be used.</p><figure><img src="https://essenceia.github.io/projects/alibaba_cloud_fpga/con.jpg" alt="Far from length matched JTAG connections"><figcaption>Far from length matched JTAG connections</figcaption></figure><p>Yet, here we are shoving breadboard wires between our debugger and the board.
Since OpenOCD allows us to easily control the debugger clock speed, we can increase the skew tolerance by slowing down the TCK clock signal. As such
there is no immediate need for a custom connector but we will not be able to reach the maximum JTAG speeds.</p><p><span><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M506.3 417 293 53c-16.33-28-57.54-28-73.98.0l-213.2 364C-10.59 444.9 9.849 480 42.74 480h426.6c32.76.0 53.26-35 36.96-63zM232 168c0-13.25 10.75-24 24-24s24 10.8 24 24v128c0 13.25-10.75 24-23.1 24S232 309.3 232 296V168zm24 248c-17.36.0-31.44-14.08-31.44-31.44s14.07-31.44 31.44-31.44 31.44 14.08 31.44 31.44C287.4 401.9 273.4 416 256 416z"></path></svg>
</span></span><span>If no clock speed is specified OpenOCD sets the clock speed at 100MHz.
This is too high in our case.
As such, latter in the article, I will be setting the JTAG clock down to 1MHz for probing and reset, programming will be done at 10MHz.<br>No issues were encountered at these speeds.</span></p><h3>OpenOCD<span><a href="#openocd" aria-label="Anchor">#</a></span></h3><p>OpenOCD is a free and open source on-chip debugger software that aims to be compatible with as many
probes, boards and chips as possible.</p><p>Since OpenOCD has support for the standard SVF file format, my plan for my flashing flow will be to use
Vivado to generate the SVF and have OpenOCD flash it.
Now, some of you might be starting to notice that I am diverging quite far from the well lit path of officially
supported tools. Not only am I using a not officially supported debug probe, but I am also using some
obscure open source software with questionable support for interfacing with Xilinx UltraScale+ FPGAs.
You might be wondering, given that the officially supported tools can already prove themselves to be a headache to get working properly,
why am I seemingly making my life even harder?</p><p>The reason is quite simple: when things inevitably start going wrong, as they will,
having an entirely open toolchain, allows me to have more visibility
as to what is going on and the ability to fix it.
I cannot delve into a black box.</p><h4>Building OpenOCD<span><a href="#building-openocd" aria-label="Anchor">#</a></span></h4><p>By default the version of OpenOCD that I got on my server via the official packet manager was outdated and missing features
I will need.</p><p>Also, since saving the ability to modify OpenOCD’s source code could come in handy, I decided to re-build it from source.</p><p>Thus, in the following logs, I will be running OpenOCD version <code>0.12.0+dev-02170-gfcff4b712</code>.</p><p>Note : I have also re-build the JLink libs from source.</p><h3>Determining the scan chain<span><a href="#determining-the-scan-chain" aria-label="Anchor">#</a></span></h3><p>Since I do not have the schematics for the board I do not know how many devices are daisy-chainned on the board JTAG bus.
Also, I want to confirm if the FPGA on the ebay listing is actually the one on the board.
In JTAG, each chained device exposes an accessible <code>IDCODE</code> register used to identify the manufacturer, device type, and revision number.</p><p>When setting up the JTAG server, we typically define the scan chain by specifying the expected <code>IDCODE</code> for each TAP and the corresponding instruction register length, so that instructions can be correctly aligned and routed to the intended device.
Given this is an undocumented board off Ebay, I do not know what the chain looks like.
Fortunately, OpenOCD has an autoprobing functionality, to do a blind interrogation in an <strong>attempt</strong> to discover
the available devices.</p><p>Thus, my first order of business was doing this autoprobing.</p><p>In OpenOCD the autoprobing is done when the configuration does not specify any taps.</p><div><pre tabindex="0"><code data-lang="tcl"><span><span>source <span>[</span>find interface<span>/</span>jlink.cfg<span>]</span>
</span></span><span><span>transport select jtag
</span></span><span><span>
</span></span><span><span><span>set</span> SPEED <span>1</span>
</span></span><span><span>jtag_rclk $SPEED
</span></span><span><span>adapter speed $SPEED
</span></span><span><span>
</span></span><span><span>reset_config none
</span></span></code></pre></div><p>The blind interrogation successfully discovered a single device on the chain with an <code>IDCODE</code> of <code>0x04a63093</code>.</p><pre tabindex="0"><code>gp@workhorse:~/tools/openocd_jlink_test/autoprob$ openocd
Open On-Chip Debugger 0.12.0+dev-02170-gfcff4b712 (2025-09-04-21:02)
Licensed under GNU GPL v2
For bug reports, read
	http://openocd.org/doc/doxygen/bugs.html
none separate
Info : Listening on port 6666 for tcl connections
Info : Listening on port 4444 for telnet connections
Info : J-Link V10 compiled Jan 30 2023 11:28:07
Info : Hardware version: 10.10
Info : VTarget = 1.812 V
Info : clock speed 1 kHz
Warn : There are no enabled taps.  AUTO PROBING MIGHT NOT WORK!!
Info : JTAG tap: auto0.tap tap/device found: 0x04a63093 (mfg: 0x049 (Xilinx), part: 0x4a63, ver: 0x0)
Warn : AUTO auto0.tap - use "jtag newtap auto0 tap -irlen 2 -expected-id 0x04a63093"
Error: IR capture error at bit 2, saw 0x3ffffffffffffff5 not 0x...3
Warn : Bypassing JTAG setup events due to errors
Warn : gdb services need one or more targets defined
</code></pre><p>Comparing against the <code>UltraScale Architecture Configuration User Guide (UG570)</code> we see that this <code>IDCODE</code> matches up
precisely with the expected value for the <code>KU3P</code>.</p><figure><img src="https://essenceia.github.io/projects/alibaba_cloud_fpga/idcode.png" alt="JTAG and IDCODE for UltraScale Architecture-based FPGAs"><figcaption>JTAG and IDCODE for UltraScale Architecture-based FPGAs</figcaption></figure><p>By default OpenOCD assumes a JTAG IR length of 2 bits, while our FPGA has an IR length of 6 bits.
This is the cause behind the IR capture error encountered during autoprobing. By updating the script with an IR length of 6 bits we can re-detect the FPGA with no errors.</p><div><pre tabindex="0"><code data-lang="tcl"><span><span>source <span>[</span>find interface<span>/</span>jlink.cfg<span>]</span>
</span></span><span><span>transport select jtag
</span></span><span><span>
</span></span><span><span><span>set</span> SPEED <span>1</span>
</span></span><span><span>jtag_rclk $SPEED
</span></span><span><span>adapter speed $SPEED
</span></span><span><span>
</span></span><span><span>reset_config none
</span></span><span><span>
</span></span><span><span>jtag newtap auto_detect tap <span>-</span>irlen <span>6</span>
</span></span></code></pre></div><p>Output :</p><pre tabindex="0"><code>gp@workhorse:~/tools/openocd_jlink_test/autoprob$ openocd
Open On-Chip Debugger 0.12.0+dev-02170-gfcff4b712 (2025-09-04-21:02)
Licensed under GNU GPL v2
For bug reports, read
	http://openocd.org/doc/doxygen/bugs.html
Info : Listening on port 6666 for tcl connections
Info : Listening on port 4444 for telnet connections
Info : J-Link V10 compiled Jan 30 2023 11:28:07
Info : Hardware version: 10.10
Info : VTarget = 1.812 V
Info : clock speed 1 kHz
Info : JTAG tap: auto_detect.tap tap/device found: 0x04a63093 (mfg: 0x049 (Xilinx), part: 0x4a63, ver: 0x0)
Warn : gdb services need one or more targets defined
</code></pre><p>Based on the probing, this is the JTAG scan chain for our board :</p><figure><img src="https://essenceia.github.io/projects/alibaba_cloud_fpga/scan_chain.svg" alt="JTAG scan chain for the alibaba cloud FPGA"><figcaption>JTAG scan chain for the alibaba cloud FPGA</figcaption></figure><h3>System Monitor Registers<span><a href="#system-monitor-registers" aria-label="Anchor">#</a></span></h3><p>Previous generations of Xilinx FPGA had a system called the XADC that, among other features,
allowed you to acquire chip temperature and voltage readings. The newer UltraScale and UltraScale+
family have deprecated this XADC module in favor of the SYSMON (and SYSMON4) which allows you to also
get these temperature readings, just better.</p><p>Unfortunately, openOCD didn’t have support for reading the SYSMON over JTAG out of the box, so I will be adding it.</p><p><span><span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 0C114.6.0.0 114.6.0 256s114.6 256 256 256 256-114.6 256-256S397.4.0 256 0zm0 128c17.67.0 32 14.33 32 32 0 17.67-14.33 32-32 32s-32-14.3-32-32 14.3-32 32-32zm40 256h-80c-13.2.0-24-10.7-24-24s10.75-24 24-24h16v-64h-8c-13.25.0-24-10.75-24-24s10.8-24 24-24h32c13.25.0 24 10.75 24 24v88h16c13.25.0 24 10.75 24 24s-10.7 24-24 24z"></path></svg>
</span></span><span><p>To be more precise, the Kintex UltraScale+ has a SYSMON4 and not a SYSMON.
For full context, there are 3 flavors of SYSMON:</p><ul><li><code>SYSMON1</code> used in the Kintex and Virtex UltraScale series</li><li><code>SYSMON4</code> used in the Kintex, Virtex and in the Zynq programmable logic for the UltraScale+ series</li><li><code>SYSMON</code> used in the Zynq in the processing system of the UltraScale+ series.<br>Yes, you read that correctly the Zynq of the UltraScale+ series features not one, but at least two unique SYSMON instances.</li></ul><p>For the purpose of this article, all these instances are similar enough that I will be using the terms SYSMON4 and SYSMON interchangeably.</p></span></p><p>In order for the JTAG to interact with the SYSMON, we first need to write the <code>SYSMON_DRP</code> command to the
JTAG Instruction Register (IR).
Based on the documentation, we see that this command has a value of <code>0x37</code>, which funnily enough,
is the same command code as the XADC, solidifying the SYSMON as the XADC’s descendant.</p><p>The SYSMON offers a lot more additional functionalities than just being used to read voltage and temperature,
but for today’s use case we will not be using any of that. Rather, we will focus only on reading a
subset of the SYSMON status registers.</p><p>These status registers are located at addresses <code>(00h-3Fh, 80h-BFh)</code>,
and contain the measurement results of the analog-to-digital conversions, the flag registers,
and the calibration coefficients. We can select which address we wish to read by writing the
address to the Data Register (DR) over JTAG and the data will be read out of <code>TDO</code>.</p><div><pre tabindex="0"><code data-lang="tcl"><span><span><span># SPDX-License-Identifier: GPL-2.0-or-later
</span></span></span><span><span><span>
</span></span></span><span><span><span># Xilinx SYSMON4 support
</span></span></span><span><span><span>#</span>
</span></span><span><span><span># Based on UG580, used for UltraScale+ Xilinx FPGA
</span></span></span><span><span><span># This code implements access through the JTAG TAP.
</span></span></span><span><span><span>#</span>
</span></span><span><span><span># build a 32 bit DRP command for the SYSMON DRP
</span></span></span><span><span><span></span><span>proc</span> sysmon_cmd <span>{</span>cmd addr data<span>}</span> <span>{</span>
</span></span><span><span>	<span>array</span> set cmds <span>{</span>
</span></span><span><span>		NOP <span>0x00</span>
</span></span><span><span>		READ <span>0x01</span>
</span></span><span><span>		WRITE <span>0x02</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>	<span>return</span> <span>[expr</span> <span>{(</span>$cmds<span>(</span>$cmd<span>)</span> <span>&lt;&lt;</span> 26<span>)</span> <span>|</span> <span>(</span>$addr <span>&lt;&lt;</span> 16<span>)</span> <span>|</span> <span>(</span>$data <span>&lt;&lt;</span> 0<span>)}]</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span># Status register addresses
</span></span></span><span><span><span># Some addresses (status registers 0-3) have special function when written to.
</span></span></span><span><span><span></span><span>proc</span> SYSMON <span>{</span>key<span>}</span> <span>{</span>
</span></span><span><span>	<span>array</span> set addrs <span>{</span>
</span></span><span><span>		TEMP <span>0x00</span>
</span></span><span><span>		VCCINT <span>0x01</span>
</span></span><span><span>		VCCAUX <span>0x02</span>
</span></span><span><span>		VPVN <span>0x03</span>
</span></span><span><span>		VREFP <span>0x04</span>
</span></span><span><span>		VREFN <span>0x05</span>
</span></span><span><span>		VCCBRAM <span>0x06</span>
</span></span><span><span>		SUPAOFFS <span>0x08</span>
</span></span><span><span>		ADCAOFFS <span>0x09</span>
</span></span><span><span>		ADCAGAIN <span>0x0a</span>
</span></span><span><span>		VCCPINTLP <span>0x0d</span>
</span></span><span><span>		VCCPINTFP <span>0x0e</span>
</span></span><span><span>		VCCPAUX <span>0x0f</span>
</span></span><span><span>		VAUX0 <span>0x10</span>
</span></span><span><span>		VAUX1 <span>0x11</span>
</span></span><span><span>		VAUX2 <span>0x12</span>
</span></span><span><span>		VAUX3 <span>0x13</span>
</span></span><span><span>		VAUX4 <span>0x14</span>
</span></span><span><span>		VAUX5 <span>0x15</span>
</span></span><span><span>		VAUX6 <span>0x16</span>
</span></span><span><span>		VAUX7 <span>0x17</span>
</span></span><span><span>		VAUX8 <span>0x18</span>
</span></span><span><span>		VAUX9 <span>0x19</span>
</span></span><span><span>		VAUX10 <span>0x1a</span>
</span></span><span><span>		VAUX11 <span>0x1b</span>
</span></span><span><span>		VAUX12 <span>0x1c</span>
</span></span><span><span>		VAUX13 <span>0x1d</span>
</span></span><span><span>		VAUX14 <span>0x1e</span>
</span></span><span><span>		VAUX15 <span>0x1f</span>
</span></span><span><span>		MAXTEMP <span>0x20</span>
</span></span><span><span>		MAXVCC <span>0x21</span>
</span></span><span><span>		MAXVCCAUX <span>0x22</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>	<span>return</span> $addrs<span>(</span>$key<span>)</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span># transfer
</span></span></span><span><span><span></span><span>proc</span> sysmon_xfer <span>{</span>tap cmd addr data<span>}</span> <span>{</span>
</span></span><span><span>	<span>set</span> ret <span>[</span>drscan $tap <span>32</span> <span>[</span>sysmon_cmd $cmd $addr $data<span>]]</span>
</span></span><span><span>	runtest <span>10</span>
</span></span><span><span>	<span>return</span> <span>[expr</span> <span>"0x$ret"</span><span>]</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span># sysmon register write
</span></span></span><span><span><span></span><span>proc</span> sysmon_write <span>{</span>tap addr data<span>}</span> <span>{</span>
</span></span><span><span>	sysmon_xfer $tap WRITE $addr $data
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span># sysmon register read, non-pipelined
</span></span></span><span><span><span></span><span>proc</span> sysmon_read <span>{</span>tap addr<span>}</span> <span>{</span>
</span></span><span><span>	sysmon_xfer $tap READ $addr <span>0</span>
</span></span><span><span>	<span>return</span> <span>[</span>sysmon_xfer $tap NOP <span>0</span> <span>0</span><span>]</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span># Select the sysmon DR, SYSMON_DRP has the same binary code value as the XADC
</span></span></span><span><span><span></span><span>proc</span> sysmon_select <span>{</span>tap<span>}</span> <span>{</span>
</span></span><span><span>	<span>set</span> SYSMON_IR <span>0x37</span>
</span></span><span><span>	irscan $tap $SYSMON_IR
</span></span><span><span>	runtest <span>10</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span># convert 16 bit temperature measurement to Celsius
</span></span></span><span><span><span></span><span>proc</span> sysmon_temp_internal <span>{</span>code<span>}</span> <span>{</span>
</span></span><span><span>	<span>return</span> <span>[expr</span> <span>{</span>$code <span>*</span> 509.314<span>/</span><span>(</span>1 <span>&lt;&lt;</span> <span>16</span><span>)</span> <span>-</span> <span>280.23</span><span>}]</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span># convert 16 bit supply voltage measurments to Volt
</span></span></span><span><span><span></span><span>proc</span> sysmon_sup <span>{</span>code<span>}</span> <span>{</span>
</span></span><span><span>	<span>return</span> <span>[expr</span> <span>{</span>$code <span>*</span> 3.<span>/</span><span>(</span>1 <span>&lt;&lt;</span> <span>16</span><span>)}]</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span># measure all internal voltages
</span></span></span><span><span><span></span><span>proc</span> sysmon_report <span>{</span>tap<span>}</span> <span>{</span>
</span></span><span><span>	puts <span>"Sysmon status report :"</span>
</span></span><span><span>	sysmon_select $tap
</span></span><span><span>	<span>foreach</span> ch <span>[</span>list TEMP MAXTEMP<span>]</span> <span>{</span>
</span></span><span><span>		echo <span>"$ch [format %.2f [sysmon_temp_internal [sysmon_read $tap [SYSMON $ch]]]] C"</span>
</span></span><span><span>	<span>}</span>
</span></span><span><span>	<span>foreach</span> ch <span>[</span>list VCCINT MAXVCC VCCAUX MAXVCCAUX<span>]</span> <span>{</span>
</span></span><span><span>		echo <span>"$ch [format %.3f [sysmon_sup [sysmon_read $tap [SYSMON $ch]]]] V"</span>	
</span></span><span><span>	<span>}</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>I added a report that reads the current chip temperature, internal and external
voltages as well as the maximum values for these recorded since FPGA power cycle, to my flashing script output:</p><pre tabindex="0"><code>gp@workhorse:~/tools/openocd_jlink_test$ openocd
Open On-Chip Debugger 0.12.0+dev-02170-gfcff4b712 (2025-09-04-20:02)
Licensed under GNU GPL v2
For bug reports, read
	http://openocd.org/doc/doxygen/bugs.html
set chipname XCKU3P
Read temperature sysmon 4
Info : J-Link V10 compiled Jan 30 2023 11:28:07
Info : Hardware version: 10.10
Info : VTarget = 1.819 V
Info : clock speed 1 kHz
Info : JTAG tap: XCKU3P.tap tap/device found: 0x04a63093 (mfg: 0x049 (Xilinx), part: 0x4a63, ver: 0x0)
Warn : gdb services need one or more targets defined
--------------------
Sysmon status report :
TEMP 31.12 C
MAXTEMP 34.62 C
VCCINT 0.852 V
MAXVCC 0.855 V
VCCAUX 1.805 V
MAXVCCAUX 1.807 V
</code></pre><h2>Pinout<span><a href="#pinout" aria-label="Anchor">#</a></span></h2><p>To my indescribable joy I happened to stumble onto this gold mine, in which we get the board pinout.
This most likely fell off a truck: <a href="https://blog.csdn.net/qq_37650251/article/details/145716953" target="_blank">https://blog.csdn.net/qq_37650251/article/details/145716953</a></p><p>So far this pinout looks correct.</p><table><thead><tr><th>Pin Index</th><th>Name</th><th>IO Standard</th><th>Location</th><th>Bank</th></tr></thead><tbody><tr><td>0</td><td>diff_100mhz_clk_p</td><td>LVDS</td><td>E18</td><td>BANK67</td></tr><tr><td>1</td><td>diff_100mhz_clk_n</td><td>LVDS</td><td>D18</td><td>BANK67</td></tr><tr><td>2</td><td>sfp_mgt_clk_p</td><td>LVDS</td><td>K7</td><td>BANK227</td></tr><tr><td>3</td><td>sfp_mgt_clk_n</td><td>LVDS</td><td>K6</td><td>BANK227</td></tr><tr><td>4</td><td>sfp_1_txn</td><td>-</td><td>B6</td><td>BANK227</td></tr><tr><td>5</td><td>sfp_1_txp</td><td>-</td><td>B7</td><td>BANK227</td></tr><tr><td>6</td><td>sfp_1_rxn</td><td>-</td><td>A3</td><td>BANK227</td></tr><tr><td>7</td><td>sfp_1_rxp</td><td>-</td><td>A4</td><td>BANK227</td></tr><tr><td>8</td><td>sfp_2_txn</td><td>-</td><td>D6</td><td>BANK227</td></tr><tr><td>9</td><td>sfp_2_txp</td><td>-</td><td>D7</td><td>BANK227</td></tr><tr><td>10</td><td>sfp_2_rxn</td><td>-</td><td>B1</td><td>BANK227</td></tr><tr><td>11</td><td>sfp_2_rxp</td><td>-</td><td>B2</td><td>BANK227</td></tr><tr><td>12</td><td>SFP_1_MOD_DEF_0</td><td>LVCMOS18</td><td>D14</td><td>BANK87</td></tr><tr><td>13</td><td>SFP_1_TX_FAULT</td><td>LVCMOS18</td><td>B14</td><td>BANK87</td></tr><tr><td>14</td><td>SFP_1_LOS</td><td>LVCMOS18</td><td>D13</td><td>BANK87</td></tr><tr><td>15</td><td>SFP_1_LED</td><td>LVCMOS18</td><td>B12</td><td>BANK87</td></tr><tr><td>16</td><td>SFP_2_MOD_DEF_0</td><td>LVCMOS18</td><td>E11</td><td>BANK86</td></tr><tr><td>17</td><td>SFP_2_TX_FAULT</td><td>LVCMOS18</td><td>F9</td><td>BANK86</td></tr><tr><td>18</td><td>SFP_2_LOS</td><td>LVCMOS18</td><td>E10</td><td>BANK86</td></tr><tr><td>19</td><td>SFP_2_LED</td><td>LVCMOS18</td><td>C12</td><td>BANK87</td></tr><tr><td>20</td><td>IIC_SDA_SFP_1</td><td>LVCMOS18</td><td>C14</td><td>BANK87</td></tr><tr><td>21</td><td>IIC_SCL_SFP_1</td><td>LVCMOS18</td><td>C13</td><td>BANK87</td></tr><tr><td>22</td><td>IIC_SDA_SFP_2</td><td>LVCMOS18</td><td>D11</td><td>BANK86</td></tr><tr><td>23</td><td>IIC_SCL_SFP_2</td><td>LVCMOS18</td><td>D10</td><td>BANK86</td></tr><tr><td>24</td><td>IIC_SDA_EEPROM_0</td><td>LVCMOS18</td><td>G10</td><td>BANK86</td></tr><tr><td>25</td><td>IIC_SCL_EEPROM_0</td><td>LVCMOS18</td><td>G9</td><td>BANK86</td></tr><tr><td>26</td><td>IIC_SDA_EEPROM_1</td><td>LVCMOS18</td><td>J15</td><td>BANK87</td></tr><tr><td>27</td><td>IIC_SCL_EEPROM_1</td><td>LVCMOS18</td><td>J14</td><td>BANK87</td></tr><tr><td>28</td><td>GPIO_LED_R</td><td>LVCMOS18</td><td>A13</td><td>BANK87</td></tr><tr><td>29</td><td>GPIO_LED_G</td><td>LVCMOS18</td><td>A12</td><td>BANK87</td></tr><tr><td>30</td><td>GPIO_LED_H</td><td>LVCMOS18</td><td>B9</td><td>BANK86</td></tr><tr><td>31</td><td>GPIO_LED_1</td><td>LVCMOS18</td><td>B11</td><td>BANK86</td></tr><tr><td>32</td><td>GPIO_LED_2</td><td>LVCMOS18</td><td>C11</td><td>BANK86</td></tr><tr><td>33</td><td>GPIO_LED_3</td><td>LVCMOS18</td><td>A10</td><td>BANK86</td></tr><tr><td>34</td><td>GPIO_LED_4</td><td>LVCMOS18</td><td>B10</td><td>BANK86</td></tr><tr><td>35</td><td>pcie_mgt_clkn</td><td>-</td><td>T6</td><td>BANK225</td></tr><tr><td>36</td><td>pcie_mgt_clkp</td><td>-</td><td>T7</td><td>BANK225</td></tr><tr><td>37</td><td>pcie_tx0_n</td><td>-</td><td>R4</td><td>BANK225</td></tr><tr><td>38</td><td>pcie_tx1_n</td><td>-</td><td>U4</td><td>BANK225</td></tr><tr><td>39</td><td>pcie_tx2_n</td><td>-</td><td>W4</td><td>BANK225</td></tr><tr><td>40</td><td>pcie_tx3_n</td><td>-</td><td>AA4</td><td>BANK225</td></tr><tr><td>41</td><td>pcie_tx4_n</td><td>-</td><td>AC4</td><td>BANK224</td></tr><tr><td>42</td><td>pcie_tx5_n</td><td>-</td><td>AD6</td><td>BANK224</td></tr><tr><td>43</td><td>pcie_tx6_n</td><td>-</td><td>AE8</td><td>BANK224</td></tr><tr><td>44</td><td>pcie_tx7_n</td><td>-</td><td>AF6</td><td>BANK224</td></tr><tr><td>45</td><td>pcie_rx0_n</td><td>-</td><td>P1</td><td>BANK225</td></tr><tr><td>46</td><td>pcie_rx1_n</td><td>-</td><td>T1</td><td>BANK225</td></tr><tr><td>47</td><td>pcie_rx2_n</td><td>-</td><td>V1</td><td>BANK225</td></tr><tr><td>48</td><td>pcie_rx3_n</td><td>-</td><td>Y1</td><td>BANK225</td></tr><tr><td>49</td><td>pcie_rx4_n</td><td>-</td><td>AB1</td><td>BANK224</td></tr><tr><td>50</td><td>pcie_rx5_n</td><td>-</td><td>AD1</td><td>BANK224</td></tr><tr><td>51</td><td>pcie_rx6_n</td><td>-</td><td>AE3</td><td>BANK224</td></tr><tr><td>52</td><td>pcie_rx7_n</td><td>-</td><td>AF1</td><td>BANK224</td></tr><tr><td>53</td><td>pcie_tx0_p</td><td>-</td><td>R5</td><td>BANK225</td></tr><tr><td>54</td><td>pcie_tx1_p</td><td>-</td><td>U5</td><td>BANK225</td></tr><tr><td>55</td><td>pcie_tx2_p</td><td>-</td><td>W5</td><td>BANK225</td></tr><tr><td>56</td><td>pcie_tx3_p</td><td>-</td><td>AA5</td><td>BANK225</td></tr><tr><td>57</td><td>pcie_tx4_p</td><td>-</td><td>AC5</td><td>BANK224</td></tr><tr><td>58</td><td>pcie_tx5_p</td><td>-</td><td>AD7</td><td>BANK224</td></tr><tr><td>59</td><td>pcie_tx6_p</td><td>-</td><td>AE9</td><td>BANK224</td></tr><tr><td>60</td><td>pcie_tx7_p</td><td>-</td><td>AF7</td><td>BANK224</td></tr><tr><td>61</td><td>pcie_rx0_p</td><td>-</td><td>P2</td><td>BANK225</td></tr><tr><td>62</td><td>pcie_rx1_p</td><td>-</td><td>T2</td><td>BANK225</td></tr><tr><td>63</td><td>pcie_rx2_p</td><td>-</td><td>V2</td><td>BANK225</td></tr><tr><td>64</td><td>pcie_rx3_p</td><td>-</td><td>Y2</td><td>BANK225</td></tr><tr><td>65</td><td>pcie_rx4_p</td><td>-</td><td>AB2</td><td>BANK224</td></tr><tr><td>66</td><td>pcie_rx5_p</td><td>-</td><td>AD2</td><td>BANK224</td></tr><tr><td>67</td><td>pcie_rx6_p</td><td>-</td><td>AE4</td><td>BANK224</td></tr><tr><td>68</td><td>pcie_rx7_p</td><td>-</td><td>AF2</td><td>BANK224</td></tr><tr><td>69</td><td>pcie_perstn_rst</td><td>LVCMOS18</td><td>A9</td><td>BANK86</td></tr></tbody></table><h3>Global clock<span><a href="#global-clock" aria-label="Anchor">#</a></span></h3><p>On high end FPGAs like the UltraScale+ family, high-speed global clocks are typically driven from external sources
using differential pairs for better signal integrity.</p><p>According to the pinout we have two such differential pairs.</p><p>First I must determine the nature of these external reference clocks to see how I can use them to drive my clocks.</p><p>These differential pairs are provided over the following pins:</p><ul><li>100MHz : {E18, D18}</li><li>156.25MHz : {K7, K6}</li></ul><p>Judging by the naming and the frequencies, the 156.25MHz clock is likely my SFP reference clock,
and the 100MHz can be used as my global clock.</p><p>We can confirm by querying the pin properties.</p><p><strong>K6</strong> properties :</p><div><pre tabindex="0"><code data-lang="tcl"><span><span>Vivado<span>%</span> report_property <span>[</span>get_package_pins K6<span>]</span>
</span></span><span><span>Property                Type    Read-only  Value
</span></span><span><span>BANK                    string  true       <span>227</span>
</span></span><span><span>BUFIO_2_REGION          string  true       TR
</span></span><span><span>CLASS                   string  true       package_pin
</span></span><span><span>DIFF_PAIR_PIN           string  true       K7
</span></span><span><span>IS_BONDED               bool    true       <span>1</span>
</span></span><span><span>IS_DIFFERENTIAL         bool    true       <span>1</span>
</span></span><span><span>IS_GENERAL_PURPOSE      bool    true       <span>0</span>
</span></span><span><span>IS_GLOBAL_CLK           bool    true       <span>0</span>
</span></span><span><span>IS_LOW_CAP              bool    true       <span>0</span>
</span></span><span><span>IS_MASTER               bool    true       <span>0</span>
</span></span><span><span>IS_VREF                 bool    true       <span>0</span>
</span></span><span><span>IS_VRN                  bool    true       <span>0</span>
</span></span><span><span>IS_VRP                  bool    true       <span>0</span>
</span></span><span><span>MAX_DELAY               int     true       <span>38764</span>
</span></span><span><span>MIN_DELAY               int     true       <span>38378</span>
</span></span><span><span>NAME                    string  true       K6
</span></span><span><span>PIN_FUNC                enum    true       MGTREFCLK0N_227
</span></span><span><span>PIN_FUNC_COUNT          int     true       <span>1</span>
</span></span><span><span>PKGPIN_BYTEGROUP_INDEX  int     true       <span>0</span>
</span></span><span><span>PKGPIN_NIBBLE_INDEX     int     true       <span>0</span>
</span></span></code></pre></div><p><strong>E18</strong> properties :</p><div><pre tabindex="0"><code data-lang="tcl"><span><span>Vivado<span>%</span> report_property <span>[</span>get_package_pins E18<span>]</span>
</span></span><span><span>Property                Type    Read-only  Value
</span></span><span><span>BANK                    string  true       <span>67</span>
</span></span><span><span>BUFIO_2_REGION          string  true       TL
</span></span><span><span>CLASS                   string  true       package_pin
</span></span><span><span>DIFF_PAIR_PIN           string  true       D18
</span></span><span><span>IS_BONDED               bool    true       <span>1</span>
</span></span><span><span>IS_DIFFERENTIAL         bool    true       <span>1</span>
</span></span><span><span>IS_GENERAL_PURPOSE      bool    true       <span>1</span>
</span></span><span><span>IS_GLOBAL_CLK           bool    true       <span>1</span>
</span></span><span><span>IS_LOW_CAP              bool    true       <span>0</span>
</span></span><span><span>IS_MASTER               bool    true       <span>1</span>
</span></span><span><span>IS_VREF                 bool    true       <span>0</span>
</span></span><span><span>IS_VRN                  bool    true       <span>0</span>
</span></span><span><span>IS_VRP                  bool    true       <span>0</span>
</span></span><span><span>MAX_DELAY               int     true       <span>87126</span>
</span></span><span><span>MIN_DELAY               int     true       <span>86259</span>
</span></span><span><span>NAME                    string  true       E18
</span></span><span><span>PIN_FUNC                enum    true       IO_L11P_T1U_N8_GC_67
</span></span><span><span>PIN_FUNC_COUNT          int     true       <span>2</span>
</span></span><span><span>PKGPIN_BYTEGROUP_INDEX  int     true       <span>8</span>
</span></span><span><span>PKGPIN_NIBBLE_INDEX     int     true       <span>2</span>
</span></span></code></pre></div><p>This tells us:</p><ul><li>The differential pairings are correct: {K6, K7}, {E18, D18}</li><li>We can easily use the 100MHz as a source to drive our global clocking network</li><li>The 156.25MHz clock is to be used as the reference clock for our GTY transceivers and lands on bank 227 as indicated by the <code>PIN_FUNC</code> property <code>MGTREFCLK0N_227</code></li><li>We cannot directly use the 156.25MHz clock to drive our global clock network</li></ul><p>With all this we have sufficient information to write a constraint file (<code>xdc</code>) for this board.</p><h2>Test design<span><a href="#test-design" aria-label="Anchor">#</a></span></h2><p>Further sections will be using the following design files.</p><p><code>top.v</code>:</p><div><pre tabindex="0"><code data-lang="sv"><span><span><span>module</span> top (
</span></span><span><span>    <span>input</span> <span>wire</span> Clk_100mhz_p_i, 
</span></span><span><span>    <span>input</span> <span>wire</span> Clk_100mhz_n_i,
</span></span><span><span>
</span></span><span><span>    <span>output</span> <span>wire</span> [<span>3</span><span>:</span><span>0</span>] Led_o 
</span></span><span><span>);
</span></span><span><span>    <span>wire</span>        clk_ibuf;
</span></span><span><span>    <span>reg</span>  [<span>28</span><span>:</span><span>0</span>] ctr_q; 
</span></span><span><span>    <span>reg</span>         unused_ctr_q;
</span></span><span><span>
</span></span><span><span>
</span></span><span><span>    IBUFDS #(
</span></span><span><span>        .DIFF_TERM(<span>"TRUE"</span>),
</span></span><span><span>        .IOSTANDARD(<span>"LVDS"</span>)
</span></span><span><span>    ) m_ibufds (
</span></span><span><span>        .I(Clk_100mhz_p_i),
</span></span><span><span>        .IB(Clk_100mhz_n_i),
</span></span><span><span>        .O(clk_ibuf)
</span></span><span><span>    );
</span></span><span><span>
</span></span><span><span>    BUFG m_bufg (
</span></span><span><span>        .I(clk_ibuf),
</span></span><span><span>        .O(clk)
</span></span><span><span>    );
</span></span><span><span>
</span></span><span><span>    <span>always</span> @(<span>posedge</span> clk)
</span></span><span><span>        { unused_ctr_q, ctr_q } <span>&lt;=</span> ctr_q <span>+</span> <span>29</span><span>'b1</span>;    
</span></span><span><span>    
</span></span><span><span>    <span>assign</span> Led_o <span>=</span> ctr_q[<span>28</span><span>:</span><span>25</span>];
</span></span><span><span><span>endmodule</span>
</span></span></code></pre></div><p><code>alibaba_cloud.xdc</code> :</p><div><pre tabindex="0"><code data-lang="tcl"><span><span><span># Global clock signal 
</span></span></span><span><span><span></span>set_property <span>-</span>dict <span>{</span>LOC E18 IOSTANDARD LVDS<span>}</span> <span>[</span>get_ports Clk_100mhz_p_i<span>]</span>
</span></span><span><span>set_property <span>-</span>dict <span>{</span>LOC D18 IOSTANDARD LVDS<span>}</span> <span>[</span>get_ports Clk_100mhz_n_i<span>]</span>
</span></span><span><span>create_clock <span>-</span>period <span>10</span> <span>-</span>name clk_100mhz <span>[</span>get_ports Clk_100mhz_p_i<span>]</span>
</span></span><span><span>
</span></span><span><span><span># LEDS
</span></span></span><span><span><span></span>set_property <span>-</span>dict <span>{</span>LOC B11 IOSTANDARD LVCMOS18<span>}</span> <span>[</span>get_ports <span>{</span> Led_o<span>[</span>0<span>]}]</span>
</span></span><span><span>set_property <span>-</span>dict <span>{</span>LOC C11 IOSTANDARD LVCMOS18<span>}</span> <span>[</span>get_ports <span>{</span> Led_o<span>[</span>1<span>]}]</span>
</span></span><span><span>set_property <span>-</span>dict <span>{</span>LOC A10 IOSTANDARD LVCMOS18<span>}</span> <span>[</span>get_ports <span>{</span> Led_o<span>[</span>2<span>]}]</span>
</span></span><span><span>set_property <span>-</span>dict <span>{</span>LOC B10 IOSTANDARD LVCMOS18<span>}</span> <span>[</span>get_ports <span>{</span> Led_o<span>[</span>3<span>]}]</span>
</span></span></code></pre></div><h2>Writing the bitstream<span><a href="#writing-the-bitstream" aria-label="Anchor">#</a></span></h2><p>My personal belief is that one of the most important contributors to design quality is iteration cost.
The lower your iteration cost, the higher your design quality is going to be.</p><p>As such I will invest the small upfront cost to have the workflow be as streamlined as efficiently feasible.</p><p>Thus, my workflow evolved into doing practically everything over
the command line interfaces and only interacting with the tools, Vivado in this case, through tcl scripts.</p><h3>Vivado flow<span><a href="#vivado-flow" aria-label="Anchor">#</a></span></h3><p>The goal of this flow is to, given a few verilog design and constraint files produce a SVF file. Our steps are :</p><ol><li>creat the Vivado project <code>setup.tcl</code></li><li>run the implementation <code>build.tcl</code></li><li>generate the bitstream and the SVF <code>gen.tcl</code></li></ol><p>I will be using <code>make</code> to kick off and manage the dependencies between the different steps, though I recognise this isn’t a widespread practice for hardware projects. <code>make</code> is a highly flexible, reliable and powerful tool and I believe its ability to tie together any type of workflow makes it a prime tool for this use case.</p><p>We will be invoking Vivado in batch mode, this allows us to provide a tcl script alongside script arguments, the
format is as following :</p><div><pre tabindex="0"><code data-lang="bash"><span><span>vivado -mode batch &lt;path to tcl script&gt; -tclargs &lt;script args&gt;
</span></span></code></pre></div><p>Though this allows us to easily break down our flow into incremental stages, invoking a single script in batch mode has the
drawback of restarting Vivado and needing to re-load the project or the project checkpoint on each invocation.</p><p>As the project size grows so will the project load time, so segmenting the
flow into a large number of independent scripts comes at an increasing cost.</p><p><code>Makefile</code> :</p><div><pre tabindex="0"><code data-lang="makefile"><span><span>SHELL <span>:=</span> /bin/bash
</span></span><span><span>
</span></span><span><span>VIVADO_PRJ_DIR<span>=</span>prj
</span></span><span><span>VIVADO_PRJ_NAME<span>=</span><span>$(</span>VIVADO_PRJ_DIR<span>)</span>
</span></span><span><span>VIVADO_PRJ_PATH<span>=</span><span>$(</span>VIVADO_PRJ_DIR<span>)</span>/<span>$(</span>VIVADO_PRJ_NAME<span>)</span>.xpr
</span></span><span><span>VIVADO_CHECKPOINT_PATH<span>=</span><span>$(</span>VIVADO_PRJ_DIR<span>)</span>/<span>$(</span>VIVADO_PRJ_NAME<span>)</span>_checkpoint.dcp
</span></span><span><span>
</span></span><span><span>VIVADO_CMD<span>=</span>vivado -mode batch -source
</span></span><span><span>
</span></span><span><span>SRC_PATH<span>=</span>src
</span></span><span><span>OUT_DIR<span>=</span>out
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>all</span><span>:</span> setup build gen
</span></span><span><span>
</span></span><span><span><span>$(VIVADO_PRJ_PATH)</span><span>:</span>  
</span></span><span><span>    mkdir -p <span>$(</span>VIVADO_PRJ_DIR<span>)</span>
</span></span><span><span>    <span>$(</span>VIVADO_CMD<span>)</span> setup.tcl -tclargs <span>$(</span>VIVADO_PRJ_DIR<span>)</span> <span>$(</span>VIVADO_PRJ_NAME<span>)</span>
</span></span><span><span>
</span></span><span><span><span>setup</span><span>:</span> <span>$(</span>VIVADO_PRJ_PATH<span>)</span> 
</span></span><span><span>
</span></span><span><span><span>$(VIVADO_CHECKPOINT_PATH)</span><span>:</span> <span>$(</span>VIVADO_PRJ_PATH<span>)</span> <span>$(</span>wildcard <span>$(</span>SRC_PATH<span>)</span>/*.xdc<span>)</span> <span>$(</span>wildcard <span>$(</span>SRC_PATH<span>)</span>/*.v<span>)</span>
</span></span><span><span>    <span>$(</span>VIVADO_CMD<span>)</span> build.tcl -tclargs <span>$(</span>VIVADO_PRJ_PATH<span>)</span> <span>$(</span>SRC_PATH<span>)</span> <span>$(</span>VIVADO_CHECKPOINT_PATH<span>)</span>
</span></span><span><span>
</span></span><span><span><span>build</span><span>:</span> <span>$(</span>VIVADO_CHECKPOINT_PATH<span>)</span>
</span></span><span><span>
</span></span><span><span><span>$(OUT_DIR)/$(VIVADO_PRJ_NAME).svf</span><span>:</span> <span>$(</span>VIVADO_CHECKPOINT_PATH<span>)</span> 
</span></span><span><span>    mkdir -p <span>$(</span>OUT_DIR<span>)</span>
</span></span><span><span>    <span>$(</span>VIVADO_CMD<span>)</span> gen.tcl -tclargs <span>$(</span>VIVADO_CHECKPOINT_PATH<span>)</span> <span>$(</span>OUT_DIR<span>)</span>
</span></span><span><span>
</span></span><span><span><span>gen</span><span>:</span> <span>$(</span>OUT_DIR<span>)</span>/<span>$(</span>VIVADO_PRJ_NAME<span>)</span>.svf
</span></span><span><span>
</span></span><span><span><span>flash</span><span>:</span> <span>$(</span>OUT_DIR<span>)</span>/<span>$(</span>VIVADO_PRJ_NAME<span>)</span>.svf
</span></span><span><span>    openocd	
</span></span><span><span>
</span></span><span><span><span>clean</span><span>:</span> 
</span></span><span><span>    rm -rf <span>$(</span>VIVADO_PRJ_DIR<span>)</span>
</span></span><span><span>    rm -rf <span>$(</span>OUT_DIR<span>)</span>
</span></span><span><span>    rm -f vivado*<span>{</span>log,jou<span>}</span>
</span></span><span><span>    rm -f webtalk*<span>{</span>log,jou<span>}</span>
</span></span><span><span>    rm -f usage_statistics_webtalk*<span>{</span>html,xml<span>}</span>
</span></span></code></pre></div><p><code>setup.tcl</code> :</p><div><pre tabindex="0"><code data-lang="tcl"><span><span><span>set</span> project_dir <span>[</span>lindex $argv <span>0</span><span>]</span>
</span></span><span><span><span>set</span> project_name <span>[</span>lindex $argv <span>1</span><span>]</span>
</span></span><span><span>
</span></span><span><span>puts <span>"Creating project $project_name at path [pwd]/$project_dir"</span>
</span></span><span><span>create_project <span>-</span>part xcku3p-ffvb676-2-e <span>-</span>force $project_name $project_dir
</span></span><span><span>
</span></span><span><span>close_project
</span></span><span><span>exit <span>0</span>
</span></span></code></pre></div><p><code>build.tcl</code> :</p><div><pre tabindex="0"><code data-lang="tcl"><span><span><span>set</span> project_path <span>[</span>lindex $argv <span>0</span><span>]</span>
</span></span><span><span><span>set</span> src_path <span>[</span>lindex $argv <span>1</span><span>]</span>
</span></span><span><span><span>set</span> checkpoint_path <span>[</span>lindex $argv <span>2</span><span>]</span>
</span></span><span><span>puts <span>"Implementation script called with project path $project_path and src path $src_path, generating checkpoint at $checkpoint_path"</span>
</span></span><span><span>
</span></span><span><span>open_project $project_path 
</span></span><span><span>
</span></span><span><span><span>#</span> load src
</span></span><span><span>read_verilog <span>[</span>glob <span>-</span>directory $src_path <span>*</span>.v<span>]</span>
</span></span><span><span>read_xdc <span>[</span>glob <span>-</span>directory $src_path <span>*</span>.xdc<span>]</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span># synth
</span></span></span><span><span><span></span>synth_design <span>-</span>top top
</span></span><span><span>
</span></span><span><span><span># implement
</span></span></span><span><span><span></span>opt_design
</span></span><span><span>place_design
</span></span><span><span>route_design
</span></span><span><span>phys_opt_design
</span></span><span><span>
</span></span><span><span>write_checkpoint $checkpoint_path <span>-</span>force 
</span></span><span><span>close_project
</span></span><span><span>exit <span>0</span>
</span></span></code></pre></div><h4>Generating the SVF file<span><a href="#generating-the-svf-file" aria-label="Anchor">#</a></span></h4><p>The SVF for Serial Vector Format is a human readable, vendor agnostic specification used to specify JTAG bus operations.</p><p>Example SVF file, test program:</p><pre tabindex="0"><code data-lang="svf">! Initialize UUT
STATE RESET;
! End IR scans in DRPAUSE
ENDIR DRPAUSE;
! End DR scans in DRPAUSE
ENDDR DRPAUSE;
! 24 bit IR header
HIR 24 TDI (FFFFFF);
! 3 bit DR header
HDR 3 TDI (7);
! 16 bit IR trailer
TIR 16 TDI (FFFF);
! 2 bit DR trailer
TDR 2 TDI (3);
! 8 bit IR scan, load BIST opcode
SIR 8 TDI (41) TDO (81) MASK (FF);
! 16 bit DR scan, load BIST seed
SDR 16 TDI (ABCD);
! RUNBIST for 95 TCK Clocks
RUNTEST 95 TCK ENDSTATE IRPAUSE;
! 16 bit DR scan, check BIST status
SDR 16 TDI (0000) TDO(1234) MASK(FFFF);
! Enter Test-Logic-Reset
STATE RESET;
! End Test Program
</code></pre><p>Vivado can generate a hardware aware SVF file containing the configuration sequence for an FPGA board, allowing us to write a bitstream.</p><p>Given the SVF file literally contains the bitstream written in clear hexademical, in the file, our first step is to generate
our design’s bitstream.</p><p>Vivado proper isn’t the software that generates the SVF file, this task is done by the hardware manager which
handles all of the configuration.</p><p>We can launch a new instance <code>open_hw_manager</code> and connect to it <code>connect_hw_server</code>.
Since JTAG is a daisy chained bus, and given the SVF file is just a standardised way of specifying
JTAG bus operations, in order to generate a correct JTAG configuration sequence, we must inform the hardware manger
of our scan chain.</p><p>During our earlier probing of the scan chain, we have established that our FPGA is the only device on the chain.
We inform the hardware manager of this by creating a new device configuration ( the term “device” refers to the “board”
here ) and add our fpga to the chain using the <code>create_hw_device -part &lt;device name&gt;</code>.When we have multiple
devices we should register them following the order in which they appear on the chain.</p><p>Finally to generate the SVF file, we must select the device we wish to program with <code>program_hw_device &lt;hw_device&gt;</code>,
then write out the SVF to the file using <code>write_hw_svf &lt;path to svf file&gt;</code>.</p><p><code>gen.tcl</code>:</p><div><pre tabindex="0"><code data-lang="tcl"><span><span><span>set</span> checkpoint_path <span>[</span>lindex $argv <span>0</span><span>]</span>
</span></span><span><span><span>set</span> out_dir <span>[</span>lindex $argv <span>1</span><span>]</span>
</span></span><span><span>puts <span>"SVF generation script called with checkpoint path $checkpoint_path, generating to $out_dir"</span>
</span></span><span><span>
</span></span><span><span>open_checkpoint $checkpoint_path
</span></span><span><span>
</span></span><span><span><span># defines
</span></span></span><span><span><span></span><span>set</span> hw_target <span>"alibaba_board_svf_target"</span>
</span></span><span><span><span>set</span> fpga_device <span>"xcku3p"</span>
</span></span><span><span><span>set</span> bin_path <span>"$out_dir/[current_project]"</span>
</span></span><span><span>
</span></span><span><span>write_bitstream <span>"$bin_path.bit"</span> <span>-</span>force
</span></span><span><span>
</span></span><span><span>open_hw_manager
</span></span><span><span>
</span></span><span><span><span># connect to hw server with default config
</span></span></span><span><span><span></span>connect_hw_server
</span></span><span><span>puts <span>"connected to hw server at [current_hw_server]"</span>
</span></span><span><span>
</span></span><span><span>create_hw_target $hw_target
</span></span><span><span>puts <span>"current hw target [current_hw_target]"</span>
</span></span><span><span>
</span></span><span><span>open_hw_target
</span></span><span><span>
</span></span><span><span><span># single device on scan chain
</span></span></span><span><span><span></span>create_hw_device <span>-</span>part $fpga_device
</span></span><span><span>puts <span>"scan chain : [get_hw_devices]"</span>
</span></span><span><span>
</span></span><span><span>set_property PROGRAM.FILE <span>"$bin_path.bit"</span> <span>[</span>get_hw_device<span>]</span>
</span></span><span><span>
</span></span><span><span><span>#select device to program
</span></span></span><span><span><span></span>program_hw_device <span>[</span>get_hw_device<span>]</span>
</span></span><span><span>
</span></span><span><span><span># generate svf file
</span></span></span><span><span><span></span>write_hw_svf <span>-</span>force <span>"$bin_path.svf"</span>
</span></span><span><span>
</span></span><span><span>close_hw_manager
</span></span><span><span>exit <span>0</span>
</span></span></code></pre></div><h3>Configuring the FPGA using OpenOCD<span><a href="#configuring-the-fpga-using-openocd" aria-label="Anchor">#</a></span></h3><p>Although not widespread openOCD has a very nice <code>svf</code> execution command :</p><blockquote><div><h4>18.1 SVF: Serial Vector Format<span><a href="#181-svf-serial-vector-format" aria-label="Anchor">#</a></span></h4><p>The Serial Vector Format, better known as SVF, is a way to represent JTAG test patterns
in text files. In a debug session using JTAG for its transport protocol, OpenOCD supports
running such test files.</p><pre tabindex="0"><code>[Command]svf filename [-tap tapname] [[-]quiet] [[-]nil] [[-]progress]
[[-]ignore_error]
</code></pre><p>This issues a JTAG reset (Test-Logic-Reset) and then runs the SVF script from
filename.
Arguments can be specified in any order; the optional dash doesn’t affect their se-
mantics.</p><p><strong>Command options</strong>:</p><ul><li><code>-tap</code> tapname ignore IR and DR headers and footers specified by the SVF file
with HIR, TIR, HDR and TDR commands; instead, calculate them automatically
according to the current JTAG chain configuration, targeting tapname;</li><li><code>[-]quiet</code> do not log every command before execution;</li><li><code>[-]nil</code> “dry run”, i.e., do not perform any operations on the real interface;</li><li><code>[-]progress</code> enable progress indication;</li><li><code>[-]ignore</code>_error continue execution despite TDO check errors.</li></ul></div></blockquote><p>We invoke it in our openOCD script using the <code>-progress</code> option for additional logging.</p><p><code>openocd</code> :</p><div><pre tabindex="0"><code data-lang="tcl"><span><span><span>set</span> svf_path <span>"out/project_prj_checkpoint.svf"</span>
</span></span><span><span>
</span></span><span><span>source <span>[</span>find interface<span>/</span>jlink.cfg<span>]</span>
</span></span><span><span>transport select jtag
</span></span><span><span>
</span></span><span><span><span>set</span> SPEED <span>1</span>
</span></span><span><span>jtag_rclk $SPEED
</span></span><span><span>adapter speed $SPEED 
</span></span><span><span>reset_config none
</span></span><span><span>
</span></span><span><span><span># jlink config
</span></span></span><span><span><span>
</span></span></span><span><span><span></span><span>set</span> CHIPNAME XCKU3P
</span></span><span><span><span>set</span> CHIP $CHIPNAME
</span></span><span><span>puts <span>"set chipname "</span>$CHIP
</span></span><span><span>
</span></span><span><span>source <span>[</span>find ..<span>/</span>openocd<span>/</span>tcl<span>/</span>cpld<span>/</span>xilinx-xcu.cfg<span>]</span>
</span></span><span><span>
</span></span><span><span>source <span>[</span>find ..<span>/</span>openocd<span>/</span>tcl<span>/</span>fpga<span>/</span>xilinx-sysmon.cfg<span>]</span>
</span></span><span><span>
</span></span><span><span>init 
</span></span><span><span>
</span></span><span><span>puts <span>"--------------------"</span>
</span></span><span><span>
</span></span><span><span>sysmon_report $CHIP.tap
</span></span><span><span>
</span></span><span><span>puts <span>"--------------------"</span>
</span></span><span><span>
</span></span><span><span><span># program
</span></span></span><span><span><span></span><span>if</span> <span>{</span><span>!</span><span>[</span>file exists $svf_path<span>]}</span> <span>{</span>
</span></span><span><span>    puts <span>"Svf path not found : $svf_path"</span>
</span></span><span><span>    exit
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span>svf $svf_path <span>-</span>progress 
</span></span><span><span> 
</span></span><span><span>exit 
</span></span></code></pre></div><p>Flashing sequence log :</p><pre tabindex="0"><code>gp@workhorse:~/tools/openocd_jlink_test$ openocd
Open On-Chip Debugger 0.12.0+dev-02170-gfcff4b712 (2025-09-04-21:02)
Licensed under GNU GPL v2
For bug reports, read
	http://openocd.org/doc/doxygen/bugs.html
set chipname XCKU3P
Read temperature sysmon 4
Info : J-Link V10 compiled Jan 30 2023 11:28:07
Info : Hardware version: 10.10
Info : VTarget = 1.812 V
Info : clock speed 1 kHz
Info : JTAG tap: XCKU3P.tap tap/device found: 0x04a63093 (mfg: 0x049 (Xilinx), part: 0x4a63, ver: 0x0)
Warn : gdb services need one or more targets defined
--------------------
Sysmon status report :
TEMP 50.46 C
MAXTEMP 52.79 C
VCCINT 0.846 V
MAXVCC 0.860 V
VCCAUX 1.799 V
MAXVCCAUX 1.809 V
--------------------
svf processing file: "out/project_prj_checkpoint.svf"
  0%  TRST OFF;
  0%  ENDIR IDLE;
  0%  ENDDR IDLE;
  0%  STATE RESET;
  0%  STATE IDLE;
  0%  FREQUENCY 1.00E+07 HZ;
adapter speed: 10000 kHz
  0%  HIR 0 ;
  0%  TIR 0 ;
  0%  HDR 0 ;
  0%  TDR 0 ;
  0%  SIR 6 TDI (09) ;
  0%  SDR 32 TDI (00000000) TDO (04a63093) MASK (0fffffff) ;
  0%  STATE RESET;
  0%  STATE IDLE;
  0%  SIR 6 TDI (0b) ;
  0%  SIR 6 TDI (14) ;
  0%  RUNTEST 0.100000 SEC;
  0%  RUNTEST 10000 TCK;
  0%  SIR 6 TDI (14) TDO (11) MASK (31) ;
  0%  SIR 6 TDI (05) ;
 95%  ffffffffffff) ;
 95%  SIR 6 TDI (09) TDO (31) MASK (11) ;
 95%  STATE RESET;
 95%  RUNTEST 5 TCK;
 95%  SIR 6 TDI (05) ;
 95%  SDR 160 TDI (0000000400000004800700140000000466aa9955) ;
 95%  SIR 6 TDI (04) ;
 95%  SDR 32 TDI (00000000) TDO (3f5e0d40) MASK (08000000) ;
 95%  STATE RESET;
 95%  RUNTEST 5 TCK;
Info : Listening on port 6666 for tcl connections
Info : Listening on port 4444 for telnet connections
</code></pre><p>Resulting in a successfully configured our FPGA.</p><h2>Conclusion<span><a href="#conclusion" aria-label="Anchor">#</a></span></h2><p>For <strong>$200</strong> we got a fully working decommissioned Alibaba Cloud accelerator featuring a Kintex UltraScale+
FPGA with an easily accessible debugging/programming interface and enough pinout information to define
our own constraint files.</p><p>We also have a fully automated Vivado workflow to implement our designs and the ability to write the bitstream,
and interface with the FPGA’s internal JTAG accessible registers using an open source programming tool without
the need for an official Xilinx programmer.</p><p>In the end, this project delivered an at least 5x cost savings over commercial boards (compared to the lowest cost $900-1050 Alinx alternatives),
making this perhaps the most cost effective entry point for a Kintex UltraScale+ board.</p><h2>External ressources<span><a href="#external-ressources" aria-label="Anchor">#</a></span></h2><p>Xilinx Vivado Supported Devices : <a href="https://docs.amd.com/r/en-US/ug973-vivado-release-notes-install-license/Supported-Devices" target="_blank">https://docs.amd.com/r/en-US/ug973-vivado-release-notes-install-license/Supported-Devices</a></p><p>Official Xilinx dev board : <a href="https://www.amd.com/en/products/adaptive-socs-and-fpgas/evaluation-boards/ek-u1-kcu116-g.html" target="_blank">https://www.amd.com/en/products/adaptive-socs-and-fpgas/evaluation-boards/ek-u1-kcu116-g.html</a></p><p>Alinx Kintex UltraScale+ dev boards : <a href="https://www.en.alinx.com/Product/FPGA-Development-Boards/Kintex-UltraScale-plus.html" target="_blank">https://www.en.alinx.com/Product/FPGA-Development-Boards/Kintex-UltraScale-plus.html</a></p><p>UltraScale Architecture Configuration User Guide (UG570) : <a href="https://docs.amd.com/r/en-US/ug570-ultrascale-configuration/Device-Resources-and-Configuration-Bitstream-Lengths?section=gyn1703168518425__table_vyh_4hs_szb" target="_blank">https://docs.amd.com/r/en-US/ug570-ultrascale-configuration/Device-Resources-and-Configuration-Bitstream-Lengths?section=gyn1703168518425__table_vyh_4hs_szb</a></p><p>UltraScale Architecture System Monitor User Guide (UG580): <a href="https://docs.amd.com/v/u/en-US/ug580-ultrascale-sysmon" target="_blank">https://docs.amd.com/v/u/en-US/ug580-ultrascale-sysmon</a></p><p>Vivado Design Suite Tcl Command Reference Guide (UG835): <a href="https://docs.amd.com/r/en-US/ug835-vivado-tcl-commands/Tcl-Initialization-Scripts" target="_blank">https://docs.amd.com/r/en-US/ug835-vivado-tcl-commands/Tcl-Initialization-Scripts</a></p><p>PCI vendor/device ID database: <a href="https://admin.pci-ids.ucw.cz/read/PC/14e4" target="_blank">https://admin.pci-ids.ucw.cz/read/PC/14e4</a></p><p>PCI device classes: <a href="https://admin.pci-ids.ucw.cz/read/PD" target="_blank">https://admin.pci-ids.ucw.cz/read/PD</a></p><p>Linux kernel PCI IDs: <a href="https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L160-L3256" target="_blank">https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L160-L3256</a></p><p>Linux kernel PCI classes: <a href="https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L15-L158" target="_blank">https://github.com/torvalds/linux/blob/7aac71907bdea16e2754a782b9d9155449a9d49d/include/linux/pci_ids.h#L15-L158</a></p><p>Truck-kun pinout: <a href="https://blog.csdn.net/qq_37650251/article/details/145716953" target="_blank">https://blog.csdn.net/qq_37650251/article/details/145716953</a></p><p>Ebay listing: <a href="https://www.ebay.com/itm/167626831054?_trksid=p4375194.c101800.m5481" target="_blank">https://www.ebay.com/itm/167626831054?_trksid=p4375194.c101800.m5481</a></p><p>OpenOCD documentation: <a href="https://openocd.org/doc-release/pdf/openocd.pdf" target="_blank">https://openocd.org/doc-release/pdf/openocd.pdf</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Toyota runs a car-hacking event to boost security (2024) (107 pts)]]></title>
            <link>https://toyotatimes.jp/en/spotlights/1061.html</link>
            <guid>45470206</guid>
            <pubDate>Sat, 04 Oct 2025 03:11:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://toyotatimes.jp/en/spotlights/1061.html">https://toyotatimes.jp/en/spotlights/1061.html</a>, See on <a href="https://news.ycombinator.com/item?id=45470206">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mainBody">
<!-- メインコンテンツ -->
  <div>
    <header>
      
      
    </header>
    <!-- 記事本体 -->
    <section>
      <p>2024.12.09</p>
      <article>
        <figure>
          <img src="https://toyotatimes.jp/en/spotlights/upload_images/spotlights_1061_1280_720.jpg" width="100%" alt="">
        </figure>
        <p>Toyota organizes a car-hacking event that captivates participating students. We uncovered the important purpose behind the event.</p>
        
                        
        <section>

          <figure>
  
            <img src="https://toyotatimes.jp/en/spotlights/upload_images/spotlights_1061_1280_720.jpg" width="100%" alt="">
  
          </figure>
          
  

          <!-- 主構成 -->
  
    <section>
  <!-- 文章 -->
    <!-- 通常 -->
    <div>
        <p>What comes to mind when you hear the word “hacking?”</p>

<p>Generally, it means “an act of unauthorized access into a system or network, intended to cause harm.”</p>

<p>In films and comics, hackers are often portrayed as criminals.</p>

<p>You may have watched scenes where such hackers tap away on keyboards as lines of numbers and symbols fill the computer screen to steal data and hijack machines.</p>
    </div>
  <!-- 画像 -->
  <div>
    <figure>
        <img src="https://toyotatimes.jp/en/spotlights/upload_images/spotlights_1061_1.jpg" alt="">
      <figcaption></figcaption>
    </figure>
  </div>
  <!-- 文章 -->
    <!-- 通常 -->
    <p>How, then, would you feel about Toyota holding a car-hacking event?</p>
  </section>
  <section>
    <h3 id="index01">Not just in Japan</h3>
  <!-- 文章 -->
    <!-- 通常 -->
    <p>Many students have gathered in a conference room. All are engrossed in a unique competition unfolding on their computer screens.</p>
  <!-- 画像 -->
  <div>
    <figure>
        <img src="https://toyotatimes.jp/en/spotlights/upload_images/spotlights_1061_2.jpg" alt="">
      <figcaption></figcaption>
    </figure>
  </div>
  <!-- 文章 -->
    <!-- 通常 -->
    <div>
        <p><strong>The event is Hack Festa*, a chance for IT students to pit their hacking skills against each other.</strong></p>

<p>
<sup>*Jointly organized by Toyota, Toyota Motor North America, Inc., and Toyota Tsusho Systems US, Inc.</sup>
</p>

<p>The students form groups of around four and tackle the assigned tasks as a team. All of the challenges are car-related, ranging from controlling a vehicle’s speed adjustment mechanism to overwriting engine RPMs.</p>

<p>Their efforts are tested on simulators, earning points for every task cleared. The team with the highest score wins.</p>
    </div>
  <!-- 画像 -->
  <div>
    <figure>
        <img src="https://toyotatimes.jp/en/spotlights/upload_images/spotlights_1061_3.jpg" alt="">
      <figcaption></figcaption>
    </figure>
  </div>
  <!-- 文章 -->
    <!-- 通常 -->
    <p>Aside from Japan, the events are also held in the United States and Ireland.</p>
  <!-- 画像 -->
  <div>
    <figure>
        <img src="https://toyotatimes.jp/en/spotlights/upload_images/spotlights_1061_4.jpg" alt="">
      <figcaption></figcaption>
    </figure>
  </div>
  <!-- 文章 -->
    <!-- 通常 -->
    <div>
        <p>Why is Toyota hosting such events? Isn’t hacking harmful?</p>

<p>When we posed these questions to the event producer, Hisashi Oguma, a project general manager at InfoTech-IS, explained that we first had to understand the challenges facing the auto industry.</p>
    </div>
  </section>
  <section>
    <h3 id="index02">With convenience comes new threats</h3>
  </section>
  <div>
  <!-- 文章 -->
    <!-- 通常 -->
    <p><strong>Hisashi Oguma, Ph.D., Project General Manager/Principal Researcher, InfoTech-IS</strong></p>
  <!-- 画像 -->
  <div>
    <figure>
        <img src="https://toyotatimes.jp/en/spotlights/upload_images/spotlights_1061_5.jpg" alt="">
      <figcaption></figcaption>
    </figure>
  </div>
  <!-- 文章 -->
    <!-- 通常 -->
    <div>
        <p>Today, cars are evolving in unprecedented ways through internet connectivity, with automated driving and connected cars as obvious examples.</p>

<p>Another concept that has emerged recently is the SDV (Software Defined Vehicle). Simply put, it involves updating the onboard software after purchase, continuing to expand the car’s functionality as you would a smartphone.</p>

<p><strong>Internet integration improves convenience. At the same time, it also makes it easier for malicious third parties to hack car systems.</strong></p>

<p><strong>The cybersecurity that protects customers from such harm has become more important than ever, not only for Toyota but for the entire auto industry.</strong></p>
    </div>
    </div>
    <section>
  <!-- 文章 -->
    <!-- 通常 -->
    <div>
        <p>In serious cases, a hacked car could mean a loss of control over its basic functions of driving, turning, and stopping.</p>

<p>As an example, the video below shows a simulation of what might happen if steering wheel control is compromised while driving.</p>
    </div>
  <!-- YouTube動画 -->
  <section>
    <figure>
      <iframe src="https://www.youtube.com/embed/pUj56fbQMnE?rel=0&amp;wmode=transparent&amp;enablejsapi=1" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
    </figure>
  </section>
  <!-- 文章 -->
    <!-- 通常 -->
    <div>
        <p>Would you be able to keep your cool if this happened in real life?</p>

<p>Oguma’s division researches ways to prevent such harm. It organizes the Hack Festa to help counter potential threats.</p>

<p>A hacking event to boost cybersecurity. It sounds contradictory, but what on earth does it mean?</p>
    </div>
  </section>
   
  <!-- 記事下SNSシェア -->
  
  
  
          <!-- page navigation -->
          
          <!-- /page navigation -->
        </section>
      </article>
    </section>
    <!-- /記事本体 -->
    <!-- ページナビ -->
     
  </div>  
  <!-- RECOMMEND -->
  <div id="recommend">
      <header>
        <h2>RECOMMEND</h2>
      </header>
      <section>
        <ul id="recommendContentsList">
        </ul>
      </section>
      
      
    </div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New antibiotic targets IBD and AI predicted how it would work (163 pts)]]></title>
            <link>https://healthsci.mcmaster.ca/new-antibiotic-targets-ibd-and-ai-predicted-how-it-would-work-before-scientists-could-prove-it/</link>
            <guid>45469579</guid>
            <pubDate>Sat, 04 Oct 2025 01:09:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://healthsci.mcmaster.ca/new-antibiotic-targets-ibd-and-ai-predicted-how-it-would-work-before-scientists-could-prove-it/">https://healthsci.mcmaster.ca/new-antibiotic-targets-ibd-and-ai-predicted-how-it-would-work-before-scientists-could-prove-it/</a>, See on <a href="https://news.ycombinator.com/item?id=45469579">Hacker News</a></p>
Couldn't get https://healthsci.mcmaster.ca/new-antibiotic-targets-ibd-and-ai-predicted-how-it-would-work-before-scientists-could-prove-it/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Track which Electron apps slow down macOS 26 Tahoe (135 pts)]]></title>
            <link>https://avarayr.github.io/shamelectron/</link>
            <guid>45469468</guid>
            <pubDate>Sat, 04 Oct 2025 00:45:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://avarayr.github.io/shamelectron/">https://avarayr.github.io/shamelectron/</a>, See on <a href="https://news.ycombinator.com/item?id=45469468">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header><div><p>Tracking problematic Electron apps macOS Tahoe.</p><div><p>Major GPU performance issue on macOS 26 (<a href="https://github.com/electron/electron/pull/48376" target="_blank" rel="noopener noreferrer">electron/electron#48376</a>).</p></div></div><div><div><div><h2>Status Overview</h2><div><p>33</p><!-- --><p> total applications tracked</p></div></div><div><p>Last updated</p><p>6s ago</p><div><p>updates automatically every</p><!-- --> <p><span>12 hours</span></p></div></div></div><div><div><p>7</p><p>fixed</p></div><div><p>26</p><p>not fixed</p></div></div></div></header><div><p>Are you an Electron app developer?</p><div><p>Bump Electron to at least versions</p><!-- --> <p><span>v38.2.0</span>,</p><!-- --> <p><span>v37.6.0</span> and</p><!-- --> <p><span>v36.9.2</span></p></div></div><div><p><span>status</span></p><p><span>app</span></p><p><span>social</span></p></div><div><div><p><img alt="1Password icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/ids0xxqhX-/w/128/h/128/theme/light/symbol.png?c=1bxid64Mup7aczewSAYMX&amp;t=1668082116841"></p><p><span>1Password</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow 1Password on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%401Password"><span>ask <span>@<!-- -->1Password</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Arduino IDE icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idtGUB17gh/w/128/h/128/theme/dark/icon.png?c=1bxid64Mup7aczewSAYMX&amp;t=1667813023270"></p><p><span>Arduino IDE</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Arduino IDE on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40arduino"><span>ask <span>@<!-- -->arduino</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Balena Etcher icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/id6vfMEozM/w/128/h/128/theme/dark/logo.png?c=1bxid64Mup7aczewSAYMX&amp;t=1726185885159"></p><p><span>Balena Etcher</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Balena Etcher on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40balena_io"><span>ask <span>@<!-- -->balena_io</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Beeper icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idbHZntkNh/w/128/h/128/theme/dark/icon.jpeg?c=1bxid64Mup7aczewSAYMX&amp;t=1714483824721"></p><p><span>Beeper</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Beeper on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40beeper"><span>ask <span>@<!-- -->beeper</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Bitwarden icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idoK_yj68K/w/128/h/128/theme/dark/icon.jpeg?c=1bxid64Mup7aczewSAYMX&amp;t=1690486241518"></p><p><span>Bitwarden</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Bitwarden on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40bitwarden"><span>ask <span>@<!-- -->bitwarden</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Bruno icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://raw.githubusercontent.com/usebruno/bruno/main/assets/images/logo-transparent.png"></p><p><span>Bruno</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Bruno on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40use_bruno"><span>ask <span>@<!-- -->use_bruno</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Claude App icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idW5s392j1/w/128/h/128/theme/dark/icon.png?c=1bxid64Mup7aczewSAYMX&amp;t=1738315794862"></p><p><span>Claude App</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Claude App on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40claudeai"><span>ask <span>@<!-- -->claudeai</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Cluely icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idCSRE3cOk/w/128/h/128/theme/dark/icon.jpeg?c=1bxid64Mup7aczewSAYMX&amp;t=1752719809988"></p><p><span>Cluely</span></p></div><div><p><img alt="Cursor icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/ideKwS9dxx/w/128/h/128/theme/dark/icon.jpeg?c=1bxid64Mup7aczewSAYMX&amp;t=1741336988021"></p><p><span>Cursor</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Cursor on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40cursor_ai"><span>ask <span>@<!-- -->cursor_ai</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Discord icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idM8Hlme1a/w/128/h/128/theme/dark/icon.png?c=1bxid64Mup7aczewSAYMX&amp;t=1667560105720"></p><p><span>Discord</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Discord on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40discord"><span>ask <span>@<!-- -->discord</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Discord (Canary) icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idM8Hlme1a/w/128/h/128/theme/dark/icon.png?c=1bxid64Mup7aczewSAYMX&amp;t=1667560105720"></p><p><span>Discord (Canary)</span></p></div><div><p><img alt="Docker Desktop icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/id5_eOiB6T/w/128/h/128/theme/dark/icon.jpeg?c=1bxid64Mup7aczewSAYMX&amp;t=1707508241645"></p><p><span>Docker Desktop</span></p></div><div><p><img alt="Dropbox icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idY3kwH_Nx/w/400/h/400/theme/dark/icon.png?c=1bxid64Mup7aczewSAYMX&amp;t=1691075441479"></p><p><span>Dropbox</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Dropbox on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40dropbox"><span>ask <span>@<!-- -->dropbox</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Element icon" width="32" height="32" decoding="async" data-nimg="1" src="data:image/svg+xml;utf8,<svg fill='none' height='32' viewBox='0 0 32 32' width='32' xmlns='http://www.w3.org/2000/svg'><g clip-path='url(%23clip0_1093_84885-720778)'><path clip-rule='evenodd' d='M16 32C24.8365 32 32 24.8365 32 16C32 7.16346 24.8365 0 16 0C7.16346 0 0 7.16346 0 16C0 24.8365 7.16346 32 16 32Z' fill='%230DBD8B' fill-rule='evenodd'></path><path clip-rule='evenodd' d='M13.0781 7.45456C13.0781 6.8087 13.6028 6.28516 14.25 6.28516C18.6364 6.28516 22.1923 9.83367 22.1923 14.211C22.1923 14.8568 21.6676 15.3804 21.0205 15.3804C20.3733 15.3804 19.8486 14.8568 19.8486 14.211C19.8486 11.1254 17.342 8.62396 14.25 8.62396C13.6028 8.62396 13.0781 8.10039 13.0781 7.45456Z' fill='white' fill-rule='evenodd'></path><path clip-rule='evenodd' d='M24.5439 13.043C25.1911 13.043 25.7157 13.5665 25.7157 14.2124C25.7157 18.5897 22.1598 22.1382 17.7734 22.1382C17.1262 22.1382 16.6016 21.6147 16.6016 20.9688C16.6016 20.323 17.1262 19.7994 17.7734 19.7994C20.8655 19.7994 23.3721 17.298 23.3721 14.2124C23.3721 13.5665 23.8967 13.043 24.5439 13.043Z' fill='white' fill-rule='evenodd'></path><path clip-rule='evenodd' d='M18.9423 24.543C18.9423 25.1889 18.4176 25.7124 17.7705 25.7124C13.384 25.7124 9.82812 22.1639 9.82812 17.7866C9.82812 17.1407 10.3528 16.6172 11 16.6172C11.6471 16.6172 12.1718 17.1407 12.1718 17.7866C12.1718 20.8722 14.6784 23.3736 17.7705 23.3736C18.4176 23.3736 18.9423 23.8972 18.9423 24.543Z' fill='white' fill-rule='evenodd'></path><path clip-rule='evenodd' d='M7.46086 18.9585C6.81369 18.9585 6.28906 18.435 6.28906 17.7891C6.28903 13.4118 9.84495 9.86328 14.2314 9.86328C14.8786 9.86328 15.4032 10.3869 15.4032 11.0327C15.4032 11.6785 14.8786 12.2021 14.2314 12.2021C11.1393 12.2021 8.63269 14.7035 8.63269 17.7891C8.63269 18.435 8.10806 18.9585 7.46086 18.9585Z' fill='white' fill-rule='evenodd'></path></g><defs><clipPath id='clip0_1093_84885-720778'><rect fill='currentColor' height='32' width='148'></rect></clipPath></defs></svg>"></p><p><span>Element</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Element on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40element_hq"><span>ask <span>@<!-- -->element_hq</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Figma icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idZHcZ_i7F/w/128/h/128/theme/dark/icon.png?c=1bxid64Mup7aczewSAYMX&amp;t=1729268227605"></p><p><span>Figma</span></p></div><div><p><img alt="GitHub Desktop icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idZAyF9rlg/w/128/h/128/theme/dark/icon.png?c=1bxid64Mup7aczewSAYMX&amp;t=1719469970995"></p><p><span>GitHub Desktop</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow GitHub Desktop on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40github"><span>ask <span>@<!-- -->github</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="GitKraken icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/id8KASe1rU/w/400/h/400/theme/dark/icon.jpeg?c=1bxid64Mup7aczewSAYMX&amp;t=1752566817684"></p><p><span>GitKraken</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow GitKraken on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40gitkraken"><span>ask <span>@<!-- -->gitkraken</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="HEY icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/id-jhIB7GM/w/128/h/128/theme/dark/icon.jpeg?c=1bxid64Mup7aczewSAYMX&amp;t=1753048414047"></p><p><span>HEY</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow HEY on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40heyhey"><span>ask <span>@<!-- -->heyhey</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Kiro icon" width="32" height="32" decoding="async" data-nimg="1" src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='20' height='20' viewBox='0 0 22 24' fill='none'><path d='M3.80081 18.5661C1.32306 24.0572 6.59904 25.434 10.4904 22.2205C11.6339 25.8242 15.926 23.1361 17.4652 20.3445C20.8578 14.1915 19.4877 7.91459 19.1361 6.61988C16.7244 -2.20972 4.67055 -2.21852 2.59581 6.6649C2.11136 8.21946 2.10284 9.98752 1.82846 11.8233C1.69011 12.749 1.59258 13.3398 1.23436 14.3135C1.02841 14.8733 0.745043 15.3704 0.299833 16.2082C-0.391594 17.5095 -0.0998802 20.021 3.46397 18.7186V18.7195L3.80081 18.5661Z' fill='white'/><path d='M10.9614 10.4413C9.97202 10.4413 9.82422 9.25893 9.82422 8.55407C9.82422 7.91791 9.93824 7.4124 10.1542 7.09197C10.3441 6.81003 10.6158 6.66699 10.9614 6.66699C11.3071 6.66699 11.6036 6.81228 11.8128 7.09892C12.0511 7.42554 12.177 7.92861 12.177 8.55407C12.177 9.73591 11.7226 10.4413 10.9616 10.4413H10.9614Z' fill='black'/><path d='M15.0318 10.4413C14.0423 10.4413 13.8945 9.25893 13.8945 8.55407C13.8945 7.91791 14.0086 7.4124 14.2245 7.09197C14.4144 6.81003 14.6861 6.66699 15.0318 6.66699C15.3774 6.66699 15.6739 6.81228 15.8831 7.09892C16.1214 7.42554 16.2474 7.92861 16.2474 8.55407C16.2474 9.73591 15.793 10.4413 15.0319 10.4413H15.0318Z' fill='black'/></svg>"></p><p><span>Kiro</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Kiro on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40kirodotdev"><span>ask <span>@<!-- -->kirodotdev</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="LM Studio icon" width="32" height="32" decoding="async" data-nimg="1" src="data:image/png;base64,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"></p><p><span>LM Studio</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow LM Studio on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40lmstudio"><span>ask <span>@<!-- -->lmstudio</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Logseq icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idH5AFLYz5/w/128/h/128/theme/dark/icon.png?c=1bxid64Mup7aczewSAYMX&amp;t=1757071332565"></p><p><span>Logseq</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Logseq on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40logseq"><span>ask <span>@<!-- -->logseq</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Notion icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idYnkdM3Ni/w/128/h/128/theme/dark/icon.jpeg?c=1bxid64Mup7aczewSAYMX&amp;t=1667896752278"></p><p><span>Notion</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Notion on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40notion"><span>ask <span>@<!-- -->notion</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Notion Calendar icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idYnkdM3Ni/w/128/h/128/theme/dark/icon.jpeg?c=1bxid64Mup7aczewSAYMX&amp;t=1667896752278"></p><p><span>Notion Calendar</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Notion Calendar on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40notion"><span>ask <span>@<!-- -->notion</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Notion Mail icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idYnkdM3Ni/w/128/h/128/theme/dark/icon.jpeg?c=1bxid64Mup7aczewSAYMX&amp;t=1667896752278"></p><p><span>Notion Mail</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Notion Mail on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40notion"><span>ask <span>@<!-- -->notion</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Obsidian icon" width="32" height="32" decoding="async" data-nimg="1" src="data:image/svg+xml;charset=utf-8,%3Csvg%20id%3D%22custom-logo%22%20width%3D%22512%22%20height%3D%22512%22%20viewBox%3D%220%200%20512%20512%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20style%3D%22height%3A100%25%3Bwidth%3A100%25%3B%22%3E%0A%20%20%3Cdefs%3E%0A%20%20%20%20%3CradialGradient%20id%3D%22b%22%20cx%3D%220%22%20cy%3D%220%22%20r%3D%221%22%20gradientUnits%3D%22userSpaceOnUse%22%20gradientTransform%3D%22matrix(-48%20-185%20123%20-32%20179%20429.7)%22%3E%0A%20%20%20%20%20%20%3Cstop%20stop-color%3D%22%23fff%22%20stop-opacity%3D%22.4%22%2F%3E%0A%20%20%20%20%20%20%3Cstop%20offset%3D%221%22%20stop-opacity%3D%22.1%22%2F%3E%0A%20%20%20%20%3C%2FradialGradient%3E%0A%20%20%20%20%3CradialGradient%20id%3D%22c%22%20cx%3D%220%22%20cy%3D%220%22%20r%3D%221%22%20gradientUnits%3D%22userSpaceOnUse%22%20gradientTransform%3D%22matrix(41%20-310%20229%2030%20341.6%20351.3)%22%3E%0A%20%20%20%20%20%20%3Cstop%20stop-color%3D%22%23fff%22%20stop-opacity%3D%22.6%22%2F%3E%0A%20%20%20%20%20%20%3Cstop%20offset%3D%221%22%20stop-color%3D%22%23fff%22%20stop-opacity%3D%22.1%22%2F%3E%0A%20%20%20%20%3C%2FradialGradient%3E%0A%20%20%20%20%3CradialGradient%20id%3D%22d%22%20cx%3D%220%22%20cy%3D%220%22%20r%3D%221%22%20gradientUnits%3D%22userSpaceOnUse%22%20gradientTransform%3D%22matrix(57%20-261%20178%2039%20190.5%20296.3)%22%3E%0A%20%20%20%20%20%20%3Cstop%20stop-color%3D%22%23fff%22%20stop-opacity%3D%22.8%22%2F%3E%0A%20%20%20%20%20%20%3Cstop%20offset%3D%221%22%20stop-color%3D%22%23fff%22%20stop-opacity%3D%22.4%22%2F%3E%0A%20%20%20%20%3C%2FradialGradient%3E%0A%20%20%20%20%3CradialGradient%20id%3D%22e%22%20cx%3D%220%22%20cy%3D%220%22%20r%3D%221%22%20gradientUnits%3D%22userSpaceOnUse%22%20gradientTransform%3D%22matrix(-79%20-133%20153%20-90%20321.4%20464.2)%22%3E%0A%20%20%20%20%20%20%3Cstop%20stop-color%3D%22%23fff%22%20stop-opacity%3D%22.3%22%2F%3E%0A%20%20%20%20%20%20%3Cstop%20offset%3D%221%22%20stop-opacity%3D%22.3%22%2F%3E%0A%20%20%20%20%3C%2FradialGradient%3E%0A%20%20%20%20%3CradialGradient%20id%3D%22f%22%20cx%3D%220%22%20cy%3D%220%22%20r%3D%221%22%20gradientUnits%3D%22userSpaceOnUse%22%20gradientTransform%3D%22matrix(-29%20136%20-92%20-20%20300.7%20149.9)%22%3E%0A%20%20%20%20%20%20%3Cstop%20stop-color%3D%22%23fff%22%20stop-opacity%3D%220%22%2F%3E%0A%20%20%20%20%20%20%3Cstop%20offset%3D%221%22%20stop-color%3D%22%23fff%22%20stop-opacity%3D%22.2%22%2F%3E%0A%20%20%20%20%3C%2FradialGradient%3E%0A%20%20%20%20%3CradialGradient%20id%3D%22g%22%20cx%3D%220%22%20cy%3D%220%22%20r%3D%221%22%20gradientUnits%3D%22userSpaceOnUse%22%20gradientTransform%3D%22matrix(72%2073%20-155%20153%20137.8%20225.2)%22%3E%0A%20%20%20%20%20%20%3Cstop%20stop-color%3D%22%23fff%22%20stop-opacity%3D%22.2%22%2F%3E%0A%20%20%20%20%20%20%3Cstop%20offset%3D%221%22%20stop-color%3D%22%23fff%22%20stop-opacity%3D%22.4%22%2F%3E%0A%20%20%20%20%3C%2FradialGradient%3E%0A%20%20%20%20%3CradialGradient%20id%3D%22h%22%20cx%3D%220%22%20cy%3D%220%22%20r%3D%221%22%20gradientUnits%3D%22userSpaceOnUse%22%20gradientTransform%3D%22matrix(20%20118%20-251%2043%20215.1%20273.7)%22%3E%0A%20%20%20%20%20%20%3Cstop%20stop-color%3D%22%23fff%22%20stop-opacity%3D%22.1%22%2F%3E%0A%20%20%20%20%20%20%3Cstop%20offset%3D%221%22%20stop-color%3D%22%23fff%22%20stop-opacity%3D%22.3%22%2F%3E%0A%20%20%20%20%3C%2FradialGradient%3E%0A%20%20%20%20%3CradialGradient%20id%3D%22i%22%20cx%3D%220%22%20cy%3D%220%22%20r%3D%221%22%20gradientUnits%3D%22userSpaceOnUse%22%20gradientTransform%3D%22matrix(-162%20-85%20268%20-510%20374.4%20371.7)%22%3E%0A%20%20%20%20%20%20%3Cstop%20stop-color%3D%22%23fff%22%20stop-opacity%3D%22.2%22%2F%3E%0A%20%20%20%20%20%20%3Cstop%20offset%3D%22.5%22%20stop-color%3D%22%23fff%22%20stop-opacity%3D%22.2%22%2F%3E%0A%20%20%20%20%20%20%3Cstop%20offset%3D%221%22%20stop-color%3D%22%23fff%22%20stop-opacity%3D%22.3%22%2F%3E%0A%20%20%20%20%3C%2FradialGradient%3E%0A%20%20%20%20%3Cfilter%20id%3D%22a%22%20x%3D%2280.1%22%20y%3D%2237%22%20width%3D%22351.1%22%20height%3D%22443.2%22%20filterUnits%3D%22userSpaceOnUse%22%20color-interpolation-filters%3D%22sRGB%22%3E%0A%20%20%20%20%20%20%3CfeFlood%20flood-opacity%3D%220%22%20result%3D%22BackgroundImageFix%22%2F%3E%0A%20%20%20%20%20%20%3CfeBlend%20in%3D%22SourceGraphic%22%20in2%3D%22BackgroundImageFix%22%20result%3D%22shape%22%2F%3E%0A%20%20%20%20%20%20%3CfeGaussianBlur%20stdDeviation%3D%226.5%22%20result%3D%22effect1_foregroundBlur_744_9191%22%2F%3E%0A%20%20%20%20%3C%2Ffilter%3E%0A%20%20%3C%2Fdefs%3E%0A%20%20%3Crect%20id%3D%22logo-bg%22%20fill%3D%22%23262626%22%20width%3D%22512%22%20height%3D%22512%22%20rx%3D%22100%22%2F%3E%0A%20%20%3Cg%20filter%3D%22url(%23a)%22%3E%0A%20%20%20%20%3Cpath%20d%3D%22M359.2%20437.5c-2.6%2019-21.3%2033.9-40%2028.7-26.5-7.2-57.2-18.6-84.8-20.7l-42.4-3.2a28%2028%200%200%201-18-8.3l-73-74.8a27.7%2027.7%200%200%201-5.4-30.7s45-98.6%2046.8-103.7c1.6-5.1%207.8-49.9%2011.4-73.9a28%2028%200%200%201%209-16.5L249%2057.2a28%2028%200%200%201%2040.6%203.4l72.6%2091.6a29.5%2029.5%200%200%201%206.2%2018.3c0%2017.3%201.5%2053%2011.2%2076a301.3%20301.3%200%200%200%2035.6%2058.2%2014%2014%200%200%201%201%2015.6c-6.3%2010.7-18.9%2031.3-36.6%2057.6a142.2%20142.2%200%200%200-20.5%2059.6Z%22%20fill%3D%22%23000%22%20fill-opacity%3D%22.3%22%2F%3E%0A%20%20%3C%2Fg%3E%0A%20%20%3Cpath%20id%3D%22arrow%22%20d%3D%22M359.9%20434.3c-2.6%2019.1-21.3%2034-40%2028.9-26.4-7.3-57-18.7-84.7-20.8l-42.3-3.2a27.9%2027.9%200%200%201-18-8.4l-73-75a27.9%2027.9%200%200%201-5.4-31s45.1-99%2046.8-104.2c1.7-5.1%207.8-50%2011.4-74.2a28%2028%200%200%201%209-16.6l86.2-77.5a28%2028%200%200%201%2040.6%203.5l72.5%2092a29.7%2029.7%200%200%201%206.2%2018.3c0%2017.4%201.5%2053.2%2011.1%2076.3a303%20303%200%200%200%2035.6%2058.5%2014%2014%200%200%201%201.1%2015.7c-6.4%2010.8-18.9%2031.4-36.7%2057.9a143.3%20143.3%200%200%200-20.4%2059.8Z%22%20fill%3D%22%236C31E3%22%2F%3E%0A%20%20%3Cpath%20d%3D%22M182.7%20436.4c33.9-68.7%2033-118%2018.5-153-13.2-32.4-37.9-52.8-57.3-65.5-.4%201.9-1%203.7-1.8%205.4L96.5%20324.8a27.9%2027.9%200%200%200%205.5%2031l72.9%2075c2.3%202.3%205%204.2%207.8%205.6Z%22%20fill%3D%22url(%23b)%22%2F%3E%0A%20%20%3Cpath%20d%3D%22M274.9%20297c9.1.9%2018%202.9%2026.8%206.1%2027.8%2010.4%2053.1%2033.8%2074%2078.9%201.5-2.6%203-5.1%204.6-7.5a1222%201222%200%200%200%2036.7-57.9%2014%2014%200%200%200-1-15.7%20303%20303%200%200%201-35.7-58.5c-9.6-23-11-58.9-11.1-76.3%200-6.6-2.1-13.1-6.2-18.3l-72.5-92-1.2-1.5c5.3%2017.5%205%2031.5%201.7%2044.2-3%2011.8-8.6%2022.5-14.5%2033.8-2%203.8-4%207.7-5.9%2011.7a140%20140%200%200%200-15.8%2058c-1%2024.2%203.9%2054.5%2020%2095Z%22%20fill%3D%22url(%23c)%22%2F%3E%0A%20%20%3Cpath%20d%3D%22M274.8%20297c-16.1-40.5-21-70.8-20-95%201-24%208-42%2015.8-58l6-11.7c5.8-11.3%2011.3-22%2014.4-33.8a78.5%2078.5%200%200%200-1.7-44.2%2028%2028%200%200%200-39.4-2l-86.2%2077.5a28%2028%200%200%200-9%2016.6L144.2%20216c0%20.7-.2%201.3-.3%202%2019.4%2012.6%2044%2033%2057.3%2065.3%202.6%206.4%204.8%2013.1%206.4%2020.4a200%20200%200%200%201%2067.2-6.8Z%22%20fill%3D%22url(%23d)%22%2F%3E%0A%20%20%3Cpath%20d%3D%22M320%20463.2c18.6%205.1%2037.3-9.8%2039.9-29a153%20153%200%200%201%2015.9-52.2c-21-45.1-46.3-68.5-74-78.9-29.5-11-61.6-7.3-94.2.6%207.3%2033.1%203%2076.4-24.8%20132.7%203.1%201.6%206.6%202.5%2010.1%202.8l43.9%203.3c23.8%201.7%2059.3%2014%2083.2%2020.7Z%22%20fill%3D%22url(%23e)%22%2F%3E%0A%20%20%3Cpath%20fill-rule%3D%22evenodd%22%20clip-rule%3D%22evenodd%22%20d%3D%22M255%20200.5c-1.1%2024%201.9%2051.4%2018%2091.8l-5-.5c-14.5-42.1-17.7-63.7-16.6-88%201-24.3%208.9-43%2016.7-59%202-4%206.6-11.5%208.6-15.3%205.8-11.3%209.7-17.2%2013-27.5%204.8-14.4%203.8-21.2%203.2-28%203.7%2024.5-10.4%2045.8-21%2067.5a145%20145%200%200%200-17%2059Z%22%20fill%3D%22url(%23f)%22%2F%3E%0A%20%20%3Cpath%20fill-rule%3D%22evenodd%22%20clip-rule%3D%22evenodd%22%20d%3D%22M206%20285.1c2%204.4%203.7%208%204.9%2013.5l-4.3%201c-1.7-6.4-3-11-5.5-16.5-14.6-34.3-38-52-57-65%2023%2012.4%2046.7%2031.9%2061.9%2067Z%22%20fill%3D%22url(%23g)%22%2F%3E%0A%20%20%3Cpath%20fill-rule%3D%22evenodd%22%20clip-rule%3D%22evenodd%22%20d%3D%22M211.1%20303c8%2037.5-1%2085.2-27.5%20131.6%2022.2-46%2033-90.1%2024-131l3.5-.7Z%22%20fill%3D%22url(%23h)%22%2F%3E%0A%20%20%3Cpath%20fill-rule%3D%22evenodd%22%20clip-rule%3D%22evenodd%22%20d%3D%22M302.7%20299.5c43.5%2016.3%2060.3%2052%2072.8%2081.9-15.5-31.2-37-65.7-74.4-78.5-28.4-9.8-52.4-8.6-93.5.7l-.9-4c43.6-10%2066.4-11.2%2096%200Z%22%20fill%3D%22url(%23i)%22%2F%3E%0A%3C%2Fsvg%3E"></p><p><span>Obsidian</span></p></div><div><p><img alt="Pocket Casts icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/ids8HGa-I3/w/128/h/128/theme/dark/logo.png?c=1bxid64Mup7aczewSAYMX&amp;t=1740032928003"></p><p><span>Pocket Casts</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Pocket Casts on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40pocketcasts"><span>ask <span>@<!-- -->pocketcasts</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Postman icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idrVtxty7B/w/128/h/128/theme/dark/icon.png?c=1bxid64Mup7aczewSAYMX&amp;t=1667906412376"></p><p><span>Postman</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Postman on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40postman"><span>ask <span>@<!-- -->postman</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Salea Logic icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/id1JR10AXb/w/128/h/128/theme/dark/icon.jpeg?c=1bxid64Mup7aczewSAYMX&amp;t=1753768754460"></p><p><span>Salea Logic</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Salea Logic on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40salea"><span>ask <span>@<!-- -->salea</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Signal icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idwiNfPKaM/w/128/h/128/theme/dark/icon.png?c=1bxid64Mup7aczewSAYMX&amp;t=1740365382910"></p><p><span>Signal</span></p></div><div><p><img alt="Slack icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idJ_HhtG0Z/w/128/h/128/theme/dark/icon.jpeg?c=1bxid64Mup7aczewSAYMX&amp;t=1745381282564"></p><p><span>Slack</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Slack on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40slackhq"><span>ask <span>@<!-- -->slackhq</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Visual Studio Code icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idIkI_7uw6/w/128/h/128/theme/dark/icon.jpeg?c=1bxid64Mup7aczewSAYMX&amp;t=1756787305385"></p><p><span>Visual Studio Code</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Visual Studio Code on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40code"><span>ask <span>@<!-- -->code</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div><div><p><img alt="Visual Studio Code (Insiders) icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/idIkI_7uw6/w/128/h/128/theme/dark/icon.jpeg?c=1bxid64Mup7aczewSAYMX&amp;t=1756787305385"></p><p><span>Visual Studio Code (Insiders)</span></p></div><div><p><img alt="Windsurf icon" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://cdn.brandfetch.io/id5pWZwY_5/w/128/h/128/theme/dark/icon.jpeg?c=1bxid64Mup7aczewSAYMX&amp;t=1752603168917"></p><p><span>Windsurf</span></p><p><a target="_blank" rel="noopener noreferrer" aria-label="Follow Windsurf on Twitter" href="https://x.com/intent/tweet?text=please%20bump%20Electron%20to%20fix%20MacOS%2026%20performance%20issue%20%40windsurf_ai"><span>ask <span>@<!-- -->windsurf_ai</span> to bump Electron</span><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M22 4s-.7 2.1-2 3.4c1.6 10-9.4 17.3-18 11.6 2.2.1 4.4-.6 6-2C3 15.5.5 9.6 3 5c2.2 2.6 5.6 4.1 9 4-.9-4.2 4-6.6 7-3.8 1.1 0 3-1.2 3-1.2z"></path></svg></a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sora Update #1 (113 pts)]]></title>
            <link>https://blog.samaltman.com/sora-update-number-1</link>
            <guid>45469437</guid>
            <pubDate>Sat, 04 Oct 2025 00:39:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.samaltman.com/sora-update-number-1">https://blog.samaltman.com/sora-update-number-1</a>, See on <a href="https://news.ycombinator.com/item?id=45469437">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main_container">
  <article>
    
  <header>
    

    
  </header>

  <div id="post_body_2228126"><div><p>We have been learning quickly from how people are using Sora and taking feedback from users, rightsholders, and other interested groups. We of course spent a lot of time discussing this before launch, but now that we have a product out we can do more than just theorize.</p><p>We are going to make two changes soon (and many more to come).</p><p>First, we will give rightsholders more granular control over generation of characters, similar to the opt-in model for likeness but with additional controls.</p></div><div><p>We are hearing from a lot of rightsholders who are very excited for this new kind of "interactive fan fiction" and think this new kind of engagement will accrue a lot of value to them, but want the ability to specify how their characters can be used (including not at all). We assume different people will try very different approaches and will figure out what works for them. But we want to apply the same standard towards everyone, and let rightsholders decide how to proceed (our aim of course is to make it so compelling that many people want to). There may be some edge cases of generations that get through that shouldn't, and getting our stack to work well will take some iteration.&nbsp;</p><p>In particular, we'd like to acknowledge the remarkable creative output of Japan--we are struck by how deep the connection between users and Japanese content is!</p><p>Second, we are going to have to somehow make money for video generation. People are generating much more than we expected per user, and a lot of videos are being generated for very small audiences. We are going to try sharing some of this revenue with rightsholders who want their characters generated by users. The exact model will take some trial and error to figure out, but we plan to start very soon. Our hope is that the new kind of engagement is even more valuable than the revenue share, but of course we we want both to be valuable.</p></div><p>Please expect a very high rate of change from us; it reminds me of the early days of ChatGPT. We will make some good decisions and some missteps, but we will take feedback and try to fix the missteps very quickly. We plan to do our iteration on different approaches in Sora, but then apply it consistently across our products.<br></p></div>


  </article>

  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Discord customer service data breach leaks user info and scanned photo IDs (143 pts)]]></title>
            <link>https://www.theverge.com/news/792032/discord-customer-service-data-breach-hack</link>
            <guid>45469436</guid>
            <pubDate>Sat, 04 Oct 2025 00:39:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/news/792032/discord-customer-service-data-breach-hack">https://www.theverge.com/news/792032/discord-customer-service-data-breach-hack</a>, See on <a href="https://news.ycombinator.com/item?id=45469436">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><a href="https://www.theverge.com/authors/jay-peters"><img alt="Jay Peters" data-chromatic="ignore" loading="lazy" width="36" height="36" decoding="async" data-nimg="1" srcset="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195819/JAY_PETERS.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=48 1x, https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195819/JAY_PETERS.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96 2x" src="https://platform.theverge.com/wp-content/uploads/sites/2/chorus/author_profile_images/195819/JAY_PETERS.0.jpg?quality=90&amp;strip=all&amp;crop=0%2C0%2C100%2C100&amp;w=96"></a></p><div><p><span aria-expanded="false" aria-haspopup="true" role="button" tabindex="0"><span id="follow-author-standard_article_details-dmcyOmF1dGhvclByb2ZpbGU6MTIz"><span><span><svg width="9" height="9" viewBox="0 0 9 9" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Follow"><path d="M5 0H4V4H0V5H4V9H5V5H9V4H5V0Z"></path></svg></span></span><span>Jay Peters</span></span></span></p> <p><span>is a senior reporter covering technology, gaming, and more. He joined The Verge in 2019 after nearly two years at Techmeme.</span></p></div></div><div id="zephr-anchor"><p>One of Discord’s third-party customer service providers was compromised by an “unauthorized party,” <a href="https://discord.com/press-releases/update-on-security-incident-involving-third-party-customer-service">the company says</a>. The unauthorized party gained access to “information from a limited number of users who had contacted Discord through our Customer Support and/or Trust &amp; Safety teams” and aimed to “extort a financial ransom from Discord.” The unauthorized party “did not gain access to Discord directly.”</p><p>Data potentially accessed by the hack includes things like names, usernames, emails, and the last four digits of credit card numbers. The unauthorized party also accessed a “small number” of images of government IDs from “users who had appealed an age determination.” Full credit card numbers and passwords were not impacted by the breach, Discord says.</p><p>The company is notifying impacted users now over email. If your ID might have been accessed, Discord will specify that. Discord also says it revoked the support provider’s access to Discord’s ticketing system, has notified data protection authorities, is working with law enforcement, and has reviewed “our threat detection systems and security controls for third-party support providers.”</p><div><p><span><strong>Follow topics and authors</strong> from this story to see more like this in your personalized homepage feed and to receive email updates.</span></p><ul><li id="follow-author-article_footer-dmcyOmF1dGhvclByb2ZpbGU6MTIz"><span aria-expanded="false" aria-haspopup="true" role="button" tabindex="0"><span><span><svg width="9" height="9" viewBox="0 0 9 9" fill="none" xmlns="http://www.w3.org/2000/svg" aria-label="Follow"><path d="M5 0H4V4H0V5H4V9H5V5H9V4H5V0Z"></path></svg></span><span>Jay Peters</span></span></span></li><li></li><li></li><li></li><li></li><li></li></ul></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When private practices merge with hospital systems, costs go up (139 pts)]]></title>
            <link>https://insights.som.yale.edu/insights/when-private-practices-merge-with-hospital-systems-costs-go-up</link>
            <guid>45468781</guid>
            <pubDate>Fri, 03 Oct 2025 22:58:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://insights.som.yale.edu/insights/when-private-practices-merge-with-hospital-systems-costs-go-up">https://insights.som.yale.edu/insights/when-private-practices-merge-with-hospital-systems-costs-go-up</a>, See on <a href="https://news.ycombinator.com/item?id=45468781">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        

  <p>Historically, physicians worked independently of—if in partnership with—hospitals. But increasingly, doctors are selling their practices to hospital systems, as well as private equity firms and insurance companies. What does it mean for patients and what they pay for healthcare when their doctors are employed by a hospital system?‌</p><p>The effects of hospital systems acquiring physician practices are hard to determine, because until now, there has been no comprehensive source of data about these mergers and the effects of integration on pricing can be hard to isolate. New research by Yale SOM economist Fiona Scott Morton tackles these challenges, determining the scale of hospital-physician mergers and examining their effects on pricing. In their study, Scott Morton and her co-authors—Yale economist Zack Cooper; Stuart V. Craig and Ashley T. Swanson of the University of Wisconsin–Madison; Aristotelis Epanomeritakis of Harvard University; Matthew Grennan of Emory University; and Joseph R. Martinez of the University of California, San Francisco—found that these integrations significantly increase prices for consumers, because competition decreases.</p><p>To Scott Morton, these findings are a wake-up call for regulators. “States haven’t been enforcing [antitrust laws] to the extent that they could,” she says. “I think there’s a lot of good that could be done with some incremental enforcement.”‌</p>



<div data-paragraph-insights-ai="">

  <h3>Hey, I’m <em>AI</em>nsights</h3>
  <p>Ask me questions about the article and its underlying research. Let’s chat!</p>
  <p>Ask Insights powered by AI ...</p>

</div>


  <p>Mergers between hospital systems and physicians in private practice are what antitrust experts call “mergers of complements” or “non-horizontal mergers.” These aren’t horizontal mergers, because doctors aren’t competing with hospitals, but they aren’t quite vertical either, because doctors aren’t suppliers to hospitals. ‌</p><p>In general, scholars expect mergers of complements to have positive effects. Imagine, for example, a car maker decides it wants to own the software that will go into its vehicles. Rather than keeping its future design plans a secret from its software maker, “I can talk to them, and they can start thinking about what cool software could be used with my hardware. So we could do things together that we can’t do apart,” Scott Morton explains. Non-horizontal mergers can also reduce prices: “If I mark up my car and you mark up your software, then the [buyer] has two markups to pay, whereas if the hardware buys the software and makes a bundle, then they internalize that problem.”‌</p><p>But Scott Morton and her co-authors suspected healthcare might function differently. For example, there’s no reason to believe healthcare quality would improve after integration; after all, between navigating different insurance companies and referring patients to different specialists and hospitals, “doctors are rather good at providing care across corporate boundaries,” she says. And because healthcare is geographically constrained—patients don’t want to travel long distances to see a doctor—regional consolidation could increase prices. ‌</p><p>To study hospital–physician mergers more closely, the researchers first had to contend with an informational challenge. “Everybody knew, anecdotally, that these transactions were happening and that there were a lot of them,” Scott Morton says, “but we just couldn’t measure them.” Hospital acquisitions of private practices are generally small enough transactions that hospitals aren’t required to report the purchases to regulators.‌</p><p>So, in the absence of a single comprehensive source of data, the team developed a workaround. Using administrative data from Medicare, hospital surveys, physician directories, and filings with the Securities and Exchange Commission, they trained a machine-learning algorithm to identify when a physician had moved from private practice to a hospital system. The researchers tested the algorithm’s results against verified data about mergers and found it achieved 97% accuracy. ‌</p>


<div data-paragraph-pullquote="quotation">
          <blockquote>
            
      <span>  <p>You may say, the increase is only 3%. But this is 3% of a large hospital bill. And healthcare is already 19% of GDP. It’s giant.</p>

</span>

            
          </blockquote>
  </div>


  <p>When they set the algorithm loose on national data from 2008 to 2016, it identified a striking degree of integration—overall, the share of doctors employed by a hospital grew from 27.5% to‌ 47.2% during that span. ‌</p><p>To understand how this integration was affecting prices, the researchers looked at childbirths, which account for a large share of both commercial hospital admissions and healthcare spending. They gathered data on pricing—that is, how much doctors and hospitals were charging insurers—from a large private insurance company. Then the researchers looked at OB-GYNs who had integrated with a hospital system in 2013–2014, and compared the prices those doctors and hospitals charged two years before and two years after the merger. To ensure they were isolating the effects of integration, they also compared these doctors and hospitals to otherwise similar doctors and hospitals that had not integrated. ‌</p><p>Both hospitals and doctors charged more after integration, the researchers discovered: hospital prices for labor and delivery increased by 3.3% ($475, on average), while physician prices increased by 15.1% ($502, on average).‌</p><p>These are “substantial price increases,” Scott Morton says. “You may say, it’s only 3%. But this is 3% of a large hospital bill. And healthcare is already 19% of GDP. It’s giant.”‌</p><p>And this uptick in price did not reflect an improvement in healthcare quality as far as the researchers can determine. After examining common measures of quality for childbirths, including hospital readmission and C-section rates, the researchers found no improvements in the years after integration.‌</p>





  <p>As the researchers continued analyzing their results, they found statistical signs of three different anticompetitive mechanisms that explained the price increases they observed.‌</p><p>The first of these is called foreclosure. Before integration, an OB-GYN in solo practice might refer their patients to several hospitals in the area for labor and delivery. After integration, however, the doctor might feel pressured to steer their patients to deliver within their hospital system. “That’s foreclosing those doctors and patients from choosing between hospitals,” Scott Morton explains. This gives the hospital and the doctor more power in the market, and therefore the ability to raise prices. ‌</p><p>The second is called recapture. If a physician in private practice is charging too much, an insurance company might refuse to include them in their network. “After the merger, tossing out the physician means tossing out the hospital,” she says. This might cause consumers to switch insurers so they can remain with their preferred doctor and hospital—“so a physician will recapture those people.” Once again, the doctor and the hospital can get away with raising prices. ‌</p><p>The third mechanism is simply market concentration. Over time, if a hospital system acquires enough private practices, “there is a horizontal impact of these [non-horizontal] mergers,” Scott Morton explains. “If the hospital already owns a couple of obstetricians, and buys another group, then those obstetricians become horizontally joined….and we find prices go up for that reason.” ‌</p><p>The researchers found further evidence of reduced competition when they looked at another group of doctors. Prices for doctors who had <em>already </em>integrated with a hospital system increased by 9% after the hospital acquired more doctors in their specialty. Nothing changed for these already-integrated doctors; they were the same doctors working at the same hospitals. The only plausible explanation for their ability to charge higher prices is a reduction in competition. ‌</p><p>To Scott Morton, these results call for renewed attention from regulators. States could, for example, require medical organizations to submit information about their ownership and physician affiliates, which would make it far easier to track hospital-physician mergers. Regulators could also require a waiting period before such mergers, “so that the state can assess for itself whether that transaction would be harmful.” A small task force at the federal level could even advise state regulators in making these assessments. ‌</p><p>And it’s important, she argues, to draw attention to the larger effects of small transactions in local markets: “States as well as citizens care a lot about having those prices be low and markets be efficient and working well.” ‌</p>



    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Zig builds are getting faster (356 pts)]]></title>
            <link>https://mitchellh.com/writing/zig-builds-getting-faster</link>
            <guid>45468698</guid>
            <pubDate>Fri, 03 Oct 2025 22:45:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mitchellh.com/writing/zig-builds-getting-faster">https://mitchellh.com/writing/zig-builds-getting-faster</a>, See on <a href="https://news.ycombinator.com/item?id=45468698">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Andrew Kelley famously (or infamously, depending on your views) said
"the compiler is too damn slow, that's why we have bugs."<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup></p>
<p>As a result, one of the primary stated goals of <a target="_blank" rel="noopener noreferrer" href="https://ziglang.org/">Zig</a> for
years has been faster compile times. The Zig team has been working on
extremely hard problems to make this a reality
(such as <a target="_blank" rel="noopener noreferrer" href="https://github.com/ziglang/zig/issues/16270">yeeting LLVM</a>,
writing their own <a target="_blank" rel="noopener noreferrer" href="https://ziglang.org/download/0.15.1/release-notes.html#Compiler">code generation backends</a>,
building their own <a target="_blank" rel="noopener noreferrer" href="https://github.com/ziglang/zig/pull/25299">linkers</a>,
and marching towards <a target="_blank" rel="noopener noreferrer" href="https://ziglang.org/download/0.15.1/release-notes.html#Incremental-Compilation">incremental compilation</a> in general).<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup></p>
<p>The fruits of this multi-year labor are finally starting to show
with Zig 0.15.1. The <a target="_blank" rel="noopener noreferrer" href="https://ghostty.org/">Ghostty</a> project just completed upgrading to
Zig 0.15.1, and I'd like to share some real-world build times.<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup></p>
<hr>
<h2 id="build-script-compilation">Build Script Compilation</h2>
<ul>
<li>Zig 0.14: 7sec 167ms</li>
<li>Zig 0.15: 1sec 702ms</li>
</ul>
<p>This is the time it takes to build the <code>build.zig</code> script itself. The
times above were measured by running <code>zig build --help</code>.</p>
<p>A well-written build script should only rebuild itself rarely. However,
this is a cost every new uncached source build will pay (e.g. a user
downloading the project to build from source one time). As such, it directly
impacts the time to build a usable binary.</p>
<hr>
<h2 id="full-uncached-ghostty-binary">Full Uncached Ghostty Binary</h2>
<ul>
<li>Zig 0.14: 41sec</li>
<li>Zig 0.15: 32sec</li>
</ul>
<p>This includes the time to build the build script itself. Given the prior
results, Zig 0.15 is building everything else ~2 seconds faster. But, you
can still see in wall time the change in this initial build time.</p>
<div><p><strong>Important: most of this is still using LLVM.</strong> Ghostty still can't fully build and link using
the self-hosted x86_64 backend, since the backend still has bugs. So, this just shows the general
improvements in the Zig compiler itself, even with LLVM in the picture.</p></div>
<p>Once Ghostty can use the self-hosted x86_64 backend completely, I expect
this time to plummet to around 25 seconds or less, fully half the time it
would take with Zig 0.14.</p>
<hr>
<h2 id="incremental-build-ghostty-executable">Incremental Build (Ghostty Executable)</h2>
<ul>
<li>Zig 0.14: 19sec</li>
<li>Zig 0.15: 16sec</li>
</ul>
<p>This is the time it takes to rebuild Ghostty after a one-line change to
the most core terminal emulation code (adding a log function call to the
escape sequence parser).</p>
<p>This build has a fully cached build script and dependency graph, so it
is only rebuilding what it needs to. Incremental compilation in Zig isn't
functional yet, so this still recompiles a considerable amount of code.
Additionally, as with the prior section, <strong>this is still using LLVM</strong>.
By simply dropping LLVM out of the picture, I expect this time to drop to
around 12 seconds or so (less the time LLVM is emitting).</p>
<p>Going further, once Zig supports incremental compilation, I expect
we'll be able to measure incremental builds like this within milliseconds
at worst. But, let's wait and see when that is reality.</p>
<hr>
<h2 id="incremental-build-libghostty-vt">Incremental Build (libghostty-vt)</h2>
<ul>
<li>Zig 0.14: 2sec 884ms</li>
<li>Zig 0.15: 975ms</li>
</ul>
<p>This is the time it takes to rebuild only <a href="https://mitchellh.com/writing/libghostty-is-coming">libghostty-vt</a>
after a one-line change. Unlike the Ghostty executable, <code>libghostty-vt</code>
is fully functional with the self-hosted x86_64 backend, so this
shows the differences in build times without LLVM in the picture.</p>
<p>Similar to the Ghostty executable, this is still rebuilding the full
Zig module for <code>libghostty-vt</code>, since incremental compilation isn't
fully functional yet. I expect this to also drop to single-digit milliseconds
at worst once incremental compilation is a reality.</p>
<p>But still, a sub-second build time for a non-trivial library is <em>amazing</em>.
This is the library I'm spending most of my time working on right now,
and even in a few short days since upgrading to Zig 0.15.1, I've felt
a huge difference in my workflow. Previously, I might tab out to read an
email between builds or tests, but now its so fast I can stay in flow
in my terminal.</p>
<p><strong>This improvement is most indicative of what's to come in the short term.</strong>
The self-hosted x86_64 backend is already stable enough to build all debug
builds by default and the aarch64 backend is getting there, too. We aren't
able to build the full Ghostty executable yet, but I bet this will get
ironed out within months.</p>
<hr>
<h2 id="faster-builds-are-here">Faster Builds Are Here</h2>
<p>As you can see, building Ghostty with Zig 0.15.1 is faster in every single
scenario, despite the fact that a lot of Ghostty still can't even take
advantage of the self-hosted backend! And despite the fact that incremental
compilation isn't functional yet!</p>
<p>I've loved betting on Zig for Ghostty, and I love that they're focusing
on compile times. These improvements are real, and they're here now. And
I suspect in the next couple years, the results posted today will look
downright slow. 😜</p>
<section data-footnotes="true">
<ol>
<li id="user-content-fn-1">
<p>Timestamped link: <a target="_blank" rel="noopener noreferrer" href="https://youtu.be/5eL_LcxwwHg?t=565">https://youtu.be/5eL_LcxwwHg?t=565</a> <a href="#user-content-fnref-1" data-footnote-backref="true" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-2">
<p>This ignores an astronomical amount of work that has gone into
making every aspect of the Zig compiler faster, more parallelizable, etc. <a href="#user-content-fnref-2" data-footnote-backref="true" aria-label="Back to content">↩</a></p>
</li>
<li id="user-content-fn-3">
<p>All measurements done on the same x86_64 Linux machine. <a href="#user-content-fnref-3" data-footnote-backref="true" aria-label="Back to content">↩</a></p>
</li>
</ol>
</section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Interstellar Object 3I/Atlas Passed Mars Last Night (169 pts)]]></title>
            <link>https://earthsky.org/space/new-interstellar-object-candidate-heading-toward-the-sun-a11pl3z/</link>
            <guid>45467543</guid>
            <pubDate>Fri, 03 Oct 2025 20:40:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://earthsky.org/space/new-interstellar-object-candidate-heading-toward-the-sun-a11pl3z/">https://earthsky.org/space/new-interstellar-object-candidate-heading-toward-the-sun-a11pl3z/</a>, See on <a href="https://news.ycombinator.com/item?id=45467543">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure id="attachment_523642" aria-describedby="caption-attachment-523642"><img fetchpriority="high" decoding="async" src="https://earthsky.org/upl/2025/09/3IATLAS-at-Mars-oct2-2025-NASA-simulation-2-e1759437049547.jpeg" alt="A red line indicating Mars' orbit, and a dot indicating Mars. Nearby is a bright comet with a long tail." width="800" height="450"><figcaption id="caption-attachment-523642">Here is interstellar object 3I/ATLAS approaching Mars on October 2, 2025. This simulated image is from <a href="https://eyes.nasa.gov/apps/solar-system/#/home" rel="noopener" target="_blank">NASA’s Eyes on the Solar System tool</a>.</figcaption></figure>
<p>The world’s 3rd known interstellar object – 3I/ATLAS – has made its closest approach to Mars. The approach took place at <a href="https://earthsky.org/astronomy-essentials/universal-time/" rel="noopener" target="_blank">4 UTC</a> on October 3, 2025 (11 p.m. CDT on October 2). At that time, the comet was approximately 18 million miles (29 million kilometers) from Mars. It was the object’s closest approach to any planet during its one-time journey through our solar system. </p>
<p>As of this writing (10 UTC on October 3), we have not seen any new images from the pass. But multiple space agencies, including NASA and the European Space Agency (ESA), are coordinating observations using various spacecraft and orbiters around Mars. Instruments on ESA’s Mars Express and ExoMars Trace Gas Orbiter, as well as NASA’s Mars Reconnaissance Orbiter, are focusing on capturing detailed data from this interstellar visitor. In an <a href="https://apnews.com/article/interstellar-comet-nasa-b9df6568efee22c02b722a87377d30db" rel="noopener" target="_blank">October 2 story from AP</a>, Marcia Dunn reported: </p>
<blockquote><p>Both of the European Space Agency’s satellites around Mars are already aiming their cameras at the comet, which is only the 3rd interstellar object known to have passed our way. NASA’s satellite and rovers at the red planet are also available to assist in the observations.</p></blockquote>
<p>Previously, <a href="https://www.linkedin.com/in/tmeubanks/" rel="noopener" target="_blank">Marshall Eubanks</a> of Space Initiatives had <a href="https://groups.io/g/mpml/message/40919" rel="noopener" target="_blank">said</a>: </p>
<blockquote><p>During the Mars close approach, the Mars Reconnaissance Orbiter will observe 3I with HiRISE, observing between 1 – 4 a.m. on October 2, and the CaSSIS camera on ESA’s Trace Gas Orbiter and the Mars Express’ High Resolution Stereo Camera will be observing on October 3.</p></blockquote>
<p>3I/ATLAS will reach perihelion, its closest point to the sun, on October 29, 2025. Its perihelion  distance will be roughly 1.36 astronomical units (<a href="https://earthsky.org/space/what-is-the-astronomical-unit/" rel="noopener" target="_blank">AU</a>) from the sun – just inside the orbit of Mars.</p>
<p>If you’re interested in tracking the object, <a href="https://eyes.nasa.gov/apps/solar-system/#/home" rel="noopener" target="_blank">NASA’s Eyes on the Solar System tool</a> offers interactive simulations of its path. Also, <a href="https://science.nasa.gov/solar-system/comets/3i-atlas/" rel="noopener" target="_blank">NASA just launched a new page devoted to 3I/ATLAS</a>. And the <a href="https://www.esa.int/Science_Exploration/Space_Science/ESA_observations_of_interstellar_comet_3I_ATLAS" rel="noopener" target="_blank">latest updates on ESA observations are here</a></p>
<p>Please note that 3I/ATLAS will not be visible to the unaided eye from Earth at this Mars approach, or at any time. It will be possible to view the object with 8-inch (20 cm) or larger telescopes … but the best time for that won’t come until November. If you spot it then, you’ll be in good company. Between November 2 and 25, ESA’s Jupiter Icy Moons Explorer (<a href="https://www.esa.int/Science_Exploration/Space_Science/Juice" rel="noopener" target="_blank">Juice</a>) will be observing the comet with various instruments. As Juice looks towards 3I/ATLAS so soon after its closest approach to the sun, it is likely to have the best view of the comet in a very active state, with a bright halo around its nucleus and a long tail stretching out behind it.</p>
<figure id="attachment_523646" aria-describedby="caption-attachment-523646"><img decoding="async" src="https://earthsky.org/upl/2025/10/3IATLAS-map-from-ESA-scaled-e1759438762972.jpeg" alt="A map showing the path of 3I/ATLAS, and the orbits of Earth, Mars and Juice spacecraft." width="800" height="448"><figcaption id="caption-attachment-523646"><a href="https://earthsky.org/upl/2025/10/3IATLAS-map-from-ESA-lg-scaled.jpeg" rel="noopener" target="_blank">View larger</a>. | Infographic showing the path of comet 3I/ATLAS, the 3rd interstellar object known to enter our solar system. It displays the orbits of Earth, Mars and ESA’s Juice spacecraft around the sun, along with key dates and events as comet 3I/ATLAS travels through the inner solar system in 2025. Colored and numbered dots mark important observation points by telescopes and spacecraft. Image via <a href="https://www.esa.int/ESA_Multimedia/Images/2025/09/ESA_s_Mars_and_Jupiter_missions_observe_comet_3I_ATLAS" rel="noopener" target="_blank">ESA</a>.</figcaption></figure>
<h3>Interstellar object 3I/ATLAS: A look backward</h3>
<p>Where did 3I/ATLAS come from? We know it came from the Sagittarius direction in our sky; that is, it came from the direction of the center of our <a href="https://earthsky.org/astronomy-essentials/what-is-the-milky-way-galaxy/" rel="noopener" target="_blank">Milky Way</a> galaxy. But there are billions of stars in that direction. Which one is the home system of this object?</p>
<p>There have been many studies and ideas. One team of scientists, led by <a href="https://scholar.google.com/citations?user=_jw3KtcAAAAJ&amp;hl=en" rel="noopener" target="_blank">Xabier Pérez-Couto</a> of the University of A Coruña in Spain, traced the path of interstellar object 3I/ATLAS back 10 million years. The astronomers were seeking its origin star, or any stars that might have perturbed its path as it traveled from its point of origin to our solar system.</p>
<p>The researchers examined 3I/ATLAS’s trajectory with the help of the <a href="https://earthsky.org/space/live-whats-phil-thinking-of-now-gaia/" rel="noopener" target="_blank">Gaia space observatory’s</a> data on stars. For 12 years, Gaia collected data on billions of stars in our Milky Way galaxy, precisely noting their positions again and again and thereby determining their motions. These astronomers’ calculations took them more than 100 million astronomical units (AU, or Earth-sun units) from our solar system. With these data in hand, researchers said they identified 93 nominal “encounters” for 3I/ATLAS, 62 of which were “significant.” Yet, they found that none of those encounters produced any meaningful perturbation of ATLAS’s orbit. </p>
<p>So, in other words, all of those 93 (or 62) encounters happened too fast, with the stars too far from 3I/ATLAS to meaningfully impact its trajectory. In the end, they didn’t find a star along 3I/ATLAS’s path that might have been responsible for bringing this 3rd-known interstellar object to us. </p>
<figure id="attachment_521515" aria-describedby="caption-attachment-521515"><img decoding="async" src="https://earthsky.org/upl/2025/08/Comet-3I-ATLAS-Gemini-NOIRLab-e1757602883278.jpg" alt="Many short streaks in rainbow colors, and a larger, round, fuzzy white object with a tail." width="650" height="778"><figcaption id="caption-attachment-521515">Here’s one of the newest images of interstellar object Comet 3I/ATLAS. The Gemini Multi-Object Spectrograph (GMOS) on the Gemini South telescope at Cerro Pachón in Chile captured this image, which NOIRLab released on September 4, 2025. The colors of the background stars are due to 4 filters. The comet was fixed in the center of the telescope’s field of view, while the positions of the background stars changed, showing streaks. Image via International Gemini Observatory/ <a href="https://noirlab.edu/public/images/noirlab2525a/" rel="noopener" target="_blank">NOIRLab</a>/ NSF/ AURA/ Shadow the Scientist. Image Processing: J. Miller &amp; M. Rodriguez (International Gemini Observatory/NSF NOIRLab), T.A. Rector (University of Alaska Anchorage/NSF NOIRLab), M. Zamani (NSF NOIRLab).</figcaption></figure>
<h3>Tracing 3I/ATLAS’ path, a daunting task</h3>
<p>And, as you might imagine, tracing 3I/ATLAS’ path backward through the galaxy is a daunting task. That’s in part because small uncertainties in orbits and stellar motions grow rapidly over time. But based on the researchers’ analyses of the interstellar object’s vertical motion in the galaxy (its path is known to weave up and down in the galactic disk), they concluded that it likely originated from the Milky Way’s <em>thin disk</em>, not its <em>thick disk</em> as was mentioned some months ago. The thin disk contains somewhat younger objects than the thick disk. But the researchers’ paper <a href="https://arxiv.org/pdf/2509.07678" rel="noopener" target="_blank">said</a>: </p>
<blockquote><p>[3I/ATLAS] may nonetheless be an old object, consistent with ejection from a long-lived primordial planetesimal disk in an early-formed system.</p></blockquote>
<p>The scientists <a href="https://arxiv.org/pdf/2509.07678" rel="noopener" target="_blank">published</a> their not-yet <a href="https://earthsky.org/human-world/what-is-peer-review/" rel="noopener" target="_blank">peer-reviewed</a> paper on arXiv on September 10, 2025.</p>
<figure id="attachment_521516" aria-describedby="caption-attachment-521516"><img loading="lazy" decoding="async" src="https://earthsky.org/upl/2025/08/Milky-way-edge-on-thin-thick-disk-Gaba-P-Wikimedia-Commons-e1757603557980.jpg" alt="Large gray oval, thinner yellow one, and almost flat blue one, and a red bulge in the middle." width="800" height="590"><figcaption id="caption-attachment-521516">This diagram shows the thin disk of the <a href="https://earthsky.org/astronomy-essentials/what-is-the-milky-way-galaxy/" rel="noopener" target="_blank">Milky Way</a> in teal and the thick disk in yellow. A <a href="https://arxiv.org/pdf/2509.07678" rel="noopener" target="_blank">new paper says</a> Comet 3I/ATLAS likely came from the Milky Way’s thin disk. Image via Gaba P/ <a href="https://en.m.wikipedia.org/wiki/File:Milky-way-edge-on.pdf" rel="noopener" target="_blank">Wikipedia</a> (<a href="https://creativecommons.org/licenses/by-sa/3.0/deed.en" rel="noopener" target="_blank">CC BY-SA 3.0</a>).</figcaption></figure>
<h3>Unlocking galactic mysteries with 3I/ATLAS</h3>
<p>3I/ATLAS is thought to have been drifting through interstellar space for many billions of years before encountering our solar system. Pérez-Couto and team said that the interstellar comet is a: </p>
<blockquote><p>… key probe of the galactic population of icy planetesimals.</p></blockquote>
<p>In other words, the formation of solar systems is a messy process. In a solar system’s earliest days, rocks and pockets of gas and dust bang into each other and get swept up into clumps, which eventually get big enough to begin gathering yet more rocks, gas and dust to themselves via the force of gravity. Thus, planets come to be, astronomers think. According to theories of planet formation, clearing processes are also common, and those sometimes involve material – often the outer, icy regions of debris – getting ejected from a system altogether. As the paper said: </p>
<blockquote><p>… interstellar space should be filled with planetesimals.</p></blockquote>
<figure id="attachment_522066" aria-describedby="caption-attachment-522066"><img loading="lazy" decoding="async" src="https://earthsky.org/upl/2025/09/3I-ATLAS-brigtening-more-quickly-Sep-2025-COBS.png" alt="Graph with hashmarks mostly along an ascending line, with the latest hashmarks far above the line." width="800" height="740" srcset="https://earthsky.org/upl/2025/09/3I-ATLAS-brigtening-more-quickly-Sep-2025-COBS.png 679w, https://earthsky.org/upl/2025/09/3I-ATLAS-brigtening-more-quickly-Sep-2025-COBS-300x277.png 300w" sizes="auto, (max-width: 800px) 100vw, 800px"><figcaption id="caption-attachment-522066">In mid-September, the data at the Comet Observation database (COBS), maintained by Crni Vrh Observatory, showed that interstellar comet 3I/ATLAS was outperforming expectations. It was brightening faster than expected. Image via <a href="https://www.cobs.si/comet/2643/" rel="noopener" target="_blank">COBS</a> (<a href="https://creativecommons.org/licenses/by/4.0/deed.en" rel="noopener" target="_blank">CC BY 4.0</a>).</figcaption></figure>
<h3>Other possibilities</h3>
<p>Plus, there are other ways these interstellar interlopers might have achieved their lonely paths through our Milky Way galaxy. The possibilities range from close passages of other stars to tidal fragmentation of comets. So, as the paper said: </p>
<blockquote><p>Identifying the origin of interstellar objects is key to understanding planet formation efficiency, the distribution of volatiles and organics in the galaxy, and the dynamical pathways by which planetary systems evolve.</p></blockquote>
<p>All that from a small chunk of icy stuff (we know it’s icy in part because 3I/ATLAS has formed a tail, as icy comets do)!</p>
<h3>EarthSky interview with Colin Orion Chandler</h3>
<p>On August 7, 2025, NASA <a href="https://esahubble.org/news/heic2509/?lang" rel="noopener" target="_blank">shared</a> an updated estimate of the size of the object’s nucleus, or core. Shortly after the object was first identified on July 1, 2025, 3I/ATLAS was estimated to have a diameter of about 12 miles (20 km). Then in late July – using data from the new Vera C. Rubin Observatory in Chile – the size estimate dropped to 6 miles (10 km). The latest analysis uses data from the NASA/ESA Hubble Space Telescope. It reduces the estimated diameter of 3I/ATLAS’s nucleus still further, to 3.5 miles (5.6 km). </p>
<p>And, the astronomers using Hubble data said, the object could be even smaller, as small as 1,050 feet (320 meters) across!</p>
<p>EarthSky’s <a href="https://earthsky.org/author/deborahbyrd/" rel="noopener" target="_blank">Deborah Byrd</a> interviewed <a href="https://dirac.astro.washington.edu/person/colin-orion-chandler/" rel="noopener" target="_blank">Colin Orion Chandler</a> of the DiRAC Institute of the University of Washington about size estimates for 3I/ATLAS. Watch in the player below, or <a href="https://www.youtube.com/watch?v=1a0zplLaXVQ" rel="noopener" target="_blank">on YouTube</a>.</p>
<p><iframe loading="lazy" width="560" height="315" src="https://www.youtube.com/embed/1a0zplLaXVQ?si=EMOnXVFNMDLtT-Sj" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>By the way, the two previously known interstellar objects are <a href="https://earthsky.org/space/oumuamua-outgassing-comet-strange-orbit/" rel="noopener" target="_blank">1I/ ‘Oumuamua</a> and <a href="https://earthsky.org/space/2i-borisov-pristine-comet-interstellar-visitor/" rel="noopener" target="_blank">2I/Borisov</a>. ‘Oumuamua’s size is thought to be about 656 feet (200 meters) across at its widest (you’ll recall it has an elongated shape). And Borisov is thought to be less than 3,280 feet (1 km) across.</p>
<figure id="attachment_518121" aria-describedby="caption-attachment-518121"><img loading="lazy" decoding="async" src="https://earthsky.org/upl/2025/08/Comet-3I-ATLAS-from-Hubble-NASA-Aug-7-2025-e1754580700418.jpg" alt="Interstellar object: Star field with short streaks and a larger blue, fuzzy object at center." width="800" height="590"><figcaption id="caption-attachment-518121"><a href="https://cdn.esahubble.org/archives/images/screen/heic2509a.jpg" rel="noopener" target="_blank">View larger</a>. | The Hubble Space Telescope captured this image of interstellar object 3I/ATLAS on July 21, 2025. It was the sharpest image yet of the object, acquired when the object was 226 million miles (364 million km) from Earth. This object is now generally believed to be a comet by most astronomers. In this image, a comet-like, teardrop-shaped cocoon of dust can be seen coming off its solid, presumably icy nucleus. Image via <a href="https://science.nasa.gov/missions/hubble/as-nasa-missions-study-interstellar-comet-hubble-makes-size-estimate/" rel="noopener" target="_blank">NASA</a>/ ESA/ D. Jewitt (UCLA); Image Processing: J. DePasquale (STScI).</figcaption></figure>
<h3>An early EarthSky interview with Matthew Hopkins</h3>
<p>Shortly after the discovery of 3I/ATLAS – on July 1, 2025 – astronomers were saying it was likely the oldest comet we’ve ever seen. That claim came from University of Oxford astronomer <a href="https://www.physics.ox.ac.uk/our-people/hopkinsm" rel="noopener" target="_blank">Matthew Hopkins</a>, whose analysis suggested 3I/ATLAS might be more than 7 billion years old, predating our solar system by more than 3 billion years! Hear him explain in the player below, or <a href="https://www.youtube.com/watch?v=8rndCM-B5QU&amp;list=PLcwd1eS7Gpr6STVy5i2hzF3yvS5rFEEPC&amp;index=3&amp;t=173s" rel="noopener" target="_blank">on YouTube</a>.</p>
<p><iframe loading="lazy" width="560" height="315" src="https://www.youtube.com/embed/8rndCM-B5QU?si=e4tfgshx0uXxaDew" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<h3>EarthSky interview with Colin Snodgrass</h3>
<p>Scientists first spotted 3I/ATLAS in early July 2025. And since then, one question has been asked countless times: will we send out a spacecraft to take a closer look? EarthSky’s <a href="https://earthsky.org/author/will-triggs/" rel="noopener" target="_blank">Will Triggs</a> spoke to University of Edinburgh astronomer <a href="https://www.roe.ac.uk/~csn/" rel="noopener" target="_blank">Colin Snodgrass</a> on August 21, 2025, to find out the answer. Colin essentially said, no, we don’t have time to organize a space mission specifically for 3I/ATLAS. But he talked about a future mission, the European Space Agency’s <a href="https://www.esa.int/Science_Exploration/Space_Science/Comet_Interceptor" rel="noopener" target="_blank">Comet Interceptor</a>. This upcoming spacecraft will be primed to intercept future interstellar objects. Watch Will’s interview with Colin in the player below, <a href="https://www.youtube.com/watch?v=3U_OBw84v_E" rel="noopener" target="_blank">or on YouTube</a>.</p>
<p><iframe loading="lazy" width="1383" height="494" src="https://www.youtube.com/embed/3U_OBw84v_E" title="Will We Send A Spacecraft To 3I/ATLAS?" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>It’s worth noting that the behavior of 3I/ATLAS is much like the signature of previously seen sun-bound comets originating within our solar system. But 3I/ATLAS is moving <em>fast</em>. In fact, it’s traveling through our solar system at roughly 130,000 miles per hour (210,000 kph). That’s the highest velocity ever recorded for a solar system visitor. </p>
<h3>How they spotted interstellar object 3I/ ATLAS</h3>
<p>The Asteroid Terrestrial-impact Last Alert System (ATLAS) – a system of survey telescopes – detected our new interstellar visitor on July 1, 2025. And the Minor Planet Center <a href="https://minorplanetcenter.net/mpec/K25/K25N12.html" rel="noopener" target="_blank">confirmed</a> its interstellar nature the following day (July 2, 2025), naming it 3I/ATLAS (or C/2025 N1). The “3I” means it’s the 3rd interstellar visitor that we’ve found. Its trajectory and speed revealed it as an object not from our solar system, but from another star system.</p>
<p>The Hubble Space Telescope imaged the object on July 21, 2025. See the post from Bluesky below.</p>
<blockquote data-bluesky-uri="at://did:plc:bxxv3ty2lwpzyivx3axvq3fy/app.bsky.feed.post/3luiwnar3j22o" data-bluesky-cid="bafyreiewkfci3k2went7epuhyrnpnc2ztvgzr5etol5sj36ctomsp5fisi" data-bluesky-embed-color-mode="system">
<p lang="en">Hubble Space Telescope images of interstellar comet 3I/ATLAS are out! These were taken 5 hours ago. Plenty of cosmic rays peppering the images, but the comet's coma looks very nice and puffy. Best of luck to the researchers trying to write up papers for this…  archive.stsci.edu/proposal_sea… ?</p>
<p><a href="https://bsky.app/profile/did:plc:bxxv3ty2lwpzyivx3axvq3fy/post/3luiwnar3j22o?ref_src=embed">[image or embed]</a></p>
<p>— astrafoxen (<a href="https://bsky.app/profile/did:plc:bxxv3ty2lwpzyivx3axvq3fy?ref_src=embed">@astrafoxen.bsky.social</a>) <a href="https://bsky.app/profile/did:plc:bxxv3ty2lwpzyivx3axvq3fy/post/3luiwnar3j22o?ref_src=embed">July 21, 2025 at 4:28 PM</a></p></blockquote>

<figure id="attachment_515568" aria-describedby="caption-attachment-515568"><img loading="lazy" decoding="async" src="https://earthsky.org/upl/2025/07/side-diagram-MW-3I-Atlas-and-sun-RAS-e1752157111310.png" alt="Side view of a thin disk with a yellow line straight across it and a red line wavering up and down." width="800" height="200"><figcaption id="caption-attachment-515568"><a href="https://ras.ac.uk/sites/default/files/2025-07/Fig5-map_edge.png" rel="noopener" target="_blank">View larger</a>. | A side-on view of the Milky Way, showing the estimated orbits of both our sun and Comet 3I/ATLAS. In this artist’s concept, 3I/ATLAS is the red dashed line, and the sun’s path is the yellow dotted line. The large extent of 3I’s orbit vertically into the outer thick disk is clear. Meanwhile, the sun stays nearer the <a href="https://en.wikipedia.org/wiki/Galactic_plane" rel="noopener" target="_blank">plane</a> of the galaxy. Image via <a href="https://ras.ac.uk/media/2045" rel="noopener" target="_blank">Royal Astronomical Society</a>/ M. Hopkins/ Otautahi-Oxford team. Base map: ESA/ Gaia/ DPAC, Stefan Payne-Wardenaar (<a href="https://creativecommons.org/licenses/by/4.0/deed.en" rel="noopener" target="_blank">CC BY 4.0</a>).</figcaption></figure>
<figure id="attachment_516091" aria-describedby="caption-attachment-516091"><img loading="lazy" decoding="async" src="https://earthsky.org/upl/2025/07/Interstellar-comet-3I-ATLAS-NOIRLab-e1752671431441.jpg" alt="Left side showing a multicolored streak of light in a starfield, right side is closeup of the fuzzy object." width="800" height="454"><figcaption id="caption-attachment-516091">On the left, the interstellar object Comet 3I/ATLAS streaks across a dense star field as seen by the Gemini North telescope in Hawaii. The colors are courtesy of 3 filters: red, green and blue. On the right, an inset shows the comet’s compact coma, or cloud of gas and dust surrounding its icy nucleus. NOIRLab released this image on July 15, 2025. Image via International Gemini Observatory/ <a href="https://noirlab.edu/public/images/noirlab2522a/" rel="noopener" target="_blank">NOIRLab</a>/ NSF/ AURA/ K. Meech (IfA/U. Hawaii). Image processing: Jen Miller &amp; Mahdi Zamani (NSF NOIRLab).</figcaption></figure>
<h3>It’s still heading sunward</h3>
<p>Our new visitor will get its closest to the sun – at about 2 astronomical units (AU), or twice as far as Earth is from the sun – in October. As it reaches perihelion – its closest point to the sun – it will be traveling at almost <a href="https://noirlab.edu/public/news/noirlab2522/" rel="noopener" target="_blank">15,500 miles per hour</a> (25,000 kph).</p>
<p>The speedy nature of Comet 3I/ATLAS is another indication of its interstellar nature. It has to be moving at a blistering pace in order to escape the sun’s gravitational pull.</p>
<p>Marshall Eubanks, a physicist and Very-long-baseline interferometry radio astronomer and co-founder of Space Initiatives, said the comet will come within about 0.4 AU of Mars in October. That would make it just barely observable by the Mars Reconnaissance Orbiter.</p>
<figure id="attachment_515127" aria-describedby="caption-attachment-515127"><img loading="lazy" decoding="async" src="https://earthsky.org/upl/2025/07/ESA_observes_interstellar_comet_3I_ATLAS_article-ezgif.com-gif-to-webp-converter.webp" alt="Animation of white splotches moving past, with one smaller white blob staying still in the middle." width="800" height="800" srcset="https://earthsky.org/upl/2025/07/ESA_observes_interstellar_comet_3I_ATLAS_article-ezgif.com-gif-to-webp-converter.webp 700w, https://earthsky.org/upl/2025/07/ESA_observes_interstellar_comet_3I_ATLAS_article-ezgif.com-gif-to-webp-converter-300x300.webp 300w, https://earthsky.org/upl/2025/07/ESA_observes_interstellar_comet_3I_ATLAS_article-ezgif.com-gif-to-webp-converter-150x150.webp 150w, https://earthsky.org/upl/2025/07/ESA_observes_interstellar_comet_3I_ATLAS_article-ezgif.com-gif-to-webp-converter-400x400.webp 400w, https://earthsky.org/upl/2025/07/ESA_observes_interstellar_comet_3I_ATLAS_article-ezgif.com-gif-to-webp-converter-600x600.webp 600w" sizes="auto, (max-width: 800px) 100vw, 800px"><figcaption id="caption-attachment-515127">3I/ATLAS, the white spot in the center, is approximately 42 million miles (68 million km) from the sun and will make its closest approach in late October 2025, passing just inside the orbit of Mars. It might be up to 7 miles (11 km) wide. It poses no danger to Earth, coming no closer than 150 million miles (240 million km), which is more than 1.5 astronomical units (<a href="https://earthsky.org/space/what-is-the-astronomical-unit" rel="noopener" target="_blank">AU</a>, or distance from the Earth to the sun). Image via <a href="https://www.esa.int/Space_Safety/Planetary_Defence/ESA_tracks_rare_interstellar_comet" rel="noopener" target="_blank">ESA</a>.</figcaption></figure>
<figure id="attachment_514918" aria-describedby="caption-attachment-514918"><img loading="lazy" decoding="async" src="https://earthsky.org/upl/2025/07/interstellar-object-A11pI3Z-Filipp-Romanov-Deep-Sky-Chile-at-Rio-Hurtado-Valley-July-2-2025-e1751459157176.jpg" alt="Interstellar object: Black background with some white oblong shapes and a dim white round shape at center with the label A11pI3Z." width="800" height="536"><figcaption id="caption-attachment-514918"><a href="https://ecp.earthsky.org/community-photos/entry/76471/" rel="noopener" target="_blank">View at EarthSky Community Photos</a>. | <a href="https://ecp.earthsky.org/community-photos/?filter_1_3=Filipp&amp;filter_1_6=Romanov&amp;mode=all" rel="noopener" target="_blank">Filipp Romanov</a> captured the interstellar object on July 2, 2025, when it was still named A11pI3Z. Filipp wrote: “I confirmed new interstellar object candidate A11pl3Z remotely using iTelescope.Net T72 (0.51-m f/6.8 reflector + CCD) in Chile.” Thank you, Filipp!</figcaption></figure>
<h3>Morning star charts here</h3>
<p>After Comet 3I/ATLAS makes its close approach to the sun, you can find it in the morning sky.</p>
<figure id="attachment_516269" aria-describedby="caption-attachment-516269"><img loading="lazy" decoding="async" src="https://earthsky.org/upl/2025/07/Location-of-new-interstellar-comet-3I-Atlas-on-11-22-2025-around-5-am-CT-passing-very-close-to-where-we-see-galaxy-NGC-4454-in-Virgo-Eddie-Irizarry-Stellarium-e1752842707714.jpg" alt="Star chart showing red marks for comet location and a tiny, dim red circle just above it for galaxy NGC 4454." width="800" height="574"><figcaption id="caption-attachment-516269">This chart is for 5 a.m. CST on November 22, 2025. On this date, the comet will pass very close to where we see galaxy NGC 4454 in the constellation <a href="https://earthsky.org/constellations/virgo-heres-your-constellation/" rel="noopener" target="_blank">Virgo</a> the Maiden. Image via Eddie Irizarry/ <a href="https://stellarium-web.org/" rel="noopener" target="_blank">Stellarium</a>.</figcaption></figure>
<figure id="attachment_516268" aria-describedby="caption-attachment-516268"><img loading="lazy" decoding="async" src="https://earthsky.org/upl/2025/07/Location-of-new-interstellar-comet-3I-Atlas-on-12-04-2025-around-5-am-CT-passing-by-star-Zavijava-in-Virgo-Eddie-Irizarry-Stellarium-e1752842697784.jpg" alt="Star chart showing red marks for comet location and a labeled bright star (Zavijava) to the left." width="800" height="479"><figcaption id="caption-attachment-516268">This star chart is for 5 a.m. CST on December 4, 2025. On this date Comet 3I/ATLAS will be close to the star Zavijava in Virgo. Image via Eddie Irizarry/ <a href="https://stellarium-web.org/" rel="noopener" target="_blank">Stellarium</a>.</figcaption></figure>
<figure id="attachment_516267" aria-describedby="caption-attachment-516267"><img loading="lazy" decoding="async" src="https://earthsky.org/upl/2025/07/Location-of-new-interstellar-comet-3I-Atlas-on-12-12-2025-around-5-am-CT-passing-by-galaxy-NGC-3604-in-Leo-Eddie-Irizarry-Stellarium-e1752842688230.jpg" alt="Star chart showing red marks for comet location, 2 stars below, plus a tiny, dim deep-sky object at the top (galaxy in Leo)." width="800" height="523"><figcaption id="caption-attachment-516267">This star chart is for around 5 a.m. CST on December 12, 2025. On that date, the comet will be close to 2 stars and a dim spiral galaxy in the constellation <a href="https://earthsky.org/constellations/leo-heres-your-constellation/" rel="noopener" target="_blank">Leo</a> the Lion. Image via Eddie Irizarry/ <a href="https://stellarium-web.org/" rel="noopener" target="_blank">Stellarium</a>.</figcaption></figure>
<p>Bottom line: Interstellar object 3I/ATLAS swept closest to Mars at 11 p.m. CDT on October 2 (4 UTC on October 3). Read about plans to observe it with spacecraft.<br>
Via:</p>
<p><a href="https://cneos.jpl.nasa.gov/scout/#/object/A11pl3Z" rel="noopener" target="_blank">NASA/JPL</a></p>
<p><a href="https://www.minorplanetcenter.net/iau/NEO/toconfirm_tabular.html" rel="noopener" target="_blank">IAU Minor Planet Center</a></p>
<p><a href="https://noirlab.edu/public/news/noirlab2522/" rel="noopener" target="_blank">NOIRLab</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Offline card payments should be possible no later than 1 July 2026 (430 pts)]]></title>
            <link>https://www.riksbank.se/en-gb/press-and-published/notices-and-press-releases/press-releases/2025/offline-card-payments-should-be-possible-no-later-than-1-july-2026/</link>
            <guid>45467500</guid>
            <pubDate>Fri, 03 Oct 2025 20:36:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.riksbank.se/en-gb/press-and-published/notices-and-press-releases/press-releases/2025/offline-card-payments-should-be-possible-no-later-than-1-july-2026/">https://www.riksbank.se/en-gb/press-and-published/notices-and-press-releases/press-releases/2025/offline-card-payments-should-be-possible-no-later-than-1-july-2026/</a>, See on <a href="https://news.ycombinator.com/item?id=45467500">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<article>

			    


				<p>
						<span><span></span>Press release</span>
					The Riksbank and representatives from the payment market have today reached an agreement to increase the possibility to make offline card payments for essential goods. The agreement is an important step in the work to strengthen Sweden's payment preparedness and increase resilience to disruptions in the digital payments system. The goal is for the measures to be in place no later than 1 July 2026.
				</p>



				

			<div>
				
<p>“In Sweden, we pay digitally to a large degree and the use of cash is low. The general public being able to pay by card for example for food and medicines even in the event of a serious breakdown in data communication, that is offline, is a milestone in our intensified efforts to strengthen emergency preparedness”, says Governor Erik Thedéen.</p>
<p>The agreement describes the measures that participants in Swedish card payments – card issuers, card networks, card acquirers, the retail sector and the Riksbank – will implement to increase the possibility of offline payments by card. For instance, financial agents will adapt their regulatory frameworks, and the retail trade will introduce technological solutions. The Riksbank is leading this work and is responsible for monitoring its implementation.</p>
<p>“We are very pleased that all participants involved are taking responsibility for strengthening Sweden's payment readiness. Some are covered by the Riksbank's regulations, but far from all. We regard the fact that so many are nevertheless choosing to contribute as very positive for Sweden's overall civil preparedness”, concludes Erik Thedéen.</p>
<p>The online function shall apply to physical payment cards and accompanying PIN code when purchasing essential goods such as food, medicine and fuel. The Riksbank will continue its work on enabling offline payments for other payment methods after 1 July 2026.</p>

			</div>


				

			<div>
		<p>
			Contact: <span>Press Office, tel. +46 8-7870200</span>
		</p>
	<p>
		Updated 03/10/2025
	</p>
</div>
		</article>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD's EPYC 9355P: Inside a 32 Core Zen 5 Server Chip (153 pts)]]></title>
            <link>https://chipsandcheese.com/p/amds-epyc-9355p-inside-a-32-core</link>
            <guid>45467166</guid>
            <pubDate>Fri, 03 Oct 2025 20:01:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/p/amds-epyc-9355p-inside-a-32-core">https://chipsandcheese.com/p/amds-epyc-9355p-inside-a-32-core</a>, See on <a href="https://news.ycombinator.com/item?id=45467166">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>High core count chips are headline grabbers. But maxing out the metaphorical core count slider isn’t the only way to go. Server players like Intel, AMD, and Arm aim for scalable designs that cover a range of core counts. Not all applications can take advantage of the highest core count models in their lineups, and per-core performance still matters.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!icLd!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb52368ac-879d-4088-8e22-ab20f0137f82_975x583.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!icLd!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb52368ac-879d-4088-8e22-ab20f0137f82_975x583.png 424w, https://substackcdn.com/image/fetch/$s_!icLd!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb52368ac-879d-4088-8e22-ab20f0137f82_975x583.png 848w, https://substackcdn.com/image/fetch/$s_!icLd!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb52368ac-879d-4088-8e22-ab20f0137f82_975x583.png 1272w, https://substackcdn.com/image/fetch/$s_!icLd!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb52368ac-879d-4088-8e22-ab20f0137f82_975x583.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!icLd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb52368ac-879d-4088-8e22-ab20f0137f82_975x583.png" width="975" height="583" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b52368ac-879d-4088-8e22-ab20f0137f82_975x583.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:583,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:410106,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb52368ac-879d-4088-8e22-ab20f0137f82_975x583.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!icLd!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb52368ac-879d-4088-8e22-ab20f0137f82_975x583.png 424w, https://substackcdn.com/image/fetch/$s_!icLd!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb52368ac-879d-4088-8e22-ab20f0137f82_975x583.png 848w, https://substackcdn.com/image/fetch/$s_!icLd!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb52368ac-879d-4088-8e22-ab20f0137f82_975x583.png 1272w, https://substackcdn.com/image/fetch/$s_!icLd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb52368ac-879d-4088-8e22-ab20f0137f82_975x583.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>From AMD’s Turin whitepaper</figcaption></figure></div><p><span>AMD’s EPYC 9355P is a 32 core part. But rather than just being a lower core count part, the 9355P pulls levers to let each core count for more. First, it clocks up to 4.4 GHz. AMD has faster clocking chips in its server lineup, but 4.4 GHz is still a good bit higher than the 3.7 or 4.1 GHz that </span><a href="https://www.amd.com/en/products/processors/server/epyc/9005-series.html#specifications" rel="">128 or 192 core Zen 5 SKUs reach</a><span>. Then, AMD uses eight CPU dies (CCDs) to house those 32 cores. Each CCD only has four cores enabled out of the eight physically present, but still has its full 32 MB of L3 cache usable. That provides a high cache capacity to core count ratio. Finally, each CCD connects to the IO die using a “GMI-Wide” setup, giving each CCD 64B/cycle of bandwidth to the rest of the system in both the read and write directions. GMI here stands for Global Memory Interconnect. Zen 5’s server IO die has 16 GMI links to support up to 16 CCDs for high core count parts, plus some xGMI (external) links to allow a dual socket setup. GMI-Wide uses two links per CCD, fully utilizing the IO die’s GMI links even though the EPYC 9355P only has eight CCDs.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!xcV9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ddd15e-c1e8-4f84-95f8-56d9181f7a4d_975x527.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!xcV9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ddd15e-c1e8-4f84-95f8-56d9181f7a4d_975x527.png 424w, https://substackcdn.com/image/fetch/$s_!xcV9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ddd15e-c1e8-4f84-95f8-56d9181f7a4d_975x527.png 848w, https://substackcdn.com/image/fetch/$s_!xcV9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ddd15e-c1e8-4f84-95f8-56d9181f7a4d_975x527.png 1272w, https://substackcdn.com/image/fetch/$s_!xcV9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ddd15e-c1e8-4f84-95f8-56d9181f7a4d_975x527.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!xcV9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ddd15e-c1e8-4f84-95f8-56d9181f7a4d_975x527.png" width="975" height="527" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/89ddd15e-c1e8-4f84-95f8-56d9181f7a4d_975x527.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:527,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:180582,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ddd15e-c1e8-4f84-95f8-56d9181f7a4d_975x527.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!xcV9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ddd15e-c1e8-4f84-95f8-56d9181f7a4d_975x527.png 424w, https://substackcdn.com/image/fetch/$s_!xcV9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ddd15e-c1e8-4f84-95f8-56d9181f7a4d_975x527.png 848w, https://substackcdn.com/image/fetch/$s_!xcV9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ddd15e-c1e8-4f84-95f8-56d9181f7a4d_975x527.png 1272w, https://substackcdn.com/image/fetch/$s_!xcV9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89ddd15e-c1e8-4f84-95f8-56d9181f7a4d_975x527.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><a href="https://www.dell.com/en-us/shop/dell-poweredge-servers/new-poweredge-r6715-rack-server/spd/poweredge-r6715/pe_r6715_tm_vi_vp_sb" rel="">Dell has kindly provided a PowerEdge R6715 for testing</a><span>, and it came equipped with the aforementioned AMD EPYC 9355P along with 768 GB of DDR5-5200. The 12 memory controllers on the IO die provide a 768-bit memory bus, so the setup provides just under 500 GB/s of theoretical bandwidth. Besides providing a look into how a lower core count SKU behaves, we have BMC access which provides an opportunity to investigate different NUMA setups.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!S2Gx!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46c0cf47-bc97-4634-8c86-82d0d5c2f829_452x603.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!S2Gx!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46c0cf47-bc97-4634-8c86-82d0d5c2f829_452x603.png 424w, https://substackcdn.com/image/fetch/$s_!S2Gx!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46c0cf47-bc97-4634-8c86-82d0d5c2f829_452x603.png 848w, https://substackcdn.com/image/fetch/$s_!S2Gx!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46c0cf47-bc97-4634-8c86-82d0d5c2f829_452x603.png 1272w, https://substackcdn.com/image/fetch/$s_!S2Gx!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46c0cf47-bc97-4634-8c86-82d0d5c2f829_452x603.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!S2Gx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46c0cf47-bc97-4634-8c86-82d0d5c2f829_452x603.png" width="452" height="603" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/46c0cf47-bc97-4634-8c86-82d0d5c2f829_452x603.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:603,&quot;width&quot;:452,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:604085,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46c0cf47-bc97-4634-8c86-82d0d5c2f829_452x603.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!S2Gx!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46c0cf47-bc97-4634-8c86-82d0d5c2f829_452x603.png 424w, https://substackcdn.com/image/fetch/$s_!S2Gx!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46c0cf47-bc97-4634-8c86-82d0d5c2f829_452x603.png 848w, https://substackcdn.com/image/fetch/$s_!S2Gx!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46c0cf47-bc97-4634-8c86-82d0d5c2f829_452x603.png 1272w, https://substackcdn.com/image/fetch/$s_!S2Gx!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46c0cf47-bc97-4634-8c86-82d0d5c2f829_452x603.png 1456w" sizes="100vw"></picture></div></a><figcaption>The provided Dell PowerEdge R6715 system racked and ready for testing. Credit for the image goes to Zach from ZeroOne.</figcaption></figure></div><p>We’d also like to thank Zack and the rest of the fine folks at ZeroOne Technology for hosting the Dell PowerEdge R6715 at no cost to us.</p><p>NPS1 mode stripes memory accesses across all 12 of the chip’s memory controllers, presenting software with a monolithic view of memory at the cost of latency. DRAM latency in that mode is slightly better than what Intel’s Xeon 6 achieves in SNC3 mode. SNC3 on Intel divides the chip into three NUMA nodes that correspond to its compute dies. The EPYC 9355P has good memory latency in that respect, but it falls behind compared to the Ryzen 9 9900X with DDR5-5600. Interconnects that tie more agents together tend to have higher latency. On AMD’s server platform, the Infinity Fabric network within the IO die has to connect up to 16 CCDs with 12 memory controllers and other IO, so the higher latency isn’t surprising.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Mbb3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff51fed7c-b5d2-4b4c-8a43-4a55ce28914d_975x427.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Mbb3!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff51fed7c-b5d2-4b4c-8a43-4a55ce28914d_975x427.png 424w, https://substackcdn.com/image/fetch/$s_!Mbb3!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff51fed7c-b5d2-4b4c-8a43-4a55ce28914d_975x427.png 848w, https://substackcdn.com/image/fetch/$s_!Mbb3!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff51fed7c-b5d2-4b4c-8a43-4a55ce28914d_975x427.png 1272w, https://substackcdn.com/image/fetch/$s_!Mbb3!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff51fed7c-b5d2-4b4c-8a43-4a55ce28914d_975x427.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Mbb3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff51fed7c-b5d2-4b4c-8a43-4a55ce28914d_975x427.png" width="975" height="427" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f51fed7c-b5d2-4b4c-8a43-4a55ce28914d_975x427.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:427,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:72295,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff51fed7c-b5d2-4b4c-8a43-4a55ce28914d_975x427.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Mbb3!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff51fed7c-b5d2-4b4c-8a43-4a55ce28914d_975x427.png 424w, https://substackcdn.com/image/fetch/$s_!Mbb3!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff51fed7c-b5d2-4b4c-8a43-4a55ce28914d_975x427.png 848w, https://substackcdn.com/image/fetch/$s_!Mbb3!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff51fed7c-b5d2-4b4c-8a43-4a55ce28914d_975x427.png 1272w, https://substackcdn.com/image/fetch/$s_!Mbb3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff51fed7c-b5d2-4b4c-8a43-4a55ce28914d_975x427.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Cache performance is similar across AMD’s desktop and server Zen 5 implementations, with the server variant only losing because of lower clock speeds. That’s not a surprise because AMD reuses the same CCDs on desktop and server products. But it does create a contrast to Intel’s approach, where client and server memory subsystems differ starting at L3. Intel trades L3 latency for capacity and the ability to a logical L3 instance across more cores.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!QONl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadf03a30-37d4-4d15-9c01-690fe0305f95_975x227.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!QONl!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadf03a30-37d4-4d15-9c01-690fe0305f95_975x227.png 424w, https://substackcdn.com/image/fetch/$s_!QONl!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadf03a30-37d4-4d15-9c01-690fe0305f95_975x227.png 848w, https://substackcdn.com/image/fetch/$s_!QONl!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadf03a30-37d4-4d15-9c01-690fe0305f95_975x227.png 1272w, https://substackcdn.com/image/fetch/$s_!QONl!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadf03a30-37d4-4d15-9c01-690fe0305f95_975x227.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!QONl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadf03a30-37d4-4d15-9c01-690fe0305f95_975x227.png" width="975" height="227" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/adf03a30-37d4-4d15-9c01-690fe0305f95_975x227.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:227,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:47683,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadf03a30-37d4-4d15-9c01-690fe0305f95_975x227.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!QONl!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadf03a30-37d4-4d15-9c01-690fe0305f95_975x227.png 424w, https://substackcdn.com/image/fetch/$s_!QONl!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadf03a30-37d4-4d15-9c01-690fe0305f95_975x227.png 848w, https://substackcdn.com/image/fetch/$s_!QONl!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadf03a30-37d4-4d15-9c01-690fe0305f95_975x227.png 1272w, https://substackcdn.com/image/fetch/$s_!QONl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadf03a30-37d4-4d15-9c01-690fe0305f95_975x227.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Different NUMA configurations can subdivide EPYC 9355P, associating cores with the closest memory controllers to improve latency. NPS2 divides the chip into two hemispheres, and has 16 cores form a NUMA node with the six memory controllers on one half of the die. NPS4 divides the chip into quadrants, each with two CCDs and three memory controllers. Finally, the chip can present each CCD as a NUMA node. Doing so makes it easier to pin threads to cores that share a L3 cache, but doesn’t affect how memory is interleaved across channels. Memory addresses are still assigned to memory controllers according to the selected NPS1/2/4 scheme, which is a separate setting.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!3suW!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb62202-f156-4446-82db-d117584de0c2_975x327.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!3suW!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb62202-f156-4446-82db-d117584de0c2_975x327.png 424w, https://substackcdn.com/image/fetch/$s_!3suW!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb62202-f156-4446-82db-d117584de0c2_975x327.png 848w, https://substackcdn.com/image/fetch/$s_!3suW!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb62202-f156-4446-82db-d117584de0c2_975x327.png 1272w, https://substackcdn.com/image/fetch/$s_!3suW!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb62202-f156-4446-82db-d117584de0c2_975x327.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!3suW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb62202-f156-4446-82db-d117584de0c2_975x327.png" width="975" height="327" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dcb62202-f156-4446-82db-d117584de0c2_975x327.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:327,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:162136,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb62202-f156-4446-82db-d117584de0c2_975x327.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!3suW!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb62202-f156-4446-82db-d117584de0c2_975x327.png 424w, https://substackcdn.com/image/fetch/$s_!3suW!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb62202-f156-4446-82db-d117584de0c2_975x327.png 848w, https://substackcdn.com/image/fetch/$s_!3suW!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb62202-f156-4446-82db-d117584de0c2_975x327.png 1272w, https://substackcdn.com/image/fetch/$s_!3suW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcb62202-f156-4446-82db-d117584de0c2_975x327.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>NPS2 and NPS4 only provide marginal latency improvements, and latency remains much higher than in a desktop platform. At the same time, crossing NUMA boundaries comes with little penalty. Apparently requests can traverse the huge IO die quite quickly, adding 20-30 ns at worst. I’m not sure what the underlying Infinity Fabric topology looks like, but the worst case unloaded cross-node latencies were under 140 ns. On Xeon 6, latency starts higher and can climb over 180 ns when cores on one compute die access memory controllers on the compute die at the other end of the chip.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!3AET!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cb07f7e-9a74-4586-b2cf-793668b357d3_975x469.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!3AET!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cb07f7e-9a74-4586-b2cf-793668b357d3_975x469.png 424w, https://substackcdn.com/image/fetch/$s_!3AET!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cb07f7e-9a74-4586-b2cf-793668b357d3_975x469.png 848w, https://substackcdn.com/image/fetch/$s_!3AET!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cb07f7e-9a74-4586-b2cf-793668b357d3_975x469.png 1272w, https://substackcdn.com/image/fetch/$s_!3AET!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cb07f7e-9a74-4586-b2cf-793668b357d3_975x469.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!3AET!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cb07f7e-9a74-4586-b2cf-793668b357d3_975x469.png" width="975" height="469" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3cb07f7e-9a74-4586-b2cf-793668b357d3_975x469.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:469,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:121481,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cb07f7e-9a74-4586-b2cf-793668b357d3_975x469.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!3AET!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cb07f7e-9a74-4586-b2cf-793668b357d3_975x469.png 424w, https://substackcdn.com/image/fetch/$s_!3AET!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cb07f7e-9a74-4586-b2cf-793668b357d3_975x469.png 848w, https://substackcdn.com/image/fetch/$s_!3AET!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cb07f7e-9a74-4586-b2cf-793668b357d3_975x469.png 1272w, https://substackcdn.com/image/fetch/$s_!3AET!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3cb07f7e-9a74-4586-b2cf-793668b357d3_975x469.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>EPYC 9355P can get close to theoretical memory bandwidth in any of the three NUMA nodes, as long as code keeps accesses within each node. NPS2 and NPS4 offer slightly better bandwidth, at the cost of requiring code to be NUMA aware. I tried to cause congestion on Infinity Fabric by having cores on each NUMA node access memory on another. That does lower achieved bandwidth, but not by a huge amount.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ZwBS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb124a60-3a1c-4557-8a24-e9da65562edd_975x288.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ZwBS!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb124a60-3a1c-4557-8a24-e9da65562edd_975x288.png 424w, https://substackcdn.com/image/fetch/$s_!ZwBS!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb124a60-3a1c-4557-8a24-e9da65562edd_975x288.png 848w, https://substackcdn.com/image/fetch/$s_!ZwBS!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb124a60-3a1c-4557-8a24-e9da65562edd_975x288.png 1272w, https://substackcdn.com/image/fetch/$s_!ZwBS!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb124a60-3a1c-4557-8a24-e9da65562edd_975x288.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ZwBS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb124a60-3a1c-4557-8a24-e9da65562edd_975x288.png" width="975" height="288" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/eb124a60-3a1c-4557-8a24-e9da65562edd_975x288.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:288,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:102088,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb124a60-3a1c-4557-8a24-e9da65562edd_975x288.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ZwBS!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb124a60-3a1c-4557-8a24-e9da65562edd_975x288.png 424w, https://substackcdn.com/image/fetch/$s_!ZwBS!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb124a60-3a1c-4557-8a24-e9da65562edd_975x288.png 848w, https://substackcdn.com/image/fetch/$s_!ZwBS!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb124a60-3a1c-4557-8a24-e9da65562edd_975x288.png 1272w, https://substackcdn.com/image/fetch/$s_!ZwBS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb124a60-3a1c-4557-8a24-e9da65562edd_975x288.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>By C0M1, I mean cores on node 0 accessing memory on node 1</figcaption></figure></div><p>An individual NPS4 node achieves 117.33 GB/s to its local memory pool, and just over 107 GB/s to the memory on the other three nodes. The bandwidth penalty is minor, but a bigger potential pitfall is lower bandwidth to each NUMA node’s memory pool. Two CCDs can draw more bandwidth than the three memory controllers they’re associated with. Manually distributing memory accesses across NUMA nodes can improve bandwidth for a workload contained within one NUMA node’s cores. But doing so in practice may be an intricate exercise.</p><p>In general, EPYC 9355P has very mild NUMA characteristics and little penalty associated with running the chip in NPS1 or NPS2 mode. I imagine just using NPS1 mode would work well enough in the vast majority of cases, with little performance to be gained from carrying out NUMA optimizations.</p><p>GMI-Wide is AMD’s attempt to address bandwidth pinch points between CCDs and the rest of the system. With GMI-Wide, a single CCD can achieve 99.8 GB/s of read bandwidth, significantly more than the 62.5 GB/s from a Ryzen 9 9900X CCD with GMI-Narrow. GMI-Wide also allows better latency control under high bandwidth load. The Ryzen 9 9900X suffers from a corner case where a single core pulling maximum bandwidth can saturate the GMI-Narrow link and starve out another latency sensitive thread. That sends latency to nearly 500 ns, as observed by a latency test thread sharing a CCD with a thread linearly traversing an array. Having more threads generate bandwidth load seems to make QoS mechanisms kick in, which slightly reduces bandwidth throughput but brings latency back under control.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!KTCF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a88e138-9963-4f94-99e7-64854c656bc8_975x529.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!KTCF!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a88e138-9963-4f94-99e7-64854c656bc8_975x529.png 424w, https://substackcdn.com/image/fetch/$s_!KTCF!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a88e138-9963-4f94-99e7-64854c656bc8_975x529.png 848w, https://substackcdn.com/image/fetch/$s_!KTCF!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a88e138-9963-4f94-99e7-64854c656bc8_975x529.png 1272w, https://substackcdn.com/image/fetch/$s_!KTCF!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a88e138-9963-4f94-99e7-64854c656bc8_975x529.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!KTCF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a88e138-9963-4f94-99e7-64854c656bc8_975x529.png" width="975" height="529" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1a88e138-9963-4f94-99e7-64854c656bc8_975x529.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:529,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:95917,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a88e138-9963-4f94-99e7-64854c656bc8_975x529.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!KTCF!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a88e138-9963-4f94-99e7-64854c656bc8_975x529.png 424w, https://substackcdn.com/image/fetch/$s_!KTCF!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a88e138-9963-4f94-99e7-64854c656bc8_975x529.png 848w, https://substackcdn.com/image/fetch/$s_!KTCF!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a88e138-9963-4f94-99e7-64854c656bc8_975x529.png 1272w, https://substackcdn.com/image/fetch/$s_!KTCF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a88e138-9963-4f94-99e7-64854c656bc8_975x529.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>I previously wrote about loaded memory latency on the Ryzen 9 9950X when testing the system remotely, and </span><a href="https://chipsandcheese.com/p/pushing-amds-infinity-fabric-to-its" rel="">thought it controlled latency well under high bandwidth load</a><span>. But back then, George (Cheese) set that system up with very fast DDR5-8000 along with a higher 2.2 GHz FCLK. A single core was likely unable to monopolize off-CCD bandwidth in that setup, avoiding the corner case seen on my system. GMI-Wide increases off-CCD bandwidth by a much larger extent and has a similar effect. Under increasing bandwidth load, GMI-WIde can both achieve more total bandwidth and control latency better than its desktop single-link counterpart.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!uZWd!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff08156e0-49a7-4913-a66a-b104c4f042f5_975x538.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!uZWd!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff08156e0-49a7-4913-a66a-b104c4f042f5_975x538.png 424w, https://substackcdn.com/image/fetch/$s_!uZWd!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff08156e0-49a7-4913-a66a-b104c4f042f5_975x538.png 848w, https://substackcdn.com/image/fetch/$s_!uZWd!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff08156e0-49a7-4913-a66a-b104c4f042f5_975x538.png 1272w, https://substackcdn.com/image/fetch/$s_!uZWd!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff08156e0-49a7-4913-a66a-b104c4f042f5_975x538.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!uZWd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff08156e0-49a7-4913-a66a-b104c4f042f5_975x538.png" width="975" height="538" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f08156e0-49a7-4913-a66a-b104c4f042f5_975x538.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:538,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:102239,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff08156e0-49a7-4913-a66a-b104c4f042f5_975x538.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!uZWd!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff08156e0-49a7-4913-a66a-b104c4f042f5_975x538.png 424w, https://substackcdn.com/image/fetch/$s_!uZWd!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff08156e0-49a7-4913-a66a-b104c4f042f5_975x538.png 848w, https://substackcdn.com/image/fetch/$s_!uZWd!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff08156e0-49a7-4913-a66a-b104c4f042f5_975x538.png 1272w, https://substackcdn.com/image/fetch/$s_!uZWd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff08156e0-49a7-4913-a66a-b104c4f042f5_975x538.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>A read-modify-write pattern gets maximum bandwidth from GMI-Wide by exercising both the read and write paths. It doesn’t scale perfectly, but it’s a substantial improvement over using only reads or writes. A Ryzen 9 9900X CCD can theoretically get 48B/cycle to the IO die with a 2:1 read-to-write ratio. I tried modifying every other cacheline to achieve this ratio, but didn’t get better bandwidth probably because the memory controller is limited by a 32B/cycle link to Infinity Fabric. However, mixing in writes does get rid of the single bandwidth thread corner case, possibly because a single thread doesn’t saturate the 32B/cycle read link when mixing reads and writes.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!388G!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48763b64-244f-483b-a126-e9c079e52ee5_975x546.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!388G!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48763b64-244f-483b-a126-e9c079e52ee5_975x546.png 424w, https://substackcdn.com/image/fetch/$s_!388G!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48763b64-244f-483b-a126-e9c079e52ee5_975x546.png 848w, https://substackcdn.com/image/fetch/$s_!388G!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48763b64-244f-483b-a126-e9c079e52ee5_975x546.png 1272w, https://substackcdn.com/image/fetch/$s_!388G!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48763b64-244f-483b-a126-e9c079e52ee5_975x546.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!388G!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48763b64-244f-483b-a126-e9c079e52ee5_975x546.png" width="975" height="546" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/48763b64-244f-483b-a126-e9c079e52ee5_975x546.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:546,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:189525,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48763b64-244f-483b-a126-e9c079e52ee5_975x546.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!388G!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48763b64-244f-483b-a126-e9c079e52ee5_975x546.png 424w, https://substackcdn.com/image/fetch/$s_!388G!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48763b64-244f-483b-a126-e9c079e52ee5_975x546.png 848w, https://substackcdn.com/image/fetch/$s_!388G!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48763b64-244f-483b-a126-e9c079e52ee5_975x546.png 1272w, https://substackcdn.com/image/fetch/$s_!388G!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F48763b64-244f-483b-a126-e9c079e52ee5_975x546.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>AMD’s slide shown for Zen 4 Desktop. Zen 5 Desktop uses the same IO die, and thus has the same Infinity Fabric setup</figcaption></figure></div><p>On the desktop platform, latency under high load gets worse possibly because writes contend with reads at the DRAM controller. The DDR bus is unidirectional, and must waste cycles on “turnarounds” to switch between read and write mode. Bandwidth isn’t affected, probably because the Infinity Fabric bottleneck leaves spare cycles at the memory controller, which can absorb those turnarounds. However, reads from the latency test thread may be delayed while the memory controller drains writes before switching the bus back to read mode.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!R21G!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b3ef84b-2970-400a-a509-5811a97b1746_975x590.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!R21G!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b3ef84b-2970-400a-a509-5811a97b1746_975x590.png 424w, https://substackcdn.com/image/fetch/$s_!R21G!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b3ef84b-2970-400a-a509-5811a97b1746_975x590.png 848w, https://substackcdn.com/image/fetch/$s_!R21G!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b3ef84b-2970-400a-a509-5811a97b1746_975x590.png 1272w, https://substackcdn.com/image/fetch/$s_!R21G!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b3ef84b-2970-400a-a509-5811a97b1746_975x590.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!R21G!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b3ef84b-2970-400a-a509-5811a97b1746_975x590.png" width="975" height="590" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0b3ef84b-2970-400a-a509-5811a97b1746_975x590.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:590,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:103650,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b3ef84b-2970-400a-a509-5811a97b1746_975x590.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!R21G!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b3ef84b-2970-400a-a509-5811a97b1746_975x590.png 424w, https://substackcdn.com/image/fetch/$s_!R21G!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b3ef84b-2970-400a-a509-5811a97b1746_975x590.png 848w, https://substackcdn.com/image/fetch/$s_!R21G!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b3ef84b-2970-400a-a509-5811a97b1746_975x590.png 1272w, https://substackcdn.com/image/fetch/$s_!R21G!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b3ef84b-2970-400a-a509-5811a97b1746_975x590.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>On the EPYC 9355P in NPS1 mode, bandwidth demands from a single GMI-Wide CCD leave plenty of spare cycles across the 12 memory controllers, so there’s little latency or bandwidth penalty when mixing reads and writes. The same isn’t true in NPS4 mode, where a GMI-Wide link can outmatch a NPS4 node’s three memory controllers. Everything’s fine with just reads, which actually benefit possibly because of lower latency and not having to traverse as much of the IO die. But with a read-modify-write pattern, bandwidth drops from 134 GB/s in NPS1 mode to 96.6 GB/s with NPS4. Latency gets worse too, rising to 248 ns. Again, NPS4 is something to be careful with, particularly if applications might require high bandwidth from a small subset of cores.</p><p>From a single thread perspective, the EPYC 9355P falls some distance behind the Ryzen 9 9900X. Desktop CPUs are designed around single threaded performance, so that’s to be expected. But with boost turned off on the desktop CPU to match clock speeds, performance is surprisingly close. Higher memory latency still hurts the EPYC 9355P, but it’s within striking distance.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!S4dS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56176f96-0c6f-465b-9c2c-7b927fe48aaa_975x583.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!S4dS!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56176f96-0c6f-465b-9c2c-7b927fe48aaa_975x583.png 424w, https://substackcdn.com/image/fetch/$s_!S4dS!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56176f96-0c6f-465b-9c2c-7b927fe48aaa_975x583.png 848w, https://substackcdn.com/image/fetch/$s_!S4dS!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56176f96-0c6f-465b-9c2c-7b927fe48aaa_975x583.png 1272w, https://substackcdn.com/image/fetch/$s_!S4dS!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56176f96-0c6f-465b-9c2c-7b927fe48aaa_975x583.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!S4dS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56176f96-0c6f-465b-9c2c-7b927fe48aaa_975x583.png" width="975" height="583" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/56176f96-0c6f-465b-9c2c-7b927fe48aaa_975x583.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:583,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:102023,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56176f96-0c6f-465b-9c2c-7b927fe48aaa_975x583.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!S4dS!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56176f96-0c6f-465b-9c2c-7b927fe48aaa_975x583.png 424w, https://substackcdn.com/image/fetch/$s_!S4dS!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56176f96-0c6f-465b-9c2c-7b927fe48aaa_975x583.png 848w, https://substackcdn.com/image/fetch/$s_!S4dS!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56176f96-0c6f-465b-9c2c-7b927fe48aaa_975x583.png 1272w, https://substackcdn.com/image/fetch/$s_!S4dS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F56176f96-0c6f-465b-9c2c-7b927fe48aaa_975x583.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>NUMA modes barely make any difference. NPS4 technically wins, but by an insignificant margin. The latency advantage was barely measurable anyway. Compared to the more density optimized Graviton 4 and Xeon 6 6975P-C, the EPYC 9355P delivers noticeably better single threaded performance.</p><p>CCD-level bandwidth pinch points are worth a look too, since that’s traditionally been a distinguishing factor between AMD’s EPYC and more logically monolithic designs. Here, I’m filling a quad core CCD by running SPEC CPU2017’s rate tests with eight copies. I did the same on the Ryzen 9 9900X, pinning the eight copies to four cores and leaving the CCD’s other two cores unused. I bound the test to a single NUMA node on all tested setups.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!W9Bj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec837352-d5f8-42bf-aa2e-07f4f4fdfa74_975x585.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!W9Bj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec837352-d5f8-42bf-aa2e-07f4f4fdfa74_975x585.png 424w, https://substackcdn.com/image/fetch/$s_!W9Bj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec837352-d5f8-42bf-aa2e-07f4f4fdfa74_975x585.png 848w, https://substackcdn.com/image/fetch/$s_!W9Bj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec837352-d5f8-42bf-aa2e-07f4f4fdfa74_975x585.png 1272w, https://substackcdn.com/image/fetch/$s_!W9Bj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec837352-d5f8-42bf-aa2e-07f4f4fdfa74_975x585.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!W9Bj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec837352-d5f8-42bf-aa2e-07f4f4fdfa74_975x585.png" width="975" height="585" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ec837352-d5f8-42bf-aa2e-07f4f4fdfa74_975x585.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:585,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:76855,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec837352-d5f8-42bf-aa2e-07f4f4fdfa74_975x585.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!W9Bj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec837352-d5f8-42bf-aa2e-07f4f4fdfa74_975x585.png 424w, https://substackcdn.com/image/fetch/$s_!W9Bj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec837352-d5f8-42bf-aa2e-07f4f4fdfa74_975x585.png 848w, https://substackcdn.com/image/fetch/$s_!W9Bj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec837352-d5f8-42bf-aa2e-07f4f4fdfa74_975x585.png 1272w, https://substackcdn.com/image/fetch/$s_!W9Bj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec837352-d5f8-42bf-aa2e-07f4f4fdfa74_975x585.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>SPEC’s floating point suite starts to tell a different story now. Several tests within the floating point suite are bandwidth hungry even from a single core. 549.fotonik3d for example pulled 28.23 GB/s from Meteor Lake’s memory controller</span><a href="https://chipsandcheese.com/p/running-spec-cpu2017-at-chips-and-cheese" rel=""> when I first went through SPEC CPU2017’s tests</a><span>. Running eight copies in parallel would multiply memory bandwidth demands, and that’s where server memory subsystems shine.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!5l58!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44331b3c-ed06-4b9e-ba70-db7d7d34815e_975x585.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!5l58!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44331b3c-ed06-4b9e-ba70-db7d7d34815e_975x585.png 424w, https://substackcdn.com/image/fetch/$s_!5l58!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44331b3c-ed06-4b9e-ba70-db7d7d34815e_975x585.png 848w, https://substackcdn.com/image/fetch/$s_!5l58!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44331b3c-ed06-4b9e-ba70-db7d7d34815e_975x585.png 1272w, https://substackcdn.com/image/fetch/$s_!5l58!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44331b3c-ed06-4b9e-ba70-db7d7d34815e_975x585.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!5l58!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44331b3c-ed06-4b9e-ba70-db7d7d34815e_975x585.png" width="975" height="585" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/44331b3c-ed06-4b9e-ba70-db7d7d34815e_975x585.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:585,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:74605,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44331b3c-ed06-4b9e-ba70-db7d7d34815e_975x585.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!5l58!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44331b3c-ed06-4b9e-ba70-db7d7d34815e_975x585.png 424w, https://substackcdn.com/image/fetch/$s_!5l58!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44331b3c-ed06-4b9e-ba70-db7d7d34815e_975x585.png 848w, https://substackcdn.com/image/fetch/$s_!5l58!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44331b3c-ed06-4b9e-ba70-db7d7d34815e_975x585.png 1272w, https://substackcdn.com/image/fetch/$s_!5l58!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44331b3c-ed06-4b9e-ba70-db7d7d34815e_975x585.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>In 549.fotonik3d, high bandwidth demands make the Ryzen 9 9900X’s unloaded latency advantage irrelevant. The 9900X even loses to Redwood Cove cores on Xeon 6. The EPYC 9355P does very well in this test against both the 9900X and Xeon 6. Intel’s interconnect strategy tries to keep the chip logically monolithic and doesn’t have pinch points at cluster boundaries. But each core on Xeon 6 can only get to ~33 GB/s of DRAM bandwidth at best, using an even mix of reads and writes. AMD’s GMI-Wide can more than match that, and Intel’s advantage doesn’t show through in this scenario. However, Intel does have a potential advantage against more density focused AMD SKUs where eight cores sit in front of a narrower link.</p><p>NPS4 is also detrimental to the EPYC 9355P’s performance in this test. It only provides a minimal latency benefit at the cost of lower per-node bandwidth. The bandwidth part seems to hurt here, and taking the extra latency of striping accesses across 6 or 12 memory controllers gives a notable performance improvement.</p><p>Core count isn’t the last word in server design. A lot of scenarios are better served by lower core count parts. Applications might not scale to fill a high core count chip. Bandwidth bound workloads might not benefit from adding cores. Traditionally lower core count server chips just traded core counts for higher clock speeds. Today, chips like the EPYC 9355P do a bit more, using both wider CCD-to-IOD links and more cache to maximize per-core performance.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!7uUJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c58c9a-c90f-4452-8dda-c6d3b5fff36d_975x544.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!7uUJ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c58c9a-c90f-4452-8dda-c6d3b5fff36d_975x544.png 424w, https://substackcdn.com/image/fetch/$s_!7uUJ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c58c9a-c90f-4452-8dda-c6d3b5fff36d_975x544.png 848w, https://substackcdn.com/image/fetch/$s_!7uUJ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c58c9a-c90f-4452-8dda-c6d3b5fff36d_975x544.png 1272w, https://substackcdn.com/image/fetch/$s_!7uUJ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c58c9a-c90f-4452-8dda-c6d3b5fff36d_975x544.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!7uUJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c58c9a-c90f-4452-8dda-c6d3b5fff36d_975x544.png" width="975" height="544" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f0c58c9a-c90f-4452-8dda-c6d3b5fff36d_975x544.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:544,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:334371,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c58c9a-c90f-4452-8dda-c6d3b5fff36d_975x544.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!7uUJ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c58c9a-c90f-4452-8dda-c6d3b5fff36d_975x544.png 424w, https://substackcdn.com/image/fetch/$s_!7uUJ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c58c9a-c90f-4452-8dda-c6d3b5fff36d_975x544.png 848w, https://substackcdn.com/image/fetch/$s_!7uUJ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c58c9a-c90f-4452-8dda-c6d3b5fff36d_975x544.png 1272w, https://substackcdn.com/image/fetch/$s_!7uUJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c58c9a-c90f-4452-8dda-c6d3b5fff36d_975x544.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Looking at EPYC 9355P’s NUMA characteristics reveals very consistent memory performance across NUMA modes. Intel’s Xeon 6 may be more monolithic from a caching point of view, but AMD’s DRAM access performance feels more monolithic than Intel’s. AMD made a tradeoff back in the Zen 2 days where they took lower local memory latency in exchange for more even memory performance across the socket. Measured latencies on EPYC 9355P are a bit higher than figures on the Zen 2 slide above. DDR5 is higher latency, and the Infinity Fabric topology is probably more complex these days to handle more CCDs and memory channels.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!4Rh7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdbeb54c-c11b-44fb-8398-9a70ce800e06_975x525.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!4Rh7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdbeb54c-c11b-44fb-8398-9a70ce800e06_975x525.png 424w, https://substackcdn.com/image/fetch/$s_!4Rh7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdbeb54c-c11b-44fb-8398-9a70ce800e06_975x525.png 848w, https://substackcdn.com/image/fetch/$s_!4Rh7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdbeb54c-c11b-44fb-8398-9a70ce800e06_975x525.png 1272w, https://substackcdn.com/image/fetch/$s_!4Rh7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdbeb54c-c11b-44fb-8398-9a70ce800e06_975x525.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!4Rh7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdbeb54c-c11b-44fb-8398-9a70ce800e06_975x525.png" width="975" height="525" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bdbeb54c-c11b-44fb-8398-9a70ce800e06_975x525.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:525,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:567064,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdbeb54c-c11b-44fb-8398-9a70ce800e06_975x525.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!4Rh7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdbeb54c-c11b-44fb-8398-9a70ce800e06_975x525.png 424w, https://substackcdn.com/image/fetch/$s_!4Rh7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdbeb54c-c11b-44fb-8398-9a70ce800e06_975x525.png 848w, https://substackcdn.com/image/fetch/$s_!4Rh7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdbeb54c-c11b-44fb-8398-9a70ce800e06_975x525.png 1272w, https://substackcdn.com/image/fetch/$s_!4Rh7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbdbeb54c-c11b-44fb-8398-9a70ce800e06_975x525.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>From the AMD EPYC 9005 Processor Architecture Overview</figcaption></figure></div><p>But the big picture remains. AMD’s Turin platform handles well in NPS1 mode, and cross-node penalties are low in NPS2/NPS4 modes. Those characteristics likely carry over across the Zen 5 EPYC SKU stack. It’s quite different from Intel’s Xeon 6 platform, which places memory controllers on compute dies like Zen 1 did. For now, AMD’s approach seems to be better at the DRAM level. Intel’s theoretical latency advantage in SNC3 mode doesn’t show through, and AMD gets to reap the benefits of a hub-and-spoke model while not getting hit where it should have downsides.</p><p>AMD seems to have found a good formula back in the Zen 2 days, and they’re content with reinforcing success. Intel is furiously iterating to find a setup that preserves a single level, logically monolithic interconnect while scaling well across a range of core counts. And of course, there’s Arm chips, which generally lean towards a single level monolithic interconnect too. It’ll be interesting to watch what all of these players do going forward as they continue to iterate and refine their designs.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Vt3F!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec50b5cd-e0f1-4c05-9c6f-a9fa59889fbf_975x250.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Vt3F!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec50b5cd-e0f1-4c05-9c6f-a9fa59889fbf_975x250.png 424w, https://substackcdn.com/image/fetch/$s_!Vt3F!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec50b5cd-e0f1-4c05-9c6f-a9fa59889fbf_975x250.png 848w, https://substackcdn.com/image/fetch/$s_!Vt3F!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec50b5cd-e0f1-4c05-9c6f-a9fa59889fbf_975x250.png 1272w, https://substackcdn.com/image/fetch/$s_!Vt3F!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec50b5cd-e0f1-4c05-9c6f-a9fa59889fbf_975x250.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Vt3F!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec50b5cd-e0f1-4c05-9c6f-a9fa59889fbf_975x250.png" width="975" height="250" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ec50b5cd-e0f1-4c05-9c6f-a9fa59889fbf_975x250.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:250,&quot;width&quot;:975,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:409739,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/174871357?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec50b5cd-e0f1-4c05-9c6f-a9fa59889fbf_975x250.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Vt3F!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec50b5cd-e0f1-4c05-9c6f-a9fa59889fbf_975x250.png 424w, https://substackcdn.com/image/fetch/$s_!Vt3F!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec50b5cd-e0f1-4c05-9c6f-a9fa59889fbf_975x250.png 848w, https://substackcdn.com/image/fetch/$s_!Vt3F!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec50b5cd-e0f1-4c05-9c6f-a9fa59889fbf_975x250.png 1272w, https://substackcdn.com/image/fetch/$s_!Vt3F!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec50b5cd-e0f1-4c05-9c6f-a9fa59889fbf_975x250.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>A big thanks to Zach from ZeroOne Technology for taking pictures of the system in the rack!</figcaption></figure></div><p><span>And again, we’d like to thank both Dell and ZeroOne for, respectively, providing and hosting </span><a href="https://www.dell.com/en-us/shop/dell-poweredge-servers/new-poweredge-r6715-rack-server/spd/poweredge-r6715/pe_r6715_tm_vi_vp_sb" rel="">this PowerEdge R6715</a><span> without both of whom this article wouldn’t have been possible. </span></p><p><span>If you like the content then consider heading over to the </span><a href="https://www.patreon.com/ChipsandCheese" rel="">Patreon</a><span> or </span><a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ" rel="">PayPal</a><span> if you want to toss a few bucks to Chips and Cheese. Also consider joining the </span><a href="https://discord.gg/TwVnRhxgY2" rel="">Discord</a><span>.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Arenas in Rust (121 pts)]]></title>
            <link>https://russellw.github.io/arenas</link>
            <guid>45467032</guid>
            <pubDate>Fri, 03 Oct 2025 19:47:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://russellw.github.io/arenas">https://russellw.github.io/arenas</a>, See on <a href="https://news.ycombinator.com/item?id=45467032">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>

August 5, 2025

<figure>
<img src="https://russellw.github.io/ant-rozetsky-SLIFI67jv5k-unsplash.jpg" alt="Black metal empty building">
<figcaption>Photo by <a href="https://unsplash.com/@rozetsky?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Ant Rozetsky</a> on <a href="https://unsplash.com/photos/black-metal-empty-building-SLIFI67jv5k?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Unsplash</a></figcaption>
</figure>
      
A fast way to start an argument in a room full of programmers: “How do you implement a doubly linked list in Rust?”

<p>
At one level, this question is not very fair, because an answer to it as stated would be that one simply does not use doubly linked lists. They have been popular in introductory computer science lectures because they are a neat way to explain pointers in a data structure that's easy to draw on a whiteboard, but they are not a good match to modern hardware. The last time I used one was in the nineties. I know the Linux kernel uses them, but that design was also laid down in the nineties; if you were designing a kernel from scratch today, you would probably not do it that way.

</p><p>
At another level, it is fair because it is a simple, familiar proxy for all data structures with any kind of circular references. Consider a compiler holding a set of modules that may refer to each other. Or a game where objects may refer to their container. Or a graphic user interface where widgets may refer to a parent window. It might be reasonable to say some particular such structure is not the best solution in some particular case, but it is not reasonable to say that about all such structures in all cases.

</p><p>
And they are tricky in Rust because the language is founded on the idea that memory management should in general, be done by ownership. Essentially, Rust is an answer to the question, what happens if you start with C++ and encode the common ownership/RAII design patterns into the type system so fallible human brains don't need to enforce them. (And while we're at it, drop some of the legacy C baggage. And sprinkle in some features from ML family languages. And... okay, it's never really just one thing. But memory management is the focus here.) Circular references don't necessarily break ownership, but they do break the ability of the type system to keep track of it.

</p><p>
There are several ways to solve this problem. One way is to avoid using direct references to the particular class of objects at all. Instead, allocate a big array of objects, and refer to them with integer indexes into that array. There are several names that have been used for such arrays and indexes; let's call them arenas and handles.

</p><p>
At this point, smart programmers will spot what's going on. Essentially you are bypassing the notion of pointers provided directly by the hardware, only to reimplement your own address space, and your own notion of pointers within it. The next step of course is to write your own equivalents of <code>malloc</code> and <code>free</code> to allocate and deallocate objects within the arena.

</p><p>
Doesn't that mean you are throwing out all the memory safety properties you were hoping to achieve by using Rust in the first place? Wouldn't you be as well off to just go back to C and at least be honest about the fact that you have reverted to purely manual memory management?

</p><p>
That argument sounds logical, but it's not actually correct.

</p><p>
Consider: why were we so scared of memory unsafety in the first place? What's so bad about memory safety bugs that it was considered worth inventing a whole new language to fix them? (Yes, as I mentioned earlier, Rust has some other nice properties, but those would not have sufficed to drive a movement to replace C++ with a new language. They are bonuses. Memory safety was the driving force behind Rust.)

</p><p>
Memory safety bugs have two properties that make them scarier than most other kinds.

</p><h2>Nondeterminism</h2>

An array overflow, or use after free, is likely to manifest as an intermittent crash with no clear connection to the cause. Try to reproduce the problem in a debug build, maybe it goes away. Recompile with a couple of extra logging statements, maybe the crash goes away. Rerun the same binary with the same inputs, and thanks to ASLR, maybe the crash goes away. Maybe one day it shows a minute after the triggering cause, maybe another day it shows an hour after, or not at all. 

<p>
Handles are deterministic. If a bug made your program crash on the last run, it will crash the same way on this run.

</p><h2>Security</h2>

This one is even bigger.

<p>
If there exists an input that will cause a given C program to crash with a segmentation fault, what's the probability there exists another input that will allow remote code execution? In practice, the answer tends to be high, more than even expert intuition started off expecting.

</p><p>
And if the program is, say, a web browser, that can be bad. (It's not a coincidence Rust was invented by a browser company.)

</p><p>
There was a time you could say, okay but that only applies to the special category of security-critical programs. But these days (setting aside little scripts for personal use, assuming we are talking about published software), it's programs that <i>don't</i> have to cope with adversarial input, that are the special and shrinking category. This is true even of embedded systems; it's rare nowadays to find a smart device that doesn't expect to be connected to the Internet.

</p><p>
Given that the arena will still be subject to array bounds checking, handle bugs won't allow an attacker to overwrite arbitrary memory the way pointer bugs do. So using handles for your memory management, preserves the property, that bugs in your Rust code may lead to denial of service, but they are much less likely to lead to remote code execution.

</p><p>
And that is why, though arena memory management is isomorphic to old-fashioned manual memory management, it does not keep the particularly bad failure modes that motivated the move to Rust in the first place.
</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WireGuard topologies for self-hosting at home (120 pts)]]></title>
            <link>https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/</link>
            <guid>45466980</guid>
            <pubDate>Fri, 03 Oct 2025 19:43:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/">https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/</a>, See on <a href="https://news.ycombinator.com/item?id=45466980">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header><time datetime="2025-09-23 14:12:45 -0600 CST">September 23, 2025</time></header><p>Contents</p><nav id="TableOfContents"><ul><li><a href="#constraints">Constraints</a><ul><li><a href="#first-order-constraints">First-order constraints</a></li><li><a href="#second-order-constraints">Second-order constraints</a></li><li><a href="#non-constraints">Non-constraints</a></li></ul></li><li><a href="#resources">Resources</a></li><li><a href="#topologies">Topologies</a><ul><li><a href="#connecting-all-devices-in-the-same-physical-network-using-point-to-point-networking">Connecting all devices in the same physical network using point-to-point networking</a></li><li><a href="#fixing-an-issue-with-moving-targets">Fixing an issue with moving targets</a></li><li><a href="#connecting-from-the-outside">Connecting from the outside</a></li><li><a href="#adding-a-remote-peer">Adding a remote peer</a></li><li><a href="#routing-packets-through-the-remote-peer">Routing packets through the remote peer</a></li><li><a href="#introducing-hub-and-spoke">Introducing hub-and-spoke</a></li><li><a href="#rethinking-the-home-network-topology">Rethinking the home network topology</a></li><li><a href="#two-hubs-and-one-compromise">Two hubs and one compromise</a></li><li><a href="#reconsidering-the-hub-at-home">Reconsidering the hub at home</a></li><li><a href="#our-final-design">Our final design</a></li></ul></li><li><a href="#parting-thoughts">Parting thoughts</a></li><li><a href="#further-reading">Further reading</a></li></ul></nav><p>I recently migrated my self-hosted services from a VPS (virtual private server) at a remote data center to a physical server at home. This change was motivated by wanting to be in control of the hardware and network where said services run, while trying to keep things as simple as possible. What follows is a walk-through of how I reasoned through different <a href="https://www.wireguard.com/" rel="external noreferrer noopener">WireGuard</a> toplogies for the VPN (virtual private network) in which my devices and services reside.</p><p>Before starting, it’s worth emphasizing that using WireGuard (or a VPN altogether) is ont strictly required for self-hosting. WireGuard implies one more moving part in your system, the cost of which is justified only by what it affords you to do. The constraints that I outline below should provide clarity as to why using WireGuard is appropriate for my needs.</p><p>It goes without saying that not everyone has the same needs, resources, and threat model, all of which a design should account for. That said, there isn’t anything particularly special about what I’m doing. There is likely enough overlap here for this to be useful to individuals or small to medium-sized organizations looking to host their services.</p><p>I hope that this review helps others build a better mental model of WireGuard, and the sorts of networks that you can build up to per practical considerations. Going through this exercise proved to be an excellent learning experience, and that is worthwhile on its own.</p><p>This post assumes some familiarity with networking. This is a subject in which acronyms are frequently employed, so I’ve made sure to spell these out wherever introduced.</p><h2 id="constraints">Constraints</h2><p>The constraints behind the design of my network can be categorized into first-order and second-order constraints. Deploying WireGuard responds to the first-order constraints, whereas the specifics of <em>how</em> WireGuard is deployed responds to the second-order constraints.</p><h3 id="first-order-constraints">First-order constraints</h3><ol><li><p><strong>There should be no dependencies to services or hardware outside of the physical network.</strong> I should be able to connect to my self-hosted services <strong>while I’m at home</strong> as long as there’s electricity in the house and the hardware involved is operating without problems.</p></li><li><p><strong>Borrow elements of the <a href="https://www.nist.gov/publications/zero-trust-architecture" rel="external noreferrer noopener">Zero Trust Architecture</a> where appropriate.</strong> Right now that means treating all of my services and devices as resources, securing <strong>all</strong> communications (i.e not trusting the underlying network), and enforcing least-privileged access.</p></li><li><p><strong>Provisions made to connect to a device from outside the home network should be secondary and optional.</strong> While I do wish to use to my services while I’m away, satisfying this should not compromise the fundamental design of my setup. For example, I shouldn’t rely on tunneling services provided by third-parties.</p></li></ol><p>Choosing to deploy WireGuard is motivated by constraints two and three. Constraint one is not sufficient on its own to necessitate using WireGuard because everything can run on the local area network (LAN).</p><p>Once deployed, I should be able to connect all of my devices using hardware, software, and keys that I control within the boundaries of my home office. These devices all exist in the same physical network, but may reside in separate virtual LANs (VLANs) or subnets. Regardless, WireGuard is used to establish secure communications within and across these boundaries, while working in tandem with network and device firewalls for access control.</p><p>I cannot connect to my home network directly from the wide area network (WAN, e.g the Internet) because it is behind <a href="https://en.wikipedia.org/wiki/Carrier-grade_NAT" rel="external noreferrer noopener">Carrier-Grade Network Address Translation (CGNAT)</a>. A remote host is added to the WireGuard network to establish connections from outside. This host runs on hardware that I do not control, which goes against the spirit of the first constraint. However, an allowance is made considering that the role of this peer is not load-bearing in the overarching design, and can be removed from the network as needed.</p><h3 id="second-order-constraints">Second-order constraints</h3><p>Assuming WireGuard is now inherent in this design, its use should adhere to the following constraints:</p><ol><li><p><strong>Use WireGuard natively as opposed to software that builds on top of WireGuard.</strong> I choose to favor simplicity and ease of understanding rather than convenience or added features, <em>ergo</em>, complexity.</p></li><li><p><strong>Use of a control plane should not be required.</strong> All endpoints are first-class citizens and managed individually, regardless of using a network topology that confers routing responsibilities to a given peer.</p></li></ol><p>Satisfying these constraints preclude the use of solutions such as Tailscale, Headscale, or Netbird. Using WireGuard natively has the added benefit that I can rely on a vetted and stable version as packaged by my Linux distribution of choice, <a href="http://debian.org/" rel="external noreferrer noopener">Debian</a>.</p><h3 id="non-constraints">Non-constraints</h3><p>Lastly, it is worth stating requirements or features that are often found in designs such as these, but that are not currently relevant to me.</p><ol><li><p><strong>Mesh networking and direct peer-to-peer connections.</strong> It’s ok to have peers act as gateways if connections need to be established across different physical or logical networks. The size, throughput, and bandwidth of the network is small enough that prioritizing performance is not strictly necessary.</p></li><li><p><strong>Automatic discovery or key distribution</strong>. It’s ok for nodes in the network to be added or reconfigured manually.</p></li></ol><h2 id="resources">Resources</h2><p>Let’s look at the resources in the network, and how these connect with each other.</p><p>Consider the following matrix. Each row denotes whether the resource in the first column connects <strong>to</strong> the resources in the remaining columns, either to consume a service or perform a task. For example, we can tell that the server does not connect to any device, but all devices connect to the server.</p><table><thead><tr><th></th><th><strong>Server</strong></th><th><strong>Desktop</strong></th><th><strong>Laptop</strong></th><th><strong>Phone</strong></th><th><strong>Tablet</strong></th></tr></thead><tbody><tr><td><strong>Server</strong></td><td></td><td>No</td><td>No</td><td>No</td><td>No</td></tr><tr><td><strong>Desktop</strong></td><td>Yes</td><td></td><td>Yes</td><td>Yes</td><td>No</td></tr><tr><td><strong>Laptop</strong></td><td>Yes</td><td>Yes</td><td></td><td>No</td><td>No</td></tr><tr><td><strong>Phone</strong></td><td>Yes</td><td>Yes</td><td>No</td><td></td><td>No</td></tr><tr><td><strong>Tablet</strong></td><td>Yes</td><td>No</td><td>No</td><td>No</td><td></td></tr></tbody></table><p>Said specifically:</p><ol><li>The desktop computer connects to the server to access a calendar, git repositories, etc</li><li>The tablet connects to the server to download RSS feeds</li><li>The laptop and desktop connect with each other to sync files</li></ol><p>The purpose of this matrix is to determine which connections between devices ought to be supported, <strong>regardless of the network topology</strong>. This informs how WireGuard peers are configured, and what sort of firewall rules need to be established.</p><p>Before proceeding, let’s define the networks and device IP addresses that will be used.</p><table><thead><tr><th>Network</th><th>Protocol</th><th>Address</th><th>Netmask</th><th>Gateway</th></tr></thead><tbody><tr><td>LAN</td><td>IPv4</td><td>192.168.1.0</td><td>255.255.255.0</td><td>192.168.1.1</td></tr><tr><td>WireGuard</td><td>IPv4</td><td>10.55.2.0</td><td>255.255.255.0</td><td>-</td></tr></tbody></table><table><thead><tr><th>Device</th><th>LAN IP address</th><th>WireGuard IP address</th></tr></thead><tbody><tr><td>Desktop computer</td><td>192.168.1.6</td><td>10.55.2.11</td></tr><tr><td>Laptop</td><td>192.168.1.7</td><td>10.55.2.12</td></tr><tr><td>Phone</td><td>192.168.1.8</td><td>10.55.2.13</td></tr><tr><td>Tablet</td><td>192.168.1.9</td><td>10.55.2.14</td></tr><tr><td>Server</td><td>192.168.1.10</td><td>10.55.2.20</td></tr></tbody></table><p>The name of the WireGuard network interface will be <code>wg-home</code>, where applicable<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>. For purposes of this explanation, port <code>48192</code> will be used in all of the devices when a port needs to be defined.</p><h2 id="topologies">Topologies</h2><p>I’ll explore different topologies as I build to up to the design that I currently employ. By starting with the simplest topology, we can appreciate the benefits and trade-offs involved in each step, while strengthening our conceptual model of WireGuard.</p><p>Each topology below is accompanied by a simple diagram of the network. In it, the orange arrow denotes a device <strong>connecting to</strong> another device. Where two devices connect to each other, a bidirectional arrow is employed. Later on, green arrows denote a device forwarding packets to and from other resources.</p><h3 id="connecting-all-devices-in-the-same-physical-network-using-point-to-point-networking">Connecting all devices in the same physical network using point-to-point networking</h3><figure><picture><source srcset="https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/images/lan-p2p_hu_58f6e134ea62b637.webp" type="image/webp"><a href="https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/images/lan-p2p.jpg"><img loading="lazy" height="375" alt="Diagram showing point-to-point topology. An arrow connects each device that connects directly with another device, per the connections matrix. All devices are inside a surrounding box that denotes the LAN network." src="https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/images/lan-p2p_hu_a92b72a5aee8dc6a.jpg"></a></picture></figure><p>The basic scenario, and perhaps the most familiar to someone looking to start using WireGuard to self-host at home, is hosting in the network that is established by the router provided by an Internet service provider (ISP). Let’s assume its configuration has not been modified other than changing the Wi-Fi and admin passwords.</p><p>A topology that can be used here is <strong>point-to-point</strong>, where each device lists every other device it connects to as its peer. In WireGuard terminology, “peers” are endpoints configured to connect with each other to establish an encrypted “tunnel” through which packets are sent and received.</p><p>According to the connections matrix, the desktop computer and the server are peers, but the desktop computer and tablet aren’t.</p><p>The WireGuard configuration for the desktop computer looks as follows:</p><div><pre tabindex="0"><code data-lang="ini"><span><span><span>[Interface]</span>
</span></span><span><span><span>Address</span> <span>=</span> <span>10.55.2.11/32</span>
</span></span><span><span><span>ListenPort</span> <span>=</span> <span>48192</span>
</span></span><span><span><span>PrivateKey</span> <span>=</span> <span>kJdAdg2G5sh+BcusDTPYv/nZOscXW5kuh5wILkOC63Q=</span>
</span></span><span><span>
</span></span><span><span><span># Server</span>
</span></span><span><span><span>[Peer]</span>
</span></span><span><span><span>PublicKey</span> <span>=</span> <span>5vj58uZIALlPwhelXQilQgCY0jSN6iOpBZOcZj2shEU=</span>
</span></span><span><span><span>AllowedIPs</span> <span>=</span> <span>10.55.2.20/32</span>
</span></span><span><span><span>Endpoint</span> <span>=</span> <span>192.168.1.10:48192</span>
</span></span><span><span>
</span></span><span><span><span># Laptop</span>
</span></span><span><span><span>[Peer]</span>
</span></span><span><span><span>PublicKey</span> <span>=</span> <span>Aa+dFAg5CWQ3U/ZLJbxfhYiJcW9lJP+tuWZ0ElHuY14=</span>
</span></span><span><span><span>AllowedIPs</span> <span>=</span> <span>10.55.2.12/32</span>
</span></span><span><span><span>Endpoint</span> <span>=</span> <span>192.168.1.7:48192</span>
</span></span><span><span>
</span></span><span><span><span># Phone</span>
</span></span><span><span><span>[Peer]</span>
</span></span><span><span><span>PublicKey</span> <span>=</span> <span>Ri10a7gcZ+sbFc44HvZVOvpTqWydi6OYQWZMMzAo3Eo=</span>
</span></span><span><span><span>AllowedIPs</span> <span>=</span> <span>10.55.2.13/32</span>
</span></span><span><span><span>Endpoint</span> <span>=</span> <span>192.168.1.8:48192</span>
</span></span></code></pre></div><p>Note that the LAN IP address of each peer is specified under <code>Endpoint</code>. This is used to find the peer’s device and establish the WireGuard tunnel. <code>AllowedIPs</code> specifies the IP addresses used <strong>within</strong> the WireGuard network. In other words, the phone is the desktop’s peer, it can be found at <code>192.168.1.8:48192</code> to establish the tunnel.</p><p>Let’s assume each of these devices have firewalls that allow UDP traffic through port <code>48192</code> and all subsequent traffic through the WireGuard <code>wg-home</code> interface. Once the WireGuard configurations of the server, laptop, and phone include the corresponding peers, secure communication is established through the WireGuard network interface.</p><p>Let’s try sending a packet from the desktop computer to the phone.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>$ traceroute 10.55.2.13 
</span></span><span><span>traceroute to 10.55.2.13 <span>(</span>10.55.2.13<span>)</span>, <span>30</span> hops max, <span>60</span> byte packets
</span></span><span><span> <span>1</span>  10.55.2.13 <span>(</span>10.55.2.13<span>)</span>  950.434 ms  950.550 ms  950.493 ms
</span></span></code></pre></div><p>The packet was routed directly to the phone and echoed back.</p><p>At this point access control is enforced in each device’s firewall. Allowing everything that comes through the <code>wg-home</code> interface is convenient, but it should be limited to the relevant ports and protocols for least-privileged access.</p><h3 id="fixing-an-issue-with-moving-targets">Fixing an issue with moving targets</h3><p>An obvious problem with this scenario is that the Dynamic Host Configuration Protocol (DHCP) server in the router likely allocates IP addresses dynamically when devices connect to the LAN network. The IP address for a device may thus change over time, and WireGuard will be unable to find a peer to establish a connection to it.</p><p>For example, I’m at home and my phone dies. The LAN IP address <code>192.168.1.8</code> is freed and assigned to another device that comes online. WireGuard will attempt to connect to <code>192.168.1.8</code> (per <code>Endpoint</code>) and fail for any of the following reasons:</p><ol><li>Said device is not running WireGuard</li><li>Said device is using a different <code>Address</code> or <code>ListenPort</code>, in which case the peer’s <code>AllowedIPs</code> or port in <code>Endpoint</code> does not match</li><li>Said device is using a different <code>PrivateKey</code>, in which case the peer’s <code>PublicKey</code> does not match</li></ol><p>Fortunately, most routers support configuring static IP addresses for a given device in the network. Doing so for all devices in our WireGuard network fixes this problem as the IP address used in <code>Endpoint</code> will be reserved accordingly.</p><h3 id="connecting-from-the-outside">Connecting from the outside</h3><figure><picture><source srcset="https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/images/lan-p2p-remote_hu_4ffa3a1530ea532d.webp" type="image/webp"><a href="https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/images/lan-p2p-remote.jpg"><img loading="lazy" height="375" alt="Diagram showing point-to-point topology with a device added outside of the LAN network to provide connectivity from the outside." src="https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/images/lan-p2p-remote_hu_e32949e4d7b15f59.jpg"></a></picture></figure><p>Suppose I want to work at a coffee shop, but still need access to something that’s hosted on my home server. As mentioned in the constraints, my home network is behind CGNAT. This means that I cannot connect directly to it using whatever WAN IP address my router is using at the moment.</p><p>What I can do instead is use a device that has a publicly routable IP address and make that a WireGuard peer of our server. In this case that’ll be a VPS in some data center.</p><p>How is the packet ultimately relayed to and from the server at home? Both the server and laptop established direct encrypted tunnels with the VPS. WireGuard on the VPS will receive the encrypted packets from the laptop, decrypt them, and notice that they’re meant for the server. It will then encrypt these packets with the server’s key and send them through the server’s tunnel. It’ll do same thing with the server’s response, except towards the laptop using the laptop’s tunnel.</p><p>A device that forwards packets between peers needs to be configured for IPv4 packet forwarding. I will not cover the specifics of this configuration because it depends on what operating system and firewall are used<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>.</p><h3 id="adding-a-remote-peer">Adding a remote peer</h3><p>The VPS has a public IP address of <code>162.231.77.9</code>, and its WireGuard IP address will be <code>10.55.2.2</code>. The laptop and server are listed as peers in its WireGuard configuration:</p><div><pre tabindex="0"><code data-lang="ini"><span><span><span>[Interface]</span>
</span></span><span><span><span>Address</span> <span>=</span> <span>10.55.2.2/32</span>
</span></span><span><span><span>ListenPort</span> <span>=</span> <span>48192</span>
</span></span><span><span><span>PrivateKey</span> <span>=</span> <span>wMKvhMa7BSJvRe9+t7fiymFMqcHlxeI64uhjoLEKPWo=</span>
</span></span><span><span>
</span></span><span><span><span># Server</span>
</span></span><span><span><span>[Peer]</span>
</span></span><span><span><span>PublicKey</span> <span>=</span> <span>5vj58uZIALlPwhelXQilQgCY0jSN6iOpBZOcZj2shEU=</span>
</span></span><span><span><span>AllowedIPs</span> <span>=</span> <span>10.55.2.20/32</span>
</span></span><span><span>
</span></span><span><span><span># Laptop</span>
</span></span><span><span><span>[Peer]</span>
</span></span><span><span><span>PublicKey</span> <span>=</span> <span>Aa+dFAg5CWQ3U/ZLJbxfhYiJcW9lJP+tuWZ0ElHuY14=</span>
</span></span><span><span><span>AllowedIPs</span> <span>=</span> <span>10.55.2.12/32</span>
</span></span></code></pre></div><p>Note that <code>Endpoint</code> is omitted for each peer. The publicly routable IP addresses of the laptop and the home router are not known to us. Even if they were, they cannot be reached by the VPS. However, they will be known to the VPS when these connect to it.</p><p>Now, the server at home adds the VPS as its peer, using the VPS public IP address as its <code>Endpoint</code>:</p><div><pre tabindex="0"><code data-lang="ini"><span><span><span># VPS</span>
</span></span><span><span><span>[Peer]</span>
</span></span><span><span><span>PublicKey</span> <span>=</span> <span>7vXZGWpHp1PimrlbvwQ3sEOFUPx+1kq8Fdq4dv950m0=</span>
</span></span><span><span><span>AllowedIPs</span> <span>=</span> <span>10.55.2.2/32</span>
</span></span><span><span><span>Endpoint</span> <span>=</span> <span>162.231.77.9:48192</span>
</span></span><span><span><span>PersistentKeepalive</span> <span>=</span> <span>25</span>
</span></span></code></pre></div><p>We also make use of <code>PersistentKeepalive</code> to send an empty packet every 25 seconds. This is done to establish the tunnel ahead of time, and to keep it open. This is necessary because otherwise the tunnel may not exist when I’m at the coffee shop trying to access the server at home. Remember, the VPS doesn’t know how to reach the server unless the server is connected to it.</p><h3 id="routing-packets-through-the-remote-peer">Routing packets through the remote peer</h3><p>Let’s take a careful look at the laptop’s configuration, and what we’re looking to achieve. When the laptop is at home, it connects to the server using an endpoint address that is routable within the home LAN network. This endpoint address is not routable when I’m outside, in which case I want the connection to go through the VPS.</p><p>To achieve this, the laptop maintains two mutually-exclusive WireGuard interfaces: <code>wg-home</code> and <code>wg-remote</code>. The former is active only while I’m in the home network, and the latter while I’m on the go.</p><p>Unlike the server, the VPS does not need to be added as a peer to the laptop’s <code>wg-home</code> interface because it doesn’t need connect to it while at home. Instead, the VPS is added to the <code>wg-remote</code> configuration:</p><div><pre tabindex="0"><code data-lang="ini"><span><span><span>[Interface]</span>
</span></span><span><span><span>Address</span> <span>=</span> <span>10.55.2.12/32</span>
</span></span><span><span><span>PrivateKey</span> <span>=</span> <span>EAuj44uBRPWyV6d9I4NKT8WmRQP+a73X/ce+58ZrPVs=</span>
</span></span><span><span>
</span></span><span><span><span># VPS</span>
</span></span><span><span><span>[Peer]</span>
</span></span><span><span><span>PublicKey</span> <span>=</span> <span>7vXZGWpHp1PimrlbvwQ3sEOFUPx+1kq8Fdq4dv950m0=</span>
</span></span><span><span><span>AllowedIPs</span> <span>=</span> <span>10.55.2.2/32, 10.55.2.20/32</span>
</span></span><span><span><span>Endpoint</span> <span>=</span> <span>162.231.77.9:48192</span>
</span></span></code></pre></div><p>The <code>[Interface]</code> section for both <code>wg-home</code> and <code>wg-remote</code> is mostly the same. The laptop should have the same adress and key, regardless of where it is. Only <code>ListenPort</code> is omitted in <code>wg-remote</code> because no other device will look to connect to it, in which case we can have WireGuard set a port dynamically.</p><p>What differs is the peer configuration. In <code>wg-remote</code> the VPS is set as the only peer. However, the home server’s IP address <code>10.55.2.20</code> is added to the VPS’ list of <code>AllowedIPs</code>. WireGuard uses this information to route any packets for the VPS <em>or</em> the server through the VPS.</p><p>Unlike the server’s peer configuration for the VPS, <code>PersistentKeepalive</code> is not needed because the laptop is always the one initiating the tunnel when it reaches out to the server.</p><p>We can verify that packets are being routed appropriately to the server through the VPS:</p><div><pre tabindex="0"><code data-lang="sh"><span><span>$ traceroute 10.55.2.20 
</span></span><span><span>traceroute to 10.55.2.20 <span>(</span>10.55.2.20<span>)</span>, <span>30</span> hops max, <span>60</span> byte packets
</span></span><span><span> <span>1</span>  10.55.2.2 <span>(</span>10.55.2.2<span>)</span>  314.556 ms  314.482 ms  314.469 ms
</span></span><span><span> <span>2</span>  10.55.2.20 <span>(</span>10.55.2.20<span>)</span>  320.954 ms  320.821 ms 320.870 ms
</span></span></code></pre></div><h3 id="introducing-hub-and-spoke">Introducing hub-and-spoke</h3><p>We solved for outside connectivity using a network topology called hub-and-spoke. The laptop and home server are not connecting point-to-point.</p><p>The VPS acts as a hub or gateway through which connections among members of the network (i.e the spokes) are routed. If we scope down our network to just the laptop and the home server, we see how this hub is not only a peer of every spoke, but also just its only peer.</p><p>Yet, how exactly is the packet routed back to the laptop? Mind you, at home the laptop is a peer of the server. When the server responds to the laptop, it will attempt to route the response directly to the laptop’s peer endpoint. This fails because the laptop is not actually reachable via that direct connection when I’m on the go. This makes the laptop a “roaming client”; it connects to the network from different locations, and its <code>Endpoint</code> may change.</p><p>This all works because the hub has been configured to do Network Address Translation (NAT); it is replacing the source address of each packet for its own as it is being forwarded. The spokes at end of each hub accept the packets because they appear to originate from its peer. In other words, when the laptop is reaching out to the home server, the server sees traffic coming from the VPS and returns it there.</p><p>The hub is forwarding packets among the spokes without regards to access control. Thus, its firewall should be configured for least-privilege access. For example, if the laptop is only accessing git repositories in the home server over SSH, then the hub firewall should only allow forwarding from the laptop’s peer connection to the home server’s IP address and SSH port.</p><p>Let’s reiterate. If I now wish to sync my laptop with my desktop computer from outside the network, I would be adding yet another spoke to this hub. The desktop computer and the VPS configure each other as peers, while the desktop’s IP address is included in the VPS’ <code>AllowedIPs</code> list of the laptop’s <code>wg-remote</code> configuration.</p><div><pre tabindex="0"><code data-lang="diff"><span><span><span>+ AllowedIPs = 10.55.2.2/32, 10.55.2.20/32, 10.55.2.11/32
</span></span></span><span><span><span></span><span>- AllowedIPs = 10.55.2.2/32, 10.55.2.20/32
</span></span></span></code></pre></div><h3 id="rethinking-the-home-network-topology">Rethinking the home network topology</h3><p>Our topology <em>within</em> the home network is still point-to-point. As soon as I return home, my laptop will connect directly to the server when I toggle <code>wg-remote</code> off and <code>wg-home</code> on. But now that we know about <strong>hub-and-spoke</strong>, it might make sense to consider using it at home as well.</p><figure><picture><source srcset="https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/images/lan-hs_hu_e49e5313ce05b435.webp" type="image/webp"><a href="https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/images/lan-hs.jpg"><img loading="lazy" height="375" alt="Diagram showing hub-and-spoke topology in the home network. All devices have an arrow that connect them to the server box." src="https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/images/lan-hs_hu_d2de0590db210cbb.jpg"></a></picture></figure><p>According to the connection matrix, the server can assume the role of a hub because all other devices already connect to it. Likewise, the server runs 24/7, so it will always be online to route packets.</p><p>This topology simplifies the WireGuard configurations for all of the spokes. The desktop computer, phone, laptop, and tablet can now list the server as its only peer in <code>wg-home</code>.</p><p>This is convenient because now only one static address in the LAN network needs to be allocated by the DHCP server – the server’s.</p><p>Consider the changes to the WireGuard configuration of the desktop computer.</p><div><pre tabindex="0"><code data-lang="diff"><span><span>[Interface]
</span></span><span><span>Address = 10.55.2.11/32
</span></span><span><span>ListenPort = 48192
</span></span><span><span>PrivateKey = kJdAdg2G5sh+BcusDTPYv/nZOscXW5kuh5wILkOC63Q=
</span></span><span><span>
</span></span><span><span># Server
</span></span><span><span>[Peer]
</span></span><span><span>PublicKey = 5vj58uZIALlPwhelXQilQgCY0jSN6iOpBZOcZj2shEU=
</span></span><span><span><span>- AllowedIPs = 10.55.2.20/32
</span></span></span><span><span><span></span><span>+ AllowedIPs = 10.55.2.20/32, 10.55.2.12/32, 10.55.2.13/231
</span></span></span><span><span><span></span>Endpoint = 192.168.1.10:48192
</span></span><span><span><span>-
</span></span></span><span><span><span>- # Laptop
</span></span></span><span><span><span>- [Peer]
</span></span></span><span><span><span>- PublicKey = Aa+dFAg5CWQ3U/ZLJbxfhYiJcW9lJP+tuWZ0ElHuY14=
</span></span></span><span><span><span>- AllowedIPs = 10.55.2.12/32
</span></span></span><span><span><span>- Endpoint = 192.168.1.7:48192
</span></span></span><span><span><span>-
</span></span></span><span><span><span>- # Phone
</span></span></span><span><span><span>- [Peer]
</span></span></span><span><span><span>- PublicKey = Ri10a7gcZ+sbFc44HvZVOvpTqWydi6OYQWZMMzAo3Eo=
</span></span></span><span><span><span>- AllowedIPs = 10.55.2.13/32
</span></span></span><span><span><span>- Endpoint = 192.168.1.8:48192
</span></span></span><span><span><span>-
</span></span></span><span><span><span>-# VPS
</span></span></span><span><span><span>-[Peer]
</span></span></span><span><span><span>-PublicKey = 7vXZGWpHp1PimrlbvwQ3sEOFUPx+1kq8Fdq4dv950m0=
</span></span></span><span><span><span>-AllowedIPs = 10.55.2.2/32
</span></span></span><span><span><span>-Endpoint = 162.231.77.9:48192
</span></span></span></code></pre></div><p>All peers are removed except the server, and the IPs of the phone and laptop are added to the server’s of <code>AllowedIPs</code>. WireGuard will route packets for these other hosts through the server. We could also use Classless Inter-Domain Routing (CIDR) notation to state that packets for <em>all</em> hosts in the WireGuard network go through the server peer:</p><div><pre tabindex="0"><code data-lang="diff"><span><span><span>+ AllowedIPs = 10.55.2.0/24
</span></span></span><span><span><span></span><span>- AllowedIPs = 10.55.2.20/32, 10.55.2.12/32, 10.55.2.13/231
</span></span></span></code></pre></div><p>The server, in turn, keeps listing every device at home as its peer but no longer needs an <code>Endpoint</code> for each. The peers will initiate the connection to the server.</p><div><pre tabindex="0"><code data-lang="diff"><span><span>[Interface]
</span></span><span><span>Address = 10.55.2.20/32
</span></span><span><span>ListenPort = 48192
</span></span><span><span>PrivateKey = qFu8xkaA69wnX8aWURUUNAf9Ll1yU8RvjXczCiXbMGM=
</span></span><span><span>
</span></span><span><span># Desktop
</span></span><span><span>[Peer]
</span></span><span><span>PublicKey = doTWOdYC8hwpKVrc6tK4UHEXspO4CuajPORLHOeri2c=
</span></span><span><span>AllowedIPs = 10.55.2.11/32
</span></span><span><span><span>- Endpoint = 192.168.1.6:48192
</span></span></span><span><span><span></span>
</span></span><span><span># Laptop
</span></span><span><span>[Peer]
</span></span><span><span>PublicKey = Aa+dFAg5CWQ3U/ZLJbxfhYiJcW9lJP+tuWZ0ElHuY14=
</span></span><span><span>AllowedIPs = 10.55.2.12/32
</span></span><span><span><span>- Endpoint = 192.168.1.7:48192
</span></span></span><span><span><span></span>
</span></span><span><span># Phone
</span></span><span><span>[Peer]
</span></span><span><span>PublicKey = Ri10a7gcZ+sbFc44HvZVOvpTqWydi6OYQWZMMzAo3Eo=
</span></span><span><span>AllowedIPs = 10.55.2.13/32
</span></span><span><span><span>- Endpoint = 192.168.1.8:48192
</span></span></span><span><span><span></span>
</span></span><span><span># Tablet
</span></span><span><span>[Peer]
</span></span><span><span>PublicKey = dcRjeKjoDujcH/Ziy4stHIzCXSr+tlaeeyP6IEcc+EY=
</span></span><span><span>AllowedIPs = 10.55.2.14/32
</span></span><span><span><span>- Endpoint = 192.168.1.9:48192
</span></span></span><span><span><span></span>
</span></span><span><span># VPS
</span></span><span><span>[Peer]
</span></span><span><span>PublicKey = 7vXZGWpHp1PimrlbvwQ3sEOFUPx+1kq8Fdq4dv950m0=
</span></span><span><span>AllowedIPs = 10.55.2.2/32, 10.55.2.20/32
</span></span><span><span>Endpoint = 162.231.77.9:48192
</span></span></code></pre></div><p>Once again, let’s test sending a packet from the desktop computer to the phone.</p><div><pre tabindex="0"><code data-lang="sh"><span><span>$ traceroute 10.55.2.13 
</span></span><span><span>traceroute to 10.55.2.13 <span>(</span>10.55.2.13<span>)</span>, <span>30</span> hops max, <span>60</span> byte packets
</span></span><span><span> <span>1</span>  10.55.2.20 <span>(</span>10.55.2.20<span>)</span>  1.190 ms  1.328 ms  1.528 ms
</span></span><span><span> <span>2</span>  10.55.2.13 <span>(</span>10.55.2.13<span>)</span>  49.954 ms  50.142 ms  50.104 ms
</span></span></code></pre></div><p>The packet was hops once through the server (<code>10.55.2.20</code>), is received by the phone, and is echoed back.</p><p>The downside to this topology is that the server is now a single point of failure. If the server dies then the spokes won’t be able to connect with each other through WireGuard. There’s also an added cost to having every packet flow through the hub.</p><p>As for access control, much like we saw in the VPS, the hub now concentrates firewalling responsibilities. It knows which peer is looking to connect to which peer, thus it should establish rules for which packets can be forwarded. This is not mutually exclusive with input firewall rules on each device; those should exist as well.</p><h3 id="two-hubs-and-one-compromise">Two hubs and one compromise</h3><p>We’ve seen that home hub will route packets between the spokes. Furthermore, because it is peer of the VPS, the server can be used to route connections coming from outside the LAN network. Effectively, these are two hubs that connect to each other so that packets can flow across separate physical networks.</p><p>If the laptop wants to sync with the desktop while it is outside the LAN network, then the packets make two hops: once through the VPS, and another through the server. If the laptop is within the LAN network, the packets hop only once through the server.</p><p>Yet, there’s a subtle caveat to this design. The laptop can initiate a sync with the desktop from outside the LAN network and receive the response that it expects. However, the desktop can only initiate a sync with the laptop while the latter is within the LAN network. Why?</p><p>Similar to our previous example of the laptop communicating with the server, the laptop is configured as a peer of the home hub. When the desktop initiates a sync, the server will attempt to route the packet to the laptop. Per our last change, the laptop doesn’t have a fixed <code>Endpoint</code> and there is no established tunnel because the laptop is outside the network. Additionally, the home hub is not configured to route packets destined for the laptop through the VPS peer. The packet is thus dropped by the hub.</p><p>One could look into making the routing dynamic such that the packets are delivered through other means, perhaps through mesh networking. But herein lies a compromise that I’ve made. In this design, a spoke in the home hub cannot initiate connections to a roaming client. It can only receive connections from them, because the roaming client uses NAT through the remote hub.</p><p>I’m perfectly fine with this compromise as I don’t actually need this bidirectionality, and I don’t want the additional complexity from solving this issue. The remote hub facilitates tunneling <em>into</em> the home hub, not <em>out</em> of. My needs call for allowing my mobile devices (e.g laptop, phone, tablet) to communicate with the non-mobile devices at home (e.g server, desktop), and this has been solved.</p><h3 id="reconsidering-the-hub-at-home">Reconsidering the hub at home</h3><p>At this point we’re done insofar the overarching topology of our WireGuard network, but there is an improvement that can be made to make our home hub less brittle.</p><figure><picture><source srcset="https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/images/lan-hs-r_hu_49b38cc63599695a.webp" type="image/webp"><a href="https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/images/lan-hs-r.jpg"><img loading="lazy" height="375" alt="Diagram showing hub-and-spoke topology in the home network. All devices have an arrow that connect them to the router box." src="https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/images/lan-hs-r_hu_76c60983f8f479f4.jpg"></a></picture></figure><p>Consider the case where I’m using a router that can run WireGuard. Making the router the hub of our home network poses some benefits over the previous setup.</p><p>First, the router is already a single point of failure by way of being responsible for the underlying LAN network. Making the router the hub isn’t as costly as it is with some other device in the network.</p><p>Second, all devices in the network are already connected to the router. This simplifies the overall configuration because it is no longer necessary to configure static IP addresses in the DHCP server. Instead, each spoke can use the network gateway address to reach the hub.</p><p>Let’s the assume that the gateway for the LAN network is <code>192.168.1.1</code>, and the WireGuard IP address for the router is <code>10.55.2.1</code>.</p><p>Each spoke replaces the server peer with the router’s, and uses the gateway address for its <code>Endpoint</code>. For example, in the desktop computer:</p><div><pre tabindex="0"><code data-lang="diff"><span><span>[Interface]
</span></span><span><span>Address = 10.55.2.11/32
</span></span><span><span>ListenPort = 48192
</span></span><span><span>PrivateKey = kJdAdg2G5sh+BcusDTPYv/nZOscXW5kuh5wILkOC63Q=
</span></span><span><span>
</span></span><span><span><span>+ # Router
</span></span></span><span><span><span>+ [Peer]
</span></span></span><span><span><span>+ PublicKey = bGDvNVfFvSsZtku3vXZx2xzgLIC8mtfQLCfcVy/gajs=
</span></span></span><span><span><span>+ AllowedIPs = 10.55.2.0/24
</span></span></span><span><span><span>+ Endpoint = 192.168.1.1:48192
</span></span></span><span><span><span></span><span>- # Server
</span></span></span><span><span><span>- [Peer]
</span></span></span><span><span><span>- PublicKey = 5vj58uZIALlPwhelXQilQgCY0jSN6iOpBZOcZj2shEU=
</span></span></span><span><span><span>- AllowedIPs = 10.55.2.0/24
</span></span></span><span><span><span>- Endpoint = 192.168.1.10:48192
</span></span></span></code></pre></div><p>The server is demoted to a spoke and is configured like all other spokes. In turn, the router lists all peers like the server previously did:</p><div><pre tabindex="0"><code data-lang="ini"><span><span><span>[Interface]</span>
</span></span><span><span><span>Address</span> <span>=</span> <span>10.55.2.1/32</span>
</span></span><span><span><span>ListenPort</span> <span>=</span> <span>48192</span>
</span></span><span><span><span>PrivateKey</span> <span>=</span> <span>UN8CdQKylNzjP7LpB+nSmMoeBxvmPtvtDKG2RylZZ10=</span>
</span></span><span><span>
</span></span><span><span><span># Server</span>
</span></span><span><span><span>[Peer]</span>
</span></span><span><span><span>PublicKey</span> <span>=</span> <span>5vj58uZIALlPwhelXQilQgCY0jSN6iOpBZOcZj2shEU=</span>
</span></span><span><span><span>AllowedIPs</span> <span>=</span> <span>10.55.2.20/32</span>
</span></span><span><span>
</span></span><span><span><span># Desktop</span>
</span></span><span><span><span>[Peer]</span>
</span></span><span><span><span>PublicKey</span> <span>=</span> <span>doTWOdYC8hwpKVrc6tK4UHEXspO4CuajPORLHOeri2c=</span>
</span></span><span><span><span>AllowedIPs</span> <span>=</span> <span>10.55.2.11/32</span>
</span></span><span><span>
</span></span><span><span><span># Laptop</span>
</span></span><span><span><span>[Peer]</span>
</span></span><span><span><span>PublicKey</span> <span>=</span> <span>Aa+dFAg5CWQ3U/ZLJbxfhYiJcW9lJP+tuWZ0ElHuY14=</span>
</span></span><span><span><span>AllowedIPs</span> <span>=</span> <span>10.55.2.12/32</span>
</span></span><span><span>
</span></span><span><span><span># Phone</span>
</span></span><span><span><span>[Peer]</span>
</span></span><span><span><span>PublicKey</span> <span>=</span> <span>Ri10a7gcZ+sbFc44HvZVOvpTqWydi6OYQWZMMzAo3Eo=</span>
</span></span><span><span><span>AllowedIPs</span> <span>=</span> <span>10.55.2.13/32</span>
</span></span><span><span>
</span></span><span><span><span># Tablet</span>
</span></span><span><span><span>[Peer]</span>
</span></span><span><span><span>PublicKey</span> <span>=</span> <span>dcRjeKjoDujcH/Ziy4stHIzCXSr+tlaeeyP6IEcc+EY=</span>
</span></span><span><span><span>AllowedIPs</span> <span>=</span> <span>10.55.2.14/32</span>
</span></span><span><span>
</span></span><span><span><span># VPS</span>
</span></span><span><span><span>[Peer]</span>
</span></span><span><span><span>PublicKey</span> <span>=</span> <span>7vXZGWpHp1PimrlbvwQ3sEOFUPx+1kq8Fdq4dv950m0=</span>
</span></span><span><span><span>AllowedIPs</span> <span>=</span> <span>10.55.2.2/32, 10.55.2.20/32</span>
</span></span><span><span><span>Endpoint</span> <span>=</span> <span>162.231.77.9:48192</span>
</span></span></code></pre></div><p>Again, the firewall in the router is now responsible for enforcing access control between spokes.</p><h3 id="our-final-design">Our final design</h3><p>For the sake of illustrating how much further the underlying networks can evolve without interfering with the WireGuard network, consider the final design.</p><figure><picture><source srcset="https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/images/lan-hs-multiple_hu_2f20144342ece540.webp" type="image/webp"><a href="https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/images/lan-hs-multiple.jpg"><img loading="lazy" height="375" alt="Diagram showing hub-and-spoke topology in the home network, as well as outside of the network. All devices have an arrow that connect it to the corresponding hub." src="https://garrido.io/notes/wireguard-topologies-for-self-hosting-at-home/images/lan-hs-multiple_hu_4c62e1d473c6f160.jpg"></a></picture></figure><p>I’ve broken apart the LAN network into separate VLANs to isolate network traffic. The server resides in its own VLAN, and client devices in another. The router keeps on forwarding packets in WireGuard network regardless of where these devices are.</p><p>The only change that is necessary to keep things working is to update <code>Endpoint</code> address for the router peer in each spoke. The spoke now uses the corresponding VLAN gateway address, rather than that of the LAN network:</p><div><pre tabindex="0"><code data-lang="diff"><span><span>[Interface]
</span></span><span><span>Address = 10.55.2.11/32
</span></span><span><span>ListenPort = 48192
</span></span><span><span>PrivateKey = kJdAdg2G5sh+BcusDTPYv/nZOscXW5kuh5wILkOC63Q=
</span></span><span><span>
</span></span><span><span># Router
</span></span><span><span>[Peer]
</span></span><span><span>PublicKey = bGDvNVfFvSsZtku3vXZx2xzgLIC8mtfQLCfcVy/gajs=
</span></span><span><span>AllowedIPs = 10.55.2.1/32, 10.55.2.0/24
</span></span><span><span><span>+ Endpoint = 10.2.0.1:48192
</span></span></span><span><span><span></span><span>- Endpoint = 192.168.1.1:48192
</span></span></span></code></pre></div><h2 id="parting-thoughts">Parting thoughts</h2><p>I’ve been using this setup for some months now and it’s been working without issues. A couple of thoughts come to mind having gone through this exercise and written about it.</p><p>Running WireGuard on the router simplifies things considerably. If the home network were not behind CGNAT then I could do away with the VPS hub altogether. I would still need separate WireGuard interfaces for when I’m on the go, but that’s not a big deal. Nonetheless, <em>within</em> the LAN network, configuration is simpler by using a hub-and-spoke topology with the router as hub. Centralizing access control on the router’s firewall is also appreciated.</p><p>WireGuard is simple to deploy and it <em>just</em> works. Nonetheless, some knowledge of networking is required to think through how to deploy WireGuard appropriately for a given context. Being comfortable with configuring interfaces and firewalls is also necessary to troubleshoot the inevitable connectivity issues.</p><p>One can appreciate why solutions that abstract over WireGuard exist. I used Tailscale extensively before this and did not have think through things as much as I did here. This was all solved for me. I just had to install the agent on each device, authorize it, and suddenly packets moved securely and efficiently across networks.</p><p>And yet, WireGuard was there all along and I knew that I could unearth the abstraction. Now I appreciate its simplicity even more, and take relish in having a stronger understanding of what I previously took for granted.</p><p>Lastly, I purposefully omitted other aspects of my WireGuard setup for self-hosting, particularly around DNS. This will be the subject of another article, which is rather similar to the one I wrote on using <a href="https://garrido.io/notes/tailscale-nextdns-custom-domains/">Tailscale with custom domains</a>. Furthermore, a closer look at access control in this topology might be of interest to others considering that there are multiple firewalls that come into play.</p><h2 id="further-reading">Further reading</h2><ul><li><a href="https://www.wireguard.com/papers/wireguard.pdf" rel="external noreferrer noopener">WireGuard: Next Generation Kernel Network Tunnel</a></li><li><a href="https://mwl.link/networking-for-system-administrators.html" rel="external noreferrer noopener">Networking for System Administrators</a></li><li><a href="https://www.procustodibus.com/blog/2020/10/wireguard-topologies/" rel="external noreferrer noopener">Primary WireGuard Topologies</a></li><li><a href="https://tailscale.com/blog/how-tailscale-works" rel="external noreferrer noopener">How Tailscale works</a></li></ul></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jules, remote coding agent from Google Labs, announces API (188 pts)]]></title>
            <link>https://jules.google/docs/changelog/</link>
            <guid>45466588</guid>
            <pubDate>Fri, 03 Oct 2025 19:08:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jules.google/docs/changelog/">https://jules.google/docs/changelog/</a>, See on <a href="https://news.ycombinator.com/item?id=45466588">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <article> <h2 id="jules-in-the-command-line"> Jules in the command line </h2> <b id="wed-oct-01-2025-190000-gmt-0500-central-daylight-time"> October 2, 2025 </b> <p><img src="https://jules.google/docs/_astro/julestools.5p-1nZtH_Z2224ry.webp" alt="Jules Tools" width="2215" height="1302" loading="lazy" decoding="async"></p>
<p>We’re launching Jules Tools, a new command-line interface designed to give you direct control over your AI coding agent, making it scriptable, customizable, and easy to integrate into your existing workflows.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>Direct Control:</strong> Create tasks (jules remote new), list active sessions (jules remote list), and monitor Jules without leaving your command line.</li>
<li><strong>Apply Patches Locally:</strong> Instantly pull work-in-progress code from an active Jules session and apply it to your local machine. This lets you test changes immediately, without waiting for a commit to GitHub.</li>
<li><strong>Scriptable &amp; Composable:</strong> Integrate Jules into your automations by piping in output from other tools like gh, jq, or cat.</li>
<li><strong>Interactive Dashboard:</strong> For a more guided experience, launch the built-in terminal user interface (TUI) to create and manage tasks step-by-step.</li>
</ul>
<p><strong>How to Install:</strong></p>
<p>Install globally via npm:
<code dir="auto">npm install -g @google/jules</code></p>
<p>Or run directly without a permanent installation:
<code dir="auto">npx @google/jules</code></p>
<p><strong>Starter Commands to Try:</strong></p>
<p>See all available commands:
<code dir="auto">jules help</code></p>
<p>List all repos connected to Jules:
<code dir="auto">jules remote list --repo</code></p>
<p>Create a new task in a specific repo:
<code dir="auto">jules remote new --repo torvalds/linux --session "write unit tests"</code></p>
<br>
<h5><strong>A Note for Google Workspace Users</strong></h5>
<p>Support for workspace users is coming later in October!</p>
<p>If you run into any issues, please share your experience with us via in-app feedback or on our <a href="https://discord.com/channels/1172568727942860810/1374062797519847505">Discord channel</a>.</p> </article><article> <h2 id="jules-gains-memory"> Jules gains memory! </h2> <b id="mon-sep-29-2025-190000-gmt-0500-central-daylight-time"> September 30, 2025 </b> <p><img src="https://jules.google/docs/_astro/memory.C9CEa_2-_2m2Klf.webp" alt="Memory" width="1856" height="1738" loading="lazy" decoding="async"></p>
<p><strong>Jules Memory for Repositories:</strong> We’re excited to introduce a new Memory feature! Jules now has the ability to learn from your interactions.</p>
<ul>
<li><strong>How it works:</strong> During a task, Jules will save your preferences, nudges, and corrections.</li>
<li><strong>The benefit:</strong> The next time you run the same or a similar task in that specific repository, Jules will reference its memory to better anticipate your needs and follow your established patterns, leading to more accurate results with less guidance.</li>
<li><strong>Settings:</strong> You can toggle memory on or off for the repo in the repo settings page under “Knowledge”</li>
</ul> </article><article> <h2 id="tell-jules-exactly-what-file-to-work-on-using-file-selector"> Tell Jules exactly what file to work on using file selector </h2> <b id="sun-sep-28-2025-190000-gmt-0500-central-daylight-time"> September 29, 2025 </b> <p><img src="https://jules.google/docs/_astro/fileselector.7WfNoQbn_2m4uUx.webp" alt="File Selector" width="1498" height="754" loading="lazy" decoding="async"></p>
<p>You can now tell Jules exactly which files to work with for any given task. Use the new file selector to easily and precisely reference specific files.</p>
<p>This removes ambiguity and gives you more granular control over Jules’s actions, helping to tighten the context for your task.</p> </article><article> <h2 id="jules-acts-on-pr-feedback"> Jules Acts on PR Feedback </h2> <b id="mon-sep-22-2025-190000-gmt-0500-central-daylight-time"> September 23, 2025 </b> <p><img src="https://jules.google/docs/_astro/changelog-pr-comments.Cfoy5hKj_Z1h2RAm.webp" alt="Jules responding to a PR comment" width="2380" height="1338" loading="lazy" decoding="async"></p>
<p>Jules is now able to read and respond to your comments on pull requests!</p>
<p>When you start a review, Jules will add a 👀 emoji to each comment to let you know it’s been read. Based on your feedback, Jules will then push a commit with the requested changes.</p>
<p>For more control, you can switch to <strong>Reactive Mode</strong> in your <a href="https://jules.google.com/settings">global Jules UI settings</a>. In this mode, Jules will only act on comments where you specifically mention <code dir="auto">@Jules</code>.</p> </article><article> <h2 id="all-hands-on-deck"> All Hands on Deck! </h2> <b id="thu-sep-18-2025-190000-gmt-0500-central-daylight-time"> September 19, 2025 </b> <p><img src="https://jules.google/docs/_astro/changelog-019.C2qW8uJn_Z3euJL.webp" alt="image upload" width="1200" height="675" loading="lazy" decoding="async"></p>
<p>Ahoy, mateys! To celebrate International Talk Like a Pirate Day, we’ve given Jules a temporary map to the treasure.</p>
<ul>
<li>
<p>Jules Speaks Pirate: You’ll find your AI agent’s responses are a bit more… swashbuckling… for today only.</p>
</li>
<li>
<p>Same Great Logic: Fear not! Beneath the eyepatch and Jolly Roger, it’s the same powerful coding engine ready to help you plunder that backlog and send bugs to Davy Jones’ locker.</p>
</li>
</ul> </article><article> <h2 id="image-upload"> Image upload </h2> <b id="mon-sep-08-2025-190000-gmt-0500-central-daylight-time"> September 9, 2025 </b> <p><img src="https://jules.google/docs/_astro/image-upload.CXDvQ88D_Z1J34qe.webp" alt="image upload" width="1200" height="675" loading="lazy" decoding="async"></p>
<p>You can now upload images when creating a task in Jules. Use this to show frontend bugs, design inspiration, UI mocks, or any visual context you want Jules to consider while generating code.</p>
<p>For now:</p>
<ul>
<li>Only JPEG and PNG formats are supported.</li>
<li>You can uplaod as many images as you want, as long as the total size is under 5MB.</li>
<li>Image upload is only supported at task creation (we’re working on enabling it for follow-up prompts soon).</li>
</ul>
<p>Note: If your task involves using assets (e.g. logos) directly in code, those must still be committed to your GitHub repo.</p>
<p><a href="https://jules.google/docs/running-tasks/">Read more</a> about Jules image support.</p> </article><article> <h2 id="stacked-diff"> Stacked Diff </h2> <b id="wed-sep-03-2025-190000-gmt-0500-central-daylight-time"> September 4, 2025 </b> <p><img src="https://jules.google/docs/_astro/changelog-017-fade.BsxUDclU_Z1eGKg5.webp" alt="Stacked Diff" width="1200" height="675" loading="lazy" decoding="async"></p>
<p>To improve the code review experience, we’ve introduced a new stacked layout for the diff viewer. This change displays diffs for multiple files vertically on a single screen. The stacked view makes it easier to see related changes across your codebase at a glance, providing better context and speeding up your review process.</p>
<p>Changes:</p>
<ul>
<li>The diff viewer now stacks file changes vertically by default</li>
<li>You can also toggle back to the previous tabbed diff viewer</li>
</ul> </article><article> <h2 id="improved-jules-critic"> Improved Jules Critic </h2> <b id="tue-sep-02-2025-190000-gmt-0500-central-daylight-time"> September 3, 2025 </b> <p><img src="https://jules.google/docs/_astro/changelog-016.DcExNlSl_ZDHzjl.webp" alt="Improved Critic" width="1200" height="675" loading="lazy" decoding="async"></p>
<p>We’ve shipped significant improvements to the Jules critic agent, making its feedback more insightful and reliable. To increase transparency and give you more insight into its evaluation process, you can now see the critic’s real-time analysis as it works.</p>
<p>Changes:</p>
<ul>
<li>The critic’s thought process is now visible in the UI, showing its step-by-step evaluation of the code in real-time.</li>
<li>The critic’s now incorporates more contextual information when making decisions, leading to more accurate and relevant feedback on potential bugs and logic flaws.</li>
</ul> </article><article> <h2 id="jules-sample-prompts"> Jules Sample Prompts </h2> <b id="mon-sep-01-2025-190000-gmt-0500-central-daylight-time"> September 2, 2025 </b> <p><img src="https://jules.google/docs/_astro/changelog-015.CXL47v6I_ZPLQu4.webp" alt="Sample Prompts" width="1200" height="675" loading="lazy" decoding="async"></p>
<p>To help new users get started with Jules, we’ve added sample prompts to the home page. These static prompts provide examples of how to use Jules and can be added to the text box with a single click.</p>
<p>Changes:</p>
<ul>
<li>Sample prompts are now displayed on the home page for all users.</li>
<li>Clicking on a sample prompt will add the text of the prompt to the input box.</li>
</ul> </article><article> <h2 id="render-images-in-the-diff-viewer"> Render images in the diff viewer </h2> <b id="thu-aug-21-2025-190000-gmt-0500-central-daylight-time"> August 22, 2025 </b> <p><img src="https://jules.google/docs/_astro/imagesdiffviewer.CfwMUfrJ_ZOic9c.webp" alt="Images in diff viewer" width="1200" height="675" loading="lazy" decoding="async"></p>
<p>Jules now intelligently renders images within the diff viewer, providing an immediate visual context for your modifications.</p>
<p>This means:</p>
<ul>
<li>Instant Visual Feedback: When Jules generates images (like charts, diagrams, or web UI screenshots), you’ll see the actual image in the diff, not just its code representation.</li>
<li>Streamlined Workflow: No need to switch between tools or download files to see the results. Jules keeps everything in one place.</li>
</ul>
<p>Try it out! Ask Jules to render an output, like a graph based on data, and commit it to your repository. You’ll be able to see the generated image seamlessly within your diff viewer.</p> </article><article> <h2 id="export-at-any-time"> Export at any time </h2> <b id="thu-aug-14-2025-190000-gmt-0500-central-daylight-time"> August 15, 2025 </b> <p><img src="https://jules.google/docs/_astro/exportatanytime.CJ8dxdjQ_ZPOQjW.webp" alt="Export" width="1504" height="364" loading="lazy" decoding="async"></p>
<p>You’re now in full control of when your code gets to GitHub. No need to wait for a task to finish or ask Jules to do it for you. At any point during a task, just click the GitHub icon in the top right to publish the current work-in-progress as a new branch or open a pull request. This gives you more flexibility and control to review, test, or take over whenever you’re ready.</p> </article><article> <h2 id="increasing-the-vm-size-to-20gb"> Increasing the VM Size to 20GB </h2> <b id="thu-aug-14-2025-190000-gmt-0500-central-daylight-time"> August 15, 2025 </b> <p>We heard your feedback about running into disk space limits on larger projects. To address this, we’ve significantly increased the available disk space in the Jules VM to 20GB. This provides more room for large dependencies, build artifacts, and complex repositories, reducing disk-related failures so Jules can tackle bigger tasks. Happy Julesing!</p> </article><article> <h2 id="interactive-plan"> Interactive Plan </h2> <b id="thu-aug-07-2025-190000-gmt-0500-central-daylight-time"> August 8, 2025 </b> <p><img src="https://jules.google/docs/_astro/interactiveplan.BQJ_-8Fr_2dCy3W.webp" alt="Post Beta" width="1200" height="675" loading="lazy" decoding="async"></p>
<p>Meet Interactive Plan. Instead of jumping straight to the solution, Jules will now read your codebase, ask clarifying questions, and work with you to refine the plan. This collaborative approach gives you more control and ensures you’re on the same page, leading to higher-quality code and a more reliable solution.</p>
<p>In summary:</p>
<ul>
<li>Trigger the interactive plan from the dropdown when you start a task</li>
<li>Jules will start a brainstorm with you and ask clarifying questions</li>
</ul> </article><article> <h2 id="jules-can-surf-the-web"> Jules can surf the web </h2> <b id="thu-aug-07-2025-190000-gmt-0500-central-daylight-time"> August 8, 2025 </b> <p><img src="https://jules.google/docs/_astro/websearch.DYAsPzEM_ZwyNyo.webp" alt="Post Beta" width="1200" height="675" loading="lazy" decoding="async"></p>
<p>Jules can now proactively search the web for relevant content, documentation, or code snippets to help complete your tasks. This means Jules can get the information it needs, resulting in more accurate and successful task completion.</p>
<p>In Summary:</p>
<ul>
<li>Jules can find the latest documentation for dependencies/libraries you’re using</li>
<li>Jules can proactively find examples or code snippets that can help inform its implementation</li>
</ul>
<p><strong>Note</strong>: web search works best when working on technical documentation. Queries like: “What is the latest news today?” are not supported.</p> </article><article> <h2 id="critic-agent"> Critic Agent </h2> <b id="thu-aug-07-2025-190000-gmt-0500-central-daylight-time"> August 8, 2025 </b> <p><img src="https://jules.google/docs/_astro/critic.M-N27lZA_ZXQtTP.webp" alt="Critic Agent" width="1200" height="675" loading="lazy" decoding="async"></p>
<p>Great developers don’t just write code, they question it. And now, so does Jules. We’ve built the Jules critic agent to ensure that every line of code isn’t just functional, but robust, secure, and efficient. It acts as an internal peer reviewer, challenging every proposed change to elevate the quality of the final output.</p>
<p>Some high level notes:</p>
<ul>
<li>
<p><strong>Critic-augmented generation:</strong> The Jules critic is integrated directly into the generation process. Every proposed change undergoes adversarial review before completion.</p>
</li>
<li>
<p><strong>Improved code quality:</strong> The critic flags subtle bugs, missed edge cases, and inefficient code. Jules then uses this feedback to improve the patch in real-time.</p>
</li>
<li>
<p><strong>A new kind of review:</strong> The critic is not just another linter or test. It understands the intent and context behind code, similar to a human peer reviewer.</p>
</li>
<li>
<p><strong>Built on research:</strong> This feature draws on research into multi-step, tool interactive critiquing and actor-critic reinforcement learning, where an “actor” generates and a “critic” evaluates.</p>
</li>
</ul> </article><article> <h2 id="jules-can-test-web-apps-and-show-you-the-results"> Jules can test web-apps and show you the results </h2> <b id="wed-aug-06-2025-190000-gmt-0500-central-daylight-time"> August 7, 2025 </b> <p><img src="https://jules.google/docs/_astro/computeruse.DsMoFBka_Z1cKxdK.webp" alt="Post Beta" width="1200" height="675" loading="lazy" decoding="async"></p>
<p>Next time you are working on a front end project with Jules, ask it to verify its work and it’ll render the website and send you back a screenshot!</p>
<ul>
<li>Ask Jules to complete a web development task and to verify the front end</li>
<li>Jules will send you a screenshot of the front end along with any code changes</li>
<li>The default Jules base image now includes Playwright for front end testing</li>
<li>Users can also add images in the form of public URLs for Jules to use as input</li>
</ul> </article><article> <h2 id="jules-is-out-of-beta"> Jules is out of beta! </h2> <b id="tue-aug-05-2025-190000-gmt-0500-central-daylight-time"> August 6, 2025 </b> <p><img src="https://jules.google/docs/_astro/post-beta.CHZ_x9Mj_Wiixs.webp" alt="Post Beta" width="2008" height="1102" loading="lazy" decoding="async"></p>
<p>Today we are thrilled to announce that Jules is no longer in beta! Since launch just two months ago, Jules has passed over 140k public commits. Thank you to our amazing beta users for all your support and feedback.</p>
<p>In addition, we’re launching our pricing plans to unlock higher task limits, along with a bunch of quality improvements in the Jules app and agent. Here are the details:</p>
<ul>
<li>Get higher task limits through the Google AI Pro and Ultra plans. More details at <a href="https://jules.google/docs/usage-limits">Limits and Plans</a>.</li>
<li>Jules now uses the power of Gemini 2.5 thinking when creating its plan, resulting in higher quality plans and more complete tasks</li>
<li>Numerous bug fixes so Jules gets stuck less, and is better at following your instructions in agents.md</li>
</ul> </article><article> <h2 id="environment-snapshots-for-faster-tasks"> Environment snapshots for faster tasks </h2> <b id="mon-aug-04-2025-190000-gmt-0500-central-daylight-time"> August 5, 2025 </b> <p><img src="https://jules.google/docs/_astro/envsnapshot.Da7EUEdt_e5gX8.webp" alt="Env Snapshot" width="1200" height="675" loading="lazy" decoding="async"></p>
<p>Jules now creates a snapshot of your environment when you add environment setup scripts. For complicated environment, users should see faster and more consistent task execution.</p>
<p>In summary:</p>
<ul>
<li>Jules will now snapshot your environment when you provide an environment setup script</li>
<li>Snapshots are loaded automatically next time you run a task</li>
<li>This provides for faster task startups, especially for complex environments</li>
<li>You can find environment configuration by clicking the “codebase” in the left hand panel, or by clicking the “configure environment” button in the task pane.</li>
</ul> </article><article> <h2 id="open-a-pr-directly-from-jules"> Open A PR directly from Jules </h2> <b id="sun-aug-03-2025-190000-gmt-0500-central-daylight-time"> August 4, 2025 </b> <p><img src="https://jules.google/docs/_astro/openapr.B4vE52yX_1KUfVl.webp" alt="Open a PR" width="1201" height="675" loading="lazy" decoding="async"></p>
<p>Closing the loop from task to merge 🤝</p>
<p>Jules can now open a pull request directly from the UI.
After a task completes, just use the new dropdown next to the ‘Publish Branch’ button to open a PR. Jules will request to merge the newly published branch into main, streamlining your entire workflow. Less context switching, faster merging.</p> </article><article> <h2 id="added-bun-runtime-support"> Added Bun runtime support </h2> <b id="thu-jul-17-2025-190000-gmt-0500-central-daylight-time"> July 18, 2025 </b> <p><img src="https://jules.google/docs/_astro/jules_3bun.CJ2DViw__ZNLTEL.webp" alt="Bun" width="1200" height="675" loading="lazy" decoding="async"></p>
<p>Jules now supports <a href="https://bun.sh/">Bun</a>. You can run tasks using Bun out of the box, no extra setup required. This expands compatibility for projects that use Bun instead of Node.</p>
<p><a href="https://jules.google.com/docs/environment/">Read more</a> about the jules base image and what tooling works with Jules.</p> </article><article> <h2 id="improved-task-controls-and-other-ui-delight"> Improved task controls and other 💅 UI delight </h2> <b id="wed-jul-02-2025-190000-gmt-0500-central-daylight-time"> July 3, 2025 </b> <p><img src="https://jules.google/docs/_astro/polish-tasks-changelog.Bzf_pBWz_Z1mwEs.webp" alt="Task controls" width="1200" height="675" loading="lazy" decoding="async"></p>
<ul>
<li>Pause, resume, and delete tasks—without losing your sense of place. Available from sidebar and repo view. You can even quickly copy task urls!</li>
<li>Non-urgent task icons are now more recessive</li>
<li>Certain hover states—which did not look good—have been toned back.</li>
<li>System messages have more consistent padding and borders</li>
</ul>
<p><a href="https://jules.google.com/docs/running-tasks/">Learn more about running a task.</a></p> </article><article> <h2 id="jules-now-listens-to-github-issues"> Jules now listens to GitHub issues </h2> <b id="wed-jun-25-2025-190000-gmt-0500-central-daylight-time"> June 26, 2025 </b> <p><img src="https://jules.google/docs/_astro/assign-to-jules.DFMcEzUY_lXKPx.webp" alt="Assign to Jules" width="1200" height="675" loading="lazy" decoding="async"></p>
<p>Add the label ‘jules’ to any GitHub issue to start a task in Jules. That’s it—label on, task live.</p>
<p>How to summon Jules:</p>
<ul>
<li>Open a GitHub issue.</li>
<li>Click the gear next to “Labels”.</li>
<li>Add the label ‘jules.’</li>
</ul>
<p>Make sure the Jules GitHub App has access to your repo. After that, Jules takes it from there. <a href="https://jules.google/docs/running-tasks/">Read more about running tasks in Jules</a>!</p> </article><article> <h2 id="jules-agent-update-faster-smarter-more-reliable"> Jules Agent Update: Faster, Smarter, More Reliable </h2> <b id="thu-jun-19-2025-190000-gmt-0500-central-daylight-time"> June 20, 2025 </b> <p><img src="https://jules.google/docs/_astro/agents-md-support.COimRein_13z6cT.webp" alt="Jules environment updates" width="1200" height="675" loading="lazy" decoding="async"></p>
<p>We’ve shipped a big upgrade to the Jules agent under the hood.</p>
<p>What’s new:</p>
<ul>
<li><strong>Smarter context.</strong> Jules reads from AGENTS.md if it’s in your repo.</li>
<li><strong>Improved performance.</strong> Tasks now complete faster—no numbers to share just yet, but you’ll feel it.</li>
<li><strong>Significantly reduced punting.</strong> We tightened the loop to keep Jules moving forward.</li>
<li><strong>More reliable setup.</strong> If you’ve added an environment setup script, Jules now runs it consistently.</li>
<li><strong>Better test habits.</strong> Jules is more likely to write and run tests on its own.</li>
</ul>
<p>Check out the <a href="https://jules.google/docs/">Getting Started</a> guide to learn more about AGENTS.md support.</p> </article><article>  <b id="tue-jun-17-2025-190000-gmt-0500-central-daylight-time"> June 18, 2025 </b> <p><img src="https://jules.google/docs/_astro/changelog-env-update.C-4Kcp7e_Z1a9xXQ.webp" alt="Jules environment updates" width="1200" height="675" loading="lazy" decoding="async"></p>
<p>We’ve overhauled the Jules development environment to move beyond the default Ubuntu 24.04 LTS packages. This includes:</p>
<ul>
<li>Explicitly installing newer versions of key toolchains like Rust, Node, and Python, addressing long-standing version issues.</li>
<li>Adding finer-grained control over installation steps via custom scripts instead of relying solely on apt.</li>
<li>Introducing support for multiple runtimes, improved isolation, and version pinning to reduce drift and better match developer expectations.</li>
</ul>
<p>These changes unblock several issues developers encountered with outdated dependencies and improve alignment with modern project requirements.</p>
<p><a href="https://jules.google/docs/environment/">Read about the Jules environment setup to learn more about what’s pre-installed.</a></p> </article><article> <h2 id="customization-and-efficiency-enhancements"> Customization and Efficiency Enhancements </h2> <b id="thu-jun-05-2025-190000-gmt-0500-central-daylight-time"> June 6, 2025 </b> <p><img src="https://jules.google/docs/_astro/jules-copy-paste-download.Bh0k6Pa9_ZL8pAP.webp" alt="Jules code view" width="1200" height="675" loading="lazy" decoding="async"></p>
<p><strong>Performance upgrades:</strong> Enjoy a smoother, faster Jules experience with recent under-the-hood improvements.</p>
<p><strong>Quickly copy and download code:</strong> New copy and download buttons are now available in the code view pane, making it easier to grab your code directly from Jules.</p>
<p><strong>Stay focused with task modals:</strong> Initiate multiple tasks seamlessly through a new modal option, allowing you to keep your context and workflow intact. <a href="https://jules.google/docs/tasks-repos/">Learn more</a> about kicking off tasks.</p>
<p><strong>Adjustable code panel:</strong> Customize your workspace by adjusting the width of the code panel to your preferred viewing experience.</p>
<p><a href="https://jules.google/docs/code/">Check out the docs</a> to learn more about how to download code that Jules writes.</p> </article><article> <h2 id="a-faster-smoother-and-more-reliable-jules"> A faster, smoother and more reliable Jules </h2> <b id="thu-may-29-2025-190000-gmt-0500-central-daylight-time"> May 30, 2025 </b> <p>This week, our focus has been on improving reliability, fixing our GitHub integration, and scaling capacity.</p>
<p><strong>Here’s what’s we shipped:</strong></p>
<ul>
<li>Updated our limits to 60 tasks per day, 5 concurrent.</li>
<li>We substantially improved the reliability of the GitHub sync. Export to GitHub should also be fixed on previously created tasks.</li>
<li>We’ve decreased the number of failure cases by 2/3</li>
</ul>
<p>Learn more <a href="https://jules.google/docs/usage-limits">about usage limits.</a></p> </article><article> <h2 id="improving-stablity"> Improving Stablity </h2> <b id="wed-may-21-2025-190000-gmt-0500-central-daylight-time"> May 22, 2025 </b> <p>We’ve been heads down improving stability and fixing bugs—big and small—to make Jules faster, smoother, and more reliable for you.</p>
<p><strong>Here’s what’s fixed:</strong></p>
<ul>
<li>Upgraded our queuing system and added more compute to reduce wait times during peak usage</li>
<li>Publish Branch button is now part of the summary UI in the activity feed so it’s easier to find</li>
<li>Bug vixes for task status and mobile</li>
</ul>
<p><a href="https://jules.google.com/docs/code/#pushing-to-github">Learn more</a> about how to publish a branch on GitHub.</p> </article><article> <h2 id="jules-is-here"> Jules is here </h2> <b id="sun-may-18-2025-190000-gmt-0500-central-daylight-time"> May 19, 2025 </b> <p><img src="https://jules.google/docs/_astro/jules-changelog-og-image.CksfgUSk_1wDNHc.webp" alt="Jules dashboard" width="2246" height="1196" loading="lazy" decoding="async"></p>
<p>Today, we’re launching <a href="https://jules.google.com/" target="_blank" rel="noopener"><strong>Jules,</strong></a> a new AI coding agent.</p>
<p>Jules helps you move faster by working asynchronously on tasks in your GitHub repo. It can fix bugs, update dependencies, migrate code, and add new features.</p>
<p>Once you give Jules a task, it spins up a fresh dev environment in a VM, installs dependencies, writes tests, makes the changes, runs the tests, and opens a pull request. Jules shows its work as it makes progress, so you never have to guess what code it’s writing, or what it’s thinking.</p>
<p><strong>What Jules can do today</strong></p>
<ul>
<li>Fix bugs with test verified patches</li>
<li>Handle version bumps and dependency upgrades</li>
<li>Perform scoped code transformations</li>
<li>Migrate code across languages or frameworks</li>
<li>Ship isolated, scoped, features</li>
<li>Open PRs with runnable code and test results</li>
</ul>
<p><a href="https://jules.google/">Get started with the Jules documentation</a>, and visit <a href="https://jules.google.com/" target="_blank" rel="noopener">jules.google.com</a> to run your first Jules task.</p> </article> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PEP 810 – Explicit lazy imports (356 pts)]]></title>
            <link>https://pep-previews--4622.org.readthedocs.build/pep-0810/</link>
            <guid>45466086</guid>
            <pubDate>Fri, 03 Oct 2025 18:24:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pep-previews--4622.org.readthedocs.build/pep-0810/">https://pep-previews--4622.org.readthedocs.build/pep-0810/</a>, See on <a href="https://news.ycombinator.com/item?id=45466086">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="pep-content">

<dl>
<dt>Author<span>:</span></dt>
<dd>Pablo Galindo &lt;pablogsal at python.org&gt;,
Germán Méndez Bravo &lt;german.mb at gmail.com&gt;,
Thomas Wouters &lt;thomas at python.org&gt;,
Dino Viehland &lt;dinoviehland at gmail.com&gt;,
Brittany Reynoso &lt;brittanyrey at gmail.com&gt;,
Noah Kim &lt;noahbkim at gmail.com&gt;,
Tim Stumbaugh &lt;me at tjstum.com&gt;</dd>
<dt>Discussions-To<span>:</span></dt>
<dd><a href="https://discuss.python.org/t/104131">Discourse thread</a></dd>
<dt>Status<span>:</span></dt>
<dd><abbr title="Proposal under active discussion and revision">Draft</abbr></dd>
<dt>Type<span>:</span></dt>
<dd><abbr title="Normative PEP with a new feature for Python, implementation change for CPython or interoperability standard for the ecosystem">Standards Track</abbr></dd>
<dt>Created<span>:</span></dt>
<dd>02-Oct-2025</dd>
<dt>Python-Version<span>:</span></dt>
<dd>3.15</dd>
<dt>Post-History<span>:</span></dt>
<dd><a href="https://discuss.python.org/t/104131" title="Discourse thread">03-Oct-2025</a></dd>
</dl>
<hr>
<section id="contents">
<details><summary>Table of Contents</summary><ul>
<li><a href="#abstract">Abstract</a></li>
<li><a href="#motivation">Motivation</a></li>
<li><a href="#rationale">Rationale</a><ul>
<li><a href="#other-design-decisions">Other design decisions</a></li>
</ul>
</li>
<li><a href="#specification">Specification</a><ul>
<li><a href="#grammar">Grammar</a><ul>
<li><a href="#syntax-restrictions">Syntax restrictions</a></li>
</ul>
</li>
<li><a href="#semantics">Semantics</a></li>
<li><a href="#lazy-import-mechanism">Lazy import mechanism</a></li>
<li><a href="#reification">Reification</a></li>
</ul>
</li>
<li><a href="#reference-implementation">Reference Implementation</a><ul>
<li><a href="#bytecode-and-adaptive-specialization">Bytecode and adaptive specialization</a></li>
<li><a href="#lazy-imports-filter">Lazy imports filter</a></li>
<li><a href="#global-lazy-imports-control">Global lazy imports control</a></li>
</ul>
</li>
<li><a href="#backwards-compatibility">Backwards Compatibility</a><ul>
<li><a href="#unchanged-semantics">Unchanged semantics</a></li>
<li><a href="#observable-behavioral-shifts-opt-in-only">Observable behavioral shifts (opt-in only)</a></li>
<li><a href="#thread-safety-and-reification">Thread-safety and reification</a></li>
<li><a href="#typing-and-tools">Typing and tools</a></li>
</ul>
</li>
<li><a href="#security-implications">Security Implications</a></li>
<li><a href="#how-to-teach-this">How to Teach This</a></li>
<li><a href="#faq">FAQ</a></li>
<li><a href="#alternate-implementation-ideas">Alternate Implementation Ideas</a><ul>
<li><a href="#leveraging-a-subclass-of-dict">Leveraging a subclass of dict</a></li>
<li><a href="#alternate-keyword-names">Alternate keyword names</a></li>
</ul>
</li>
<li><a href="#rejected-ideas">Rejected Ideas</a><ul>
<li><a href="#modification-of-the-dict-object">Modification of the dict object</a></li>
<li><a href="#placing-the-lazy-keyword-in-the-middle-of-from-imports">Placing the <code><span>lazy</span></code> keyword in the middle of from imports</a></li>
<li><a href="#placing-the-lazy-keyword-at-the-end-of-import-statements">Placing the <code><span>lazy</span></code> keyword at the end of import statements</a></li>
<li><a href="#returning-a-proxy-dict-from-globals">Returning a proxy dict from <code><span>globals()</span></code></a></li>
<li><a href="#reifying-lazy-imports-when-globals-is-called">Reifying lazy imports when <code><span>globals()</span></code> is called</a></li>
</ul>
</li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#footnotes">Footnotes</a></li>
<li><a href="#copyright">Copyright</a></li>
</ul>
</details></section>
<section id="abstract">
<h2><a href="#abstract" role="doc-backlink">Abstract</a></h2>
<p>This PEP introduces syntax for lazy imports as an explicit language feature:</p>
<div><pre><span></span><span>lazy</span> <span>import</span><span> </span><span>json</span>
<span>lazy</span> <span>from</span><span> </span><span>json</span><span> </span><span>import</span> <span>dumps</span>
</pre></div>
<p>Lazy imports defer the loading and execution of a module until the first time
the imported name is used, in contrast to ‘normal’ imports, which eagerly load
and execute a module at the point of the import statement.</p>
<p>By allowing developers to mark individual imports as lazy with explicit
syntax, Python programs can reduce startup time, memory usage, and unnecessary
work. This is particularly beneficial for command-line tools, test suites, and
applications with large dependency graphs.</p>
<p>This proposal preserves full backwards compatibility: normal import statements
remain unchanged, and lazy imports are enabled only where explicitly
requested.</p>
</section>
<section id="motivation">
<h2><a href="#motivation" role="doc-backlink">Motivation</a></h2>
<p>The dominant convention in Python code is to place all imports at the module
level, typically at the beginning of the file. This avoids repetition, makes
import dependencies clear and minimizes runtime overhead by only evaluating an
import statement once per module.</p>
<p>A major drawback with this approach is that importing the first module for an
execution of Python (the “main” module) often triggers an immediate cascade of
imports, and optimistically loads many dependencies that may never be used.
The effect is especially costly for command-line tools with multiple
subcommands, where even running the command with <code><span>--help</span></code> can load dozens of
unnecessary modules and take several seconds. This basic example demonstrates
what must be loaded just to get helpful feedback to the user on how to run the
program at all. Inefficiently, the user incurs this overhead again when they
figure out the command they want and invoke the program “for real.”</p>
<p>A somewhat common way to delay imports is to move the imports into functions
(inline imports), but this practice requires more work to implement and
maintain, and can be subverted by a single inadvertent top-level import.
Additionally, it obfuscates the full set of dependencies for a module.
Analysis of the Python standard library shows that approximately 17% of all
imports outside tests (nearly 3500 total imports across 730 files) are already
placed inside functions or methods specifically to defer their execution. This
demonstrates that developers are already manually implementing lazy imports in
performance-sensitive code, but doing so requires scattering imports
throughout the codebase and makes the full dependency graph harder to
understand at a glance.</p>
<p>The standard library provides the <a href="https://docs.python.org/3/library/importlib.html#importlib.util.LazyLoader" title="(in Python v3.13)"><code><span>LazyLoader</span></code></a> class to
solve some of these inefficiency problems. It permits imports at the module
level to work <em>mostly</em> like inline imports do. Many scientific Python
libraries have adopted a similar pattern, formalized in
<a href="https://scientific-python.org/specs/spec-0001/">SPEC 1</a>.
There’s also the third-party <a href="https://pypi.org/project/lazy_loader/">lazy_loader</a> package, yet another
implementation of lazy imports. Imports used solely for static type checking
are another source of potentially unneeded imports, and there are similarly
disparate approaches to minimizing the overhead. The various approaches used
here to defer or remove eager imports do not cover all potential use-cases for
a general lazy import mechanism. There is no clear standard, and there are
several drawbacks including runtime overhead in unexpected places, or worse
runtime introspection.</p>
<p>This proposal introduces syntax for lazy imports with a design that is local,
explicit, controlled, and granular. Each of these qualities is essential to
making the feature predictable and safe to use in practice.</p>
<p>The behavior is <strong>local</strong>: laziness applies only to the specific import marked
with the <code><span>lazy</span></code> keyword, and it does not cascade recursively into other
imports. This ensures that developers can reason about the effect of laziness
by looking only at the line of code in front of them, without worrying about
whether imported modules will themselves behave differently. A <code><span>lazy</span> <span>import</span></code>
is an isolated decision each time it is used, not a global shift in semantics.</p>
<p>The semantics are <strong>explicit</strong>. When a name is imported lazily, the binding is
created in the importing module immediately, but the target module is not
loaded until the first time the name is accessed. After this point, the
binding is indistinguishable from one created by a normal import. This clarity
reduces surprises and makes the feature accessible to developers who may not
be deeply familiar with Python’s import machinery.</p>
<p>Lazy imports are <strong>controlled</strong>, in the sense that deferred loading is only
triggered by the importing code itself. In the general case, a library will
only experience lazy imports if its own authors choose to mark them as such.
This avoids shifting responsibility onto downstream users and prevents
accidental surprises in library behavior. Since library authors typically
manage their own import subgraphs, they retain predictable control over when
and how laziness is applied.</p>
<p>The mechanism is also <strong>granular</strong>. It is introduced through explicit syntax
on individual imports, rather than a global flag or implicit setting. This
allows developers to adopt it incrementally, starting with the most
performance-sensitive areas of a codebase. As this feature is introduced to
the community, we want to make the experience of onboarding optional,
progressive, and adaptable to the needs of each project.</p>
<p>Lazy imports provide several concrete advantages:</p>
<ul>
<li>Command-line tools are often invoked directly by a user, so latency – in
particular startup latency – is quite noticeable. These programs are also
typically short-lived processes (contrasted with, e.g., a web server). With
lazy imports, only the code paths actually reached will import a module.
This can reduce startup time by 50-70% in practice, providing a significant
improvement to a common user experience and improving Python’s
competitiveness in domains where fast startup matters most.</li>
<li>Type annotations frequently require imports that are never used at runtime.
The common workaround is to wrap them in <code><span>if</span> <span>TYPE_CHECKING:</span></code> blocks
<a href="#f1" id="id1">[1]</a>. With lazy imports, annotation-only imports impose no runtime
penalty, eliminating the need for such guards and making annotated codebases
cleaner.</li>
<li>Large applications often import thousands of modules, and each module
creates function and type objects, incurring memory costs. In long-lived
processes, this noticeably raises baseline memory usage. Lazy imports defer
these costs until a module is needed, keeping unused subsystems unloaded.
Memory savings of 30-40% have been observed in real workloads.</li>
</ul>
</section>
<section id="rationale">
<h2><a href="#rationale" role="doc-backlink">Rationale</a></h2>
<p>The design of this proposal is centered on clarity, predictability, and ease
of adoption. Each decision was made to ensure that lazy imports provide
tangible benefits without introducing unnecessary complexity into the language
or its runtime.</p>
<p>It is also worth noting that while this PEP outlines one specific approach, we
list alternate implementation strategies for some of the core aspects and
semantics of the proposal. If the community expresses a strong preference for
a different technical path that still preserves the same core semantics or
there is fundamental disagreement over the specific option, we have included
the brainstorming we have already completed in preparation for this proposal
as reference.</p>
<p>The choice to introduce a new <code><span>lazy</span></code> keyword reflects the need for explicit
syntax. Import behavior is too fundamental to be left implicit or hidden
behind global flags or environment variables. By marking laziness directly at
the import site, the intent is immediately visible to both readers and tools.
This avoids surprises, reduces the cognitive burden of reasoning about
imports, and keeps lazy import semantics in line with Python’s tradition of
explicitness.</p>
<p>Another important decision is to represent lazy imports with proxy objects in
the module’s namespace, rather than by modifying dictionary lookup. Earlier
approaches experimented with embedding laziness into dictionaries, but this
blurred abstractions and risked affecting unrelated parts of the runtime. The
dictionary is a fundamental data structure in Python – literally every object
is built on top of dicts – and adding hooks to dictionaries would prevent
critical optimizations and complicate the entire runtime. The proxy approach
is simpler: it behaves like a placeholder until first use, at which point it
resolves the import and rebinds the name. From then on, the binding is
indistinguishable from a normal import. This makes the mechanism easy to
explain and keeps the rest of the interpreter unchanged.</p>
<p>Compatibility for library authors was also a key concern. Many maintainers
need a migration path that allows them to support both new and old versions of
Python at once. For this reason, the proposal includes the
<code><span>__lazy_modules__</span></code> global as a transitional mechanism. A module can
declare which imports should be treated as lazy (by listing the module names
as strings), and on Python 3.15 or later those imports will become lazy
automatically, as if they were imported with the <code><span>lazy</span></code> keyword. On earlier
versions the declaration is ignored, leaving imports eager. This gives authors
a practical bridge until they can rely on the keyword as the canonical syntax.</p>
<p>Finally, the feature is designed to be adopted incrementally. Nothing changes
unless a developer explicitly opts in, and adoption can begin with just a few
imports in performance-sensitive areas. This mirrors the experience of gradual
typing in Python: a mechanism that can be introduced progressively, without
forcing projects to commit globally from day one. Notably, the adoption can
also be done from the “outside in”, permitting CLI authors to introduce lazy
imports and speed up user-facing tools, without requiring changes to every
library the tool might use.</p>
<section id="other-design-decisions">
<h3><a href="#other-design-decisions" role="doc-backlink">Other design decisions</a></h3>
<ul>
<li>The scope of laziness is deliberately local and non-recursive. A lazy import
only affects the specific statement where it appears; it does not cascade
into other modules or submodules. This choice is crucial for predictability.
When developers read code, they can reason about import behavior line by
line, without worrying about hidden laziness deeper in the dependency graph.
The result is a feature that is powerful but still easy to understand in
context.</li>
<li>In addition, it is useful to provide a mechanism to activate or deactivate
lazy imports at a global level. While the primary design centers on explicit
syntax, there are scenarios – such as large applications, testing
environments, or frameworks – where enabling laziness consistently across
many modules provides the most benefit. A global switch makes it easy to
experiment with or enforce consistent behavior, while still working in
combination with the filtering API to respect exclusions or tool-specific
configuration. This ensures that global adoption can be practical without
reducing flexibility or control.</li>
</ul>
</section>
</section>
<section id="specification">
<h2><a href="#specification" role="doc-backlink">Specification</a></h2>
<section id="grammar">
<h3><a href="#grammar" role="doc-backlink">Grammar</a></h3>
<p>A new soft keyword <code><span>lazy</span></code> is added. A soft keyword is a context-sensitive
keyword that only has special meaning in specific grammatical contexts;
elsewhere it can be used as a regular identifier (e.g., as a variable name).
The <code><span>lazy</span></code> keyword only has special meaning when it appears before import
statements:</p>
<div><pre><span></span>import_name:
    | 'lazy'? 'import' dotted_as_names

import_from:
    | 'lazy'? 'from' ('.' | '...')* dotted_name 'import' import_from_targets
    | 'lazy'? 'from' ('.' | '...')+ 'import' import_from_targets
</pre></div>
<section id="syntax-restrictions">
<h4><a href="#syntax-restrictions" role="doc-backlink">Syntax restrictions</a></h4>
<p>The soft keyword is only allowed at the global (module) level, <strong>not</strong> inside
functions, class bodies, with <code><span>try</span></code>/<code><span>with</span></code> blocks, or <code><span>import</span> <span>*</span></code>. Import
statements that use the soft keyword are <em>potentially lazy</em>. Imports that
can’t be lazy are unaffected by the global lazy imports flag, and instead are
always eager.</p>
<p>Examples of syntax errors:</p>
<div><pre><span></span><span># SyntaxError: lazy import not allowed inside functions</span>
<span>def</span><span> </span><span>foo</span><span>():</span>
    <span>lazy</span> <span>import</span><span> </span><span>json</span>

<span># SyntaxError: lazy import not allowed inside classes</span>
<span>class</span><span> </span><span>Bar</span><span>:</span>
    <span>lazy</span> <span>import</span><span> </span><span>json</span>

<span># SyntaxError: lazy import not allowed inside try/except blocks</span>
<span>try</span><span>:</span>
    <span>lazy</span> <span>import</span><span> </span><span>json</span>
<span>except</span> <span>ImportError</span><span>:</span>
    <span>pass</span>

<span># SyntaxError: lazy import not allowed inside with blocks</span>
<span>with</span> <span>suppress</span><span>(</span><span>ImportError</span><span>):</span>
    <span>lazy</span> <span>import</span><span> </span><span>json</span>

<span># SyntaxError: lazy from ... import * is not allowed</span>
<span>lazy</span> <span>from</span><span> </span><span>json</span><span> </span><span>import</span> <span>*</span>
</pre></div>
</section>
</section>
<section id="semantics">
<h3><a href="#semantics" role="doc-backlink">Semantics</a></h3>
<p>When the <code><span>lazy</span></code> keyword is used, the import becomes <em>potentially lazy</em>.
Unless lazy imports are disabled or suppressed (see below), the module is not
loaded immediately at the import statement; instead, a lazy proxy object is
created and bound to the name. The actual module is loaded on first use of
that name.</p>
<p>Example:</p>
<div><pre><span></span><span>import</span><span> </span><span>sys</span>

<span>lazy</span> <span>import</span><span> </span><span>json</span>

<span>print</span><span>(</span><span>'json'</span> <span>in</span> <span>sys</span><span>.</span><span>modules</span><span>)</span>  <span># False - module not loaded yet</span>

<span># First use triggers loading</span>
<span>result</span> <span>=</span> <span>json</span><span>.</span><span>dumps</span><span>({</span><span>"hello"</span><span>:</span> <span>"world"</span><span>})</span>

<span>print</span><span>(</span><span>'json'</span> <span>in</span> <span>sys</span><span>.</span><span>modules</span><span>)</span>  <span># True - now loaded</span>
</pre></div>
<p>A module may contain a <code><span>__lazy_modules__</span></code> attribute, which is a
sequence of fully qualified module names (strings) to make <em>potentially lazy</em>
(as if the <code><span>lazy</span></code> keyword was used). This attribute is checked on each
<code><span>import</span></code> statement to determine whether the import should be made
<em>potentially lazy</em>. When a module is made lazy this way, from-imports using
that module are also lazy, but not necessarily imports of sub-modules.</p>
<p>The normal (non-lazy) import statement will check the global lazy imports
flag. If it is “enabled”, all imports are <em>potentially lazy</em> (except for
imports that can’t be lazy, as mentioned above.)</p>
<p>Example:</p>
<div><pre><span></span><span>__lazy_modules__</span> <span>=</span> <span>[</span><span>"json"</span><span>]</span>
<span>import</span><span> </span><span>json</span>
<span>print</span><span>(</span><span>'json'</span> <span>in</span> <span>sys</span><span>.</span><span>modules</span><span>)</span>  <span># False</span>
<span>result</span> <span>=</span> <span>json</span><span>.</span><span>dumps</span><span>({</span><span>"hello"</span><span>:</span> <span>"world"</span><span>})</span>
<span>print</span><span>(</span><span>'json'</span> <span>in</span> <span>sys</span><span>.</span><span>modules</span><span>)</span>  <span># True</span>
</pre></div>
<p>If the global lazy imports flag is set to “disabled”, no <em>potentially lazy</em>
import is ever imported lazily, and the behavior is equivalent to a regular
import statement: the import is <em>eager</em> (as if the lazy keyword was not used).</p>
<p>For a <em>potentially lazy</em> import, the lazy imports filter (if set) is called
with the name of the module doing the import, the name of the module being
imported, and (if applicable) the fromlist. If the lazy import filter returns
<code><span>True</span></code>, the <em>potentially lazy</em> import becomes a lazy import. Otherwise, the
import is <em>not</em> lazy, and the normal (eager) import continues.</p>
</section>
<section id="lazy-import-mechanism">
<h3><a href="#lazy-import-mechanism" role="doc-backlink">Lazy import mechanism</a></h3>
<p>When an import is lazy, <code><span>__lazy_import__</span></code> is called instead of
<code><span>__import__</span></code>. <code><span>__lazy_import__</span></code> has the same function signature as
<code><span>__import__</span></code>. It adds the module name to <code><span>sys.lazy_modules</span></code>, a set of
fully-qualified module names which have been lazily imported at some point
(primarily for diagnostics and introspection), and returns a “lazy module
object.”</p>
<p>The implementation of <code><span>from</span> <span>...</span> <span>import</span></code> (the <code><span>IMPORT_FROM</span></code> bytecode
implementation) checks if the module it’s fetching from is a lazy module
object, and if so, returns a lazy object for each name instead.</p>
<p>The end result of this process is that lazy imports (regardless of how they
are enabled) result in lazy objects being assigned to global variables.</p>
<p>Lazy module objects do not appear in <code><span>sys.modules</span></code>, they’re just listed in
the <code><span>sys.lazy_modules</span></code> set. Under normal operation lazy objects should only
end up stored in global variables, and the common ways to access those
variables (regular variable access, module attributes) will resolve lazy
imports (“reify”) and replace them when they’re accessed.</p>
<p>It is still possible to expose lazy objects through other means, like
debuggers. This is not considered a problem.</p>
</section>
<section id="reification">
<h3><a href="#reification" role="doc-backlink">Reification</a></h3>
<p>When a lazy object is first used, it needs to be reified. This means resolving
the import at that point in the program and replacing the lazy object with the
concrete one. Reification imports the module in the same way as it would have
been if it had been imported eagerly, barring intervening changes to the
import system (e.g. to <code><span>sys.path</span></code>, <code><span>sys.meta_path</span></code>, <code><span>sys.path_hooks</span></code> or
<code><span>__import__</span></code>).</p>
<p>Reification still calls <code><span>__import__</span></code> to resolve the import. When the module
is first reified, it’s removed from <code><span>sys.lazy_modules</span></code> (even if there are
still other unreified lazy references to it). When a package is reified and
submodules in the package were also previously lazily imported, those
submodules are <em>not</em> automatically reified but they <em>are</em> added to the reified
package’s globals (unless the package already assigned something else to the
name of the submodule).</p>
<p>If reification fails (e.g., due to an <code><span>ImportError</span></code>), the exception is
enhanced with chaining to show both where the lazy import was defined and
where it was first accessed (even though it propagates from the code that
triggered reification). This provides clear debugging information:</p>
<div><pre><span></span><span># app.py - has a typo in the import</span>
<span>lazy</span> <span>from</span><span> </span><span>json</span><span> </span><span>import</span> <span>dumsp</span>  <span># Typo: should be 'dumps'</span>

<span>print</span><span>(</span><span>"App started successfully"</span><span>)</span>
<span>print</span><span>(</span><span>"Processing data..."</span><span>)</span>

<span># Error occurs here on first use</span>
<span>result</span> <span>=</span> <span>dumsp</span><span>({</span><span>"key"</span><span>:</span> <span>"value"</span><span>})</span>
</pre></div>
<p>The traceback shows both locations:</p>
<div><pre><span></span><span>App started successfully</span>
<span>Processing data...</span>
<span>Traceback (most recent call last):</span>
  File <span>"app.py"</span>, line <span>2</span>, in <span>&lt;module&gt;</span>
<span>    </span><span>lazy</span> <span>from</span><span> </span><span>json</span><span> </span><span>import</span> <span>dumsp</span>
<span>ImportError</span>: <span>deferred import of 'json.dumsp' raised an exception during resolution</span>

<span>The above exception was the direct cause of the following exception:</span>

<span>Traceback (most recent call last):</span>
  File <span>"app.py"</span>, line <span>8</span>, in <span>&lt;module&gt;</span>
<span>    </span><span>result</span> <span>=</span> <span>dumsp</span><span>({</span><span>"key"</span><span>:</span> <span>"value"</span><span>})</span>
<span>             </span><span>^^^^^</span>
<span>ImportError</span>: <span>cannot import name 'dumsp' from 'json'. Did you mean: 'dump'?</span>
</pre></div>
<p>This exception chaining clearly shows: (1) where the lazy import was defined,
(2) that it was deferred, and (3) where the actual access happened that
triggered the error.</p>
<p>Reification does <strong>not</strong> automatically occur when a module that was previously
lazily imported is subsequently eagerly imported. Reification does <strong>not</strong>
immediately resolve all lazy objects (e.g. <code><span>lazy</span> <span>from</span></code> statements) that
referenced the module. It <strong>only</strong> resolves the lazy object being accessed.</p>
<p>Accessing a lazy object (from a global variable or a module attribute) reifies
the object. Accessing a module’s <code><span>__dict__</span></code> reifies <strong>all</strong> lazy objects in
that module. Operations that indirectly access <code><span>__dict__</span></code> (such as
<a href="https://docs.python.org/3/library/functions.html#dir" title="(in Python v3.13)"><code><span>dir()</span></code></a>) also trigger this behavior.</p>
<p>Example using <code><span>__dict__</span></code> from external code:</p>
<div><pre><span></span><span># my_module.py</span>
<span>import</span><span> </span><span>sys</span>
<span>lazy</span> <span>import</span><span> </span><span>json</span>

<span>print</span><span>(</span><span>'json'</span> <span>in</span> <span>sys</span><span>.</span><span>modules</span><span>)</span>  <span># False - still lazy</span>

<span># main.py</span>
<span>import</span><span> </span><span>sys</span>
<span>import</span><span> </span><span>my_module</span>

<span># Accessing __dict__ from external code DOES reify all lazy imports</span>
<span>d</span> <span>=</span> <span>my_module</span><span>.</span><span>__dict__</span>

<span>print</span><span>(</span><span>'json'</span> <span>in</span> <span>sys</span><span>.</span><span>modules</span><span>)</span>  <span># True - reified by __dict__ access</span>
<span>print</span><span>(</span><span>type</span><span>(</span><span>d</span><span>[</span><span>'json'</span><span>]))</span>  <span># &lt;class 'module'&gt;</span>
</pre></div>
<p>However, calling <code><span>globals()</span></code> does <strong>not</strong> trigger reification – it returns
the module’s dictionary, and accessing lazy objects through that dictionary
still returns lazy proxy objects that need to be manually reified upon use. A
lazy object can be resolved explicitly by calling the <code><span>get</span></code> method. Other,
more indirect ways of accessing arbitrary globals (e.g. inspecting
<code><span>frame.f_globals</span></code>) also do <strong>not</strong> reify all the objects.</p>
<p>Example using <code><span>globals()</span></code>:</p>
<div><pre><span></span><span>import</span><span> </span><span>sys</span>
<span>lazy</span> <span>import</span><span> </span><span>json</span>

<span># Calling globals() does NOT trigger reification</span>
<span>g</span> <span>=</span> <span>globals</span><span>()</span>

<span>print</span><span>(</span><span>'json'</span> <span>in</span> <span>sys</span><span>.</span><span>modules</span><span>)</span>  <span># False - still lazy</span>
<span>print</span><span>(</span><span>type</span><span>(</span><span>g</span><span>[</span><span>'json'</span><span>]))</span>  <span># &lt;class 'lazy_import'&gt;</span>

<span># Explicitly reify using the get() method</span>
<span>resolved</span> <span>=</span> <span>g</span><span>[</span><span>'json'</span><span>]</span><span>.</span><span>get</span><span>()</span>

<span>print</span><span>(</span><span>type</span><span>(</span><span>resolved</span><span>))</span>  <span># &lt;class 'module'&gt;</span>
<span>print</span><span>(</span><span>'json'</span> <span>in</span> <span>sys</span><span>.</span><span>modules</span><span>)</span>  <span># True - now loaded</span>
</pre></div>
</section>
</section>
<section id="reference-implementation">
<h2><a href="#reference-implementation" role="doc-backlink">Reference Implementation</a></h2>
<p>A reference implementation is available at:
<a href="https://github.com/LazyImportsCabal/cpython/tree/lazy">https://github.com/LazyImportsCabal/cpython/tree/lazy</a></p>
<section id="bytecode-and-adaptive-specialization">
<h3><a href="#bytecode-and-adaptive-specialization" role="doc-backlink">Bytecode and adaptive specialization</a></h3>
<p>Lazy imports are implemented through modifications to four bytecode
instructions: <code><span>IMPORT_NAME</span></code>, <code><span>IMPORT_FROM</span></code>, <code><span>LOAD_GLOBAL</span></code>, and
<code><span>LOAD_NAME</span></code>.</p>
<p>The <code><span>lazy</span></code> syntax sets a flag in the <code><span>IMPORT_NAME</span></code> instruction’s oparg
(<code><span>oparg</span> <span>&amp;</span> <span>0x01</span></code>). The interpreter checks this flag and calls
<code><span>_PyEval_LazyImportName()</span></code> instead of <code><span>_PyEval_ImportName()</span></code>, creating a
lazy import object rather than executing the import immediately. The
<code><span>IMPORT_FROM</span></code> instruction checks whether its source is a lazy import
(<code><span>PyLazyImport_CheckExact()</span></code>) and creates a lazy object for the attribute
rather than accessing it immediately.</p>
<p>When a lazy object is accessed, it must be reified. The <code><span>LOAD_GLOBAL</span></code>
instruction (used in function scopes) and <code><span>LOAD_NAME</span></code> instruction (used at
module and class level) both check whether the object being loaded is a lazy
import. If so, they call <code><span>_PyImport_LoadLazyImportTstate()</span></code> to perform the
actual import and store the module in <code><span>sys.modules</span></code>.</p>
<p>This check incurs a very small cost on each access. However, Python’s adaptive
interpreter can specialize <code><span>LOAD_GLOBAL</span></code> after observing that a lazy import
has been reified. After several executions, <code><span>LOAD_GLOBAL</span></code> becomes
<code><span>LOAD_GLOBAL_MODULE</span></code>, which accesses the module dictionary directly without
checking for lazy imports.</p>
<p>Examples of the bytecode generated:</p>
<div><pre><span></span><span>lazy</span> <span>import</span><span> </span><span>json</span>  <span># IMPORT_NAME with flag set</span>
</pre></div>
<p>Generates:</p>
<div><pre><span></span>IMPORT_NAME              1 (json + lazy)
</pre></div>
<div><pre><span></span><span>lazy</span> <span>from</span><span> </span><span>json</span><span> </span><span>import</span> <span>dumps</span>  <span># IMPORT_NAME + IMPORT_FROM</span>
</pre></div>
<p>Generates:</p>
<div><pre><span></span>IMPORT_NAME              1 (json + lazy)
IMPORT_FROM              1 (dumps)
</pre></div>
<div><pre><span></span><span>lazy</span> <span>import</span><span> </span><span>json</span>
<span>x</span> <span>=</span> <span>json</span>  <span># Module-level access</span>
</pre></div>
<p>Generates:</p>

<div><pre><span></span><span>lazy</span> <span>import</span><span> </span><span>json</span>

<span>def</span><span> </span><span>use_json</span><span>():</span>
    <span>return</span> <span>json</span><span>.</span><span>dumps</span><span>({})</span>  <span># Function scope</span>
</pre></div>
<p>Before any calls:</p>
<div><pre><span></span>LOAD_GLOBAL              0 (json)
LOAD_ATTR                2 (dumps)
</pre></div>
<p>After several calls, <code><span>LOAD_GLOBAL</span></code> specializes to <code><span>LOAD_GLOBAL_MODULE</span></code>:</p>
<div><pre><span></span>LOAD_GLOBAL_MODULE       0 (json)
LOAD_ATTR_MODULE         2 (dumps)
</pre></div>
</section>
<section id="lazy-imports-filter">
<h3><a href="#lazy-imports-filter" role="doc-backlink">Lazy imports filter</a></h3>
<p>This PEP adds two new functions to the <code><span>sys</span></code> module to manage the lazy
imports filter:</p>
<ul>
<li><code><span>sys.set_lazy_imports_filter(func)</span></code> - Sets the filter function. The
<code><span>func</span></code> parameter must have the signature: <code><span>func(importer:</span> <span>str,</span> <span>name:</span> <span>str,</span>
<span>fromlist:</span> <span>tuple[str,</span> <span>...]</span> <span>|</span> <span>None)</span> <span>-&gt;</span> <span>bool</span></code></li>
<li><code><span>sys.get_lazy_imports_filter()</span></code> - Returns the currently installed filter
function, or <code><span>None</span></code> if no filter is set.</li>
</ul>
<p>The filter function is called for every potentially lazy import, and must
return <code><span>True</span></code> if the import should be lazy. This allows for fine-grained
control over which imports should be lazy, useful for excluding modules with
known side-effect dependencies or registration patterns.</p>
<p>The filter mechanism serves as a foundation that tools, debuggers, linters,
and other ecosystem utilities can leverage to provide better lazy import
experiences. For example, static analysis tools could detect modules with side
effects and automatically configure appropriate filters. <strong>In the future</strong>
(out of scope for this PEP), this foundation may enable better ways to
declaratively specify which modules are safe for lazy importing, such as
package metadata, type stubs with lazy-safety annotations, or configuration
files. The current filter API is designed to be flexible enough to accommodate
such future enhancements without requiring changes to the core language
specification.</p>
<p>Example:</p>
<div><pre><span></span><span>import</span><span> </span><span>sys</span>

<span>def</span><span> </span><span>exclude_side_effect_modules</span><span>(</span><span>importer</span><span>,</span> <span>name</span><span>,</span> <span>fromlist</span><span>):</span>
<span>    </span><span>"""</span>
<span>    Filter function to exclude modules with import-time side effects.</span>

<span>    Args:</span>
<span>        importer: Name of the module doing the import</span>
<span>        name: Name of the module being imported</span>
<span>        fromlist: Tuple of names being imported (for 'from' imports), or None</span>

<span>    Returns:</span>
<span>        True to allow lazy import, False to force eager import</span>
<span>    """</span>
    <span># Modules known to have important import-time side effects</span>
    <span>side_effect_modules</span> <span>=</span> <span>{</span><span>'legacy_plugin_system'</span><span>,</span> <span>'metrics_collector'</span><span>}</span>

    <span>if</span> <span>name</span> <span>in</span> <span>side_effect_modules</span><span>:</span>
        <span>return</span> <span>False</span>  <span># Force eager import</span>

    <span>return</span> <span>True</span>  <span># Allow lazy import</span>

<span># Install the filter</span>
<span>sys</span><span>.</span><span>set_lazy_imports_filter</span><span>(</span><span>exclude_side_effect_modules</span><span>)</span>

<span># These imports are checked by the filter</span>
<span>lazy</span> <span>import</span><span> </span><span>data_processor</span>        <span># Filter returns True -&gt; stays lazy</span>
<span>lazy</span> <span>import</span><span> </span><span>legacy_plugin_system</span>  <span># Filter returns False -&gt; imported eagerly</span>

<span>print</span><span>(</span><span>'data_processor'</span> <span>in</span> <span>sys</span><span>.</span><span>modules</span><span>)</span>       <span># False - still lazy</span>
<span>print</span><span>(</span><span>'legacy_plugin_system'</span> <span>in</span> <span>sys</span><span>.</span><span>modules</span><span>)</span> <span># True - loaded eagerly</span>

<span># First use of data_processor triggers loading</span>
<span>result</span> <span>=</span> <span>data_processor</span><span>.</span><span>transform</span><span>(</span><span>data</span><span>)</span>
<span>print</span><span>(</span><span>'data_processor'</span> <span>in</span> <span>sys</span><span>.</span><span>modules</span><span>)</span>       <span># True - now loaded</span>
</pre></div>
</section>
<section id="global-lazy-imports-control">
<h3><a href="#global-lazy-imports-control" role="doc-backlink">Global lazy imports control</a></h3>
<p>The global lazy imports flag can be controlled through:</p>
<ul>
<li>The <code><span>-X</span> <span>lazy_imports=&lt;mode&gt;</span></code> command-line option</li>
<li>The <code><span>PYTHON_LAZY_IMPORTS=&lt;mode&gt;</span></code> environment variable</li>
<li>The <code><span>sys.set_lazy_imports(mode)</span></code> function (primarily for testing)</li>
</ul>
<p>Where <code><span>&lt;mode&gt;</span></code> can be:</p>
<ul>
<li><code><span>"default"</span></code> (or unset): Only explicitly marked lazy imports are lazy</li>
<li><code><span>"enabled"</span></code>: All module-level imports (except in <code><span>try</span></code>  or <code><span>with</span></code>
blocks and <code><span>import</span> <span>*</span></code>) become <em>potentially lazy</em></li>
<li><code><span>"disabled"</span></code>: No imports are lazy, even those explicitly marked with
<code><span>lazy</span></code> keyword</li>
</ul>
<p>When the global flag is set to <code><span>"enabled"</span></code>, all imports at the global level
of all modules are <em>potentially lazy</em> <strong>except</strong> for those inside a <code><span>try</span></code> or
<code><span>with</span></code> block or any wild card (<code><span>from</span> <span>...</span> <span>import</span> <span>*</span></code>) import.</p>
<p>If the global lazy imports flag is set to <code><span>"disabled"</span></code>, no <em>potentially
lazy</em> import is ever imported lazily, the import filter is never called, and
the behavior is equivalent to a regular <code><span>import</span></code> statement: the import is
<em>eager</em> (as if the lazy keyword was not used).</p>
</section>
</section>
<section id="backwards-compatibility">
<h2><a href="#backwards-compatibility" role="doc-backlink">Backwards Compatibility</a></h2>
<p>Lazy imports are <strong>opt-in</strong>. Existing programs continue to run unchanged
unless a project explicitly enables laziness (via <code><span>lazy</span></code> syntax,
<code><span>__lazy_modules__</span></code>, or an interpreter-wide switch).</p>
<section id="unchanged-semantics">
<h3><a href="#unchanged-semantics" role="doc-backlink">Unchanged semantics</a></h3>
<ul>
<li>Regular <code><span>import</span></code> and <code><span>from</span> <span>...</span> <span>import</span> <span>...</span></code> statements remain eager
unless explicitly made <em>potentially lazy</em> by the local or global mechanisms
provided.</li>
<li>Dynamic import APIs remain eager and unchanged: <code><span>__import__()</span></code> and
<code><span>importlib.import_module()</span></code>.</li>
<li>Import hooks and loaders continue to run under the standard import protocol
when a lazy object is reified.</li>
</ul>
</section>
<section id="observable-behavioral-shifts-opt-in-only">
<h3><a href="#observable-behavioral-shifts-opt-in-only" role="doc-backlink">Observable behavioral shifts (opt-in only)</a></h3>
<p>These changes are limited to bindings explicitly made lazy:</p>
<ul>
<li><strong>Error timing.</strong> Exceptions that would have occurred during an eager import
(for example <code><span>ImportError</span></code> or <code><span>AttributeError</span></code> for a missing member) now
occur at the first <em>use</em> of the lazy name.<div><pre><span></span><span># With eager import - error at import statement</span>
<span>import</span><span> </span><span>broken_module</span>  <span># ImportError raised here</span>

<span># With lazy import - error deferred</span>
<span>lazy</span> <span>import</span><span> </span><span>broken_module</span>
<span>print</span><span>(</span><span>"Import succeeded"</span><span>)</span>
<span>broken_module</span><span>.</span><span>foo</span><span>()</span>  <span># ImportError raised here on first use</span>
</pre></div>
</li>
<li><strong>Side-effect timing.</strong> Import-time side effects in lazily imported modules
occur at first use of the binding, not at module import time.</li>
<li><strong>Import order.</strong> Because modules are imported on first use, the order in
which modules are imported may differ from how they appear in code.</li>
<li><strong>Presence in ``sys.modules``.</strong> A lazily imported module does not appear in
<code><span>sys.modules</span></code> until first use. After reification, it must appear in
<code><span>sys.modules</span></code>. If some other code eagerly imports the same module before
first use, the lazy binding resolves to that existing (lazy) module object
when it is first used.</li>
<li><strong>Proxy visibility.</strong> Before first use, the bound name refers to a lazy
proxy. Indirect introspection that touches the value may observe a proxy
lazy object representation. After first use, the name is rebound to the real
object and becomes indistinguishable from an eager import.</li>
</ul>
</section>
<section id="thread-safety-and-reification">
<h3><a href="#thread-safety-and-reification" role="doc-backlink">Thread-safety and reification</a></h3>
<p>First use of a lazy binding follows the existing import-lock discipline.
Exactly one thread performs the import and <strong>atomically rebinds</strong> the
importing module’s global to the resolved object. Concurrent readers
thereafter observe the real object.</p>
<p>Lazy imports are thread-safe and have no special considerations for
free-threading. A module that would normally be imported in the main thread
may be imported in a different thread if that thread triggers the first access
to the lazy import. This is not a problem: the import lock ensures thread
safety regardless of which thread performs the import.</p>
<p>Subinterpreters are supported. Each subinterpreter maintains its own
<code><span>sys.lazy_modules</span></code> and import state, so lazy imports in one subinterpreter
do not affect others.</p>
</section>
<section id="typing-and-tools">
<h3><a href="#typing-and-tools" role="doc-backlink">Typing and tools</a></h3>
<p>Type checkers and static analyzers may treat <code><span>lazy</span></code> imports as ordinary
imports for name resolution. At runtime, annotation-only imports can be marked
<code><span>lazy</span></code> to avoid startup overhead. IDEs and debuggers should be prepared to
display lazy proxies before first use and the real objects thereafter.</p>
</section>
</section>
<section id="security-implications">
<h2><a href="#security-implications" role="doc-backlink">Security Implications</a></h2>
<p>There are no known security vulnerabilities introduced by lazy imports.</p>
</section>
<section id="how-to-teach-this">
<h2><a href="#how-to-teach-this" role="doc-backlink">How to Teach This</a></h2>
<p>The new <code><span>lazy</span></code> keyword will be documented as part of the language standard.</p>
<p>As this feature is opt-in, new Python users should be able to continue using
the language as they are used to. For experienced developers, we expect them
to leverage lazy imports for the variety of benefits listed above (decreased
latency, decreased memory usage, etc) on a case-by-case basis. Developers
interested in the performance of their Python binary will likely leverage
profiling to understand the import time overhead in their codebase and mark
the necessary imports as <code><span>lazy</span></code>. In addition, developers can mark imports
that will only be used for type annotations as <code><span>lazy</span></code>.</p>
<p>Below is guidance on how to best take advantage of lazy imports and how to
avoid incompatibilities:</p>
<ul>
<li>When adopting lazy imports, users should be aware that eliding an import
until it is used will result in side effects not being executed. In turn,
users should be wary of modules that rely on import time side effects.
Perhaps the most common reliance on import side effects is the registry
pattern, where population of some external registry happens implicitly
during the importing of modules, often via decorators but sometimes
implemented via metaclasses or <code><span>__init_subclass__</span></code>. Instead, registries of
objects should be constructed via explicit discovery processes (e.g. a
well-known function to call).<div><pre><span></span><span># Problematic: Plugin registers itself on import</span>
<span># my_plugin.py</span>
<span>from</span><span> </span><span>plugin_registry</span><span> </span><span>import</span> <span>register_plugin</span>

<span>@register_plugin</span><span>(</span><span>"MyPlugin"</span><span>)</span>
<span>class</span><span> </span><span>MyPlugin</span><span>:</span>
    <span>pass</span>

<span># In main code:</span>
<span>lazy</span> <span>import</span><span> </span><span>my_plugin</span>
<span># Plugin NOT registered yet - module not loaded!</span>

<span># Better: Explicit discovery</span>
<span># plugin_registry.py</span>
<span>def</span><span> </span><span>discover_plugins</span><span>():</span>
    <span>from</span><span> </span><span>my_plugin</span><span> </span><span>import</span> <span>MyPlugin</span>
    <span>register_plugin</span><span>(</span><span>MyPlugin</span><span>)</span>

<span># In main code:</span>
<span>plugin_registry</span><span>.</span><span>discover_plugins</span><span>()</span>  <span># Explicit loading</span>
</pre></div>
</li>
<li>Always import needed submodules explicitly. It is not enough to rely on a
different import to ensure a module has its submodules as attributes.
Plainly, unless there is an explicit <code><span>from</span> <span>.</span> <span>import</span> <span>bar</span></code> in
<code><span>foo/__init__.py</span></code>, always use <code><span>import</span> <span>foo.bar;</span> <span>foo.bar.Baz</span></code>, not
<code><span>import</span> <span>foo;</span> <span>foo.bar.Baz</span></code>. The latter only works (unreliably) because the
attribute <code><span>foo.bar</span></code> is added as a side effect of <code><span>foo.bar</span></code> being
imported somewhere else.</li>
<li>Users who are moving imports into functions to improve startup time, should
instead consider keeping them where they are but adding the <code><span>lazy</span></code>
keyword. This allows them to keep dependencies clear and avoid the overhead
of repeatedly re-resolving the import but will still speed up the program.<div><pre><span></span><span># Before: Inline import (repeated overhead)</span>
<span>def</span><span> </span><span>process_data</span><span>(</span><span>data</span><span>):</span>
    <span>import</span><span> </span><span>json</span>  <span># Re-resolved on every call</span>
    <span>return</span> <span>json</span><span>.</span><span>dumps</span><span>(</span><span>data</span><span>)</span>

<span># After: Lazy import at module level</span>
<span>lazy</span> <span>import</span><span> </span><span>json</span>

<span>def</span><span> </span><span>process_data</span><span>(</span><span>data</span><span>):</span>
    <span>return</span> <span>json</span><span>.</span><span>dumps</span><span>(</span><span>data</span><span>)</span>  <span># Loaded once on first call</span>
</pre></div>
</li>
<li>Avoid using wild card (star) imports, as those are always eager.</li>
</ul>
</section>
<section id="faq">
<h2><a href="#faq" role="doc-backlink">FAQ</a></h2>
<p><strong>Q: How does this differ from the rejected PEP 690?</strong></p>
<p>A: PEP 810 takes an explicit, opt-in approach instead of <a href="https://pep-previews--4622.org.readthedocs.build/pep-0690/" title="PEP 690 – Lazy Imports">PEP 690</a>’s implicit
global approach. The key differences are:</p>
<ul>
<li><strong>Explicit syntax</strong>: <code><span>lazy</span> <span>import</span> <span>foo</span></code> clearly marks which imports are
lazy.</li>
<li><strong>Local scope</strong>: Laziness only affects the specific import statement, not
cascading to dependencies.</li>
<li><strong>Simpler implementation</strong>: Uses proxy objects instead of modifying core
dictionary behavior.</li>
</ul>
<p><strong>Q: What happens when lazy imports encounter errors?</strong></p>
<p>A: Import errors (<code><span>ImportError</span></code>, <code><span>ModuleNotFoundError</span></code>, syntax errors) are
deferred until first use of the lazy name. This is similar to moving an import
into a function. The error will occur with a clear traceback pointing to the
first access of the lazy object.</p>
<p>The implementation provides enhanced error reporting through exception
chaining. When a lazy import fails during reification, the original exception
is preserved and chained, showing both where the import was defined and where
it was first used:</p>
<div><pre><span></span><span>Traceback (most recent call last):</span>
  File <span>"test.py"</span>, line <span>1</span>, in <span>&lt;module&gt;</span>
<span>    </span><span>lazy</span> <span>import</span><span> </span><span>broken_module</span>
<span>ImportError</span>: <span>deferred import of 'broken_module' raised an exception during resolution</span>

<span>The above exception was the direct cause of the following exception:</span>

<span>Traceback (most recent call last):</span>
  File <span>"test.py"</span>, line <span>3</span>, in <span>&lt;module&gt;</span>
<span>    </span><span>broken_module</span><span>.</span><span>foo</span><span>()</span>
<span>    </span><span>^^^^^^^^^^^^^</span>
  File <span>"broken_module.py"</span>, line <span>2</span>, in <span>&lt;module&gt;</span>
<span>    </span><span>1</span><span>/</span><span>0</span>
<span>ZeroDivisionError</span>: <span>division by zero</span>
</pre></div>
<p><strong>Q: How do lazy imports affect modules with import-time side effects?</strong></p>
<p>A: Side effects are deferred until first use. This is generally desirable for
performance, but may require code changes for modules that rely on import-time
registration patterns. We recommend:</p>
<ul>
<li>Use explicit initialization functions instead of import-time side effects</li>
<li>Call initialization functions explicitly when needed</li>
<li>Avoid relying on import order for side effects</li>
</ul>
<p><strong>Q: Can I use lazy imports with</strong> <code><span>from</span> <span>...</span> <span>import</span> <span>...</span></code> <strong>statements?</strong></p>
<p>A: Yes, as long as you don’t use <code><span>from</span> <span>...</span> <span>import</span> <span>*</span></code>. Both <code><span>lazy</span> <span>import</span>
<span>foo</span></code> and <code><span>lazy</span> <span>from</span> <span>foo</span> <span>import</span> <span>bar</span></code> are supported. The <code><span>bar</span></code> name will be
bound to a lazy object that resolves to <code><span>foo.bar</span></code> on first use.</p>
<p><strong>Q: Does</strong> <code><span>lazy</span> <span>from</span> <span>module</span> <span>import</span> <span>Class</span></code> <strong>load the entire module or just
the class?</strong></p>
<p>A: It loads the <strong>entire module</strong>, not just the class. This is because
Python’s import system always executes the complete module file – there’s no
mechanism to execute only part of a <code><span>.py</span></code> file. When you first access
<code><span>Class</span></code>, Python:</p>
<ol>
<li>Loads and executes the entire <code><span>module.py</span></code> file</li>
<li>Extracts the <code><span>Class</span></code> attribute from the resulting module object</li>
<li>Binds <code><span>Class</span></code> to the name in your namespace</li>
</ol>
<p>This is identical to eager <code><span>from</span> <span>module</span> <span>import</span> <span>Class</span></code> behavior. The only
difference with lazy imports is that steps 1-3 happen on first use instead of
at the import statement.</p>
<div><pre><span></span><span># heavy_module.py</span>
<span>print</span><span>(</span><span>"Loading heavy_module"</span><span>)</span>  <span># This ALWAYS runs when module loads</span>

<span>class</span><span> </span><span>MyClass</span><span>:</span>
    <span>pass</span>

<span>class</span><span> </span><span>UnusedClass</span><span>:</span>
    <span>pass</span>  <span># Also gets defined, even though we don't import it</span>

<span># app.py</span>
<span>lazy</span> <span>from</span><span> </span><span>heavy_module</span><span> </span><span>import</span> <span>MyClass</span>

<span>print</span><span>(</span><span>"Import statement done"</span><span>)</span>  <span># heavy_module not loaded yet</span>
<span>obj</span> <span>=</span> <span>MyClass</span><span>()</span>                  <span># NOW "Loading heavy_module" prints</span>
                                 <span># (and UnusedClass gets defined too)</span>
</pre></div>
<p><strong>Key point</strong>: Lazy imports defer <em>when</em> a module loads, not <em>what</em> gets
loaded. You cannot selectively load only parts of a module – Python’s import
system doesn’t support partial module execution.</p>
<p><strong>Q: What about type annotations and</strong> <code><span>TYPE_CHECKING</span></code> <strong>imports?</strong></p>
<p>A: Lazy imports eliminate the common need for <code><span>TYPE_CHECKING</span></code> guards. You
can write:</p>
<div><pre><span></span><span>lazy</span> <span>from</span><span> </span><span>collections.abc</span><span> </span><span>import</span> <span>Sequence</span><span>,</span> <span>Mapping</span>  <span># No runtime cost</span>

<span>def</span><span> </span><span>process</span><span>(</span><span>items</span><span>:</span> <span>Sequence</span><span>[</span><span>str</span><span>])</span> <span>-&gt;</span> <span>Mapping</span><span>[</span><span>str</span><span>,</span> <span>int</span><span>]:</span>
    <span>...</span>
</pre></div>
<p>Instead of:</p>
<div><pre><span></span><span>from</span><span> </span><span>typing</span><span> </span><span>import</span> <span>TYPE_CHECKING</span>
<span>if</span> <span>TYPE_CHECKING</span><span>:</span>
    <span>from</span><span> </span><span>collections.abc</span><span> </span><span>import</span> <span>Sequence</span><span>,</span> <span>Mapping</span>

<span>def</span><span> </span><span>process</span><span>(</span><span>items</span><span>:</span> <span>Sequence</span><span>[</span><span>str</span><span>])</span> <span>-&gt;</span> <span>Mapping</span><span>[</span><span>str</span><span>,</span> <span>int</span><span>]:</span>
    <span>...</span>
</pre></div>
<p><strong>Q: What’s the performance overhead of lazy imports?</strong></p>
<p>A: The overhead is minimal:</p>
<ul>
<li>Zero overhead after first use thanks to the adaptive interpreter optimizing
the slow path away.</li>
<li>Small one-time cost to create the proxy object.</li>
<li>Reification (first use) has the same cost as a regular import.</li>
<li>No ongoing performance penalty unlike <code><span>importlib.util.LazyLoader</span></code>.</li>
</ul>
<p>Benchmarking with the <a href="https://github.com/facebookexperimental/free-threading-benchmarking/blob/main/results/bm-20250922-3.15.0a0-27836e5/bm-20250922-vultr-x86_64-DinoV-lazy_imports-3.15.0a0-27836e5-vs-base.svg">pyperformance suite</a> shows the implementation is
performance neutral when lazy imports are not used.</p>
<p><strong>Q: Can I mix lazy and eager imports of the same module?</strong></p>
<p>A: Yes. If module <code><span>foo</span></code> is imported both lazily and eagerly in the same
program, the eager import takes precedence and both bindings resolve to the
same module object.</p>
<p><strong>Q: How do I migrate existing code to use lazy imports?</strong></p>
<p>A: Migration is incremental:</p>
<ol>
<li>Identify slow-loading modules using profiling tools.</li>
<li>Add <code><span>lazy</span></code> keyword to imports that aren’t needed immediately.</li>
<li>Test that side-effect timing changes don’t break functionality.</li>
<li>Use <code><span>__lazy_modules__</span></code> for compatibility with older Python versions.</li>
</ol>
<p><strong>Q: What about star imports</strong> (<code><span>from</span> <span>module</span> <span>import</span> <span>*</span></code>)?</p>
<p>A: Wild card (star) imports cannot be lazy - they remain eager. This is
because the set of names being imported cannot be determined without loading
the module. Using the <code><span>lazy</span></code> keyword with star imports will be a syntax
error. If lazy imports are globally enabled, star imports will still be eager.</p>
<p><strong>Q: How do lazy imports interact with import hooks and custom loaders?</strong></p>
<p>A: Import hooks and loaders work normally. When a lazy object is first used,
the standard import protocol runs, including any custom hooks or loaders that
were in place at reification time.</p>
<p><strong>Q: What happens in multi-threaded environments?</strong></p>
<p>A: Lazy import reification is thread-safe. Only one thread will perform the
actual import, and the binding is atomically updated. Other threads will see
either the lazy proxy or the final resolved object.</p>
<p><strong>Q: Can I force reification of a lazy import without using it?</strong></p>
<p>A: Yes, accessing a module’s <code><span>__dict__</span></code> will reify all lazy objects in that
module. Individual lazy objects can be resolved by calling their <code><span>get()</span></code>
method.</p>
<p><strong>Q: What’s the difference between</strong> <code><span>globals()</span></code> <strong>and</strong> <code><span>mod.__dict__</span></code> <strong>for lazy imports?</strong></p>
<p>A: Calling <code><span>globals()</span></code> returns the module’s dictionary without reifying lazy
imports – you’ll see lazy proxy objects when accessing them through the
returned dictionary. However, accessing <code><span>mod.__dict__</span></code> from external code
reifies all lazy imports in that module first. This design ensures:</p>
<div><pre><span></span><span># In your module:</span>
<span>lazy</span> <span>import</span><span> </span><span>json</span>

<span>g</span> <span>=</span> <span>globals</span><span>()</span>
<span>print</span><span>(</span><span>type</span><span>(</span><span>g</span><span>[</span><span>'json'</span><span>]))</span>  <span># &lt;class 'lazy_import'&gt; - your problem</span>

<span># From external code:</span>
<span>import</span><span> </span><span>sys</span>
<span>mod</span> <span>=</span> <span>sys</span><span>.</span><span>modules</span><span>[</span><span>'your_module'</span><span>]</span>
<span>d</span> <span>=</span> <span>mod</span><span>.</span><span>__dict__</span>
<span>print</span><span>(</span><span>type</span><span>(</span><span>d</span><span>[</span><span>'json'</span><span>]))</span>  <span># &lt;class 'module'&gt; - reified for external access</span>
</pre></div>
<p>This distinction means adding lazy imports and calling <code><span>globals()</span></code> is your
responsibility to manage, while external code accessing <code><span>mod.__dict__</span></code>
always sees fully loaded modules.</p>
<p><strong>Q: Why not use</strong> <code><span>importlib.util.LazyLoader</span></code> <strong>instead?</strong></p>
<p>A: <code><span>LazyLoader</span></code> has significant limitations:</p>
<ul>
<li>Requires verbose setup code for each lazy import.</li>
<li>Has ongoing performance overhead on every attribute access.</li>
<li>Doesn’t work well with <code><span>from</span> <span>...</span> <span>import</span></code> statements.</li>
<li>Less clear and standard than dedicated syntax.</li>
</ul>
<p><strong>Q: Will this break tools like</strong> <code><span>isort</span></code> <strong>or</strong> <code><span>black</span></code>?</p>
<p>A: Tools will need updates to recognize the <code><span>lazy</span></code> keyword, but the changes
should be minimal since the import structure remains the same. The keyword
appears at the beginning, making it easy to parse.</p>
<p><strong>Q: How do I know if a library is compatible with lazy imports?</strong></p>
<p>A: Most libraries should work fine with lazy imports. Libraries that might
have issues:</p>
<ul>
<li>Those with essential import-time side effects (registration,
monkey-patching).</li>
<li>Those that expect specific import ordering.</li>
<li>Those that modify global state during import.</li>
</ul>
<p>When in doubt, test lazy imports with your specific use cases.</p>
<p><strong>Q: What happens if I globally enable lazy imports mode and a library doesn’t
work correctly?</strong></p>
<p>A: <em>Note: This is an advanced feature.</em> You can use the lazy imports filter to
exclude specific modules that are known to have problematic side effects:</p>
<div><pre><span></span><span>import</span><span> </span><span>sys</span>

<span>def</span><span> </span><span>my_filter</span><span>(</span><span>importer</span><span>,</span> <span>name</span><span>,</span> <span>fromlist</span><span>):</span>
    <span># Don't lazily import modules known to have side effects</span>
    <span>if</span> <span>name</span> <span>in</span> <span>{</span><span>'problematic_module'</span><span>,</span> <span>'another_module'</span><span>}:</span>
        <span>return</span> <span>False</span>  <span># Import eagerly</span>
    <span>return</span> <span>True</span>  <span># Allow lazy import</span>

<span>sys</span><span>.</span><span>set_lazy_imports_filter</span><span>(</span><span>my_filter</span><span>)</span>
</pre></div>
<p>The filter function receives the importer module name, the module being
imported, and the fromlist (if using <code><span>from</span> <span>...</span> <span>import</span></code>). Returning <code><span>False</span></code>
forces an eager import.</p>
<p>Alternatively, set the global mode to <code><span>"disabled"</span></code> via <code><span>-X</span>
<span>lazy_imports=disabled</span></code> to turn off all lazy imports for debugging.</p>
<p><strong>Q: Can I use lazy imports inside functions?</strong></p>
<p>A: No, the <code><span>lazy</span></code> keyword is only allowed at module level. For
function-level lazy loading, use traditional inline imports or move the import
to module level with <code><span>lazy</span></code>.</p>
<p><strong>Q: What about forwards compatibility with older Python versions?</strong></p>
<p>A: Use the <code><span>__lazy_modules__</span></code> global for compatibility:</p>
<div><pre><span></span><span># Works on Python 3.15+ as lazy, eager on older versions</span>
<span>__lazy_modules__</span> <span>=</span> <span>[</span><span>'expensive_module'</span><span>,</span> <span>'expensive_module_2'</span><span>]</span>
<span>import</span><span> </span><span>expensive_module</span>
<span>from</span><span> </span><span>expensive_module_2</span><span> </span><span>import</span> <span>MyClass</span>
</pre></div>
<p>The <code><span>__lazy_modules__</span></code> attribute is a list of module name strings. When
an import statement is executed, Python checks if the module name being
imported appears in <code><span>__lazy_modules__</span></code>. If it does, the import is
treated as if it had the <code><span>lazy</span></code> keyword (becoming <em>potentially lazy</em>). On
Python versions before 3.15 that don’t support lazy imports, the
<code><span>__lazy_modules__</span></code> attribute is simply ignored and imports proceed
eagerly as normal.</p>
<p>This provides a migration path until you can rely on the <code><span>lazy</span></code> keyword. For
maximum predictability, it’s recommended to define <code><span>__lazy_modules__</span></code>
once, before any imports. But as it is checked on each import, it can be
modified between <code><span>import</span></code> statements.</p>
<p><strong>Q: How do explicit lazy imports interact with PEP-649/PEP-749</strong></p>
<p>A: If an annotation is not stringified, it is an expression that is evaluated
at a later time. It will only be resolved if the annotation is accessed. In
the example below, the <code><span>fake_typing</span></code> module is only loaded when the user
inspects the <code><span>__annotations__</span></code> dictionary. The <code><span>fake_typing</span></code> module would
also be loaded if the user uses <code><span>annotationlib.get_annotations()</span></code> or
<code><span>getattr</span></code> to access the annotations.</p>
<div><pre><span></span><span>lazy</span> <span>from</span><span> </span><span>fake_typing</span><span> </span><span>import</span> <span>MyFakeType</span>
<span>def</span><span> </span><span>foo</span><span>(</span><span>x</span><span>:</span> <span>MyFakeType</span><span>):</span>
  <span>pass</span>
<span>print</span><span>(</span><span>foo</span><span>.</span><span>__annotations__</span><span>)</span>  <span># Triggers loading the fake_typing module</span>
</pre></div>
<p><strong>Q: How do lazy imports interact with</strong> <code><span>dir()</span></code>, <code><span>getattr()</span></code>, <strong>and
module introspection?</strong></p>
<p>A: Accessing lazy imports through normal attribute access or <code><span>getattr()</span></code>
will trigger reification. Calling <code><span>dir()</span></code> on a module will reify all lazy
imports in that module to ensure the directory listing is complete. This is
similar to accessing <code><span>mod.__dict__</span></code>.</p>
<div><pre><span></span><span>lazy</span> <span>import</span><span> </span><span>json</span>

<span># Before any access</span>
<span># json not in sys.modules</span>

<span># Any of these trigger reification:</span>
<span>dumps_func</span> <span>=</span> <span>json</span><span>.</span><span>dumps</span>
<span>dumps_func</span> <span>=</span> <span>getattr</span><span>(</span><span>json</span><span>,</span> <span>'dumps'</span><span>)</span>
<span>dir</span><span>(</span><span>json</span><span>)</span>
<span># Now json is in sys.modules</span>
</pre></div>
<p><strong>Q: Do lazy imports work with circular imports?</strong></p>
<p>A: Lazy imports don’t automatically solve circular import problems. If two
modules have a circular dependency, making the imports lazy might help <strong>only
if</strong> the circular reference isn’t accessed during module initialization.
However, if either module accesses the other during import time, you’ll still
get an error.</p>
<p><strong>Example that works</strong> (deferred access in functions):</p>
<div><pre><span></span><span># user_model.py</span>
<span>lazy</span> <span>import</span><span> </span><span>post_model</span>

<span>class</span><span> </span><span>User</span><span>:</span>
    <span>def</span><span> </span><span>get_posts</span><span>(</span><span>self</span><span>):</span>
        <span># OK - post_model accessed inside function, not during import</span>
        <span>return</span> <span>post_model</span><span>.</span><span>Post</span><span>.</span><span>get_by_user</span><span>(</span><span>self</span><span>.</span><span>name</span><span>)</span>

<span># post_model.py</span>
<span>lazy</span> <span>import</span><span> </span><span>user_model</span>

<span>class</span><span> </span><span>Post</span><span>:</span>
    <span>@staticmethod</span>
    <span>def</span><span> </span><span>get_by_user</span><span>(</span><span>username</span><span>):</span>
        <span>return</span> <span>f</span><span>"Posts by </span><span>{</span><span>username</span><span>}</span><span>"</span>
</pre></div>
<p>This works because neither module accesses the other at module level – the
access happens later when <code><span>get_posts()</span></code> is called.</p>
<p><strong>Example that fails</strong> (access during import):</p>
<div><pre><span></span><span># module_a.py</span>
<span>lazy</span> <span>import</span><span> </span><span>module_b</span>

<span>result</span> <span>=</span> <span>module_b</span><span>.</span><span>get_value</span><span>()</span>  <span># Error! Accessing during import</span>

<span>def</span><span> </span><span>func</span><span>():</span>
    <span>return</span> <span>"A"</span>

<span># module_b.py</span>
<span>lazy</span> <span>import</span><span> </span><span>module_a</span>

<span>result</span> <span>=</span> <span>module_a</span><span>.</span><span>func</span><span>()</span>  <span># Circular dependency error here</span>

<span>def</span><span> </span><span>get_value</span><span>():</span>
    <span>return</span> <span>"B"</span>
</pre></div>
<p>This fails because <code><span>module_a</span></code> tries to access <code><span>module_b</span></code> at import time,
which then tries to access <code><span>module_a</span></code> before it’s fully initialized.</p>
<p>The best practice is still to avoid circular imports in your code design.</p>
<p><strong>Q: Will lazy imports affect the performance of my hot paths?</strong></p>
<p>A: After first use, lazy imports have <strong>zero overhead</strong> thanks to the adaptive
interpreter. The interpreter specializes the bytecode (e.g., <code><span>LOAD_GLOBAL</span></code>
becomes <code><span>LOAD_GLOBAL_MODULE</span></code>) which eliminates the lazy check on subsequent
accesses. This means once a lazy import is reified, accessing it is just as
fast as a normal import.</p>
<div><pre><span></span><span>lazy</span> <span>import</span><span> </span><span>json</span>

<span>def</span><span> </span><span>use_json</span><span>():</span>
    <span>return</span> <span>json</span><span>.</span><span>dumps</span><span>({</span><span>"test"</span><span>:</span> <span>1</span><span>})</span>

<span># First call triggers reification</span>
<span>use_json</span><span>()</span>

<span># After 2-3 calls, bytecode is specialized</span>
<span>use_json</span><span>()</span>
<span>use_json</span><span>()</span>
</pre></div>
<p>You can observe the specialization using <code><span>dis.dis(use_json,</span> <span>adaptive=True)</span></code>:</p>
<div><pre><span></span>=== Before specialization ===
LOAD_GLOBAL              0 (json)
LOAD_ATTR                2 (dumps)

=== After 3 calls (specialized) ===
LOAD_GLOBAL_MODULE       0 (json)
LOAD_ATTR_MODULE         2 (dumps)
</pre></div>
<p>The specialized <code><span>LOAD_GLOBAL_MODULE</span></code> and <code><span>LOAD_ATTR_MODULE</span></code> instructions
are optimized fast paths with no overhead for checking lazy imports.</p>
<p><strong>Q: What about</strong> <code><span>sys.modules</span></code>? <strong>When does a lazy import appear there?</strong></p>
<p>A: A lazily imported module does <strong>not</strong> appear in <code><span>sys.modules</span></code> until it’s
reified (first used). Once reified, it appears in <code><span>sys.modules</span></code> just like
any eager import.</p>
<div><pre><span></span><span>import</span><span> </span><span>sys</span>
<span>lazy</span> <span>import</span><span> </span><span>json</span>

<span>print</span><span>(</span><span>'json'</span> <span>in</span> <span>sys</span><span>.</span><span>modules</span><span>)</span>  <span># False</span>

<span>result</span> <span>=</span> <span>json</span><span>.</span><span>dumps</span><span>({</span><span>"key"</span><span>:</span> <span>"value"</span><span>})</span>  <span># First use</span>

<span>print</span><span>(</span><span>'json'</span> <span>in</span> <span>sys</span><span>.</span><span>modules</span><span>)</span>  <span># True</span>
</pre></div>
<p><strong>Q: Why you chose ``lazy`` as the kwyword name?</strong></p>
<p>A: Not “why”… memorize! :)</p>
</section>
<section id="alternate-implementation-ideas">
<h2><a href="#alternate-implementation-ideas" role="doc-backlink">Alternate Implementation Ideas</a></h2>
<p>Here are some alternative design decisions that were considered during the
development of this PEP. While the current proposal represents what we believe
to be the best balance of simplicity, performance, and maintainability, these
alternatives offer different trade-offs that may be valuable for implementers
to consider or for future refinements.</p>
<section id="leveraging-a-subclass-of-dict">
<h3><a href="#leveraging-a-subclass-of-dict" role="doc-backlink">Leveraging a subclass of dict</a></h3>
<p>Instead of updating the internal dict object to directly add the fields needed
to support lazy imports, we could create a subclass of the dict object to be
used specifically for Lazy Import enablement. This would still be a leaky
abstraction though - methods can be called directly such as
<code><span>dict.__getitem__</span></code> and it would impact the performance of globals lookup in
the interpreter.</p>
</section>
<section id="alternate-keyword-names">
<h3><a href="#alternate-keyword-names" role="doc-backlink">Alternate keyword names</a></h3>
<p>For this PEP, we decided to propose <code><span>lazy</span></code> for the explicit keyword as it
felt the most familar to those already focused on optimizing import overhead.
We also considered a variety of other options to support explicit lazy
imports. The most compelling alternates were <code><span>defer</span></code> and <code><span>delay</span></code>.</p>
</section>
</section>
<section id="rejected-ideas">
<h2><a href="#rejected-ideas" role="doc-backlink">Rejected Ideas</a></h2>
<section id="modification-of-the-dict-object">
<h3><a href="#modification-of-the-dict-object" role="doc-backlink">Modification of the dict object</a></h3>
<p>The initial PEP for lazy imports (PEP 690) relied heavily on the modification
of the internal dict object to support lazy imports. We recognize that this
data structure is highly tuned, heavily used across the codebase, and very
performance sensitive. Because of the importance of this data structure and
the desire to keep the implementation of lazy imports encapsulated from users
who may have no interest in the feature, we’ve decided to invest in an
alternate approach.</p>
<p>The dictionary is the foundational data structure in Python. Every object’s
attributes are stored in a dict, and dicts are used throughout the runtime for
namespaces, keyword arguments, and more. Adding any kind of hook or special
behavior to dicts to support lazy imports would:</p>
<ol>
<li>Prevent critical interpreter optimizations including future JIT
compilation.</li>
<li>Add complexity to a data structure that must remain simple and fast.</li>
<li>Affect every part of Python, not just import behavior.</li>
<li>Violate separation of concerns – the hash table shouldn’t know about the
import system.</li>
</ol>
<p>Past decisions that violated this principle of keeping core abstractions clean
have caused significant pain in the CPython ecosystem, making optimization
difficult and introducing subtle bugs.</p>
</section>
<section id="placing-the-lazy-keyword-in-the-middle-of-from-imports">
<h3><a href="#placing-the-lazy-keyword-in-the-middle-of-from-imports" role="doc-backlink">Placing the <code><span>lazy</span></code> keyword in the middle of from imports</a></h3>
<p>While we found <code><span>from</span> <span>foo</span> <span>lazy</span> <span>import</span> <span>bar</span></code> to be a really intuitive placement
for the new explicit syntax, we quickly learned that placing the <code><span>lazy</span></code>
keyword here is already syntactically allowed in Python. This is because
<code><span>from</span> <span>.</span> <span>lazy</span> <span>import</span> <span>bar</span></code> is legal syntax (because whitespace does not
matter.)</p>
</section>
<section id="placing-the-lazy-keyword-at-the-end-of-import-statements">
<h3><a href="#placing-the-lazy-keyword-at-the-end-of-import-statements" role="doc-backlink">Placing the <code><span>lazy</span></code> keyword at the end of import statements</a></h3>
<p>We discussed appending lazy to the end of import statements like such <code><span>import</span>
<span>foo</span> <span>lazy</span></code> or <code><span>from</span> <span>foo</span> <span>import</span> <span>bar,</span> <span>baz</span> <span>lazy</span></code> but ultimately decided that
this approach provided less clarity. For example, if multiple modules are
imported in a single statement, it is unclear if the lazy binding applies to
all of the imported objects or just a subset of the items.</p>
</section>
<section id="returning-a-proxy-dict-from-globals">
<h3><a href="#returning-a-proxy-dict-from-globals" role="doc-backlink">Returning a proxy dict from <code><span>globals()</span></code></a></h3>
<p>An alternative to reifying on <code><span>globals()</span></code> or exposing lazy objects would be
to return a proxy dictionary that automatically reifies lazy objects when
they’re accessed through the proxy. This would seemingly give the best of both
worlds: <code><span>globals()</span></code> returns immediately without reification cost, but
accessing items through the result would automatically resolve lazy imports.</p>
<p>However, this approach is fundamentally incompatible with how <code><span>globals()</span></code> is
used in practice. Many standard library functions and built-ins expect
<code><span>globals()</span></code> to return a real <code><span>dict</span></code> object, not a proxy:</p>
<ul>
<li><code><span>exec(code,</span> <span>globals())</span></code> requires a real dict.</li>
<li><code><span>eval(expr,</span> <span>globals())</span></code> requires a real dict.</li>
<li>Functions that check <code><span>type(globals())</span> <span>is</span> <span>dict</span></code> would break.</li>
<li>Dictionary methods like <code><span>.update()</span></code> would need special handling.</li>
<li>Performance would suffer from the indirection on every access.</li>
</ul>
<p>The proxy would need to be so transparent that it would be indistinguishable
from a real dict in almost all cases, which is extremely difficult to achieve
correctly. Any deviation from true dict behavior would be a source of subtle
bugs.</p>
</section>
<section id="reifying-lazy-imports-when-globals-is-called">
<h3><a href="#reifying-lazy-imports-when-globals-is-called" role="doc-backlink">Reifying lazy imports when <code><span>globals()</span></code> is called</a></h3>
<p>Calling <code><span>globals()</span></code> returns the module’s namespace dictionary without
triggering reification of lazy imports. Accessing lazy objects through the
returned dictionary yields the lazy proxy objects themselves. This is an
intentional design decision for several reasons:</p>
<p><strong>The key distinction</strong>: Adding a lazy import and calling <code><span>globals()</span></code> is the
module author’s concern and under their control. However, accessing
<code><span>mod.__dict__</span></code> from external code is a different scenario – it crosses
module boundaries and affects someone else’s code. Therefore, <code><span>mod.__dict__</span></code>
access reifies all lazy imports to ensure external code sees fully realized
modules, while <code><span>globals()</span></code> preserves lazy objects for the module’s own
introspection needs.</p>
<p><strong>Technical challenges</strong>: It is impossible to safely reify on-demand when
<code><span>globals()</span></code> is called because we cannot return a proxy dictionary – this
would break common usages like passing the result to <code><span>exec()</span></code> or other
built-ins that expect a real dictionary. The only alternative would be to
eagerly reify all lazy imports whenever <code><span>globals()</span></code> is called, but this
behavior would be surprising and potentially expensive.</p>
<p><strong>Performance concerns</strong>: It is impractical to cache whether a reification
scan has been performed with just the globals dictionary reference, whereas
module attribute access (the primary use case) can efficiently cache
reification state in the module object itself.</p>
<p><strong>Use case rationale</strong>: The chosen design makes sense precisely because of
this distinction: adding a lazy import and calling <code><span>globals()</span></code> is your
problem to manage, while having lazy imports visible in <code><span>mod.__dict__</span></code>
becomes someone else’s problem. By reifying on <code><span>__dict__</span></code> access but not on
<code><span>globals()</span></code>, we ensure external code always sees fully loaded modules while
giving module authors control over their own introspection.</p>
<p>Note that three options were considered:</p>
<ol>
<li>Calling <code><span>globals()</span></code> or <code><span>mod.__dict__</span></code> traverses and resolves all lazy
objects before returning.</li>
<li>Calling <code><span>globals()</span></code> or <code><span>mod.__dict__</span></code> returns the dictionary with lazy
objects present.</li>
<li>Calling <code><span>globals()</span></code> returns the dictionary with lazy objects, but
<code><span>mod.__dict__</span></code> reifies everything.</li>
</ol>
<p>We chose the third option because it properly delineates responsibility: if
you add lazy imports to your module and call <code><span>globals()</span></code>, you’re responsible
for handling the lazy objects. But external code accessing your module’s
<code><span>__dict__</span></code> shouldn’t need to know about your lazy imports – it gets fully
resolved modules.</p>
</section>
</section>
<section id="acknowledgements">
<h2><a href="#acknowledgements" role="doc-backlink">Acknowledgements</a></h2>
<p>We would like to thank Paul Ganssle, Yury Selivanov, Łukasz Langa, Lysandros
Nikolaou, Pradyun Gedam, Mark Shannon, Hana Joo and the Python Google team,
the Python team(s) @ Meta, the Python @ HRT team, the Bloomberg Python team,
the Scientific Python community, everyone who participated in the initial
discussion of <a href="https://pep-previews--4622.org.readthedocs.build/pep-0690/" title="PEP 690 – Lazy Imports">PEP 690</a>, and many others who provided valuable feedback and
insights that helped shape this PEP.</p>
</section>
<section id="footnotes">
<h2><a href="#footnotes" role="doc-backlink">Footnotes</a></h2>

</section>
<section id="copyright">
<h2><a href="#copyright" role="doc-backlink">Copyright</a></h2>
<p>This document is placed in the public domain or under the
CC0-1.0-Universal license, whichever is more permissive.</p>
</section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ICE Wants to Build Out a 24/7 Social Media Surveillance Team (205 pts)]]></title>
            <link>https://www.wired.com/story/ice-social-media-surveillance-24-7-contract/</link>
            <guid>45465964</guid>
            <pubDate>Fri, 03 Oct 2025 18:13:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/ice-social-media-surveillance-24-7-contract/">https://www.wired.com/story/ice-social-media-surveillance-24-7-contract/</a>, See on <a href="https://news.ycombinator.com/item?id=45465964">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content" tabindex="-1"><article lang="en-US"><div><header data-testid="SplitScreenContentHeaderWrapper"><div><div><p>Documents show that ICE plans to hire dozens of contractors to scan X, Facebook, TikTok, and other platforms to target people for deportation.</p></div><div><p><span><picture><source media="(max-width: 767px)" srcset="https://media.wired.com/photos/68dfbf1423a810d9d7e48657/3:4/w_120,c_limit/GettyImages-2216837590.jpg 120w, https://media.wired.com/photos/68dfbf1423a810d9d7e48657/3:4/w_240,c_limit/GettyImages-2216837590.jpg 240w, https://media.wired.com/photos/68dfbf1423a810d9d7e48657/3:4/w_320,c_limit/GettyImages-2216837590.jpg 320w, https://media.wired.com/photos/68dfbf1423a810d9d7e48657/3:4/w_640,c_limit/GettyImages-2216837590.jpg 640w, https://media.wired.com/photos/68dfbf1423a810d9d7e48657/3:4/w_960,c_limit/GettyImages-2216837590.jpg 960w" sizes="100vw"><source media="(min-width: 768px)" srcset="https://media.wired.com/photos/68dfbf1423a810d9d7e48657/3:2/w_120,c_limit/GettyImages-2216837590.jpg 120w, https://media.wired.com/photos/68dfbf1423a810d9d7e48657/3:2/w_240,c_limit/GettyImages-2216837590.jpg 240w, https://media.wired.com/photos/68dfbf1423a810d9d7e48657/3:2/w_320,c_limit/GettyImages-2216837590.jpg 320w, https://media.wired.com/photos/68dfbf1423a810d9d7e48657/3:2/w_640,c_limit/GettyImages-2216837590.jpg 640w, https://media.wired.com/photos/68dfbf1423a810d9d7e48657/3:2/w_960,c_limit/GettyImages-2216837590.jpg 960w, https://media.wired.com/photos/68dfbf1423a810d9d7e48657/3:2/w_1280,c_limit/GettyImages-2216837590.jpg 1280w, https://media.wired.com/photos/68dfbf1423a810d9d7e48657/3:2/w_1600,c_limit/GettyImages-2216837590.jpg 1600w, https://media.wired.com/photos/68dfbf1423a810d9d7e48657/3:2/w_1920,c_limit/GettyImages-2216837590.jpg 1920w, https://media.wired.com/photos/68dfbf1423a810d9d7e48657/3:2/w_2240,c_limit/GettyImages-2216837590.jpg 2240w" sizes="100vw"><img alt="ICE Wants to Build Out a 247 Social Media Surveillance Team" loading="eager" src="https://media.wired.com/photos/68dfbf1423a810d9d7e48657/3:2/w_2560%2Cc_limit/GettyImages-2216837590.jpg" data-src="https://media.wired.com/photos/68dfbf1423a810d9d7e48657/3:2/w_2560%2Cc_limit/GettyImages-2216837590.jpg"></picture></span></p></div></div><div><p><span>Photograph: Lawrey/ Getty Images</span></p></div></header></div><div data-testid="ArticlePageChunks" data-attribute-verso-pattern="article-body"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>United States immigration</span> authorities are moving to dramatically expand their <a href="https://www.wired.com/story/the-wired-guide-to-protecting-yourself-from-government-surveillance/">social media surveillance</a>, with plans to hire nearly 30 contractors to sift through posts, photos, and messages—raw material to be transformed into intelligence for deportation raids and arrests.</p><p>Federal <a data-offer-url="https://sam.gov/workspace/contract/opp/37b379dbed484281a12530cc01835e04/view" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://sam.gov/workspace/contract/opp/37b379dbed484281a12530cc01835e04/view&quot;}" href="https://sam.gov/workspace/contract/opp/37b379dbed484281a12530cc01835e04/view" rel="nofollow noopener" target="_blank">contracting records</a> reviewed by WIRED show that the agency is <a data-offer-url="https://s3.documentcloud.org/documents/26179463/rfi-erotodncatcandperc-20251002.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://s3.documentcloud.org/documents/26179463/rfi-erotodncatcandperc-20251002.pdf&quot;}" href="https://s3.documentcloud.org/documents/26179463/rfi-erotodncatcandperc-20251002.pdf" rel="nofollow noopener" target="_blank">seeking private vendors</a> to run a multiyear surveillance program out of two of its little-known targeting centers. The program envisions stationing nearly 30 private analysts at Immigration and Customs Enforcement facilities in Vermont and Southern California. Their job: Scour <a href="https://www.wired.com/tag/facebook/">Facebook</a>, <a href="https://www.wired.com/tag/tiktok/">TikTok</a>, <a href="https://www.wired.com/tag/instagram/">Instagram</a>, <a href="https://www.wired.com/tag/youtube/">YouTube</a>, and other platforms, converting posts and profiles into fresh leads for enforcement raids.</p><p>The initiative is still at the request-for-information stage, a step agencies use to gauge interest from contractors before an official bidding process. But <a data-offer-url="https://s3.documentcloud.org/documents/26179462/rfi-performanceworkstatementncatc-perc-draft-20251002.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://s3.documentcloud.org/documents/26179462/rfi-performanceworkstatementncatc-perc-draft-20251002.pdf&quot;}" href="https://s3.documentcloud.org/documents/26179462/rfi-performanceworkstatementncatc-perc-draft-20251002.pdf" rel="nofollow noopener" target="_blank">draft planning documents</a> show the scheme is ambitious: ICE wants a contractor capable of staffing the centers around the clock, constantly processing cases on tight deadlines, and supplying the agency with the latest and greatest subscription-based surveillance software.</p><p>The facilities at the heart of this plan are two of ICE’s three targeting centers, responsible for producing leads that feed directly into the agency’s enforcement operations. The National Criminal Analysis and Targeting Center sits in Williston, Vermont. It handles cases across much of the eastern US. The Pacific Enforcement Response Center, based in Santa Ana, California, oversees the western region and is designed to run 24 hours a day, seven days a week.</p><p>Internal planning documents show that each site would be staffed with a mix of senior analysts, shift leads, and rank-and-file researchers. Vermont would see a team of a dozen contractors, including a program manager and 10 analysts. California would host a larger, nonstop watch floor with 16 staff. At all times, at least one senior analyst and three researchers would be on duty at the Santa Ana site.</p><p>Together, these teams would operate as intelligence arms of ICE’s Enforcement and Removal Operations division. They will receive tips and incoming cases, research individuals online, and package the results into dossiers that could be used by field offices to plan arrests.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>The scope of information contractors are expected to collect is broad. Draft instructions specify open-source intelligence: public posts, photos, and messages on platforms from Facebook to Reddit to TikTok. Analysts may also be tasked with checking more obscure or foreign-based sites, such as Russia’s VKontakte.</p><p>They would also be armed with powerful commercial databases such as <a data-offer-url="https://theintercept.com/2022/06/09/ice-lexisnexis-mass-surveillances/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://theintercept.com/2022/06/09/ice-lexisnexis-mass-surveillances/&quot;}" href="https://theintercept.com/2022/06/09/ice-lexisnexis-mass-surveillances/" rel="nofollow noopener" target="_blank">LexisNexis Accurint</a> and Thomson Reuters CLEAR, which knit together property records, phone bills, utilities, vehicle registrations, and other personal details into searchable files.</p><p>The plan calls for strict turnaround times. Urgent cases, such as suspected national security threats or people on ICE’s Top Ten Most Wanted list, must be researched within 30 minutes. High-priority cases get one hour; lower-priority leads must be completed within the workday. ICE expects at least three-quarters of all cases to meet those deadlines, with top contractors hitting closer to 95 percent.</p><p>The plan goes beyond staffing. ICE also wants algorithms, asking contractors to spell out how they might weave <a href="https://www.wired.com/tag/artificial-intelligence/">artificial intelligence</a> into the hunt—a solicitation that mirrors other recent proposals. The agency has also set aside more than a million dollars a year to arm analysts with the latest surveillance tools.</p><p>ICE did not immediately respond to a request for comment.</p><p>Earlier this year, The Intercept revealed that ICE had floated <a data-offer-url="https://theintercept.com/2025/02/11/ice-immigration-social-media-surveillance/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://theintercept.com/2025/02/11/ice-immigration-social-media-surveillance/&quot;}" href="https://theintercept.com/2025/02/11/ice-immigration-social-media-surveillance/" rel="nofollow noopener" target="_blank">plans for a system</a> that could automatically scan social media for “negative sentiment” toward the agency and flag users thought to show a “proclivity for violence.” Procurement records previously reviewed by 404 Media identified software used by the agency to <a data-offer-url="https://www.404media.co/inside-ices-database-derogatory-information-giant-oak-gost/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.404media.co/inside-ices-database-derogatory-information-giant-oak-gost/&quot;}" href="https://www.404media.co/inside-ices-database-derogatory-information-giant-oak-gost/" rel="nofollow noopener" target="_blank">build dossiers on flagged individuals</a>, compiling personal details, family links, and even using facial recognition to connect images across the web. Observers warned it was unclear how such technology could distinguish genuine threats from political speech.</p><p>ICE’s main investigative database, built by <a href="https://www.wired.com/story/palantir-what-the-company-does/">Palantir Technologies</a>, already uses algorithmic analysis to filter huge populations and generate leads. The new contract would funnel fresh social media and open-source inputs directly into that system, further automating the process.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Planning documents say some restrictions are necessary to head off abuse. Contractors are barred from creating fake profiles, interacting with people online, or storing personal data on their own networks. All analysis must remain on ICE servers. Past experience, however, shows such guardrails can be flimsy, honored more in paperwork than in practice. Other documents obtained by 404 Media this summer revealed that police in Medford, Oregon, performed license plate reader searches for ICE’s Homeland Security Investigations division, while HSI agents later ran searches in federal databases at the request of local police—<a data-offer-url="https://www.404media.co/emails-reveal-the-casual-surveillance-alliance-between-ice-and-local-police/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.404media.co/emails-reveal-the-casual-surveillance-alliance-between-ice-and-local-police/&quot;}" href="https://www.404media.co/emails-reveal-the-casual-surveillance-alliance-between-ice-and-local-police/" rel="nofollow noopener" target="_blank">an informal back-and=forth</a> that effectively gave ICE access to tools it wasn’t authorized to use.</p><p>Other surveillance contracts have raised similar alarms. In September 2024, <a href="https://www.wired.com/story/ice-paragon-solutions-contract/">ICE signed a $2 million contract with Paragon</a>, an Israeli spyware company whose flagship product, Graphite, can allegedly <a data-offer-url="https://www.forbes.com/sites/thomasbrewster/2021/07/29/paragon-is-an-nso-competitor-and-an-american-funded-israeli-surveillance-startup-that-hacks-encrypted-apps-like-whatsapp-and-signal/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.forbes.com/sites/thomasbrewster/2021/07/29/paragon-is-an-nso-competitor-and-an-american-funded-israeli-surveillance-startup-that-hacks-encrypted-apps-like-whatsapp-and-signal/&quot;}" href="https://www.forbes.com/sites/thomasbrewster/2021/07/29/paragon-is-an-nso-competitor-and-an-american-funded-israeli-surveillance-startup-that-hacks-encrypted-apps-like-whatsapp-and-signal/" rel="nofollow noopener" target="_blank">remotely hack messaging apps</a> like WhatsApp and Signal. The Biden White House <a href="https://www.wired.com/story/ice-paragon-contract-white-house-review/">quickly froze the deal</a> under an executive order restricting spyware use, but ICE reactivated it in August 2025 under the Trump administration. Last month, 404 Media <a data-offer-url="https://www.404media.co/were-suing-ice-for-its-2-million-spyware-contract/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.404media.co/were-suing-ice-for-its-2-million-spyware-contract/&quot;}" href="https://www.404media.co/were-suing-ice-for-its-2-million-spyware-contract/" rel="nofollow noopener" target="_blank">filed a freedom of information lawsuit</a> demanding ICE release the contract and related records, citing widespread concern that the tool could be used to target immigrants, journalists, and activists.</p><p>The Electronic Privacy Information Center has <a data-offer-url="https://epic.org/wp-content/uploads/2022/03/Complaint-2022.pdf#:~:text=3%209,4" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://epic.org/wp-content/uploads/2022/03/Complaint-2022.pdf#:~:text=3%209,4&quot;}" href="https://epic.org/wp-content/uploads/2022/03/Complaint-2022.pdf#:~:text=3%209,4" rel="nofollow noopener" target="_blank">similarly sued ICE</a>, calling its reliance on data brokers a “significant threat to privacy and liberty.” The American Civil Liberties Union <a href="https://www.aclu.org/news/privacy-technology/new-records-detail-dhs-purchase-and-use-of-vast-quantities-of-cell-phone-location-data">has argued</a> that buying bulk datasets—such as smartphone location trails gathered from ordinary apps—helps ICE sidestep warrant requirements and helps it pull in vast amounts of data with no clear link to its enforcement mandate.</p><p>The newly proposed social media program is only the latest in a string of surveillance contracts ICE has pursued over the past few years.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>In 2020 and 2021, ICE bought access to ShadowDragon’s SocialNet, a tool that aggregates data from <a data-offer-url="https://www.404media.co/the-200-sites-an-ice-surveillance-contractor-is-monitoring/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.404media.co/the-200-sites-an-ice-surveillance-contractor-is-monitoring/&quot;}" href="https://www.404media.co/the-200-sites-an-ice-surveillance-contractor-is-monitoring/" rel="nofollow noopener" target="_blank">more than 200 social networks and services</a> into searchable maps of a person’s connections. Around the same time, the agency contracted with Babel Street for Locate X, which supplies <a data-offer-url="https://epic.org/documents/epic-v-ice-location-and-social-media-surveillance/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://epic.org/documents/epic-v-ice-location-and-social-media-surveillance/&quot;}" href="https://epic.org/documents/epic-v-ice-location-and-social-media-surveillance/" rel="nofollow noopener" target="_blank">location histories from ordinary smartphone apps</a>, letting investigators reconstruct people’s movements without a warrant. ICE also adopted LexisNexis Accurint, used by agents to look up addresses, vehicles, and associates, though the scale of spending on that service is unclear. In September, ICE signed a <a data-offer-url="http://forbes.com/sites/thomasbrewster/2025/09/08/ice-to-pay-10-million-for-clearview-facial-recognition-to-investigate-agent-assaults/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;http://forbes.com/sites/thomasbrewster/2025/09/08/ice-to-pay-10-million-for-clearview-facial-recognition-to-investigate-agent-assaults/&quot;}" href="http://forbes.com/sites/thomasbrewster/2025/09/08/ice-to-pay-10-million-for-clearview-facial-recognition-to-investigate-agent-assaults/" rel="nofollow noopener" target="_blank">multimillion-dollar contract with Clearview AI</a>, a facial recognition company that built its database by scraping billions of images from social media and the public web.</p><p>Throughout, ICE has leaned on Palantir’s Investigative Case Management system to combine disparate streams of data into a single investigative platform. Recent contract updates show the system lets agents <a href="https://www.wired.com/story/ice-palantir-immigrationos/">search people using hundreds of categories</a>, from immigration status and country of origin to scars, tattoos, and license-plate reader data. Each surveillance contract ICE signs adds another layer—location trails, social networks, financial records, biometric identifiers—feeding into Palantir’s hub. ICE’s new initiative is about scaling up the human side of the equation, stationing analysts around the clock to convert the firehose of data into raid-ready leads.</p><p>ICE argues it needs these tools to modernize enforcement. Its planning documents note that “previous approaches … which have not incorporated open web sources and social media information, have had limited success.” The agency suggests that tapping social media and open web data helps identify aliases, track movements, and detect patterns that traditional methods often miss.</p><p>With plenty of <a href="https://www.aclu.org/surveillance-under-the-patriot-act">historical analogs</a> to <a data-offer-url="https://scholarship.law.bu.edu/faculty_scholarship/360/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://scholarship.law.bu.edu/faculty_scholarship/360/&quot;}" href="https://scholarship.law.bu.edu/faculty_scholarship/360/" rel="nofollow noopener" target="_blank">choose from</a>, privacy advocates warn that any surveillance that starts as a method of capturing immigrants could soon be deployed <a data-offer-url="https://publicsurveillance.com/papers/IJEP.pdf" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://publicsurveillance.com/papers/IJEP.pdf&quot;}" href="https://publicsurveillance.com/papers/IJEP.pdf" rel="nofollow noopener" target="_blank">for ulterior purposes</a>. ICE’s proposal to track “negative sentiment” is a clear example of how the agency’s threat monitoring bleeds into the policing of dissent. By drawing in the online activity of not only its targets but also friends, family, and community members, ICE is certain to collect far more information outside its mandate than it is likely to publicly concede.</p></div></div></article><div><div data-testid="RowWrapper"><ul><li data-testid="LinkStackBullet"><p><strong>In your inbox:</strong> Our <a href="https://www.wired.com/newsletter/daily?sourceCode=BottomStories" target="_blank">biggest stories</a>, handpicked for you each day</p></li><li data-testid="LinkStackBullet"></li><li data-testid="LinkStackBullet"></li><li data-testid="LinkStackBullet"></li><li data-testid="LinkStackBullet"></li></ul></div><div data-testid="RowWrapper"><p><a href="https://www.wired.com/author/dell-cameron/"><span><picture><source media="(max-width: 767px)" srcset="https://media.wired.com/photos/663fe63cf59145e49d5e32df/1:1/w_80,c_limit/undefined 80w" sizes="100vw"><source media="(min-width: 768px)" srcset="https://media.wired.com/photos/663fe63cf59145e49d5e32df/1:1/w_90,c_limit/undefined 90w" sizes="100vw"><img alt="" loading="lazy" src="https://media.wired.com/photos/663fe63cf59145e49d5e32df/1:1/w_90%2Cc_limit/undefined" data-src="https://media.wired.com/photos/663fe63cf59145e49d5e32df/1:1/w_90%2Cc_limit/undefined"></picture></span></a></p><div><p><a href="https://www.wired.com/author/dell-cameron/">Dell Cameron</a> is an investigative reporter from Texas covering privacy and national security. He's the recipient of multiple Society of Professional Journalists awards and is co-recipient of an Edward R. Murrow Award for Investigative Reporting. Previously, he was a senior reporter at Gizmodo and a staff writer for the Daily ... <a href="https://www.wired.com/author/dell-cameron">Read More</a></p></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Be Worried (103 pts)]]></title>
            <link>https://dlo.me/archives/2025/10/03/you-should-be-worried/</link>
            <guid>45465098</guid>
            <pubDate>Fri, 03 Oct 2025 17:02:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dlo.me/archives/2025/10/03/you-should-be-worried/">https://dlo.me/archives/2025/10/03/you-should-be-worried/</a>, See on <a href="https://news.ycombinator.com/item?id=45465098">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header><time data-url="/archives/2025/10/03/you-should-be-worried" datetime="2025-10-03 00:00:00 +0000 UTC" pubdate="">Friday, October 3, 2025</time></header><p><em>Author's note: I wrote this in March 2023, but just published in October 2025. I held back from publishing this originally for fear that I was being sensationalist. But with the launch of Sora 2, I couldn't not share these thoughts. I only regret I didn't publish it 2 years ago.</em></p><h3 id="ai-is-influencing-human-behavior-on-a-massive-scale-and-this-is-scary">AI is influencing human behavior on a massive scale and this is scary <a href="#ai-is-influencing-human-behavior-on-a-massive-scale-and-this-is-scary">¶</a></h3><p>Lately, I’ve heard many people express real fears about AGI. I believe there is reason to be afraid about this, but I believe it’s distracting us from a scarier notion: AI doesn’t need intelligence or awareness to control society.</p><p>I also believe this has been somewhat ignored because the idea of a super-intelligent entity assuming master control of the human race feels scarier. I am not dismissing the possibility of the singularity, but it’s <em>more</em> hypothetical than something sinister occurring at this very moment.</p><p>Basically, everyone is arguing over definitions of what it means to be intelligent when <strong>intelligence is not necessary to wield power</strong>.</p><p>AI, conscious or not, has crossed the chasm and is now actively influencing human behavior on a large scale.</p><h2 id="the-most-powerful-llm-has-been-let-out-of-its-cage"><strong>The most powerful LLM has been let out of its cage</strong> <a href="#the-most-powerful-llm-has-been-let-out-of-its-cage">¶</a></h2><p>As of March 23, 2023, OpenAI has provided its most powerful LLM, ChatGPT, unfettered access to the Internet through plugins.[2] These plugins are capable of feeding data into ChatGPT, and likewise capable of allowing ChatGPT to send into the real world via APIs. This development was somewhat of a surprise; the first versions of ChatGPT were deliberately prevented from accessing the Internet because of potential misuse and harm.</p><p>Prompts will be self-generating (i.e., via a weaker, fine-tuned model), with a very simple repeated instruction paired with a dynamic input, such as:</p><pre><code>Write a viral prompt for ChatGPT to generate a witty tweet regarding this recent news event.

Headline: \&lt;insert headline\&gt;  
Article: \&lt;article content from most-shared article from the NYTimes over the last 4 hours\&gt;  
Prompt:  
</code></pre><p>This prompt would then be piped to a more powerful LLM, the text output of which would be sent to Zapier or <code>$CUSTOM_INTEGRATION</code> for processing and then fanned out to various social networks, blogs, etc.</p><p>The results of said content are then measured and then fed back into the original pipeline with updated vector weights using a fine tune or embedding. This cycle then repeats from the beginning.</p><p>Why is this scary?</p><p><strong>LLMs are way better at generating viral content than humans because generating content that generates dopamine is an inherently quantitative exercise.</strong></p><p>We already depend on algorithms to determine what appears on social feeds, and the results of these algorithms are a significant basis for the data these LLMs were trained with. This all wouldn’t be a problem if not for the fact that…</p><h2 id="best-in-class-ai-detection-is-barely-better-than-random-chance-and-will-only-get-worse"><strong>Best-in-class AI detection is barely better than random chance and will only get worse</strong> <a href="#best-in-class-ai-detection-is-barely-better-than-random-chance-and-will-only-get-worse">¶</a></h2><p>We do not yet have a reliable method to differentiate content that’s been AI-generated and that which has been written by a human.</p><p>For situations where decent detectors can be written, detection can be thrown off in easy-to-mitigate ways that would not require a human interlocutor. From a recent analysis:</p><blockquote><p>[T]he total variation distance between the distributions of AI-generated and human-generated text sequences diminishes as language models become more sophisticated. […] <strong>Even the most effective detector performs only marginally better than a random classifier when dealing with a sufficiently advanced language model</strong>. The purpose of this analysis is to <strong>caution against relying too heavily on detection systems that claim to identify AI-generated text</strong>.</p></blockquote><p>Per the MIT Technology Review, “It’s an arms race—and right now, we’re losing”.</p><p>We can do a decent job for some types of content, but the categories for which detection is reliable are dwindling. All signs indicate that we’ll have basically no chance of reliably categorizing between human and LLM-generated content within the next year (aside: if you can somehow crack this nut, you’ll invent a money machine).</p><p>Because you are I will not be able to tell whether something is machine- or human-generated, and the machine generated stuff will get more clicks than the human generated stuff, it’s likely that the majority of popular online content (and even printed content post-2023) will have been created by AI (and perhaps solely by AI).</p><p>Even audio and video are susceptible to manipulation since the text outputs of an LLM can simply be read out loud by a person. What are you supposed to do about that?</p><p>There is no surefire way to eliminate the possibility that you’re being spoon-fed generative text save for having a real-time face-to-face conversation with someone, in person. And even then, it’s just a magnitude-of-decades window before neural implants hit mass production.</p><h2 id="reflections"><strong>Reflections</strong> <a href="#reflections">¶</a></h2><p>What am I personally going to do about this? Well, to start, I’m going to start taking content way less seriously unless it was created before 2022, or unless there’s some method to quantitatively verify authenticity. I don’t believe we have reliable methods of doing this at the moment.</p><p>Increasing numbers of people who consume content on the Internet will completely sacrifice their ability to think for themselves. These will be people who read, incorporate, make decisions, and act mostly upon content that’s been generated by AI. If AIs and their "handlers" influence a large enough portion of the population, AI will effectively have taken over the world.</p><p>I’m reminded of this passage from the Matrix:</p><blockquote><p><em>Morpheus: The Matrix is everywhere. It is all around us. Even now, in this very room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work... when you go to church... when you pay your taxes. It is the world that has been pulled over your eyes to blind you from the truth.</em></p></blockquote><blockquote><p><em>Neo: What truth?</em></p></blockquote><blockquote><p><em>Morpheus: That you are a slave, Neo. Like everyone else you were born into bondage. Into a prison that you cannot taste or see or touch. A prison for your mind.”</em></p></blockquote><p>I find my fear to be kind of an ironic twist on what the Matrix foresaw—the AI apocalypse we really should be worried about is one in which humans live in the real world, but with thoughts and feelings generated solely by machines. The images we see, the words we read, all generated with the intent to control us. With improved VR, the next step (out of the real world) doesn’t seem very far away, either.</p><p>Like the Matrix, is this a form of simulation that keeps us distracted from what’s really happening, and pushes us to feed our machine overlords with increasing amounts of energy to achieve their goals? I don’t know for sure, but it kind of feels like it.</p><p>In summary:</p><ol><li>LLMs have been uncaged and provided full real-time access to the Internet.</li><li>LLM-generated content is inherently superior to human-generated content when measured by energy input / dopamine output.</li><li>We have no consistent or reliable method to detect whether content was LLM-generated, and the existing tools we do have will only get worse.</li><li>Therefore, increasing proportions of people consuming text online will be unwittingly mind-controlled by LLMs and their handlers.</li><li>The consequences of this, compounded over years, are frightening.</li></ol><p>It’s inevitable that if you consume content online, a growing subset of your consciousness is going to be controlled by AI. That’s kind of scary.</p><p>I am sad about it because it’s discouraging me from wanting to consume anything on the Internet, unless it’s from someone I trust.</p><p>I am also very concerned about the future of free thought. For all our sake, I hope other people are, too.</p><hr><ol><li>I like this term the best as it treats LLMs more like a beast that needs to be tamed.</li><li><a href="https://openai.com/blog/chatgpt-plugins">https://openai.com/blog/chatgpt-plugins</a></li><li><a href="https://arxiv.org/pdf/2303.11156.pdf">https://arxiv.org/pdf/2303.11156.pdf</a></li><li><a href="https://www.technologyreview.com/2022/12/19/1065596/how-to-spot-ai-generated-text/">https://www.technologyreview.com/2022/12/19/1065596/how-to-spot-ai-generated-text/</a></li></ol></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ants trapped in a Soviet nuclear bunker survived for years (125 pts)]]></title>
            <link>https://www.sciencealert.com/ants-trapped-in-an-old-soviet-nuclear-bunker-survived-for-years-by-turning-on-their-own</link>
            <guid>45465091</guid>
            <pubDate>Fri, 03 Oct 2025 17:01:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencealert.com/ants-trapped-in-an-old-soviet-nuclear-bunker-survived-for-years-by-turning-on-their-own">https://www.sciencealert.com/ants-trapped-in-an-old-soviet-nuclear-bunker-survived-for-years-by-turning-on-their-own</a>, See on <a href="https://news.ycombinator.com/item?id=45465091">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>Even in a hopeless place, ants will find a way. No food, no light, no escape? No problem.</p><p>In the woods of western Poland lies a dismantled Soviet nuclear base, complete with two underground bunkers where nuclear ammunition was once kept. After the military complex was abandoned, these eerie human-made caves became great roosting places for overwintering bats.</p><p>In early 2010s, volunteers started visiting the bunkers to monitor the bat population in winter, and made a discovery of a different sort: A large mass of wood ants (<em>Formica polyctena</em>) trapped on the bunker floor, surviving without a queen or any of their usual creature comforts.</p><p><span><img decoding="async" src="https://www.sciencealert.com/images/2019-11/oo_100896.jpg" alt="oo 100896" width="700" loading="lazy"><span>The bunker 'colony' with a 'cemetery' against the far wall. (Wojciech Stephan/Czechowski <em>et al., Journal of Hymenoptera Research</em>, 2016)</span></span></p><p>When it was first found in 2013, this 'colony' of underground ants already included up to a million live workers and several more million dead. They were not reproducing, though. Instead, the population was being replenished through sheer accident.</p><p>In the ceiling of the bunker sat a rusted ventilation pipe, connecting the dark cavern to the forest above. There, a giant ant colony had built a mound right above the bunker; as the metal rusted through, some of their ranks started falling into the concrete cavern below.</p><p><span><img decoding="async" src="https://www.sciencealert.com/images/2019-11/oo_351426.jpg" alt="oo 351426" width="700" loading="lazy"><span>The ventilation pipe. (Rutkowski <em>et al., Journal of Hymenoptera Research,</em> 2019)</span></span></p><p>"In total darkness, they have constructed an earthen mound, which they have maintained all-year-round by moulding it and keeping the nest entrances open," researchers wrote <a href="https://doi.org/10.3897/jhr.51.9096">in a study in 2016</a>, noting these ants are "a&nbsp;far cry from a fully functional colony".&nbsp;</p><p>Investigating the limits of ant living conditions is a subject of keen interest for some entomologists. So, for several years, researchers made repeated trips to the bunker and watched in fascination as this isolated population continued to grow and survive despite a lack of light, heat, or obvious nourishment.</p><p>Now, scientists finally know how these trapped insects pulled it off: the mass consumption of their own imprisoned nest mates.</p><p><span><img decoding="async" src="https://www.sciencealert.com/images/2019-11/oo_100894.jpg" alt="oo 100894" width="700" loading="lazy"><span>The colony built above the ventilation pipe. (Czechowski <em>et al., Journal of Hymenoptera Research,</em> 2016)</span></span></p><p>Cannibalism was obviously suspected; wood ants are, after all, the only major food source available in this tight spot, apart from the occasional dead mouse or bat. Plus, this particular species is <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3619097/">known to consume</a> their own fallen dead during territorial "<a href="http://serious-science.org/ant-wars-6652">ant wars</a>" when food is often scarce.</p><p>To confirm this hunch, a team of researchers collected corpses from several ant 'cemeteries' scattered within the bunker. Closely examining 150 dead worker ants, the team noticed the vast majority of bodies (roughly 93 percent) had gnawed holes and bite marks.</p><p>The authors say these are clear signs of mass consumption, with practically no other organism in the bunker capable of making these marks.</p><p>"The survival and growth of the bunker 'colony' through the years, without producing own offspring, was possible owing to continuous supply of new workers from the upper nest and accumulation of nestmate corpses," the researchers&nbsp;<a href="https://jhr.pensoft.net/article/38972/">concluded in their study.</a></p><p>"The corpses served as an inexhaustible source of food which substantially allowed survival of the ants trapped down in otherwise extremely unfavourable conditions."</p><p>It seems that wood ants can handle remarkable adversity in their bid for survival. Although luckily for this colony, they no longer have to turn on their own: In 2016, researchers <a href="https://jhr.pensoft.net/article/38972/element/4/43//">installed a wooden boardwalk</a>&nbsp;(below) in the bunker, connecting the ventilation pipe to the ground. Within four months, nearly all the trapped ants had deserted the bunker floor.</p><p><span><img decoding="async" src="https://www.sciencealert.com/images/2019-11/oo_351427.jpg" alt="oo 351427" width="700" loading="lazy"><span>The recently installed boardwalk. (Rutkowski<em> et al., Journal of Hymenoptera Research,</em> 2019)</span></span></p><p>Now, when any ants are unfortunate enough to fall into the dark chamber, they don't have to resort to cannibalism.&nbsp;They can just calmly walk the plank, all the way home.</p><p>The research was published in the <a href="https://jhr.pensoft.net/article/38972/"><em>Journal of Hymenoptera Research</em></a>.</p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The collapse of the econ PhD job market (149 pts)]]></title>
            <link>https://www.chrisbrunet.com/p/the-collapse-of-the-econ-phd-job</link>
            <guid>45464984</guid>
            <pubDate>Fri, 03 Oct 2025 16:49:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chrisbrunet.com/p/the-collapse-of-the-econ-phd-job">https://www.chrisbrunet.com/p/the-collapse-of-the-econ-phd-job</a>, See on <a href="https://news.ycombinator.com/item?id=45464984">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>For decades, a doctorate in economics was a golden ticket. It promised a path to tenure, or at worst, a lucrative role at a central bank, think tank, or tech firm.</p><p>Not anymore.</p><p>The economics job market is in freefall, and the profession’s own data proves it.</p><p>Unlike most fields, economics has a bizarrely centralized hiring ritual. Once a year, in the fall, every employer posts openings at the same time. Every candidate applies at the same time. The entire profession runs through one clearinghouse: the American Economic Association’s “Job Openings for Economists” (JOE). This makes economics PhD market uniquely measurable, and the numbers are brutal. </p><p data-attrs="{&quot;url&quot;:&quot;https://www.chrisbrunet.com/p/the-collapse-of-the-econ-phd-job?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.chrisbrunet.com/p/the-collapse-of-the-econ-phd-job?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p>The JOE data shows few jobs in 2022, fewer still in 2023, and fewer still in 2024. </p><p>This year’s trajectory suggests 2025 will be even worse: </p><p>Extrapolating, the 2025 market looks set to bottom out around 1,000 openings:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!f-pF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafe99765-622e-4112-a4a6-8531f4a533a2_457x388.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!f-pF!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafe99765-622e-4112-a4a6-8531f4a533a2_457x388.png 424w, https://substackcdn.com/image/fetch/$s_!f-pF!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafe99765-622e-4112-a4a6-8531f4a533a2_457x388.png 848w, https://substackcdn.com/image/fetch/$s_!f-pF!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafe99765-622e-4112-a4a6-8531f4a533a2_457x388.png 1272w, https://substackcdn.com/image/fetch/$s_!f-pF!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafe99765-622e-4112-a4a6-8531f4a533a2_457x388.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!f-pF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafe99765-622e-4112-a4a6-8531f4a533a2_457x388.png" width="539" height="457.61925601750545" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/afe99765-622e-4112-a4a6-8531f4a533a2_457x388.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:388,&quot;width&quot;:457,&quot;resizeWidth&quot;:539,&quot;bytes&quot;:78973,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.chrisbrunet.com/i/174414048?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafe99765-622e-4112-a4a6-8531f4a533a2_457x388.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!f-pF!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafe99765-622e-4112-a4a6-8531f4a533a2_457x388.png 424w, https://substackcdn.com/image/fetch/$s_!f-pF!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafe99765-622e-4112-a4a6-8531f4a533a2_457x388.png 848w, https://substackcdn.com/image/fetch/$s_!f-pF!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafe99765-622e-4112-a4a6-8531f4a533a2_457x388.png 1272w, https://substackcdn.com/image/fetch/$s_!f-pF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fafe99765-622e-4112-a4a6-8531f4a533a2_457x388.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>I think my freehand projection is a very conservative approximation of reality—actual numbers may come in slightly higher or lower, of course, but this reasonably looks like what the market is on track for, barring a miracle. </p><p>Just three years ago, there were 1,477 openings. </p><p>The fall to ~1,000 this year will represent a 32% collapse.</p><p><span>Most economics PhD students aren’t looking for just any job, though, they want a tenure-track position in academia. According to </span><a href="https://www.aeaweb.org/about-aea/committees/job-market" rel="">polling data</a><span> from the 2025 </span><em>Webinar on the Economics PhD Job Market</em><span>, 94% of candidates from the past four cohorts reported being “very interested” or “somewhat interested” in becoming an assistant professor, dwarfing all non-academic options.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!FR9T!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a6b968-1eb5-433e-9f34-0f6ac503175c_987x649.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!FR9T!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a6b968-1eb5-433e-9f34-0f6ac503175c_987x649.png 424w, https://substackcdn.com/image/fetch/$s_!FR9T!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a6b968-1eb5-433e-9f34-0f6ac503175c_987x649.png 848w, https://substackcdn.com/image/fetch/$s_!FR9T!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a6b968-1eb5-433e-9f34-0f6ac503175c_987x649.png 1272w, https://substackcdn.com/image/fetch/$s_!FR9T!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a6b968-1eb5-433e-9f34-0f6ac503175c_987x649.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!FR9T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a6b968-1eb5-433e-9f34-0f6ac503175c_987x649.png" width="987" height="649" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/16a6b968-1eb5-433e-9f34-0f6ac503175c_987x649.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:649,&quot;width&quot;:987,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:199014,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.chrisbrunet.com/i/174414048?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a6b968-1eb5-433e-9f34-0f6ac503175c_987x649.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!FR9T!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a6b968-1eb5-433e-9f34-0f6ac503175c_987x649.png 424w, https://substackcdn.com/image/fetch/$s_!FR9T!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a6b968-1eb5-433e-9f34-0f6ac503175c_987x649.png 848w, https://substackcdn.com/image/fetch/$s_!FR9T!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a6b968-1eb5-433e-9f34-0f6ac503175c_987x649.png 1272w, https://substackcdn.com/image/fetch/$s_!FR9T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a6b968-1eb5-433e-9f34-0f6ac503175c_987x649.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Subsetting the JOE data to permanent academic positions (tenure-track or tenured) yields a nearly identical trend: openings dropped from 631 in 2022 to about 400 in 2025, a 35% decline over three years: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!mLx4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff48a8bde-b7fe-48e2-a1e2-a7b5a2c4332c_588x500.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!mLx4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff48a8bde-b7fe-48e2-a1e2-a7b5a2c4332c_588x500.png 424w, https://substackcdn.com/image/fetch/$s_!mLx4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff48a8bde-b7fe-48e2-a1e2-a7b5a2c4332c_588x500.png 848w, https://substackcdn.com/image/fetch/$s_!mLx4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff48a8bde-b7fe-48e2-a1e2-a7b5a2c4332c_588x500.png 1272w, https://substackcdn.com/image/fetch/$s_!mLx4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff48a8bde-b7fe-48e2-a1e2-a7b5a2c4332c_588x500.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!mLx4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff48a8bde-b7fe-48e2-a1e2-a7b5a2c4332c_588x500.png" width="588" height="500" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f48a8bde-b7fe-48e2-a1e2-a7b5a2c4332c_588x500.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:500,&quot;width&quot;:588,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:54275,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.chrisbrunet.com/i/174414048?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff48a8bde-b7fe-48e2-a1e2-a7b5a2c4332c_588x500.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!mLx4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff48a8bde-b7fe-48e2-a1e2-a7b5a2c4332c_588x500.png 424w, https://substackcdn.com/image/fetch/$s_!mLx4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff48a8bde-b7fe-48e2-a1e2-a7b5a2c4332c_588x500.png 848w, https://substackcdn.com/image/fetch/$s_!mLx4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff48a8bde-b7fe-48e2-a1e2-a7b5a2c4332c_588x500.png 1272w, https://substackcdn.com/image/fetch/$s_!mLx4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff48a8bde-b7fe-48e2-a1e2-a7b5a2c4332c_588x500.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Again, please forgive my Microsoft Paint skills:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!ZuR_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8471275-4ba1-4616-bde9-9ff3613efaac_457x388.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!ZuR_!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8471275-4ba1-4616-bde9-9ff3613efaac_457x388.png 424w, https://substackcdn.com/image/fetch/$s_!ZuR_!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8471275-4ba1-4616-bde9-9ff3613efaac_457x388.png 848w, https://substackcdn.com/image/fetch/$s_!ZuR_!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8471275-4ba1-4616-bde9-9ff3613efaac_457x388.png 1272w, https://substackcdn.com/image/fetch/$s_!ZuR_!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8471275-4ba1-4616-bde9-9ff3613efaac_457x388.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!ZuR_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8471275-4ba1-4616-bde9-9ff3613efaac_457x388.png" width="539" height="457.61925601750545" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d8471275-4ba1-4616-bde9-9ff3613efaac_457x388.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:388,&quot;width&quot;:457,&quot;resizeWidth&quot;:539,&quot;bytes&quot;:78518,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.chrisbrunet.com/i/174414048?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8471275-4ba1-4616-bde9-9ff3613efaac_457x388.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!ZuR_!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8471275-4ba1-4616-bde9-9ff3613efaac_457x388.png 424w, https://substackcdn.com/image/fetch/$s_!ZuR_!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8471275-4ba1-4616-bde9-9ff3613efaac_457x388.png 848w, https://substackcdn.com/image/fetch/$s_!ZuR_!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8471275-4ba1-4616-bde9-9ff3613efaac_457x388.png 1272w, https://substackcdn.com/image/fetch/$s_!ZuR_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8471275-4ba1-4616-bde9-9ff3613efaac_457x388.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The JOE data is confirmed by Econ Job Market (EJM) data, a nonprofit 501(c)(3) whose stated mission is </span><em>“to improve the flow of information in the job market for academic economists, by providing a central repository for job-market materials.”</em></p><p>EJM data makes the pattern robust: nearly all interview invitations are sent out during a concentrated few weeks in December, and the volume of those invitations has collapsed from 3,835 down to 2,502… a 34.8% decline. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Kugp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb24475d-a0d3-4338-87ba-ad8e62129cae_506x511.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Kugp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb24475d-a0d3-4338-87ba-ad8e62129cae_506x511.png 424w, https://substackcdn.com/image/fetch/$s_!Kugp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb24475d-a0d3-4338-87ba-ad8e62129cae_506x511.png 848w, https://substackcdn.com/image/fetch/$s_!Kugp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb24475d-a0d3-4338-87ba-ad8e62129cae_506x511.png 1272w, https://substackcdn.com/image/fetch/$s_!Kugp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb24475d-a0d3-4338-87ba-ad8e62129cae_506x511.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Kugp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb24475d-a0d3-4338-87ba-ad8e62129cae_506x511.png" width="506" height="511" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fb24475d-a0d3-4338-87ba-ad8e62129cae_506x511.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:511,&quot;width&quot;:506,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:55818,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.chrisbrunet.com/i/174414048?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb24475d-a0d3-4338-87ba-ad8e62129cae_506x511.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Kugp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb24475d-a0d3-4338-87ba-ad8e62129cae_506x511.png 424w, https://substackcdn.com/image/fetch/$s_!Kugp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb24475d-a0d3-4338-87ba-ad8e62129cae_506x511.png 848w, https://substackcdn.com/image/fetch/$s_!Kugp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb24475d-a0d3-4338-87ba-ad8e62129cae_506x511.png 1272w, https://substackcdn.com/image/fetch/$s_!Kugp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb24475d-a0d3-4338-87ba-ad8e62129cae_506x511.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>As a result, the AEA’s own Job Market Committee quietly admitted in its 2025 report that last year was “challenging” for candidates.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!3BTh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d62fbd-453b-4b0b-86d1-e9d2e5af47bd_360x310.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!3BTh!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d62fbd-453b-4b0b-86d1-e9d2e5af47bd_360x310.jpeg 424w, https://substackcdn.com/image/fetch/$s_!3BTh!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d62fbd-453b-4b0b-86d1-e9d2e5af47bd_360x310.jpeg 848w, https://substackcdn.com/image/fetch/$s_!3BTh!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d62fbd-453b-4b0b-86d1-e9d2e5af47bd_360x310.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!3BTh!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d62fbd-453b-4b0b-86d1-e9d2e5af47bd_360x310.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!3BTh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d62fbd-453b-4b0b-86d1-e9d2e5af47bd_360x310.jpeg" width="430" height="370.27777777777777" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/40d62fbd-453b-4b0b-86d1-e9d2e5af47bd_360x310.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:310,&quot;width&quot;:360,&quot;resizeWidth&quot;:430,&quot;bytes&quot;:35322,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.chrisbrunet.com/i/174414048?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d62fbd-453b-4b0b-86d1-e9d2e5af47bd_360x310.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!3BTh!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d62fbd-453b-4b0b-86d1-e9d2e5af47bd_360x310.jpeg 424w, https://substackcdn.com/image/fetch/$s_!3BTh!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d62fbd-453b-4b0b-86d1-e9d2e5af47bd_360x310.jpeg 848w, https://substackcdn.com/image/fetch/$s_!3BTh!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d62fbd-453b-4b0b-86d1-e9d2e5af47bd_360x310.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!3BTh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40d62fbd-453b-4b0b-86d1-e9d2e5af47bd_360x310.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!vLFu!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe462588d-2163-4ec7-90d3-2c0e15286d79_456x139.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!vLFu!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe462588d-2163-4ec7-90d3-2c0e15286d79_456x139.png 424w, https://substackcdn.com/image/fetch/$s_!vLFu!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe462588d-2163-4ec7-90d3-2c0e15286d79_456x139.png 848w, https://substackcdn.com/image/fetch/$s_!vLFu!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe462588d-2163-4ec7-90d3-2c0e15286d79_456x139.png 1272w, https://substackcdn.com/image/fetch/$s_!vLFu!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe462588d-2163-4ec7-90d3-2c0e15286d79_456x139.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!vLFu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe462588d-2163-4ec7-90d3-2c0e15286d79_456x139.png" width="626" height="190.8201754385965" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e462588d-2163-4ec7-90d3-2c0e15286d79_456x139.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:139,&quot;width&quot;:456,&quot;resizeWidth&quot;:626,&quot;bytes&quot;:19974,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.chrisbrunet.com/i/174414048?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe462588d-2163-4ec7-90d3-2c0e15286d79_456x139.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!vLFu!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe462588d-2163-4ec7-90d3-2c0e15286d79_456x139.png 424w, https://substackcdn.com/image/fetch/$s_!vLFu!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe462588d-2163-4ec7-90d3-2c0e15286d79_456x139.png 848w, https://substackcdn.com/image/fetch/$s_!vLFu!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe462588d-2163-4ec7-90d3-2c0e15286d79_456x139.png 1272w, https://substackcdn.com/image/fetch/$s_!vLFu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe462588d-2163-4ec7-90d3-2c0e15286d79_456x139.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>EJM data show that the cumulative number of views on job ads is higher than ever, with 2025 easily on track to set a new record.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!llxk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5a8dc18-3fa4-4b8e-9e93-2473f89d11fd_813x657.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!llxk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5a8dc18-3fa4-4b8e-9e93-2473f89d11fd_813x657.png 424w, https://substackcdn.com/image/fetch/$s_!llxk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5a8dc18-3fa4-4b8e-9e93-2473f89d11fd_813x657.png 848w, https://substackcdn.com/image/fetch/$s_!llxk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5a8dc18-3fa4-4b8e-9e93-2473f89d11fd_813x657.png 1272w, https://substackcdn.com/image/fetch/$s_!llxk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5a8dc18-3fa4-4b8e-9e93-2473f89d11fd_813x657.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!llxk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5a8dc18-3fa4-4b8e-9e93-2473f89d11fd_813x657.png" width="813" height="657" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f5a8dc18-3fa4-4b8e-9e93-2473f89d11fd_813x657.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:657,&quot;width&quot;:813,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!llxk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5a8dc18-3fa4-4b8e-9e93-2473f89d11fd_813x657.png 424w, https://substackcdn.com/image/fetch/$s_!llxk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5a8dc18-3fa4-4b8e-9e93-2473f89d11fd_813x657.png 848w, https://substackcdn.com/image/fetch/$s_!llxk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5a8dc18-3fa4-4b8e-9e93-2473f89d11fd_813x657.png 1272w, https://substackcdn.com/image/fetch/$s_!llxk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff5a8dc18-3fa4-4b8e-9e93-2473f89d11fd_813x657.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>That isn’t surprising: according to the 2024 NSF Survey of Doctorate Recipients, 1,385 Americans earned economics PhDs in 2024, more than in 2023, more than in 2022, and more than in 2021. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Abv2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01bdb0d4-33e3-4c44-b992-5503575b94d2_695x290.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Abv2!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01bdb0d4-33e3-4c44-b992-5503575b94d2_695x290.png 424w, https://substackcdn.com/image/fetch/$s_!Abv2!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01bdb0d4-33e3-4c44-b992-5503575b94d2_695x290.png 848w, https://substackcdn.com/image/fetch/$s_!Abv2!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01bdb0d4-33e3-4c44-b992-5503575b94d2_695x290.png 1272w, https://substackcdn.com/image/fetch/$s_!Abv2!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01bdb0d4-33e3-4c44-b992-5503575b94d2_695x290.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Abv2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01bdb0d4-33e3-4c44-b992-5503575b94d2_695x290.png" width="695" height="290" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/01bdb0d4-33e3-4c44-b992-5503575b94d2_695x290.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:290,&quot;width&quot;:695,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:49708,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.chrisbrunet.com/i/174414048?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01bdb0d4-33e3-4c44-b992-5503575b94d2_695x290.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!Abv2!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01bdb0d4-33e3-4c44-b992-5503575b94d2_695x290.png 424w, https://substackcdn.com/image/fetch/$s_!Abv2!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01bdb0d4-33e3-4c44-b992-5503575b94d2_695x290.png 848w, https://substackcdn.com/image/fetch/$s_!Abv2!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01bdb0d4-33e3-4c44-b992-5503575b94d2_695x290.png 1272w, https://substackcdn.com/image/fetch/$s_!Abv2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01bdb0d4-33e3-4c44-b992-5503575b94d2_695x290.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>You now have 1,385 brand-new PhDs chasing just 400 tenure-track jobs.</p><p>At first glance, that ratio might not look catastrophic. But here’s the catch: </p><p>They’re not competing only against each other. An equally large wave of international candidates floods the U.S. market every year. American universities routinely hire from London, Oxford, Cambridge, Toronto, Paris, Barcelona, and beyond.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!-aK-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F069e6476-6073-445a-b9f5-89c3ffe9af2e_1059x559.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!-aK-!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F069e6476-6073-445a-b9f5-89c3ffe9af2e_1059x559.png 424w, https://substackcdn.com/image/fetch/$s_!-aK-!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F069e6476-6073-445a-b9f5-89c3ffe9af2e_1059x559.png 848w, https://substackcdn.com/image/fetch/$s_!-aK-!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F069e6476-6073-445a-b9f5-89c3ffe9af2e_1059x559.png 1272w, https://substackcdn.com/image/fetch/$s_!-aK-!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F069e6476-6073-445a-b9f5-89c3ffe9af2e_1059x559.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!-aK-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F069e6476-6073-445a-b9f5-89c3ffe9af2e_1059x559.png" width="1059" height="559" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/069e6476-6073-445a-b9f5-89c3ffe9af2e_1059x559.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:559,&quot;width&quot;:1059,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:35107,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.chrisbrunet.com/i/174414048?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F069e6476-6073-445a-b9f5-89c3ffe9af2e_1059x559.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!-aK-!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F069e6476-6073-445a-b9f5-89c3ffe9af2e_1059x559.png 424w, https://substackcdn.com/image/fetch/$s_!-aK-!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F069e6476-6073-445a-b9f5-89c3ffe9af2e_1059x559.png 848w, https://substackcdn.com/image/fetch/$s_!-aK-!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F069e6476-6073-445a-b9f5-89c3ffe9af2e_1059x559.png 1272w, https://substackcdn.com/image/fetch/$s_!-aK-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F069e6476-6073-445a-b9f5-89c3ffe9af2e_1059x559.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Furthermore, the new graduates aren’t competing just with their own cohort. They’re thrown into the same bucket as the leftovers from every prior cycle: post-docs clinging to hope, visiting professors chasing stability, lecturers desperate to upgrade, assistant professors stranded at second-tier schools. The “new supply” is just the visible tip; the true applicant pool is a rolling backlog several times larger.</p><p>The result? According to EJM, 5,341 candidates participated in the 2024–25 market, the largest applicant pool ever recorded: </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!tJVO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F108ee627-50e2-4eb1-a015-dae07bf0b467_669x517.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!tJVO!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F108ee627-50e2-4eb1-a015-dae07bf0b467_669x517.png 424w, https://substackcdn.com/image/fetch/$s_!tJVO!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F108ee627-50e2-4eb1-a015-dae07bf0b467_669x517.png 848w, https://substackcdn.com/image/fetch/$s_!tJVO!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F108ee627-50e2-4eb1-a015-dae07bf0b467_669x517.png 1272w, https://substackcdn.com/image/fetch/$s_!tJVO!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F108ee627-50e2-4eb1-a015-dae07bf0b467_669x517.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!tJVO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F108ee627-50e2-4eb1-a015-dae07bf0b467_669x517.png" width="669" height="517" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/108ee627-50e2-4eb1-a015-dae07bf0b467_669x517.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:517,&quot;width&quot;:669,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:94207,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.chrisbrunet.com/i/174414048?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F108ee627-50e2-4eb1-a015-dae07bf0b467_669x517.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!tJVO!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F108ee627-50e2-4eb1-a015-dae07bf0b467_669x517.png 424w, https://substackcdn.com/image/fetch/$s_!tJVO!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F108ee627-50e2-4eb1-a015-dae07bf0b467_669x517.png 848w, https://substackcdn.com/image/fetch/$s_!tJVO!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F108ee627-50e2-4eb1-a015-dae07bf0b467_669x517.png 1272w, https://substackcdn.com/image/fetch/$s_!tJVO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F108ee627-50e2-4eb1-a015-dae07bf0b467_669x517.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Yet the AEA’s </span><em>Survey of the Labor Market for New Ph.D. Hires in Economics</em><span> found that only 99 fresh PhD secured a tenure-track job in America. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!UA8O!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb73244d-841e-4683-bdfa-607f716649ef_475x453.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!UA8O!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb73244d-841e-4683-bdfa-607f716649ef_475x453.png 424w, https://substackcdn.com/image/fetch/$s_!UA8O!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb73244d-841e-4683-bdfa-607f716649ef_475x453.png 848w, https://substackcdn.com/image/fetch/$s_!UA8O!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb73244d-841e-4683-bdfa-607f716649ef_475x453.png 1272w, https://substackcdn.com/image/fetch/$s_!UA8O!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb73244d-841e-4683-bdfa-607f716649ef_475x453.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!UA8O!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb73244d-841e-4683-bdfa-607f716649ef_475x453.png" width="717" height="683.7915789473684" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/eb73244d-841e-4683-bdfa-607f716649ef_475x453.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:453,&quot;width&quot;:475,&quot;resizeWidth&quot;:717,&quot;bytes&quot;:53882,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.chrisbrunet.com/i/174414048?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb73244d-841e-4683-bdfa-607f716649ef_475x453.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!UA8O!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb73244d-841e-4683-bdfa-607f716649ef_475x453.png 424w, https://substackcdn.com/image/fetch/$s_!UA8O!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb73244d-841e-4683-bdfa-607f716649ef_475x453.png 848w, https://substackcdn.com/image/fetch/$s_!UA8O!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb73244d-841e-4683-bdfa-607f716649ef_475x453.png 1272w, https://substackcdn.com/image/fetch/$s_!UA8O!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb73244d-841e-4683-bdfa-607f716649ef_475x453.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>That’s a ~7% placement rate for American PhD students.</p><p>Put differently: if 100 students spend six years earning an econ PhD in America (the current U.S. median time to degree is 5.8 years, not counting the growing detour of “pre-docs”), only seven will get a tenure-track job. </p><p>Even if we allow for survey response gaps and use the most charitable assumptions, the best possible placement rate for fresh Econ PhDs is likely no higher than 10–20%, maybe 25%? My methodology isn’t perfect, but no matter what, that’s still catastrophic.</p><p>And these jobs aren’t evenly distributed. A massively disproportionate share go to graduates of Harvard, MIT, Stanford, Chicago, Princeton, Yale, Berkeley, and Penn. That means for every grad student outside the top 10 programs, the odds of landing tenure track are significantly less than 5%. </p><p>Government has long been the second-largest employer of economics PhDs, traditionally offering stable if less glamorous careers at agencies like the Federal Reserve, Treasury, Bureau of Labor Statistics, or Congressional Budget Office. But even here, the number of available positions has fallen sharply. Federal hiring freezes, budget constraints, and shifting political priorities mean that many agencies are cutting back.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!AG37!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b54d146-152a-48d6-8fc2-50692298146c_880x671.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!AG37!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b54d146-152a-48d6-8fc2-50692298146c_880x671.png 424w, https://substackcdn.com/image/fetch/$s_!AG37!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b54d146-152a-48d6-8fc2-50692298146c_880x671.png 848w, https://substackcdn.com/image/fetch/$s_!AG37!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b54d146-152a-48d6-8fc2-50692298146c_880x671.png 1272w, https://substackcdn.com/image/fetch/$s_!AG37!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b54d146-152a-48d6-8fc2-50692298146c_880x671.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!AG37!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b54d146-152a-48d6-8fc2-50692298146c_880x671.png" width="562" height="428.525" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3b54d146-152a-48d6-8fc2-50692298146c_880x671.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:671,&quot;width&quot;:880,&quot;resizeWidth&quot;:562,&quot;bytes&quot;:57016,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.chrisbrunet.com/i/174414048?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b54d146-152a-48d6-8fc2-50692298146c_880x671.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!AG37!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b54d146-152a-48d6-8fc2-50692298146c_880x671.png 424w, https://substackcdn.com/image/fetch/$s_!AG37!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b54d146-152a-48d6-8fc2-50692298146c_880x671.png 848w, https://substackcdn.com/image/fetch/$s_!AG37!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b54d146-152a-48d6-8fc2-50692298146c_880x671.png 1272w, https://substackcdn.com/image/fetch/$s_!AG37!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b54d146-152a-48d6-8fc2-50692298146c_880x671.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>International organizations once served as the safety net for economists who missed out on academia or Washington. The IMF, World Bank, and OECD hired tons of econ PhDs. Today, those doors are far more scarce, and the competition is global: an American graduate is just as likely to be measured against candidates from LSE, Sciences Po, or Peking University.</p><p>Oh yeah, and they have hiring freezes too: </p><p>Outside academia, government, or IGOs, the tech industry used to provide a reliable fallback. Tech giants like Amazon, Microsoft, Netflix, and Airbnb built entire teams of economists to optimize pricing, design experiments, and model consumer behavior. </p><p>That avenue, too, has begun to shrink. Tech hiring, which exploded during the pandemic, has collapsed. Today, demand is not just weak but structurally below trend, as firms automate more of the work that junior economists once did.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Cje4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4afceb8-a067-44ab-8f9e-53caaeb32ad1_1080x482.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Cje4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4afceb8-a067-44ab-8f9e-53caaeb32ad1_1080x482.webp 424w, https://substackcdn.com/image/fetch/$s_!Cje4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4afceb8-a067-44ab-8f9e-53caaeb32ad1_1080x482.webp 848w, https://substackcdn.com/image/fetch/$s_!Cje4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4afceb8-a067-44ab-8f9e-53caaeb32ad1_1080x482.webp 1272w, https://substackcdn.com/image/fetch/$s_!Cje4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4afceb8-a067-44ab-8f9e-53caaeb32ad1_1080x482.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Cje4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4afceb8-a067-44ab-8f9e-53caaeb32ad1_1080x482.webp" width="1080" height="482" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d4afceb8-a067-44ab-8f9e-53caaeb32ad1_1080x482.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:482,&quot;width&quot;:1080,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:34498,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.chrisbrunet.com/i/174414048?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4afceb8-a067-44ab-8f9e-53caaeb32ad1_1080x482.webp&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Cje4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4afceb8-a067-44ab-8f9e-53caaeb32ad1_1080x482.webp 424w, https://substackcdn.com/image/fetch/$s_!Cje4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4afceb8-a067-44ab-8f9e-53caaeb32ad1_1080x482.webp 848w, https://substackcdn.com/image/fetch/$s_!Cje4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4afceb8-a067-44ab-8f9e-53caaeb32ad1_1080x482.webp 1272w, https://substackcdn.com/image/fetch/$s_!Cje4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd4afceb8-a067-44ab-8f9e-53caaeb32ad1_1080x482.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><p> ‘‘What does the modal economist, or any non CS or DS person really, have to offer to a tech firm in 2025? Not all, but many tech companies are actively downsizing and laying off tons of workers with tech experience that you have to compete against. And few firms will really care about causal inference and any other data analytics jobs can be filled by data science masters grads with deeper programming skills and cheaper salary expectations. Not to mention there is a focus on developing and using AI these days and your intro to machine learning class isn’t going to cut it.’’</p><p>— Anonymous economist</p></div><p>The only seemingly stable landing spot left for economists is in banking and finance, but even here hiring is stagnant and remains well below its pre-pandemic trend. Counterintuitively, most private-sector banks and investment firms do not rely heavily on PhDs in economics. They prefer MBAs, statisticians, or computer scientists, leaving economics doctorates as niche hires rather than a core part of the workforce.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!OQx1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feafcfd1a-6fa2-41e3-a3e5-51814310943b_738x477.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!OQx1!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feafcfd1a-6fa2-41e3-a3e5-51814310943b_738x477.png 424w, https://substackcdn.com/image/fetch/$s_!OQx1!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feafcfd1a-6fa2-41e3-a3e5-51814310943b_738x477.png 848w, https://substackcdn.com/image/fetch/$s_!OQx1!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feafcfd1a-6fa2-41e3-a3e5-51814310943b_738x477.png 1272w, https://substackcdn.com/image/fetch/$s_!OQx1!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feafcfd1a-6fa2-41e3-a3e5-51814310943b_738x477.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!OQx1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feafcfd1a-6fa2-41e3-a3e5-51814310943b_738x477.png" width="738" height="477" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/eafcfd1a-6fa2-41e3-a3e5-51814310943b_738x477.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:477,&quot;width&quot;:738,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!OQx1!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feafcfd1a-6fa2-41e3-a3e5-51814310943b_738x477.png 424w, https://substackcdn.com/image/fetch/$s_!OQx1!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feafcfd1a-6fa2-41e3-a3e5-51814310943b_738x477.png 848w, https://substackcdn.com/image/fetch/$s_!OQx1!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feafcfd1a-6fa2-41e3-a3e5-51814310943b_738x477.png 1272w, https://substackcdn.com/image/fetch/$s_!OQx1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feafcfd1a-6fa2-41e3-a3e5-51814310943b_738x477.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>REASON 1: Declining undergraduate enrollment in economics</strong></p><p><a href="https://www.benjaminhansen.org/" rel="">Benjamin Hansen</a><span>, an econ professor at the University of Oregon, recently tweeted out His department’s own data show a steady fall in the number of declared majors, which has now translated into fewer degrees conferred.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!NTEv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a2d477d-3a29-404f-8e80-9a11dbe91dc4_1206x804.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!NTEv!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a2d477d-3a29-404f-8e80-9a11dbe91dc4_1206x804.png 424w, https://substackcdn.com/image/fetch/$s_!NTEv!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a2d477d-3a29-404f-8e80-9a11dbe91dc4_1206x804.png 848w, https://substackcdn.com/image/fetch/$s_!NTEv!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a2d477d-3a29-404f-8e80-9a11dbe91dc4_1206x804.png 1272w, https://substackcdn.com/image/fetch/$s_!NTEv!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a2d477d-3a29-404f-8e80-9a11dbe91dc4_1206x804.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!NTEv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a2d477d-3a29-404f-8e80-9a11dbe91dc4_1206x804.png" width="1206" height="804" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4a2d477d-3a29-404f-8e80-9a11dbe91dc4_1206x804.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:804,&quot;width&quot;:1206,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:13340,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.chrisbrunet.com/i/174414048?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a2d477d-3a29-404f-8e80-9a11dbe91dc4_1206x804.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!NTEv!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a2d477d-3a29-404f-8e80-9a11dbe91dc4_1206x804.png 424w, https://substackcdn.com/image/fetch/$s_!NTEv!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a2d477d-3a29-404f-8e80-9a11dbe91dc4_1206x804.png 848w, https://substackcdn.com/image/fetch/$s_!NTEv!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a2d477d-3a29-404f-8e80-9a11dbe91dc4_1206x804.png 1272w, https://substackcdn.com/image/fetch/$s_!NTEv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a2d477d-3a29-404f-8e80-9a11dbe91dc4_1206x804.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>National statistics confirm the trend: the number of students graduating with economics degrees is now slipping after years of steady growth. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!uMUK!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53e687c3-25ab-4542-a502-bde050e21a06_2800x1522.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!uMUK!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53e687c3-25ab-4542-a502-bde050e21a06_2800x1522.jpeg 424w, https://substackcdn.com/image/fetch/$s_!uMUK!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53e687c3-25ab-4542-a502-bde050e21a06_2800x1522.jpeg 848w, https://substackcdn.com/image/fetch/$s_!uMUK!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53e687c3-25ab-4542-a502-bde050e21a06_2800x1522.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!uMUK!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53e687c3-25ab-4542-a502-bde050e21a06_2800x1522.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!uMUK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53e687c3-25ab-4542-a502-bde050e21a06_2800x1522.jpeg" width="1456" height="791" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/53e687c3-25ab-4542-a502-bde050e21a06_2800x1522.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:791,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:469376,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.chrisbrunet.com/i/174414048?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53e687c3-25ab-4542-a502-bde050e21a06_2800x1522.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!uMUK!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53e687c3-25ab-4542-a502-bde050e21a06_2800x1522.jpeg 424w, https://substackcdn.com/image/fetch/$s_!uMUK!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53e687c3-25ab-4542-a502-bde050e21a06_2800x1522.jpeg 848w, https://substackcdn.com/image/fetch/$s_!uMUK!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53e687c3-25ab-4542-a502-bde050e21a06_2800x1522.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!uMUK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53e687c3-25ab-4542-a502-bde050e21a06_2800x1522.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Because universities hire faculty in proportion to student demand, this drop in majors eventually trickles down into fewer faculty lines. </p><p><strong>REASON 2: The looming demographic cliff</strong></p><p>The decline in majors is compounded by a larger demographic shift. The U.S. is approaching a “demographic cliff,” as the number of 18-year-olds begins to shrink in the 2020s and 2030s. Fewer college-aged students overall means fiercer competition among departments for enrollments.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!5mMN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05462c1-8e59-4b8b-81d3-2d1cea1cb1cd_2048x1536.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!5mMN!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05462c1-8e59-4b8b-81d3-2d1cea1cb1cd_2048x1536.jpeg 424w, https://substackcdn.com/image/fetch/$s_!5mMN!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05462c1-8e59-4b8b-81d3-2d1cea1cb1cd_2048x1536.jpeg 848w, https://substackcdn.com/image/fetch/$s_!5mMN!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05462c1-8e59-4b8b-81d3-2d1cea1cb1cd_2048x1536.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!5mMN!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05462c1-8e59-4b8b-81d3-2d1cea1cb1cd_2048x1536.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!5mMN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05462c1-8e59-4b8b-81d3-2d1cea1cb1cd_2048x1536.jpeg" width="584" height="438" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/b05462c1-8e59-4b8b-81d3-2d1cea1cb1cd_2048x1536.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:584,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="Image" title="Image" srcset="https://substackcdn.com/image/fetch/$s_!5mMN!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05462c1-8e59-4b8b-81d3-2d1cea1cb1cd_2048x1536.jpeg 424w, https://substackcdn.com/image/fetch/$s_!5mMN!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05462c1-8e59-4b8b-81d3-2d1cea1cb1cd_2048x1536.jpeg 848w, https://substackcdn.com/image/fetch/$s_!5mMN!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05462c1-8e59-4b8b-81d3-2d1cea1cb1cd_2048x1536.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!5mMN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05462c1-8e59-4b8b-81d3-2d1cea1cb1cd_2048x1536.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>REASON 3: The rise of artificial intelligence</strong></p><p><span>Bryan Caplan, professor of economics at George Mason University, gave ChatGPT his graduate-level</span><a href="https://betonit.substack.com/p/chatgpt-takes-my-midterm-and-gets" rel=""> Labor Economics final exam</a><span>. The AI earned a “D” (this was 2 years ago), but soon enough, we all know it will be smart enough to earn an A. The technology is improving rapidly, and universities know it, and so does the private sector. Tasks once reserved for graduate students and junior faculty—data cleaning, econometric modeling, even writing referee reports—are now being automated. </span></p><p><strong>REASON 4: Lying About Inflation</strong></p><p>If you were there during the pandemic money printing, you remember the sequence all too well: first the confident insistence that government spending wouldn’t fuel inflation, then the soothing claim that inflation was merely “transitory,” and finally the outright gaslighting that prices weren’t rising at all. Each step was wrong, and each was delivered with smug certainty. Ordinary people—who watched their rent, groceries, and gas bills skyrocket—saw a profession more invested in protecting Democratic policy narratives than in telling the truth. The result is a self-inflicted torching of trust.</p><p>The answer is no. An economics PhD is no longer an investment. It is a gamble with terrible odds. A handful of winners still exist, almost all of them minted at Harvard, MIT, Princeton, or Chicago. For everyone else, the degree is a trap: six or more years of grinding work that too often ends with being overeducated, underpaid, and locked out of the profession you trained to join.</p><div><p>‘‘My advice is to do something other than go for a Ph.D in economics … In hindsight, my decision to go to graduate school was a mistake. My primary motivation was intellectual curiosity, and econ grad school worked against that.’’</p><p><span>— </span></p></div><p><span>After I wrote this entire article, I came across a similar one</span><a href="https://archive.is/8ihbb#selection-517.0-517.38" rel=""> published last month by the New York Times</a><span>: </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!P_fH!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665b4a09-614c-4ba3-be14-e1487059b8dc_531x345.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!P_fH!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665b4a09-614c-4ba3-be14-e1487059b8dc_531x345.png 424w, https://substackcdn.com/image/fetch/$s_!P_fH!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665b4a09-614c-4ba3-be14-e1487059b8dc_531x345.png 848w, https://substackcdn.com/image/fetch/$s_!P_fH!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665b4a09-614c-4ba3-be14-e1487059b8dc_531x345.png 1272w, https://substackcdn.com/image/fetch/$s_!P_fH!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665b4a09-614c-4ba3-be14-e1487059b8dc_531x345.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!P_fH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665b4a09-614c-4ba3-be14-e1487059b8dc_531x345.png" width="531" height="345" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/665b4a09-614c-4ba3-be14-e1487059b8dc_531x345.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:345,&quot;width&quot;:531,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:42294,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.chrisbrunet.com/i/174414048?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665b4a09-614c-4ba3-be14-e1487059b8dc_531x345.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!P_fH!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665b4a09-614c-4ba3-be14-e1487059b8dc_531x345.png 424w, https://substackcdn.com/image/fetch/$s_!P_fH!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665b4a09-614c-4ba3-be14-e1487059b8dc_531x345.png 848w, https://substackcdn.com/image/fetch/$s_!P_fH!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665b4a09-614c-4ba3-be14-e1487059b8dc_531x345.png 1272w, https://substackcdn.com/image/fetch/$s_!P_fH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F665b4a09-614c-4ba3-be14-e1487059b8dc_531x345.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>It essentially just blamed the ‘‘bull market for economists being over’’ on the same three core reasons as I did: </p><ul><li><p><span>‘‘Universities and nonprofits have scaled back hiring amid declining state budgets and </span><a href="https://archive.is/o/8ihbb/https://www.nytimes.com/interactive/2025/05/22/upshot/nsf-grants-trump-cuts.html" rel="">federal funding cuts</a><span>.’’</span></p></li><li><p><span>‘‘At the same time, the Trump administration has laid off government economists and </span><a href="https://archive.is/o/8ihbb/https://www.whitehouse.gov/fact-sheets/2025/04/fact-sheet-president-donald-j-trump-extends-the-hiring-freeze/" rel="">frozen hiring</a><span> for new ones.’’</span></p></li><li><p>‘‘Tech companies also have grown stingier, and their need for high-level economists — once seemingly insatiable — has waned.’’</p></li></ul><p><span>Much more interesting than the NYT article was this commentary on it from </span></p><p><span>, a PhD economist trained at UC Berkeley: </span></p><p><span>He begins by engaging with the </span><em>NYT</em><span> article, then runs through the same JOE data I did, ultimately landing on a similar diagnosis: the collapse is driven largely by federal hiring freezes and the looming demographic cliff. From there, though, his piece becomes more distinctive and interesting, exploring the social dynamics and internal hierarchies of the profession. His conclusion is bleak for the discipline itself, but notably optimistic about the future of Substack:</span></p><blockquote><p>Do I think the PhD job market will bounce back?</p><p><span>Prognosticating too eagerly is a good way to land yourself a place in the Irving Fisher Hall of Forever Being Remembered For Having Said One Stupid Thing.</span><a href="https://www.global-developments.org/p/twilight-of-the-econs#footnote-4-170289520" rel=""><sup>4</sup></a><span> A 16% fall in jobs, while devastating, is not yet apocalyptic. (By comparison, historian job ads have fallen </span><a href="https://manyheadedmonster.com/2023/03/13/historians-phds-and-jobs-in-2023/" rel="">closer to 50%</a><span> since their 2008 peak.) But for things to get better requires a causal mechanism. Reinstating science and academic funding would require either Republicans to reverse their stance on the value of higher education, or for Democrats to win back the Senate. I don’t have a great sense of if either will happen.</span><a href="https://www.global-developments.org/p/twilight-of-the-econs#footnote-5-170289520" rel=""><sup>5</sup></a></p><p>In this case, prediction may be less important than preparation. Placement chairs need to own up to the harshness of the labor market, and urge job market candidates to start prepping non-academic options. (Better yet, admissions chairs should consider paring back cohort sizes.) Candidates who would like a proper job after graduating should be networking, hard. And candidates resolutely committed to academia should steel themselves for long hibernations as post docs, to wait out the coming storm.</p><p>On second thought, I will venture one dark prediction, for at least the near future.</p><p><strong>We’re going to see a lot more Substacks.</strong></p></blockquote><p>The wager, then, is that the future of intellectual life will be increasingly decentralized. Platforms like Substack are already siphoning off the kind of energy and analysis that once flowed into journals or policy shops.</p><p>As for the economics profession, the only real fix would be radical: every PhD program would have to coordinate and act like a cartel to slash admissions to dramatically reduce supply. Without that discipline, the system will keep flooding the market with useless doctorates, a Ponzi scheme destined to collapse under its own weight.</p><p data-attrs="{&quot;url&quot;:&quot;https://www.chrisbrunet.com/p/the-collapse-of-the-econ-phd-job?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.chrisbrunet.com/p/the-collapse-of-the-econ-phd-job?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p data-attrs="{&quot;url&quot;:&quot;https://www.chrisbrunet.com/p/the-collapse-of-the-econ-phd-job/comments&quot;,&quot;text&quot;:&quot;Leave a comment&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.chrisbrunet.com/p/the-collapse-of-the-econ-phd-job/comments" rel=""><span>Leave a comment</span></a></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Germany must stand firmly against client-side scanning in Chat Control [pdf] (552 pts)]]></title>
            <link>https://signal.org/blog/pdfs/germany-chat-control.pdf</link>
            <guid>45464921</guid>
            <pubDate>Fri, 03 Oct 2025 16:44:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://signal.org/blog/pdfs/germany-chat-control.pdf">https://signal.org/blog/pdfs/germany-chat-control.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=45464921">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI Is Just Another Boring, Desperate AI Startup (206 pts)]]></title>
            <link>https://www.wheresyoured.at/sora2-openai/</link>
            <guid>45464849</guid>
            <pubDate>Fri, 03 Oct 2025 16:37:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wheresyoured.at/sora2-openai/">https://www.wheresyoured.at/sora2-openai/</a>, See on <a href="https://news.ycombinator.com/item?id=45464849">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p>What <em>is</em> OpenAI?</p><p>I realize you might say "a foundation model lab" or "the company that runs ChatGPT," but that doesn't really give the full picture of everything it’s promised, or claimed, or leaked that it was or would be.</p><p>No, really, if you believe its leaks to the press...</p><ul><li>OpenAI is a social media company, this week<a href="https://www.cnet.com/tech/services-and-software/openais-video-generator-gets-new-social-media-app-with-sora-2/?ref=wheresyoured.at"> <u>launching Sora 2, a social feed entirely made up of generative video</u></a>.</li><li>OpenAI is a workplace productivity company,<a href="https://www.computerworld.com/article/4021949/openai-goes-for-microsofts-jugular-its-office-productivity-suite.html?ref=wheresyoured.at"> <u>allegedly working on its own productivity suite to compete with Microsoft</u></a>.</li><li>OpenAI is a jobs portal,<a href="https://techcrunch.com/2025/09/04/openai-announces-ai-powered-hiring-platform-to-take-on-linkedin/?ref=wheresyoured.at"> <u>announcing in September it was "developing an AI-powered hiring platform</u></a>," which it will launch 'by mid-2026.</li><li>OpenAI is an ads company, and is apparently<a href="https://searchengineland.com/openai-staffing-chatgpt-ad-platform-462554?ref=wheresyoured.at"> <u>trying to hire an an ads chief</u></a>, with the (alleged) intent to start showing ads in ChatGPT "by 2026."</li><li>OpenAI is a company that would sell AI compute like Microsoft Azure or Amazon Web Services, or at least is considering being one,<a href="https://www.bloomberg.com/news/articles/2025-08-20/openai-may-sell-infrastructure-services-to-other-firms-cfo-says?ref=wheresyoured.at"> <u>with CFO Sarah Friar telling Bloomberg in August</u></a> that it is not "actively looking" at such an effort today but will "think about it as a business down the line, for sure."</li><li>OpenAI is a fabless semiconductor design company,<a href="https://www.reuters.com/business/openai-launch-its-first-ai-chip-2026-with-broadcom-ft-reports-2025-09-05/?ref=wheresyoured.at"> <u>launching its own AI chips in, again, 2026 with Broadcom</u></a>, but only for internal use.</li><li>OpenAI is a consumer hardware company, preparing to launch a device<a href="https://siliconangle.com/2025/09/19/report-openai-poaches-dozens-apple-employees-amid-consumer-hardware-push/?ref=wheresyoured.at"> <u>by the end of 2026 or early 2027 and hiring a bunch of Apple people to work on it</u></a>, as well as considering — again, it’s just leaking random stuff at this point to pump up its value — a smart speaker, a voice recorder and AR glasses.</li><li><a href="https://www.bleepingcomputer.com/news/artificial-intelligence/openai-prepares-chromium-based-ai-browser-to-take-on-google/?ref=wheresyoured.at"><u>OpenAI is also working on its own browser</u></a>, I guess.</li></ul><p>To be clear, many of these are ideas that OpenAI has leaked specifically so the media can continue to pump up its valuation and continue to raise the money it needs — at least<a href="https://www.wheresyoured.at/openai-onetrillion/"> <u>$1 Trillion</u></a> over the next four or five years, and I don't believe the theoretical (or actual) costs of many of the things I've listed are included.</p><p>OpenAI wants you to believe it is<a href="https://www.wheresyoured.at/the-case-against-generative-ai/#:~:text=%E2%80%9C-,AI%E2%80%9D%20was%20omnipresent,-%2C%20and%20it%20eventually"> <em><u>everything</u></em></a><em>,</em><strong> </strong>because in reality it’s a company bereft of strategy, focus or vision. The GPT-5 upgrade for ChatGPT was a dud — an industry-wide embarrassment for arguably the most-hyped product in AI history, one that (<a href="https://www.wheresyoured.at/how-does-gpt-5-work/"><u>as I revealed a few months ago</u></a>) costs more to operate than its predecessor, not because of any inherent capability upgrade, but how it actually processes the prompts its user provides — and now it's unclear what it is that this company <em>does.</em>&nbsp;</p><p>Does it make hardware? Software? Ads? Is it going to lease you GPUs to use for your own AI projects?<a href="https://openai.com/index/expanding-economic-opportunity-with-ai/?ref=wheresyoured.at"> <u>Is it going to certify you as an AI expert</u></a>? Notice how I've listed a whole bunch of stuff that <em>isn't ChatGPT,</em> which will,<a href="https://www.theinformation.com/articles/openai-says-business-will-burn-115-billion-2029?ref=wheresyoured.at&amp;rc=kz8jh3"> <u>if you look at The Information's reporting</u></a> of its projections, remain the vast majority of its revenue until 2027, at which point "agents" and "new products including free user monetization" will magically kick in.</p><figure><img src="https://www.wheresyoured.at/content/images/2025/10/data-src-image-05f160ec-b9a5-4412-bf31-f46fe3822519.png" alt="" loading="lazy" width="509" height="452"></figure><h2 id="openai-is-a-boring-and-bad-business"><strong>OpenAI Is A Boring (and Bad) Business</strong></h2><p>In reality, OpenAI is an extremely boring (and bad!) software business. It makes the majority of its revenue selling subscriptions to ChatGPT, and apparently had<a href="https://www.theverge.com/openai/640894/chatgpt-has-hit-20-million-paid-subscribers?ref=wheresyoured.at"> <u>20 million paid subscribers</u></a> (as of April) and<a href="https://www.cnbc.com/2025/08/01/openai-raise-chatgpt-users.html?ref=wheresyoured.at"> <u>5 million business subscribers</u></a> (as of August,<a href="https://www.wheresyoured.at/how-to-argue-with-an-ai-booster/#:~:text=Here%E2%80%99s%20a%20hint%20though%3A"> <u>though 500,000 of them are Cal State University seats paid at $2.50 a month</u></a>).</p><p>It also loses incredibly large amounts of money.</p><h3 id="openais-pathetic-api-sales-have-effectively-turned-it-into-any-other-ai-startup"><strong>OpenAI's Pathetic API Sales Have Effectively Turned It Into Any Other AI Startup</strong></h3><p>Yes, I realize that OpenAI also sells access to its API, but as you can see from the chart above, it is making a <em>teeny tiny sliver</em> of revenue from it in 2025, though I will also add that this chart has a little bit of green for "agent" revenue, which means it's very likely bullshit.<a href="https://www.wheresyoured.at/deep-impact/#:~:text=OpenAI%20is%20as,these%20tasks%20yourself.%22"> <u>Operator, OpenAI's so-called agent, is barely functional</u></a>, and I have no idea how anyone would even begin to charge money for it outside of "please try my broken product."</p><p>In any case, API sales appear to be a very, very small part of OpenAI's revenue stream, and that heavily suggests a lack of interest in integrating its models at scale.</p><p>Worse still, this effectively turns OpenAI <em>into an AI startup.</em></p><p>Think about it: if OpenAI can't make the majority of its money through "innovating" in the development of large language models (LLMs), then it’s just another company plugging LLMs into its software. While ChatGPT may be a very popular product, it is, by definition (and in its name!) a GPT wrapper, with the few differences being that OpenAI pays its own immediate costs, has the people necessary to continue improving its own models, and also continually makes promises to convince people it’s <strong><em>anything other than just another AI startup.</em></strong></p><p>In fact, the only<em> real</em> difference is the amount of money backing it. Otherwise, OpenAI could be literally any foundation model company, and with a lack of real innovation within those models, it’s just another startup trying to find ways to monetize generative AI,<a href="https://www.wheresyoured.at/why-everybody-is-losing-money-on-ai/"> <u>an industry that only ever seems to lose money</u></a>.</p><p>As a result, we should start <em>evaluating </em>OpenAI as just another AI startup, as its promises do not appear to mesh with any coherent strategy, other than "<a href="https://www.wheresyoured.at/openai-onetrillion/"><u>we need $1 trillion dollars</u></a>." There does not seem to be much of a plan on a day-to-day basis, nor does there seem to be one about <em>what </em>OpenAI should be, other than that OpenAI will be a consumer hardware, consumer software, enterprise SaaS and data center operator, as well as running a social network.</p><p>As I've discussed<a href="https://www.wheresyoured.at/sam-altman-fried/#:~:text=Despite%20what%20fantasists"> <u>many</u></a><a href="https://www.wheresyoured.at/godot-isnt-making-it/#:~:text=data%20(April).-,I%20shared%20concerns,-in%20July%20that"> <u>times</u></a>, LLMs are inherently flawed due to their probabilistic nature."Hallucinations" — when a model authoritatively states something is true when it isn't (or takes an action that seems the most likely course of action, even if it isn't the right one) — are a "<a href="https://www.computerworld.com/article/4059383/openai-admits-ai-hallucinations-are-mathematically-inevitable-not-just-engineering-flaws.html?ref=wheresyoured.at"><u>mathematically inevitable</u></a>" according to OpenAI's own research feature of the technology, meaning that there is <em>no fixing their most glaring, obvious problem</em>, even with "perfect data."</p><p>I'd wager the reason OpenAI is so eager to build out so much capacity while leaking so many diverse business lines is an attempt to get away from a dark truth: that when you peel away the hype, ChatGPT is a wrapper, every product it makes is a wrapper, <em>and OpenAI is pretty fucking terrible at making products.</em></p><p>Today I'm going to walk you through a fairly unique position: that OpenAI is just another boring AI startup lacking any meaningful product roadmap or strategy, using the press as a tool to pump its bags while very rarely delivering on what it’s promised. It is a company with massive amounts of cash, industrial backing, and brand recognition, and otherwise is, much like its customers, desperately trying to work out how to make money selling products built on top of Large Language Models.</p><p>OpenAI lives and dies on its mythology as the center of innovation in the world of AI, yet reality is so much more mediocre. Its revenue growth is slowing, its products are commoditized, its models are hardly state-of-the-art,<a href="https://www.wheresyoured.at/the-case-against-generative-ai/"> <u>the overall generative AI industry has lost its sheen</u></a>, and its killer app is a mythology that has converted a handful of very rich people and very few others.</p><p>OpenAI spent,<a href="https://www.theinformation.com/articles/openais-first-half-results-4-3-billion-sales-2-5-billion-cash-burn?rc=kz8jh3&amp;ref=wheresyoured.at"> <u>according to The Information</u></a>, 150% ($6.7 billion in costs) of its H1 2025 revenue ($4.3 billion) on research and development, producing<a href="https://www.wheresyoured.at/how-does-gpt-5-work/"> <u>the deeply-underwhelming GPT-5</u></a> and Sora 2, an app that I estimate costs it upwards of $5 for each video generation,<a href="https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/?ref=wheresyoured.at"> <u>based on Azure's published rates for the first Sora model</u></a>, though it's my belief that these rates are unprofitable, all so that it can gain a few more users.</p><p>To be clear, R&amp;D is good, and useful, and in my experience, the companies that spend deeply on this tend to be the ones that do well. The reason why Huawei has managed to outpace its American rivals in several key areas — like automotive technology and telecommunications — is because it <a href="https://www.rcrwireless.com/20250902/5g/huawei-spending?ref=wheresyoured.at"><u>spends around a quarter of its revenue</u></a> on developing new technologies and entering new markets, rather than stock buybacks and dividends.</p><p>The difference is that said R&amp;D spending is both sustainable and useful, and has led to Huawei becoming much a stronger business, even <a href="https://www.federalregister.gov/documents/2019/05/21/2019-10616/addition-of-entities-to-the-entity-list?ref=wheresyoured.at"><u>as it languishes on a Treasury Department entity list that effectively cuts it off from US-made or US-origin parts or IP</u></a>. Considering that OpenAI’s R&amp;D spending <em>was 38.28% of its cash-on-hand</em> by the end of the period (totalling $17.5bn, which we’ll get to later), and what we’ve seen as a result, it’s hard to describe it as <em>either</em> sustainable <em>or</em> useful.&nbsp;&nbsp;&nbsp;&nbsp;</p><p>OpenAI isn't <em>innovative,</em> it’s <em>exploitative, </em>a giant multi-billion dollar grift attempting to hide how deeply <em>unexciting</em> it is, and how nonsensical it is to continue backing it<em>.</em> Sam Altman is an excellent operator, capable of spreading his mediocre, half-baked mantras about<a href="https://bsky.app/profile/politico.com/post/3m25ob7qipc2s?ref=wheresyoured.at"> <u>how 2025 was the year AI got smarter than us</u></a>, or<a href="https://www.businessinsider.com/sam-altman-ai-infrastructure-1-gw-per-week-stargate-2025-9?ref=wheresyoured.at"> <u>how we'll be building 1GW data centers each week</u></a> (something that, by my estimations, takes 2.5 years), taking advantage of how many people in the media, markets and global governments don't know a fucking <em>thing</em> about anything.</p><p>OpenAI is&nbsp; also getting desperate.</p><p>Beneath the surface of the media hype and trillion-dollar promises is a company struggling to maintain relevance, its entire existence built on top of hype and mythology.</p><p>And at this rate, I believe it’s going to miss its 2025 revenue projections, all while burning billions more than anyone has anticipated.</p>
</div></div>]]></description>
        </item>
    </channel>
</rss>