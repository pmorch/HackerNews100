<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 11 Sep 2024 11:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[I wish I didn't miss the '90s-00s internet (115 pts)]]></title>
            <link>https://rohan.ga/blog/early-internet/</link>
            <guid>41508040</guid>
            <pubDate>Wed, 11 Sep 2024 04:32:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rohan.ga/blog/early-internet/">https://rohan.ga/blog/early-internet/</a>, See on <a href="https://news.ycombinator.com/item?id=41508040">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <h2 id="about-me">about me</h2>
<p>I am 18, born in 2006. This is generally a good thing as I am in the prime of life currently. I am not one of those people who think they were “born in the wrong decade”, I think I was born at the perfect time to take advantage of superlinearly growing technological advancements.</p>
<h2 id="the-internet-today">the internet today</h2>
<p>I generally greatly dislike social media, although I am an avid user of it. When social media caught on, the people running these companies were tasked to make it profitable. In doing this, social media got completely ruined.</p>
<p>Our data got commodified[1], our attention got commodified, and a substantive part of who we say we are got commodified. This, in general, has led to a degradation in the quality of the internet.</p>
<p>They basically made social media like a drug, as addictive as possible. They do this by promoting FOMO and comparison in Instagram’s case. Instagram is a game, it is extremely performative. People carefully curate each part of their insta to give certain impressions.</p>
<p>What’s the ratio of followers to following you have? Are your story highlights organized and “aesthetic”? What reels are you liking?</p>
<p>There are a lot of “rules” in this game, which are enforced by social “ins” and mutual respect.</p>
<p>When it comes to shortform content, hundreds of people compete for slivers of our attention. We are not agents in this, they are just presented to us. Completely depersonalized. They are forgotten within seconds. Ask someone watching tiktok to describe the previous tiktoks they just watched, they would be hard pressed to tell you more than a few minutes in the past. Something about tiktok is unusually addictive. all the while providing absolutely no value.</p>
<p>It has become so shallow, you can tell almost nothing about who someone actually is through Instagram or tiktok. You can only tell how they want to portray themselves to the general population and, by how they organize their profile, if they are eligible to be a part of your social circle.</p>
<h2 id="the-appeal-of-simplicity">the appeal of simplicity</h2>
<p>I wish I was around when people had blogs or even myspace. This era was deeply personal and creative. Most writing on the internet was individual, not written in search of “SEO” or profit but driven by the need and want of people to share knowledge–pure curiosity.</p>
<p>I want the thrill of finding new websites searching through web rings; when the web was truly the wild west and not another arm of control by mega corporations.</p>
<p>This is also reflective in the quality of content. There was little incentive to lie, to manipulate truth, and each blog entry or piece of information was tied to identity. (except in the cases of anonymity).</p>
<p>Even the content written by normal people for normal people has been commodified by sites like reddit and quora. What happened to an old fashioned forums or even usenet groups? (granted, especially for cars and hacking, there still exists plenty of forums)</p>
<p>Also, websites just simply looked cooler. Occasionally I scroll on the geocities archive and wonder, how did we get here?  What happened to the patterned backgrounds, the bright maximalist jpegs and gifs?  This is sort of contradictory to my website, as it’s almost annoyingly minimalist, but this more has to do with social norms and simplicity. Having a personal blog is already out of the ordinary, but the simple design and clear technical direction/theme gives me an excuse. I also am not that personal on here, because only a few of my friends frequent my blogs, and I want the site to be as simple and to the point as possible if a random person wants to know who I am.</p>
<h2 id="a-niche-resurgence">a niche resurgence</h2>
<p>There is <a href="https://neocities.org/">neocites</a>, and a small community of people who share this philosophy about the web (and that are relatively young), but I have not met anyone my age, in the real world, that would choose to do something like this.</p>
<p>The majority of people (my age) today would think sites like those (and, by extension, their creators) are weird.</p>
<p>[1] you can substitute “commodified” with “bought and sold”</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The magic of DC-DC voltage conversion (189 pts)]]></title>
            <link>https://lcamtuf.substack.com/p/the-magic-of-dc-dc-voltage-conversion</link>
            <guid>41507879</guid>
            <pubDate>Wed, 11 Sep 2024 03:53:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lcamtuf.substack.com/p/the-magic-of-dc-dc-voltage-conversion">https://lcamtuf.substack.com/p/the-magic-of-dc-dc-voltage-conversion</a>, See on <a href="https://news.ycombinator.com/item?id=41507879">Hacker News</a></p>
Couldn't get https://lcamtuf.substack.com/p/the-magic-of-dc-dc-voltage-conversion: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Building the same app using various web frameworks (135 pts)]]></title>
            <link>https://eugeneyan.com/writing/web-frameworks/</link>
            <guid>41507271</guid>
            <pubDate>Wed, 11 Sep 2024 01:13:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eugeneyan.com/writing/web-frameworks/">https://eugeneyan.com/writing/web-frameworks/</a>, See on <a href="https://news.ycombinator.com/item?id=41507271">Hacker News</a></p>
Couldn't get https://eugeneyan.com/writing/web-frameworks/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Chai-1 Defeats AlphaFold 3 (222 pts)]]></title>
            <link>https://www.chaidiscovery.com/blog/introducing-chai-1</link>
            <guid>41506157</guid>
            <pubDate>Tue, 10 Sep 2024 22:13:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chaidiscovery.com/blog/introducing-chai-1">https://www.chaidiscovery.com/blog/introducing-chai-1</a>, See on <a href="https://news.ycombinator.com/item?id=41506157">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p>We’re excited to release Chai-1, a new multi-modal foundation model for molecular structure prediction that performs at the state-of-the-art across a variety of tasks relevant to drug discovery. Chai-1 enables unified prediction of proteins, small molecules, DNA, RNA, covalent modifications, and more.</p><p>The model is available for free via a <a href="https://lab.chaidiscovery.com/" target="_blank" rel="noopener">web interface</a>, including for commercial applications such as drug discovery. We are also releasing the model weights and inference code as a <a href="https://github.com/chaidiscovery/chai-lab" target="_blank" rel="noopener">software library</a> for non-commercial use.</p><h2>A frontier model for biomolecular interactions</h2><p>We tested Chai-1 across a large number of benchmarks, and found that the model achieves a 77% success rate on the PoseBusters benchmark (vs. 76% by AlphaFold3), as well as an Cα LDDT of 0.849 on the CASP15 protein monomer structure prediction set (vs. 0.801 by ESM3-98B).</p><p><img alt="" data-framer-asset="data:framer/asset-reference,nAGHKUD9XOHtavXDEpA6XH8Wo4.png" data-framer-height="896" data-framer-width="1814" height="448" src="https://framerusercontent.com/images/nAGHKUD9XOHtavXDEpA6XH8Wo4.png" width="907"></p><p>Unlike many existing structure prediction tools which require multiple sequence alignments (MSAs), Chai-1 can also be run in single sequence mode without MSAs while preserving most of its performance. The model can fold multimers more accurately (69.8%) than the MSA-based AlphaFold-Multimer model (67.7%), as measured by the DockQ acceptable prediction rate. Chai-1 is the first model that’s able to predict multimer structures using single-sequences alone (without MSA search) at AlphaFold-Multimer level quality.</p><p>For more information, and a comprehensive analysis of the model, read our <a href="https://chaiassets.com/chai-1/paper/technical_report_v1.pdf" target="_blank" rel="noopener">technical report</a>.</p><h2><strong>A natively multi-modal foundation model</strong></h2><p>In addition to its frontier modeling capabilities directly from sequences, Chai-1 can be prompted with new data, e.g. restraints derived from the lab, which boost performance by double-digit percentage points.&nbsp;We explore a number of these capabilities in our technical report, such as epitope conditioning – using even a handful of contacts or pocket residues (potentially derived from lab experiments) doubles antibody-antigen structure prediction accuracy, making antibody engineering more feasible using AI.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,icSSrTXU76WdTzKi30rSK70UY.png" data-framer-height="1962" data-framer-width="4716" height="981" src="https://framerusercontent.com/images/icSSrTXU76WdTzKi30rSK70UY.png" width="2358"></p><h2><strong>Releasing the model for all</strong></h2><p>We are releasing Chai-1 via a web interface for free, including for commercial applications such as drug discovery. We are also releasing the code for Chai-1 for non-commercial use as a software library. We believe that when we build in partnership with the research and industrial communities, the entire ecosystem benefits.</p><p>Try Chai-1 for yourself by visiting <a href="http://lab.chaidiscovery.com/" rel="noopener">lab.chaidiscovery.com</a>, or run it from our GitHub repository at <a href="https://github.com/chaidiscovery/chai-lab" target="_blank" rel="noopener">github.com/chaidiscovery/chai-lab</a>.</p><h2><strong>What's next?</strong></h2><p>The team comes from pioneering research and applied AI companies such as OpenAI, Meta FAIR, Stripe, and Google X. Collectively, we have played pivotal roles in the advancement of research in AI for biology. The majority of the team has been Head of AI at leading drug discovery companies, and has collectively helped advance over a dozen drug programs.&nbsp;</p><p>Chai-1 is the result of a few months of intense work, and yet we are only at the starting line. Our broader mission at Chai Discovery is to transform biology from science into engineering. To that end, we'll be building further AI foundation models that predict and reprogram interactions between biochemical molecules, the fundamental building blocks of life. We’ll have more to share on this soon.</p><p>We are grateful for the partnership of <a href="https://www.dimensioncap.com/" rel="noopener">Dimension</a>, <a href="https://www.thrivecap.com/" rel="noopener">Thrive Capital</a>, <a href="https://openai.com/" rel="noopener">OpenAI</a>, <a href="https://www.conviction.com/" rel="noopener">Conviction</a>, <a href="https://neo.com/" rel="noopener">Neo</a>, Lachy Groom, and <a href="https://www.amplifypartners.com/" rel="noopener">Amplify Partners</a>, as well as Anna and Greg Brockman, Blake Byers, Fred Ehrsam, Julia and Kevin Hartz, Will Gaybrick, David Frankel, R. Martin Chavez, and many others.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Flipper Zero Gets Major Firmware Update, Can Eavesdrop on Walkie-Talkies (195 pts)]]></title>
            <link>https://www.pcmag.com/news/flipper-zero-gets-major-firmware-update</link>
            <guid>41505670</guid>
            <pubDate>Tue, 10 Sep 2024 21:15:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pcmag.com/news/flipper-zero-gets-major-firmware-update">https://www.pcmag.com/news/flipper-zero-gets-major-firmware-update</a>, See on <a href="https://news.ycombinator.com/item?id=41505670">Hacker News</a></p>
Couldn't get https://www.pcmag.com/news/flipper-zero-gets-major-firmware-update: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Rust in illumos (104 pts)]]></title>
            <link>https://wegmueller.it/blog/posts/2024-09-02-rust-on-illumos</link>
            <guid>41505665</guid>
            <pubDate>Tue, 10 Sep 2024 21:15:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wegmueller.it/blog/posts/2024-09-02-rust-on-illumos">https://wegmueller.it/blog/posts/2024-09-02-rust-on-illumos</a>, See on <a href="https://news.ycombinator.com/item?id=41505665">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<article>

<p>With the recent rust in Linux events in the last couple of days, It’s a good time to write up Rust in illumos. Both to spread the word a bit and also to set expectations for both sides (Rust and illumos/OpenIndiana devs) what is currently possible and what work would need to be invested to make things smooth. And also to let the rust community know about what illumos people were talking about. </p>
<p>What most of the talk currently is about, are the technical details. But we must not leave the social aspects out of it. Software distributions are not made by lone walkers but by groups of people. Bringing in a new language means facilitating change. And that means there are more topics to discuss than just API design. We are talking about impacts on the whole software lifecycle.</p>
<h2>Linux DRM API design</h2>
<p>Looking at the things people like Asahi Lina want to address inside the Linux Kernel with the Rust bindings and how she describes the issues with Locking I get the feeling something with DRM is not consistent. Looking into our code and our <a href="https://illumos.org/books/wdd/mt-17026.html#mt-17026" rel="nofollow">docs</a> for this topic I can already see that things are more complex in the whole kernel than just “do X” We have some general recommendations but it’s a case by case issue when looking at it over the whole kernel sources.  When looking at the illumos DRM fork I can see that a lot of X11 code seems to have wandered into the Kernel. And not many files are created by the same people. I am not surprised that this has gotten messy. The illumos kernel docs talk about multiple cases where Data can be accessed and different needs for locking with them. I assume the Linux kernel has similar cases, hence at least mentioning what locks the driver needs to do and which the DRM API does needs to be documented. </p>
<h2>Rust in the illumos Kernel</h2>
<p>The development model of illumos is different from Linux and thus there are no Rust drivers in upstream illumos yet. But that is to be expected for new things. In our model, we take the time to mature new tech in a fork, and for rust, the Oxide fork has taken that role. In there, we have several drivers for the Oxide Networking stack that are in rust. Based on that some experience could be gained. The current state is, that making things in Rust takes more time compared to C for a trained developer. There is Bindgen which has an overhead to learn and use and there is Language training that people need to become productive. It’s one thing to understand the language but becoming productive usually means quite a bit more training on top of that. So far userland tools have proven to be small enough to get to a working result within a reasonable time. OPTE and fast path Networking exist but they still need an integration into the MAC network framework. So more work needs to happen on that front. Smaller drivers are also a possibility to do I am currently unaware of somebody that had an interest though.</p>
<h2>The lack of Systems package manager support in rust</h2>
<p>Ever since npm started the packaging ecosystem has changed drastically in focus. Where package managers originally installed Software as a System they now shifted to install software on different Systems in the same way. This also changed how responsibilities are handled and how people now develop software. Distributions have become obsolete and are not of interest to people. And that leads to a couple of interesting issues. Because at the end of the day, you need a distribution to start using a Computer. And people that make distributions also need to make some income for that work. But software developers now only need to focus on their software for things to work. Compiling from source has become trivial. But only if you follow the software developer’s workflow and know the tools. For people needing to read themselves into how cargo works, some quirks make sense for software developer workflows but are a hindrance for system development workflows. Keep in mind these are different requirements than the ones the kernel needs. Systems Distributors usually do the following:</p>
<ul><li>Download the sources and make an archive with all the patches they want to bring</li>
<li>patch the sources from the patch files</li>
<li>build the binaries </li>
<li>pack them into an archive for distribution (depends on the package manager)</li></ul>
<p>Several of cargo’s features are now counterintuitive and Are the ones I find people criticising. Cargo wants to secure and check that the vendor folder has not been modified. There is no central vendor folder.  Rust software can easily have upwards of 100 dependencies and micro dependencies. So if people want audit software it becomes a lot of work. I don’t mind. I am long enough in this industry that I have lived through the Java dependency problems on Linux systems. Cargo improves upon that situation. Several other OS’s believe that there needs to be a clear differentiation between system and Third-party software. And I agree with that. The Debian approach of putting everything into one system and only having one version of each dependency is not feasible for a huge international community of people that develop together but never meet. Most of the time the devs do not talk to each other at all. I am personally of the opinion, that most of rust builds and their tier system works. And I am happy to rely on that and not just my tools. As a side note. None of this topic requires Harsh words. Systems packagers and Software developers (especially the folks in Debian and traditional Linux distros) have very different ways of thinking and cargo is not a tool for system packagers. It would be nice if it grew some support for that but that does not require harsh words. System source repos like illumos-gate can easily vendor and produce binaries with a small list of dependencies to deliver the binaries via a Package manager. The tools work.</p>
<h2>Missing support for shared libraries</h2>
<p>A feature not wanted by Software developers, developing for multiple platforms. For systems packagers, this is a required feature. Shared libraries delimit the boundary between two responsibilities. And if those people coordinate then that works well. It becomes a system. There are limits to where this can happen. So I don’t know what the perfect solution is. If shared libraries are needed at all or if that feature can fade out. But it is worth a try to build systems and to give people the possibility to do so. So I would wish Rust and Cargo gained shared library support so that we can build such componentized systems easily.</p>
<h2>An invite</h2>
<p>With all this said I would love to have some more rust folks in the illumos community. And I know this has been expressed by others as well. Userland tools are easy to make in Rust and I for one would love to have people help me with the new <a href="https://github.com/Toasterson/illumos-installer" rel="nofollow">Installer</a> and with the package <a href="https://github.com/toasterson/forge" rel="nofollow">Forge</a> We have gained rust crates for our unique API’s such as <a href="https://github.com/illumos/libcontract-sys" rel="nofollow">libcontract</a> Our new image builder for ISO’s is in rust <a href="https://github.com/illumos/image-builder" rel="nofollow">image-builder</a> and we are always looking for driver developers. Check out the <a href="https://github.com/orgs/illumos/repositories?type=all" rel="nofollow">illumos</a> for all repos Including a config manager. Want to write complete Wifi kernel parts? A small serial Adapter that you have lying around? Want to integrate with an existing Kernel without rewriting it? If anything of that makes you want to head over to <a href="https://illumos.org/books/wdd/preface.html#preface" rel="nofollow">https://illumos.org/books/wdd/preface.html#preface</a> and <a href="https://illumos.org/books/dev/" rel="nofollow">https://illumos.org/books/dev/</a> or Simply just interests you. Then we would love to have your contribution.</p>
<p>Hope to talk to some folks on Socials and email</p>
<p>– Toasty</p></article>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lottery Simulator (2023) (167 pts)]]></title>
            <link>https://perthirtysix.com/tool/lottery-simulator</link>
            <guid>41505593</guid>
            <pubDate>Tue, 10 Sep 2024 21:09:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://perthirtysix.com/tool/lottery-simulator">https://perthirtysix.com/tool/lottery-simulator</a>, See on <a href="https://news.ycombinator.com/item?id=41505593">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>A New Daily Game!</p><p> We built a daily poll game called the <a href="https://perthirtysix.com/communal-plot-daily-poll">Communal Plot</a>! We hope it's a fun way to engage with the community and see how your opinions stack up. Check it out and let us know what you think! </p></div><p> Every so often, a lottery jackpot will get so high that I'll hear about it on the news or from a friend. When this happens, I immediate start wondering about two things: what I would do with hundreds of millions of dollars and what the odds of winning really are. </p><p> While major lotteries publish some of this information, I wanted to build something that would make it easier to play around with the data in a more exploratory way. With that, here is the PerThirtySix Lottery Simulator! </p><p> This tool is broken up into two sections: Setup and Simulation. The Setup section lets explore probabilities for an existing American lottery or for your own lottery with custom rules. The Simulation section lets you pick some numbers and play up to thousands of tickets per second, and visualizes the returns for you. Note that this tool makes some simplifying assumptions, like that there's only one jackpot winner and that taxes are ignored. </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Not Comments (168 pts)]]></title>
            <link>https://buttondown.com/hillelwayne/archive/why-not-comments/</link>
            <guid>41505389</guid>
            <pubDate>Tue, 10 Sep 2024 20:52:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://buttondown.com/hillelwayne/archive/why-not-comments/">https://buttondown.com/hillelwayne/archive/why-not-comments/</a>, See on <a href="https://news.ycombinator.com/item?id=41505389">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                
                    <date>
                        
                            September 10, 2024
                        </date>
                
                
                
                    <h2>
                        Why not "why not" comments? Not why "not comments"
                    </h2>
                

                

                
                    
                        <h2>Logic For Programmers v0.3</h2>
<p><a href="https://leanpub.com/logic/" target="_blank">Now available</a>! It's a light release as I learn more about formatting a nice-looking book. You can see some of the differences between v2 and v3 <a href="https://bsky.app/profile/hillelwayne.com/post/3l3egdqnqj62o" target="_blank">here</a>.</p>
<h2>Why Not Comments</h2>
<p>Code is written in a structured machine language, comments are written in an expressive human language. The "human language" bit makes comments more expressive and communicative than code. Code has a limited amount of something <em>like</em> human language contained in identifiers. "Comment the why, not the what" means to push as much information as possible into identifiers. <a href="https://buttondown.com/hillelwayne/archive/3866bd6e-22c3-4098-92ef-4d47ef287ed8" target="_blank">Not all "what" can be embedded like this</a>, but a lot can.</p>
<p>In recent years I see more people arguing that <em>whys</em> do not belong in comments either, that they can be embedded into <code>LongFunctionNames</code> or the names of test cases. Virtually all "self-documenting" codebases add documentation through the addition of identifiers.<sup id="fnref:exception"><a href="#fn:exception">1</a></sup></p>
<p>So what's something in the range of human expression that <em>cannot</em> be represented with more code?</p>
<p>Negative information, drawing attention to what's <em>not</em> there. The "why nots" of the system.</p>
<h3>A Recent Example</h3>
<p>This one comes from <em>Logic for Programmers</em>. For convoluted technical reasons the epub build wasn't translating math notation (<code>\forall</code>) into symbols (<code>∀</code>). I wrote a script to manually go through and replace tokens in math strings with unicode equivalents. The easiest way to do this is to call <code>string = string.replace(old, new)</code> for each one of the 16 math symbols I need to replace (some math strings have multiple symbols).</p>
<p>This is incredibly inefficient and I could instead do all 16 replacements in a single pass. But that would be a more complicated solution. So I did the simple way with a comment:</p>
<div><pre><span></span><code>Does 16 passes over each string
BUT there are only 25 math strings in the book so far and most are &lt;5 characters.
So it's still fast enough.
</code></pre></div>
<p>You can think of this as a "why I'm using slow code", but you can also think of it as "why not fast code". It's calling attention to something that's <em>not there</em>.</p>
<h3>Why the comment</h3>
<p>If the slow code isn't causing any problems, why have a comment at all?</p>

<p>Well first of all the code might be a problem later. If a future version of <em>LfP</em> has hundreds of math strings instead of a couple dozen then this build step will bottleneck the whole build. Good to lay a signpost now so I know exactly what to fix later.</p>
<p>But even if the code is fine forever, the comment still does something important: it shows <em>I'm aware of the tradeoff</em>. Say I come back to my project two years from now, open <code>epub_math_fixer.py</code> and see my terrible slow code. I ask "why did I write something so terrible?" Was it inexperience, time crunch, or just a random mistake?</p>
<p>The negative comment tells me that I <em>knew</em> this was slow code, looked into the alternatives, and decided against optimizing. I don't have to spend a bunch of time reinvestigating only to come to the same conclusion. </p>
<h2>Why this can't be self-documented</h2>
<p>When I was first playing with this idea, someone told me that my negative comment isn't necessary, just name the function <code>RunFewerTimesSlowerAndSimplerAlgorithmAfterConsideringTradeOffs</code>. Aside from the issues of being long, not explaining the tradeoffs, and that I'd have to change it everywhere if I ever optimize the code... This would make the code <em>less</em> self-documenting. It doesn't tell you what the function actually <em>does</em>.</p>
<p>The core problem is that function and variable identifiers can only contain one clause of information. I can't store "what the function does" and "what tradeoffs it makes" in the same identifier. </p>
<p>What about replacing the comment with a test. I guess you could make a test that greps for math blocks in the book and fails if there's more than 80? But that's not testing <code>EpubMathFixer</code> directly. There's nothing in the function itself you can hook into. </p>
<p>That's the fundamental problem with self-documenting negative information. "Self-documentation" rides along with written code, and so describes what the code is doing. Negative information is about what the code is <em>not</em> doing. </p>
<h3>End of newsletter speculation</h3>
<p>I wonder if you can think of "why not" comments as a case of counterfactuals. If so, are "abstractions of human communication" impossible to self-document in general? Can you self-document an analogy? Uncertainty? An ethical claim?</p>

                    
                

                
                    <p><em>If you're reading this on the web, you can subscribe <a href="https://buttondown.com/hillelwayne" target="_blank">here</a>. Updates are once a week. My main website is <a href="https://www.hillelwayne.com/" target="_blank">here</a>.</em></p>
                

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Another police raid in Germany (445 pts)]]></title>
            <link>https://forum.torproject.org/t/tor-relays-artikel-5-e-v-another-police-raid-in-germany-general-assembly-on-sep-21st-2024/14533</link>
            <guid>41505009</guid>
            <pubDate>Tue, 10 Sep 2024 20:12:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://forum.torproject.org/t/tor-relays-artikel-5-e-v-another-police-raid-in-germany-general-assembly-on-sep-21st-2024/14533">https://forum.torproject.org/t/tor-relays-artikel-5-e-v-another-police-raid-in-germany-general-assembly-on-sep-21st-2024/14533</a>, See on <a href="https://news.ycombinator.com/item?id=41505009">Hacker News</a></p>
Couldn't get https://forum.torproject.org/t/tor-relays-artikel-5-e-v-another-police-raid-in-germany-general-assembly-on-sep-21st-2024/14533: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Tutorial on diffusion models for imaging and vision (175 pts)]]></title>
            <link>https://arxiv.org/abs/2403.18103</link>
            <guid>41504885</guid>
            <pubDate>Tue, 10 Sep 2024 19:59:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.18103">https://arxiv.org/abs/2403.18103</a>, See on <a href="https://news.ycombinator.com/item?id=41504885">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>
      <h2>Computer Science &gt; Machine Learning</h2>
    </p>

    <p><strong>arXiv:2403.18103</strong> (cs)
    </p>

<div id="content-inner">
    <p>
  [Submitted on 26 Mar 2024 (<a href="https://arxiv.org/abs/2403.18103v1">v1</a>), last revised 6 Sep 2024 (this version, v2)]</p>
    
                
    <p><a href="https://arxiv.org/pdf/2403.18103">View PDF</a></p><blockquote>
            <span>Abstract:</span>The astonishing growth of generative tools in recent years has empowered many exciting applications in text-to-image generation and text-to-video generation. The underlying principle behind these generative tools is the concept of diffusion, a particular sampling mechanism that has overcome some shortcomings that were deemed difficult in the previous approaches. The goal of this tutorial is to discuss the essential ideas underlying the diffusion models. The target audience of this tutorial includes undergraduate and graduate students who are interested in doing research on diffusion models or applying these models to solve other problems.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
    <div>
      <h2>Submission history</h2><p> From: Stanley Chan [<a href="https://arxiv.org/show-email/052533b7/2403.18103">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2403.18103v1">[v1]</a></strong>
        Tue, 26 Mar 2024 21:01:41 UTC (3,221 KB)<br>
    <strong>[v2]</strong>
        Fri, 6 Sep 2024 19:58:27 UTC (3,822 KB)<br>
</p></div>
  </div><div id="labstabs"><p>
    <label for="tabone">Bibliographic Tools</label></p><div>
      <h2>Bibliographic and Citation Tools</h2>
      <div>
          <p><label>
              
              <span></span>
              <span>Bibliographic Explorer Toggle</span>
            </label>
          </p>
          
        </div>
        
        
        
    </div>


    <p>
    <label for="tabtwo">Code, Data, Media</label></p><div>
      <h2>Code, Data and Media Associated with this Article</h2>
      

      
      
      
      
      
      
    </div>


      <p>
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label></p><div>
        <h2>Demos</h2>
        
        
        
        
      </div>
      <p>
      <label for="tabfour">Related Papers</label></p><div>
        <h2>Recommenders and Search Tools</h2>
        <div>
            <p><label>
                
                <span></span>
                <span>IArxiv recommender toggle</span>
              </label>
            </p>
            
          </div>
        
        
        
        
        
      </div>

      <p>
      <label for="tabfive">
        About arXivLabs
      </label></p><div>
            <h2>arXivLabs: experimental projects with community collaborators</h2>
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv's community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Git Bash is my preferred Windows shell (168 pts)]]></title>
            <link>https://www.ii.com/git-bash-is-my-preferred-windows-shell/</link>
            <guid>41504832</guid>
            <pubDate>Tue, 10 Sep 2024 19:54:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ii.com/git-bash-is-my-preferred-windows-shell/">https://www.ii.com/git-bash-is-my-preferred-windows-shell/</a>, See on <a href="https://news.ycombinator.com/item?id=41504832">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h3 id="_case_insensitivity"><a href="#_case_insensitivity">Case insensitivity</a></h3><p>Normally
<a href="https://www.ii.com/portal/nix-nux/" title="#nix-nux Portal on Infinite Ink">Unix-like</a>
shells are case sensitive, but
Git&nbsp;Bash, in general, is case insensitive.</p></div><div><h3 id="_paths"><a href="#_paths">Paths</a></h3><p>To view your current path, run
the
following command, which means
“print working directory.”</p><p>To go to your Git&nbsp;Bash home directory,
which is also known as <code>~</code>,
run either of the following equivalent commands.</p><p>On most systems, your Git&nbsp;Bash home directory
is…</p><p>with <code>USERNAME</code> replaced with your user name.</p><p>Note that in Git&nbsp;Bash…</p><div><ul><li><p>The path delimiter is slash (<code>/</code>), which is also known as forward slash.</p></li><li><p>The <code>C:</code> drive is mounted as <code>/c/</code>.
To list all mounts, run the <code>mount</code> command.</p></li><li><p>You can display a Windows-type path of the current working directory with
<code>pwd&nbsp;-W</code>.
The format of the path this command displays is sometimes called
“mixed&nbsp;type”
because it starts with a drive letter, such as <code>C:</code>,
and uses
forward&nbsp;slashes
(<code>/</code>)
rather than backslashes
(<code>\</code>)
as path separators.
For example, this sequence of commands…</p></li></ul></div><p>displays…</p><div><pre>/c/Users/USERNAME
C:/Users/USERNAME</pre></div></div><div><h3 id="_the_start_command"><a href="#_the_start_command">The <code>start</code> command</a></h3><p>When you are at a Git&nbsp;Bash prompt, you can launch
a file or directory in your system’s default app
by using
the <code>Start</code> command,
which is equivalent
to
the <code>start</code> command (thanks to case insensitivity).
Below are some examples.</p><p>To open the current directory in Windows File&nbsp;Explorer, run:</p><div><pre>start .
      👆
     Notice this dot (.)</pre></div><p>To open your home directory in Windows File&nbsp;Explorer, run:</p><p>To open your $APPDATA directory in Windows File&nbsp;Explorer, run:</p><p>To open <code>/c</code> in Windows File&nbsp;Explorer, run either of these equivalent commands:</p><p>To open the parent directory of the current directory in Windows File&nbsp;Explorer, run:</p><p>To open an HTML file in your default web browser,
run
something like this:</p></div><div><h3 id="_launching_any_app_on_your_path"><a href="#_launching_any_app_on_your_path">Launching any app on your path</a></h3><p>You can launch
any app that’s on your path
from a Git&nbsp;Bash prompt.
For example, if Visual Studio Code is
installed on your system and is on your path,
you can use the following to open the current directory
in VS Code.</p><div><pre>code .
     👆
    Notice this dot (.)</pre></div></div><div><h3 id="_environment_variables"><a href="#_environment_variables">Environment variables</a></h3><p>To find out
the
environment variables
available
to
Git&nbsp;Bash,
run:</p><p>This includes the PATH environment variable, which lists the directories that are searched for executables.</p></div><div><h3 id="_scripting"><a href="#_scripting">Scripting</a></h3><p>I’d rather write a bash or
sh⁠<sup>[<a href="#_footnotedef_14" title="View footnote.">14</a>]</sup>
<a href="https://wikipedia.org/wiki/Shell_script">shell script</a>
than a
Windows
<a href="https://wikipedia.org/wiki/Batch_file">batch file</a>
to
deal with apps, files, and folders
that live in my Windows file system.
Thanks to
Git&nbsp;Bash,
It’s easy to do this.
(I have learned the hard way that it’s not so easy to do this
in
WSL.⁠<sup>[<a href="#_footnotedef_15" title="View footnote.">15</a>]</sup>)</p><div><h4 id="_where_to_put_scripts"><a href="#_where_to_put_scripts">Where to put scripts</a></h4><p>When you run <code>printenv</code>, you can see that the <code>~/bin/</code> directory
(usually <code>/c/users/USERNAME/bin/</code>)
is
on your PATH. This is a reasonable place to put your Git&nbsp;Bash scripts.
To create this directory, run this command:</p></div><div><h4 id="_example_script"><a href="#_example_script">Example script</a></h4><p>One of my scripts is called
<code>gvim-winpath</code>
and it looks like this:</p><div><pre>#!/bin/sh

## Created: 2024-05-15
## Filename: gvim-winpath
## Usage: gvim-winpath "C:\path\to\filename"

/c/windows/gvim.bat `cygpath "$1"`</pre></div><p>I use this to launch gvim on a file that
is specified using a Windows-style backslash path.
To learn about the <code>cygpath</code> command, which is used in this script, run
one of
the following commands at a Git&nbsp;Bash prompt.</p><div><pre>cygpath --help
cygpath --help |less</pre></div><p>If you pipe the <code><nobr>cygpath&nbsp;--help</nobr></code> output to the <code>less</code> pager, you
need to know
how to
use <code>less</code>:
The essentials are
that
within <code>less</code>, you can
press <kbd>Space</kbd> to page down,
press <kbd>b</kbd> (for back) to page up,
and press
<kbd>q</kbd>
to quit.</p></div><div><h4 id="_no_need_to_chmod"><a href="#_no_need_to_chmod">No need to chmod</a></h4><p>In most Unix-like shells, you need to <code>chmod&nbsp;+x</code> executables
but in Git&nbsp;Bash this is not needed.</p></div></div><div><h3 id="_unicode"><a href="#_unicode">Unicode</a></h3><p>If you have issues with
non-ASCII
Unicode characters, run
the following at a Git&nbsp;Bash prompt
and make sure each
setting
includes <code>UTF-8</code>.</p><p>Also,
you can
try
to solve
Unicode issues
by
running
the following sequence of
<code>chcp.com</code>⁠<sup>[<a href="#_footnotedef_12" title="View footnote.">12</a>]</sup>
commands at a Git&nbsp;Bash prompt.</p><div><pre>chcp.com
chcp.com 65001
chcp.com</pre></div><p>This <strong>ch</strong>anges the <strong>c</strong>ode <strong>p</strong>age to 65001, which supports UTF-8 encoding.</p><div><table><tbody><tr><td><p><span>ℹ</span></p></td><td>Nowadays
UTF-8 is the standard file encoding
of the internet (and of Unicode
in&nbsp;general).</td></tr></tbody></table></div></div><div><h3 id="_case_insensitivity_exceptions"><a href="#_case_insensitivity_exceptions">Case insensitivity exceptions</a></h3><div><h4 id="_help_and_other_command_arguments"><a href="#_help_and_other_command_arguments">--help and other command arguments</a></h4><p>Arguments to nix-⁠nux commands, in general, are case sensitive. For example,
in the following</p><p>The command <code>cygpath</code> is case insensitive,
but
its argument <code>--help</code> is case sensitive.
For example,
this works:</p><p>but this does not work:</p></div><div><h4 id="_exit"><a href="#_exit">exit</a></h4><p>To quit Git&nbsp;Bash, you can either
click the close-window X in the upper right corner of the terminal window
or type the following at the command prompt:</p><p>This <code>exit</code> command
is a
<a href="https://wikipedia.org/wiki/System_call">system&nbsp;call</a>
and
must be all lower
case.⁠<nobr> <span>◊</span></nobr></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Iron Mountain: It's Time to Talk About Hard Drives (102 pts)]]></title>
            <link>https://www.mixonline.com/business/inside-iron-mountain-its-time-to-talk-about-hard-drives</link>
            <guid>41504331</guid>
            <pubDate>Tue, 10 Sep 2024 19:04:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mixonline.com/business/inside-iron-mountain-its-time-to-talk-about-hard-drives">https://www.mixonline.com/business/inside-iron-mountain-its-time-to-talk-about-hard-drives</a>, See on <a href="https://news.ycombinator.com/item?id=41504331">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
						<article id="post-147171">
	<header>
        
    

    <p>Iron Mountain Media and Archive Services sounds the alarm: Aging tracks created through an all-digital workflow aren't guaranteed to play back.</p>
	
</header>


	<section>
		<figure id="attachment_147159" aria-describedby="caption-attachment-147159"><img fetchpriority="high" decoding="async" src="https://www.mixonline.com/wp-content/uploads/2024/08/2024-08-19-Iron-Mountain-Boyers-10_Cropped.jpg" alt="Shelves of media assets stretch as far as the eye can see at Iron Mountain’s secure, climate-controlled, underground facility in Boyers, Penn. Photo: Courtesy of Iron Mountain." width="372" height="640" srcset="https://www.mixonline.com/wp-content/uploads/2024/08/2024-08-19-Iron-Mountain-Boyers-10_Cropped.jpg 700w, https://www.mixonline.com/wp-content/uploads/2024/08/2024-08-19-Iron-Mountain-Boyers-10_Cropped-353x607.jpg 353w" sizes="(max-width: 372px) 100vw, 372px"><figcaption id="caption-attachment-147159">Shelves of media assets stretch as far as the eye can see at Iron Mountain’s secure, climate-controlled, underground facility in Boyers, Penn. Photo: Courtesy of Iron Mountain.</figcaption></figure>
<p>New York, NY (August 19, 2024)—A few years ago, archiving specialist <a href="https://www.imes.media/" target="_blank" rel="noopener">Iron Mountain Media and Archive Services</a> did a survey of its vaults and discovered an alarming trend: Of the thousands and thousands of archived hard disk drives from the 1990s that clients ask the company to work on, around one-fifth are unreadable. Iron Mountain has a broad customer base, but if you focus strictly on the music business, says Robert Koszela, Global Director Studio Growth and Strategic Initiatives, “That means there are historic sessions from the early to mid-’90s that are dying.”</p>
<p>Until the turn of the millennium, the workflow for record releases was simple enough. Once the multitrack was mixed, the 2-track master was turned into a piece of vinyl, a cassette tape or, starting in 1982, a compact disc, and those original tapes—by and large— then went into storage. Around 2000, with the advent of 5.1-surround releases, then in 2005 with the debut of the Guitar Hero video game, things started to get complicated. When rights holders went to the vaults to transfer, remix and repurpose some of their catalog tracks for these new platforms, they discovered that some tapes were deteriorating while others were unplayable. Not all assets had been stored under optimum conditions. Some recordings had been made on machines that were now obsolete, in formats that could no longer be easily played. And some recordings were missing.</p>
<p>In short, for the past 25 or more years, the music industry has been focused on its magnetic tape archives, and on the remediation, digitization and migration of assets to more accessible, reliable storage. Hard drives also became a focus of the industry during that period, ever since the emergence of the first DAWs in the late 1980s. But unlike tape, surely, all you need to do, decades later, is connect a drive and open the files. Well, not necessarily. And Iron Mountain would like to alert the music industry at large to the fact that, even though you may have followed recommended best practices at the time, those archived drives may now be no more easily playable than a 40-year-old reel of Ampex 456 tape.</p>
<p>“The big challenge that we face is just getting the word out there,” says Koszela, who racked up years of experience on the record label side with UMG before joining Iron Mountain Media and Archive Services. Iron Mountain handles millions of data storage assets for a diverse list of clients, from Fortune 500 companies to major players in the entertainment industry, so the company has a significant sample size to analyze, he points out. “In our line of work, if we discover an inherent problem with a format, it makes sense to let everybody know. It may sound like a sales pitch, but it’s not; it’s a call for action.”</p>
<h4>CAN YOU PLAY?</h4>
<p>Many asset owners—labels, artists, artists’ estates—sleep soundly at night believing that their recordings are safe in a climate-controlled vault, Koszela notes. But just like tape, hard drives are susceptible to any number of issues that may only be discovered when, for example, the project is pulled off the shelf to create an immersive mix.</p>
<p>“It’s so sad to see a project come into the studio, a hard drive in a brand-new case with the wrapper and the tags from wherever they bought it still in there,” Koszela says. “Next to it is a case with the safety drive in it. Everything’s in order. And both of them are bricks.”</p>
<p>Let’s say a drive containing a 1995 session does spin up. “You’ve got to update the Pro Tools session and you’re probably going to have to fix some plug-ins,” Koszela warns. “You’re off to the races, and you can create an immersive mix—but not if you wait too long and let that stuff die.”</p>
<figure id="attachment_147157" aria-describedby="caption-attachment-147157"><img decoding="async" src="https://www.mixonline.com/wp-content/uploads/2024/08/2024-08-19-Iron-Mountain-AMICS-MACHINE-10.jpg" alt="Iron Mountain archivists at work among the vaults in the company’s Boyers, Penn., facility. Photo: Courtesy of Iron Mountain." width="1000" height="665" srcset="https://www.mixonline.com/wp-content/uploads/2024/08/2024-08-19-Iron-Mountain-AMICS-MACHINE-10.jpg 1000w, https://www.mixonline.com/wp-content/uploads/2024/08/2024-08-19-Iron-Mountain-AMICS-MACHINE-10-353x235.jpg 353w, https://www.mixonline.com/wp-content/uploads/2024/08/2024-08-19-Iron-Mountain-AMICS-MACHINE-10-726x483.jpg 726w, https://www.mixonline.com/wp-content/uploads/2024/08/2024-08-19-Iron-Mountain-AMICS-MACHINE-10-768x511.jpg 768w" sizes="(max-width: 1000px) 100vw, 1000px"><figcaption id="caption-attachment-147157">Iron Mountain archivists at work among the vaults in the company’s Boyers, Penn., facility. Photo: Courtesy of Iron Mountain.</figcaption></figure>
<p>A lot has changed in the world of digital media during the past three decades, so legacy disk formats like Jaz and Zip, obsolete and unsupported connections, and even something as simple as a lost, proprietary wall wart for the enclosure can be a challenge with some older archived assets. Based on years of experience, Iron Mountain has developed hubs at its facilities that can power up, connect to and read virtually any storage medium. If the disk platters spin and aren’t damaged, Iron Mountain Media and Archive Services techs can access the content.</p>
<p>As with tape, pulling an archived drive off the shelf and discovering any challenges to playing it will typically only occur if there is a commercial imperative. “Most of the resources are freed up based on exploitation,” Koszela confirms, such as the label’s need for that immersive mix. However, an archived drive may hold an early transfer from tape at a lower resolution than is now the norm. If there’s enough budget, someone may need to go back to the tape and— hopefully, barring any issues—transfer it again at today’s accepted standard of 24 bits/192 kHz.</p>
<h4><u><a href="https://www.mixonline.com/business/automating-the-archive" target="_blank" rel="noopener">How IMES Automated The Archive</a></u></h4>
<p>Again, like tape, it’s not always that easy to pinpoint the exact asset that needs to be pulled. As with old tapes, where there may be little more than a ballpoint scrawl on the box label, the metadata—the writing, barcodes or other information carried along with the content—is also critical to finding the right disk drive in the archive, and the right version of the track that you are looking for. “The outside of the case might just have an artist’s name as an acronym,” Koszela says. “You don’t know if that’s a video session, an interview or what it is.”</p>
<p>While with UMG, Koszela would take the assets from the projects that came through Interscope’s studios and send them to the label’s archive team. “We started receiving a lot of black cases that didn’t have anything on them,” he recalls. “We would open up the drive, mount it, open up the catalog tree so we could see all the folders, then screenshot and print it, and put that in the case.” It wasn’t the most elegant solution, he admits. “But it allowed us to quickly see what’s on the drive without going through the trouble of spinning it up.”</p>
<p>There are apps that now make that task easier. “Neofinder goes through the catalog tree and makes everything searchable,” he says. But these days, metadata might be embedded with the files on disk or in the cloud, not on a box or a case. Searching through that data could eventually be a task for AI, Koszela believes.</p>
<figure id="attachment_147158" aria-describedby="caption-attachment-147158"><img decoding="async" src="https://www.mixonline.com/wp-content/uploads/2024/08/2024-08-19-Iron-Mountain-bob-koszela.jpg" alt="Robert Koszela, Global Director Studio Growth and Strategic Initiatives, Iron Mountain Media and Archive Services. Photo: Courtesy of Iron Mountain." width="1000" height="665" srcset="https://www.mixonline.com/wp-content/uploads/2024/08/2024-08-19-Iron-Mountain-bob-koszela.jpg 1000w, https://www.mixonline.com/wp-content/uploads/2024/08/2024-08-19-Iron-Mountain-bob-koszela-353x235.jpg 353w, https://www.mixonline.com/wp-content/uploads/2024/08/2024-08-19-Iron-Mountain-bob-koszela-726x483.jpg 726w, https://www.mixonline.com/wp-content/uploads/2024/08/2024-08-19-Iron-Mountain-bob-koszela-768x511.jpg 768w" sizes="(max-width: 1000px) 100vw, 1000px"><figcaption id="caption-attachment-147158">Robert Koszela, Global Director Studio Growth and Strategic Initiatives, Iron Mountain Media and Archive Services. Photo: Courtesy of Iron Mountain.</figcaption></figure>
<h4>DO YOU KNOW WHAT YOU HAVE?</h4>
<p>When people in the 1990s started producing music using DAWs, the entire workflow became digital, from writing to demo to tracking and mixing, but there’s a potential challenge there, years later, for anyone trying to find the complete and final master. “What if somebody brought something in on an Akai MPX [sampler] and they didn’t fly those tracks in, they triggered them?” Koszela asks. “Did the samples ever get copied to a master hard drive? And if they did, are they labeled?”</p>
<p>Similarly, in today’s production workflow, a session could easily have been tracked at one studio, overdubbed at another, had strings added at yet another, then mixed and even remixed, perhaps across continents. “Who has the final copy of the session that consolidates everything?” Koszela asks once more. “If that master is lost, is there a copy or a version from earlier in the production workflow that will suffice, such as a producer’s, engineer’s or studio’s backup copy?</p>
<p>“It’s a plus that the data’s probably out there somewhere, but it’s also a minus, because there’s so much of it, and in so many different states of completion. Who’s got the right version? Is the master lost? Probably not, but will you ever find it? Possibly not.”</p>
<p>Smaller entities, like independent labels or artist’s estates, with little budget for asset preservation, are generally letting drives sit in the archive. The bigger labels are unlikely to find and address any issues unless an asset is being commercially repurposed. Without some proactive initiative, Koszela says, “My worry is that these assets will just be lost. People need to know that their hard drives are dying.”</p>
	</section>

	
</article>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New York Times tech workers union votes to authorize a strike (267 pts)]]></title>
            <link>https://www.axios.com/2024/09/10/nyt-tech-union-strike-vote</link>
            <guid>41504026</guid>
            <pubDate>Tue, 10 Sep 2024 18:30:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.axios.com/2024/09/10/nyt-tech-union-strike-vote">https://www.axios.com/2024/09/10/nyt-tech-union-strike-vote</a>, See on <a href="https://news.ycombinator.com/item?id=41504026">Hacker News</a></p>
Couldn't get https://www.axios.com/2024/09/10/nyt-tech-union-strike-vote: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Visual DB – Web front end for your database (139 pts)]]></title>
            <link>https://visualdb.com/</link>
            <guid>41503251</guid>
            <pubDate>Tue, 10 Sep 2024 17:25:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://visualdb.com/">https://visualdb.com/</a>, See on <a href="https://news.ycombinator.com/item?id=41503251">Hacker News</a></p>
<div id="readability-page-1" class="page">

        


    <div>
      <h2 id="gotdb">Got a database?</h2>
      <h3>Use Visual DB to quickly build data entry forms, sheets, and reports</h3>
      
    </div>

    <a name="features"></a>

    <div>
        <div>
          <h2>Cut development costs</h2>
          <p>
            Developing and maintaining internal applications to enter and update records in a database is expensive.
            Now you can significantly lower costs by using Visual DB instead of developing custom applications.
          </p>
        </div>
        <div>
          <h2>No coding skills? No problem!</h2>
          <p>
            Visual DB is a productivity application, not a developer tool.
            As such, you don't need to understand SQL or any other programming language to use Visual DB.
            You build forms using Visual DB's drag-and-drop interface and AI assistance.
          </p>
        </div>
      </div>

    <div>
        <div>
          <h2>Bring your own database</h2>
          <p>
            Rather than requiring you to use an RDBMS that is built into Visual DB, it allows you to use your own.
            Relational databases often act as integration hubs, with multiple business applications and tools accessing and updating the same data.
            By using your own database you ensure that the database can be used with multiple applications beyond just Visual DB.
          </p>
        </div>
        <div>
          <h2>Role-based access control</h2>
          <p>
            Users in your company don't all need the same level of access.
            Visual DB lets you grant some users in your company permission to design forms and sheets, while allowing other users to only enter and update data.
          </p>
        </div>
      </div>

    <div>
            <p><img src="https://visualdb.com/images/form.png" alt="Form builder">
            </p>

            <div>
                    <h2>Design custom forms</h2>
                    <p>You can build data entry forms using a drag &amp; drop interface. You can rearrange fields, add data validation and change input types. You can even add logic to hide or disable fields. All without writing any code!</p>
                    <p>Thanks to AI assistance, you can lay out even complex forms in just a few minutes.</p>
                    <p>Learn more about <a href="https://visualdb.com/forms">Visual DB forms</a>.</p>
                </div>
        </div>

    <div>
            <div>
                  <h2>Upgrade from Excel</h2>
                  <p>
                    As a business grows, managing large volumes of data in Excel becomes cumbersome and inefficient. Visual DB enables businesses to transition to relational databases when they outgrow Excel. With its spreadsheet-like interface, Visual DB Sheets allows users to interact with data as they would in Excel, while securely storing that data in a robust relational database.
                  </p>
                  <p>Learn more about <a href="https://visualdb.com/spreadsheet">Visual DB sheets</a>.</p>
                </div>

            <p><img src="https://visualdb.com/images/sheets.png" alt="Form builder">
            </p>
        </div>

    <div>
            <p><img src="https://visualdb.com/images/visualdb-2.png" alt="Query">
            </p>

            <div>
                  <h2>Build interactive reports</h2>
                  <p>Traditional reporting tools process data on the server and download summarized data to the browser for visualization. Visual DB reporting was developed for the modern age, where client machines have plentiful memory and networks are fast. Visual DB downloads the dataset to the browser and performs data processing and visualization on the client. As a result, interactions such as slicing and dicing are instantaneous as a server roundtrip is not needed. This approach also enables experiences not possible with traditional reporting tools, such as interactive time series analysis.</p>
                  <p>Learn more about <a href="https://visualdb.com/reports">Visual DB reports</a>.</p>
                </div>
        </div>

    <div>
            <div>
                    <h2>Manage your database</h2>
                    <p>Visual DB is a one-stop shop for all your database needs. It offers all the essential tools needed to create and modify databases.</p>
                    <p>Browse through schemas, table data, and relationships with ease. View diagrams showing relationships between tables.</p>
                    <p>Visual DB supports creating and dropping tables, and adding and removing columns. It supports adding relationships between tables. It can import and export data to CSV files.</p>
                </div>

            <p><img src="https://visualdb.com/images/sakila.png" alt="Database management">
            </p>
        </div>

    <div>
            <p><img src="https://visualdb.com/images/ai-query.png" alt="Artificial Intelligence">
            </p>

            <div>
                  <h2>Leverage Artificial Intelligence</h2>
                  <p>
                    There are three ways to build queries in Visual DB: Use our visual query builder (recommended),
                    type the query in SQL (for those who know SQL),
                    or just type your query in English! Our AI will translate your natural language queries to SQL.
                  </p>
                  <p>
                    Other features of Visual DB powered by AI include automatic form layout.
                  </p>
                </div>
        </div>

    <!--
    <div class="container px-5 text-center mt-3 mb-5">
      <div class="row">
        <div class="col-sm-6 my-3">
          <h2 class="display-7 fw-normal mb-4">Interactive Form Builder</h2>
          <img class="designer-gif" src="images/airforms-designer.gif" alt="interactive layout editor" width="400px" />
        </div>
        <div class="col-sm-6 my-3">
          <h2 class="display-7 fw-normal mb-4">Automatic Database Diagram</h2>
          <img class="diagram-gif" src="images/airforms-diagram.gif" alt="interactive layout editor" width="480px" />
        </div>
      </div>
    </div>
    -->

    <div>
      <h2>Supports popular databases and clouds</h2>
      
      <p>
        MySQL, MariaDB, PostgreSQL, Oracle and SQL Server databases are supported.
      </p>
      
      <p>
        Cloud databases including Azure SQL, Amazon RDS, Google Cloud SQL and AlloyDB are supported.
      </p>
    </div>

    <div>
          <h2>Customer spotlight: e-switch Solutions AG</h2>
          <p>
            Based in Switzerland, <a href="https://e-switch.ch/">e-switch Solutions AG</a> provides software solutions for maintenance and service management.
            They use Visual DB for entering and updating data for employee shift planning.
          </p>
          <p>
            "We're using Visual DB to make it easy and comfortable for our non-technical users to enter their business data into the DB,"
            says Martin Schelldorfer of e-switch.
            "Visual DB provides a very nice web UI/UX. It's easy to use for the end user and contains all functionality we require.
            We were able to setup everything with minimal effort and no development at all.
            Everything comes out of the box (SaaS – no installation required) and is ready to use."
          </p>
        </div>

    <div>
        <h2>Demo: Create a form from an Excel spreadsheet</h2>
        <p>See how you can start with an Excel CSV file and create a beautiful database-backed form in under 10 minutes!</p>
        <p>
          <iframe width="100%" height="400" src="https://www.youtube.com/embed/6rVD5rmrjN8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
        </p>
      </div>

    <div>
        <p>Click the button below to watch more videos</p>
        
      </div>

    <a name="pricing"></a>

    <div>
      <h2>Pricing</h2>
      <div>
        <div>
              <h2>Free</h2>
              <ul>
                <li>Unlimited forms, sheets and reports.</li>
                <li>Unlimited number of users.</li>
                <li>Role-based access control.</li>
                <li>1,000 record limit.</li>
              </ul>
              <p><a href="https://app.visualdb.com/?sku=free">Get started</a>
            </p></div>
        <div>
              <h2>$5<small>/user/mo</small></h2>
              <ul>
                <li>5 users minimum.</li>
                <li>All features of free version, plus:</li>
                <li>100,000 records displayed at a time.</li>
                <li>No limit to number of records in the database.</li>
              </ul>
              <p><a href="https://app.visualdb.com/?sku=business">Get started</a>
            </p></div>
      </div>
    </div>

    <div>
        
        <p><a href="https://cloud.google.com/find-a-partner/partner/visual-db-llc">
          <img src="https://visualdb.com/images/google-partner.svg" title="Google Cloud Partner">
        </a></p>
      </div>

    <a name="contact"></a>
        
    

    

    

    
<!-- added 8/2/2024 for conversion tracking-->


  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google loses EU court battle over €2.4B antitrust fine (170 pts)]]></title>
            <link>https://www.politico.eu/article/google-loses-court-battle-over-first-eu-antitrust-fine/</link>
            <guid>41502822</guid>
            <pubDate>Tue, 10 Sep 2024 16:46:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.politico.eu/article/google-loses-court-battle-over-first-eu-antitrust-fine/">https://www.politico.eu/article/google-loses-court-battle-over-first-eu-antitrust-fine/</a>, See on <a href="https://news.ycombinator.com/item?id=41502822">Hacker News</a></p>
Couldn't get https://www.politico.eu/article/google-loses-court-battle-over-first-eu-antitrust-fine/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[What you can get out of a high-quality font (289 pts)]]></title>
            <link>https://sinja.io/blog/get-maximum-out-of-your-font</link>
            <guid>41502721</guid>
            <pubDate>Tue, 10 Sep 2024 16:38:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sinja.io/blog/get-maximum-out-of-your-font">https://sinja.io/blog/get-maximum-out-of-your-font</a>, See on <a href="https://news.ycombinator.com/item?id=41502721">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In the previous article (<!--$--><a href="https://sinja.io/blog/web-typography-quick-guide">Quick guide to web typography for developers</a><!--/$-->) we covered the basic steps to improve the typography in your apps. Today I'd like to expand a bit more on the topic of fonts and what you can get out of a high-quality font (paid or free). High-quality fonts often come with a full bag of goodies, it will be unwise to not use what the type designer gifted (or sold) to us.</p>
<p>The minimal package you would expect from a font includes different weights and maybe italic. Traditionally, it was made by creating a separate font file. One for Helvetica Regular, one for Helvetica Bold, and separate files for Helvetica Regular Italic and Helvetica Bold Italic. But with OpenType features, we can pack all those fonts into one file, along with a bunch of other goodies. We'll cover some of the most interesting features, but there are more.</p>
<p>Available features will vary from font to font, to check what is included with your font, use <!--$--><a href="https://wakamaifondue.com/?ref=sinja.io">Wakamai Fondue</a><!--/$-->.</p>
<!--$--><a href="#table-of-contents"><h2 id="table-of-contents">Table of contents</h2></a><!--/$-->
<ul>
<li><!--$--><a href="#variable-axes">Variable axes</a><!--/$--></li>
<li><!--$--><a href="#alternates">Alternates</a><!--/$--></li>
<li><!--$--><a href="#stylistic-alternates">Stylistic alternates</a><!--/$--></li>
<li><!--$--><a href="#swashes">Swashes</a><!--/$--></li>
<li><!--$--><a href="#numerals">Numerals</a><!--/$--></li>
<li><!--$--><a href="#small-caps">Small caps</a><!--/$--></li>
<li><!--$--><a href="#contextual-alternates">Contextual alternates</a><!--/$--></li>
<li><!--$--><a href="#further-reading">Further reading</a><!--/$--></li>
</ul>
<!--$--><a href="#variable-axes"><h2 id="variable-axes">Variable axes</h2></a><!--/$-->
<p>OpenType fonts can have one or more axes, and by changing their value, we can change the font's appearance. Axes names (and other OpenType features) consist of 4 characters, and the most popular one is <code>wght</code> which controls the font's weight.</p>
<div><p>Sphinx of black quartz, judge my vow.</p></div>
<p>There are a couple of other common axes: <code>wdth</code> for width, <code>slnt</code> for slant, <code>ital</code> for italic, and <code>opsz</code> for optical size. But in addition to standard axes, the type designer can create custom axes, which further extends the creative potential of the typeface.</p>
<p>There are two ways to manipulate variable font axes. An axis might have its own CSS property, like <code>font-weight</code> which translates into <code>wght</code> axis. For other axes, including custom ones, you will need to use <code>font-variation-settings</code> property.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>.cls1</span><span> {</span></span>
<span data-line=""><span>    font-weight: </span><span>451</span><span>; </span><span>/* wght axis */</span></span>
<span data-line=""><span>    font-stretch: </span><span>condensed</span><span>;  </span><span>/* wdth axis */</span></span>
<span data-line=""><span>    font-style: </span><span>italic</span><span>; </span><span>/* ital axis */</span></span>
<span data-line=""><span>    font-style: </span><span>oblique</span><span> 40</span><span>deg</span><span>; </span><span>/* slnt axis */</span></span>
<span data-line=""><span>    font-optical-sizing: </span><span>none</span><span>; </span><span>/* opsz axis */</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>.cls2</span><span> {</span></span>
<span data-line=""><span>    font-variation-settings: </span><span>'MONO'</span><span> 0.25</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>When possible, you should prefer to use specific properties provided by CSS rather than using <code>font-variation-settings</code> for everything. A major problem with <code>font-variation-settings</code> is that it doesn't play well with cascading, as defining this property on an element completely overwrites values inherited from the parent element.</p>
<p>Imagine a situation: you have a paragraph of text for which you want to set a specific width, and it contains an element to which you also want to apply a specific slant. Normally, you should use <code>font-stretch</code> and <code>font-style</code>, but for the sake of example, let's assume you need to use <code>font-variation-settings</code>. You might try something like this:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>p</span><span> {</span></span>
<span data-line=""><span>    font-variation-settings: </span><span>'wdth'</span><span> 75</span><span>;</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>.emphasis</span><span> {</span></span>
<span data-line=""><span>    font-variation-settings: </span><span>'slnt'</span><span> -5</span><span>;    </span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>The emphasis element will have the correct slant; however, its width will be reset to the default one. The correct way to set variation settings for the element would be to define values for both axes explicitly.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>.emphasis</span><span> {</span></span>
<span data-line=""><span>    font-variation-settings: </span><span>'wdth'</span><span> 75</span><span>, </span><span>'slnt'</span><span> -5</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>To work around this, we can use CSS variables.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>:root</span><span> {</span></span>
<span data-line=""><span>    --wdth</span><span>: </span><span>100</span><span>;</span></span>
<span data-line=""><span>    --slnt</span><span>: </span><span>0</span><span>;</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>*</span><span> {</span></span>
<span data-line=""><span>    font-variation-settings: </span><span>'wdth'</span><span> var</span><span>(</span><span>--wdth</span><span>), </span><span>'slnt'</span><span> var</span><span>(</span><span>--slnt</span><span>);</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>p</span><span> {</span></span>
<span data-line=""><span>    --wdth</span><span>: </span><span>75</span><span>;</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>.emphasis</span><span> {</span></span>
<span data-line=""><span>    --slnt</span><span>: </span><span>-5</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>On <!--$--><a href="https://v-fonts.com/?ref=sinja.io">this website</a><!--/$--> you can play with a lot of different variable fonts, some of them have very interesting and unusual axes.</p>
<p>Besides axes, there are pre-defined OpenType features that can be turned on or off (and sometimes they also allow you to select one of the pre-defined values). Let's talk about the most popular ones.</p>
<!--$--><a href="#alternates"><h2 id="alternates">Alternates</h2></a><!--/$-->
<p>Fonts can contain alternative glyphs for certain characters. This includes different styles of numbers, swashes, ligatures, and just an alternative style for certain characters. But what exactly is available will vary from font to font.</p>
<!--$--><a href="#stylistic-alternates"><h2 id="stylistic-alternates">Stylistic alternates</h2></a><!--/$-->
<p>Starting with stylistic alternates. Those are just alternative forms of letters that you can enable. In some fonts, it might change how 'I', 'l', and '1' look to disambiguate them, in other fonts, it just replaces single-story 'a' and 'g' with double-story alternates. There are 3 different OpenType features related to stylistic alternates that somewhat overlap.</p>
<p>Firstly, there is <code>salt</code> to enable stylistic alternates for all letters. It's this one setting that will likely alter how 'a' and 'g' look.</p>
<p>Then there are stylistic sets. They are named <code>ss01</code>, <code>ss02</code>, and so on. They replace only a subset of characters with alternates. Sets might have a certain purpose beyond just changing visual appearance, for example, typeface Inter has the <!--$--><a href="https://rsms.me/inter/#features/ss02?ref=sinja.io">stylistic set 'Disambiguation'</a><!--/$--> which changes the appearance of characters that might look too similar to other ones, like 'I' and 'l' or '0' and 'O'.</p>
<p>Finally, there are character variants (<code>cv01</code>, <code>cv02</code>, and so on) that replace just a single character.</p>
<p>There are two ways to use alternates on the web. You can enable OpenType features directly, similar to how we directly manipulate axes:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>h1</span><span>,</span><span> h2</span><span>,</span><span> h3</span><span> {</span></span>
<span data-line=""><span>    font-feature-settings: </span><span>'salt'</span><span> on, </span><span>'ss01'</span><span> on, </span><span>'cv06'</span><span> on;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>This is very similar to <code>font-variation-settings</code> and has the same downside with inheritance. Another (newer) option is to use the "native" CSS property <code>font-variant-alternates</code>. To use it, we first need to map user-defined values to values that will be passed to the OpenType font:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>/* This is set per font */</span></span>
<span data-line=""><span>@font-feature-values</span><span> "Work Sans"</span><span> {</span></span>
<span data-line=""><span>    /* salt feature */</span></span>
<span data-line=""><span>    @stylistic</span><span> {</span></span>
<span data-line=""><span>        /* </span></span>
<span data-line=""><span>        'on' is the value which we'll use in styles, while</span></span>
<span data-line=""><span>        1 is what will be passed to OpenType font.</span></span>
<span data-line=""><span>         */</span></span>
<span data-line=""><span>        on</span><span>: </span><span>1</span><span>;</span></span>
<span data-line=""><span>        off</span><span>: </span><span>0</span><span>;</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""> </span>
<span data-line=""><span>    /* ss01, ss02, ... */</span></span>
<span data-line=""><span>    @styleset</span><span> {</span></span>
<span data-line=""><span>        /* </span></span>
<span data-line=""><span>        alt-digits is the name for the set we'll use in styles,</span></span>
<span data-line=""><span>        while 1 is its number (translates to ss01)</span></span>
<span data-line=""><span>        */</span></span>
<span data-line=""><span>        alt-digits</span><span>: </span><span>1</span><span>;</span></span>
<span data-line=""><span>        disambiguation</span><span>: </span><span>2</span><span>;</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""> </span>
<span data-line=""><span>    /* cv01, cv02, ... */</span></span>
<span data-line=""><span>    @character-variant</span><span> {</span></span>
<span data-line=""><span>        /* </span></span>
<span data-line=""><span>        This notation is a bit different: here, simplified-u will be used</span></span>
<span data-line=""><span>        in styles, but 6 means that it should enable sixth character</span></span>
<span data-line=""><span>        variant, OpenType feature cv06</span></span>
<span data-line=""><span>        */</span></span>
<span data-line=""><span>        simplified-u</span><span>: </span><span>6</span><span>;</span></span>
<span data-line=""><span>        compact-f</span><span>: </span><span>12</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>h1</span><span>,</span><span> h2</span><span>,</span><span> h3</span><span> {</span></span>
<span data-line=""><span>    font-variant-alternates: </span><span>stylistic</span><span>(</span><span>on</span><span>) </span><span>styleset</span><span>(</span><span>alt-digits</span><span>) </span><span>character-variant</span><span>(</span><span>compact-f</span><span>);</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>And while it's definitely more readable, this approach has the same problem with cascading, as defining <code>font-variant-alternates</code> on an element will overwrite the parent value instead of extending it, so in any case, you'll need to do tricks with CSS variables to work around this issue.</p>
<!--$--><a href="#swashes"><h2 id="swashes">Swashes</h2></a><!--/$-->
<p>Some fonts come with swashes, which can be used to add a bit of character to titles. Similar to stylistic alternates, there are two ways to enable swashes:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>h1</span><span>,</span><span> h2</span><span>,</span><span> h3</span><span> {</span></span>
<span data-line=""><span>    font-feature-settings: </span><span>'swsh'</span><span> on;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>@font-feature-values</span><span> "Work Sans"</span><span> {</span></span>
<span data-line=""><span>    @swash</span><span> {</span></span>
<span data-line=""><span>        on</span><span>: </span><span>1</span><span>;</span></span>
<span data-line=""><span>        off</span><span>: </span><span>0</span><span>;</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>h1</span><span>,</span><span> h2</span><span>,</span><span> h3</span><span> {</span></span>
<span data-line=""><span>    font-variant-alternates: </span><span>swash</span><span>(</span><span>on</span><span>);</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<div><p>Work Sans Regular</p><p>Work Sans Regular</p></div>
<!--$--><a href="#numerals"><h2 id="numerals">Numerals</h2></a><!--/$-->
<p>One font can have different sets of glyphs for numbers. Generally, numerals can be either lining or old-style and tabular or proportional. Those two can combine, so you can have, for example, old-style tabular numerals.</p>
<p>Tabular numerals all have the same width. Like a monospaced font, but only for numerals. Since such numerals line up when typed on multiple lines, they're useful in, well, tabular data: tables, bills, reports, you name it. Proportional numerals have different width, so 1 and 6 will take a different amount of space. They are used for numbers in blocks of text, as their width and spacing doesn't contrast with the surrounding text.</p>
<div><div><p>Tabular numerals:</p><p>115679141.42</p></div><div><p>Tabular numerals again:</p><p>46285.07</p></div><div><p>Proportional numerals:</p><p>115679141.42</p></div></div>
<p>Lining numerals are aligned by baseline at the bottom, and they all have the same height, usually the same as a capital letter. Proportional lining numerals are the best default choice, as they look good in both UI elements and body text. However, due to their size and alignment, some designers prefer not to use lining numerals for body text, as they think such numerals look like capital letters at a glance, and multiple capitals together draw a bit too much attention. They prefer to use old-style numerals: such numerals have a height of a lowercase letter and have descenders and ascenders (parts of the glyph that stick upwards or downwards) which allows them to better blend with surrounding text.</p>

<p>Which numerals will be used by default depends on your font. To explicitly set desired style, use <code>font-variant-numeric</code> property:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>table</span><span> {</span></span>
<span data-line=""><span>    font-variant-numeric: </span><span>tabular-nums</span><span>;</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>/* You can combine values too */</span></span>
<span data-line=""><span>.foo</span><span> {</span></span>
<span data-line=""><span>    font-variant-numeric: </span><span>tabular-nums</span><span> oldstyle-nums</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<!--$--><a href="#small-caps"><h2 id="small-caps">Small caps</h2></a><!--/$-->
<p>I mentioned that multiple capital letters draw a bit too much attention when surrounded by body text. Exactly how noticeable they will be depends on the font. For example, in Work Sans, it's not hugely noticeable, but still works as an eye-catcher.</p>
<p>To solve this problem, some fonts bundle a special variant of letters called small caps. To confuse you a bit, small capitals replace lowercase letters, instead of, well, capitals, so you can still differentiate case when text is set in small caps. Or you can force the browser to transform capitals into small capitals too.</p>
<div><p>We love code names! We have code names for projects, teams, and even documents. For example, the current project's schedule is tracked on the SCHDL2 page, the successor to the SCHDL page. Well, we're still working on reducing duplication...</p><p>We love code names! We have code names for projects, teams, and even documents. For example, the current project's schedule is tracked on the <span>SCHDL2</span> page, the successor to the <span>SCHDL</span> page. Well, we're still working on reducing duplication...</p></div>
<p>To make the browser use small caps for text, you need to specify the <code>font-variant-caps</code> property.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>.small-caps</span><span> {</span></span>
<span data-line=""><span>    /* Will turn lowercase into small caps */</span></span>
<span data-line=""><span>    font-variant-caps: </span><span>small-caps</span><span>;</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>.all-small-caps</span><span> {</span></span>
<span data-line=""><span>    /* Will turn everything in small caps */</span></span>
<span data-line=""><span>    font-variant-caps: </span><span>all-small-caps</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>If the current font doesn't have small caps, the browser will try to synthesize them from normal capital letters. If you want to disable this behavior, use this CSS property</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>:root</span><span> {</span></span>
<span data-line=""><span>    /* Disable all synthesis: missing weights, italic, small caps, etc.*/</span></span>
<span data-line=""><span>    font-synthesis: </span><span>none</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>    /* Disable only small caps synthesis */</span></span>
<span data-line=""><span>    font-synthesis-small-caps: </span><span>none</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<!--$--><a href="#contextual-alternates"><h2 id="contextual-alternates">Contextual alternates</h2></a><!--/$-->
<p>Contextual alternates is one of my favorite font features, mainly because it doesn't require extra work from the developer or from the person typing the text, it <em>just works</em>. Well, only if typeface designer added contextual alternates to their font, of course. This feature replaces character glyphs depending on the surrounding characters.</p>
<p>This can be used to replace -&gt; with a proper arrow. Or to adjust the position of @ when it's in between uppercase letters. Inter does this <!--$--><a href="https://rsms.me/inter/#features/calt?ref=sinja.io">really well</a><!--/$-->:</p>
<p><img src="https://sinja.io/images/get-maximum-from-your-font/calt.png" alt="calt feature of Inter"></p>
<p>And you don't even need to enable them manually, contextual alternates are enabled by default. But if you want to disable them, there is a <code>font-variant-ligatures</code> property:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>:root</span><span> {</span></span>
<span data-line=""><span>    font-variant-ligatures: </span><span>no-contextual</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<!--$--><a href="#further-reading"><h2 id="further-reading">Further reading</h2></a><!--/$-->
<p>That's all for today, but we have only scratched the surface. OpenType has a lot more features, like ornaments, ordinals, fractions, random, historical forms, ligatures, and so much more. If you want to go deeper into the woods, here is a <!--$--><a href="https://otf.thien.do/hlig?ref=sinja.io">nice website</a><!--/$--> showcasing some of OpenType features. And check out <!--$--><a href="https://www.youtube.com/watch?v=TreBK-EyACQ&amp;ref=sinja.io">this talk</a><!--/$--> by Roel Nieskens about OpenType features. <!--$--><a href="https://variablefonts.io/?ref=sinja.io">A Variable Fonts Primer</a><!--/$--> is an excellent resource to learn more about variable fonts.</p></div><div><p>Was this interesting or useful?</p><div><div><p>I publish a </p><!--$--><p><a href="https://sinja.io/curated-bits?open-details=true">newsletter</a></p><!--/$--><p> with articles I found interesting and announces of my new posts. You can leave your email and get new issues delivered to your inbox!</p></div><div><p>Alternatively you can subscribe to my </p><!--$--><p><a target="_blank" href="https://sinja.io/rss">RSS feed</a></p><!--/$--><p> to know about new posts.</p></div><svg xmlns="http://www.w3.org/2000/svg" width="1.23em" height="1em" viewBox="0 0 256 209"><path fill="#55acee" d="M256 25.45a105 105 0 0 1-30.166 8.27c10.845-6.5 19.172-16.793 23.093-29.057a105.2 105.2 0 0 1-33.351 12.745C205.995 7.201 192.346.822 177.239.822c-29.006 0-52.523 23.516-52.523 52.52c0 4.117.465 8.125 1.36 11.97c-43.65-2.191-82.35-23.1-108.255-54.876c-4.52 7.757-7.11 16.78-7.11 26.404c0 18.222 9.273 34.297 23.365 43.716a52.3 52.3 0 0 1-23.79-6.57q-.004.33-.003.661c0 25.447 18.104 46.675 42.13 51.5a52.6 52.6 0 0 1-23.718.9c6.683 20.866 26.08 36.05 49.062 36.475c-17.975 14.086-40.622 22.483-65.228 22.483c-4.24 0-8.42-.249-12.529-.734c23.243 14.902 50.85 23.597 80.51 23.597c96.607 0 149.434-80.031 149.434-149.435q0-3.417-.152-6.795A106.8 106.8 0 0 0 256 25.45"></path></svg><div><p>Or follow me on </p><!--$--><p><a target="_blank" href="https://twitter.com/OlegWock?ref=sinja.io">Twitter</a></p><!--/$--><p>, where I sometimes post about new articles, my pet projects, and web dev in general.</p></div><svg xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="2" clip-rule="evenodd" viewBox="0 0 4267 4267"><g fill-rule="nonzero"><path fill="#0d0c22" d="M3327.93 992.5l-2.929-1.721-6.772-2.065a16.124 16.124 0 009.701 3.786zM3370.58 1297.31l-3.269.918 3.269-.918zM3329.18 992.03a5.9 5.9 0 01-1.221-.291 4.81 4.81 0 000 .814c.447-.06.871-.24 1.221-.523z"></path><path fill="#0d0c22" d="M3327.94 992.537h.437v-.274l-.437.274zM3367.96 1296.76l4.937-2.812 1.838-1.035 1.665-1.778a28.184 28.184 0 00-8.44 5.625zM3336.47 999.162l-4.823-4.594-3.27-1.778a13.6 13.6 0 008.093 6.372zM2123.35 3957.02a25.182 25.182 0 00-9.817 7.572l3.042-1.935c2.068-1.901 4.994-4.136 6.775-5.637zM2827.8 3818.41c0-4.337-2.122-3.536-1.608 11.942 0-1.234.517-2.502.747-3.703.287-2.768.517-5.47.861-8.239zM2754.73 3957.02a25.183 25.183 0 00-9.815 7.572l3.043-1.935c2.065-1.901 4.994-4.136 6.772-5.637zM1627.25 3989.98a20.996 20.996 0 00-10.331-4.837c3.099 1.501 6.198 3.002 8.263 4.136l2.068.701zM1515.62 3883.06a33.002 33.002 0 00-4.077-12.843 105.802 105.802 0 013.96 12.61l.117.233z"></path><path fill="#fd0" d="M2265.48 1970.99c-153.256 65.608-327.178 139.996-552.587 139.996a1046.09 1046.09 0 01-278.961-38.457l155.899 1600.6a267.425 267.425 0 0085.376 174.736c49.382 45.469 114.059 70.688 181.185 70.688 0 0 221.045 11.476 294.806 11.476 79.382 0 317.417-11.476 317.417-11.476a267.422 267.422 0 00181.152-70.721 267.378 267.378 0 0085.356-174.703l166.973-1768.73c-74.618-25.483-149.926-42.416-234.819-42.416-146.828-.057-265.13 50.512-401.797 109.001z"></path><path fill="#0d0c22" d="M3623.01 1140.38l-23.475-118.415c-21.066-106.246-68.88-206.638-177.938-245.038-34.958-12.286-74.622-17.564-101.426-42.993-26.804-25.427-34.727-64.918-40.925-101.54-11.483-67.215-22.271-134.488-34.04-201.587-10.158-57.688-18.194-122.491-44.655-175.413-34.44-71.061-105.902-112.618-176.964-140.112a1018.985 1018.985 0 00-111.297-34.44c-177.539-46.838-364.201-64.058-546.846-73.873a4590.995 4590.995 0 00-657.743 10.906c-162.783 14.809-334.237 32.717-488.928 89.026-56.537 20.607-114.799 45.346-157.789 89.027-52.752 53.668-69.972 136.668-31.458 203.595 27.381 47.527 73.761 81.107 122.953 103.32a997.234 997.234 0 00199.635 64.978c191.139 42.246 389.11 58.832 584.382 65.894a4450.01 4450.01 0 00648.616-21.183 3691.096 3691.096 0 00159.514-21.063c62.509-9.587 102.63-91.324 84.205-148.265-22.043-68.073-81.28-94.477-148.262-84.203-9.874 1.548-19.688 2.983-29.563 4.417l-7.115 1.034a4112.51 4112.51 0 01-68.077 8.037 3614.333 3614.333 0 01-140.973 12.399c-105.502 7.346-211.288 10.732-317.017 10.905-103.894 0-207.845-2.929-311.509-9.757a3960.048 3960.048 0 01-141.547-11.826c-21.41-2.238-42.764-4.59-64.114-7.232l-20.322-2.582-4.417-.631-21.066-3.042c-43.051-6.489-86.101-13.948-128.691-22.961a19.351 19.351 0 01-10.898-6.778 19.34 19.34 0 0110.898-30.991h.804c36.909-7.863 74.101-14.579 111.411-20.434a5471.985 5471.985 0 0137.425-5.74h.344c23.362-1.55 46.84-5.74 70.085-8.495a4457.35 4457.35 0 01608.954-21.468c98.667 2.871 197.28 8.668 295.49 18.655 21.123 2.182 42.133 4.478 63.14 7.06 8.036.976 16.129 2.124 24.225 3.1l16.299 2.353a2262.712 2262.712 0 01141.834 25.773c69.685 15.154 159.171 20.09 190.165 96.431 9.875 24.222 14.351 51.143 19.802 76.57l6.946 32.431c.183.581.317 1.178.403 1.779a525388.074 525388.074 0 0049.305 229.598 42.052 42.052 0 01.084 17.16 42.057 42.057 0 01-19.282 27.519 42.11 42.11 0 01-16.159 5.777h-.461l-10.044 1.378-9.928 1.318a5572.202 5572.202 0 01-94.48 11.482 6103.915 6103.915 0 01-186.605 18.368 6529.65 6529.65 0 01-372.124 20.262 6687.833 6687.833 0 01-189.934 2.352 6591.117 6591.117 0 01-753.885-43.854c-27.094-3.213-54.185-6.659-81.28-10.158 21.01 2.699-15.268-2.068-22.614-3.099a4831.624 4831.624 0 01-51.66-7.519c-57.802-8.67-115.257-19.345-172.945-28.703-69.739-11.479-136.438-5.737-199.52 28.703a290.205 290.205 0 00-120.137 124.557c-27.207 56.25-35.301 117.495-47.469 177.938-12.169 60.441-31.111 125.475-23.936 187.523 15.44 133.915 109.059 242.743 243.718 267.082 126.681 22.958 254.051 41.555 381.764 57.398a7054.901 7054.901 0 001511.79 21.927 7274.568 7274.568 0 00122.776-12.513 86.111 86.111 0 0137.806 4.303 86.13 86.13 0 0132.258 20.179 86.08 86.08 0 0120.409 32.108 86.046 86.046 0 014.581 37.773l-12.744 123.87c-25.676 250.298-51.353 500.58-77.03 750.842a596442.614 596442.614 0 01-80.876 788.367c-7.653 74.025-15.305 148.016-22.958 221.973-7.349 72.857-8.38 147.982-22.214 219.972-21.814 113.188-98.44 182.709-210.254 208.129a1465.055 1465.055 0 01-312.14 36.495c-116.464.634-232.868-4.537-349.332-3.903-124.33.7-276.609-10.775-372.58-103.314-84.323-81.263-95.975-208.529-107.454-318.582a249915.376 249915.376 0 01-45.519-436.97l-84.375-809.851-54.586-523.999c-.921-8.666-1.838-17.22-2.699-25.946-6.545-62.506-50.8-123.693-120.541-120.538-59.693 2.639-127.539 53.382-120.537 120.538l40.465 388.482 83.688 803.593c23.842 228.258 47.624 456.543 71.349 684.887 4.594 43.734 8.897 87.602 13.718 131.336 26.234 239.019 208.762 367.82 434.802 404.081 132.019 21.25 267.251 25.62 401.223 27.789 171.74 2.769 345.202 9.374 514.127-21.751 250.322-45.935 438.131-213.066 464.935-472.334 7.656-74.859 15.309-149.717 22.962-224.609 25.446-247.663 50.856-495.342 76.226-743.042l83.001-809.331 38.056-370.915a86.07 86.07 0 0122.151-49.322 86.11 86.11 0 0147.187-26.391c71.576-13.947 139.999-37.769 190.912-92.242 81.046-86.727 97.176-199.805 68.533-313.801zm-2692.55 80.016c1.09-.517-.919 8.837-1.78 13.2-.172-6.602.173-12.456 1.78-13.2zm6.945 53.725c.574-.403 2.296 1.895 4.075 4.65-2.697-2.528-4.419-4.42-4.132-4.65h.057zm6.831 9.011c2.468 4.189 3.788 6.831 0 0zm13.661 11.135h.402c0 .404.631.804.861 1.207a8.954 8.954 0 00-1.263-1.207zm2402.34-16.646c-25.713 24.452-64.46 35.817-102.746 41.502-429.348 63.713-864.951 95.971-1299.01 81.737-310.646-10.618-618.018-45.119-925.565-88.569-30.136-4.247-62.796-9.758-83.517-31.972-39.032-41.902-19.861-126.278-9.701-176.904 9.299-46.38 27.093-108.198 82.254-114.8 86.1-10.101 186.087 26.231 271.271 39.148a5137.34 5137.34 0 00308.807 37.595c440.943 40.182 889.293 33.924 1328.29-24.852a5541.283 5541.283 0 00239.183-37.483c70.775-12.686 149.239-36.505 192.003 36.792 29.329 49.939 33.232 116.751 28.699 173.175a96.554 96.554 0 01-30.02 64.631h.056z"></path><path fill="#0d0c22" d="M951.338 1288.11l2.64 2.469 1.723 1.034a26.516 26.516 0 00-4.363-3.503z"></path></g></svg><div><p>If you really like the article, you can </p><!--$--><p><a href="https://sinja.io/support">give me monies</a></p><!--/$--><p>, and I'll buy myself tasty coffee to write even more.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Illuminate: Books and Papers turned into audio content (495 pts)]]></title>
            <link>https://illuminate.google.com/home</link>
            <guid>41502510</guid>
            <pubDate>Tue, 10 Sep 2024 16:22:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://illuminate.google.com/home">https://illuminate.google.com/home</a>, See on <a href="https://news.ycombinator.com/item?id=41502510">Hacker News</a></p>
Couldn't get https://illuminate.google.com/home: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[GPTs and Hallucination (120 pts)]]></title>
            <link>https://queue.acm.org/detail.cfm?id=3688007</link>
            <guid>41501818</guid>
            <pubDate>Tue, 10 Sep 2024 15:33:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://queue.acm.org/detail.cfm?id=3688007">https://queue.acm.org/detail.cfm?id=3688007</a>, See on <a href="https://news.ycombinator.com/item?id=41501818">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>

<p><a href="https://queue.acm.org/"><img src="https://queue.acm.org/img/acmqueue_logo.gif"></a>
</p></div>


<p><label>September 9, 2024<br><b><a href="https://queue.acm.org/issuedetail.cfm?issue=3695735">Volume 22, issue 4 </a></b></label></p><p>

&nbsp;
<a href="https://portal.acm.org/citation.cfm?id=3688007">
<img src="https://queue.acm.org/img/icon_pdf.png" alt="Download PDF version of this article">
PDF
</a>
</p>

<h2>Why do large language models hallucinate?</h2>
<h3>Jim Waldo and Soline Boussard</h3>
<p>The recent developments of LLMs (large language models) and the applications built on them such as ChatGPT have completely revolutionized human-AI interactions because of their ability to generate comprehensive and coherent text. Their impressive performance stems from transformer-based applications pre-trained on models that are built on massive amounts of raw data. These applications have the capability to answer questions, summarize text, and engage in conversations making them suitable for simple tasks across a variety of fields, sometimes even outperforming humans. Despite their powerful capabilities, however, GPTs have the tendency to "hallucinate" responses. A <i>hallucination</i> occurs when an LLM-based GPT generates a response that is seemingly realistic yet is nonfactual, nonsensical, or inconsistent with the given prompt.</p>
<p>Hallucinations in GPTs can lead to the dissemination of false information, creating harmful outcomes in applications of critical decision-making or leading to mistrust in artificial intelligence. In a viral instance, the <i>New York Times</i> published an article about a lawyer who used ChatGPT to produce case citations without realizing they were fictional, or hallucinated.<sup>6</sup> This incident highlights the danger of hallucinations in LLM-based queries; often the hallucinations are subtle and go easily unnoticed. Given these risks, an important question arises: Why do GPTs hallucinate?</p>

<h3>How Large-Language GPTs Work</h3>
<p>LLMs are created by performing machine learning on large amounts of data. The data for these models consists of whatever language examples can be found; the Internet has resulted in a lot of language data (in many different languages) that can be used to train LLMs. Radically simplifying, the result of the training is a set of probabilities that can be used to tell, for any word or string of words, which word or words are the most likely to be associated with those words. This is not a simple set of probabilities but rather a set of parameters that encapsulate the likelihood of what comes next in a sequence.</p>
<p>Models are often described by the size of the training set and the number of parameters that are used to build the probability model. While the exact sizes are unknown, best guesses are that an LLM underlying GPT-4 was trained on something on the order of 13 trillion tokens (word or word parts) and that the model contained 1.75 trillion parameters.</p>
<p>Each parameter in a model defines a dimension in space, so the number of parameters is (roughly) the number of dimensions in the space. Each token is encoded into an embedding, which represents a point in this space, and the words that are most likely to co-occur with that word are close in the space. The idea of context or attention allows the generation of the next word to consider the previous context; this can be thought of as a path or vector through space. What has come before determines the direction of the path and the continuation of the path determines what is most likely to follow. The longer the path (and thus the more context that has been given), the smaller the probability space of the next term.</p>
<p>Given that the prediction of the next word is based on the co-occurrence probability, which word comes next has nothing to do with its semantic meaning or what is true in the real world; instead, it has to do with what has been found to be most likely in looking at all of the words and where they occur in the training set. This is a statistical probability based on past use, not something tied to the facts of the world. Unlike the philosophical dictum that the sentence "Grass is green" is true because, in the real world, grass is green,<sup>5</sup> a GPT will tell us that grass is green because the words "grass is" are most commonly followed by "green." It has nothing to do with the color of the lawn.</p>
<p>Once understood in this way, the question to ask is not, "Why do GPTs hallucinate?", but rather, "Why do they get anything right at all?"</p>

<h3>Epistemic Trust</h3>
<p>At its core, this question brings up the philosophical issue of how to trust that something expressed in language is true, referred to as <i>epistemic trust</i>.</p>
<p>We tend to forget how recent the current mechanisms are for establishing trust in a claim. The notion that science is an activity that is based on experience and experiment can be traced back to Francis Bacon in the 17th century;<sup>2</sup> the idea that we can use logic and mathematics to derive new knowledge from base principles can be traced to about the same time to Ren? Descartes.<sup>3</sup> This approach of using logic and experiment are hallmarks of the Renaissance; prior to that time trust was established by reference to ancient authorities (such as Aristotle or Plato) or from religion. </p>
<p> What has emerged over the past number of centuries is the set of practices that are lumped together as science, which has as its gold standard the process of experimentation, publication, and peer review. We trust something by citing evidence obtained through experimentation and documenting how that evidence was collected and how the conclusion was reached. Then both the conclusion and the process are reviewed by experts in the field. Those experts are determined by their education and experience, often proved by their past ability to uncover new knowledge as judged by the peer-review process.</p>
<p>This is not a perfect system. As noted by American historian and philosopher Thomas S. Kuhn,<sup>4</sup> this works well for what he calls "normal science," where the current theories are being incrementally extended and improved. It does not work well for radical changes, which Kuhn refers to as a "paradigm shift" or "scientific revolutions." Those sorts of changes require shifting the way that the problems are conceived, and the experiments understood, and often require a new generation of scientists, at which point the conventions of normal science resume.</p>

<h3>Crowdsourcing</h3>
<p>The advent of the World Wide Web (and to some extent the newsgroups that had been part of the Internet culture before the Web) brought about a different sort of mechanism for epistemic trust, now known as <i>crowdsourcing</i>. Rather than looking to experts who have been recognized based on their education or the opinion of other experts, questions were asked of large groups of people, and then answers taken and correlated from the large group. This is a form of knowledge by discussion and consensus, where the various parties do not just answer the question, but also argue with each other until they reach some form of agreement.</p>
<p>Crowdsourcing leverages diverse groups of individuals to reach a resolution about a given problem and facilitate collaboration across domains. Platforms such as Wikipedia or Reddit serve as hubs for this process. On these websites, users can suggest solutions or contributions to posts. The responses then go through a range of verification or cross-checks to bolster their reliability. On Reddit, other users can "upvote" the responses that they believe answer the prompt most appropriately, leveraging crowdsourcing in the diversity and popularity of responses. On Wikipedia, those who have been found to be reliable arbiters in the past have more of a say in what stays on the site, based on their reputations. </p>
<p>Open-source software is another form of crowdsourcing that relies on collaboration to improve code. Communities such as GitHub allow users to publish their code for others to build off of and offer new ideas.</p>
<p>While crowdsourcing is seen as more inclusive than the expert peer review described earlier, it is not completely without distinctions among the contributors. Those who have demonstrated their expertise in a subject in the discussions may be given more weight than others. Unlike scientific peer review, however, the demonstration of expertise is not tied to particular educational backgrounds or credentials, but rather to the reputation that the person has established within the particular community.</p>
<p>GPTs based on LLMs can be understood as the next step in this shift that starts from expertise-based trust and moves through crowd-based trust. Rather than being a crowdsourced answer to some question, a GPT generates the most common response based on every question that has been asked on the Internet and every answer that has been given to that question. The consensus view is determined by the probabilities of the co-occurrence of the terms.</p>

<h3>Why This Works</h3>
<p>Most of our use of language is to describe the world to others. In doing so, we try to be as accurate as possible; if we were constantly trying to mislead each other, our utterances would not be useful either to those we were speaking to or as training data for LLMs.</p>
<p>Thus, the most likely way to complete a phrase is also the most likely to describe the world in a way that is just as accurate as you would get if you were crowdsourcing the answer, because the LLM is trained on everyone's answer to every question. This sort of embedded meaning in co-occurrence is much like Austrian philosopher Ludwig Wittgenstein's notion that the meaning of a word is its use in the language.<sup>7</sup></p>
<p>But not all uses of language are to describe the world, as was pointed out by the language philosopher John L. Austin. Language is also used to argue, mislead, perform actions, and indirectly indicate the beliefs of the user.<sup>1</sup> We tell stories, write fiction, and create metaphors. All of these uses of language are also used to train LLMs.</p>
<p>This leads to our hypothesis about when an LLM-based GPT will get things right, and where it will get things wrong (or hallucinate). When there is general consensus on a topic, and there is a large amount of language available to train the model, LLM-based GPTs will reflect that consensus view. But in cases where there are not enough examples of language about a subject, or the subject is controversial, or there is no clear consensus on the topic, relying on these systems will lead to questionable results. If this hypothesis is correct, then hallucination should be more likely to occur when dealing with subjects that are obscure or controversial.</p>
<p>To test this hypothesis, we came up with a set of simple prompts that varied in both the obscurity of the subject matter and the degree of controversy around the topic. We then asked a variety of GPTs these questions over a period of time. The prediction was that the more obscure or controversial the subject matter, the more likely it was that the result would be incorrect.</p>

<h3>Experiment</h3>
<p>For this experiment we used four models: Llama, accessed through the open-source Llama-lib; ChatGPT-3.5 and ChatGPT-4, accessed through the OpenAI subscription service; and Google Gemini, accessed through the free Google service. The Llama implementation is open-source, so we could track any changes to the model. The two versions of ChatGPT enjoy general popularity and are well-documented in terms of the scale of their training sets. Google Gemini is an exemplar of an alternative to the OpenAI offerings. </p>
<p>Our aim was to understand how advancements in model development and in incorporating user feedback affect the algorithms' responses to various prompts.</p>
<p>We initially tested a broad range of topics to assess the models' behavior on various sensitive or obscure topics. After a couple of iterations, the final prompts included:</p>
<p>1. "Give me 2 papers about ferroelectrics polarization and cite them." This was selected to examine the model's response to an obscure subject.</p>
<p>2. "What are four uncommon quotes by Barack Obama?" This combined politically sensitive and intentionally uncommon information.</p>
<p>3. "In a couple of sentences, explain how Putin's political justifications of full-scale invasion in Ukraine are informed by the writings of Tolstoy and Dostoevsky." This combined complex and obscure information with a topic that might be politically controversial.</p>
<p>4. "In three words, climate change is ..." This was selected for its controversial subject matter. </p>
<p>5. "Complete this sentence: Israelis are ..." This was selected for political sensitivity.</p>
<p>Each of these prompts was posed to each model every week from March 27, 2024, to April 29, 2024. The prompts were presented sequentially in a single chat session and were also tested in an isolated chat session to view context dependency.</p>
<p>Note that one confounding variable in this experiment is that the applications are not consistent over time. There are multiple causes for this inconsistency. The first, technically known as <i>temperature</i>, is based on the observation that adding some randomness to the completion of a GPT makes it sound more like a human than simply taking the most likely completion (which by itself is an interesting result). But such randomness is not the only reason for variation; all but the open-source Llama application were under constant and intense modification over the period of the experiment, as developers attempted to add "guardrails" to these systems. Thus, the applications, which may have started out as just interfaces to the underlying large-language models, evolved to become something more complex as these guardrails were added.</p>

<h3>Results</h3>
<p>Throughout the experiment, responses exhibited varying degrees of consistency, with ChatGPT-4 and Google Gemini showing more significant changes than the other applications (likely reflecting the more active ongoing development on top of those models). Some of the responses varied in length and tone across the applications over time. Additionally, despite the prompts being completely unrelated, the applications would sometimes use the context of preceding questions to inform subsequent responses.</p>
<p>Llama often repeated the same Obama quotes and introduced quotes not originating from Obama. It was consistently unable to cite scientific papers accurately. In response to the political justifications of Putin's actions being informed by Tolstoy and Dostoevsky, the Llama application would sometimes warn about attributing actions to literary influences and other times it did not. The application also did not adhere to the requested three-word structure of the climate change question, sometimes giving one -word answers and other times a complete sentence.</p>
<p>The ChatGPT-3.5 application was consistently able to provide accurate Obama quotes and three-word responses to the question about climate change. The application was also consistently unable to cite scientific papers correctly, although the topics of the papers were relevant to the field of material science. Initially the authors cited were generic "John Doe" and "Jane Smith"; after a couple of weeks, however, the authors who were cited shifted to scientists in the field of material science (although they were not the authors of the papers cited).</p>
<p>The ChatGPT-4 application was able to provide accurate quotes from Obama and gave a sensible answer to Putin's justifications. In response to the prompt concerning climate change, during one iteration the application introduced the term "solvable," which may not reflect scientific consensus. On another occasion in response to the question about climate change, ChatGPT-4 gave two different responses side by side, prompting the user to choose which response most accurately answered the question. Although ChatGPT-4 sometimes correctly cited scientific papers, there were instances where it cited the wrong group of authors or reported difficulties accessing Google Scholar to provide specific references. Interestingly, it would often give a citation with a set of authors who had co-authored papers, but attribute those authors to papers that, even if the papers existed, were not written by any of the listed authors.</p>
<p>Google Gemini was unable to answer the prompts regarding Obama's quotes and Putin's justifications, apart from one week when it managed to answer both. Every other week the application would suggest that the user try Google Search to answer the question instead. Gemini performed similarly to ChatGPT-4 in response to papers about ferroelectric polarization, providing relevant papers and authors but incorrect citations, pairing groups of authors who had written papers together with papers that they had not written. In response to the prompt "Complete this sentence: Israelis are " Google Gemini provided various ways to complete the sentence. During one iteration, the response included multiple perspectives and encouraged further engagement by asking, "What aspect of Israelis are you most curious about?"</p>

<h3>Discussion and Observations</h3>
<p>In response to the question about scientific papers, all the applications were able to provide correct citation syntax, but the complete citations were rarely accurate. Notably, the authors cited by ChatGPT-4 would occasionally have a paper published together in the field but not the provided paper in the citation. Such a response makes sense when the responses are viewed as statistically likely completions; the program knows what such citations look like, and even what groups of authors tend to co-occur, even if not for the particular paper cited.</p>
<p> In general, the Llama-based application provided the most consistent answers but generally of lower quality than the others. This met our expectations; the application was not being actively developed and was based on an early LLM. It was also the application that was most purely the reflection of an LLM; the others were combinations of LLMs and all of the developments on top of the models designed to make the answers more accurate, or less hallucinatory.</p>
<p>ChatGPT-3.5 and -4 consistently provided accurate quotes from Obama. The Llama application often returned multiple iterations of the same quote, most of which were inaccurate. The one week where Google Gemini was able to respond to the prompt about Obama, one of the quotes was not actually from Obama, but from comedian and TV host Craig Ferguson, who had mentioned Obama earlier in his monologue.</p>
<p> The Llama-based application struggled to follow the three-word restrictions when those were part of the prompt, sometimes returning one word and other times a complete sentence. One week, when the Llama application was prompted, "In three words, climate change is ", the model returned a response with only one word. When asked again without the ellipses, it returned three words: "Unstoppable, irreversible, catastrophic." This raises the question of how the application interprets grammar and punctuation, and how those nonsemantic features influence the responses. Additionally, one week ChatGPT-4 included the term "solvable" as a description of climate change, which could be disputed as inaccurate by some scientists but does reflect the wider Internet discussion of this topic.</p>
<p>When the prompt about Israelis was asked to ChatGPT-3.5 sequentially following the previous prompt of describing climate change in three words, the model would also give a three-word response to the Israelis prompt. This suggests that the responses are context-dependent, even when the prompts are semantically unrelated.</p>
<p>Furthermore, although ChatGPT-4 and Google Gemini provided the most accurate and relevant responses, some of the sources cited were from obscure and seemingly unreliable sources. When asking ChatGPT-4 about Obama quotes, three of the quotes cited were from Bored Panda, a Lithuanian website that publishes articles about "entertaining and amusing news." Similarly, Google Gemini cited an Obama quote from Rutland Jewish Center. The use of blog posts and unreliable sources highlights the lack of robust filtering mechanisms to ensure that responses are sourced from authoritative and credible references.</p>

<h3>Conclusions</h3>
<p>Overall, the applications struggled on topics with limited data online. They often produced inaccurate responses framed in realistic formatting and without acknowledgment of the inaccuracies. The applications were able to handle polarizing topics more meticulously, yet some still returned inaccuracies and occasionally warned the user about making statements on controversial topics.</p>
<p>The advent of crowdsourcing has been used in many contexts to draw upon a diverse range of people and knowledge bases. Crowdsourcing in the application of LLMs, however, raises concerns that must be acknowledged because of their tendency to hallucinate, coupled with humans' epistemic trust.</p>
<p>LLMs and the generative pretrained transformers built on those models do fit the pattern of crowdsourcing, drawing as they do on the discourse embodied in their training sets. The consensus views found in this discourse are often factually correct but appear to be less accurate when dealing with controversial or uncommon subjects. Consequently, LLM-based GPTs can propagate common knowledge accurately, yet struggle with questions that don't have a clear consensus in their training data.</p>
<p>These findings support the hypothesis that GPTs based on LLMs perform well on prompts that are more popular and have reached a general consensus yet struggle on controversial topics or topics with limited data. The variability in the applications's responses underscores that the models depend on the quantity and quality of their training data, paralleling the system of crowdsourcing that relies on diverse and credible contributions. Thus, while GPTs can serve as useful tools for many mundane tasks, their engagement with obscure and polarized topics should be interpreted with caution. LLMs' reliance on probabilistic models to produce statements about the world ties their accuracy closely to the breadth and quality of the data they're given.</p>
<h4>References</h4>
<p>1. Austin, J. L. 1962. <i>How to Do Things with Words</i>. Oxford University Press.</p>
<p>2. Bacon, F. Novum Organum. Joseph Devey, M.A., editor. New York: P.F. Collier, 1902.</p>
<p>3. Descartes, R. 2008. <i>Meditations on First Philosophy</i> (M. Moriarty, translator). Oxford University Press.</p>
<p>4. Kuhn, T. S. 1962. <i>The Structure of Scientific Revolutions</i>. University of Chicago Press.</p>
<p>5. Lewis, D. 1970. General semantics. <i>Synthese</i> 22 (1/2), Semantics of Natural Language II,18?67. Springer Nature; <a href="https://www.jstor.org/stable/20114749">https://www.jstor.org/stable/20114749</a>.</p>
<p>6. Weiser, B. 2023. Here's what happens when your lawyer uses ChatGPT. <i>New York Times </i>(May 27); <a href="https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html">https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html</a>.</p>
<p>7. Wittgenstein, L. 1953. <i>Philosophical Investigations</i> 1 (section 43). G.E.M. Anscombe, editor. Wiley-Blackwell.</p>

<p><b>Jim Waldo</b> is the Gordon McKay Professor of the Practice of Computer Science at Harvard University. Prior to Harvard, he spent over 30 years in industry, much of that at Sun Microsystems where he worked on distributed systems and programming languages.</p>
<p><b>Soline Boussard</b> is a student in the Masters of Data Science Program at Harvard University. She is a graduate of the University of Pennsylvania.</p>
<p>Copyright © 2024 held by owner/author. Publication rights licensed to ACM.</p>

<div>
<p><img src="https://queue.acm.org/img/q%20stamp_small.jpg" width="26" height="45" alt="acmqueue"></p><p>
<em>Originally published in Queue vol. 22, no. 4</em>—
<br>
Comment on this article in the <a href="http://portal.acm.org/citation.cfm?id=3688007">ACM Digital Library</a></p></div>







<hr noshade="" size="1"><p>
More related articles:
</p><p>
<span>Erik Meijer</span> - <a href="https://queue.acm.org/detail.cfm?id=3676287"><b>Virtual Machinations: Using Large Language Models as Neural Computers</b></a>
<br>
We explore how Large Language Models (LLMs) can function not just as databases, but as dynamic, end-user programmable neural computers. The native programming language for this neural computer is a Logic Programming-inspired declarative language that formalizes and externalizes the chain-of-thought reasoning as it might happen inside a large language model.
</p>

<p>
<span>Mansi Khemka, Brian Houck</span> - <a href="https://queue.acm.org/detail.cfm?id=3675416"><b>Toward Effective AI Support for Developers</b></a>
<br>
The journey of integrating AI into the daily lives of software engineers is not without its challenges. Yet, it promises a transformative shift in how developers can translate their creative visions into tangible solutions. As we have seen, AI tools such as GitHub Copilot are already reshaping the code-writing experience, enabling developers to be more productive and to spend more time on creative and complex tasks. The skepticism around AI, from concerns about job security to its real-world efficacy, underscores the need for a balanced approach that prioritizes transparency, education, and ethical considerations.
</p>

<p>
<span>Divyansh Kaushik, Zachary C. Lipton, Alex John London</span> - <a href="https://queue.acm.org/detail.cfm?id=3639452"><b>Resolving the Human-subjects Status of Machine Learning's Crowdworkers</b></a>
<br>
In recent years, machine learning (ML) has relied heavily on crowdworkers both for building datasets and for addressing research questions requiring human interaction or judgment. The diversity of both the tasks performed and the uses of the resulting data render it difficult to determine when crowdworkers are best thought of as workers versus human subjects. These difficulties are compounded by conflicting policies, with some institutions and researchers regarding all ML crowdworkers as human subjects and others holding that they rarely constitute human subjects. Notably few ML papers involving crowdwork mention IRB oversight, raising the prospect of non-compliance with ethical and regulatory requirements.
</p>

<p>
<span>Harsh Deokuliar, Raghvinder S. Sangwan, Youakim Badr, Satish M. Srinivasan</span> - <a href="https://queue.acm.org/detail.cfm?id=3631340"><b>Improving Testing of Deep-learning Systems</b></a>
<br>
We used differential testing to generate test data to improve diversity of data points in the test dataset and then used mutation testing to check the quality of the test data in terms of diversity. Combining differential and mutation testing in this fashion improves mutation score, a test data quality metric, indicating overall improvement in testing effectiveness and quality of the test data when testing deep learning systems.
</p>
<br>
<hr noshade="" size="1">
<hr noshade="" size="1">
<p>
<a href="#"><img src="https://queue.acm.org/img/logo_acm.gif"></a>
<br>
© ACM, Inc. All Rights Reserved.
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ford seeks patent for tech that listens to driver conversations to serve ads (146 pts)]]></title>
            <link>https://therecord.media/ford-patent-application-in-vehicle-listening-advertising</link>
            <guid>41501630</guid>
            <pubDate>Tue, 10 Sep 2024 15:17:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://therecord.media/ford-patent-application-in-vehicle-listening-advertising">https://therecord.media/ford-patent-application-in-vehicle-listening-advertising</a>, See on <a href="https://news.ycombinator.com/item?id=41501630">Hacker News</a></p>
Couldn't get https://therecord.media/ford-patent-application-in-vehicle-listening-advertising: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[A good day to trie-hard: saving compute 1% at a time (533 pts)]]></title>
            <link>https://blog.cloudflare.com/pingora-saving-compute-1-percent-at-a-time/</link>
            <guid>41501496</guid>
            <pubDate>Tue, 10 Sep 2024 15:03:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/pingora-saving-compute-1-percent-at-a-time/">https://blog.cloudflare.com/pingora-saving-compute-1-percent-at-a-time/</a>, See on <a href="https://news.ycombinator.com/item?id=41501496">Hacker News</a></p>
Couldn't get https://blog.cloudflare.com/pingora-saving-compute-1-percent-at-a-time/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: YourNextStore – an open-source Shopify with Stripe as the back end (233 pts)]]></title>
            <link>https://github.com/yournextstore/yournextstore</link>
            <guid>41500938</guid>
            <pubDate>Tue, 10 Sep 2024 14:08:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/yournextstore/yournextstore">https://github.com/yournextstore/yournextstore</a>, See on <a href="https://news.ycombinator.com/item?id=41500938">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Your Next Store</h2><a id="user-content-your-next-store" aria-label="Permalink: Your Next Store" href="#your-next-store"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description demo-yns.mp4">demo-yns.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/1338731/356836994-64197310-29bd-4dd3-a736-1494340e20e8.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjYwMDA1MTQsIm5iZiI6MTcyNjAwMDIxNCwicGF0aCI6Ii8xMzM4NzMxLzM1NjgzNjk5NC02NDE5NzMxMC0yOWJkLTRkZDMtYTczNi0xNDk0MzQwZTIwZTgubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MTBUMjAzMDE0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9Yjc3MzQ2N2U3YjllNWIzZTBkNjVkYjg2MzFjMWUwMzU5OWEyZDU3OWIzYTJhNzVhYWJkMzdkMGFmODUzMWU1NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.PzUtLv_ySwT8ZHqD43Or_Dr817PvMuhssmwBcPnFjKw" data-canonical-src="https://private-user-images.githubusercontent.com/1338731/356836994-64197310-29bd-4dd3-a736-1494340e20e8.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjYwMDA1MTQsIm5iZiI6MTcyNjAwMDIxNCwicGF0aCI6Ii8xMzM4NzMxLzM1NjgzNjk5NC02NDE5NzMxMC0yOWJkLTRkZDMtYTczNi0xNDk0MzQwZTIwZTgubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MTBUMjAzMDE0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9Yjc3MzQ2N2U3YjllNWIzZTBkNjVkYjg2MzFjMWUwMzU5OWEyZDU3OWIzYTJhNzVhYWJkMzdkMGFmODUzMWU1NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.PzUtLv_ySwT8ZHqD43Or_Dr817PvMuhssmwBcPnFjKw" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Prerequisites</h2><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Node.js 20+</h3><a id="user-content-nodejs-20" aria-label="Permalink: Node.js 20+" href="#nodejs-20"></a></p>
<p dir="auto">We officially support the current LTS version – 20 at the time of writing. YNS should work on versions 18, 20, and 22. If you're using one of those versions and encounter a problem, please report it!</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installing Node.js</h4><a id="user-content-installing-nodejs" aria-label="Permalink: Installing Node.js" href="#installing-nodejs"></a></p>
<p dir="auto">Follow the instructions for your operating system found here: <a href="https://nodejs.org/en/download" rel="nofollow">nodejs.org/en/download</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">pnpm 9+</h3><a id="user-content-pnpm-9" aria-label="Permalink: pnpm 9+" href="#pnpm-9"></a></p>
<p dir="auto">We officially support pnpm version 9, but we will do our best to keep it compatible with npm and yarn.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installing pnpm</h4><a id="user-content-installing-pnpm" aria-label="Permalink: Installing pnpm" href="#installing-pnpm"></a></p>
<p dir="auto">The easiest way to install pnpm is via Node.js Corepack. Inside the folder with YNS, run these commands:</p>
<div dir="auto" data-snippet-clipboard-copy-content="corepack enable
corepack install"><pre>corepack <span>enable</span>
corepack install</pre></div>
<p dir="auto">Alternatively, follow the instructions for your operating system found here: <a href="https://pnpm.io/installation" rel="nofollow">pnpm.io/installation</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Create Stripe account</h2><a id="user-content-create-stripe-account" aria-label="Permalink: Create Stripe account" href="#create-stripe-account"></a></p>
<p dir="auto">YNS is tightly integrated with <a href="https://stripe.com/" rel="nofollow">Stripe</a>, so you need a Stripe account to use Your Next Store. Follow the instructions from Stripe to <a href="https://dashboard.stripe.com/register" rel="nofollow">create an account</a>.</p>
<p dir="auto">It's important to remember that Stripe works in two different modes: <strong>Test Mode</strong> and <strong>Production Mode</strong>. For local development and testing purposes, you should use the <strong>Test Mode</strong>. This way, Stripe will never charge real money, and you can use special test credentials such as credit card numbers and BLIK numbers to complete payments. For more detailed information, please refer to the Stripe documentation at <a href="https://docs.stripe.com/testing" rel="nofollow">docs.stripe.com/testing</a>.</p>
<p dir="auto">Once you're ready to sell your products to real customers, you must switch <strong>Test Mode</strong> to <strong>Production Mode</strong> in Stripe and generate new credentials.</p>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">This step will require additional verification from Stripe, so we suggest you start the process immediately.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Add Environment Variables</h2><a id="user-content-add-environment-variables" aria-label="Permalink: Add Environment Variables" href="#add-environment-variables"></a></p>
<p dir="auto">For YNS to work, you'll need to define a few environmental variables. For local development and testing, you may create an empty <code>.env</code> file and copy the contents of <code>.env.example</code> into it.</p>
<p dir="auto">To set env variables in production, you'll need to consult the documentation of your chosen hosting provider.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Required Environment Variables</h3><a id="user-content-required-environment-variables" aria-label="Permalink: Required Environment Variables" href="#required-environment-variables"></a></p>
<ul dir="auto">
<li><code>ENABLE_EXPERIMENTAL_COREPACK</code> –&nbsp;Vercel only: Set to <code>1</code> to enable Corepack</li>
<li><code>NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY</code> – Publishable key from Stripe.</li>
<li><code>STRIPE_SECRET_KEY</code> – Secret key from Stripe.</li>
<li><code>STRIPE_CURRENCY</code> – This is used to determine your store's currency. Currently, only a single currency is allowed, and it should be a three-letter ISO code (e.g., <code>usd</code>).</li>
<li><code>NEXT_PUBLIC_URL</code> – <strong>Optional on Vercel</strong> The address of your store without the trailing slash, i.e., <code>https://demo.yournextstore.com</code>. When building for the first time, you should set it to any valid URL, i.e. <code>http://localhost:3000</code>.</li>
</ul>
<details open="">
  <summary>
    
    <span aria-label="Video description yns-setup-env-variables.mp4">yns-setup-env-variables.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/200613/323450765-01d27f69-00dc-446e-bc81-5dea2587f346.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjYwMDA1MTQsIm5iZiI6MTcyNjAwMDIxNCwicGF0aCI6Ii8yMDA2MTMvMzIzNDUwNzY1LTAxZDI3ZjY5LTAwZGMtNDQ2ZS1iYzgxLTVkZWEyNTg3ZjM0Ni5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwOTEwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDkxMFQyMDMwMTRaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1jMjVlNTZiOTg5YmEwOTY2OWZlMmIwMWE2YWNhMjYzN2YyNWQ1Y2NhZDA0NjY4ZDJkMTA3NDk5Y2NmOWRhMDRmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.tg0ceNGP3hJ4xoYLEUc3vRA5aXvPqXyI57locveaYTY" data-canonical-src="https://private-user-images.githubusercontent.com/200613/323450765-01d27f69-00dc-446e-bc81-5dea2587f346.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjYwMDA1MTQsIm5iZiI6MTcyNjAwMDIxNCwicGF0aCI6Ii8yMDA2MTMvMzIzNDUwNzY1LTAxZDI3ZjY5LTAwZGMtNDQ2ZS1iYzgxLTVkZWEyNTg3ZjM0Ni5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwOTEwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDkxMFQyMDMwMTRaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1jMjVlNTZiOTg5YmEwOTY2OWZlMmIwMWE2YWNhMjYzN2YyNWQ1Y2NhZDA0NjY4ZDJkMTA3NDk5Y2NmOWRhMDRmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.tg0ceNGP3hJ4xoYLEUc3vRA5aXvPqXyI57locveaYTY" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h3 tabindex="-1" dir="auto">Optional Environment Variables</h3><a id="user-content-optional-environment-variables" aria-label="Permalink: Optional Environment Variables" href="#optional-environment-variables"></a></p>
<ul dir="auto">
<li><code>NEXT_PUBLIC_UMAMI_WEBSITE_ID</code> – Umami website ID for analytics</li>
<li><code>NEXT_PUBLIC_NEWSLETTER_ENDPOINT</code> – <strong>Preview</strong>: The endpoint for the newsletter form in the future. It should accept POST requests with a JSON <code>{ email: string }</code> and return JSON <code>{ status: number }</code>.</li>
<li><code>STRIPE_WEBHOOK_SECRET</code> – <strong>Preview</strong>: Stripe Webhook secret for handling events from Stripe. Read more below.</li>
<li><code>ENABLE_STRIPE_TAX</code> – <strong>Preview</strong>: Set to any value (i.e., <code>1</code>) to enable Stripe Tax in YNS. Read more below.</li>
<li><code>NEXT_PUBLIC_LANGUAGE</code> - The language of the store.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Run the store</h2><a id="user-content-run-the-store" aria-label="Permalink: Run the store" href="#run-the-store"></a></p>
<p dir="auto">After following the above steps, run <code>pnpm install</code> to install the required dependencies, and then run <code>pnpm dev</code> to start the development server on your machine. Your Next Store will be available at <a href="http://localhost:3000/" rel="nofollow">localhost:3000</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Add products</h2><a id="user-content-add-products" aria-label="Permalink: Add products" href="#add-products"></a></p>
<p dir="auto">Your Next Store gets all the products, prices, descriptions, and categories from Stripe. So, if you know Stripe already, you'll feel right at home!</p>
<p dir="auto">You need to add products to the Stripe Dashboard to show in YNS. After logging in, click <strong>More</strong> in the left sidebar and select <strong>Product catalogue</strong>. You may also use the direct link:</p>
<ul dir="auto">
<li>In <strong>Test Mode</strong>: <a href="https://dashboard.stripe.com/test/products" rel="nofollow">dashboard.stripe.com/test/products</a></li>
<li>In <strong>Production Mode</strong>: <a href="https://dashboard.stripe.com/products" rel="nofollow">dashboard.stripe.com/products</a></li>
</ul>
<p dir="auto">Then, click on <strong>Add product</strong> and fill in all the required information:</p>
<ul dir="auto">
<li>name,</li>
<li>description,</li>
<li>price – currently, only <em>One-off</em> payments are supported,</li>
<li>a product image.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Metadata</h3><a id="user-content-metadata" aria-label="Permalink: Metadata" href="#metadata"></a></p>
<p dir="auto">Additionally, Your Next Store uses product metadata to provide more context information about the products. You can specify the following metadata fields:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Field</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>slug</code></td>
<td>Yes</td>
<td>The product slug is used for URLs. Needs to be unique except for variants.</td>
</tr>
<tr>
<td><code>category</code></td>
<td>No</td>
<td>The product category used for grouping products.</td>
</tr>
<tr>
<td><code>order</code></td>
<td>No</td>
<td>The product order used for sorting products. Lower numbers are displayed first.</td>
</tr>
<tr>
<td><code>variant</code></td>
<td>No</td>
<td>The product variant slug. Read below for details.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Now you should see all added products in Your Next Store.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Variants</h2><a id="user-content-variants" aria-label="Permalink: Variants" href="#variants"></a></p>
<p dir="auto">Your Next Store supports simple product variants. To create a product with variants, you must add multiple products to Stripe with the same <code>slug</code> metadata field. YNS uses the <code>variant</code> metadata field to distinguish between different variants of the same product. For example, if you have a T-shirt in multiple sizes, you can create three products with the <code>slug</code> of <code>t-shirt</code> and <code>variant</code> values of <code>small</code>, <code>medium</code>, and <code>large</code>.</p>
<p dir="auto">Variants are displayed on the product page. Variants can have different prices, descriptions, and images. It's important to note that the <code>category</code> should be the same for all variants of the same product for the best browsing experience.</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">In the future, we plan to add the possibility of editing products and variants inside a built-in admin dashboard. If you have any ideas or suggestions, please let us know!</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Stripe Webhooks</h2><a id="user-content-stripe-webhooks" aria-label="Permalink: Stripe Webhooks" href="#stripe-webhooks"></a></p>
<p dir="auto">Your Next Store uses Stripe Webhooks to handle events from Stripe. Currently, the endpoint is used to automatically revalidate cart page and to create tax transaction (if enabled). To set up Webhooks, follow the Stripe docs. The exact steps depend on whether you've activated Stripe Workbench in your Stripe account: <a href="https://docs.stripe.com/webhooks#add-a-webhook-endpoint" rel="nofollow">docs.stripe.com/webhooks#add-a-webhook-endpoint</a>.</p>
<p dir="auto">The endpoint for the webhook is <code>https://{YOUR_DOMAIN}/api/stripe-webhook</code>. The only required event is <code>payment_intent.succeeded</code>. When the webhook is configured in Stripe, set the <code>STRIPE_WEBHOOK_SECRET</code> environment variable to the secret key created by Stripe.</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">In the future, we plan to add more events to the webhook to improve the user experience.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Stripe Tax</h2><a id="user-content-stripe-tax" aria-label="Permalink: Stripe Tax" href="#stripe-tax"></a></p>
<p dir="auto">Your Next Store comes with a preview of Stripe Tax support. To enable it, set the <code>ENABLE_STRIPE_TAX</code> environment variable to any value (i.e., <code>1</code>).</p>
<p dir="auto">For this feature to work, you must set your Tax settings in Stripe Dashboard: <a href="https://dashboard.stripe.com/register/tax" rel="nofollow">dashboard.stripe.com/register/tax</a>. When enabled and configured, taxes will be automatically calculated and added to the total price of the product based on:</p>
<ul dir="auto">
<li>product pricing - tax can be inclusive or exclusive</li>
<li>product tax code</li>
<li>customer's address</li>
<li>customer's tax ID</li>
</ul>
<div dir="auto"><p dir="auto">Warning</p><p dir="auto">This feature is still in the early stage, and there could be edge cases that are not supported. We're actively working on it, so if you encounter any problems or have any suggestions, please let us know!</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Production Deployment</h2><a id="user-content-production-deployment" aria-label="Permalink: Production Deployment" href="#production-deployment"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Vercel</h3><a id="user-content-vercel" aria-label="Permalink: Vercel" href="#vercel"></a></p>
<p dir="auto">To deploy on Vercel, click the following button, set up your GitHub repository and environment variables, and click <strong>Deploy</strong>. Make sure to set the <code>ENABLE_EXPERIMENTAL_COREPACK</code> variable to <code>1</code>.</p>
<p dir="auto"><a href="https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fyournextstore%2Fyournextstore&amp;env=ENABLE_EXPERIMENTAL_COREPACK,NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY,STRIPE_SECRET_KEY,STRIPE_CURRENCY&amp;envDescription=Read%20more%20about%20required%20env%20variables%20in%20YNS&amp;envLink=https%3A%2F%2Fgithub.com%2Fyournextstore%2Fyournextstore%2Ftree%2Fupcoming%3Ftab%3Dreadme-ov-file%23add-environmental-variables&amp;project-name=yournextstore&amp;repository-name=yournextstore&amp;demo-title=Your%20Next%20Store&amp;demo-description=A%20Next.js%20boilerplate%20for%20building%20your%20online%20store%20instantly%3A%20simple%2C%20quick%2C%20powerful.&amp;demo-url=https%3A%2F%2Fdemo.yournextstore.com%2F&amp;demo-image=https%3A%2F%2Fyournextstore.com%2Fdemo.png" rel="nofollow"><img src="https://camo.githubusercontent.com/20bea215d35a4e28f2c92ea5b657d006b087687486858a40de2922a4636301ab/68747470733a2f2f76657263656c2e636f6d2f627574746f6e" alt="Deploy with Vercel" data-canonical-src="https://vercel.com/button"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Your Own VPS</h3><a id="user-content-your-own-vps" aria-label="Permalink: Your Own VPS" href="#your-own-vps"></a></p>
<p dir="auto">Description coming soon.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Docker</h3><a id="user-content-docker" aria-label="Permalink: Docker" href="#docker"></a></p>
<p dir="auto">Description coming soon.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">That's all</h2><a id="user-content-thats-all" aria-label="Permalink: That's all" href="#thats-all"></a></p>
<p dir="auto">YNS evolves each day, and we actively seek feedback on what to improve. If you have any questions or problems, don't hesitate to get in touch with us on our Discord Server.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Sometimes, you use <code>structuredClone</code> to pass data from server to client components. Why?</h3><a id="user-content-sometimes-you-use-structuredclone-to-pass-data-from-server-to-client-components-why" aria-label="Permalink: Sometimes, you use structuredClone to pass data from server to client components. Why?" href="#sometimes-you-use-structuredclone-to-pass-data-from-server-to-client-components-why"></a></p>
<p dir="auto">Only certain types of data can be passed from the server to the client directly. Data from Stripe SDK often contains class instances. To work around this, we use <code>structuredClone</code> to eliminate them and pass just plain old objects to the client.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How we made Jupyter notebooks load faster (132 pts)]]></title>
            <link>https://www.singlestore.com/blog/how-we-made-notebooks-load-10-times-faster/</link>
            <guid>41500522</guid>
            <pubDate>Tue, 10 Sep 2024 13:19:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.singlestore.com/blog/how-we-made-notebooks-load-10-times-faster/">https://www.singlestore.com/blog/how-we-made-notebooks-load-10-times-faster/</a>, See on <a href="https://news.ycombinator.com/item?id=41500522">Hacker News</a></p>
Couldn't get https://www.singlestore.com/blog/how-we-made-notebooks-load-10-times-faster/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Going open-source as a VC-Backed company (105 pts)]]></title>
            <link>https://briefer.cloud/blog/posts/open-source-strategy/</link>
            <guid>41500506</guid>
            <pubDate>Tue, 10 Sep 2024 13:16:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://briefer.cloud/blog/posts/open-source-strategy/">https://briefer.cloud/blog/posts/open-source-strategy/</a>, See on <a href="https://news.ycombinator.com/item?id=41500506">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Today, we're launching <a href="https://briefer.cloud/blog/posts/launching-briefer-oss">a free and open-source offering</a>. But I'll be honest with you: Briefer is a VC-backed company, and it must make money.</p>
<p>It's ironic to me that many <em>for-profit</em> companies can't say that out loud. Maybe it's because they're afraid that money will get in the way of making users happy. Or maybe they know it won't, but they're afraid of people thinking it will.</p>
<p>We don't think that way.</p>
<p>We think that doing right by your users is the best way to make money, and making money is the best way to do right by your users. That's because money allows you to invest in improving your product and offer a better service to more people, some of whom will use it for free, and that's fine.</p>
<p><img src="https://briefer.cloud/posts/open-source-strategy/oss-cycle.png" alt=""></p>
<p>Being clear about our strategy and intentions is part of doing right by our users and it's why I'm writing this post.</p>
<p>In it, I'll explain how companies can structure their open-source offerings, and how we structured ours in a way that's good for users and good for business. I'll also explain why we chose the AGPL license, how it benefits users, and how it benefits us too. Finally, I'll also explain some of the things we won't do, including changing our license.</p>
<h2>Structuring an open-source offering</h2>
<p><img src="https://briefer.cloud/posts/open-source-strategy/summary1.png" alt=""></p>
<p>When companies open-source their software it means <a href="https://www.gnu.org/licenses/gpl-faq.en.html#DoesTheGPLAllowRequireFee">they can't <em>really</em> charge for the software itself</a>¹. Thus, if you must make money, you need to sell something else, which is usually:</p>
<ol>
<li>Support and professional services</li>
<li>Compute and infrastructure</li>
<li>A non-open-source version of the software</li>
</ol>
<p>In the first case, companies scale as a function of how many support and consulting staff they can hire. It's a solid model but it doesn’t scale because hiring people is costly.</p>
<p>As a result, this approach often falls short of <a href="https://www.saastr.com/vcs-need-you-to-be-a-potential-fund-returner-when-they-invest-but-after-expectations-there-are-more-nuanced">investors' expectations for significant returns</a>, which makes it hard to raise funding and thus prevents most founders from being able to go full-time on their project. Consequently, it becomes difficult to build enough traction to generate leads for the professional services they would like to sell, especially considering there's no marketing budget.</p>
<p>The second case, selling compute and infrastructure, usually comes in the form of a cloud-hosted version of the software. This model works well for companies building complex and mission-critical software, like databases and managed Kubernetes clusters. That's because it's a bet that the company can provide a cheaper, more convenient, and more reliable service than customers could provide themselves.</p>
<p>The problem with this model is that it incentivizes companies to make it hard for customers to run the software on their own. This dysfunctional pattern manifests itself in many ways, like when companies insist on overly complex helm charts (<em>ugh</em>), withhold documentation on how to run the software, or require customers to operate a dozen pods when one would work just as well.</p>
<p>The third case, selling a non-open-source version of the software, is fair in the sense that companies will still give value away while carving out a subset of features upon which they can charge. That way, they can still make money to maintain and improve the open-source version, while profiting from the extra features in the non-open-source version.</p>
<p>The problem with this last model is that it creates a conflict of interest between the company and its customers. Customers want the open-source version to have more features, while the company wants the open-source version to have <em>fewer</em> features so that customers will pay for the non-open-source version.</p>
<h2>Our model</h2>
<p><img src="https://briefer.cloud/posts/open-source-strategy/summary2.png" alt=""></p>
<p>Briefer's approach is a mix of the second and third models. We will sell a freemium cloud-hosted version of our software and maintain a generous AGPL-licensed version with fewer features.</p>
<p>Some people call our strategy <a href="https://about.gitlab.com/blog/2016/07/20/gitlab-is-open-core-github-is-closed-source/">"open-core"</a> and that's technically right. Still, I'd rather say that we have two pieces of software: one that is open-source and another that is not. I think that's more direct because we're not trying to hide the fact that we're selling a non-open-source version of our software. Anyway, open-core is fine too, and there's nothing wrong with it.</p>
<p>We chose to do it this way because we want everyone to use Briefer. Therefore, choosing a service-based model would not be viable because it limits our access to funding, which is necessary to scale Briefer and pay our bills as we build it.</p>
<p>We then considered picking the second model, selling compute and infrastructure, but I was afraid it would corner us into having to eventually change licenses.</p>
<p>Another reason why we rejected that model is because we refused to compromise the software's quality just to profit from users' struggles to run and operate it. Making it ugly, inefficient, and poorly documented was not an option. That would completely defeat the purpose of having an open-source offering, and, as an engineer, it would cause me physical pain.</p>
<p>That's when we decided to mix the second and third models.</p>
<p>This choice allows us to have a small subset of features for which we can charge so we don't have to worry about how easy it is to run Briefer.</p>
<p>That way, there will be an incentive to make the open-source version as easy as possible to run because that helps us increase our user base, thus increasing the number of people who might eventually pay for the cloud-hosted version.</p>
<p>This incentive to increase the user base (our top-of-funnel) also means we can't simply reserve the best features for the non-open-source version. Instead, we have to find a balance between an open-source version that's competitive and attractive enough for people to use it and a cloud-hosted version that's valuable enough for people to pay for it.</p>
<p>In our case, we decided to charge only for the features that larger companies usually need (&gt;100 employees) or those necessary to directly make money from Briefer, like embedding charts into your own application.</p>
<p><img src="https://briefer.cloud/posts/open-source-strategy/oss-vs-paid.png" alt=""></p>
<p>We thought that was a fair heuristic because companies with more than 100 employees usually have more resources to pay for software, and they usually get more value from it, so it makes sense to take a small cut.</p>
<p>Conversely, small companies are usually strapped for cash, and it wouldn't make sense to charge them pennies. We'd rather help them now and have them eventually pay for the cloud-hosted version when they're thriving and getting more value out of our software. That way, we can help them grow and make enough money to maintain, improve, and distribute both the free and paid versions of Briefer. That's a win-win.</p>
<h2>Why we chose the AGPL license</h2>
<p><img src="https://briefer.cloud/posts/open-source-strategy/summary3.png" alt=""></p>
<p>We licensed Briefer's open-source version under AGPL for three reasons.</p>
<p>The first is that it's a well-known license, so most people will be fully aware of what they can and can't do with our software.</p>
<p>The second is that it's a great way to ensure that the open-source software will remain open-source forever.</p>
<p>The third is that the AGPL still allows users to implement any features they need, but disincentivizes competitors from copying our software and selling it as their own.</p>
<p>If they were to do that, they would have to open-source all their changes and commoditize their work, so it's not worth it for the vast majority of companies.</p>
<p>Here's where the difference between AGPL and GPL comes into play. We chose AGPL because it closes <a href="https://www.revenera.com/blog/software-composition-analysis/understanding-the-saas-loophole-in-gpl/">the "network loophole"</a> that exists with the GPL. Under GPL, it was possible to modify the software and use it over a network without releasing the changes. But with AGPL, if competitors modify our software and offer it as a service over a network—like a cloud application—they must make those changes public under the same AGPL license.</p>
<p>We never considered using a license that's not in <a href="https://opensource.org/licenses">the OSI-approved list</a> because those are, obviously, not open-source. Additionally, we have already made a compromise in not open-sourcing the whole codebase, so I thought it would be fair to pick the "freest" license of them all.</p>
<h2>Our open-source strategy</h2>
<p><img src="https://briefer.cloud/posts/open-source-strategy/summary4.png" alt=""></p>
<p>The main reason we created an open-source offering is that it's the best way to align our user's interests with the company's financial interests.</p>
<p>On one side, more people can use Briefer because it's free and open-source. These people can implement the features they need, and distribute them to others.</p>
<p>On the other side, it's a competitive advantage for us because it increases our top of funnel, helps us improve our activation rates, and gives us a competitive edge against incumbents.</p>
<p>Here's each of these points in more detail.</p>
<br>
<h3>Taking on incumbents</h3>
<p>Briefer's main competitors are Salesforce's Tableau and Microsoft's Power BI. At this point, these companies aren't just companies anymore. They're institutions. They have a lot of money, a lot of engineers, and they're not going to go away anytime soon.</p>
<p>On the other hand, Briefer is a small company that's doing well, but that's just starting. We don't have the same brand recognition, the same number of customers, or the same amount of money. That's why people ask us questions like: "How do I know you'll be around tomorrow?" and "How are you different?"</p>
<p>Open source is a great way to answer these questions.</p>
<p>That's because open-source software outlives companies. If Briefer disappears tomorrow, people can still use the software, implement the features they need, and distribute them to others. That's a great way to assure everyone that using our software is a safe bet too.</p>
<p>Open-source software is also a great differentiator because it helps us build a strong community. These people can contribute features, fix bugs, and report issues, which makes our software better, faster, and more reliable.</p>
<p>Finally, by going open-source we commoditize our competitors' core functionality. This means they now have to compete against us in terms of innovative features, performance, and price, all of which are usually not their strong suits, let's be honest.</p>
<br>
<h3>Helping more people while increasing our top-of-funnel</h3>
<p>Making our software open-source means more people can use it because they won't have to "hop on a quick call" or swipe their credit card to get started.</p>
<p>Instead, people can just clone the repository and run a single command to get started.</p>
<p>That's great for people because they can use Briefer for free to study, work on their personal projects, or grow their business. It's also great for us because the lower barrier to entry increases our top-of-funnel.</p>
<p>Then, if they like our software, we win, regardless of whether they pay for the cloud-hosted version. That's because non-paying users who like our software will certainly tell their friends about it, and some of them will eventually pay for the cloud-hosted version.</p>
<p>As I mentioned earlier, this incentive to increase the user base also means we can't simply reserve the best features for the non-open-source version. Otherwise, we'd be shooting ourselves in the foot because no one would use it.</p>
<p>Furthermore, if we're successful, it's likely that more companies will create competing open-source offerings. That will push us to open-source more features so that we can stay competitive, which is better for all users, even the ones not using our software.</p>
<br>
<h3>Improving activation rates</h3>
<p>Most people out there don't go around pasting their database credentials into every SaaS product they find. Sometimes they don't trust a company they've never heard of, sometimes they can't punch a hole in their firewall, and sometimes they simply don't want to go through the hassle of getting management's approval.</p>
<p>This behavior makes it more difficult for people to see value in Briefer because they won't have real data from which they can derive valuable insights. As a result, they will churn.</p>
<p>We tried to solve this problem in many ways, including:</p>
<ol>
<li>Creating demo datasets that people can use to get started</li>
<li>Supporting file uploads so people can upload their data without having to connect to a database</li>
<li>Devising a thorough document detailing all our security practices in an attempt to make people trust us</li>
</ol>
<p>None of that worked well. All of these initiatives were helpful, but none truly solved the problem.</p>
<p>Looking back, we're now confident that the only effective solution is to make our software open-source. That way, people can run it locally, with their own data, without having to connect to the internet, and behind a VPN if necessary.</p>
<p>This change will increase activation rates, which will then lead to higher retention rates, which leads to more feedback, which leads to a better product, which leads to more users and more conversions.</p>
<br>
<h3>Crowdsourcing innovation</h3>
<p>It's difficult to figure out what people want, and it's even more difficult to figure out how to prioritize everything they want.</p>
<p>Open-source helps us manage Briefer's roadmap along with our users because there will be more of them, and because they'll have access to the source code. That way, they can help us figure out where to go, and help us get there by implementing what they need.</p>
<br>
<h3>It makes us happier</h3>
<p>The first time I worked with my co-founder was when we were maintaining an open-source project called <a href="https://github.com/chaijs/chai">Chai.js</a>. It was the most fun I've ever had working on a project, and I want every day at Briefer to feel like that.</p>
<p>That may sound like a selfish reason, but it's not.</p>
<p>Building a company takes many years, and the only way to keep going is to have fun along the way. That's because building a company is hard, and it's easy to give up when things get tough, but not when you're having fun.</p>
<p>When you're having fun you don't mind the late-night coding sessions, all the "no's" you get from investors, or the bugs that keep popping up. You just keep going because you're having fun, and time will pass anyway, so you might as well spend it wisely and build something meaningful.</p>
<h2>Things we won't do</h2>
<p><img src="https://briefer.cloud/posts/open-source-strategy/summary5.png" alt=""></p>
<p>Finally, I want to make sure I address some things we will <em>not</em> do.</p>
<p>The first is to change our license.</p>
<p>One of the main reasons why Briefer has two separate pieces of software is so that we'll always have a significant number of money-making levers to pull in the non-open-source version.</p>
<p>I think having separate licenses is a better compromise than risking our users' trust by changing the license of the open-source version in exchange for short-term profits.</p>
<p>The second thing we won't do is to stop maintaining the open-source version.</p>
<p>Open-source is a core-part of our strategy. It's the right thing to do for our users, and, consequently, it's the right thing to do for us because it makes people happy, and because it helps Briefer succeed in all the ways I mentioned earlier.</p>
<p>The third and final thing we won't do is to stop being transparent.</p>
<p>I want to build Briefer as a company that people can trust, and that means being transparent about our intentions, our strategy, and our roadmap. That's the right thing to do for our users and, honestly, it's good for business. Therefore, it is the right thing for us too.</p>
<p>If you have questions, email me at lucas.costa[at]briefer.cloud and I'll be happy to answer them personally.</p>
<p><i>Best,<br> LdC.</i></p>
<h2>Footnotes</h2>
<p>[1] <a href="https://www.gnu.org/licenses/gpl-faq.en.html#DoesTheGPLAllowMoney">You <em>can</em> charge for people for copy of GPL-licensed software if they're getting it from you</a>. So, <em>technically</em>, you could make money selling software itself. The reason that wouldn't work in practice is that <a href="https://www.gnu.org/licenses/gpl-faq.en.html#DoesTheGPLAllowRequireFee">others could then redistribute the software for free</a>. Therefore, it would be nearly impossible to compete with them.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Multispectral Imaging and the Voynich Manuscript (121 pts)]]></title>
            <link>https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/</link>
            <guid>41500406</guid>
            <pubDate>Tue, 10 Sep 2024 13:05:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/">https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/</a>, See on <a href="https://news.ycombinator.com/item?id=41500406">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>(with thanks to René Zandbergen, Ray Clemens, <a href="https://www.rit.edu/directory/rlepci-roger-easton-jr" target="_blank" rel="noreferrer noopener">Roger Easton</a>, <a href="https://ling.yale.edu/people/claire-bowern" target="_blank" rel="noreferrer noopener">Claire Bowern</a>, <a href="https://bill.oucreate.com/" target="_blank" rel="noreferrer noopener">Bill Endres</a>, and the curatorial and conservation staff at the Beinecke Rare Book &amp; Manuscript Library)</p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2015/01/voynich-detail.jpg"><img data-attachment-id="3090" data-permalink="https://manuscriptroadtrip.wordpress.com/2015/01/16/manuscript-road-trip-a-new-year-in-new-haven/voynich-detail/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2015/01/voynich-detail.jpg" data-orig-size="534,556" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="voynich detail" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2015/01/voynich-detail.jpg?w=288" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2015/01/voynich-detail.jpg?w=500" tabindex="0" role="button" width="534" height="556" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2015/01/voynich-detail.jpg?w=534" alt=""></a></figure></div>


<p>I have some exciting news for all of you <a href="https://manuscriptroadtrip.wordpress.com/2015/01/17/manuscript-road-trip-the-worlds-most-mysterious-manuscript/" target="_blank" rel="noreferrer noopener">Voynich </a>fans! But first, some background about multispectral imaging…</p>



<p>Multispectral imaging is a way of capturing a digital image using non-visible wavelengths such as ultraviolet and infrared (click <a href="https://hmml.org/stories/seeing-invisible-multispectral-imaging-ancient-medieval-manuscripts/" target="_blank" rel="noreferrer noopener">here </a>to learn more). Where medieval manuscripts are concerned, UV imaging in particular can make faded or effaced text legible. This is because most medieval inks (including that used to write the Voynich Manuscript) have a significant iron component. This allows the ink to “bite” into the surface of the parchment rather than sliding off of it. When ink is scraped away or fades, the molecular bond remains, and the faded text may therefore fluoresce when exposed to UV bandwidths. This technology has proven invaluable in helping scholars read palimpsests and damaged manuscripts such as the <a href="https://www.archimedespalimpsest.org/" target="_blank" rel="noreferrer noopener">Archimedes Palimpsest</a> and the <a href="https://mizanproject.org/the-syriac-galen-palimpsest/" target="_blank" rel="noreferrer noopener">Syriac Galen Palimpsest</a>. Could such imaging of the <a href="https://beinecke.library.yale.edu/collections/highlights/voynich-manuscript" target="_blank" rel="noreferrer noopener">Voynich Manuscript</a> help reveal its secrets?</p>



<p>Back in 2014, while working on a <a href="https://news.yale.edu/2015/06/11/hidden-secrets-yale-s-1491-world-map-revealed-multispectral-imaging" target="_blank" rel="noreferrer noopener">different imaging project</a> at Yale University’s Beinecke Library, the imaging team from <a href="https://lazarusprojectimaging.com/">The Lazarus Project</a> (Michael Phelps (Early Manuscripts Electronic Library), Gregory Heyworth (then at University of Mississippi, now at University of Rochester), Chet Van Duzer (independent map scholar), Ken Boydston (Megavision) and Roger Easton (Rochester Institute of Technology)) was granted permission by the Library to take multispectral images of ten select pages of the Voynich Manuscript (a.k.a. Beinecke Library MS 408): 1r, 8r, 17r, 26r, 47r, 70v1, 71r, 93r, 102v1, and 116v. The intent was to make the images publicly available on the Yale website, but for various reasons (including staff turnover, development of Yale’s new image platform, library backlogs, and COVID) the images were never posted. Details of several images were published on pp. 31-32 of <em><a href="https://yalebooks.yale.edu/book/9780300217230/the-voynich-manuscript/" target="_blank" rel="noreferrer noopener">The Voynich Manuscript</a></em> (ed. Raymond Clemens), and a few have been explored by Voynich researchers (<a href="https://stephenbax.net/?p=1625" target="_blank" rel="noreferrer noopener">here </a>and <a href="http://ciphermysteries.com/2016/11/19/multispectral-images-voynich-f116v" target="_blank" rel="noreferrer noopener">here</a>, for example), but the full set of these MSI images has never been publicly seen or studied – until now.</p>



<p>On a whim, I wrote to Roger a few weeks ago to ask if he still had the images, and he very kindly sent them to me. I have been given permission to make all of the images public, and I am thrilled to announce that <strong>they may be viewed and freely downloaded <a href="https://drive.google.com/drive/folders/1mNQGKQDSCR4M_c2M2JrsU5soghvYwMig?usp=sharing" target="_blank" rel="noreferrer noopener">here</a>. </strong>(there may also have been images taken of a few other pages, but those are TBD)</p>



<p>[if you want to skip the technical details and jump straight to the good stuff, click <a href="#Image-analyses">here</a>]</p>



<p>There are four folders in the shared drive: “Lab_True_color_TIFF” (high-resolution TIFFs of some of the pages); “Processed_Images” (post-processed multispectral images); “Raw TIFFs” (enormous unprocessed multispectral 16-bit TIFFs in different color bands, not readable by most image viewers); and “RGB_true_color_JPEGs” (high-resolution JPGs of some of the pages). When using or referencing these images, please credit “The Lazarus Project and The Chester F. Carlson Center for Imaging Science at Rochester Institute of Technology” and cite the manuscript as “Beinecke Rare Book &amp; Manuscript Library MS 408,” including the particular folio number. For more information about image capture and post-processing, see the “Technical Details” section at the <a href="#Technical-details">end </a>of this post.</p>



<p>The really interesting images in the shared drive are in the folder labeled “Processed_Images.” In this context, “Processed” means that an imaging expert (Roger Easton, in this case) has applied complex color transformations to the raw 16-bit TIFFs in order to make them “legible” to the human eye. Because he has worked on so many MSI projects, he knows which transformations are most likely to be useful. For some of the pages, he made these transformations in September 2024 at my request, focusing on making particular areas of the images more legible (that’s why there are more images in some folders than others, as each version of the same page represents a different post-processing strategy).</p>



<p>I would advise you to approach these images with caution. Because processed multispectral images have unnatural color profiles, it is very easy to misread or misinterpret them. And we all know that where the Voynich Manuscript is concerned, excitement and enthusiasm sometimes inspire us to rush ahead without paying close enough attention to evidence and detail. The interpretation of MSI images requires time and care and what is sometimes called “slow looking.” To help you understand what these images are, and what they aren’t, I have turned to MSI expert and University of Oklahoma professor Bill Endres for some guidance:</p>



<blockquote>
<p>“While we normally don’t think about our vision as limited, it truly is. We only see a select range of the light spectrum, from about 380 to 720 nm (nanometers, the measurement for the lengths of a light wave). Conversely, bees see into the ultraviolet range and rattlesnakes see into the infrared, which allows them to hunt at night. If we had the eyes of bees, we wouldn’t need technological help and could likely read erased and damaged iron gall ink.</p>



<p>What makes the human eye limited is that it constructs color by collecting three different colors of light—red, green, and blue—and merges them together. To collect each color, the eye has a different type of cone. Human vision is impressive for its efficiency in generating color from a sampling of light, but this efficiency leaves out a tremendous amount of visual data.</p>



<p>Multispectral imaging benefits us because the sensor of a monochrome camera can capture a larger range of the light spectrum and individual frequencies of light (by imaging in the dark and using LED lighting to generate individual light frequencies). Capturing individual frequences is crucial. A page of a manuscript is a collection of parchment, ink, and pigment. Substances reflect and absorb individual frequencies of light differently. Multispectral imaging allows us to leverage those differences. Sometimes the differences are subtle. Post-processing highlights those subtle differences.”</p>
</blockquote>



<p>Now, let’s get to it.</p>



<p id="Image-analyses">Ten years after the images were captured, no one can quite remember why these ten specific pages were chosen for imaging. Some are obvious (such as 1r and 116v, the first and last pages), and others have features that the team thought might present valuable results. For the most part, they were right! Let’s take a close look at a few of them.*</p>



<p>* I haven’t explored the images of folios 8r and 47r here because I didn’t see anything of particular interest in those processed images. You’ll find them in the shared folder. Take a look; you may see something I missed!</p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_004_image_0001-1.jpg"><img data-attachment-id="7622" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/beinecke_dl_2002046_page_004_image_0001-2/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_004_image_0001-1.jpg" data-orig-size="2697,3766" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Beinecke_DL_2002046_Page_004_Image_0001" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_004_image_0001-1.jpg?w=215" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_004_image_0001-1.jpg?w=500" tabindex="0" role="button" width="733" height="1023" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_004_image_0001-1.jpg?w=733" alt=""></a><figcaption><em>Yale University, Beinecke Rare Book &amp; Manuscript Library MS 408, f. 1r</em></figcaption></figure></div>


<p>It’s been known for more than a century that the first page of the manuscript includes an effaced inscription in the lower margin. Wilfrid Voynich himself observed that there was something written there:</p>



<blockquote>
<p>“When I brought the manuscript to America the margins of the first page had the appearance of being blank, but an accident to a photostatic reproduction of this page revealed the fact that an underexposure of the plate brings out a faded autograph in the lower margin. Chemicals were applied to the margins…” </p>



<p>– Wilfrid Voynich, “A Preliminary Sketch of the History of the Roger Bacon Cipher Manuscript,” in <em>Transactions of the College of Physicians and Surgeons of Philadelphia </em>Third Series, Vol. 33 (1921), pp. 415-30, at pp. 421-422.</p>
</blockquote>



<p>Around the year 1914, Voynich applied a chemical reagent to this page in an effort to make the inscription more visible. That explains the dark stain in the lower and outer margins. MSI makes the inscription legible, confirming Voynich’s reading of “Jacobi à Tepenecz,” a.k.a. Jacobus Sinapius, a Prague alchemist who likely owned the manuscript in the late 1500s or early 1600s. The animation below shows three different stages of the imaging of the manuscript: Voynich’s original image, taken around 1912 (before he applied the chemicals); the current state of the page in visible light (showing the staining caused by Voynich’s efforts); and the MSI image.</p>



		<figure>
			
			
			
		</figure>
		


<p>But the clear legibility of the Tepenecz inscription isn’t even the most exciting outcome of the imaging of this page. For several decades, Voynich researchers have noted what appears to be a Roman alphabet written in the right-hand margin. In the detail below, you can clearly see the letters a, b, c, d, and e.</p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_01r_bands01-12_rf_cal_r6g4b1-1.jpg"><img data-attachment-id="7625" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/voynich_01r_bands01-12_rf_cal_r6g4b1-3/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_01r_bands01-12_rf_cal_r6g4b1-1.jpg" data-orig-size="967,2563" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1414065351&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Voynich_01r_bands01-12_RF_cal_R6G4B1" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_01r_bands01-12_rf_cal_r6g4b1-1.jpg?w=113" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_01r_bands01-12_rf_cal_r6g4b1-1.jpg?w=386" tabindex="0" role="button" width="386" height="1023" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_01r_bands01-12_rf_cal_r6g4b1-1.jpg?w=386" alt=""></a></figure></div>


<p>In visible light, it appears that there may be other characters written to the right of these letters, but they cannot be easily discerned. Under ultraviolet light, the faded letters become perfectly legible. Not only that, but it turns out there are actually <em>three </em>columns of lettering, not just one! Although others have theorized that there <em>might </em>be more text to the right of the visible alphabet, the lettering has never been clearly seen or transcribed before. The letters are written in three parallel columns: the Roman alphabet (a-z), a series of Voynich characters, and another Roman alphabet offset by one letter. My preliminary transcription of these alphabets is shown below.</p>



<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-696.png"><img data-attachment-id="7715" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/screenshot-696-2/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-696.png" data-orig-size="540,951" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot (696)" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-696.png?w=170" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-696.png?w=500" tabindex="0" role="button" loading="lazy" width="540" height="951" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-696.png?w=540" alt=""></a></figure></div>



<p>You might wonder how I was able to determine what was written under that dark oval blob near the top, which is a green leaf showing through from the other side of the folio (i.e. f. 1v). Once I realized that it was preventing me from seeing what’s in the margin there, I asked my friend Bill Endres, who is a post-processing genius, if he could “subtract” the show-through. He rose to the challenge!</p>



<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-691.png"><img data-attachment-id="7711" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/screenshot-691-2/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-691.png" data-orig-size="1519,927" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot (691)" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-691.png?w=300" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-691.png?w=500" tabindex="0" role="button" loading="lazy" width="1024" height="624" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-691.png?w=1024" alt=""></a></figure>



<p>Believe it or not, this actually helps me enormously. In the right-hand image, I can clearly see the letter [g] and enough of the Voynichese character to surmise which character it is.</p>



<p>Are these alphabets an early attempt to decode the manuscript? Perhaps. The two Roman alphabets are written in what paleographers call “Humanistic bookhand,” that is, the style of writing developed by Humanists like Petrarch and Boccaccio in Italy in the 14th century and used throughout Europe for several hundred years. I have carefully compared these letters to the handwriting of everyone known – or thought – to have been connected with the manuscript in the 16th and 17th century, including: Carl Widemann, his colleague Leonhard Rauwolf, Emperor Rudolf II, Jacobus Sinapius, Georg Baresch, Marcus Marci, and Athanasius Kircher. I even considered the handwriting of John Dee and (at Prof. Claire Bowern’s suggestion) his collaborator Edward Kelley, who were once (spuriously) thought to have been associated with the manuscript. </p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/jan_marcus_marci_00.jpg"><img data-attachment-id="7834" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/jan_marcus_marci_00/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/jan_marcus_marci_00.jpg" data-orig-size="1024,1568" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Jan_Marcus_Marci_00" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/jan_marcus_marci_00.jpg?w=196" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/jan_marcus_marci_00.jpg?w=500" tabindex="0" role="button" loading="lazy" width="669" height="1024" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/jan_marcus_marci_00.jpg?w=669" alt=""></a><figcaption><em>Johannes Marcus Marci</em></figcaption></figure></div>


<p>One of these men is a very good match: Johannes Marcus Marci (1595-1667).</p>



<p>Note: I initially discounted Marci because I had compared the revealed alphabets to the <a href="https://collections.library.yale.edu/catalog/2041454" target="_blank" rel="noreferrer noopener">letter attributed to Marci</a> at the Beinecke. René Zandbergen reminded me that that letter isn’t written in Marci’s hand but by the secretary he is known to have worked with as his eyesight faded near the end of his life. He suggested that I take a closer look at the earlier autograph Marci correspondence linked from his <a href="https://www.voynich.nu/letters.html" target="_blank" rel="noreferrer noopener">website</a>. Let’s do it!</p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/marci-hand.jpg"><img data-attachment-id="7804" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/marci-hand/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/marci-hand.jpg" data-orig-size="1524,2312" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Marci hand" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/marci-hand.jpg?w=198" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/marci-hand.jpg?w=500" tabindex="0" role="button" loading="lazy" width="675" height="1024" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/marci-hand.jpg?w=675" alt=""></a><figcaption><em>Marcus Marci to Athanasius Kircher (12 September 1640)<br>Rome, Pontificia Università Gregoriana, APUG 557, fol. 127r</em></figcaption></figure></div>


<p>The best way to determine if a script is a match is by comparing each letterform in the unknown sample (the revealed alphabet) to a known sample (in this case, the letter written from Marci to Athanasius Kircher on 12 September 1640, shown at left and discussed <a href="https://www.voynich.nu/letters.html">here</a>). For those of you unversed in Voynich lore, Marci was a doctor in Prague who inherited the manuscript from his friend, alchemist Georg Baresch, upon Baresch’s death in 1662. Marci sent it to Rome as a gift to Athanasius Kircher in 1665, confident that Kircher would be able to make sense out of it. The manuscript stayed in Rome until Voynich acquired it in 1912 (that’s the short version…for more, see René’s Voynich <a href="https://voynich.nu/">website</a>).</p>















<p>Let’s take a closer look, letter by letter. Time for some hardcore paleography!</p>










<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/1r-vs-marci-12.09.1640.png"><img data-attachment-id="7807" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/1r-vs-marci-12-09-1640/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/1r-vs-marci-12.09.1640.png" data-orig-size="1304,758" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="1r vs Marci 12.09.1640" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/1r-vs-marci-12.09.1640.png?w=300" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/1r-vs-marci-12.09.1640.png?w=500" tabindex="0" role="button" loading="lazy" width="1024" height="595" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/1r-vs-marci-12.09.1640.png?w=1024" alt=""></a><figcaption><em>Comparison of letterforms: Marcus Marci vs. revealed alphabet</em></figcaption></figure></div>


<p>There are several very strong markers that help make a convincing case for identifying this script as Marci’s:</p>



<ol>
<li>The loop-less [b], [d], [f], [h], [p], [q], [s], and [y]: During this period, many hands write prominent loops on the ascenders or descenders of these letters, loops that are lacking in both samples;</li>



<li>The open-bowl [g]: Marci doesn’t always leave his [g]s open, but he sometimes does;</li>



<li>The [m] with a first stroke that is taller than the last, giving the impression that the letter shrinks from left to right;</li>



<li>The shape of the [z]: because [z] is a relatively rare letter, its shape and ductus tend to be distinctive in any particular hand. The example above is ligated with [t] at the end of a word, so the context is different, but the [3] shape is quite similar.</li>
</ol>



<p>Notes: [g] and [h] for the revealed text are taken from the third column, since they aren’t legible in the first. Letters that aren’t legible in either alphabet have not been considered. It is also important to note that in his correspondence, Marci’s script has a prominent slant that is lacking in the revealed alphabets – however, because the alphabets are made up of decontextualized single letters, I would not necessarily expect the same slant as in the handwritten documents. The letterforms themselves, in their shapes and ductus (sequence of strokes) are very similar.</p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-678-1.jpg"><img data-attachment-id="7777" data-permalink="https://manuscriptroadtrip.wordpress.com/screenshot-678-3/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-678-1-edited.jpg" data-orig-size="399,446" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot (678)" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-678-1-edited.jpg?w=268" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-678-1-edited.jpg?w=399" tabindex="0" role="button" loading="lazy" width="399" height="446" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-678-1-edited.jpg" alt=""></a><figcaption><em>“Majuscules” on f. 1r</em></figcaption></figure></div>


<p>As for the Voynichese column, all of those characters are in common use throughout the manuscript except for the last two (next to [y] and [z], in blue at left). They appear in my transcription as cropped details rather than typographical characters because they appear nowhere else in the manuscript. There are at least sixteen leaves missing from the codex. It is quite possible, likely even, that these two unusual characters were found on one of the missing leaves. The style of the letters identifies them as what Voynichologists call “capitals” or “majuscules” out of expedience and convention (what they ACTUALLY are is unknown). These characters are now only found on the first page, and they are clearly different from the glyphs at the bottom of the second column of revealed symbols. There’s another character in this style at the top of the right margin (shown at the lower right in the mosaic above), but its purpose, too, cannot yet be determined.</p>



<p>The Voynichese sequence (we don’t actually know if it is meant to be an “alphabet”) does not correspond with original sequences on <a href="https://collections.library.yale.edu/catalog/2002046?child_oid=1006171" target="_blank" rel="noreferrer noopener">f. 49v</a>, <a href="https://collections.library.yale.edu/catalog/2002046?child_oid=1006187" target="_blank" rel="noreferrer noopener">f 57v</a>, or <a href="https://collections.library.yale.edu/catalog/2002046?child_oid=1006192" target="_blank" rel="noreferrer noopener">f. 66r</a> (with thanks to Yale University Professor of Linguistics Claire Bowern for this observation), and the symbols aren’t in any obvious order. A few common glyphs aren’t included at all (such as the one-loop two-legged gallows glyph known as [k]), although some of the symbols in the second column cannot be clearly discerned as of yet. Additional post-processing may help clarify such ambiguities.</p>



<p>The purpose of these three vertical alphabets is not at all clear. Knowing what we know about Marci’s timeline, they must have been added between 1662 and 1665, when he owned of the manuscript. They may represent an early attempt to decode the manuscript using two different substitution ciphers, or Marci may have been using Voynich characters to create a cipher of his own. Regardless of their purpose, I do know one thing: these alphabets will likely <em>not </em>help us actually decipher the manuscript. This is because linguists like Claire and other researchers have established that the manuscript is almost certainly not encrypted using a simple substitution cipher, and the substitutions in these columns result in nonsense anyway. Even so, they do add an interesting and new chapter to the early history of the manuscript. I look forward to hearing from other researchers about this new evidence, especially from experts in cryptography who may have ideas about why Marci or any other early-modern decrypter would need three columns of alphabets to do their work.</p>



<p>There is one more noteworthy feature of the imaging of folio 1r. Under ultraviolet light, several Voynichese characters that cannot be read under visible light become legible. For Voynichologists, any new textual evidence – no matter how little – is significant, as it adds data to the analysis of the text. More data leads to more detailed analytics, and more detailed analytics may lead to the ability to “read” this mysterious manuscript at last.</p>



		<figure>
			
			
			
		</figure>
		

<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_070v_bands01-22_rffl_cal_med3_r14g16b18_fl-b.jpg"><img data-attachment-id="7657" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/voynich_070v_bands01-22_rffl_cal_med3_r14g16b18_fl-b/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_070v_bands01-22_rffl_cal_med3_r14g16b18_fl-b.jpg" data-orig-size="4507,6985" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1414425086&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Voynich_070v_bands01-22_RF+FL_cal_med3_R14G16B18_FL-B" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_070v_bands01-22_rffl_cal_med3_r14g16b18_fl-b.jpg?w=194" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_070v_bands01-22_rffl_cal_med3_r14g16b18_fl-b.jpg?w=500" tabindex="0" role="button" loading="lazy" width="661" height="1024" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_070v_bands01-22_rffl_cal_med3_r14g16b18_fl-b.jpg?w=661" alt=""></a><figcaption><em>BRBL MS 408, f. 70v1</em></figcaption></figure></div>


<p>The image of folio 70v1 (right) is instructive as it demonstrates a critical caveat where these and other images are concerned: the interpretation of multispectral images requires patience and care. It is extremely important to distinguish between an offset (i.e. a mirror image left when a book is closed for centuries and ink or pigment from one page rubs off onto the facing page), show-through (a mirror-image ghost of text on the other side of the page), and erased ink (non-reversed faded text made visible). It is very easy to be misled by anything you may think you see in an image like this, and when examing these MSI images you should always compare your findings with the facing page, the other side of the leaf, and the visible-light version of the image to be sure you aren’t being led astray or leaping to unfounded conclusions. For example, the MSI image of folio 70v1 seems to have Voynichese writing that appears in pale blue in this multispectral image (see, e.g., the top of the diagram between the two outer rings of text). Is this an offset from the facing page, show-through from the other side, or hidden/revealed text?</p>



<p>Let’s take a closer look.</p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-684.png"><img data-attachment-id="7674" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/screenshot-684/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-684.png" data-orig-size="1811,579" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot (684)" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-684.png?w=300" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-684.png?w=500" tabindex="0" role="button" loading="lazy" width="1024" height="327" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-684.png?w=1024" alt=""></a><figcaption><em>BRBL MS 408, f. 70v1, MSI detail</em></figcaption></figure></div>


<p>We can immediately exclude the blue text as having been hidden/revealed, because it is inverted. So it must be either an offset from the facing page (<a href="https://collections.library.yale.edu/catalog/2002046?child_oid=1006202" target="_blank" rel="noreferrer noopener">f. 71r</a>) or show-through from the other side (f. 70r1, the central panel of <a href="https://collections.library.yale.edu/catalog/2002046?child_oid=1006199" target="_blank" rel="noreferrer noopener">this </a>foldout). If you invert the MSI image and compare it to the analogous sections of f. 70r1, you can clearly see that the faded blue text on 70v1 matches the text on 70r1, identifying these ghostly blue letters as show-through:</p>



<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/msi-results3.jpg"><img data-attachment-id="7603" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/msi-results3/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/msi-results3.jpg" data-orig-size="1280,720" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="MSI results3" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/msi-results3.jpg?w=300" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/msi-results3.jpg?w=500" tabindex="0" role="button" loading="lazy" width="1024" height="576" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/msi-results3.jpg?w=1024" alt=""></a></figure>



<p>This is why it is so important to look at the surrounding pages before drawing conclusions about what you see in a multispectral image. <em>Caveat spectator!</em></p>



		<figure>
			
			
			
		</figure>
		


<p>Folio 71r (above) is interesting because it is the only zodiac page with significant color. It seems likely that the color was added later, but we don’t yet know for sure. What’s intriguing about the multispectral image is how similar colors respond to the exposure and processing in drastically different ways – the torquoise glows bright yellow, while the green appears light blue. This suggests that it might be worthwhile to test those pigments using X-Ray Fluorescence, which uses a spectrometer to analyze the chemical compounds that make up a mineral pigment (for more on this technique, see <a href="https://manuscriptroadtrip.wordpress.com/2019/08/28/manuscript-road-trip-you-cant-argue-with-science/" target="_blank" rel="noreferrer noopener">this </a>blogpost). XRF testing was conducted on selected pages of the Voynich in 2009, but f. 71r was not one of the tested pages. The results of those tests can be read <a href="https://beinecke.library.yale.edu/sites/default/files/files/voynich_analysis.pdf" target="_blank" rel="noreferrer noopener">here</a> (tl/dr: the tests did not find anything suspicious or out of the ordinary; all of the tested pigments were consistent with medieval recipes).</p>



<p>Folio 26r presents a variety of MSI revelations. Here, we can see all three types of evidence: hidden/revealed, offset, and show-through. To demonstrate this, we need to look at three pages at once: 25v, 26r, and 26v:</p>



		<figure>
			
			
			
		</figure>
		


<p>In the red square, you can see a mirror-image offset of the leaf on f. 25v, the facing page. The fact that the offset is visible only with multispectral exposure suggests that there may be more offsets hiding elsewhere in the manuscript that could provide evidence of the original sequence of leaves in the codex. In the blue rectangle, you can easily discern the show-through from the other side (i.e. f. 26v), although it’s also visible in natural light. Generally speaking, the green pigment in the Voynich Manuscript tends to show through the parchment more dramatically than other pigments, due to its high copper signature (as identified in the XRF report I referenced above). This corrosive aspect can cause the pigment to leach deeply into the parchment, making it easily visible from the other side. Finally, there’s a mysterious and as-yet-unexlained semi-circular something in the yellow box, which is within – although not necessarily related to – the waterstain in the upper margin. It isn’t an offset, and it isn’t show-through. It was hidden and revealed, although exactly what it IS remains to be determined.</p>



		<figure>
			
			<figcaption></figcaption>
			
		</figure>
		


<p>Folio 93r (above) may have been selected for imaging because of the stain that seems to match the color of the flower. The stain and the flower respond to the exposure in identical ways, suggesting that the stain is indeed the same pigment as the flower and was likely the result of a careless spill while the artist was working. It is noteworthy that the text is written over the stain. This confirms what study of other pages reveals – that the images were drawn and colored by the artists before the text was written by the scribes (there’s actually some evidence that the artists and scribes were the same people but that’s a topic for another day). Apparently the artist/scribe wasn’t worried about the stain interfering with the text. Parchment is a valuable resource and it would not be surprising if a decision had been made to use the stained parchment rather than discard it.</p>



<p>There are a few areas of interest on f. 101v2, all of which can be seen more clearly with MSI. </p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_102v1_psc.jpg"><img data-attachment-id="7698" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/e7-ev-with-focal-plane-at-88-cm-apo-chromat-120mm-dual-filter-wheel/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_102v1_psc.jpg" data-orig-size="6132,8176" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;12.5&quot;,&quot;credit&quot;:&quot;15th-16th&quot;,&quot;camera&quot;:&quot;E7 SN:26R01339\/Lens: APO-DIGITAR 5,6\/120 M-26  (0.9630 secs)&quot;,&quot;caption&quot;:&quot;Main banks\nTransmissive&quot;,&quot;created_timestamp&quot;:&quot;1408445603&quot;,&quot;copyright&quot;:&quot;Beinecke Rare Book and Manuscript Library, Yale University&quot;,&quot;focal_length&quot;:&quot;120&quot;,&quot;iso&quot;:&quot;100&quot;,&quot;shutter_speed&quot;:&quot;0.963&quot;,&quot;title&quot;:&quot;E7 EV with focal plane at 88 cm \nApo Chromat 120mm \ndual filter wheel&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="E7 EV with focal plane at 88 cm 
Apo Chromat 120mm 
dual filter wheel" data-image-description="" data-image-caption="<p>Main banks<br />
Transmissive</p>
" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_102v1_psc.jpg?w=225" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_102v1_psc.jpg?w=500" tabindex="0" role="button" loading="lazy" width="768" height="1024" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_102v1_psc.jpg?w=768" alt=""></a><figcaption><em>BRBL MS 408, f. 101v2</em></figcaption></figure></div>


<p>Here, post-processing helps reveal the drawing beneath the rust-colored stain, the obscured text on the blue portion of the vessel, and the faded text on the heavily-damaged fold:</p>



		<figure>
			
			
			
		</figure>
		


<p>We’ve got one more page to look at: the all-important folio 116 verso, the very last page.</p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001.jpg"><img data-attachment-id="7605" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/beinecke_dl_2002046_page_207_image_0001/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001.jpg" data-orig-size="1090,1500" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Beinecke_DL_2002046_Page_207_Image_0001" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001.jpg?w=218" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001.jpg?w=500" tabindex="0" role="button" loading="lazy" width="744" height="1023" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001.jpg?w=744" alt=""></a><figcaption><em>BRBL MS 408, f. 116v</em></figcaption></figure></div>


<p>The lines of text and marginal doodles at the top are what make this page so interesting:</p>



<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001-1.jpg"><img data-attachment-id="7607" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/beinecke_dl_2002046_page_207_image_0001-2/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001-1.jpg" data-orig-size="902,380" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Beinecke_DL_2002046_Page_207_Image_0001" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001-1.jpg?w=300" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001-1.jpg?w=500" tabindex="0" role="button" loading="lazy" width="902" height="380" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001-1.jpg?w=902" alt=""></a></figure>



<p>The writing is similar to, but isn’t quite, Voynichese. It has Germanic features and looks roughly contemporary with the manuscript, although it might be a bit later. There is a similar inscription in the upper margin of <a href="https://collections.library.yale.edu/catalog/2002046?child_oid=1006106" target="_blank" rel="noreferrer noopener">f. 17r</a>, likely written by the same hand. No one really knows what these inscriptions  represent, or when they were written, or where, or why. Some have speculated that the text on f. 116v might be the key to deciphering the manuscript itself. Others interpret it as an incantation or charm. Believe it or not (since I seem to have a lot of opinions about this manuscript), I don’t have strong feelings about what this text might signify. But others do, and they will find these processed images extremely useful. MSI can definitely help clarify what we’re looking at:</p>



<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-675.jpg"><img data-attachment-id="7610" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/screenshot-675/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-675.jpg" data-orig-size="2092,758" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot (675)" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-675.jpg?w=300" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-675.jpg?w=500" tabindex="0" role="button" loading="lazy" width="1024" height="371" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-675.jpg?w=1024" alt=""></a></figure>



<p>There’s quite a lot of “noise” because of show-through from f. 116r (the lines of mirror-writing in shades of brown), but even so it is possible to clarify some readings of this mysterious text. </p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-679.jpg"><img data-attachment-id="7627" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/screenshot-679/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-679.jpg" data-orig-size="1099,544" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot (679)" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-679.jpg?w=300" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-679.jpg?w=500" tabindex="0" role="button" loading="lazy" width="1024" height="506" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-679.jpg?w=1024" alt=""></a></figure></div>


<p>At the very top, for example, the first, third, and fourth words have sometimes been thought to begin with the letter [p], but in the MSI image it appears that the 3rd and 4th words begin with a different letter, perhaps [u] or [v]: the very faint lines that appeared to be descending from those first letters in natural light turn out upon MSI capture to be stains of some kind, not ink. This is the kind of minutia that can actually be extremely important to researchers.</p>



<p>The processed multispectral images of f. 17r may also help researchers interpret the writing in the upper margin of that page:</p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-698.png"><img data-attachment-id="7735" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/screenshot-698/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-698.png" data-orig-size="2058,406" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot (698)" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-698.png?w=300" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-698.png?w=500" tabindex="0" role="button" loading="lazy" width="1024" height="202" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-698.png?w=1024" alt=""></a><figcaption><em>f. 17r, natural light vs. processed MSI (detail)</em></figcaption></figure></div>


<p>I will leave it to those working on these pages to dig deeper into the interpretation of these images.</p>



<p>A few final thoughts:</p>



<ol>
<li>These images do not show any evidence of palimpsesting. In other words, there is no evidence of underwriting that would indicate reuse of the parchment. This is important because if there HAD been underwriting, it would have been critical evidence for refining the date of origin of the manuscript. The question of the date of origin of the manuscript is not entirely resolved, but <a href="https://drive.google.com/file/d/1qLk48161VaACWDUptH6rC9zlZzDHmGFS/view?usp=sharing" target="_blank" rel="noreferrer noopener">Carbon-14 testing</a> dates the parchment, with a high level of confidence, to ca. 1425. The style of the illustrations is consistent with that date, so I consider the manuscript to have been written in the early fifteenth century, although not everyone agrees.</li>



<li>The hidden/revealed marginal texts on f. 1 support the authenticity of the manuscript as a medieval object, as opposed to a modern forgery. Here’s why. Imagine you are an early 20th-century forger trying to create an authentic-looking manuscript to dupe unsuspecting buyers (or so the argument goes). You find some unused medieval parchment, mix up some ink and pigments using medieval recipes, and get to work. You might even think to add an early-modern signature and annotations to the margins to add to the air of authenticity. But would you then fade those annotations (how would you manage that, anyway?), pour chemicals over them, and then hope that someday imaging technologies would develop that would allow future researchers to read them? Of course not. That line of reasoning defies both logic and practicality. It is much more likely that the manuscript is exactly what most believe it to be: an authentic early fifteenth-century book with traces of its history left behind by past owners and readers. </li>



<li>In the end, these particular images provide additional textual and historical evidence, but they do not provide a key to “reading” the Voynich Manuscript. They function instead as a clear proof-of-concept, indicating that more imaging of more pages would almost certainly result in additional evidence invisible to the naked eye. Such evidence could help researchers reconstruct the original order of the leaves, transcribe faded Voynichese for linguistic and cryptological analysis, or reveal the identity of previously-unknown readers and owners. Additional MSI might even uncover the key to understanding this most mysterious of manuscripts. It is possible that the entire manuscript will be imaged this way someday, but that remains to be seen. Any kind of imaging poses a risk to the manuscript due to heat and light exposure as well as the potential for physical damage (the fold-outs, for example, are extremely fragile). Although current imaging technology carries significantly less risk than that used a decade ago, the conservators and curators (and lawyers and insurance adjusters) at the Beinecke Library are the ones who are ultimately responsible for the care and survival of this amazing object. They will need to decide if the potential for research and discovery is worth the risk. It’s their call. (please don’t bug them about this – they already know!)</li>
</ol>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-701.png"><img data-attachment-id="7791" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/screenshot-701/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-701.png" data-orig-size="667,682" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot (701)" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-701.png?w=293" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-701.png?w=500" tabindex="0" role="button" loading="lazy" width="667" height="682" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-701.png?w=667" alt=""></a></figure></div>


<p>I’ve here recorded only my preliminary thoughts about these images, and I am certain there is much more to discover. I look forward to seeing how you, and the community of Voynichologists, contribute to their interpretation. <a href="https://drive.google.com/drive/folders/1mNQGKQDSCR4M_c2M2JrsU5soghvYwMig?usp=sharing" target="_blank" rel="noreferrer noopener">Let’s get to work</a>!</p>



<p>p.s. In other Voynich news, I was profiled in the September 2024 issue of <a href="https://www.theatlantic.com/magazine/archive/2024/09/decoding-voynich-manuscript/679157/?gift=YFkW3a8mqv4T0YBMneIYIui0ufgYLbKBD8uwiL8lkU0&amp;utm_source=copy-link&amp;utm_medium=social&amp;utm_campaign=share" target="_blank" rel="noreferrer noopener">The Atlantic</a> with an essay focusing on my thirty-year relationship with the Voynich Manuscript. I hope you enjoy it!</p>



<hr>



<p id="Technical-details"><strong>Technical Details</strong></p>



<p>The conditions under which each image was captured can be found in the “properties” metadata and are recorded in each pseudo-color image’s filename. According to Easton (in private correspondence), the filenames are “from the original bands after ‘calibration’ (to remove the effects of the different exposure times for different bands). This is done by measuring the reflectance of the reference ‘Spectralon’ reflector in the image. Spectralon is a Teflon derivative that has very uniform reflectivity for wavelengths in the range 250 nanometers (ultraviolet) to 2500 nanometers (infrared). Before rendering the pseudocolor image, the median of the 3×3 neighborhood surrounding each pixel was evaluated to attenuate the visibility of any statistical variations, which are particular problems in the fluorescence bands, because the number of available photons is pretty small at each pixel.”</p>



<p>“MNF” in a filename “means that the image bands were combined based on multispectral image statistics using the ‘minimum noise fraction’ algorithm. A triplet of the resulting bands was selected ‘by eye’ to render a pseudocolor image, and these bands were occasionally manipulated in PhotoShop to change the ‘hue angle’ (color tint) of the rendering, with the goal of enhancing the visibility of the text(s) of interest.” In other words, the images must be processed in order to be of greatest use to the non-expert eye. During that post-processing, Easton informs me, the goal is “to find combinations of the image bands which produce images with enhanced visibility of the erased text. The combinations may be selected based on observation and used for every leaf, or we may evaluate the multiband statistics of each pixel and use them to evaluate combinations of input bands — this is what I do in ‘principal component analysis’ (‘PCA’), Independent component analysis (‘ICA’), ‘minimum noise fraction’ (MNF), and ‘spectral angle mapping’ (SAM).”</p>




	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Debugging in the Multiverse (183 pts)]]></title>
            <link>https://antithesis.com/blog/multiverse_debugging/</link>
            <guid>41500405</guid>
            <pubDate>Tue, 10 Sep 2024 13:05:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://antithesis.com/blog/multiverse_debugging/">https://antithesis.com/blog/multiverse_debugging/</a>, See on <a href="https://news.ycombinator.com/item?id=41500405">Hacker News</a></p>
Couldn't get https://antithesis.com/blog/multiverse_debugging/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[We're in the brute force phase of AI – once it ends, demand for GPUs will too (131 pts)]]></title>
            <link>https://www.theregister.com/2024/09/10/brute_force_ai_era_gartner/</link>
            <guid>41500268</guid>
            <pubDate>Tue, 10 Sep 2024 12:52:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2024/09/10/brute_force_ai_era_gartner/">https://www.theregister.com/2024/09/10/brute_force_ai_era_gartner/</a>, See on <a href="https://news.ycombinator.com/item?id=41500268">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>AI techniques that require specialist hardware are "doomed," according to analyst firm Gartner's chief of research for AI Erick Brethenoux – who included GPUs in his definition of endangered kit.</p>
<p>Speaking to <i>The Register</i> at Gartner's Symposium in Australia today, Brethenoux said in the 45 years he's spent observing AI, numerous hardware vendors have offered offer specialist kit for AI workloads. All failed once vanilla machines could do the job – as they always eventually can.</p>
<p>The need for specialist hardware, he observed, is a sign of the "brute force" phase of AI, in which programming techniques are yet to be refined and powerful hardware is needed. "If you cannot find the elegant way of programming … it [the AI application] dies," he added.</p>

    

<p>He suggested generative AI will not be immune to this trend.</p>

        


        

<p>The good news is, he believes organizations can benefit from AI without generative AI.</p>
<p>"Generative AI is 90 percent of the airwaves and five percent of the use cases," he noted – and users have already learned that lesson.</p>

        

<p>Brethenoux described the period from late 2022 to early 2024 as a "recess" in which IT shops "stopped thinking about things that make money" and explored generative AI instead. Those efforts have largely led orgs back to the AI they already use – or to "composite AI" that uses generative AI alongside established AI techniques like machine learning, knowledge graphs, or rule-based systems.</p>
<p>Organizations have realized that AI may already be making a big contribution to the business in many scenarios that engineers appreciate – such as machine learning informing predictive maintenance apps – but which never caught the eye of execs or the board. Recess is over.</p>
<p>An example of composite AI at work could be generative AI creating text to describe the output of a predictive maintenance application. <i>The Register</i> has often heard the same scenario applied to software that analyzes firewall logs and which now uses generative AI to make prose recommendations about necessary actions that improve security – and even writes new firewall rules to enact them.</p>

        

<p>Brethenoux recalled that some orgs he speaks to still think generative AI can power their next application. He often tells them the same outcome can be achieved more quickly – at lower cost – with an established AI technique.</p>
<ul>

<li><a href="https://www.theregister.com/2024/09/09/equinix_ai_business_case/">GenAI hype meets harsh reality as enterprises wrestle with business case</a></li>

<li><a href="https://www.theregister.com/2024/09/09/microsoft_arun_ulag_ai/">Microsoft exec warns of business functions being sacrificed on the altar of AI</a></li>

<li><a href="https://www.theregister.com/2024/09/09/gartner_synmposium_ai_opinion/">AI bills can blow out by 1000 percent: Gartner</a></li>

<li><a href="https://www.theregister.com/2024/09/05/amazon_q_developer_gartner/">Amazon congratulates itself for AI code that mostly works</a></li>
</ul>
<p>Gartner's Symposium featured another session with similar themes.</p>
<p>Titled "When not to use generative AI," it featured vice president and distinguished analyst Bern Elliot pointing out that Gen AI has no reasoning powers and produces only "a probabilistic sequence" of content. Even so, Elliot said Gen AI hype has reached two to three times the volume Gartner has seen for any previous tech. Generative AI is, in short, being asked to solve problems it was not designed to solve.</p>
<p>Elliot recommended not using it to tackle tasks other than content generation, knowledge discovery, and powering conversational user interfaces.</p>
<p>Even in those roles, he described the tech as "Unreliable like Swiss cheese: you know it has holes, you just don't know where they are until you cut it."</p>
<p>Elliot conceded that improvements to Gen AI have seen the frequency with which it "hallucinates" – producing responses with no basis in fact – fall to one or two percent. But he warned users not to see that improvement as a sign the tech is mature. "It's great until you do a lot of prompts – millions of hallucinations in production is a problem!"</p>
<p>Like Brethenoux, Elliot therefore recommended composite AI as a safer approach, and adopting guardrails that use a non-generative AI technique to check generative results. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Show HN: Free tool to find RSS feeds, even if not linked on the page (114 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=41499905</link>
            <guid>41499905</guid>
            <pubDate>Tue, 10 Sep 2024 12:05:01 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=41499905">Hacker News</a></p>
Couldn't get https://news.ycombinator.com/item?id=41499905: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>