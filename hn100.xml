<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 10 May 2024 12:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[The most backdoor-looking bug I've ever seen (2021) (119 pts)]]></title>
            <link>https://words.filippo.io/dispatches/telegram-ecdh/</link>
            <guid>40315274</guid>
            <pubDate>Fri, 10 May 2024 03:33:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://words.filippo.io/dispatches/telegram-ecdh/">https://words.filippo.io/dispatches/telegram-ecdh/</a>, See on <a href="https://news.ycombinator.com/item?id=40315274">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <!--kg-card-begin: markdown--><p>This is the story of a bug that was discovered and fixed in Telegram's self-rolled cryptographic protocol about seven years ago. The bug didn't get any press, and no one seems to know about it, probably because it was only published in Russian.</p>
<p>To this day, it's the most backdoor-looking bug I've ever seen.</p>
<p>Google Translate does a good enough job on the <a href="https://habrahabr.ru/post/206900/?ref=words.filippo.io">original article</a>, which is still available on Habr, but I'm going to walk you through it along with some context.</p>
<p>Telegram is a popular chat app that uses its own... bizarre protocol to encrypt chats, called MTProto. The protocol is used both to encrypt all messages to the Telegram server, and to encrypt opt-in 1:1 end-to-end "Secret Chats".<sup><a href="#fn1" id="fnref1">[1]</a></sup> In text I can't do justice to the facial expressions of cryptographers when you mention Telegram's protocol, so just believe me that it's <em>weird</em>.</p>
<p>The current consensus seems to be that the latest version is not broken in known ways that are severe or relevant enough to affect end users, assuming the implementation is correct. That is about as safe as leaving exposed wires around your house because they are either not live or placed high enough that no one should touch them.</p>
<p>The original version was, however, completely broken, in the most puzzling of ways.</p>
<p>End-to-end Telegram chat sessions use <a href="https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange?ref=words.filippo.io">finite-field Diffie-Hellman</a><sup><a href="#fn2" id="fnref2">[2]</a></sup> to establish a shared key between the two participants. The negotiation happens through messages relayed by the Telegram server. Diffie-Hellman is a fundamental building block of many cryptosystems, and it allows two parties to establish a shared secret that any eavesdroppers can't derive. It is however only one part of a secure key exchange, because an attacker capable of intercepting the messages could simply establish two separate sessions with the two parties, carrying out a <a href="https://en.wikipedia.org/wiki/Person-in-the-middle_attack?ref=words.filippo.io">Person-in-the-Middle</a> attack. The parties need some way to verify they derived the same secret. In TLS, they use a signature from a certificate. In most secure chat apps, there is a fingerprint ("Safety Numbers" in Signal) that the two parties can compare out-of-band.<sup><a href="#fn3" id="fnref3">[3]</a></sup> What's important is that if the two sides derived the same secret, they can be sure no one else has access to it.</p>
<p>The Telegram key exchange is described in <a href="https://web.archive.org/web/20131220000537/https://core.telegram.org/api/end-to-end#key-generation">the "Key Generation" section of Telegram's end-to-end API docs</a>. Concretely, Alice requests the DH parameters <code>(p, g)</code> from Telegram, painstakingly verifies them, computes a random <code>a</code> value, and sends <code>g^a mod p</code> to Telegram. Bob receives <code>(p, g, g^a mod p)</code>, similarly computes <code>b</code> and <code>g^b mod p</code>, and sends the latter back (along with a truncated hash of the derived key, for some reason).</p>
<p>Now, normally the two sides would compute the shared key as <code>(g^a)^b mod p</code> and <code>(g^b)^a mod p</code>. Instead, the original version of MTProto computed it as</p>
<pre><code>(g^a)^b mod p XOR nonce
</code></pre>
<p>where <code>nonce</code> was an arbitrary, supposedly random value sent by the server along with the peer's public contribution.</p>
<p>This was a completely non-standard and useless addition, and all it did was let the server perform an undetected Person-in-the-Middle attack. Let's see how.</p>
<p>In a normal PitM, the server negotiates two separate Diffie-Hellman sessions with Alice and Bob, who end up with different shared keys, which they could detect by comparing fingerprints.</p>
<pre><code>Alice                     Telegram              Bob

a = random()       
A = g^a mod p       -&gt;
                        t = random()
                        T = g^t mod p -&gt;
                                          b = random()
                                      &lt;-  B = g^b mod p
                                          key = T^b mod p
                    &lt;-  T
key = T^a mod p

                    T^a mod p != T^b mod p
</code></pre>
<p>With the nonce addition, however, the server could "fix" Alice's key to match Bob's by manipulating Alice's nonce. The two parties would end up with the same fingerprint, and couldn't tell that an attack happened, but the server (and no one else) would know the shared key, allowing it to decrypt all messages.</p>
<pre><code>nonce_bob = random()
key_bob = T^b mod p  XOR  nonce_bob

nonce_alice = A^t mod p  XOR  B^t mod p  XOR  nonce_bob
key_alice = T^a mod p  XOR  nonce_alice =
  T^a mod p  XOR  (A^t mod p  XOR  B^t mod p  XOR  nonce_bob) =
  B^t mod p  XOR  nonce_bob = key_bob
</code></pre>
<p>Why do I say this addition was useless? Because it literally had no purpose! Indeed, the vulnerability was <a href="https://web.archive.org/web/diff/20131220000537/20131225140924/http://core.telegram.org/api/end-to-end">fixed by silently removing the nonce step from the docs</a>.<sup><a href="#fn4" id="fnref4">[4]</a></sup> <a href="https://core.telegram.org/constructor/encryptedChatRequested?layer=11&amp;ref=words.filippo.io">A later API revision</a> removed the nonce parameter with the caption "Improve secret chats". All <a href="https://web.archive.org/web/20131028041748/http://core.telegram.org/constructor/encryptedChatRequested">the original API reference</a> said about the nonce is "Random server sequence for calculation of key".</p>
<p>I never heard a plausible explanation for why the designers of MTProto went out of their way to add useless complexity to their protocol, with the only outcome of making undetectable interception possible.</p>
<p><strong>Edit (2021-01-11)</strong>: <a href="https://twitter.com/asdofindia/status/1348491279798128641?ref=words.filippo.io">@asdofindia linked me on Twitter</a> to <a href="https://telegram.org/blog/crowdsourcing-a-more-secure-future?ref=words.filippo.io">an official statement by Telegram about this</a> that I couldn't find anymore. It claims the nonce was there to protect clients with weak random number generators. Here's what I had buried into a footnote when I couldn't find a citation to attribute that explanation to Telegram:</p>
<blockquote>
<p>This doesn't make sense for a number of reasons: 1) clients with weak randomness are likely to be toast anyway, because Telegram's bizarro not-a-MAC relies on randomness in the payload to avoid an offline decryption oracle (there is a plaintext hash of the payload on the wire, I told you this was weird!); 2) the API also allows clients to request random bytes from the server to XOR with their secret share; and 3) defending against weak randomness by relying on a server contribution defends against everything but the server, which is the relevant attacker in the end-to-end setting. (Said another way, anyone that can intercept client-server messages can see the extra randomness, making it moot.) Non-practitioners might think this is a reasonable defense in depth, belts and suspenders kind of thing, but in cryptography engineering adding complexity to defend against scenarios that lead to compromise anyway is simply pointless.</p>
</blockquote>
<p>Anyway, it's been a while, the world is a different place now, and maybe <a href="https://en.wikipedia.org/wiki/Hanlon%27s_razor?ref=words.filippo.io">Hanlon's razor</a> cuts deeper than I thought. I think there are better reasons not to use Telegram today than this old bug<sup><a href="#fn1" id="fnref1:1">[1:1]</a></sup>, but it's still what I think about every time people talk about far-fetched "bugdoors". The bar is high!</p>
<h2 id="the-picture">The picture</h2>
<p>In other news, this newsletter is going to pivot into Rome photoblogging. (Not really, if you made it this far and like cryptography engineering, you should <a href="https://buttondown.email/cryptography-dispatches?tag=header&amp;ref=words.filippo.io">subscribe</a> or <a href="https://twitter.com/FiloSottile?ref=words.filippo.io">follow me on Twitter</a>.)</p>
<p><img src="https://words.filippo.io/content/images/2022/01/ee618b89-a8fa-45a2-af01-6f9955d2c99a.jpeg" alt="St. Peter's reflecting in the Tevere" loading="lazy"></p>
<hr>
<section>
<ol>
<li id="fn1"><p>By the way, aside from all the cryptographic weirdness and the unexplained backdoor-looking bug, the real reason you should not trust Telegram's encryption is that it's off by default, inconvenient to use, and simply unavailable in groups, meaning most messages flow unencrypted on Telegram's servers. Nonetheless, Telegram markets itself as a secure chat app, with misleading copy along the lines of "everything is encrypted, Secret Chats are just <em>more</em> encrypted!" They explain in their FAQ that it's all about backups, and that other more secure apps "<a href="https://telegram.org/faq?ref=words.filippo.io#dev_page_content:~:text=Other%20apps%20ignore%20the%20need%20for,before%20ever%20reaching%20a%20million%20users.">never reach a million users</a>". <a href="https://twitter.com/signalapp/status/1347240006444675072?ref=words.filippo.io">In other news</a>. <a href="#fnref1">↩︎</a> <a href="#fnref1:1">↩︎</a></p>
</li>
<li id="fn2"><p>Diffie-Hellman over finite fields is how it was originally designed, but today we'd use Elliptic-Curve Diffie-Hellman, which is faster, has smaller outputs, and is safer. FFDH has many of <a href="https://buttondown.email/cryptography-dispatches/archive/557475c5-9781-47e0-a640-5734bc849bc7?ref=words.filippo.io">the same issues as DSA</a> (FFDH is to DSA like ECDH is to ECDSA and EdDSA.) Current-day MTProto 2.0 still uses FFDH, but that's far from the most anachronistic choice in it. <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p>This is admittedly not a particularly strong authentication strategy, but it relies on the assumption that even if 1% of users check their fingerprints, systematic PitM is likely to be detected, and high-risk users can be extra careful and consistently check fingerprints. I hope solutions like key transparency can improve this picture in the coming years without changing the default UX. <a href="#fnref3">↩︎</a></p>
</li>
<li id="fn4"><p>Can we talk about how cool the Wayback Machine Compare feature is? Now is a good time to <a href="https://archive.org/donate/">donate to the Internet Archive</a>, by the way. <a href="#fnref4">↩︎</a></p>
</li>
</ol>
</section>
<!--kg-card-end: markdown-->
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wprs – rootless remote desktop for Wayland (and X11, via XWayland) applications (109 pts)]]></title>
            <link>https://github.com/wayland-transpositor/wprs</link>
            <guid>40313798</guid>
            <pubDate>Thu, 09 May 2024 23:02:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/wayland-transpositor/wprs">https://github.com/wayland-transpositor/wprs</a>, See on <a href="https://news.ycombinator.com/item?id=40313798">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">wprs</h2><a id="user-content-wprs" aria-label="Permalink: wprs" href="#wprs"></a></p>
<p dir="auto">Like <a href="https://en.wikipedia.org/wiki/Xpra" rel="nofollow">xpra</a>, but for Wayland, and written in
Rust.</p>
<p dir="auto">wprs implements rootless remote desktop access for remote Wayland (and X11, via
XWayland) applications.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto"><code>cargo build --profile=release-lto  # or release, but debug is unusably slow</code></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">On the remote host, enable wprsd:</p>
<div dir="auto" data-snippet-clipboard-copy-content="loginctl enable-linger
systemctl --user enable wprsd.service
systemctl --user start wprsd.service"><pre>loginctl enable-linger
systemctl --user <span>enable</span> wprsd.service
systemctl --user start wprsd.service</pre></div>
<p dir="auto">On the local host:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# starts application on the remote host (starts ssh connection, forwards sockets, starts wprsc, runs application)
wprs <remote_host> run <application>

# stops local wprs connections, leaving remote session running (tear down ssh connection and forwarded sockets, stops wprsc)
wprs <remote_host> detach

# attaches to remote wprs session (starts ssh connection, forwards sockets, starts wprsc)
wprs <remote_host> attach"><pre><span><span>#</span> starts application on the remote host (starts ssh connection, forwards sockets, starts wprsc, runs application)</span>
wprs <span>&lt;</span>remote_host<span>&gt;</span> run <span>&lt;</span>application<span>&gt;</span>

<span><span>#</span> stops local wprs connections, leaving remote session running (tear down ssh connection and forwarded sockets, stops wprsc)</span>
wprs <span>&lt;</span>remote_host<span>&gt;</span> detach

<span><span>#</span> attaches to remote wprs session (starts ssh connection, forwards sockets, starts wprsc)</span>
wprs <span>&lt;</span>remote_host<span>&gt;</span> attach</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">System Tuning</h2><a id="user-content-system-tuning" aria-label="Permalink: System Tuning" href="#system-tuning"></a></p>
<p dir="auto">Increasing linux's socket buffer limits as described in
<a href="https://wiki.archlinux.org/title/sysctl#Increase_the_memory_dedicated_to_the_network_interfaces" rel="nofollow">https://wiki.archlinux.org/title/sysctl#Increase_the_memory_dedicated_to_the_network_interfaces</a>
will result in improved performance.</p>
<p dir="auto">TODO: test ssh socket forwarding performance with different values of
wmem_default. wprs uses setsockopt to increase its buffer size, but it doesn't
seem that ssh does.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration Files</h2><a id="user-content-configuration-files" aria-label="Permalink: Configuration Files" href="#configuration-files"></a></p>
<p dir="auto">You can create configuration files for <code>wprsc</code> and <code>wprsd</code> instead of passing additional
arguments to <code>wprs</code>. To see what options are available, run <code>wprsc --help</code> and
<code>wprsd --help</code>.</p>
<p dir="auto">To generate the default configs, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# on your local machine
wprsc --print-default-config-and-exit=true > ~/.config/wprs/wprsc.ron"><pre><span><span>#</span> on your local machine</span>
wprsc --print-default-config-and-exit=true <span>&gt;</span> <span>~</span>/.config/wprs/wprsc.ron</pre></div>
<p dir="auto">and</p>
<div dir="auto" data-snippet-clipboard-copy-content="# on your remote machine
wprsd --print-default-config-and-exit=true > ~/.config/wprs/wprsd.ron"><pre><span><span>#</span> on your remote machine</span>
wprsd --print-default-config-and-exit=true <span>&gt;</span> <span>~</span>/.config/wprs/wprsd.ron</pre></div>
<p dir="auto">Then update the <code>wprsc.ron</code> and <code>wprsd.ron</code> files with your desired settings.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Current Limitations</h2><a id="user-content-current-limitations" aria-label="Permalink: Current Limitations" href="#current-limitations"></a></p>
<p dir="auto">Currently only the the Core and XDG shell protocols are implemented. In
particular, hardware rendering/dmabuf support is not yet implemented.</p>
<ul dir="auto">
<li>Damage passthough is not yet implemented.</li>
<li>Touch event support is not yet implemented.</li>
<li>Drag-and-drop may be wonky in some cases.</li>
<li>XWayland drag-and-drop is not (yet?) implemented.</li>
<li>webauthn security keys don't yet work in browsers</li>
</ul>
<p dir="auto">Generally, wprs will aim to support as many protocols as feasible, it's a
question of time and prioritization.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<p dir="auto">On the remote (server) side, <code>wprsd</code> implements a wayland compositor using
<a href="https://github.com/Smithay/smithay">Smithay</a>. Instead of compositing and
rendering though, wprsd serializes the state of the wayland session and sends it
to the connected wprsc client using a custom protocol.</p>
<p dir="auto">On the local (client) side, <code>wprsc</code> implements a wayland client (using the
<a href="https://github.com/Smithay/client-toolkit">Smithay Client Toolkit</a> that creates
local wayland objects that correspond to remote wayland objects. For example, if
a remote application running against wprsd creates a surface and an
xdg-toplevel, wprsc will create a surface with the same contents, an
xdg-toplevel with the same metadata, etc.. From the local compositor's point of
view, wprsc is just a normal application with a bunch of windows. Input and
other events from the local compositor that wprsc are serialized and sent to
wprsd, which forwards them to the appropriate application (the owner of the
surface which the wprsc surface which received the events corresponds to).</p>
<p dir="auto">wprs supports session resumption (wprsc disconnection and later reconnection and
wprsc restarts). The wayland protocol is not natively resumable in this way
because it relies on shared state between the compositor and client
applications. By implementing a wayland compositor locally relative to the
application, wprsd stores all state necessary for wayland applications and is
also able to store sufficient state (e.g., the buffer contents for each surface
as of the last commit) for a newly-connected wprsc to correctly set up all
necessary wayland objects. wprsc is stateless, but wprsd is not, so a wprsd
restart will still terminate all wayland applications running against it, like
with any other wayland compositor.</p>
<p dir="auto">Communication between wprsd and wprsc happens over unix domain sockets; wprsd
creates a socket and wprsc connects to it. The default mode of operation is to,
on the client side, use ssh to forward a local socket to the remote wprsd
socket, but a different transport could be used with, for example, socat or a
custom proxy application. A launcher script (<code>wprs</code>) is provided which sets up
the ssh socket forwarding.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Protocol</h3><a id="user-content-protocol" aria-label="Permalink: Protocol" href="#protocol"></a></p>
<p dir="auto">The custom protocol used to serialize and transmit wayland state between wprsc
and wprsd is a simplified version of the wayland protocol. Wayland objects are
represented as rust types and serialized using
<a href="https://github.com/rkyv/rkyv">rkyv</a>. Unlike the wayland protocol, the wprs
protocol tries to be idempotent when possible. For example, instead of the
repeated back-and-forth involved in created a surface, creating an xdg-surface,
creating an xdg-toplevel, waiting for it to be configured, creating a buffer,
attaching the buffer, and comitting it, wprsd will send a single commit message
to wprsc with the complete state of the surface (surface's attached buffer
contents (if any), its role (if any) and any associated metadata, etc.) and
wprsc will execute the appropriate dance with the local compositor.</p>
<p dir="auto">Frame callbacks are scheduled locally by wprsd at the configured framerate, they
are not forwarded from wprsc as that would introduce an unacceptable amount of
frame latency due to network round-trips. When no wprsc is connected, wprsd
pauses sending frame callbacks to wayland applications.</p>
<p dir="auto">Buffer compression is handled using a custom multithreaded and SIMD-accelerated
lossless image compression algorithm:</p>
<ol dir="auto">
<li>Transpose the image from an <a href="https://en.wikipedia.org/wiki/AoS_and_SoA" rel="nofollow">array of structures to a struct of
arrays</a>. This makes the subequent
steps significantly faster by letting them be implemented with SIMD
instructions and additionally improves the compression ratio because each
color channel is more closely spatially correlated with itself than with the
other
channels.</li>
<li>Apply an adjacent (wrapping) difference to each color channel (differential
pulse-code modulation). This improves the compression ratio by taking
advantage of spatial correlation and transforms (for example) a solid-colored
line into a single color byte and then a sequence of 0-bytes, or a gradient
into a sequence of 1-bytes, etc.</li>
<li>Transform each color channel into a
<a href="https://en.wikipedia.org/wiki/Y%E2%80%B2UV" rel="nofollow">YUV</a>-like color space: <code>y := g, u := b - g, v := r - g, a := a</code>. This improves the compression ratio in a
similar way as the previous step but by taking advantage of cross-color
correlation.</li>
<li>Compress the data with zstd.
This algorithm was designed for reasonably good compression ratios while being
extremely last: single-digit milliseconds per frame. Decompression is done by
inverting those steps.</li>
</ol>
<p dir="auto">This protocol is <em>not stable</em>: there is no guarantee that different versions of
wprsc and wprsd, or wprsc and wprsd built with different versions of
dependencies or even rustc will be compatible. This may change in the future,
but it will not happen soon.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Comparison to Waypipe</h3><a id="user-content-comparison-to-waypipe" aria-label="Permalink: Comparison to Waypipe" href="#comparison-to-waypipe"></a></p>
<p dir="auto"><a href="https://gitlab.freedesktop.org/mstoeckl/waypipe" rel="nofollow">Waypipe</a>'s model is analogous
to X forwarding, while wprs's model is analgous to Xpra. Waypipe ~transparently
forwards messages between the local compositor and the remote application, so
the client ends up being stateful and sessions can only be resumed through
network reconnections, not client restarts. There are tradeoffs to the two
approaches. Waypipe's approach is partially forward-compatible: it can support
new wayland protocols automatically, however those protocols may be broken if
they use shared resources in a way that waypipe doesn't know how to handle.
wprs, on the other hand, requires explicit implementation for every wayland
protocol.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">XWayland</h3><a id="user-content-xwayland" aria-label="Permalink: XWayland" href="#xwayland"></a></p>
<p dir="auto">XWayland support is implemented as a separate binary, <code>xwayland-xdg-shell</code>. The
binary implements a wayland compositor (but only for the protocol features used
by xwayland) and client, just like wprsd and wprsc, but in a single binary (so
skipping the serialization/deserialization). This is the same model as
<a href="https://github.com/talex5/wayland-proxy-virtwl#xwayland-support">xwayland-proxy-virtwl</a>,
which is itself inspired by
<a href="https://chromium.googlesource.com/chromiumos/platform2/+/main/vm_tools/sommelier/" rel="nofollow">sommelier</a>.
xwayland-xdg-shell was primarily written (instead of just using
xwayland-proxy-virtwl) so as to share a common design/codebase with wprs and to
make use of common wayland development in the form of Smithay and its wayland
crates. Additionally, xwayland-xdg-shell is more narrowly focused and its sole
purpose is xwayland support, not virtio-gpu or virtwl.</p>
<p dir="auto">Like xwayland-proxy-virtwl, xwayland-xdg-proxy can be used to implement external
xwayland support for any wayland compositor instead of re-implementing it inside
the compositor. Aside from eliminating the need to implement xwayland support in
every compositor, this approach has been reported to result in better xwayland
scaling than native xwayland support in some compositor, and it allows xwayland
applications to be treated more like regular wayland applications instead of
getting special access.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Security</h3><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<p dir="auto">wprsd is a wayland compositor, so it has access to all surfaces displayed by
applications running against it and it can inject input into them. Any process
which implements the wprs protocol and connects to the wprs socket will have the
same access. For that reason, the wprs socket is created in a directory which
only the user has access to ($XDG_RUNTIME_DIR) and the socket itself is only
readable/writable by the user. Malicious applications running as the same user
as wprsd can still access this socket, but at that point you have bigger
problems.</p>
<p dir="auto">wprs does not do any auth of its own, it relies entirely on whatever transport
is being used (ssh, in the default case).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Thanks</h2><a id="user-content-thanks" aria-label="Permalink: Thanks" href="#thanks"></a></p>
<p dir="auto">Huge thanks to the following excellent projects for making this project
significantly easier than it otherwise would have been:</p>
<ul dir="auto">
<li><a href="https://github.com/Smithay">Smithay</a></li>
<li><a href="https://github.com/rkyv/rkyv">rkyv</a></li>
<li><a href="https://github.com/tokio-rs/tracing">tracing</a></li>
<li><a href="https://github.com/wolfpld/tracy">Tracy</a></li>
</ul>
<p dir="auto">Thanks to <a href="https://gitlab.freedesktop.org/mstoeckl/waypipe" rel="nofollow">Waypipe</a> and
<a href="https://github.com/talex5/wayland-proxy-virtwl#xwayland-support">xwayland-proxy-virtwl</a>
for paving the way in this problem space.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple apologizes for iPad 'Crush' ad that 'missed the mark' (208 pts)]]></title>
            <link>https://www.theverge.com/2024/5/9/24153113/apple-ipad-ad-crushing-apology</link>
            <guid>40313733</guid>
            <pubDate>Thu, 09 May 2024 22:50:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/5/9/24153113/apple-ipad-ad-crushing-apology">https://www.theverge.com/2024/5/9/24153113/apple-ipad-ad-crushing-apology</a>, See on <a href="https://news.ycombinator.com/item?id=40313733">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Apple has apologized after a commercial meant to showcase its <a href="https://www.theverge.com/24151128/apple-ipad-pro-2024-hands-on">brand-new iPad Pro</a> drew widespread criticism among the creative community. In a statement <a href="https://adage.com/article/digital-marketing-ad-tech-news/apple-apologizes-ipad-pro-crushed-ad-it-missed-mark/2559321">provided to <em>Ad Age</em></a>, Tor Myhren, Apple’s vice president of marketing, said the company “missed the mark.”</p><p>“Creativity is in our DNA at Apple, and it’s incredibly important to us to design products that empower creatives all over the world,” Myhren told <em>Ad Age</em>. “Our goal is to always celebrate the myriad of ways users express themselves and bring their ideas to life through iPad. We missed the mark with this video, and we’re sorry.”</p><p>On Tuesday, Apple introduced the <a href="https://www.theverge.com/24151128/apple-ipad-pro-2024-hands-on">M4-powered iPad Pro</a>, which the company described as its thinnest product ever. To advertise all the creative possibilities with the iPad, it released a “Crush!” commercial that shows things like a piano, record player, paint, and other works flattening under the pressure of a hydraulic press. At the end, only one thing remains: an iPad Pro.</p><p>The ad rubbed some creatives the wrong way. Hugh Grant <a href="https://twitter.com/HackedOffHugh/status/1788183871504204257">called it</a> a “destruction of human experience,” while <em>Handmaid’s Tale</em> director Reed Morano <a href="https://twitter.com/reedmorano/status/1788298509780685261">told Apple CEO</a> Tim Cook to “read the room” in a post on X. Apple didn’t immediately respond to <em>The Verge</em>’s request for comment.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The world has probably passed peak pollution (154 pts)]]></title>
            <link>https://www.sustainabilitybynumbers.com/p/peak-pollution</link>
            <guid>40313451</guid>
            <pubDate>Thu, 09 May 2024 22:11:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sustainabilitybynumbers.com/p/peak-pollution">https://www.sustainabilitybynumbers.com/p/peak-pollution</a>, See on <a href="https://news.ycombinator.com/item?id=40313451">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>The health impacts of air pollution are often underrated. There are a </span><a href="https://ourworldindata.org/data-review-air-pollution-deaths" rel="">range of estimates</a><span> for how many people die prematurely from local air pollution every year.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-144199988" href="https://www.sustainabilitybynumbers.com/p/peak-pollution#footnote-1-144199988" target="_self" rel="">1</a></span><span> All are in the low millions. The World Health Organization </span><a href="https://www.who.int/health-topics/air-pollution" rel="">estimates around</a><span> 7 million.</span></p><p>The good news, then, is that the world is probably passed “peak pollution”. I say “probably” because confidently declaring a peak is, apparently, the best way to make sure it doesn’t happen.</p><p><span>Here, I’m talking specifically about emissions of harmful </span><em>local</em><span> air pollutants: gases like nitrogen oxides (NOx), sulphur dioxide which causes acid rain, carbon monoxide, black carbon, organic carbon, non-methane volatile organic compounds. I’m not talking about greenhouse gases.</span></p><p><span>The </span><a href="https://github.com/JGCRI/CEDS/tree/master" rel="">Community Emissions Data System (CEDS)</a><span> recently extended its long-term dataset on emissions of air pollutants up to the end of 2022.</span></p><p><span>I updated this data in our explorer tool </span><a href="https://ourworldindata.org/explorers/air-pollution" rel="">on Our World in Data</a><span> (where you can explore the trends by country).</span></p><p>What’s striking is that emissions appear to have peaked for all of these pollutants, with the exception of ammonia, which is almost entirely produced by agriculture. Organic carbon and NMVOCs are not quite out of the clear yet, but might not reach their previous peaks again.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png" width="1456" height="1028" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1028,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8407a523-3c57-4fd2-a5a5-43c092800f6e_1600x1130.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Of course, emissions are not falling everywhere. They’ve fallen steeply in richer countries like the US and much of Europe. And the big turning point for the global figures has been the rapid turnaround in China. Emissions have declined rapidly in the last decade, with huge gains for public health.</p><p><span>It’s in low and lower-middle income countries where emissions are still rising, and pollution levels in cities are the highest. This is not surprising: air pollution is one of the few areas where the “</span><a href="https://en.wikipedia.org/wiki/Kuznets_curve#Environmental_Kuznets_curve" rel="">Environmental Kuznets Curve</a><span>” tells a pretty accurate and consistent story.</span></p><p>Air pollution increases as countries develop, gain access to energy, and industrialise. They then fall once a country gets rich enough to impose pollution standards and limits without infringing on development and the move away from energy poverty.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png" width="1456" height="1028" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1028,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F463777f7-346d-40ca-94da-d0f2027445c0_1600x1130.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The goal now is to see if countries can move through this curve much faster – and with lower levels of pollution – than countries like the US or the UK did. This should be doable: we’ve learned a lot over the last 50 years about how to produce energy with less pollution, what technologies work and don’t work, and have reduced the costs of solutions that were expensive in their early days.</p><p><span>Note that this is not a finger-pointing exercise where rich countries tell poorer ones not to pollute. We’re mostly talking about </span><em>local</em><span> air pollution. The negative impacts of pollution are felt by domestic populations. It’s about how we ensure that the poorest countries can gain access to energy, alleviate poverty, and develop while limiting the number of people who die prematurely from air pollution in the process.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cubic millimetre of brain mapped in spectacular detail (151 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-024-01387-9</link>
            <guid>40313193</guid>
            <pubDate>Thu, 09 May 2024 21:36:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-024-01387-9">https://www.nature.com/articles/d41586-024-01387-9</a>, See on <a href="https://news.ycombinator.com/item?id=40313193">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <p>Researchers have mapped a tiny piece of the human brain in astonishing detail. The resulting cell atlas, which was described today in <i>Science</i><sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup> and is <a href="https://h01-release.storage.googleapis.com/data.html" data-track="click" data-label="https://h01-release.storage.googleapis.com/data.html" data-track-category="body text link">available online</a>, reveals new patterns of connections between brain cells called neurons, as well as cells that wrap around themselves to form knots, and pairs of neurons that are almost mirror images of each other.</p><p>The 3D map covers a volume of about one cubic millimetre, one-millionth of a whole brain, and contains roughly 57,000 cells and 150 million synapses — the connections between neurons. It incorporates a colossal 1.4 petabytes of data. “It’s a little bit humbling,” says Viren Jain, a neuroscientist at Google in Mountain View, California, and a co-author of the paper. “How are we ever going to really come to terms with all this complexity?”</p><h2>Slivers of brain</h2><p>The brain fragment was taken from a 45-year-old woman when she underwent surgery to treat her epilepsy. It came from the cortex, a part of the brain involved in learning, problem-solving and processing sensory signals. The sample was immersed in preservatives and stained with heavy metals to make the cells easier to see. Neuroscientist Jeff Lichtman at Harvard University in Cambridge, Massachusetts, and his colleagues then cut the sample into around 5,000 slices — each just 34 nanometres thick — that could be imaged using electron microscopes.</p><p>Jain’s team then built artificial-intelligence models that were able to stitch the microscope images together to reconstruct the whole sample in 3D. “I remember this moment, going into the map and looking at one individual synapse from this woman’s brain, and then zooming out into these other millions of pixels,” says Jain. “It felt sort of spiritual.”</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-01387-9/d41586-024-01387-9_27068610.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-01387-9/d41586-024-01387-9_27068610.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Rendering of a neuron with a round base and many branches, on a black background." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-01387-9/d41586-024-01387-9_27068610.jpg">
  <figcaption>
   <p><span>A single neuron (white) shown with 5,600 of the axons (blue) that connect to it. The synapses that make these connections are shown in green.</span><span>Credit: Google Research &amp; Lichtman Lab (Harvard University). Renderings by D. Berger (Harvard University)</span></p>
  </figcaption>
 </picture>
</figure><p>When examining the model in detail, the researchers discovered unconventional neurons, including some that made up to 50 connections with each other. “In general, you would find a couple of connections at most between two neurons,” says Jain. Elsewhere, the model showed neurons with tendrils that formed knots around themselves. “Nobody had seen anything like this before,” Jain adds.</p><p>The team also found pairs of neurons that were near-perfect mirror images of each other. “We found two groups that would send their dendrites in two different directions, and sometimes there was a kind of mirror symmetry,” Jain says. It is unclear what role these features have in the brain.</p><h2>Proofreaders needed</h2><p>The map is so large that most of it has yet to be manually checked, and it could still contain errors created by the process of stitching so many images together. “Hundreds of cells have been ‘proofread’, but that’s obviously a few per cent of the 50,000 cells in there,” says Jain. He hopes that others will help to proofread parts of the map they are interested in. The team plans to produce similar maps of brain samples from other people — but a map of the entire brain is unlikely in the next few decades, he says.</p><p>“This paper is really the tour de force creation of a human cortex data set,” says Hongkui Zeng, director of the Allen Institute for Brain Science in Seattle. The vast amount of data that has been made freely accessible will “allow the community to look deeper into the micro-circuitry in the human cortex”, she adds.</p><p>Gaining a deeper understanding of how the cortex works could offer clues about how to treat some psychiatric and neurodegenerative diseases. “This map provides unprecedented details that can unveil new rules of neural connections and help to decipher the inner working of the human brain,” says Yongsoo Kim, a neuroscientist at Pennsylvania State University in Hershey.</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sioyek is a PDF viewer with a focus on textbooks and research papers (255 pts)]]></title>
            <link>https://github.com/ahrm/sioyek</link>
            <guid>40313143</guid>
            <pubDate>Thu, 09 May 2024 21:28:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ahrm/sioyek">https://github.com/ahrm/sioyek</a>, See on <a href="https://news.ycombinator.com/item?id=40313143">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Sioyek</h2><a id="user-content-sioyek" aria-label="Permalink: Sioyek" href="#sioyek"></a></p>
<p dir="auto">Sioyek is a PDF viewer with a focus on textbooks and research papers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contents</h2><a id="user-content-contents" aria-label="Permalink: Contents" href="#contents"></a></p>
<ul dir="auto">
<li><a href="#install">Installation</a></li>
<li><a href="#documentation">Documentation</a></li>
<li><a href="#feature-video-overview">Video Demo</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#build-instructions">Build Instructions</a></li>
<li><a href="#donation">Buy Me a Coffee (or a Book!)</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Official packages</h3><a id="user-content-official-packages" aria-label="Permalink: Official packages" href="#official-packages"></a></p>
<p dir="auto">There are installers for Windows, macOS and Linux. See <a href="https://github.com/ahrm/sioyek/releases">Releases page</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Homebew Cask</h3><a id="user-content-homebew-cask" aria-label="Permalink: Homebew Cask" href="#homebew-cask"></a></p>
<p dir="auto">There is a homebrew cask available here: <a href="https://formulae.brew.sh/cask/sioyek" rel="nofollow">https://formulae.brew.sh/cask/sioyek</a>. Install by running:</p>
<div data-snippet-clipboard-copy-content="brew install --cask sioyek"><pre><code>brew install --cask sioyek
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Third-party packages for Linux</h3><a id="user-content-third-party-packages-for-linux" aria-label="Permalink: Third-party packages for Linux" href="#third-party-packages-for-linux"></a></p>
<p dir="auto">If you prefer to install sioyek with a package manager, you can look at this list. Please note that they are provided by third party packagers. USE AT YOUR OWN RISK! If you're reporting a bug for a third-party package, please mention which package you're using.</p>
<table>
<thead>
<tr>
<th>Distro</th>
<th>Link</th>
<th>Maintainer</th>
</tr>
</thead>
<tbody>
<tr>
<td>Flathub</td>
<td><a href="https://flathub.org/apps/details/com.github.ahrm.sioyek" rel="nofollow">sioyek</a></td>
<td><a href="https://flathub.org/apps/details/com.github.ahrm.sioyek" rel="nofollow">@nbenitez</a></td>
</tr>
<tr>
<td>Alpine</td>
<td><a href="https://pkgs.alpinelinux.org/packages?name=sioyek" rel="nofollow">sioyek</a></td>
<td><a href="https://github.com/jirutka">@jirutka</a></td>
</tr>
<tr>
<td>Arch</td>
<td><a href="https://aur.archlinux.org/packages/sioyek" rel="nofollow">AUR sioyek</a></td>
<td><a href="https://github.com/goggle">@goggle</a></td>
</tr>
<tr>
<td>Arch</td>
<td><a href="https://aur.archlinux.org/packages/sioyek-git/" rel="nofollow">AUR sioyek-git</a></td>
<td><a href="https://github.com/hrdl-github">@hrdl-github</a></td>
</tr>
<tr>
<td>Arch</td>
<td><a href="https://aur.archlinux.org/packages/sioyek-appimage/" rel="nofollow">AUR sioyek-appimage</a></td>
<td><a href="https://github.com/DhruvaSambrani">@DhruvaSambrani</a></td>
</tr>
<tr>
<td>Debian</td>
<td><a href="https://packages.debian.org/sioyek" rel="nofollow">sioyek</a></td>
<td><a href="https://github.com/viccie30">@viccie30</a></td>
</tr>
<tr>
<td>NixOS</td>
<td><a href="https://search.nixos.org/packages?channel=unstable&amp;show=sioyek&amp;from=0&amp;size=50&amp;sort=relevance&amp;type=packages&amp;query=sioyek" rel="nofollow">sioyek</a></td>
<td><a href="https://github.com/podocarp">@podocarp</a></td>
</tr>
<tr>
<td>openSUSE</td>
<td><a href="https://build.opensuse.org/package/show/Publishing/sioyek" rel="nofollow">Publishing</a></td>
<td><a href="https://github.com/uncomfyhalomacro">@uncomfyhalomacro</a></td>
</tr>
<tr>
<td>openSUSE</td>
<td><a href="https://build.opensuse.org/package/show/openSUSE:Factory/sioyek" rel="nofollow">Factory</a></td>
<td><a href="https://github.com/uncomfyhalomacro">@uncomfyhalomacro</a></td>
</tr>
<tr>
<td>Ubuntu</td>
<td><a href="https://packages.ubuntu.com/sioyek" rel="nofollow">sioyek</a></td>
<td><a href="https://github.com/viccie30">@viccie30</a></td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">You can view the official documentation <a href="https://sioyek-documentation.readthedocs.io/en/latest/" rel="nofollow">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Feature Video Overview</h2><a id="user-content-feature-video-overview" aria-label="Permalink: Feature Video Overview" href="#feature-video-overview"></a></p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=yTmCI0Xp5vI" rel="nofollow"><img src="https://camo.githubusercontent.com/3bbdb00658bc336a5da25c541e8141b7d38738232eba44310c960639633bb396/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f79546d43493058703576492f302e6a7067" alt="Sioyek feature overview" data-canonical-src="https://img.youtube.com/vi/yTmCI0Xp5vI/0.jpg"></a></p>
<p dir="auto">For a more in-depth tutorial, see this video:</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=RaHRvnb0dY8" rel="nofollow"><img src="https://camo.githubusercontent.com/b1817153034c5bf93e9b14a37292ae3bf2e949d900169928f9720ece7832d452/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f52614852766e62306459382f302e6a7067" alt="Sioyek Tutorial" data-canonical-src="https://img.youtube.com/vi/RaHRvnb0dY8/0.jpg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quick Open</h3><a id="user-content-quick-open" aria-label="Permalink: Quick Open" href="#quick-open"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description recent_docs.mp4">recent_docs.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125321111-9b29dc00-e351-11eb-873e-94ea30016a05.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTExMS05YjI5ZGMwMC1lMzUxLTExZWItODczZS05NGVhMzAwMTZhMDUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9M2I5NGE4NzVlMDQ4NjA4YjRmZjFkNzUzODVkZWVmMzMwNTYwZDU2Y2Y2ZmZjNWM3ZjUzNmM4MjExNzg0OWI0YiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.CgEbCL-dJokkAf-td825PN-E02l_-7J2M1w6T2cQgIo" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125321111-9b29dc00-e351-11eb-873e-94ea30016a05.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTExMS05YjI5ZGMwMC1lMzUxLTExZWItODczZS05NGVhMzAwMTZhMDUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9M2I5NGE4NzVlMDQ4NjA4YjRmZjFkNzUzODVkZWVmMzMwNTYwZDU2Y2Y2ZmZjNWM3ZjUzNmM4MjExNzg0OWI0YiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.CgEbCL-dJokkAf-td825PN-E02l_-7J2M1w6T2cQgIo" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">You can quickly search and open any file you have previously interacted with using sioyek.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Table of Contents</h3><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description toc.mp4">toc.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125321313-cf050180-e351-11eb-9275-c2759c684af5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTMxMy1jZjA1MDE4MC1lMzUxLTExZWItOTI3NS1jMjc1OWM2ODRhZjUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OTZhOGM4NmVhNGQyOWE4MTZjYTllMjhkZDBiN2FlOGUwOGU0MjlkYmVmODk1NGJhMmZiODA1NTM3MWIyMDBmYiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.e26EJM2s5-akKXn_7bEAC91YYYBtavP0lVelx94xGT8" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125321313-cf050180-e351-11eb-9275-c2759c684af5.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTMxMy1jZjA1MDE4MC1lMzUxLTExZWItOTI3NS1jMjc1OWM2ODRhZjUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OTZhOGM4NmVhNGQyOWE4MTZjYTllMjhkZDBiN2FlOGUwOGU0MjlkYmVmODk1NGJhMmZiODA1NTM3MWIyMDBmYiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.e26EJM2s5-akKXn_7bEAC91YYYBtavP0lVelx94xGT8" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">You can search and jump to table of contents entries.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Smart Jump</h3><a id="user-content-smart-jump" aria-label="Permalink: Smart Jump" href="#smart-jump"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description jump.mp4">jump.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125321419-e5ab5880-e351-11eb-9688-95374a22774f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTQxOS1lNWFiNTg4MC1lMzUxLTExZWItOTY4OC05NTM3NGEyMjc3NGYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YzhjYjZjZjQ1ZGU5OTlhNTFkZjU1NjJiNjk1ZjIyYWYzMTA2M2EyNDBmNzE3M2FlZjZjMDJiMzhlNDQxNDlmOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.mcFpCPbTqmL1-BVLl5ZJ4LzElUkQRX-sk34sPf8c3OY" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125321419-e5ab5880-e351-11eb-9688-95374a22774f.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTQxOS1lNWFiNTg4MC1lMzUxLTExZWItOTY4OC05NTM3NGEyMjc3NGYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YzhjYjZjZjQ1ZGU5OTlhNTFkZjU1NjJiNjk1ZjIyYWYzMTA2M2EyNDBmNzE3M2FlZjZjMDJiMzhlNDQxNDlmOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.mcFpCPbTqmL1-BVLl5ZJ4LzElUkQRX-sk34sPf8c3OY" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">You can jump to any referenced figure or bibliography item <em>even if the PDF file doesn't provide links</em>. You can also search the names of bibliography items in google scholar/libgen by middle clicking/shift+middle clicking on their name.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Overview</h3><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description overview.mp4">overview.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/154683015-0bae4f92-78e2-4141-8446-49dd7c2bd7c9.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzE1NDY4MzAxNS0wYmFlNGY5Mi03OGUyLTQxNDEtODQ0Ni00OWRkN2MyYmQ3YzkubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGJhYTQ1ZmI0M2JiMDU4MDZmMDY0ZGMxMjZiNjg4NWE0OGNlMzJmMGNjYzk0NzgxOTBhYzVhZTJhMzFjYTIxMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.73JERWG0v6CTuvhWn5BzMFMGTGWOqNXl7OgjBWog3Cg" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/154683015-0bae4f92-78e2-4141-8446-49dd7c2bd7c9.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzE1NDY4MzAxNS0wYmFlNGY5Mi03OGUyLTQxNDEtODQ0Ni00OWRkN2MyYmQ3YzkubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGJhYTQ1ZmI0M2JiMDU4MDZmMDY0ZGMxMjZiNjg4NWE0OGNlMzJmMGNjYzk0NzgxOTBhYzVhZTJhMzFjYTIxMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.73JERWG0v6CTuvhWn5BzMFMGTGWOqNXl7OgjBWog3Cg" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">You can open a quick overview of figures/references/tables/etc. by right clicking on them (Like Smart Jump, this feature works even if the document doesn't provide links).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mark</h3><a id="user-content-mark" aria-label="Permalink: Mark" href="#mark"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description mark.mp4">mark.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125321811-505c9400-e352-11eb-85e0-ffc3ae5f8cb8.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTgxMS01MDVjOTQwMC1lMzUyLTExZWItODVlMC1mZmMzYWU1ZjhjYjgubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MzlhZjA5YzFlOGYwNjIyMTUwMTY2YmU5ZTdmM2JhOWRiYWY5NjY0ZDljNWJjZTc0OGRhNTY5NjI0YmRjODEyNSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.AmelCLzLeOa-CMTR1mZdOh2LpzR787V2c46m0RFUDRI" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125321811-505c9400-e352-11eb-85e0-ffc3ae5f8cb8.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMTgxMS01MDVjOTQwMC1lMzUyLTExZWItODVlMC1mZmMzYWU1ZjhjYjgubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MzlhZjA5YzFlOGYwNjIyMTUwMTY2YmU5ZTdmM2JhOWRiYWY5NjY0ZDljNWJjZTc0OGRhNTY5NjI0YmRjODEyNSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.AmelCLzLeOa-CMTR1mZdOh2LpzR787V2c46m0RFUDRI" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Sometimes when reading a document you need to go back a few pages (perhaps to view a definition or something) and quickly jump back to where you were. You can achieve this by using marks. Marks are named locations within a PDF file (each mark has a single character name for example 'a' or 'm') which you can quickly jump to using their name. In the aforementioned example, before going back to the definition you mark your location and later jump back to the mark by invoking its name. Lower case marks are local to the document and upper case marks are global (this should be very familiar to you if you have used vim).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bookmarks</h3><a id="user-content-bookmarks" aria-label="Permalink: Bookmarks" href="#bookmarks"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description bookmarks.mp4">bookmarks.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125322503-1a6bdf80-e353-11eb-8018-5e8fc43b8d05.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMjUwMy0xYTZiZGY4MC1lMzUzLTExZWItODAxOC01ZThmYzQzYjhkMDUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjcyZGI1Y2QzMzdlZTEwMTJkMzAyZjE5ZmVhZTEwZjkyNjY4N2YwNzBiMjdhMGFjMTA4MzA3ODhjNjM5YTM4ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.eXA1H8apV3pXKK-90BZi2Y5BkjkJw0VeEWVkzYQ6tQ4" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125322503-1a6bdf80-e353-11eb-8018-5e8fc43b8d05.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMjUwMy0xYTZiZGY4MC1lMzUzLTExZWItODAxOC01ZThmYzQzYjhkMDUubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjcyZGI1Y2QzMzdlZTEwMTJkMzAyZjE5ZmVhZTEwZjkyNjY4N2YwNzBiMjdhMGFjMTA4MzA3ODhjNjM5YTM4ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.eXA1H8apV3pXKK-90BZi2Y5BkjkJw0VeEWVkzYQ6tQ4" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Bookmarks are similar to marks except they are named by a text string and they are all global.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Highlights</h3><a id="user-content-highlights" aria-label="Permalink: Highlights" href="#highlights"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description highlights.mp4">highlights.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/130956728-7e0a87fa-4ada-4108-a8fc-9d9d04180f56.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEzMDk1NjcyOC03ZTBhODdmYS00YWRhLTQxMDgtYThmYy05ZDlkMDQxODBmNTYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTk0ZThiZTZiZmU0ZjhhMzhiOTcwOGFlY2FmMzEwZTU0YmQ4MGFmYTgxYjBlY2E0MTlkYjNkNDViZThhZDI2MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.HbkEAIDP6hnEgafwmccUAEOeyipy5fPzhHHkKfiAaGo" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/130956728-7e0a87fa-4ada-4108-a8fc-9d9d04180f56.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEzMDk1NjcyOC03ZTBhODdmYS00YWRhLTQxMDgtYThmYy05ZDlkMDQxODBmNTYubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTk0ZThiZTZiZmU0ZjhhMzhiOTcwOGFlY2FmMzEwZTU0YmQ4MGFmYTgxYjBlY2E0MTlkYjNkNDViZThhZDI2MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.HbkEAIDP6hnEgafwmccUAEOeyipy5fPzhHHkKfiAaGo" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Highlight text using different kinds of highlights. You can search among all the highlights.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Portals (this feature is most useful for users with multiple monitors)</h3><a id="user-content-portals-this-feature-is-most-useful-for-users-with-multiple-monitors" aria-label="Permalink: Portals (this feature is most useful for users with multiple monitors)" href="#portals-this-feature-is-most-useful-for-users-with-multiple-monitors"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description portal.mp4">portal.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125322657-41c2ac80-e353-11eb-985e-8f3ce9808f67.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMjY1Ny00MWMyYWM4MC1lMzUzLTExZWItOTg1ZS04ZjNjZTk4MDhmNjcubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGMxZWU2NTNmYzdiN2NmODI1ZmQzNDcwN2E4OTUwNmY2ZmEyN2NmNDFiM2IwOTQyMjQ2NTEwZjhiZmFhMGMwYyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.tnn-Y723I7iEw-gZjhO75Rwk5mFab8XLd-BKNDCrQEE" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125322657-41c2ac80-e353-11eb-985e-8f3ce9808f67.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMyMjY1Ny00MWMyYWM4MC1lMzUzLTExZWItOTg1ZS04ZjNjZTk4MDhmNjcubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OGMxZWU2NTNmYzdiN2NmODI1ZmQzNDcwN2E4OTUwNmY2ZmEyN2NmNDFiM2IwOTQyMjQ2NTEwZjhiZmFhMGMwYyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.tnn-Y723I7iEw-gZjhO75Rwk5mFab8XLd-BKNDCrQEE" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">Suppose you are reading a paragraph which references a figure which is not very close to the current location. Jumping back and forth between the current paragraph and the figure can be very annoying. Using portals, you can link the paragraph's location to the figure's location. Sioyek shows the closest portal destination in a separate window (which is usually placed on a second monitor). This window is automatically updated to show the closest portal destination as the user navigates the document.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description config.mp4">config.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/6392321/125337160-e4832700-e363-11eb-8801-0bee58121c2d.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMzNzE2MC1lNDgzMjcwMC1lMzYzLTExZWItODgwMS0wYmVlNTgxMjFjMmQubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NGE3NmViNjU3YjQxZTQ3ZTYzODEzNmY3NDIxMTE4ZmE3YTlmOWZkYjY4MThiZTc2YzJhYjNlZDFmYzZjY2I5NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.ruxUPlLfOjuotsHBJRfFgE5VcBmMNTjluhNgVTLDRDU" data-canonical-src="https://private-user-images.githubusercontent.com/6392321/125337160-e4832700-e363-11eb-8801-0bee58121c2d.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUzMDY3MDQsIm5iZiI6MTcxNTMwNjQwNCwicGF0aCI6Ii82MzkyMzIxLzEyNTMzNzE2MC1lNDgzMjcwMC1lMzYzLTExZWItODgwMS0wYmVlNTgxMjFjMmQubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDUxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA1MTBUMDIwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NGE3NmViNjU3YjQxZTQ3ZTYzODEzNmY3NDIxMTE4ZmE3YTlmOWZkYjY4MThiZTc2YzJhYjNlZDFmYzZjY2I5NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.ruxUPlLfOjuotsHBJRfFgE5VcBmMNTjluhNgVTLDRDU" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto">You can customize all key bindings and some UI elements by editing <code>keys_user.config</code> and <code>prefs_user.config</code>. The default configurations are in <code>keys.config</code> and <code>prefs.config</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build Instructions</h2><a id="user-content-build-instructions" aria-label="Permalink: Build Instructions" href="#build-instructions"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Linux</h3><a id="user-content-linux" aria-label="Permalink: Linux" href="#linux"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Fedora</h4><a id="user-content-fedora" aria-label="Permalink: Fedora" href="#fedora"></a></p>
<p dir="auto">Run the following commands to install dependencies, clone the repository and compile sioyek on Fedora (tested on Fedora Workstation 36).</p>
<div data-snippet-clipboard-copy-content="sudo dnf install qt5-qtbase-devel qt5-qtbase-static qt5-qt3d-devel harfbuzz-devel
git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
./build_linux.sh"><pre><code>sudo dnf install qt5-qtbase-devel qt5-qtbase-static qt5-qt3d-devel harfbuzz-devel
git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
./build_linux.sh
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Generic distribution</h4><a id="user-content-generic-distribution" aria-label="Permalink: Generic distribution" href="#generic-distribution"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Install Qt 5 and make sure <code>qmake</code> is in <code>PATH</code>.</p>
<p dir="auto">Run <code>qmake --version</code> to make sure the <code>qmake</code> in path is using Qt 5.x.</p>
</li>
<li>
<p dir="auto">Install <code>libharfbuzz</code>:</p>
</li>
</ol>
<div data-snippet-clipboard-copy-content="sudo apt install libharfbuzz-dev"><pre><code>sudo apt install libharfbuzz-dev
</code></pre></div>
<ol start="3" dir="auto">
<li>Clone the repository and build:</li>
</ol>
<div data-snippet-clipboard-copy-content="git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
./build_linux.sh"><pre><code>git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
./build_linux.sh
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows</h3><a id="user-content-windows" aria-label="Permalink: Windows" href="#windows"></a></p>
<ol dir="auto">
<li>Install Visual Studio (tested on 2019, other relatively recent versions should work too)</li>
<li>Install Qt 5 and make sure qmake is in <code>PATH</code>.</li>
<li>Clone the repository and build using 64 bit Visual Studio Developer Command Prompt:</li>
</ol>
<div data-snippet-clipboard-copy-content="git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
build_windows.bat"><pre><code>git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
build_windows.bat
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mac</h3><a id="user-content-mac" aria-label="Permalink: Mac" href="#mac"></a></p>
<ol dir="auto">
<li>Install Xcode.</li>
<li>Clone the repository and build: (The code below is in Zsh, which is the default shell on macOS.)</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="(
setopt PIPE_FAIL PRINT_EXIT_VALUE ERR_RETURN SOURCE_TRACE XTRACE

git clone --recursive https://github.com/ahrm/sioyek
cd sioyek
chmod +x build_mac.sh

brew install 'qt@5' freeglut mesa harfbuzz

export PATH=&quot;/opt/homebrew/opt/qt@5/bin:$PATH&quot;
#: The above is needed to make =qmake= from =qt= be found.
#: Find the path using =brew info 'qt@5'=.

MAKE_PARALLEL=8 ./build_mac.sh

mv build/sioyek.app /Applications/
sudo codesign --force --sign - --deep /Applications/sioyek.app
)"><pre>(
setopt PIPE_FAIL PRINT_EXIT_VALUE ERR_RETURN SOURCE_TRACE XTRACE

git clone --recursive https://github.com/ahrm/sioyek
<span>cd</span> sioyek
chmod +x build_mac.sh

brew install <span><span>'</span>qt@5<span>'</span></span> freeglut mesa harfbuzz

<span>export</span> PATH=<span><span>"</span>/opt/homebrew/opt/qt@5/bin:<span>$PATH</span><span>"</span></span>
<span><span>#</span>: The above is needed to make =qmake= from =qt= be found.</span>
<span><span>#</span>: Find the path using =brew info 'qt@5'=.</span>

MAKE_PARALLEL=8 ./build_mac.sh

mv build/sioyek.app /Applications/
sudo codesign --force --sign - --deep /Applications/sioyek.app
)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Donation</h2><a id="user-content-donation" aria-label="Permalink: Donation" href="#donation"></a></p>
<p dir="auto">If you enjoy sioyek, please consider donating to support its development.</p>
<p dir="auto"><a href="https://www.buymeacoffee.com/ahrm" rel="nofollow"><img src="https://camo.githubusercontent.com/4412aa44a78a18c03862fd7da2de5bd81e3817a3adec90fdd41671170a206abd/68747470733a2f2f63646e2e6275796d6561636f666665652e636f6d2f627574746f6e732f64656661756c742d6f72616e67652e706e67" alt="Buy Me A Coffee" height="41" width="174" data-canonical-src="https://cdn.buymeacoffee.com/buttons/default-orange.png"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The history of 'OK' (2023) (101 pts)]]></title>
            <link>https://people.howstuffworks.com/history-ok.htm</link>
            <guid>40312434</guid>
            <pubDate>Thu, 09 May 2024 20:07:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://people.howstuffworks.com/history-ok.htm">https://people.howstuffworks.com/history-ok.htm</a>, See on <a href="https://news.ycombinator.com/item?id=40312434">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="editorial-body">

					
	
				
	
																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																									
				


	


	<div id="page0" data-slide="0" data-track-gtm="Content">	
					<div>
<figure>
			
		
	
								
		
		
																																																								
			<picture>
				<source media="(max-width: 320px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjMyMH0sInRvRm9ybWF0IjoiYXZpZiJ9fQ==" type="image/avif"><source media="(min-width: 321px) and (max-width: 599px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjQyMH0sInRvRm9ybWF0IjoiYXZpZiJ9fQ==" type="image/avif"><source media="(min-width: 600px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjgyOH0sInRvRm9ybWF0IjoiYXZpZiJ9fQ==" type="image/avif">
				<source media="(max-width: 320px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjMyMH19fQ=="><source media="(min-width: 321px) and (max-width: 599px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjQyMH19fQ=="><source media="(min-width: 600px)" srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjgyOH19fQ==">
				<img src="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMS5qcGciLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjgyOH19fQ==" alt="OK word" width="828" height="465">
			</picture>
			

		
	

					
	
	<figcaption>
											The spread of "OK" shows how important an all-purpose word can be. <span>Foxys Graphic/Shutterstock</span>
								</figcaption>

	</figure>
	</div>
	





	
		<p>"OK" is probably the <a href="https://www.npr.org/2010/11/20/131390650/ok-how-two-letters-made-america-s-greatest-word">most spoken word</a> in the world — besides English, people say "OK" in a dozen languages, including Spanish, Italian and Russian — and yet almost nobody can tell you what those two letters stand for or where the word came from.</p>

		
	
					
				
				


	
		<p>Was it borrowed from the <a href="https://www.google.com/books/edition/OK/mPhj9DIXCWAC?hl=en&amp;gbpv=1&amp;bsq=okeh">indigenous Choctaw word</a> "okeh," meaning, roughly, "OK"? Did it originate with a Boston baker named Otto Kimmel who liked to <a href="https://www.google.com/books/edition/OK/mPhj9DIXCWAC?hl=en&amp;gbpv=1">frost his initials</a> into his cookies? Does it have anything to do with the state of Oklahoma (OK) or the musical "<a href="https://youtu.be/kGXu0j6QDJ8">Oklahoma!</a>"?</p>

		
	
								

		
	
											

						
				
				


	
		<p>Nope, nope and nope. In fact, there's <a href="https://www.google.com/books/edition/OK/mPhj9DIXCWAC?hl=en&amp;gbpv=1&amp;bsq=okeh">no evidence</a> that "okeh" was part of the Chocktaw language.</p>

		
	
													
				
				


	
		<p>"OK is the greatest American word," says <a href="https://cla.umn.edu/about/directory/profile/aliber">Anatoly Liberman</a>, a linguist, translator and language professor at the University of Minnesota. "The history of OK is a history of incredible success, but nobody could have predicted that success."</p>

		
	
					
				
				


	
		<p>As you'll see, OK began as a piece of insider slang from the late 1830s and rode a (losing) presidential campaign to nationwide fame and eventually worldwide ubiquity.</p>

		
	
					
				
						
						

				
				

		
	




</div>		<div data-track-gtm="TOC">
			<p><strong>Contents</strong></p><ol>
						
					
																
										
					<li>
						<a data-target="pt1" href="#pt1">The Acronym Craze of the 1830s</a>
					</li>
							
					
																
										
					<li>
						<a data-target="pt2" href="#pt2">Misspelling Words Was Also a Thing</a>
					</li>
							
					
																
										
					<li>
						<a data-target="pt3" href="#pt3">The Very First Use of OK</a>
					</li>
							
					
																
										
					<li>
						<a data-target="pt4" href="#pt4">"Old Kinderhook" Takes "OK" National</a>
					</li>
										</ol>
		</div>



		
	<div id="page-wrap1" x-data="{ pageVisible : true }"><h2 data-page-nbr="1" data-logged="false" data-page-url="history-ok1.htm">
			


<span @click="pageVisible = !pageVisible" aria-expanded="true" aria-controls="page1" role="button">

			<svg :class="{ 'rotate-180' : !pageVisible, '' : pageVisible }" xmlns="http://www.w3.org/2000/svg" width="22" height="10" viewBox="0 0 28.396 13.211"><path d="M4398.8,5158.252l13.224,10.507,12.357-10.507" transform="translate(-4397.399 -5156.842)" fill="none" stroke-linecap="round" stroke-width="2"></path></svg>
	
		<span>The Acronym Craze of the 1830s</span>

	
</span>		</h2>

				

<div id="page1" data-slide="1" data-track-gtm="Content" x-show.transition="pageVisible === true">	
						
	





	
		<p>In the early 19th century, new printing technologies dramatically reduced the cost of publishing a daily newspaper, and there was a resulting explosion of inexpensive new dailies known collectively as the <a href="https://blogs.ubc.ca/etec540sept09/2009/10/19/the-rise-of-penny-newspapers-and-their-influence-on-mass-media/">penny press</a>. Competing for readers, penny papers in cities like New York, Philadelphia and Boston published not only straight news stories, but also witty takes on the latest political scandals, social scenes and popular trends.</p>

		
	
					
				
				


	
		<p>Think of it as the internet of the 1830s. And much like the internet, the lively back-and-forth chatter between penny paper editors gave birth to a new way of writing and eventually a new way of speaking.</p>

		
	
								

		
	
										
				
				


	
		<p>"Beginning in the summer of 1838, there developed in Boston a remarkable vogue of using abbreviations. It might well be called a craze," <a href="https://www.jstor.org/stable/453580">wrote</a> the famed etymologist <a href="https://www.nytimes.com/2002/10/18/nyregion/allen-read-96-the-ok-expert-is-dead.html">Allen Walker Read</a>, who was the first person to trace the full history of OK.</p>

		
	
					
				
				


	
		<p>Take these examples from Boston's Morning Post, whose editor, Charles Gordon Greene, sprinkled his columns with winking acronyms for everything and anything:</p>

		
	
					
				
				


	
		<div>
	<ul><li><span>O.F.M. ("our first men")</span></li><li><span>W.O.O.O.F.C. ("with one of our first citizens")</span></li><li><span>R.T.B.S. ("remains to be seen")</span></li><li><span>D.L.E.C. ("do let 'em come")</span></li><li><span>G.T.D.H.D. ("give the devil his due")</span></li><li><span>W.Y.G. ("will you go?")</span></li></ul>
</div>


		
	
		
				
				


	
		<p>By 1939, the "initial language," as it was sometimes called, had arrived in New York City and had already leapt from print to fashionable slang. "This is a species of spoken shorthand, which is getting into very general use among loafers and gentlemen of the fancy," <a href="https://www.jstor.org/stable/453580">wrote the editors</a> of New York's Evening Tattler.</p>

		
	
					
				
				


	
		<p>The editors even claimed to have overheard a conversation between two young sweethearts, where the girl turned to her beau and said, "O.K.K.B.W.P." "What could she have meant," wrote the Evening Tattler, "but 'One Kind Kiss Before We Part'?"</p>

		
	
					
				
				
	
									
				

		
	




</div>

			</div>


		
	<div id="page-wrap2" x-data="{ pageVisible : true }"><h2 data-page-nbr="2" data-logged="false" data-page-url="history-ok2.htm">
			


<span @click="pageVisible = !pageVisible" aria-expanded="true" aria-controls="page2" role="button">

			<svg :class="{ 'rotate-180' : !pageVisible, '' : pageVisible }" xmlns="http://www.w3.org/2000/svg" width="22" height="10" viewBox="0 0 28.396 13.211"><path d="M4398.8,5158.252l13.224,10.507,12.357-10.507" transform="translate(-4397.399 -5156.842)" fill="none" stroke-linecap="round" stroke-width="2"></path></svg>
	
		<span>Misspelling Words Was Also a Thing</span>

	
</span>		</h2>

				

<div id="page2" data-slide="2" data-track-gtm="Content" x-show.transition="pageVisible === true">	





	
		<p>In addition to the abbreviation craze, 19th-century Americans thought it was really funny to purposely misspell stuff. Read, the etymologist, cited the example of the comic writer George W. Arnold, who used the pen name "Joe Strickland" to write mangled letters to his fictional family, like this one from a trip abroad: "when I got here tha axt me if I was evver in Turky before. no ses I. but i've had a darn menny turkeys in me."</p>

		
	
					
				
				


	
		<p>By the late 1830s, the (hilarious) misspelling trend had combined with the acronym craze to produce punchy abbreviations like:</p>

		
	
								

		
	
										
				
				


	
		<div>
	<ul><li><span>K.G. for "no go" (as if spelled "know go")</span></li><li><span>K.Y. for "no use" (as if spelled "know yuse")</span></li><li><span>O.W. for "all right" (as if spelled "oll wright")</span></li></ul>
</div>


		
	
		
				
				


	
		<p>Absolutely no one says K.G. or O.W. anymore, but believe it or not, that witty wordplay laid the groundwork for the arrival of a two-letter abbreviation that would conquer the world.</p>

		
	
					
				
				
	
				
				

		
	




</div>

			</div>


		
	<div id="page-wrap3" x-data="{ pageVisible : true }"><h2 data-page-nbr="3" data-logged="false" data-page-url="history-ok3.htm">
			


<span @click="pageVisible = !pageVisible" aria-expanded="true" aria-controls="page3" role="button">

			<svg :class="{ 'rotate-180' : !pageVisible, '' : pageVisible }" xmlns="http://www.w3.org/2000/svg" width="22" height="10" viewBox="0 0 28.396 13.211"><path d="M4398.8,5158.252l13.224,10.507,12.357-10.507" transform="translate(-4397.399 -5156.842)" fill="none" stroke-linecap="round" stroke-width="2"></path></svg>
	
		<span>The Very First Use of OK</span>

	
</span>		</h2>

				

<div id="page3" data-slide="3" data-track-gtm="Content" x-show.transition="pageVisible === true">	





	
		<p>Before we get to the fateful date of March 21, 1839, let's tip our hats one more time to Allen Walker Read, the man who solved the mystery of OK's origins. Keep in mind that Read was working in the 1960s, decades before searchable digital newspaper archives.</p>

		
	
					
				
				


	
		<p>"Read must have spent hundreds of hours digging through tons and tons of physical newspapers, journals, private letters and other documents," says Liberman, who writes the weekly <a href="https://blog.oup.com/category/series-columns/oxford_etymologist/">Oxford Etymologist</a> blog and knows firsthand how hard it is to track down the history of words. "What that man did was absolutely astounding."</p>

		
	
								

		
	
										
				
				


	
		<p>OK, back to our story.</p>

		
	
					
				
				


	
		<p>In the spring of 1839, the editor of Boston's Morning Post, Charles Gordon Greene, was engaged in some good-natured trash talk with the editors of the Providence Journal in Rhode Island<i>.</i> It had to do with a semi-satirical citizens group in Boston called the Anti-Bell-Ringing Society (or A.B.R.S.), of which Greene was a member.</p>

		
	
					
				
				


	
		<p>The Providence paper poked fun at Greene and the A.B.R.S. and Greene had to set the record straight. So it was that on March 21, 1839, at the end of a short paragraph defending the A.B.R.S., Greene printed the following words: "<i>o.k.</i> — all correct."</p>

		
	
					
				
				


	
		<p>See what he did there? Similar to using O.W. for "oll wright," Greene had coined a new misspelled acronym: O.K. for "oll korrect." Three days after Greene introduced OK to the world, the Providence Journal editors responded with an "O.K." of their own.</p>

		
	
					
				
				


	
		<p>Like other offbeat acronyms of the day, O.K. was an inside joke randomly thrust into general circulation. But unlike O.W. or K.G., which enjoyed brief popularity in the 1830s, O.K. didn't die out.</p>

		
	
					
				
				


	
		<p>"Nobody knew that this facetious abbreviation would have such a long and happy life," says Liberman.</p>

		
	
					
				
				
	
				
				

		
	




</div>

			</div>


		
	<div id="page-wrap4" x-data="{ pageVisible : true }"><h2 data-page-nbr="4" data-logged="false" data-page-url="history-ok4.htm">
			


<span @click="pageVisible = !pageVisible" aria-expanded="true" aria-controls="page4" role="button">

			<svg :class="{ 'rotate-180' : !pageVisible, '' : pageVisible }" xmlns="http://www.w3.org/2000/svg" width="22" height="10" viewBox="0 0 28.396 13.211"><path d="M4398.8,5158.252l13.224,10.507,12.357-10.507" transform="translate(-4397.399 -5156.842)" fill="none" stroke-linecap="round" stroke-width="2"></path></svg>
	
		<span>"Old Kinderhook" Takes "OK" National</span>

	
</span>		</h2>

				

<div id="page4" data-slide="4" data-track-gtm="Content" x-show.transition="pageVisible === true">	
					<div>
<figure>
			
		
	
								
		
		
																																																								
			<picture>
				<source media="(max-width: 320px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjozMjB9LCJ0b0Zvcm1hdCI6ImF2aWYifX0=" type="image/avif"><source media="(min-width: 321px) and (max-width: 599px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjo0MjB9LCJ0b0Zvcm1hdCI6ImF2aWYifX0=" type="image/avif"><source media="(min-width: 600px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjo4Mjh9LCJ0b0Zvcm1hdCI6ImF2aWYifX0=" type="image/avif">
				<source media="(max-width: 320px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjozMjB9fX0="><source media="(min-width: 321px) and (max-width: 599px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjo0MjB9fX0="><source media="(min-width: 600px)" data-srcset="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjo4Mjh9fX0=">
				<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAJCAQAAACRI2S5AAAAEElEQVR42mNkIAAYRxWAAQAG9gAKqv6+AwAAAABJRU5ErkJggg==" data-src="https://media.hswstatic.com/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL09LLWhpc3RvcnktMy0uanBnIiwiZWRpdHMiOnsicmVzaXplIjp7IndpZHRoIjo4Mjh9fX0=" alt="Martin Van Buren 1840 campaign" width="828" height="648">
			</picture>
			

		
	

					
	
	<figcaption>
											This political cartoon shows Martin Van Buren, who ran unsuccessfully against William Henry Harrison, known as the "log cabin and hard cider" candidate, during the 1840 presidential campaign. Van Buren's "OK" nickname is prominent. <span>Bettman/Getty Images</span>
								</figcaption>

	</figure>
	</div>
	





	
		<p>If you thought that the word OK <a href="https://www.npr.org/templates/story/story.php?storyId=5170008">originated</a> with Martin Van Buren, you'd be half right. The eighth president of the United States <a href="https://www.nps.gov/mava/index.htm">hailed from the small town</a> of Kinderhook, New York. Like his mentor and fellow Democrat Andrew Jackson, who was known as "Old Hickory," Van Buren's nickname was "Old Kinderhook."</p>

		
	
					
				
				


	
		<p>In the 1840 presidential election, William Henry Harrison and the Whig party challenged the incumbent Van Buren. Harrison's supporters came up with the catchy (for its time) campaign slogan (<a href="https://potus-geeks.livejournal.com/425230.html">and song</a>), "Tippecanoe and Tyler Too." The Democrats swung back with a slogan of their own: "O.K." <a href="https://www.history.com/news/the-birth-of-ok-175-years-ago">As in</a>, "Old Kinderhook is OK!"</p>

		
	
								

		
	
										
				
				


	
		<p>"[Van Buren] got the nickname Old Kinderhook, and early in 1840, OK clubs sprung up with the slogan, 'OK is OK.' So taking that funny little word and making it a mainstay of the political conversation in 1840, suddenly OK was <i>way</i> OK," said the late linguist Allan Metcalf <a href="https://www.npr.org/2010/11/20/131390650/ok-how-two-letters-made-america-s-greatest-word">in a 2010 NPR interview</a>. Metcalf was the author of "<a href="https://www.amazon.com/OK-Improbable-Story-Americas-Greatest/dp/0199892539">OK: The Improbable Story of America's Greatest Word</a>." Van Buren lost badly, but OK definitely won.</p>

		
	
					
				
				


	
		<p>After 1840, the word spread like wildfire and never looked back. Originally, OK appeared in telegraph messages (which may account for its international spread) and documents but not in everyday speech as it was "slangy." But that changed over time. </p>

		
	
					
				
				


	
		<p><a href="https://www.bbc.com/news/magazine-12503686">In an article for BBC Magazine</a>, Metcalf speculated as to why OK was popular all over the world: "It's not that it was needed to 'fill a gap' in any language. Before 1839, English speakers had 'yes,' 'good,' 'fine,' 'excellent,' 'satisfactory' and 'all right.' What OK provided that the others did not was neutrality, a way to affirm or to express agreement without having to offer an opinion. ... OK allows us to view a situation in simplest terms, just OK or not."</p>

		
	
					
				
				
	
					

				

		
	




</div>

			</div>





				</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It's always TCP_NODELAY (703 pts)]]></title>
            <link>https://brooker.co.za/blog/2024/05/09/nagle.html</link>
            <guid>40310896</guid>
            <pubDate>Thu, 09 May 2024 17:54:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brooker.co.za/blog/2024/05/09/nagle.html">https://brooker.co.za/blog/2024/05/09/nagle.html</a>, See on <a href="https://news.ycombinator.com/item?id=40310896">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post">


<p>It's not the 1980s anymore, thankfully.</p>

<p>The first thing I check when debugging latency issues in distributed systems is whether <a href="https://linux.die.net/man/7/tcp">TCP_NODELAY</a> is enabled. And it’s not just me. Every distributed system builder I know has lost hours to latency issues quickly fixed by enabling this simple socket option, suggesting that the default behavior is wrong, and perhaps that the whole concept is outmoded.</p>

<p>First, let’s be clear about what we’re talking about. There’s no better source than John Nagle’s <a href="https://datatracker.ietf.org/doc/html/rfc896">RFC896</a> from 1984<sup><a href="#foot1">1</a></sup>. First, the problem statement:</p>

<blockquote>
  <p>There is a special problem associated with small  packets.   When TCP  is  used  for  the transmission of single-character messages originating at a keyboard, the typical result  is  that  41  byte packets (one  byte  of data, 40 bytes of header) are transmitted for each byte of useful data.  This 4000%  overhead  is  annoying but tolerable on lightly loaded networks.</p>
</blockquote>

<p>In short, Nagle was interested in better amortizing the cost of TCP headers, to get better throughput out of the network. Up to 40x better throughput! These tiny packets had two main causes: human-interactive applications like shells, where folks were typing a byte at a time, and poorly implemented programs that dribbled messages out to the kernel through many <code>write</code> calls. Nagle’s proposal for fixing this was simple and smart:</p>

<blockquote>
  <p>A  simple and elegant solution has been discovered.</p>
</blockquote>

<blockquote>
  <p>The solution is to inhibit the sending of new TCP  segments  when new  outgoing  data  arrives  from  the  user  if  any previously transmitted data on the connection remains unacknowledged.</p>
</blockquote>

<p>When many people talk about Nagle’s algorithm, they talk about timers, but RFC896 doesn’t use any kind of timer other than the round-trip time on the network.</p>

<p><em>Nagle’s Algorithm and Delayed Acks</em></p>

<p>Nagle’s nice, clean, proposal interacted poorly with another TCP feature: delayed <code>ACK</code>. The idea behind delayed <code>ACK</code> is to delay sending the acknowledgement of a packet at least until there’s some data to send back (e.g. a <code>telnet</code> session echoing back the user’s typing), or until a timer expires. <a href="https://datatracker.ietf.org/doc/html/rfc813">RFC813</a> from 1982 is that first that seems to propose delaying <code>ACKs</code>:</p>

<blockquote>
  <p>The receiver of data will   refrain   from   sending   an   acknowledgement   under   certain circumstances, in which case it must set a timer which  will  cause  the acknowledgement  to be sent later.  However, the receiver should do this only where it is a reasonable guess that some other event will intervene and prevent the necessity of the timer  interrupt.</p>
</blockquote>

<p>which is then formalized further in <a href="https://datatracker.ietf.org/doc/html/rfc1122">RFC1122</a> from 1989. The interaction between these two features causes a problem: Nagle’s algorithm is blocking sending more data until an <code>ACK</code> is received, but delayed ack is delaying that <code>ack</code> until a response is ready. Great for keeping packets full, not so great for latency-sensitive pipelined applications.</p>

<p>This is a point Nagle has made himself several times. For example in this <a href="https://news.ycombinator.com/item?id=10608356">Hacker News comment</a>:</p>

<blockquote>
  <p>That still irks me. The real problem is not tinygram prevention. It’s ACK delays, and that stupid fixed timer. They both went into TCP around the same time, but independently. I did tinygram prevention (the Nagle algorithm) and Berkeley did delayed ACKs, both in the early 1980s. The combination of the two is awful.</p>
</blockquote>

<p>As systems builders this is should be a familiar situation: two reasonable features of the system that interact to create an undesirable behavior. This kind of interaction is one of the things that makes protocol design so hard.</p>

<p><em>Is Nagle blameless?</em></p>

<p>Unfortunately, it’s not just delayed ACK. Even without delayed ack and that <em>stupid fixed timer</em>, the behavior of Nagle’s algorithm probably isn’t what we want in distributed systems. A single in-datacenter RTT is typically around 500μs, then a couple of milliseconds between datacenters in the same region, and up to hundreds of milliseconds going around the globe. Given the vast amount of work a modern server can do in even a few hundred microseconds, delaying sending data for even one RTT isn’t clearly a win.</p>

<p>To make a clearer case, let’s turn back to the justification behind Nagle’s algorithm: amortizing the cost of headers and avoiding that 40x overhead on single-byte packets. But does anybody send single byte packets anymore? Most distributed databases and systems don’t. Partially that’s because they simply have more to say, partially its because of additional overhead of protocols like TLS, and partially its because of encoding and serialization overhead. But mostly, they have more to say.</p>

<p>The core concern of not sending tiny messages is still a very real one, but we’ve very effectively pushed that into the application layer. Sending a byte at a time wrapped in JSON isn’t going to be very efficient, no matter what Nagle’s algorithm does.</p>

<p><em>Is Nagle needed?</em></p>

<p>First, the uncontroversial take: if you’re building a latency-sensitive distributed system running on modern datacenter-class hardware, enable <code>TCP_NODELAY</code> (disable Nagle’s algorithm) without worries. You don’t need to feel bad. It’s not a sin. It’s OK. Just go ahead.</p>

<p>More controversially, I suspect that Nagle’s algorithm just isn’t needed on modern systems, given the traffic and application mix, and the capabilities of the hardware we have today. In other words, <code>TCP_NODELAY</code> should be the default. That’s going to make some “<code>write</code> every byte” code slower than it would otherwise be, but those applications should be fixed anyway if we care about efficiency.</p>

<p><em>Footnotes</em></p>

<ol>
  <li><a name="foot1"></a> I won’t got into it here, but RFC896 is also one of the earliest statements I can find of metastable behavior in computer networks. In it, Nagle says: “This condition is stable. Once the  saturation point has been reached, if the algorithm for selecting packets to be dropped is fair, the network will continue to operate in a degraded condition.”</li>
</ol>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leaked deck reveals how OpenAI is pitching publisher partnerships (292 pts)]]></title>
            <link>https://www.adweek.com/media/openai-preferred-publisher-program-deck/</link>
            <guid>40310228</guid>
            <pubDate>Thu, 09 May 2024 16:56:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.adweek.com/media/openai-preferred-publisher-program-deck/">https://www.adweek.com/media/openai-preferred-publisher-program-deck/</a>, See on <a href="https://news.ycombinator.com/item?id=40310228">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-continue-reading-wrapper="">
                                    



<div>
		
		<p>Mark your calendar for Mediaweek, October 29-30 in New York City. We’ll unpack the biggest shifts shaping the future of media—from tv to retail media to tech—and how marketers can prep to stay ahead. <a href="https://event.adweek.com/mediaweek-2024/4442894?ref=nativep1&amp;itm_source=ROS&amp;itm_medium=display&amp;itm_campaign=nativeMediaweek24&amp;itm_content=p1"><strong>Register</strong></a> with early-bird rates before sale ends!</p>
	</div>


<div><p>The generative artificial intelligence firm OpenAI has been pitching partnership opportunities to news publishers through an initiative called the Preferred Publishers Program, according to a deck obtained by ADWEEK and interviews with four industry executives.</p><p>OpenAI has been courting premium publishers dating back to July 2023, when it struck a licensing agreement with the Associated Press. It has since inked public partnerships with Axel Springer, The Financial Times, <a href="https://www.adweek.com/media/le-monde-english-subscribers-olympics/" target="_blank" rel="noreferrer noopener">Le Monde</a>, Prisa and Dotdash Meredith, although it has declined to share the specifics of any of its deals.</p><p>A representative for OpenAI disputed the accuracy of the information in the deck, which is more than three months old. The <a href="https://www.adweek.com/category/artificial-intelligence/" target="_blank">gen AI</a> firm also negotiates deals on a per-publisher basis, rather than structuring all of its deals uniformly, the representative said.</p><p>“We are engaging in productive conversations and partnerships with many news publishers around the world,” said a representative for OpenAI. “Our confidential documents are for discussion purposes only and ADWEEK’s reporting contains a number of mischaracterizations and outdated information.”</p><p>Nonetheless, the leaked deck reveals the basic structure of the partnerships OpenAI is proposing to media companies, as well as the incentives it is offering for their collaboration.</p><section> <p><a href="https://www.adweek.com/media/publishers-ai-licensing-negotiations-mark-an-inflection-point/" target="_blank"><img decoding="async" src="https://static-prod.adweek.com/wp-content/uploads/2023/11/publisher-ai-licensing-2023-640x360.jpg" alt="Copyright law favors creators, but commercial compromise offers a hedge against uncertainty."></a></p>  </section><h4><strong>Details from the pitch deck</strong></h4><p>The Preferred Publisher Program has five primary components, according to the deck.</p><p>First, it is available only to “select, high-quality editorial partners,” and its purpose is to help ChatGPT users more easily discover and engage with publishers’ brands and content.</p><p>Additionally, members of the program receive priority placement and “richer brand expression” in chat conversations, and their content benefits from more prominent link treatments. Finally, through PPP, OpenAI also offers licensed financial terms to publishers.</p><p>The financial incentives participating publishers can expect to receive are grouped into two buckets: guaranteed value and variable value.</p><p>Guaranteed value is a licensing payment that compensates the publisher for allowing OpenAI to access its backlog of data, while variable value is contingent on display success, a metric based on the number of users engaging with linked or displayed content.</p><p>The resulting financial offer would combine the guaranteed and variable values into one payment, which would be structured on an annual basis.&nbsp;</p><p>“The PPP program is more about scraping than training,” said one executive. “OpenAI has presumably already ingested and trained on these publishers’ archival data, but it needs access to contemporary content to answer contemporary queries.”</p><!--nextpage--><p>In return for these payments, OpenAI would gain two benefits. </p><p>It would have the ability to train on a publisher’s content and the license to display that information in ChatGPT products, complete with attribution and links. It would also get to announce the publisher as a preferred partner and work with them to build out these experiences.</p><h4><strong>Participation boosts publisher payouts</strong></h4><p>According to the deck, publisher participation in PPP creates a better experience for OpenAI users, which will help shift engagement toward browsing, i.e. queries that result in responses with links.</p><p>Roughly 25% of ChatGPT users already use the browse function, but the company expects that a majority of users will do so once the feature is broadly rolled out. If more users engage with publishers’ links, the <a href="https://www.adweek.com/category/media-news/" target="_blank" rel="noreferrer noopener">media companies</a> could earn larger payments for their variable value.&nbsp;</p><p>PPP members will see their content receive its “richer brand expression” through a series of content display products: the branded hover link, the anchored link and the in-line treatment.</p><p>In the hover treatment, which is available today, OpenAI will hyperlink keywords in its responses to search queries. The links appear as blue text and reveal a clickable tab when moused over. </p><p>In the anchor treatment, branded, clickable buttons appear below ChatGPT’s response to a user query. And the in-line product inserts a pullquote into the text of ChatGPT’s response, whose font is larger and includes a clickable, branded link.&nbsp;</p><p>All three content display products seek to cite the publishers whose writing is being used to answer the search query, although the setup will likely lead fewer users to visit publishers’ websites.&nbsp;</p><p>A recent model from The Atlantic found that if a search engine like Google were to integrate AI into search, it would answer a user’s query 75% of the time without requiring a clickthrough to its website.</p><h4><strong>Where publishers go from here</strong></h4><p>The details of the program add further color to the complicated relationship between digital publishers and OpenAI. The uncertain legal standing of the data-scraping methodology that OpenAI uses to power its large-language models has made licensing negotiations between the two parties complex.</p><p>While some publishers have opted to partner with OpenAI, others, <a href="https://www.adweek.com/media/open-ai-response-new-york-times-lawsuit/" target="_blank">including recent NewFronts participant The New York Times</a> and eight Alden Global Capital titles, have sued the tech firm on the grounds that it has used copyrighted articles without permission.</p><p>The vast majority of news publishers, as well as independent websites, have neither partnered with OpenAI nor taken legal action. According to one media executive, through programs such as Preferred Publisher, OpenAI is looking to change that.</p><p>“At the recent Aspen Conference in New York on AI and the news,” the person said, “OpenAI was very open about their need to attract publishers into their partnership program.”&nbsp;</p><!--nextpage--><p><em>This story has updated to include a response from OpenAI.</em></p></div>
<div id="meter-count">
  
  
    
    
  
  <a href="#" onclick="ShowAndHide()">
    
  </a>
  
  
  <div>
        <p>
          <h3>Enjoying Adweek's Content? Register for More Access!</h3>
        </p>
      </div>
</div>                                                                    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leaked FBI Email Reportedly Shows Desperation to Justify Warrantless Wiretaps (161 pts)]]></title>
            <link>https://gizmodo.com/leaked-fbi-email-warrantless-wiretaps-section-702-1851464520</link>
            <guid>40309957</guid>
            <pubDate>Thu, 09 May 2024 16:34:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/leaked-fbi-email-warrantless-wiretaps-section-702-1851464520">https://gizmodo.com/leaked-fbi-email-warrantless-wiretaps-section-702-1851464520</a>, See on <a href="https://news.ycombinator.com/item?id=40309957">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Congress reauthorized America’s warrantless wiretapping program last month after some <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/national-security-threat-likely-nukes-in-space-1851257693&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/national-security-threat-likely-nukes-in-space-1851257693">successful fearmongering</a></span> by national security hawks on Capitol Hill. But an internal FBI email, leaked to <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.wired.com/story/fbi-section-702-us-person-queries-email/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.wired.com/story/fbi-section-702-us-person-queries-email/" target="_blank" rel="noopener noreferrer">Wired</a></span> on Wednesday, may accidentally reveal how the federal law enforcement agency plans to overstep the spirit of the law, while technically maintaining the letter of the law.<br></p><div data-video-id="196937" data-monetizable="false" data-position="sidebar" data-video-title="Approaching Queerness in Doctor Who" data-video-blog-id="4" data-video-network="gizmodo" data-video-duration="72" data-playlist="196937,196931,196911" data-current="196937"><div><p>Approaching Queerness in Doctor Who</p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/196937/196937_240p.mp4" label="240p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/196937/196937_480p.mp4" label="480p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/196937/196937_720p.mp4" label="720p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/196937/196937_1080p.mp4" label="1080p" type="video/mp4"><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/22499.vtt" srclang="en"></video><div><ul><li data-label="">Off</li><li data-label="English">English</li></ul></div></div><p>The controversial spying program is Section 702 in the Foreign Intelligence Surveillance Act (FISA) and allows the interception of foreign communications that sometimes include American citizens. The program ostensibly includes safeguards to ensure the law isn’t being used to unnecessarily spy on Americans, but it’s pretty clear from this new email that the FBI likes being able to get communications from Americans.<br></p><p>The email obtained by <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.wired.com/story/fbi-section-702-us-person-queries-email/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.wired.com/story/fbi-section-702-us-person-queries-email/" target="_blank" rel="noopener noreferrer">Wired</a></span> dated April 20 was written by FBI Deputy Director Paul Abbate and sent out to employees internally.<br></p><p>“To continue to demonstrate why tools like this are essential to our mission, we need to <em>use</em> them, while also holding ourselves accountable for doing so properly and in compliance with legal requirements,” the email reads, according to Wired, which notes that the italicization on the word “use” was in the original email.</p><p>The FBI email made things even more explicit by encouraging searches for Americans when looking through intercepted communications.<br></p><p>“I urge everyone to continue to look for ways to appropriately use US  person queries to advance the mission, with the added confidence that  this new pre-approval requirement will help ensure that those queries  are fully compliant with the law,” the email reads.</p><p>The FBI’s response to Wired is particularly interesting, making it worth quoting at length. From Wired:<br></p><blockquote data-type="BlockQuote"><p>Following publication, FBI spokesperson Susan McKee provided a statement  from the bureau that mischaracterized WIRED’s reporting, inaccurately  claiming it “alleged that that the FBI instructed its employees to  violate the law or FBI policies.” The statement added that Abbate’s  email “emphasized Congress’ recognition of the vital importance of FISA  Section 702 to protect the American people and was sent to ensure that  FBI personnel were immediately aware of, and in compliance with, the  privacy enhancing changes the law has put in place.”</p></blockquote><p>Obviously, the FBI is going to say everyone at the agency follows the law since they quite literally are the law. But Wired spoke with Rep. Zoe Lofgren, a Democrat from California who notes this newly leaked email “directly contradicts earlier assertions” by the FBI when the agency was trying to get the law reauthorized.</p><p>It’s all a mess. The FBI got exactly what it wanted with the reauthorization of Section 702, something that was never really in doubt, even with pressure from a handful of politicians who opposed it. To paraphrase former president Richard Nixon, it’s <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.nixonlibrary.gov/media/video/excerpt-frost-interview-final&quot;,{&quot;metric25&quot;:1}]]" href="https://www.nixonlibrary.gov/media/video/excerpt-frost-interview-final" target="_blank" rel="noopener noreferrer">not illegal</a></span> when the FBI does it. But what are you going to do in such a <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.vox.com/2014/4/18/5624310/martin-gilens-testing-theories-of-american-politics-explained&quot;,{&quot;metric25&quot;:1}]]" href="https://www.vox.com/2014/4/18/5624310/martin-gilens-testing-theories-of-american-politics-explained" target="_blank" rel="noopener noreferrer">ridiculously rigged system</a></span>? </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Familial Transmission of Personality Is Higher Than Shown in Typical Studies (127 pts)]]></title>
            <link>https://osf.io/preprints/psyarxiv/7ygp6</link>
            <guid>40309840</guid>
            <pubDate>Thu, 09 May 2024 16:24:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://osf.io/preprints/psyarxiv/7ygp6">https://osf.io/preprints/psyarxiv/7ygp6</a>, See on <a href="https://news.ycombinator.com/item?id=40309840">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[ESP32 Drum Synth Machine (211 pts)]]></title>
            <link>https://github.com/zircothc/DRUM_2004_V1</link>
            <guid>40309759</guid>
            <pubDate>Thu, 09 May 2024 16:17:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/zircothc/DRUM_2004_V1">https://github.com/zircothc/DRUM_2004_V1</a>, See on <a href="https://news.ycombinator.com/item?id=40309759">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      


    <div>
      <p><a href="#start-of-content" data-skip-target-assigned="false">Skip to content</a>
      
      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p><react-partial partial-name="keyboard-shortcuts-dialog" data-ssr="false">
  
  
  
</react-partial>




      

        

            


<header role="banner" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  <h2>Navigation Menu</h2>

  

  <div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:zircothc/DRUM_2004_V1" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="Xls1zmfjVRFg9r4Wl7imLC4d7t70EC8VFGEzJ3l8PkSTdyhMRwvLvr6FuGcBXE97nR8Ke0WUSEFhP2Op2rYh0Q" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="zircothc/DRUM_2004_V1" data-current-org="" data-current-owner="zircothc" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=%2BiiCrl8XCZuAbyfAa9SDOHr3qvpevm5qEbQhyJ5E7dgHAE61KDk5rdJcw3r8O2y5EM1VVRGGeCAcnvPZAMPdBvItIhbHV0XYmg%2Fv8CP8RyrNEfe%2FO5azXeP83%2F0HfBzzThpa0pxtmgMeyOyb4XMSLxiL230rkV5xdveUyYtTqNEBGgJlJY5TshB80I2tgtZ6Os3NZL2gmjgXM9vDNSDwHD25huy6KhKPg0NpkNddr0SY6FIkrm2fjLlC3oqfuwpNkEgayvjoI7eDRnt67C1s%2Fz2aEhb9BHhYTqxbEYCpXRNdYPNAFBbjCQPJN9yoDnoEubkElq7AvFFM3NylFzAzSvtZjzOE1yO0JplkVZvZaeOlAUOj2K0dk%2F%2FSOwgbMQHPFoQesHKli8w8R2C3332APP4N2Vnm2xWkH6ctA5FLcp3EBJKNBNVzl4MatYE58pi4SwNAbI21ugU4PfXxFLyWGciBcMvFNnQhpAcYNENsQzIi5ugAkpKIzJQnqtEHdidObAsVK3H6mhqBCyrZrDPYIjfu--YhueA04nZryfZRfM--cAd3gG%2Fp%2FjkDIvZzaBkuUA%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=zircothc%2FDRUM_2004_V1" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/zircothc/DRUM_2004_V1&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="81dd76527504e666ffcf483d0fe30a1f780d08432723a3a9b88f6d261439a7e9" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div>
</header>

      
    </div>

  








    


    
    <include-fragment data-base-src="https://github.com/notifications/beta/shelf"></include-fragment>





  <div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  





    






  
  

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div id="repo-content-pjax-container">

  

  <include-fragment src="/zircothc/DRUM_2004_V1/spoofed_commit_check/a9e446620f1cd0590298ad68cbce2fedecb34a5c" data-test-selector="spoofed-commit-check"></include-fragment>

  <div data-view-component="true">        


















<react-partial partial-name="repos-overview" data-ssr="true">
  
  
  <div data-target="react-partial.reactRoot"><div><h2>Repository files navigation</h2><nav aria-label="Repository files"><ul role="list"><li><a href="#" aria-current="page"><span data-component="icon"></span><span data-component="text" data-content="README">README</span></a></li></ul></nav></div><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DRUM_2004_V1</h2><a id="user-content-drum_2004_v1" aria-label="Permalink: DRUM_2004_V1" href="#drum_2004_v1"></a></p>
<p dir="auto">ESP32 DRUM SYNTH MACHINE</p>
<p dir="auto">This is my DRUM SYNTH LOFI MACHINE.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/17828930/326164677-c8327dc2-a3f7-4d81-8d82-ebfe2a7c45c3.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ2NzctYzgzMjdkYzItYTNmNy00ZDgxLThkODItZWJmZTJhN2M0NWMzLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU1MjAxMzAzNmNiNGJlZWY0Y2RkNDgzZTMxZGU2NGMxYTJiOTYzYjRmMDJiYzgyN2JmZjE3NjExNjQ3ZGFhNDcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Uyua3Yza2N0QuZGlt3lN_wHPan8EM8u4O6Llrz74cEc"><img src="https://private-user-images.githubusercontent.com/17828930/326164677-c8327dc2-a3f7-4d81-8d82-ebfe2a7c45c3.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ2NzctYzgzMjdkYzItYTNmNy00ZDgxLThkODItZWJmZTJhN2M0NWMzLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTU1MjAxMzAzNmNiNGJlZWY0Y2RkNDgzZTMxZGU2NGMxYTJiOTYzYjRmMDJiYzgyN2JmZjE3NjExNjQ3ZGFhNDcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Uyua3Yza2N0QuZGlt3lN_wHPan8EM8u4O6Llrz74cEc" alt="IMG_20240406_150440"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Synth engine:</h2><a id="user-content-synth-engine" aria-label="Permalink: Synth engine:" href="#synth-engine"></a></p>
<ul dir="auto">
<li>Wavetable synthesizer based on DZL Arduino library "The Synth" (<a href="https://github.com/dzlonline/the_synth">https://github.com/dzlonline/the_synth</a>)</li>
<li>16 sound polyphony</li>
<li>Sound parameters: Table, Length, Envelope, Pitch, Modulation, + Volume, Pan and Filter.</li>
<li>Filter (LowPassFilter) comes from Mozzi library (<a href="https://github.com/sensorium/Mozzi">https://github.com/sensorium/Mozzi</a>)</li>
</ul>
<p dir="auto">SEQUENCER:</p>
<ul dir="auto">
<li>16 step/pattern editor and random generators (pattern, sound parameters and notes)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hardware:</h2><a id="user-content-hardware" aria-label="Permalink: Hardware:" href="#hardware"></a></p>
<ul dir="auto">
<li>Lolin S2 Mini (ESP32 S2)</li>
<li>PCM5102A I2s dac</li>
<li>24 push buttons (8x3)</li>
<li>Rotary encoder</li>
<li>OLED display I2c</li>
<li>32 LED WS2812B</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Software:</h2><a id="user-content-software" aria-label="Permalink: Software:" href="#software"></a></p>
<p dir="auto">IDE:
Arduino 1.8.19</p>
<p dir="auto">Boards:
Expressif Systems 2.0.14</p>
<p dir="auto">Board: Lolin S2 Mini</p>
<p dir="auto">Libraries:</p>
<ul dir="auto">
<li>Sequencer Timer - uClock: <a href="https://github.com/midilab/uClock">https://github.com/midilab/uClock</a></li>
<li>RGB Leds - Adafruit Neopixel: <a href="https://github.com/adafruit/Adafruit_NeoPixel">https://github.com/adafruit/Adafruit_NeoPixel</a></li>
<li>OLED - u8g2: <a href="https://github.com/olikraus/u8g2">https://github.com/olikraus/u8g2</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Notes:</h2><a id="user-content-notes" aria-label="Permalink: Notes:" href="#notes"></a></p>
<p dir="auto">Schematics uploaded.</p>
<p dir="auto">Join solder pads near SCK pin in PCM5102A module.</p>
<p dir="auto">Video demo of the prototype:</p>
<p dir="auto"><a href="https://www.youtube.com/watch?v=rXl1gpWJp-g" rel="nofollow"><img src="https://camo.githubusercontent.com/6859cb54d06b51e0e0690b1ff5f288e2ec91981a74f66c0be1331ed00486f80d/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f72586c316770574a702d672f302e6a7067" alt="IMG_20240406_150231" data-canonical-src="https://img.youtube.com/vi/rXl1gpWJp-g/0.jpg"></a></p>
<p dir="auto">Waiting PCBs to build the first one :)
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/17828930/326164735-feb9b928-f76a-4b51-93ea-a7afbd6a5c28.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ3MzUtZmViOWI5MjgtZjc2YS00YjUxLTkzZWEtYTdhZmJkNmE1YzI4LmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNlZTFjNjg4YzljOWQyMTUwMjE1NjQ1YmI3MDExYjk2ZDgxZmY5MmJlZDYyNDVmMzEyMDc0YzkyMjI2NDAxZjUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Zq6yD07v-buQ7WaQxTmfWfWiBKceY6Unmnnss2J2fDs"><img src="https://private-user-images.githubusercontent.com/17828930/326164735-feb9b928-f76a-4b51-93ea-a7afbd6a5c28.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ3MzUtZmViOWI5MjgtZjc2YS00YjUxLTkzZWEtYTdhZmJkNmE1YzI4LmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWNlZTFjNjg4YzljOWQyMTUwMjE1NjQ1YmI3MDExYjk2ZDgxZmY5MmJlZDYyNDVmMzEyMDc0YzkyMjI2NDAxZjUmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Zq6yD07v-buQ7WaQxTmfWfWiBKceY6Unmnnss2J2fDs" alt="IMG_20240406_150231"></a>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/17828930/326164951-e1001f26-0993-4221-90d1-e9a2f710af0f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ5NTEtZTEwMDFmMjYtMDk5My00MjIxLTkwZDEtZTlhMmY3MTBhZjBmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA2ZmM0NTAzMDkyM2VmZjBiYzFmODI5MzcyMzBhMTZiNGU4YjFkODk0NzNhM2UwZGY5NjQ2ZGVmNmZmYzc3OTQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.DAz14RYXzspi3g35ofVXaOrf49ivpYcGhtBmBx8F5ZA"><img src="https://private-user-images.githubusercontent.com/17828930/326164951-e1001f26-0993-4221-90d1-e9a2f710af0f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTUyODg3MDQsIm5iZiI6MTcxNTI4ODQwNCwicGF0aCI6Ii8xNzgyODkzMC8zMjYxNjQ5NTEtZTEwMDFmMjYtMDk5My00MjIxLTkwZDEtZTlhMmY3MTBhZjBmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTA5VDIxMDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA2ZmM0NTAzMDkyM2VmZjBiYzFmODI5MzcyMzBhMTZiNGU4YjFkODk0NzNhM2UwZGY5NjQ2ZGVmNmZmYzc3OTQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.DAz14RYXzspi3g35ofVXaOrf49ivpYcGhtBmBx8F5ZA" alt="board"></a></p>
</article></div></div>
</react-partial>

        </div></div>

</turbo-frame>


    </main>
  </div>

          




    <ghcc-consent id="ghcc" data-initial-cookie-consent-allowed="" data-cookie-consent-required="true"></ghcc-consent>


  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Ellipsis – Automated PR reviews and bug fixes (104 pts)]]></title>
            <link>https://www.ellipsis.dev/</link>
            <guid>40309719</guid>
            <pubDate>Thu, 09 May 2024 16:14:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ellipsis.dev/">https://www.ellipsis.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=40309719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" data-framer-hydrate-v2="{&quot;routeId&quot;:&quot;augiA20Il&quot;,&quot;localeId&quot;:&quot;default&quot;,&quot;breakpoints&quot;:[{&quot;hash&quot;:&quot;72rtr7&quot;,&quot;mediaQuery&quot;:&quot;(min-width: 1440px)&quot;},{&quot;hash&quot;:&quot;103465x&quot;,&quot;mediaQuery&quot;:&quot;(min-width: 1280px) and (max-width: 1439px)&quot;},{&quot;hash&quot;:&quot;142h2bu&quot;,&quot;mediaQuery&quot;:&quot;(min-width: 810px) and (max-width: 1279px)&quot;},{&quot;hash&quot;:&quot;1pkud1z&quot;,&quot;mediaQuery&quot;:&quot;(max-width: 809px)&quot;}]}" data-framer-ssr-released-at="2024-05-06T12:43:44.396Z" data-framer-page-optimized-at="2024-05-09T17:51:33.965Z"><figure data-framer-name="Background" name="Background"></figure><div data-framer-name="cta" name="cta"><div data-framer-name="Content Wrapper" name="Content Wrapper"><a data-border="true" data-framer-name="Main" name="Main" href="https://docs.ellipsis.dev/code#from-a-pr" target="_blank" rel="noopener"></a><div><p data-framer-name="Enhance the way you" data-framer-component-type="RichTextContainer"><h2 data-styles-preset="LdKDZ3RUB"><span data-text-fill="true">Ship faster with AI</span></h2></p></div><p data-styles-preset="ppPqjGXDa">Ellipsis is an AI devtool that reviews pull requests and converts GitHub comments into working, tested code.</p></div><div data-framer-name="Content Wrapper" name="Content Wrapper"><p data-styles-preset="skV12T4aE">Backed by</p><div data-framer-name="YC_logo" name="YC_logo"><p><img decoding="async" sizes="(min-width: 1440px) 150px, (min-width: 1280px) and (max-width: 1439px) 150px, (min-width: 810px) and (max-width: 1279px) 150px, (max-width: 809px) 150px" srcset="https://framerusercontent.com/images/3EGsNY7lyfSWhMVz634fBMRDR4.png?scale-down-to=512 512w,https://framerusercontent.com/images/3EGsNY7lyfSWhMVz634fBMRDR4.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/3EGsNY7lyfSWhMVz634fBMRDR4.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/3EGsNY7lyfSWhMVz634fBMRDR4.png 4096w" src="https://framerusercontent.com/images/3EGsNY7lyfSWhMVz634fBMRDR4.png?scale-down-to=2048" alt="" data-framer-original-sizes="150px"></p></div></div></div><div data-framer-name="FEATURES" name="FEATURES"><div data-framer-name="Headings Wrapper" name="Headings Wrapper"><p><span><strong>But that's only the beginning</strong></span>. Ellipsis is capable of reviewing, writing, and answering questions about your source code.</p></div><div data-framer-name="Features Wrapper" name="Features Wrapper"><div data-framer-name="Feature Block" name="Feature Block" data-border="true"><p data-framer-name="100+ Languages Support" data-framer-component-type="RichTextContainer"><h4>20+ Languages Supported</h4></p></div><div data-framer-name="Feature Block" name="Feature Block"><div><p><img decoding="async" loading="lazy" src="https://framerusercontent.com/images/HlAwD5YbZZw5JczQDnm3WQhEw.png" alt=""></p></div><div data-framer-name="Content Wrapepr" name="Content Wrapepr"><p data-styles-preset="skV12T4aE">Ship faster by converting requirements into working, tested code.</p></div></div><div data-framer-name="Feature Block" name="Feature Block" data-border="true"><p data-framer-name="Modern and easy UI" data-framer-component-type="RichTextContainer"><h4>Free 7 day trial</h4></p><p>Includes unlimited code reviews, summaries, and generations.</p></div></div><div data-framer-name="Content Wrapper" name="Content Wrapper"><p>Ellipsis doesn't store or train on your source code. It will never commit to your default branch, and will only add new commits or open new pull requests when you explicitly request it. </p></div><div data-framer-name="Pricing" name="Pricing"><div><p>License per seat, reassign seats at any time.</p></div><div><div data-border="true"><div data-framer-name="Variant 1" tabindex="0"><p>Install in your repository</p></div><div data-framer-name="Variant 1" tabindex="0"><p>Email us at team@ellipsis.dev</p></div><div data-framer-name="Variant 1" tabindex="0"><p>If approved, get unlimited code reviews</p></div></div><div data-border="true"><div><p data-framer-component-type="RichTextContainer"><h5>per developer-month</h5></p></div><div><div data-framer-name="Variant 1" tabindex="0"><p>Automatic code reviews on every commit</p></div><div data-framer-name="Variant 1" tabindex="0"><p>Bug fixes &amp; code generations</p></div><div data-framer-name="Variant 1" tabindex="0"><p>Question &amp; answer functionality</p></div></div></div><div data-border="true"><div><p data-framer-component-type="RichTextContainer"><h2>Contact us</h2></p></div><div><div data-framer-name="Variant 1" tabindex="0"><p>Weekly code change summaries</p></div><div data-framer-name="Variant 1" tabindex="0"><p>Priority support, with SLA</p></div></div></div></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Muddy (YC S19) – Multiplayer browser for getting work done (221 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=40309342</link>
            <guid>40309342</guid>
            <pubDate>Thu, 09 May 2024 15:38:48 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=40309342">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td colspan="2"></td><td><div><p>Hey HN! This is Jimmy, Ron and Austa from Muddy (<a href="https://feelmuddy.com/">https://feelmuddy.com/</a>). Muddy is a browser for work that automatically keeps project files organized in the same place where you use and share them. Here’s a demo: <a href="https://www.youtube.com/watch?v=tZr49aN3sjQ" rel="nofollow">https://www.youtube.com/watch?v=tZr49aN3sjQ</a>. Download and try it out here: <a href="https://feelmuddy.com/">https://feelmuddy.com/</a>.</p><p>Building together in the past, we were incredibly frustrated with how much friction there is to get anything done on our computers. I was losing time everyday digging through chat logs looking for that one important link or breaking others out of flow by asking where something is.</p><p>Web apps promised to help us get more done—and they do, but each in its own silo, so there’s still a ton of redundancy to deal with. Every app has its own way of organizing files, its own notification inbox, its own search system. Conversations live everywhere and there isn’t a single view to see everything about a project. Remember when files simply lived in folders rather than the “cloud”?</p><p>We started dedicating time to organizing our files in shared docs and limiting new apps we used. This helped – but the second we didn’t stay on top of organization, links became stale and things got messy again.</p><p>Muddy started as a hack week project we built for ourselves—a single place to use web apps with others, but personalized for each user automatically. Everyone gets their own view for every project, designed around how they work.</p><p>Muddy users work on projects in spaces, which are like automatic tab groups. Users share apps (any site works—a Github PR, Figma file, Trello board—whatever you want) into the project’s shared timeline and Muddy automatically opens relevant tabs for you. It’s a single click to open up all the apps you need for the project.</p><p>Under the hood, Muddy works in the background to keep track of the timeline and uses a LLM to continuously organize apps and keep everything on to date. It considers signals like the popularity of a file, naming conventions, and conversations to figure out what’s relevant. So everyone is presented with an updated list of important tabs, without anyone lifting a finger. Our actual browser is based on Chromium.</p><p>When you need to revisit something from weeks ago, you can rewind the project timeline to that point in a single click. Apps open up in the timeline so you’ll see your files right away. For sites that don’t have built in collaboration features (like documentation), Muddy lets you do annotations directly on the website.</p><p>Projects sometimes get big and need to be broken up. Across all your spaces, Muddy can answer questions like ChatGPT, cite your files as sources, and return apps directly. This is possible since Muddy’s AI shares your browser and can use your authenticated apps locally (with privacy in mind).</p><p>Other browsers like Chrome and Arc focus on solo productivity with sharing as a bolt-on. We think productivity depends on how well you can work with others, and should be the first class consideration. And doing organizational work manually is unsustainable.</p><p>Muddy will have paid subscriptions for teams with additional features like shared passwords, team organization, custom shortcuts, and SSO management. Those aren’t built out yet and the base product will be free. No part of our revenue will come from data monetization.</p><p>We’d love for you to give Muddy a spin! You can download Muddy for Mac or Windows on our website and add others once inside: <a href="https://feelmuddy.com/">https://feelmuddy.com/</a>. We’ll be around to answer questions and look forward to any and all feedback!</p></div></td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft PlayReady – Complete Client Identity Compromise (181 pts)]]></title>
            <link>https://seclists.org/fulldisclosure/2024/May/5</link>
            <guid>40308261</guid>
            <pubDate>Thu, 09 May 2024 13:53:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://seclists.org/fulldisclosure/2024/May/5">https://seclists.org/fulldisclosure/2024/May/5</a>, See on <a href="https://news.ycombinator.com/item?id=40308261">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="nst-content">

<!--X-Body-Begin-->
<!--X-User-Header-->
<a href="https://seclists.org/fulldisclosure/"><img src="https://seclists.org/images/fulldisclosure-logo.png" width="80" alt="fulldisclosure logo"></a>
<h2><a href="https://seclists.org/fulldisclosure/">Full Disclosure</a>
mailing list archives</h2>
<!--X-User-Header-End-->
<!--X-TopPNI-->


<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->

<hr>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->


<em>From</em>: Security Explorations &lt;contact () security-explorations com&gt;<br>

<em>Date</em>: Thu, 9 May 2024 10:02:26 +0200<br>

<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
<hr>
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre>Hello All,

We have come up with two attack scenarios that make it possible to
extract private ECC keys used by a PlayReady client (Windows SW DRM
scenario) for the communication with a license server and identity
purposes.

More specifically, we successfully demonstrated the extraction of the
following keys:
- private signing key used to digitally sign license requests issued
by PlayReady client,
- private encryption key used to decrypt license responses received by
the client (decrypt license blobs carrying encrypted content keys).

A proof for the above (which Microsoft should be able to confirm) is
available at this link:
<a rel="nofollow" href="https://security-explorations.com/samples/wbpmp_id_compromise_proof.txt">https://security-explorations.com/samples/wbpmp_id_compromise_proof.txt</a>

While PlayReady security is primary about security of content keys,
ECC keys that make up client identity are even more important. Upon
compromise, these keys can be used to mimic a PlayReady client outside
of a Protected Media Path environment and regardless of the imposed
security restrictions.

In that context, extraction of ECC keys used as part of a PlayReady
client identity constitute an ultimate compromise of a PlayReady
client on Windows ("escape" of the PMP environment, ability to request
licenses and decrypt content keys).

Content key extraction from Protected Media Path process (through XOR
key or white-box crypto data structures) in a combination with this
latest identity compromise attack means that there is nothing left to
break when it comes to Windows SW DRM implementation.

Let this serve as a reminder that PlayReady content protection
implemented in software and on a client side has little chances of a
“survival” (understood as a state of not being successfully reverse
engineered and compromised). In that context, this is vendor’s
responsibility to constantly increase the bar and with the use of all
available technological means.

Thank you.

Best Regards,
Adam Gowdiak

----------------------------------
Security Explorations -
AG Security Research Lab
<a rel="nofollow" href="https://security-explorations.com/">https://security-explorations.com</a>
----------------------------------
_______________________________________________
Sent through the Full Disclosure mailing list
<a rel="nofollow" href="https://nmap.org/mailman/listinfo/fulldisclosure">https://nmap.org/mailman/listinfo/fulldisclosure</a>
Web Archives &amp; RSS: <a rel="nofollow" href="https://seclists.org/fulldisclosure/">https://seclists.org/fulldisclosure/</a></pre>
<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
<hr>
<!--X-Follow-Ups-End-->
<!--X-References-->
<!--X-References-End-->
<!--X-BotPNI-->

<h3>Current thread:</h3>
<ul>
<li><strong>Microsoft PlayReady - complete client identity compromise</strong> <em>Security Explorations (May 09)</em>
</li></ul>


<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VideoPrism: A foundational visual encoder for video understanding (102 pts)]]></title>
            <link>https://research.google/blog/videoprism-a-foundational-visual-encoder-for-video-understanding/</link>
            <guid>40308044</guid>
            <pubDate>Thu, 09 May 2024 13:32:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.google/blog/videoprism-a-foundational-visual-encoder-for-video-understanding/">https://research.google/blog/videoprism-a-foundational-visual-encoder-for-video-understanding/</a>, See on <a href="https://news.ycombinator.com/item?id=40308044">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                
                <section>
  <h2>Quick links</h2>
  <ul>
    
    
      
    
  </ul>
</section>
                
                


<div>
        <p><span>Posted by Long Zhao, Senior Research Scientist, and Ting Liu, Senior Staff Software Engineer, Google Research</span> </p>
<p>An astounding number of videos are available on the Web, covering a variety of content from everyday moments people share to historical moments to scientific observations, each of which contains a unique record of the world. The right tools could help researchers analyze these videos, transforming how we understand the world around us.</p>

<p>Videos offer dynamic visual content far more rich than static images, capturing movement, changes, and dynamic relationships between entities. Analyzing this complexity, along with the immense diversity of publicly available video data, demands models that go beyond traditional image understanding. Consequently, many of the approaches that best perform on video understanding still rely on specialized models tailor-made for particular tasks. Recently, there has been exciting progress in this area using video foundation models (ViFMs), such as <a href="https://arxiv.org/abs/2109.14084" target="_blank" rel="noopener noreferrer">VideoCLIP</a>, <a href="https://arxiv.org/abs/2212.03191" target="_blank" rel="noopener noreferrer">InternVideo</a>, <a href="https://arxiv.org/abs/2212.04979" target="_blank" rel="noopener noreferrer">VideoCoCa</a>, and <a href="https://arxiv.org/abs/2303.16058" target="_blank" rel="noopener noreferrer">UMT</a>. However, building a ViFM that handles the sheer diversity of video data remains a challenge.</p>
<p>With the goal of building a single model for general-purpose video understanding, we introduce “<a href="https://arxiv.org/abs/2402.13217" target="_blank" rel="noopener noreferrer">VideoPrism: A Foundational Visual Encoder for Video Understanding</a>”. VideoPrism is a ViFM designed to handle a wide spectrum of video understanding tasks, including classification, localization, retrieval, captioning, and question answering (QA). We propose innovations in both the pre-training data as well as the modeling strategy. We pre-train VideoPrism on a massive and diverse dataset: 36 million high-quality video-text pairs and 582 million video clips with noisy or machine-generated parallel text. Our pre-training approach is designed for this hybrid data, to learn both from video-text pairs and the videos themselves. VideoPrism is incredibly easy to adapt to new video understanding challenges, and achieves state-of-the-art performance using a single frozen model.</p>


<p>VideoPrism is a general-purpose video encoder that enables state-of-the-art results over a wide spectrum of video understanding tasks, including classification, localization, retrieval, captioning, and question answering, by producing video representations from a single frozen model.</p>

<h2>Pre-training data</h2>
<p>A powerful ViFM needs a very large collection of videos on which to train — similar to other foundation models (FMs), such as those for large language models (LLMs). Ideally, we would want the pre-training data to be a representative sample of all the videos in the world. While naturally most of these videos do not have perfect captions or descriptions, even imperfect text can provide useful information about the semantic content of the video.</p>
<p>To give our model the best possible starting point, we put together a massive pre-training corpus consisting of several public and private datasets, including <a href="https://rowanzellers.com/merlot/" target="_blank" rel="noopener noreferrer">YT-Temporal-180M</a>, <a href="https://arxiv.org/abs/2307.06942" target="_blank" rel="noopener noreferrer">InternVid</a>, <a href="https://arxiv.org/abs/2204.00679" target="_blank" rel="noopener noreferrer">VideoCC</a>, <a href="https://arxiv.org/abs/2007.14937" target="_blank" rel="noopener noreferrer">WTS-70M</a>, etc. This includes 36 million carefully selected videos with high-quality captions, along with an additional 582 million clips with varying levels of noisy text (like auto-generated transcripts). To our knowledge, this is the largest and most diverse video training corpus of its kind.</p>
<table>
<tbody>
<tr>
<td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrhfnM1Rg_xbS1b3ZtydWc0M7zOchLpi5qdj65UaR3mOYbV8SQQqKhUhltYwmkPNqrULdeVeE1nU3gnRkjR7pE-yFaiVRC1al-BxZecsO0aojXFzSDhfv45oZoOBeYA93IiNeCGdnUryh4HLc3w7Qr2PX0fy6-4qFMTKBORA_PfHspp7Nr1OW0WnAvn-S9/s1999/image18.png" target="_blank" rel="noopener noreferrer"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgrhfnM1Rg_xbS1b3ZtydWc0M7zOchLpi5qdj65UaR3mOYbV8SQQqKhUhltYwmkPNqrULdeVeE1nU3gnRkjR7pE-yFaiVRC1al-BxZecsO0aojXFzSDhfv45oZoOBeYA93IiNeCGdnUryh4HLc3w7Qr2PX0fy6-4qFMTKBORA_PfHspp7Nr1OW0WnAvn-S9/s16000/image18.png" data-original-height="779" data-original-width="1999"></a></td>
</tr>
<tr>
<td>Statistics on the video-text pre-training data. The large variations of the&nbsp;<a href="https://arxiv.org/abs/2104.14806" target="_blank" rel="noopener noreferrer">CLIP similarity scores</a>&nbsp;(the higher, the better) demonstrate the diverse caption quality of our pre-training data, which is a byproduct of the various ways used to harvest the text.</td>
</tr>
</tbody>
</table>

<h2>Two-stage training</h2>
<p>The VideoPrism model architecture stems from the standard <a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noopener noreferrer">vision transformer</a> (ViT) with a factorized design that sequentially encodes spatial and temporal information following <a href="https://arxiv.org/abs/2103.15691" target="_blank" rel="noopener noreferrer">ViViT</a>. Our training approach leverages both the high-quality video-text data and the video data with noisy text mentioned above. To start, we use <a href="https://en.wikipedia.org/wiki/Self-supervised_learning#Contrastive_self-supervised_learning" target="_blank" rel="noopener noreferrer">contrastive learning</a> (an approach that minimizes the distance between positive video-text pairs while maximizing the distance between negative video-text pairs) to teach our model to match videos with their own text descriptions, including imperfect ones. This builds a foundation for matching semantic language content to visual content.</p>
<p>After video-text contrastive training, we leverage the collection of videos without text descriptions. Here, we build on the <a href="https://arxiv.org/abs/2212.04500" target="_blank" rel="noopener noreferrer">masked video modeling framework</a> to predict masked patches in a video, with a few improvements. We train the model to predict both the video-level global embedding and token-wise embeddings from the first-stage model to effectively leverage the knowledge acquired in that stage. We then randomly shuffle the predicted tokens to prevent the model from learning shortcuts.</p>
<p>What is unique about VideoPrism’s setup is that we use two complementary pre-training signals: text descriptions and the visual content within a video. Text descriptions often focus on what things look like, while the video content provides information about movement and visual dynamics. This enables VideoPrism to excel in tasks that demand an understanding of both appearance and motion.</p>

<h2>Results</h2>
<p>We conduct extensive evaluation on VideoPrism across four broad categories of video understanding tasks, including video classification and localization, video-text retrieval, video captioning, question answering, and scientific video understanding. VideoPrism achieves state-of-the-art performance on 30 out of 33 video understanding benchmarks — all with minimal adaptation of a single, frozen model.</p>
<table>
<tbody>
<tr>
<td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiUtXCxgEXrgAZJ2B-Mn8L0DP7VkFUfUbI1yLTgGYSbWtn_Q5AjgGRgi3yQ5PMB3fVFlHLzDP4yhlCeGaPpdXr5I1-TNYelYMUBYiXx16qNYTpqKwAqXX7-EFV-4Asn6qYFWOb6_5p71n5Zzxbt-ZeUy5yIj2aieKXl0LnFOqdhKXa56xm4ZoXbccYDz3H/s1999/image20.png" target="_blank" rel="noopener noreferrer"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiUtXCxgEXrgAZJ2B-Mn8L0DP7VkFUfUbI1yLTgGYSbWtn_Q5AjgGRgi3yQ5PMB3fVFlHLzDP4yhlCeGaPpdXr5I1-TNYelYMUBYiXx16qNYTpqKwAqXX7-EFV-4Asn6qYFWOb6_5p71n5Zzxbt-ZeUy5yIj2aieKXl0LnFOqdhKXa56xm4ZoXbccYDz3H/w628-h640/image20.png" width="628" height="640" data-original-height="1999" data-original-width="1959"></a></td>
</tr>
<tr>
<td>VideoPrism compared to the previous best-performing FMs.</td>
</tr>
</tbody>
</table>

<h3>Classification and localization</h3>
<p>We evaluate VideoPrism on an existing large-scale video understanding benchmark (<a href="https://arxiv.org/abs/2307.03166" target="_blank" rel="noopener noreferrer">VideoGLUE</a>) covering classification and localization tasks. We find that (1) VideoPrism outperforms all of the other state-of-the-art FMs, and (2) no other single model consistently came in second place. This tells us that VideoPrism has learned to effectively pack a variety of video signals into one encoder — from semantics at different granularities to appearance and motion cues — and it works well across a variety of video sources.</p>


<h3>Combining with LLMs</h3>
<p>We further explore combining VideoPrism with LLMs to unlock its ability to handle various video-language tasks. In particular, when paired with a text encoder (following <a href="https://arxiv.org/abs/2111.07991" target="_blank" rel="noopener noreferrer">LiT</a>) or a language decoder (such as <a href="https://arxiv.org/abs/2305.10403" target="_blank" rel="noopener noreferrer">PaLM-2</a>), VideoPrism can be utilized for video-text retrieval, video captioning, and video QA tasks. We compare the combined models on a broad and challenging set of vision-language benchmarks. VideoPrism sets the new state of the art on most benchmarks. From the visual results, we find that VideoPrism is capable of understanding complex motions and appearances in videos (e.g., the model can recognize the different colors of spinning objects on the window in the visual examples below). These results demonstrate that VideoPrism is strongly compatible with language models.</p>


<p>We show qualitative results using VideoPrism with a text encoder for video-text retrieval (first row) and adapted to a language decoder for video QA (second and third row). For video-text retrieval examples, the blue bars indicate the embedding similarities between the videos and the text queries.</p>

<h3>Scientific applications</h3>
<p>Finally, we test VideoPrism on datasets used by scientists across domains, including fields such as ethology, behavioral neuroscience, and ecology. These datasets typically require domain expertise to annotate, for which we leverage existing scientific datasets open-sourced by the community including <a href="https://data.caltech.edu/records/zrznw-w7386" target="_blank" rel="noopener noreferrer">Fly vs. Fly</a>, <a href="https://data.caltech.edu/records/s0vdx-0k302" target="_blank" rel="noopener noreferrer">CalMS21</a>, <a href="https://shirleymaxx.github.io/ChimpACT/" target="_blank" rel="noopener noreferrer">ChimpACT</a>, and <a href="https://dirtmaxim.github.io/kabr/" target="_blank" rel="noopener noreferrer">KABR</a>. VideoPrism not only performs exceptionally well, but actually surpasses models designed specifically for those tasks. This suggests tools like VideoPrism have the potential to transform how scientists analyze video data across different fields.</p>
<table>
<tbody>
<tr>
<td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3v-C36GWUp8CkaCVqFvaXYKW6-1SvCo99Ogiul-fSTkftyc-t4z5CNUgEWlJkRmzranQrYHldtBvjeJXsqdB4ZbgBkyaZv-_I9QE5U7kus_Z8QWlVqfzX0JfELSDPfGj9V4QqhUMwX_EkyPM-vG7pdYMXN0kj1-s98IZJl3U8CpvqoOHyAsuwXIVt7M4_/s1200/image5.png" target="_blank" rel="noopener noreferrer"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3v-C36GWUp8CkaCVqFvaXYKW6-1SvCo99Ogiul-fSTkftyc-t4z5CNUgEWlJkRmzranQrYHldtBvjeJXsqdB4ZbgBkyaZv-_I9QE5U7kus_Z8QWlVqfzX0JfELSDPfGj9V4QqhUMwX_EkyPM-vG7pdYMXN0kj1-s98IZJl3U8CpvqoOHyAsuwXIVt7M4_/w640-h397/image5.png" width="640" height="397" data-original-height="742" data-original-width="1200"></a></td>
</tr>
<tr>
<td>VideoPrism outperforms the domain experts on various scientific benchmarks. We show the absolute score differences to highlight the relative improvements of VideoPrism. We report mean average precision (mAP) for all datasets, except for KABR which uses class-averaged top-1 accuracy.</td>
</tr>
</tbody>
</table>

<h2>Conclusion</h2>
<p>With VideoPrism, we introduce a powerful and versatile video encoder that sets a new standard for general-purpose video understanding. Our emphasis on both building a massive and varied pre-training dataset and innovative modeling techniques has been validated through our extensive evaluations. Not only does VideoPrism consistently outperform strong baselines, but its unique ability to generalize positions it well for tackling an array of real-world applications. Because of its potential broad use, we are committed to continuing further responsible research in this space, guided by our <a href="http://ai.google/principles" target="_blank" rel="noopener noreferrer">AI Principles</a>. We hope VideoPrism paves the way for future breakthroughs at the intersection of AI and video analysis, helping to realize the potential of ViFMs across domains such as scientific discovery, education, and healthcare.</p>

<h2>Acknowledgements</h2>
<p><em>This blog post is made on behalf of all the VideoPrism authors: Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schroff, Ming-Hsuan Yang, David A. Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, and Boqing Gong. We sincerely thank David Hendon for their product management efforts, and Alex Siegman, Ramya Ganeshan, and Victor Gomes for their program and resource management efforts. We also thank Hassan Akbari, Sherry Ben, Yoni Ben-Meshulam, Chun-Te Chu, Sam Clearwater, Yin Cui, Ilya Figotin, Anja Hauth, Sergey Ioffe, Xuhui Jia, Yeqing Li, Lu Jiang, Zu Kim, Dan Kondratyuk, Bill Mark, Arsha Nagrani, Caroline Pantofaru, Sushant Prakash, Cordelia Schmid, Bryan Seybold, Mojtaba Seyedhosseini, Amanda Sadler, Rif A. Saurous, Rachel Stigler, Paul Voigtlaender, Pingmei Xu, Chaochao Yan, Xuan Yang, and Yukun Zhu for the discussions, support, and feedback that greatly contributed to this work. We are grateful to Jay Yagnik, Rahul Sukthankar, and Tomas Izo for their enthusiastic support for this project. Lastly, we thank Tom Small, Jennifer J. Sun, Hao Zhou, Nitesh B. Gundavarapu, Luke Friedman, and Mikhail Sirotenko for the tremendous help with making this blog post.</em></p>

    </div>

                

                


<section aria-label="List of footnotes">
  <ol>
    
  </ol>
</section>

                


            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[No "Zero-Shot" Without Exponential Data (172 pts)]]></title>
            <link>https://arxiv.org/abs/2404.04125</link>
            <guid>40307832</guid>
            <pubDate>Thu, 09 May 2024 13:08:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2404.04125">https://arxiv.org/abs/2404.04125</a>, See on <a href="https://news.ycombinator.com/item?id=40307832">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2404.04125">View PDF</a>
    <a href="https://arxiv.org/html/2404.04125v2">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Web-crawled pretraining datasets underlie the impressive "zero-shot" evaluation performance of multimodal models, such as CLIP for classification/retrieval and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of "zero-shot" generalization is for such multimodal models, as it is not known to what extent their pretraining datasets encompass the downstream concepts targeted for during "zero-shot" evaluation. In this work, we ask: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets? We comprehensively investigate this question across 34 models and five standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. We consistently find that, far from exhibiting "zero-shot" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream "zero-shot" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions. Furthermore, upon benchmarking models on long-tailed data sampled based on our analysis, we demonstrate that multimodal models across the board perform poorly. We contribute this long-tail test set as the "Let it Wag!" benchmark to further research in this direction. Taken together, our study reveals an exponential need for training data which implies that the key to "zero-shot" generalization capabilities under large-scale training paradigms remains to be found.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Vishaal Udandarao [<a href="https://arxiv.org/show-email/3ec97a25/2404.04125">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2404.04125v1">[v1]</a></strong>
        Thu, 4 Apr 2024 17:58:02 UTC (37,862 KB)<br>
    <strong>[v2]</strong>
        Mon, 8 Apr 2024 21:14:43 UTC (37,863 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Exploring HN by mapping and analyzing 40M posts and comments for fun (443 pts)]]></title>
            <link>https://blog.wilsonl.in/hackerverse/</link>
            <guid>40307519</guid>
            <pubDate>Thu, 09 May 2024 12:31:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.wilsonl.in/hackerverse/">https://blog.wilsonl.in/hackerverse/</a>, See on <a href="https://news.ycombinator.com/item?id=40307519">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <article>
      
<p><img src="https://blog.wilsonl.in/hackerverse/map.png" alt="Semantic map of Hacker News posts."></p>
<p>The above is a map of all Hacker News posts since its <a href="https://news.ycombinator.com/item?id=1">founding</a>, laid semantically i.e. where there <em>should</em> be some relationship between positions and distances. I've been building it and some other interesting stuff over the past few weeks, to play around with <a href="https://platform.openai.com/docs/guides/embeddings">text embeddings</a>. Given that HN has a lot of interesting, curated content and <a href="https://github.com/HackerNews/API">exposes all its content programatically</a>, I thought it'd be a fun place to start.</p>
<p>A quick primer of embeddings: they are a powerful and cool way to represent <em>something</em> (in this case, text) as a point in a high-dimensional <a href="https://en.wikipedia.org/wiki/Latent_space">space</a>, which in practical terms just means an array of floats, one for its coordinate in that dimension. The absolute position doesn't really mean much, but their relativity to each other is where much of their usefulness comes in, because "similar" things should be nearby, while dissimilar things are far apart. Text embeddings these days often <a href="https://en.wikipedia.org/wiki/BERT_(language_model)">come from language models</a>, given their SOTA understanding of the meaning of text, and it's pretty trivial to generate them given the high-quality open source <a href="https://huggingface.co/spaces/mteb/leaderboard">models</a> and <a href="https://huggingface.co/sentence-transformers">libraries</a>, that are freely accessible to anyone with a CPU or GPU.</p>
<p>Going in, my theories for what I could do <em>if</em> I had the embeddings were:</p>
<ul>
<li>Powerful search: given HN's curated high bar of content, I knew there were lots of interesting insights and things I've missed over the years. It'd be cool if I could query something like "how to communicate well" and instantly surface the best advice over the years for communicating.</li>
<li>Recommendations: it might be possible to build a personalized discovery engine by navigating the latent space of HN content biased towards/away from dis/interest areas.</li>
<li>Analysis: there are a lot of opinions on HN. It should be possible to calculate the sentiment and popularity of various topics within the community, find opposing viewpoints, etc.</li>
</ul>
<p>These sounded pretty interesting, so I decided to dive right in. In this blog post, I'll lay out my journey starting from no data and no code, to interactive search, analysis, and spatial visualization tools leveraging millions of HN content, dive into all the interesting diverse problems and solutions that came up along the way, and hopefully give you some indication (and hopefully motivation) of the power and applicability of embeddings in many areas.</p>
<p>You may also have better ideas of using the data or the demo that I came up with. I'm also opening up all the data and source code that I built as part of this journey, and invite you to use them to play around, suggest and refine ideas, or kick off your own creative projects and learning journeys. Over 30 million comments and 4 million posts are available to download <a href="https://github.com/wilsonzlin/hackerverse/releases/tag/dataset-39996091">here</a>, which include metadata (IDs, scores, authors, etc.), embeddings, and texts (including crawled web pages). The code is also <a href="https://github.com/wilsonzlin/hackerverse">completely open source</a>; feel free to fork, open PRs, or raise issues. If you do end up using the code or data, I'd appreciate a reference back to this project and blog post.</p>
<p>If you want to jump right to the demo, <a href="#demo">click here</a>. Otherwise, let's dive in!</p>
<h2 id="fetching-items-from-hn">Fetching items from HN</h2>
<p>HN has a very simple <a href="https://github.com/HackerNews/API">public API</a>:</p>
<pre><code>GET /v0/item/$ITEM_ID.json
Host: hacker-news.firebaseio.com
</code></pre><p>Everything is an <em>item</em>, and the response always has the same JSON object structure:</p>
<pre><code><span>{</span>
  <span>"by"</span><span>:</span> <span>"dhouston"</span><span>,</span>
  <span>"descendants"</span><span>:</span> <span>71</span><span>,</span>
  <span>"id"</span><span>:</span> <span>8863</span><span>,</span>
  <span>"score"</span><span>:</span> <span>111</span><span>,</span>
  <span>"time"</span><span>:</span> <span>1175714200</span><span>,</span>
  <span>"title"</span><span>:</span> <span>"My YC app: Dropbox - Throw away your USB drive"</span><span>,</span>
  <span>"type"</span><span>:</span> <span>"story"</span><span>,</span>
  <span>"url"</span><span>:</span> <span>"http://www.getdropbox.com/u/2/screencast.html"</span>
<span>}</span>
</code></pre><p>There's also a <a href="https://hacker-news.firebaseio.com/v0/maxitem.json">maxitem.json</a> API, which gives the largest ID. As of this writing, the max item ID is over 40 million. Even with a very nice and low 10 ms mean response time, this would take over 4 days to crawl, so we need some parallelism.</p>
<p>I decided to write a <a href="https://github.com/wilsonzlin/hackerverse/tree/master/enqueuer">quick service</a> in Node.js to do this. An initial approach with a <a href="https://github.com/wilsonzlin/xtjs-lib/blob/master/Semaphore.ts">semaphore</a> and then queueing up the fetch Promises, despite being simple and async, ended up being too slow, with most CPU time being spent in userspace JS code.</p>
<p>It's a good reminder that Node.js can handle async I/O pretty well, but it's still fundamentally a single-threaded dynamic language, and those few parts running JS code can still drag down performance. I moved to using the <a href="https://nodejs.org/api/worker_threads.html">worker threads</a> API and distributed the fetches across all CPUs, which ended up saturating all cores on my machine, mostly spent in kernel space (a good sign). The final code ended up looking something like:</p>
<pre><code><span>new</span> <span>WorkerPool</span>(__filename, <span>cpus</span>().<span>length</span>)
  .<span>workerTask</span>(<span>"process"</span>, <span>async</span> (<span>id</span>: <span>number</span>) =&gt; {
    <span>const</span> item = <span>await</span> <span>fetchItem</span>(id);
    <span>await</span> <span>processItem</span>(item);
  })
  .<span>leader</span>(<span>async</span> (pool) =&gt; {
    <span>let</span> nextId = <span>await</span> <span>getNextIdToResumefrom</span>();
    <span>const</span> maxId = <span>await</span> <span>fetchHnMaxId</span>();

    <span>let</span> nextIdToCommit = nextId;
    <span>const</span> idsPendingCommit = <span>new</span> <span>Set</span>&lt;<span>number</span>&gt;();
    <span>let</span> flushing = <span>false</span>;
    <span>const</span> <span>maybeFlushId</span> = <span>async</span> (<span></span>) =&gt; {
      <span>if</span> (flushing) {
        <span>return</span>;
      }
      flushing = <span>true</span>;
      <span>let</span> didChange = <span>false</span>;
      <span>while</span> (idsPendingCommit.<span>has</span>(nextIdToCommit)) {
        idsPendingCommit.<span>delete</span>(nextIdToCommit);
        nextIdToCommit++;
        didChange = <span>true</span>;
      }
      <span>if</span> (didChange) {
        <span>await</span> <span>recordNextIdToResumeFrom</span>(nextIdToCommit);
      }
      flushing = <span>false</span>;
    };

    <span>const</span> <span>CONCURRENCY</span> = <span>cpus</span>().<span>length</span> * <span>16</span>;
    <span>await</span> <span>Promise</span>.<span>all</span>(
      <span>Array</span>.<span>from</span>({ <span>length</span>: <span>CONCURRENCY</span> }, <span>async</span> () =&gt; {
        <span>while</span> (nextId &lt;= maxId) {
          <span>const</span> id = nextId++;
          <span>await</span> pool.<span>execute</span>(<span>"process"</span>, id);
          idsPendingCommit.<span>add</span>(id);
          <span>maybeFlushId</span>();
        }
      }),
    );
  })
  .<span>go</span>();
</code></pre><p><a href="https://github.com/wilsonzlin/xtjs-lib/blob/master/WorkerPool.ts">WorkerPool</a> is a helper class I made to simplify using worker threads, by making it easy to make type-checked requests between the main thread and a pool of worker threads. The parallelism fetched things out-of-order, so for idempotency, I recorded the marker in order so I don't skip anything if it is interrupted.</p>
<p>Some interesting things I noticed about the HN items returned by the API:</p>
<ul>
<li>Scores never seem to be below -1.</li>
<li>You can't get the downvotes for posts, and the votes at all for comments.</li>
<li>Some posts and comments have blank titles and texts/URLs, despite not being flagged or deleted; I presume they were moderated.</li>
<li>It's possible for comment IDs to have be smaller than an ancestor, probably due to a moderator moving the comment tree.</li>
</ul>
<p>I've exported the HN crawler (in TypeScript) to its own <a href="https://github.com/wilsonzlin/crawler-toolkit-hn">project</a>, if you're ever in need to fetch HN items.</p>
<h2 id="generating-embeddings">Generating embeddings</h2>
<p>My initial theory was that the title alone would be enough to semantically represent a post, so I dove right in with just the data collected so far to generate some embeddings. The <a href="https://huggingface.co/spaces/mteb/leaderboard">Massive Text Embedding Benchmark (MTEB)</a> is a good place to compare the latest SOTA models, where I found <a href="https://huggingface.co/BAAI/bge-m3">BGE-M3</a>, the latest iteration of the popular <a href="https://huggingface.co/BAAI/bge-base-en-v1.5">FlagEmbedding</a> models that came out last year. Their v3 version supports generating "lexical weights" for basically free, which are essentially sparse bags-of-words, a map from token ID to weight, which you can use with an algorithm like BM25 in addition to the normal "dense" embeddings for hybrid retrieval. According to their <a href="https://arxiv.org/pdf/2402.03216">paper</a>, this increases retrieval performance significantly.</p>
<p>The infrastructure required for generating embeddings is not so trivial:</p>
<ul>
<li>Models are computationally expensive, with good ones having anything from millions to billions of parameters.</li>
<li>Like most AI models, they are much more efficient to compute on GPUs, but GPU clusters are expensive.</li>
<li>Inference can take hundreds of milliseconds, meaning a processing rate in the ballpark of <em>tens</em> per second. That's almost a <em>year</em> to process 40 million inputs on one GPU.</li>
<li>The GPUs are likely separate from our data and server machines. A way to ensure a flowing full pipe from our data to our GPUs will ensure our GPUs are not expensively idling.</li>
</ul>
<p>Fortunately, I discovered <a href="https://runpod.io/">RunPod</a>, a provider of machines with GPUs that you can deploy your containers onto, at a cost far cheaper than major cloud providers. They also have more cost-effective GPUs like RTX 4090, while still running in datacenters with fast Internet connections. This made scaling up a price-accessible option to mitigate the inference time required.</p>
<p>The GPUs were scattered around the world, so DB connection latency and connection overhead became a problem, and the decentralized client-side pooling caused a lot of server overhead. I created <a href="https://github.com/wilsonzlin/db-rpc">db-rpc</a> to mitigate these aspects. It's a simple service that proxies SQL queries over HTTP/2 to a local DB with a large shared connection pool. HTTP/2 supports multiplexing (= many queries, one connection, no blocking), lighter connections, and quicker handshakes, so for the simple queries I'm making (CRUD statements), it worked great.</p>
<p>A simple message queue was needed to distribute the item IDs to embed to the various GPU workers, which is exactly what AWS SQS offers. Unfortunately, it has quite low rate limits and is expensive per message, which is annoying given the millions of tiny job messages. Some batching can mitigate this partially, but I often need something like this, so I created <a href="https://github.com/wilsonzlin/queued">queued</a>, a RocksDB-based queue service written in Rust. It handles 100K+ op/s with one node, so there's no worrying about batching, message sizes, rate limits, and costs. RocksDB's write-optimized design makes for a great queue storage backing.</p>
<p>The optimizations paid off: after scaling to ~150 GPUs, all 40 million posts and comments were embedded in only a few hours. A snapshot from my Grafana dashboard indicates some of the impact:</p>
<p><img src="https://blog.wilsonl.in/hackerverse/embedder-grafana.png" alt="Grafana dashboard of a period during the embeddings process."></p>
<p>The GPU utilization stayed peak for the entire time, and the processing rate was stable. Connection latencies remained steady and low on average despite many distributed workers and concurrent queries, although there was not as much batching because it was quite expensive to embed each input (at around 600 ms <em>per input</em>), as previously mentioned.</p>
<h2 id="adding-additional-context-by-crawing-the-web">Adding additional context by crawing the web</h2>
<p>Unfortunately, my theory about the titles being enough did not pay off. While it worked well for most posts because they have descriptive titles, a lot have <a href="https://news.ycombinator.com/item?id=21686264">"strange"</a>, <a href="https://news.ycombinator.com/item?id=25771953">creative</a>, <a href="https://news.ycombinator.com/item?id=21379174">ambiguous</a> titles that don't play well with the embedding model. Also, the model tended to cluster <em>Ask HN</em> and <em>Show HN</em> posts together, regardless of topic, probably because, given that the entire input was just the title, those two phrases were significant. I needed more context to give to the model.</p>
<p>For text posts and comments, the answer is simple. However, for the vast majority of link posts, this would mean crawling those pages being linked to. So I wrote up a quick <a href="https://github.com/wilsonzlin/hackerverse/tree/master/crawler">Rust service</a> to fetch the URLs linked to and parse the HTML for metadata (title, picture, author, etc.) and text. This was CPU-intensive so an initial Node.js-based version was 10x slower and a Rust rewrite was worthwhile. Fortunately, other than that, it was mostly smooth and painless, likely because HN links are pretty good (responsive servers, non-pathological HTML, etc.).</p>
<p>Extracting text involved parsing the HTML using the excellent <a href="https://docs.rs/scraper/latest/scraper/">scraper</a>, removing <a href="https://github.com/wilsonzlin/hackerverse/blob/14fde395984a693e0d05c3bfc6f37bb2a7f7f549/crawler/parse.rs#L45">semantically non-primary HTML5 elements</a>, and <a href="https://github.com/wilsonzlin/hackerverse/blob/14fde395984a693e0d05c3bfc6f37bb2a7f7f549/crawler/parse.rs#L140">traversing the remaining tree</a>.</p>
<p>A <em>lot</em> of content even on Hacker News suffers from the well-known <a href="https://en.wikipedia.org/wiki/Link_rot">link rot</a>: around 200K resulted in a 404, DNS lookup failure, or connection timeout, which is a sizable "hole" in the dataset that would be nice to mend. Fortunately, the <a href="https://en.wikipedia.org/wiki/Internet_Archive">Internet Archive</a> has an <a href="https://archive.org/help/wayback_api.php">API</a> that we can use to use to programmatically fetch archived copies of these pages. So, as a final push for a more "complete" dataset, I used the Wayback API to fetch the last few thousands of articles, some dating back years, which was very annoying because IA has very, very low rate limits (around 5 per minute).</p>
<p>Our end tally of pages that could not be fetched:</p>
<p><img src="https://blog.wilsonl.in/hackerverse/fetch_err.webp" alt="Pie chart of fetch error counts."></p>
<p>Not bad, out of 4 million. That's less than 5% of all fetched pages.</p>
<h2 id="embeddings,-attempt-two">Embeddings, attempt two</h2>
<p>Web pages are long, but luckily the BGE-M3 model supports a context window of 8192 tokens. However, the model is really slow, as we saw previously, so I decided to switch to <a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en">jina-embeddings-v2-small-en</a> which has a far smaller parameter count, while still having good performance (according to MTEB). This saved a lot of time and money, as the inference time dropped to 6 ms (100x faster):</p>
<p><img src="https://blog.wilsonl.in/hackerverse/embedder-grafana-jinav2small.png" alt="Grafana dashboard of embedder using jina-embeddings-v2-small-en."></p>
<p>The GPUs could not actually be saturated, because any increase in batch size would cause OOMs due to the extended length of the inputs.</p>
<p>Some pages did not have a lot of text (e.g. more creative visual pages), or could not be fetched at all. To still ensure these posts still had decent context, I packed the top HN comments for those posts after the page text as extra "insurance":</p>
<pre><code><span>const</span> topComments = item.<span>kids</span>?.<span>slice</span>() ?? [];
<span>const</span> <span>MAX_LEN</span> = <span>1024</span> * <span>64</span>; <span>// 64 KiB.</span>
<span>while</span> (topComments.<span>length</span> &amp;&amp; embInput.<span>length</span> &lt; <span>MAX_LEN</span>) {
  <span>// Embellish with top-level top comments (`item.kids` are ranked already). This is useful if the page isn't primarily text, could not be fetched, etc.</span>
  <span>const</span> i = <span>await</span> <span>fetchItem</span>(topComments.<span>shift</span>()!);
  <span>// We don't want to include negative comments as part of the post's text representation.</span>
  <span>if</span> (!i || i.<span>type</span> !== <span>"comment"</span> || i.<span>dead</span> || i.<span>deleted</span> || i.<span>score</span>! &lt; <span>0</span>) {
    <span>continue</span>;
  }
  <span>const</span> text = <span>extractText</span>(i.<span>text</span> ?? <span>""</span>);
  <span>if</span> (!addedSep) {
    <span>// Use Markdown syntax, colon, and ASCII border to really emphasise separation.</span>
    embInput += <span>"\n\n# Comments:\n=========="</span>;
    addedSep = <span>true</span>;
  } <span>else</span> {
    embInput += <span>"\n\n----------"</span>;
  }
  embInput += <span>"\n\n"</span> + text;
}
embInput = embInput.<span>slice</span>(<span>0</span>, <span>MAX_LEN</span>);
</code></pre><p>For comments, many refer to parents or ancestors, so wouldn't make sense alone. Using a similar approach, I traversed comments' ancestors and built up a longer context with everything up to the post title. Now, the inputs for posts and comments were nice and full of context, hopefully corresponding to more accurate, useful embeddings.</p>
<p>One thing I did this time round was to create a <code>kv</code> table that could hold arbitrary keys and values, and store these large values (embeddings, texts, etc.) there. When stored alongside the row, they would make the row "fat" and updates to any column (even for a tiny value) were expensive. Schema changes were also expensive. These weren't worth the benefits of having these mostly-opaque values within a row, which were basically none.</p>
<h2 id="umap">UMAP</h2>
<p><a href="https://umap-learn.readthedocs.io/en/latest/index.html">Uniform Manifold Approximation and Projection (UMAP)</a> is a dimensionality reduction technique, which means to take our large 1024-dimension embedding vectors and turn them into points in fewer dimensional space <em>while</em> still (trying) to preserve most of the semantic relationships between points. <a href="https://scikit-learn.org/stable/modules/decomposition.html#pca">PCA</a> and <a href="https://scikit-learn.org/stable/modules/manifold.html#t-sne">t-SNE</a> are similar algorithms in this space you may have heard of, but UMAP is newer and <a href="https://umap-learn.readthedocs.io/en/latest/performance.html">makes the case</a> that it offers a better performance-accuracy trade-off than the others. Regardless of which algorithm, dim. reduction is often used to visualize embeddings, because it's hard to "see" points in 1024-dimensional space. We'll use UMAP to reduce our embeddings to 2D space, so we can scatter plot it and do some basic eyeballing for basic checks and anything interesting with our dataset.</p>
<p>Generating the 2D embeddings is not hard. UMAP takes in two main things: a PyNNDescent graph, and the original embeddings. There are <a href="https://umap-learn.readthedocs.io/en/latest/parameters.html">a few hyperparameters</a> that affect the distribution of points in the lower dim. space, as well as the primary parameter: the target dimensionality.</p>
<pre><code><span>import</span> umap

<span>with</span> <span>open</span>(<span>"ann.joblib"</span>, <span>"rb"</span>) <span>as</span> f:
    ann = joblib.load(f)
knn_indices, knn_dists = ann.neighbor_graph
mat_emb = np.memmap(<span>"emb.mat"</span>, dtype=np.float32, mode=<span>"r"</span>, shape=(N_ITEMS, <span>512</span>))

mapper = umap.UMAP(
    precomputed_knn=(knn_indices, knn_dists, ann),
    n_components=<span>2</span>,
    metric=<span>"cosine"</span>,
    n_neighbors=N_NEIGHBORS,
    min_dist=MIN_DIST,
)
mapper.fit(mat_emb)
<span># Save the lower dim. embeddings.</span>
mat_umap = mapper.embedding_
<span>with</span> <span>open</span>(<span>"emb_umap.mat"</span>, <span>"wb"</span>) <span>as</span> f:
    f.write(mat_umap.tobytes())
<span># Save the UMAP model for later use.</span>
<span>with</span> <span>open</span>(<span>"umap-model.joblib"</span>, <span>"wb"</span>) <span>as</span> f:
    joblib.dump(mapper, f)
</code></pre><p>Training is highly parallel and, with millions of high dim. inputs, can take a while, so I spun up a c7i.metal-48xl VM on EC2. After about an hour and a half, maxing out the 96-core processor, it was done, and an equivalent <code>(N_ITEMS, 2)</code> matrix was available. I saved both the 2D embeddings, as well as the trained model which can be later used to transform other embeddings without running the fitting process again.</p>
<p>Let's now plot these 2D embeddings and see what we find.</p>
<pre><code><span>import</span> matplotlib.pyplot <span>as</span> plt

mat_umap = load_umap()

plt.figure(figsize=(<span>10</span>, <span>10</span>))
plt.gca().invert_yaxis()
plt.scatter(mat_umap[:, <span>0</span>], mat_umap[:, <span>1</span>], s=<span>1</span>)

plt.savefig(<span>"umap.webp"</span>, dpi=<span>300</span>, bbox_inches=<span>"tight"</span>)
</code></pre><p><img src="https://blog.wilsonl.in/hackerverse/umap.webp" alt="Plot of all post UMAP embeddings."></p>
<p>Oops, too many points. Let's do a quick and easy way to reduce the points by tiling into a fine (but finite) grid and selecting only the highest-scoring post from each cell. Let's also add some titles so we see how those points relate:</p>
<pre><code><span>import</span> pyarrow

df = pyarrow.feather.read_feather(<span>"posts.arrow"</span>, memory_map=<span>True</span>)
df_titles = pyarrow.feather.read_feather(<span>"post_titles.arrow"</span>, memory_map=<span>True</span>)
df = df.merge(df_titles, on=<span>"id"</span>, how=<span>"inner"</span>)
df[<span>"x"</span>] = mat_umap[:, <span>0</span>]
df[<span>"y"</span>] = mat_umap[:, <span>1</span>]

x_range = df[<span>"x"</span>].<span>max</span>() - df[<span>"x"</span>].<span>min</span>()
y_range = df[<span>"y"</span>].<span>max</span>() - df[<span>"y"</span>].<span>min</span>()
grid_size = <span>100</span>
df[<span>"cell_x"</span>] = df[<span>"x"</span>] // (x_range / grid_size)
df[<span>"cell_y"</span>] = df[<span>"y"</span>] // (y_range / grid_size)
df = df.sort_values(<span>"score"</span>, ascending=<span>False</span>).drop_duplicates([<span>"cell_x"</span>, <span>"cell_y"</span>])

plt.figure(figsize=(<span>50</span>, <span>50</span>))
plt.gca().invert_yaxis()
plt.scatter(df[<span>"x"</span>], df[<span>"y"</span>], s=<span>1</span>)
<span>for</span> i, row <span>in</span> df.sample(n=<span>1000</span>).iterrows():
    plt.annotate(row[<span>"text"</span>], (row[<span>"x"</span>], row[<span>"y"</span>]), fontsize=<span>3</span>)
</code></pre><p>It's a bit hard to see (there are still lots of points and text labels, so the image is very high resolution), but if you open the image and zoom in, you can see the 2D space and the semantic relationships between points a bit more clearly:</p>
<p><img src="https://blog.wilsonl.in/hackerverse/umap-labelled.webp" alt="Plot of some post UMAP embeddings, with some labelled."></p>
<p>It also looks like adding more context paid off, as those previously-mentioned posts with "exotic" titles are now placed in more accurate points near related content.</p>
<h2 id="cosine-similiarity">Cosine similiarity</h2>
<p>All the data is now ready. A lot of using embeddings involves finding the similarity between them. This is basically the inverse of the distance: if something is far away (high distance), it's not similar (low similarity), and vice versa. From school, we may think of one way to measure the distance between two points:</p>
<pre><code>x1, y1 = (<span>2</span>, <span>3</span>)
x2, y2 = (<span>4</span>, <span>5</span>)
dist = math.sqrt((x2 - x1)**<span>2</span> + (y2 - y1)**<span>2</span>)
</code></pre><p>Most of us knows the formula as the <a href="https://en.wikipedia.org/wiki/Pythagorean_theorem">Pythagorean theorem</a>, and this way of calculating distance is known as the <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a>, which feels intuitive to us. However, it's not the only way of calculating distance, and it's not the one commonly used for text embeddings.</p>
<p>You may have seen the cosine metric used a lot w.r.t. embeddings and semantic similarity. You might be wondering why the euclidean distance isn't used instead, given how more "normal" it seems in our world. Here's an example to demonstrate why:</p>
<p><img src="https://blog.wilsonl.in/hackerverse/cosine-vs-euclid.webp" alt="Cosine vs. euclid distance metrics."></p>
<p>We can see that in this case, where perhaps the X axis represents "more cat" and Y axis "more dog", using the euclidean distance (i.e. physical distance length), a pitbull is somehow more similar to a Siamese cat than a "dog", whereas intuitively we'd expect the opposite. The fact that a pitbull is "very dog" somehow makes it closer to a "very cat". Instead, if we take the angle distance between lines (i.e. cosine distance, or 1 minus angle), the world makes sense again.</p>
<p>In summary, cosine distances are useful when magitude does not matter (as much), and when using the magnitude would be misleading. This is often the case with text content, where an intense long discussion about X should be similar to X and not an intense long discussion about Y. A good blog post on this is <a href="https://cmry.github.io/notes/euclidean-v-cosine">Euclidean vs. Cosine Distance</a> by Chris Emmery.</p>
<p>Going forward, we'll use the cosine distance/metric/similarity a lot. The core of it is:</p>
<pre><code>dist = corpus_embeddings @ query_embeddings.T
sim = <span>1</span> - dist
</code></pre><p>The <code>@</code> operator performs a dot product. Normally, you then divide by the product of their magnitudes, but in this case, they are unit vectors, so that isn't needed.</p>
<h2 id="building-a-map">Building a map</h2>
<p>Wouldn't it be cool to have an interactive visualization of the latent space of all these embeddings, a "map of the Hacker News universe" so to speak? Something like Google Maps, where you can pan and zoom and move around, with terrain and landmarks and coordinates? It sounds like a fun and challenging thing to try and do!</p>
<p>I first scoped out how this map should work:</p>
<ul>
<li>Zooming in (via pinching or the mousewheel) should increase the amount of points shown, and points should get farther apart.</li>
<li>Some points should be labelled, but not all because it would get cluttered and text would overlap.</li>
<li>Clicking a point should show more details about that post.</li>
<li>It should be in the browser, as a web app, that works well on mobile and desktop and touch and mouse.</li>
</ul>
<p>Basically, it should work like you'd expect, like Google Maps.</p>
<h2 id="preparing-the-map-data">Preparing the map data</h2>
<p>There are millions of points, not ideal to send all at once to the client. Working backwards, there are two "axes" that dictate the points shown: position and zoom, so structuring and segmenting the data alongst those lines is a good start.</p>
<p>The first thing that came to mind was tiling: divide the map space into a grid, then pack all points into each cell. The client can load tiles on-demand as they need to, instead of the whole map. There are probably more sophisticated ways of tiling, esp. given that points aren't distributed evenly and clients have different viewports, but the effectiveness is great given its simplicity. A tile can simply be referred to by its (x, y) coordinate, and stored in any KV datastore (e.g. S3), making it easy to distribute and requiring no server-side logic.</p>
<p>For zooming, my approach was a loop for each Level of Detail (LOD), subdividing into 2x more grid cells per axis (and therefore 4x more points), but making sure to first copy over all points picked by the previous level, because the user doesn't expect a point to disappear as they zoom in.</p>
<p>I aimed for each tile to be less than 20 KiB when packed, a balance between fast to load on mediocre Internet and sizable to avoid excessive fetches. This limited tiles to around 1,500 points: 4+4 bytes for the (x, y), 4 bytes for the ID, and 2 bytes for the score.</p>
<p>To ensure a diverse set of points per tile, I further subdivided each tile to pick at least something from each area; this reduced the chance that a less-populated area in the fringes seems completely non-existent on the map from afar. Any remaining capacity was taken by sampling uniformly, which should visually represent the background distribution on the map.</p>
<p>The code is pretty straightforward and self-explanatory in the <a href="https://github.com/wilsonzlin/hackerverse/blob/master/build-map/main.py">build-map</a> service, so I won't repeat the code here.</p>
<h2 id="building-the-web-app">Building the web app</h2>
<p>It took a while to figure out the best approach. Using even thousands of DOM elements (one for each point) completely trashed performance, so that was out of the picture. Having a giant Canvas, dynamically rendering points only as they come into view, and changing its DOM position and scale when panning/zooming, didn't work either; the points would grow in size when zooming in, and at sufficiently large zoom levels, the image became too big (clearing the Canvas took too long, and memory usage was extreme). Eventually I settled on a Canvas and drawing on every update of the "viewport", which represents the position and zoom level of the current user. Despite needing to redraw potentially thousands of points on every frame (to feel smooth and responsive), this worked really well, and was the simplest approach.</p>
<p>I kept the labelling algorithm simple: repeatedly pick the highest scoring post (a reasonable metric), unless the label would intersect with another existing label. <a href="https://en.wikipedia.org/wiki/R-tree">R-trees</a> can do the job of finding box collisions well, and thankfully <a href="https://github.com/mourner/rbush">RBush</a> exists, an excellent and fast R-tree implementation in JS. There exists a <a href="https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/measureText">measureText()</a> API in the browser, but fetching thousands (or more) titles just to measure them and pick a handful was neither fast nor efficient, so I packed post title lengths into a byte array (so a thousand would only be ~1 KB) and simply used a tuned formula to approximate the length, which worked reasonably well:</p>
<p><img src="https://blog.wilsonl.in/hackerverse/map-bboxes.png" alt="Map of posts with bounding boxes around labels."></p>
<p>The one-off initial calculations of these boxes and collisions was CPU intensive and caused stuttering, so I moved it to its own thread using <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers">Web Workers</a>. I also experimented with <a href="https://developer.mozilla.org/en-US/docs/Web/API/OffscreenCanvas">OffscreenCanvas</a>, but it didn't do much; the render logic was very efficient already, and given that 99% of the app was the map (represented by the Canvas), having "the rest of the app" still respond while the map was rendering was not very useful.</p>
<h2 id="adding-some-visual-appeal-and-guidance">Adding some visual appeal and guidance</h2>
<p>There was something still disorienting and "dull" about the map. Real maps have landmarks, cities, borders, terrains, and colors; there is a sense of direction, orientation, and navigation as you browse and navigate the map. Let's try to add some of those, analogizing where necessary.</p>
<p>Terrain and borders will require some analogizing, since there are no real geographical or geopolitical features of this map. If you look on Google Maps, you'll notice that there are shades of terrain, but no smooth gradients. They are contours, quickly informing the viewer of the intensity of something compared to everywhere else, in their case vegetation. For our map, intensity could represent density of points, quickly signifying where there's a lot of interest, activity, content, engagement, popularity, discussion, etc. It would also provide some of the orientation, because a user can sense their movement based on the shifting terrain.</p>
<p><a href="https://scikit-learn.org/stable/modules/density.html">Kernel Density Estimation</a> would seem like the perfect tool for this job. Basically, you take a bunch of discrete values and generate a smooth curve around it, inferring the underlying distribution. Matthew Conlen <a href="https://mathisonian.github.io/kde/">created a cool visual walkthrough</a> explaining intuitively in more detail. Unfortunately, attempts using standard libraries like <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html">SciPy</a> took too long. However, as I was playing around with them, something occurred to me when I saw "Gaussian kernel" (one option for KDE): why not try Gaussian blurring? They seem to be related: when you blur an image, you have a <a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)">kernel</a> and apply a Gaussian function, which when you look at <a href="https://en.wikipedia.org/wiki/File:Normal_Distribution_PDF.svg">its plot</a>, it essentially "pushes out" values from the kernel's center (in layman's terms) — this is how you get the <em>smoothing</em> effect from blurs. Would applying such a blur also "push out" and smoothing the many discrete sharp points into a meaningful representation of the approximate density?</p>
<p>My approach will be to map each point to a cell in a large grid with the same aspect ratio, where each cell's value will be the count of points mapped to it. If the grid is too small, everything's clumped together, but if it's too big, everything's too far apart and it's mostly sparse everywhere, so finding a balance is necessary.</p>
<pre><code>ppc = <span>32</span> <span># Points per cell (approximate).</span>
x_min, x_max = (xs.<span>min</span>(), xs.<span>max</span>())
y_min, y_max = (ys.<span>min</span>(), ys.<span>max</span>())
grid_width = <span>int</span>((x_max - x_min) * ppc)
grid_height = <span>int</span>((y_max - y_min) * ppc)

gv = pd.DataFrame({
    <span>"x"</span>: (xs - x_min).clip(<span>0</span>, grid_width - <span>1</span>).astype(<span>"int32"</span>),
    <span>"y"</span>: (ys - y_min).clip(<span>0</span>, grid_height - <span>1</span>).astype(<span>"int32"</span>),
})
gv = gv.groupby([<span>"x"</span>, <span>"y"</span>]).size().reset_index(name=<span>"density"</span>)

grid = np.zeros((grid_height, grid_width), dtype=np.float32)
grid[gv[<span>"y"</span>], gv[<span>"x"</span>]] = gv[<span>"density"</span>]
grid = gaussian_filter(grid, sigma=<span>1</span>)

g_min, g_max = grid.<span>min</span>(), grid.<span>max</span>()
level_size = (g_max - g_min) / levels
grid = (grid - g_min) // level_size
<span># Some values may lie exactly on the max.</span>
grid = np.clip(grid, <span>0</span>, levels - <span>1</span>)
</code></pre><p>Let's quickly render an image, using the values as the alpha component, to see how it looks so far.</p>
<pre><code>alpha = grid.astype(np.float32) / (levels - <span>1</span>) * <span>255</span>

img = np.full(
    (grid_height, grid_width, <span>4</span>),
    (<span>144</span>, <span>224</span>, <span>190</span>, <span>0</span>), <span># Green-ish.</span>
    dtype=np.uint8,
)
img[:, :, <span>3</span>] = alpha.astype(np.uint8)

Image.fromarray(img, <span>"RGBA"</span>).save(<span>"terrain.webp"</span>)
</code></pre><p><img src="https://blog.wilsonl.in/hackerverse/terrain-linear.webp" alt="Initial terrain map."></p>
<p>It does not look so good. It looks like most cells were zero, and there are seemingly very few areas with any actual posts.</p>
<p>I have a theory: just as the distribution of votes/likes on social media across posts exhibit power-law distributions, perhaps the same is true of the popularity of topics? If there were a few topics that are posted about 10x more than most others, then it could explain the above map, as few areas would fall in the middle tiers. Let's apply a <code>log</code> to the values:</p>
<pre><code>gv[<span>"density"</span>] = np.log(gv[<span>"density"</span>] + <span>1</span>)
</code></pre><p><img src="https://blog.wilsonl.in/hackerverse/terrain-logarithmic.webp" alt="Logarithmic terrain map."></p>
<p>That looks much nicer. It also has implicit "borders" at the places where different <em>log(density)</em> levels meet, which is cool.</p>
<p>Instead of rendering it as a giant image, which is inefficient to transport and blurry when zoomed in, I'll create SVG paths instead, given that there are literally only 4 colors in this entire figure. On the client, I'll draw and fill in these paths that form a polygon. This will ensure that the "terrain" looks sharp (including at borders) even when zoomed in. I'll use <a href="https://opencv.org/">OpenCV</a>'s built-in <a href="https://en.wikipedia.org/wiki/Contour_line">contour</a> functions to calculate the path around these levels and export them as a closed polygon.</p>
<pre><code>shapes: <span>Dict</span>[<span>int</span>, <span>List</span>[npt.NDArray[np.float32]]] = {}
<span>for</span> level <span>in</span> <span>range</span>(levels):
    shapes[level] = []
    num_shapes, labelled_image = cv2.connectedComponents((grid == level).astype(np.uint8))
    <span># Ignore label 0 as it's the background.</span>
    <span>for</span> shape_no <span>in</span> <span>range</span>(<span>1</span>, num_shapes):
        shape_mask = labelled_image == shape_no
        <span># Use RETR_EXTERNAL as we only want the outer edges, and don't care about inner holes since they'll be represented by other larger-level shapes.</span>
        shape_contours, _ = cv2.findContours(shape_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        <span>for</span> shape_border_points <span>in</span> shape_contours:
            <span># The resulting shape is (N, 1, 2), where N is the number of points. Remove unnecessary second dimension.</span>
            shape_border_points = shape_border_points.squeeze(<span>1</span>)
            <span>if</span> shape_border_points.shape[<span>0</span>] &lt; <span>4</span>:
                <span># Not a polygon.</span>
                <span>continue</span>
            <span># We want level 0 only when it cuts out an inner hole in a larger level.</span>
            <span>if</span> level == <span>0</span> <span>and</span> (<span>0</span>, <span>0</span>) <span>in</span> shape_border_points:
                <span>continue</span>

            <span># Convert back to original scale.</span>
            shape_border_points[:, <span>0</span>] = shape_border_points[:, <span>0</span>] / ppc + x_min
            shape_border_points[:, <span>1</span>] = shape_border_points[:, <span>1</span>] / ppc + y_min
            shapes[level].append(shape_border_points.astype(np.float32))
</code></pre><h2 id="cities">Cities</h2>
<p>I also wanted to add some "cities" (representing the common topic within some radius), so you can find your way around and not get lost in the many points and titles shown at once, and have some sense of direction and where to start. The UMAP model has been saved, so all that's necessary is to embed the city names and get their (x, y) position using the UMAP model:</p>
<pre><code>CITIES = [<span>"Programming"</span>, <span>"Startups"</span>, <span>"Marketing"</span>, ...]
embs = model.encode(CITIES)
points = umapper.transform(embs)
</code></pre><p>There were some automated ways of doing this that I explored a bit. Using LLMs to generate these automatically. K-means clustering to figure out optimal points and radii. Unfortunately, it was not so trivial: it was hard to prompt the LLM to output what I expected, likely because describing the task was not trivial, and K-means did not find a lot of meaningful clusters that I would (as a human labeller) group together. You'd expect that there would be some hierarchy, but some topics are really popular on their own (sometimes even more than their logical "parent"), like "Programming" vs. "Rust", so they needed to be shown at the same detail level. Ultimately, it only required a few cities before it looked good, so manually walking the map and jotting down a few cities only took an hour or so.</p>
<h2 id="pushing-things-to-the-edge">Pushing things to the edge</h2>
<p>As you're browsing the map, you want it to feel snappy and responsive, because you're trying to find something, get immersed, get orientated, etc., and having parts be blank or partial interrupts this flow. Therefore, I knew I needed to reduce the time it took to get the map onto the screen. The rendering part was fast, it was fetching the data that took a while; I had started by putting all the map data on Cloudflare R2 in the ENAM region, but the latency was too high (600 ms to a few seconds) despite the physical latency being ~200 ms. However, even 200 ms was not really great, given that <a href="https://www.pubnub.com/blog/how-fast-is-realtime-human-perception-and-technology/">100 ms is the treshold where things feel "instant"</a>. Given that the limitation was a law of physics, there was only one real way to reduce that latency and that was to move the data closer to the user. So I spun up a few tiny servers in major regions: Virginia, San Jose, London, and Sydney (near me). I wrote a basic Rust server to ship out the data and get the most bang-for-buck from these tiny servers (plus, why spend all this effort only to have a slow server?), and had some tiny JS to pick the nearest server from the client:</p>
<pre><code><span>const</span> <span>EDGES</span> = [
  <span>"ap-sydney-1"</span>,
  <span>"uk-london-1"</span>,
  <span>"us-ashburn-1"</span>,
  <span>"us-sanjose-1"</span>,
] <span>as</span> <span>const</span>;

<span>const</span> edge = <span>await</span> <span>Promise</span>.<span>race</span>(
  <span>EDGES</span>.<span>map</span>(<span>async</span> (edge) =&gt; {
    <span>// Run a few times to avoid potential cold start biases.</span>
    <span>for</span> (<span>let</span> i = <span>0</span>; i &lt; <span>3</span>; i++) {
      <span>await</span> <span>fetch</span>(<span>`https://<span>${edge}</span>.edge-hndr.wilsonl.in/healthz`</span>);
    }
    <span>return</span> edge;
  }),
);
</code></pre><p>Some anycast, CDN, etc. solution may have been even cooler, but likely costly and overkill.</p>
<p>One thing that puzzled me was how much more memory was being used by the process compared to the actual data itself. The data is built once then pushed to all the edge servers, and it's in MessagePack format, so it has the bloat from type markers and property names. Once deserialized into Rust structures, it should all be memory offsets. So I was surprised when memory usage was 2-4x the source data size. I could only really think of four reasons:</p>
<ul>
<li>I used the wrong type (e.g. <code>f64</code> instead of <code>f32</code>).</li>
<li>Struct padding.</li>
<li>Vec, HashMap overallocation.</li>
<li>Memory allocator fragmentation or other inefficiency.</li>
</ul>
<p>I didn't look too much into this, but it was the one thing left that otherwise made the edge setup pretty neat and efficient. If anyone has any ideas, let me know.</p>
<h2 id="testing-out-the-search">Testing out the search</h2>
<p>Now that we got our app and data all up and running, let's see if our theory about better query comprehension and search results pans out. Let's try a simple query: "entering the tech industry".</p>
<p><img src="https://blog.wilsonl.in/hackerverse/search-entering-the-tech-industry.png" alt="Search results for &quot;entering the tech industry&quot;"></p>
<p>It gives some nice results, both upvoted and less noticed ones, and seem to be pretty relevant and helpful. Compare this to HN's current search service:</p>
<p><img src="https://blog.wilsonl.in/hackerverse/search-algolia-entering-the-tech-industry.png" alt="Search results for &quot;entering the tech industry&quot; by Algolia"></p>
<p>We can see the power of semantic embeddings over something like literal text matching. Of course, we could try more queries and get better results, but the embeddings-powered search engine got it immediately, did not return only part of the results, and there was no need to optimize the query or "reverse engineer" the algorithm.</p>
<p>How about a question, instead of just matching?</p>
<p><img src="https://blog.wilsonl.in/hackerverse/search-what-happened-to-wework.png" alt="Search results for &quot;what happened to wework&quot;"></p>
<p>It gave us results about WeWork over the years, from layoffs to stock tank to bankruptcy, a nice holistic view. Notice that the results don't literally contain the words "what happened", and most aren't even questions. The model seems to have "understood" our query, which is pretty nice considering this isn't some generative model, just cosine distances.</p>
<p>There is one quirk: the bottom result is completely irrelevant. This is largely my fault; I did not filter out results that are too dissimilar, so it ended up including generic results. This is something trivially fixable though.</p>
<p>Given the curated quality of HN posts (and scores), can we find some sage advice over the years about something important, like say career growth, just by typing something simple?</p>
<p><img src="https://blog.wilsonl.in/hackerverse/search-career-growth.png" alt="Search results for &quot;career growth&quot;"></p>
<p>Seems pretty good to me: not literally matching the words so creative, interesting, diverse essays showed up. The curation of HN really helps here; I did a quick comparison to the results from a well-known search engine and these results seemed far better. Obviously they have a much more difficult job and far more sophisticated pipelines, but it goes to show that good data + powerful semantic embeddings can go pretty far.</p>
<p>I've hard-coded some interesting queries I found into the app, which show up as suggestions; everything from "linus rants" to "self bootstrapping" to "cool things with css". Try them out and any other queries. What works well? What doesn't? Did you find anything interesting, rare, and/or useful? Let me know!</p>
<p>One thing I haven't mentioned yet is that the results are not <em>directly</em> the similarity matches. There are weights involved in calculating the final score (i.e. rank), and cosine similarity is a big one but not the only one.</p>
<p>Another important one is score. This may not seem as necessary given how powerful these embedding models are, but it is, even with 100% accurate models. Consider two posts talking about some topic, say Rust. One is written by <a href="https://news.ycombinator.com/user?id=steveklabnik">steveklabnik</a>. Another one says that Rust is terrible because it's a garbage-collected dynamic scripting language. To the model, these are both (mostly) talking about Rust, so they are very close together. Yet, it's obvious to most people that the latter should not be ranked as high as the former. (It should probably be filtered out entirely.) This highlights the importance of <em>social proof</em> in search and recommendation systems, because there are things that the model doesn't understand (and maybe never can) because of context, trends, events, etc. Incorporating the score ensures some social proof is taken into consideration.</p>
<p>Another weight is time. Some queries usually prefer newer content to older ones. The common example is news about some recent event; usually it's more important to show the latest updates than yesterday's or some distant but similar event. We can incorporate this by adding a negative weight component proportional to <em>log(age)</em>, so that non-fresh content quickly drops off.</p>
<h2 id="automatic-virtual-subcommunities">Automatic virtual subcommunities</h2>
<p>Another neat feature enabled by these embeddings is "virtual" subcommunities. Just type a community name (or description) and all the posts that meet some similarity threshold show up, like your very own subreddit on the fly. Hacker News doesn't have the ability to further subdivide posts, so this was a cool way to have a curated set of posts focused on a specific interest.</p>
<p><img src="https://blog.wilsonl.in/hackerverse/community.png" alt="Apple virtual subcommunity."></p>
<p>If you're wondering where those snippets and images came from, that's what the crawler was extracting and storing as metadata for each page. It usually comes in handy down the line when you want to show/list the pages as results somewhere, as otherwise you'd only have a URL. While I could've also saved the <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/link">site icon metadata</a>, it's tricky to parse, so I decided to keep it simple by just fetching <code>/favicon.ico</code> of the domain on the client side.</p>
<p>It's just as possible to show interesting comment threads and discussions. Unfortunately, scores aren't available to us, so we can only sort by timestamp, but luckily most comments on HN are pretty useful and insightful:</p>
<p><img src="https://blog.wilsonl.in/hackerverse/community-comments.png" alt="Entrepreneurship virtual community comments."></p>
<p>It would be an interesting challenge to try and rank the comments without the score, perhaps involving user comment histories, engagement around that comment, and the post, topic, and contents. This may be something as simple as a linear equation, to a deep learning model.</p>
<p>We can also see who are the most influential, active, passionate about something by calculating how many comments a user makes proportional to the similarity.</p>
<p><img src="https://blog.wilsonl.in/hackerverse/community-cloudflare-users.png" alt="Cloudflare virtual community top users."></p>
<p>We can see that this works well: <a href="https://news.ycombinator.com/user?id=jgrahamc">jgrahamc</a> and <a href="https://news.ycombinator.com/user?id=eastdakota">eastdakota</a> are the CTO and CEO of Cloudflare respectively. We managed to do this without needing to classify each comment or use a more fragile and inaccurate keyword-based search. All it takes is some matrix operations:</p>
<pre><code>q = model.embed(<span>"cloudflare"</span>)
df_comments[<span>"sim"</span>] = mat_comments @ q
min_threshold = <span>0.8</span>
df_comments_relevant = df_comments[df[<span>"sim"</span>] &gt;= min_threshold]
df_scores = df_comments_relevant.groupby(<span>"author"</span>).agg({<span>"sim"</span>: <span>"sum"</span>}).reset_index().sort_by(<span>"sim"</span>, ascending=<span>False</span>)[:<span>20</span>]
</code></pre><p>One important realisation was that pre-filtering is really slow and usually unnecessary compared to post-filtering. By pre-filtering, I mean removing rows before doing similarity matching. This is because you end up needing to also remove those corresponding rows from the embedding matrix, and that can mean having to reconstruct (read: gigabytes of memory copying) the entire matrix or use much slower partially-vectorized computations. It's usually better to just filter the rows after finding the most similar rows i.e. post-filter.</p>
<p>Note that a minimum threshold is important, because "dissimilarity" can be as high as 0.6, which makes the set of non-relevant items (in this case, users) have high scores due to the size of such sets. In this example, <em>most</em> comments aren't talking about Cloudflare, so any user that has a lot of comments would otherwise dominate this leaderboard just by sheer volume of comments as 100,000 * 0.6 is still higher than 500 * 0.999.</p>
<h2 id="analyzing-the-entire-dataset">Analyzing the entire dataset</h2>
<p>What can we do with the 30 million comments? Two things I wanted to try to analyze at scale were popularity and sentiment. Could I see how HN feels about something over time, and the impact that major events has on the sentiment? Can I track the growth and fall of various interests and topics, and how they compare against their competition?</p>
<p>I don't have sentiment data, but there are lots of high-quality open source sentiment classification models, on HuggingFace, using <a href="https://huggingface.co/docs/transformers/en/index">Transformers</a>. I decided to use the <a href="https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest">TweetEval</a> model as it was similarly trained on social media content. Tweets are short, and I didn't know how well it'd work on contextualized comments (which I added when generating embeddings), so to keep it aligned with the model I only used the comments themselves without adding any context. A queue was created, the comments were pushed, a GPU cluster spun up to process the jobs, and the results were stored.</p>
<p>The model was much smaller, so increasing the batch size was a good idea to get more bang-for-buck from the GPUs. Increasing the batch size uses more VRAM, but decreases the amount of host-GPU memory transfer (which can be the bottleneck given how fast GPUs are) and possibly increases parallelism. It's finicky though, because, at least for transformer-based models, it can sometimes cause memory spikes and OOM errors. This is because the input matrix has to be a "rectangle", so all inputs (which are converted into tokens) must be padded to the longest input length to maintain this constraint. If you have a batch where 4 texts are length 1, then the input size and internal state is relatively small. But if you instead have a batch size of 5 and the next text has length 1024, then all those sizes suddenly jump by thousands. I added some basic code to conservatively guess the optimal batch size given the VRAM, but I'm curious if this problem has already been tackled more dynamically, given its implications for efficiency.</p>
<p>Once the data was ready, it was time for some number crunching. Let's check out the sentiment of Rust over time:</p>
<p><img src="https://blog.wilsonl.in/hackerverse/sentiment-rust.png" alt="Sentiment of Rust."></p>
<p>Values below 0 represent the count of negative comments (where confidence of negative sentiment &gt; 0.5) and above 0 represent positive (where confidence of positive sentiment &gt; 0.5); I probably need to polish and clear it up a bit more. Nonetheless, we can see that there's generally a lot of positive sentiment about Rust (which isn't surprising if you've been around on HN). There was a spike in positivity around the 1.0 announcement, which makes sense, and the more negative posts correlated with a lot of negative comments (according to the model). This is similar to how bots measure sentiment on social media and predict the price of stocks; using powerful semantic embeddings would probably beat any keyword- or bag-of-words-based algorithm. I will say, assuming the model is accurate and I did a reasonable job, there seems to be a lot of negative sentiment on HN <em>in general</em>.</p>
<p>We can also estimate the popularity of Rust compared to other languages by weighing the score and similarity. Unfortunately, HN does not expose comment scores, so we can't use them.</p>
<p><img src="https://blog.wilsonl.in/hackerverse/popularity-languages.png" alt="Popularity of Go, JavaScript, Python, and Rust."></p>
<p>It seems like Rust is doing great, but not as popular as the other languages. Some of the similarity thresholds may need tuning, so I may be wrong here; have a play with it yourself and try various queries and thresholds. Share anything interesting you find!</p>
<p>These were very basic demos and analyses of the data available, and I'm sure there are infinitely more ways to slice and dice the data in interesting, insightful, useful, sophisticated ways. I have many more ideas myself, but wanted to open up the code and data sooner so you can build on top of this, either with more ideas and suggestions, or to play with your own research and visualization projects.</p>
<h2 id="big-data-number-crunching-with-a-gpu">Big data number crunching with a GPU</h2>
<p>One last thing before I wrap this long post up. The analysis queries were taking a while (10-30 seconds) to number-crunch for each one, which was annoying when playing around with it. This was on a 32-core machine, so it was not for a lack of horsepower. I was thinking of various ways to index, preprocess, or prepare the data, when it occurred to me that there already exists a powerful device for lots of vectorized number-crunching, and it's why we run all our AI models on it, but it doesn't have to be restricted to those. Fortunately, libraries like <a href="https://cupy.dev/">CuPy</a> and <a href="https://docs.rapids.ai/api/cudf/stable/">cuDF</a> exist, which basically have the same API as NumPy and pandas (respectively) but run everything on the GPU, so it was pretty trivial to port over. Now, queries run in hundreds of milliseconds, and life is great. It's so fast I didn't even bother using a built ANN graph.</p>
<p>The only tricky thing was loading the data on the GPU. Given how large the matrix of embeddings was (30M x 512), it was critical to manage memory effectively, because it wasn't actually possible to fit anything more than 1x the matrix in either system or video memory. Some key points:</p>
<ul>
<li>Loading in batches can cause a lot of allocations, which can fragment memory, so in reality you may not be able to load in chunks and then concatenate at the end. (Concatenation also requires contiguous memory, which usually means copying into a separate memory location.)</li>
<li>If you read the bytes from disk, load into a NumPy array, convert into a CuPy array, and then copy over to the GPU, that's 4 copies, 3 of which are in memory.</li>
<li>CuPy seems to need to have the entire matrix in system memory first before it can copy over to the GPU. For example, <code>cupy.asarray(np_matrix)</code> actually creates a copy of <code>np_matrix</code> in system memory first.</li>
</ul>
<p>Ultimately I ended up memory-mapping the matrix on disk, preallocating an uninitialized matrix on the GPU of the same size, then copying over in chunks. This had the benefit of avoiding reading from disk into Python memory first, and using exactly 1x the system RAM and VRAM.</p>
<h2 id="demo">Demo</h2>
<p>You can find the app at <a href="https://hn.wilsonl.in/">hn.wilsonl.in</a>. The main page is the map and search, but you can find the other tools (communities and analysis) by clicking the button in the top right. If you find an interesting community, analysis, etc., feel free to share the URL with others; the query is always stored in the URL.</p>
<p>Note that the demo dataset is cut off at around 10 April 2024, so it contains recent but not live posts and comments.</p>
<h2 id="what's-next">What's next</h2>
<p>There is much more I wanted to explore, learn, build, but I did not get the time. Some ideas I'm thinking of going into:</p>
<ul>
<li>Live data that is continuously kept up to date.</li>
<li>Deep learning powered recommendations system, a <a href="https://en.wikipedia.org/wiki/StumbleUpon">StumbleUpon</a> over the curated HN web.</li>
<li>Improving search results by training a reranker.</li>
<li>Interesting "paths" and "journeys" along the map.</li>
<li>Analyzing users more: who are the most similar/opposite to each other? Who is the most expert in various niches?</li>
<li>…</li>
</ul>
<p>However, I'm more interested in hearing from the community. What do you want to see? How useful were these tools? What were shortcomings or things I overlooked? What other cool ideas do you have? Share any thoughts, feedback, interesting findings, complaints—there's likely a lot more potential with these data and tools, and I'm hoping that, by opening it up, there's some interested people who will push this further than I can alone.</p>
<p>If there's any interest in diving deeper or clarifying any aspect of this project, let me know, I'd be happy to. Once again, you can find all the <a href="https://github.com/wilsonzlin/hackerverse/releases/tag/dataset-39996091">data</a> and <a href="https://github.com/wilsonzlin/hackerverse">code</a> on GitHub.</p>
<p>If you managed to make it all the way here, thanks for reading!</p>

    </article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: An SQS Alternative on Postgres (228 pts)]]></title>
            <link>https://github.com/tembo-io/pgmq</link>
            <guid>40307454</guid>
            <pubDate>Thu, 09 May 2024 12:21:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tembo-io/pgmq">https://github.com/tembo-io/pgmq</a>, See on <a href="https://news.ycombinator.com/item?id=40307454">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Postgres Message Queue (PGMQ)</h2><a id="user-content-postgres-message-queue-pgmq" aria-label="Permalink: Postgres Message Queue (PGMQ)" href="#postgres-message-queue-pgmq"></a></p>
<p dir="auto">A lightweight message queue. Like <a href="https://aws.amazon.com/sqs/" rel="nofollow">AWS SQS</a> and <a href="https://github.com/smrchy/rsmq">RSMQ</a> but on Postgres.</p>
<p dir="auto">Try it for free at <a href="https://tembo.io/" rel="nofollow">tembo.io</a></p>
<p dir="auto"><a href="https://join.slack.com/t/tembocommunity/shared_invite/zt-293gc1k0k-3K8z~eKW1SEIfrqEI~5_yw" rel="nofollow"><img src="https://camo.githubusercontent.com/503899b9c81b00cb3eb72ac40b042daad8fcd5aec105d593c763646fdb04bc5b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25343074656d626f2d636f6d6d756e6974793f6c6f676f3d736c61636b266c6162656c3d736c61636b" alt="Static Badge" data-canonical-src="https://img.shields.io/badge/%40tembo-community?logo=slack&amp;label=slack"></a>
<a href="https://ossrank.com/p/3809" rel="nofollow"><img src="https://camo.githubusercontent.com/ada21af437440bd8ccb611d66b4671e5f16e289b37ce4de49128bf9abf64ddf3/68747470733a2f2f736869656c64732e696f2f656e64706f696e743f75726c3d68747470733a2f2f6f737372616e6b2e636f6d2f736869656c642f33383039" alt="OSSRank" data-canonical-src="https://shields.io/endpoint?url=https://ossrank.com/shield/3809"></a>
<a href="https://pgxn.org/dist/pgmq/" rel="nofollow"><img src="https://camo.githubusercontent.com/10f6f5d6df8fa4673d210d4336688f8247718d022fcc6f9bb846639e36c26119/68747470733a2f2f62616467652e667572792e696f2f70672f70676d712e737667" alt="PGXN version" data-canonical-src="https://badge.fury.io/pg/pgmq.svg"></a></p>
<p dir="auto"><strong>Documentation</strong>: <a href="https://tembo-io.github.io/pgmq/" rel="nofollow">https://tembo-io.github.io/pgmq/</a></p>
<p dir="auto"><strong>Source</strong>: <a href="https://github.com/tembo-io/pgmq">https://github.com/tembo-io/pgmq</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Lightweight - No background worker or external dependencies, just Postgres functions packaged in an extension</li>
<li>Guaranteed "exactly once" delivery of messages to a consumer within a visibility timeout</li>
<li>API parity with <a href="https://aws.amazon.com/sqs/" rel="nofollow">AWS SQS</a> and <a href="https://github.com/smrchy/rsmq">RSMQ</a></li>
<li>Messages stay in the queue until explicitly removed</li>
<li>Messages can be archived, instead of deleted, for long-term retention and replayability</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support</h2><a id="user-content-support" aria-label="Permalink: Support" href="#support"></a></p>
<p dir="auto">Postgres 12-16.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#postgres-message-queue-pgmq">Postgres Message Queue (PGMQ)</a>
<ul dir="auto">
<li><a href="#features">Features</a></li>
<li><a href="#support">Support</a></li>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#client-libraries">Client Libraries</a></li>
<li><a href="#sql-examples">SQL Examples</a>
<ul dir="auto">
<li><a href="#creating-a-queue">Creating a queue</a></li>
<li><a href="#send-two-messages">Send two messages</a></li>
<li><a href="#read-messages">Read messages</a></li>
<li><a href="#pop-a-message">Pop a message</a></li>
<li><a href="#archive-a-message">Archive a message</a></li>
<li><a href="#delete-a-message">Delete a message</a></li>
<li><a href="#drop-a-queue">Drop a queue</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#configuration">Configuration</a>
<ul dir="auto">
<li><a href="#partitioned-queues">Partitioned Queues</a></li>
<li><a href="#visibility-timeout-vt">Visibility Timeout (vt)</a></li>
<li><a href="#-contributors">✨ Contributors</a></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">The fastest way to get started is by running the Tembo docker image, where PGMQ comes pre-installed.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run -d --name postgres -e POSTGRES_PASSWORD=postgres -p 5432:5432 quay.io/tembo/pgmq-pg:latest"><pre>docker run -d --name postgres -e POSTGRES_PASSWORD=postgres -p 5432:5432 quay.io/tembo/pgmq-pg:latest</pre></div>
<p dir="auto">If you'd like to build from source, you can follow the instructions in <a href="https://github.com/tembo-io/pgmq/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Updating</h3><a id="user-content-updating" aria-label="Permalink: Updating" href="#updating"></a></p>
<p dir="auto">To update PGMQ versions, follow the instructions in <a href="https://github.com/tembo-io/pgmq/blob/main/UPDATING.md">UPDATING.md</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Client Libraries</h2><a id="user-content-client-libraries" aria-label="Permalink: Client Libraries" href="#client-libraries"></a></p>
<ul dir="auto">
<li><a href="https://github.com/tembo-io/pgmq/tree/main/pgmq-rs">Rust</a></li>
<li><a href="https://github.com/tembo-io/pgmq/tree/main/tembo-pgmq-python">Python</a></li>
</ul>
<p dir="auto">Community</p>
<ul dir="auto">
<li><a href="https://github.com/craigpastro/pgmq-go">Go</a></li>
<li><a href="https://github.com/v0idpwn/pgmq-elixir">Elixir</a></li>
<li><a href="https://github.com/v0idpwn/off_broadway_pgmq">Elixir + Broadway</a></li>
<li><a href="https://github.com/adamalexandru4/pgmq-spring">Java (Spring Boot)</a></li>
<li><a href="https://github.com/Muhammad-Magdi/pgmq-js">Javascript (NodeJs)</a></li>
<li><a href="https://github.com/brianpursley/Npgmq">.NET</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">SQL Examples</h2><a id="user-content-sql-examples" aria-label="Permalink: SQL Examples" href="#sql-examples"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Connect to Postgres
psql postgres://postgres:postgres@0.0.0.0:5432/postgres"><pre><span><span>#</span> Connect to Postgres</span>
psql postgres://postgres:postgres@0.0.0.0:5432/postgres</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="-- create the extension in the &quot;pgmq&quot; schema
CREATE EXTENSION pgmq;"><pre><span><span>--</span> create the extension in the "pgmq" schema</span>
CREATE EXTENSION pgmq;</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Creating a queue</h3><a id="user-content-creating-a-queue" aria-label="Permalink: Creating a queue" href="#creating-a-queue"></a></p>
<p dir="auto">Every queue is its own table in the <code>pgmq</code> schema. The table name is the queue name prefixed with <code>q_</code>.
For example, <code>pgmq.q_my_queue</code> is the table for the queue <code>my_queue</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="-- creates the queue
SELECT pgmq.create('my_queue');"><pre><span><span>--</span> creates the queue</span>
<span>SELECT</span> <span>pgmq</span>.<span>create</span>(<span><span>'</span>my_queue<span>'</span></span>);</pre></div>
<div data-snippet-clipboard-copy-content=" create
-------------

(1 row)"><pre lang="text"><code> create
-------------

(1 row)
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Send two messages</h3><a id="user-content-send-two-messages" aria-label="Permalink: Send two messages" href="#send-two-messages"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="-- messages are sent as JSON
SELECT * from pgmq.send('my_queue', '{&quot;foo&quot;: &quot;bar1&quot;}');
SELECT * from pgmq.send('my_queue', '{&quot;foo&quot;: &quot;bar2&quot;}');"><pre><span><span>--</span> messages are sent as JSON</span>
<span>SELECT</span> <span>*</span> <span>from</span> <span>pgmq</span>.<span>send</span>(<span><span>'</span>my_queue<span>'</span></span>, <span><span>'</span>{"foo": "bar1"}<span>'</span></span>);
<span>SELECT</span> <span>*</span> <span>from</span> <span>pgmq</span>.<span>send</span>(<span><span>'</span>my_queue<span>'</span></span>, <span><span>'</span>{"foo": "bar2"}<span>'</span></span>);</pre></div>
<p dir="auto">The message id is returned from the send function.</p>
<div data-snippet-clipboard-copy-content=" send
-----------
         1
(1 row)

 send
-----------
         2
(1 row)"><pre lang="text"><code> send
-----------
         1
(1 row)

 send
-----------
         2
(1 row)
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Read messages</h3><a id="user-content-read-messages" aria-label="Permalink: Read messages" href="#read-messages"></a></p>
<p dir="auto">Read <code>2</code> message from the queue. Make them invisible for <code>30</code> seconds.
If the messages are not deleted or archived within 30 seconds, they will become visible again
and can be read by another consumer.</p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT * FROM pgmq.read('my_queue', 30, 2);"><pre><span>SELECT</span> <span>*</span> <span>FROM</span> <span>pgmq</span>.<span>read</span>(<span><span>'</span>my_queue<span>'</span></span>, <span>30</span>, <span>2</span>);</pre></div>
<div data-snippet-clipboard-copy-content=" msg_id | read_ct |          enqueued_at          |              vt               |     message
--------+---------+-------------------------------+-------------------------------+-----------------
      1 |       1 | 2023-08-16 08:37:54.567283-05 | 2023-08-16 08:38:29.989841-05 | {&quot;foo&quot;: &quot;bar1&quot;}
      2 |       1 | 2023-08-16 08:37:54.572933-05 | 2023-08-16 08:38:29.989841-05 | {&quot;foo&quot;: &quot;bar2&quot;}"><pre lang="text"><code> msg_id | read_ct |          enqueued_at          |              vt               |     message
--------+---------+-------------------------------+-------------------------------+-----------------
      1 |       1 | 2023-08-16 08:37:54.567283-05 | 2023-08-16 08:38:29.989841-05 | {"foo": "bar1"}
      2 |       1 | 2023-08-16 08:37:54.572933-05 | 2023-08-16 08:38:29.989841-05 | {"foo": "bar2"}
</code></pre></div>
<p dir="auto">If the queue is empty, or if all messages are currently invisible, no rows will be returned.</p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT pgmq.read('my_queue', 30, 1);"><pre><span>SELECT</span> <span>pgmq</span>.<span>read</span>(<span><span>'</span>my_queue<span>'</span></span>, <span>30</span>, <span>1</span>);</pre></div>
<div data-snippet-clipboard-copy-content=" msg_id | read_ct | enqueued_at | vt | message
--------+---------+-------------+----+---------"><pre lang="text"><code> msg_id | read_ct | enqueued_at | vt | message
--------+---------+-------------+----+---------
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pop a message</h3><a id="user-content-pop-a-message" aria-label="Permalink: Pop a message" href="#pop-a-message"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="-- Read a message and immediately delete it from the queue. Returns `None` if the queue is empty.
SELECT pgmq.pop('my_queue');"><pre><span><span>--</span> Read a message and immediately delete it from the queue. Returns `None` if the queue is empty.</span>
<span>SELECT</span> <span>pgmq</span>.<span>pop</span>(<span><span>'</span>my_queue<span>'</span></span>);</pre></div>
<div data-snippet-clipboard-copy-content=" msg_id | read_ct |          enqueued_at          |              vt               |     message
--------+---------+-------------------------------+-------------------------------+-----------------
      1 |       1 | 2023-08-16 08:37:54.567283-05 | 2023-08-16 08:38:29.989841-05 | {&quot;foo&quot;: &quot;bar1&quot;}"><pre lang="text"><code> msg_id | read_ct |          enqueued_at          |              vt               |     message
--------+---------+-------------------------------+-------------------------------+-----------------
      1 |       1 | 2023-08-16 08:37:54.567283-05 | 2023-08-16 08:38:29.989841-05 | {"foo": "bar1"}
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Archive a message</h3><a id="user-content-archive-a-message" aria-label="Permalink: Archive a message" href="#archive-a-message"></a></p>
<p dir="auto">Archiving a message removes it from the queue and inserts it to the archive table.</p>
<div dir="auto" data-snippet-clipboard-copy-content="-- Archive message with msg_id=2.
SELECT pgmq.archive('my_queue', 2);"><pre><span><span>--</span> Archive message with msg_id=2.</span>
<span>SELECT</span> <span>pgmq</span>.<span>archive</span>(<span><span>'</span>my_queue<span>'</span></span>, <span>2</span>);</pre></div>
<div data-snippet-clipboard-copy-content=" archive
--------------
 t
(1 row)"><pre lang="text"><code> archive
--------------
 t
(1 row)
</code></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="-- Archive tables have the prefix `a_`:
SELECT * FROM pgmq.a_my_queue;"><pre><span><span>--</span> Archive tables have the prefix `a_`:</span>
<span>SELECT</span> <span>*</span> <span>FROM</span> <span>pgmq</span>.<span>a_my_queue</span>;</pre></div>
<div data-snippet-clipboard-copy-content=" msg_id | read_ct |         enqueued_at          |          archived_at          |              vt               |     message
--------+---------+------------------------------+-------------------------------+-------------------------------+-----------------
      2 |       1 | 2023-04-25 00:55:40.68417-05 | 2023-04-25 00:56:35.937594-05 | 2023-04-25 00:56:20.532012-05 | {&quot;foo&quot;: &quot;bar2&quot;}"><pre lang="text"><code> msg_id | read_ct |         enqueued_at          |          archived_at          |              vt               |     message
--------+---------+------------------------------+-------------------------------+-------------------------------+-----------------
      2 |       1 | 2023-04-25 00:55:40.68417-05 | 2023-04-25 00:56:35.937594-05 | 2023-04-25 00:56:20.532012-05 | {"foo": "bar2"}
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Delete a message</h3><a id="user-content-delete-a-message" aria-label="Permalink: Delete a message" href="#delete-a-message"></a></p>
<p dir="auto">Send another message, so that we can delete it.</p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT pgmq.send('my_queue', '{&quot;foo&quot;: &quot;bar3&quot;}');"><pre><span>SELECT</span> <span>pgmq</span>.<span>send</span>(<span><span>'</span>my_queue<span>'</span></span>, <span><span>'</span>{"foo": "bar3"}<span>'</span></span>);</pre></div>
<div data-snippet-clipboard-copy-content=" send
-----------
        3
(1 row)"><pre lang="text"><code> send
-----------
        3
(1 row)
</code></pre></div>
<p dir="auto">Delete the message with id <code>3</code> from the queue named <code>my_queue</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT pgmq.delete('my_queue', 3);"><pre><span>SELECT</span> <span>pgmq</span>.<span>delete</span>(<span><span>'</span>my_queue<span>'</span></span>, <span>3</span>);</pre></div>
<div data-snippet-clipboard-copy-content=" delete
-------------
 t
(1 row)"><pre lang="text"><code> delete
-------------
 t
(1 row)
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Drop a queue</h3><a id="user-content-drop-a-queue" aria-label="Permalink: Drop a queue" href="#drop-a-queue"></a></p>
<p dir="auto">Delete the queue <code>my_queue</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="SELECT pgmq.drop_queue('my_queue');"><pre><span>SELECT</span> <span>pgmq</span>.<span>drop_queue</span>(<span><span>'</span>my_queue<span>'</span></span>);</pre></div>
<div data-snippet-clipboard-copy-content=" drop_queue
-----------------
 t
(1 row)"><pre lang="text"><code> drop_queue
-----------------
 t
(1 row)
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Partitioned Queues</h2><a id="user-content-partitioned-queues" aria-label="Permalink: Partitioned Queues" href="#partitioned-queues"></a></p>
<p dir="auto">You will need to install <a href="https://github.com/pgpartman/pg_partman/">pg_partman</a> if you want to use <code>pgmq</code> partitioned queues.</p>
<p dir="auto"><code>pgmq</code> queue tables can be created as a partitioned table by using <code>pgmq.create_partitioned()</code>. <a href="https://github.com/pgpartman/pg_partman/">pg_partman</a>
handles all maintenance of queue tables. This includes creating new partitions and dropping old partitions.</p>
<p dir="auto">Partitions behavior is configured at the time queues are created, via <code>pgmq.create_partitioned()</code>. This function has three parameters:</p>
<p dir="auto"><code>queue_name: text</code>: The name of the queue. Queues are Postgres tables prepended with <code>q_</code>. For example, <code>q_my_queue</code>. The archive is instead prefixed by <code>a_</code>, for example <code>a_my_queue</code>.</p>
<p dir="auto"><code>partition_interval: text</code> - The interval at which partitions are created. This can be either any valid Postgres <code>Duration</code> supported by pg_partman, or an integer value. When it is a duration, queues are partitioned by the time at which messages are sent to the table (<code>enqueued_at</code>). A value of <code>'daily'</code> would create a new partition each day. When it is an integer value, queues are partitioned by the <code>msg_id</code>. A value of <code>'100'</code> will create a new partition every 100 messages. The value must agree with <code>retention_interval</code> (time based or numeric). The default value is <code>daily</code>.</p>
<p dir="auto"><code>retention_interval: text</code> - The interval for retaining partitions. This can be either any valid Postgres <code>Duration</code> supported by pg_partman, or an integer value. When it is a duration, partitions containing data greater than the duration will be dropped. When it is an integer value, any messages that have a <code>msg_id</code> less than <code>max(msg_id) - retention_interval</code> will be dropped. For example, if the max <code>msg_id</code> is 100 and the <code>retention_interval</code> is 60, any partitions with <code>msg_id</code> values less than 40 will be dropped. The value must agree with <code>partition_interval</code> (time based or numeric). The default is <code>'5 days'</code>. Note: <code>retention_interval</code> does not apply to messages that have been deleted via <code>pgmq.delete()</code> or archived with <code>pgmq.archive()</code>. <code>pgmq.delete()</code> removes messages forever and <code>pgmq.archive()</code> moves messages to the corresponding archive table forever (for example, <code>a_my_queue</code>).</p>
<p dir="auto">In order for automatic partition maintenance to take place, several settings must be added to the <code>postgresql.conf</code> file, which is typically located in the postgres <code>DATADIR</code>.
<code>pg_partman_bgw.interval</code>
in <code>postgresql.conf</code>. Below are the default configuration values set in Tembo docker images.</p>
<p dir="auto">Add the following to <code>postgresql.conf</code>. Note, changing <code>shared_preload_libraries</code> requires a restart of Postgres.</p>
<p dir="auto"><code>pg_partman_bgw.interval</code> sets the interval at which <code>pg_partman</code> conducts maintenance. This creates new partitions and dropping of partitions falling out of the <code>retention_interval</code>. By default, <code>pg_partman</code> will keep 4 partitions "ahead" of the currently active partition.</p>
<div data-snippet-clipboard-copy-content="shared_preload_libraries = 'pg_partman_bgw' # requires restart of Postgres
pg_partman_bgw.interval = 60
pg_partman_bgw.role = 'postgres'
pg_partman_bgw.dbname = 'postgres'"><pre><code>shared_preload_libraries = 'pg_partman_bgw' # requires restart of Postgres
pg_partman_bgw.interval = 60
pg_partman_bgw.role = 'postgres'
pg_partman_bgw.dbname = 'postgres'
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Visibility Timeout (vt)</h2><a id="user-content-visibility-timeout-vt" aria-label="Permalink: Visibility Timeout (vt)" href="#visibility-timeout-vt"></a></p>
<p dir="auto">pgmq guarantees exactly once delivery of a message within a visibility timeout. The visibility timeout is the amount of time a message is invisible to other consumers after it has been read by a consumer. If the message is NOT deleted or archived within the visibility timeout, it will become visible again and can be read by another consumer. The visibility timeout is set when a message is read from the queue, via <code>pgmq.read()</code>. It is recommended to set a <code>vt</code> value that is greater than the expected time it takes to process a message. After the application successfully processes the message, it should call <code>pgmq.delete()</code> to completely remove the message from the queue or <code>pgmq.archive()</code> to move it to the archive table for the queue.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">✨ Contributors</h2><a id="user-content--contributors" aria-label="Permalink: ✨ Contributors" href="#-contributors"></a></p>
<p dir="auto">Thanks goes to these incredible people:</p>
<a href="https://github.com/tembo-io/pgmq/graphs/contributors">
  <img src="https://camo.githubusercontent.com/edb80ae9a4639740585e6fd4f6856ed67a19b5708989b6eff212cb9b32511e5b/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d74656d626f2d696f2f70676d71" data-canonical-src="https://contrib.rocks/image?repo=tembo-io/pgmq">
</a>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>