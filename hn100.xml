<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 23 Aug 2024 04:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Python's Preprocessor – Pydong (218 pts)]]></title>
            <link>https://pydong.org/posts/PythonsPreprocessor/</link>
            <guid>41322758</guid>
            <pubDate>Thu, 22 Aug 2024 17:54:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pydong.org/posts/PythonsPreprocessor/">https://pydong.org/posts/PythonsPreprocessor/</a>, See on <a href="https://news.ycombinator.com/item?id=41322758">Hacker News</a></p>
Couldn't get https://pydong.org/posts/PythonsPreprocessor/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: InstantDB – A Modern Firebase (544 pts)]]></title>
            <link>https://github.com/instantdb/instant</link>
            <guid>41322281</guid>
            <pubDate>Thu, 22 Aug 2024 17:08:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/instantdb/instant">https://github.com/instantdb/instant</a>, See on <a href="https://news.ycombinator.com/item?id=41322281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a href="https://instantdb.com/" rel="nofollow">
    <themed-picture data-catalyst-inline="true"><picture>
      <source media="(prefers-color-scheme: dark)" srcset="https://camo.githubusercontent.com/2af63e8197df473d79408acf76aa06b72d1bd76f0a851f8983a8b487ee7faf6b/68747470733a2f2f696e7374616e7464622e636f6d2f726561646d65732f6c6f676f5f776974685f746578745f6461726b5f6d6f64652e737667" data-canonical-src="https://instantdb.com/readmes/logo_with_text_dark_mode.svg">
      <img alt="Shows the Instant logo" src="https://camo.githubusercontent.com/fcbab3cc92cfb8b401f1b4ecbc4d31ac2e0323c368b1425689b3e294f5f24ad6/68747470733a2f2f696e7374616e7464622e636f6d2f726561646d65732f6c6f676f5f776974685f746578745f6c696768745f6d6f64652e737667" data-canonical-src="https://instantdb.com/readmes/logo_with_text_light_mode.svg">
    </picture></themed-picture>
  </a>
</p>
<p dir="auto">
  <a href="https://discord.com/invite/VU53p7uQcE" rel="nofollow">
    <img height="20" src="https://camo.githubusercontent.com/8fc18af31e1f1939c19c29462d1e29a0a9bbacb77c7a3ec147be30efde001827/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f31303331393537343833323433313838323335" data-canonical-src="https://img.shields.io/discord/1031957483243188235">
  </a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/89f509177e307d8b573fe98e36deae42e2498fa84230420f8f26ab09359f0a2c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f696e7374616e7464622f696e7374616e74"><img src="https://camo.githubusercontent.com/89f509177e307d8b573fe98e36deae42e2498fa84230420f8f26ab09359f0a2c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f696e7374616e7464622f696e7374616e74" alt="stars" data-canonical-src="https://img.shields.io/github/stars/instantdb/instant"></a>
</p>
<p dir="auto">
   <a href="https://instantdb.com/docs" rel="nofollow">Get Started</a> · 
   <a href="https://instantdb.com/examples" rel="nofollow">Examples</a> · 
   <a href="https://instantdb.com/tutorial" rel="nofollow">Try the Demo</a> · 
   <a href="https://instantdb.com/docs" rel="nofollow">Docs</a> · 
   <a href="https://discord.com/invite/VU53p7uQcE" rel="nofollow">Discord</a>
</p><p dir="auto">Instant is a client-side database that makes it easy to build real-time and collaborative apps like Notion or Figma.</p>
<p dir="auto">You write <a href="https://www.instantdb.com/docs/instaql" rel="nofollow">relational queries</a> in the shape of the data you want and Instant handles all the data fetching, permission checking, and offline caching. When you <a href="https://www.instantdb.com/docs/instaml" rel="nofollow">change data</a>, optimistic updates and rollbacks are handled for you as well. Plus, every query is multiplayer by default.</p>
<p dir="auto">We also support <a href="https://www.instantdb.com/docs/presence-and-topics" rel="nofollow">ephemeral</a> updates, like cursors, or who's online. Currently we have SDKs for <a href="https://www.instantdb.com/docs/start-vanilla" rel="nofollow">Javascript</a>, <a href="https://www.instantdb.com/docs/" rel="nofollow">React</a>, and <a href="https://www.instantdb.com/docs/start-rn" rel="nofollow">React Native</a>.</p>
<p dir="auto">How does it look? Here's a barebones chat app in about 10 lines:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// ༼ つ ◕_◕ ༽つ Real-time Chat
// ----------------------------------
// * Updates instantly
// * Multiplayer
// * Works offline
function Chat() {
  // 1. Read
  const { isLoading, error, data } = useQuery({
    messages: {},
  });

  // 2. Write
  const addMessage = (message) => {
    transact(tx.messages[id()].update(message));
  }

  // 3. Render!
  return <UI data={data} onAdd={addMessage} />
}"><pre><span>// ༼ つ ◕_◕ ༽つ Real-time Chat</span>
<span>// ----------------------------------</span>
<span>// * Updates instantly</span>
<span>// * Multiplayer</span>
<span>// * Works offline</span>
<span>function</span> <span>Chat</span><span>(</span><span>)</span> <span>{</span>
  <span>// 1. Read</span>
  <span>const</span> <span>{</span> isLoading<span>,</span> error<span>,</span> data <span>}</span> <span>=</span> <span>useQuery</span><span>(</span><span>{</span>
    <span>messages</span>: <span>{</span><span>}</span><span>,</span>
  <span>}</span><span>)</span><span>;</span>

  <span>// 2. Write</span>
  <span>const</span> <span>addMessage</span> <span>=</span> <span>(</span><span>message</span><span>)</span> <span>=&gt;</span> <span>{</span>
    <span>transact</span><span>(</span><span>tx</span><span>.</span><span>messages</span><span>[</span><span>id</span><span>(</span><span>)</span><span>]</span><span>.</span><span>update</span><span>(</span><span>message</span><span>)</span><span>)</span><span>;</span>
  <span>}</span>

  <span>// 3. Render!</span>
  <span>return</span> <span>&lt;</span><span>UI</span> <span>data</span><span>=</span><span>{</span><span>data</span><span>}</span> <span>onAdd</span><span>=</span><span>{</span><span>addMessage</span><span>}</span> <span>/</span><span>&gt;</span>
<span>}</span></pre></div>
<p dir="auto">Want to see for yourself? <a href="https://instantdb.com/tutorial" rel="nofollow">try a demo in your browser.</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Motivation</h2><a id="user-content-motivation" aria-label="Permalink: Motivation" href="#motivation"></a></p>
<p dir="auto">Writing modern apps are full of schleps. Most of the time you start with the server: stand up databases, caches, ORMs, and endpoints. Then you write client-side code: stores, selectors, mutators. Finally you paint a screen. If you add multiplayer you need to think about stateful servers, and if you support offline mode, you need to think about IndexedDB and transaction queues.</p>
<p dir="auto">To make things worse, whenever you add a new feature, you go through the same song and dance over and over again: add models, write endpoints, stores, selectors, and finally the UI.</p>
<p dir="auto">Could it be better?</p>
<p dir="auto">In 2021, <strong>we realized that most of the schleps we face as UI engineers are actually database problems problems in disguise.</strong> (We got into greater detail <a href="https://instantdb.com/essays/next_firebase" rel="nofollow">in this essay</a>)</p>
<p dir="auto">
  <a href="#">
    <img alt="Shows how Instant compresses schleps" src="https://camo.githubusercontent.com/5767f96f66d7e1d87f07bfeab64f3590a30e11792bdeca79001247f8c70e6412/68747470733a2f2f696e7374616e7464622e636f6d2f726561646d65732f636f6d7072657373696f6e2e737667" data-canonical-src="https://instantdb.com/readmes/compression.svg">
  </a>
</p>
<p dir="auto">If you had a database on the client, you wouldn't need to think about stores, selectors, endpoints, or local caches: just write queries. If these queries were multiplayer by default, you wouldn't have to worry about stateful servers. And if your database supported rollback, you'd get optimistic updates for free.</p>
<p dir="auto">So we built Instant. Instant gives you a database you can use in the client, so you can focus on what’s important: building a great UX for your users, and doing it quickly.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architectural Overview</h2><a id="user-content-architectural-overview" aria-label="Permalink: Architectural Overview" href="#architectural-overview"></a></p>
<p dir="auto">Here's how Instant works at a high level:</p>
<p dir="auto">
  <a href="#">
    <img alt="Shows how Instant compresses schleps" src="https://camo.githubusercontent.com/9f9534e626714562bc04bbfc0d2d2a2295a9375ec44c5467b9ff7474395aba86/68747470733a2f2f696e7374616e7464622e636f6d2f726561646d65732f6172636869746563747572652e737667" data-canonical-src="https://instantdb.com/readmes/architecture.svg">
  </a>
</p>
<p dir="auto">Under the hood, we store all user data as triples in one big Postgres database. A multi-tenant setup lets us offer a free tier that never pauses.</p>
<p dir="auto">A sync server written in Clojure talks to Postgres. We wrote a query engine that understands datalog and <a href="https://www.instantdb.com/docs/instaql" rel="nofollow">InstaQL</a>, a relational language that looks a lot like GraphQL:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// give me all users, their posts and comments
{ users: { posts: { comments: {} } } }"><pre><span>// give me all users, their posts and comments</span>
<span>{</span> <span>users</span>: <span>{</span> <span>posts</span>: <span>{</span> <span>comments</span>: <span>{</span><span>}</span> <span>}</span> <span>}</span> <span>}</span></pre></div>
<p dir="auto">Taking inspiration from <a href="https://asana.com/inside-asana/worldstore-distributed-caching-reactivity-part-1" rel="nofollow">Asana’s WorldStore</a> and <a href="https://www.figma.com/blog/how-figmas-multiplayer-technology-works/#syncing-object-properties" rel="nofollow">Figma’s LiveGraph</a>, we tail postgres’ WAL to detect novelty and invalidate relevant queries.</p>
<p dir="auto">For the frontend, we wrote a client-side triple store. The SDK handles persisting a cache of recent queries to IndexedDB on web, and AsyncStorage in React Native.</p>
<p dir="auto">All data goes through a permission system powered by Google's <a href="https://github.com/google/cel-java">CEL library</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">The easiest way to get started with Instant is by signing up on instantdb.com. <a href="https://instantdb.com/docs" rel="nofollow">You can create a functional app in 5 minute or less.</a>.</p>
<p dir="auto">If you have any questions, you can jump in on our <a href="https://discord.com/invite/VU53p7uQcE" rel="nofollow">discord</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">You can start by joining our <a href="https://discord.com/invite/VU53p7uQcE" rel="nofollow">discord</a> and introducing yourself. Even if you don't contribute code, we always love feedback.</p>
<p dir="auto">If you want to make changes, start by reading the <a href="https://github.com/instantdb/instant/blob/main/client"><code>client</code></a> and <a href="https://github.com/instantdb/instant/blob/main/server"><code>server</code></a> READMEs. There you'll find instructions to start Instant locally.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Peloton to charge $95 activation fee for used bikes (109 pts)]]></title>
            <link>https://www.cnbc.com/2024/08/22/peloton-to-charge-95-activation-fee-for-used-bikes-.html</link>
            <guid>41322266</guid>
            <pubDate>Thu, 22 Aug 2024 17:06:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/08/22/peloton-to-charge-95-activation-fee-for-used-bikes-.html">https://www.cnbc.com/2024/08/22/peloton-to-charge-95-activation-fee-for-used-bikes-.html</a>, See on <a href="https://news.ycombinator.com/item?id=41322266">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/PTON/">Peloton</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> on Thursday said it will <a href="https://www.cnbc.com/2024/08/22/peloton-pton-earnings-q4-2024.html">start charging new subscribers</a> a one-time $95 activation fee if they bought their hardware on the secondary market as more consumers snag lightly used equipment for a fraction of the typical retail price.</p><p>The used equipment activation fee for subscribers in the U.S. and Canada comes as Peloton starts to see a meaningful increase in new members who bought used Bikes or Treads from peer-to-peer markets such as Facebook Marketplace.&nbsp;</p><p>During its fiscal fourth quarter, which ended June 30, Peloton said it saw a "steady stream of paid connected fitness subscribers" who bought hardware on the secondary market. The company said the segment grew 16% year over year.</p><p>"We believe a meaningful share of these subscribers are incremental, and they exhibit lower net churn rates than rental subscribers," the company said in a letter to shareholders.&nbsp;</p><p>"It's also worth highlighting that this activation fee will be a source of incremental revenue and gross profit for us, helping to support our investments in improving the fitness experience for our members," interim co-CEO Christopher Bruzzo later added on a call with analysts.&nbsp;</p><p>While plenty of Peloton subscribers are avid users of the home workout machines, some have likened them to glorified clothes racks because so many people stop using the equipment. Those people paid Peloton for that hardware originally, but importantly, many of them have canceled their monthly subscription, which is how Peloton <a href="https://www.cnbc.com/2024/07/02/peloton-staves-off-liquidity-crunch-in-global-refinance.html">makes the bulk of its money</a>.&nbsp;</p><p>The ability to attract new, budget-conscious members from the secondary market who are willing to pay for a monthly subscription is a unique opportunity for Peloton to grow revenue without any upfront cost, on top of the revenue from the original sale.&nbsp;</p><p>Ari Kimmelfeld — whose startup Trade My Stuff, formerly known as Trade My Spin, sells used Peloton equipment — estimates there are around a million Bikes collecting dust in homes around the world that could be a source of new revenue for the company.&nbsp;</p><p>He told CNBC he previously met with Peloton executives to discuss ways to collaborate, because every time he sells a used piece of equipment, it could lead to more than $500 in new revenue per year for Peloton. With the new used equipment activation fee, that number could grow to more than $600 for the first year.&nbsp;</p><p>"We save the customer a lot more than $95," Kimmelfeld told CNBC on Thursday after the new activation fee was announced. "I don't think it'll stop or slow down people from buying secondary equipment … because you can get a bike delivered faster and cheaper on the secondary market, even with the $95, let's call it a tax, from Peloton."&nbsp;</p><p>Trade My Stuff sells first-generation Bikes for $499, compared with $1,445 new. It offers the Bike+ for $1,199, compared with $2,495 new. It also sells used Treads for $1,999, compared with $2,995 new.&nbsp;</p><p>Since launching his business, Kimmelfeld has worked with people looking to sell their used Peloton equipment and has since sold a "few thousand" Bikes. In 14 cities around the country, including Los Angeles, Denver and New York City, the company offers same- or next-day delivery. Outside of those locales, it provides delivery within three to five days. That compares with a new Peloton purchase, which can take significantly longer to deliver.&nbsp;</p><p>The used equipment activation fee is designed to ensure that new members "receive the same high-quality onboarding experience Peloton is known for," the company said. Bruzzo said that those who buy a used Bike or Bike+ have access to a virtual custom fitting ahead of their first ride, as well as a history summary that shows how many rides those bikes had before they were resold.&nbsp;</p><p>"We're also offering these new members discounts on accessories such as bike shoes, bike mats and spare parts," said Bruzzo. "We'll continue to lean into this important channel and find additional ways to improve the new member experience, for example, providing early education about the broad range of fitness modalities that we offer and the many series and programs our instructors provide to new members."</p></div><div id="RegularArticle-RelatedContent-1"><h2>Don’t miss these insights from CNBC PRO</h2><div><ul><li><a href="https://www.cnbc.com/2024/08/16/the-60/40-portfolio-shined-during-the-market-turbulence-.html">The 60/40 portfolio excelled during the market storm — and Vanguard sees a strong decade ahead</a></li><li><a href="https://www.cnbc.com/2024/08/20/investor-mark-mobius-names-one-risk-that-could-set-back-us-markets.html">Veteran investor Mark Mobius says this 'historically significant' factor could set back U.S. stocks</a></li><li><a href="https://www.cnbc.com/2024/08/14/jefferies-names-3-chip-stocks-to-buy-after-the-sell-off-giving-all-over-50percent-upside.html">Jefferies names 3 chip stocks to buy after the sell-off, giving all over 50% upside</a></li><li><a href="https://www.cnbc.com/2024/08/20/novo-nordisk-vs-eli-lilly-analysts-weigh-in-as-the-obesity-drug-battle-heats-up.html">Novo Nordisk vs. Eli Lilly: Analysts weigh in as the obesity-drug battle heats up</a><br></li></ul></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Aerc: A well-crafted TUI for email (217 pts)]]></title>
            <link>https://blog.sergeantbiggs.net/posts/aerc-a-well-crafted-tui-for-email/</link>
            <guid>41321981</guid>
            <pubDate>Thu, 22 Aug 2024 16:34:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.sergeantbiggs.net/posts/aerc-a-well-crafted-tui-for-email/">https://blog.sergeantbiggs.net/posts/aerc-a-well-crafted-tui-for-email/</a>, See on <a href="https://news.ycombinator.com/item?id=41321981">Hacker News</a></p>
Couldn't get https://blog.sergeantbiggs.net/posts/aerc-a-well-crafted-tui-for-email/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Continuous reinvention: A brief history of block storage at AWS (274 pts)]]></title>
            <link>https://www.allthingsdistributed.com/2024/08/continuous-reinvention-a-brief-history-of-block-storage-at-aws.html</link>
            <guid>41321063</guid>
            <pubDate>Thu, 22 Aug 2024 14:59:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.allthingsdistributed.com/2024/08/continuous-reinvention-a-brief-history-of-block-storage-at-aws.html">https://www.allthingsdistributed.com/2024/08/continuous-reinvention-a-brief-history-of-block-storage-at-aws.html</a>, See on <a href="https://news.ycombinator.com/item?id=41321063">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header></header><hr><section><p><time itemprop="datePublished" datetime="2024-08-22">August 22, 2024</time> • 4800 words</p><span itemprop="articleBody"><p><em><a href="https://www.linkedin.com/in/msolson/">Marc Olson</a> has been part of the team shaping Elastic Block Store (EBS) for over a decade. In that time, he’s helped to drive the dramatic evolution of EBS from a simple block storage service relying on shared drives to a massive network storage system that delivers over 140 trillion daily operations.</em></p><p><em>In this post, Marc provides a fascinating insider’s perspective on the journey of EBS. He shares hard-won lessons in areas such as queueing theory, the importance of comprehensive instrumentation, and the value of incrementalism versus radical changes. Most importantly, he emphasizes how constraints can often breed creative solutions. It’s an insightful look at how one of AWS’s foundational services has evolved to meet the needs of our customers (and the pace at which they’re innovating).</em></p><p><em>–W</em></p><hr><center><h2>Continuous reinvention: A brief history of block storage at AWS</h2></center><p>I’ve built system software for most of my career, and before joining AWS it was mostly in the networking and security spaces. When I joined AWS nearly 13 years ago, I entered a new domain—storage—and stepped into a new challenge. Even back then the scale of AWS dwarfed anything I had worked on, but many of the same techniques I had picked up until that point remained applicable—distilling problems down to first principles, and using successive iteration to incrementally solve problems and improve performance.</p><p>If you look around at AWS services today, you’ll find a mature set of core building blocks, but it wasn’t always this way. <a href="https://www.allthingsdistributed.com/2008/08/amazon_ebs_elastic_block_store.html">EBS launched on August 20, 2008</a>, nearly two years after EC2 became available in beta, with a simple idea to provide network attached block storage for EC2 instances. We had one or two storage experts, and a few distributed systems folks, and a solid knowledge of computer systems and networks. How hard could it be? In retrospect, if we knew at the time how much we didn’t know, we may not have even started the project!</p><p>Since I’ve been at EBS, I’ve had the opportunity to be part of the team that’s evolved EBS from a product built using shared hard disk drives (HDDs), to one that is capable of delivering hundreds of thousands of IOPS (IO operations per second) to a single EC2 instance. It’s remarkable to reflect on this because EBS is capable of delivering more IOPS to a single instance today than it could deliver to an entire Availability Zone (AZ) in the early years on top of HDDs. Even more amazingly, today EBS in aggregate delivers over 140 trillion operations daily across a distributed SSD fleet. But we definitely didn’t do it overnight, or in one big bang, or even perfectly. When I started on the EBS team, I initially worked on the EBS client, which is the piece of software responsible for converting instance IO requests into EBS storage operations. Since then I’ve worked on almost every component of EBS and have been delighted to have had the opportunity to participate so directly in the evolution and growth of EBS.</p><p>As a storage system, EBS is a bit unique. It’s unique because our primary workload is system disks for EC2 instances, motivated by the hard disks that used to sit inside physical datacenter servers. A lot of storage services place durability as their primary design goal, and are willing to degrade performance or availability in order to protect bytes. EBS customers care about durability, and we provide the primitives to help them achieve high durability with io2 Block Express volumes and volume snapshots, but they also care a lot about the performance and availability of EBS volumes. EBS is so closely tied as a storage primitive for EC2, that the performance and availability of EBS volumes tends to translate almost directly to the performance and availability of the EC2 experience, and by extension the experience of running applications and services that are built using EC2. The story of EBS is the story of understanding and evolving performance in a very large-scale distributed system that spans layers from guest operating systems at the top, all the way down to custom SSD designs at the bottom. In this post I’d like to tell you about the journey that we’ve taken, including some memorable lessons that may be applicable to your systems. After all, systems performance is a complex and really challenging area, and it’s a complex language across many domains.</p><h2 id="queueing-theory-briefly">Queueing theory, briefly <a href="#queueing-theory-briefly"></a></h2><p>Before we dive too deep, let’s take a step back and look at how computer systems interact with storage. The high-level basics haven’t changed through the years—a storage device is connected to a bus which is connected to the CPU. The CPU queues requests that travel the bus to the device. The storage device either retrieves the data from CPU memory and (eventually) places it onto a durable substrate, or retrieves the data from the durable media, and then transfers it to the CPU’s memory.</p><figure><img src="https://www.allthingsdistributed.com/images/mo-computer-arch.png" alt="Architecture with direct attached disk" loading="lazy"><figcaption>High-level computer architecture with direct attached disk</figcaption></figure><p>You can think of this like a bank. You walk into the bank with a deposit, but first you have to traverse a queue before you can speak with a bank teller who can help you with your transaction. In a perfect world, the number of patrons entering the bank arrive at the exact rate at which their request can be handled, and you never have to stand in a queue. But the real world isn’t perfect. The real world is asynchronous. It’s more likely that a few people enter the bank at the same time. Perhaps they have arrived on the same streetcar or train. When a group of people all walk into the back at the same time, some of them are going to have to wait for the teller to process the transactions ahead of them.</p><p>As we think about the time to complete each transaction, and empty the queue, the average time waiting in line (latency) across all customers may look acceptable, but the first person in the queue had the best experience, while the last had a much longer delay. There are a number of things the bank can do to improve the experience for all customers. The bank could add more tellers to process more requests in parallel, it could rearrange the teller workflows so that each transaction takes less time, lowering both the total time and the average time, or it could create different queues for either latency insensitive customers or consolidating transactions that may be faster to keep the queue low. But each of these options comes at an additional cost—hiring more tellers for a peak that may never occur, or adding more real estate to create separate queues. While imperfect, unless you have infinite resources, queues are necessary to absorb peak load.</p><figure><img src="https://www.allthingsdistributed.com/images/mo-simplified-ec2-ebs-queueing.png" alt="Simple diagram of EC2 and EBS queueing from 2012" loading="lazy"><figcaption>Simplified diagram of EC2 and EBS queueing (c. 2012)</figcaption></figure><p>In network storage systems, we have several queues in the stack, including those between the operating system kernel and the storage adapter, the host storage adapter to the storage fabric, the target storage adapter, and the storage media. In legacy network storage systems, there may be different vendors for each component, and different ways that they think about servicing the queue. You may be using a dedicated, lossless network fabric like fiber channel, or using iSCSI or NFS over TCP, either with the operating system network stack, or a custom driver. In either case, tuning the storage network often takes specialized knowledge, separate from tuning the application or the storage media.</p><p>When we first built EBS in 2008, the storage market was largely HDDs, and the latency of our service was dominated by the latency of this storage media. Last year, Andy Warfield went in-depth about the <a href="https://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html#technical-scale-scale-and-the-physics-of-storage">fascinating mechanical engineering behind HDDs</a>. As an engineer, I still marvel at everything that goes into a hard drive, but at the end of the day they are mechanical devices and physics limits their performance. There’s a stack of platters that are spinning at high velocity. These platters have tracks that contain the data. Relative to the size of a track (&lt;100 nanometers), there’s a large arm that swings back and forth to find the right track to read or write your data. Because of the physics involved, the IOPS performance of a hard drive has remained relatively constant for the last few decades at approximately 120-150 operations per second, or 6-8 ms average IO latency. One of the biggest challenges with HDDs is that tail latencies can easily drift into the hundreds of milliseconds with the impact of queueing and command reordering in the drive.</p><p>We didn’t have to worry much about the network getting in the way since end-to-end EBS latency was dominated by HDDs and measured in the 10s of milliseconds. Even our early data center networks were beefy enough to handle our user’s latency and throughput expectations. The addition of 10s of microseconds on the network was a small fraction of overall latency.</p><p>Compounding this latency, hard drive performance is also variable depending on the other transactions in the queue. Smaller requests that are scattered randomly on the media take longer to find and access than several large requests that are all next to each other. This random performance led to wildly inconsistent behavior. Early on, we knew that we needed to spread customers across many disks to achieve reasonable performance. This had a benefit, it dropped the peak outlier latency for the hottest workloads, but unfortunately it spread the inconsistent behavior out so that it impacted many customers.</p><p>When one workload impacts another, we call this a “noisy neighbor.” Noisy neighbors turned out to be a critical problem for the business. As AWS evolved, we learned that we had to focus ruthlessly on a high-quality customer experience, and that inevitably meant that we needed to achieve strong performance isolation to avoid noisy neighbors causing interference with other customer workloads.</p><p>At the scale of AWS, we often run into challenges that are hard and complex due to the scale and breadth of our systems, and our focus on maintaining the customer experience. Surprisingly, the fixes are often quite simple once you deeply understand the system, and have enormous impact due to the scaling factors at play. We were able to make some improvements by changing scheduling algorithms to the drives and balancing customer workloads across even more spindles. But all of this only resulted in small incremental gains. We weren’t really hitting the breakthrough that truly eliminated noisy neighbors. Customer workloads were too unpredictable to achieve the consistency we knew they needed. We needed to explore something completely different.</p><h2 id="set-long-term-goals-but-dont-be-afraid-to-improve-incrementally">Set long term goals, but don’t be afraid to improve incrementally <a href="#set-long-term-goals-but-dont-be-afraid-to-improve-incrementally"></a></h2><p>Around the time I started at AWS in 2011, solid state disks (SSDs) became more mainstream, and were available in sizes that started to make them attractive to us. In an SSD, there is no physical arm to move to retrieve data—random requests are nearly as fast as sequential requests—and there are multiple channels between the controller and NAND chips to get to the data. If we revisit the bank example from earlier, replacing an HDD with an SSD is like building a bank the size of a football stadium and staffing it with superhumans that can complete transactions orders of magnitude faster. A year later we started using SSDs, and haven’t looked back.</p><p>We started with a small, but meaningful milestone: we built a new storage server type built on SSDs, and a new EBS volume type called Provisioned IOPS. Launching a new volume type is no small task, and it also limits the workloads that can take advantage of it. For EBS, there was an immediate improvement, but it wasn’t everything we expected.</p><p>We thought that just dropping SSDs in to replace HDDs would solve almost all of our problems, and it certainly did address the problems that came from the mechanics of hard drives. But what surprised us was that the system didn’t improve nearly as much as we had hoped and noisy neighbors weren’t automatically fixed. We had to turn our attention to the rest of our stack—the network and our software—that the improved storage media suddenly put a spotlight on.</p><p>Even though we needed to make these changes, we went ahead and launched in August 2012 with a maximum of 1,000 IOPS, 10x better than existing EBS standard volumes, and ~2-3 ms average latency, a 5-10x improvement with significantly improved outlier control. Our customers were excited for an EBS volume that they could begin to build their mission critical applications on, but we still weren’t satisfied and we realized that the performance engineering work in our system was really just beginning. But to do that, we had to measure our system.</p><h2 id="if-you-cant-measure-it-you-cant-manage-it">If you can’t measure it, you can’t manage it <a href="#if-you-cant-measure-it-you-cant-manage-it"></a></h2><p>At this point in EBS’s history (2012), we only had rudimentary telemetry. To know what to fix, we had to know what was broken, and then prioritize those fixes based on effort and rewards. Our first step was to build a method to instrument every IO at multiple points in every subsystem—in our client initiator, network stack, storage durability engine, and in our operating system. In addition to monitoring customer workloads, we also built a set of canary tests that run continuously and allowed us to monitor impact of changes—both positive and negative—under well-known workloads.</p><p>With our new telemetry we identified a few major areas for initial investment. We knew we needed to reduce the number of queues in the entire system. Additionally, the Xen hypervisor had served us well in EC2, but as a general-purpose hypervisor, it had different design goals and many more features than we needed for EC2. We suspected that with some investment we could reduce complexity of the IO path in the hypervisor, leading to improved performance. Moreover, we needed to optimize the network software, and in our core durability engine we needed to do a lot of work organizationally and in code, including on-disk data layout, cache line optimization, and fully embracing an asynchronous programming model.</p><p>A really consistent lesson at AWS is that system performance issues almost universally span a lot of layers in our hardware and software stack, but even great engineers tend to have jobs that focus their attention on specific narrower areas. While the much celebrated ideal of a “full stack engineer” is valuable, in deep and complex systems it’s often even more valuable to create cohorts of experts who can collaborate and get really creative across the entire stack and all their individual areas of depth.</p><p>By this point, we already had separate teams for the storage server and for the client, so we were able to focus on these two areas in parallel. We also enlisted the help of the EC2 hypervisor engineers and formed a cross-AWS network performance cohort. We started to build a blueprint of both short-term, tactical fixes and longer-term architectural changes.</p><h2 id="divide-and-conquer">Divide and conquer <a href="#divide-and-conquer"></a></h2><figure><img src="https://www.allthingsdistributed.com/images/mo-physalia.png" alt="Whiteboard showing how the team removed the contronl from from the IO path with Physalia" loading="lazy"><figcaption>Removing the control plane from the IO path with Physalia</figcaption></figure><p>When I was an undergraduate student, while I loved most of my classes, there were a couple that I had a love-hate relationship with. “Algorithms” was taught at a graduate level at my university for both undergraduates and graduates. I found the coursework intense, but I eventually fell in love with the topic, and <a href="https://www.amazon.com/dp/026204630X">Introduction to Algorithms</a>, commonly referred to as CLR, is one of the few textbooks I retained, and still occasionally reference. What I didn’t realize until I joined Amazon, and seems obvious in hindsight, is that you can design an organization much the same way you can design a software system. Different algorithms have different benefits and tradeoffs in how your organization functions. Where practical, Amazon chooses a divide and conquer approach, and keeps teams small and focused on a self-contained component with well-defined APIs.</p><p>This works well when applied to components of a retail website and control plane systems, but it’s less intuitive in how you could build a high-performance data plane this way, and at the same time improve performance. In the EBS storage server, we reorganized our monolithic development team into small teams focused on specific areas, such as data replication, durability, and snapshot hydration. Each team focused on their unique challenges, dividing the performance optimization into smaller sized bites. These teams are able to iterate and commit their changes independently—made possible by rigorous testing that we’ve built up over time. It was important for us to make continual progress for our customers, so we started with a blueprint for where we wanted to go, and then began the work of separating out components while deploying incremental changes.</p><p>The best part of incremental delivery is that you can make a change and observe its impact before making the next change. If something doesn’t work like you expected, then it’s easy to unwind it and go in a different direction. In our case, the blueprint that we laid out in 2013 ended up looking nothing like what EBS looks like today, but it gave us a direction to start moving toward. For example, back then we never would have imagined that Amazon would one day <a href="https://aws.amazon.com/blogs/aws/aws-nitro-ssd-high-performance-storage-for-your-i-o-intensive-applications/">build its own SSDs</a>, with a technology stack that could be tailored specifically to the needs of EBS.</p><h2 id="always-question-your-assumptions">Always question your assumptions! <a href="#always-question-your-assumptions"></a></h2><p>Challenging our assumptions led to improvements in every single part of the stack.</p><p>We started with software virtualization. Until late 2017 all EC2 instances ran on the Xen hypervisor. With devices in Xen, there is a ring queue setup that allows guest instances, or domains, to share information with a privileged driver domain (dom0) for the purposes of IO and other emulated devices. The EBS client ran in dom0 as a kernel block device. If we follow an IO request from the instance, just to get off of the EC2 host there are many queues: the instance block device queue, the Xen ring, the dom0 kernel block device queue, and the EBS client network queue. In most systems, performance issues are compounding, and it’s helpful to focus on components in isolation.</p><p>One of the first things that we did was to write several “loopback” devices so that we could isolate each queue to gauge the impact of the Xen ring, the dom0 block device stack, and the network. We were almost immediately surprised that with almost no latency in the dom0 device driver, when multiple instances tried to drive IO, they would interact with each other enough that the goodput of the entire system would slow down. We had found another noisy neighbor! Embarrassingly, we had launched EC2 with the Xen defaults for the number of block device queues and queue entries, which were set many years prior based on the limited storage hardware that was available to the Cambridge lab building Xen. This was very unexpected, especially when we realized that it limited us to only 64 IO outstanding requests for an entire host, not per device—certainly not enough for our most demanding workloads.</p><p>We fixed the main issues with software virtualization, but even that wasn’t enough. In 2013, we were well into the development of our first <a href="https://www.allthingsdistributed.com/2020/09/reinventing-virtualization-with-nitro.html">Nitro offload card</a> dedicated to networking. With this first card, we moved the processing of VPC, our software defined network, from the Xen dom0 kernel, into a dedicated hardware pipeline. By isolating the packet processing data plane from the hypervisor, we no longer needed to steal CPU cycles from customer instances to drive network traffic. Instead, we leveraged Xen’s ability to pass a virtual PCI device directly to the instance.</p><p>This was a fantastic win for latency and efficiency, so we decided to do the same thing for EBS storage. By moving more processing to hardware, we removed several operating system queues in the hypervisor, even if we weren’t ready to pass the device directly to the instance just yet. Even without passthrough, by offloading more of the interrupt driven work, the hypervisor spent less time servicing the requests—the hardware itself had dedicated interrupt processing functions. This second Nitro card also had hardware capability to handle EBS encrypted volumes with no impact to EBS volume performance. Leveraging our hardware for encryption also meant that the encryption key material is kept separate from the hypervisor, which further protects customer data.</p><figure><img src="https://www.allthingsdistributed.com/images/mo-network-tuning.png" alt="Diagram showing experiments in network tuning to improve throughput and reduce latency" loading="lazy"><figcaption>Experimenting with network tuning to improve throughput and reduce latency</figcaption></figure><p>Moving EBS to Nitro was a huge win, but it almost immediately shifted the overhead to the network itself. Here the problem seemed simple on the surface. We just needed to tune our wire protocol with the latest and greatest data center TCP tuning parameters, while choosing the best congestion control algorithm. There were a few shifts that were working against us: AWS was experimenting with different data center cabling topology, and our AZs, once a single data center, were growing beyond those boundaries. Our tuning would be beneficial, as in the example above, where adding a small amount of random latency to requests to storage servers counter-intuitively reduced the average latency and the outliers due to the smoothing effect it has on the network. These changes were ultimately short lived as we continuously increased the performance and scale of our system, and we had to continually measure and monitor to make sure we didn’t regress.</p><p>Knowing that we would need something better than TCP, in 2014 we started laying the foundation for Scalable Relatable Diagram (SRD) with “<a href="https://ieeexplore.ieee.org/document/9167399">A Cloud-Optimized Transport Protocol for Elastic and Scalable HPC</a>”. Early on we set a few requirements, including a protocol that could improve our ability to recover and route around failures, and we wanted something that could be easily offloaded into hardware. As we were investigating, we made two key observations: 1/ we didn’t need to design for the general internet, but we could focus specifically on our data center network designs, and 2/ in storage, the execution of IO requests that are in flight could be reordered. We didn’t need to pay the penalty of TCP’s strict in-order delivery guarantees, but could instead send different requests down different network paths, and execute them upon arrival. Any barriers could be handled at the client before they were sent on the network. What we ended up with is a protocol that’s useful not just for storage, but for networking, too. When used in <a href="https://aws.amazon.com/about-aws/whats-new/2022/11/elastic-network-adapter-ena-express-amazon-ec2-instances/">Elastic Network Adapter (ENA) Express</a>, SRD improves the performance of your TCP stacks in your guest. SRD can drive the network at higher utilization by taking advantage of multiple network paths and reducing the overflow and queues in the intermediate network devices.</p><p>Performance improvements are never about a single focus. It’s a discipline of continuously challenging your assumptions, measuring and understanding, and shifting focus to the most meaningful opportunities.</p><h2 id="constraints-breed-innovation">Constraints breed innovation <a href="#constraints-breed-innovation"></a></h2><p>We weren’t satisfied that only a relatively small number of volumes and customers had better performance. We wanted to bring the benefits of SSDs to everyone. This is an area where scale makes things difficult. We had a large fleet of thousands of storage servers running millions of non-provisioned IOPS customer volumes. Some of those same volumes still exist today. It would be an expensive proposition to throw away all of that hardware and replace it.</p><p>There was empty space in the chassis, but the only location that didn’t cause disruption in the cooling airflow was between the motherboard and the fans. The nice thing about SSDs is that they are typically small and light, but we couldn’t have them flopping around loose in the chassis. After some trial and error—and help from our material scientists—we found heat resistant, industrial strength hook and loop fastening tape, which also let us service these SSDs for the remaining life of the servers.</p><figure><img src="https://www.allthingsdistributed.com/images/mo-manual-ssd.png" alt="An SSD in one of our servers" loading="lazy"><figcaption>Yes, we manually put an SSD into every server!</figcaption></figure><p>Armed with this knowledge, and a lot of human effort, over the course of a few months in 2013, EBS was able to put a single SSD into each and every one of those thousands of servers. We made a small change to our software that staged new writes onto that SSD, allowing us to return completion back to your application, and then flushed the writes to the slower hard disk asynchronously. And we did this with no disruption to customers—we were converting a propeller aircraft to a jet while it was in flight. The thing that made this possible is that we designed our system from the start with non-disruptive maintenance events in mind. We could retarget EBS volumes to new storage servers, and update software or rebuild the empty servers as needed.</p><p>This ability to migrate customer volumes to new storage servers has come in handy several times throughout EBS’s history as we’ve identified new, more efficient data structures for our on-disk format, or brought in new hardware to replace the old hardware. There are volumes still active from the first few months of EBS’s launch in 2008. These volumes have likely been on hundreds of different servers and multiple generations of hardware as we’ve updated and rebuilt our fleet, all without impacting the workloads on those volumes.</p><h2 id="reflecting-on-scaling-performance">Reflecting on scaling performance <a href="#reflecting-on-scaling-performance"></a></h2><p>There’s one more journey over this time that I’d like to share, and that’s a personal one. Most of my career prior to Amazon had been in either early startup or similarly small company cultures. I had built managed services, and even distributed systems out of necessity, but I had never worked on anything close to the scale of EBS, even the EBS of 2011, both in technology and organization size. I was used to solving problems by myself, or maybe with one or two other equally motivated engineers.</p><p>I really enjoy going super deep into problems and attacking them until they’re complete, but there was a pivotal moment when a colleague that I trusted pointed out that I was becoming a performance bottleneck for our organization. As an engineer who had grown to be an expert in the system, but also who cared really, really deeply about all aspects of EBS, I found myself on every escalation and also wanting to review every commit and every proposed design change. If we were going to be successful, then I had to learn how to scale myself–I wasn’t going to solve this with just ownership and bias for action.</p><p>This led to even more experimentation, but not in the code. I knew I was working with other smart folks, but I also needed to take a step back and think about how to make them effective. One of my favorite tools to come out of this was peer debugging. I remember a session with a handful of engineers in one of our lounge rooms, with code and a few terminals projected on a wall. One of the engineers exclaimed, “Uhhhh, there’s no way that’s right!” and we had found something that had been nagging us for a while. We had overlooked where and how we were locking updates to critical data structures. Our design didn’t usually cause issues, but occasionally we would see slow responses to requests, and fixing this removed one source of jitter. We don’t always use this technique, but the neat thing is that we are able to combine our shared systems knowledge when things get really tricky.</p><p>Through all of this, I realized that empowering people, giving them the ability to safely experiment, can often lead to results that are even better than what was expected. I’ve spent a large portion of my career since then focusing on ways to remove roadblocks, but leave the guardrails in place, pushing engineers out of their comfort zone. There’s a bit of psychology to engineering leadership that I hadn’t appreciated. I never expected that one of the most rewarding parts of my career would be encouraging and nurturing others, watching them own and solve problems, and most importantly celebrating the wins with them!</p><h2 id="conclusion">Conclusion <a href="#conclusion"></a></h2><p>Reflecting back on where we started, we knew we could do better, but we weren’t sure how much better. We chose to approach the problem, not as a big monolithic change, but as a series of incremental improvements over time. This allowed us to deliver customer value sooner, and course correct as we learned more about changing customer workloads. We’ve improved the shape of the EBS latency experience from one averaging more than 10 ms per IO operation to consistent sub-millisecond IO operations with our highest performing io2 Block Express volumes. We accomplished all this without taking the service offline to deliver a new architecture.</p><p>We know we’re not done. Our customers will always want more, and that challenge is what keeps us motivated to innovate and iterate.</p><ul><li><a href="https://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html?utm_campaign=related+posts&amp;utm_source=brief-history-ebs">Building and operating a pretty big storage system called S3</a></li><li><a href="https://www.allthingsdistributed.com/2023/11/standing-on-the-shoulders-of-giants-colm-on-constant-work.html?utm_campaign=related+posts&amp;utm_source=brief-history-ebs">Reliability, constant work, and a good cup of coffee</a></li><li><a href="https://www.allthingsdistributed.com/2022/11/amazon-1998-distributed-computing-manifesto.html?utm_campaign=related+posts&amp;utm_source=brief-history-ebs">The Distributed Computing Manifesto</a></li></ul></span></section><hr></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beastie Boys dismantled their gold record plaque,it didn't contain their music (114 pts)]]></title>
            <link>https://djmag.com/news/beastie-boys-dismantled-their-pauls-boutique-gold-record-plaque-find-it-didnt-contain-their</link>
            <guid>41319955</guid>
            <pubDate>Thu, 22 Aug 2024 13:20:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://djmag.com/news/beastie-boys-dismantled-their-pauls-boutique-gold-record-plaque-find-it-didnt-contain-their">https://djmag.com/news/beastie-boys-dismantled-their-pauls-boutique-gold-record-plaque-find-it-didnt-contain-their</a>, See on <a href="https://news.ycombinator.com/item?id=41319955">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="text">
<p><span><span><span><span><span><span><span>Beastie Boys have shared the story of how they discovered that the gold record plaque they received after the certification of ‘Paul’s Boutique’ didn’t actually contain a pressing of their own music.</span></span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>Ad-Rock (Adam Horovitz) and Mike D (Michael Diamond) from <a href="https://djmag.com/news/beastie-boys-announce-special-edition-check-your-head-30th-anniversary-reissue" rel="noopener" target="_blank">the New York hip-hop innovators</a> recently revealed the issue with their gold record plaque for the seminal 1989 album during an interview on the Conan O’Brien Needs a Friend podcast.</span></span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>The certification came when ‘Paul’s Boutique’ sold 500,000 copies just two months after its release. However, when the bandmates closely examined the plaque years later, they noticed that the record displayed inside did not contain the same amount of tracks as the album’s tracklisting.</span></span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>“So we’re at our studio here in California and I was smoking the pot,” Ad-Rock said, recalling the discovery. “This was a long time ago. We had a gold record on the wall, it was our record, ‘</span></span><span><span>Paul’s Boutique’</span></span><span><span>. I was looking at it and I could see it had our label and I could see that it had like nine songs on the one side. But I was looking at the actual gold record and it only had four songs on it.”</span></span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>Curious about the discrepancy, the band decided to break the glass and remove the record from the plaque. Upon inspection, they found that instead of ‘</span></span><span><span>Paul’s Boutique’,</span></span><span><span>&nbsp;the vinyl contained piano versions of songs like Barry Manilow’s music and Morris Albert’s ‘Feelings’.</span></span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>When asked by O’Brien if this was common for all gold record plaques, Ad-Rock replied, “I don’t know about anybody else...” while Mike D speculated that major stars like Barbra Streisand or Donna Summer probably received plaques with their actual records.</span></span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>While the plaque didn’t feature their music, '</span></span><span><span>Paul’s Boutique'</span></span><span><span> continued to gain recognition after its gold certification. The album was certified platinum in 1995 and reached double platinum status in 1999.</span></span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>Listen to the podcast below.</span></span></span></span></span></span></span></p>
<p><span><span><span><span><span><span><span>Revisit Ben Cardew’s </span></span></span></span></span></span></span><span><span><span><span><span><span><span>Solid Gold</span></span><span><span> feature on how Beastie Boys' cult classic second album was a high-water mark <a href="https://djmag.com/features/solid-gold-beastie-boys-pauls-boutique" rel="noopener" target="_blank">for rich sampling and undiluted fun</a>.</span></span></span></span></span></span></span></p>
<p>From May, <a href="https://djmag.com/features/how-beastie-boys-ill-communication-set-benchmark-90s-eclecticism" rel="noopener" target="_blank">read DJ Mag’s feature</a> on how the legendary band's fourth album, ‘<span><span>Ill Communication’,&nbsp;</span></span><span><span>set a benchmark for ’90s eclecticism.</span></span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Async2 – The .NET Runtime Async experiment concludes (133 pts)]]></title>
            <link>https://steven-giesel.com/blogPost/59752c38-9c99-4641-9853-9cfa97bb2d29</link>
            <guid>41319224</guid>
            <pubDate>Thu, 22 Aug 2024 11:52:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://steven-giesel.com/blogPost/59752c38-9c99-4641-9853-9cfa97bb2d29">https://steven-giesel.com/blogPost/59752c38-9c99-4641-9853-9cfa97bb2d29</a>, See on <a href="https://news.ycombinator.com/item?id=41319224">Hacker News</a></p>
<div id="readability-page-1" class="page"><div b-gj8cwnhyng=""><p>The .NET team has been working on a new experiment called async2, which is a new implementation of the async/await pattern that is designed to be more efficient and more flexible than the current implementation. It started with green threads and ended with an experiment that moves <code>async</code> and <code>await</code> to the runtime. This post will cover the journey of <code>async2</code> and the conclusion of the experiment.</p>
<h2 id="where-all-began-green-threads">Where all began - Green threads</h2>
<p>Let's start here: <a href="https://github.com/dotnet/runtimelab/issues/2398"><em>"Green Thread Experiment Results"</em></a>. The team invested in an experiment to evaluate the feasibility of using green threads in the .NET runtime. But wait a second, what are green threads?</p>
<h3 id="green-threads">Green threads</h3>
<p>Green threads are user-space threads that are managed by a runtime library or a virtual machine (VM) instead of the operating system.<sup>Source: <a href="https://en.wikipedia.org/wiki/Green_thread">Wikipedia</a></sup> They are lightweight and can be created and managed more quickly than kernel threads. Green threads are also known as "coroutines" or "fibers" in other programming languages. The idea is that you, as a developer, don't have to worry about threads.</p>
<p>Currently, with threads and to some extend with <code>async</code>/<code>await</code>, a new stack is created. You can easily see that in your favourite IDE, if you debug:</p>
<p><img src="https://linkdotnetblog.azureedge.net/blog/20240812_Async2/stacks.webp" alt="stack"></p>
<p>Green threads are different. <a href="https://stackoverflow.com/a/19098856/1892523">The memory of a green thread is allocated on the heap</a>. But all of this comes with a cost: As they aren't managed by the OS, they can't take advantage of multiple cores inherently. But for I/O-bound operations, they are a good fit.</p>
<h3 id="abandoning-green-threads">Abandoning green threads</h3>
<p>The key-challenges were (which lead to the abandonment of the green threads experiment):</p>
<ul>
<li>Complex interaction between green threads and existing async model</li>
<li>Interop with native code was complex and slower than using regular threads</li>
<li>Compatibility issues with security mitigations like shadow stacks</li>
<li>Uncertainty about whether it would be possible to make green threads faster than async in important scenarios, given the effort required for improvement.</li>
</ul>
<p>This lead to the conclusion that green threads are not the right way to go for the .NET runtime and gave birth to the <code>async2</code> experiment. From here on out, I will keep the term <code>async2</code> for the experiment, as it is the codename for the experiment.</p>
<h2 id="async2-the.net-runtime-async-experiment"><code>async2</code> - The .NET Runtime Async experiment</h2>
<p>Now, <code>async2</code> is obviously only a codename. The goal of the experiment was to move <code>async</code> and <code>await</code> to the runtime. The main motivation behind this was to make <code>async</code> more efficient and more flexible. As <code>async</code> is already used as an identifier in C#, the team decided to use <code>async2</code> as a codename for the experiment. <strong>If</strong> that thing ever makes it into the runtime, it will be called <code>async</code> - so it will be a replacement for the current <code>async</code> implementation. But let's start at the beginning.</p>
<h3 id="async-is-a-compiler-feature"><code>async</code> is a compiler feature</h3>
<p>I talked about this from time to time in my blog posts. For example:</p>
<ul>
<li><a href="https://steven-giesel.com/blogPost/720a48fd-0abe-4c32-83ac-26926d501895/the-state-machine-in-c-with-asyncawait"><em>"The state machine in C# with async/await"</em></a></li>
<li><a href="https://steven-giesel.com/blogPost/69dc05d1-9c8a-4002-9d0a-faf4d2375bce/c-lowering"><em>"C# Lowering"</em></a></li>
</ul>
<p>The current implementation of <code>async</code> and <code>await</code> is a compiler feature. The compiler generates a state machine for the <code>async</code> method. The runtime doesn't know anything about <code>async</code> and <code>await</code>. There is no trace of an <code>async</code>-like keyword in IL or in the JIT-compiled code. And that is where the experiment started.</p>
<p>Starting point is this nice GitHub issue: <a href="https://github.com/dotnet/runtime/issues/94620">"<em>.NET 9 Runtime Async Experiment</em>"</a>, which basically describes the whole experiment in more detail with an ongoing discussion from the community.</p>
<h3 id="async-is-a-runtime-feature"><code>async</code> is a runtime feature</h3>
<p>The goal of the experiment was to move <code>async</code> and <code>await</code> to the runtime. This would allow the runtime to have more control over the pattern itself. With that there would be also some different semantics:</p>
<h3 id="async2-and-executioncontext-and-synchronizationcontext"><code>async2</code> and <code>ExecutionContext</code> and <code>SynchronizationContext</code></h3>
<p><code>async2</code> would not have save or restore of <code>SynchronizationContext</code> and <code>ExecutionContext</code> at function boundaries, instead allowing callers to observe changes. With the <code>ExecutionContext</code>, this would shift a big change in how <code>AsyncLocal</code> behaves.</p>
<p>Today, <code>AsyncLocal</code> is used to store data that flows with the logical call context. It gets copied to the new context. That said, if a function deep down the call stack changes the value of an <code>AsyncLocal</code>, the caller will <strong>not</strong> see the updated value, only functions further down the logical async flow. Here an example:</p>
<pre><code>await new AsyncLocalTest().DoOuter();

public class AsyncLocalTest
{
    private readonly AsyncLocal&lt;string&gt; _asyncLocal = new();

    public async Task DoOuter()
    {
        _asyncLocal.Value = "Outer";
        Console.WriteLine($"DoOuter: {_asyncLocal.Value}");
        await DoInner();
        Console.WriteLine($"DoOuter: {_asyncLocal.Value}");
    }

    private async Task DoInner()
    {
        _asyncLocal.Value = "Inner";
        Console.WriteLine($"DoInner: {_asyncLocal.Value}");
        await Task.Yield();
        Console.WriteLine($"DoInner: {_asyncLocal.Value}");
    }
}
</code></pre>
<p>The output of this code is:</p>
<pre><code>DoOuter: Outer
DoInner: Inner
DoInner: Inner
DoOuter: Outer
</code></pre>
<p>With <code>async2</code> those changes are not "reverted" which would lead to a different output:</p>
<pre><code>DoOuter: Outer
DoInner: Inner
DoInner: Inner
DoOuter: Inner
</code></pre>
<h3 id="comparison-with-the-current-implementation-and-some-results">Comparison with the current implementation and some results</h3>
<p>The whole document, that describes all the details, can be found here: <a href="https://github.com/dotnet/runtimelab/blob/feature/async2-experiment/docs/design/features/runtime-handled-tasks.md">https://github.com/dotnet/runtimelab/blob/feature/async2-experiment/docs/design/features/runtime-handled-tasks.md</a></p>
<p>The team found out that the approach of putting <code>async</code> into the JIT might yield the best results overall. Here a basic overview:</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th><code>async</code></th>
<th><code>async2</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>Performance</td>
<td>Generally slower than <code>async2</code>, especially for deep call stacks</td>
<td>Generally faster than <code>async</code>, with performance comparable to synchronous code in non-suspended scenarios</td>
</tr>
<tr>
<td>Exception Handling</td>
<td>Slow and inefficient, causing GC pauses and impacting responsive performance of applications</td>
<td>Improved EH handling, reducing the impact on application responsiveness</td>
</tr>
<tr>
<td>Stack Depth Limitation</td>
<td>Limited by stack depth, which can cause issues for deep call stacks</td>
<td>No explicit limitations on stack depth, allowing <code>async2</code> to handle deeper call stacks more efficiently</td>
</tr>
<tr>
<td>Memory Consumption</td>
<td>Generally lower than <code>async2</code>, especially in scenarios with many suspended tasks</td>
<td>Higher memory consumption due to capturing entire stack frames and registers, but still acceptable compared to other factors like pause times</td>
</tr>
</tbody>
</table>
<h2 id="where-do-we-go-from-here">Where do we go from here?</h2>
<p>As the name suggests, this is just an experiment, that may lead to a replacement of <code>async</code> <strong>in some years</strong>. Yes, <strong>years</strong>. It might take a while until this is production-ready. And for the transition phase, there has to be interop for <code>async</code> ↔ <code>async2</code>. Anyway - a very good starting point and I am looking forward to the future of <code>async2</code>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Electric Clojure v3: Differential Dataflow for UI [video] (104 pts)]]></title>
            <link>https://hyperfiddle-docs.notion.site/Talk-Electric-Clojure-v3-Differential-Dataflow-for-UI-Getz-2024-2e611cebd73f45dc8cc97c499b3aa8b8</link>
            <guid>41319003</guid>
            <pubDate>Thu, 22 Aug 2024 11:24:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hyperfiddle-docs.notion.site/Talk-Electric-Clojure-v3-Differential-Dataflow-for-UI-Getz-2024-2e611cebd73f45dc8cc97c499b3aa8b8">https://hyperfiddle-docs.notion.site/Talk-Electric-Clojure-v3-Differential-Dataflow-for-UI-Getz-2024-2e611cebd73f45dc8cc97c499b3aa8b8</a>, See on <a href="https://news.ycombinator.com/item?id=41319003">Hacker News</a></p>
Couldn't get https://hyperfiddle-docs.notion.site/Talk-Electric-Clojure-v3-Differential-Dataflow-for-UI-Getz-2024-2e611cebd73f45dc8cc97c499b3aa8b8: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[No "Hello", No "Quick Call", and No Meetings Without an Agenda (219 pts)]]></title>
            <link>https://switowski.com/blog/no-hello-no-quick-call-no-agendaless-meetings/</link>
            <guid>41318408</guid>
            <pubDate>Thu, 22 Aug 2024 09:46:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://switowski.com/blog/no-hello-no-quick-call-no-agendaless-meetings/">https://switowski.com/blog/no-hello-no-quick-call-no-agendaless-meetings/</a>, See on <a href="https://news.ycombinator.com/item?id=41318408">Hacker News</a></p>
Couldn't get https://switowski.com/blog/no-hello-no-quick-call-no-agendaless-meetings/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[What is an SBAT and why does everyone suddenly care (351 pts)]]></title>
            <link>https://mjg59.dreamwidth.org/70348.html</link>
            <guid>41318222</guid>
            <pubDate>Thu, 22 Aug 2024 09:11:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mjg59.dreamwidth.org/70348.html">https://mjg59.dreamwidth.org/70348.html</a>, See on <a href="https://news.ycombinator.com/item?id=41318222">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Short version: <a href="https://github.com/rhboot/shim/blob/main/SBAT.md">Secure Boot Advanced Targeting</a> and if that's enough for you you can skip the rest you're welcome.</p><p>Long version: When UEFI Secure Boot was specified, everyone involved was, well, a touch naive. The basic security model of Secure Boot is that all the code that ends up running in a kernel-level privileged environment should be validated before execution - the firmware verifies the bootloader, the bootloader verifies the kernel, the kernel verifies any additional runtime loaded kernel code, and now we have a trusted environment to impose any other security policy we want. Obviously people might screw up, but the spec included a way to revoke any signed components that turned out not to be trustworthy: simply add the hash of the untrustworthy code to a variable, and then refuse to load anything with that hash even if it's signed with a trusted key.</p><p>Unfortunately, as it turns out, scale. Every Linux distribution that works in the Secure Boot ecosystem generates their own bootloader binaries, and each of them has a different hash. If there's a vulnerability identified in the source code for said bootloader, there's a large number of different binaries that need to be revoked. And, well, the storage available to store the variable containing all these hashes is limited. There's simply not enough space to add a new set of hashes every time it turns out that grub (a bootloader initially written for a simpler time when there was no boot security and which has several separate image parsers and also a font parser and look you know where this is going) has another mechanism for a hostile actor to cause it to execute arbitrary code, so another solution was needed.</p><p>And that solution is SBAT. The general concept behind SBAT is pretty straightforward. Every important component in the boot chain declares a security generation that's incorporated into the signed binary. When a vulnerability is identified and fixed, that generation is incremented. An update can then be pushed that defines a minimum generation - boot components will look at the next item in the chain, compare its name and generation number to the ones stored in a firmware variable, and decide whether or not to execute it based on that. Instead of having to revoke a large number of individual hashes, it becomes possible to push one update that simply says "Any version of grub with a security generation below this number is considered untrustworthy".</p><p>So why is this suddenly relevant? SBAT was developed collaboratively between the Linux community and Microsoft, and Microsoft chose to push a Windows update that told systems not to trust versions of grub with a security generation below a certain level. This was because those versions of grub had genuine security vulnerabilities that would allow an attacker to compromise the Windows secure boot chain, and we've seen real world examples of malware wanting to do that (<a href="https://media.defense.gov/2023/Jun/22/2003245723/-1/-1/0/CSI_BlackLotus_Mitigation_Guide.PDF">Black Lotus</a> did so using a vulnerability in the Windows bootloader, but a vulnerability in grub would be just as viable for this). Viewed purely from a security perspective, this was a legitimate thing to want to do.</p><p>(An aside: the "Something has gone seriously wrong" message that's associated with people having a bad time as a result of this update? That's a message from <a href="https://github.com/rhboot/shim/">shim</a>, not any Microsoft code. Shim pays attention to SBAT updates in order to avoid violating the security assumptions made by other bootloaders on the system, so even though it was Microsoft that pushed the SBAT update, it's the Linux bootloader that refuses to run old versions of grub as a result. This is absolutely working as intended)</p><p>The problem we've ended up in is that several Linux distributions had not shipped versions of grub with a newer security generation, and so those versions of grub are assumed to be insecure (it's worth noting that grub is signed by individual distributions, not Microsoft, so there's no externally introduced lag here). Microsoft's stated intention was that Windows Update would only apply the SBAT update to systems that were Windows-only, and any dual-boot setups would instead be left vulnerable to attack until the installed distro updated its grub and shipped an SBAT update itself. Unfortunately, as is now obvious, that didn't work as intended and at least some dual-boot setups applied the update and that distribution's Shim refused to boot that distribution's grub.</p><p>What's the summary? Microsoft (understandably) didn't want it to be possible to attack Windows by using a vulnerable version of grub that could be tricked into executing arbitrary code and then introduce a bootkit into the Windows kernel during boot. Microsoft did this by pushing a Windows Update that updated the SBAT variable to indicate that known-vulnerable versions of grub shouldn't be allowed to boot on those systems. The distribution-provided Shim first-stage bootloader read this variable, read the SBAT section from the installed copy of grub, realised these conflicted, and refused to boot grub with the "Something has gone seriously wrong" message. This update was not supposed to apply to dual-boot systems, but did anyway. Basically:</p><p>1) Microsoft applied an update to systems where that update shouldn't have been applied<br>2) Some Linux distros failed to update their grub code and SBAT security generation when exploitable security vulnerabilities were identified in grub</p><p>The outcome is that some people can't boot their systems. I think there's plenty of blame here. Microsoft should have done more testing to ensure that dual-boot setups could be identified accurately. But also distributions shipping signed bootloaders should make sure that they're updating those and updating the security generation to match, because otherwise they're shipping a vector that can be used to attack other operating systems and that's kind of a violation of the social contract around all of this.</p><p>It's unfortunate that the victims here are largely end users faced with a system that suddenly refuses to boot the OS they want to boot. That should never happen. I don't think asking arbitrary end users whether they want secure boot updates is likely to result in good outcomes, and while I vaguely tend towards UEFI Secure Boot not being something that benefits most end users it's also a thing you really don't want to discover you want after the fact so I have sympathy for it being default on, so I do sympathise with Microsoft's choices here, other than the failed attempt to avoid the update on dual boot systems.</p><p>Anyway. I was extremely involved in the implementation of this for Linux back in 2012 and wrote the first prototype of Shim (which is now a massively better bootloader maintained by a wider set of people and that I haven't touched in years), so if you want to blame an individual please do feel free to blame me. This is something that shouldn't have happened, and unless you're either Microsoft or a Linux distribution it's not your fault. I'm sorry.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A Ghidra extension for exporting parts of a program as object files (230 pts)]]></title>
            <link>https://github.com/boricj/ghidra-delinker-extension</link>
            <guid>41318133</guid>
            <pubDate>Thu, 22 Aug 2024 08:54:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/boricj/ghidra-delinker-extension">https://github.com/boricj/ghidra-delinker-extension</a>, See on <a href="https://news.ycombinator.com/item?id=41318133">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Object file exporter extension for Ghidra</h2><a id="user-content-object-file-exporter-extension-for-ghidra" aria-label="Permalink: Object file exporter extension for Ghidra" href="#object-file-exporter-extension-for-ghidra"></a></p>
<p dir="auto">This Ghidra extension enables exporting parts of a program as object files. These object files have valid metadata (symbols, relocation tables…) and as such can be reused directly by a toolchain for further processing.</p>
<p dir="auto">Use-cases include:</p>
<ul dir="auto">
<li><a href="https://boricj.net/tenchu1/2024/05/31/part-11.html" rel="nofollow">Advanced binary patching</a>, by leveraging the linker to mend both original and modified parts together instead of doing this work by hand&nbsp;;</li>
<li><a href="https://boricj.net/atari-jaguar-sdk/2024/01/02/part-5.html" rel="nofollow">Software ports</a>, by isolating system-independent code from a program and replacing the rest&nbsp;;</li>
<li>Converting <a href="https://boricj.net/atari-jaguar-sdk/2023/12/18/part-3.html" rel="nofollow">programs</a> or object files from one file format to another&nbsp;;</li>
<li><a href="https://boricj.net/tenchu1/2024/03/11/part-5.html" rel="nofollow">Creating</a> <a href="https://boricj.net/tenchu1/2024/03/18/part-6.html" rel="nofollow">libraries</a>, by extracting parts of a program and reusing them in another context&nbsp;;</li>
<li>Decompilation projects, by splitting a program into multiple object files and reimplementing these <em>Ship of Theseus</em>-style&nbsp;;</li>
<li>…</li>
</ul>
<p dir="auto">Matrix of supported instruction set architectures and object files:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th>x86</th>
<th>MIPS</th>
</tr>
</thead>
<tbody>
<tr>
<td>COFF</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr>
<td>ELF</td>
<td>✅</td>
<td>✅</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building (CLI)</h2><a id="user-content-building-cli" aria-label="Permalink: Building (CLI)" href="#building-cli"></a></p>
<ul dir="auto">
<li>Clone this repository&nbsp;;</li>
<li>Define the <code>GHIDRA_INSTALL_DIR</code> environment variable to point to your Ghidra installation directory&nbsp;;</li>
<li>Run <code>gradle buildExtension</code>.</li>
</ul>
<p dir="auto">The Ghidra extension archive will be created inside the <code>dist/</code> directory.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<ul dir="auto">
<li>Download the extension from the <a href="https://github.com/boricj/ghidra-delinker-extension/releases">releases page</a> or build it locally&nbsp;;</li>
<li>Install the extension in your Ghidra instance with <code>File &gt; Install Extensions…</code>&nbsp;;</li>
<li>Enable the <code>RelocationTableSynthesizedPlugin</code> plugin with <code>File &gt; Configure</code> inside a CodeBrowser window.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<ol dir="auto">
<li>Select a set of addresses in the Listing view&nbsp;;</li>
<li>Run the <code>Relocation table synthesizer</code> analyzer (available in one-shot mode)&nbsp;;</li>
<li>Invoke a relocatable object file exporter with <code>File &gt; Export Program…</code></li>
</ol>
<p dir="auto">The reconstructed relocations can be viewed with <code>Window &gt; Relocation table (synthesized)</code>.</p>
<ul dir="auto">
<li><g-emoji alias="warning">⚠️</g-emoji> The <em>relocation table synthesizer</em> analyzer relies on a fully populated Ghidra database (with correctly declared symbols, data types and references) in order to work. <strong>Incorrect or missing information may lead to broken or undiscovered relocations</strong> during the analysis.</li>
<li><g-emoji alias="warning">⚠️</g-emoji> The object file exporters rely on the results of the <em>relocation table synthesizer</em> analyzer in order to work. When in doubt, <strong>run this analyzer right before exporting an object file</strong> to make sure the relocation table contents are up-to-date with the current state of the program.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">How does it work?</h2><a id="user-content-how-does-it-work" aria-label="Permalink: How does it work?" href="#how-does-it-work"></a></p>
<p dir="auto">Object files are made of three parts:</p>
<ul dir="auto">
<li>Relocatable section bytes&nbsp;;</li>
<li>A symbol table&nbsp;;</li>
<li>A relocation table.</li>
</ul>
<p dir="auto">When a linker is invoked to generate an executable from a bunch of object files, it will:</p>
<ul dir="auto">
<li>Lay out their sections in memory&nbsp;;</li>
<li>Compute the addresses of the symbols in the virtual address space&nbsp;;</li>
<li>Apply the relocations based on the final addresses of the symbols onto the section bytes.</li>
</ul>
<p dir="auto">Normally the relocation table is discarded after this process, as well as the symbol table if debugging symbols aren't kept, leaving only the un-relocatable section bytes.
However, through careful analysis this data can be recreated, which allows us to then effectively <em>delink</em> the program back into object files.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hardware Virtualization (160 pts)]]></title>
            <link>https://www.haiku-os.org/blog/dalme/2024-08-19_gsoc_2024_hardware_virtualization_final_report/</link>
            <guid>41318033</guid>
            <pubDate>Thu, 22 Aug 2024 08:33:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.haiku-os.org/blog/dalme/2024-08-19_gsoc_2024_hardware_virtualization_final_report/">https://www.haiku-os.org/blog/dalme/2024-08-19_gsoc_2024_hardware_virtualization_final_report/</a>, See on <a href="https://news.ycombinator.com/item?id=41318033">Hacker News</a></p>
Couldn't get https://www.haiku-os.org/blog/dalme/2024-08-19_gsoc_2024_hardware_virtualization_final_report/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Ethernet History Deepdive – Why Do We Have Different Frame Types? (130 pts)]]></title>
            <link>https://lostintransit.se/2024/08/21/ethernet-history-deepdive-why-do-we-have-different-frame-types/</link>
            <guid>41318013</guid>
            <pubDate>Thu, 22 Aug 2024 08:29:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lostintransit.se/2024/08/21/ethernet-history-deepdive-why-do-we-have-different-frame-types/">https://lostintransit.se/2024/08/21/ethernet-history-deepdive-why-do-we-have-different-frame-types/</a>, See on <a href="https://news.ycombinator.com/item?id=41318013">Hacker News</a></p>
Couldn't get https://lostintransit.se/2024/08/21/ethernet-history-deepdive-why-do-we-have-different-frame-types/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[SwiftUI for Mac 2024 (121 pts)]]></title>
            <link>https://troz.net/post/2024/swiftui-mac-2024/</link>
            <guid>41318000</guid>
            <pubDate>Thu, 22 Aug 2024 08:27:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://troz.net/post/2024/swiftui-mac-2024/">https://troz.net/post/2024/swiftui-mac-2024/</a>, See on <a href="https://news.ycombinator.com/item?id=41318000">Hacker News</a></p>
Couldn't get https://troz.net/post/2024/swiftui-mac-2024/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Isaiah – open-source and self-hosted app to manage everything Docker (175 pts)]]></title>
            <link>https://github.com/will-moss/isaiah</link>
            <guid>41317988</guid>
            <pubDate>Thu, 22 Aug 2024 08:25:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/will-moss/isaiah">https://github.com/will-moss/isaiah</a>, See on <a href="https://news.ycombinator.com/item?id=41317988">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Isaiah</h2><a id="user-content-isaiah" aria-label="Permalink: Isaiah" href="#isaiah"></a></p>
    <p dir="auto">
      Self-hostable clone of lazydocker for the web.<br>Manage your Docker fleet with ease
    </p>
    <p dir="auto">
      <a href="#table-of-contents">Table of Contents</a> -
      <a href="#deployment-and-examples">Install</a> -
      <a href="#configuration">Configure</a>
    </p>

<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/isaiah/blob/master/assets/CAPTURE-1.png"><img width="1604" src="https://github.com/will-moss/isaiah/raw/master/assets/CAPTURE-1.png"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/isaiah/blob/master/assets/CAPTURE-2.png"><img width="1604" src="https://github.com/will-moss/isaiah/raw/master/assets/CAPTURE-2.png"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/isaiah/blob/master/assets/CAPTURE-3.png"><img width="1604" src="https://github.com/will-moss/isaiah/raw/master/assets/CAPTURE-3.png"></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/isaiah/blob/master/assets/CAPTURE-4.png"><img width="1604" src="https://github.com/will-moss/isaiah/raw/master/assets/CAPTURE-4.png"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/isaiah/blob/master/assets/CAPTURE-5.png"><img width="1604" src="https://github.com/will-moss/isaiah/raw/master/assets/CAPTURE-5.png"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/isaiah/blob/master/assets/CAPTURE-6.png"><img width="1604" src="https://github.com/will-moss/isaiah/raw/master/assets/CAPTURE-6.png"></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/isaiah/blob/master/assets/CAPTURE-7.png"><img width="1604" src="https://github.com/will-moss/isaiah/raw/master/assets/CAPTURE-7.png"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/isaiah/blob/master/assets/CAPTURE-8.png"><img width="1604" src="https://github.com/will-moss/isaiah/raw/master/assets/CAPTURE-8.png"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/isaiah/blob/master/assets/CAPTURE-9.png"><img width="1604" src="https://github.com/will-moss/isaiah/raw/master/assets/CAPTURE-9.png"></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/isaiah/blob/master/assets/CAPTURE-10.png"><img width="1604" src="https://github.com/will-moss/isaiah/raw/master/assets/CAPTURE-10.png"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/isaiah/blob/master/assets/CAPTURE-11.png"><img width="1604" src="https://github.com/will-moss/isaiah/raw/master/assets/CAPTURE-11.png"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/isaiah/blob/master/assets/CAPTURE-12.png"><img width="1604" src="https://github.com/will-moss/isaiah/raw/master/assets/CAPTURE-12.png"></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/isaiah/blob/master/assets/CAPTURE-13.png"><img width="1604" src="https://github.com/will-moss/isaiah/raw/master/assets/CAPTURE-13.png"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/isaiah/blob/master/assets/CAPTURE-14.png"><img width="1604" src="https://github.com/will-moss/isaiah/raw/master/assets/CAPTURE-14.png"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/will-moss/isaiah/blob/master/assets/CAPTURE-15.png"><img width="1604" src="https://github.com/will-moss/isaiah/raw/master/assets/CAPTURE-15.png"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#deployment-and-examples">Deployment and Examples</a>
<ul dir="auto">
<li><a href="#deploy-with-docker">Deploy with Docker</a></li>
<li><a href="#deploy-with-docker-compose">Deploy with Docker Compose</a></li>
<li><a href="#deploy-as-a-standalone-application">Deploy as a standalone application</a>
<ul dir="auto">
<li><a href="#using-an-existing-binary">Using an existing binary</a></li>
<li><a href="#building-the-binary-manually">Building the binary manually</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#multi-node-deployment">Multi-node deployment</a>
<ul dir="auto">
<li><a href="#general-information">General information</a></li>
<li><a href="#setup">Setup</a></li>
</ul>
</li>
<li><a href="#multi-host-deployment">Multi-host deployment</a>
<ul dir="auto">
<li><a href="#general-information-1">General information</a></li>
<li><a href="#setup-1">Setup</a></li>
</ul>
</li>
<li><a href="#forward-proxy-authentication--trusted-sso">Forward Proxy Authentication / Trusted SSO</a></li>
<li><a href="#configuration">Configuration</a></li>
<li><a href="#theming">Theming</a></li>
<li><a href="#troubleshoot">Troubleshoot</a></li>
<li><a href="#security">Security</a></li>
<li><a href="#disclaimer">Disclaimer</a></li>
<li><a href="#contribute">Contribute</a></li>
<li><a href="#credits">Credits</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">Isaiah is a self-hostable service that enables you to manage all your Docker resources on a remote server. It is an attempt at recreating the <a href="https://github.com/jesseduffield/lazydocker">lazydocker</a> command-line application from scratch, while making it available as a web application without compromising on the features.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto">Isaiah has all these features implemented :</p>
<ul dir="auto">
<li>For stacks :
<ul dir="auto">
<li>Bulk update</li>
<li>Up, Down, Pause, Unpause, Restart, Update</li>
<li>Create and Edit stacks using <code>docker-compose.yml</code> files in your browser</li>
<li>Inspect (live logs, <code>docker-compose.yml</code>, services)</li>
</ul>
</li>
<li>For containers :
<ul dir="auto">
<li>Bulk stop, Bulk remove, Prune</li>
<li>Remove, Pause, Unpause, Restart, Rename, Update, Edit, Open in browser</li>
<li>Open a shell inside the container (from your browser)</li>
<li>Inspect (live logs, stats, env, full configuration, top)</li>
</ul>
</li>
<li>For images :
<ul dir="auto">
<li>Prune</li>
<li>Remove</li>
<li>Run (create and start a container using the image)</li>
<li>Open on Docker Hub</li>
<li>Pull a new image (from Docker Hub)</li>
<li>Bulk pull all latest images (from Docker Hub)</li>
<li>Inspect (full configuration, layers)</li>
</ul>
</li>
<li>For volumes :
<ul dir="auto">
<li>Prune</li>
<li>Remove</li>
<li>Browse volume files (from your browser, via shell)</li>
<li>Inspect (full configuration)</li>
</ul>
</li>
<li>For networks :
<ul dir="auto">
<li>Prune</li>
<li>Remove</li>
<li>Inspect (full configuration)</li>
</ul>
</li>
<li>Built-in automatic Docker host discovery</li>
<li>Built-in authentication by master password (supplied raw or sha256-hashed)</li>
<li>Built-in authentication by forward proxy authentication headers (e.g. Authelia / Trusted SSO)</li>
<li>Built-in terminal emulator (with support for opening a shell on the server)</li>
<li>Responsive for Desktop, Tablet, and Mobile</li>
<li>Support for multiple layouts</li>
<li>Support for custom CSS theming (with variables for colors already defined)</li>
<li>Support for keyboard navigation</li>
<li>Support for mouse navigation</li>
<li>Support for search through Docker resources and container logs</li>
<li>Support for ascending and descending sort by any supported field</li>
<li>Support for customizable user settings (line-wrap, timestamps, prompt, etc.)</li>
<li>Support for custom Docker Host / Context.</li>
<li>Support for extensive configuration with <code>.env</code></li>
<li>Support for HTTP and HTTPS</li>
<li>Support for standalone / proxy / multi-node / multi-host deployment</li>
</ul>
<p dir="auto">On top of these, one may appreciate the following characteristics :</p>
<ul dir="auto">
<li>Written in Go (for the server) and Vanilla JS (for the client)</li>
<li>Holds in a ~4 MB single file executable</li>
<li>Holds in a ~4 MB Docker image</li>
<li>Works exclusively over Websocket, with very little bandwidth usage</li>
<li>Uses the official Docker SDK for 100% of the Docker features</li>
</ul>
<p dir="auto">For more information, read about <a href="#configuration">Configuration</a> and <a href="#deployment-and-examples">Deployment</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Deployment and Examples</h2><a id="user-content-deployment-and-examples" aria-label="Permalink: Deployment and Examples" href="#deployment-and-examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deploy with Docker</h3><a id="user-content-deploy-with-docker" aria-label="Permalink: Deploy with Docker" href="#deploy-with-docker"></a></p>
<p dir="auto">You can run Isaiah with Docker on the command line very quickly.</p>
<p dir="auto">You can use the following commands :</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Create a .env file
touch .env

# Edit .env file ...

# Option 1 : Run Isaiah attached to the terminal (useful for debugging)
docker run \
  --env-file .env \
  -v /var/run/docker.sock:/var/run/docker.sock:ro \
  -p <YOUR-PORT-MAPPING> \
  mosswill/isaiah

# Option 2 : Run Isaiah as a daemon
docker run \
  -d \
  --env-file .env \
  -v /var/run/docker.sock:/var/run/docker.sock:ro \
  -p <YOUR-PORT-MAPPING> \
  mosswill/isaiah

# Option 3 : Quick run with default settings
docker run -v /var/run/docker.sock:/var/run/docker.sock:ro -p 3000:3000 mosswill/isaiah"><pre><span><span>#</span> Create a .env file</span>
touch .env

<span><span>#</span> Edit .env file ...</span>

<span><span>#</span> Option 1 : Run Isaiah attached to the terminal (useful for debugging)</span>
docker run \
  --env-file .env \
  -v /var/run/docker.sock:/var/run/docker.sock:ro \
  -p <span>&lt;</span>YOUR-PORT-MAPPING<span>&gt;</span> \
  mosswill/isaiah

<span><span>#</span> Option 2 : Run Isaiah as a daemon</span>
docker run \
  -d \
  --env-file .env \
  -v /var/run/docker.sock:/var/run/docker.sock:ro \
  -p <span>&lt;</span>YOUR-PORT-MAPPING<span>&gt;</span> \
  mosswill/isaiah

<span><span>#</span> Option 3 : Quick run with default settings</span>
docker run -v /var/run/docker.sock:/var/run/docker.sock:ro -p 3000:3000 mosswill/isaiah</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deploy with Docker Compose</h3><a id="user-content-deploy-with-docker-compose" aria-label="Permalink: Deploy with Docker Compose" href="#deploy-with-docker-compose"></a></p>
<p dir="auto">To help you get started quickly, multiple example <code>docker-compose</code> files are located in the <a href="https://github.com/will-moss/isaiah/blob/master/examples">"examples/"</a> directory.</p>
<p dir="auto">Here's a description of every example :</p>
<ul dir="auto">
<li>
<p dir="auto"><code>docker-compose.simple.yml</code>: Run Isaiah as a front-facing service on port 80., with environment variables supplied in the <code>docker-compose</code> file directly.</p>
</li>
<li>
<p dir="auto"><code>docker-compose.volume.yml</code>: Run Isaiah as a front-facing service on port 80, with environment variables supplied as a <code>.env</code> file mounted as a volume.</p>
</li>
<li>
<p dir="auto"><code>docker-compose.ssl.yml</code>:  Run Isaiah as a front-facing service on port 443, listening for HTTPS requests, with certificate and private key provided as mounted volumes.</p>
</li>
<li>
<p dir="auto"><code>docker-compose.proxy.yml</code>: A full setup with Isaiah running on port 80, behind a proxy listening on port 443.</p>
</li>
<li>
<p dir="auto"><code>docker-compose.traefik.yml</code>: A sample setup with Isaiah running on port 80, behind a Traefik proxy listening on port 443.</p>
</li>
<li>
<p dir="auto"><code>docker-compose.agent.yml</code>: A sample setup with Isaiah operating as an Agent in a multi-node deployment.</p>
</li>
<li>
<p dir="auto"><code>docker-compose.host.yml</code>: A sample setup with Isaiah expecting to communicate with other hosts in a multi-host deployment.</p>
</li>
</ul>
<p dir="auto">When your <code>docker-compose</code> file is on point, you can use the following commands :</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Option 1 : Run Isaiah in the current terminal (useful for debugging)
docker-compose up

# Option 2 : Run Isaiah in a detached terminal (most common)
docker-compose up -d

# Show the logs written by Isaiah (useful for debugging)
docker logs <NAME-OF-YOUR-CONTAINER>"><pre><span><span>#</span> Option 1 : Run Isaiah in the current terminal (useful for debugging)</span>
docker-compose up

<span><span>#</span> Option 2 : Run Isaiah in a detached terminal (most common)</span>
docker-compose up -d

<span><span>#</span> Show the logs written by Isaiah (useful for debugging)</span>
docker logs <span>&lt;</span>NAME-OF-YOUR-CONTAINER<span>&gt;</span></pre></div>
<blockquote>
<p dir="auto">Warning : Always make sure that your Docker Unix socket is mounted, else Isaiah won't be able to communicate with the Docker API.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deploy as a standalone application</h3><a id="user-content-deploy-as-a-standalone-application" aria-label="Permalink: Deploy as a standalone application" href="#deploy-as-a-standalone-application"></a></p>
<p dir="auto">You can deploy Isaiah as a standalone application, either by downloading an existing binary that fits your architecture,
or by building the binary yourself on your machine.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Using an existing binary</h4><a id="user-content-using-an-existing-binary" aria-label="Permalink: Using an existing binary" href="#using-an-existing-binary"></a></p>
<p dir="auto">An install script was created to help you install Isaiah in one line, from your terminal :</p>
<blockquote>
<p dir="auto">As always, check the content of every file you pipe in bash</p>
</blockquote>
<div dir="auto" data-snippet-clipboard-copy-content="curl https://raw.githubusercontent.com/will-moss/isaiah/master/scripts/remote-install.sh | bash"><pre>curl https://raw.githubusercontent.com/will-moss/isaiah/master/scripts/remote-install.sh <span>|</span> bash</pre></div>
<p dir="auto">This script will try to automatically download a binary that matches your operating system and architecture, and put it
in your <code>/usr/[local/]bin/</code> directory to ease running it. Later on, you can run :</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Create a new .env file
touch .env

# Edit .env file ...

# Run Isaiah
isaiah"><pre><span><span>#</span> Create a new .env file</span>
touch .env

<span><span>#</span> Edit .env file ...</span>

<span><span>#</span> Run Isaiah</span>
isaiah</pre></div>
<p dir="auto">In case you feel uncomfortable running the install script, you can head to the <code>Releases</code>, find the binary that meets your system, and install it yourself.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Building the binary manually</h4><a id="user-content-building-the-binary-manually" aria-label="Permalink: Building the binary manually" href="#building-the-binary-manually"></a></p>
<p dir="auto">In this case, make sure that your system meets the following requirements :</p>
<ul dir="auto">
<li>You have Go 1.21 installed</li>
<li>You have Node 20+ installed along with npm and npx</li>
</ul>
<p dir="auto">When all the prerequisites are met, you can run the following commands in your terminal :</p>
<blockquote>
<p dir="auto">As always, check the content of everything you run inside your terminal</p>
</blockquote>
<div dir="auto" data-snippet-clipboard-copy-content="# Retrieve the code
git clone https://github.com/will-moss/isaiah
cd isaiah

# Run the local install script
./scripts/local-install.sh

# Move anywhere else, and create a dedicated directory
cd ~
mkdir isaiah-config
cd isaiah-config

# Create a new .env file
touch .env

# Edit .env file ...

# Option 1 : Run Isaiah in the current terminal
isaiah

# Option 2 : Run Isaiah as a background process
isaiah &amp;

# Option 3 : Run Isaiah using screen
screen -S isaiah
isaiah
<CTRL+A> <D>

# Optional : Remove the cloned repository
# cd <back to the cloned repository>
# rm -rf ./isaiah"><pre><span><span>#</span> Retrieve the code</span>
git clone https://github.com/will-moss/isaiah
<span>cd</span> isaiah

<span><span>#</span> Run the local install script</span>
./scripts/local-install.sh

<span><span>#</span> Move anywhere else, and create a dedicated directory</span>
<span>cd</span> <span>~</span>
mkdir isaiah-config
<span>cd</span> isaiah-config

<span><span>#</span> Create a new .env file</span>
touch .env

<span><span>#</span> Edit .env file ...</span>

<span><span>#</span> Option 1 : Run Isaiah in the current terminal</span>
isaiah

<span><span>#</span> Option 2 : Run Isaiah as a background process</span>
isaiah <span>&amp;</span>

<span><span>#</span> Option 3 : Run Isaiah using screen</span>
screen -S isaiah
isaiah
<span>&lt;</span>CTRL+A<span>&gt;</span> <span>&lt;</span>D<span>&gt;</span>

<span><span>#</span> Optional : Remove the cloned repository</span>
<span><span>#</span> cd &lt;back to the cloned repository&gt;</span>
<span><span>#</span> rm -rf ./isaiah</span></pre></div>
<p dir="auto">The local install script will try to perform a production build on your machine, and move <code>isaiah</code> to your <code>/usr/[local/]bin/</code> directory
to ease running it. In more details, the following actions are performed :</p>
<ul dir="auto">
<li>Local install of Babel, LightningCSS, Less, and Terser</li>
<li>Prefixing, Transpilation, and Minification of CSS and JS assets</li>
<li>Building of the Go source code into a single-file executable (with CSS and JS embed)</li>
<li>Cleaning of the artifacts generated during the previous steps</li>
<li>Removal of the previous <code>isaiah</code> executable, if any in <code>/usr/[local/]bin/</code></li>
<li>Moving the new <code>isaiah</code> executable in <code>/usr/[local/]bin</code> with <code>755</code> permissions.</li>
</ul>
<p dir="auto">If you encounter any issue during this process, please feel free to tweak the install script or reach out by opening an issue.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Multi-node deployment</h2><a id="user-content-multi-node-deployment" aria-label="Permalink: Multi-node deployment" href="#multi-node-deployment"></a></p>
<p dir="auto">Using Isaiah, you can manage multiple nodes with their own distinct Docker resources from a single dashboard.</p>
<p dir="auto">Before delving into that part, please get familiar with the general information below.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">General information</h3><a id="user-content-general-information" aria-label="Permalink: General information" href="#general-information"></a></p>
<p dir="auto">You may find these information useful during your setup and reading :</p>
<ul dir="auto">
<li>Isaiah distinguishes two types of nodes : <code>Master</code> and <code>Agent</code>.</li>
<li>The word <code>node</code> refers to any machine (virtual or not) holding Docker resources.</li>
<li>The <code>Master</code> node has three responsabilities :
<ul dir="auto">
<li>Serving the web interface.</li>
<li>Managing the Docker resources inside the environment on which it is already installed.</li>
<li>Acting as a central proxy between the client (you) and the remote Agent nodes.</li>
</ul>
</li>
<li>The <code>Master</code> node has the following characteristics :
<ul dir="auto">
<li>There should be only one Master node in a multi-node deployment.</li>
<li>The Master node should be the only part of your deployment that is publicly exposed on the network.</li>
</ul>
</li>
<li>The <code>Agent</code> nodes have the following characteristics :
<ul dir="auto">
<li>They are headless instances of Isaiah, and they can't exist without a Master node.</li>
<li>As with the Master node, they have their own authentication if you don't disable it explicitly.</li>
<li>On startup, they perform registration with their Master node using as a Websocket client</li>
<li>For as long as the Master node is alive, a Websocket connection remains established between them.</li>
<li>The Agent node should never be publicly exposed on the network.</li>
<li>The Agent node never communicates with the client (you). Everything remains between the nodes.</li>
<li>There is no limit to how many Agent nodes can connect to a Master node.</li>
</ul>
</li>
</ul>
<p dir="auto">In other words, one <code>Master</code> acts as a <code>Proxy</code> between the <code>Client</code> and the <code>Agents</code>.<br>
For example, when a <code>Client</code> wants to stop a Docker container inside an <code>Agent</code>, the <code>Client</code> first requests it from <code>Master</code>.
Then, <code>Master</code> forwards it to the designated <code>Agent</code>.
When the <code>Agent</code> has finished, they reply to <code>Master</code>, and <code>Master</code> forwards that response to the initial <code>Client</code>.</p>
<p dir="auto">Schematically, it looks like this :</p>
<ul dir="auto">
<li>Client ------------&gt; Master : Stop container C-123 on Agent AG-777</li>
<li>Master ------------&gt; Agent  : Stop container C-123</li>
<li>Agent  ------------&gt; Master : Container C-123 was stopped</li>
<li>Master ------------&gt; Client : Container C-123 was stopped on Agent AG-777</li>
</ul>
<p dir="auto">Now that we understand how everything works, let's see how to set up a multi-node deployment.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Setup</h3><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto">First, please ensure the following :</p>
<ul dir="auto">
<li>Your <code>Master</code> node is running, exposed on the network, and available in your web browser</li>
<li>Your <code>Agent</code> node has Isaiah installed and configured with the following settings :
<ul dir="auto">
<li><code>SERVER_ROLE</code> equal to <code>Agent</code></li>
<li><code>MASTER_HOST</code> configured to reach the <code>Master</code> node</li>
<li><code>MASTER_SECRET</code> equal to the <code>AUTHENTICATION_SECRET</code> setting on the <code>Master</code> node, or empty when authentication is disabled</li>
<li><code>AGENT_NAME</code> equal to a unique string of your choice</li>
</ul>
</li>
</ul>
<p dir="auto">Then, launch Isaiah on each <code>Agent</code> node, and you should see logs indicating whether connection with <code>Master</code> was established. Eventually, you will see <code>Master</code> or <code>The name of your agent</code> in the lower right corner of your screen as agents register.</p>
<p dir="auto">If encounter any issue, please read the <a href="#troubleshoot">Troubleshoot</a> section.</p>
<blockquote>
<p dir="auto">You may want to note that you don't need to expose ports on the machine / Docker container running Isaiah when it is configured as an Agent.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Multi-host deployment</h2><a id="user-content-multi-host-deployment" aria-label="Permalink: Multi-host deployment" href="#multi-host-deployment"></a></p>
<p dir="auto">Using Isaiah, you can manage multiple hosts with their own distinct Docker resources from a single dashboard.</p>
<p dir="auto">Before delving into that part, please get familiar with the general information below.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">General information</h3><a id="user-content-general-information-1" aria-label="Permalink: General information" href="#general-information-1"></a></p>
<p dir="auto">The big difference between multi-node and multi-host deployments is that you won't need to install Isaiah on every single node
if you are using multi-host. In this setup, Isaiah is installed only on one server, and communicates with other Docker hosts
directly over TCP / Unix sockets. It makes it easier to manage multiple remote Docker environments without having to setup Isaiah
on all of them.</p>
<p dir="auto">Please note that, in a multi-host setup, there must be a direct access between the main host (where Isaiah is running)
and the other ones. Usually, they should be on the same network, or visible through a secured gateway / VPN / filesystem mount.</p>
<p dir="auto">Let's see how to set up a multi-host deployment.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Setup</h3><a id="user-content-setup-1" aria-label="Permalink: Setup" href="#setup-1"></a></p>
<p dir="auto">In order to help you get started, a <a href="https://github.com/will-moss/isaiah/blob/master/app/sample.docker_hosts">sample file</a> was created.</p>
<p dir="auto">First, please ensure the following :</p>
<ul dir="auto">
<li>Your <code>Master</code> host is running, exposed on the network, and available in your web browser</li>
<li>Your <code>Master</code> host has the setting <code>MULTI_HOST_ENABLED</code> set to <code>true</code>.</li>
<li>Your <code>Master</code> host has access to the other Docker hosts over TCP / Unix socket.</li>
</ul>
<p dir="auto">Second, please create a <code>docker_hosts</code> file next to Isaiah's executable, using the sample file cited above:</p>
<ul dir="auto">
<li>Every line should contain two strings separated by a single space.</li>
<li>The first string is the name of your host, and the second string is the path to reach it.</li>
<li>The path to your host should look like this : [PROTOCOL]://[URI]</li>
<li>Example 1 : Local unix:///var/run/docker.sock</li>
<li>Example 2 : Remote tcp://my-domain.tld:4382</li>
</ul>
<blockquote>
<p dir="auto">If you're using Docker, you can mount the file at the root of the filesystem, as in :<br>
<code>docker ... -v my_docker_hosts:/docker_hosts ...</code></p>
</blockquote>
<p dir="auto">Finally, launch Isaiah on the Master host, and you should see logs indicating whether connection with remote hosts was established.
Eventually, you will see <code>Master</code> with <code>The name of your host</code> in the lower right corner of your screen.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Forward Proxy Authentication / Trusted SSO</h2><a id="user-content-forward-proxy-authentication--trusted-sso" aria-label="Permalink: Forward Proxy Authentication / Trusted SSO" href="#forward-proxy-authentication--trusted-sso"></a></p>
<p dir="auto">If you wish to deploy Isaiah behind a secure proxy or authentication portal, you must configure Forward Proxy Authentication.</p>
<p dir="auto">This will enable you to :</p>
<ul dir="auto">
<li>Log in, once and for all, into your authentication portal.</li>
<li>Connect to Isaiah without having to type your <code>AUTHENTICATION_SECRET</code> every time.</li>
<li>Protect Isaiah using your authentication proxy rather than the current mechanism (cleartext / hashed password).</li>
<li>Manage the access to Isaiah from your authentication portal rather than through your <code>.env</code> configuration.</li>
</ul>
<p dir="auto">Before proceeding, please ensure the following :</p>
<ul dir="auto">
<li>Your proxy supports HTTP/2 and Websockets.</li>
<li>Your proxy can communicate with Isaiah on the network.</li>
<li>Your proxy forwards authentication headers to Isaiah (but not to the browser).</li>
</ul>
<blockquote>
  <br>
  For example, if you're using Nginx Proxy Manager (NPM), you should do the following :
  <ul dir="auto">
    <li>In the tab "Details"</li>
    <ul dir="auto">
      <li>Tick the box "Websockets support"</li>
      <li>Tick the box "HTTP/2 support"</li>
      <li>Tick the box "Block common exploits"</li>
      <li>Tick the box "Force SSL"</li>
   </ul>
   <br>
   <li>In the tab "Advanced"</li>
   <ul dir="auto">
      <li>In your custom location block, add the lines :</li>
      <ul dir="auto">
        <li>proxy_set_header Upgrade $http_upgrade;</li>
        <li>proxy_set_header Connection "upgrade";</li>
      </ul>
    </ul>
  </ul>
  <br>
</blockquote>
<p dir="auto">Then, configure Isaiah using the following variables :</p>
<ul dir="auto">
<li>Set <code>FORWARD_PROXY_AUTHENTICATION_ENABLED</code> to <code>true</code>.</li>
<li>Set <code>FORWARD_PROXY_AUTHENTICATION_HEADER_KEY</code> to the name of the forwarded authentication header your proxy sends to Isaiah.</li>
<li>Set <code>FORWARD_PROXY_AUTHENTICATION_HEADER_VALUE</code> to the value of the header that Isaiah should expect (or use <code>*</code> if all values are accepted).</li>
</ul>
<blockquote>
<p dir="auto">By default, Isaiah is configured to work with Authelia out of the box. Hence, you can just set <code>FORWARD_PROXY_AUTHENTICATION_ENABLED</code> to <code>true</code> and be done with it.</p>
</blockquote>
<p dir="auto">If everything was properly set up, you will encounter the following flow :</p>
<ul dir="auto">
<li>Navigate to <code>isaiah.your-domain.tld</code>.</li>
<li>Get redirected to <code>authentication-portal.your-domain.tld</code>.</li>
<li>Fill in your credentials.</li>
<li>Authentication was successful.</li>
<li>Get redirected to <code>isaiah.your-domain.tld</code>.</li>
<li>Isaiah <strong>does not</strong> prompt you for the password, you're automatically logged in.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">To run Isaiah, you will need to set the following environment variables in a <code>.env</code> file located next to your executable :</p>
<blockquote>
<p dir="auto"><strong>Note :</strong> Regular environment variables provided on the commandline work too</p>
</blockquote>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Parameter</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>SSL_ENABLED</code></td>
<td><code>boolean</code></td>
<td>Whether HTTPS should be used in place of HTTP. When configured, Isaiah will look for <code>certificate.pem</code> and <code>key.pem</code> next to the executable for configuring SSL. Note that if Isaiah is behind a proxy that already handles SSL, this should be set to <code>false</code>.</td>
<td>False</td>
</tr>
<tr>
<td><code>SERVER_PORT</code></td>
<td><code>integer</code></td>
<td>The port Isaiah listens on.</td>
<td>3000</td>
</tr>
<tr>
<td><code>SERVER_MAX_READ_SIZE</code></td>
<td><code>integer</code></td>
<td>The maximum size (in bytes) per message that Isaiah will accept over Websocket. Note that, in a multi-node deployment, you may need to incrase the value of that setting. (Shouldn't be modified, unless your server randomly restarts the Websocket session for no obvious reason)</td>
<td>100000</td>
</tr>
<tr>
<td><code>AUTHENTICATION_ENABLED</code></td>
<td><code>boolean</code></td>
<td>Whether a password is required to access Isaiah. (Recommended)</td>
<td>True</td>
</tr>
<tr>
<td><code>AUTHENTICATION_SECRET</code></td>
<td><code>string</code></td>
<td>The master password used to secure your Isaiah instance against malicious actors.</td>
<td>one-very-long-and-mysterious-secret</td>
</tr>
<tr>
<td><code>AUTHENTICATION_HASH</code></td>
<td><code>string</code></td>
<td>The master password's hash (sha256 format) used to secure your Isaiah instance against malicious actors. Use this setting instead of <code>AUTHENTICATION_SECRET</code> if you feel uncomfortable providing a cleartext password.</td>
<td>Empty</td>
</tr>
<tr>
<td><code>DISPLAY_CONFIRMATIONS</code></td>
<td><code>boolean</code></td>
<td>Whether the web interface should display a confirmation message after every succesful operation.</td>
<td>True</td>
</tr>
<tr>
<td><code>TABS_ENABLED</code></td>
<td><code>string</code></td>
<td>Comma-separated list of tabs to display in the interface. (Case-insensitive) (Available: Stacks, Containers, Images, Volumes, Networks)</td>
<td>stacks,containers,images,volumes,networks</td>
</tr>
<tr>
<td><code>COLUMNS_CONTAINERS</code></td>
<td><code>string</code></td>
<td>Comma-separated list of fields to display in the <code>Containers</code> panel. (Case-sensitive) (Available: ID, State, ExitCode, Name, Image, Created)</td>
<td>State,ExitCode,Name,Image</td>
</tr>
<tr>
<td><code>COLUMNS_IMAGES</code></td>
<td><code>string</code></td>
<td>Comma-separated list of fields to display in the <code>Images</code> panel. (Case-sensitive) (Available: ID, Name, Version, Size)</td>
<td>Name,Version,Size</td>
</tr>
<tr>
<td><code>COLUMNS_VOLUMES</code></td>
<td><code>string</code></td>
<td>Comma-separated list of fields to display in the <code>Volumes</code> panel. (Case-sensitive) (Available: Name, Driver, MountPoint)</td>
<td>Driver,Name</td>
</tr>
<tr>
<td><code>COLUMNS_NETWORKS</code></td>
<td><code>string</code></td>
<td>Comma-separated list of fields to display in the <code>Networks</code> panel. (Case-sensitive) (Available: ID, Name, Driver)</td>
<td>Driver,Name</td>
</tr>
<tr>
<td><code>COLUMNS_STACKS</code></td>
<td><code>string</code></td>
<td>Comma-separated list of fields to display in the <code>Stacks</code> panel. (Case-sensitive) (Available: Name, Status)</td>
<td>Status,Name</td>
</tr>
<tr>
<td><code>SORTBY_CONTAINERS</code></td>
<td><code>string</code></td>
<td>Field used to sort the rows in the <code>Containers</code> panel. (Case-sensitive) (Available: ID, State, ExitCode, Name, Image, Created)</td>
<td>Empty</td>
</tr>
<tr>
<td><code>SORTBY_IMAGES</code></td>
<td><code>string</code></td>
<td>Field used to sort the rows in the <code>Images</code> panel. (Case-sensitive) (Available: ID, Name, Version, Size)</td>
<td>Empty</td>
</tr>
<tr>
<td><code>SORTBY_VOLUMES</code></td>
<td><code>string</code></td>
<td>Field used to sort the rows in the <code>Volumes</code> panel. (Case-sensitive) (Available: Name, Driver, MountPoint)</td>
<td>Empty</td>
</tr>
<tr>
<td><code>SORTBY_NETWORKS</code></td>
<td><code>string</code></td>
<td>Field used to sort the rows in the <code>Networks</code> panel. (Case-sensitive) (Available: Id, Name, Driver)</td>
<td>Empty</td>
</tr>
<tr>
<td><code>SORTBY_STACKS</code></td>
<td><code>string</code></td>
<td>Field used to sort the rows in the <code>Stacks</code> panel. (Case-sensitive) (Available: Name, Status)</td>
<td>Empty</td>
</tr>
<tr>
<td><code>CONTAINER_HEALTH_STYLE</code></td>
<td><code>string</code></td>
<td>Style used to display the containers' health state. (Available: long, short, icon)</td>
<td>long</td>
</tr>
<tr>
<td><code>CONTAINER_LOGS_TAIL</code></td>
<td><code>integer</code></td>
<td>Number of lines to retrieve when requesting the last container logs</td>
<td>50</td>
</tr>
<tr>
<td><code>CONTAINER_LOGS_SINCE</code></td>
<td><code>string</code></td>
<td>The amount of time from now to use for retrieving the last container logs</td>
<td>60m</td>
</tr>
<tr>
<td><code>STACKS_DIRECTORY</code></td>
<td><code>string</code></td>
<td>The path to the directory that will be used to store the <code>docker-compose.yml</code> files generated while creating and editing stacks. It must be a valid path to an existing and writable directory.</td>
<td><code>.</code> (current directory)</td>
</tr>
<tr>
<td><code>TTY_SERVER_COMMAND</code></td>
<td><code>string</code></td>
<td>The command used to spawn a new shell inside the server where Isaiah is running</td>
<td><code>/bin/sh -i</code></td>
</tr>
<tr>
<td><code>TTY_CONTAINER_COMMAND</code></td>
<td><code>string</code></td>
<td>The command used to spawn a new shell inside the containers that Isaiah manages</td>
<td><code>/bin/sh -c eval $(grep ^$(id -un): /etc/passwd | cut -d : -f 7-) -i</code></td>
</tr>
<tr>
<td><code>CUSTOM_DOCKER_HOST</code></td>
<td><code>string</code></td>
<td>The host to use in place of the one defined by the DOCKER_HOST default variable</td>
<td>Empty</td>
</tr>
<tr>
<td><code>CUSTOM_DOCKER_CONTEXT</code></td>
<td><code>string</code></td>
<td>The Docker context to use in place of the current Docker context set on the system</td>
<td>Empty</td>
</tr>
<tr>
<td><code>SKIP_VERIFICATIONS</code></td>
<td><code>boolean</code></td>
<td>Whether Isaiah should skip startup verification checks before running the HTTP(S) server. (Not recommended)</td>
<td>False</td>
</tr>
<tr>
<td><code>SERVER_ROLE</code></td>
<td><code>string</code></td>
<td>For multi-node deployments only. The role of the current instance of Isaiah. Can be either <code>Master</code> or <code>Agent</code> and is case-sensitive.</td>
<td>Master</td>
</tr>
<tr>
<td><code>MASTER_HOST</code></td>
<td><code>string</code></td>
<td>For multi-node deployments only. The host used to reach the Master node, specifying the IP address or the hostname, and the port if applicable (e.g. my-server.tld:3000).</td>
<td>Empty</td>
</tr>
<tr>
<td><code>MASTER_SECRET</code></td>
<td><code>string</code></td>
<td>For multi-node deployments only. The secret password used to authenticate on the Master node. Note that it should equal the <code>AUTHENTICATION_SECRET</code> setting on the Master node.</td>
<td>Empty</td>
</tr>
<tr>
<td><code>AGENT_NAME</code></td>
<td><code>string</code></td>
<td>For multi-node deployments only. The name associated with the Agent node as it is displayed on the web interface. It should be unique for each Agent.</td>
<td>Empty</td>
</tr>
<tr>
<td><code>MULTI_HOST_ENABLED</code></td>
<td><code>boolean</code></td>
<td>Whether Isaiah should be run in multi-host mode. When enabled, make sure to have your <code>docker_hosts</code> file next to the executable.</td>
<td>False</td>
</tr>
<tr>
<td><code>FORWARD_PROXY_AUTHENTICATION_ENABLED</code></td>
<td><code>boolean</code></td>
<td>Whether Isaiah should accept authentication headers from a forward proxy.</td>
<td>False</td>
</tr>
<tr>
<td><code>FORWARD_PROXY_AUTHENTICATION_HEADER_KEY</code></td>
<td><code>string</code></td>
<td>The name of the authentication header sent by the forward proxy after a succesful authentication.</td>
<td>Remote-User</td>
</tr>
<tr>
<td><code>FORWARD_PROXY_AUTHENTICATION_HEADER_VALUE</code></td>
<td><code>string</code></td>
<td>The value accepted by Isaiah for the authentication header. Using <code>*</code> means that all values are accepted (except emptiness). This parameter can be used to enforce that only a specific user or group can access Isaiah (e.g. <code>admins</code> or <code>john</code>).</td>
<td>*</td>
</tr>
<tr>
<td><code>CLIENT_PREFERENCE_XXX</code></td>
<td><code>string</code></td>
<td>Please read <a href="#the-web-interface-does-not-save-my-preferences">this troubleshooting paragraph</a>. These settings enable you to define your client preferences on the server, for when your browser can't use the <code>localStorage</code> due to limitations, or private browsing.</td>
<td>Empty</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<blockquote>
<p dir="auto"><strong>Note :</strong> Boolean values are case-insensitive, and can be represented via "ON" / "OFF" / "TRUE" / "FALSE" / 0 / 1.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>Note :</strong> To sort rows in reverse using the <code>SORTBY_</code> parameters, prepend your field with the minus symbol, as in <code>-Name</code></p>
</blockquote>
<blockquote>
<p dir="auto"><strong>Note :</strong> Use either <code>AUTHENTICATION_SECRET</code> or <code>AUTHENTICATION_HASH</code> but not both at the same time.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>Note</strong> : You can generate a sha256 hash using an online tool, or using the following commands :
<strong>On OSX</strong> : <code>echo -n your-secret | shasum -a 256</code>
<strong>On Linux</strong> : <code>echo -n your-secret | sha256sum</code></p>
</blockquote>
<p dir="auto">Additionally, once Isaiah is fully set up and running, you can open the Parameters Manager by pressing the <code>X</code> key.
Using this interface, you can toggle the following options based on your preferences :</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>enableMenuPrompt</code></td>
<td>Whether an extra prompt should warn you before trying to stop / pause / restart a Docker container.</td>
</tr>
<tr>
<td><code>enableLogLinesWrap</code></td>
<td>Whether log lines streamed from Docker containers should be wrapped (as opposed to extend beyond your screen).</td>
</tr>
<tr>
<td><code>enableTimestampDisplay</code></td>
<td>Whether log lines' timestamps coming from Docker containers should be displayed.</td>
</tr>
<tr>
<td><code>enableOverviewOnLaunch</code></td>
<td>Whether an overview panel should show first before anything when launching Isaiah in your browser.</td>
</tr>
<tr>
<td><code>enableLogLinesStrippedBackground</code></td>
<td>Whether alternated log lines should have a brighter background to enhance readability.</td>
</tr>
<tr>
<td><code>enableJumpFuzzySearch</code></td>
<td>Whether, in Jump mode, fuzzy search should be used, as opposed to default substring search.</td>
</tr>
<tr>
<td><code>enableSyntaxHightlight</code></td>
<td>Whether syntax highlighting should be enabled (when viewing docker-compose.yml files).</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<blockquote>
<p dir="auto">Note : You must have Isaiah open in your browser and be authenticated to access these options. Once set up, these options will be saved to your localStorage.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Theming</h2><a id="user-content-theming" aria-label="Permalink: Theming" href="#theming"></a></p>
<p dir="auto">You can customize Isaiah's web interface using your own custom CSS. At runtime, Isaiah will look for a file named <code>custom.css</code> right next to the executable.
If this file exists, it will be loaded in your browser and it will override any existing CSS rule.</p>
<p dir="auto">In order to help you get started, a <a href="https://github.com/will-moss/isaiah/blob/master/app/sample.custom.css">sample file</a> was created.
It shows how to modify the CSS variables responsible for the colors of the interface. (All the values are the ones used by default)
You can copy that file, update it, and rename it to <code>custom.css</code>.</p>
<p dir="auto">If you're using Docker, you should mount a <code>custom.css</code> file at the root of your container's filesystem.
Example : <code>docker ... -v my-custom.css:/custom.css ...</code></p>
<p dir="auto">Finally, you will find below a table that describes what each CSS color variable means :</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>color-terminal-background</code></td>
<td>Background of the interface</td>
</tr>
<tr>
<td><code>color-terminal-base</code></td>
<td>Texts of the interface</td>
</tr>
<tr>
<td><code>color-terminal-accent</code></td>
<td>Elements that are interactive or must catch the attention</td>
</tr>
<tr>
<td><code>color-terminal-accent-selected</code></td>
<td>Panel's title when the panel is in focus</td>
</tr>
<tr>
<td><code>color-terminal-hover</code></td>
<td>Panel's rows that are in focus / hover</td>
</tr>
<tr>
<td><code>color-terminal-border</code></td>
<td>Panels' borders color</td>
</tr>
<tr>
<td><code>color-terminal-danger</code></td>
<td>The color used to convey danger / failure</td>
</tr>
<tr>
<td><code>color-terminal-warning</code></td>
<td>Connection indicator when connection is lost</td>
</tr>
<tr>
<td><code>color-terminal-accent-alternative</code></td>
<td>Connection indicator when connection is established</td>
</tr>
<tr>
<td><code>color-terminal-log-row-alternative</code></td>
<td>The color used as background for each odd row in the logs tab</td>
</tr>
<tr>
<td><code>color-terminal-json-key</code></td>
<td>The color used to distinguish keys from values in the inspector when displaying a long configuration</td>
</tr>
<tr>
<td><code>color-terminal-json-value</code></td>
<td>The color used to distinguish values from keys in the inspector when displaying a long configuration</td>
</tr>
<tr>
<td><code>color-terminal-cell-failure</code></td>
<td>Container health state when exited</td>
</tr>
<tr>
<td><code>color-terminal-cell-success</code></td>
<td>Container health state when running</td>
</tr>
<tr>
<td><code>color-terminal-cell-paused</code></td>
<td>Container health state when paused</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">On a side note, creating custom layouts using only CSS isn't implemented yet as it requires interaction with Javascript.
That said, implementing this feature should be quick and simple since the way layouts are managed currently is already modular.</p>
<p dir="auto">Ultimately, please note that Isaiah already comes with three themes : dawn, moon, and the default one.
The first two themes are based on Rosé Pine, and new themes may be implemented later.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshoot</h2><a id="user-content-troubleshoot" aria-label="Permalink: Troubleshoot" href="#troubleshoot"></a></p>
<p dir="auto">Should you encounter any issue running Isaiah, please refer to the following common problems with their solutions.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Isaiah is unreachable over HTTP / HTTPS</h4><a id="user-content-isaiah-is-unreachable-over-http--https" aria-label="Permalink: Isaiah is unreachable over HTTP / HTTPS" href="#isaiah-is-unreachable-over-http--https"></a></p>
<p dir="auto">Please make sure that the following requirements are met :</p>
<ul dir="auto">
<li>
<p dir="auto">If Isaiah runs as a standalone application without proxy :</p>
<ul dir="auto">
<li>Make sure your server / firewall accepts incoming connections on Isaiah's port.</li>
<li>Make sure your DNS configuration is correct. (Usually, such record should suffice : <code>A isaiah XXX.XXX.XXX.XXX</code> for <code>https://isaiah.your-server-tld</code>)</li>
<li>Make sure your <code>.env</code> file is well configured according to the <a href="#configuration">Configuration</a> section.</li>
</ul>
</li>
<li>
<p dir="auto">If Isaiah runs on Docker :</p>
<ul dir="auto">
<li>Perform the previous (standalone) verifications first.</li>
<li>Make sure you mounted your server's Docker Unix socket onto the container that runs Isaiah (/var/run/docker.sock)</li>
<li>Make sure your Docker container is accessible remotely</li>
</ul>
</li>
<li>
<p dir="auto">If Isaiah runs behind a proxy :</p>
<ul dir="auto">
<li>Perform the previous (standalone) verifications first.</li>
<li>Make sure that <code>SERVER_PORT</code> (Isaiah's port) are well set in <code>.env</code>.</li>
<li>Check your proxy forwarding rules.</li>
</ul>
</li>
</ul>
<p dir="auto">In any case, the crucial part is <a href="#configuration">Configuration</a> and making sure your Docker / Proxy setup is correct as well.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">The emulated shell behaves unconsistently or displays unexpected characters</h4><a id="user-content-the-emulated-shell-behaves-unconsistently-or-displays-unexpected-characters" aria-label="Permalink: The emulated shell behaves unconsistently or displays unexpected characters" href="#the-emulated-shell-behaves-unconsistently-or-displays-unexpected-characters"></a></p>
<p dir="auto">Please note that the emulated shell works by performing the following steps :</p>
<ul dir="auto">
<li>Open a headless terminal on the remote server / inside the remote Docker container.</li>
<li>Capture standard output, standard error, and bind standard input to the web interface.</li>
<li>Display standard output and standard error on the web interface as they are streamed over Websocket from the terminal.</li>
</ul>
<p dir="auto">According to this implementation, the remote terminal never receives key presses. It only receives commands.</p>
<p dir="auto">Also, the following techniques are used to try to enhance the user experience on the web interface :</p>
<ul dir="auto">
<li>Enable clearing the shell (HTML) screen via "Ctrl+L" (while the real terminal remains untouched)</li>
<li>Enable quitting the (HTML) shell via "Ctrl+D" (by sending an "exit" command to the real terminal)</li>
<li>Handle "command mirror" by appending "# ISAIAH" to every command sent by the user (to distinguish it from command output)</li>
<li>Handle both "\r" and "\n" newline characters</li>
<li>Use a time-based approach to detect when a command is finished if it doesn't output anything that shows clear ending</li>
<li>Remove all escape sequences meant for coloring the terminal output</li>
<li>Handle up and down arrow keys to cycle through commands history locally</li>
</ul>
<p dir="auto">Therefore it appears that, unless we use a VNC-like solution, the emulation can neither be enhanced nor use keyboard-based features (such as tab completion).</p>
<p dir="auto">Unless a contributor points the project in the right direction, and as far as my skills go, I personally believe that the current implementation has reached its maximum potential.</p>
<p dir="auto">I leave here a few ideas that I believe could be implemented, but may require more knowledge, time, testing :</p>
<ul dir="auto">
<li>Convert escape sequences to CSS colors</li>
<li>Wrap every command in a "block" (begin - command - end) to easily distinguish user-sent commands from output</li>
<li>Sending to the real terminal the key presses captured from the web (a.k.a sending key presses to a running process)</li>
</ul>
<p dir="auto">Ultimately, please also note that in a multi-node / multi-host setup, the extra network latency and unexpected buffering from remote terminals may cause additional display artifacts.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">An error happens when spawning a new shell on the server / inside a Docker container</h4><a id="user-content-an-error-happens-when-spawning-a-new-shell-on-the-server--inside-a-docker-container" aria-label="Permalink: An error happens when spawning a new shell on the server / inside a Docker container" href="#an-error-happens-when-spawning-a-new-shell-on-the-server--inside-a-docker-container"></a></p>
<p dir="auto">The default commands used to spawn a shell, although being more or less standard, may not fit your environment.
In this case, please edit the <code>TTY_SERVER_COMMAND</code> and <code>TTY_CONTAINER_COMMAND</code> settings to define a command that works better in your setup.</p>
<p dir="auto">Also, please note that if you have deployed Isaiah using Docker, trying to open a system shell (<code>S</code> key) will not work.
Isaiah being confined to its Docker container, it won't be able to open a shell out of it (on your hosting system).</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">The connection with the remote server randomly stops or restarts</h4><a id="user-content-the-connection-with-the-remote-server-randomly-stops-or-restarts" aria-label="Permalink: The connection with the remote server randomly stops or restarts" href="#the-connection-with-the-remote-server-randomly-stops-or-restarts"></a></p>
<p dir="auto">This is a known incident that happens when the Websocket server receives a data message that exceeds its maximum read size.
You should be able to fix that by updating the <code>SERVER_MAX_READ_SIZE</code> setting to a higher value (default is 100,000 bytes).
This operation shouldn't cause any problem or impact performances.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">I can neither click nor use the keyboard, nothing happens</h4><a id="user-content-i-can-neither-click-nor-use-the-keyboard-nothing-happens" aria-label="Permalink: I can neither click nor use the keyboard, nothing happens" href="#i-can-neither-click-nor-use-the-keyboard-nothing-happens"></a></p>
<p dir="auto">In such a case, please check the icon in the lower right corner.
If you see an orange warning symbol, it means that the connection with the server was lost.
When the connection is lost, all inputs are disabled, until the connection is reestablished (a new attempt is performed every second).</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">The interface is stuck loading indefinitely</h4><a id="user-content-the-interface-is-stuck-loading-indefinitely" aria-label="Permalink: The interface is stuck loading indefinitely" href="#the-interface-is-stuck-loading-indefinitely"></a></p>
<p dir="auto">This incident arises when a crash occurs while inside a shell or performing a Docker command.
The quickest "fix" for that is to refresh your browser tab (Ctrl+R/Cmd+R).</p>
<p dir="auto">The real "fix" (if any) could be to implement a "timeout" (client-side or server-side) after which, the "loading" state is automatically discarded</p>
<p dir="auto">If you encounter this incident consistently, please reach out by opening an issue so we look deeper into that part</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">The web interface seems to randomly crash and restart</h4><a id="user-content-the-web-interface-seems-to-randomly-crash-and-restart" aria-label="Permalink: The web interface seems to randomly crash and restart" href="#the-web-interface-seems-to-randomly-crash-and-restart"></a></p>
<p dir="auto">If you haven't already, please read about the <code>SERVER_MAX_READ_SIZE</code> setting in the <a href="#configuration">Configuration</a> section.</p>
<p dir="auto">That incident occurs when the Websocket messages sent from the client to the server are too big.
The server's reaction to overly large messages sent over Websocket is to close the connection with the client.
When that happens, Isaiah (as a client in your browser) automatically reopens a connection with the server, hence explaining the "crash-restart" cycle.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">The web interface does not save my preferences</h4><a id="user-content-the-web-interface-does-not-save-my-preferences" aria-label="Permalink: The web interface does not save my preferences" href="#the-web-interface-does-not-save-my-preferences"></a></p>
<p dir="auto">First, please ensure that your browser supports the <code>localStorage</code> API.</p>
<p dir="auto">Second, please ensure that you're not using the <code>private browsing</code> or <code>incognito</code> or <code>anonymous browsing</code> mode. This mode will
turn off the <code>localStorage</code>, hence disabling the user preferences saved by Isaiah in your browser.</p>
<p dir="auto">If you wish to use Isaiah inside a private browser window while still having your preferences stored somewhere, use the
<code>CLIENT_PREFERENCE_XXX</code> settings in your deployment. These settings will be stored server-side, and understood by your browser
without ever using <code>localStorage</code>, hence circumventing the limitation of the private browsing mode.</p>
<p dir="auto">All the preferences described in the second table of <a href="#configuration">Configuration</a> are available server-side, using their uppercased-underscore counterpart.
See below :</p>
<ul dir="auto">
<li><code>theme</code> becomes <code>CLIENT_PREFERENCE_THEME</code></li>
<li><code>enableOverviewOnLaunch</code> becomes <code>CLIENT_PREFERENCE_ENABLE_OVERVIEW_ON_LAUNCH</code></li>
<li><code>enableMenuPrompt</code> becomes <code>CLIENT_PREFERENCE_ENABLE_MENU_PROMPT</code></li>
<li><code>enableLogLinesWrap</code> becomes <code>CLIENT_PREFERENCE_ENABLE_LOG_LINES_WRAP</code></li>
<li><code>enableJumpFuzzySearch</code> becomes <code>CLIENT_PREFERENCE_ENABLE_JUMP_FUZZY_SEARCH</code></li>
<li><code>enableTimestampDisplay</code> becomes <code>CLIENT_PREFERENCE_ENABLE_TIMESTAMP_DISPLAY</code></li>
<li><code>enableLogLinesStrippedBackground</code> becomes <code>CLIENT_PREFERENCE_ENABLE_LOG_LINES_STRIPPED_BACKGROUND</code></li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">A feature that works on desktop is missing from the mobile user interface</h4><a id="user-content-a-feature-that-works-on-desktop-is-missing-from-the-mobile-user-interface" aria-label="Permalink: A feature that works on desktop is missing from the mobile user interface" href="#a-feature-that-works-on-desktop-is-missing-from-the-mobile-user-interface"></a></p>
<p dir="auto">Please note that you can horizontally scroll the mobile controls located in the bottom part of your screen to reveal all of them.
If, for any reason, you still encounter a case when a feature is missing on your mobile device, please open an issue
indicating the browser you're using, your screen's viewport size, and the model of your phone.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">In a multi-node deployment, the agent's registration with master is stuck loading indefinitely</h4><a id="user-content-in-a-multi-node-deployment-the-agents-registration-with-master-is-stuck-loading-indefinitely" aria-label="Permalink: In a multi-node deployment, the agent's registration with master is stuck loading indefinitely" href="#in-a-multi-node-deployment-the-agents-registration-with-master-is-stuck-loading-indefinitely"></a></p>
<p dir="auto">This issue arises when the authentication settings between Master and Agent nodes are incompatible.<br>
To fix it, please make sure that :</p>
<ul dir="auto">
<li>When authentication is enabled on Master, the Agent has a <code>MASTER_SECRET</code> setting defined.</li>
<li>When authentication is disabled on Master, the Agent has no <code>MASTER_SECRET</code> setting defined.</li>
</ul>
<p dir="auto">Also don't forget to restart your nodes when changing settings.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Something else</h4><a id="user-content-something-else" aria-label="Permalink: Something else" href="#something-else"></a></p>
<p dir="auto">Please feel free to open an issue, explaining what happens, and describing your environment.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security</h2><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<p dir="auto">Due to the very nature of Isaiah, I can't emphasize enough how important it is to harden your server :</p>
<ul dir="auto">
<li>Always enable the authentication (with <code>AUTHENTICATION_ENABLED</code> and <code>AUTHENTICATION_SECRET</code> settings) unless you have your own authentication mechanism built into a proxy.</li>
<li>Always use a long and secure password to prevent any malicious actor from taking over your Isaiah instance.</li>
<li>You may also consider putting Isaiah on a private network accessible only through a VPN.</li>
</ul>
<p dir="auto">Keep in mind that any breach or misconfiguration on your end could allow a malicious actor to fully take over your server.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Disclaimer</h2><a id="user-content-disclaimer" aria-label="Permalink: Disclaimer" href="#disclaimer"></a></p>
<p dir="auto">I believe that, although we're both in the open-source sphere and have all the best intentions, it is important to state the following :</p>
<ul dir="auto">
<li>Isaiah isn't a competitor or any attempt at replacing the lazydocker project. Funnily enough, I'm myself more comfortable running lazydocker through SSH rather than in a browser.</li>
<li>I've browsed almost all the open issues on lazydocker, and tried to implement and improve what I could (hence the <code>TTY_CONTAINER_COMMAND</code> variable, as an example, or even the Image pulling feature).</li>
<li>Isaiah was built from absolute zero (for both the server and the client), and was ultimately completed using knowledge from lazydocker that I'm personally missing (e.g. the container states and icons).</li>
<li>Before creating Isaiah, I tried to "serve lazydocker over websocket" (trying to send keypresses to the lazydocker process, and retrieving the output via Websocket), but didn't succeed, hence the full rewrite.</li>
<li>I also tried to start Isaiah from the lazydocker codebase and implement a web interface on top of it, but it seemed impractical or simply beyond my skills, hence the full rewrite.</li>
</ul>
<p dir="auto">Ultimately, thanks to the people behind lazydocker both for the amazing project (that I'm using daily) and for paving the way for Isaiah.</p>
<p dir="auto">PS : Please also note that Isaiah isn't exactly 100% feature-equivalent with lazydocker (e.g. charts are missing)
PS2 : What spurred me to build Isaiah in the first place is a bunch of comments on the Reddit self-hosted community, stating that Portainer and other available solutions were too heavy or hard to use. A Redditor said that having lazydocker over the web would be amazing, so I thought I'd do just that.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contribute</h2><a id="user-content-contribute" aria-label="Permalink: Contribute" href="#contribute"></a></p>
<p dir="auto">This is one of my first ever open-source projects, and I'm not a Docker / Github / Docker Hub / Git guru yet.</p>
<p dir="auto">If you can help in any way, please do! I'm looking forward to learning from you.</p>
<p dir="auto">From the top of my head, I'm sure there's already improvement to be made on :</p>
<ul dir="auto">
<li>Terminology (using the proper words to describe technical stuff)</li>
<li>Coding practices (e.g. writing better comments, avoiding monkey patches)</li>
<li>Shell emulation (e.g. improving on what's done already)</li>
<li>Release process (e.g. making explicit commits, pushing Docker images properly to Docker Hub)</li>
<li>Github settings (e.g. using discussions, wiki, etc.)</li>
<li>And more!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<p dir="auto">Hey hey ! It's always a good idea to say thank you and mention the people and projects that help us move forward.</p>
<p dir="auto">Big thanks to the individuals / teams behind these projects :</p>
<ul dir="auto">
<li><a href="https://github.com/jesseduffield/lazydocker">laydocker</a> : Isaiah wouldn't exist if Lazydocker hadn't been created prior, and to say that it is an absolutely incredible and very advanced project is an understatement.</li>
<li><a href="https://github.com/tailwindlabs/heroicons">Heroicons</a> : For the great icons.</li>
<li><a href="https://github.com/olahol/melody">Melody</a> : For the awesome Websocket implementation in Go.</li>
<li><a href="https://github.com/goreleaser/goreleaser">GoReleaser</a> : For the amazing release tool.</li>
<li><a href="https://github.com/krisk/fuse">Fuse</a> : For the amazing fuzzy-search library.</li>
<li>The countless others!</li>
</ul>
<p dir="auto">And don't forget to mention Isaiah if it makes your life easier!</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mourning and moving on: rituals for leaving a career (2014) (183 pts)]]></title>
            <link>https://franceshocutt.com/2014/09/10/on-mourning-and-moving-on-rituals-for-leaving-a-career/</link>
            <guid>41317280</guid>
            <pubDate>Thu, 22 Aug 2024 06:03:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://franceshocutt.com/2014/09/10/on-mourning-and-moving-on-rituals-for-leaving-a-career/">https://franceshocutt.com/2014/09/10/on-mourning-and-moving-on-rituals-for-leaving-a-career/</a>, See on <a href="https://news.ycombinator.com/item?id=41317280">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		<p><a href="http://modelviewculture.com/pieces/i-didn-t-want-to-lean-out">I decided to leave what had been a promising career in organic chemistry</a> about a year ago. Deciding to leave my program, and then to leave the field entirely, was one of the hardest decisions I have made. I had more resources than many in my position: savings and financial support, enough work experience to feel confident that I was making a realistic decision, and supportive friends and mentors. Still, that decision meant that I lost my plans, my confidence in my career trajectory, and my identity as a practicing scientist.</p>
<p>One of my biggest losses was a clear(ish) path forward. In chemistry, and particularly in academia, your mentors and your observations help you form a mental career map of sorts. Undergrad (graduation) leads to a bench job or graduate school (the latter may be much like that bench job, but with worse management and less compensation); the bench job leads to a dead end (in Big Pharma, at least), an “alternate career path”, or to graduate school. Graduate school traditionally leads to academia, industry, or work at national labs (or that “alternate career path”). Academia has the postdoc-to-postdoc-to-tenure track path; “industry” covers a lot of territory, but there is an expectation of moving either laterally or vertically within and between various companies (<a href="http://chemjobber.blogspot.com/">assuming there are jobs</a>); and similarly, there are opportunities for career progress and moving up the ladder in national labs. None of these are easy paths, of course, but I was surrounded by the institutional knowledge that they were possible.</p>
<p>When I left, I left the territory that my maps covered. That same institutional knowledge whispered that leaving a program is failing; that “alternate career paths” are well and good for those who couldn’t hack it on the “normal” paths; that a master’s degree is an admission of inferiority, not a proud acheivement. I had never judged my friends and partners who had left their own programs and changed fields, but it was somehow different when it was me.</p>
<p>Humans aren’t very good with change. We create meaning around the stress and soften transitions with rituals and rites of passage. Each of the change-points on the map I described would have been marked with a ritual: graduations, heading to happy hour after quals, the <a href="http://www.mcsweeneys.net/articles/faq-the-snake-fight-portion-of-your-thesis-defense">ritual challenge</a> of the <a href="http://xkcd.com/1403/">thesis defense</a> and the addition of “Dr.” to one’s full name, a handshake and congratulations on a raise or promotion, ordering business cards with a new title, heading to lunch with coworkers when a new coworker arrives or when one leaves for grad school, going through the arcane and labryinthine process of setting up accounts and office space at a new institution. We go through rituals to enter a program, and the process of graduate school itself is arguably a rite of passage that culminates in a final challenge, renaming, and shared food and drink. There is nothing to smooth the process of choosing to leave.</p>
<p>When I made my final decision to leave, I could feel what I was losing and that I needed to mourn. My grandmother had died at the beginning of the year, so grief, and irreversible change were already on my mind. My family grieved by coming together to share food, drink, stories, and ritual. None of those elements need to be restricted to mourning a death. I wanted the support of my community for this loss as well. </p>
<p>I invited my friends to a wake of sorts. No one ended up coming in mourning wear, but a dear friend brought me funeral lilies with a sheepish expression and that set the tone for the evening. We ate, we drank, and we chatted. Eventually I talked a bit about the choice I’d made, why I’d invited them, and my hopes for my future. My friends shared their hopes, reassurances, and anger on my behalf and their own wishes. I led a series of toasts and curses for what I’d been through and what I wished were different. I acknowledged what I had gotten from that part of my life. I cried for what I’d experienced and what I’d lost.</p>
<p>Those of us who leave the paths “everyone” knows are no less brave and resourceful than those who follow them. I’ve posted the invitation I sent out for the “wake” I held below the cut. If you think that anything I’ve shared here might help you navigate your own changes, please take whatever is helpful, change it to fit you, and pass it on. We can map and mark our new paths together.</p>
<p><span id="more-140"></span><br>
Greetings, all:</p>
<p>You are formally invited to</p>
<p>A WAKE</p>
<p>for</p>
<p>THE RESEARCH SCIENCE CAREER</p>
<p>of</p>
<p>FRANCES HOCUTT</p>
<p>FRIDAY from 7 PM to MIDNIGHT</p>
<p>I have decided to permanently leave the UW chemistry department and,<br>
most likely, the field of organic chemistry. This is a significant<br>
personal loss. I have been interested in and good at organic chemistry<br>
for the last decade and had been planning to use those skills in a<br>
research career to figure out more about the world and change it for<br>
the better with SCIENCE! I have finally decided that the culture of<br>
the field is too toxic for me to want to continue and I have chosen to<br>
leave to pursue other interests.</p>
<p>You are all invited to help me mourn this loss, to celebrate the good<br>
things I’ll be taking forward from it, and to look ahead at where I’m<br>
going next. I will provide: delicious coconut lentil curry with rice,<br>
caramel sauce and chocolate ganache to put on things, and a few<br>
beverages (alcoholic and not). Please bring some combination of: food,<br>
board games, delightful music, projects to work on, more delicious<br>
beverages, things to put caramel on, and your awesome selves. Kids are<br>
welcome but please be aware there are cats and my house is not<br>
kid-proofed. Please do not bring foods with peanuts or tree nuts in<br>
them.</p>
<p>I ask that those of you currently connected to the chemistry<br>
department keep this information private. I am still trying to work<br>
some things out with the department and would prefer to handle<br>
informing people there myself.</p>
<p>Please do RSVP so I can get a rough head-count, but feel free to show<br>
up at any time during the evening. Dressing in your personal version<br>
of mourning wear, the more over-the-top the better, is highly<br>
encouraged but not required.</p>
<p>Frances</p>

			
			
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Celebrating 6 years since Valve announced Steam Play Proton for Linux (316 pts)]]></title>
            <link>https://www.gamingonlinux.com/2024/08/celebrating-6-years-since-valve-announced-steam-play-proton-for-linux/</link>
            <guid>41316999</guid>
            <pubDate>Thu, 22 Aug 2024 05:05:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gamingonlinux.com/2024/08/celebrating-6-years-since-valve-announced-steam-play-proton-for-linux/">https://www.gamingonlinux.com/2024/08/celebrating-6-years-since-valve-announced-steam-play-proton-for-linux/</a>, See on <a href="https://news.ycombinator.com/item?id=41316999">Hacker News</a></p>
Couldn't get https://www.gamingonlinux.com/2024/08/celebrating-6-years-since-valve-announced-steam-play-proton-for-linux/: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>