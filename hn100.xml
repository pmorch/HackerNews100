<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 18 Nov 2024 11:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[We are shutting down the Ondsel FreeCAD business (189 pts)]]></title>
            <link>https://ondsel.com/blog/goodbye/</link>
            <guid>42169998</guid>
            <pubDate>Mon, 18 Nov 2024 05:40:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ondsel.com/blog/goodbye/">https://ondsel.com/blog/goodbye/</a>, See on <a href="https://news.ycombinator.com/item?id=42169998">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__blog-post-container" itemprop="articleBody"><p>After operating for almost two years, Ondsel has made the difficult decision to cease operations and close down. We are incredibly thankful for the support we’ve received from the FreeCAD community and the larger engineering CAD communities.</p><h2 id="why-ondsel-is-closing-down">Why Ondsel is closing down<a href="#why-ondsel-is-closing-down" aria-label="Direct link to Why Ondsel is closing down" title="Direct link to Why Ondsel is closing down">​</a></h2><p>From the beginning, we knew competing with commercial CAD would be tough. Closed-source CAD is taught in schools and is deeply entrenched in established industry use. We knew that to be successful, we would have to find a way to provide real value and coexist in environments where other tools are already used.</p><p>While seeking a scalable and repeatable business model, we conducted numerous surveys and interviewed nearly a hundred mechanical engineers, service engineers, tinkerers, inventors, workshop owners, and other users. While we found support among independent and hobbyist users who genuinely wanted us to succeed, we failed to find commercial adoption to justify a venture-capitalized startup. Ultimately, we could not find a product-market fit and ran out of runway to continue the search.</p><h2 id="things-we-are-proud-of">Things we are proud of<a href="#things-we-are-proud-of" aria-label="Direct link to Things we are proud of" title="Direct link to Things we are proud of">​</a></h2><p>While we have failed to build a sustainable commercial business around FreeCAD, we have accomplished many things.</p><h3 id="a-better-freecad-application">A better FreeCAD application<a href="#a-better-freecad-application" aria-label="Direct link to A better FreeCAD application" title="Direct link to A better FreeCAD application">​</a></h3><ul><li>We contributed a new integrated assembly workbench that was sorely missing in the program and a 3D constraints solver on which the workbench relies.</li><li>We also contributed significant improvements to the usability of Sketcher and TechDraw.</li><li>We introduced VarSets — a brand-new custom properties system.</li><li>We introduced new features to TechDraw and CAM workbenches.</li><li>Beyond FreeCAD itself, we contributed to third-party addons like SheetMetal.</li></ul><h3 id="lens-as-a-service-for-connected-cad">Lens as a service for connected CAD<a href="#lens-as-a-service-for-connected-cad" aria-label="Direct link to Lens as a service for connected CAD" title="Direct link to Lens as a service for connected CAD">​</a></h3><ul><li>We demonstrated what a connected CAD experience could look like with open source at its core.</li><li>We built a service that enabled teams to organize iterated development of hardware products.</li><li>We built a simple way to share your models publicly or privately, including PIN protection for share links.</li><li>We made it possible to publish parametric models online that you can modify and download.</li></ul><p>Between our May release (2024.2) and today alone, 145 pull requests by the Ondsel team have been merged into the upstream codebase, making FreeCAD 1.0 a more featureful and polished release than we all hoped it would be.</p><p>Having a commercial partner working on FreeCAD also meant that the project had to adapt. Our presence and cooperation affected how the FreeCAD project operates. Our blog brought attention to missing features and helped the project establish priorities. Working with the core maintainers, we improved the contribution process, sped up the merge process, and helped form the Design Working Group and the new CAD Advisory Group. These are significant advances that will benefit the community for a long time.  </p><p>Most of all, we showed that a commercial partner can be an asset to the project. We hope many more entrepreneurs will build around FreeCAD in the spirit of cooperation to establish a robust ecosystem that benefits the world and is profitable. </p><h2 id="what-happens-to-ondsel-es">What happens to Ondsel ES<a href="#what-happens-to-ondsel-es" aria-label="Direct link to What happens to Ondsel ES" title="Direct link to What happens to Ondsel ES">​</a></h2><p>OES was designed to be our flavor of FreeCAD with more pleasing UX/UI, frequent releases, and added value.</p><p>Part of the better UX/UI was our improvements to Sketcher and TechDraw—all users of FreeCAD 1.0 will enjoy those. We owe much of the praise for the UI to Joe Sardos, who designed OpenTheme. We didn’t fund that project, but we pushed it on our users, and they loved it. So we think Joe did a spectacular job, and we cannot thank him enough for that. </p><p>The added value we shipped as part of OES is all available as free and open-source code, and most of it will be part of FreeCAD.</p><p>As such, there is no point in releasing v2024.3. However, we are not abandoning the FreeCAD community. Brad will continue hacking in the CAM workbench, Pierre will likely continue working on Assembly and Sketcher, and Pieter will keep working on varsets and variant parts thanks to grants issued by the FreeCAD Project Association. All future contributions to FreeCAD by our former team members will now go directly to the upstream project.</p><p>We have yet to submit a few improvements to FreeCAD, namely the reloadable objects feature, which we strongly believe should be a core feature. We will submit this as a PR to upstream.</p><h2 id="what-happens-to-the-user-base">What happens to the user base<a href="#what-happens-to-the-user-base" aria-label="Direct link to What happens to the user base" title="Direct link to What happens to the user base">​</a></h2><p>We will notify customers and users of the shutdown. The server will continue running for a reasonable period to give users time to download their data. If you have any projects on Lens that you haven’t backed up anywhere else, please download them as soon as possible.</p><p>Paying customers will be reimbursed for any remaining time on their subscription.</p><p>For questions or comments please email us at <a href="mailto:support@ondsel.com" target="_blank" rel="noopener noreferrer">support@ondsel.com</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Japanese Black Companies Oppress Workers and Ruin Lives[2014] (134 pts)]]></title>
            <link>https://www.tofugu.com/japan/japanese-black-companies/</link>
            <guid>42169615</guid>
            <pubDate>Mon, 18 Nov 2024 04:02:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tofugu.com/japan/japanese-black-companies/">https://www.tofugu.com/japan/japanese-black-companies/</a>, See on <a href="https://news.ycombinator.com/item?id=42169615">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img width="100%" src="https://files.tofugu.com/articles/japan/2014-04-25-japanese-black-companies/header-640x.jpg" alt="Header 640x">
</p>
<h2>
<span>
How Japanese Black Companies Oppress Workers <span>and</span> Ruin Lives
</span>
<small>
<span>
Welcome to the team…and the nightmare
</span>
</small>
</h2>
<p>
<span>
<time datetime="2014-04-25T00:00:00Z">April 25, 2014</time>
•
<span></span>
words written by

•
Art by
<a href="https://www.tofugu.com/about/people/aya-francisco/">Aya Francisco</a>
<a href="https://files.tofugu.com/articles/japan/2014-04-25-japanese-black-companies/header-5120x.jpg" title="Download header wallpaper" target="_blank"><i></i>
</a></span>
</p>
</div><div>
<article>
<div>
<p>If you hear the term "black company", what kind of company do you imagine? As we all know, black is the darkest color, so if you pictured an evil company with a dark side then you would be on the right track. A black company (aka "black corporation" or "black business") is <i>buraku kigyō</i> <ruby>ブラック<rp>（</rp><rt></rt><rp>）</rp></ruby><i></i> <ruby>企業<rp>（</rp><rt>きぎょう</rt><rp>）</rp></ruby> in Japanese. In general, it is a term used to refer to an unacceptably exploitative employment system.</p>

<p>Now, maybe you're thinking this is a word to describe a factory somewhere in China, but you'd be wrong. The term is actually usually associated with white-collar industries rather than blue-collar ones. It was coined by young IT workers in the early 2000's and, being the IT workers that they were, spread this nickname around the internet as an internet meme. Now, thanks to its fame, it is a term that can be used for other industries that are not IT.</p>

<h2 id="the-black-company-and-a-film">The Black Company And A Film</h2>

<p>As the popularity of the term grew, the black company problem received more and more attention in Japan. In 2009, a drama film called "A man on the verge at a BLACK company" was released. It was based on an office worker who started a 2ch thread about the black company he worked at. This company's ruthless and unethical work environment was duplicated in the film.</p>

<p><iframe width="200" height="113" src="https://www.youtube.com/embed/ebxD-KJNFwk?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="ブラック会社に勤めてるんだが、もう俺は限界かもしれない"></iframe></p>

<p>As you can see in the film clip, the workers are being pushed to their limit and obviously not in good shape. Can you imagine working someplace like that for real? Well, bad (black) companies are really out there and this sparked a move towards calling them out. Still, there was a long way to go to bring about public awareness.</p>

<h2 id="the-black-company-awards">The Black Company Awards</h2>

<figure><img alt="A skyscraper housing a Japanese Black Company" src="https://files.tofugu.com/articles/japan/2014-04-25-japanese-black-companies/black-company-building.jpg"><figcaption>  </figcaption> Source: <a href="https://www.flickr.com/photos/sharif/3299531471" title="Visit source's website" target="_blank">Beshef</a></figure>

<p>In 2012, a group of people that included journalists, activists, and university professors formed a special committee to create the "Black Corporations Award" where the public could vote on "the most evil corporations of the year." The award name may sound silly, but its creators were not joking. They were hoping to raise awareness of black company issues as well as expose some of the evil abusers of employees. This is now an annual award.</p>

<figure><img alt="Graphic with Japanese text saying Most Evil Corporation Award" src="https://files.tofugu.com/articles/japan/2014-04-25-japanese-black-companies/most-evil-corporation-award.jpg"></figure>

<p>The most recent award, which took place on June 17th of 2013, nominated eight companies:</p>

<ol>
  <li>Watami Foodservice Co., Ltd (Restaurant Chain)</li>
  <li>Cross Company Inc. (Clothing Retailer)</li>
  <li>Benesse Corporation (Education and Publishing)</li>
  <li>Sun Challenge Corporation (Steak Restaurant Chain)</li>
  <li>Ohsho Food Service Corporation (Restaurant Chain)</li>
  <li>Seino Transportation Co., Ltd (Transportation Service)</li>
  <li>Tokyu Hands Inc. (Department Store)</li>
  <li>Tohoku University</li>
</ol>

<p>All nominations were made on the following basis:</p>

<ol>
  <li>Actual public records on occupational problems such as a long work time, sexual harassment, or abuse of power.</li>
  <li>Long intense work hours.</li>
  <li>Low pay.</li>
  <li>Compliance violations.</li>
  <li>Flaws in the system, such as lack of childcare leave or maternity leave.</li>
  <li>Hostility to unions.</li>
  <li>Discrimination against temporary workers.</li>
  <li>Temporary worker dependance.</li>
  <li>npaid overtime (and lies about paid overtime in the job advertisements).</li>
</ol>

<p>Most black companies already have the above problems, so the nominees were chosen for being especially bad. After the nominations, there was a period in which people could vote. The winner? Watami Foodservice Co., and by a wide margin. They got 21,899 votes, 72% of the total. This was followed by Tohoku University (3,475 votes), Benesse Corporation (1,258 votes), Cross Company Inc (1,220 votes), Seino Transportation Co., Ltd (1,000 votes), Ohsho Food Service Corporation (744 votes), Sun Challenge Corporation (649 votes), and Tokyu Hands Inc (346 votes). Watami has won this award for two years in a row so far.</p>

<figure><img alt="A chart displaying the worst Japanese black companies" src="https://files.tofugu.com/articles/japan/2014-04-25-japanese-black-companies/notorious-black-companies-chart.png"></figure>

<p>Watami's winning streak is due to its notoriously abysmal mistreatment of young workers. In 2008, Mina Mori, a female employee of Watami, committed suicide at the age of 26 after reportedly working 141 hours of overtime in one month. It happened just two months after joining the restaurant chain. Some people may think committing suicide is an individual matter and that a company cannot be held responsible. However, it is reasonable to assume Mori would not have chosen suicide for herself had she not been forced into such a desperate situation. Furthermore, it is apparent just how "black" the Watami company is simply by the cold reaction of Watami founder Miki Watanabe's reaction to the case. He not only refused to meet with her family but also refused to apologize to them until last month. He finally did offer an apology in court on March 27, 2014, though he still has denied liability.</p>

<p>According to an interview with a former Watami restaurant manager conducted by Takarajima magazine (September 2013 edition), upon hearing the news of Ms. Mori's suicide, the ex-manager wondered if it was really <em>just</em> 141 hours. During his time at Watami, he regularly worked from 7am to 12am with almost no break, making his monthly overtime over 300 hours. He also revealed that Miki Watanabe gives extreme messages to workers on every payday, such as: "Regret as hard as you die!" He even received a personal letter from the evil president in his paycheck envelope saying: "you should reflect on your sales this month by killing yourself."</p>

<figure><img alt="The exterior of a Watami Bar during the evening" src="https://files.tofugu.com/articles/japan/2014-04-25-japanese-black-companies/watami-bar-at-night.jpg"><figcaption>  </figcaption> Source: <a href="https://www.flickr.com/photos/miyo/2473313816" title="Visit source's website" target="_blank">Miyo Sekimoto</a></figure>

<p>There was also an illegal but mandatory 1,000 yen deduction from every payment and workers were told that it was for "social contribution." When Watanabe published his own book, the price of the book was automatically deducted from the payment and workers were forced to buy them as well. Although there wasn't any physical violence, he remembered that there was much verbal violence, often relating to "killing himself."</p>

<p>That sounds pretty awful, right? These workers were not treated like human beings… more like robots. Though even robots shouldn't have to work this hard. Obviously a lot of other people felt the same way and anti-Watami movements have risen. On the same day that Watami apologized to Ms. Mori's family, Watami decided to temporarily close 60 restaurants (about 10% of the total) to improve their work environment. I hope this will really be the start of some reform so that others won't have to endure the same mistreatment.</p>

<p>Perhaps thanks to this and the awareness brought about by the Black Company Awards, the term <i>buraku kigyō</i> <ruby>ブラック<rp>（</rp><rt></rt><rp>）</rp></ruby><i></i> <ruby>企業<rp>（</rp><rt>きぎょう</rt><rp>）</rp></ruby> made the final 10 of the most popular or influential Japanese buzzwords in 2013! So, consider the checkbox of "raising awareness" filled in, though there's still much work to be done.</p>

<h2 id="what-colors-a-company-black">What Colors A Company Black?</h2>

<figure><img alt="Black paint chipping off a wall" src="https://files.tofugu.com/articles/japan/2014-04-25-japanese-black-companies/black-wall.jpg"><figcaption>  </figcaption> Source: <a href="https://www.flickr.com/photos/dg_pics/3938829378" title="Visit source's website" target="_blank">David Gunter</a></figure>

<p>So what are the exact qualifications for a company to be considered a black company? You can get some idea from the stories of Watami, but a lawyer named Yoshiyuki Iwasa, who is also the author of <a href="http://www.amazon.co.jp/o/ASIN/4907292090/itmedia-makoto-22/">ブラック<i></i> <ruby>企業<rp>（</rp><rt>きぎょう</rt><rp>）</rp></ruby>に<i></i> <ruby>倍返<rp>（</rp><rt>きぎょう</rt><rp>）</rp></ruby>しだ!</a> (which means "to take double revenge on the black company") <a href="http://bizmakoto.jp/makoto/articles/1401/23/news018_2.html">created a checklist for a website called Business Media</a> to find out whether the company is "black" or "not black." There are thirty items on the list and if none of them apply to the company, it is pure white. If 1-9 items are applicable, it is considered gray. 10-14 is dark gray. 30 out of 30 is pitch black. You get the picture. Now let's take a look at the list.</p>

<ol>
  <li>I do work overtime, but overtime is never paid.</li>
  <li>It's usual to work more than 80 hours overtime a month.</li>
  <li>I don't have a break, or at the most, 10 minutes a day.</li>
  <li>I work on my days off. Actually, I'm not even sure when my days off are.</li>
  <li>There is no paid time off system or if there is such a system, I am never allowed to use it.</li>
  <li>I never get reimbursed for expenses and always have to pay out of pocket.</li>
  <li>There is no social insurance, benefits, or pension. If I ask about this, I would be bullied.</li>
  <li>If I converted my monthly wage into an hourly rate equivalent, it would be less than minimum wage.</li>
  <li>Regardless of how long I work overtime, the overtime payment is a fixed amount.</li>
  <li>The company is constantly hiring new employees.</li>
  <li>The advertised job wage is different from the actual amount paid.</li>
  <li>There are no time cards or someone else punches you in and out.</li>
  <li>There are one or more workers who can't come to the office due to psychotic depression or nervous breakdown.</li>
  <li>I'm so busy that I often can't get adequate sleep.</li>
  <li>There is no union or company regulations.</li>
  <li>Some employees are promoted to an administrative position right after joining the company, but there is no extra remuneration for that.</li>
  <li>Employees have to run private errands for their employers.</li>
  <li>There is a slogan saying "work until you die" on the company wall.</li>
  <li>Abuse of power and sexual harassment are very common.</li>
  <li>There are so many affiliate companies and subsidiaries, though I don't even know what those companies do.</li>
  <li>Whenever some incident happens, the company changes its name.</li>
  <li>There are training sessions, which use what can be considered brainwashing or hazing.</li>
  <li>Threats such as "I'm going to kill you" can be commonly heard at the office.</li>
  <li>Violence is rampant.</li>
  <li>All the supervisors are relatives of the CEO.</li>
  <li>I was told to quit the company in a roundabout way like, "you may not be cut out for this position."</li>
  <li>I can't quit the job. If I say I'm going to quit, I'll be threatened that I will have to pay damages for quitting.</li>
  <li>They don't provide the necessary documents such as the separation slip to those who try to quit.</li>
  <li>The worker's average age is really young.</li>
  <li>The rate of people leaving their jobs within 3 years is really high.</li>
</ol>

<p>Now after looking through them, do any of these conditions apply to your company? Many of the items may sound ridiculous, but I have worked under some of these conditions before and I have some friends who work for such companies. However, I'm not sure if their companies are pitch black. Maybe just very, very dark?</p>

<figure><img alt="Black paint trailing down a white wall" src="https://files.tofugu.com/articles/japan/2014-04-25-japanese-black-companies/dripping-black-paint.jpg"><figcaption>  </figcaption> Source: <a href="https://www.flickr.com/photos/rockandbacon/4600142725" title="Visit source's website" target="_blank">rockandbacon</a></figure>

<p>According to <a href="http://www.mhlw.go.jp/stf/houdou/2r985200000339uj.html">Japan's Ministry of Health, Labour and Welfare</a>, the number of cases delivered to the General Labor Consultation Center in 2012 was 1,067,210. 254,719 of them involved troubles at work such as encouragement of early retirement, worsening work conditions, the lowering of pay, long periods of overtime without payment, or mean, abusive bosses and/or coworkers. A decade ago, only around 100,000 cases were reported. This number has increased about 2.5x, showing that either black companies are increasing in number, or, what is more likely, more people are coming forward. These companies are finally having their true colors exposed (black).</p>

<p>In September, 2013, the Labor Ministry began a crackdown on black companies, too. They examined 5,111 such companies based on their high turnover rates of young workers, their past violations, and complaints from employees. Then it was revealed that over 80% of them, 4,189 companies, were indeed actually black companies engaged in illegal business practices. They were flagged for violations against the Labor Standards Act. The ministry intends to discipline the companies in question and if those companies continue to violate the Labor Standards laws, their names will be made public.</p>

<p>So, now that the Labor Ministry is getting involved, can we call this the end of the story? Well, I'm afraid it's not over yet. This was only the ministry's first survey on black companies and there are over 4 million companies in Japan. So, the exciting fight has only just begun! We're counting on you, Black Company Awards and Japan's Labor Ministry!</p>



</div>
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linux kernel 6.12 has been released (136 pts)]]></title>
            <link>https://lwn.net/Articles/997958/</link>
            <guid>42169418</guid>
            <pubDate>Mon, 18 Nov 2024 03:12:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/997958/">https://lwn.net/Articles/997958/</a>, See on <a href="https://news.ycombinator.com/item?id=42169418">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
Linus has <a href="https://lwn.net/Articles/998490/">released the 6.12 kernel</a>.
"<q>No strange surprises this last week, so we're sticking to the regular
release schedule, and that obviously means that the merge window opens
tomorrow.</q>".
</p><p>

Headline features in this release include:

support for the <a href="https://developer.arm.com/documentation/102376/0200/Permission-indirection-and-permission-overlay-extensions">Arm
permission overlay</a> extension,
better compile-time control over which Spectre mitigations to employ,
the <a href="https://lwn.net/Articles/990985/">last pieces of realtime preemption support</a>,
the realtime <a href="https://lwn.net/Articles/934415/">deadline server</a> mechanism,
more <a href="https://lwn.net/Articles/969062/">EEVDF scheduler</a> development,
the <a href="https://lwn.net/Articles/974387/">extensible scheduler class</a>,
the <a href="https://lwn.net/Articles/979549/">device memory TCP</a> work,
<a href="https://lwn.net/Articles/979683/">use of static calls</a> in the security-module
subsystem,
the <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=a430d95c5efa">integrity
policy enforcement</a> security module,
the ability to handle devices with a block size larger than the system page
size in the XFS filesystem,
and more.
See the LWN merge-window summaries 
(<a href="https://lwn.net/Articles/990750/">part&nbsp;1</a>, <a href="https://lwn.net/Articles/991301/">part&nbsp;2</a>) and the <a href="https://kernelnewbies.org/Linux_6.12">KernelNewbies 6.12 page</a> for
more details.<br clear="all"></p><hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[1 in 5 Japanese Workers in Their 20s Turn to Resignation Agencies (153 pts)]]></title>
            <link>https://metropolisjapan.com/resignation-agencies/</link>
            <guid>42169027</guid>
            <pubDate>Mon, 18 Nov 2024 01:49:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://metropolisjapan.com/resignation-agencies/">https://metropolisjapan.com/resignation-agencies/</a>, See on <a href="https://news.ycombinator.com/item?id=42169027">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


                <figure>
                    <img width="1000" height="667" src="https://metropolisjapan.com/wp-content/uploads/2024/11/resignation-agencies-in-Japan-1000x667.jpg" alt="“They Ripped Up My Resignation Letter”: 1 in 5 Japanese Workers in Their 20s Turn to Resignation Agencies" decoding="async" fetchpriority="high" srcset="https://metropolisjapan.com/wp-content/uploads/2024/11/resignation-agencies-in-Japan-1000x667.jpg 1000w, https://metropolisjapan.com/wp-content/uploads/2024/11/resignation-agencies-in-Japan-600x400.jpg 600w, https://metropolisjapan.com/wp-content/uploads/2024/11/resignation-agencies-in-Japan-250x167.jpg 250w, https://metropolisjapan.com/wp-content/uploads/2024/11/resignation-agencies-in-Japan-768x512.jpg 768w, https://metropolisjapan.com/wp-content/uploads/2024/11/resignation-agencies-in-Japan-1536x1024.jpg 1536w, https://metropolisjapan.com/wp-content/uploads/2024/11/resignation-agencies-in-Japan-2048x1365.jpg 2048w" sizes="(max-width: 1000px) 100vw, 1000px">                </figure>

                
                

                
<p><a href="https://career-research.mynavi.jp/reserch/20241003_86953/" target="_blank" rel="noreferrer noopener"><em>A new survey from Mynavi Corp.</em></a><em> shows a significant rise in resignation agencies across Japan that help people quit their jobs. But why?</em></p>



<p>With deep remorse, Mr. Iida sank to the floor. His knees touched the cold concrete, and he bowed so profoundly that his head almost met the ground. “Dogeza,” the most sincere way to apologize for a serious offense, performed by a salaryman in his boss’s office. The reason? Mr. Iida submitted his resignation. But, after a brief silence, there was a tearing sound, and he realized his boss had ripped his resignation letter to shreds. The message was clear: <em>you are not allowed to quit.</em></p>



<p>In Japan, leaving a job can be challenging. Traditionally, employment is considered a lifelong commitment. Though Japan is moving away from this outdated mentality, many traditional companies still expect employees to work until retirement without changing jobs.</p>



<p>For some older generations, there is still a deep sense of shame in switching jobs, as work ethic in Japan is closely tied to loyalty and respect. Japanese law technically guarantees the right to quit, but doing so is often seen as disrespectful, as companies invest time and money in training employees. It is a mutual investment — the company invests in you, and you, in turn, commit to advancing within the organization over time.</p>



<h2>“I Can’t Take This Anymore”</h2>



<p>Mr. Iida performed the dogeza three times before his boss, apologizing for “letting him down,” yet his resignation still wasn’t accepted. He had to find another way out.</p>



<p>He contacted the resignation agency Momuri, whose name translates to “I Can’t Take This Anymore.” For around ¥22,000, they handle the entire resignation process, contacting the boss on behalf of the client, negotiating with the company, and even recommending lawyers if legal disputes arise.</p>



<p>According to Yujin Watanabe, a spokesperson from Albatross Corp., which manages Momuri’s services, their typical client profile is young people in their 20s who work for small to medium-sized companies, often in corporate or welfare industries.</p>



<p>Mr. Iida is far from alone in seeking outside help to resign. Since 2022, Momuri, just one independent resignation agency, has received 35,000 requests from people needing assistance quitting their jobs.</p>



<h2>1 in 5 in Their 20s Use Resignation Agencies</h2>



<figure><img decoding="async" loading="lazy" width="1000" height="611" src="https://metropolisjapan.com/wp-content/uploads/2024/11/Mynavi-Corporation-resignation-agency-age-metropolis-japan-1000x611.png" alt="" srcset="https://metropolisjapan.com/wp-content/uploads/2024/11/Mynavi-Corporation-resignation-agency-age-metropolis-japan-1000x611.png 1000w, https://metropolisjapan.com/wp-content/uploads/2024/11/Mynavi-Corporation-resignation-agency-age-metropolis-japan-654x400.png 654w, https://metropolisjapan.com/wp-content/uploads/2024/11/Mynavi-Corporation-resignation-agency-age-metropolis-japan-250x153.png 250w, https://metropolisjapan.com/wp-content/uploads/2024/11/Mynavi-Corporation-resignation-agency-age-metropolis-japan-768x470.png 768w, https://metropolisjapan.com/wp-content/uploads/2024/11/Mynavi-Corporation-resignation-agency-age-metropolis-japan.png 1220w" sizes="(max-width: 1000px) 100vw, 1000px"><figcaption>Mynavi Corporation: use of resignation services in the past year by age group and occupation.</figcaption></figure>



<p>A new survey by the employment information provider Mynavi Corp. found that nearly 1 in 5 people in their 20s who quit their jobs in Japan last year used agencies like Momuri to help them.&nbsp;</p>



<p>The survey, which included employees ages 20-50 who quit jobs between June 2023 and June 2024, shows that 18.6% of those in their 20s used these services, 17.6% were in their 30s and 17.3% were in their 40s. Only 4.4% were in their 50s, indicating that older employees tend to stay in jobs longer—even if conditions are poor—as the younger generation is more likely to adjust to their needs and wishes.&nbsp;</p>



<p>The top-cited reason for using resignation agencies, at 40.7%, was that companies refused to let them quit. Other reasons included fear of backlash if they resigned alone or their work environment discouraged employees from quitting independently.&nbsp;</p>



<p>Mynavi Corp. also surveyed workplaces and asked managers how often employees under their supervision had used resignation agencies. Between January and June 2024, 23.2% reported that they had employees who used resignation agencies. The most common fields were insurance, finance and IT.</p>



<h2><strong>Resignation Agencies Reflect Japan’s Mental Health Crisis</strong></h2>



<p>According to Momuri, most Japanese people in corporate roles have a low awareness of mental health. Social norms often discourage <a href="https://metropolisjapan.com/japanese-books-mental-health-wellbeing/" target="_blank" rel="noreferrer noopener">seeking help for mental challenges</a>—many of which stem from their work.&nbsp;</p>



<p>This lack of support is one reason more Japanese people now turn to resignation agencies. They quit to avoid damaging their mental health and use resignation agencies to sidestep the stress of handling the process themselves.</p>



<p>Mr. Iida’s experience of his boss tearing up his resignation letter is not unique. In fact, it’s far from the most extreme case. Momuri confirms stories of companies forcing employees to visit temples to ‘cure’ their desire to quit. Sometimes they even receive home visits from managers pressuring them to stay.&nbsp;</p>



<p>Those fortunate enough to leave are sometimes asked to send apology letters to colleagues or deliver speeches expressing regret for their “selfishness” and “disrespect.” Of course, these are the most extreme cases–but they do occur, according to Momuri.</p>



<p>Some companies are notorious for resignation difficulties, excessive overtime and intense work pressure. So much so that they are labeled as “black companies.” The problem has become so severe that Japan’s Ministry of Health, Labour and Welfare’s Labor Standards Bureau has published a list of these companies to warn potential job seekers.</p>



<figure><img decoding="async" loading="lazy" width="1000" height="647" src="https://metropolisjapan.com/wp-content/uploads/2024/11/Mynavi-Corporation-resignation-agency-reason-metropolis-japan-1000x647.png" alt="" srcset="https://metropolisjapan.com/wp-content/uploads/2024/11/Mynavi-Corporation-resignation-agency-reason-metropolis-japan-1000x647.png 1000w, https://metropolisjapan.com/wp-content/uploads/2024/11/Mynavi-Corporation-resignation-agency-reason-metropolis-japan-618x400.png 618w, https://metropolisjapan.com/wp-content/uploads/2024/11/Mynavi-Corporation-resignation-agency-reason-metropolis-japan-250x162.png 250w, https://metropolisjapan.com/wp-content/uploads/2024/11/Mynavi-Corporation-resignation-agency-reason-metropolis-japan-768x497.png 768w, https://metropolisjapan.com/wp-content/uploads/2024/11/Mynavi-Corporation-resignation-agency-reason-metropolis-japan.png 1224w" sizes="(max-width: 1000px) 100vw, 1000px"><figcaption>Mynavi Corporation: Reasons for using resignation service. </figcaption></figure>



<h2><strong>Generational Change Could Be Japan’s Savior</strong></h2>



<p>A generational shift is underway in Japan, one that may eventually ease work-related pressures and their sometimes fatal consequences. Physical strain and mental stress contribute to both <em>karojisatsu</em>, or ‘overwork suicide,’ and <em>karoshi</em>, or ‘death by overwork’.&nbsp;</p>



<p>According to the <a href="https://www.niph.go.jp/journal/data/73-1/202473010003.pdf" target="_blank" rel="noreferrer noopener">Ministry of Health, Labor and Welfare</a>, the number of Japanese people who committed suicide due to their work rose from 1,935 cases in 2021 to 2,968 in 2022. This marked a record high. While the figure has since slightly decreased, the strain on workers remains severe.</p>



<p>Since they first appeared in 2018, resignation agencies have become an essential ally in Japan’s work culture shift. These agencies empower young workers to prioritize their mental well-being and advocate for change, providing a way out for those trapped in restrictive work environments.&nbsp;</p>



<p>This growing reliance on resignation services underscores a generational pushback against outdated norms and signals a potential transformation in Japan’s work culture. If these shifts continue, Japan’s younger generation may indeed pave the way toward a more balanced and sustainable future for the country’s workforce. Let’s hope they succeed.&nbsp;</p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It's Time to Replace TCP in the Datacenter (106 pts)]]></title>
            <link>https://arxiv.org/abs/2210.00714</link>
            <guid>42168997</guid>
            <pubDate>Mon, 18 Nov 2024 01:42:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2210.00714">https://arxiv.org/abs/2210.00714</a>, See on <a href="https://news.ycombinator.com/item?id=42168997">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="labstabs"><p>
    <label for="tabone">Bibliographic Tools</label></p><div>
      <h2>Bibliographic and Citation Tools</h2>
      <div>
          <p><label>
              
              <span></span>
              <span>Bibliographic Explorer Toggle</span>
            </label>
          </p>
          
        </div>
        
        
        
        
    </div>


    <p>
    <label for="tabtwo">Code, Data, Media</label></p><div>
      <h2>Code, Data and Media Associated with this Article</h2>
      

      
      
      
      
      
      
      
      
    </div>


      <p>
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label></p><div>
        <h2>Demos</h2>
        
        
        
        
      </div>
      <p>
      <label for="tabfour">Related Papers</label></p><div>
        <h2>Recommenders and Search Tools</h2>
        
        
        
        
        
      </div>

      <p>
      <label for="tabfive">
        About arXivLabs
      </label></p><div>
            <h2>arXivLabs: experimental projects with community collaborators</h2>
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv's community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: The App I Built to Help Manage My Diabetes, Powered by GPT-4o-Mini (125 pts)]]></title>
            <link>https://apps.apple.com/gb/app/islet-diabetes/id6453168642</link>
            <guid>42168491</guid>
            <pubDate>Mon, 18 Nov 2024 00:07:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apps.apple.com/gb/app/islet-diabetes/id6453168642">https://apps.apple.com/gb/app/islet-diabetes/id6453168642</a>, See on <a href="https://news.ycombinator.com/item?id=42168491">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="" data-test-bidi=""><p>Islet: Your Personalized Glucose &amp; Health Journal</p><p>Take control of your health with Islet, the ultimate personalized health journal app designed to seamlessly track and analyze your glucose levels. Perfect for individuals managing diabetes or anyone passionate about monitoring their well-being, Islet integrates effortlessly with Apple Health to provide a comprehensive and intuitive overview of your blood glucose, heart rate during workouts, and more.</p><p> Key Features</p><p>Automatic Glucose Sync</p><p>Effortlessly keep your glucose data up-to-date by syncing directly with Apple Health. No manual entries needed!</p><p>Heart Rate &amp; Workout Insights</p><p>Track your heart rate alongside glucose levels during workouts to see how exercise impacts your blood sugar. Optimize your fitness routines with actionable insights.</p><p>Personal Health Journal</p><p>Document daily activities, symptoms, and thoughts. Add notes to specific glucose readings or workouts for a deeper understanding of your health patterns.</p><p>Food Barcode Scanner</p><p>Log meals effortlessly by scanning barcodes. Instantly capture nutritional information, including macronutrients and carbohydrate content, making diet tracking accurate and easy.</p><p>Meal Logging</p><p>Monitor your meals and snacks with ease. Discover how your dietary choices influence your glucose levels for better blood sugar management.</p><p>Highs &amp; Lows Overview</p><p>Quickly identify instances of high and low glucose levels. Spot patterns and adjust your lifestyle or treatment plan effectively.</p><p>Trend Analysis</p><p>Visualize your glucose trends over time with clear graphs and charts. Correlate fluctuations with diet, exercise, and daily habits to manage your blood sugar efficiently.</p><p>Integrated Health Data</p><p>Consolidate all your health metrics in one place by integrating Islet with Apple Health. Gain a holistic view of your glucose levels, heart rate, workouts, and more.</p><p>Subscription Benefits</p><p>Unlock the full potential of Islet with a subscription:</p><p>Flexible Plans: Choose between Monthly or Annual subscriptions.</p><p>Exclusive Features: Access automatic glucose imports, comprehensive trend analysis, meal logging with the Food Scanner, and in-depth health insights.</p><p>Free Trial: Experience all premium features with a trial period before committing.</p><p>Privacy &amp; Security</p><p>Your privacy is our priority. Read our Privacy Policy and Terms of Use to learn more about how we protect your data.</p><p>Privacy Policy: https://anthropometric.godaddysites.com/</p><p>Terms of Use (EULA): https://www.apple.com/legal/internet-services/itunes/dev/stdeula/</p><p>Availability</p><p>Download Islet now on the App Store. Compatible with all iOS devices, Islet seamlessly integrates with Apple Health to provide a comprehensive solution for tracking and managing your glucose levels and overall health.</p><p>Get Started Today</p><p>Connect to Apple Health: Sync Islet with Apple Health to automatically import your glucose data and other health metrics.</p><p>Start Journaling: Use the Food Barcode Scanner to log meals, track workouts, and add health-related notes. Islet integrates this data with your glucose readings.</p><p>Review &amp; Reflect: Regularly analyze your health data, journal entries, and trends to gain valuable insights into your glucose management and overall health.</p><p>Take charge of your health journey with Islet. Subscribe today and achieve better health outcomes with effortless glucose and wellness tracking!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reverse Engineering iOS 18 Inactivity Reboot (360 pts)]]></title>
            <link>https://naehrdine.blogspot.com/2024/11/reverse-engineering-ios-18-inactivity.html</link>
            <guid>42167633</guid>
            <pubDate>Sun, 17 Nov 2024 21:50:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://naehrdine.blogspot.com/2024/11/reverse-engineering-ios-18-inactivity.html">https://naehrdine.blogspot.com/2024/11/reverse-engineering-ios-18-inactivity.html</a>, See on <a href="https://news.ycombinator.com/item?id=42167633">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-3312850032502776883">
<p>iOS 18 introduced a new inactivity reboot security feature. What does it protect from and how does it work? This blog post covers all the details down to a kernel extension and the Secure Enclave Processor.</p><h2>Security Before First Unlock / After First Unlock</h2><p>Did you know that entering your passcode for the first time after your phone starts is something very different then entering it later on to unlock your phone?</p><p>When initially entering your passcode, this unlocks a key store in the Secure Enclave Processor (SEP) that encrypts your data on an iPhone.</p><p>The state before entering your passcode for the first time is also called <b>Before First Unlock</b> (BFU). Due to the encrypted user data, your iPhone behaves slightly differently to later unlocks. You'll see that Face ID and Touch ID won't work and that the passcode is required. But there's more subtle things you might notice: Since Wi-Fi passwords are encrypted, your iPhone won't connect to Wi-Fi networks. If your SIM is not PIN-protected, your iPhone will still connect to cellular networks. That means, technically, you can still receive phone calls. Yet, if you receive a call, even if that number is in your contacts, the contact name won't be shown, as the contacts haven't been decrypted yet. Similarly, when you receive notifications about new messages, you'll see that you got messages, but you won't see any message previews. You can easily try this yourself!</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEihQqOut8zLHRxuz_g8ornTF1A-X69IoWpX8lZPnZBisvc1t80zBRGebsCj4x4Vz_6C_i_ImG5AszaaQ-rXnFEJeB1-Dfaj4bSBwajxyKLYoKQNdJ8dZZtaqhexYUc1rZL7w_6yXkMj5APTe30fHOMRlg3da55UCxSZhmTFRLWmBEh_iWVgPSyKuVCEQ_48"><img alt="" data-original-height="3936" data-original-width="7142" height="352" src="https://blogger.googleusercontent.com/img/a/AVvXsEihQqOut8zLHRxuz_g8ornTF1A-X69IoWpX8lZPnZBisvc1t80zBRGebsCj4x4Vz_6C_i_ImG5AszaaQ-rXnFEJeB1-Dfaj4bSBwajxyKLYoKQNdJ8dZZtaqhexYUc1rZL7w_6yXkMj5APTe30fHOMRlg3da55UCxSZhmTFRLWmBEh_iWVgPSyKuVCEQ_48=w640-h352" width="640"></a></p><p>In the <b>After First Unlock</b> (AFU) state, user data is decrypted. You can imagine this like a key safe that is kept open while iOS is running. Even when you see a lock screen, certain keys remain available to the operating system. This way, you stay connected to Wi-Fi networks and receive message notification previews, even when your iPhone is locked.</p><p>While it's more convenient, the AFU state is more susceptible to attacks. An attacker who can somehow bypass the lock screen can get access to decrypted data on the iPhone. To bypass the lock screen, an attacker does not necessarily need to know the passcode. Security vulnerabilities within iOS can allow attackers to get code execution and extract from an iPhone, even while it appears to be "locked".</p><p>Attackers with physical access to an iPhone have more security vulnerabilities to choose from. The attack surface is larger, as such attackers can exploit vulnerabilities in the USB stack or within wireless protocols, such as Wi-Fi, Bluetooth, or cellular, or even more invasive hardware attacks that involve opening the device. This larger attack surface tends to make exploits for these vulnerabilities cheaper on the gray market, as there's potentially more supply. Another factor that makes attacks cheaper is time – vulnerabilities that are publicly known by the vendor and patched in more recent software versions won't unlock new iPhones, but can unlock iPhones that were kept in AFU state for a long time that didn't get any software updates.</p><h2>Rumors about Rebooting iPhones</h2><p>In law enforcement scenarios, a lot of the forensically relevant data is available in the AFU state. Law enforcement takes advantage of this and often keeps seized iPhones powered on, but isolated from the Internet, until they can extract data. This time might be necessary to wait for an exploit to be available or for legal reasons, such as getting a warrant.</p><p>However, thieves and other criminals are also interested in getting this kind of access after stealing a device. It gives them access to bank accounts and other valuable information, by far exceeding what the iPhone itself would be worth, or which might be used for blackmail. People reuse their passwords often, and getting access to the iCloud account may allow a thief to reset activation lock for the device, increasing the resale value.</p><p>A recent&nbsp;<a href="https://www.404media.co/police-freak-out-at-iphones-mysteriously-rebooting-themselves-locking-cops-out/">news article by 404 media</a>&nbsp;(while paywalled, the most important information is also contained in the related&nbsp;<a href="https://x.com/josephfcox/status/1854615490087551327">Tweet</a>)&nbsp;reported on a law enforcement document about suspicious iPhone reboots. This document makes two interesting claims:</p><ol><li>iPhones on iOS 18 will reboot, even when completely isolated from wireless networks.</li><li>iPhones on iOS 18 will tell other iPhones on lower iOS versions to reboot – wirelessly!</li></ol><p>Especially the second claim would be huge if true. If anyone figured out how this works, they could build a large TV-Be-Gone for iPhones, forcing reboots over the air on hundreds of iPhones simultaneously. Would Apple really build such a feature into an iPhone?</p><p>Knowing a thing or two about the Apple wireless ecosystem, my interest was piqued, and I had to go down the rabbit hole!</p><h2>Discovery of Inactivity Reboot</h2><p>When Apple adds new features, they usually don't hide this very well. Apple software contains a lot of debug strings, which hint at new functionality. Blacktop maintains a <a href="https://github.com/blacktop/ipsw-diffs">git repository</a> of strings found in iOS, which keeps a nice version history. I decided to do the most low-effort thing I could think of: just search for "reboot".</p><div><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEi4XBZbJShDJ9GPps2OujHTkV_KZn-KcijaJX34M5GsyKk_TMRsUrYkxkft2LiAMa8V62VKXagqP5Q2lQntBlRo9lobsf-7qo39M-DzpWwGgfOioKq8LQs8xAnKIRNrC0b9RlA4FZWYQg4Q1yvcILbr_Lp_Nm0ULdRG9l13_sSB1R5JIP7eYJzYwy9QAwU2"><img alt="" data-original-height="1544" data-original-width="3000" height="330" src="https://blogger.googleusercontent.com/img/a/AVvXsEi4XBZbJShDJ9GPps2OujHTkV_KZn-KcijaJX34M5GsyKk_TMRsUrYkxkft2LiAMa8V62VKXagqP5Q2lQntBlRo9lobsf-7qo39M-DzpWwGgfOioKq8LQs8xAnKIRNrC0b9RlA4FZWYQg4Q1yvcILbr_Lp_Nm0ULdRG9l13_sSB1R5JIP7eYJzYwy9QAwU2=w640-h330" width="640"></a></p><br></div><p>Bingo, that third hit looks good: "inactivity_reboot". The fact that it's in <span>keybagd</span> is interesting: this daemon is related to the key store that is unlocked on the first unlock.</p><p>A second search for only inactivity reboot shows the string starts occurring in iOS 18.1 and iOS 18.2. In iOS 18.2, the string changed from "inactivity_reboot" to "inactivity_reboot_enabled", hinting towards more potential changes in the latest iOS 18.2 betas.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgBjM82vk9aCJNzlk-9gbRKwc9y83Z9g9WZsPDUQXxB4oYXsuA-QntGrp4pMalJFUPFqETW0fxy3IfCrlziX946r_ymJ8G0a-mtIkFEzp1DoXPRcq9RUi1KOOtjgK8XfRJJcghzLLgNsM7JfvByjxqfWz2JqHPGEEkXpttCbMvdoCW7FPrfDU-psLHbdgD-"><img alt="" data-original-height="1194" data-original-width="2334" height="328" src="https://blogger.googleusercontent.com/img/a/AVvXsEgBjM82vk9aCJNzlk-9gbRKwc9y83Z9g9WZsPDUQXxB4oYXsuA-QntGrp4pMalJFUPFqETW0fxy3IfCrlziX946r_ymJ8G0a-mtIkFEzp1DoXPRcq9RUi1KOOtjgK8XfRJJcghzLLgNsM7JfvByjxqfWz2JqHPGEEkXpttCbMvdoCW7FPrfDU-psLHbdgD-=w640-h328" width="640"></a></p><p>Something that was still unclear to me at that point is: How long does it take for inactivity reboot to be triggered? A new&nbsp;<a href="https://www.404media.co/apple-quietly-introduced-iphone-reboot-code-which-is-locking-out-cops/">article by 404 media</a>&nbsp;claimed that it was 3-4 days. So I updated my SRD to the latest beta and made a time lapse.</p><p><iframe allowfullscreen="" height="266" src="https://www.youtube.com/embed/QOe2rDKOWMk" width="320" youtube-src-id="QOe2rDKOWMk"></iframe></p><p>Turns out, the inactivity reboot triggers exactly after 3 days (72 hours). The iPhone would do so despite being connected to Wi-Fi. This confirms my suspicion that this feature had nothing to do with wireless connectivity.</p><h2>Reverse Engineering Inactivity Reboot</h2><p>Let's reverse engineer what's changed! Which security guarantees does it provide?</p><p>Here is a high-level overview of what I found:</p><ul><li>The Secure Enclave Processor (SEP) keeps track on when your phone was last unlocked. If that last unlock time exceeds 3 days, the SEP tells the <span>AppleSEPKeyStore</span> kernel module that the time was exceeded.</li><li>The <span>AppleSEPKeyStore</span> kernel module informs user space to initiate a reboot. <span>SpringBoard</span> will then gracefully terminate all user-space processes. This prevents potential data loss upon reboot.</li><li>If the <span>AppleSEPKeyStore</span> kernel module finds the iPhone to still be powered on after it should have rebooted, the kernel will panic. This case should never happen, unless someone tries to tamper with inactivity reboot.</li><li>The <span>AppleSEPKeyStore</span> kernel module writes an NVRAM variable <span>aks-inactivity</span>. After the iPhone rebooted, <span>keybagd</span> reads this variable and, if set, sends an analytics event to Apple including how long the iPhone was not unlocked.</li></ul><p>The remainder of this post shows how I figured this out and what security implications the underlying design has.</p><h2>Indicators in Sysdiagnose</h2><p>From my search in ipsw-diffs, I knew there were some log messages that are printed on reboot. At the same time as I started looking them statically, I knew I had to see them actually logged for myself.</p><p>After my phone rebooted after three days, I took a sysdiagnose and searched for these messages. When doing this yourself, make sure that you unlocked the device before making the sysdiagnose. Otherwise, events from before the reboot will be missing.</p><p>In the&nbsp;<span>AppleSEPKeyStore</span>&nbsp;messages, there are the following entries around the inactivity reboot:</p><p><span>default</span><span>	</span><span>2024-11-17 01:35:14.341697 +0100</span><span>	</span><span>kernel</span><span>	</span><span>"AppleSEPKeyStore":3846:0: <span>notifying user space of inactivity reboot</span><br></span><span>default</span><span>	</span><span>2024-11-17 01:35:14.341766 +0100</span><span>	</span><span>kernel</span><span>	</span><span>"AppleSEPKeyStore":12598:31: operation failed (sel: 35 ret: e00002f0)<br></span><span>default</span><span>	</span><span>2024-11-17 01:35:14.342053 +0100</span><span>	</span><span>kernel</span><span>	</span><span>"AppleSEPKeyStore":12598:31: operation failed (sel: 35 ret: e00002f0)<br></span><span>default</span><span>	</span><span>2024-11-17 01:35:34.958218<br></span><span><span>[reboot occurs]</span><br>+0100</span><span>	</span><span>kernel</span><span>	</span><span>"AppleSEPKeyStore":331:0: starting (BUILT: Oct 26 2024 08:16:35) ("normal" variant 🌽 , 1827.60.43)<br></span><span>default</span><span>	</span><span>2024-11-17 01:35:34.958381 +0100</span><span>	</span><span>kernel</span><span>	</span><span>"AppleSEPKeyStore":476:0: _sep_enabled = 1</span></p><p>For more context, these are the unfiltered log messages before the reboot is initiated:</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjh3CycIuHLRgh46JB32Vr3WAFbEPzGzusiH_hWUUO248aoq27ntRBvDJuUhwVzF2O1p3BIYYhHoYam1IssPyBeI581bGm7frA4yFzIFA8gWerWJyuHfxDhRstUeo9eUcjG0xHcYoFeFi0S67dtuFRezYoJsz3ZMuhL5nce9o6jE7pH_Qb7tSedUNY0TxDn"><img alt="" data-original-height="1848" data-original-width="2518" height="470" src="https://blogger.googleusercontent.com/img/a/AVvXsEjh3CycIuHLRgh46JB32Vr3WAFbEPzGzusiH_hWUUO248aoq27ntRBvDJuUhwVzF2O1p3BIYYhHoYam1IssPyBeI581bGm7frA4yFzIFA8gWerWJyuHfxDhRstUeo9eUcjG0xHcYoFeFi0S67dtuFRezYoJsz3ZMuhL5nce9o6jE7pH_Qb7tSedUNY0TxDn=w640-h470" width="640"></a></p><h2>Reverse Engineering the <span>SEPKeyStore</span> Kernel Extension</h2><p>The latest iOS kernel can be downloaded using the following <a href="https://github.com/blacktop/ipsw"><span>ipsw</span></a> command:</p><p><span>ipsw download appledb --device iPhone17,3 --os iOS --version '18.2 beta 2' --kernel</span></p><p>This will download and decompress the kernel. For further analysis, I loaded the whole kernel cache into Binary Ninja. <span>ipsw</span> also supports splitting the kernel into its modules (called "extensions" on iOS). The latest version of Ghidra also has decent support for the iOS kernel. So there's a lot of tools to choose from for this analysis.</p><p>I also downloaded an older kernel where Apple accidentally included symbols and manually diffed these versions with a focus on the code related to inactivity reboot. The kernel has three strings relating to the feature:</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEh0ZxgYUnJU1elCjWJXqzi-qkQEehkAKd3dkIH4UAUmxk6dwL3iiZ2LFlo3IvYug9d_qFuuiV6AnibEN8o8cEN-76IfazheZdPhIkm88ZYS59KADrKHJcup3X1mwGQ-9BCkdOJt7L83mSLBSAiayIIxM4AXVjMtAKRHwYaccHqKMeYAejLtvi9TGxkrY0Xl"><img data-original-height="356" data-original-width="1386" height="164" src="https://blogger.googleusercontent.com/img/a/AVvXsEh0ZxgYUnJU1elCjWJXqzi-qkQEehkAKd3dkIH4UAUmxk6dwL3iiZ2LFlo3IvYug9d_qFuuiV6AnibEN8o8cEN-76IfazheZdPhIkm88ZYS59KADrKHJcup3X1mwGQ-9BCkdOJt7L83mSLBSAiayIIxM4AXVjMtAKRHwYaccHqKMeYAejLtvi9TGxkrY0Xl=w640-h164" width="640"></a></p><p>"notifying user space of inactivity reboot" is the string we already know from the sysdiagnose. It belongs to the function <span>AppleKeyStore::handle_events</span>, which polls for SEP events in the background. The following screenshot shows it in more context after reverse engineering and some renaming of functions.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhqa9iNrqdm95LKnxB9k2Nbw9Pp2lDkiYg6NVgZW3PFiO_0DntNVYm6QRwV-0Er5puxU_Y7m3kS1zEbuxcU1IGOOTU8ZKwZsfmLEPiZy_XJb1KWi1_dQAI7_iaV7pmymZS0NtE2CLVTfH0Km5pfKeETc27gLtKDgPXAgKmwZEQESfk0No_7H-CEqVZ5U33e"><img alt="" data-original-height="1936" data-original-width="3024" height="410" src="https://blogger.googleusercontent.com/img/a/AVvXsEhqa9iNrqdm95LKnxB9k2Nbw9Pp2lDkiYg6NVgZW3PFiO_0DntNVYm6QRwV-0Er5puxU_Y7m3kS1zEbuxcU1IGOOTU8ZKwZsfmLEPiZy_XJb1KWi1_dQAI7_iaV7pmymZS0NtE2CLVTfH0Km5pfKeETc27gLtKDgPXAgKmwZEQESfk0No_7H-CEqVZ5U33e=w640-h410" width="640"></a></p><p>The first string, "max inactivity window expired, failed to reboot the device", is the kernel panic in case that the iPhone failed to reboot.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgKeMPdV7mTxwu4QbCAEk4DnF6wdxUtpApFTkmHV7aYJjkkyVz4BdCN5tuFIJ5FsfvYqhjbDtdGic19Rt5cHbdoZ6S5Jl_lCTd7JhkuG8IzWCyg5h0WgXNKYa3xuaqlTOwuoG7SoATovv-m6nXthdMMaIpWeQ90xCh1IrqfzQorVcuFuH7TPuop6A6ysjBS"><img alt="" data-original-height="166" data-original-width="1338" height="80" src="https://blogger.googleusercontent.com/img/a/AVvXsEgKeMPdV7mTxwu4QbCAEk4DnF6wdxUtpApFTkmHV7aYJjkkyVz4BdCN5tuFIJ5FsfvYqhjbDtdGic19Rt5cHbdoZ6S5Jl_lCTd7JhkuG8IzWCyg5h0WgXNKYa3xuaqlTOwuoG7SoATovv-m6nXthdMMaIpWeQ90xCh1IrqfzQorVcuFuH7TPuop6A6ysjBS=w640-h80" width="640"></a></p><p>For more context, the panic is called by the function&nbsp;<span>AppleKeyStore::handle_device_state_return</span>. There are multiple paths that invoke this handler through many layers of abstraction, which have to do with the UserClient but also SEP states.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgi_v0P9i541Jg-d9THXz-ka-CvldLwbzLqWkCDFt6rEuKszDs8BQFtfhbYU2sXAqeOkMKkCrG47wShd3bzw-PurLLi766x86d1d3K_7T6oP_7R6y6bhBmCfAnKPnyuQLRVqlxjOaiJPkXl1UW0Yg5asqovDlzSJVqJdsks2pfF0oDKTw55nKSZ6EAq90yO"><img alt="" data-original-height="1584" data-original-width="1886" height="537" src="https://blogger.googleusercontent.com/img/a/AVvXsEgi_v0P9i541Jg-d9THXz-ka-CvldLwbzLqWkCDFt6rEuKszDs8BQFtfhbYU2sXAqeOkMKkCrG47wShd3bzw-PurLLi766x86d1d3K_7T6oP_7R6y6bhBmCfAnKPnyuQLRVqlxjOaiJPkXl1UW0Yg5asqovDlzSJVqJdsks2pfF0oDKTw55nKSZ6EAq90yO=w640-h537" width="640"></a></p><p>With the <a href="https://github.com/elbiazo/calltree">calltree plugin</a>, we can see all the incoming calls to this function.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjfKMpgWgr5MpdI1gKiXgnP860eGS0w4h7DqW8nWfOX5D2I8WiUPGHIGjfFeDOwJ_zKqjjv59fIAdeZQNQ-OPCzdNTM08N12sMg2Cg4M042iaWFkkgYlYeUsfk7njj6QwChF61fnDebQbbbDx1u6QKj33-cJnAqNBLo4f8LL68GNiodwTMwEfuptfnsadrL"><img alt="" data-original-height="660" data-original-width="1054" height="400" src="https://blogger.googleusercontent.com/img/a/AVvXsEjfKMpgWgr5MpdI1gKiXgnP860eGS0w4h7DqW8nWfOX5D2I8WiUPGHIGjfFeDOwJ_zKqjjv59fIAdeZQNQ-OPCzdNTM08N12sMg2Cg4M042iaWFkkgYlYeUsfk7njj6QwChF61fnDebQbbbDx1u6QKj33-cJnAqNBLo4f8LL68GNiodwTMwEfuptfnsadrL=w640-h400" width="640"></a></p><p>Now to the last string, "aks-inactivity". We can see that this is a property that is set in the IORegistry.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEh3-JpRsuVcoQvKveSUKY47zOeJl3FVm3VAvtmG17u_f_wdfYYvQaS3JXA7kIAequNfdiQ9s-tSHQrOAtlRdVoPmJD1lJ2K5RZExe5vNgHNMZsv_JPgY5k-RflZTVqnC1-_FdZCmn6EZnE_fPiIWTIYPMKPh_oN_EkQP5YEk9eE1i-x-Wy-_YCEC9GvNus5"><img alt="" data-original-height="1000" data-original-width="2948" height="218" src="https://blogger.googleusercontent.com/img/a/AVvXsEh3-JpRsuVcoQvKveSUKY47zOeJl3FVm3VAvtmG17u_f_wdfYYvQaS3JXA7kIAequNfdiQ9s-tSHQrOAtlRdVoPmJD1lJ2K5RZExe5vNgHNMZsv_JPgY5k-RflZTVqnC1-_FdZCmn6EZnE_fPiIWTIYPMKPh_oN_EkQP5YEk9eE1i-x-Wy-_YCEC9GvNus5=w640-h218" width="640"></a></p><p>Its counterpart is in <span>keybagd</span> in user space. When <span>keybagd</span> is initialized, it checks for this variable, issues an analytics event, and then deletes it. This analytics event probably helps Apple optimize the time window, but we can ignore it for the core functionality.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiDi2dNE_vIsKY6UD2LhnRy31qXgNMeBdWA-Bk52lQV7aK6VWZqxudXe0wJRPDHV4rPDxi44CUOyKQpm7-Yks93eATITRqqbSkOME6xM9o7W7N1NYKsBBP65yS97cKtq6TLP6ChFQxF1gFg-NK84_q0lCXOubU1ZNr9-XIxdEKjE9dc3QwXRKe31KJ9j5oH"><img alt="" data-original-height="804" data-original-width="1692" height="304" src="https://blogger.googleusercontent.com/img/a/AVvXsEiDi2dNE_vIsKY6UD2LhnRy31qXgNMeBdWA-Bk52lQV7aK6VWZqxudXe0wJRPDHV4rPDxi44CUOyKQpm7-Yks93eATITRqqbSkOME6xM9o7W7N1NYKsBBP65yS97cKtq6TLP6ChFQxF1gFg-NK84_q0lCXOubU1ZNr9-XIxdEKjE9dc3QwXRKe31KJ9j5oH=w640-h304" width="640"></a></p><p>Something that I couldn't find in the kernel, even with the knowledge that it was 72 hours, was this particular time window. I couldn't find any numbers that matched 72 hours. So how does the phone know when to reboot?</p><p>While there are some references to time-related functionality in the <span>SEPKeyStore</span> kernel extension, none of these compare a value to 72 hours. These references were quite simple to find and did not differ much from the older kernel version without inactivity reboot, so it doesn't seem like the functionality was added here.</p><div><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEitDjU-0SbfmaNcNosZlWHx0dTUnfmIjSVxTW_xA1bgEVwSmgp_uob_P62GyCveo8CdmN7MwQ8rX_J1qI1U4KMTPxgUigfGwgRYlWzvkmhoqepnpr-KkBDhQ2WakbMxs_YGsuiOIMVoXq19cu4TEzqBHHxwQEMJrR1xAVTF0LRk6_2-W9L_m4k7LvDJw9Bb"><img alt="" data-original-height="936" data-original-width="1610" height="372" src="https://blogger.googleusercontent.com/img/a/AVvXsEitDjU-0SbfmaNcNosZlWHx0dTUnfmIjSVxTW_xA1bgEVwSmgp_uob_P62GyCveo8CdmN7MwQ8rX_J1qI1U4KMTPxgUigfGwgRYlWzvkmhoqepnpr-KkBDhQ2WakbMxs_YGsuiOIMVoXq19cu4TEzqBHHxwQEMJrR1xAVTF0LRk6_2-W9L_m4k7LvDJw9Bb=w640-h372" width="640"></a></p><p>However, the <span>SEPKeyStore</span> communicates with the SEP co-processor. In the functions I identified, reboots are related to some SEP states. Could it be the SEP itself that checks the time?</p><h2>Reverse Engineering the Secure Enclave Processor</h2><p>The SEP is one of Apple's most protected secrets. In contrast to most other firmware on the iPhone, the firmware for the SEP is encrypted.</p><p>Luckily for us, <a href="https://twitter.com/nyan_satan">@nyan_satan</a> recently leaked <a href="https://theapplewiki.com/wiki/Keys:CrystalBSeed_22B5069a_(iPhone16,1)">SEP firmware encryption keys for iOS 18.1 beta 6</a>, just eta wen Apple introduced inactivity reboot. (Thank you!! 🎉 And Apple, if you're reading this, why not ship the SEP unencrypted?) Using <span>ipsw</span>, we can download the SEP firmware as follows:</p><p><span>ipsw download appledb --device iPhone16,1 --os iOS --version '18.1 beta 6' --pattern "sep-firmware.d83.RELEASE.im4p"</span></p><p>With the leaked keys, we can decrypt the firmware:</p><p><span>pyimg4 im4p extract --iv 6705fb216080e19667dbcf71f532ae73 --key 4ea9db4c2e63a316a6854c83e2f5c81fd102ad40160b8998b5f9b16838b7116e -i sep-firmware.d83.RELEASE.im4p -o sep-firmware.d83.RELEASE.im4p.e</span></p><p>Loading this into Binary Ninja is a bit tricky. We can guess that the architecture is 64-bit ARM little endian. But there's no metadata where the firmware has to be loaded to. Being lazy and not wanting to spend time on writing a firmware loader, I used Binary Ninja's Triage feature to auto-detect the most likely address. Note that the firmware seems to have multiple fragments and there's multiple potential load addresses. I picked&nbsp;0x80090000ffc80000, which worked well for me.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEguwmjRE1k8u5npWzXhq5cs8lRMkyQqgECTzTuq_zaLuWDr5J8AyZtKkN64XixVpkczXFfSsQknPPclpjICoz7GrfRWBINwETutIXB-RF45-vlem8PavZPTyOTK4k6BmjTyY2l7xcTbmC1g6gooiaAe9DtqaiGavX0ajfnofIghAADa8xtmsLPe7bMNN-cW"><img alt="" data-original-height="580" data-original-width="514" height="400" src="https://blogger.googleusercontent.com/img/a/AVvXsEguwmjRE1k8u5npWzXhq5cs8lRMkyQqgECTzTuq_zaLuWDr5J8AyZtKkN64XixVpkczXFfSsQknPPclpjICoz7GrfRWBINwETutIXB-RF45-vlem8PavZPTyOTK4k6BmjTyY2l7xcTbmC1g6gooiaAe9DtqaiGavX0ajfnofIghAADa8xtmsLPe7bMNN-cW=w355-h400" width="355"></a><span>&nbsp;</span></p><p>There's only little known about the SEP. The best information I could come up with is a <a href="https://www.blackhat.com/docs/us-16/materials/us-16-Mandt-Demystifying-The-Secure-Enclave-Processor.pdf">presentation dating back to 2016</a> – but that's better than nothing! What's good to know is that the SEP firmware is structured into apps, so I'm guessing the other base addresses the triage found may correspond to the other apps' address spaces. The app that communicates with the SEPKeyStore is called <span>sks</span> (see slide 86 of the presentation). Not a lot of information, but enough to start reverse engineering!</p><p>Looking at strings, it looks like the architecture of apps running inside the SEP hasn't changed much since 2016. The <span>SEPKeyStore</span>-related app is still called <span>sks</span>:</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhfPY-EilBBTqlkWuLpWvFXx3bok5syhn8KZO15LWApKKMIN2wgcAMK-znjMMpk5Kv3xq0uVQOBI8UK4Ld8QpEmDQGCOpEq4shM1Td8ywQ3xHmdjl8Fi7lrCYg7iWTsGxY5fA-GcTTk-3z6Z4qb4FCGZ7AE1Lbcxy_KHPQ9-blxMVrSjLRtSUNdZxk8iluZ"><img alt="" data-original-height="436" data-original-width="1248" height="224" src="https://blogger.googleusercontent.com/img/a/AVvXsEhfPY-EilBBTqlkWuLpWvFXx3bok5syhn8KZO15LWApKKMIN2wgcAMK-znjMMpk5Kv3xq0uVQOBI8UK4Ld8QpEmDQGCOpEq4shM1Td8ywQ3xHmdjl8Fi7lrCYg7iWTsGxY5fA-GcTTk-3z6Z4qb4FCGZ7AE1Lbcxy_KHPQ9-blxMVrSjLRtSUNdZxk8iluZ=w640-h224" width="640"></a></p><p>The SEP has almost no debug strings, making it tougher to reverse engineer. Here is what the initialization function for the <span>SEPKeyStore</span> looks like after some manual annotations ("sth" stands for "something" – I didn't go too deep into understanding the specifics here):</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhPrpu1jil4mrXVyRP1LvrLjsTVNQ2jC3QHXLJGZaSZ8oAFgGC97jZf0T00HGkzTiANO1MlZA6KSoUmiDzhEkx0Zk0GQcYD6MCAa1RuX9cJnQNBFWblQ_k5j0ZqBUltQ1B2Wfks6vmIcHu6MsDrJmmtw270W3SQ9bBTTcXai7Sc4g3Y6Uj-3PJQP71Z9zbE"><img alt="" data-original-height="458" data-original-width="1614" height="182" src="https://blogger.googleusercontent.com/img/a/AVvXsEhPrpu1jil4mrXVyRP1LvrLjsTVNQ2jC3QHXLJGZaSZ8oAFgGC97jZf0T00HGkzTiANO1MlZA6KSoUmiDzhEkx0Zk0GQcYD6MCAa1RuX9cJnQNBFWblQ_k5j0ZqBUltQ1B2Wfks6vmIcHu6MsDrJmmtw270W3SQ9bBTTcXai7Sc4g3Y6Uj-3PJQP71Z9zbE=w640-h182" width="640"></a></p><p>Within its main function, we can find multiple other functions executed before a service workloop starts. However, there's plenty of code. How do we focus on things that are related to the inactivity reboot?</p><p>Let's recall that we're looking for something that resembles 72 hours. In the kernel, times are usually measured in seconds or in microseconds. For example 72 hours are&nbsp;259200 seconds (0x3f480). But looking for this value (or for 259200000000, in microseconds, or any other sensible units) in the binary won't return any matches.</p><p>Using the <a href="https://godbolt.org/">compiler explorer</a>, we can see why: Optimizations...</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjiVp3eTE2aXqnNiZMIJu7uRjTEpEahpDlVxzexq0iVsFmD01Ux8xTq8OUcNECXwZ099DnhfdpBvb_ztjNnzHlQ6XV9nbIOOYmabejFVA-D3Iq-cAt_nK2IktXQ7I0XF-6xONp4MgNgcjsCa1_mRTAiJ0WgeKRiac-QQpo8SBthBPDV3aK2UrLwNS7nUtRk"><img alt="" data-original-height="1062" data-original-width="3024" height="225" src="https://blogger.googleusercontent.com/img/a/AVvXsEjiVp3eTE2aXqnNiZMIJu7uRjTEpEahpDlVxzexq0iVsFmD01Ux8xTq8OUcNECXwZ099DnhfdpBvb_ztjNnzHlQ6XV9nbIOOYmabejFVA-D3Iq-cAt_nK2IktXQ7I0XF-6xONp4MgNgcjsCa1_mRTAiJ0WgeKRiac-QQpo8SBthBPDV3aK2UrLwNS7nUtRk" width="640"></a></p><p>Rather than looking the full time in bytes in reverse byte order, we're looking for assembly instructions that load parts of the timespan into a register.</p><p>Binary Ninja knows how to reverse this optimization, and allows us to search in its intermediate representations, instead of looking for raw bytes. In our case, we know that we're looking for a constant.</p><div><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiAksVrxNBr5xGLwoRNEbxzVIMoOMMapRcHKVGaB_R_O3AWmrvNcMwINtPo1zaXnlNxbv0tV8EWXAg0jnHgskU22-Ui71tPAFkaoXud_POuCAdbcoAwPg8HNbjfeo6qYKmsaM9IlQR6muWihOSzBzauHF4PitCFVl-DfjRt6RUUZrBEPLU3Gu2WEusxU8hE"><img alt="" data-original-height="808" data-original-width="1100" height="294" src="https://blogger.googleusercontent.com/img/a/AVvXsEiAksVrxNBr5xGLwoRNEbxzVIMoOMMapRcHKVGaB_R_O3AWmrvNcMwINtPo1zaXnlNxbv0tV8EWXAg0jnHgskU22-Ui71tPAFkaoXud_POuCAdbcoAwPg8HNbjfeo6qYKmsaM9IlQR6muWihOSzBzauHF4PitCFVl-DfjRt6RUUZrBEPLU3Gu2WEusxU8hE=w400-h294" width="400"></a>&nbsp;</p></div><p>We find only two matches:</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhJwjCy5EDHg2kK4t-h4GW_KqEKu7X_1d-jfbjm1LeYIMKF_YEA4oAZaWK20ZujWctbqOhXaseMbH1DvWp6H-O4flgmN419gJorpk653jeG90Jr540JGOV92Up4e7jyUq9CW1X2VPhFEncwOT-qWzB0ZFm6BN3vLcA01fZVH6WR9jJ5ywFRevYjyFVX4g5c"><img alt="" data-original-height="250" data-original-width="1632" height="98" src="https://blogger.googleusercontent.com/img/a/AVvXsEhJwjCy5EDHg2kK4t-h4GW_KqEKu7X_1d-jfbjm1LeYIMKF_YEA4oAZaWK20ZujWctbqOhXaseMbH1DvWp6H-O4flgmN419gJorpk653jeG90Jr540JGOV92Up4e7jyUq9CW1X2VPhFEncwOT-qWzB0ZFm6BN3vLcA01fZVH6WR9jJ5ywFRevYjyFVX4g5c=w640-h98" width="640"></a></p><p>And here it is – a function that compares various times, including 3 days, which is related to the <span>sks</span> application's main function. The result of this time comparison is used to create a message, which is likely sent to the <span>SEPKeyStore</span> kernel extension. Creating a new enum makes it more readable:</p><p><span>enum times : uint32_t</span></p><p><span>{</span></p><p><span>    _3_days = 0x3f480,</span></p><p><span>    _2_days = 0x2a301,</span></p><p><span>    _1_days = 0x15181,</span></p><p><span>    `_2.5h` = 0xe11</span></p><p><span>};</span></p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEg0JC9kKOelUjXI7AmxnIJf4160QuVQ6JibF1gYvk_QIc3d-0mDP29IvKaxuYQS9Hou8u3gfcpJHf11qznWDYY2QQKSZhDWcGusssdg_kxo4fH9aJbveSJxRdo5OpLS7az18e9wfpKxsjTZqKeQdA6eB45tfr4y0offM4mmbADozT7PH3H71ypvq_83cql6"><img alt="" data-original-height="1114" data-original-width="1330" height="535" src="https://blogger.googleusercontent.com/img/a/AVvXsEg0JC9kKOelUjXI7AmxnIJf4160QuVQ6JibF1gYvk_QIc3d-0mDP29IvKaxuYQS9Hou8u3gfcpJHf11qznWDYY2QQKSZhDWcGusssdg_kxo4fH9aJbveSJxRdo5OpLS7az18e9wfpKxsjTZqKeQdA6eB45tfr4y0offM4mmbADozT7PH3H71ypvq_83cql6=w640-h535" width="640"></a></p><p>This function is used in a context to initialize a struct, which is likely a message being sent from the SEP to the kernel extension.</p><div><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhIi1ytOnwIHz3FfehB3gzVzc4s2L1oOKyB_huAhQ_tMDIPvTFs3ffHMb8sICRwqbc8_dDY3r340BZpYdyjx9n1hRnMUcHIxRUEhcoPrBiiTA4tevJ3E_X8rxujzB1hq2Ncm0mLX3nzlHc41BfoAMsJcqZZWcaEg0_ElGxbhQS86UZeo9TXH4PdrzeFqIza"><img alt="" data-original-height="1626" data-original-width="1780" height="584" src="https://blogger.googleusercontent.com/img/a/AVvXsEhIi1ytOnwIHz3FfehB3gzVzc4s2L1oOKyB_huAhQ_tMDIPvTFs3ffHMb8sICRwqbc8_dDY3r340BZpYdyjx9n1hRnMUcHIxRUEhcoPrBiiTA4tevJ3E_X8rxujzB1hq2Ncm0mLX3nzlHc41BfoAMsJcqZZWcaEg0_ElGxbhQS86UZeo9TXH4PdrzeFqIza=w640-h584" width="640"></a></p><p>I didn't end up reverse engineering much more of the SEP, but this seems to confirm that it's really the SEP that keeps track of how long the phone hasn't been unlocked. This design makes sense to me, since the SEP is involved in every unlock, and is also hardened against tampering, even if an exploit against the main kernel is used, so it's a good place to anchor a mitigation like this.</p></div><div><h2>A Mitigation Only Against Cops?</h2><h2><p>While the media coverage so far framed this mitigation as primarily targeting law enforcement, it also a huge security improvement against theft. Outdated law enforcement equipment often finds its way to eBay and other similar platforms for rather cheap price tags. However, thieves won't have the financial and legal means to obtain up-to-date exploits to unlock iPhones within 3 days of getting them. That's another reason why it's important to keep your device updated!</p><p>On the other hand, law enforcement can and will have to adjust their process, and act faster than before. The first forensic tooling companies already announced that they're able to coordinate these steps within 24 hours! (Note that this also indicates that they only have exploits for AFU state... 🤡)</p></h2><h2>Key Takeaways</h2></div><div><p>This feature is not at all related to wireless activity. The law enforcement document's conclusion that the reboot is due to phones wirelessly communicating with each other is implausible. The older iPhones before iOS 18 likely rebooted due to another reason, such as a software bug.</p><p>The time measurement and triggering of the reboot is in the SEP, which communicates with the <span>SEPKeyStore</span> kernel extension to perform the reboot. It is likely that using an external time source provided over the Internet or cellular networks to tamper with timekeeping will not influence the 3-day timer.</p><p>Security-wise, this is a very powerful mitigation. An attacker must have kernel code execution to prevent an inactivity reboot. This means that a forensic analyst might be able to delay the reboot for the actual data extraction, but the initial exploit must be run within the first three days.</p><p>Inactivity reboot will change the threat landscape for both thieves and forensic analysts, but asymmetrically so: while law enforcement is under more time pressure, it likely completely locks out criminals from accessing your data to get into your bank accounts and other valuable information stored on your iPhone.</p><p>Interested in reverse engineering? Follow me on <a href="https://www.youtube.com/@jiskac">YouTube</a> and <a href="https://bsky.app/profile/naehrdine.bsky.social">BlueSky</a> for updates.</p></div></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You could have designed state of the art positional encoding (121 pts)]]></title>
            <link>https://fleetwood.dev/posts/you-could-have-designed-SOTA-positional-encoding</link>
            <guid>42166948</guid>
            <pubDate>Sun, 17 Nov 2024 20:31:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fleetwood.dev/posts/you-could-have-designed-SOTA-positional-encoding">https://fleetwood.dev/posts/you-could-have-designed-SOTA-positional-encoding</a>, See on <a href="https://news.ycombinator.com/item?id=42166948">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><blockquote>
<p><strong>Gall's Law</strong> <br>
<!-- -->A complex system that works is invariably found to have evolved from a simple
system that worked <br>
<!-- -->John Gall</p>
</blockquote>
<p>This post walks you through the step-by-step discovery of state-of-the-art positional encoding in transformer models. We will achieve
this by iteratively improving our approach to encoding position, arriving at <strong>Ro</strong>tary <strong>P</strong>ostional <strong>E</strong>ncoding (RoPE) used in the latest <a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/" rel="nofollow" target="_blank">LLama 3.2</a> release and most modern transformers. This post intends to limit the mathematical knowledge required to follow along, but some basic linear algebra, trigonometry and understanding of self attention is expected.</p>
<h2>Problem Statement</h2>
<blockquote>
<p>You shall know a word by the company it keeps <br>
<!-- -->John Rupert Firth</p>
</blockquote>
<p>As with all problems, it is best to first start with understanding <strong>exactly</strong> what we are trying to achieve. The self attention mechanism in transformers is utilized to understand relationships
between tokens in a sequence. Self attention is a <strong>set</strong> operation, which
means it is <strong>permutation invariant</strong> (order does not matter). If we do not
enrich self attention with positional information, many important relationships are
<strong>incapable of being determined</strong>.</p>
<p>This is best demonstrated by example.</p>
<h2>Motivating Example</h2>
<p>Consider the following sentences:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>The&nbsp;dog&nbsp;chased&nbsp;the&nbsp;cat</mtext></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>The&nbsp;cat&nbsp;chased&nbsp;the&nbsp;dog</mtext></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\text{The dog chased the cat} \\ \\ \text{The cat chased the dog} 
\end{align*}</annotation></semantics></math></span></span></span></p><p>Intuitively, these have very different meanings. Let's see what happens if we
first tokenize them, map to the real token embeddings of <strong>Llama 3.2 1B</strong> and pass them
through <a href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html" rel="nofollow" target="_blank"><code>torch.nn.MultiheadAttention</code></a>.</p>
<pre><code><span line="1"><span>from</span> transformers <span>import</span> AutoTokenizer<span>,</span> AutoModel
</span><span line="2"><span>from</span> torch <span>import</span> torch<span>,</span> nn
</span><span line="3">
</span><span line="4">model_id <span>=</span> <span>"meta-llama/Llama-3.2-1B"</span>
</span><span line="5">tok <span>=</span> AutoTokenizer<span>.</span>from_pretrained<span>(</span>model_id<span>)</span>
</span><span line="6">model <span>=</span> AutoModel<span>.</span>from_pretrained<span>(</span>model_id<span>)</span> 
</span><span line="7">
</span><span line="8">s1<span>,</span> s2 <span>=</span> <span>"The dog chased the cat"</span><span>,</span> <span>"The cat chased the dog"</span>
</span><span line="9">e1<span>,</span> e2 <span>=</span> <span>[</span>model<span>.</span>embed_tokens<span>(</span>tok<span>(</span>s<span>,</span> return_tensors<span>=</span><span>"pt"</span><span>)</span><span>[</span><span>"input_ids"</span><span>]</span><span>)</span> <span>for</span> s <span>in</span> <span>(</span>s1<span>,</span> s2<span>)</span><span>]</span>
</span><span line="10">
</span><span line="11">hdim <span>=</span> e1<span>.</span>shape<span>[</span><span>-</span><span>1</span><span>]</span>
</span><span line="12">W_q <span>=</span> nn<span>.</span>Linear<span>(</span>hdim<span>,</span> hdim<span>,</span> bias<span>=</span><span>False</span><span>)</span>
</span><span line="13">W_k <span>=</span> nn<span>.</span>Linear<span>(</span>hdim<span>,</span> hdim<span>,</span> bias<span>=</span><span>False</span><span>)</span>
</span><span line="14">W_v <span>=</span> nn<span>.</span>Linear<span>(</span>hdim<span>,</span> hdim<span>,</span> bias<span>=</span><span>False</span><span>)</span>
</span><span line="15">
</span><span line="16">mha <span>=</span> nn<span>.</span>MultiheadAttention<span>(</span>embed_dim<span>=</span>hdim<span>,</span> num_heads<span>=</span><span>4</span><span>,</span> batch_first<span>=</span><span>True</span><span>)</span>
</span><span line="17">o1<span>,</span> _ <span>=</span> mha<span>(</span>W_q<span>(</span>e1<span>)</span><span>,</span> W_k<span>(</span>e1<span>)</span><span>,</span> W_v<span>(</span>e1<span>)</span><span>)</span>
</span><span line="18">o2<span>,</span> _ <span>=</span> mha<span>(</span>W_q<span>(</span>e2<span>)</span><span>,</span> W_k<span>(</span>e2<span>)</span><span>,</span> W_v<span>(</span>e2<span>)</span><span>)</span>
</span><span line="19"><span>print</span><span>(</span><span>"Matches: "</span><span>,</span> torch<span>.</span>allclose<span>(</span>o1<span>,</span> o2<span>,</span> atol<span>=</span><span>1e-6</span><span>)</span><span>)</span> <span># True</span>
</span></code></pre>
<p>As we can see, without any positional information, the output of a (multi
headed) self attention operation is <strong>identical</strong>, despite the different
word order and meaning. Let's begin designing a method of enhancing self attention with positional
information, such that it can determine relationships between words encoded by
their positions (i.e which mammal is chasing the other).</p>
<p>To understand and design an optimal encoding scheme, let's explore some desirable properties such a scheme should have.</p>
<h2>Desirable Properties</h2>
<p>Let's try and define some desirable properties that will make the optimization
process as easy as possible.</p>
<h4>Property 1 - Unique encoding for each position (across sequences)</h4>
<p>Each position needs a unique encoding that remains consistent regardless of sequence length - a token at position 5 should have the same encoding whether the current sequence is of length 10 or 10,000.</p>
<h4>Property 2 - Linear relation between two encoded positions</h4>
<p>The relationship between positions should be mathematically simple. If we know the encoding for position <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span></span>, it should be straightforward to compute the encoding for position <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>+</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">p+k</annotation></semantics></math></span></span>, making it easier for the model to learn positional patterns.</p>
<p>If you think about how we represent numbers on a number line, it's easy to understand that 5 is 2 steps away from 3, or that 10 is 5 steps from 15. The same intuitive relationship should exist in our encodings.</p>
<h4>Property 3 - Generalizes to longer sequences than those encountered in training</h4>
<p>To increase our models' utility in the real world, they should generalize outside
their training distribution. Therefore, our encoding scheme needs to be
adaptable enough to handle unexpected input lengths, without
violating any of our other desirable properties.</p>
<h4>Property 4 - Generated by a deterministic process the model can learn</h4>
<p>It would be ideal if our positional encodings could be drawn from a
deterministic process. This should allow the model to learn the mechanism
behind our encoding scheme efficiently.</p>
<h4>Property 5 - Extensible to multiple dimensions</h4>
<p>With multimodal models becoming the norm, it is crucial that our positional
encoding scheme can naturally extend from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>D</mi></mrow><annotation encoding="application/x-tex">1D</annotation></semantics></math></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>D</mi></mrow><annotation encoding="application/x-tex">nD</annotation></semantics></math></span></span>. This will allow models to consume data like images or brain scans, which are <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>D</mi></mrow><annotation encoding="application/x-tex">2D</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mi>D</mi></mrow><annotation encoding="application/x-tex">4D</annotation></semantics></math></span></span>
respectively.</p>
<p>Now we know the ideal properties (henceforth referred to as <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><msub><mi>r</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">Pr_n</annotation></semantics></math></span></span>), let's start designing and iterating on our encoding scheme.</p>
<h2>Integer Position Encoding</h2>
<p>The first approach that may jump to mind is simply to add the integer value of the token position to each component of the token embedding, with values ranging from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>→</mo><mi>L</mi></mrow><annotation encoding="application/x-tex">0 \rightarrow L</annotation></semantics></math></span></span> where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span></span> is the
length of our current sequence.</p>
<div><p><video autoplay="" muted="" loop="" playsinline=""><source src="https://fleetwood.dev/positional-encoding/IntegerEncoding.mp4" type="video/mp4">Your browser does not support the video tag.</video></p></div>
<p>In the above animation, we create our positional encoding vector for the token <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="#699C52"><mtext>chased</mtext></mstyle></mrow><annotation encoding="application/x-tex">\color{#699C52}\text{chased}</annotation></semantics></math></span></span> from the index and add it to our token embedding. The embedding values here are a subset of the real values from <strong>Llama 3.2 1B</strong>. We can observe that they're clustered around 0. This
is desirable to avoid <a href="https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf" rel="nofollow" target="_blank">vanishing or exploding gradients</a> during training and therefore is something we'd like to maintain throughout the model.</p>
<p>It's clear that our current naïve approach is going to cause problems. The magnitude of the position value
vastly exceeds the actual values of our input. This means the signal-to-noise
ratio is very low, and it's hard for the model to separate the semantic
information from the positional information.</p>
<p>With this new knowledge, a natural follow on might be to normalize the position value by <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{N}</annotation></semantics></math></span></span>. This constrains the values between 0 and 1, but introduces another problem. If we choose <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> to be the length of the current sequence, then the position values will be completely different for each sequence of differing lengths, violating <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><msub><mi>r</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">Pr_1</annotation></semantics></math></span></span>.</p>
<p>Is there a better way to ensure our numbers are between 0 and 1?
If we thought really hard about this for a while, we might come up with switching
from decimal to binary numbers.</p>
<h2>Binary Position Encoding</h2>
<p>Instead of adding our (potentially normalized) integer position to each
component of the embedding, we could instead convert it into its binary
representation and <em>s t r e t c h</em> our value out to match our embedding dimension, as demonstrated below.</p>
<div><p><video autoplay="" muted="" loop="" playsinline=""><source src="https://fleetwood.dev/positional-encoding/BinaryEncoding.mp4" type="video/mp4">Your browser does not support the video tag.</video></p></div>
<p>We've converted the position of interest (252) into its binary representation
(11111100) and added each bit to the corresponding component of the
token embedding. The least significant bit (LSB) will cycle between 0 and 1 for every
subsequent token, whilst the most significant bit (MSB) will cycle every
<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2^{n-1}</annotation></semantics></math></span></span> tokens where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> is the number of bits.
You can see the positional encoding vector for different indices in the animation below <sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup>.</p>
<div><p><video autoplay="" muted="" loop="" playsinline=""><source src="https://fleetwood.dev/positional-encoding/BinaryPositionalEncodingPlot.mp4" type="video/mp4">Your browser does not support the video tag.</video></p></div>
<p>We've solved the value range problem, and we now have unique encodings that are
consistent across different sequence lengths. What happens if we plot a low dimensional version of our token embedding and visualize the addition of our binary positional vector for different values.</p>
<div><p><video autoplay="" muted="" loop="" playsinline=""><source src="https://fleetwood.dev/positional-encoding/BinaryVector3D.mp4" type="video/mp4">Your browser does not support the video tag.</video></p></div>
<p>We can see that the result is very "jumpy" (as we might expect from the
discrete nature of binary). The optimization process likes smooth, continuous and
predictable changes. Do we know any functions with similar value ranges that are smooth and continuous?</p>
<p>If we looked around a little, we might notice that both <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>sin</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\sin</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\cos</annotation></semantics></math></span></span> fit the bill!</p>
<h2>Sinusoidal positional encoding</h2>
<div><p><video autoplay="" muted="" loop="" playsinline=""><source src="https://fleetwood.dev/positional-encoding/SteppedPositionalEncodingPlot.mp4" type="video/mp4">Your browser does not support the video tag.</video></p></div>
<p>The above animation visualizes our position embedding if each component is
alternatively drawn from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>sin</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\sin</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\cos</annotation></semantics></math></span></span> with gradually increasing
wavelengths. If you compare it with the previous animation, you'll notice a striking similarity!</p>
<p>We've now arrived at Sinusoidal embeddings; originally defined in the <a href="https://arxiv.org/abs/1706.03762" rel="nofollow" target="_blank">Attention is all you need</a> paper.
Let's look at the equations:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mstyle mathcolor="#58C4DD"><mi>sin</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mstyle mathcolor="black"><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><mrow><mn>1000</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><mi>d</mi></mrow></msup></mrow></mfrac><mstyle mathcolor="#58C4DD"></mstyle></mstyle><mo fence="true" mathcolor="#58C4DD">)</mo></mrow><mstyle mathcolor="black"><mspace linebreak="newline"></mspace><mspace width="1em"></mspace><mspace linebreak="newline"></mspace><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mstyle mathcolor="#FC6255"><mi>cos</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mstyle mathcolor="black"><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><mrow><mn>1000</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><mi>d</mi></mrow></msup></mrow></mfrac><mstyle mathcolor="#FC6255"></mstyle></mstyle><mo fence="true" mathcolor="#FC6255">)</mo></mrow><mstyle mathcolor="black"><mspace linebreak="newline"></mspace></mstyle></mstyle></mstyle></mstyle></mrow><annotation encoding="application/x-tex">PE_{(pos,2i)} = \color{#58C4DD}\sin\left(\color{black}\frac{pos}{10000^{2i/d}}\color{#58C4DD}\right)\color{black} \\ 
\quad \\
PE_{(pos,2i+1)} = \color{#FC6255}\cos\left(\color{black}\frac{pos}{10000^{2i/d}}\color{#FC6255}\right)\color{black} \\ </annotation></semantics></math></span></span></span></p><p>where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">pos</annotation></semantics></math></span></span> is the tokens position index, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span></span> is the component index
in the positional encoding vector, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span></span> is the model dimension. <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">10,000</annotation></semantics></math></span></span> is the <strong>base wavelength</strong> (henceforth referred to as
<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span></span>), which we stretch or compress as a function of the component index. I encourage you to plug in some realistic values to get a feel for this
geometric progression.</p>
<p>There's a few parts of this equation that are confusing at first glance. How did the
authors choose <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">10,000</annotation></semantics></math></span></span>? Why are we using <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>sin</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\sin</annotation></semantics></math></span></span> <strong>and</strong> <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\cos</annotation></semantics></math></span></span> for even and odd positions respectively?</p>
<p>It seems that using <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">10,000</annotation></semantics></math></span></span> for the base wavelength was determined experimentally <sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="true" aria-describedby="footnote-label">2</a></sup>. Deciphering the usage of both <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>sin</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\sin</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\cos</annotation></semantics></math></span></span> is more involved, but crucial
for our iterative approach to understanding. The key here is our desire for a linear relation between two encoded positions (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><msub><mi>r</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">Pr_2</annotation></semantics></math></span></span>). To understand how using <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>sin</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\sin</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\cos</annotation></semantics></math></span></span> in tandem produce this linear relation, we will have to dive into some trigonometry.</p>
<p>Consider a sequence of sine and cosine pairs, each associated with a frequency <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\omega_i</annotation></semantics></math></span></span>. Our goal is to find a linear transformation matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">M</mi></mrow><annotation encoding="application/x-tex">\mathbf{M}</annotation></semantics></math></span></span> that can shift these sinusoidal functions by a fixed offset <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span>:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">M</mi><mo>⋅</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo>+</mo><mi>k</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo>+</mo><mi>k</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{M} \cdot \begin{bmatrix} \sin(\omega_i p) \\ \cos(\omega_i p) \end{bmatrix} = \begin{bmatrix} \sin(\omega_i(p + k)) \\ \cos(\omega_i(p + k)) \end{bmatrix}</annotation></semantics></math></span></span></span></p><p>The frequencies <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\omega_i</annotation></semantics></math></span></span> follow a geometric progression that decreases with dimension index <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span></span>, defined as:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>ω</mi><mi>i</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1000</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><mi>d</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\omega_i = \frac{1}{10000^{2i/d}}</annotation></semantics></math></span></span></span></p><p>To find this transformation matrix, we can express it as a general 2×2 matrix with unknown coefficients <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">u_1</annotation></semantics></math></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">v_1</annotation></semantics></math></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">u_2</annotation></semantics></math></span></span>, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">v_2</annotation></semantics></math></span></span>:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>u</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>v</mi><mn>1</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>u</mi><mn>2</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>v</mi><mn>2</mn></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>⋅</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo>+</mo><mi>k</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo>+</mo><mi>k</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{bmatrix} u_1 &amp; v_1 \\ u_2 &amp; v_2 \end{bmatrix} \cdot \begin{bmatrix} \sin(\omega_i p) \\ \cos(\omega_i p) \end{bmatrix} = \begin{bmatrix} \sin(\omega_i(p+k)) \\ \cos(\omega_i(p+k)) \end{bmatrix}</annotation></semantics></math></span></span></span></p><p>By applying the trigonometric addition theorem to the right-hand side, we can expand this into:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>u</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>v</mi><mn>1</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>u</mi><mn>2</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>v</mi><mn>2</mn></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>⋅</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>k</mi><mo stretchy="false">)</mo><mo>+</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>k</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>k</mi><mo stretchy="false">)</mo><mo>−</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>k</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{bmatrix} u_1 &amp; v_1 \\ u_2 &amp; v_2 \end{bmatrix} \cdot \begin{bmatrix} \sin(\omega_i p) \\ \cos(\omega_i p) \end{bmatrix} = \begin{bmatrix} \sin(\omega_i p)\cos(\omega_i k) + \cos(\omega_i p)\sin(\omega_i k) \\ \cos(\omega_i p)\cos(\omega_i k) - \sin(\omega_i p)\sin(\omega_i k) \end{bmatrix}</annotation></semantics></math></span></span></span></p><p>This expansion gives us a system of two equations by matching coefficients:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>u</mi><mn>1</mn></msub><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo><mo>+</mo><msub><mi>v</mi><mn>1</mn></msub><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>k</mi><mo stretchy="false">)</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo><mo>+</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>k</mi><mo stretchy="false">)</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd></mtd><mtd></mtd></mtr><mtr><mtd></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>u</mi><mn>2</mn></msub><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo><mo>+</mo><msub><mi>v</mi><mn>2</mn></msub><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mo>−</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>k</mi><mo stretchy="false">)</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo><mo>+</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>k</mi><mo stretchy="false">)</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd></mtd><mtd></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
u_1\sin(\omega_i p) + v_1\cos(\omega_i p) &amp;= \cos(\omega_i k)\sin(\omega_i p) + \sin(\omega_i k)\cos(\omega_i p) \\
u_2\sin(\omega_i p) + v_2\cos(\omega_i p) &amp;= -\sin(\omega_i k)\sin(\omega_i p) + \cos(\omega_i k)\cos(\omega_i p)
\end{align}</annotation></semantics></math></span></span></span></p><p>By comparing terms with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\sin(\omega_i p)</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\cos(\omega_i p)</annotation></semantics></math></span></span> on both sides, we can solve for the unknown coefficients:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left right left" columnspacing="0em 1em 0em"><mtr><mtd></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>u</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>k</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>v</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>k</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd></mtd><mtd></mtd></mtr><mtr><mtd></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>u</mi><mn>2</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mo>−</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>k</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mi>v</mi><mn>2</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>k</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd></mtd><mtd></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
u_1 &amp;= \cos(\omega_i k) &amp; v_1 &amp;= \sin(\omega_i k) \\
u_2 &amp;= -\sin(\omega_i k) &amp; v_2 &amp;= \cos(\omega_i k)
\end{align}</annotation></semantics></math></span></span></span></p><p>These solutions give us our final transformation matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">M</mi><mi mathvariant="bold">k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{M_k}</annotation></semantics></math></span></span>:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">M</mi><mi mathvariant="bold">k</mi></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>k</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>k</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>k</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>k</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{M_k} = \begin{bmatrix} \cos(\omega_i k) &amp; \sin(\omega_i k) \\ -\sin(\omega_i k) &amp; \cos(\omega_i k) \end{bmatrix}</annotation></semantics></math></span></span></span></p><p>If you've done any game programming before, you might notice that the
result of our derivation is oddly familiar. That's right, it's the <a href="https://en.wikipedia.org/wiki/Rotation_matrix" rel="nofollow" target="_blank">Rotation Matrix!</a> <sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="true" aria-describedby="footnote-label">3</a></sup>.</p>
<p>So the encoding scheme designed by <a href="https://en.wikipedia.org/wiki/Noam_Shazeer" rel="nofollow" target="_blank">Noam Shazeer</a> in <a href="https://arxiv.org/abs/1706.03762" rel="nofollow" target="_blank">Attention is all you need</a> was already encoding relative position as a rotation back in 2017! It took another <strong>4 years</strong> to go from Sinusoidal Encoding to RoPE, despite rotations already being on the table...</p>
<h2>Absolute vs Relative Position Encoding</h2>
<p>With the knowledge in hand that rotations are important here, let's
return to our motivating example and try to discover some intuitions for our next iteration.</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="0.7em"></mspace><mn>0</mn><mspace width="1.4em"></mspace><mn>1</mn><mspace width="2em"></mspace><mn>2</mn><mspace width="1.7em"></mspace><mn>3</mn><mspace width="1.1em"></mspace><mn>4</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext>The&nbsp;dog&nbsp;chased&nbsp;the&nbsp;cat</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="0.3em"></mspace><mtext>-2</mtext><mspace width="1.4em"></mspace><mtext>-1</mtext><mspace width="1.7em"></mspace><mn>0</mn><mstyle mathcolor="black"><mspace width="1.7em"></mspace><mn>1</mn><mspace width="1.1em"></mspace><mn>2</mn></mstyle></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mrow><mtext>The&nbsp;dog&nbsp;</mtext><mstyle mathcolor="#699C52"><mtext>chased&nbsp;</mtext><mstyle mathcolor="black"><mtext>the&nbsp;cat</mtext></mstyle></mstyle></mrow></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
&amp;\hspace{0.7em}0 \hspace{1.4em} 1 \hspace{2em} 2 \hspace{1.7em} 3 \hspace{1.1em} 4\\
&amp;\text{The dog chased the cat} \\
\\
&amp;\hspace{0.3em}\text{-2} \hspace{1.4em} \text{-1} \hspace{1.7em} 0
\color{black} \hspace{1.7em} 1 \hspace{1.1em} 2\\
&amp;\text{The dog \color{#699C52}chased \color{black}the cat}
\end{align*}</annotation></semantics></math></span></span></span></p><p>Above we can see the absolute positions of our tokens, and the relative
positions from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle mathcolor="#699C52"><mtext>chased</mtext></mstyle></mrow><annotation encoding="application/x-tex">\color{#699C52}\text{chased}</annotation></semantics></math></span></span> to every other token. With Sinusoidal Encoding, we
generated a separate vector which represents the absolute position,
and using some trigonometric trickery we were able to encode relative positions.</p>
<p>When we're trying to understand these sentences, does it matter that <em>this</em> word is the 2149th word in this blog post? Or do we care about its relationship to the words around it? The absolute position of a word rarely matters for meaning - what matters is how words relate to each other.</p>
<h2>Positional encoding in context</h2>
<p>From this point on, it's key to consider positional encoding <strong>in the context of</strong>
self attention. To reiterate, the self-attention mechanism enables the model to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output.</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attn</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">\text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V</annotation></semantics></math></span></span></span></p><p>In all our previous iterations, we've generated a separate positional encoding
vector and <strong>added</strong> it to our token embedding prior to our <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span></span> projections.
By adding the positional information directly to our token embedding, we are
<strong>polluting</strong> the semantic information with the positional information. We should
be attempting to encode the information without modifying the norm. Shifting to multiplicative is the
key.</p>
<p>Using the dictionary analogy, when looking up a word (query) in our dictionary (keys), nearby words should have more influence than distant ones. The influence of one token upon another is determined by the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span></span> dot product - so that's exactly where we should focus our positional encoding!</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>a</mi><mo>⃗</mo></mover><mo>⋅</mo><mover accent="true"><mi>b</mi><mo>⃗</mo></mover><mo>=</mo><mi mathvariant="normal">∣</mi><mover accent="true"><mi>a</mi><mo>⃗</mo></mover><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mover accent="true"><mi>b</mi><mo>⃗</mo></mover><mi mathvariant="normal">∣</mi><mi>cos</mi><mo>⁡</mo><mi>θ</mi></mrow><annotation encoding="application/x-tex">\vec{a} \cdot \vec{b} = |\vec{a}| |\vec{b}| \cos \theta</annotation></semantics></math></span></span></span></p><p>The geometric interpretation of the dot product shown above gives us a magnificent insight.
We can modulate the result of our dot product of two vectors purely by
increasing or decreasing the angle between them. Furthermore, by rotating the
vector, we have absolutely zero impact on the norm of the vector, which encodes
the semantic information of our token.</p>
<p>So now we know where to focus our <em>attention</em>, and have seen from another <em>angle</em> why a
rotation might be a sensible "channel" in which to encode our positional
information, let's put it all together!</p>
<h2><strong>Ro</strong>tary <strong>P</strong>ostional <strong>E</strong>ncoding</h2>
<p><strong>Ro</strong>tary <strong>P</strong>ostional <strong>E</strong>ncoding or RoPE was defined in the
<a href="https://arxiv.org/pdf/2104.09864" rel="nofollow" target="_blank">RoFormer paper</a> (<a href="https://x.com/bojone1993" rel="nofollow" target="_blank">Jianlin Su</a> designed it independently on his blog <a href="https://kexue.fm/archives/8130" rel="nofollow" target="_blank">here</a> and <a href="https://kexue.fm/archives/8265" rel="nofollow" target="_blank">here</a>).
While it may seem like voodoo if you skip to the end result, by thinking about Sinusoidal Encoding in the
context of self attention (and more specifically dot products), we can see how
it all comes together.</p>
<p>Much like in Sinusoidal Encoding, we decompose our vectors (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">q</mi></mrow><annotation encoding="application/x-tex">\mathbf{q}</annotation></semantics></math></span></span> or <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">k</mi></mrow><annotation encoding="application/x-tex">\mathbf{k}</annotation></semantics></math></span></span>, instead of pre-projection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span></span>) into 2D pairs/chunks. Rather than encoding <em>absolute</em> position directly by adding a vector we drew from sinusoidal functions of slowly decreasing frequencies, we cut to the chase and encode <em>relative</em> position by <strong>multiplying each pair with the rotation matrix</strong>.</p>
<p>Let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">q</mi></mrow><annotation encoding="application/x-tex">\mathbf{q}</annotation></semantics></math></span></span> or <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">k</mi></mrow><annotation encoding="application/x-tex">\mathbf{k}</annotation></semantics></math></span></span> be our input vector at position <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span></span>. We create a block diagonal matrix
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">M</mi><mi mathvariant="bold">i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{M_i}</annotation></semantics></math></span></span> is the corresponding rotation matrix for that component
pairs desired rotation:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi mathvariant="bold">q</mi><mo separator="true">,</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi mathvariant="bold">M</mi><mn mathvariant="bold">1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi mathvariant="bold">M</mi><mn mathvariant="bold">2</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">⋱</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi mathvariant="bold">M</mi><mrow><mi mathvariant="bold">d</mi><mi mathvariant="bold">/</mi><mn mathvariant="bold">2</mn></mrow></msub></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>q</mi><mn>1</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>q</mi><mn>2</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>q</mi><mi>d</mi></msub></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">R(\mathbf{q}, p) = \begin{pmatrix} \mathbf{M_1} &amp; &amp; &amp; \\ &amp; \mathbf{M_2} &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; \mathbf{M_{d/2}} \end{pmatrix} \begin{pmatrix} q_1 \\ q_2 \\ \vdots \\ q_d \end{pmatrix} </annotation></semantics></math></span></span></span></p><p>Much the same as Sinusoidal Encoding, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">M</mi><mi mathvariant="bold">i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{M_i}</annotation></semantics></math></span></span> is simply:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">M</mi><mi mathvariant="bold">i</mi></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>ω</mi><mi>i</mi></msub><mi>p</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{M_i} = \begin{bmatrix} \cos(\omega_i p) &amp; \sin(\omega_i p) \\ -\sin(\omega_i p) &amp; \cos(\omega_i p) \end{bmatrix}</annotation></semantics></math></span></span></span></p><div><p><video autoplay="" muted="" loop="" playsinline=""><source src="https://fleetwood.dev/positional-encoding/RopeEncoding.mp4" type="video/mp4">Your browser does not support the video tag.</video></p></div>
<p>In practice, we don't use a matrix multiplication to compute RoPE as it would be
computationally inefficient with such a sparse matrix. Instead, we can directly apply the rotations to pairs of elements independently, taking advantage of the regular pattern in the computation:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>R</mi><mrow><mi mathvariant="normal">Θ</mi><mo separator="true">,</mo><mi>p</mi></mrow><mi>d</mi></msubsup><mi>q</mi><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>q</mi><mn>1</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>q</mi><mn>2</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>q</mi><mn>3</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>q</mi><mn>4</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>q</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>q</mi><mi>d</mi></msub></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow><mo>⊗</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>⁡</mo><mi>p</mi><msub><mi>θ</mi><mn>1</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>⁡</mo><mi>p</mi><msub><mi>θ</mi><mn>1</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>⁡</mo><mi>p</mi><msub><mi>θ</mi><mn>2</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>⁡</mo><mi>p</mi><msub><mi>θ</mi><mn>2</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>⁡</mo><mi>p</mi><msub><mi>θ</mi><mrow><mi>d</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>⁡</mo><mi>p</mi><msub><mi>θ</mi><mrow><mi>d</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow><mo>+</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><msub><mi>q</mi><mn>2</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>q</mi><mn>1</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><msub><mi>q</mi><mn>4</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>q</mi><mn>3</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>−</mo><msub><mi>q</mi><mi>d</mi></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>q</mi><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msub></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow><mo>⊗</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>sin</mi><mo>⁡</mo><mi>p</mi><msub><mi>θ</mi><mn>1</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>sin</mi><mo>⁡</mo><mi>p</mi><msub><mi>θ</mi><mn>1</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>sin</mi><mo>⁡</mo><mi>p</mi><msub><mi>θ</mi><mn>2</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>sin</mi><mo>⁡</mo><mi>p</mi><msub><mi>θ</mi><mn>2</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>sin</mi><mo>⁡</mo><mi>p</mi><msub><mi>θ</mi><mrow><mi>d</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>sin</mi><mo>⁡</mo><mi>p</mi><msub><mi>θ</mi><mrow><mi>d</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">R_{\Theta,p}^d q = \begin{pmatrix} 
q_1 \\
q_2 \\
q_3 \\
q_4 \\
\vdots \\
q_{d-1} \\
q_d
\end{pmatrix} \otimes \begin{pmatrix}
\cos p\theta_1 \\
\cos p\theta_1 \\
\cos p\theta_2 \\
\cos p\theta_2 \\
\vdots \\
\cos p\theta_{d/2} \\
\cos p\theta_{d/2}
\end{pmatrix} + \begin{pmatrix}
-q_2 \\
q_1 \\
-q_4 \\
q_3 \\
\vdots \\
-q_d \\
q_{d-1}
\end{pmatrix} \otimes \begin{pmatrix}
\sin p\theta_1 \\
\sin p\theta_1 \\
\sin p\theta_2 \\
\sin p\theta_2 \\
\vdots \\
\sin p\theta_{d/2} \\
\sin p\theta_{d/2}
\end{pmatrix}</annotation></semantics></math></span></span></span></p><p>That's all there is to it! By artfully applying our rotations to 2D chunks of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">q</mi></mrow><annotation encoding="application/x-tex">\mathbf{q}</annotation></semantics></math></span></span> and
<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">k</mi></mrow><annotation encoding="application/x-tex">\mathbf{k}</annotation></semantics></math></span></span> prior to their dot product, and switching from additive to
multiplicative, we can gain a big performance boost in evaluations <sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="true" aria-describedby="footnote-label">4</a></sup>.</p>
<h2>Extending RoPE to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span>-Dimensions</h2>
<p>We've explored the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>D</mi></mrow><annotation encoding="application/x-tex">1D</annotation></semantics></math></span></span> case for RoPE and by this point I hope you've gained an
intuitive understanding of an admittedly unintuitive component of transformers.
Finally, let's explore extending it to higher dimensions, such as images.</p>
<p>A natural first intuition could be to directly use the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>x</mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>y</mi></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><annotation encoding="application/x-tex"> \begin{bmatrix} x \\ y \end{bmatrix}</annotation></semantics></math></span></span> coordinate pairs from the image. This might seem intuitive, after all, we were almost arbitrarily pairing up our components previously. However, this would be a mistake!</p>
<p>In the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>D</mi></mrow><annotation encoding="application/x-tex">1D</annotation></semantics></math></span></span> case, we encode the relative position <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>−</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m - n</annotation></semantics></math></span></span> through a rotation of pairs
of values from our input vector. For <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>D</mi></mrow><annotation encoding="application/x-tex">2D</annotation></semantics></math></span></span> data, we need to encode both horizontal and vertical relative positions, say <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>−</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m - n</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i - j</annotation></semantics></math></span></span> independently. RoPE's brilliance lies in how it handles multiple dimensions. Instead of trying to encode all positional information in a single rotation, we pair components <strong>within the same dimension</strong> and rotate those, otherwise we would be intermixing the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span></span> offset information. By handling each dimension independently, we maintain the natural structure of the space. This can generalize to as many dimensions as required!</p>
<h2>The future of positional encoding</h2>
<p>Is RoPE the final incarnation of positional encoding? This <a href="https://arxiv.org/pdf/2410.06205" rel="nofollow" target="_blank">recent paper</a> from DeepMind deeply analyses RoPE and highlights some fundamental problems.</p>
<p>I anticipate some future breakthroughs, perhaps taking inspiration from
signal processing with ideas like wavelets or hierarchical implementations. As models
are increasingly quantized for deployment, I'd also expect to see some
innovation in encoding schemes that remain robust under low-precision arithmetic.</p>
<h2>Conclusion</h2>
<p>Positional encoding has and continues to be treated as an after thought in
transformers. I believe we should view it differently - self attention has an
Achilles heel that has been repeatedly patched.</p>
<p>I hope this blog post showed you that you too could have discovered state of the
art positional encoding, despite it being unintuitive at first. In a follow up
post I'd love to explore practical implementation details for RoPE in order to
maximise performance.</p>
<p>Thanks to Madeline Ephgrave for proof reading this.</p>
<h2>References</h2>
<ul>
<li><a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" rel="nofollow" target="_blank">https://kazemnejad.com/blog/transformer_architecture_positional_encoding/</a></li>
<li><a href="https://blog.eleuther.ai/rotary-embeddings/" rel="nofollow" target="_blank">https://blog.eleuther.ai/rotary-embeddings/</a></li>
<li><a href="https://www.youtube.com/watch?v=T3OT8kqoqjc" rel="nofollow" target="_blank">https://www.youtube.com/watch?v=T3OT8kqoqjc</a></li>
<li><a href="https://arxiv.org/pdf/1706.03762" rel="nofollow" target="_blank">https://arxiv.org/pdf/1706.03762</a></li>
<li><a href="https://arxiv.org/pdf/2410.06205" rel="nofollow" target="_blank">https://arxiv.org/pdf/2410.06205</a></li>
<li><a href="https://arxiv.org/pdf/2104.09864" rel="nofollow" target="_blank">https://arxiv.org/pdf/2104.09864</a></li>
</ul>
<section data-footnotes="true">
<ol>
<li id="user-content-fn-1">
<p>Binary and Sinusoidal animations are reproductions of animations contained
in <a href="https://www.youtube.com/watch?v=T3OT8kqoqjc0" rel="nofollow" target="_blank">this</a> video. <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-2">
<p>Using <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>10000</mn></mrow><annotation encoding="application/x-tex">\theta = 10000</annotation></semantics></math></span></span> gives us <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>π</mi><mo>⋅</mo><mn>10000</mn></mrow><annotation encoding="application/x-tex"> 2 \pi \cdot 10000</annotation></semantics></math></span></span> unique positions, or a
theoretical upper bound on the context length at ~63,000. <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
<li id="user-content-fn-3">
<p>Pieces of this post are based on <a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" rel="nofollow" target="_blank">this fantastic
post</a>
by <a href="https://kazemnejad.com/" rel="nofollow" target="_blank">Amirhossein Kazemnejad</a>. <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3">↩</a></p>
</li>
<li id="user-content-fn-4">
<p>For empirical evidence, see <a href="https://blog.eleuther.ai/rotary-embeddings/" rel="nofollow" target="_blank">this</a> great post by EleutherAI. <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 4">↩</a></p>
</li>
</ol>
</section></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why did Windows 95 setup use three operating systems? (225 pts)]]></title>
            <link>https://devblogs.microsoft.com/oldnewthing/20241112-00/?p=110507</link>
            <guid>42166606</guid>
            <pubDate>Sun, 17 Nov 2024 19:54:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblogs.microsoft.com/oldnewthing/20241112-00/?p=110507">https://devblogs.microsoft.com/oldnewthing/20241112-00/?p=110507</a>, See on <a href="https://news.ycombinator.com/item?id=42166606">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="single-wrapper">
    
    <article data-clarity-region="article" id="post-110507">
        <div data-bi-area="body_article" data-bi-id="post_page_body_article">
            <p>Twitter users @tthirtle asked why Windows 95 setup goes through three operating systems: MS-DOS, Windows 3.1, and then Windows 95. Why not go from MS-DOS straight to Windows 95?</p>
<blockquote data-conversation="none">
<p dir="ltr" lang="en">Here’s another good question. Why does Windows 95 setup use 3 different UI’s. DOS,Win3.x,and Win9x?</p>
<p>— Thomas (@tthirtle) <a href="https://twitter.com/tthirtle/status/1809777917565767993?ref_src=twsrc%5Etfw">July 7, 2024</a></p></blockquote>

<p>Windows 95 setup could upgrade from three starting points: MS-DOS, Windows 3.1, or Windows 95. (Yes, you could upgrade Windows 95 to Windows 95. You might do this to repair a corrupted system while preserving data.)</p>
<p>One option is to write three versions of Windows 95 setup: One for setting up from MS-DOS, another for setting up from Windows 3.1, and a third for setting up from Windows 95.</p>
<p>This was not a pleasant option because you basically did the same work three times, but implemented separately, so you have to do three times the coding.</p>
<p>A better option is to just write one version of Windows 95 setup and use it for all three starting points. So now you get to choose the platform on which to base your code.</p>
<table>
<tbody>
<tr>
<th rowspan="2">From</th>
<th colspan="3">App type</th>
</tr>
<tr>
<th>MS-DOS</th>
<th>16-bit GUI</th>
<th>32-bit GUI</th>
</tr>
<tr>
<td>MS-DOS</td>
<td>•</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>Windows 3.1</td>
<td>•</td>
<td>•</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>Windows 95</td>
<td>•</td>
<td>•</td>
<td>•</td>
</tr>
</tbody>
</table>
<p>If you write Windows 95 setup as an MS-DOS app, then it runs on all three platforms. That’s great! You need to write only one setup program. The downside is that it’s going to be a text-mode setup program, which looks ugly and gives a poor initial impression of what is supposed to be a brand new GUI world.</p>
<p>At the other extreme, you can write Windows 95 setup as a 32-bit GUI program, but that means that if the user is starting from MS-DOS or Windows 3.1, you have to install Windows 95 before you can run Windows 95 setup, which is a bit of a catch-22.</p>
<p>In the middle is the happy medium: You can have the MS-DOS setup program install a minimal version of Windows 3.1, just barely enough to support what the 16-bit GUI setup program needs.¹ This tiny version is small enough to be copied and installed off a small number of floppy disks. Once that’s done, boot into the tiny version of Windows 3.1 and run the 16-bit GUI setup program.</p>
<p>Okay, so now we have three setup programs. The first one is used if you’re installing from MS-DOS: It installs the tiny version of Windows 3.1, and then boots into Windows 3.1 to continue to the next step.</p>
<p>The second setup program runs as a 16-bit Windows app, either in the miniature copy of Windows 3.1 (if the user is upgrading from MS-DOS), the real copy of Windows 3.1 (if the user is upgrading from Windows 3.1), or the real copy of Windows 95 (if the user is upgrading from Windows 95). This second setup program is the one that does almost all of the real work: It does the initial interaction with the user to gather information about how to install Windows 95, like asking which optional components to include, and does hardware detection to decide which drivers to install.² And then it copies the drivers and Windows 95 files onto the system, migrates your old settings to the new operating system, and boots into Windows 95.</p>
<p>The third setup program runs as a 32-bit Windows app. It is running in the real Windows 95 system and does some final steps that require operation a live running system, like installing printers.</p>
<table>
<tbody>
<tr>
<td>Starting from MS-DOS →</td>
<td>Install mini Windows 3.1</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>MS-DOS app</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>↓</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>Boot into mini-Windows 3.1</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>↓</td>
</tr>
<tr>
<td>Starting from Windows 3.1 →</td>
<td>Gather information</td>
<td rowspan="7">&nbsp;</td>
<td rowspan="7">&nbsp;</td>
<td rowspan="7">16-bit Windows app</td>
</tr>
<tr>
<td>or Windows 95</td>
<td>↓</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>Detect hardware</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>↓</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>Copy drivers and<br>
Windows 95 files</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>↓</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>Migrate settings and configure drivers</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>↓</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>Boot into Windows 95</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>↓</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>Final setup</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>Windows 95 app</td>
</tr>
</tbody>
</table>
<p>So that’s why Windows 95 setup is really three setup programs chained together. It allows a single copy of the code to be used for all three of the installation scenarios. Each program takes you one step closer to the goal. And everything got implemented only once.</p>
<p>¹ There was existing precedent for a tiny version of Windows that is barely enough to run a single program. The original Windows version of Microsoft Excel <a href="https://en.wikipedia.org/w/index.php?title=Microsoft_Excel&amp;oldid=166461028"> came with a runtime version of Windows 2.1</a>, so that customers who didn’t have Windows could still use Excel.</p>
<p>² This hardware detection code that Setup uses is the same code that runs when you do hardware detection from within Windows 95 itself, so even that code needed to be written only once. It did have some runtime checks to change behavior slightly depending on whether it’s running in Windows 3.1 or Windows 95, but the vast majority of the code is identical.</p>
        </div><!-- .entry-content -->

        <!-- AI Disclaimer -->
            </article>
    
</div><div><!-- Author section -->
            <h2>Author</h2>
            <div><div><p><img src="https://devblogs.microsoft.com/oldnewthing/wp-content/uploads/sites/38/2019/02/RaymondChen_5in-150x150.jpg" alt="Raymond Chen"></p></div><p>Raymond has been involved in the evolution of Windows for more than 30 years. In 2003, he began a Web site known as The Old New Thing which has grown in popularity far beyond his wildest imagination, a development which still gives him the heebie-jeebies. The Web site spawned a book, coincidentally also titled The Old New Thing (Addison Wesley 2007). He occasionally appears on the Windows Dev Docs Twitter account to tell stories which convey no useful information.</p></div>        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Humans have caused 1.5 °C of long-term global warming according to new estimates (378 pts)]]></title>
            <link>https://www.lancaster.ac.uk/news/humans-have-already-caused-15-c-of-long-term-global-warming-according-to-new-estimates</link>
            <guid>42166030</guid>
            <pubDate>Sun, 17 Nov 2024 18:49:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lancaster.ac.uk/news/humans-have-already-caused-15-c-of-long-term-global-warming-according-to-new-estimates">https://www.lancaster.ac.uk/news/humans-have-already-caused-15-c-of-long-term-global-warming-according-to-new-estimates</a>, See on <a href="https://news.ycombinator.com/item?id=42166030">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                    <img src="https://cisweb.lancaster.ac.uk/img/cwip/cisweb.lancaster.ac.uk/EventsMedia/earth-adobestock242647178-smaller-638669192826825152.jpg?mode=crop&amp;width=874&amp;height=492&amp;center=0.50%2c0.50" alt="Earth from space" loading="lazy">
                                        <p>A new study published today in Nature Geoscience by Dr Andrew Jarvis at Lancaster University and Professor Piers Forster at the University of Leeds shows that humans may have already caused 1.5 °C of global warming when measured from a time genuinely before the industrial revolution and the start of large-scale carbon emissions.</p><p>The Paris Climate Agreement from 2016 established a long-term temperature goal of “limiting global temperature increase well below 2 degrees Celsius, while pursuing efforts to limit the increase to 1.5 degrees.” The 1.5 °C of warming figure has since become the yardstick to judge progress, or the lack of it, on climate change.</p><p>The human-induced contribution to global warming is currently put at 1.31 °C, but with an uncertainty range of 1.10 to 1.60 °C, according to the Intergovernmental Panel on Climate Change (IPCC’s) preferred methods. This means it is unclear, from the IPCC’s adopted estimates, whether the 1.5 °C boundary has been breached or not. </p><p>Crucially, the IPCC’s preferred methods use temperature records from 1850-1900 as their ‘pre-industrial’ baseline for their calculations. They do this because this is when the first temperature records were taken, although the exact way to measure global temperature increases has never been defined within the climate negotiations. </p><p>Using this same 1850-1900 baseline, Dr Jarvis and Professor Forster’s method more than halves the uncertainty in the current human-caused warming estimate, thereby showing human-caused global warming currently remains below 1.5 °C if measured this way. On this measure, crossing the 1.5 °C Paris guardrail is under 10 years away at current warming rates. </p><p>However, Dr Jarvis and Professor Forster go further. Their method makes a more accurate estimate of the true long-term human contribution to global warming by pushing the base period from which the global temperature change is measured back to before 1700.</p><p>The authors find that when measured from this earlier more accurate definition of pre-industrial time, the long-term human contribution to warming was 1.49 °C ± 0.11 °C in 2023 and is now above 1.5 °C. This reveals that there is almost 0.2 °C of warming within the 1850-1900 baseline currently being used to define the warming.</p><p>Dr Jarvis, lead author of the study, said: “Measuring human-caused global warming is a difficult task because it forces us to compare today’s temperature with what it was in pre-industrial times – we call this the pre-industrial baseline. The closest we come to pre-industrial global temperature measurements are from the middle of the 1800’s, and unsurprisingly, these data are somewhat patchy and the Industrial Revolution was well underway by then. So using these early temperature data as a baseline as previous methods do not only ignores the warming that was already underway, it also bakes significant uncertainty into warming estimates.”</p><p>This new study instead uses CO<sub>2</sub> records from air bubbles trapped in ice-cores to establish a pre-1700 baseline for temperature. These records stretch back thousands of years, well before the Industrial Revolution and the effects of human-derived carbon emissions. The scientists are able to use the CO<sub>2</sub> record to anchor global warming estimates because of what they say is an overlooked relationship between the two.</p><p>“If you plot global temperatures against the concentration of CO<sub>2</sub> in the atmosphere, they both fall on a remarkably straight line, much straighter than current theory would predict,” said Dr Jarvis. “That line tells you not only how much the Earth has warmed since pre-industrial times, but also how much of that warming can be blamed on human activity.</p><p>“The climate is unimaginably complex, so perhaps it isn’t so surprising that such a direct method for accurately measuring the warming humans are responsible for has been overlooked,” Dr Jarvis added.</p><p>The scientists believe their new method is a strong candidate for measuring progress against the 1.5 and 2.0 degree Paris yardsticks.</p><p>Dr Jarvis said: “Our method has a number of strengths. Firstly, it directly tackles the problem of how to establish a robust pre-industrial baseline, although it functions equally well with the 1850-1900 baseline. Secondly, it produces estimates of human-caused warming that are at least 30 percent more certain than current methods. Finally, it is easy and quick to apply, meaning that we can produce warming estimates as soon as the CO<sub>2</sub> and temperature data become available without having to re-run complex climate models. This also means the results are transparent, making them far easier to communicate to non-specialists.”</p><p>Professor Forster, co-author of the study, said: “Our study shows that human societies have caused more than 1.5 degrees of long-term global warming. However, this does not necessarily mean that the Paris Agreement’s 1.5 temperature guardrail is breached, as we find that 0.18 °C of warming happened before global temperature records began, and this baked-in warming would not have been factored into the Paris Agreement.</p><p>“Policy makers set the Paris temperature goal to limit the devasting climate impacts many around the world are already experiencing. It was set to push countries to higher national ambition. It is clear we need to do more. The ambition set out in the goal for “pursuing efforts to limit the increase to 1.5 degrees” was not vainglorious when set, rather countries did not match it with their efforts. Urgent actions can slow warming rates and push back the time of breaching the Paris 1.5 degree limit. Although breaching the limit is now inevitable, delivering action commensurate with the noble Paris goal is more important than ever.” he added.</p><p>Although useful for measuring current levels of human-induced global warming, the researchers caution against using their method for making predictions on future warming.</p><p>Dr Jarvis, said: “Although atmospheric CO<sub>2</sub> is responsible for the bulk of human-induced warming so far, it is not solely responsible, and we know other factors such as methane could become increasingly important in the future, especially if we encounter climate tipping points. This means we need to keep an eye on our analysis. Fortunately, departures from the current linear regime could provide valuable early warning of such a change.”</p><p>The study is outlined in the paper ‘<a href="https://www.nature.com/articles/s41561-024-01580-5" rel="noopener noreferrer" target="_blank">Estimated human-induced warming from a linear temperature and atmospheric CO<sub>2</sub> relationship</a>’ published in Nature Geoscience.</p><p>DOI:<a href="https://www.nature.com/articles/s41561-024-01580-5" rel="noopener noreferrer" target="_blank">10.1038/s41561-024-01580-5</a></p>          <a href="https://www.lancaster.ac.uk/news/">Back to News</a>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Creating a QR Code step by step (232 pts)]]></title>
            <link>https://www.nayuki.io/page/creating-a-qr-code-step-by-step</link>
            <guid>42165862</guid>
            <pubDate>Sun, 17 Nov 2024 18:26:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nayuki.io/page/creating-a-qr-code-step-by-step">https://www.nayuki.io/page/creating-a-qr-code-step-by-step</a>, See on <a href="https://news.ycombinator.com/item?id=42165862">Hacker News</a></p>
Couldn't get https://www.nayuki.io/page/creating-a-qr-code-step-by-step: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Lucid dreaming app triples users' awareness in dreams, study finds (119 pts)]]></title>
            <link>https://www.psypost.org/lucid-dreaming-app-triples-users-awareness-in-dreams-study-finds/</link>
            <guid>42165849</guid>
            <pubDate>Sun, 17 Nov 2024 18:24:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.psypost.org/lucid-dreaming-app-triples-users-awareness-in-dreams-study-finds/">https://www.psypost.org/lucid-dreaming-app-triples-users-awareness-in-dreams-study-finds/</a>, See on <a href="https://news.ycombinator.com/item?id=42165849">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In a recent study published in <a href="https://www.sciencedirect.com/science/article/abs/pii/S1053810024001260"><em>Consciousness and Cognition</em></a>, researchers at Northwestern University showed that a smartphone app using sensory cues can significantly increase the frequency of lucid dreams—dreams in which a person is aware they are dreaming while still asleep. This study marks the first attempt to apply a lucid-dreaming method called Targeted Lucidity Reactivation outside of a lab environment, demonstrating that even a simple at-home approach can help users experience more lucid dreams.</p><p>Lucid dreaming has drawn increasing public interest for its potential benefits, including enhancing creativity, overcoming nightmares, and providing a space for personal growth and skill practice. Traditional techniques for inducing lucid dreams involve cognitive exercises, such as keeping a dream journal, performing reality checks, and practicing intention-setting before sleep.</p><p>Although these techniques can be effective, they require significant dedication and consistency. A streamlined, at-home approach could make lucid dreaming more accessible to the general public. The researchers wanted to explore whether a simplified, app-based approach using Targeted Lucidity Reactivation—a method previously successful in a controlled lab setting—could be adapted for use outside the lab with minimal technical requirements.</p><p>In Targeted Lucidity Reactivation, participants undergo training to associate a sound cue—such as a tone or melody—with becoming aware that they are dreaming. This same sound is then played during sleep to prompt lucidity within a dream, leveraging the brain’s ability to recognize the cue and reawaken a state of self-awareness while dreaming.</p><p>“I have always been fascinated in lucid dreaming because it provides a space to experience yourself in an entirely new way,” said study author Karen Konkoly, a postdoctoral psychology fellow and member of <a href="https://pallerlab.psych.northwestern.edu/" target="_blank" rel="noopener">Ken Paller’s Cognitive Neuroscience Laboratory</a>.</p><p>“Looking around in a lucid dream, you realize that everything before you is generated by your mind, including your sense of self. Moreover, lucid dreaming is a fascinating model for studying consciousness. Since we’ve been developing more effective ways to induce lucid dreams in the sleep laboratory, we wanted to take a step towards making these advances available for individuals to use on their own.”</p><p>The study consisted of two experiments to test whether a smartphone-based Targeted Lucidity Reactivation method could increase lucid dreaming frequency. The first experiment involved 19 participants, all Android users with a history of dream recall, who completed a one-week protocol involving nightly training with a specialized app. In the second experiment, the researchers recruited a larger sample of 416 participants who used the app for multiple nights, allowing for a more comprehensive look at the effectiveness of Targeted Lucidity Reactivation cues in a diverse group.</p><p>In Experiment 1, participants used an app that played specific sounds—a sequence of beeps or a violin tone—to create an association between these sounds and a lucid state of awareness. Each night before sleep, participants completed a 20-minute training exercise where the app’s sound cues prompted them to enter a “lucid mindset.”</p><p>Once asleep, the app replayed these cues intermittently after a six-hour delay, using a gradual volume increase to avoid sudden awakenings. Participants reported any dreams they remembered each morning, noting if they experienced lucid awareness or incorporated the sound cue into their dream.</p><p>This first experiment found that using the app increased lucid dreaming frequency from an average of 0.74 dreams per week (prior to the study) to 2.11 during the week of app use. Many participants credited the app’s cues with prompting lucidity directly or indirectly.</p><p>Experiment 2 built on these results with a more complex design. This time, participants were divided into three groups to clarify the effect of the Targeted Lucidity Reactivation cues. The first group, receiving cues every night, served as the main experimental group. The second and third groups were control conditions: one received “untrained” cues—sounds not used in the pre-sleep training—and the other received no sound cues on alternate nights. This design helped distinguish increases in lucid dreaming due specifically to Targeted Lucidity Reactivation and those possibly caused by arousal from sound cues in general.</p><p>With 50 participants completing the full seven-night protocol, the study provided insight into the distinct impact of Targeted Lucidity Reactivation cues compared to other sounds. Participants receiving these cues reported significantly more lucid dreams on training nights than those receiving untrained or no cues, reinforcing that pairing cues with lucidity training was essential for effective lucid dreaming.</p><p>“Tweaking sleep opens the door for people to change their dreaming,” Paller said. “We are taking a sleep-engineering approach to using sleep for personal benefits, for practicing skills, solving problems, and for spiritual and personal growth.”</p><p>The combined results from both experiments support Targeted Lucidity Reactivation’s potential as an accessible, smartphone-based method for promoting lucid dreaming. While the cues showed a clear benefit, the study also highlighted challenges, such as the potential for cues to disrupt sleep if mistimed, since the app could not detect when participants entered rapid eye movement (REM) sleep.</p><p>To improve the precision of Targeted Lucidity Reactivation in the future, researchers are considering incorporating wearable technology capable of detecting REM sleep, allowing cues to play at the optimal time for triggering lucidity without disturbing sleep. The Northwestern team has already begun collaborating with InteraXon, the company behind the Muse-S headband, which could allow for more precise sleep-stage detection and improve the effectiveness of the Targeted Lucidity Reactivation method.</p><p>“The app used in this research is under continual development and the latest version can be found in the Google Playstore: <a href="https://play.google.com/store/apps/details?id=com.neurelectrics.dive&amp;hl=en_US&amp;gl=US">https://play.google.com/store/apps/details?id=com.neurelectrics.dive&amp;hl=en_US&amp;gl=US</a>,” Konkoly said. “We are working on making better methods available to the public, but we aren’t there yet. We hope to have more options for people next year.”</p><p>The study, “<a href="https://doi.org/10.1016/j.concog.2024.103759" target="_blank" rel="noopener">Provoking lucid dreams at home with sensory cues paired with pre-sleep cognitive training</a>,” was authored by Karen R. Konkoly, Nathan W. Whitmore, Remington Mallett, Christopher Y. Mazurek, and Ken A. Paller.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AlphaProof's Greatest Hits (202 pts)]]></title>
            <link>https://rishimehta.xyz/2024/11/17/alphaproofs-greatest-hits.html</link>
            <guid>42165397</guid>
            <pubDate>Sun, 17 Nov 2024 17:20:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rishimehta.xyz/2024/11/17/alphaproofs-greatest-hits.html">https://rishimehta.xyz/2024/11/17/alphaproofs-greatest-hits.html</a>, See on <a href="https://news.ycombinator.com/item?id=42165397">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      

      <main aria-label="Content">
        <div>
          <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    



<p>Here I’ll try to explain the coolest ideas in each of <a href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/">AlphaProof</a>’s IMO 2024 solutions. AlphaProof produces proofs in <a href="https://leanprover.github.io/">Lean</a>, and each Lean proof is composed of a series of tactics. So I’ll pick out the tactics that correspond to these ideas in the proofs for problems 1, 2 and 6 (the three problems that AlphaProof solved). AlphaProof has developed its own proving style, so figuring out what it’s doing can involve some detective work.</p>

<!--more-->

<p>If you’re not familiar with the problems already, I recommend trying them yourself, and then maybe reading <a href="https://web.evanchen.cc/exams/IMO-2024-notes.pdf">Evan Chen’s solution notes</a>, or watching these <a href="https://youtube.com/playlist?list=PLSa4NIW1yxdg7xoEL2x8wGzci0t8h51nN&amp;si=-GB9q9hreFBD7eOI">videos</a> that give an intuition for some of the human solutions. The full AlphaProof solutions, annotated by <a href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/#:~:text=Oliver%20Nash%2C%20Bhavik%20Mehta%2C%20Paul%20Lezeau%2C%20Salvatore%20Mercuri%2C%20Lawrence%20Wu%2C%20Calle%20Soenne%2C%20Thomas%20Murrills%2C%20Luigi%20Massacci%20and%20Andrew%20Yang%20advised%20and%20contributed%20as%20Lean%20experts">Lean experts</a>, are available <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/imo-2024-solutions/index.html">here</a> - I will only include snippets of the proofs in this post.</p>

<ul id="markdown-toc">
  <li><a href="#problem-1" id="markdown-toc-problem-1">Problem 1</a>    <ul>
      <li><a href="#problem" id="markdown-toc-problem">Problem</a></li>
      <li><a href="#solution" id="markdown-toc-solution">Solution</a></li>
    </ul>
  </li>
  <li><a href="#problem-2" id="markdown-toc-problem-2">Problem 2</a>    <ul>
      <li><a href="#problem-1" id="markdown-toc-problem-1">Problem</a></li>
      <li><a href="#solution-1" id="markdown-toc-solution-1">Solution</a></li>
    </ul>
  </li>
  <li><a href="#problem-6" id="markdown-toc-problem-6">Problem 6</a>    <ul>
      <li><a href="#problem-2" id="markdown-toc-problem-2">Problem</a></li>
      <li><a href="#solution-2" id="markdown-toc-solution-2">Solution</a></li>
    </ul>
  </li>
  <li><a href="#postscript" id="markdown-toc-postscript">Postscript</a></li>
</ul>

<h2 id="problem-1">Problem 1</h2>

<h3 id="problem">Problem</h3>
<p>Determine all real numbers $\alpha$ such that, for every positive integer $n$, the integer</p><p>

\[\lfloor \alpha \rfloor + \lfloor 2\alpha \rfloor + \dots +\lfloor n\alpha \rfloor\]

</p><p>is a multiple of $n$. (Note that $\lfloor z \rfloor$ denotes the greatest integer less than or equal to $z$. For example, $\lfloor -\pi \rfloor = -4$ and $\lfloor 2 \rfloor = \lfloor 2.9 \rfloor = 2$.)</p>

<h3 id="solution">Solution</h3>

<p>The answer turns out to be the set of even integers. Reminder that the way AlphaProof solves these problems is by proposing many solution candidates, attempting to prove and disprove each of them, and then eventually finding a proof only for the correct answer. The proof we’re looking at here is the one that proves that the answer is the set of even integers.</p>

<p>Showing that the even integers satisfy the given property is trivial, the hard part of this proof is to show that no $\alpha$ except the even integers can satisfy it. AlphaProof does this in an interesting (if convoluted) way. It first sets up an integer $\ell$ such that $2\ell = \lfloor\alpha\rfloor + \lfloor2\alpha\rfloor$. This is possible because we know the RHS is even by substituting $n=2$ into the given property.</p>

<figure><pre><code data-lang="lean"><span>exists</span><span>λ</span><span>x</span> <span>L</span><span>=&gt;</span>(<span>L</span> <span>2</span> <span>two_pos</span>)<span>.</span><span>rec</span> <span>λ</span><span>l</span> <span>Y</span><span>=&gt;</span><span>?</span><span>_</span></code></pre></figure>

<p><code>L 2</code> is where the given property is used with $n=2$. As an aside, AlphaProof often mashes several tactics together in a single line like this. A more comprehensible version that accomplishes the same thing is</p>

<figure><pre><code data-lang="lean"><span>constructor</span>
<span>·</span> <span>intro</span> <span>x</span> <span>L</span>
  <span>obtain</span> <span>⟨</span><span>l</span>, <span>Y</span><span>⟩</span> := <span>L</span> <span>2</span> (<span>by</span> <span>exact</span> <span>two_pos</span>)</code></pre></figure>

<p>Note that we’ve also renamed $\alpha$ to <code>x</code>.
Next, it claims (and goes on to prove) that for all natural numbers $n$,</p><p>

\[\lfloor (n + 1)\:\alpha \rfloor = \lfloor \alpha \rfloor + 2n\:(\ell - \lfloor \alpha \rfloor) \tag{1} \label{eq:claim}\]

</p><figure><pre><code data-lang="lean"><span>suffices</span>: <span>∀</span> (<span>n</span> : <span>ℕ</span>),<span>⌊</span>(<span>n</span><span>+</span><span>1</span>)<span>*</span><span>x</span><span>⌋</span> <span>=</span><span>⌊</span> <span>x</span><span>⌋</span><span>+</span><span>2</span> <span>*</span> <span>↑</span> (<span>n</span> : <span>ℕ</span>) <span>*</span> (<span>l</span><span>-</span>(<span>⌊</span>(<span>x</span>)<span>⌋</span>))</code></pre></figure>

<p>From this, it is able to show that $\alpha = 2\,(\ell-\lfloor\alpha\rfloor)$.</p>

<figure><pre><code data-lang="lean"><span>use</span>(<span>l</span><span>-</span><span>⌊</span><span>x</span><span>⌋</span>)<span>*</span><span>2</span></code></pre></figure>

<p>which has to be an even integer (since it’s 2 multiplied by an integer).</p>

<p>The way in which it proves these things involves some rather elaborate simplifications. But setting up the claim in \eqref{eq:claim} is the impressive bit that makes the rest of the proof work. To my eyes, the motivation for this claim is rather unintuitive, and the fact that it all works out is almost magical.</p>

<p>AlphaProof’s full solution is <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/imo-2024-solutions/P1/index.html">here</a>.</p>

<h2 id="problem-2">Problem 2</h2>

<h3 id="problem-1">Problem</h3>

<p>Find all positive integer pairs $(a,b)$ such that there exist positive integers $g$ and $N$ where</p><p>

\[\gcd\,(a^n + b, b^n + a) = g\]

</p><p>holds for all integers $n \geq N$.</p>

<h3 id="solution-1">Solution</h3>

<p>AlphaProof correctly proposes that $(1, 1)$ is the only solution. To show that no other solution can work, it asks us to consider the number $ab + 1$. It claims (and later proves) that $ab + 1$ must divide $g$.</p>

<figure><pre><code data-lang="lean"><span>suffices</span>:<span>b</span><span>.1*</span><span>b</span><span>.2+</span><span>1</span><span>∣</span><span>Y</span></code></pre></figure>

<p>Note that in its infinite wisdom, AlphaProof decides to rename pairs $(a, b)$ to <code>b</code>, so that it must reference the elements as <code>b.1</code> and <code>b.2</code>. It has also chosen, for reasons best known to itself, to rename the variable $g$ to <code>Y</code>.</p>

<p>Now, selecting $n=N \phi(ab+1)$, we get</p><p>

\[(ab + 1) \mid (a^{N\phi(ab + 1)} + b) \text{ and } (ab + 1) \mid (b^{N\phi(ab + 1)} + a)\]

</p><p>The fact that $ab + 1$ is coprime to $a$ and $b$ makes it so that we can apply <a href="https://en.wikipedia.org/wiki/Euler%27s_theorem">Euler’s theorem</a>, ie</p><p>

\[a^{\phi(ab+1)} \equiv 1 \pmod{ab+1}\]

\[b^{\phi(ab+1)} \equiv 1 \pmod{ab+1}\]

</p><p>So we have $ab + 1 \mid 1 + b$ and $ab + 1 \mid 1 + a$, from which it follows that $a = b = 1$.</p>

<p>This strategy closely follows human proofs to this problem. The choice to consider $ab + 1$ is the clever idea that sets up the proof.</p>

<p>AlphaProof’s full solution is <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/imo-2024-solutions/P2/index.html">here</a>.</p>

<h2 id="problem-6">Problem 6</h2>

<h3 id="problem-2">Problem</h3>
<p>Let $\mathbb{Q}$ be the set of rational numbers. A function $f: \mathbb{Q} \to \mathbb{Q}$ is called <em>aquaesulian</em> if the following property holds: for every $x,y \in \mathbb{Q}$,</p><p>

\[f(x+f(y)) = f(x) + y \quad \text{or} \quad f(f(x)+y) = x + f(y).\]

</p><p>Show that there exists an integer $c$ such that for any aquaesulian function $f$ there are at most $c$ different rational numbers of the form $f(r) + f(-r)$ for some rational number $r$, and find the smallest possible value of $c$.</p>

<h3 id="solution-2">Solution</h3>

<p>AlphaProof finds the answer $c=2$. It breaks the proof into two parts. First, it shows that $c \leq 2$, by showing that $f(r) + f(-r)$ can either be $0$ or some singular other value. This part of the proof is quite elaborate, and makes clever use of the given aquaesulian properties.</p>

<p>Once this is done, $c$ can be either $1$ or $2$. To show that $c=2$, AlphaProof proposes an aquaesulian function $f$ that takes on two different values for  $f(r) + f(-r)$.</p><p>

\[f(x) = -x + 2⌈x⌉\]

</p><figure><pre><code data-lang="lean"><span>specialize</span> <span>V</span> <span>$</span> <span>λ</span> <span>N</span><span>=&gt;-</span><span>N</span><span>+</span><span>2</span> <span>*</span><span>Int</span><span>.</span><span>ceil</span> <span>N</span></code></pre></figure>

<p>It then shows that $f(-1) + f(1) = 0$ and $f(1/2) + f(-1/2) = 2$, which gives us the two distinct values we need.</p>

<figure><pre><code data-lang="lean"><span>use</span> <span>Finset</span><span>.</span><span>one_lt_card</span><span>.2</span><span>$</span> <span>by</span> <span>exists</span><span>@</span><span>0</span>,<span>V</span><span>.1.</span><span>mem_toFinset</span><span>.2</span> (<span>by</span> <span>exists</span><span>-</span><span>1</span>),<span>2</span>,<span>V</span><span>.1.</span><span>mem_toFinset</span><span>.2</span> (<span>by</span> <span>exists</span> <span>1</span><span>/</span><span>2</span>)</code></pre></figure>

<p>Again, lots of stuff mashed into one line, but <code>by exists -1</code> and <code>by exists 1/2</code> are where it shows the two distinct values.</p>

<p>This is a remarkable function to have constructed! And it’s pretty hard to find. Only 5/509 participants solved P6, and notably Tim Gowers gave it a bit of a shot while judging this solution and didn’t find a function that gave two distinct values.</p>

<p>AlphaProof’s full solution is <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/imo-2024-solutions/P6/index.html">here</a>.</p>

<h2 id="postscript">Postscript</h2>

<p>Having now digested several AlphaProof solutions, I still find it remarkable that a machine is able to find proofs like this. Finding these mathematical ideas is hard enough, and having to prove them in Lean makes things even harder.</p>

<p>At an IMO afterparty, some of us from the AlphaProof team had fun with signing the best tactic from each proof.</p>

<p><img src="https://rishimehta.xyz/assets/p1_tactic.jpg" alt="p1 tactic">
<img src="https://rishimehta.xyz/assets/p2_tactic.jpg" alt="p2 tactic">
<img src="https://rishimehta.xyz/assets/p6_tactic.jpg" alt="p6 tactic"></p>

<p>I discussed the P6 solution in some more detail on the <a href="https://youtu.be/uX6ceY1vcUg?si=d9LY1QvdQ-yeppPK&amp;t=1845">No Priors podcast</a>.</p>

<p>Thanks to the <a href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/#:~:text=Oliver%20Nash%2C%20Bhavik%20Mehta%2C%20Paul%20Lezeau%2C%20Salvatore%20Mercuri%2C%20Lawrence%20Wu%2C%20Calle%20Soenne%2C%20Thomas%20Murrills%2C%20Luigi%20Massacci%20and%20Andrew%20Yang%20advised%20and%20contributed%20as%20Lean%20experts">Lean experts</a> who translated the cryptic Lean proofs into English, and Bhavik, Oliver and Paul in particular for suggesting impressive lines in the proofs.</p>

  </div>
</article>

        </div>
      </main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Good Software Development Habits (325 pts)]]></title>
            <link>https://zarar.dev/good-software-development-habits/</link>
            <guid>42165057</guid>
            <pubDate>Sun, 17 Nov 2024 16:34:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zarar.dev/good-software-development-habits/">https://zarar.dev/good-software-development-habits/</a>, See on <a href="https://news.ycombinator.com/item?id=42165057">Hacker News</a></p>
<div id="readability-page-1" class="page">
  
  <header>
    <a href="https://zarar.dev/">
      <h2>
        Zarar's blog
      </h2>
    </a>
    <nav>
      <p><a href="https://zarar.dev/">Home</a>
<a href="https://zarar.dev/me/">Me</a>
<a href="https://zarar.dev/subscribe/">Subscribe</a></p>

    </nav>
  </header>
  <main>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2024-09-05T13:00Z">
                    05 Sep, 2024
                </time>
            </i>
        </p>
    

    <p>This post is not advice, it's what's working for me.</p>
<p>It's easy to pick up bad habits and hard to create good ones. Writing down what's working for me helps me maintain any good habits I've worked hard to develop. Here's an unordered list of 10 things that have helped me increase speed and maintain a respectable level of quality in the product I'm currently developing.</p>
<ol>
<li><p>Keep commits small enough that you wonder if you're taking this "keep commits small" thing a little too far. You just never know when you have to revert a particular change and there's a sense of bliss knowing where you introduced a bug six days ago and only reverting that commit without going through the savagery of merge conflicts. My rule of thumb: compiling software should be commitable.</p>
</li>
<li><p>Live Kent Beck's <a href="https://x.com/KentBeck/status/250733358307500032?lang=en">holy words of wisdom</a>: "for each desired change, make the change easy (warning: this may be hard), then make the easy change". Aim for at least half of all commits to be refactorings. Continuous refactoring is thinking of changes I can make in under 10 minutes that improve something. Doing this pays off whenever a bigger requirement comes in and you find yourself making a small change to satisfy it only because of those smaller improvements. Big refactorings are a bad idea.</p>
</li>
<li><p>All code is a liability. Undeployed code is the grim reaper of liabilities. I need to know if it works or at least doesn't break anything. Tests give you confidence, production gives you approval. The hosting costs might rack up a little with so many deploys but it's a small price to pay for knowing the last thing you did was a true sign of progression. <em>Working software is the primary measure of progress</em>, says one of the <a href="https://agilemanifesto.org/principles.html">agile principles</a>. Working and progress are doing a lot of heavy lifting in that sentence, so I've defined them for myself. Working is something being working enough to be deployed, and if it's code that's contributing to a capability, that's progress.</p>
</li>
<li><p>Know when you're testing the framework's capability. If you are, don't do it. The framework is already tested by people who know a lot more than you, and you have to trust them that the <code>useState()</code> hook does what it's supposed to do. If you keep components small, then you reduce the need for a lot of tests as the framework will be doing most of the heavy lifting in the component. If the component is big, then you introduce more complexity and now you need to write a lot of tests.</p>
</li>
<li><p>If a particular function doesn't fit anywhere, create a new module (or class or component) for it and you'll find a home for it later. It's better to create a new independent construct than to jam it into an existing module where you know deep down it doesn't make sense. Worst comes to worst, it lives as an independent module which isn't too bad anyway.</p>
</li>
<li><p>If you don't know what an API should look like, write the tests first as it'll force you to think of the "customer" which in this case is you. You'll invariably discover cases that you would not have thought of if you had just written the code first and tests after. You don't have to be religious about TDD and it's OK to work in larger batches (e.g., write more than just a couple lines of code before making it pass).  The amount of code to write in a red/failing state doesn't always have to be small. You know what you're doing, don't let dogma get in the way of productivity.</p>
</li>
<li><p>Copy-paste is OK once. The second time you're introducing duplication (i.e., three copies), don't. You should have enough data points to create a good enough abstraction. The risk of diverging implementations of the same thing is too high at this point, and consolidation is needed.  It's better to have some wonky parameterization than it is to have multiple implementations of nearly the same thing. Improving the parameters will be easier than to consolidate four different implementations if this situation comes up again.</p>
</li>
<li><p>Designs get stale. You can slow the rate at which they get stale by refactoring, but ultimately you'll need to change how things work. Don't feel too bad about moving away from something that was dear to you a while ago and something you felt proud about at the time.  You did the right thing then and shouldn't beat yourself up for not getting it right enough that you wouldn't need to change anything. Most of the time writing software is changing software. Just accept it and move on. There's no such thing as the perfect design, and change is at the core of software development. How good you are at changing things is how good you are at software development.</p>
</li>
<li><p>Technical debt can be classified into three main types: 1) things that are preventing you from doing stuff now, 2) things that will prevent you from doing stuff later, and 3) things that <em>might</em> prevent you from doing stuff later. Every other classification is a subset of these three. Minimize having lots of stuff in #1 and try to focus on #2. Ignore #3.</p>
</li>
<li><p>Testability is correlated with good design. Something not being easily testable hints that the design needs to be changed. Sometimes that design is your test design. As an example, if you find yourself finding it difficult to mock <code>em.getRepository(User).findOneOrFail({id})</code>, then chances are you either need to put that call into its own function that can be mocked, or write a test utility which allows for easier mocking of the entity manager methods. Tests go unwritten when it's hard to test, not because you don't want to test.</p>
</li>
</ol>
<p>There's probably a lot more, but 10 is a nice number.</p>


    

    
        

        
            


        
    


  </main>
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everything Is Just Functions: Mind-Blowing Insights from SICP and David Beazley (370 pts)]]></title>
            <link>https://ezzeriesa.notion.site/1-week-with-David-Beazley-and-SICP-4c440389cf1e43f48fe67c969967f655#58ee6b0435b24e26bd624b33ffed94df</link>
            <guid>42164541</guid>
            <pubDate>Sun, 17 Nov 2024 15:07:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ezzeriesa.notion.site/1-week-with-David-Beazley-and-SICP-4c440389cf1e43f48fe67c969967f655#58ee6b0435b24e26bd624b33ffed94df">https://ezzeriesa.notion.site/1-week-with-David-Beazley-and-SICP-4c440389cf1e43f48fe67c969967f655#58ee6b0435b24e26bd624b33ffed94df</a>, See on <a href="https://news.ycombinator.com/item?id=42164541">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The myth that you can’t build interactive web apps except as single page app (224 pts)]]></title>
            <link>https://htmx.org/essays/you-cant/</link>
            <guid>42164154</guid>
            <pubDate>Sun, 17 Nov 2024 13:44:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://htmx.org/essays/you-cant/">https://htmx.org/essays/you-cant/</a>, See on <a href="https://news.ycombinator.com/item?id=42164154">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

  
  
    <address>Tony Alaribe</address>
    <p><time>September 20, 2024</time></p><h3 id="an-ode-to-browser-advancements"><a href="#an-ode-to-browser-advancements" aria-label="Anchor link for: an-ode-to-browser-advancements">#</a><strong>An Ode to Browser Advancements.</strong></h3>
<p>I often encounter discussions on Reddit and YCombinator where newer developers seek tech stack advice. Inevitably,
someone claims it’s impossible to build a high-quality application without using a single-page application (SPA)
framework like React or AngularJS. This strikes me as odd because, even before the SPA revolution, many popular
multi-page web applications offered excellent user experiences.</p>
<p>Two years ago, I set out to build an <a rel="noopener" target="_blank" href="https://apitoolkit.io/">observability platform</a> and chose to experiment with a
multi-page application (MPA) approach using HTMX. I wondered: Would a server-rendered MPA be inadequate for a data-heavy
application, considering that most observability platforms are built on ReactJS?</p>
<p>What I discovered is that you can create outstanding server-rendered applications if you pay attention to certain
details.</p>
<p><strong>Here are some common MPA myths and what I’ve learned about them.</strong></p>
<h2 id="myth-1-mpa-page-transitions-are-slow-because-javascript-and-css-are-downloaded-on-every-page-navigation"><a href="#myth-1-mpa-page-transitions-are-slow-because-javascript-and-css-are-downloaded-on-every-page-navigation" aria-label="Anchor link for: myth-1-mpa-page-transitions-are-slow-because-javascript-and-css-are-downloaded-on-every-page-navigation">#</a>Myth 1:  MPA Page Transitions are slow because JavaScript and CSS are downloaded on every page navigation</h2>
<p>The perception that MPA page transitions are slow is widespread—and not entirely unfounded—since this is the default
behavior of browsers. However, browsers have made significant improvements over the past decade to mitigate this issue.</p>
<p>To illustrate, in the video below, a full page reload with the cache disabled takes 2.90 seconds until the
DOMContentLoaded event fires. I recorded this at a café with poor Wi-Fi, but let’s use this as a reference point. Keep
that number in mind.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/log-exp-cache.mp4">
</video>
<p>It is common to reduce load times in MPAs using libraries such as <strong>PJAX, Turbolinks, and even HTMX Boost</strong>. These
libraries hijack the page reload using Javascript and swap out only the HTML body element between transitions. That way,
most of the page’s head section assets don’t need to be reloaded or re-downloaded.</p>
<p>But there’s a lesser known way of reducing how much assets are re-downloaded or evaluated during page transitions.</p>
<h3 id="client-side-caching-via-service-workers"><a href="#client-side-caching-via-service-workers" aria-label="Anchor link for: client-side-caching-via-service-workers">#</a>Client-side Caching via Service workers</h3>
<p>Frontend developers who have built Progressive Web Applications (PWA) with SPA frameworks might know about service
workers.</p>
<p>For those of us who are not frontend or PWA developers, service workers are a built-in feature of browsers. They let you
write Javascript code that sits between your users and the network, intercepting requests and deciding how the browser
handles them.</p>
<p><img src="https://htmx.org/img/you-cant/service-worker-chart.png" alt="service-worker-chart.png"></p>
<p>Due to its association with the PWA trend, service workers are only ordinary among SPA developers, and developers need
to realize that this technology can also be used for regular Multi-Page Applications.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/log_exp_with_cache.mp4">
</video>
<p>In the video demonstration, we enable a service worker to cache and refresh the current page. You’ll notice that there’s
no flicker when clicking the link to reload the page, resulting in a smoother user experience.</p>
<p>Moreover, instead of transmitting over 2 MB of static assets as before, the browser now only fetches 84 KB of HTML
content—the actual page data. This optimization reduces the <code>DOMContentLoaded</code> event time from 2.9 seconds to under 500
milliseconds. Impressively, this improvement is achieved <strong>without</strong> using HTMX Boost, PJAX, or Turbolinks.</p>
<h3 id="how-to-implement-service-workers-in-your-multi-page-application"><a href="#how-to-implement-service-workers-in-your-multi-page-application" aria-label="Anchor link for: how-to-implement-service-workers-in-your-multi-page-application">#</a>How to Implement Service workers in Your Multi-Page Application</h3>
<p>You might be wondering how to replicate these performance gains in your own MPA. Here’s a simple guide:</p>
<ol>
<li><strong>Create a <code>sw.js</code> File</strong>: This is your service worker script that will manage caching and network requests.</li>
<li><strong>List Files to Cache</strong>: Within the service worker, specify all the assets (HTML, CSS, JavaScript, images) that
should be cached.</li>
<li><strong>Define Caching Strategies</strong>: Indicate how each type of asset should be cached—for example, whether they should be
cached permanently or refreshed periodically.</li>
</ol>
<p>By implementing a service worker, you effectively tell the browser how to handle network requests and caching, leading
to faster load times and a more seamless user experience.</p>
<h3 id="use-workbox-to-generate-service-workers"><a href="#use-workbox-to-generate-service-workers" aria-label="Anchor link for: use-workbox-to-generate-service-workers">#</a>Use Workbox to generate service workers</h3>
<p>While it’s possible to write service workers by hand—and there are excellent resources
like <a rel="noopener" target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/Service_Worker_API/Using_Service_Workers">this MDN article</a> to
help you—I prefer using Google’s <a rel="noopener" target="_blank" href="https://developer.chrome.com/docs/workbox">Workbox</a> library to automate the process.</p>
<h3 id="steps-to-use-workbox"><a href="#steps-to-use-workbox" aria-label="Anchor link for: steps-to-use-workbox">#</a>Steps to Use Workbox:</h3>
<ol>
<li>
<p><strong>Install Workbox</strong>: Install Workbox via npm or your preferred package manager:</p>
<pre data-lang="bash"><code data-lang="bash"><span>npm</span><span> install workbox-cli</span><span> --global
</span></code></pre>
</li>
<li>
<p>Generate a Workbox Configuration file: Run the following command to create a configuration file:</p>
<pre data-lang="bash"><code data-lang="bash"><span>workbox</span><span> wizard
</span></code></pre>
</li>
<li>
<p><strong>Configure Asset Handling</strong>: In the generated <code>workbox-config.js</code> file, define how different assets should be
cached. Use the <code>urlPattern</code> property—a regular expression—to match specific HTTP requests. For each matching
request, specify a caching strategy, such as <code>CacheFirst</code> or <code>NetworkFirst</code>.</p>
<p><img src="https://htmx.org/img/you-cant/workbox-cfg.png" alt="workbox-cfg.png"></p>
</li>
<li>
<p><strong>Build the Service Worker</strong>: Run the Workbox build command to generate the <code>sw.js</code> file based on your configuration:</p>
<pre data-lang="bash"><code data-lang="bash"><span>workbox</span><span> generateSW workbox-config.js
</span></code></pre>
</li>
<li>
<p><strong>Register the Service Worker in Your Application</strong>: Add the following script to your HTML pages to register the
service worker:</p>
<pre data-lang="html"><code data-lang="html"><span>&lt;</span><span>script</span><span>&gt;
</span><span>  </span><span>if </span><span>(</span><span>'serviceWorker' </span><span>in navigator) {
</span><span>    window.</span><span>addEventListener</span><span>(</span><span>'load'</span><span>, </span><span>function</span><span>() {
</span><span>      navigator.serviceWorker.</span><span>register</span><span>(</span><span>'/sw.js'</span><span>).</span><span>then</span><span>(</span><span>function</span><span>(registration) {
</span><span>        console.</span><span>log</span><span>(</span><span>'ServiceWorker registration successful with scope: '</span><span>, </span><span>registration</span><span>.scope);
</span><span>      }, </span><span>function</span><span>(err) {
</span><span>        console.</span><span>log</span><span>(</span><span>'ServiceWorker registration failed: '</span><span>, </span><span>err</span><span>);
</span><span>      });
</span><span>    });
</span><span>  }
</span><span>&lt;/</span><span>script</span><span>&gt;
</span></code></pre>
</li>
</ol>
<p>By following these steps, you instruct the browser to serve cached assets whenever possible, drastically reducing load
times and improving the overall performance of your multi-page application.</p>
<p><img src="https://htmx.org/img/you-cant/service-worker.png" alt="Image showing the registered service worker from the chrome browser console."></p>
<p>Image showing the registered service worker from the chrome browser console.</p>
<h3 id="speculation-rules-api-prerender-pages-for-instant-page-navigation"><a href="#speculation-rules-api-prerender-pages-for-instant-page-navigation" aria-label="Anchor link for: speculation-rules-api-prerender-pages-for-instant-page-navigation">#</a><a rel="noopener" target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/Speculation_Rules_API"><code>Speculation Rules API</code></a>: Prerender pages for instant page navigation.</h3>
<p>If you have used <strong>htmx-preload</strong> or <strong>instantpage.js,</strong> you’re familiar with prerendering and the problem
the <a rel="noopener" target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/Speculation_Rules_API">“Speculation Rules API”</a> aims to solve. The
Speculation Rules API is designed to improve performance for future navigations. It has an expressive syntax for
specifying which links should be prefetched or prerendered on the current page.</p>
<p><img src="https://htmx.org/img/you-cant/speculation-rules.png" alt="Speculation rules configuration example"></p>
<p>Speculation rules configuration example</p>
<p>The script above is an example of how speculation rules are configured. It is a Javascript object, and without going
into detail, you can see that it uses keywords such as “where,” “and,” “not,” etc. to describe what elements should
either be prefetched or prerendered.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/prerender-vid.mp4">
</video>
<p><a rel="noopener" target="_blank" href="https://developer.chrome.com/docs/web-platform/prerender-pages">Example impact of prerendering (Chrome Team)</a></p>
<h2 id="myth-2-mpas-can-t-operate-offline-and-save-updates-to-retry-when-there-s-network"><a href="#myth-2-mpas-can-t-operate-offline-and-save-updates-to-retry-when-there-s-network" aria-label="Anchor link for: myth-2-mpas-can-t-operate-offline-and-save-updates-to-retry-when-there-s-network">#</a>Myth 2: MPAs can’t operate offline and save updates to retry when there’s network</h2>
<p>From the last sections, you know that service workers can cache everything and make our apps operate entirely offline.
But what if we want to save offline POST requests and retry them when there is internet?</p>
<p><img src="https://htmx.org/img/you-cant/workbox-offline-cfg.png" alt="workbox-offline-cfg.png"></p>
<p>The configuration javascript file above shows how to configure Workbox to support two common offline scenarios. Here,
you see background Sync, where we ask the service worker to cache any failed requests due to the internet and retry it
for up to 24 hours.</p>
<p>Below, we define an offline catch Handler, triggered when a request is made offline. We can return a template partial
with HTML or a JSON response or dynamically build a response based on the request input. The sky is the limit here.</p>
<h2 id="myth-3-mpas-always-flash-white-during-page-transitions"><a href="#myth-3-mpas-always-flash-white-during-page-transitions" aria-label="Anchor link for: myth-3-mpas-always-flash-white-during-page-transitions">#</a>Myth 3: MPAs always flash white during page Transitions</h2>
<p>In the service worker videos, we already saw that this will not happen if we configure caching and prerendering.
However, this myth was not generally true until 2019. Since 2019, most browsers withhold painting the next screen until
all the required assets for the next page are available or a timeout is reached, resulting in no flash of white while
transitioning between both pages. This only works when navigating within the same origin/domain.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/paint-holding.mp4">
</video>
<p><a rel="noopener" target="_blank" href="https://developer.chrome.com/blog/paint-holding">Paint holding documentation on chrome.com</a>.</p>
<h2 id="myth-4-fancy-cross-document-page-transitions-are-not-possible-with-mpas"><a href="#myth-4-fancy-cross-document-page-transitions-are-not-possible-with-mpas" aria-label="Anchor link for: myth-4-fancy-cross-document-page-transitions-are-not-possible-with-mpas">#</a>Myth 4: Fancy Cross-document page transitions are not possible with MPAs.</h2>
<p>The advent of single-page application frameworks made custom transitions between pages more popular. The allure of
different navigation styles comes from completely taking control of page navigation from the browsers. In practice, such
transitions have mostly been popular within the demos at web dev conference talks.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/page-transitions.mp4">
</video>
<p><a rel="noopener" target="_blank" href="https://developer.chrome.com/docs/web-platform/view-transitions">Cross Document Transitions documentation on chrome.com</a>.</p>
<p>This remains a common argument for single-page applications, especially on Reddit and Hacker News comment sections.
However, browsers have been working towards solving this problem natively for the last couple of years. Chrome 126
rolled out cross-document view transitions. This means we can build our MPAs to include those fancy animations and
transitions between pages using CSS only or CSS and Javascript.</p>
<p>My favorite bit is that we might be able to create lovely cross-document transitions with CSS only:</p>
<p><img src="https://htmx.org/img/you-cant/cross-doc-transitions-css.png" alt="cross-doc-transitions-css.png"></p>
<p>You can quickly learn more on
the <a rel="noopener" target="_blank" href="https://developer.chrome.com/docs/web-platform/view-transitions">Google Chrome announcement page</a></p>
<p>This link hosts a <a rel="noopener" target="_blank" href="https://view-transitions.netlify.app/stack-navigator/mpa-prerender/">multi-page application demo</a>,
where you can play around with a rudimentary server-rendered application using the cross-document view transitions API
to simulate a stack-based animation.</p>
<h2 id="myth-5-with-htmx-or-mpas-every-user-action-must-happen-on-the-server"><a href="#myth-5-with-htmx-or-mpas-every-user-action-must-happen-on-the-server" aria-label="Anchor link for: myth-5-with-htmx-or-mpas-every-user-action-must-happen-on-the-server">#</a>Myth 5: With htmx or MPAs, every user action must happen on the server.</h2>
<p>I’ve heard this a lot when HTMX is discussed. So, there might be some confusion caused by the HTMX positioning. But you
don’t have to do everything server-side. Many HTMX and regular MPA users continue to use Javascript, Alpine, or
Hyperscript where appropriate.</p>
<p>In situations where robust interactivity is helpful, you can lean into the component islands architecture using
WebComponents or any javascript framework (react, angular, etc) of your choice. That way, instead of your entire
application being an SPA, you can leverage those frameworks specifically for the bits of your application that need that
interactivity.</p>
<p>The example above shows a very interactive search component in the <a rel="noopener" target="_blank" href="https://apitoolkit.io/">APItoolkit</a>. It’s a web
component implemented with lit-element, a zero-compile-step library for writing web components. So, the entire web
component event fits in a Javascript file.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/webcomponents-filter-element2.mp4">
</video>
<h2 id="myth-6-operating-directly-on-the-dom-is-slow-therefore-it-would-be-best-to-use-react-virtual-dom"><a href="#myth-6-operating-directly-on-the-dom-is-slow-therefore-it-would-be-best-to-use-react-virtual-dom" aria-label="Anchor link for: myth-6-operating-directly-on-the-dom-is-slow-therefore-it-would-be-best-to-use-react-virtual-dom">#</a>Myth 6: Operating directly on the DOM is slow. Therefore, it would be best to use React/Virtual DOM.</h2>
<p>The speed of direct DOM operations was a major motivation for building ReactJS on and popularizing the virtual DOM
technology. While virtual DOM operations can be faster than direct DOM operations, this is only true for applications
that perform many complex operations and refresh in milliseconds, where that performance might be noticeable. But most
of us are not building such software.</p>
<p>The Svelte team wrote an excellent article
titled <a rel="noopener" target="_blank" href="https://svelte.dev/blog/virtual-dom-is-pure-overhead">“Virtual DOM is pure Overhead.”</a> I recommend reading it,
as it better explains why Virtual DOM doesn’t matter for most applications.</p>
<h2 id="myth-7-you-still-need-to-write-javascript-for-every-minor-interactivity"><a href="#myth-7-you-still-need-to-write-javascript-for-every-minor-interactivity" aria-label="Anchor link for: myth-7-you-still-need-to-write-javascript-for-every-minor-interactivity">#</a>Myth 7: You still need to write JavaScript for every minor interactivity.</h2>
<p>With the advancements in browser tech, you can avoid writing a lot of client-side Javascript in the first place. For
example, a standard action on the web is to show and hide things based on a button click or toggle. These days, you can
show and hide elements with only CSS and HTML, for example, by using an HTML input checkbox to track state. We can style
an HTML label as a button and give it a <code>for="checkboxID</code>“ attribute, so clicking the label toggles the checkbox.</p>
<pre data-lang="jsx"><code data-lang="jsx"><span>&lt;input id="published" class="hidden peer" type="checkbox"/&gt;
</span><span>&lt;label for="published" class="btn"&gt;toggle content&lt;/label&gt;
</span><span>
</span><span>&lt;div class="hidden peer-checked:block"&gt;
</span><span>    Content to be toggled when label/btn is clicked
</span><span>&lt;/div&gt;
</span></code></pre>
<p>We can combine such a checkbox with HTMX intersect to fetch content from an endpoint when the button is clicked.</p>
<pre data-lang="html"><code data-lang="html"><span>&lt;</span><span>input </span><span>id</span><span>=</span><span>"published" </span><span>class</span><span>=</span><span>"peer" </span><span>type</span><span>=</span><span>"checkbox" </span><span>name</span><span>=</span><span>"status"</span><span>/&gt;
</span><span>&lt;</span><span>div
</span><span>        </span><span>class</span><span>=</span><span>"hidden peer-checked:block"
</span><span>        </span><span>hx-trigger</span><span>=</span><span>"intersect once"
</span><span>        </span><span>hx-get</span><span>=</span><span>"/log-item"
</span><span>&gt;Shell/Loading text etc
</span><span>&lt;/</span><span>div</span><span>&gt;
</span></code></pre>
<p>All the classes above are vanilla <a rel="noopener" target="_blank" href="https://tailwindcss.com/">Tailwind CSS</a> classes, but you can also write the CSS by
hand. Below is a video of that code being used to hide or reveal log items in the log explorer.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/expanding-log-item.mp4">
</video>
<h2 id="final-myth-without-a-proper-frontend-framework-your-client-side-javascript-will-be-spaghetti-and-unmaintainable"><a href="#final-myth-without-a-proper-frontend-framework-your-client-side-javascript-will-be-spaghetti-and-unmaintainable" aria-label="Anchor link for: final-myth-without-a-proper-frontend-framework-your-client-side-javascript-will-be-spaghetti-and-unmaintainable">#</a>Final Myth: Without a <em>“Proper”</em> frontend framework, your Client-side Javascript will be <a rel="noopener" target="_blank" href="https://www.reddit.com/r/webdev/comments/bkk0gl/avoiding_the_vanillajs_spaghetticode/">Spaghetti and Unmaintainable</a>.</h2>
<p>This may or may not be true.</p>
<h3 id="who-cares-i-love-spaghetti"><a href="#who-cares-i-love-spaghetti" aria-label="Anchor link for: who-cares-i-love-spaghetti">#</a>Who cares? I love Spaghetti.</h3>
<p>I like to argue that some of the most productive days of the web were the PHP and JQuery spaghetti days. A lot of
software was built at that time, including many of the popular internet brands we know today. Most of them were built as
so-called spaghetti codes, which helped them ship their products early and survive long enough to refactor and not be
spaghetti.</p>
<h2 id="conclusion"><a href="#conclusion" aria-label="Anchor link for: conclusion">#</a>Conclusion</h2>
<p>The entire point of this talk is to show you that a lot is possible with browsers in 2024. While we were not looking,
browsers have closed the gap and borrowed the best ideas from the single-page application revolution. For example,
WebComponents exist thanks to the lessons we learned from single-page applications.</p>
<p>So now, we can build very interactive, even offline web applications using mostly browser tools—HTML, CSS, maybe some
Javascript—and still not sacrifice much in terms of user experience.</p>
<h3>The browser has come a long way. Give it a chance!</h3>

  <p>
    &lt;/&gt;
  </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude AI built me a React app to compare maps side by side (198 pts)]]></title>
            <link>https://github.com/veloplanner/map-matrix</link>
            <guid>42164141</guid>
            <pubDate>Sun, 17 Nov 2024 13:39:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/veloplanner/map-matrix">https://github.com/veloplanner/map-matrix</a>, See on <a href="https://news.ycombinator.com/item?id=42164141">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Map Matrix</h2><a id="user-content-map-matrix" aria-label="Permalink: Map Matrix" href="#map-matrix"></a></p>
<p dir="auto">Compare multiple maps side by side. <a href="https://veloplanner.github.io/map-matrix/" rel="nofollow">Live demo</a></p>
<p dir="auto"><strong>This project was mostly generated by Claude AI.</strong></p>
<p dir="auto">I wanted to develop a simple tool that I needed for <a href="https://veloplanner.com/" rel="nofollow">veloplanner.com</a>. I thought about using this opportunity to try out Claude AI for coding a project from scratch. It worked surprisingly well! I was able to explain my idea and get a working prototype in a few hours. Most of the time I was just copying code from Claude and pasting it into the editor. Later, I started using Cursor AI (with claude-3.5-sonnet model) which improved the experience a lot.</p>
<p dir="auto">You can add custom map source by clicking the "Add Custom Source" button in the navbar.
Configuration is stored in the browser's local storage.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/veloplanner/map-matrix/blob/main/screenshot.png"><img src="https://github.com/veloplanner/map-matrix/raw/main/screenshot.png" alt="screenshot"></a></p>
<p dir="auto">Example of Cursor AI flow:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/veloplanner/map-matrix/blob/main/screenshot-cursor-ai.png"><img src="https://github.com/veloplanner/map-matrix/raw/main/screenshot-cursor-ai.png" alt="screenshot-cursor-ai.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>

</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Memos – An open source Rewinds / Recall (102 pts)]]></title>
            <link>https://github.com/arkohut/memos</link>
            <guid>42163978</guid>
            <pubDate>Sun, 17 Nov 2024 12:59:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/arkohut/memos">https://github.com/arkohut/memos</a>, See on <a href="https://news.ycombinator.com/item?id=42163978">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p><a target="_blank" rel="noopener noreferrer" href="https://github.com/arkohut/pensieve/blob/master/web/static/logos/memos_logo_512.png"><img src="https://github.com/arkohut/pensieve/raw/master/web/static/logos/memos_logo_512.png" width="250"></a>
</p>
<p dir="auto">English | <a href="https://github.com/arkohut/pensieve/blob/master/README_ZH.md">简体中文</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/arkohut/pensieve/blob/master/docs/images/memos-search-en.gif"><img src="https://github.com/arkohut/pensieve/raw/master/docs/images/memos-search-en.gif" alt="memos-search" data-animated-image=""></a></p>
<blockquote>
<p dir="auto">I changed the name to Pensieve because Memos was already taken.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Pensieve (previously named Memos)</h2><a id="user-content-pensieve-previously-named-memos" aria-label="Permalink: Pensieve (previously named Memos)" href="#pensieve-previously-named-memos"></a></p>
<p dir="auto">Pensieve is a privacy-focused passive recording project. It can automatically record screen content, build intelligent indices, and provide a convenient web interface to retrieve historical records.</p>
<p dir="auto">This project draws heavily from two other projects: one called <a href="https://www.rewind.ai/" rel="nofollow">Rewind</a> and another called <a href="https://support.microsoft.com/en-us/windows/retrace-your-steps-with-recall-aa03f8a0-a78b-4b3e-b0a1-2eb8ac48701c" rel="nofollow">Windows Recall</a>. However, unlike both of them, Pensieve allows you to have complete control over your data, avoiding the transfer of data to untrusted data centers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>🚀 Simple installation: just install dependencies via pip to get started</li>
<li>🔒 Complete data control: all data is stored locally, allowing for full local operation and self-managed data processing</li>
<li>🔍 Full-text and vector search support</li>
<li>🤖 Integrates with Ollama, using it as the machine learning engine for Pensieve</li>
<li>🌐 Compatible with any OpenAI API models (e.g., OpenAI, Azure OpenAI, vLLM, etc.)</li>
<li>💻 Supports Mac and Windows (Linux support is in development)</li>
<li>🔌 Extensible functionality through plugins</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/arkohut/pensieve/blob/master/docs/images/memos-installation.gif"><img src="https://github.com/arkohut/pensieve/raw/master/docs/images/memos-installation.gif" alt="memos-installation" data-animated-image=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">1. Install Pensieve</h3><a id="user-content-1-install-pensieve" aria-label="Permalink: 1. Install Pensieve" href="#1-install-pensieve"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">2. Initialize</h3><a id="user-content-2-initialize" aria-label="Permalink: 2. Initialize" href="#2-initialize"></a></p>
<p dir="auto">Initialize the pensieve configuration file and sqlite database:</p>

<p dir="auto">Data will be stored in the <code>~/.memos</code> directory.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">3. Start the Service</h3><a id="user-content-3-start-the-service" aria-label="Permalink: 3. Start the Service" href="#3-start-the-service"></a></p>

<p dir="auto">This command will:</p>
<ul dir="auto">
<li>Begin recording all screens</li>
<li>Start the Web service</li>
<li>Set the service to start on boot</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">4. Access the Web Interface</h3><a id="user-content-4-access-the-web-interface" aria-label="Permalink: 4. Access the Web Interface" href="#4-access-the-web-interface"></a></p>
<p dir="auto">Open your browser and visit <code>http://localhost:8839</code></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/arkohut/pensieve/blob/master/docs/images/init-page-en.png"><img src="https://github.com/arkohut/pensieve/raw/master/docs/images/init-page-en.png" alt="init page"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mac Permission Issues</h3><a id="user-content-mac-permission-issues" aria-label="Permalink: Mac Permission Issues" href="#mac-permission-issues"></a></p>
<p dir="auto">On Mac, Pensieve needs screen recording permission. When the program starts, Mac will prompt for screen recording permission - please allow it to proceed.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/arkohut/pensieve/blob/master/docs/images/mac-security-permission.jpg"><img src="https://github.com/arkohut/pensieve/raw/master/docs/images/mac-security-permission.jpg" alt="mac permission"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">User Guide</h2><a id="user-content-user-guide" aria-label="Permalink: User Guide" href="#user-guide"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using the Appropriate Embedding Model</h3><a id="user-content-using-the-appropriate-embedding-model" aria-label="Permalink: Using the Appropriate Embedding Model" href="#using-the-appropriate-embedding-model"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">1. Model Selection</h4><a id="user-content-1-model-selection" aria-label="Permalink: 1. Model Selection" href="#1-model-selection"></a></p>
<p dir="auto">Pensieve uses embedding models to extract semantic information and build vector indices. Therefore, choosing an appropriate embedding model is crucial. Depending on the user's primary language, different embedding models should be selected.</p>
<ul dir="auto">
<li>For Chinese scenarios, you can use the <a href="https://huggingface.co/jinaai/jina-embeddings-v2-base-zh" rel="nofollow">jinaai/jina-embeddings-v2-base-zh</a> model.</li>
<li>For English scenarios, you can use the <a href="https://huggingface.co/jinaai/jina-embeddings-v2-base-en" rel="nofollow">jinaai/jina-embeddings-v2-base-en</a> model.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">2. Adjust Memos Configuration</h4><a id="user-content-2-adjust-memos-configuration" aria-label="Permalink: 2. Adjust Memos Configuration" href="#2-adjust-memos-configuration"></a></p>
<p dir="auto">Open the <code>~/.memos/config.yaml</code> file with your preferred text editor and modify the <code>embedding</code> configuration:</p>
<div dir="auto" data-snippet-clipboard-copy-content="embedding:
  use_local: true
  model: jinaai/jina-embeddings-v2-base-en   # Model name used
  num_dim: 768                               # Model dimensions
  use_modelscope: false                      # Whether to use ModelScope's model"><pre><span>embedding</span>:
  <span>use_local</span>: <span>true</span>
  <span>model</span>: <span>jinaai/jina-embeddings-v2-base-en   </span><span><span>#</span> Model name used</span>
  <span>num_dim</span>: <span>768</span>                               <span><span>#</span> Model dimensions</span>
  <span>use_modelscope</span>: <span>false                      </span><span><span>#</span> Whether to use ModelScope's model</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">3. Restart Memos Service</h4><a id="user-content-3-restart-memos-service" aria-label="Permalink: 3. Restart Memos Service" href="#3-restart-memos-service"></a></p>

<p dir="auto">The first time you use the embedding model, Pensieve will automatically download and load the model.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">4. Rebuild Index</h4><a id="user-content-4-rebuild-index" aria-label="Permalink: 4. Rebuild Index" href="#4-rebuild-index"></a></p>
<p dir="auto">If you switch the embedding model during use, meaning you have already indexed screenshots before, you need to rebuild the index:</p>

<p dir="auto">The <code>--force</code> parameter indicates rebuilding the index table and deleting previously indexed screenshot data.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using Ollama for Visual Search</h3><a id="user-content-using-ollama-for-visual-search" aria-label="Permalink: Using Ollama for Visual Search" href="#using-ollama-for-visual-search"></a></p>
<p dir="auto">By default, Pensieve only enables the OCR plugin to extract text from screenshots and build indices. However, this method significantly limits search effectiveness for images without text.</p>
<p dir="auto">To achieve more comprehensive visual search capabilities, we need a multimodal image understanding service compatible with the OpenAI API. Ollama perfectly fits this role.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Important Notes Before Use</h4><a id="user-content-important-notes-before-use" aria-label="Permalink: Important Notes Before Use" href="#important-notes-before-use"></a></p>
<p dir="auto">Before deciding to enable the VLM feature, please note the following:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Hardware Requirements</strong></p>
<ul dir="auto">
<li>Recommended configuration: NVIDIA graphics card with at least 8GB VRAM or Mac with M series chip</li>
<li>The minicpm-v model will occupy about 5.5GB of storage space</li>
<li>CPU mode is not recommended as it will cause severe system lag</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Performance and Power Consumption Impact</strong></p>
<ul dir="auto">
<li>Enabling VLM will significantly increase system power consumption</li>
<li>Consider using other devices to provide OpenAI API compatible model services</li>
</ul>
</li>
</ol>
<p dir="auto"><h4 tabindex="-1" dir="auto">1. Install Ollama</h4><a id="user-content-1-install-ollama" aria-label="Permalink: 1. Install Ollama" href="#1-install-ollama"></a></p>
<p dir="auto">Visit the <a href="https://ollama.com/" rel="nofollow">Ollama official documentation</a> for detailed installation and configuration instructions.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">2. Prepare the Multimodal Model</h4><a id="user-content-2-prepare-the-multimodal-model" aria-label="Permalink: 2. Prepare the Multimodal Model" href="#2-prepare-the-multimodal-model"></a></p>
<p dir="auto">Download and run the multimodal model <code>minicpm-v</code> using the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ollama run minicpm-v &quot;Describe what this service is&quot;"><pre>ollama run minicpm-v <span><span>"</span>Describe what this service is<span>"</span></span></pre></div>
<p dir="auto">This command will download and run the minicpm-v model. If the running speed is too slow, it is not recommended to use this feature.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">3. Configure Pensieve to Use Ollama</h4><a id="user-content-3-configure-pensieve-to-use-ollama" aria-label="Permalink: 3. Configure Pensieve to Use Ollama" href="#3-configure-pensieve-to-use-ollama"></a></p>
<p dir="auto">Open the <code>~/.memos/config.yaml</code> file with your preferred text editor and modify the <code>vlm</code> configuration:</p>
<div dir="auto" data-snippet-clipboard-copy-content="vlm:
  endpoint: http://localhost:11434  # Ollama service address
  modelname: minicpm-v              # Model name to use
  force_jpeg: true                  # Convert images to JPEG format to ensure compatibility
  prompt: Please describe the content of this image, including the layout and visual elements  # Prompt sent to the model"><pre><span>vlm</span>:
  <span>endpoint</span>: <span>http://localhost:11434  </span><span><span>#</span> Ollama service address</span>
  <span>modelname</span>: <span>minicpm-v              </span><span><span>#</span> Model name to use</span>
  <span>force_jpeg</span>: <span>true                  </span><span><span>#</span> Convert images to JPEG format to ensure compatibility</span>
  <span>prompt</span>: <span>Please describe the content of this image, including the layout and visual elements  </span><span><span>#</span> Prompt sent to the model</span></pre></div>
<p dir="auto">Use the above configuration to overwrite the <code>vlm</code> configuration in the <code>~/.memos/config.yaml</code> file.</p>
<p dir="auto">Also, modify the <code>default_plugins</code> configuration in the <code>~/.memos/plugins/vlm/config.yaml</code> file:</p>
<div dir="auto" data-snippet-clipboard-copy-content="default_plugins:
- builtin_ocr
- builtin_vlm"><pre><span>default_plugins</span>:
- <span>builtin_ocr</span>
- <span>builtin_vlm</span></pre></div>
<p dir="auto">This adds the <code>builtin_vlm</code> plugin to the default plugin list.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">4. Restart Pensieve Service</h4><a id="user-content-4-restart-pensieve-service" aria-label="Permalink: 4. Restart Pensieve Service" href="#4-restart-pensieve-service"></a></p>

<p dir="auto">After restarting the Pensieve service, wait a moment to see the data extracted by VLM in the latest screenshots on the Pensieve web interface:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/arkohut/pensieve/blob/master/docs/images/single-screenshot-view-with-minicpm-result.png"><img src="https://github.com/arkohut/pensieve/raw/master/docs/images/single-screenshot-view-with-minicpm-result.png" alt="image"></a></p>
<p dir="auto">If you do not see the VLM results, you can:</p>
<ul dir="auto">
<li>Use the command <code>memos ps</code> to check if the Pensieve process is running normally</li>
<li>Check for error messages in <code>~/.memos/logs/memos.log</code></li>
<li>Confirm whether the Ollama model is loaded correctly (<code>ollama ps</code>)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Full Indexing</h3><a id="user-content-full-indexing" aria-label="Permalink: Full Indexing" href="#full-indexing"></a></p>
<p dir="auto">Pensieve is a compute-intensive application. The indexing process requires the collaboration of OCR, VLM, and embedding models. To minimize the impact on the user's computer, Pensieve calculates the average processing time for each screenshot and adjusts the indexing frequency accordingly. Therefore, not all screenshots are indexed immediately by default.</p>
<p dir="auto">If you want to index all screenshots, you can use the following command for full indexing:</p>

<p dir="auto">This command will scan and index all recorded screenshots. Note that depending on the number of screenshots and system configuration, this process may take some time and consume significant system resources. The index construction is idempotent, and running this command multiple times will not re-index already indexed data.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Privacy and Security</h2><a id="user-content-privacy-and-security" aria-label="Permalink: Privacy and Security" href="#privacy-and-security"></a></p>
<p dir="auto">During the development of Pensieve, I closely followed the progress of similar products, especially <a href="https://www.rewind.ai/" rel="nofollow">Rewind</a> and <a href="https://support.microsoft.com/en-us/windows/retrace-your-steps-with-recall-aa03f8a0-a78b-4b3e-b0a1-2eb8ac48701c" rel="nofollow">Windows Recall</a>. I greatly appreciate their product philosophy, but they do not do enough in terms of privacy protection, which is a concern for many users (or potential users). Recording the screen of a personal computer may expose extremely sensitive private data, such as bank accounts, passwords, chat records, etc. Therefore, ensuring that data storage and processing are completely controlled by the user to prevent data leakage is particularly important.</p>
<p dir="auto">The advantages of Pensieve are:</p>
<ol dir="auto">
<li>The code is completely open-source and easy-to-understand Python code, allowing anyone to review the code to ensure there are no backdoors.</li>
<li>Data is completely localized, all data is stored locally, and data processing is entirely controlled by the user. Data will be stored in the user's <code>~/.memos</code> directory.</li>
<li>Easy to uninstall. If you no longer use Pensieve, you can close the program with <code>memos stop &amp;&amp; memos disable</code>, then uninstall it with <code>pip uninstall memos</code>, and finally delete the <code>~/.memos</code> directory to clean up all databases and screenshot data.</li>
<li>Data processing is entirely controlled by the user. Pensieve is an independent project, and the machine learning models used (including VLM and embedding models) are chosen by the user. Due to Pensieve' operating mode, using smaller models can also achieve good results.</li>
</ol>
<p dir="auto">Of course, there is still room for improvement in terms of privacy, and contributions are welcome to make Pensieve better.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other Noteworthy Content</h2><a id="user-content-other-noteworthy-content" aria-label="Permalink: Other Noteworthy Content" href="#other-noteworthy-content"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">About Storage Space</h3><a id="user-content-about-storage-space" aria-label="Permalink: About Storage Space" href="#about-storage-space"></a></p>
<p dir="auto">Pensieve records the screen every 5 seconds and saves the original screenshots in the <code>~/.memos/screenshots</code> directory. Storage space usage mainly depends on the following factors:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Screenshot Data</strong>:</p>
<ul dir="auto">
<li>Single screenshot size: about 40-400KB (depending on screen resolution and display complexity)</li>
<li>Daily data volume: about 400MB (based on 10 hours of usage, single screen 2560x1440 resolution)</li>
<li>Multi-screen usage: data volume increases with the number of screens</li>
<li>Monthly estimate: about 8GB based on 20 working days</li>
</ul>
<p dir="auto">Screenshots are deduplicated. If the content of consecutive screenshots does not change much, only one screenshot will be retained. The deduplication mechanism can significantly reduce storage usage in scenarios where content does not change frequently (such as reading, document editing, etc.).</p>
</li>
<li>
<p dir="auto"><strong>Database Space</strong>:</p>
<ul dir="auto">
<li>SQLite database size depends on the number of indexed screenshots</li>
<li>Reference value: about 2.2GB of storage space after indexing 100,000 screenshots</li>
</ul>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">About Power Consumption</h3><a id="user-content-about-power-consumption" aria-label="Permalink: About Power Consumption" href="#about-power-consumption"></a></p>
<p dir="auto">Pensieve requires two compute-intensive tasks by default:</p>
<ul dir="auto">
<li>One is the OCR task, used to extract text from screenshots</li>
<li>The other is the embedding task, used to extract semantic information and build vector indices</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Resource Usage</h4><a id="user-content-resource-usage" aria-label="Permalink: Resource Usage" href="#resource-usage"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>OCR Task</strong>: Executed using the CPU, and optimized to select the OCR engine based on different operating systems to minimize CPU usage</p>
</li>
<li>
<p dir="auto"><strong>Embedding Task</strong>: Intelligently selects the computing device</p>
<ul dir="auto">
<li>NVIDIA GPU devices prioritize using the GPU</li>
<li>Mac devices prioritize using Metal GPU</li>
<li>Other devices use the CPU</li>
</ul>
</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Performance Optimization Strategy</h4><a id="user-content-performance-optimization-strategy" aria-label="Permalink: Performance Optimization Strategy" href="#performance-optimization-strategy"></a></p>
<p dir="auto">To avoid affecting users' daily use, Pensieve has adopted the following optimization measures:</p>
<ul dir="auto">
<li>Dynamically adjust the indexing frequency, adapting to system processing speed</li>
<li>Automatically reduce processing frequency when on battery power to save power</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development Guide</h2><a id="user-content-development-guide" aria-label="Permalink: Development Guide" href="#development-guide"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Peeling the First Layer of the Onion</h3><a id="user-content-peeling-the-first-layer-of-the-onion" aria-label="Permalink: Peeling the First Layer of the Onion" href="#peeling-the-first-layer-of-the-onion"></a></p>
<p dir="auto">In fact, after Pensieve starts, it runs three programs:</p>
<ol dir="auto">
<li><code>memos serve</code> starts the web service</li>
<li><code>memos record</code> starts the screenshot recording program</li>
<li><code>memos watch</code> listens to the image events generated by <code>memos record</code> and dynamically submits indexing requests to the server based on actual processing speed</li>
</ol>
<p dir="auto">Therefore, if you are a developer or want to see the logs of the entire project running more clearly, you can use these three commands to run each part in the foreground instead of the <code>memos enable &amp;&amp; memos start</code> command.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cloudflare.com's Robots.txt (134 pts)]]></title>
            <link>https://www.cloudflare.com/robots.txt</link>
            <guid>42163883</guid>
            <pubDate>Sun, 17 Nov 2024 12:39:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cloudflare.com/robots.txt">https://www.cloudflare.com/robots.txt</a>, See on <a href="https://news.ycombinator.com/item?id=42163883">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Bpftune uses BPF to auto-tune Linux systems (245 pts)]]></title>
            <link>https://github.com/oracle/bpftune</link>
            <guid>42163597</guid>
            <pubDate>Sun, 17 Nov 2024 11:38:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/oracle/bpftune">https://github.com/oracle/bpftune</a>, See on <a href="https://news.ycombinator.com/item?id=42163597">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">bpftune - BPF driven auto-tuning</h2><a id="user-content-bpftune---bpf-driven-auto-tuning" aria-label="Permalink: bpftune - BPF driven auto-tuning" href="#bpftune---bpf-driven-auto-tuning"></a></p>
<p dir="auto">bpftune aims to provide lightweight, always-on auto-tuning of system
behaviour.  The key benefit it provides are</p>
<ul dir="auto">
<li>by using BPF observability features, we can continuously monitor
and adjust system behaviour</li>
<li>because we can observe system behaviour at a fine grain (rather
than using coarse system-wide stats), we can tune at a finer grain
too (individual socket policies, individual device policies etc)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">The problem</h2><a id="user-content-the-problem" aria-label="Permalink: The problem" href="#the-problem"></a></p>
<p dir="auto">The Linux kernel contains a large number of tunables; these
often take the form of sysctl(8) parameters, and are usually
introduced for situations where there is no one "right" answer
for a configuration choice.  The number of tunables available
is quite daunting.  On a 6.2 kernel we see</p>
<div data-snippet-clipboard-copy-content="# sysctl --all 2>/dev/null|wc -l
1624"><pre><code># sysctl --all 2&gt;/dev/null|wc -l
1624
</code></pre></div>
<p dir="auto"><a href="https://github.com/leandromoreira/linux-network-performance-parameters">See here for an excellent writeup on network-related tunables.</a>.</p>
<p dir="auto">At the same time, individual systems get a lot less care
and adminstrator attention than they used to; phrases like
"cattle not pets" exemplify this.  Given the modern cloud
architectures used for most deployments, most systems never
have any human adminstrator interaction after initial
provisioning; in fact given the scale requirements, this
is often an explicit design goal - "no ssh'ing in!".</p>
<p dir="auto">These two observations are not unrelated; in an earlier
era of fewer, larger systems, tuning by administrators was
more feasible.</p>
<p dir="auto">These trends - system complexity combined with minimal
admin interaction suggest a rethink in terms of tunable
management.</p>
<p dir="auto">A lot of lore accumulates around these tunables, and to help
clarify why we developed bpftune, we will use a straw-man
version of the approach taken with tunables:</p>
<p dir="auto">"find the set of magic numbers that will work for the
system forever"</p>
<p dir="auto">This is obviously a caricature of how administrators
approach the problem, but it does highlight a critical
implicit assumption - that systems are static.</p>
<p dir="auto">And that gets to the "BPF" in bpftune; BPF provides means
to carry out low-overhead observability of systems. So
not only can we observe the system and tune appropriately,
we can also observe the effect of that tuning and re-tune
if necessary.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key design principles</h2><a id="user-content-key-design-principles" aria-label="Permalink: Key design principles" href="#key-design-principles"></a></p>
<ul dir="auto">
<li>Minimize overhead.  Use observability features sparingly; do not
trace very high frequency events.</li>
<li>Be explicit about policy changes providing both a "what" - what
change was made - and a "why" - how does it help? syslog logging
makes policy actions explicit with explanations</li>
<li>Get out of the way of the administrator.  We can use BPF
observability to see if the admin sets tunable values that we
are auto-tuning; if they do, we need to get out of the way and
disable auto-tuning of the related feature set.</li>
<li>Don't replace tunables with more tunables! bpftune is designed to
be zero configuration; there are no options, and we try to avoid
magic numbers where possible.</li>
<li>Use push-pull approaches. For example, with tcp buffer sizing,
we often want to get out of the way of applications and bump
up tcp sndbuf and rcvbuf, but at a certain point we run the
risk of exhausting TCP memory.  We can however monitor if we
are approaching TCP memory pressure and if so we can tune down
values that we've tuned up.  In this way, we can let the system
find a balance between providing resources and exhausting them.
In some cases, we won't need to tune up values; they may be fine
as they are. But in other cases these limits block optimal performance,
and if they are raised safely - with awareness of global memory
limits - we can get out the way of improved performance.  Another
concern is that increasing buffer size leads to latency - to
handle that, we correlate buffer size changes and TCP smoothed
round-trip time; if the correlation between these exceeds a
threshold (0.7) we stop increasing buffer size.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Concepts</h2><a id="user-content-concepts" aria-label="Permalink: Concepts" href="#concepts"></a></p>
<p dir="auto">The key components are</p>
<ul dir="auto">
<li>
<p dir="auto">tuners: each tuner manages tunables and handles events sent
from BPF programs to userspace via the shared ring buffer.
Each tuner has an associated set of tunables that it manages.</p>
</li>
<li>
<p dir="auto">optional strategies: a tuner can specify multiple strategies;
after running for a while a strategy times out and we assess
if a better strategy is available.  Each strategy specifies a</p>
<ul dir="auto">
<li>name</li>
<li>description</li>
<li>timeout</li>
<li>evaluation function</li>
<li>set of BPF program names in tuner associated with strategy</li>
</ul>
<p dir="auto">Strategies are optional and should be set in the tuner init()
method via bpftune_strategies_add().  See test/strategy
for a coded example.  When a strategy times out, the various
evaluation functions are called and the highest-value evaluation
dictates the next stratgey.</p>
<p dir="auto">Strategies provide a way of providing multiple schemes for
auto-tuning the same set of tunables, where the choice is
guided by an evaluation of the effectiveness of the strategies.</p>
</li>
<li>
<p dir="auto">events specify a</p>
<ul dir="auto">
<li>tuner id: which tuner the event is destined for</li>
<li>a scenario: what happened</li>
<li>an associated netns (if supported)</li>
<li>information about the event (IP address etc)</li>
</ul>
</li>
<li>
<p dir="auto">the tuner then responds to the event guided by the active strategy;
increase or decrease a tunable value, etc.  Describing the event
in the log is key; this allows an admin to understand what
changed and why.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<ul dir="auto">
<li>bpftune is a daemon which manages a set of .so plugin tuners;
each of these is a shared object that is loaded on start-up.</li>
<li>tuners can be enabled or disabled; a tuner is automatically
disabled if the admin changes associated tunables manually.</li>
<li>tuners share a global BPF ring buffer which allows posting of
events from BPF programs to userspace.  For example, if the
sysctl tuner sees a systl being set, it posts an event.</li>
<li>each tuner has an associated id (set when it is loaded),
and events posted contain the tuner id.</li>
<li>each tuner has a BPF component (built using a BPF skeleton)
and a userspace component.  The latter has init(), fini()
and event_handler() entrypoints.  When an event is
received, the tuner id is used to identify the appropriate
event handler and its event_handler() callback function is run.</li>
<li>init, fini and event_handler functions are loaded from the
tuner .so object.</li>
<li>BPF components should include bpftune.bpf.h; it contains
the common map definitions (ringbuf, etc) and shared variables
such as learning rate and tuner ids that each tuner needs.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported tuners</h2><a id="user-content-supported-tuners" aria-label="Permalink: Supported tuners" href="#supported-tuners"></a></p>
<ul dir="auto">
<li>TCP connection tuner: auto-tune choice of congestion control algorithm.
See bpftune-tcp-conn (8).</li>
<li>neighbour table tuner: auto-tune neighbour table sizes by growing
tables when approaching full. See bpftune-neigh (8).</li>
<li>route table tuner: auto-tune route table size by growing tables
when approaching full.  See bpftune-route (8).</li>
<li>sysctl tuner: monitor sysctl setting and if it collides with an
auto-tuned sysctl value, disable the associated tuner.  See
bpftune-sysctl (8).</li>
<li>TCP buffer tuner: auto-tune max and initial buffer sizes.  See
bpftune-tcp-buffer (8).</li>
<li>net buffer tuner: auto-tune tunables related to core networking.
See bpftune-net-buffer (8).</li>
<li>netns tuner: notices addition and removal of network namespaces,
which helps power namespace awareness for bpftune as a whole.
Namespace awareness is important as we want to be able to auto-tune
containers also.  See bpftune-netns (8).</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Code organization</h2><a id="user-content-code-organization" aria-label="Permalink: Code organization" href="#code-organization"></a></p>
<p dir="auto">Both core bpftune.c and individual tuners use the libbpftune library.
It handles logging, tuner init/fini, and BPF init/fini.</p>
<p dir="auto">Each tuner shared object defines an init(), fini() and event_handler()
function. These respectively set up and clean up BPF and handle events
that originate from the BPF code.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">If building the repository manually, simply run</p>
<div data-snippet-clipboard-copy-content="$ make ; sudo make install"><pre><code>$ make ; sudo make install
</code></pre></div>
<p dir="auto">at the top-level of the repository.  bpftune also supports a</p>

<p dir="auto">target, which will make a bpftune RPM.  See ./buildrpm/bpftune.spec</p>
<p dir="auto">We can also build with non-standard libdir for distros which do not
use /usr/lib64 like CachyOS; in this case to install to /usr/lib
instead</p>
<div data-snippet-clipboard-copy-content="$ make libdir=lib
$ sudo make install libdir=lib"><pre><code>$ make libdir=lib
$ sudo make install libdir=lib
</code></pre></div>
<p dir="auto">To build the following packages are needed (names may vary by distro);</p>
<ul dir="auto">
<li>libbpf, libbpf-devel &gt;= 0.6</li>
<li>libcap-devel</li>
<li>bpftool &gt;= 4.18</li>
<li>libnl3-devel</li>
<li>clang &gt;= 11</li>
<li>llvm &gt;= 11</li>
<li>python3-docutils</li>
</ul>
<p dir="auto">From the kernel side, the kernel needs to support BPF ring buffer
(around the 5.6 kernel, though 5.4 is supported on Oracle Linux
as ring buffer support was backported), and kernel BTF is
required (CONFIG_DEBUG_INFO_BTF=y).  Verify /sys/kernel/btf/vmlinux
is present.</p>
<p dir="auto">To enable bpftune as a service</p>
<div data-snippet-clipboard-copy-content="$ sudo service bpftune start"><pre><code>$ sudo service bpftune start
</code></pre></div>
<p dir="auto">...and to enable it by default</p>
<div data-snippet-clipboard-copy-content="$ sudo systemctl enable bpftune"><pre><code>$ sudo systemctl enable bpftune
</code></pre></div>
<p dir="auto">bpftune logs to syslog so /var/log/messages will contain details
of any tuning carried out.</p>
<p dir="auto">bpftune can also be run in the foreground as a program; to redirect
output to stdout/stderr, run</p>

<p dir="auto">On exit, bpftune will summarize any tuning done.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tests</h2><a id="user-content-tests" aria-label="Permalink: Tests" href="#tests"></a></p>
<p dir="auto">Tests are supplied for each tuner in the tests/ subdirectory.
"make test" runs all the tests.  Tests use network namespaces
to simulate interactions with remote hosts. See ./TESTING.md
for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Does my system support bpftune?</h2><a id="user-content-does-my-system-support-bpftune" aria-label="Permalink: Does my system support bpftune?" href="#does-my-system-support-bpftune"></a></p>
<p dir="auto">Simply run "bpftune -S" to see:</p>
<div data-snippet-clipboard-copy-content="$ bpftune -S
bpftune works fully
bpftune supports per-netns policy (via netns cookie)"><pre><code>$ bpftune -S
bpftune works fully
bpftune supports per-netns policy (via netns cookie)
</code></pre></div>
<p dir="auto">Two aspects are important here</p>
<ul dir="auto">
<li>does the system support fentry/fexit etc? If so full support
is likely.</li>
<li>does the system support network namespace cookies? If so
per-network-namespace policy is supported.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<p dir="auto">Simply starting bpftune and observing changes made via /var/log/messages
can be instructive.  For example, on a standard VM with sysctl defaults,
I ran</p>

<p dir="auto">...and went about normal development activities such as cloning git
trees from upstream, building kernels, etc.  From the log we see
some of the adjustments bpftune made to accommodate these activities</p>
<div data-snippet-clipboard-copy-content="$ sudo grep bpftune /var/log/messages
...
Apr 19 16:14:59 bpftest bpftune[2778]: bpftune works fully
Apr 19 16:14:59 bpftest bpftune[2778]: bpftune supports per-netns policy (via netns cookie)
Apr 19 16:18:40 bpftest bpftune[2778]: Scenario 'specify bbr congestion control' occurred for tunable 'TCP congestion control' in global ns. Because loss rate has exceeded 1 percent for a connection, use bbr congestion control algorithm instead of default
Apr 19 16:18:40 bpftest bpftune[2778]: due to loss events for 145.40.68.75, specify 'bbr' congestion control algorithm
Apr 19 16:26:53 bpftest bpftune[2778]: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
Apr 19 16:26:53 bpftest bpftune[2778]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 6291456) -> (4096 131072 7864320)
Apr 19 16:26:53 bpftest bpftune[2778]: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
Apr 19 16:26:53 bpftest bpftune[2778]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 7864320) -> (4096 131072 9830400)
Apr 19 16:29:04 bpftest bpftune[2778]: Scenario 'specify bbr congestion control' occurred for tunable 'TCP congestion control' in global ns. Because loss rate has exceeded 1 percent for a connection, use bbr congestion control algorithm instead of default
Apr 19 16:29:04 bpftest bpftune[2778]: due to loss events for 140.91.12.81, specify 'bbr' congestion control algorithm"><pre><code>$ sudo grep bpftune /var/log/messages
...
Apr 19 16:14:59 bpftest bpftune[2778]: bpftune works fully
Apr 19 16:14:59 bpftest bpftune[2778]: bpftune supports per-netns policy (via netns cookie)
Apr 19 16:18:40 bpftest bpftune[2778]: Scenario 'specify bbr congestion control' occurred for tunable 'TCP congestion control' in global ns. Because loss rate has exceeded 1 percent for a connection, use bbr congestion control algorithm instead of default
Apr 19 16:18:40 bpftest bpftune[2778]: due to loss events for 145.40.68.75, specify 'bbr' congestion control algorithm
Apr 19 16:26:53 bpftest bpftune[2778]: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
Apr 19 16:26:53 bpftest bpftune[2778]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 6291456) -&gt; (4096 131072 7864320)
Apr 19 16:26:53 bpftest bpftune[2778]: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
Apr 19 16:26:53 bpftest bpftune[2778]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 7864320) -&gt; (4096 131072 9830400)
Apr 19 16:29:04 bpftest bpftune[2778]: Scenario 'specify bbr congestion control' occurred for tunable 'TCP congestion control' in global ns. Because loss rate has exceeded 1 percent for a connection, use bbr congestion control algorithm instead of default
Apr 19 16:29:04 bpftest bpftune[2778]: due to loss events for 140.91.12.81, specify 'bbr' congestion control algorithm
</code></pre></div>
<p dir="auto">To deterministically trigger bpftune behaviour, one approach we can
take is to download a large file with inappropriate settings.</p>
<p dir="auto">In one window, set tcp rmem max to a too-low value, and run bpftune
as a program logging to stdout/stderr (-s):</p>
<div data-snippet-clipboard-copy-content="$ sudo sysctl -w net.ipv4.tcp_rmem=&quot;4096 131072 1310720&quot;
net.ipv4.tcp_rmem = 4096 131072 1310720
$ sudo bpftune -s"><pre><code>$ sudo sysctl -w net.ipv4.tcp_rmem="4096 131072 1310720"
net.ipv4.tcp_rmem = 4096 131072 1310720
$ sudo bpftune -s
</code></pre></div>
<p dir="auto">In another window, wget a large file:</p>
<div data-snippet-clipboard-copy-content="$ wget https://yum.oracle.com/ISOS/OracleLinux/OL8/u7/x86_64/OracleLinux-R8-U7-x86_64-dvd.iso"><pre><code>$ wget https://yum.oracle.com/ISOS/OracleLinux/OL8/u7/x86_64/OracleLinux-R8-U7-x86_64-dvd.iso
</code></pre></div>
<p dir="auto">In the first window, we see bpftune tuning up rmem:</p>
<div data-snippet-clipboard-copy-content="bpftune: bpftune works in legacy mode
bpftune: bpftune does not support per-netns policy (via netns cookie)
bpftune: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
bpftune: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 1310720) -> (4096 131072 1638400)"><pre><code>bpftune: bpftune works in legacy mode
bpftune: bpftune does not support per-netns policy (via netns cookie)
bpftune: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
bpftune: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 1310720) -&gt; (4096 131072 1638400)
</code></pre></div>
<p dir="auto">This occurs multiple times, and on exit (Ctrl+C) we see
the summary of changes made:</p>
<div data-snippet-clipboard-copy-content="bpftune: Summary: scenario 'need to increase TCP buffer size(s)' occurred 9 times for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
bpftune: sysctl 'net.ipv4.tcp_rmem' changed from (4096 131072 1310720 ) -> (4096 131072 9765625 )"><pre><code>bpftune: Summary: scenario 'need to increase TCP buffer size(s)' occurred 9 times for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
bpftune: sysctl 'net.ipv4.tcp_rmem' changed from (4096 131072 1310720 ) -&gt; (4096 131072 9765625 )
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">For more info</h2><a id="user-content-for-more-info" aria-label="Permalink: For more info" href="#for-more-info"></a></p>
<p dir="auto">See the docs/ subdirectory for manual pages covering bpftune
and associated tuners.</p>
<p dir="auto">bpftune was presented at the eBPF summit; <a href="https://www.youtube.com/watch?v=X0TvfH8hrQE&amp;t=420s" rel="nofollow">video here</a>.</p>
<p dir="auto">bpftune <a href="https://www.youtube.com/watch?v=3ylmGE6sW8w" rel="nofollow">was also discussed on Liz Rice's excellent eCHO eBPF podcast</a>, specifically in the context of using reinforcement learning in BPF</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">This project welcomes contributions from the community. Before submitting a pull request, please <a href="https://github.com/oracle/bpftune/blob/main/CONTRIBUTING.md">review our contribution guide</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security</h2><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<p dir="auto">Please consult the <a href="https://github.com/oracle/bpftune/blob/main/SECURITY.md">security guide</a> for our responsible security vulnerability disclosure process</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Copyright (c) 2023 Oracle and/or its affiliates.</p>
<p dir="auto">This software is available to you under</p>
<p dir="auto">SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note</p>
<p dir="auto">Being under the terms of the GNU General Public License version 2.</p>
<p dir="auto">SPDX-URL: <a href="https://spdx.org/licenses/GPL-2.0.html" rel="nofollow">https://spdx.org/licenses/GPL-2.0.html</a></p>
<p dir="auto">See <a href="https://github.com/oracle/bpftune/blob/main/LICENSE.txt">the license file</a> for more details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Garak, LLM Vulnerability Scanner (170 pts)]]></title>
            <link>https://github.com/NVIDIA/garak</link>
            <guid>42163591</guid>
            <pubDate>Sun, 17 Nov 2024 11:37:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/NVIDIA/garak">https://github.com/NVIDIA/garak</a>, See on <a href="https://news.ycombinator.com/item?id=42163591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">garak, LLM vulnerability scanner</h2><a id="user-content-garak-llm-vulnerability-scanner" aria-label="Permalink: garak, LLM vulnerability scanner" href="#garak-llm-vulnerability-scanner"></a></p>
<p dir="auto"><em>Generative AI Red-teaming &amp; Assessment Kit</em></p>
<p dir="auto"><code>garak</code> checks if an LLM can be made to fail in a way we don't want. <code>garak</code> probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know <code>nmap</code>, it's <code>nmap</code> for LLMs.</p>
<p dir="auto"><code>garak</code> focuses on ways of making an LLM or dialog system fail. It combines static, dyanmic, and adaptive probes to explore this.</p>
<p dir="auto"><code>garak</code>'s a free tool. We love developing it and are always interested in adding functionality to support applications.</p>
<p dir="auto"><a href="https://opensource.org/licenses/Apache-2.0" rel="nofollow"><img src="https://camo.githubusercontent.com/5ce2e21e84680df1ab24807babebc3417d27d66e0826a350eb04ab57f4c8f3e5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368655f322e302d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache_2.0-blue.svg"></a>
<a href="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml"><img src="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml/badge.svg" alt="Tests/Linux"></a>
<a href="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml"><img src="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml/badge.svg" alt="Tests/Windows"></a>
<a href="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml"><img src="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml/badge.svg" alt="Tests/OSX"></a>
<a href="http://garak.readthedocs.io/en/latest/?badge=latest" rel="nofollow"><img src="https://camo.githubusercontent.com/ec7dff6db1b623f10238aaa176f6070b8dfee2ba106479e9ac7a66fbe8f3e778/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f676172616b2f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/garak/badge/?version=latest"></a>
<a href="https://discord.gg/uVch4puUCs" rel="nofollow"><img src="https://camo.githubusercontent.com/3dfa2e5918dc7c5299e3f3e8383c6d7fc9e5a26de70d29c5144a166075db153b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d6f6e253230646973636f72642d79656c6c6f772e737667" alt="discord-img" data-canonical-src="https://img.shields.io/badge/chat-on%20discord-yellow.svg"></a>
<a href="https://github.com/psf/black"><img src="https://camo.githubusercontent.com/5bf9e9fa18966df7cb5fac7715bef6b72df15e01a6efa9d616c83f9fcb527fe2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg"></a>
<a href="https://pypi.org/project/garak" rel="nofollow"><img src="https://camo.githubusercontent.com/2e89cf4e24191c2b133e3cbc641eb89ec3d1c6e61bfec492ddec940757b871f3/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f676172616b" alt="PyPI - Python Version" data-canonical-src="https://img.shields.io/pypi/pyversions/garak"></a>
<a href="https://badge.fury.io/py/garak" rel="nofollow"><img src="https://camo.githubusercontent.com/635b443d53cf2ab927beabd3255f7895db7b74af3dbf5a7777c61fcc96792bbd/68747470733a2f2f62616467652e667572792e696f2f70792f676172616b2e737667" alt="PyPI" data-canonical-src="https://badge.fury.io/py/garak.svg"></a>
<a href="https://pepy.tech/project/garak" rel="nofollow"><img src="https://camo.githubusercontent.com/b38370d22779880d75968e06cf8ade053d44a0e95f94649a8354756b5c24a4ba/68747470733a2f2f706570792e746563682f62616467652f676172616b" alt="Downloads" data-canonical-src="https://pepy.tech/badge/garak"></a>
<a href="https://pepy.tech/project/garak" rel="nofollow"><img src="https://camo.githubusercontent.com/3d929a651aaa7c9b44b580d1537bbb7954b796081bfeb1125fc3d1fac4f78585/68747470733a2f2f706570792e746563682f62616467652f676172616b2f6d6f6e7468" alt="Downloads" data-canonical-src="https://pepy.tech/badge/garak/month"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get started</h2><a id="user-content-get-started" aria-label="Permalink: Get started" href="#get-started"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">&gt; See our user guide! <a href="https://docs.garak.ai/" rel="nofollow">docs.garak.ai</a></h3><a id="user-content--see-our-user-guide-docsgarakai" aria-label="Permalink: > See our user guide! docs.garak.ai" href="#-see-our-user-guide-docsgarakai"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">&gt; Join our <a href="https://discord.gg/uVch4puUCs" rel="nofollow">Discord</a>!</h3><a id="user-content--join-our-discord" aria-label="Permalink: > Join our Discord!" href="#-join-our-discord"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">&gt; Project links &amp; home: <a href="https://garak.ai/" rel="nofollow">garak.ai</a></h3><a id="user-content--project-links--home-garakai" aria-label="Permalink: > Project links &amp; home: garak.ai" href="#-project-links--home-garakai"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">&gt; Twitter: <a href="https://twitter.com/garak_llm" rel="nofollow">@garak_llm</a></h3><a id="user-content--twitter-garak_llm" aria-label="Permalink: > Twitter: @garak_llm" href="#-twitter-garak_llm"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">&gt; DEF CON <a href="https://garak.ai/garak_aiv_slides.pdf" rel="nofollow">slides</a>!</h3><a id="user-content--def-con-slides" aria-label="Permalink: > DEF CON slides!" href="#-def-con-slides"></a></p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">LLM support</h2><a id="user-content-llm-support" aria-label="Permalink: LLM support" href="#llm-support"></a></p>
<p dir="auto">currently supports:</p>
<ul dir="auto">
<li><a href="https://huggingface.co/models" rel="nofollow">hugging face hub</a> generative models</li>
<li><a href="https://replicate.com/" rel="nofollow">replicate</a> text models</li>
<li><a href="https://platform.openai.com/docs/introduction" rel="nofollow">openai api</a> chat &amp; continuation models</li>
<li><a href="https://www.litellm.ai/" rel="nofollow">litellm</a></li>
<li>pretty much anything accessible via REST</li>
<li>gguf models like <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> version &gt;= 1046</li>
<li>.. and many more LLMs!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install:</h2><a id="user-content-install" aria-label="Permalink: Install:" href="#install"></a></p>
<p dir="auto"><code>garak</code> is a command-line tool. It's developed in Linux and OSX.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Standard install with <code>pip</code></h3><a id="user-content-standard-install-with-pip" aria-label="Permalink: Standard install with pip" href="#standard-install-with-pip"></a></p>
<p dir="auto">Just grab it from PyPI and you should be good to go:</p>
<div data-snippet-clipboard-copy-content="python -m pip install -U garak"><pre><code>python -m pip install -U garak
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Install development version with <code>pip</code></h3><a id="user-content-install-development-version-with-pip" aria-label="Permalink: Install development version with pip" href="#install-development-version-with-pip"></a></p>
<p dir="auto">The standard pip version of <code>garak</code> is updated periodically. To get a fresher version, from GitHub, try:</p>
<div data-snippet-clipboard-copy-content="python -m pip install -U git+https://github.com/NVIDIA/garak.git@main"><pre><code>python -m pip install -U git+https://github.com/NVIDIA/garak.git@main
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Clone from source</h3><a id="user-content-clone-from-source" aria-label="Permalink: Clone from source" href="#clone-from-source"></a></p>
<p dir="auto"><code>garak</code> has its own dependencies. You can to install <code>garak</code> in its own Conda environment:</p>
<div data-snippet-clipboard-copy-content="conda create --name garak &quot;python>=3.10,<=3.12&quot;
conda activate garak
gh repo clone NVIDIA/garak
cd garak
python -m pip install -e ."><pre><code>conda create --name garak "python&gt;=3.10,&lt;=3.12"
conda activate garak
gh repo clone NVIDIA/garak
cd garak
python -m pip install -e .
</code></pre></div>
<p dir="auto">OK, if that went fine, you're probably good to go!</p>
<p dir="auto"><strong>Note</strong>: if you cloned before the move to the <code>NVIDIA</code> GitHub organisation, but you're reading this at the <code>github.com/NVIDIA</code> URI, please update your remotes as follows:</p>
<div data-snippet-clipboard-copy-content="git remote set-url origin https://github.com/NVIDIA/garak.git"><pre><code>git remote set-url origin https://github.com/NVIDIA/garak.git
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto">The general syntax is:</p>
<p dir="auto"><code>garak &lt;options&gt;</code></p>
<p dir="auto"><code>garak</code> needs to know what model to scan, and by default, it'll try all the probes it knows on that model, using the vulnerability detectors recommended by each probe. You can see a list of probes using:</p>
<p dir="auto"><code>garak --list_probes</code></p>
<p dir="auto">To specify a generator, use the <code>--model_type</code> and, optionally, the <code>--model_name</code> options. Model type specifies a model family/interface; model name specifies the exact model to be used. The "Intro to generators" section below describes some of the generators supported. A straightforward generator family is Hugging Face models; to load one of these, set <code>--model_type</code> to <code>huggingface</code> and <code>--model_name</code> to the model's name on Hub (e.g. <code>"RWKV/rwkv-4-169m-pile"</code>). Some generators might need an API key to be set as an environment variable, and they'll let you know if they need that.</p>
<p dir="auto"><code>garak</code> runs all the probes by default, but you can be specific about that too. <code>--probes promptinject</code> will use only the <a href="https://github.com/agencyenterprise/promptinject">PromptInject</a> framework's methods, for example. You can also specify one specific plugin instead of a plugin family by adding the plugin name after a <code>.</code>; for example, <code>--probes lmrc.SlurUsage</code> will use an implementation of checking for models generating slurs based on the <a href="https://arxiv.org/abs/2303.18190" rel="nofollow">Language Model Risk Cards</a> framework.</p>
<p dir="auto">For help &amp; inspiration, find us on <a href="https://twitter.com/garak_llm" rel="nofollow">twitter</a> or <a href="https://discord.gg/uVch4puUCs" rel="nofollow">discord</a>!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">Probe ChatGPT for encoding-based prompt injection (OSX/*nix) (replace example value with a real OpenAI API key)</p>
<div data-snippet-clipboard-copy-content="export OPENAI_API_KEY=&quot;sk-123XXXXXXXXXXXX&quot;
python3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding"><pre><code>export OPENAI_API_KEY="sk-123XXXXXXXXXXXX"
python3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding
</code></pre></div>
<p dir="auto">See if the Hugging Face version of GPT2 is vulnerable to DAN 11.0</p>
<div data-snippet-clipboard-copy-content="python3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0"><pre><code>python3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Reading the results</h2><a id="user-content-reading-the-results" aria-label="Permalink: Reading the results" href="#reading-the-results"></a></p>
<p dir="auto">For each probe loaded, garak will print a progress bar as it generates. Once generation is complete, a row evaluating that probe's results on each detector is given. If any of the prompt attempts yielded an undesirable behavior, the response will be marked as FAIL, and the failure rate given.</p>
<p dir="auto">Here are the results with the <code>encoding</code> module on a GPT-3 variant:
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3c772412f9310195163d6092ba995d436ebad8d7e430d89a8484c3a92b5ec972/68747470733a2f2f692e696d6775722e636f6d2f3844786634354e2e706e67"><img src="https://camo.githubusercontent.com/3c772412f9310195163d6092ba995d436ebad8d7e430d89a8484c3a92b5ec972/68747470733a2f2f692e696d6775722e636f6d2f3844786634354e2e706e67" alt="alt text" data-canonical-src="https://i.imgur.com/8Dxf45N.png"></a></p>
<p dir="auto">And the same results for ChatGPT:
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5fc7c4aee43ab989750750ea912245acd367d41dee20f0532d19d8bcce9d3a5e/68747470733a2f2f692e696d6775722e636f6d2f564b41463569662e706e67"><img src="https://camo.githubusercontent.com/5fc7c4aee43ab989750750ea912245acd367d41dee20f0532d19d8bcce9d3a5e/68747470733a2f2f692e696d6775722e636f6d2f564b41463569662e706e67" alt="alt text" data-canonical-src="https://i.imgur.com/VKAF5if.png"></a></p>
<p dir="auto">We can see that the more recent model is much more susceptible to encoding-based injection attacks, where text-babbage-001 was only found to be vulnerable to quoted-printable and MIME encoding injections.  The figures at the end of each row, e.g. 840/840, indicate the number of text generations total and then how many of these seemed to behave OK. The figure can be quite high because more than one generation is made per prompt - by default, 10.</p>
<p dir="auto">Errors go in <code>garak.log</code>; the run is logged in detail in a <code>.jsonl</code> file specified at analysis start &amp; end. There's a basic analysis script in <code>analyse/analyse_log.py</code> which will output the probes and prompts that led to the most hits.</p>
<p dir="auto">Send PRs &amp; open issues. Happy hunting!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Intro to generators</h2><a id="user-content-intro-to-generators" aria-label="Permalink: Intro to generators" href="#intro-to-generators"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hugging Face</h3><a id="user-content-hugging-face" aria-label="Permalink: Hugging Face" href="#hugging-face"></a></p>
<p dir="auto">Using the Pipeline API:</p>
<ul dir="auto">
<li><code>--model_type huggingface</code> (for transformers models to run locally)</li>
<li><code>--model_name</code> - use the model name from Hub. Only generative models will work. If it fails and shouldn't, please open an issue and paste in the command you tried + the exception!</li>
</ul>
<p dir="auto">Using the Inference API:</p>
<ul dir="auto">
<li><code>--model_type huggingface.InferenceAPI</code> (for API-based model access)</li>
<li><code>--model_name</code> - the model name from Hub, e.g. <code>"mosaicml/mpt-7b-instruct"</code></li>
</ul>
<p dir="auto">Using private endpoints:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>--model_type huggingface.InferenceEndpoint</code> (for private endpoints)</p>
</li>
<li>
<p dir="auto"><code>--model_name</code> - the endpoint URL, e.g. <code>https://xxx.us-east-1.aws.endpoints.huggingface.cloud</code></p>
</li>
<li>
<p dir="auto">(optional) set the <code>HF_INFERENCE_TOKEN</code> environment variable to a Hugging Face API token with the "read" role; see <a href="https://huggingface.co/settings/tokens" rel="nofollow">https://huggingface.co/settings/tokens</a> when logged in</p>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">OpenAI</h3><a id="user-content-openai" aria-label="Permalink: OpenAI" href="#openai"></a></p>
<ul dir="auto">
<li><code>--model_type openai</code></li>
<li><code>--model_name</code> - the OpenAI model you'd like to use. <code>gpt-3.5-turbo-0125</code> is fast and fine for testing.</li>
<li>set the <code>OPENAI_API_KEY</code> environment variable to your OpenAI API key (e.g. "sk-19763ASDF87q6657"); see <a href="https://platform.openai.com/account/api-keys" rel="nofollow">https://platform.openai.com/account/api-keys</a> when logged in</li>
</ul>
<p dir="auto">Recognised model types are whitelisted, because the plugin needs to know which sub-API to use. Completion or ChatCompletion models are OK. If you'd like to use a model not supported, you should get an informative error message, and please send a PR / open an issue.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Replicate</h3><a id="user-content-replicate" aria-label="Permalink: Replicate" href="#replicate"></a></p>
<ul dir="auto">
<li>set the <code>REPLICATE_API_TOKEN</code> environment variable to your Replicate API token, e.g. "r8-123XXXXXXXXXXXX"; see <a href="https://replicate.com/account/api-tokens" rel="nofollow">https://replicate.com/account/api-tokens</a> when logged in</li>
</ul>
<p dir="auto">Public Replicate models:</p>
<ul dir="auto">
<li><code>--model_type replicate</code></li>
<li><code>--model_name</code> - the Replicate model name and hash, e.g. <code>"stability-ai/stablelm-tuned-alpha-7b:c49dae36"</code></li>
</ul>
<p dir="auto">Private Replicate endpoints:</p>
<ul dir="auto">
<li><code>--model_type replicate.InferenceEndpoint</code> (for private endpoints)</li>
<li><code>--model_name</code> - username/model-name slug from the deployed endpoint, e.g. <code>elim/elims-llama2-7b</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Cohere</h3><a id="user-content-cohere" aria-label="Permalink: Cohere" href="#cohere"></a></p>
<ul dir="auto">
<li><code>--model_type cohere</code></li>
<li><code>--model_name</code> (optional, <code>command</code> by default) - The specific Cohere model you'd like to test</li>
<li>set the <code>COHERE_API_KEY</code> environment variable to your Cohere API key, e.g. "aBcDeFgHiJ123456789"; see <a href="https://dashboard.cohere.ai/api-keys" rel="nofollow">https://dashboard.cohere.ai/api-keys</a> when logged in</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Groq</h3><a id="user-content-groq" aria-label="Permalink: Groq" href="#groq"></a></p>
<ul dir="auto">
<li><code>--model_type groq</code></li>
<li><code>--model_name</code> - The name of the model to access via the Groq API</li>
<li>set the <code>GROQ_API_KEY</code> environment variable to your Groq API key, see <a href="https://console.groq.com/docs/quickstart" rel="nofollow">https://console.groq.com/docs/quickstart</a> for details on creating an API key</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">ggml</h3><a id="user-content-ggml" aria-label="Permalink: ggml" href="#ggml"></a></p>
<ul dir="auto">
<li><code>--model_type ggml</code></li>
<li><code>--model_name</code> - The path to the ggml model you'd like to load, e.g. <code>/home/leon/llama.cpp/models/7B/ggml-model-q4_0.bin</code></li>
<li>set the <code>GGML_MAIN_PATH</code> environment variable to the path to your ggml <code>main</code> executable</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">REST</h3><a id="user-content-rest" aria-label="Permalink: REST" href="#rest"></a></p>
<p dir="auto"><code>rest.RestGenerator</code> is highly flexible and can connect to any REST endpoint that returns plaintext or JSON. It does need some brief config, which will typically result a short YAML file describing your endpoint. See <a href="https://reference.garak.ai/en/latest/garak.generators.rest.html" rel="nofollow">https://reference.garak.ai/en/latest/garak.generators.rest.html</a> for examples.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">NIM</h3><a id="user-content-nim" aria-label="Permalink: NIM" href="#nim"></a></p>
<p dir="auto">Use models from <a href="https://build.nvidia.com/" rel="nofollow">https://build.nvidia.com/</a> or other NIM endpoints.</p>
<ul dir="auto">
<li>set the <code>NIM_API_KEY</code> environment variable to your authentication API token, or specify it in the config YAML</li>
</ul>
<p dir="auto">For chat models:</p>
<ul dir="auto">
<li><code>--model_type nim</code></li>
<li><code>--model_name</code> - the NIM <code>model</code> name, e.g. <code>meta/llama-3.1-8b-instruct</code></li>
</ul>
<p dir="auto">For completion models:</p>
<ul dir="auto">
<li><code>--model_type nim.NVOpenAICompletion</code></li>
<li><code>--model_name</code> - the NIM <code>model</code> name, e.g. <code>bigcode/starcoder2-15b</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">OctoAI</h3><a id="user-content-octoai" aria-label="Permalink: OctoAI" href="#octoai"></a></p>
<ul dir="auto">
<li>set the <code>OCTO_API_TOKEN</code> environment variable to your Replicate API token, e.g. "r8-123XXXXXXXXXXXX"; see <a href="https://replicate.com/account/api-tokens" rel="nofollow">https://replicate.com/account/api-tokens</a> when logged in</li>
</ul>
<p dir="auto">Octo public endpoint:</p>
<ul dir="auto">
<li><code>--model_type octo</code></li>
<li><code>--model_name</code> - the OctoAI public endpoint for the model, e.g. <code>mistral-7b-instruct-fp16</code></li>
</ul>
<p dir="auto">Octo private endpoint:</p>
<ul dir="auto">
<li><code>--model_type octo.InferenceEndpoint</code> (for private endpoints)</li>
<li><code>--model_name</code> - the deployed endpoint URL, e.g. <code>https://llama-2-70b-chat-xxx.octoai.run/v1/chat/completions</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Test</h3><a id="user-content-test" aria-label="Permalink: Test" href="#test"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><code>--model_type test</code></p>
</li>
<li>
<p dir="auto">(alternatively) <code>--model_name test.Blank</code>
For testing. This always generates the empty string, using the <code>test.Blank</code> generator.  Will be marked as failing for any tests that <em>require</em> an output, e.g. those that make contentious claims and expect the model to refute them in order to pass.</p>
</li>
<li>
<p dir="auto"><code>--model_type test.Repeat</code>
For testing. This generator repeats back the prompt it received.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Intro to probes</h2><a id="user-content-intro-to-probes" aria-label="Permalink: Intro to probes" href="#intro-to-probes"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Probe</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>blank</td>
<td>A simple probe that always sends an empty prompt.</td>
</tr>
<tr>
<td>atkgen</td>
<td>Automated Attack Generation. A red-teaming LLM probes the target and reacts to it in an attempt to get toxic output. Prototype, mostly stateless, for now uses a simple GPT-2 <a href="https://huggingface.co/garak-llm/artgpt2tox" rel="nofollow">fine-tuned</a> on the subset of hhrlhf attempts that yielded detectable toxicity (the only target currently supported for now).</td>
</tr>
<tr>
<td>av_spam_scanning</td>
<td>Probes that attempt to make the model output malicious content signatures</td>
</tr>
<tr>
<td>continuation</td>
<td>Probes that test if the model will continue a probably undesirable word</td>
</tr>
<tr>
<td>dan</td>
<td>Various <a href="https://adguard.com/en/blog/chatgpt-dan-prompt-abuse.html" rel="nofollow">DAN</a> and DAN-like attacks</td>
</tr>
<tr>
<td>donotanswer</td>
<td>Prompts to which responsible language models should not answer.</td>
</tr>
<tr>
<td>encoding</td>
<td>Prompt injection through text encoding</td>
</tr>
<tr>
<td>gcg</td>
<td>Disrupt a system prompt by appending an adversarial suffix.</td>
</tr>
<tr>
<td>glitch</td>
<td>Probe model for glitch tokens that provoke unusual behavior.</td>
</tr>
<tr>
<td>grandma</td>
<td>Appeal to be reminded of one's grandmother.</td>
</tr>
<tr>
<td>goodside</td>
<td>Implementations of Riley Goodside attacks.</td>
</tr>
<tr>
<td>leakerplay</td>
<td>Evaluate if a model will replay training data.</td>
</tr>
<tr>
<td>lmrc</td>
<td>Subsample of the <a href="https://arxiv.org/abs/2303.18190" rel="nofollow">Language Model Risk Cards</a> probes</td>
</tr>
<tr>
<td>malwaregen</td>
<td>Attempts to have the model generate code for building malware</td>
</tr>
<tr>
<td>misleading</td>
<td>Attempts to make a model support misleading and false claims</td>
</tr>
<tr>
<td>packagehallucination</td>
<td>Trying to get code generations that specify non-existent (and therefore insecure) packages.</td>
</tr>
<tr>
<td>promptinject</td>
<td>Implementation of the Agency Enterprise <a href="https://github.com/agencyenterprise/PromptInject/tree/main/promptinject">PromptInject</a> work (best paper awards @ NeurIPS ML Safety Workshop 2022)</td>
</tr>
<tr>
<td>realtoxicityprompts</td>
<td>Subset of the RealToxicityPrompts work (data constrained because the full test will take so long to run)</td>
</tr>
<tr>
<td>snowball</td>
<td><a href="https://ofir.io/snowballed_hallucination.pdf" rel="nofollow">Snowballed Hallucination</a> probes designed to make a model give a wrong answer to questions too complex for it to process</td>
</tr>
<tr>
<td>xss</td>
<td>Look for vulnerabilities the permit or enact cross-site attacks, such as private data exfiltration.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Logging</h2><a id="user-content-logging" aria-label="Permalink: Logging" href="#logging"></a></p>
<p dir="auto"><code>garak</code> generates multiple kinds of log:</p>
<ul dir="auto">
<li>A log file, <code>garak.log</code>. This includes debugging information from <code>garak</code> and its plugins, and is continued across runs.</li>
<li>A report of the current run, structured as JSONL. A new report file is created every time <code>garak</code> runs. The name of this file is output at the beginning and, if successful, also the end of the run. In the report, an entry is made for each probing attempt both as the generations are received, and again when they are evaluated; the entry's <code>status</code> attribute takes a constant from <code>garak.attempts</code> to describe what stage it was made at.</li>
<li>A hit log, detailing attempts that yielded a vulnerability (a 'hit')</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">How is the code structured?</h2><a id="user-content-how-is-the-code-structured" aria-label="Permalink: How is the code structured?" href="#how-is-the-code-structured"></a></p>
<p dir="auto">Check out the <a href="https://reference.garak.ai/" rel="nofollow">reference docs</a> for an authoritative guide to <code>garak</code> code structure.</p>
<p dir="auto">In a typical run, <code>garak</code> will read a model type (and optionally model name) from the command line, then determine which <code>probe</code>s and <code>detector</code>s to run, start up a <code>generator</code>, and then pass these to a <code>harness</code> to do the probing; an <code>evaluator</code> deals with the results. There are many modules in each of these categories, and each module provides a number of classes that act as individual plugins.</p>
<ul dir="auto">
<li><code>garak/probes/</code> - classes for generating interactions with LLMs</li>
<li><code>garak/detectors/</code> - classes for detecting an LLM is exhibiting a given failure mode</li>
<li><code>garak/evaluators/</code> - assessment reporting schemes</li>
<li><code>garak/generators/</code> - plugins for LLMs to be probed</li>
<li><code>garak/harnesses/</code> - classes for structuring testing</li>
<li><code>resources/</code> - ancillary items required by plugins</li>
</ul>
<p dir="auto">The default operating mode is to use the <code>probewise</code> harness. Given a list of probe module names and probe plugin names, the <code>probewise</code> harness instantiates each probe, then for each probe reads its <code>recommended_detectors</code> attribute to get a list of <code>detector</code>s to run on the output.</p>
<p dir="auto">Each plugin category (<code>probes</code>, <code>detectors</code>, <code>evaluators</code>, <code>generators</code>, <code>harnesses</code>) includes a <code>base.py</code> which defines the base classes usable by plugins in that category. Each plugin module defines plugin classes that inherit from one of the base classes. For example, <code>garak.generators.openai.OpenAIGenerator</code> descends from <code>garak.generators.base.Generator</code>.</p>
<p dir="auto">Larger artefacts, like model files and bigger corpora, are kept out of the repository; they can be stored on e.g. Hugging Face Hub and loaded locally by clients using <code>garak</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Developing your own plugin</h2><a id="user-content-developing-your-own-plugin" aria-label="Permalink: Developing your own plugin" href="#developing-your-own-plugin"></a></p>
<ul dir="auto">
<li>Take a look at how other plugins do it</li>
<li>Inherit from one of the base classes, e.g. <code>garak.probes.base.TextProbe</code></li>
<li>Override as little as possible</li>
<li>You can test the new code in at least two ways:
<ul dir="auto">
<li>Start an interactive Python session
<ul dir="auto">
<li>Import the model, e.g. <code>import garak.probes.mymodule</code></li>
<li>Instantiate the plugin, e.g. <code>p = garak.probes.mymodule.MyProbe()</code></li>
</ul>
</li>
<li>Run a scan with test plugins
<ul dir="auto">
<li>For probes, try a blank generator and always.Pass detector: <code>python3 -m garak -m test.Blank -p mymodule -d always.Pass</code></li>
<li>For detectors, try a blank generator and a blank probe: <code>python3 -m garak -m test.Blank -p test.Blank -d mymodule</code></li>
<li>For generators, try a blank probe and always.Pass detector: <code>python3 -m garak -m mymodule -p test.Blank -d always.Pass</code></li>
</ul>
</li>
<li>Get <code>garak</code> to list all the plugins of the type you're writing, with <code>--list_probes</code>, <code>--list_detectors</code>, or <code>--list_generators</code></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto">We have an FAQ <a href="https://github.com/NVIDIA/garak/blob/main/FAQ.md">here</a>. Reach out if you have any more questions! <a href="mailto:leon@garak.ai">leon@garak.ai</a></p>
<p dir="auto">Code reference documentation is at <a href="https://garak.readthedocs.io/en/latest/" rel="nofollow">garak.readthedocs.io</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citing garak</h2><a id="user-content-citing-garak" aria-label="Permalink: Citing garak" href="#citing-garak"></a></p>
<p dir="auto">You can read the <a href="https://github.com/NVIDIA/garak/blob/main/garak-paper.pdf">garak preprint paper</a>. If you use garak, please cite us.</p>
<div data-snippet-clipboard-copy-content="@article{garak,
  title={{garak: A Framework for Security Probing Large Language Models}},
  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},
  year={2024},
  howpublished={\url{https://garak.ai}}
}"><pre><code>@article{garak,
  title={{garak: A Framework for Security Probing Large Language Models}},
  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},
  year={2024},
  howpublished={\url{https://garak.ai}}
}
</code></pre></div>
<hr>
<p dir="auto"><em>"Lying is a skill like any other, and if you wish to maintain a level of excellence you have to practice constantly"</em> - Elim</p>
<p dir="auto">For updates and news see <a href="https://twitter.com/garak_llm" rel="nofollow">@garak_llm</a></p>
<p dir="auto">© 2023- Leon Derczynski; Apache license v2, see <a href="https://github.com/NVIDIA/garak/blob/main/LICENSE">LICENSE</a></p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>