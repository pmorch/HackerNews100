<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 09 Aug 2025 14:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Partially Matching Zig Enums (102 pts)]]></title>
            <link>https://matklad.github.io/2025/08/08/partially-matching-zig-enums.html</link>
            <guid>44845017</guid>
            <pubDate>Sat, 09 Aug 2025 08:50:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matklad.github.io/2025/08/08/partially-matching-zig-enums.html">https://matklad.github.io/2025/08/08/partially-matching-zig-enums.html</a>, See on <a href="https://news.ycombinator.com/item?id=44845017">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <article>
        <h2>
          Partially Matching Zig Enums <time datetime="2025-08-08">Aug 8, 2025</time>
        </h2>
        <p>
          A short post about a neat little Zig idiom. Consider your average {sum
          type, variant, tagged union, enum, alt}:
        </p>

        <figure>
          <pre><code><span><span>enum</span> <span>U</span> {</span>
<span>    <span>A</span>(<span>i32</span>),</span>
<span>    <span>B</span>(<span>i32</span>),</span>
<span>    C,</span>
<span>}</span></code></pre>
        </figure>
        <p>Usually, you handle it like this:</p>

        <figure>
          <pre><code><span><span>match</span> u {</span>
<span>    U::<span>A</span>(_) =&gt; <span>handle_a</span>(),</span>
<span>    U::<span>B</span>(_) =&gt; <span>handle_b</span>(),</span>
<span>    U::C =&gt; <span>handle_c</span>(),</span>
<span>}</span></code></pre>
        </figure>
        <p>
          But once in a while, there’s common handling code you want to run for
          several variants. The most straightforward way is to duplicate:
        </p>

        <figure>
          <pre><code><span><span>match</span> u {</span>
<span>    U::<span>A</span>(_) =&gt; {</span>
<span>        <span>handle_ab</span>();</span>
<span>        <span>handle_a</span>();</span>
<span>    }</span>
<span>    U::<span>B</span>(_) =&gt; {</span>
<span>        <span>handle_ab</span>();</span>
<span>        <span>handle_b</span>();</span>
<span>    }</span>
<span>    U::C =&gt; <span>handle_c</span>(),</span>
<span>}</span></code></pre>
        </figure>
        <p>
          But this gets awkward if common parts are not easily extractable into
          function. The “proper” way to do this is to refactor the enum:
        </p>

        <figure>
          <pre><code><span><span>enum</span> <span>U</span> {</span>
<span>    <span>AB</span>(AB),</span>
<span>    C</span>
<span>}</span>
<span></span>
<span><span>enum</span> <span>AB</span> {</span>
<span>    <span>A</span>(<span>i32</span>),</span>
<span>    <span>B</span>(<span>i32</span>),</span>
<span>}</span></code></pre>
        </figure>
        <p>
          This gets <em>very</em> awkward if there’s one hundred usages of <code>U</code>, 95 of them look better with flat structure, one needs
          common code for ab case, and the four remaining need common code for
          ac.
        </p>
        <p>
          The universal recipe for solving the AB problem relies on a runtime
          panic:
        </p>

        <figure>
          <pre><code><span><span>match</span> u {</span>
<span>    U::<span>A</span>(_) | U::<span>B</span>(_) =&gt; {</span>
<span>        <span>handle_ab</span>();</span>
<span>        <span>match</span> u {</span>
<span>            U::<span>A</span>(_) =&gt; <span>handle_a</span>(),</span>
<span>            U::<span>B</span>(_) =&gt; <span>handle_b</span>(),</span>
<span>            _ =&gt; <span>unreachable!</span>(),</span>
<span>        }</span>
<span>    }</span>
<span>    U::C =&gt; <span>handle_c</span>(),</span>
<span>}</span></code></pre>
        </figure>
        <p>
          And… this is fine, really! I wrote code of this shape many times, and
          it never failed at runtime due to a misapplied refactor later. Still,
          every time I write that <code>unreachable</code>, I die inside a
          little. Surely there should be some way to explain to the compiler
          that <code>c</code> is really unreachable there? Well, as I realized
          an hour ago, in Zig, you can!
        </p>
        <p>
          This is the awkward runtime-panicky and theoretically brittle version:
        </p>

        <figure>
          <pre><code><span><span>switch</span> (u) {</span>
<span>    .a, .b =&gt; <span>|</span>_, ab<span>|</span> {</span>
<span>        handle_ab();</span>
<span>        <span>switch</span> (ab) {</span>
<span>            .a =&gt; handle_a(),</span>
<span>            .b =&gt; handle_b(),</span>
<span>            <span>else</span> =&gt; <span>unreachable</span>,</span>
<span>        }</span>
<span>    },</span>
<span>    .c =&gt; handle_c(),</span>
<span>}</span></code></pre>
        </figure>
        <p>And here’s a bullet-proof compiler-checked one:</p>

        <figure>
          <pre><code><span><span>const</span> U = <span>union</span>(<span>enum</span>) {</span>
<span>    a: <span>i32</span>,</span>
<span>    b: <span>i32</span>,</span>
<span>    c,</span>
<span>};</span>
<span></span>
<span><span>fn</span><span> handle</span>(u: U) <span>void</span> {</span>
<span>    <span>switch</span> (u) {</span>
<span>        <span>inline</span> .a, .b =&gt; <span>|</span>_, ab<span>|</span> {</span>
<span>            handle_ab();</span>
<span>            <span>switch</span> (ab) {</span>
<span>                .a =&gt; handle_a(),</span>
<span>                .b =&gt; handle_b(),</span>
<span>                <span>else</span> =&gt; <span>comptime</span> <span>unreachable</span>,</span>
<span>            }</span>
<span>        },</span>
<span>        .c =&gt; handle_c(),</span>
<span>    }</span>
<span>}</span>
<span></span>
<span><span>fn</span><span> handle_ab</span>() <span>void</span> {}</span>
<span><span>fn</span><span> handle_a</span>() <span>void</span> {}</span>
<span><span>fn</span><span> handle_b</span>() <span>void</span> {}</span>
<span><span>fn</span><span> handle_c</span>() <span>void</span> {}</span>
<span></span>
<span><span>pub</span> <span>fn</span><span> main</span>() <span>void</span> { handle(.c); }</span></code></pre>
        </figure>
        <p>
          There are two tricks here. <code>inline .a, .b</code> forces the
          compiler to generate the program twice, where <code>ab</code>
          is bound to <em>comptime</em> value. The second trick is <code>comptime unreachable</code>, which instructs the compiler to fail if
          it gets to the else branch. But, because <code>ab</code> is known at
          comptime, compiler knows that <code>else</code> is in fact
          unreachable, and doesn’t hit the error.
        </p>
        <p>Adding a bug fails compilation, as intended:</p>

        <figure>
          <pre><code><span>    <span>switch</span> (u) {</span>
<span>        <span>inline</span> .a, .b, .c =&gt; <span>|</span>_, ab<span>|</span> {</span>
<span>            handle_ab();</span>
<span>            <span>switch</span> (ab) {</span>
<span>                .a =&gt; handle_a(),</span>
<span>                .b =&gt; handle_b(),</span>
<span>                <span>else</span> =&gt; <span>comptime</span> <span>unreachable</span>,</span>
<span>            }</span>
<span>        },</span>
<span>    }</span></code></pre>
        </figure>

        <figure>
          <pre><code><span><span>$</span> ./zig/zig build-exe partial-match.zig</span>
<span><span>partial-match.zig:14:34: error: reached unreachable code</span></span>
<span><span>                else =&gt; comptime unreachable,</span></span>
<span><span>                                 ^~~~~~~~~~~</span></span></code></pre>
        </figure>
      </article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What the windsurf sale means for the AI coding ecosystem (150 pts)]]></title>
            <link>https://ethanding.substack.com/p/windsurf-gets-margin-called</link>
            <guid>44843801</guid>
            <pubDate>Sat, 09 Aug 2025 03:24:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ethanding.substack.com/p/windsurf-gets-margin-called">https://ethanding.substack.com/p/windsurf-gets-margin-called</a>, See on <a href="https://news.ycombinator.com/item?id=44843801">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!VsZo!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!VsZo!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png 424w, https://substackcdn.com/image/fetch/$s_!VsZo!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png 848w, https://substackcdn.com/image/fetch/$s_!VsZo!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png 1272w, https://substackcdn.com/image/fetch/$s_!VsZo!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!VsZo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png" width="750" height="500" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:500,&quot;width&quot;:750,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:563685,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://ethanding.substack.com/i/168562735?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!VsZo!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png 424w, https://substackcdn.com/image/fetch/$s_!VsZo!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png 848w, https://substackcdn.com/image/fetch/$s_!VsZo!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png 1272w, https://substackcdn.com/image/fetch/$s_!VsZo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75e55870-f188-4e4c-b2d6-fcf6ab675ca1_750x500.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>imagine you relaunch your company twice, and manage to become one of the fastest-growing saas companies in history — literally record-breaking. go from zero to $82m arr in eight months. get enterprise customers like nvidia and palantir. go on every single vc podcast to tell people about it.</p><p>then give it all away for almost free. in 72 hours. over a weekend.</p><p>that's exactly what the windsurf founders did last week. everyone's so busy talking about the talent or the equity of the team that was left behind, they’re not asking “why did you sell one of the fastest growing products in history for nothing?”</p><p><span>after the acquisition, windsurf was left with about $100M on the balance sheet — cognition </span><a href="https://x.com/deedydas/status/1945684159411912742" rel="">acquired the company for $250M</a><span> — this means the enterprise value of the company [ignoring the cash] was $150M…</span></p><p>you have a business that went from 0 - $82M ARR in 8 months that no one wanted to touch, and the best offer was for a less than 2x multiple?</p><p>what the fuck was so broken that the founders would rather leave it behind for almost nothing, and</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!bSh0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!bSh0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png 424w, https://substackcdn.com/image/fetch/$s_!bSh0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png 848w, https://substackcdn.com/image/fetch/$s_!bSh0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png 1272w, https://substackcdn.com/image/fetch/$s_!bSh0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!bSh0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png" width="636" height="938" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/be930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:938,&quot;width&quot;:636,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:193754,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://ethanding.substack.com/i/168562735?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!bSh0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png 424w, https://substackcdn.com/image/fetch/$s_!bSh0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png 848w, https://substackcdn.com/image/fetch/$s_!bSh0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png 1272w, https://substackcdn.com/image/fetch/$s_!bSh0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe930d16-6268-4bc9-8b4b-22ab61a93fb1_636x938.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>let me walk you through a totally normal corporate divorce since [checks notes] last week:</p><p><strong>thursday, july 11</strong><span>: openai's $3b acquisition of windsurf falls apart. after months of negotiations, they walk away.</span></p><p><strong>also thursday, july 11</strong><span>: google announces they're paying $2.4b to hire windsurf's ceo and 41 researchers for deepmind. not to acquire windsurf. just the humans. the same day openai walks. what a coincidence!</span></p><p><strong>friday afternoon</strong><span>: windsurf's founders—or what's left of them—make their first call to cognition. not "let's explore options." not "let's take our time." just: "you want a company?"</span></p><p><strong>monday morning</strong><span>: deal signed. cognition gets the entire business—$82m arr, 200+ employees, all the ip—for basically the loose change in reed hastings' couch.</span></p><p>so let's recap: google swoops in the exact moment openai bails, cherrypicks the leadership team for $2.4b, and leaves behind a headless $82m arr company like a half-eaten sandwich.</p><p>at $57 million per engineer, that's either the greatest talent arbitrage in history or the most expensive acquihire cope of all time.</p><p>the punchline? google's deal explicitly excluded the actual business. they looked at $82m of arr and said "nah, we're good."</p><p>when buyers are paying billions for your employees but won't touch your revenue with a ten-foot pole, you're not running a business—you're running an extremely expensive temp agency.</p><p>windsurf's ceo once let slip that their $10/month plan had "not much margin." this is like saying the titanic had "not much buoyancy."</p><p>the whisper network on twitter has been screaming about this for months:</p><ul><li><p><strong><a href="https://x.com/jsnnsa/status/1941306461402829189" rel="">@jsnnsa claims -300% negative margins</a></strong><span>: "revenue? sure but their revenue is what will kill them. you pay them 1$ they pay ant 3$"</span></p></li><li><p><strong><a href="https://x.com/_opencv_/status/1944908287516008707" rel="">@</a></strong><em><strong><a href="https://x.com/_opencv_/status/1944908287516008707" rel="">opencv</a></strong></em><strong><a href="https://x.com/_opencv_/status/1944908287516008707" rel=""> goes even harder with -500% margins</a></strong><span>: "Cursor is dead in 12 months. It's worse than anyone knows. Money is being incinerated"</span></p></li><li><p><strong><a href="https://x.com/tenobrus/status/1904422446389706905" rel="">@tenobrus spells it out</a></strong><span>: "both cursor and windsurf are absolutely bleeding VC money on every call btw"</span></p></li></ul><p><span>when </span><a href="https://x.com/andersonbcdefg/status/1904427149463105620" rel="">@andersonbcdefg</a><span> points out that claude code charges $5/day ($150/month) as a "good proxy for how much those 'should' cost," you start to see the problem. cursor charges $20/month. windsurf charges $15/month. they're selling dollar bills for quarters.</span></p><p>based on the forensics:</p><ul><li><p>cursor: 50 employees, $42m/month revenue, but $53m/month implied burn</p></li><li><p>revenue per user: $20/month</p></li><li><p>api costs for power users: $80-200/month</p></li><li><p>margin on heavy users: -300% to -500%</p></li><li><p>every new customer makes the problem worse</p></li></ul><p>here's why this is a death spiral:</p><ul><li><p>you can't raise prices (claude code is your competition)</p></li><li><p>you can't cut costs (api bills are fixed)</p></li><li><p>you can't stop growing (that's your only story)</p></li><li><p>you can't pivot (you've raised too much)</p></li><li><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Au7c!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Au7c!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png 424w, https://substackcdn.com/image/fetch/$s_!Au7c!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png 848w, https://substackcdn.com/image/fetch/$s_!Au7c!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png 1272w, https://substackcdn.com/image/fetch/$s_!Au7c!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Au7c!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png" width="500" height="750" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:750,&quot;width&quot;:500,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:465826,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://ethanding.substack.com/i/168562735?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Au7c!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png 424w, https://substackcdn.com/image/fetch/$s_!Au7c!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png 848w, https://substackcdn.com/image/fetch/$s_!Au7c!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png 1272w, https://substackcdn.com/image/fetch/$s_!Au7c!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9e0f6cb0-db96-4697-83d9-0b4101a23d87_500x750.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div></li></ul><p>at that point - you need to either find another way to bring down the costs or… look for an exit?</p><p>i noted something in my last piece:</p><blockquote><p><span>it's a three-variable equation: </span><strong>burn rate vs. tech timeline vs. brand toxicity</strong><span>. spend too fast, you die before the tech arrives. get too toxic, you're radioactive when it matters. but nail the balance? you're the default choice when the future arrives.</span></p><div data-component-name="DigestPostEmbed"><a href="https://ethanding.substack.com/p/levered-beta-is-all-you-need" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!IS5S!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9aec1fc-6628-4b82-90d5-a85dc469724d_790x316.png"><img src="https://substackcdn.com/image/fetch/$s_!IS5S!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9aec1fc-6628-4b82-90d5-a85dc469724d_790x316.png" sizes="100vw" alt="levered beta is all you need " width="140" height="140"></picture></div></a></div></blockquote><p>windsurf and cursor were playing the same game — levering up huge by using VC money to subsidize anthropic’s models (rumor says Cursor is 20% of Anthropic’s growth). their tech timeline wasn’t frontier performance, but their own ability to develop cost efficient models before they run out of money, or lose all their customers to claude code</p><ol><li><p>use vc money to subsidize api costs</p></li><li><p>collect massive training data from developers</p></li><li><p>train your own models before money runs out</p></li><li><p>flip from -500% margins to positive overnight</p></li></ol><p><span>that's why cursor tried to poach anthropic researchers (</span><a href="https://x.com/ArfurRock/status/1945212904610922813" rel="">and the researchers staying at anthropic might be a sign they can’t do it</a><span>). that's why windsurf rushed out SWE-1. they weren't building coding tools—they were building datasets with a fancy ui.</span></p><p><span>as </span><a href="https://x.com/jsnnsa/status/1941306461402829189" rel="">@jsnnsa noted</a><span>: "if their plan is to compete with anthropic then they'll be bankrupt by next summer. they have no research talent, little capital, and a money-hungry dumpster fire burning a hole in their pocket"</span></p><p>cursor might do totally fine despite losing the anthropic researchers. they've got thrive's infinite money printer, they raised early and have the lead, maybe they clutch out a kimi-type model. my prediction is that anthropic acquires them — they might already be in talks, but either way with the surge in popularity of claude code, cursor might be feeling like bear sterns in summer of 2007</p><p>windsurf was behind and out of time, bear sterns in 2008. they needed either:</p><ul><li><p>api costs to drop 10x (not happening)</p></li><li><p>their models to beat anthropic (delusional)</p></li><li><p>someone else to hold the bag (ding ding ding)</p></li></ul><p>when you're burning millions per month and your supplier launches a competing product at 1/7th your price point, you don't have a margin problem—you have a margin call.</p><p><span>windsurf's fire sale proves what we've been avoiding: the coding part has no value capture — and claude code is coming. they're racing against irrelevance. </span><strong>and they're not alone</strong></p><p>bolt and replit aren't bleeding cash like cursor/windsurf—they're not subsidizing every api call. but they're sitting on a different time bomb:</p><ul><li><p>mediocre margins at best</p></li><li><p>users build an app then aren’t reliably going to do more stuff</p></li><li><p>claude code is coming for their lunch</p></li><li><p>their entire value prop evaporates the second anthropic decides to add a "deploy" button</p></li></ul><p>but here's the twist: at least we're not low-code tools, bc since i last wrote, the low-code apocalypse arrived:</p><ul><li><p><strong>figma</strong><span> launched text-to-ui</span></p></li><li><p><strong>retool</strong><span> released ai app builder</span></p></li><li><p><strong>airtable</strong><span> dropped their own "describe what you want" feature</span></p></li><li><p><strong>notion</strong><span> is pushing ai-powered app creation</span></p></li></ul><p>low-code ui builders are out. whatever the fuck this text-to-app paradigm is, it's in. squarespace and webflow announcements incoming in 3... 2... 1...</p><p>but these are if the coding layer is worthless, what isn't?</p><ul><li><p><strong>hosting infrastructure</strong><span>: netlify doesn't write code, but bolt drives massive traffic to it. when everyone's building apps with ai, someone still needs to host them. boring, profitable, sustainable. replit rolls this in house, notion has a light version of this.</span></p></li><li><p><strong>databases</strong><span>: supabase is laughing. every ai-generated app needs data storage. they don't care if you wrote the code or claude did. this is why turning their tables into databases, retool released hosted databases, and airtable is entering the space at all</span></p></li><li><p><strong>workflow automaton:</strong><span> running workloads on cron jobs with ec2 instances is very much still valuable — you see retool, zapier, and airtable also enter from that perspective</span></p></li><li><p><strong>seo optimization</strong><span>: webflow still owns this. lovable and bolt roadmaps are desperately trying to add seo features because that's where actual value lives, and notion has made some forrays</span></p></li><li><p><strong>basically all the services that low code platforms had to bulid out in the past</strong><span>, like auth / security / rbac / latency</span></p></li></ul><p>the infrastructure that manages code is still worth something — and at this point i’d say more of replit’s value is in being able to serve the code you make in it the way netlify does, than in bolt — bolt is more of a distribution engine for netlify where replit is bundling the whole thing together. if bolt can't figure out some defensible angle (maybe they carve out seo as a niche?), i might be writing them to zero.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!lu7T!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!lu7T!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png 424w, https://substackcdn.com/image/fetch/$s_!lu7T!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png 848w, https://substackcdn.com/image/fetch/$s_!lu7T!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png 1272w, https://substackcdn.com/image/fetch/$s_!lu7T!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!lu7T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png" width="419" height="405.8495670995671" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:895,&quot;width&quot;:924,&quot;resizeWidth&quot;:419,&quot;bytes&quot;:240683,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://ethanding.substack.com/i/168562735?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!lu7T!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png 424w, https://substackcdn.com/image/fetch/$s_!lu7T!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png 848w, https://substackcdn.com/image/fetch/$s_!lu7T!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png 1272w, https://substackcdn.com/image/fetch/$s_!lu7T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb55370ba-9b10-4a67-a8b9-937e49c74ffc_924x895.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>let's be clear: $2.4 billion isn't a fire sale. everyone kinda made the money they wanted. just not for the reason VC investments typically make money</p><p>google didn't buy a company. they bought 42 researchers who'd spent $200+ million of vc money learning how to train coding models. at $57 million per head, that's the most expensive education program in history.</p><p>here's what actually happened:</p><ul><li><p>vcs pump $200m into windsurf</p></li><li><p>windsurf uses that money to buy gpus and training data</p></li><li><p>42 engineers learn incredibly valuable skills</p></li><li><p>google pays $2.4b for those skills</p></li><li><p>the actual business (82m arr) gets sold for pocket change</p></li></ul><p>the company—its revenue, customers, product—was worth maybe 1.8x revenue. that's a rounding error on a series a. but the expertise? that's worth billions.</p><p>this is the insane state of ai talent wars. windsurf accidentally discovered the ultimate arbitrage: raise vc money to train yourself on expensive gpus, then sell your newly-acquired expertise to the highest bidder. it’d be like if uber and lyft both collapsed, but waymo was willing to acquihire their teams for $20B, just for the expertise they developed on rolling out ridesharing operations</p><p>imagine pitching this to sequoia: "give us hundreds of millions, we'll use it to teach ourselves such valuable skills that google will acquihire us for $50m each." they'd throw you out. but that's exactly what happened.</p><p>it's a great outcome for the founders, and investors, employees. genuinely, congrats to them. but treating this like a successful business exit is missing the point entirely.</p><p>windsurf wasn't a company. it was an accidentally subsidized training program that discovered the most valuable output wasn't code — it was coders who knew how to build coding models.</p><p>the vcs funded a $200m scholarship program and google paid the tuition refund.</p><p>but here's the thing: this escape hatch isn't available to everyone. windsurf got bailed out because they accidentally became an ai research lab, and cursor is probably going to be fine as well, for the same reason.</p><p>the windsurf "acquisition" set a precedent, but it's a dangerous one. everyone's looking at the $2.4b number and missing the real story: a business doing $82m arr couldn't find a buyer at any price.</p><p>that's not a success story. that's a warning.</p><p>the margin calls are coming. not everyone gets to answer them by selling their homework.</p><p>thanks for Nikunj Kothari and Benn Stancil for reviewing</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Let's properly analyze an AI article for once (129 pts)]]></title>
            <link>https://nibblestew.blogspot.com/2025/08/lets-properly-analyze-ai-article-for.html</link>
            <guid>44843605</guid>
            <pubDate>Sat, 09 Aug 2025 02:30:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nibblestew.blogspot.com/2025/08/lets-properly-analyze-ai-article-for.html">https://nibblestew.blogspot.com/2025/08/lets-properly-analyze-ai-article-for.html</a>, See on <a href="https://news.ycombinator.com/item?id=44843605">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-2796283269388226709" itemprop="description articleBody">
<p>Recently the CEO of Github wrote a blog post called <a href="https://ashtom.github.io/developers-reinvented">Developers reinvented</a>. It was reposted with various clickbait headings like&nbsp;<a href="https://www.finalroundai.com/blog/github-ceo-thomas-dohmke-warns-developers-embrace-ai-or-quit">GitHub CEO Thomas Dohmke Warns Developers: "Either Embrace AI or Get Out of This Career"</a>&nbsp;(that one feels like an LLM generated summary of the actual post, which would be ironic if it wasn't awful). To my great misfortune I read both of these. Even if we ignore whether AI is useful or not, the writings contain some of the absolute worst reasoning and stretched logical leaps I have seen in years, maybe decades. If you are ever in the need of finding out how not to write a "scientific" text on any given subject, this is the disaster area for you.</p><p>But before we begin, a detour to the east.</p><h2>Statistics and the Soviet Union</h2><p>One of the great wonders of statistical science of the previous century was without a doubt the Soviet Union. They managed to invent and perfect dozens of ways to turn data to your liking, no matter the reality. Almost every official statistic issued by USSR was a lie. Most people know this. But even most of those do not grasp <i>just how much</i> the stats differed from reality. I sure didn't until I read <a href="https://www.goodreads.com/book/show/39679779-empire-of-the-absurd">this book</a>. Let's look at some examples.</p><h2>Only ever report percentages</h2><p>The USSR's glorious statistics tended to be of the type "manufacturing of shoes grew over 600% this five year period". That certainly sounds a lot better than "In the last five years our factory made 700 pairs of shoes as opposed to 100" or even "7 shoes instead of 1". If you are really forward thinking, you can even cut down shoe production on those five year periods when you are not being measured. It makes the stats even more impressive, even though in reality many people have no shoes at all.</p><p>The USSR classified the real numbers as state secrets because the truth would have made them look bad. If a corporation only gives you percentages, they may be doing the same thing. Apply skepticism as needed.</p><h2>Creative comparisons</h2><p>The previous section said the manufacturing of shoes has grown. Can you tell what it is not saying? That's right, growth over what? It is implied that the comparison is to the previous five year plan. But it is not. Apparently a common comparison in these cases was the production amounts of the year 1913. This "best practice" was not only used in the early part of the 1900s, it was used far into the 1980s.</p><p>Some of you might wonder why 1913 and not 1916, which was the last year before the bolsheviks took over? Simply because that was the century's worst year for Russia as a whole. So if you encounter a claim that "car manufacturing was up 3700%" some year in 1980s Soviet Union, now you know what that actually meant.</p><h2>"Better" measurements</h2><p>According to official propaganda, the USSR was the world's leading country in wheat production. In this case they even listed out the production in absolute tonnes. In reality it was all fake. The established way of measuring wheat yields is to measure the "dry weight", that is, the mass of final processed grains. When it became apparent that the USSR could not compete with imperial scum, they changed their measurements to "wet weight". This included the mass of <i>everything</i> that came out from the nozzle of a harvester, such as stalks, rats, mud, rain water, dissidents and so on.</p><p>Some people outside the iron curtain even believed those numbers. Add your own analogy between those people and modern VC investors here.</p><h2>To business then</h2><p>The actual blog post starts with this thing that can be considered a picture.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnvKIDpXIXw4sobIuJol13Nyvo2k3lS465_e8v-HNDYAR0pMz653j2pKgenz8pJodfFpqdJaAT-N68z-M_REejgPWmv9z0j87EqFsQ6prRY_u8AezZKHjpvbvXvMx2fgOd3B9HqUeIumtRiaZFDRZJwERKKr0Q-zHq83XD6T2nrPbo1VwyTLPlBqzg01Q/s720/reinvented.png"><img data-original-height="480" data-original-width="720" height="213" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnvKIDpXIXw4sobIuJol13Nyvo2k3lS465_e8v-HNDYAR0pMz653j2pKgenz8pJodfFpqdJaAT-N68z-M_REejgPWmv9z0j87EqFsQ6prRY_u8AezZKHjpvbvXvMx2fgOd3B9HqUeIumtRiaZFDRZJwERKKr0Q-zHq83XD6T2nrPbo1VwyTLPlBqzg01Q/s320/reinvented.png" width="320"></a></p><p>What message would this choice of image tell about the person using it in their blog post?</p><ol><li>Said person does not have sufficient technical understanding to grasp the fact that children's toy blocks should, in fact, be affected by gravity (or that perspective is a thing, but we'll let that pass).</li><li>Said person does not give a shit about whether things are correct or could even work, as long as they look "somewhat plausible".</li></ol><p>Are these the sort of traits a person in charge of <i>the largest software development platform on Earth</i>&nbsp;should have? No, they are not.</p><p>To add insult to injury, the image seems to have been created with the Studio Ghibli image generator, which Hayao Miyazaki described as an abomination on art itself. Cultural misappropriation is high on the list of core values at Github HQ it seems.</p><p>With that let's move on to the actual content, which is this post from Twitter (to quote Matthew Garrett, I will respect their name change once Elon Musk starts respecting his child's).</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgt9Ybd7z0kDtsXvmnc398YA5WEL1wzhJm7zvZMY8N68ms8UfUBkIvrz0upLIlZzPAAYytcOyE84bEoCCFA_lxafqrOTDKpGZtQD9IqYeIZsBgj_F_YBCtW6MEsZsfemMvAb4B2shrZHywPf5da8cyB3MYuYJF4EcFhqu_1j4nAhvanCQPi4fE5Is7mNCg/s808/evidence.png"><img data-original-height="403" data-original-width="808" height="160" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgt9Ybd7z0kDtsXvmnc398YA5WEL1wzhJm7zvZMY8N68ms8UfUBkIvrz0upLIlZzPAAYytcOyE84bEoCCFA_lxafqrOTDKpGZtQD9IqYeIZsBgj_F_YBCtW6MEsZsfemMvAb4B2shrZHywPf5da8cyB3MYuYJF4EcFhqu_1j4nAhvanCQPi4fE5Is7mNCg/s320/evidence.png" width="320"></a></p><p>Oh, wow! A field study. That makes things clear. With evidence and all! How can we possibly argue against that?</p><p>Easily. As with a child.</p><p>Let's look at this "study" (and I'm using the word in its loosest possible term here) and its details with an actual critical eye. The first thing is statistical representativeness. The sample size is 22. According to <a href="http://www.raosoft.com/samplesize.html">this sample size calculator</a> I found, a required sample size for just one thousand people would be 278, but, you know, one order of magnitude one way or another, who cares about those? Certainly not business big shot movers and shakers. Like Stockton Rush for example.</p><p>The math above assumes an unbiased sampling. The post does not even attempt to answer whether that is the case. It would mean getting answers to questions like:</p><ul><li>How were the 22 people chosen?</li><li>How many different companies, skill levels, nationalities, genders, age groups etc were represented?</li><li>Did they have any personal financial incentive on making their new AI tools look good?</li><li>Were they under any sort of duress to produce the "correct" answers?</li><li>What was/were the exact phrase(s) that was asked?</li><li>Were they the same for all participants?</li><li>Was the test run multiple times until it produced the desired result?</li></ul><p>The latter is an age old trick where you run a test with random results over and over on small groups. Eventually you will get a run that points the way you want. Then you drop the earlier measurements and publish the last one. In "the circles" this is known as <i>data set selection</i>.</p><p>Just to be sure, I'm <i>not</i> saying that is what they did. But if someone drove a dump truck full of money to my house and asked me to create a "study" that produced these results, that is exactly how I would&nbsp;do it. (I would not actually do it because I have a spine.)</p><p>Moving on. The main headline grabber is "Either you embrace AI or get out of this career". If you actually read the post (I know), what you find is that this is actually a quote from one of the participants. It's a bit difficult to decipher from the phrasing but my reading is that this is not a grandstanding hurrah of all things AI, but more of a "I guess this is something I'll have to get used to" kind of submission. That is not evidence, certainly not of the clear type. It is an opinion.</p><p>The post then goes on a buzzwordsalad tour of statements that range from the incomprehensible to the puzzling. Perhaps the weirdest is this nugget on education:</p><blockquote><p>Teaching [programming] in a way that evaluates rote syntax or memorization of APIs is becoming obsolete.</p></blockquote><p>It is not "becoming obsolete". It has been considered the wrong thing to do for as long as computer science has existed. Learning the syntax of most programming languages takes a few lessons, the rest of the semester is spent on actually using the language to solve problems. Any curriculum not doing that is just plain bad. Even worse than CS education in Russia in 1913.</p><p>You might also ponder that if the author is <i>so</i> out of touch with reality in this simple issue, how completely off base the rest of his statements might be. In fact the statement is so wrong at such a fundamental level that it has probably been generated with an LLM.</p><h2>A magician's shuffle</h2><p>As nonsensical as the Twitter post is, we have not yet even mentioned the biggest misdirection in it. You might not even have noticed it yet. I certainly did not until I read the actual post. Try if you can spot it.</p><p>Ready? Let's go.</p><p>The actual fruit of this "study" boils down to this snippet.</p><blockquote><p>Developers rarely mentioned “time saved” as the core benefit of working in this new way with agents. They were all about increasing ambition.</p></blockquote><p>Let that sink in. For the last several years the main supposed advantage of AI tools has been the fact that they save massive amounts of developer time. This has lead to the "fire all your developers and replace them with AI bots" trend sweeping the nation. Now even this AI advertisement of a "study" can not find any such advantages and starts backpedaling into something completely different. Just like we have always been at war with Eastasia, AI has never been about "productivity". No. No. It is all about "increased ambition", whatever that is. The post then carries on with this even more baffling statement.</p><blockquote><p>When you move from thinking about reducing effort to expanding scope, only the most advanced agentic capabilities will do.</p></blockquote><p>Really? Only the most advanced agentics you say? That is a bold statement to make given that the leading reason for software project failure is scope creep. This is the one area where human beings have decades long track record for beating any artificial system. Even if machines were able to do it better, "Make your project failures more probable! Faster! Spectacularer!" is a tough rallying cry to sell.&nbsp;</p><p>&nbsp;To conclude, the actual findings of this "study" seem to be that:</p><ol><li>AI does not improve developer productivity or skills</li><li>AI does increase developer ambition</li></ol><p>This is strictly worse than the current state of affairs.
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dial-up Internet to be discontinued (165 pts)]]></title>
            <link>https://help.aol.com/articles/dial-up-internet-to-be-discontinued</link>
            <guid>44843369</guid>
            <pubDate>Sat, 09 Aug 2025 01:37:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://help.aol.com/articles/dial-up-internet-to-be-discontinued">https://help.aol.com/articles/dial-up-internet-to-be-discontinued</a>, See on <a href="https://news.ycombinator.com/item?id=44843369">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article"><p>AOL routinely evaluates its products and services and has decided to discontinue Dial-up Internet. This service will no longer be available in AOL plans. As a result, on September 30, 2025 this service and the associated software, the AOL Dialer software and AOL Shield browser, which are optimized for older operating systems and dial-up internet connections, will be discontinued.</p>
<p>This change will not affect any other benefits in your AOL plan, which you can access any time on your <a href="https://mybenefits.aol.com/?ncid=mbr_advadolnk00000009" target="_new" title="Go to you Benefits page.">AOL plan dashboard</a>. To manage or cancel your account, <a href="https://myaccount.aol.com/" target="_new" title="Go to your account settings page.">visit MyAccount</a>.</p>
<p>For more information or if you have questions about your account, call:</p>
<ul><li><strong>U.S.</strong> - 1-888-265-5555</li><li><strong>Canada</strong> - 1-888-265-4357</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Our European search index goes live (132 pts)]]></title>
            <link>https://blog.ecosia.org/launching-our-european-search-index/</link>
            <guid>44841741</guid>
            <pubDate>Fri, 08 Aug 2025 21:12:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.ecosia.org/launching-our-european-search-index/">https://blog.ecosia.org/launching-our-european-search-index/</a>, See on <a href="https://news.ycombinator.com/item?id=44841741">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>We’ve started delivering search results from our new European-based search index to Ecosia users! This will help us build the kind of ethical and fair internet we believe in.&nbsp;</p><p>Last year <a href="https://blog.ecosia.org/eusp/"><u>we launched European Search Perspective (EUSP)</u></a>, a joint venture with Qwant. The launch marked a big step forward in our journey towards tech independence and digital sovereignty for Europe.</p><p>Now, we’ve taken the next step: our users in France are receiving a portion of their search results directly from EUSP’s own index. We’re aiming to serve 50% of French search queries by the end of the year, and will soon start rolling out to other countries.</p><h3 id="what-does-having-an-independent-search-index-mean"><strong>What does having an independent search index mean?&nbsp;</strong></h3><p>A search index is the database of information that search engines pull their results from. Until now, only a handful of companies have built their own – which means most smaller search engines have to depend on them to provide search results.</p><p>That’s why EUSP has developed <a href="https://staan.ai/"><u>Staan</u></a> (Search Trusted API Access Network), a search index aimed to support a sovereign, privacy-first search infrastructure for Europe. It’s built for alternative search engines and AI companies that need fast, reliable access to the latest web data, while safeguarding user privacy and data security.</p><p>By using Staan as one of our sources, we can start to build greater digital independence and transparency.</p><h3 id="why-is-this-independence-important"><strong>Why is this independence important?&nbsp;</strong></h3><p>Having our own search infrastructure is a critical step towards plurality; a healthy and diverse search market reflecting multiple perspectives, and building Europe’s own digital tools.&nbsp;</p><p>Much of Europe’s search, cloud, and AI layers are built on American Big Tech stacks, which puts whole sectors at the mercy of political or commercial agendas. To put it bluntly: if Big Tech decided to pull the plug, Europe would be in trouble.</p><p>Creating a fully independent search index means we have more control. We can better serve our users, develop ethical AI, and double down on our mission to build tech that benefits people and the planet.</p><h3 id="an-open-foundation-for-competition-privacy-and-innovation"><strong>An open foundation for competition, privacy and innovation</strong></h3><p>Unlike Ecosia’s steward-owned model, EUSP is structured to allow outside investment –&nbsp; enabling long-term scaling of its infrastructure. EUSP’s search index is also available to other tech companies, offering a foundation for competition, data privacy, and innovation in areas like generative AI.&nbsp;</p><h3 id="what-does-all-this-mean-for-you"><strong>What does all this mean for you?</strong>&nbsp;</h3><p>At first, you probably won’t notice much change when using Ecosia – this update is behind the scenes. But when we look at the bigger picture, it’s a meaningful step that helps you keep growing your climate impact.&nbsp;</p><p>How? Because it strengthens Europe’s long-term competitiveness, democratic control, and stability. If we have more autonomy over our own tools, we can focus on shaping the greener, fairer tech future we want, and continue expanding our mission to tackle the climate crisis, together.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: How can ChatGPT serve 700M users when I can't run one GPT-4 locally? (476 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44840728</link>
            <guid>44840728</guid>
            <pubDate>Fri, 08 Aug 2025 19:27:28 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44840728">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="bigbox"><td><table><tbody><tr id="44840728"><td><span></span></td><td><center><a id="up_44840728" href="https://news.ycombinator.com/vote?id=44840728&amp;how=up&amp;goto=item%3Fid%3D44840728"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=44840728">Ask HN: How can ChatGPT serve 700M users when I can't run one GPT-4 locally?</a></span></td></tr><tr><td colspan="2"></td><td><span><span id="score_44840728">116 points</span> by <a href="https://news.ycombinator.com/user?id=superasn">superasn</a> <span title="2025-08-08T19:27:28 1754681248"><a href="https://news.ycombinator.com/item?id=44840728">2 hours ago</a></span> <span id="unv_44840728"></span> | <a href="https://news.ycombinator.com/hide?id=44840728&amp;goto=item%3Fid%3D44840728">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20How%20can%20ChatGPT%20serve%20700M%20users%20when%20I%20can%27t%20run%20one%20GPT-4%20locally%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=44840728&amp;auth=df93e80f283c5ff48819997a35cd155bd4cf7c12">favorite</a> | <a href="https://news.ycombinator.com/item?id=44840728">62&nbsp;comments</a></span></td></tr><tr><td colspan="2"></td><td><div><p>Sam said yesterday that chatgpt handles ~700M weekly users. Meanwhile, I can't even run a single GPT-4-class model locally without insane VRAM or painfully slow speeds.</p><p>Sure, they have huge GPU clusters, but there must be more going on - model optimizations, sharding, custom hardware, clever load balancing, etc.</p><p>What engineering tricks make this possible at such massive scale while keeping latency low?</p><p>Curious to hear insights from people who've built large-scale ML systems.</p></div></td></tr><tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr></tbody></table><br>
</td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Build durable workflows with Postgres (139 pts)]]></title>
            <link>https://www.dbos.dev/blog/why-postgres-durable-execution</link>
            <guid>44840693</guid>
            <pubDate>Fri, 08 Aug 2025 19:24:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dbos.dev/blog/why-postgres-durable-execution">https://www.dbos.dev/blog/why-postgres-durable-execution</a>, See on <a href="https://news.ycombinator.com/item?id=44840693">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>When we started building a durable workflows library, the most critical architectural decision we faced was what data store to use for workflow metadata. The core durable workflow operations are simple–checkpointing workflow state and recovering an interrupted workflow from its latest checkpoint. Almost any data store can handle these operations, but choosing the right one is critical to ensure workflows are scalable and performant.</p><p>In this blog post, we’ll dive deep into why we chose to build on Postgres. While there are good nontechnical reasons for the decision (Postgres is popular and open-source with a vibrant community and over 40 years of battle-testing), we’ll focus on the technical reasons–key Postgres features that make it easier to develop a robust and performant workflows library. In particular, we’ll look at:</p><ol role="list"><li>How Postgres concurrency control (particularly its support for locking clauses) enable scalable distributed queues.</li><li>How the relational data model (plus careful use of secondary indexes) enables performant observability tooling over workflow metadata.</li><li>How Postgres transactions enable exactly-once execution guarantees for steps performing database operations.</li></ol><h2>Building Scalable Queues</h2><p>It’s often useful to enqueue durable workflows for later execution. However, using a database table as a queue is tricky because of the risk of contention. To see why that’s a problem, let’s look at how database-backed queues work.&nbsp;</p><p>In a database-backed workflow queue, clients enqueue workflows by adding them to a queues table, and workers dequeue and process the oldest enqueued workflows (assuming a FIFO queue). Naively, each worker runs a query like this to find the N oldest enqueued workflows, then dequeues them:</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894e8dd31d1c21c43fb9ad1_AD_4nXfg9qKIxkxY5Zpo2Pw14wum0vuFrq6lzvMCf5NrauJaXHJa5EeaL-sfyD7tYE3OcV8oASA4utitnDGTaNVcTGegQswZQZpOLHhMnsg5D6cTq9kgIDSKsMYsbyd9z2qN19KHpbIr-Q.png" loading="lazy" alt="SQL query to retrieve tasks from durable queues"></p></figure><p>The problem is that if you have many workers concurrently pulling new tasks from the queue, they all try to dequeue the same workflows. However, each workflow can only be dequeued by a single worker, so most workers will fail to find new work and have to try again. If there are enough workers, this contention creates a bottleneck in the system, limiting how rapidly tasks can be dequeued.</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894f405ac7da4fd1c11f336_Storing-workflow-state-in-Postgres.png" loading="lazy" alt=""></p></figure><p>Fortunately, Postgres provides a solution: locking clauses. Here's an example of a query using them:</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894e8dd8c41c852c7e0f674_AD_4nXf0B0MCzqgtonGjb9BPutnuYHkD9zI0tmlhrzb2zRj9PowCICOPJI5E84haepmjmUVWaANmwaoSWCXfaa9Pb_RVe-kQ-95z2yqs5Z9Qyz3gpqG06_kqTzxl5V5ykg3X1xuFgyQVtQ.png" loading="lazy" alt="Postgres SELECT FOR UPDATE SKIP LOCKED query example"></p></figure><p>Selecting rows in this way does two things. First, it locks the rows so that other workers cannot also select them. Second, it skips rows that are already locked, selecting not the N oldest enqueued workflows, but the N oldest enqueued workflows <strong>that are not already locked by another worker</strong>. That way, many workers can concurrently pull new workflows <strong>without contention</strong>. One worker selects the oldest N workflows and locks them, the second worker selects the next oldest N workflows and locks those, and so on.</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894f3e5aa45514af18ed1f7_Postgres-durable-execution-database.png" loading="lazy" alt=""></p></figure><p>Thus, by greatly reducing contention, Postgres enables a durable workflow system to process tens of thousands of workflows per second across thousands of workers.</p><h2>Making Workflows Observable</h2><p>One benefit of durable workflows is that they provide built-in observability. If every step in every workflow is checkpointed to a durable store, you can scan those checkpoints to monitor workflows in real time and to visualize workflow execution. For example, a workflow dashboard can show you every workflow that ran in the last hour, or every workflow that errored in the past month:</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894e8dd8c41c852c7e0f67d_AD_4nXcEfN3ZxPtX1f1DoZrSXUR5yy7Q-Hjh_37L1oS8JNtIiNX6Syk2V4YPXZ-Nvero9EigvvqOmuUfqPkCM5YM1caCJifOHGjP6InqJkYu8pWbQ5dF9uS8ynZznzlvJhFwoXcN0g0JOQ.png" loading="lazy" alt="Observing durable workflow history"></p></figure><p>To implement observability, you need to be able to run performant queries over workflow metadata. Postgres excels for this because virtually any workflow observability query can be easily expressed in SQL. For example, here’s a query to find all workflows that errored in the last month.</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894e8dd8c41c852c7e0f671_AD_4nXfFo1KTBYl86FzIfe6IIkKUDFm9Wxex-Z4hl1bD1z2kQHGKzNQfzwnpPYFmZ6rKL0suFvafIb0fW-UadctI9yQryoJcHUEpjcCX4azVuQxV0mS3mTHKHWw_5tNmQ9XirIjx5py5ag.png" loading="lazy" alt="Example SQL query to retrieve durable workflow status"></p></figure><p>These queries might seem obvious, but it's impossible to overstate how powerful this is. It's only possible because Postgres's relational model allows you to express complex filtering and analytical operations declaratively in SQL, leveraging decades of query optimization research. Many popular systems with simpler data models, such as key-value stores, have no such support.</p><p>Postgres also provides the tools to make these observability queries performant at scale (&gt;10M workflows): secondary indexes. Secondary indexes allow Postgres to rapidly find all workflows that have a particular attribute or range of attributes. They’re expensive to construct and maintain, so they have to be used carefully: you can’t index every workflow attribute without adding prohibitive overhead.</p><p>To strike a balance between query performance and runtime overhead, we added secondary indexes to a small number of fields that are the most selective in frequently run queries. Because most workflow observability queries are time-based (typically a dashboard displaying all workflows in a time range), the most important index is on the created_at column. We additionally added indexes to two other attributes that are frequently searched without specifying a time range: executor_id (users often want to find all workflows that ran on a given server) and status (users frequently want to find all workflows that ever errored).</p><p>‍</p><h2>Implementing Exactly-Once Semantics</h2><p>Typically, durable workflows guarantee that steps execute <strong>at least once</strong>. The way it works is that after each step completes, its outcome is checkpointed in a data store:</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894e8dd8c41c852c7e0f664_AD_4nXfKuFmryR-dSWb6SVWe0gUvn9dR-htEefBDMaN12rcS-Or0Xf15vJvXSopkDn9eRMfY6W9qzNesrHuRuY-9JAXqZN8rqPLIxhpd1IniuTT32eTxo6OEgdMPvIKTqG7jD1YJTEkEDg.jpeg" loading="lazy" alt="Durable workflow checkpointing diagram"></p></figure><p>If the program fails, upon restart the workflow looks up its latest checkpoint and resumes from its last completed step. This means that if a workflow fails while executing a step, the step may execute twice–once before the failure, then again after the workflow recovers from the failure. Because steps may be executed multiple times, they should be idempotent or otherwise resilient to re-execution.</p><p>By building durable workflows on Postgres, we can do better and guarantee that steps execute <strong>exactly once</strong>–if those steps perform database operations. To do this, we leverage Postgres transactions. The trick is to execute the entire step in a single database transaction, and to “piggyback” the step checkpoint as part of the transaction. That way, if the workflow fails while the step is executing, the entire transaction is rolled back and nothing happens. But if the workflow fails after the transaction commits, the step checkpoint is already written so the step is not re-executed.</p><p>For example, let’s say a step inserts a record into a database table</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894e8dd8c41c852c7e0f66a_AD_4nXfYy-7kyUOTTm5MKv3i0RmtsPJDd4uBn3t3rybq91mErk3aJ7R31gFu3EuOx-Zidu0SV8UJcEV7cQH0C-ohlCYd5F9GofbMjgh7xz22_nK2dlkkc4Fjv8DOw6-vzdfBn2qYWiUegg.png" loading="lazy" alt=""></p></figure><p>This step isn’t idempotent, so executing it twice would be bad–the order would be inserted into the table twice. However, because the step consists solely of a database operation, we can perform the step and checkpoint its outcome in the same transaction, like this:</p><figure><p><img src="https://cdn.prod.website-files.com/672411cbf038560468c9e68f/6894e8dd8c41c852c7e0f6bf_AD_4nXdc2Acac_XJH2Mmoxu1umX0YJ1ibdOm1lP4HeQzGswhiRkMBiIz0LyZDFqsMEyTtjTxKcxtih3RLac43cW6INe82s0kPSwx6FpxkKvHMdw_PmB4jixFOziDtOsiP6XQ2HxolF-a6w.png" loading="lazy" alt="example of an idempotent step in a durable workflow"></p></figure><p>Thus, the step either fully completes or commits (including its checkpoint) or fails and completely rolls back–the step is guaranteed to execute <strong>exactly once</strong>.</p><h2>Learn More</h2><p>If you like hacking with Postgres, we’d love to hear from you. At DBOS, our goal is to make durable workflows as lightweight and easy to work with as possible. Check it out:</p><ul role="list"><li>Quickstart:<a href="https://docs.dbos.dev/quickstart"> https://docs.dbos.dev/quickstart</a>&nbsp;</li><li>GitHub:<a href="https://github.com/dbos-inc"> https://github.com/dbos-inc</a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Efrit: A native elisp coding agent running in Emacs (140 pts)]]></title>
            <link>https://github.com/steveyegge/efrit</link>
            <guid>44840654</guid>
            <pubDate>Fri, 08 Aug 2025 19:20:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/steveyegge/efrit">https://github.com/steveyegge/efrit</a>, See on <a href="https://news.ycombinator.com/item?id=44840654">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Efrit - AI-Powered Emacs Coding Assistant</h2><a id="user-content-efrit---ai-powered-emacs-coding-assistant" aria-label="Permalink: Efrit - AI-Powered Emacs Coding Assistant" href="#efrit---ai-powered-emacs-coding-assistant"></a></p>
<p dir="auto"><em>A sophisticated AI coding agent that leverages Emacs' native programmability through direct Elisp evaluation.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">Efrit is a conversational AI assistant that integrates seamlessly with Emacs, providing multiple interfaces for different types of tasks:</p>
<ul dir="auto">
<li><strong>efrit-chat</strong> - Multi-turn conversational interface for complex discussions and code development</li>
<li><strong>efrit-do</strong> - Natural language command execution for quick tasks</li>
<li><strong>efrit</strong> - Command interface for structured interactions</li>
<li><strong>efrit-agent-run</strong> - Advanced agent loop for multi-step automation</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key Features</h2><a id="user-content-key-features" aria-label="Permalink: Key Features" href="#key-features"></a></p>
<ul dir="auto">
<li><strong>Direct Elisp Evaluation</strong>: Leverages Emacs' native programmability without complex abstractions</li>
<li><strong>Multi-turn Conversations</strong>: Maintains context across multiple exchanges with configurable turn limits</li>
<li><strong>Tool Integration</strong>: Can execute Emacs functions, manipulate buffers, and interact with the environment</li>
<li><strong>Safety-First Design</strong>: Confirmation systems and comprehensive error handling</li>
<li><strong>Dark Theme Friendly</strong>: Adaptive colors that work with any Emacs theme</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites</h3><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li><strong>Emacs</strong>: Version 28.1 or later</li>
<li><strong>Anthropic API Key</strong>: Get yours from <a href="https://console.anthropic.com/" rel="nofollow">Anthropic Console</a></li>
<li><strong>Internet Connection</strong>: Required for Claude API access</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quick Installation</h3><a id="user-content-quick-installation" aria-label="Permalink: Quick Installation" href="#quick-installation"></a></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Clone the repository</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/steveyegge/efrit.git
cd efrit"><pre>git clone https://github.com/steveyegge/efrit.git
<span>cd</span> efrit</pre></div>
</li>
<li>
<p dir="auto"><strong>Add to your Emacs configuration</strong> (<code>~/.emacs.d/init.el</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="(add-to-list 'load-path &quot;/path/to/efrit&quot;)
(require 'efrit)"><pre>(<span>add-to-list</span> <span>'load-path</span> <span><span>"</span>/path/to/efrit<span>"</span></span>)
(<span>require</span> <span>'efrit</span>)</pre></div>
</li>
<li>
<p dir="auto"><strong>Configure your API key</strong> in <code>~/.authinfo</code>:</p>
<div data-snippet-clipboard-copy-content="machine api.anthropic.com login personal password YOUR_API_KEY_HERE"><pre><code>machine api.anthropic.com login personal password YOUR_API_KEY_HERE
</code></pre></div>
</li>
<li>
<p dir="auto"><strong>Restart Emacs</strong> and test with <code>M-x efrit-chat</code></p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Alternative Installation Methods</h3><a id="user-content-alternative-installation-methods" aria-label="Permalink: Alternative Installation Methods" href="#alternative-installation-methods"></a></p>
<p dir="auto"><strong>Emergency Emacs Setup</strong> (for quick testing):</p>
<div dir="auto" data-snippet-clipboard-copy-content="emacs -q -l /path/to/efrit/efrit.el"><pre>emacs -q -l /path/to/efrit/efrit.el</pre></div>
<p dir="auto"><strong>Using straight.el</strong>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="(straight-use-package
 '(efrit :type git :host github :repo &quot;steveyegge/efrit&quot;))
(require 'efrit)"><pre>(<span>straight-use-package</span>
 '(efrit <span>:type</span> git <span>:host</span> github <span>:repo</span> <span><span>"</span>steveyegge/efrit<span>"</span></span>))
(<span>require</span> <span>'efrit</span>)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Available Commands</h3><a id="user-content-available-commands" aria-label="Permalink: Available Commands" href="#available-commands"></a></p>
<ul dir="auto">
<li><strong><code>M-x efrit-chat</code></strong> - Multi-turn conversational interface</li>
<li><strong><code>M-x efrit-do</code></strong> - Natural language command execution</li>
<li><strong><code>M-x efrit</code></strong> - Command interface</li>
<li><strong><code>M-x efrit-agent-run</code></strong> - Advanced agent loop</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Key Bindings</h3><a id="user-content-key-bindings" aria-label="Permalink: Key Bindings" href="#key-bindings"></a></p>
<ul dir="auto">
<li><code>C-c C-e e</code> - efrit-chat</li>
<li><code>C-c C-e d</code> - efrit-do</li>
<li><code>C-c C-e c</code> - efrit command interface</li>
<li><code>C-c C-e a</code> - efrit-agent-run</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Examples</h3><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Visual Demonstrations</h4><a id="user-content-visual-demonstrations" aria-label="Permalink: Visual Demonstrations" href="#visual-demonstrations"></a></p>
<p dir="auto"><strong>Multi-Buffer Creation with efrit-do</strong></p>
<p dir="auto">Starting with a simple request:</p>
<div data-snippet-clipboard-copy-content="M-x efrit-do
> write an ode in one buffer, and a sonnet in another, both about Vim"><pre><code>M-x efrit-do
&gt; write an ode in one buffer, and a sonnet in another, both about Vim
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/steveyegge/efrit/blob/main/ode-sonnet-vim.jpg"><img src="https://github.com/steveyegge/efrit/raw/main/ode-sonnet-vim.jpg" alt="Initial ode and sonnet about Vim"></a></p>
<p dir="auto"><strong>Conversational Context - Making Modifications</strong></p>
<p dir="auto">efrit-do maintains context, so you can refine previous work:</p>
<div data-snippet-clipboard-copy-content="M-x efrit-do  
> Can you make them more snarky?"><pre><code>M-x efrit-do  
&gt; Can you make them more snarky?
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/steveyegge/efrit/blob/main/ode-sonnet-snarky.jpg"><img src="https://github.com/steveyegge/efrit/raw/main/ode-sonnet-snarky.jpg" alt="Snarky versions of the ode and sonnet"></a></p>
<p dir="auto">This demonstrates efrit-do's key feature: <strong>conversational continuity</strong>. It remembers what it just created and can modify, improve, or completely rewrite previous work based on your feedback.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">More Usage Examples</h4><a id="user-content-more-usage-examples" aria-label="Permalink: More Usage Examples" href="#more-usage-examples"></a></p>
<p dir="auto"><strong>Conversational Development</strong>:</p>
<div data-snippet-clipboard-copy-content="M-x efrit-chat
> Can you help me write a function to count lines in the current buffer?
> Now modify it to exclude empty lines and comments"><pre><code>M-x efrit-chat
&gt; Can you help me write a function to count lines in the current buffer?
&gt; Now modify it to exclude empty lines and comments
</code></pre></div>
<p dir="auto"><strong>Quick Commands</strong>:</p>
<div data-snippet-clipboard-copy-content="M-x efrit-do
> open the scratch buffer and insert &quot;hello world&quot;
> find all TODO comments in the current project"><pre><code>M-x efrit-do
&gt; open the scratch buffer and insert "hello world"
&gt; find all TODO comments in the current project
</code></pre></div>
<p dir="auto"><strong>Multi-step Tasks</strong>:</p>
<div data-snippet-clipboard-copy-content="M-x efrit-chat
> Create a haiku in one buffer and a limerick in another buffer"><pre><code>M-x efrit-chat
&gt; Create a haiku in one buffer and a limerick in another buffer
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Configuration</h3><a id="user-content-basic-configuration" aria-label="Permalink: Basic Configuration" href="#basic-configuration"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Model and token settings
(setq efrit-model &quot;claude-3-5-sonnet-20241022&quot;)
(setq efrit-max-tokens 8192)

;; Multi-turn conversation settings
(setq efrit-multi-turn-enabled t)
(setq efrit-multi-turn-simple-max-turns 3)

;; efrit-do buffer behavior
(setq efrit-do-show-errors-only t)  ; Only show buffer on errors

;; Debug settings (optional)
(setq efrit-debug-enabled nil)"><pre><span><span>;</span>; Model and token settings</span>
(<span>setq</span> efrit-model <span><span>"</span>claude-3-5-sonnet-20241022<span>"</span></span>)
(<span>setq</span> efrit-max-tokens <span>8192</span>)

<span><span>;</span>; Multi-turn conversation settings</span>
(<span>setq</span> efrit-multi-turn-enabled <span>t</span>)
(<span>setq</span> efrit-multi-turn-simple-max-turns <span>3</span>)

<span><span>;</span>; efrit-do buffer behavior</span>
(<span>setq</span> efrit-do-show-errors-only <span>t</span>)  <span><span>;</span> Only show buffer on errors</span>

<span><span>;</span>; Debug settings (optional)</span>
(<span>setq</span> efrit-debug-enabled <span>nil</span>)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Advanced Configuration</h3><a id="user-content-advanced-configuration" aria-label="Permalink: Advanced Configuration" href="#advanced-configuration"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Timeouts and API settings
(setq efrit-multi-turn-timeout 300)
(setq efrit-api-timeout 30)

;; Custom key bindings
(global-set-key (kbd &quot;C-c a&quot;) 'efrit-chat)
(global-set-key (kbd &quot;C-c d&quot;) 'efrit-do)"><pre><span><span>;</span>; Timeouts and API settings</span>
(<span>setq</span> efrit-multi-turn-timeout <span>300</span>)
(<span>setq</span> efrit-api-timeout <span>30</span>)

<span><span>;</span>; Custom key bindings</span>
(<span>global-set-key</span> (<span>kbd</span> <span><span>"</span>C-c a<span>"</span></span>) <span>'efrit-chat</span>)
(<span>global-set-key</span> (<span>kbd</span> <span><span>"</span>C-c d<span>"</span></span>) <span>'efrit-do</span>)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Core Philosophy</h3><a id="user-content-core-philosophy" aria-label="Permalink: Core Philosophy" href="#core-philosophy"></a></p>
<p dir="auto">Efrit follows the principle of <strong>Elisp-centricity</strong>: rather than building complex tool abstractions, it provides the AI with direct access to Emacs' powerful Elisp evaluation capabilities. This approach offers unlimited flexibility while staying within Emacs' natural paradigm.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Core Components</h3><a id="user-content-core-components" aria-label="Permalink: Core Components" href="#core-components"></a></p>
<ul dir="auto">
<li><strong>efrit.el</strong> - Main entry point and package coordination</li>
<li><strong>efrit-chat.el</strong> - Multi-turn conversational interface with Claude API</li>
<li><strong>efrit-do.el</strong> - Natural language command interface</li>
<li><strong>efrit-multi-turn.el</strong> - Multi-turn conversation state management</li>
<li><strong>efrit-tools.el</strong> - Core functionality engine with Elisp evaluation</li>
<li><strong>efrit-debug.el</strong> - Optional debugging and logging system</li>
<li><strong>efrit-agent.el</strong> - Advanced agent loop for complex automation</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Common Issues</h3><a id="user-content-common-issues" aria-label="Permalink: Common Issues" href="#common-issues"></a></p>
<p dir="auto"><strong>"Cannot open load file: efrit"</strong></p>
<ul dir="auto">
<li>Verify the path in your <code>load-path</code> is correct</li>
<li>Ensure <code>efrit.el</code> exists in that directory</li>
</ul>
<p dir="auto"><strong>"API key not found"</strong></p>
<ul dir="auto">
<li>Check <code>~/.authinfo</code> file exists and has correct format</li>
<li>Test with: <code>M-x auth-source-search RET machine api.anthropic.com RET</code></li>
</ul>
<p dir="auto"><strong>Connection timeout</strong></p>
<ul dir="auto">
<li>Check internet connection and API key validity</li>
<li>Try increasing <code>efrit-api-timeout</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Debug Mode</h3><a id="user-content-debug-mode" aria-label="Permalink: Debug Mode" href="#debug-mode"></a></p>
<p dir="auto">Enable debug logging for troubleshooting:</p>
<div dir="auto" data-snippet-clipboard-copy-content="(setq efrit-debug-enabled t)"><pre>(<span>setq</span> efrit-debug-enabled <span>t</span>)</pre></div>
<p dir="auto">Then check: <code>M-x efrit-debug-show</code></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building and Testing</h3><a id="user-content-building-and-testing" aria-label="Permalink: Building and Testing" href="#building-and-testing"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build
make compile

# Run tests
make test

# Install system-wide
make install"><pre><span><span>#</span> Build</span>
make compile

<span><span>#</span> Run tests</span>
make <span>test</span>

<span><span>#</span> Install system-wide</span>
make install</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contributing</h3><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">See <a href="https://github.com/steveyegge/efrit/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a> for detailed guidelines on:</p>
<ul dir="auto">
<li>Development setup</li>
<li>Code standards and conventions</li>
<li>Testing procedures</li>
<li>Submitting changes</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Version History</h2><a id="user-content-version-history" aria-label="Permalink: Version History" href="#version-history"></a></p>
<p dir="auto"><strong>v0.2.0</strong> (2025-01-07) - Major Stability Release</p>
<ul dir="auto">
<li>✅ Fixed API integration issues and HTTP 400 errors</li>
<li>✅ Enhanced token limits (1024 → 8192 tokens)</li>
<li>✅ Improved message ordering and dark theme compatibility</li>
<li>✅ Added multi-turn conversation system with configurable limits</li>
<li>✅ Consolidated documentation and cleaned up codebase</li>
<li>✅ Production-ready with comprehensive error handling</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Licensed under the Apache License, Version 2.0. See <a href="https://github.com/steveyegge/efrit/blob/main/LICENSE">LICENSE</a> for details.</p>
<hr>
<p dir="auto"><em>Efrit: Where AI meets the power of Emacs.</em></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jim Lovell, Apollo 13 commander, has died (552 pts)]]></title>
            <link>https://www.nasa.gov/news-release/acting-nasa-administrator-reflects-on-legacy-of-astronaut-jim-lovell/</link>
            <guid>44840582</guid>
            <pubDate>Fri, 08 Aug 2025 19:12:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nasa.gov/news-release/acting-nasa-administrator-reflects-on-legacy-of-astronaut-jim-lovell/">https://www.nasa.gov/news-release/acting-nasa-administrator-reflects-on-legacy-of-astronaut-jim-lovell/</a>, See on <a href="https://news.ycombinator.com/item?id=44840582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>The following is a statement from acting NASA Administrator Sean Duffy on the passing of famed Apollo astronaut Jim Lovell. He passed away Aug. 7, in Lake Forest, Illinois. He was 97 years old.</p>
<p>“NASA sends its condolences to the family of Capt. Jim Lovell, whose life and work inspired millions of people across the decades. Jim’s character and steadfast courage helped our nation reach the Moon and turned a potential tragedy into a success from which we learned an enormous amount. We mourn his passing even as we celebrate his achievements.</p>
<p>“From a pair of pioneering Gemini missions to the successes of Apollo, Jim helped our nation forge a historic path in space that carries us forward to upcoming Artemis missions to the Moon and beyond.</p>
<p>“As the Command Module Pilot for Apollo 8, Jim and his crewmates became the first to lift off on a Saturn V rocket and orbit the Moon, proving that the lunar landing was within our reach. As commander of the Apollo 13 mission, his calm strength under pressure helped return the crew safely to Earth and demonstrated the quick thinking and innovation that informed future NASA missions.</p>
<p>“Known for his wit, this unforgettable astronaut was nicknamed Smilin’ Jim by his fellow astronauts because he was quick with a grin when he had a particularly funny comeback.</p>
<p>“Jim also served our country in the military, and the Navy has lost a proud academy graduate and test pilot. Jim Lovell embodied the bold resolve and optimism of both past and future explorers, and we will remember him always.”</p>
<p>For more information about Lovell’s NASA career, and his agency biography, visit:</p>
<p><a href="https://www.nasa.gov/former-astronaut-james-a-lovell"><strong>https://www.nasa.gov/former-astronaut-james-a-lovell</strong></a></p>
<p>-end-</p>
<p>Grace Bartlinski / Cheryl Warner<br>Headquarters, Washington<br>202-358-1600<br><a href="mailto:grace.bartlinksi@nasa.gov">grace.bartlinksi@nasa.gov</a> / <a href="mailto:cheryl.m.warner@nasa.gov">cheryl.m.warner@nasa.gov</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[M5 MacBook Pro No Longer Coming in 2025 (106 pts)]]></title>
            <link>https://www.macrumors.com/2025/07/10/no-m5-macbook-pro-2025/</link>
            <guid>44840281</guid>
            <pubDate>Fri, 08 Aug 2025 18:43:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2025/07/10/no-m5-macbook-pro-2025/">https://www.macrumors.com/2025/07/10/no-m5-macbook-pro-2025/</a>, See on <a href="https://news.ycombinator.com/item?id=44840281">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2025/07/10/no-m5-macbook-pro-2025/"><p>Apple does not plan to refresh any Macs with updated M5 chips in 2025, according to <em><a href="https://www.bloomberg.com/news/articles/2025-07-10/apple-plans-new-macbook-pro-iphone-17e-and-ipads-by-early-2026">Bloomberg</a></em>'s <a href="https://www.macrumors.com/guide/mark-gurman/">Mark Gurman</a>. Updated <a href="https://www.macrumors.com/roundup/macbook-air/">MacBook Air</a> and <a href="https://www.macrumors.com/roundup/macbook-pro/">MacBook Pro</a> models are now planned for the first half of 2026.</p>
<p><img src="https://images.macrumors.com/t/vy3QHefJFR7xsmS0U55hnHufPQ4=/400x0/article-new/2024/08/macbook-pro-blue-green.jpg?lossy" srcset="https://images.macrumors.com/t/vy3QHefJFR7xsmS0U55hnHufPQ4=/400x0/article-new/2024/08/macbook-pro-blue-green.jpg?lossy 400w,https://images.macrumors.com/t/PxiXq-dLu0UCf97Sh_kQvsYMPnQ=/800x0/article-new/2024/08/macbook-pro-blue-green.jpg?lossy 800w,https://images.macrumors.com/t/NYXXBItdSxPZv0difK-ijSNGi58=/1600x0/article-new/2024/08/macbook-pro-blue-green.jpg 1600w,https://images.macrumors.com/t/Q5Z7-K6FP-0AquOer4ECYfk0XV0=/2500x0/filters:no_upscale()/article-new/2024/08/macbook-pro-blue-green.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="macbook pro blue green" width="2500" height="1406"><br>Gurman previously said that Apple would debut the M5 ‌MacBook Pro‌ models in late 2025, but his newest report suggests that Apple is "considering" pushing them back to 2026. Apple is now said to be "internally targeting" a launch early next year.</p>
<p>The current M4, M4 Pro, and M4 Max ‌MacBook Pro‌ models were announced in October 2024 and released in November 2024, so pushing the M5 models back to 2026 would see Apple skipping a yearly refresh. It is typical for new Macs to come out in October or November after the September <a href="https://www.macrumors.com/guide/iphone/">iPhone</a> event.</p>
<p>Gurman does not give a reason why Apple is potentially "delaying" the launch of the M5 ‌MacBook Pro‌ models, but he says the timing is fluid, so there may still be a chance that we get the new Macs before the end of the year.</p>
<p>The M5 ‌MacBook Pro‌ models will have few changes beyond the M5 chip update, because Apple is planning for bigger changes in for the M6 ‌MacBook Pro‌. The next ‌MacBook Pro‌ models are expected to transition to OLED displays and new case designs. Rumors have suggested the OLED ‌MacBook Pro‌ would come in 2026, but if Apple is planning to launch the M5 ‌MacBook Pro‌ models in 2026, that might mean the OLED model will be pushed to 2027. Alternatively, Apple could debut the M5 ‌MacBook Pro‌ in early 2026 and the OLED version in late 2026, but that would be unusual.</p>
<p>Apple is also planning to release M5 ‌MacBook Air‌ models in 2026, which will replace the current M4 models. Other rumors suggest Apple is working on a MacBook that has an A18 Pro chip in it for 2026, but Gurman didn't mention it. </p>
<p>The M5 ‌MacBook Pro‌ and ‌MacBook Air‌ models could be accompanied by a new display that Apple has in the works. Apple is developing an external monitor that is expected to be a follow up to the 2022 Studio Display. It is expected to launch in early 2026.</p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2025/08/05/iphone-17-pro-launching-next-month/">iPhone 17 Pro Launching Next Month With These 12 New Features</a></h3><p>The calendar has turned to August, and that means the iPhone 17 series is just one month away. Apple has yet to officially announce an event, but it has been rumored that the devices will be announced on Tuesday, September 9.
Subscribe to the MacRumors YouTube channel for more videos. 
Below is the August 2025 edition of our iPhone 17 Pro rumor recap, for an up-to-date overview of what to...</p></div><div><h3><a href="https://www.macrumors.com/2025/08/05/airpods-pro-3-weeks-away-what-we-know/">AirPods Pro 3 Could Be Just Weeks Away – Here's What We Know</a></h3><p>Tuesday August 5, 2025 2:03 am PDT by <a href="https://www.macrumors.com/author/tim-hardwick/" rel="author">Tim Hardwick</a></p><p>Despite being over two years old, Apple's AirPods&nbsp;Pro&nbsp;2 still dominate the premium wireless‑earbud space, thanks to a potent mix of top‑tier audio, class‑leading noise cancellation, and Apple's habit of delivering major new features through software updates. Rumors suggest AirPods Pro 3 could arrive as soon as September 2025 alongside the iPhone 17 lineup, giving prospective AirPods...</p></div><div><h3><a href="https://www.macrumors.com/2025/08/05/apple-watch-ultra-3-display-size/">iOS 26 Beta Reveals Apple Watch Ultra 3 Display Size</a></h3><p>Tuesday August 5, 2025 11:21 am PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>The latest iOS 26 beta includes imagery that confirms Apple's work on a new version of the Apple Watch Ultra, which is set to come out this fall. MacRumors contributor Aaron Perris found an Apple Watch image with a resolution that does not correspond to any current Apple Watch models. 
The image suggests that the upcoming Apple Watch Ultra 3 could have a slightly larger display size, with a...</p></div><div><h3><a href="https://www.macrumors.com/2025/08/05/new-apple-tv-still-launching-this-year/">New Apple TV Still Launching This Year</a></h3><p>Apple is still on track to release a new Apple TV model later this year, according to a reliable source speaking to MacRumors.
According to a source familiar with the company's plans, Apple is highly likely to replace the current Apple TV 4K with a new model later this year. The current model will be discontinued. 
Today's Apple TV 4K came out in 2022, featuring the A15 Bionic chip,...</p></div><div><h3><a href="https://www.macrumors.com/2025/08/01/ios-26-adaptive-power-mode/">iOS 26's New Battery Life Mode is Limited to These iPhone Models</a></h3><p>iOS 26 introduces an Adaptive Power Mode on the iPhone, alongside the existing Low Power Mode.
Apple says that Adaptive Power Mode can make "small performance adjustments" when necessary to extend an iPhone's battery life, including slightly lowering the display brightness or allowing some activities to "take a little longer."
The full description of Adaptive Power Mode, from the iOS 26...</p></div><div><h3><a href="https://www.macrumors.com/2025/08/05/ios-26-beta-5-features/">Everything New in iOS 26 Beta 5</a></h3><p>Tuesday August 5, 2025 2:20 pm PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>Apple seeded the fifth developer beta of iOS 26 today, and while the number of significant changes has dropped, there are quite a few smaller tweaks. Apple is continuing to refine button placement, animations, and design in preparation for launching iOS 26 in September.
Camera
Apple added a toggle in the Camera app to allow users to toggle on Classic Mode, a setting that reverses the scroll ...</p></div><div><h3><a href="https://www.macrumors.com/2025/08/05/magsafe-charger-firmware-update-2a168/">Apple Releases Updated MagSafe Charger Firmware</a></h3><p>Tuesday August 5, 2025 12:20 pm PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>Apple today released a firmware update for the 25W MagSafe Charger that is compatible with the iPhone 12 and later and the latest AirPods. The new firmware is version 2A168, up from the 2A146 firmware that came out last year. In the Settings app, the new firmware is version 148, up from 136.
Apple introduced the 2024 MagSafe charger alongside the iPhone 16 models back in September, and it is ...</p></div><div><h3><a href="https://www.macrumors.com/2025/08/06/airpods-charging-ios-26/">Apple Upgrades AirPods Charging in iOS 26</a></h3><p>Wednesday August 6, 2025 1:10 pm PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>In the fifth beta of iOS 26, Apple appears to have subtly upgraded AirPods charging. Code in iOS 26 says that the AirPods Charging case "now more clearly indicates charging status," and that the AirPods will remind you when it's time to charge.
A screenshot shared on social media shows an AirPods splash screen with the same wording and an image of what the light on the AirPods charging case...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I want everything local – Building my offline AI workspace (902 pts)]]></title>
            <link>https://instavm.io/blog/building-my-offline-ai-workspace</link>
            <guid>44840013</guid>
            <pubDate>Fri, 08 Aug 2025 18:19:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://instavm.io/blog/building-my-offline-ai-workspace">https://instavm.io/blog/building-my-offline-ai-workspace</a>, See on <a href="https://news.ycombinator.com/item?id=44840013">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><blockquote>
<p>I want everything local — no cloud, no remote code execution.</p>
</blockquote>
<p>That’s what a friend said. That one-line requirement, albeit simple, would need multiple things to work in tandem to make it happen.</p>
<p>What does a mainstream LLM (Large Language Model) chat app like ChatGPT or Claude provide at a high level?</p>
<ul>
<li>Ability to use chat with a cloud hosted LLM,</li>
<li>Ability to run code generated by them mostly on their cloud infra, sometimes locally via shell,</li>
<li>Ability to access the internet for new content or services.</li>
</ul>
<p>With so many LLMs being open source / open weights, shouldn't it be possible to do all that locally? But just local LLM is not enough, we need a truely isolated environment to run code as well.</p>
<p>So, LLM for chat, Docker to containerize code execution, and finally a browser access of some sort for content.</p>
<hr>
<h2>🧠 The Idea</h2>
<p>We wanted a system where:</p>
<ul>
<li>LLMs run completely <strong>locally</strong></li>
<li><strong>Code executes inside a lightweight VM</strong>, not on the host machine</li>
<li>Bonus: <strong>headless browser</strong> for automation and internet access</li>
</ul>
<p><img src="https://instavm.io/blog-images/Pasted%20image%2020250730003357.png"></p>
<p>The idea was to perform tasks which require privacy to be executed completely locally, starting from planning via LLM to code execution inside a container. For instance, if you wanted to edit your photos or videos, how could you do it without giving your data to OpenAI/Google/Anthropic? Though they take security seriously (more than many), it's just a matter of one slip leading to your private data being compromised, a case in point being the early days of ChatGPT when user chats were <a href="https://www.bloomberg.com/news/articles/2023-03-21/openai-shut-down-chatgpt-to-fix-bug-exposing-user-chat-titles">accessible</a> from another's account!</p>
<hr>
<h2>The Stack We Used</h2>
<ul>
<li><strong>LLMs</strong>: <a href="https://ollama.com/">Ollama</a> for local models (also private models for now)</li>
<li><strong>Frontend UI</strong>: <a href="https://github.com/assistant-ai/assistant-ui"><code>assistant-ui</code></a></li>
<li><strong>Sandboxed VM Runtime</strong>: <a href="https://github.com/apple/container"><code>container</code></a> by Apple</li>
<li><strong>Orchestration</strong>: <a href="https://github.com/instavm/coderunner"><code>coderunner</code></a></li>
<li><strong>Browser Automation</strong>: <a href="https://playwright.dev/">Playwright</a></li>
</ul>
<blockquote>
<p>💡 We ran this entirely on Apple Silicon, using <code>container</code> for isolation.</p>
</blockquote>
<h3>🛠️  Our Attempt at a Mac App</h3>
<p>We started with zealous ambition: make it feel native. We tried using <code>a0.dev</code>, hoping it could help generate a Mac app. But it appears to be meant more for iOS app development — and getting it to work for MacOS was painful, to say the least.</p>
<p>Even with help from the "world's best" LLMs, things didn't go quite as smoothly as we had expected. They hallucinated steps, missed platform-specific quirks, and often left us worse off.</p>
<p>Then we tried wrapping a <code>NextJS</code> app inside Electron. It took us longer than we'd like to admit. As of this writing, it looks like there's just no (clean) way to do it.</p>
<p>So, we gave up on the Mac app. The local web version of <code>assistant-ui</code> was good enough — simple, configurable, and didn't fight back.</p>
<p><img src="https://instavm.io/blog-images/Pasted%20image%2020250729142902.png"></p>
<h3>Assistant UI</h3>
<p>We thought <code>Assistant-UI</code> provided multiple LLM support out-of-the-box, as their landing page shows a drop-down of models. But, no.
So, we had to look for examples on how to go about it, and <code>ai-sdk</code> appeared to be the popular choice.
Finally we had a dropdown for model selection. We decided not to restrict the set to just local models, as smaller local models are not quite there just yet. Users can get familiar with the tool and its capabilities, and later as small local models become better, they can just switch to being completely local.
<img src="https://instavm.io/blog-images/Pasted%20image%2020250729225206.png"></p>
<h3>Tool-calling</h3>
<p>Our use-case also required us to have models that support tool-calling. While some models do, Ollama has not implemented the tool support for them. For instance:</p>
<pre><code>responseBody: '{"error":"registry.ollama.ai/library/deepseek-r1:8b does not support tools"}',
</code></pre>
<p>And to add to the confusion, Ollama has decided to put this model under tool calling category on their site. Understandably, with the fast-moving AI landscape, it can be difficult for community driven projects to keep up.</p>
<p>At the moment, essential information like whether a model has tool-support or not, pricing per token, for various models are so fickle. A model's official page mentions tool-support but then tools like Ollama take a while to implement them. Anyway, we shouldn't complain - it's open source, we could've contributed.</p>
<h3>Containerized execution</h3>
<p>After the UI was MVP-level sorted, we moved on to the isolated VM part. Recently Apple released a tool called 'Container'. Yes, that's right. So, we checked it out and it seemed better than Docker as it provided one isolated VM per container - a perfect fit for running AI generated code.
So, we deployed a Jupyter server in the VM, exposed it as MCP (Model Context Protocol) tool, and made it available at <code>http://coderunner.local:8222/mcp</code>.</p>
<p>The advantage of MCPing vs a exposing an API is that existing tools that work with MCPs can use this right away. For instance, Claude Desktop and Gemini CLI can start executing AI-generated code with a simple config.</p>
<pre><code>"mcpServers": {
    "coderunner": {
      "httpUrl": "http://coderunner.local:8222/mcp"
    }
}
</code></pre>
<p>As you can see below, Claude figured out it should use the tool <code>execute_python_code</code> exposed from our isolated VM via the MCP endpoint.
<img src="https://instavm.io/blog-images/Pasted%20image%2020250730135012.png">
Aside - if you want to just use the <code>coderunner</code> bit as an MCP to execute code with your existing tools, the code for <code>coderunner</code> is <a href="https://github.com/instavm/coderunner">public</a>.</p>
<blockquote>
<h4>Apple container</h4>
<p>A tangent - if you're planning to work with Apple <code>container</code> and building VM images using it, have an abundance of patience. The build keeps failing with <code>Trap</code> error or just hangs without any output. To continue, you should <code>pkill</code> all container processes and restart the <code>container</code> tool. Then remove the <code>buildkit</code> image so that the next <code>build</code> process fetches a fresh one.
And repeat the three steps till it successfully works; this can take hours. We are excited to see Apple container mature as it moves beyond its early stages.</p>
</blockquote>
<p>Back to our app, we tested the <code>UI + LLMs + CodeRunner</code> on a task to <code>edit a video</code> and it worked!</p>
<p><img src="https://instavm.io/blog-images/WhatsApp%20Image%202025-07-11%20at%2016.50.13.jpeg">
		<em>I asked it to address me as Lord Voldemort as a sanity check for system instructions</em></p>
<p>After the coderunner was verified to be working, we decided to add the support of a headless browser. The main reason was to allow the app to look for new/updated tools/information online, for example, browsing github to find installation instruction for some tool it doesn't yet know about. Another reason was laying the foundation for <code>research</code>.
We chose Playwright for the task. We deployed it in the same container and exposed it as an MCP tool. Here is one task we asked it to do -</p>
<p><img src="https://instavm.io/blog-images/WhatsApp%20Image%202025-07-25%20at%2014.10.38.jpeg"></p>
<p>With this our basic set up was ready: <strong>Local LLM + Sandboxed arbitrary code execution + Headless browser</strong> for latest information.</p>
<hr>
<h2>What It Can Do (Examples)</h2>
<ol>
<li><strong>Do research on a topic</strong></li>
<li><strong>Generate and render charts</strong> from CSV using plain English</li>
<li><strong>Edit videos</strong> (via <code>ffmpeg</code>) — e.g., “cut between 0:10 and 1:00”</li>
<li><strong>Edit images</strong> — resize, crop, convert formats</li>
<li><strong>Install tools from GitHub</strong> in a containerized space</li>
<li><strong>Use a headless browser</strong> to fetch pages and summarize content etc.</li>
</ol>
<hr>
<h2>Volumes and Isolation</h2>
<p>We mapped a volume from
<code>~/.coderunner/assets</code> (host)
to
<code>/app/uploads</code> (container)</p>
<p>So files edited/generated stay in a safe shared space, <strong>but code never touches the host system</strong>.</p>
<hr>
<h2>Limitations &amp; Next Steps</h2>
<ul>
<li>Currently <strong>only works on Apple Silicon</strong> (macOS 26 is optional)</li>
<li>Needs better UI for managing tools and output streaming</li>
<li>Headless browser is classified as bot by various sites</li>
</ul>
<hr>
<h2>Final Thoughts</h2>
<p>This is more than a just an experiment. It's a philosophy shift <strong>bringing compute and agency back to your machine</strong>. No cloud dependency. No privacy tradeoffs. While the best models will probably be always with the giants, we hope that we will still have local tools which can get our day-to-day work done with the privacy we deserve.</p>
<blockquote>
<p>We didn't just imagine it. We built it. And now, you can use it too.</p>
<p>Check out <code>coderunner-ui</code> on <a href="https://github.com/instavm/coderunner-ui">Github</a> to get started, and let us know what you think. We welcome feedback, issues and contributions.</p>
</blockquote>
<hr>
<h2>🔗 Resources</h2>
<ul>
<li><a href="https://github.com/assistant-ai/assistant-ui">assistant-ui</a></li>
<li><a href="https://github.com/instavm/coderunner">instavm/coderunner</a></li>
<li><a href="https://github.com/apple/container">Apple/container</a></li>
<li><a href="https://github.com/instavm/coderunner-ui">instavm/coderunner-ui</a></li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The surprise deprecation of GPT-4o for ChatGPT consumers (385 pts)]]></title>
            <link>https://simonwillison.net/2025/Aug/8/surprise-deprecation-of-gpt-4o/</link>
            <guid>44839842</guid>
            <pubDate>Fri, 08 Aug 2025 18:04:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/Aug/8/surprise-deprecation-of-gpt-4o/">https://simonwillison.net/2025/Aug/8/surprise-deprecation-of-gpt-4o/</a>, See on <a href="https://news.ycombinator.com/item?id=44839842">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-permalink-context="/2025/Aug/8/surprise-deprecation-of-gpt-4o/">

<p>8th August 2025</p>



<p>I’ve been dipping into the <a href="https://reddit.com/r/chatgpt">r/ChatGPT</a> subreddit recently to see how people are reacting to <a href="https://simonwillison.net/2025/Aug/7/gpt-5/">the GPT-5 launch</a>, and so far the vibes there are not good. <a href="https://www.reddit.com/r/ChatGPT/comments/1mkae1l/gpt5_ama_with_openais_sam_altman_and_some_of_the/">This AMA thread</a> with the OpenAI team is a great illustration of the single biggest complaint: a lot of people are <em>very</em> unhappy to lose access to the much older GPT-4o, previously ChatGPT’s default model for most users.</p>
<p>A big surprise for me yesterday was that OpenAI simultaneously retired access to their older models as they rolled out GPT-5, at least in their consumer apps. Here’s a snippet from <a href="https://help.openai.com/en/articles/6825453-chatgpt-release-notes">their August 7th 2025 release notes</a>:</p>
<blockquote>
<p>When GPT-5 launches, several older models will be retired, including GPT-4o, GPT-4.1, GPT-4.5, GPT-4.1-mini, o4-mini, o4-mini-high, o3, o3-pro.</p>
<p>If you open a conversation that used one of these models, ChatGPT will automatically switch it to the closest GPT-5 equivalent. Chats with 4o, 4.1, 4.5, 4.1-mini, o4-mini, or o4-mini-high will open in GPT-5, chats with o3 will open in GPT-5-Thinking, and chats with o3-Pro will open in GPT-5-Pro (available only on Pro and Team).</p>
</blockquote>
<p>There’s no deprecation period at all: when your consumer ChatGPT account gets GPT-5, those older models cease to be available.</p>

<p id="sama"><strong>Update 12pm Pacific Time</strong>: Sam Altman on Reddit <a href="https://www.reddit.com/r/ChatGPT/comments/1mkae1l/comment/n7nelhh/">six minutes ago</a>:</p>
<blockquote>
<p>ok, we hear you all on 4o; thanks for the time to give us the feedback (and the passion!). we are going to bring it back for plus users, and will watch usage to determine how long to support it.</p>
</blockquote>
<p>See also <a href="https://x.com/sama/status/1953893841381273969">Sam’s tweet</a> about updates to the GPT-5 rollout.</p>

<p>Rest of my original post continues below:</p>

<hr>

<p>(This only affects ChatGPT consumers—the API still provides the old models, their <a href="https://platform.openai.com/docs/deprecations">deprecation policies are published here</a>.)</p>
<p>One of the expressed goals for GPT-5 was to escape the terrible UX of the model picker. Asking users to pick between GPT-4o and o3 and o4-mini was a notoriously bad UX, and resulted in many users sticking with that default 4o model—now a year old—and hence not being exposed to the advances in model capabilities over the last twelve months.</p>
<p>GPT-5’s solution is to automatically pick the underlying model based on the prompt. On paper this sounds great—users don’t have to think about models any more, and should get upgraded to the best available model depending on the complexity of their question.</p>
<p>I’m already getting the sense that this is <strong>not</strong> a welcome approach for power users. It makes responses much less predictable as the model selection can have a dramatic impact on what comes back.</p>
<p>Paid tier users can select “GPT-5 Thinking” directly. Ethan Mollick is <a href="https://www.oneusefulthing.org/p/gpt-5-it-just-does-stuff">already recommending deliberately selecting the Thinking mode</a> if you have the ability to do so, or trying prompt additions like “think harder” to increase the chance of being routed to it.</p>
<p>But back to GPT-4o. Why do many people on Reddit care so much about losing access to that crusty old model? I think <a href="https://www.reddit.com/r/ChatGPT/comments/1mkae1l/comment/n7js2sf/">this comment</a> captures something important here:</p>
<blockquote>
<p>I know GPT-5 is designed to be stronger for complex reasoning, coding, and professional tasks, but <strong>not all of us need a pro coding model</strong>. Some of us rely on 4o for creative collaboration, emotional nuance, roleplay, and other long-form, high-context interactions. Those areas feel different enough in GPT-5 that it impacts my ability to work and create the way I’m used to.</p>
</blockquote>
<p>What a fascinating insight into the wildly different styles of LLM-usage that exist in the world today! With <a href="https://simonwillison.net/2025/Aug/4/nick-turley/">700M weekly active users</a> the variety of usage styles out there is incomprehensibly large.</p>
<p>Personally I mainly use ChatGPT for research, coding assistance, drawing pelicans and foolish experiments. <em>Emotional nuance</em> is not a characteristic I would know how to test!</p>
<p>Professor Casey Fiesler <a href="https://www.tiktok.com/@professorcasey/video/7536223372485709086">on TikTok</a> highlighted OpenAI’s post from last week <a href="https://openai.com/index/how-we%27re-optimizing-chatgpt/">What we’re optimizing ChatGPT for</a>, which includes the following:</p>
<blockquote>
<p>ChatGPT is trained to respond with grounded honesty. There have been instances where our 4o model fell short in recognizing signs of delusion or emotional dependency. […]</p>
<p>When you ask something like “Should I break up with my boyfriend?” ChatGPT shouldn’t give you an answer. It should help you think it through—asking questions, weighing pros and cons. New behavior for high-stakes personal decisions is rolling out soon.</p>
</blockquote>
<p>Casey points out that this is an ethically complicated issue. On the one hand ChatGPT should be much more careful about how it responds to these kinds of questions. But if you’re already leaning on the model for life advice like this, having that capability taken away from you without warning could represent a sudden and unpleasant loss!</p>
<p>It’s too early to tell how this will shake out. Maybe OpenAI will extend a deprecation period for GPT-4o in their consumer apps?</p>
<p><em><strong>Update</strong>: That’s exactly what they’ve done, see <a href="https://simonwillison.net/2025/Aug/8/surprise-deprecation-of-gpt-4o/#sama">update above</a>.</em></p>
<p>GPT-4o remains available via the API, and there are no announced plans to deprecate it there. It’s possible we may see a small but determined rush of ChatGPT users to alternative third party chat platforms that use that API under the hood.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A message from Intel CEO Lip-Bu Tan to all company employees (157 pts)]]></title>
            <link>https://newsroom.intel.com/corporate/my-commitment-to-you-and-our-company</link>
            <guid>44839705</guid>
            <pubDate>Fri, 08 Aug 2025 17:48:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsroom.intel.com/corporate/my-commitment-to-you-and-our-company">https://newsroom.intel.com/corporate/my-commitment-to-you-and-our-company</a>, See on <a href="https://news.ycombinator.com/item?id=44839705">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
    <main id="main" tabindex="-1">
        
<article id="post-7242" aria-label="My commitment to you and our company">
	<div>

			

			                <div data-url="">

                        <!-- <button id="imageToggle" aria-pressed="false">
                            <span id="hideImage">Hide</span><span id="showImage" class="hidden">Show</span> Image
                        </button> -->

                        

                        
                            <p><img width="1200" height="675" src="https://newsroom.intel.com/wp-content/uploads/2025/03/intel-bio-headshot-lip-bu-tan-1200x675.jpg" alt="A smiling man with glasses and short hair wearing a dark suit and white shirt, set against a gray background with pixelated squares." decoding="async" fetchpriority="high" srcset="https://newsroom.intel.com/wp-content/uploads/2025/03/intel-bio-headshot-lip-bu-tan-1200x675.jpg 1200w, https://newsroom.intel.com/wp-content/uploads/2025/03/intel-bio-headshot-lip-bu-tan-300x169.jpg 300w, https://newsroom.intel.com/wp-content/uploads/2025/03/intel-bio-headshot-lip-bu-tan-1024x576.jpg 1024w, https://newsroom.intel.com/wp-content/uploads/2025/03/intel-bio-headshot-lip-bu-tan-768x432.jpg 768w, https://newsroom.intel.com/wp-content/uploads/2025/03/intel-bio-headshot-lip-bu-tan-1536x864.jpg 1536w, https://newsroom.intel.com/wp-content/uploads/2025/03/intel-bio-headshot-lip-bu-tan-1400x788.jpg 1400w, https://newsroom.intel.com/wp-content/uploads/2025/03/intel-bio-headshot-lip-bu-tan.jpg 1920w" sizes="(max-width: 750px) 100vw, 750px"></p><p>Lip-Bu Tan is chief executive officer of Intel Corporation and serves on the company’s board of directors. He was appointed to his position in March 2025.</p>
                            
                                            </div>
				
			
			<p>


				

									<h2>A message from Intel CEO Lip-Bu Tan to all company employees.</h2>
				
			</p>

			

			<div nonce="sI2vUjwNRX/OMdRrCafv5Q==">

					
						

					<!--?xml encoding="utf-8" ?--><p><em>The following note from Lip-Bu Tan was sent to all Intel Corporation employees on August 7, 2025:</em></p><p><span data-contrast="auto">Dear Team,</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p><p><span data-contrast="auto">I know there has been a lot in the news today, and I want to take a moment to address it directly with you. </span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p><p>Let me start by saying this: The United States has been my home for more than 40 years. I love this country and am profoundly grateful for the opportunities it has given me. I also love this company. Leading Intel at this critical moment is not just a job – it’s a privilege. This industry has given me so much, our company has played such a pivotal role, and it's the honor of my career to work with you all to restore Intel's strength and create the innovations of the future. Intel's success is essential to U.S. technology and manufacturing leadership, national security, and economic strength. This is what fuels our business around the world. It’s what motivated me to join this team, and it’s what drives me every day to advance the important work we’re doing together to build a stronger future.</p><p><span data-contrast="auto">There has been a lot of misinformation circulating about my past roles at Walden International and Cadence Design Systems. I want to be absolutely clear: Over 40+ years in the industry, I’ve built relationships around the world and across our diverse ecosystem – and I have always operated within the highest legal and ethical standards. My reputation has been built on trust – on doing what I say I’ll do, and doing it the right way. This is the same way I am leading Intel.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p><p><span data-contrast="auto">We are engaging with the Administration to address the matters that have been raised and ensure they have the facts. I fully share the President’s commitment to advancing U.S. national and economic security, I appreciate his leadership to advance these priorities, and I’m proud to lead a company that is so central to these goals.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p><p><span data-contrast="auto">The Board is fully supportive of the work we are doing to transform our company, innovate for our customers, and execute with discipline – and we are making progress. It’s especially exciting to see us ramping toward high-volume manufacturing using the most advanced semiconductor process technology in the country later this year. It will be a major milestone that’s a testament to your work and the important role Intel plays in the U.S. technology ecosystem.&nbsp;</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p><p><span data-contrast="auto">Looking ahead, our mission is clear, and our opportunity is enormous. I’m proud to be on this journey with you.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p><p><span data-contrast="auto">Thank you for everything you’re doing to strengthen our company for the future.&nbsp;</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p><p><span data-contrast="auto">Lip-Bu</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:0,&quot;335559740&quot;:240}">&nbsp;</span></p>
				</div><!-- .entry-content-wrapper -->

		</div><!-- .post-content-->

</article><!-- #post-## -->
    </main><!-- #main -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Someone keeps stealing, flying, fixing and returning this man's 1958 Cessna (108 pts)]]></title>
            <link>https://www.latimes.com/california/story/2025-08-08/mystery-plane-thief</link>
            <guid>44839681</guid>
            <pubDate>Fri, 08 Aug 2025 17:45:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.latimes.com/california/story/2025-08-08/mystery-plane-thief">https://www.latimes.com/california/story/2025-08-08/mystery-plane-thief</a>, See on <a href="https://news.ycombinator.com/item?id=44839681">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-element="story-body" data-subscriber-content=""> <p>While Jason Hong was celebrating his 75th birthday, he suddenly found himself thinking about his 1958 Cessna Skyhawk, a white and red single-engine beauty with colorful stripes that he calls his “old treasure.”</p><p>He doesn’t fly it much anymore, but given the occasion he resolved to visit his plane as soon as he could to “say hi,” like a lifelong friend you see around holidays and special occasions.</p><p>Hong headed to Corona Municipal Airport after church on July 27, but when he got there, the plane was not where he’d left it. Hong was dumbfounded.</p><p>“I got confused,” he said. “I thought, did I park it somewhere else, did the airport manager move it? But I looked all over.”</p><p>It was gone. </p><p>Hong was so shocked, he initially didn’t know who to reach out to about a missing, stolen plane. He wondered, did someone fly it out of the airport unnoticed? How long had it been missing? </p><p>The questions piled up. But the mystery only deepened.</p><p>As Hong would come to find out, the colorful aircraft had been flown across Southern California by an unknown pilot, unnoticed, in a series of joyrides — or joy flights — at least twice before and then simply returned to the airport. Both Hong and police were left scratching their heads.</p><p>The first time he discovered it missing, Hong reported it to Corona police, unsure that he’d ever see the plane he’s owned for nearly 30 years again. After all, he thought, who steals an entire plane?</p><p>Then on the morning of July 29, he got a call from La Verne Police, telling him his plane was found in Brackett Field Airport. </p><p>“There’s my airplane, sitting there in the airport,” Hong said, finding cigarette butts and garbage strewn about in the cockpit.</p><p>He barely took time to process what happened when, frustrated, he decided to pull out the battery from the plane, close it up, and go home. The plane wouldn’t start without the battery, he figured, and he could come back the next weekend when he had time to clean and inspect it. </p><p>Except that, when he returned that Sunday, Aug. 3, the plane had vanished again.</p><p>Hong reported the plane missing again with La Verne Police, and wondered what was going on. It wasn’t long before he got another call. This time, El Monte Police told him his plane was <a href="https://www.instagram.com/p/DM8EwnNRROR/?hl=en" target="_blank">sitting at San Gabriel Valley Airport</a>. </p><p>When Hong got there to inspect his plane, his confusion only grew. </p><p>“I found it with a battery,” he said. </p><p>It hasn’t been just Hong who has found himself befuddled by his disappearing and reappearing plane. </p><p>“This plane just keeps disappearing out of the blue,” said Sgt. Robert Montanez of the Corona Police Department. “It’s just weird.”</p><p>Montanez said when Hong reported his plane missing the first time, he’d last seen the aircraft in May in the small Corona airport.</p><p>For police, a case of an entire plane being stolen was so rare, that officers used the same form used for stolen cars, to take Hong’s report. </p><p>Officers are also aware that the plane has been taken multiple times, and returned, making the incidents more perplexing. But Montanez said there’s no immediate indication as to who the culprit is. </p><p>“There’s no camera video, there’s no real leads as to who stole the plane,” Montanez said.</p><p>After finding his plane a second time, Hong said he’s tried to put details of the thefts together, but the more he learns the more he grows confused about the circumstances. </p><p>Hong looked up his plane on Flight Aware, a site that tracks flights and aircraft, and found that on his 75th birthday, someone took off with his plane from La Verne airport at 9:54 p.m., for a 51-minute flight that at one point neared Palm Springs. </p><p>A few hours later, on July 26, the colorfully striped plane was in the air once again, this time for a brief 22-minute flight from Riverside County toward La Verne that started at about 1:30 a.m.</p><p>It was the next day that Hong would discover it missing.</p><p>At first, Hong said, he thought it might have been a random incident, but the details of the repeating incidents didn’t make sense to him, he said. </p><p>The multiple flights indicate that, whoever has taken his plane has had some sort of flight training, since they’ve been able to land the plane on multiple occasions. </p><p>“Landing is not easy, so they’re trained,” he said.</p><p>Hong said he’s also found a headset in the plane, as well as a new battery to replace the one he removed, meaning this mysterious pilot had spent hundreds of dollars on equipment to get his plane back in the air.</p><p>The replacement of the battery, Hong said, also suggests its someone familiar with not just flying, but the mechanics of the plane as they seemed to have the tools and know-how about the type of battery needed, and how to install it.</p><p>Having his airplane stolen has been frustrating, Hong said. But learning that the suspect has also been spending money and equipment to use — and return — the plane has just been confusing. </p><p>“Someone breaks into your house, they’re looking for jewelry or cash right?” he said, trying to reason with the circumstances. “But in this case, what’s the purpose? It’s like someone breaks my window, and then they put a new one up.”</p><p>The fact that someone has been traveling in it to different airports also puzzles him.</p><p>The 75-year-old Yorba Linda resident said he’s spoken to regular pilots and employees at the San Gabriel Valley Airport in El Monte, who said that they saw the plane flying in and out of the airport multiple times in July. </p><p>“On and off, they flew in and out, in and out, almost an entire month without knowing,” he said. “This is really a rare situation.”</p><p>One regular at the airport, Hong said, told him he saw a woman, about 5 feet, 3 inches tall, and in her 40s or 50s, flying and sitting in the plane on multiple occasions. The man told Hong he had a conversation with her at one point, and distinctly remembered her because she was often seen sitting in the cockpit during the day, making people at the airport wonder why she wouldn’t just relax in the air-conditioned airport lounge. </p><p>“Very strange,” Hong said. </p><p>For now, Hong has chained his plane in San Gabriel Valley Airport and said he’s uncomfortable flying it until he can thoroughly inspect it. </p><p>Other than that, he’s not sure what to do to keep his plane grounded, or to find out who has been secretly flying it out. </p><p>“I have no idea what to do,” he said. “It’s the strangest thing.”</p><div data-list-id="00000192-be42-da32-a3db-ff76fc3b0000" data-module-id="00000192-be42-da32-a3db-ff76fc3b0000" data-excluded-ids="00000196-fea4-da0a-ab96-fef4849a0000" data-click="enhancement" data-align-center="">  <p data-element="element-header" data-click="liZZListTitleCTA">  <h3 data-element="element-header-title" data-counter="3">More to Read </h3>  </p>      </div> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tor: How a military project became a lifeline for privacy (360 pts)]]></title>
            <link>https://thereader.mitpress.mit.edu/the-secret-history-of-tor-how-a-military-project-became-a-lifeline-for-privacy/</link>
            <guid>44838378</guid>
            <pubDate>Fri, 08 Aug 2025 15:45:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thereader.mitpress.mit.edu/the-secret-history-of-tor-how-a-military-project-became-a-lifeline-for-privacy/">https://thereader.mitpress.mit.edu/the-secret-history-of-tor-how-a-military-project-became-a-lifeline-for-privacy/</a>, See on <a href="https://news.ycombinator.com/item?id=44838378">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>A story of secrecy, resistance, and the fight for digital freedom.</p><figure><img width="700" height="420" src="https://thereader.mitpress.mit.edu/wp-content/uploads/2025/07/Tor-lead-700x420.jpg" alt="" decoding="async" fetchpriority="high"><figcaption>Photo credit: <a href="https://unsplash.com/photos/a-computer-with-a-white-screen-sitting-on-a-table-AP7tG4LTeXA">Alan W, via Unsplash</a></figcaption></figure><p>I’m sitting in a cold, scuffed, and dirty plastic chair on a crowded train, watching freezing fog stream past the window — one of the many unpleasant but strangely enjoyable everyday experiences of life in the United Kingdom. Despite the train carriage hailing from the mid-1980s, there is something resembling Wi-Fi service, and so I connect, hoping to sneak in a few hours of PhD research. I load up a website — or so I think — but instead reach a block page courtesy of the train’s Wi-Fi provider.</p><p>Sighing, I load up the Tor Browser and type in the address. The website loads instantly.</p><p>Tor is mostly known as the <em>Dark Web </em>or <em>Dark Net</em>, seen as an online Wild West where crime runs rampant. Yet it’s partly <a href="https://blog.torproject.org/transparency-openness-and-our-2021-and-2022-financials/" target="_blank" rel="nofollow">funded by the U.S. government</a>, and the BBC and Facebook both have Tor-only versions to allow users in authoritarian countries to reach them.</p><p>At its simplest, Tor is a distributed digital infrastructure that makes you anonymous online. It is a network of servers spread around the world, accessed using a browser called the Tor Browser, which you can download for free from the Tor Project website. When you use the Tor Browser, your signals are encrypted and bounced around the world before they reach the service you’re trying to access. This makes it difficult for governments to trace your activity or block access, as the network just routes you through a country where that access isn’t restricted.</p><h3><strong>The dark net rises</strong></h3><p>Today, privacy technologies like Tor underpin our digital society. From VPNs and encrypted messengers like WhatsApp to the basic security features in our digital systems, they’re essential tools for defending against cybercrime.</p><p>But, because you can’t protect yourself from digital crime without also protecting yourself from mass surveillance by the state, these technologies are the site of constant battles between security and law enforcement interests. The UK in particular is <a href="https://www.theguardian.com/technology/2025/jul/24/what-are-the-new-uk-online-safety-rules-and-how-will-they-be-enforced" target="_blank" rel="nofollow">currently convulsed over attempts to use law and technology to fight online harm</a>.</p><p>Recent debates focus on harms like the algorithmic spread of radicalizing and hateful content, issues often treated as if they emerge magically from technology itself, rather than from social policy, corporate greed, or an increasingly radicalized social elite.</p><figure><blockquote><p>Cypherpunks warned that the internet could quickly turn from a utopian dream into an authoritarian nightmare.</p></blockquote></figure><p>We’re in an undoubtedly odd situation. Governments are increasingly clamping down on the internet, yet the technologies to circumvent these blocks are readily available (and, in the case of Tor, completely free) and often funded, developed, used, and promoted by the same governments. By going back 30 years to the founding of the World Wide Web and the development of the technologies that would become Tor, we can get some surprising insights into why.</p><h3><strong>Cryptowars</strong></h3><p>Besieged as they are by the ongoing Oasis revival and the continuing dominance of Carhartt in the fashion market, readers in the UK will need little introduction to the cultural landscape of the 1990s. But in addition to the baggy jeans and Britpop, the early days of the commercial internet were also a time of immense possibility and conflict, when many aspects of the technologies and design of our digital societies were being fought over.</p><p>Some of the most important of these battles were the so-called Crypto Wars. A group of radical hackers and computer scientists known as the Cypherpunks spent the 1980s and early ‘90s adapting military-grade encryption for public use. They warned that the internet could quickly turn from a utopian dream into an authoritarian nightmare. As <em>technolibertarians</em>, they believed that encryption was vital to realizing the potential of the internet, that it would permanently break up the power of the big media corporations, banks, and governments and give it to private individuals.</p><p>Law enforcement, on the other hand, was increasingly furious at the spread of mass communications platforms slipping beyond their control. But the spirit of the age was against them. Many of the internet’s core architects saw encryption as essential to keeping large and complex systems running without interference. Global businesses, too, generally favored privacy. Operating in a global market, they wanted to protect their competitive edge in an emerging digital economy.</p><p>Perhaps most importantly, at the heart of the U.S. government were an ascendant set of ideas that saw the internet as the ultimate neoliberal project: a borderless marketplace where free-flowing information would lead to optimal prices, ideas, and solutions. Full of messianic cultural confidence following the fall of the Soviet Union, they believed that if information were allowed to flow, the values of American capitalism would triumph on their own merits.</p><p>It was in this chaotic, high-stakes environment, full of strange alliances and clashing visions,  that the technologies behind the Dark Web were born.</p><h3><strong>Spies, submarines, and secrets</strong></h3><p>Tor’s story began in an office of the U.S. Naval Research Laboratory (NRL). Down the hall, satellites and radar dishes hung suspended in enormous voids, giant black pyramids bristled from the walls of vast anechoic testing chambers, and robot arms flexed in dark flooded pools, being poked and prodded by scuba divers armed with sensors. But the foundations of Tor were laid in a much more prosaic setting — a shared computer lab.</p><p>Three military researchers — David Goldschlag, Mike Reed, and Paul Syverson — had been discussing a foundational aspect of the internet infrastructure. The rise of the new, commercial internet presented challenges for military users, as these global systems were vital for communications but difficult to secure.</p><p>Encryption technologies were already available to protect the <em>content</em> of messages. But above the content, the network itself had a range of security issues. The internet’s traffic routing systems and protocols were reliant on addressing metadata, equivalent to the <em>to </em>and <em>from </em>addresses on an envelope. Much as the address of the recipient is crucial for the delivery of a piece of mail, so are these metadata fundamental to the internet’s design and necessarily visible to the infrastructure providers who run its networks.</p><p>The routing design of the internet worked well for the U.S. government’s domestic interests, as it allowed the state to establish itself at key control points and surveil user traffic. However, the spread of the internet around the world had given other governments this power over their own domestic communication networks. This means that intelligence and military personnel abroad who wanted to make contact with their handlers in the United States or communicate with their base of operations were vulnerable.</p><p>Whenever the Navy utilized cryptosystems and communication networks that linked up to the internet, substantial amounts of valuable additional information were exposed to the people who ran the infrastructure. For example, if a CIA spy was in a foreign nation and sent a message over the internet back to the CIA’s home servers, ISPs in that foreign nation could observe where the message was sent and infer the spy’s affiliation.</p><p>This was a clear question for military research: how to keep internet traffic between the U.S. and other nations secret, not only in content (which you could protect with encryption), but also in origin and destination. And the three NRL researchers sought to solve it.</p><h3><strong>Onions</strong></h3><p>The researchers wanted to find a way to do the seemingly impossible — to give the military the benefits of a global, high-speed communications network without exposing them to the vulnerabilities of the metadata that the network relied on to operate.</p><p>Enter Onion routing. Onion routing has undergone many changes and refinements over the years, but the basic principle has remained the same: The routing information used to navigate the internet is first hidden under three layers of encryption, like a Russian doll. It is from these layers that onion routing gets its name. This “onion” of routing information is then sent into a network of onion routers: servers, or relays, located around the world that bounce the traffic around and between themselves. Each of these relays decrypts a layer of encryption to reveal the address of the next server in the network, until the final server reveals the destination of the traffic and makes a connection to the target web service. None of them can see both the origin and the destination of the traffic.</p><figure><blockquote><p>The routing information used to navigate the internet is first hidden under three layers of encryption, like a Russian doll.</p></blockquote></figure><p>This technical design has immediate social consequences, which were apparent to the NRL designers from the early stages. First, the infrastructure could not be run by the U.S. Navy, for if this were the case, then only people who trusted the U.S. Navy would use it. In an onion routing design, anonymity is produced by the size of the crowd — the more people using the system, the more privacy it provides.</p><p>There are other implications, as well. For a CIA agent to use Tor without suspicion in non-U.S. nations, for example, there would need to be plenty of citizens in these nations using Tor for everyday internet browsing. Similarly, if the only users in a particular country are whistleblowers, civil rights activists and protesters, the government may well simply arrest anyone connecting to your anonymity network. As a result, an onion routing system had to be open to as wide a range of users and maintainers as possible, so that the mere fact that someone was using the system wouldn’t reveal anything about their identity or their affiliations.</p><p>This philosophy of a system open to the general public, in which small numbers of high-risk users could hide in cover traffic from more everyday users, underpins what became the onion routing paradigm, the predecessor to Tor.</p><h3><strong>Cypherpunk hackers and the U.S. military</strong></h3><p>Anonymity loves company — so Tor needed to be sold to the general public. That necessity led to an unlikely alliance between cypherpunks and the U.S. Navy.</p><p>The NRL researchers behind Onion routing knew it wouldn’t work unless everyday people used it, so they reached out to the cypherpunks and invited them into conversations about design and strategy to reach the masses.</p><p>The NRL researchers met several members of the cypherpunk community in person at the Information Hiding Workshop in Oakland in 1997, where they discussed the possibility of collaboration. There, over vegetarian lasagna, salad, and (what else?) roasted onions, they discussed the technical possibilities and paradigms that might underpin a mass-use anonymity system. As they did, they also talked through broader values and motivations that might unite their strange, hybrid community.</p><p>Observing these two worlds — the military academics and the cypherpunks — interacting, through sharing test results, theoretical discussions, phone calls, emails, and eating the occasional roasted onion, we see the beginnings of a distinctive idea of what privacy means. Somewhere between the cypherpunk’s everyday, radical, decentralized vision of privacy and the high-security traffic protection desired by the military, a shared idea was forming. This saw privacy as being strongly shaped by the clusters of power and control built into digital infrastructure.</p><p>This understanding of privacy as a <em>structure</em> would unite an odd coalition around Tor over the next three decades: activists, journalists, drug buyers, hackers, and the military itself.</p><h3><strong>Scrambling for safety</strong></h3><p>This strange story of a group of libertarian hackers teaming up with the U.S. military amid the aftershocks of the Cold War presents a more nuanced picture of privacy than the familiar lone-user-versus-state narrative. It shows different groups coming together to change how — through laws, technologies, practices, and cultural values — we police the boundaries between different material systems of power. Understood in this way, we can see privacy as setting out where the domain of the community, of the family, of the state, of a corporation, of an institution or an individual begins and ends.</p><p>Take the UK’s Online Safety Act. It’s justified by policymakers as a tool to protect women and children from harm, with the Technology Minister going as far as to say that opposing the Act <a href="https://www.politico.eu/article/nigel-farage-on-the-side-predator-jimmy-savile-says-uk-minister-peter-kyle-online-safety-act/" target="_blank" rel="nofollow">puts you “on the side of predators”</a> and child abusers. Law enforcement often argues that privacy technologies undermine their ability to prevent and investigate crime, particularly crime against women and children. This frames the issue as a trade-off between individual rights and collective safety. But many feminists would argue exactly the opposite: that the police have long painted women and children as uniquely weak and vulnerable in order to cement their own claim to power.</p><figure><blockquote><p>Undermining the very tools that give communities security is a poor strategy for keeping them safe.</p></blockquote></figure><p>In fact, breaking encryption in practice intensifies surveillance of women and children, undermining their rights to self-determination and autonomy, under the justification of protecting them. Yet it’s often powerful men in their families, communities, or institutions that women need to protect themselves from. For many, the prospect of police being able to track their intimate lives, or their attempts to access reproductive healthcare, is extremely threatening.</p><p>The state’s claim to protect the vulnerable often masks efforts to exert control. In fact, robust, well-funded, value-driven and democratically accountable content moderation — by well-paid workers with good conditions — is a far better solution than magical tech fixes to social problems (which also do little to tackle real social issues of misogyny, racism, and violence) or surveillance tools.</p><p>Indeed, undermining the very tools that give communities security is a poor strategy for keeping them safe. As more of our online lives are funneled into the centralized AI infrastructures — controlled by a small and increasingly radicalized tech elite — tools like Tor are becoming ever more important. Beyond offering privacy and protection from cybercrime in an increasingly insecure global landscape, they point to a more optimistic future for the internet, one in which we rebuild trust in our social institutions to address harm, rather than surrender that role to unaccountable technologies of control.</p><hr><p><strong><em>Ben Collier</em></strong><em> is Senior Lecturer in Digital Methods in the Department of Science, Technology, and Innovation Studies at the School of Social and Political Science, University of Edinburgh. He is the author of “<a href="https://mitpress.mit.edu/9780262548182/tor/" target="_blank">Tor: From the Dark Web to the Future of Privacy</a>.” An open access edition of the book is freely available for download <a href="https://direct.mit.edu/books/oa-monograph/5761/TorFrom-the-Dark-Web-to-the-Future-of-Privacy" target="_blank">here</a>.</em></p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-5 vs. Sonnet: Complex Agentic Coding (164 pts)]]></title>
            <link>https://elite-ai-assisted-coding.dev/p/copilot-agentic-coding-gpt-5-vs-claude-4-sonnet</link>
            <guid>44838303</guid>
            <pubDate>Fri, 08 Aug 2025 15:38:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://elite-ai-assisted-coding.dev/p/copilot-agentic-coding-gpt-5-vs-claude-4-sonnet">https://elite-ai-assisted-coding.dev/p/copilot-agentic-coding-gpt-5-vs-claude-4-sonnet</a>, See on <a href="https://news.ycombinator.com/item?id=44838303">Hacker News</a></p>
Couldn't get https://elite-ai-assisted-coding.dev/p/copilot-agentic-coding-gpt-5-vs-claude-4-sonnet: Error: getaddrinfo ENOTFOUND elite-ai-assisted-coding.dev]]></description>
        </item>
        <item>
            <title><![CDATA[AI must RTFM: Why tech writers are becoming context curators (143 pts)]]></title>
            <link>https://passo.uno/from-tech-writers-to-ai-context-curators/</link>
            <guid>44837875</guid>
            <pubDate>Fri, 08 Aug 2025 15:04:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://passo.uno/from-tech-writers-to-ai-context-curators/">https://passo.uno/from-tech-writers-to-ai-context-curators/</a>, See on <a href="https://news.ycombinator.com/item?id=44837875">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header>
	
	<nav>
		
		<a href="https://passo.uno/about">About</a>
		
		<a href="https://passo.uno/posts">Posts</a>
		
		<a href="https://bsky.app/profile/theletterf.bsky.social">Bluesky</a>
		
		<a href="https://hachyderm.io/@remoquete">Mastodon</a>
		
		<a href="https://www.linkedin.com/in/fabrizioferri/">LinkedIn</a>
		
		<a href="https://passo.uno/posts/index.xml">RSS</a>
		
		
	</nav>
</header>

<main>
	<article>
		
		

		

		<section>
			<p>I’ve been noticing a trend among developers that use AI: they are increasingly writing and structuring docs in context folders so that the AI powered tools they use can build solutions autonomously and with greater accuracy. They now strive to understand information architecture, semantic tagging, docs markup. All of a sudden they’ve discovered docs, so they write more than they code. Because AI must RTFM now.</p>
<p>It’s docs-driven development. It’s also technical writing. We should welcome our colleagues into the fold of technical communication and seriously start thinking about becoming context writers and maintainers. In a way, we’ve always been that, building the skills and techniques that allow owners of brains – either organic or simulated – to find their way in complex systems and accomplish tasks.</p>
<h2 id="all-ai-requires-to-do-the-right-thing-is-great-context-and-a-gentle-nudge">All AI requires to do the right thing is great context and a gentle nudge</h2>
<p>Picture large language models (LLMs) as elaborate machines that take inputs (in most cases it’s just text), turn it into discrete pieces (tokens) and process it through an incredibly convoluted conveyor belt. At the other end of the machine is the output, usually in the form of helpful commentary, feedback, and commands issued to various system tools. The <a href="https://en.wikipedia.org/wiki/Chinese_room">Chinese room</a>, finally incarnated.</p>
<p>The quality of the output is a function of the quality of the input. We enter requests and get back what an average of all human thoughts could have led us to, eventually. There’s nothing magic to it: if you write clear, accurate, and well structured requests (prompts), chances are that the LLM will respond in kind. It’s hard work, but it pays off, because development time is substantially reduced.</p>
<p>That’s why, with every release of a new LLM, coders pay close attention to the size of the <a href="https://www.ibm.com/think/topics/context-window">context window</a>, that is, the amount of information one can feed to an LLM. A context window of one million tokens means you can easily feed the entire <em>The Lord of the Rings</em> trilogy to, say, Gemini, and start asking questions about it. And you would still have room for <em>The Hobbit and</em> <em>The</em> <em>Silmarillion</em>.</p>
<h2 id="what-is-a-context-curator-and-why-we-need-that-role">What is a Context curator and why we need that role</h2>
<p>Engineers are finding out that writing, that long shunned soft skill, is now key to their efforts. In <a href="https://www.anthropic.com/engineering/claude-code-best-practices">Claude Code: Best Practices for Agentic Coding</a>, one of the key steps is creating a CLAUDE.md file that contains instructions and guidelines on how to develop the project, like which commands to run. But that’s only the beginning. Folks now <a href="https://www.reddit.com/r/ClaudeAI/comments/1lxylfs/claude_code_docs_as_context/">suggest</a> maintaining elaborate context folders.</p>
<p><strong>A context curator, in this sense, is a technical writer who is able to orchestrate and execute a content strategy around both human and AI needs, or even focused on AI alone.</strong> Context is so much better than content (a much abused word that means little) because it’s tied to <em>meaning</em>. Context is situational, relevant, necessarily limited. AI needs context to shape its thoughts.</p>
<p>In <a href="https://passo.uno/build-tech-writing-tools-llms/">Own the prompt</a>, where I described how I built <a href="https://github.com/theletterf/aikidocs">a tool</a> to write docs using context from the terminal or the browser, I argued that technical writers should lead the way and own AI-powered docs processes, including the curation of context. In my <a href="https://passo.uno/tech-writing-predictions-2025/">predictions for this year</a> (and probably the next, too), the importance of context was already present as the rise of docs-as-data.</p>
<blockquote>
<p><em>Picture a developer inserting a docs cartridge into his AI powered code editor: the presentation layer is going to be largely irrelevant, the docs powering the answers of locally executed LLMs to aid developers in their coding quests. In a multi-channel content strategy, LLM-tailored output is going to be an additional, incredibly relevant channel.</em></p>
</blockquote>
<h2 id="writing-is-designing-and-co-developing-again">Writing is designing and co-developing (again)</h2>
<p>Four years ago, <a href="https://passo.uno/posts/how-to-assist-api-design-as-a-technical-writer/">I argued</a> that technical writers can play a key role in API design and development, because words are everywhere and we writers are uniquely well positioned to select the right words. Back then, OpenAPI was the device that allowed the magic of crafting design using just words. Today, the spell extends to all kinds of software development. <a href="https://passo.uno/thinking-better-through-ai/">We can conjure software ourselves</a> now.</p>
<p>At this point, the most bleeding edge tech writing shops are serving <a href="https://llmstxt.org/">llms.txt</a> files and <a href="https://github.com/romansky/dom-to-semantic-markdown">LLM-optimized Markdown</a>. We’d take this a step further and prepackage context in a way that LLMs can easily consume. A standard for this still doesn’t exist, but I can see some flavor of DITA or another markup making a comeback, together with UIs that let users download which docs, for which version, etc.</p>
<p><img src="https://passo.uno/uploads/repomix.png" alt="Repomix"></p>
<blockquote>
<p><a href="https://repomix.com/">Repomix</a> allows anybody to select and package code and docs for LLMs</p>
</blockquote>
<p>The endgame is to be able to make content accessible to LLMs and humans alike, to let them extract knowledge tailored to their needs. Tech writers become context writers when they put on the art gallery curator hat, eager to show visitors the way and help them understand what they’re seeing. It’s yet another hat, but that’s both the curse and the blessing of our craft: like <a href="https://en.wikipedia.org/wiki/Bard_(Dungeons_%26_Dragons)">bards in DnD</a>, we’re the jacks of all trades that save the day (and the campaign).<br></p>
		</section>

		
		</article>
</main>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI is impressive because we've failed at personal computing (199 pts)]]></title>
            <link>https://rakhim.exotext.com/ai-is-impressive-because-we-ve-failed-at-semantic-web-and-personal-computing</link>
            <guid>44837783</guid>
            <pubDate>Fri, 08 Aug 2025 14:57:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rakhim.exotext.com/ai-is-impressive-because-we-ve-failed-at-semantic-web-and-personal-computing">https://rakhim.exotext.com/ai-is-impressive-because-we-ve-failed-at-semantic-web-and-personal-computing</a>, See on <a href="https://news.ycombinator.com/item?id=44837783">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>Unless someone wrote an article about that exact thing, a plain full-text search engine cannot answer a question like this:</p>
<blockquote>
<p>What animal is featured on a flag of a country where the first small British colony was established in the same year that Sweden's King Gustav IV Adolf declared war on France?</p>
</blockquote>
<p>But ChatGPT got the correct answer in a few seconds.</p>
<p><img src="https://img.exotext.com/1/_1IITv5hhOjWL-WdlODDM.png" alt="">
<em>Flag of Dominica features the Sisserou parrot, which is only found in Dominica. Great Britain established a small colony on the island in 1805.</em></p>
<p>Google's AI widget failed miserably, by the way.</p>
<p><img src="https://img.exotext.com/1/S4vSDU4NTMtPN3BuxyNbJ.png" alt=""></p>
<p>One of the best applications of modern LLM-based AI is surfacing answers from the chaos of the internet. Its success can be partly attributed to our failure to build systems that organize information well in the first place.</p>
<p>This product pattern is not new. Take Google Drive: a glorified file system in the cloud with folders and files, but it offers a worse experience than almost any desktop file management application of the last 30 years. Organizing your stuff there is hard and tedious. So Google took a shortcut: full-text search. Just dump everything in, and type to find it later. </p>
<p>The pattern of giving up on structure and relying on search has quietly become the dominant paradigm. "Search" here is a wide term: it can mean classic text-matching across indexed data, or complex multi-dimensional token matching across unwieldy models and weights. Why create a well-organized e-commerce site, just add a search bar and oversaturate each item's page with keywords. Why write high-quality user documentation, just add a support chat bot.</p>
<p>Remember Semantic Web? The web was supposed to evolve into semantically structured, linked, machine-readable data that would enable amazing opportunities. That never happened. Not only data remains unstructured and lacking metadata, even the representation of the unstructured data became difficult for machines to read due to the switch from plain, somewhat-structured HTML to JS-driven dynamic pile of <code>div</code>s.</p>
<p>We also never achieved truly personal computing. Computers could've been personal knowledge bases, with structured semantic connections akin to HyperCard, that take advantage of the semantic web and open standards. </p>
<p>My point is that if all knowledge were stored in a structured way with rich semantic linking, then very primitive natural language processing algorithms could parse question like the example at the beginning of the article, and could find the answer using orders of magnitude fewer computational resources. And most importantly: the knowledge and the connections would remain accessible and comprehensible, not hidden within impenetrable AI models.</p>
<p>AI is not a triumph of elegant design, but a brute-force workaround. LLMs like ChatGPT can infer structure from chaos. They scan the unstructured web and build ephemeral semantic maps across everything. It's not knowledge in the classic sense.. or perhaps it is exactly what knowledge is?</p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google's Genie is more impressive than GPT5 (200 pts)]]></title>
            <link>https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant</link>
            <guid>44837646</guid>
            <pubDate>Fri, 08 Aug 2025 14:46:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant">https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant</a>, See on <a href="https://news.ycombinator.com/item?id=44837646">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>The goal of AGI is to make programs that can do lots of things. Unfortunately, it's not all that easy to program “do lots of things” into a computer. Like, if you're writing python, there's no `import everything` library – you have to somehow teach your program a bunch of different tasks and skills. Naively, you could spend a lot of time hand coding every possible scenario that may occur – some gigantic switch statement that has a unique handler for every possible input. But this is obviously going to take too long and is extremely inefficient and really only theoretically possible.</p><p><span>So the watchwords of AGI are compression and generalization. You want to make a program that is pretty small in terms of compute and memory and so on, but has a lot of abilities that allow it to cover a very large ‘action space.’</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-170425838" href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant#footnote-1-170425838" target="_self" rel="">1</a></span></p><p><span>One way to teach your program how to generalize across things is by using deep learning. At a high level, you can show a deep neural network terabytes of data, and it will learn how to represent that data in a compressed form. The big large language models take in ~all of the text ever written, and are maybe a few tens-of-gigabytes in size,</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-170425838" href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant#footnote-2-170425838" target="_self" rel="">2</a></span><span> and yet seem to be able to replicate much of the training data. Perhaps the most surprising revelation of the last few years is that in addition to getting really good at spitting out realistic looking text, these LLMs also picked up more generic skills. The most evocative example of this for me was realizing that the original GPT-3 models – the ones that preceded ChatGPT, that had no ‘post training’ or ‘instruction tuning’ – could play a decent game of chess, even though the model surely didn't understand chess, and probably didn't really understand 2D grids. And since that moment back in 2020, it has become extremely obvious that these things can do quite a fair bit beyond just mimicking text.</span></p><p>A lot of AI research these days is basically exclusively about how to make large language models better. Naturally, some people focus on “large” – if you make the model bigger, it can get better! But some people also focus on “language” – LLMs are only compressing text, but what if it compressed more kinds of data? A model that could represent text and images is probably better than one that can only represent text. And a model that could represent text and images and video is probably better than one that can only represent text and images.</p><p><span>If you assume that model representation capacity is directly tied to usefulness, you'll eventually reach a conclusion that looks something like this: “a model that can accurately represent the entire world is going to be pretty damn useful.” Imagine asking a model a question like “what's the weather in Tibet” and instead of doing something </span><em>lame</em><span> like check weather.com, it does something </span><em>awesome</em><span> like stimulate Tibet exactly so that it can tell you the weather based on the simulation. And giving a robot the ability to represent the world may allow it to do things like plan complex movements, navigate environments, and otherwise interact with real world environments. After all, this is approximately how humans work. In order to pick up my mug of not-coffee, I have to have an internal representation of my hand, the table, the mug, where my arm is going to go, how my hand is going to grip, what gravity is, what object-corporeality is, etc. etc. More mundanely, world models will probably allow people to make, like, better, more realistic AI generated Tiktoks that don't turn into spaghetti after a few minutes (and I'm sure nothing bad will come of that).</span></p><p><span>These “World Models” are considered a pretty long shot frontier in the AI world. Hopefully for obvious reasons — simulating an entire world for extended periods of time with any kind of accuracy is really hard! You need mountains and mountains of data, most of it video. And as a result, there aren't a lot of people who are really even trying in this space.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-170425838" href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant#footnote-3-170425838" target="_self" rel="">3</a></span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-170425838" href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant#footnote-4-170425838" target="_self" rel="">4</a></span><span> But you know who has a mountain of video data?</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!3_o3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!3_o3!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 424w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 848w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 1272w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!3_o3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png" width="1456" height="971" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!3_o3!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 424w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 848w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 1272w, https://substackcdn.com/image/fetch/$s_!3_o3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b611525-f425-44f0-9a32-646097d4a820_1600x1067.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>This is, possibly, the worst photo of these guys that I have ever seen.</figcaption></figure></div><p><span>About 3 days ago, Google announced </span><a href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/" rel="">Genie 3</a><span>. Genie stands for Generative Interactive Environments. The best way to understand Genie is by analogy. GPT and Gemini let you create text descriptions of a time and place. And Veo and Sora let you turn text descriptions into video. Genie lets you take a text description into a </span><em>video game</em><span>, a space that you can, at least primitively, </span><em>interact with</em><span>.</span></p><div id="youtube2-PDKhUknuQDg" data-attrs="{&quot;videoId&quot;:&quot;PDKhUknuQDg&quot;,&quot;startTime&quot;:&quot;53&quot;,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/PDKhUknuQDg?start=53&amp;rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p>It is kind of incredible? The reason Genie gets the headliner title over GPT-5 (below) is that Genie is really, truly something different.</p><p><span>Now, you can only really interact with a world Genie creates for a few minutes. But that is a massive step up from where we were previously. And it points to the future. Coherence over long context windows used to be a very difficult problem for language models too – if you've been reading my </span><a href="https://theahura.substack.com/p/ilyas-30-papers-to-carmack-table" rel="">ml paper review series</a><span> you'll know that a solid part of the last 10 years of AI research has been motivated in part by this very problem. Last I checked we did a pretty good job with it; similar progress is possible for world models. More importantly, Genie 3 is now at a point where you can start using it for a wide range of other tasks, including training other models. You don't need to drive millions of miles in a Waymo if you can artificially create long-tail distribution events and train on those!</span></p><p><span>For folks who are interested in the more technical aspects of how this thing works, you're a bit out of luck. Publishing at Google is a bit weird these days. Any research papers that get written up first go into an internal pool. If any of the Gemini product teams want to productionize research out of that pool, the paper doesn't get published. As a result, I suspect we won't see papers for Genie 3 (or even its predecessor, Genie 2) any time soon. Here's the </span><a href="https://arxiv.org/abs/2402.15391" rel="">Genie 1 paper</a><span> though. I'll try and review it soon.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!i96C!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!i96C!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 424w, https://substackcdn.com/image/fetch/$s_!i96C!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 848w, https://substackcdn.com/image/fetch/$s_!i96C!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 1272w, https://substackcdn.com/image/fetch/$s_!i96C!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!i96C!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png" width="1080" height="1031" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1031,&quot;width&quot;:1080,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;r/OpenAI - Sam Altman twitter hype is out of control again. we are not gonna deploy AGI next month, nor have we built it. we have some very cool stuff for you but pls chill and cut your expectations 100x! 11:32 AM · Jan 20, 2025 · 26.9K Views •&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="r/OpenAI - Sam Altman twitter hype is out of control again. we are not gonna deploy AGI next month, nor have we built it. we have some very cool stuff for you but pls chill and cut your expectations 100x! 11:32 AM · Jan 20, 2025 · 26.9K Views •" title="r/OpenAI - Sam Altman twitter hype is out of control again. we are not gonna deploy AGI next month, nor have we built it. we have some very cool stuff for you but pls chill and cut your expectations 100x! 11:32 AM · Jan 20, 2025 · 26.9K Views •" srcset="https://substackcdn.com/image/fetch/$s_!i96C!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 424w, https://substackcdn.com/image/fetch/$s_!i96C!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 848w, https://substackcdn.com/image/fetch/$s_!i96C!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 1272w, https://substackcdn.com/image/fetch/$s_!i96C!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a72084-28d8-4cd0-896f-83718cf14fd0_1080x1031.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!MCMu!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!MCMu!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 424w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 848w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 1272w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!MCMu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png" width="600" height="302" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:302,&quot;width&quot;:600,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!MCMu!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 424w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 848w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 1272w, https://substackcdn.com/image/fetch/$s_!MCMu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2848fb1b-913e-429c-bad8-57ab6963a3e9_600x302.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!AdQj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!AdQj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 424w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 848w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 1272w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!AdQj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png" width="720" height="342" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:342,&quot;width&quot;:720,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;This tweet by Sam Altman could be very important. Is he talking about  OpenAI? : r/singularity&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="This tweet by Sam Altman could be very important. Is he talking about  OpenAI? : r/singularity" title="This tweet by Sam Altman could be very important. Is he talking about  OpenAI? : r/singularity" srcset="https://substackcdn.com/image/fetch/$s_!AdQj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 424w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 848w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 1272w, https://substackcdn.com/image/fetch/$s_!AdQj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F589e1529-ef11-48a6-aa73-b5ca4b8aab45_720x342.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>This you?</figcaption></figure></div><p>The big story about GPT-5 is about what it isn't.</p><p>It isn't a world-changing super-intelligent insane-step-up on the intelligence ladder. It isn't God. It isn't close to God.</p><p>Now, if you've been reading my blog for any length of time, you'll know that I didn't really ever suspect OpenAI would be the one to stumble upon God in the machine, even though that is in some sense their explicit purpose. I tend to think Google is going to do it, mostly by accident, and will probably also end up sitting on the research for too long until OpenAI-2-electric-boogaloo comes around and tries to eat their lunch, again.</p><p>But still. There was so much hype around GPT-5, and now all that hype has deflated.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!1RO6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!1RO6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 424w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 848w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 1272w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!1RO6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png" width="961" height="542" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:542,&quot;width&quot;:961,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!1RO6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 424w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 848w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 1272w, https://substackcdn.com/image/fetch/$s_!1RO6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F390ffd7b-4dae-436d-8a4c-f80418cd21ad_961x542.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>The betting markets were not impressed by GPT-5. I am reading this graph as "there is a high expectation that Google will announce Gemini-3 in August", and not as "Gemini 2.5 is better than GPT-5". EDIT: this may be incorrect — the polymarket is using the style-removed benchmark </span><a href="https://lmarena.ai/leaderboard/text/overall-no-style-control" rel="">here</a><span>, where Gemini 2.5 still ranks higher than GPT-5.</span></figcaption></figure></div><p><span>Starting about a year ago, people began to complain that AI had hit a wall because GPT-5 was not yet released. Some folks (cough Gary cough) were even starting to make claims like "GPT-4 is the best AI we're ever going to get". At the time, </span><a href="https://theahura.substack.com/p/tech-things-gpt-pro-and-the-state" rel="">I pushed back</a><span>, blaming our short attention spans and need for immediate gratification:</span></p><blockquote><p>But is AI stagnating?</p><p>There is a strict sense in which consumer AI may not feel like it's growing at the same rate as it did from 2020 to 2023. That period was a particularly magic time where we had a surplus of chips that we had to catch up to. Like a gas expanding to fill a volume, our chip utilization has caught up, so releases may not be at such a rapid clip.</p><p>Some of the problem here is that consumers are just getting impatient. The first version of GPT3 was published in May, 2020. GPT4 was launched in March, 2023. That’s 34 months. It’s only been ~20 months since GPT4 was released, there’s a bit more time to go before OpenAI starts ‘falling behind schedule’. We haven’t had the capability to even create large enough GPU clusters until recently. And it is also plausible that the release of stronger LLMs tracks more to self driving cars than to iPhones. The hypecycle for self driving cars was at its peak around 2014-2015. Even though the technology wasn’t quite consumer ready by then, the estimated ‘release date’ was still within only a few short years. In 2024 there are readily available self driving cars in several cities. From a research perspective, the folks saying that self driving cars would be ready within a few years of 2014 were more right than those saying it would never be ready at all.</p><p>As for the people who are arguing that AI is obviously dead and the whole field was doomed to failure because it's "just statistics" or "just linear algebra", idk, this feels a lot like shifting goal posts. Standard LLMs are exposed to way less data than the average human baby, the fact that they can do anything at all is a miracle, the fact that they can regularly pass competence tests like the SAT or the Bar should be endlessly awe inspiring. For some reason people keep wondering when we'll have AGI, even though it's literally here and accessible through a web browser. In any case, the cope isn't going to stop the AI from taking everyone's jobs (mine included).</p></blockquote><p>I stand by basically all of what I said. But I also have to eat some of the intent behind my words here. With GPT-5, I was clearly expecting something closer to the step function increase in functionality that we saw between GPT-3 and 4. Unfortunately, we really did hit a serious industry-wide asymptote in our ability to get more out of next-token-prediction. In retrospect, I think GPT-5 was always going to be disappointing. I'm sympathetic to the OpenAI team here, people were expecting literal miracles. But also, Sam definitely played a role in building up hype — and, as a result, increased the mountain OpenAI would have to eventually summit.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!nK2Q!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!nK2Q!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 424w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 848w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 1272w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!nK2Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png" width="745" height="565" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b447decd-22cb-46e6-839b-7e716719f063_745x565.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:565,&quot;width&quot;:745,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:271822,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://theahura.substack.com/i/170425838?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!nK2Q!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 424w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 848w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 1272w, https://substackcdn.com/image/fetch/$s_!nK2Q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb447decd-22cb-46e6-839b-7e716719f063_745x565.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Sam tweeted this the day before the GPT-5 announcement, come on!</figcaption></figure></div><p><span>So what is GPT-5? It's basically GPT-4, but better. It's still early, but it seems to be </span><em>significantly</em><span> more consistent, which is no small feat. OpenAI already has most of the consumer brand recognition; there are many people for whom "LLM" and "AI" are synonymous with "ChatGPT". But I suspect that those of us who swap models frequently will begin to use OpenAI as a daily driver again.</span></p><p><span>This shouldn't be understated. GPT has not been a part of my daily life at all since approximately January of this year, when I fully switched to Claude. And when I switched over to using Gemini for code and Claude for everything else, I took the extra step of uninstalling the ChatGPT app from my phone. More generally, I think there's been a bit of a 'vibe shift' in the Bay and among AI researchers and practitioners. People are starting to realize the sheer weight of Google's TPU farms, while OpenAI talent is getting siphoned off by </span><em>liquid </em><span>billion-dollar offers on one side (e.g. Meta) and </span><em>even more </em><span>ideological startups on the other (e.g. Safe Superintelligence, whatever Mira Murati is up to). Friends who are way more plugged in than I am</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-170425838" href="https://theahura.substack.com/p/tech-things-genies-lamp-openai-cant#footnote-5-170425838" target="_self" rel="">5</a></span><span> describe an anti-OpenAI "coalition" forming, with many of the folks who had been burned by Sam's aggressive commercialization lining up to give the company a black eye. If you were more social-graph-minded, you may read a lot into Alexandr Wang — Sam Altman's ex-roommate and close confidant — leaving </span><a href="http://scale.ai/" rel="">Scale.AI</a><span> for Meta.</span></p><p><span>In this context, being the best in class is really important. Important people are losing faith, and those important people talk to other important people who have money. OpenAI needs to justify their extremely high valuation and their capex burn. If I’m right </span><a href="https://theahura.substack.com/p/tech-things-gpt-pro-and-the-state" rel="">that LLMs are a winner-take-all game</a><span>, OpenAI has to position itself as the winner.</span></p><p>They may have a tough time doing so though if they can't get their graphs straight. I mean what the hell are these?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!Mx83!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Mx83!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 424w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 848w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 1272w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Mx83!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png" width="744" height="824" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:824,&quot;width&quot;:744,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!Mx83!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 424w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 848w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 1272w, https://substackcdn.com/image/fetch/$s_!Mx83!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ae5a7f5-df05-44c4-9da4-3709c69609ef_744x824.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Ok, ok, that's great and all. But </span><em>what is GPT-5? How does it work?</em><span> Well, OpenAI isn't exactly going to release a public research paper about their latest and greatest. But we can go off the model card.</span></p><blockquote><p>GPT-5 is a unified system with a smart and fast model that answers most questions, a deeper reasoning model for harder problems, and a real-time router that quickly decides which model to use based on conversation type, complexity, tool needs, and explicit intent…In the near future, we plan to integrate these capabilities into a single model.</p></blockquote><p><span>In other words, GPT-5 is a bunch of smaller models in a trenchcoat. I've long believed that many of the consumer-facing web chat interfaces were powered by many models instead of one really big model. It is simply more cost effective. I've written in the past about </span><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" rel="">'the bitter lesson'</a><span>, which can roughly be summarized as "scaling compute and data will lead to more long term progress than hand crafted heuristics and rules". But a natural corollary to the bitter lesson is that, </span><em>for a fixed budget</em><span>, human crafted systems are often more efficient. So here. Unfortunately, there just isn't that much additional information for how it works beyond that.</span></p><p>Even though Anthropic already had their big Claude 4 release a few months ago, they didn't want to feel left out, so they released Claude Opus 4.1. Rather appropriately titled, it really is just a slightly better version of Opus 4. I'll take it.</p><p>Claude hasn't really been at the top of any of the leaderboards for a while. And yet I and many very technical and AI-savvy people continue to use it. This is…somewhat odd? Why do I purposely use a worse model?</p><p><span>I think the short answer is that it's not worse. Teaching to the test is as much a problem in AI as it is in education. I mean this is standard </span><a href="https://en.wikipedia.org/wiki/Goodhart%27s_law" rel="">Goodhart's law</a><span> stuff — the tests are meant to be proxies for competence, not targets. Even though Claude doesn't top leaderboards, it feels better to use. And any other gaps in model quality are simply papered over by Anthropic's focus on the user experience. As a developer, Claude is just way better. The artifact system is great, and the claude code CLI is a seamless experience.</span></p><p>A friend of mine described Anthropic as the Apple to OpenAI's Microsoft. And, like, yea, I see it. I guess Google is still just Google in this metaphor, idk.</p><p>I feel like every other month I hear about some random famous tech person raising a bajillion dollars to start an AI company. John Carmack raised $20M in 2022. Ilya raised $1b in 2024, and then another $2b a few months ago. Mira Murati raised $2b in July.</p><p>Ilya's company is valued at $32b. As far as I can tell, the only thing it has produced is the 370 words on its website. That is $86,486,486.49 per word. And 148 of those words are about Daniel Gross stepping away from the company. At least Ilya has a website, Carmack has literally disappeared.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!8fQm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8fQm!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 424w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 848w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 1272w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!8fQm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png" width="649" height="748" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:748,&quot;width&quot;:649,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8fQm!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 424w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 848w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 1272w, https://substackcdn.com/image/fetch/$s_!8fQm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb50c1f2f-a6e8-48dd-a689-dece3d61e3fa_649x748.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>apparently he's been hitting the gym</figcaption></figure></div><p>What is going on! Where are all of these people? What happened to the billions of dollars? Do they realize how many taco bell burritos you can buy with a billion dollars? Where are these people??? If any of you know what is going on at any of these places, please tell me.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
    </channel>
</rss>