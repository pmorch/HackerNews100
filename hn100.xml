<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 28 Sep 2023 06:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[HNInternal: Tell HN: Please don't print –-help to stderr in your CLI tools (128 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37682859</link>
            <guid>37682859</guid>
            <pubDate>Wed, 27 Sep 2023 23:22:53 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37682859">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="37683030"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37683030" href="https://news.ycombinator.com/vote?id=37683030&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>There's a case where this might be valid, which is where a command-line invocation is <i>incorrect</i> and a utility provides help <i>as part of an error message</i>.  In <i>that</i> case, writing to stderr is justifiable.<p>(Of course, an alternative argument is that commands should fail silently but emit a nonzero return value.)</p><p>When invoked <i>directly</i>, as with '-h' '--help', etc., <i>help output should write to stdout</i>, and <i>not</i> stderr.</p><p>StackOverflow has tackled this question, 2nd response follows the course I suggest:</p><p>&lt;<a href="https://stackoverflow.com/questions/1068020/app-help-should-go-to-stdout-or-stderr" rel="nofollow noreferrer">https://stackoverflow.com/questions/1068020/app-help-should-...</a>&gt;</p><p>And in this case, the first response:</p><p>&lt;<a href="https://stackoverflow.com/questions/2199624/should-the-command-line-usage-be-printed-on-stdout-or-stderr" rel="nofollow noreferrer">https://stackoverflow.com/questions/2199624/should-the-comma...</a>&gt;</p><p>I'm looking for any specific guidance from, e.g., GNU but am not finding any.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37683516"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37683516" href="https://news.ycombinator.com/vote?id=37683516&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>&gt; In that case, writing to stderr is justifiable.<p>More than justifiable, I'd say it's the correct thing to do in that case. Otherwise, the caller (which can be another script) may end up working with the help message thinking it was the output it expected.</p><p>The whole rule should be something like "Print to stdout if it's part of what's asked by the caller. Print to stderr if it wasn't asked but the user should know about it." So outputting it to stdout should happen when it's asked via --help, and outputting it to stderr should happen when it's part of an error.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37683629"><td></td></tr>
                <tr id="37683655"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37683655" href="https://news.ycombinator.com/vote?id=37683655&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>I think you missed the point. The guidance you quote doesn't cover the case described.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37683905"><td></td></tr>
                  <tr id="37683894"><td></td></tr>
                  <tr id="37684032"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37684032" href="https://news.ycombinator.com/vote?id=37684032&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>The traditional Unix command "philosophy" is that commands <i>succeed</i> quietly, using their termination status to indicate all is well. <i>Failing</i> quietly was never a part of it.<p>Chatter on success reads like a cheesy sci fi script.</p><pre><code>  &gt; copy * dir
  34 files copied, captain!

  &gt; mount plasma_cannon /dev/sdc
  plasma_cannon mounted, ready to fire, captain!
</code></pre>
A message like "incorrect arguments, use --help" can itself go to stderr. Not --help itself though.<p>Some GNU guidance is in the GNU Coding Standards:</p><p><a href="https://www.gnu.org/prep/standards/standards.html#Command_002dLine-Interfaces" rel="nofollow noreferrer">https://www.gnu.org/prep/standards/standards.html#Command_00...</a></p><p>That does say that --help and --version should go to standard output.</p><p>The document also gives a list of common options; i.e. don't invent your own name for an option, if something matches in this list.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37684456"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37684456" href="https://news.ycombinator.com/vote?id=37684456&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>&gt; Failing quietly was never a part of it.<p>Not part of it, but not against it. It's useful to stay quiet when the program is meant for conditions and failure is normal. For example: `test`/`[`, `false`, `grep` (when no matches are found), etc. Also when the program is meant as a sort of wrapper to other programs, like `ssh localhost false`, `script -qec false /dev/null`, `true | xargs false`, etc.</p><p>&gt; A message like "incorrect arguments, use --help" can itself go to stderr. Not --help itself though.</p><p>I don't agree that it's incorrect to save the user the step of calling --help, when it's obvious they need to see that info from an incorrect call. Once decided that including the --help message in an error is right, I don't think it's correct to include it in stdout when it's not expected.</p><p>This isn't an odd behavior either, including the --help message (or at least just the synopsis) in stderr on incorrect options is the behavior I'm seeing in utilities like GNU's `bash`, `grep`, and OpenBSD's `netcat`, for example.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37684182"><td></td></tr>
                  <tr id="37684001"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37684001" href="https://news.ycombinator.com/vote?id=37684001&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>This is exactly how I write my Bash functions. I didn't even realize it was a standard, it just made the most sense to me. Being given help via a --help argument is intentional and thus appropriate for STDOUT (and a return code of 0); being given help after an argument error makes sense to go to STDERR (and a return code of 2, "USAGE").<p>Since you can nest functions in Bash (did you know?), I usually have a help function within the main function that is called from both logic branches and just outputs to the right file descriptor.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37684195"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37684195" href="https://news.ycombinator.com/vote?id=37684195&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>&gt; (did you know?)<p>Yes, but they are not scoped to the parent:</p><pre><code>  $ foo()
  &gt; {
  &gt;    bar()
  &gt;    {
  &gt;       echo 42
  &gt;    }
  &gt; }
  $ foo
  $ bar # bar can be called though we are not in foo!
  42
</code></pre>
Probably the most profitable use for this is for individual functions to override some callback.<p>But without even dynamic scope, you have no nice way of restoring the previous one, which could be one that some intermediate caller installed for itself.</p><p>It could be used to delay the definition of functions. Say that for whatever reason, we put a large number of functions into a file and don't need them all to be defined at once. There can be functions which, when invoked, define sets of functions.</p><p>A module could be written in which certain functions are intended to be clobbered by the user with its own implementation. A function which defines those functions to their original state would be useful to recover from a bad redefinition, without reloading that module.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37683454"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37683454" href="https://news.ycombinator.com/vote?id=37683454&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>I disagree. stderr is a misnomer, in modern usage it has effectively become stdlog/stdinfo. The only thing that should go to stdout is the result of the program.<p>For example, many programs will print usage/help when used incorrectly. Imagine you upgrade the "read_reactor" tool, and your usage in your "control_reactor" becomes invalid - suddenly you're piping help message data to the control rods. By sending it to stderr instead, no bogus data would be piped and, as a bonus, you would see the help message after invoking your script because (as you have experienced) stderr is not piped by default.</p><p>If you want to send it to less: read_reactor -h 2&gt;&amp;1 | less
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37683536"><td></td></tr>
                <tr id="37683870"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37683870" href="https://news.ycombinator.com/vote?id=37683870&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>I disagree, the program's purpose isn't to show the help. Asking directly for help is one of the things in the path to getting desired output, along with getting an error.<p>If you're following such and such standards that says it should go to stdout, it should go to stdout though. I don't take that as a given.</p><p>I agree with this remaining open after all of these years: <a href="https://github.com/commandlineparser/commandline/issues/399">https://github.com/commandlineparser/commandline/issues/399</a> OP should add 2&gt;&amp;1 before the pipe or replace the pipe with |&amp; (bash) or &amp;| (fish)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37683911"><td></td></tr>
                <tr id="37683964"><td></td></tr>
                              <tr id="37683506"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37683506" href="https://news.ycombinator.com/vote?id=37683506&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>When passing dash dash help, the help is the output of the program, so it should go to stdout. When help is printed because the invocation was invalid, it should go to stderr. This is what most programs do, btw.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37683514"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37683514" href="https://news.ycombinator.com/vote?id=37683514&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>&gt; Imagine you upgrade the "read_reactor" tool, and your usage in your "control_reactor" becomes invalid<p>As others have noted, that's an error which shouldn't go to stdout.</p><p>But help text is not an error. It's arguably the expected and primay output of the help function.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37684024"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37684024" href="https://news.ycombinator.com/vote?id=37684024&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>... <i>Is</i> there a concept of a STDLOG or a STDINFO ? (STDDEBUG etc.)<p>These sound useful, in any event.</p><p>I'd also like to be able to pipe both STDOUT and STDERR to the next in a sequence of pipes, but eh
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37685356"><td></td></tr>
            <tr id="37684204"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37684204" href="https://news.ycombinator.com/vote?id=37684204&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>You could use &amp;3 etc, but that's non-standard. 2&gt;&amp;1 does pipe both into stdout, you would have to do 1&gt;/dev/null 2&gt;&amp;1 to get stderr only going to stdout.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37683234"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37683234" href="https://news.ycombinator.com/vote?id=37683234&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>print --help to stdout, but if the usage is part of an error (ex: because of bad arguments), then print it to stderr.<p>Many tools, for consistency or for laziness always print usage to stderr. But it is better than always printing it to stdout. Errors should never go to stdout, and paging stderr can easily be done with 2&gt;&amp;1.</p><p>Edit: and maybe, if your --help output is several pages long, consider leaving out the details to a manpage.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37683361"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37683361" href="https://news.ycombinator.com/vote?id=37683361&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>What's a page? I set all my terminals to 22x23. Will --help run a few quick ioctls to calculate screen size?<p>But yeah, agree. It's way more preferable to have surprise output on stderr than surprises mingling with stdout, and it's good to be prepared for that.</p><p>Frankly, I nearly quit piping to a pager when GUI terminal backscroll became easy and infinite.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37683461"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37683461" href="https://news.ycombinator.com/vote?id=37683461&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>&gt; What's a page?<p>80x25</p><p>&gt; I set all my terminals to 22x23.</p><p>You are a very silly man and silliness should not be catered for.</p><p>&gt; Will --help run a few quick ioctls to calculate screen size?</p><p>Your terminal can wrap text around just fine. If it can't, ask for refund.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37683504"><td></td></tr>
                <tr id="37683865"><td></td></tr>
            <tr id="37684325"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37684325" href="https://news.ycombinator.com/vote?id=37684325&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>In college we used to print out "one-page" ASCII art on the LPs. Naturally, it could get rather risqué. But I would just stare into them, marveling at the ingenuity. Of course I'd already been into it awhile, thanks to C64 Print Shop.<p>Also my father produced tonnes of scratch paper in narrow strips. I found a way to weave them into a perpetually-growing "tractor-feed snake" which I placed in a padded crib and brought to grade school to show off. I would typically juggle a few bits and bobs to attract even more attention...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37683659"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37683659" href="https://news.ycombinator.com/vote?id=37683659&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>I think the suggestion was not that the tool should page the output, but that the user can easily do so:<pre><code>    tool [args] 2&gt;&amp;1 | less
</code></pre>
Presumably "less" knows how big your terminal is.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37684041"><td></td></tr>
                <tr id="37684931"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37684931" href="https://news.ycombinator.com/vote?id=37684931&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span><pre><code>  $ /bin/bash --version
  GNU bash, version 3.2.57(1)-release (x86_64-apple-darwin21)
</code></pre>
on the off chance anyone was curious/didn't remember. Yes, I have $HOMEBREW_PREFIX/bin/bash and it's 5.2.15<p>In that same vein, I wondered if that syntax was supported in zsh since modern macOS went whole hog and ... I gained one more pebble on the huge pile of steaming turds why I detest zsh</p><pre><code>  $ /bin/zsh -exc '{ echo alpha; echo beta; } |&amp; wc -l'
  +zsh:1&gt; wc -l
       4
</code></pre>
dafuq?<pre><code>  $ /bin/zsh -exc '{ echo alpha; echo beta; } |&amp; cat'
  +zsh:1&gt; cat
  +zsh:1&gt; echo alpha
  alpha
  +zsh:1&gt; echo beta
  beta
</code></pre>
oh, aren't you just the funniest. har. de. har.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37685240"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_37685240" href="https://news.ycombinator.com/vote?id=37685240&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>I'm not sure that I follow: you chose options "-exc" so:<pre><code>  XTRACE (-x, ksh: -x)
  Print commands and their arguments as they are executed.
 </code></pre>
So if you use "zsh -ec" does it still surprise you?<p>-x is the most invaluable tool in my shell-debugging toolkit. It is great to see every command evaluated and run alongside the script output itself. I use it multiple times per day at work.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37683930"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37683930" href="https://news.ycombinator.com/vote?id=37683930&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>But in an ideal world, if every tool is careful to restrict it to one page or <i>less</i>, you'd literally never need a pager any<i>more</i>.<p>Alternatively, tools could build in pagers for fewer pipeline surprises, like the all-encompassing systemd.</p><p>Even better, I could envision a framework where any tool that produces output can be automatically subject to pagination, with auto terminal detection and the whole bit. Think of libreadline but for output. You could thereby eliminate plenty of <i>ad hoc</i> hacks.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="37683421"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37683421" href="https://news.ycombinator.com/vote?id=37683421&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>If you used --help, then decided to pipe it to less, and it disappeared, then you can't have been confused for <i>too</i> long. But agreed that an explicit --help should print to stdout.<p>However, if you used the tool incorrectly (passed the wrong args) and you expected the usage information to go to stdout rather than stderr, I would disagree vehemently. stdout is (generally) for parseable information, whereas stderr is kind of a garbage bin of everything else.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37683670"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37683670" href="https://news.ycombinator.com/vote?id=37683670&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>"But agreed that an explicit --help should print to stderr."<p>I think you meant stdout here, not stderr.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37683971"><td></td></tr>
                        <tr id="37685285"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37685285" href="https://news.ycombinator.com/vote?id=37685285&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>Question regarding this: should I print output to stdout and log to stderr? Or should only log levels Error and above go to stderr?
What about structured logging? Where does that go?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37684022"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37684022" href="https://news.ycombinator.com/vote?id=37684022&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>I always write my usage printing routine to take a FILE pointer, similar to the following,<pre><code>  static void usage(const char *arg0, FILE *fp);
</code></pre>
and then at the very end of main's getopt switch something like,<pre><code>  case 'h':
    usage(argv[0], stdout);
    return 0;
  default:
    usage(argv[0], stderr);
    return EXIT_FAILURE;
  }
</code></pre>
By default getopt prints (to stderr) a one line message for unknown options or missing arguments, so I usually don't bother catching ':' or '?' explicitly.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37683058"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37683058" href="https://news.ycombinator.com/vote?id=37683058&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>In the meantime until every tool is rewritten to account for your recommendation, just add 2&gt;&amp;1 and stderr will redirect to stdout, so you can easily pipe it to other stuff</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37683136"><td></td></tr>
                <tr id="37683383"><td></td></tr>
                <tr id="37683682"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37683682" href="https://news.ycombinator.com/vote?id=37683682&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>Ah bummer, every command i run need to be backward compatible with Bell Labs first release</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37683501"><td></td></tr>
                              <tr id="37683184"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37683184" href="https://news.ycombinator.com/vote?id=37683184&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>Yes, errors (and perhaps debug messages, diagnostics, etc.) should go to stderr.<p>The output of --help is not an error message, it's the legitimate and expected output of the program when invoked with that argument.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37684090"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37684090" href="https://news.ycombinator.com/vote?id=37684090&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>For this reason I have a zsh function in my .zshrc with bat (which pages by default, if it's longer than your console height):<p><a href="https://github.com/sharkdp/bat#highlighting---help-messages">https://github.com/sharkdp/bat#highlighting---help-messages</a></p><pre><code>  # in your .bashrc/.zshrc/*rc
  alias bathelp='bat --plain --language=help'
  help() {
      "$@" --help 2&gt;&amp;1 | bathelp
  }
</code></pre>
This highlights the help output with colors so it looks nicer, works with most help outputs, as it highlights the first part which is the flag/argument in one color and the description in another color</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37683274"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37683274" href="https://news.ycombinator.com/vote?id=37683274&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>I agree this makes sense. The help text is the primary output of the program in this case, so it should be stdout. (However, if you want longer documentation then a man page might be better anyways.)<p>You would use stderr for status messages, error messages, and other stuff that is not the primary output of the program. (In some cases, this might include help text; like another comment says, if you specified wrong arguments (not --help) then a short description might be a part of the error message.)</p><p>One program that does write error messages to stdout and that annoys me significantly is Ghostscript. (Although you can tell it to write it to stderr, doing that causes all output to be written to stderr; I want output from "print", "==", etc to be written to stdout.)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37684067"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37684067" href="https://news.ycombinator.com/vote?id=37684067&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>what really drives me crazy is when invoked<pre><code>    $ foo -h
</code></pre>
I get "haha, you idiot, there is no such option -h, rerun with --help"<p>when I rerun with --help, it prints a worse than useless usage string and says "for more help, type --help-advanced"
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37683262"><td></td></tr>
            <tr id="37683898"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37683898" href="https://news.ycombinator.com/vote?id=37683898&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>It depends.<p>Standard output is for program output. Standard error is something of an unfortunate name since it's actually a side channel for all non-output messages meant for the user to read.</p><p>So what's "output" anyway? Whatever the user asked the program to compute. If you pass a --help option, the help message is clearly the program's output because it's what the user asked for. If you use it incorrectly and the program prints usage information, the help message should go to standard error while the output is empty because the operation failed.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37684625"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37684625" href="https://news.ycombinator.com/vote?id=37684625&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>Is there any list of do’s and don’ts for cli? Common flags such as -h are used for help, but I’m not sure what other soft rules exist and don’t want to clobber well established paradigms when coding cli interfaces.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37683494"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37683494" href="https://news.ycombinator.com/vote?id=37683494&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>This feels like an easy one.<p>Not quite the same, but I really dislike programs which log status updates and more fundamental output to the same place.</p><p>IIRC, ffmpeg is like this with everything going to stderr, making metadata parsing more difficult than it needs to be.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37685094"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37685094" href="https://news.ycombinator.com/vote?id=37685094&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>In ffmpegs defense it is designed that the video stream can go on stdout. I guess it could log to stdout if you specify an explicit file and stderr when you want the video on stdout. But sometimes being consistent is better than being convenient.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37683527"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37683527" href="https://news.ycombinator.com/vote?id=37683527&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>&gt; IIRC, ffmpeg is like this with everything going to stderr, making metadata parsing more difficult than it needs to be.<p>If you're trying to parse metadata, ffprobe has a set of options for structured output to stdout. Parsing this will be <i>dramatically</i> easier than whatever you're doing.</p><pre><code>    ffprobe -print_format json -show_format -show_streams example.mp4
</code></pre>
(There's more -show_stuff options, but they're probably more detailed than you want. Run ffprobe --help for details.)</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37685068"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37685068" href="https://news.ycombinator.com/vote?id=37685068&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>&gt; Run ffprobe --help for details.<p>But we all wanted to know, does it stdout or stderr? :-D</p><p>(It prints help to stdout, and a bunch of junk to stderr)</p><pre><code>  $ ffprobe --help 2&gt;/dev/null
  Simple multimedia streams analyzer
  usage: ffprobe [OPTIONS] INPUT_FILE

  $ ffprobe --help &gt;/dev/null
  ffprobe version 6.0 Copyright (c) 2007-2023 the FFmpeg developers
    built with Apple clang version 14.0.0 (clang-1400.0.29.202)</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37682932"><td></td></tr>
                <tr id="37683647"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37683647" href="https://news.ycombinator.com/vote?id=37683647&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><p><span>Java (since version 9) has -version which prints to stderr and --version which prints to stdout (also with slightly different content). Same for -showversion and --show-version and to get back on topic -help -h and -? print to stderr while --help prints to stdout.<p><a href="https://docs.oracle.com/en/java/javase/17/docs/specs/man/java.html" rel="nofollow noreferrer">https://docs.oracle.com/en/java/javase/17/docs/specs/man/jav...</a></p><p>But since the more standard double-dash variants printing to stdout where added with Java 9 I would actually laud it as good backward compatibility.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                            <tr id="37683377"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37683377" href="https://news.ycombinator.com/vote?id=37683377&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>Disagree here. It's also unenforceable so there isn't much point trying to make rules out of it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37683423"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37683423" href="https://news.ycombinator.com/vote?id=37683423&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>Just because it's unenforceable doesn't mean you shouldn't make a rule. In fact it seems just the opposite, that's exactly the time to make a rule. If it were enforceable you wouldn't need a rule, you could just enforce it and have it be true.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37683446"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37683446" href="https://news.ycombinator.com/vote?id=37683446&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>It takes one popular tool breaking convention to make the rule obsolete. There's no point.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37683837"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37683837" href="https://news.ycombinator.com/vote?id=37683837&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>Its probably better to measure rules by success rate than pass/fail. One popular tool breaking convention sounds like one popular exception, which is probably better than no convention.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37683427"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37683427" href="https://news.ycombinator.com/vote?id=37683427&amp;how=up&amp;goto=item%3Fid%3D37682859"></a></center>    </td><td><br><div>
                  <p><span>Agree or not (i don’t), we have loads of unenforceable conventions in software. Hell, if this field is known for anything at all, it’s people getting worked up over subtle deviations from longstanding conventions.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37683050"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open Source doesn't win by being cheaper (205 pts)]]></title>
            <link>https://github.com/getlago/lago/wiki/Open-Source-does-not-win-by-being-cheaper</link>
            <guid>37682684</guid>
            <pubDate>Wed, 27 Sep 2023 23:04:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/getlago/lago/wiki/Open-Source-does-not-win-by-being-cheaper">https://github.com/getlago/lago/wiki/Open-Source-does-not-win-by-being-cheaper</a>, See on <a href="https://news.ycombinator.com/item?id=37682684">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wiki-body" data-view-component="true">
                <p>I cannot count the number of times I’ve heard, “This product is X, but open source.”</p>
<p>And I’ll admit it—I’ve done the same when describing Lago. When I’m not in the “startup pitch” mood, I default to, “We’re Stripe Billing, but open source”. Or my co-founder might say, “We’re like an open-source Chargebee.” Frankly, it gets the job done.</p>
<p>Of course, if that’s all there was to us, we would have failed by now. What we’ve learned is that open-source tools can’t rely on being an open-source alternative to an already successful business. A developer can’t just imitate a product, tag on an MIT license, and call it a day. As awesome as open source is, in a vacuum, it’s not enough to succeed.</p>
<p>To be clear, I’m talking about open-source projects that compete with popular paid solutions. Commercial open source, so to speak. Some community-driven, sponsored products—like <a href="https://react.dev/" rel="nofollow">React</a>, <a href="https://typeorm.io/" rel="nofollow">TypeORM</a>, or <a href="https://code.visualstudio.com/" rel="nofollow">VSCode</a>—have different priorities. They are either bankrolled by a larger organization (e.g., Meta for React) or rely on donations to fund development (e.g., TypeORM). They aren’t businesses at heart.</p>
<p>But open-source companies, like us, need to be more than just an open-source alternative to succeed. They either need a concrete reason for why they are open source or have to surpass their competitors.</p>
<h2 id="user-content-profit-not-usage-is-a-measure-of-success"><a href="#profit-not-usage-is-a-measure-of-success">Profit, not usage, is a measure of success</a></h2>
<p>Excluding non-profit projects that are sponsored by donations or parent organizations, a typical open-source business needs profit to be the ultimate north star.</p>
<p>For some, this might be a tough pill to swallow. But a for-profit business exists for profit. Definitionally and practically. Profit is what allows the company to hire employees, grow, and sustain itself—it is quite literally what funds ongoing development. And there’s nothing wrong with that; the fact that businesses can be profitable while building free software is a great thing. It’s not that open-source companies aspire to gouge customers—but they do aspire to remain in business.</p>
<p>This is precisely what has enabled <a href="https://www.mongodb.com/" rel="nofollow">MongoDB</a> to grow into one of the largest databases with over <a href="https://www.mongodb.com/company#:~:text=We%20have%204%2C600%2B%20people%20contributing%20to%20our%20success." rel="nofollow">4,600</a> employees. (Granted, MongoDB eventually switched to a <a href="https://www.mongodb.com/licensing/server-side-public-license" rel="nofollow">special SSPL license</a> to add specific restrictions on Cloud Providers from distributing a service without contributing to the project, which isn’t OSI approved, but is open-source practically).</p>
<p>But why make this point? Well, splitting the hair between profit and usage is important to measuring long-term success. If a project gets great adoption but cannot drive revenue, it will die. Some wishful thinking might argue that the community will take over, but there’s been little evidence to indicate this happens.</p>
<h2 id="user-content-how-open-source-does-not-winby-being-cheaper"><a href="#how-open-source-does-not-winby-being-cheaper">How open source does not win—by being cheaper</a></h2>
<p>Catering to the price-conscious is a losing battle.</p>
<p>Imagine a buddy who wants to create an open-source version of <a href="https://amplitude.com/" rel="nofollow">Amplitude</a>. She argues that Amplitude is pretty expensive, particularly prohibitive for some early-stage companies, and larger companies could save a fortune by using an open-source version.</p>
<p>In theory, sure. While this pitch might resonate with early-stage companies, those same price-conscious early-stage companies will either use an open-source version or a free tier. That is not enough to sustain the business. Building a cheaper alternative is typically a ticket to future bankruptcy.</p>
<p>What about larger companies? With some caveats, larger companies aren’t worried about going out of business because their Amplitude is too expensive. They might be price-conscious in terms of negotiating a contract in context of their budget, but most SaaS software is just a line item at the end of the day. What matters more is that the solution is (a) a good solution, (b) around for the long haul, and (c) easy to manage. And, unfortunately, deploying an open-source solution can be tricky to manage.</p>
<p>The caveat is if the solution is a massive cost on the overall budget, then a corporation may seek out a more price-friendly solution; there are plenty of companies that needed to kick Oracle when their database costs skyrocketed due to usage. But most open-source solutions aren’t replacing a top-three line item, and therefore price isn’t the north star deciding factor.</p>
<h2 id="user-content-how-open-source-winsby-solving-a-transparency-problem"><a href="#how-open-source-winsby-solving-a-transparency-problem">How open source wins—by solving a transparency problem</a></h2>
<p>A great case for an open-source solution is when a transparency problem is present. What is a transparency problem? It’s when a solution being closed source creates distrust between the client and vendor.</p>
<p>Let’s return to the previous example—open-source Amplitude. That product actually does exist, and it is winning: <a href="https://posthog.com/" rel="nofollow">PostHog</a>. Growing with Airbus, DHL, and Staples as clients, PostHog combines a few product SaaS solutions together and is open source. It’s actually super open source—even the blog source code and <a href="https://posthog.com/roadmap" rel="nofollow">roadmap</a> is available publicly. We’ve modeled a lot of our open-source mantras after them.</p>
<p>PostHog is positioned as a better product than its competitors because analytics tools need to process sensitive client data such as IP addresses, names, session recordings, etc. In a world of increasing data regulations (e.g., GDPR and CCPA), it can be daunting for third parties to store that data. So PostHog provides an alternative—either (a) self-host an analytics solution yourself or (b) hire PostHog as a third party to do it, but with transparency into how that data is stored and how possibly migrating to self-hosted in the future would work.</p>
<p>This split is important. Even if the most privacy-conscious technique is to self-host the open-source solution (which often leads to no revenue for the developer), many companies will still opt for a hosted model. But they do so now with assurances of how the software works—line by line—and the process of migrating to a self-hosted model in the future if necessary. It’s not that open-source companies win by preventing the need for a third party; they win by allowing for the open audit of how it works.</p>
<p>Nowadays, there are many early open-source solutions taking advantage of these selling points. <a href="https://www.medplum.com/" rel="nofollow">Medplum</a> is an open-source electronic health record platform competing with closed-source incumbents; being open-source gives users assurance of exactly what is and isn’t supported by the platform. <a href="https://supertokens.com/" rel="nofollow">SuperTokens</a> is an open-source alternative to authentication solutions like <a href="http://auth0.com/" rel="nofollow">Auth0</a>; logins deal with sensitive data including names, emails, and passwords, and by being open source, Supertokens is able to capture more trust. <a href="https://tableflow.com/" rel="nofollow">TableFlow</a> is an open-source alternative to CSV import platforms like <a href="https://flatfile.com/?" rel="nofollow">Flatfile</a>; once again, the imported data is sensitive.</p>
<p>One of the biggest ones is <a href="https://min.io/" rel="nofollow">Minio</a>, which is an open-source alternative to AWS S3 storage. Given that S3 often stores customer PII (either inadvertently via screenshots or actual structured JSON files), Minio is a great alternative to companies mindful of who has access to user data. Of course, <a href="https://aws.amazon.com/compliance/privacy-features/#:~:text=We%20prohibit%2C%20and%20our%20systems,or%20to%20comply%20with%20law." rel="nofollow">AWS claims that AWS personnel</a> doesn’t have direct access to customer data, but by being closed-source, that statement is just a function of trust.</p>
<p>The list goes on and on. We could say the same about our <a href="https://www.getlago.com/blog/engineers-need-open-source-to-end-their-billing-nightmares" rel="nofollow">own story</a>; we deal with billing and product usage information, which is pretty high on the list of sensitive content. By being open source, we’re able to better build trust among our users.</p>
<h2 id="user-content-how-open-source-winsby-solving-an-extensibility-problem"><a href="#how-open-source-winsby-solving-an-extensibility-problem">How open source wins—by solving an extensibility problem</a></h2>
<p>One of the big benefits of open source is that it opens the development of niche features to the community. While the core product is typically maintained by a central engineering team, integrations or plugins are often built by community developers and then occasionally merged into the main branch. Conversely, closed-source solutions struggle with this because they rely on their engineering team</p>
<p>This is particularly advantageous for open-source companies building systems that require lots of connections with other libraries, frameworks, or applications. For instance, Airbyte, an open-source ELT platform, blew up because of community-driven additions to its connectors. The same could be said about Elastic, an even bigger originally open-source company that hosts a <a href="https://www.elastic.co/integrations/data-integrations" rel="nofollow">bastion of data integrations</a>.</p>
<p>One of the founders I’ve met is Advait Ruia of Supertokens. For them, extensibility was a core part of their value proposition, as their community members are able to build integrations with uncommon authentication providers, benefiting everyone.</p>
<h2 id="user-content-how-open-source-winsby-being-better"><a href="#how-open-source-winsby-being-better">How open source wins—by being better</a></h2>
<p>Both of the above issues contribute to commercial open-source being a better product in the long run. But by tapping the community for feedback and help, open-source projects can also accelerate past closed-source solutions. PostHog—<a href="https://posthog.com/blog/15-million-series-b" rel="nofollow">who raised a $15M Series B</a>—started as an alternative to Amplitude and <a href="https://www.fullstory.com/" rel="nofollow">FullStory</a>, but has accelerated into a massive, encompassing solution that even competes with <a href="https://launchdarkly.com/" rel="nofollow">LaunchDarkly</a> and <a href="https://www.pendo.io/" rel="nofollow">Pendo</a>. This happened over the last few years, and they heavily credit their community as one of the core reasons.</p>
<p>Open-source projects—not just commercial open source—have served as a critical driver for the improvement of products for decades. However, some software is going to remain closed source. It’s just the nature of first-mover advantage. But when transparency and extensibility are an issue, an open-source successor becomes a real threat.</p>
<p>‍</p>

              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lisp Badge LE (137 pts)]]></title>
            <link>http://www.technoblogy.com/show?3Z2Y</link>
            <guid>37682273</guid>
            <pubDate>Wed, 27 Sep 2023 22:27:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.technoblogy.com/show?3Z2Y">http://www.technoblogy.com/show?3Z2Y</a>, See on <a href="https://news.ycombinator.com/item?id=37682273">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

<p>27th September 2023</p>
<p>This is a self-contained low-power computer with its own display and keyboard that you can program in uLisp, a version of the high-level language Lisp for microcontrollers:</p>
<p><img src="http://www.technoblogy.com/pictures/kvm/lispbadgepulse.jpg" alt="LispBadgePulse.jpg" width="560" height="370"></p>
<p><em>The Lisp Badge LE, a low-power computer programmed in Lisp<br>with a self-contained keyboard and display.</em></p>
<p>It's based on an AVR128DB48 which provides 128K bytes of flash memory, and 16K bytes of RAM. There's an integrated 45-key keyboard optimised for Lisp, using reverse-mounted buttons giving room for large key legends on the PCB.</p>
<p>It has a low-power monochrome display, readable in daylight without a backlight, so it's ideal for programming in the garden or on the beach! This gives 10 lines of 41 characters, or graphics with a resolution of 250x122 pixels, supported by several graphics commands.</p>
<p>You can use it to run programs that interface to components such as LEDs and push-buttons via the I/O pins, read the analogue inputs, and operate external devices via the I2C and SPI interfaces.</p>
<h3>Introduction</h3>
<p>A few years ago I designed the <a href="http://www.technoblogy.com/show?2AEE">Lisp Badge</a>, a self-contained computer with its own display and keyboard, based on an ATmega1284, that you could program in Lisp. Since then I've been thinking about how I could improve it, and made a list of features I'd like to add.</p>
<p>On the one hand I wanted it to have a better keyboard, and be low power, powered from a button cell, with an eInk display that you could see in daylight. On the other hand I wanted it to have a colour TFT graphics display, and use a fast 32-bit ARM processor, with support for floating-point arithmetic.</p>
<p>I soon realised that these requirements are incompatible in a single design, and so set about designing two different Lisp Badges to meet the two sets of requirements. This Lisp Badge LE (low energy) is the first of those designs, and has the following new features:</p>
<h4>Processor and memory</h4>
<p>It’s based on an AVR128DB48 (or AVR128DA48) running at 24MHz, and provides 2800 Lisp objects, about the same as the original Lisp Badge. You can save the entire workspace to flash.</p>
<h4>Current consumption</h4>
<p>The Lisp Badge LE draws only 6mA from its CR2032 button cell, and so should have a life of about 40 hours. There’s a physical on/off switch too for long periods of inactivity.</p>
<h4>Lisp language</h4>
<p>The Lisp Badge LE runs the AVR version of uLisp which provides 16-bit integer arithmetic, arbitrary length symbols and strings, lists, multi-dimensional arrays, Arduino interface functions, debugging features, and built-in documentation.</p>
<h4>Display</h4>
<p>The display is a low-power monochrome graphics display&nbsp;<sup id="cite_ref1"><a href="#cite_note1">[1]</a></sup> which I explored in an earlier article; see <a href="http://www.technoblogy.com/show?3YB0">Monochrome Low-Power Display Library</a>. It has&nbsp;a resolution of 250x122 pixels, and a text resolution of 10 lines of 41 characters per line. It supports reading back from the display, which makes it possible to support a full range of graphics functions, including plotting points,&nbsp;drawing lines, drawing outline and filled rectangles circles or triangles, and plotting characters and text at normal size or enlarged by any integer scale factor.</p>
<h4>Keyboard</h4>
<p>The keyboard takes advantage of push buttons that mount on the reverse of the board, with the button caps protruding through holes in the PCB. This makes it much easier to use than on the original Lisp Badge because it's easier to press the keys, and there's space for larger key legends. The push buttons are available from Adafruit&nbsp;<sup id="cite_ref2"><a href="#cite_note2">[2]</a></sup>. or The Pi Hut in the UK&nbsp;<sup id="cite_ref3"><a href="#cite_note3">[3]</a></sup>.</p>
<p>It uses the same&nbsp;45-key layout as the original Lisp badge, with upper and lower-case characters, digits, and the symbols required by uLisp. However, it now provides an addition&nbsp;<strong>META</strong>&nbsp;modifier key in addition to <strong>SHIFT</strong>, allowing you to enter characters that don't have a dedicated key on the keyboard.</p>
<h4>Peripherals</h4>
<p>There’s a large piezo speaker that supports playing notes, and a reverse-mounting LED that shines through a hole on the front of the board.</p>
<p>Here's the full specification:</p>
<div>
<h3>Lisp Badge – Specification</h3>
<p><strong>Size:</strong> 107mm x 61mm (4.2" x 2.4").</p>
<p><strong>Display</strong>: 41 characters x 10 lines, or 250 x 122 pixels.</p>
<p><strong>Keyboard:</strong>&nbsp;Integrated 45-key keyboard providing upper and lower-case characters, digits, and the symbols required by uLisp.</p>
<p>The <strong>META</strong> key pressed in conjunction with another key gives access to the following characters not available from the main keyboard:</p>
<table>
<thead>
<tr>
<td><strong>META +</strong></td>
<td>A</td>
<td>C</td>
<td>D</td>
<td>E</td>
<td>P</td>
<td>Q</td>
<td>T</td>
<td>U</td>
<td>V</td>
<td>&lt;</td>
<td>&gt;</td>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Character</strong></td>
<td>&amp;</td>
<td>:</td>
<td>$</td>
<td>!</td>
<td>%</td>
<td>?</td>
<td>@</td>
<td>^</td>
<td>|</td>
<td>{</td>
<td>}</td>
</tr>
</tbody>
</table>
<p><strong>Memory available</strong>: 2800 Lisp cells (11200 bytes).</p>
<p><strong>Flash</strong>: 16384 bytes of flash are reserved for use to save the Lisp workspace using <strong>save-image</strong>.</p>
<p><strong>Processor:</strong> AVR128DB48</p>
<p><strong>Clock speed:</strong> 24 MHz.</p>
<p><strong>Current consumption:</strong> Approx. 6 mA. A CR2032 cell has a typical capacity of 225 mAh, so this should give a life of about 40 hours.</p>
<p><strong>Types supported</strong>: list, symbol, integer, character, string, stream, and array.</p>
<p>An integer is a sequence of digits, optionally prefixed with "+" or "-". Integers can be between -32768 and 32767. You can enter numbers in hexadecimal, octal, or binary with the notations #x2A, #o52, or #b101010, all of which represent 42.</p>
<p>User-defined symbol names can have arbitrary names. Any sequence that isn't an integer can be used as a symbol; so, for example, 12a is a valid symbol.</p>
<p>There is one namespace for functions and variables; in other words, you cannot use the same name for a function and a variable.</p>
<p>Includes a mark and sweep garbage collector. Garbage collection takes 5 msec.</p>
<h4><strong>Language</strong></h4>
<p>uLisp, a subset of Common Lisp, with the following 196 Lisp functions and special forms:</p>
<p><strong>* + - / /= 1+ 1- &lt; &lt;= = &gt; &gt;= ? abs analogread analogreadresolution analogreference analogwrite and append apply apropos apropos-list aref array-dimensions arrayp ash assoc atom bit boundp break caaar caadr caar cadar caddr cadr car case cdaar cdadr cdar cddar cdddr cddr cdr char char-code characterp check-key closure cls code-char concatenate cond cons consp dacreference decf defcode defun defvar delay digitalread digitalwrite documentation dolist dotimes draw-char draw-circle draw-line draw-pixel draw-rect&nbsp;</strong><strong>draw-triangle&nbsp;</strong><strong>edit eq equal error eval evenp fill-circle fill-rect fill-screen&nbsp;</strong><strong>fill-triangle</strong><strong>&nbsp;first for-millis format funcall gc get-pixel globals glyph-pixel if ignore-errors incf integerp keyboard keywordp lambda length let let* list list-library listp load-image locals logand logbitp logior lognot logxor loop make-array makunbound mapc mapcan mapcar max member millis min minusp mod not note nothing nth null numberp oddp or pinmode plot plot3d plusp pop pprint pprintall prin1 prin1-to-string princ princ-to-string print progn push quote random read read-byte read-from-string read-line register require rest restart-i2c return reverse room save-image search second set set-cursor setf setq sleep sort streamp string string&lt; string= string&gt; stringp subseq symbolp t terpri third time trace truncate unless untrace unwind-protect when with-i2c with-output-to-string with-sd-card with-serial with-spi write-byte write-line write-string zerop</strong></p>
<p>It also provides 37 keywords such as <strong>:input</strong>, <strong>:output</strong>, and <strong>:led-builtin</strong>, as a convenient way of entering Arduino constants.</p>
<p>For a full definition see <a href="http://www.ulisp.com/show?3L" target="_blank">uLisp Language Reference</a>.</p>
<h4>Graphics extensions</h4>
<p>The Lisp Badge LE includes a graphics library to allow you to do plotting on the display; for details see&nbsp;<a href="http://www.ulisp.com/show?31GT" target="_blank">Graphics extensions</a>. These work using a coordinate system with the origin at top left:</p>
<p><img src="http://www.technoblogy.com/pictures/kvm/graphicsdisplaycoords5.gif" alt="GraphicsDisplayCoords5.gif" width="265" height="135"></p><p>
The following example shows a plot of the fractal Dragon Curve; for the program see&nbsp;<a href="http://www.ulisp.com/show?4JLZ" target="_blank">Dragon curve</a>:
</p><p><img src="http://www.technoblogy.com/pictures/kvm/lispbadgeledragon.jpg" alt="LispBadgeLEDragon.jpg" width="560" height="370"></p>
<p><em>A fractal Dragon Curve drawn in Lisp on the Lisp Badge LE using the graphics extensions.</em></p>
<h4>Assembler</h4>
<p>The Lisp Badge LE also includes an AVR assembler&nbsp;which allows you to generate and run machine-code functions, written in AVR mnemonics, using an assembler written in Lisp. For details see <a href="http://www.ulisp.com/show?3IUO">AVR assembler overview</a>.</p>
<p>The language includes several extensions specifically for the Lisp Badge, including <strong>plot</strong> and <strong>plot3d</strong>, for plotting graphs and 3d functions, and <strong>keyboard</strong> and <strong>check-key</strong> for reading the keyboard in real time. For details see <a href="http://www.ulisp.com/show?4JJ6" target="_blank">Lisp Badge LE extensions</a>.</p>
<h4><strong>Interfaces</strong></h4>
<p>These interfaces are brought to headers at the edge of the Lisp Badge LE board. The numbers in brackets refer to Arduino pin numbers:</p>
<ul>
<li>Eight analogue input pins using <strong>analogread</strong>: PD0 to PD7 (22 to 29).</li>
<li>VCC, GND, and UPDI.</li>
<li>Two analogue outputs using <strong>analogwrite</strong>:&nbsp;MOSI0 (4) and MISO0 (5).</li>
<li>Digital input and output using <strong>pinmode</strong>, <strong>digitalread</strong>, and <strong>digitalwrite</strong>:&nbsp;TX0 (0), RX0 (1), SCL0 (2), SDA0 (3), MOSI0 (4), MISO0 (5), SCK0 (6),&nbsp;and PD0 to PD7 (22 to 29)</li>
<li>I2C interface using <strong>with-i2c</strong> and <strong>restart-i2c</strong>: SCL0 (2) and SDA0 (3), plus VCC and GND.</li>
<li>SPI interface using <strong>with-spi</strong>: MOSI0 (4), MISO0 (5), and SCK0 (6), plus VCC and GND.</li>
<li>Serial interface (FTDI) using&nbsp;<strong>with-serial</strong>: TX0 (0) and RX0 (1), plus DTR, VCC, and GND.</li>
</ul>
<p>Analogue output PB4 (12) is connected to a piezo speaker, which can use&nbsp;<strong>analogwrite</strong> or<strong> note.</strong></p>
<p>The <strong>SHIFT</strong> (33) and <strong>META</strong> (11) keys can be used as digital inputs, read with <strong>digitalread</strong> and referenced as <strong>:shift-key</strong> and <strong>:meta-key</strong>.</p>
<p>PA7 (7) is connected to an LED on the front panel, referenced as <strong>:led-builtin</strong>. You can turn it on and off with <strong>digitalwrite</strong>, or vary its brightness with <strong>analogwrite</strong>.</p>
</div>
<h3>Entering programs</h3>
<p>You can enter commands and programs by typing them at the keyboard, and pressing ENTER. A keyboard buffer is provided that buffers a full screen of text, and you can use the DEL key to delete characters and correct typing mistakes. The line editor&nbsp;includes parenthesis matching which&nbsp;automatically highlights matching brackets in inverse video as you type in a program. This makes it much easier to enter a program correctly, and is especially helpful in telling you how many closing brackets to type at the end of defining a function.</p>
<p>For example, the following program pulsates the built-in LED slowly on and off:</p>
<pre>(defun pulse (&amp;optional (up t))
  (dotimes (x 256)
    (analogwrite :led-builtin (if up x (- 255 x)))
    (delay 8))
  (pulse (not up)))</pre>
<p>It's shown nicely formatted here for clarity, but you can type it in as one continuous line, only pressing ENTER at the end. To run the program type:</p>
<pre>(pulse)</pre>
<h4>Connecting to a computer</h4>
<p>You can connect the Lisp Badge to a computer by plugging a 3.3V FTDI USB-to-serial converter onto the FTDI connector on the top right of the Lisp Badge, and then connecting this to the computer using a USB cable. You can then use the Serial Monitor in the Arduino IDE to enter and edit programs as described in <a href="http://www.ulisp.com/show?19XT">Using uLisp</a>.</p>
<p>I used the 3.3V FTDI Basic Breakout from Sparkfun <sup id="cite_ref4"><a href="#cite_note4">[4]</a></sup>. When using it with the Lisp Badge it powers the Lisp Badge, and so the battery should be left switched off.</p>
<h3>The circuit</h3>
<p>Here's the main part of the circuit:</p>
<p><img src="http://www.technoblogy.com/pictures/kvm/lispbadgele.gif" alt="LispBadgeLE.gif" width="720" height="401"></p>
<p><em>Circuit of the Lisp Badge, based on an AVR128DB48 microcontroller.</em></p>
<p>The keyboard arranges the keys in a matrix of four rows and 11 columns:</p>
<p><img src="http://www.technoblogy.com/pictures/kvm/lispbadgekybd.gif" alt="LispBadgeKybd.gif" width="720" height="206"></p>
<p><em>The circuit of the Lisp Badge keyboard matrix.</em></p>
<h4>► Parts list</h4>

<h3>Construction</h3>
<p>I created a PCB in Eagle, and ordered a set of boards from <a href="https://www.pcbway.com/" target="_blank">PCBWay</a>. I chose white PCBs to contrast with the black buttons. The board is the same width as the original Lisp Badge, but slightly taller to accomodate the larger display.</p>
<p>After a bit of experimentation I chose a board thickness of 1.2mm. With the standard 1.6mm thick PCB material the reverse-mounted buttons didn't protrude far enough through the holes, and 0.8mm made the boards too bendy, but 1.2mm was ideal.</p>
<p>The components, apart from the display, are mounted on the back of the board:</p>
<p><img src="http://www.technoblogy.com/pictures/kvm/lispbadgeleback.jpg" alt="LispBadgeLEBack.jpg" width="560" height="370"></p>
<p><em>The reverse side of the Lisp Badge printed circuit board.</em></p>
<p>The push buttons are mounted at a 15° angle, which makes it possible to pack them closer together. I've taken advantage of the fact that the terminals on each side of the push buttons are connected together to link the rows and columns of the key matrix across the board, and simplify the PCB track layout. A consequence of this is that if one push button is omitted, or has a faulty connection, all the other buttons to its left or below it across the keyboard may not work. Understanding this should help you track down any faulty soldering.</p>
<p>The PCB uses 0805&nbsp;resistors and capacitors. The LED is a reverse-mounting 1206 LED <sup id="cite_ref5"><a href="#cite_note5">[5]</a></sup> to make the front of the board flush.</p>
<p>The board will accommodate either a 16 x 16mm SMD piezo speaker <sup id="cite_ref6"><a href="#cite_note6">[6]</a></sup>, or a 11 x 9mm SMD piezo speaker&nbsp;<sup id="cite_ref7"><a href="#cite_note7">[7]</a></sup>.</p>
<p>The battery holder is an SMD 20mm coin cell holder available from Sparkfun <sup id="cite_ref8"><a href="#cite_note8">[8]</a></sup>, or from Proto-PIC in the UK&nbsp;<sup id="cite_ref9"><a href="#cite_note9">[9]</a></sup>. Note that on many button cells the negative terminal is concave, and doesn't make good contact with the pad on the PCB. I therefore recommend melting some solder on the circular PCB contact to provide extra thickness before mounting the battery holder.</p>
<p>The display is held in place with a double-sided self-adhesive foam pad, and then soldered to the board with the seven header pins. There are also four mounting holes if you prefer to use screw mounting. I recommend leaving the display uninstalled until you have tested the rest of the board, because it's likely to be sensitive to overheating.</p>
<p>I used a Youyue 858D+ hot air gun at 275°C to solder the small SMD components and push buttons, and then used a conventional soldering iron for the display, battery holder, switch, piezo speaker, and connectors.</p>
<p>The PCB has space for a 5 x 3.2mm 32.768kHz crystal and its capacitors which you could use with the RTC peripheral to provide accurate timekeeping. However, I didn't fit it on the prototype as the internal clock is already pretty accurate.</p>
<h3>The program</h3>
<p>The Lisp Badge program is based on the code for the AVR version of uLisp Version 4.4 (see <a href="http://www.ulisp.com/show?1AA0" target="_blank">Download uLisp</a>), with the addition of routines to handle the display and keyboard, and the plot extensions.</p>
<h4>Display</h4>
<p>The display routine is based on the routine I developed for the display in my project <a href="http://www.technoblogy.com/show?3YB0">Monochrome Low-Power Display Library</a>. The display driver is the ST7302&nbsp;<sup id="cite_ref10"><a href="#cite_note10">[10]</a></sup>, and it shares some similarities with the ST7735 used for colour TFT displays. Here's the display layout:</p>
<p><img src="http://www.technoblogy.com/pictures/kvm/graphicdisplaylayout6.gif" alt="GraphicDisplayLayout3.gif" width="436" height="198"></p>
<p>The display is divided into 11 lines, each of which is 12 pixels high, and 125 columns, each of which is 2 pixels wide. Note that because 122 isn't an exact multiple of 12, only part of the last line actually appears on the display.</p>
<p>Unlike on the colour TFT displays each pixel isn't individually addressable; in fact, the minimum update is to write three bytes to the display, which defines the state of a block of 2x12 pixels corresponding to one column and one row. Fortunately the display supports reading back the display memory, so it's not necessary to maintain a copy of the display in RAM to do things like line drawing; instead, to set a single pixel we can read the 2x12 block, change one pixel, and write it back.</p>
<h4>Reading from the display</h4>
<p>The&nbsp;routine&nbsp;<strong>ReadBlock()</strong>&nbsp;reads a 12x2 pixel block from the display memory specified by&nbsp;the column, from 0 to 124, and the line, from 0 to 10:</p>
<pre>uint32_t ReadBlock (uint8_t column, uint8_t line) {
  uint32_t pix = 0;
  PORT_TOGGLE(1&lt;&lt;cs);
  Command2(CASET, 25+line, 25+line);
  Command2(RASET, column, column);
  Command(RAMRD);
  PORT_INPUT(1&lt;&lt;mosi);                     // mosi input
  for (uint8_t i=0; i&lt;25; i++) {
    PORT_TOGGLE(1&lt;&lt;sck);
    pix = pix&lt;&lt;1 | (PORT_IN&gt;&gt;mosi &amp; 1);
    PORT_TOGGLE(1&lt;&lt;sck);
  }
  PORT_OUTPUT(1&lt;&lt;mosi);                    // mosi output
  PORT_TOGGLE(1&lt;&lt;cs);
  return pix;
}</pre>
<p>When reading from display memory you have to do a dummy read, which is why the main loop is executed 25 times rather than 24 times.</p>
<p>The companion routine <strong>PlotBlock()</strong> writes back a 12x2 pixel block to the display memory:</p>
<pre>void PlotBlock (uint32_t block, uint8_t column, uint8_t line) {
  PORT_TOGGLE(1&lt;&lt;cs);
  Command2(CASET, 25+line, 25+line);
  Command2(RASET, column, column);
  Command(RAMWR); Data(block&gt;&gt;16); Data(block&gt;&gt;8); Data(block);
  PORT_TOGGLE(1&lt;&lt;cs);
}</pre>
<h4>Scrolling</h4>
<p>The display doesn't provide hardware scrolling in the vertical direction, so scrolling is done in software by calling <strong>ScrollDisplay()</strong>, which uses the <strong>ReadBlock()</strong>&nbsp;and <strong>PlotBlock()</strong> routines:</p>
<pre>void ScrollDisplay () {
  uint32_t block, block2;
  for (uint8_t x = 0; x &lt; Columns*3; x++) {
    block = ReadBlock(x, 0);
    for (uint8_t y = 1; y &lt; Lines; y++) {
      block2 = ReadBlock(x, y);
      if (block2 != block) {                   // Optimisation if already the same
        PlotBlock(block2, x, y-1);
        block = block2;
      }
    }
  }
  ClearLine(LastLine);
}</pre>
<p>The routine optimises the scrolling by only writing blocks when necessary if they have been changed by the scrolling.</p>
<h4>Keyboard</h4>
<p>The keyboard uses the AVR128DB48's Timer/Counter TCB3 to generate an interrupt at about 250Hz. Each call of the interrupt service routine takes the next column low in the keyboard matrix, and it then tests to see if any of the four row inputs has been pulled low by the pressing of a push button. If so, the button's position is looked up in the key table <strong>Keymap[]</strong> to translate it to the ASCII code of the key. This also takes into account the state of the SHIFT and META keys, which are connected to dedicated inputs.</p>
<p>A&nbsp;keyboard buffer buffers a full screen of text, so you can use the DEL key to delete characters and correct typing mistakes. The line editor includes parenthesis matching which automatically highlights matching brackets in inverse video as you type in a program.</p>
<h3>Installing a bootloader</h3>
<p>To program the Lisp Badge I recommend using Spence Konde's <a href="https://github.com/SpenceKonde/DxCore" target="_blank">DxCore on GitHub</a>.</p>
<p>The first step is to install a bootloader using a UPDI programmer. I used a SerialUPDI programmer, based on a 3.3V FTDI USB-to-serial converter. Because the display is only tolerant of 3.3V you should power the Lisp Badge from 3.3V while programming, or do the programming before connecting the display to the board.</p>
<p>Choose the <strong>AVR DB-series (Optiboot)&nbsp;</strong>option under the&nbsp;<strong>DxCore</strong>&nbsp;heading on the&nbsp;<strong>Board</strong>&nbsp;option on the <strong>Tools</strong> menu. Check that the subsequent options are set as follows (leave other options at their defaults):</p>
<p><strong>Chip: "AVR128DB48"<br>Clock Speed: "24MHz internal"</strong><br> <strong>Bootloader Serial Port (Bootloader burn req'd): "USART0 (default pins): TX PA0, RX PA1"</strong></p>
<p>Set the <strong>Programmer</strong> option as appropriate for the UPDI programmer you are using. Then choose <strong>Burn Bootloader</strong>.</p>
<h3>Uploading the Lisp Badge code</h3>
<p>The next step is to install the Lisp Badge code via the serial connection, using the bootloader you have just installed. Plug a 3.3V FTDI USB-to-serial converter into the FTDI connector on the PCB. I used the SparkFun FTDI Basic Breakout&nbsp;<sup id="cite_ref11"><a href="#cite_note11">[11]</a></sup>.</p>
<p>Leave the <strong>DxCore</strong> options set as described above, and choose the USB port from the <strong>Port</strong> submenu. Then choose <strong>Upload</strong> from the <strong>Sketch</strong> menu to upload uLisp.</p>
<p>Note that this stage will fail if you didn't set the <strong>Bootloader Serial Port</strong> correctly in the previous step, and you'll need to go back and repeat that step with the correct setting.</p>
<p>You should then be able to choose <strong>Serial Monitor</strong> from the <strong>Tools</strong> menu in the Arduino IDE, and type Lisp commands at the uLisp prompt.</p>
<h3>Resources</h3>
<p>Get the latest Lisp Badge LE source from GitHub, together with the Eagle files for the PCB so you can make yourself a board, at:&nbsp;<a href="https://github.com/technoblogy/lisp-badge-le" target="_blank">Lisp Badge&nbsp;LE</a>.</p>
<p>Or order a board from PCBWay here: <a href="https://www.pcbway.com/project/shareproject/W157031ASS72_Lisp_Badge_7_7f9492ee.html" target="_blank">Lisp Badge LE</a>.</p><hr>
<ol>
<li id="cite_note1"><a href="#cite_ref1">^</a> <a href="https://www.aliexpress.com/item/1005005692052464.html" target="_blank">2.13" 122*250 E-paper ink screen TFT Module</a> on AliExpress.</li>
<li id="cite_note2"><a href="#cite_ref2">^</a> <a href="https://www.adafruit.com/product/5410" target="_blank">Reverse Mount Tactile Switch Buttons</a>&nbsp;on Adafruit.</li>
<li id="cite_note3"><a href="#cite_ref3">^</a> <a href="https://thepihut.com/products/reverse-mount-tactile-switch-buttons-6mm-square-10-pack" target="_blank">Reverse Mount Tactile Switch Buttons</a>&nbsp;on The Pi Hut.</li>
<li id="cite_note4"><a href="#cite_ref4">^</a> <a href="https://www.sparkfun.com/products/9873" target="_blank">FTDI Basic Breakout 3.3V</a>&nbsp;on Sparkfun.</li>
<li id="cite_note5"><a href="#cite_ref5">^</a> <a href="https://uk.farnell.com/kingbright/kptl-3216qbc-d-01/led-smd-1206-optic-blue-reverse/dp/2217907" target="_blank">Kingbright KPTL-3216QBC</a> on Farnell.com.</li>
<li id="cite_note6"><a href="#cite_ref6">^</a> <a href="https://uk.rs-online.com/web/p/piezo-buzzers/1722259" target="_blank">Square Wave External Piezo Buzzer</a> on RS Online.</li>
<li id="cite_note7"><a href="#cite_ref7">^</a> <a href="https://uk.rs-online.com/web/p/piezo-buzzers/7541971" target="_blank">KMTG1102-A1 Piezo Buzzer</a> on RS Online.</li>
<li id="cite_note8"><a href="#cite_ref8">^</a> <a href="https://www.sparkfun.com/products/11892" target="_blank">Coin Cell Battery Holder - 20mm (SMD)</a> on SparkFun.</li>
<li id="cite_note9"><a href="#cite_ref9">^</a> <a href="https://proto-pic.co.uk/product/sparkfun-prt-11892-coin-cell-battery-holder-20mm-smd/" target="_blank">Coin Cell Battery Holder - 20mm (SMD)</a> on Proto-PIC.</li>
<li id="cite_note10"><a href="#cite_ref10">^</a> <a href="https://github.com/zhcong/ST7302-for-arduino/raw/main/doc/ST7302_V0.0.pdf" target="_blank">ST7302 Datasheet</a>&nbsp;download from GitHub.</li>
<li id="cite_note11"><a href="#cite_ref11">^</a> <a href="https://www.sparkfun.com/products/9873" target="_blank">FTDI Basic Breakout 3.3V</a> on SparkFun.</li>
</ol>

<hr>


<p><a href="http://disqus.com/">blog comments powered by </a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[War Thunder user leaks restricted military documents for AH-64D Apache Longbow (107 pts)]]></title>
            <link>https://nichegamer.com/war-thunder-user-leaks-restricted-military-documents-for-ah-64d-apache-longbow/</link>
            <guid>37681554</guid>
            <pubDate>Wed, 27 Sep 2023 21:25:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nichegamer.com/war-thunder-user-leaks-restricted-military-documents-for-ah-64d-apache-longbow/">https://nichegamer.com/war-thunder-user-leaks-restricted-military-documents-for-ah-64d-apache-longbow/</a>, See on <a href="https://news.ycombinator.com/item?id=37681554">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-303486">
	<div>

		
		
					<div>
				<p><img decoding="async" loading="lazy" src="https://media.nichegamer.com/wp-content/uploads/2023/09/AH-64D_Apache_Longbow-resize.jpg" alt="AH-64D Apache Longbow" width="1705" height="960"></p>
<p><em>War Thunder</em> has leaked restricted military documents&nbsp;<em><span><strong>AGAIN</strong></span></em> for the third time in a month. This time it’s the AH-64D Apache Longbow attack helicoptor.</p>
<p>User BarteG98PL posted the “fully unclassified” technical manual for the Apache Longbow on the&nbsp;<em>War Thunder</em> forums.</p><!--?php if(! rcp_user_has_active_membership()): ?-->

<!--?php endif; ?-->
<p><em>War Thunder</em> players are increasingly testing the waters for what “restricted information” means and are posting “unclassified” documents that still have some big caveats.</p>
<p>In the case of the <a href="https://nichegamer.com/war-thunder-player-leaks-restricted-military-documents-again/">Eurofighter Typhoon DA7</a>, the documents were readily available for most consumers but are embargoed to non-NATO countries. The <a href="https://twitter.com/7thRanger/status/1701274530168045751">F-117 Nighthawk</a> was a similar case of documents acquired legally but are restricted under certain circumstances.</p>
<p>The case for the AH-64D Apache Longbow is a bit stronger though, as the leaked document allegedly included a big <em>“<a href="https://forum.warthunder.com/t/technical-manual-for-ah-64d-longbow-apache/27350/4">DOD AND DOD CONTRACTORS ONLY</a>”&nbsp;</em>warning all over it.</p>
<p><img decoding="async" loading="lazy" src="https://media.nichegamer.com/wp-content/uploads/2023/09/Apache-War-Thunder-09-26-2023.jpg" alt="War Thunder Apache" width="1058" height="444"></p>
<p><em>War Thunder</em>&nbsp;<a href="https://nichegamer.com/tag/war-thunder/" target="_blank" rel="noopener">has been available</a>&nbsp;on Windows PC, Linux, Mac (all via&nbsp;<a href="https://store.steampowered.com/app/236390/War_Thunder/" target="_blank" rel="noopener noreferrer">Steam</a>), Android, iOS, PlayStation 4, PlayStation 5, Xbox One, and Xbox Series X|S.</p>
			</div><!-- .entry-content -->

            
<p><span></span> <a href="https://nichegamer.com/tag/gaijin-entertainment/" rel="tag">Gaijin Entertainment</a>, <a href="https://nichegamer.com/tag/war-thunder/" rel="tag">War Thunder</a></p><div>
                    <p><img alt="" src="https://secure.gravatar.com/avatar/24c08c680c02cdc6713c14656e69048b?s=100&amp;r=pg" srcset="https://secure.gravatar.com/avatar/24c08c680c02cdc6713c14656e69048b?s=200&amp;r=pg 2x" height="100" width="100" loading="lazy" decoding="async">                    </p>
                    <div>
                        
                        <p>A basement-dwelling ogre, Brandon's a fan of indie games and slice of life anime. Has too many games and not enough time.</p><br>
  </div>
                </div>
		
  

<!-- AD TAG CODE 

	</div>
	<!-- /.card-body -->

</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Valve releases Counter-Strike 2 (118 pts)]]></title>
            <link>https://store.steampowered.com/app/730/CounterStrike_2/</link>
            <guid>37681505</guid>
            <pubDate>Wed, 27 Sep 2023 21:22:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://store.steampowered.com/app/730/CounterStrike_2/">https://store.steampowered.com/app/730/CounterStrike_2/</a>, See on <a href="https://news.ycombinator.com/item?id=37681505">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/Product" id="tabletGrid">
		
		<meta itemprop="image" content="https://cdn.cloudflare.steamstatic.com/steam/apps/730/capsule_231x87.jpg?t=1695853301">
					
		
		<div data-gpnav="columns">
		                      
         		<div><p><img src="https://cdn.cloudflare.steamstatic.com/steamcommunity/public/images/apps/730/8dbc71957312bbd3baea65848b545be9eae2a355.jpg"></p></div>
		<p>Counter-Strike 2</p>
		

	</div>
		


		

		

				

		<div data-panel="[]">

					
					
					
					
					
					
					<div id="game_area_purchase">

																			

						
												                        																			<!--[if lte IE 7]>
<style type="text/css">
.game_area_purchase_game_dropdown_right_panel .btn_addtocart { float: none; }
</style>
<![endif]-->



		
	<div>
		<div id="game_area_purchase_section_add_to_cart_54029">
		
		
		<h2>Buy Prime Status Upgrade</h2>
							<div><p>Counter-Strike: Global Offensive and Counter-Strike 2 players with Prime Status are matched with other Prime Status players and are eligible to receive Prime-exclusive souvenir items, item drops, and weapon cases.</p><p>

This package grants Prime Account Status in Counter-Strike: Global Offensive and in Counter-Strike 2.</p></div>
		
										
		
	</div>
			<p>
			This product is not eligible for refund. <a href="https://store.steampowered.com/steam_refunds">Learn more</a>		</p>
			</div>
<div data-panel="[]" data-ds-bundleid="232" data-ds-bundle-data="{&quot;m_nDiscountPct&quot;:&quot;55&quot;,&quot;m_bMustPurchaseAsSet&quot;:0,&quot;m_rgItems&quot;:[{&quot;m_nPackageID&quot;:7,&quot;m_rgIncludedAppIDs&quot;:[10,80],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:1050,&quot;m_nFinalPriceInCents&quot;:1050,&quot;m_nFinalPriceWithBundleDiscount&quot;:472},{&quot;m_nPackageID&quot;:25,&quot;m_rgIncludedAppIDs&quot;:[300],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:1099,&quot;m_nFinalPriceInCents&quot;:1099,&quot;m_nFinalPriceWithBundleDiscount&quot;:495},{&quot;m_nPackageID&quot;:29,&quot;m_rgIncludedAppIDs&quot;:[20],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:549,&quot;m_nFinalPriceInCents&quot;:549,&quot;m_nFinalPriceWithBundleDiscount&quot;:247},{&quot;m_nPackageID&quot;:30,&quot;m_rgIncludedAppIDs&quot;:[30],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:549,&quot;m_nFinalPriceInCents&quot;:549,&quot;m_nFinalPriceWithBundleDiscount&quot;:247},{&quot;m_nPackageID&quot;:31,&quot;m_rgIncludedAppIDs&quot;:[40],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:549,&quot;m_nFinalPriceInCents&quot;:549,&quot;m_nFinalPriceWithBundleDiscount&quot;:247},{&quot;m_nPackageID&quot;:32,&quot;m_rgIncludedAppIDs&quot;:[50],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:549,&quot;m_nFinalPriceInCents&quot;:549,&quot;m_nFinalPriceWithBundleDiscount&quot;:247},{&quot;m_nPackageID&quot;:33,&quot;m_rgIncludedAppIDs&quot;:[60],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:549,&quot;m_nFinalPriceInCents&quot;:549,&quot;m_nFinalPriceWithBundleDiscount&quot;:247},{&quot;m_nPackageID&quot;:34,&quot;m_rgIncludedAppIDs&quot;:[70],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:1050,&quot;m_nFinalPriceInCents&quot;:1050,&quot;m_nFinalPriceWithBundleDiscount&quot;:472},{&quot;m_nPackageID&quot;:35,&quot;m_rgIncludedAppIDs&quot;:[130],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:549,&quot;m_nFinalPriceInCents&quot;:549,&quot;m_nFinalPriceWithBundleDiscount&quot;:247},{&quot;m_nPackageID&quot;:36,&quot;m_rgIncludedAppIDs&quot;:[220],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:1099,&quot;m_nFinalPriceInCents&quot;:1099,&quot;m_nFinalPriceWithBundleDiscount&quot;:495},{&quot;m_nPackageID&quot;:37,&quot;m_rgIncludedAppIDs&quot;:[240],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:1099,&quot;m_nFinalPriceInCents&quot;:1099,&quot;m_nFinalPriceWithBundleDiscount&quot;:495},{&quot;m_nPackageID&quot;:38,&quot;m_rgIncludedAppIDs&quot;:[280,360],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:1099,&quot;m_nFinalPriceInCents&quot;:1099,&quot;m_nFinalPriceWithBundleDiscount&quot;:495},{&quot;m_nPackageID&quot;:79,&quot;m_rgIncludedAppIDs&quot;:[380,320],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:899,&quot;m_nFinalPriceInCents&quot;:899,&quot;m_nFinalPriceWithBundleDiscount&quot;:405},{&quot;m_nPackageID&quot;:515,&quot;m_rgIncludedAppIDs&quot;:[400],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:1099,&quot;m_nFinalPriceInCents&quot;:1099,&quot;m_nFinalPriceWithBundleDiscount&quot;:495},{&quot;m_nPackageID&quot;:516,&quot;m_rgIncludedAppIDs&quot;:[420],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:825,&quot;m_nFinalPriceInCents&quot;:825,&quot;m_nFinalPriceWithBundleDiscount&quot;:371},{&quot;m_nPackageID&quot;:1053,&quot;m_rgIncludedAppIDs&quot;:[500],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:1099,&quot;m_nFinalPriceInCents&quot;:1099,&quot;m_nFinalPriceWithBundleDiscount&quot;:495},{&quot;m_nPackageID&quot;:2481,&quot;m_rgIncludedAppIDs&quot;:[550],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:1099,&quot;m_nFinalPriceInCents&quot;:1099,&quot;m_nFinalPriceWithBundleDiscount&quot;:495},{&quot;m_nPackageID&quot;:7877,&quot;m_rgIncludedAppIDs&quot;:[620],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:1099,&quot;m_nFinalPriceInCents&quot;:1099,&quot;m_nFinalPriceWithBundleDiscount&quot;:495},{&quot;m_nPackageID&quot;:329385,&quot;m_rgIncludedAppIDs&quot;:[730],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:null,&quot;m_nFinalPriceInCents&quot;:0,&quot;m_nFinalPriceWithBundleDiscount&quot;:0},{&quot;m_nPackageID&quot;:330198,&quot;m_rgIncludedAppIDs&quot;:[440],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:null,&quot;m_nFinalPriceInCents&quot;:0,&quot;m_nFinalPriceWithBundleDiscount&quot;:0},{&quot;m_nPackageID&quot;:330209,&quot;m_rgIncludedAppIDs&quot;:[570],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:null,&quot;m_nFinalPriceInCents&quot;:0,&quot;m_nFinalPriceWithBundleDiscount&quot;:0},{&quot;m_nPackageID&quot;:330213,&quot;m_rgIncludedAppIDs&quot;:[450390],&quot;m_bPackageDiscounted&quot;:false,&quot;m_nBasePriceInCents&quot;:null,&quot;m_nFinalPriceInCents&quot;:0,&quot;m_nFinalPriceWithBundleDiscount&quot;:0}],&quot;m_bIsCommercial&quot;:false,&quot;m_bRestrictGifting&quot;:true}" data-ds-itemkey="Bundle_232" data-ds-tagids="[1663,19,1774,3859,3839,1693,3942]" data-ds-descids="[2,5]" data-ds-crtrids="[4,33078398,40102679]">
		
				
		<h2>
			Buy Valve Complete Pack							<span data-tooltip-text="Bundles are a special discount on a set of products.  If you already own some of the products contained in the bundle, purchasing the bundle will allow you to &quot;complete the set&quot;, paying only for the products you don't already own while still receiving the full bundle discount on each of those products.">
					BUNDLE					<span>(?)</span>
				</span>
					</h2>

		
		<p>
			<b>Includes 22 items:</b>

							<a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/sub/7/">Counter-Strike: Condition Zero</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/300/Day_of_Defeat_Source/">Day of Defeat: Source</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/20/Team_Fortress_Classic/">Team Fortress Classic</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/30/Day_of_Defeat/">Day of Defeat</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/40/Deathmatch_Classic/">Deathmatch Classic</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/50/HalfLife_Opposing_Force/">Opposing Force</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/60/Ricochet/">Ricochet</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/70/HalfLife/">Half-Life</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/130/HalfLife_Blue_Shift/">Half-Life: Blue Shift</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/220/HalfLife_2/">Half-Life 2</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/240/CounterStrike_Source/">Counter-Strike: Source</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/sub/38/">Half-Life 1: Source</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/sub/79/">Half-Life 2: Episode One</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/400/Portal/">Portal</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/420/HalfLife_2_Episode_Two/">Half-Life 2: Episode Two</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/500/Left_4_Dead/">Left 4 Dead</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/550/Left_4_Dead_2/">Left 4 Dead 2</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/620/Portal_2/">Portal 2</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/730/CounterStrike_2/">Counter-Strike: Global Offensive</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/440/Team_Fortress_2/">Team Fortress 2</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/570/Dota_2/">Dota 2</a>, <a data-panel="{&quot;focusable&quot;:false}" href="https://store.steampowered.com/app/450390/The_Lab/">The Lab</a>			
					</p>

		
	</div>
					</div>
					<!-- game_area_purchase -->

					
											
					
					

					
				
													
				

				

								
				
				


									
									<div id="aboutThisGame" data-panel="{&quot;type&quot;:&quot;PanelGroup&quot;}">
							<h2>About This Game</h2><p>
							For over two decades, Counter-Strike has offered an elite competitive experience, one shaped by millions of players from across the globe. And now the next chapter in the CS story is about to begin. This is Counter-Strike 2.</p><p>A free upgrade to CS:GO, Counter-Strike 2 marks the largest technical leap in Counter-Strike’s history. Built on the Source 2 engine, Counter-Strike 2 is modernized with realistic physically-based rendering, state of the art networking, and upgraded Community Workshop tools.</p><p>In addition to the classic objective-focused gameplay that Counter-Strike pioneered in 1999, Counter-Strike 2 features:</p><ul><li>All-new CS Ratings with the updated Premier mode<br></li><li>Global and Regional leaderboards<br></li><li>Upgraded and overhauled maps<br></li><li>Game-changing dynamic smoke grenades<br></li><li>Tick-rate-independent gameplay<br></li><li>Redesigned visual effects and audio<br></li><li>All items from CS:GO moving forward to CS2</li></ul>						</div>
					<div id="game_area_content_descriptors">
			<h2>Mature Content Description</h2>
			<p>The developers describe the content like this:</p>
			<p><i>
			Includes intense violence and blood.				</i>
			</p>
		</div>
	
				
				
					<div>
		<h2>System Requirements</h2>
					
				<div>
							<div data-os="win">
							<ul>
								<strong>Minimum:</strong><br><ul><li><strong>OS:</strong> Windows® 10<br></li><li><strong>Processor:</strong> 4 hardware CPU threads - Intel® Core™ i5 750 or higher<br></li><li><strong>Memory:</strong> 8 GB RAM<br></li><li><strong>Graphics:</strong> Video card must be 1 GB or more and should be a DirectX 11-compatible with support for Shader Model 5.0<br></li><li><strong>DirectX:</strong> Version 11<br></li><li><strong>Storage:</strong> 85 GB available space</li></ul>							</ul>
						</div>
							<div data-os="linux">
							<ul>
								<strong>Minimum:</strong><br><ul><li><strong>OS:</strong> Ubuntu 20.04<br></li><li><strong>Processor:</strong> 4 hardware CPU threads - Intel® Core™ i5 750 or higher<br></li><li><strong>Memory:</strong> 8 GB RAM<br></li><li><strong>Graphics:</strong> AMD GCN+ or NVIDIA Kepler+ with up-to-date Vulkan drivers.  Support for VK_EXT_graphics_pipeline_library highly recommended.<br></li><li><strong>Storage:</strong> 85 GB available space<br></li><li><strong>Sound Card:</strong> Highly recommended</li></ul>							</ul>
						</div>
					</div>
	</div>
	

				
				
				
				
														<div id="recommended_block">
							
							<h2>More like this</h2>
						</div>
									
				

				
		
		<div>
				
				<h2>What Curators Say</h2>
				<p>
					13,402 Curators have reviewed this product. Click <a href="https://store.steampowered.com/curators/curatorsreviewing/?appid=730&amp;snr=1_5_9__top-curators">here</a> to see them.				</p>
			</div>
		


			</div>


		<div id="app_reviews_hash">
							<h2>Customer reviews</h2>

		
		<div id="review_histograms_container">
			<canvas id="review_graph_canvas"></canvas>
			<div id="review_histogram_rollup_section">
						<p>Overall Reviews:</p>
						<p><span data-tooltip-html="88% of the 7,559,050 user reviews for this game are positive.">Very Positive</span>
													<span>(7,559,050 reviews)</span>
												<a data-tooltip-text="Since this product is free, this summary includes all reviews."><img src="https://store.cloudflare.steamstatic.com/public/shared/images/ico/icon_questionmark.png"></a>
					</p></div><!--
			--><div id="review_histogram_recent_section">
						<p>Recent Reviews:</p>
						<p><span data-tooltip-html="89% of the 118,563 user reviews in the last 30 days are positive.">Very Positive</span>
													<span>(118,563 reviews)</span>
												<a data-tooltip-text="Since this product is free, this summary includes all reviews."><img src="https://store.cloudflare.steamstatic.com/public/shared/images/ico/icon_questionmark.png"></a>
					</p></div>
		</div>

		<div id="reviews_filter_options">
			<div>
				<p>Review Type</p>
				<div>
					<p>
						<label for="review_type_all">All&nbsp;<span>(7,559,050)</span></label><br>
						
						<label for="review_type_positive">Positive&nbsp;<span>(6,715,673)</span></label><br>
						
						<label for="review_type_negative">Negative&nbsp;<span>(843,377)</span></label>
					</p>
				</div>
			</div>
			<div>
				<p>Purchase Type</p>
				<div>
					<p>
						<label for="purchase_type_all">All&nbsp;<span>(7,559,050)</span></label><br>
						
						<label for="purchase_type_steam">Steam Purchasers&nbsp;<span>(3,940,590)</span> <a data-tooltip-text="These are reviews written by customers that purchased the game directly from Steam."><img src="https://store.cloudflare.steamstatic.com/public/shared/images/ico/icon_questionmark_dark.png"></a></label><br>
						
						<label for="purchase_type_non_steam">Other&nbsp;<span>(3,618,460)</span> <a data-tooltip-text="These are reviews written by customers that did not purchase the game on Steam. (This may include legitimate sources such as other digital stores, retail stores, testing purposes, or press review purposes. Or, from inappropriate sources such as copies given in exchange for reviews.)"><img src="https://store.cloudflare.steamstatic.com/public/shared/images/ico/icon_questionmark_dark.png"></a></label>
					</p>
				</div>
			</div>
						<div>
				<p>Language</p>
				<div>
						<p>
						<label for="review_language_all">All Languages&nbsp;<span>(7,559,050)</span></label><br>
						
						<label for="review_language_mine">Your Languages&nbsp;<span>(2,085,118)</span> <a data-tooltip-html="Your preferences are currently set to show content authored in these languages: English.<br><br> Click 'customize' below to modify your preferences."><img src="https://store.cloudflare.steamstatic.com/public/shared/images/ico/icon_questionmark_dark.png"></a></label></p>
					</div>
			</div>
						<div id="reviews_date_range_menu">
				<p>Date Range</p>
				<div>
						<div><p>
							To view reviews within a date range, please click and drag a selection on a graph above or click on a specific bar.							</p><p>
							<span onclick="SetReviewsGraphVisibility( true ); "><span>Show graph</span></span></p></div>
						<p>
						<label for="review_date_range_all">Lifetime</label><br>
						
						<label for="review_date_range_histogram">Only Specific Range (Select on graph above)&nbsp;</label><br>
						
						<label for="review_date_range_exclude_histogram">Exclude Specific Range (Select on graph above)&nbsp;</label><br>
					</p></div>
			</div>
						<div>
				<p>Playtime</p>
				<div>

												
						
						<p>
							Filter reviews by the user's playtime when the review was written:						</p>

													<p>
							<label for="review_playtime_preset_0">No Minimum</label><br>
														
							<label for="review_playtime_preset_1">Over 1 hour</label><br>
														
							<label for="review_playtime_preset_10">Over 10 hours</label><br>
														
							<label for="review_playtime_preset_100">Over 100 hours</label></p><p><span id="app_reviews_playtime_range_text_min">No minimum</span> to <span id="app_reviews_playtime_range_text_max">No maximum</span>
						</p>
						
					</div>
			</div>
						<p><span>Display As: </span>
				
			</p>

			

			<p><span id="review_show_graph_button" onclick="SetReviewsGraphVisibility( true ); "><span>Show graph</span> </span>
				<span id="review_hide_graph_button" onclick="SetReviewsGraphVisibility( false ); "><span>Hide graph</span> </span>
			</p>

			
		</div>

		<div id="reviews_active_filters" data-panel="{&quot;focusable&quot;:true,&quot;clickOnActivate&quot;:true}" onclick="ShowReviewSettingsModal();">
				<p>Filters</p>

								
								
				
				
				
				<p>Excluding Off-topic Review Activity</p>
				<p>Playtime: <span id="review_playtime_preset_text"></span></p>
			</div>

		

		
		
		
		
		


		
		
		
		
		
		<div id="Reviews_summary">
						<p>There are no more reviews that match the filters set above</p>
						<p>Adjust the filters above to see other reviews</p>
						
					</div>
		
		

						</div>

						
		

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Desalination system could produce freshwater that is cheaper than tap water (163 pts)]]></title>
            <link>https://news.mit.edu/2023/desalination-system-could-produce-freshwater-cheaper-0927</link>
            <guid>37681004</guid>
            <pubDate>Wed, 27 Sep 2023 20:41:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.mit.edu/2023/desalination-system-could-produce-freshwater-cheaper-0927">https://news.mit.edu/2023/desalination-system-could-produce-freshwater-cheaper-0927</a>, See on <a href="https://news.ycombinator.com/item?id=37681004">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
          

            <p>Engineers at MIT and in China are aiming to turn seawater into drinking water with a completely passive device that is inspired by the ocean, and powered by the sun.</p>

<p>In a paper appearing today in the journal <em>Joule,</em> the team outlines the design for a new solar desalination system that takes in saltwater and heats it with natural sunlight.</p>

<p>The configuration of the device allows water to circulate in swirling eddies, in a manner similar to the much larger “thermohaline” circulation of the ocean. This circulation, combined with the sun’s heat, drives water to evaporate, leaving salt behind. The resulting water vapor can then be condensed and collected as pure, drinkable water. In the meantime, the leftover salt continues to circulate through and out of the device, rather than accumulating and clogging the system.</p>

<p>The new system has a higher water-production rate and a higher salt-rejection rate than all other passive solar desalination concepts currently being tested.</p>

<p>The researchers estimate that if the system is scaled up to the size of a small suitcase, it could produce about 4 to 6 liters of drinking water per hour and last several years before requiring replacement parts. At this scale and performance, the system could produce drinking water at a rate and price that is cheaper than tap water.</p>

<p>“For the first time, it is possible for water, produced by sunlight, to be even cheaper than tap water,” says Lenan Zhang, a research scientist in MIT’s Device Research Laboratory.</p>

<p>The team envisions a scaled-up device could passively produce enough drinking water to meet the daily requirements of a small family. The system could also supply off-grid, coastal communities where seawater is easily accessible.</p>

<p>Zhang’s study co-authors include MIT graduate student Yang Zhong and Evelyn Wang, the Ford Professor of Engineering, along with Jintong Gao, Jinfang You, Zhanyu Ye, Ruzhu Wang, and Zhenyuan Xu of Shanghai Jiao Tong University in China.</p>

<p><strong>A powerful convection</strong></p>

<p>The team’s new system improves on their <a href="https://news.mit.edu/2020/passive-solar-powered-water-desalination-0207" target="_blank">previous design</a> — a similar concept of multiple layers, called stages. Each stage contained an evaporator and a condenser that used heat from the sun to passively separate salt from incoming water. That design, which the team tested on the roof of an MIT building, efficiently converted the sun’s energy to evaporate water, which was then condensed into drinkable water. But the salt that was left over quickly accumulated as crystals that clogged the system after a few days. In a real-world setting, a user would have to place stages on a frequent basis, which would significantly increase the system’s overall cost.</p>

<p>In a follow-up effort, they <a href="https://news.mit.edu/2022/solar-desalination-system-inexpensive-0214" target="_blank">devised a solution</a> with a similar layered configuration, this time with an added feature that helped to circulate the incoming water as well as any leftover salt. While this design prevented salt from settling and accumulating on the device, it desalinated water at a relatively low rate.</p>

<p>In the latest iteration, the team believes it has landed on a design that achieves both a high water-production rate, and high salt rejection, meaning that the system can quickly and reliably produce drinking water for an extended period. The key to their new design is a combination of their two previous concepts: a multistage system of evaporators and condensers, that is also configured to boost the circulation of water — and salt — within each stage.</p>

<p>“We introduce now an even more powerful convection, that is similar to what we typically see in the ocean, at kilometer-long scales,” Xu says.</p>

<p>The small circulations generated in the team’s new system is similar to the “thermohaline” convection in the ocean — a phenomenon that drives the movement of water around the world, based on differences in sea temperature (“thermo”) and salinity (“haline”).</p>

<p>“When seawater is exposed to air, sunlight drives water to evaporate. Once water leaves the surface, salt remains. And the higher the salt concentration, the denser the liquid, and this heavier water wants to flow downward,” Zhang explains. “By mimicking this kilometer-wide phenomena in small box, we can take advantage of this feature to reject salt.”</p>

<p><strong>Tapping out</strong></p>

<p>The heart of the team’s new design is a single stage that resembles a thin box, topped with a dark material that efficiently absorbs the heat of the sun. Inside, the box is separated into a top and bottom section. Water can flow through the top half, where the ceiling is lined with an evaporator layer that uses the sun’s heat to warm up and evaporate any water in direct contact. The water vapor is then funneled to the bottom half of the box, where a condensing layer air-cools the vapor into salt-free, drinkable liquid. The researchers set the entire box at a tilt within a larger, empty vessel, then attached a tube from the top half of the box down through the bottom of the vessel, and floated the vessel in saltwater.</p>

<p>In this configuration, water can naturally push up through the tube and into the box, where the tilt of the box, combined with the thermal energy from the sun, induces the water to swirl as it flows through. The small eddies help to bring water in contact with the upper evaporating layer while keeping salt circulating, rather than settling and clogging.</p>

<p>The team built several prototypes, with one, three, and 10 stages, and tested their performance in water of varying salinity, including natural seawater and water that was seven times saltier.</p>

<p>From these tests, the researchers calculated that if each stage were scaled up to a square meter, it would produce up to 5 liters of drinking water per hour, and that the system could desalinate water without accumulating salt for several years. Given this extended lifetime, and the fact that the system is entirely passive, requiring no electricity to run, the team estimates that the overall cost of running the system would be cheaper than what it costs to produce tap water in the United States.</p>

<p>“We show that this device is capable of achieving a long lifetime,” Zhong says. “That means that, for the first time, it is possible for drinking water produced by sunlight to be cheaper than tap water. This opens up the possibility for solar desalination to address real-world problems.”</p>

<p>“This is a very innovative approach that effectively mitigates key challenges in the field of desalination,” says Guihua Yu, who develops sustainable water and energy storage systems at the University of Texas at Austin, and was not involved in the research. “The design is particularly beneficial for regions struggling with high-salinity water. Its modular design makes it highly suitable for household water production, allowing for scalability and adaptability to meet individual needs.”</p>

<p>Funding for the research at Shanghai Jiao Tong University was supported by the Natural Science Foundation of China.</p>        

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DAWproject: Open exchange format for DAWs (116 pts)]]></title>
            <link>https://github.com/bitwig/dawproject</link>
            <guid>37680449</guid>
            <pubDate>Wed, 27 Sep 2023 20:01:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/bitwig/dawproject">https://github.com/bitwig/dawproject</a>, See on <a href="https://news.ycombinator.com/item?id=37680449">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-dawproject" dir="auto"><a href="#dawproject">DAWproject</a></h2>
<p dir="auto">Open exchange format for user data between Digital Audio Workstations (DAWs)</p>
<h2 tabindex="-1" id="user-content-motivation" dir="auto"><a href="#motivation">Motivation</a></h2>
<p dir="auto">The DAWproject format provides a (vendor-agnostic) way of transferring user data between different music applications (DAWs).</p>
<p dir="auto">Currently, there is no file-format which is purpose-built for this task.
Standard MIDI files can represent note data, but it is often a lower-level representation (no ramps) of data than what the DAW uses internally, which forces consolidation on export. AAF only covers audio and doesn't have any concept of musical-time, which limits it to post-audio workflows . Most plug-ins do allow you to save presets to a shared location, but this has to be done for each instance. What most users end up doing is just exporting audio as stems.</p>
<p dir="auto">The aim of this project is to export all translatable project data (audio/note/automation/plug-in) along with the structure surrounding it into a single DAWproject file.</p>
<p dir="auto">The table below aims to explain the scope format from a music-production perspective and how it compares to other methods of data transfer.</p>
<table>
<thead>
<tr>
<th></th>
<th>DAWproject</th>
<th>Standard MIDI Files</th>
<th>Advanced Authoring Format (AAF)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Intended Use</td>
<td>Music Production</td>
<td>MIDI Sequencing</td>
<td>Video Post-Production</td>
</tr>
<tr>
<td>Time Format<br>(seconds/beats)</td>
<td>Beats and seconds can be combined</td>
<td>Beats</td>
<td>Seconds</td>
</tr>
<tr>
<td>Audio</td>
<td>Audio<br>Events/Clips<br>Fades<br>Crossfades<br>Amplitude<br>Pan<br>Time Warping<br>Transpose</td>
<td>-</td>
<td>Audio<br>Events/Clips<br>Fades<br>Crossfades<br>Amplitude<br>Pan</td>
</tr>
<tr>
<td>Notes</td>
<td>Notes<br>Note Expressions</td>
<td>Notes</td>
<td>-</td>
</tr>
<tr>
<td>Automation</td>
<td>Tempo<br>Time Signature<br>MIDI Messages<br>Volume<br>Pan<br>Mute<br>Sends<br>Plug-in Parameters<br>Built-in Device Parameters</td>
<td>Tempo<br>Time Signature<br>MIDI Messages<br>SysEx Messages</td>
<td>Volume<br>Pan<br>Video Related Parameters</td>
</tr>
<tr>
<td>Plug-ins</td>
<td>Stores full plug-in state<br>and automation of parameters</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Built-in Devices</td>
<td>Generic EQ<br>Generic Compressor<br>Generic Gate<br>Generic Limiter</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Clip Launcher</td>
<td>Clips<br>Scenes</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" id="user-content-status" dir="auto"><a href="#status">Status</a></h2>
<p dir="auto">The format is version 1.0 and is stable.</p>
<h2 tabindex="-1" id="user-content-goals" dir="auto"><a href="#goals">Goals</a></h2>
<ul dir="auto">
<li>Package all user data of a project/song into a single file.
<ul dir="auto">
<li>Audio timeline data</li>
<li>Note timeline data</li>
<li>Note expression data</li>
<li>Automation timeline data</li>
<li>Audio data (embedded or referenced)</li>
<li>Plug-in states (always embedded)</li>
</ul>
</li>
<li>The format should be able to preserve as much user created data as feasible.</li>
<li>The format should be able to express the track and timeline structures of the exporting DAW as is, leaving it up to the importer to use this data and flatten it as needed.</li>
<li>Simple to implement</li>
<li>Built upon established open standards</li>
<li>Language agnostic, no special dependencies</li>
<li>Open &amp; free</li>
</ul>
<h2 tabindex="-1" id="user-content-non-goals" dir="auto"><a href="#non-goals">Non-goals</a></h2>
<ul dir="auto">
<li>Being the native file-format for a DAW</li>
<li>Optimal performance (like a binary format could provide)</li>
<li>Storing low-level MIDI events directly (but rather relying on higher level abstractions)</li>
<li>Storing non-session data (view settings, preferences)</li>
</ul>
<h2 tabindex="-1" id="user-content-format-specification" dir="auto"><a href="#format-specification">Format Specification</a></h2>
<ul dir="auto">
<li>
<p dir="auto">File Extension: .dawproject</p>
</li>
<li>
<p dir="auto">Container: ZIP</p>
</li>
<li>
<p dir="auto">Format: XML (project.xml, metadata.xml)</p>
</li>
<li>
<p dir="auto">Text encoding: UTF-8</p>
</li>
<li>
<p dir="auto">The exporting DAW is free to choose the directory structure it wants for media and plug-in files.</p>
</li>
<li>
<p dir="auto"><a href="https://htmlpreview.github.io/?https://github.com/bitwig/dawproject/blob/main/Reference.html" rel="nofollow">DAWproject XML Reference</a></p>
</li>
<li>
<p dir="auto"><a href="https://github.com/bitwig/dawproject/blob/main/Project.xsd">Project XML Schema</a></p>
</li>
<li>
<p dir="auto"><a href="https://github.com/bitwig/dawproject/blob/main/MetaData.xsd">MetaData XML Schema</a></p>
</li>
</ul>
<h2 tabindex="-1" id="user-content-language-support" dir="auto"><a href="#language-support">Language Support</a></h2>
<p dir="auto">DAWproject is based on plain XML/ZIP and can be used with any programming language that can parse those.</p>
<p dir="auto">The DOM of DAWproject is defined by a set of Java classes which have XML-related annotations and HTML-induced Javadoc comments.
Those are used (via reflection) to generate XML Documentation and Schemas. Potentially, the same approach could be used to generate code for other languages (contributions welcome).</p>
<h2 tabindex="-1" id="user-content-building-the-library-documentation-and-tests" dir="auto"><a href="#building-the-library-documentation-and-tests">Building the Library, Documentation and Tests</a></h2>
<p dir="auto">Requires Java Runtime version 16 or later.</p>
<p dir="auto">To build (using Gradle):</p>

<h2 tabindex="-1" id="user-content-example-project" dir="auto"><a href="#example-project">Example project</a></h2>
<p dir="auto">The exporting application is free to structure tracks and timelines in a way that fits its internal model.
The choice is left to the importing application to either use the level of structure provided (if applicable) or to flatten/convert it to match its model.</p>
<p dir="auto">As an example, here's the project.xml of a simple file saved in Bitwig Studio 5.0 with one instrument track and one audio track. As the audio clips in Bitwig Studio are themselves a timeline of audio events, you will notice that there are two levels of  elements, id25 representing the clip timeline on the arrangement, and id26 representing the  audio events inside the clip.</p>
<div dir="auto" data-snippet-clipboard-copy-content="<?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?>
<Project version=&quot;1.0&quot;>
  <Application name=&quot;Bitwig Studio&quot; version=&quot;5.0&quot;/>
  <Transport>
    <Tempo max=&quot;666.000000&quot; min=&quot;20.000000&quot; unit=&quot;bpm&quot; value=&quot;149.000000&quot; id=&quot;id0&quot; name=&quot;Tempo&quot;/>
    <TimeSignature denominator=&quot;4&quot; numerator=&quot;4&quot; id=&quot;id1&quot;/>
  </Transport>
  <Structure>
    <Track contentType=&quot;notes&quot; loaded=&quot;true&quot; id=&quot;id2&quot; name=&quot;Bass&quot; color=&quot;#a2eabf&quot;>
      <Channel audioChannels=&quot;2&quot; destination=&quot;id15&quot; role=&quot;regular&quot; solo=&quot;false&quot; id=&quot;id3&quot;>
        <Devices>
          <ClapPlugin deviceID=&quot;org.surge-synth-team.surge-xt&quot; deviceName=&quot;Surge XT&quot; deviceRole=&quot;instrument&quot; loaded=&quot;true&quot; id=&quot;id7&quot; name=&quot;Surge XT&quot;>
            <Parameters/>
            <Enabled value=&quot;true&quot; id=&quot;id8&quot; name=&quot;On/Off&quot;/>
            <State path=&quot;plugins/d19b1f6e-bbb6-42fe-a6c9-54b41d97a05d.clap-preset&quot;/>
          </ClapPlugin>
        </Devices>
        <Mute value=&quot;false&quot; id=&quot;id6&quot; name=&quot;Mute&quot;/>
        <Pan max=&quot;1.000000&quot; min=&quot;0.000000&quot; unit=&quot;normalized&quot; value=&quot;0.500000&quot; id=&quot;id5&quot; name=&quot;Pan&quot;/>
        <Volume max=&quot;2.000000&quot; min=&quot;0.000000&quot; unit=&quot;linear&quot; value=&quot;0.659140&quot; id=&quot;id4&quot; name=&quot;Volume&quot;/>
      </Channel>
    </Track>
    <Track contentType=&quot;audio&quot; loaded=&quot;true&quot; id=&quot;id9&quot; name=&quot;Drumloop&quot; color=&quot;#b53bba&quot;>
      <Channel audioChannels=&quot;2&quot; destination=&quot;id15&quot; role=&quot;regular&quot; solo=&quot;false&quot; id=&quot;id10&quot;>
        <Mute value=&quot;false&quot; id=&quot;id13&quot; name=&quot;Mute&quot;/>
        <Pan max=&quot;1.000000&quot; min=&quot;0.000000&quot; unit=&quot;normalized&quot; value=&quot;0.500000&quot; id=&quot;id12&quot; name=&quot;Pan&quot;/>
        <Volume max=&quot;2.000000&quot; min=&quot;0.000000&quot; unit=&quot;linear&quot; value=&quot;0.177125&quot; id=&quot;id11&quot; name=&quot;Volume&quot;/>
      </Channel>
    </Track>
    <Track contentType=&quot;audio notes&quot; loaded=&quot;true&quot; id=&quot;id14&quot; name=&quot;Master&quot;>
      <Channel audioChannels=&quot;2&quot; role=&quot;master&quot; solo=&quot;false&quot; id=&quot;id15&quot;>
        <Mute value=&quot;false&quot; id=&quot;id18&quot; name=&quot;Mute&quot;/>
        <Pan max=&quot;1.000000&quot; min=&quot;0.000000&quot; unit=&quot;normalized&quot; value=&quot;0.500000&quot; id=&quot;id17&quot; name=&quot;Pan&quot;/>
        <Volume max=&quot;2.000000&quot; min=&quot;0.000000&quot; unit=&quot;linear&quot; value=&quot;1.000000&quot; id=&quot;id16&quot; name=&quot;Volume&quot;/>
      </Channel>
    </Track>
  </Structure>
  <Arrangement id=&quot;id19&quot;>
    <Lanes timeUnit=&quot;beats&quot; id=&quot;id20&quot;>
      <Lanes track=&quot;id2&quot; id=&quot;id21&quot;>
        <Clips id=&quot;id22&quot;>
          <Clip time=&quot;0.0&quot; duration=&quot;8.0&quot; playStart=&quot;0.0&quot;>
            <Notes id=&quot;id23&quot;>
              <Note time=&quot;0.000000&quot; duration=&quot;0.250000&quot; channel=&quot;0&quot; key=&quot;65&quot; vel=&quot;0.787402&quot; rel=&quot;0.787402&quot;/>
              <Note time=&quot;1.000000&quot; duration=&quot;0.250000&quot; channel=&quot;0&quot; key=&quot;65&quot; vel=&quot;0.787402&quot; rel=&quot;0.787402&quot;/>
              <Note time=&quot;4.000000&quot; duration=&quot;0.250000&quot; channel=&quot;0&quot; key=&quot;65&quot; vel=&quot;0.787402&quot; rel=&quot;0.787402&quot;/>
              <Note time=&quot;5.000000&quot; duration=&quot;0.250000&quot; channel=&quot;0&quot; key=&quot;65&quot; vel=&quot;0.787402&quot; rel=&quot;0.787402&quot;/>
              <Note time=&quot;0.500000&quot; duration=&quot;0.250000&quot; channel=&quot;0&quot; key=&quot;64&quot; vel=&quot;0.787402&quot; rel=&quot;0.787402&quot;/>
              <Note time=&quot;4.500000&quot; duration=&quot;0.250000&quot; channel=&quot;0&quot; key=&quot;64&quot; vel=&quot;0.787402&quot; rel=&quot;0.787402&quot;/>
              <Note time=&quot;1.500000&quot; duration=&quot;2.500000&quot; channel=&quot;0&quot; key=&quot;53&quot; vel=&quot;0.787402&quot; rel=&quot;0.787402&quot;/>
              <Note time=&quot;5.500000&quot; duration=&quot;0.250000&quot; channel=&quot;0&quot; key=&quot;53&quot; vel=&quot;0.787402&quot; rel=&quot;0.787402&quot;/>
              <Note time=&quot;6.000000&quot; duration=&quot;2.000000&quot; channel=&quot;0&quot; key=&quot;53&quot; vel=&quot;0.787402&quot; rel=&quot;0.787402&quot;/>
            </Notes>
          </Clip>
        </Clips>
      </Lanes>
      <Lanes track=&quot;id9&quot; id=&quot;id24&quot;>
        <Clips id=&quot;id25&quot;>
          <Clip time=&quot;0.0&quot; duration=&quot;8.00003433227539&quot; playStart=&quot;0.0&quot; loopStart=&quot;0.0&quot; loopEnd=&quot;8.00003433227539&quot; fadeTimeUnit=&quot;beats&quot; fadeInTime=&quot;0.0&quot; fadeOutTime=&quot;0.0&quot; name=&quot;Drumfunk3 170bpm&quot;>
            <Clips id=&quot;id26&quot;>
              <Clip time=&quot;0.0&quot; duration=&quot;8.00003433227539&quot; contentTimeUnit=&quot;beats&quot; playStart=&quot;0.0&quot; fadeTimeUnit=&quot;beats&quot; fadeInTime=&quot;0.0&quot; fadeOutTime=&quot;0.0&quot;>
                <Warps contentTimeUnit=&quot;seconds&quot; timeUnit=&quot;beats&quot; id=&quot;id28&quot;>
                  <Audio algorithm=&quot;stretch&quot; channels=&quot;2&quot; duration=&quot;2.823541666666667&quot; sampleRate=&quot;48000&quot; id=&quot;id27&quot;>
                    <File path=&quot;audio/Drumfunk3 170bpm.wav&quot;/>
                  </Audio>
                  <Warp time=&quot;0.0&quot; contentTime=&quot;0.0&quot;/>
                  <Warp time=&quot;8.00003433227539&quot; contentTime=&quot;2.823541666666667&quot;/>
                </Warps>
              </Clip>
            </Clips>
          </Clip>
        </Clips>
      </Lanes>
      <Lanes track=&quot;id14&quot; id=&quot;id29&quot;>
        <Clips id=&quot;id30&quot;/>
      </Lanes>
    </Lanes>
  </Arrangement>
  <Scenes/>
</Project>"><pre>&lt;?<span>xml</span><span> version</span>=<span><span>"</span>1.0<span>"</span></span><span> encoding</span>=<span><span>"</span>UTF-8<span>"</span></span><span> standalone</span>=<span><span>"</span>yes<span>"</span></span>?&gt;
&lt;<span>Project</span> <span>version</span>=<span><span>"</span>1.0<span>"</span></span>&gt;
  &lt;<span>Application</span> <span>name</span>=<span><span>"</span>Bitwig Studio<span>"</span></span> <span>version</span>=<span><span>"</span>5.0<span>"</span></span>/&gt;
  &lt;<span>Transport</span>&gt;
    &lt;<span>Tempo</span> <span>max</span>=<span><span>"</span>666.000000<span>"</span></span> <span>min</span>=<span><span>"</span>20.000000<span>"</span></span> <span>unit</span>=<span><span>"</span>bpm<span>"</span></span> <span>value</span>=<span><span>"</span>149.000000<span>"</span></span> <span>id</span>=<span><span>"</span>id0<span>"</span></span> <span>name</span>=<span><span>"</span>Tempo<span>"</span></span>/&gt;
    &lt;<span>TimeSignature</span> <span>denominator</span>=<span><span>"</span>4<span>"</span></span> <span>numerator</span>=<span><span>"</span>4<span>"</span></span> <span>id</span>=<span><span>"</span>id1<span>"</span></span>/&gt;
  &lt;/<span>Transport</span>&gt;
  &lt;<span>Structure</span>&gt;
    &lt;<span>Track</span> <span>contentType</span>=<span><span>"</span>notes<span>"</span></span> <span>loaded</span>=<span><span>"</span>true<span>"</span></span> <span>id</span>=<span><span>"</span>id2<span>"</span></span> <span>name</span>=<span><span>"</span>Bass<span>"</span></span> <span>color</span>=<span><span>"</span>#a2eabf<span>"</span></span>&gt;
      &lt;<span>Channel</span> <span>audioChannels</span>=<span><span>"</span>2<span>"</span></span> <span>destination</span>=<span><span>"</span>id15<span>"</span></span> <span>role</span>=<span><span>"</span>regular<span>"</span></span> <span>solo</span>=<span><span>"</span>false<span>"</span></span> <span>id</span>=<span><span>"</span>id3<span>"</span></span>&gt;
        &lt;<span>Devices</span>&gt;
          &lt;<span>ClapPlugin</span> <span>deviceID</span>=<span><span>"</span>org.surge-synth-team.surge-xt<span>"</span></span> <span>deviceName</span>=<span><span>"</span>Surge XT<span>"</span></span> <span>deviceRole</span>=<span><span>"</span>instrument<span>"</span></span> <span>loaded</span>=<span><span>"</span>true<span>"</span></span> <span>id</span>=<span><span>"</span>id7<span>"</span></span> <span>name</span>=<span><span>"</span>Surge XT<span>"</span></span>&gt;
            &lt;<span>Parameters</span>/&gt;
            &lt;<span>Enabled</span> <span>value</span>=<span><span>"</span>true<span>"</span></span> <span>id</span>=<span><span>"</span>id8<span>"</span></span> <span>name</span>=<span><span>"</span>On/Off<span>"</span></span>/&gt;
            &lt;<span>State</span> <span>path</span>=<span><span>"</span>plugins/d19b1f6e-bbb6-42fe-a6c9-54b41d97a05d.clap-preset<span>"</span></span>/&gt;
          &lt;/<span>ClapPlugin</span>&gt;
        &lt;/<span>Devices</span>&gt;
        &lt;<span>Mute</span> <span>value</span>=<span><span>"</span>false<span>"</span></span> <span>id</span>=<span><span>"</span>id6<span>"</span></span> <span>name</span>=<span><span>"</span>Mute<span>"</span></span>/&gt;
        &lt;<span>Pan</span> <span>max</span>=<span><span>"</span>1.000000<span>"</span></span> <span>min</span>=<span><span>"</span>0.000000<span>"</span></span> <span>unit</span>=<span><span>"</span>normalized<span>"</span></span> <span>value</span>=<span><span>"</span>0.500000<span>"</span></span> <span>id</span>=<span><span>"</span>id5<span>"</span></span> <span>name</span>=<span><span>"</span>Pan<span>"</span></span>/&gt;
        &lt;<span>Volume</span> <span>max</span>=<span><span>"</span>2.000000<span>"</span></span> <span>min</span>=<span><span>"</span>0.000000<span>"</span></span> <span>unit</span>=<span><span>"</span>linear<span>"</span></span> <span>value</span>=<span><span>"</span>0.659140<span>"</span></span> <span>id</span>=<span><span>"</span>id4<span>"</span></span> <span>name</span>=<span><span>"</span>Volume<span>"</span></span>/&gt;
      &lt;/<span>Channel</span>&gt;
    &lt;/<span>Track</span>&gt;
    &lt;<span>Track</span> <span>contentType</span>=<span><span>"</span>audio<span>"</span></span> <span>loaded</span>=<span><span>"</span>true<span>"</span></span> <span>id</span>=<span><span>"</span>id9<span>"</span></span> <span>name</span>=<span><span>"</span>Drumloop<span>"</span></span> <span>color</span>=<span><span>"</span>#b53bba<span>"</span></span>&gt;
      &lt;<span>Channel</span> <span>audioChannels</span>=<span><span>"</span>2<span>"</span></span> <span>destination</span>=<span><span>"</span>id15<span>"</span></span> <span>role</span>=<span><span>"</span>regular<span>"</span></span> <span>solo</span>=<span><span>"</span>false<span>"</span></span> <span>id</span>=<span><span>"</span>id10<span>"</span></span>&gt;
        &lt;<span>Mute</span> <span>value</span>=<span><span>"</span>false<span>"</span></span> <span>id</span>=<span><span>"</span>id13<span>"</span></span> <span>name</span>=<span><span>"</span>Mute<span>"</span></span>/&gt;
        &lt;<span>Pan</span> <span>max</span>=<span><span>"</span>1.000000<span>"</span></span> <span>min</span>=<span><span>"</span>0.000000<span>"</span></span> <span>unit</span>=<span><span>"</span>normalized<span>"</span></span> <span>value</span>=<span><span>"</span>0.500000<span>"</span></span> <span>id</span>=<span><span>"</span>id12<span>"</span></span> <span>name</span>=<span><span>"</span>Pan<span>"</span></span>/&gt;
        &lt;<span>Volume</span> <span>max</span>=<span><span>"</span>2.000000<span>"</span></span> <span>min</span>=<span><span>"</span>0.000000<span>"</span></span> <span>unit</span>=<span><span>"</span>linear<span>"</span></span> <span>value</span>=<span><span>"</span>0.177125<span>"</span></span> <span>id</span>=<span><span>"</span>id11<span>"</span></span> <span>name</span>=<span><span>"</span>Volume<span>"</span></span>/&gt;
      &lt;/<span>Channel</span>&gt;
    &lt;/<span>Track</span>&gt;
    &lt;<span>Track</span> <span>contentType</span>=<span><span>"</span>audio notes<span>"</span></span> <span>loaded</span>=<span><span>"</span>true<span>"</span></span> <span>id</span>=<span><span>"</span>id14<span>"</span></span> <span>name</span>=<span><span>"</span>Master<span>"</span></span>&gt;
      &lt;<span>Channel</span> <span>audioChannels</span>=<span><span>"</span>2<span>"</span></span> <span>role</span>=<span><span>"</span>master<span>"</span></span> <span>solo</span>=<span><span>"</span>false<span>"</span></span> <span>id</span>=<span><span>"</span>id15<span>"</span></span>&gt;
        &lt;<span>Mute</span> <span>value</span>=<span><span>"</span>false<span>"</span></span> <span>id</span>=<span><span>"</span>id18<span>"</span></span> <span>name</span>=<span><span>"</span>Mute<span>"</span></span>/&gt;
        &lt;<span>Pan</span> <span>max</span>=<span><span>"</span>1.000000<span>"</span></span> <span>min</span>=<span><span>"</span>0.000000<span>"</span></span> <span>unit</span>=<span><span>"</span>normalized<span>"</span></span> <span>value</span>=<span><span>"</span>0.500000<span>"</span></span> <span>id</span>=<span><span>"</span>id17<span>"</span></span> <span>name</span>=<span><span>"</span>Pan<span>"</span></span>/&gt;
        &lt;<span>Volume</span> <span>max</span>=<span><span>"</span>2.000000<span>"</span></span> <span>min</span>=<span><span>"</span>0.000000<span>"</span></span> <span>unit</span>=<span><span>"</span>linear<span>"</span></span> <span>value</span>=<span><span>"</span>1.000000<span>"</span></span> <span>id</span>=<span><span>"</span>id16<span>"</span></span> <span>name</span>=<span><span>"</span>Volume<span>"</span></span>/&gt;
      &lt;/<span>Channel</span>&gt;
    &lt;/<span>Track</span>&gt;
  &lt;/<span>Structure</span>&gt;
  &lt;<span>Arrangement</span> <span>id</span>=<span><span>"</span>id19<span>"</span></span>&gt;
    &lt;<span>Lanes</span> <span>timeUnit</span>=<span><span>"</span>beats<span>"</span></span> <span>id</span>=<span><span>"</span>id20<span>"</span></span>&gt;
      &lt;<span>Lanes</span> <span>track</span>=<span><span>"</span>id2<span>"</span></span> <span>id</span>=<span><span>"</span>id21<span>"</span></span>&gt;
        &lt;<span>Clips</span> <span>id</span>=<span><span>"</span>id22<span>"</span></span>&gt;
          &lt;<span>Clip</span> <span>time</span>=<span><span>"</span>0.0<span>"</span></span> <span>duration</span>=<span><span>"</span>8.0<span>"</span></span> <span>playStart</span>=<span><span>"</span>0.0<span>"</span></span>&gt;
            &lt;<span>Notes</span> <span>id</span>=<span><span>"</span>id23<span>"</span></span>&gt;
              &lt;<span>Note</span> <span>time</span>=<span><span>"</span>0.000000<span>"</span></span> <span>duration</span>=<span><span>"</span>0.250000<span>"</span></span> <span>channel</span>=<span><span>"</span>0<span>"</span></span> <span>key</span>=<span><span>"</span>65<span>"</span></span> <span>vel</span>=<span><span>"</span>0.787402<span>"</span></span> <span>rel</span>=<span><span>"</span>0.787402<span>"</span></span>/&gt;
              &lt;<span>Note</span> <span>time</span>=<span><span>"</span>1.000000<span>"</span></span> <span>duration</span>=<span><span>"</span>0.250000<span>"</span></span> <span>channel</span>=<span><span>"</span>0<span>"</span></span> <span>key</span>=<span><span>"</span>65<span>"</span></span> <span>vel</span>=<span><span>"</span>0.787402<span>"</span></span> <span>rel</span>=<span><span>"</span>0.787402<span>"</span></span>/&gt;
              &lt;<span>Note</span> <span>time</span>=<span><span>"</span>4.000000<span>"</span></span> <span>duration</span>=<span><span>"</span>0.250000<span>"</span></span> <span>channel</span>=<span><span>"</span>0<span>"</span></span> <span>key</span>=<span><span>"</span>65<span>"</span></span> <span>vel</span>=<span><span>"</span>0.787402<span>"</span></span> <span>rel</span>=<span><span>"</span>0.787402<span>"</span></span>/&gt;
              &lt;<span>Note</span> <span>time</span>=<span><span>"</span>5.000000<span>"</span></span> <span>duration</span>=<span><span>"</span>0.250000<span>"</span></span> <span>channel</span>=<span><span>"</span>0<span>"</span></span> <span>key</span>=<span><span>"</span>65<span>"</span></span> <span>vel</span>=<span><span>"</span>0.787402<span>"</span></span> <span>rel</span>=<span><span>"</span>0.787402<span>"</span></span>/&gt;
              &lt;<span>Note</span> <span>time</span>=<span><span>"</span>0.500000<span>"</span></span> <span>duration</span>=<span><span>"</span>0.250000<span>"</span></span> <span>channel</span>=<span><span>"</span>0<span>"</span></span> <span>key</span>=<span><span>"</span>64<span>"</span></span> <span>vel</span>=<span><span>"</span>0.787402<span>"</span></span> <span>rel</span>=<span><span>"</span>0.787402<span>"</span></span>/&gt;
              &lt;<span>Note</span> <span>time</span>=<span><span>"</span>4.500000<span>"</span></span> <span>duration</span>=<span><span>"</span>0.250000<span>"</span></span> <span>channel</span>=<span><span>"</span>0<span>"</span></span> <span>key</span>=<span><span>"</span>64<span>"</span></span> <span>vel</span>=<span><span>"</span>0.787402<span>"</span></span> <span>rel</span>=<span><span>"</span>0.787402<span>"</span></span>/&gt;
              &lt;<span>Note</span> <span>time</span>=<span><span>"</span>1.500000<span>"</span></span> <span>duration</span>=<span><span>"</span>2.500000<span>"</span></span> <span>channel</span>=<span><span>"</span>0<span>"</span></span> <span>key</span>=<span><span>"</span>53<span>"</span></span> <span>vel</span>=<span><span>"</span>0.787402<span>"</span></span> <span>rel</span>=<span><span>"</span>0.787402<span>"</span></span>/&gt;
              &lt;<span>Note</span> <span>time</span>=<span><span>"</span>5.500000<span>"</span></span> <span>duration</span>=<span><span>"</span>0.250000<span>"</span></span> <span>channel</span>=<span><span>"</span>0<span>"</span></span> <span>key</span>=<span><span>"</span>53<span>"</span></span> <span>vel</span>=<span><span>"</span>0.787402<span>"</span></span> <span>rel</span>=<span><span>"</span>0.787402<span>"</span></span>/&gt;
              &lt;<span>Note</span> <span>time</span>=<span><span>"</span>6.000000<span>"</span></span> <span>duration</span>=<span><span>"</span>2.000000<span>"</span></span> <span>channel</span>=<span><span>"</span>0<span>"</span></span> <span>key</span>=<span><span>"</span>53<span>"</span></span> <span>vel</span>=<span><span>"</span>0.787402<span>"</span></span> <span>rel</span>=<span><span>"</span>0.787402<span>"</span></span>/&gt;
            &lt;/<span>Notes</span>&gt;
          &lt;/<span>Clip</span>&gt;
        &lt;/<span>Clips</span>&gt;
      &lt;/<span>Lanes</span>&gt;
      &lt;<span>Lanes</span> <span>track</span>=<span><span>"</span>id9<span>"</span></span> <span>id</span>=<span><span>"</span>id24<span>"</span></span>&gt;
        &lt;<span>Clips</span> <span>id</span>=<span><span>"</span>id25<span>"</span></span>&gt;
          &lt;<span>Clip</span> <span>time</span>=<span><span>"</span>0.0<span>"</span></span> <span>duration</span>=<span><span>"</span>8.00003433227539<span>"</span></span> <span>playStart</span>=<span><span>"</span>0.0<span>"</span></span> <span>loopStart</span>=<span><span>"</span>0.0<span>"</span></span> <span>loopEnd</span>=<span><span>"</span>8.00003433227539<span>"</span></span> <span>fadeTimeUnit</span>=<span><span>"</span>beats<span>"</span></span> <span>fadeInTime</span>=<span><span>"</span>0.0<span>"</span></span> <span>fadeOutTime</span>=<span><span>"</span>0.0<span>"</span></span> <span>name</span>=<span><span>"</span>Drumfunk3 170bpm<span>"</span></span>&gt;
            &lt;<span>Clips</span> <span>id</span>=<span><span>"</span>id26<span>"</span></span>&gt;
              &lt;<span>Clip</span> <span>time</span>=<span><span>"</span>0.0<span>"</span></span> <span>duration</span>=<span><span>"</span>8.00003433227539<span>"</span></span> <span>contentTimeUnit</span>=<span><span>"</span>beats<span>"</span></span> <span>playStart</span>=<span><span>"</span>0.0<span>"</span></span> <span>fadeTimeUnit</span>=<span><span>"</span>beats<span>"</span></span> <span>fadeInTime</span>=<span><span>"</span>0.0<span>"</span></span> <span>fadeOutTime</span>=<span><span>"</span>0.0<span>"</span></span>&gt;
                &lt;<span>Warps</span> <span>contentTimeUnit</span>=<span><span>"</span>seconds<span>"</span></span> <span>timeUnit</span>=<span><span>"</span>beats<span>"</span></span> <span>id</span>=<span><span>"</span>id28<span>"</span></span>&gt;
                  &lt;<span>Audio</span> <span>algorithm</span>=<span><span>"</span>stretch<span>"</span></span> <span>channels</span>=<span><span>"</span>2<span>"</span></span> <span>duration</span>=<span><span>"</span>2.823541666666667<span>"</span></span> <span>sampleRate</span>=<span><span>"</span>48000<span>"</span></span> <span>id</span>=<span><span>"</span>id27<span>"</span></span>&gt;
                    &lt;<span>File</span> <span>path</span>=<span><span>"</span>audio/Drumfunk3 170bpm.wav<span>"</span></span>/&gt;
                  &lt;/<span>Audio</span>&gt;
                  &lt;<span>Warp</span> <span>time</span>=<span><span>"</span>0.0<span>"</span></span> <span>contentTime</span>=<span><span>"</span>0.0<span>"</span></span>/&gt;
                  &lt;<span>Warp</span> <span>time</span>=<span><span>"</span>8.00003433227539<span>"</span></span> <span>contentTime</span>=<span><span>"</span>2.823541666666667<span>"</span></span>/&gt;
                &lt;/<span>Warps</span>&gt;
              &lt;/<span>Clip</span>&gt;
            &lt;/<span>Clips</span>&gt;
          &lt;/<span>Clip</span>&gt;
        &lt;/<span>Clips</span>&gt;
      &lt;/<span>Lanes</span>&gt;
      &lt;<span>Lanes</span> <span>track</span>=<span><span>"</span>id14<span>"</span></span> <span>id</span>=<span><span>"</span>id29<span>"</span></span>&gt;
        &lt;<span>Clips</span> <span>id</span>=<span><span>"</span>id30<span>"</span></span>/&gt;
      &lt;/<span>Lanes</span>&gt;
    &lt;/<span>Lanes</span>&gt;
  &lt;/<span>Arrangement</span>&gt;
  &lt;<span>Scenes</span>/&gt;
&lt;/<span>Project</span>&gt;</pre></div>
<h2 tabindex="-1" id="user-content-daw-support" dir="auto"><a href="#daw-support">DAW Support</a></h2>
<p dir="auto">DAWproject 1.0 is currently supported by the following DAWs</p>
<ul dir="auto">
<li>Bitwig Studio 5.0.9</li>
<li>PreSonus Studio One 6.5</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ray-Ban Meta Smart Glasses (394 pts)]]></title>
            <link>https://www.meta.com/smart-glasses/</link>
            <guid>37678860</guid>
            <pubDate>Wed, 27 Sep 2023 18:18:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.meta.com/smart-glasses/">https://www.meta.com/smart-glasses/</a>, See on <a href="https://news.ycombinator.com/item?id=37678860">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[OpenBSD PF versus FreeBSD PF (176 pts)]]></title>
            <link>https://mwl.io/archives/23127</link>
            <guid>37678714</guid>
            <pubDate>Wed, 27 Sep 2023 18:09:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mwl.io/archives/23127">https://mwl.io/archives/23127</a>, See on <a href="https://news.ycombinator.com/item?id=37678714">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-23127">
		<!-- .entry-header -->

	
	<div>
		<p>I encountered yet another discussion about <a href="https://www.openbsd.org/faq/pf/">OpenBSD PF</a> versus <a href="https://docs.freebsd.org/en/books/handbook/firewalls/">FreeBSD PF</a>. For those who are new to the discussion: OpenBSD developers created PF in 2001, and it rapidly improved to become the most approachable open source packet filter. FreeBSD ported PF over to its kernel in 2004, with occasional updates since. Today a whole bunch of folks who don’t program echo cultish wisdom that one or the other version of PF has fallen behind, not kept up on improvements, or otherwise betrayed their community. My subtler comments have been misinterpreted, so let’s try this.</p>
<p>These claims are garbage.</p>
<p>First, and most importantly: <a href="https://freshbsd.org/freebsd/src/branch/main?q=pf+openbsd">FreeBSD PF developers work with OpenBSD devs all the time</a>, and <a href="https://freshbsd.org/openbsd/src/branch/HEAD?q=pf+freebsd">OpenBSD PF developers pull stuff from FreeBSD</a><span id="easy-footnote-1-23127"></span><span><a href="#easy-footnote-bottom-1-23127" title="Commit history links courtesy of <a href=&quot;https://cathode.church/@meena/111133478906743121&quot;>Mina</a>. Yes, I could have looked it up, but she <em>thought</em> to do so."><sup>1</sup></a></span>. You get a lot of noise about certain people being jerks about the other project–and both projects absolutely have jerks. (And yes, anyone who has read my books knows that I am a cross-platform jerk.) But for the most part, folks want to work together.</p>
<p>PF is absolutely an OpenBSD creation, though, so why isn’t the OpenBSD version the Single Source of Truth? Why doesn’t FreeBSD just consider OpenBSD a vendor and pull that code in? Because the OpenBSD and FreeBSD kernels are wholly different.</p>
<p>Back when I wrote <a href="https://mwl.io/nonfiction/nope">Absolute BSD</a>, I could realistically write a single book that would basically apply to the three major open source BSDs. Yes, the various projects objected to being lumped together, but if you knew any one of them you could stumble through the others. This is no longer true. FreeBSD’s kernel uses a wholly different locking model than OpenBSD. OpenBSD’s memory protections have no equivalent in FreeBSD. These are not things you can manage with a shim layer or kernel ABI. These are big, complicated, intrusive differences. You can’t tar up one version and dump it in the other’s kernel. It won’t work. If you do a hack job of making it work, it will perform badly.</p>
<p>Yes, you can find “proof” that one PF or the other is faster under particular workloads on specific hardware. I have no doubt that some of them are not only accurate, but honest. There are other workloads, though, and other hardware, and other conditions. Regardless of who wins a particular race, the constant competition to achieve peak performance benefits everyone. I’m not going to link to any of the benchmarks, because <a href="https://mwl.io/nonfiction/wtf#l2e1">I have made my opinions on benchmarking very clear elsewhere</a>.<span id="easy-footnote-2-23127"></span><span><a href="#easy-footnote-bottom-2-23127" title="&amp;#8220;The natural impulse to compare your server against others, to whup your neighbor, is exactly that: natural. It’s like flipping rocks to find tasty grubs. Sleeping in trees coveting the upper-class caves inhabited by the snooty grizzly bears. Perishing within twenty years of What Is Vitamin C Anyway? We invented civilization to escape dysentery, cleaning ourselves with mud, and benchmarking.&amp;#8221;"><sup>2</sup></a></span> Pick what you want and roll with it.</p>
<p>Every PF developer is trundling along, doing their best to make things work.</p>
<p>Are features missing from one or the other? Yep. I’m not going to list examples because, as the above links show, each project plucks what they find useful from the other. These things are freely given, with glad hearts, but they take time to integrate. Filling message boards with staunch declarations that <em>my team’s PF is better</em> is not only tedious, it wholly misses the point.</p>
<p>People are working together to improve the world.</p>
<p>And the PF syntax is the most approachable in all of open source Unix.</p>
<p>(Partisan fanboy comments will be mercilessly whacked.)</p>
<ol><li><span id="easy-footnote-bottom-1-23127"></span>Commit history links courtesy of <a href="https://cathode.church/@meena/111133478906743121">Mina</a>. Yes, I could have looked it up, but she <em>thought</em> to do so.<a href="#easy-footnote-1-23127"></a></li><li><span id="easy-footnote-bottom-2-23127"></span>“The natural impulse to compare your server against others, to whup your neighbor, is exactly that: natural. It’s like flipping rocks to find tasty grubs. Sleeping in trees coveting the upper-class caves inhabited by the snooty grizzly bears. Perishing within twenty years of What Is Vitamin C Anyway? We invented civilization to escape dysentery, cleaning ourselves with mud, and benchmarking.”<a href="#easy-footnote-2-23127"></a></li></ol>	</div><!-- .entry-content -->

	 <!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta Quest 3 (239 pts)]]></title>
            <link>https://www.meta.com/quest/quest-3/</link>
            <guid>37678318</guid>
            <pubDate>Wed, 27 Sep 2023 17:45:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.meta.com/quest/quest-3/">https://www.meta.com/quest/quest-3/</a>, See on <a href="https://news.ycombinator.com/item?id=37678318">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Tao of Programming (1987) (165 pts)]]></title>
            <link>https://www.mit.edu/~xela/tao.html</link>
            <guid>37678181</guid>
            <pubDate>Wed, 27 Sep 2023 17:37:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mit.edu/~xela/tao.html">https://www.mit.edu/~xela/tao.html</a>, See on <a href="https://news.ycombinator.com/item?id=37678181">Hacker News</a></p>
<div id="readability-page-1" class="page">
<hr>

<h4>Translated by Geoffrey James</h4>
<hr>
<p><i>Note: I copied this from 
<tt><a href="http://misspiggy.gsfc.nasa.gov/tao.html">http://misspiggy.gsfc.nasa.gov/tao.html</a></tt>
and stripped out all of the IMHO extraneous formatting.
<br>---Alex</i>
</p><hr>
<h3>BOOK 1</h3>
<h2><i>The Silent Void</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b>"When you have learned to snatch the error code
from the trap frame, it will be time for you to leave."</b></span>
</p><hr>
<h4>1.1</h4>
<p>
Something mysterious is formed, born in the silent void.  waiting
alone and unmoving, it is at once still and yet in constant motion.
It is the source of all programs.  I do not know its name, so I will
call it the Tao of Programming.
</p><blockquote>
If the Tao is great, then the operating system is great.
<br>
If the operating system is great, then the compiler is great.
<br>
If the compiler is great, then the application is great.
<br>
The user is pleased, and there is harmony in the world.
</blockquote>
The Tao of Programming flows far away and returns on the wind of
morning.
<hr>
<h4>1.2</h4>
<p> 
The Tao gave birth to machine language.  Machine language gave birth
to the assembler.
</p><p>
The assembler gave birth to the compiler.  Now there are ten thousand
languages.
</p><p>
Each language has its purpose, however humble.  Each language
expresses the Yin and Yang of software.  Each language has its place
within the Tao.
</p><p>
But do not program in <tt>COBOL</tt> if you can avoid it.
</p><hr>
<h4>1.3</h4>
<p>
In the beginning was the Tao.  The Tao gave birth to Space and Time.
</p><p>
Therefore Space and Time are the Yin and Yang of programming.
</p><p>
Programmers that do not comprehend the Tao are always running out of
time and space for their programs.  Programmers that comprehend the
Tao always have enough time and space to accomplish their goals.
</p><p>
How could it be otherwise?
</p><hr>
<h4>1.4</h4>
<p>
The wise programmer is told about Tao and follows it.  The average
programmer is told about Tao and searches for it.  The foolish
programmer is told about Tao and laughs at it.
</p><p>
If it were not for laughter, there would be no Tao.
</p><p>
The highest sounds are hardest to hear.  Going forward is a way to
retreat.  Great talent shows itself late in life.  Even a perfect
program still has bugs.
</p><hr>
<h3>BOOK 2</h3>
<h2><i>The Ancient Masters</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b> "After three days without programming, life
becomes meaningless."</b></span>
</p><hr>
<h4>2.1</h4>
<p>
The programmers of old were mysterious and profound.  We cannot fathom
their thoughts, so all we do is describe their appearance.
</p><blockquote>
Aware, like a fox crossing the water.
<br>
Alert, like a general on the battlefield.  
<br>
Kind, like a hostess greeting her guests.  
<br>
Simple, like uncarved blocks of wood.  
<br>
Opaque, like black pools in darkened caves.
</blockquote>
Who can tell the secrets of their hearts and minds?
<p>
The answer exists only in Tao.
</p><hr>
<h4>2.2</h4>
<p>
The Grand Master Turing once dreamed that he was a machine.  When he
awoke, he exclaimed:
</p><p>
"I don't know whether I am Turing dreaming that I am a machine, or a
machine dreaming that I am Turing!"
</p><hr>
<h4>2.3</h4>
<p>
A programmer from a very large computer company went to a software
conference and then returned to report to his manager, saying: "What
sort of programmers work for other companies?  They behaved badly and
were unconcerned with appearances.  Their hair was long and unkempt
and their clothes were wrinkled and old.  They crashed our hospitality
suite and they made rude noises during my presentation."
</p><p>
The manager said: "I should have never sent you to the conference.
Those programmers live beyond the physical world.  They consider life
absurd, an accidental coincidence.  They come and go without knowing
limitations.  Without a care, they live only for their programs.  Why
should they bother with social conventions?
</p><p>
They are alive within the Tao."
</p><hr>
<h4>2.4</h4>
<p>
A novice asked the Master: "Here is a programmer that never designs,
documents or tests his programs.  Yet all who know him consider him
one of the best programmers in the world.  Why is this?"
</p><p>
The Master replied: "That programmer has mastered the Tao.  He has
gone beyond the need for design; he does not become angry when the
system crashes, but accepts the universe without concern.  He has gone
beyond the need for documentation; he no longer cares if anyone else
sees his code.  He has gone beyond the need for testing; each of his
programs are perfect within themselves, serene and elegant, their
purpose self-evident.  Truly, he has entered the mystery of Tao."
</p><hr>
<h3>BOOK 3</h3>
<h2><i>Design</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b> "When a program is being tested, it is too late
to make design changes."</b></span>
</p><hr>
<h4>3.1</h4>
<p>
There once was a man who went to a computer trade show.  Each day as
he entered, the man told the guard at the door:
</p><p>
"I am a great thief, renowned for my feats of shoplifting.  Be
forewarned, for this trade show shall not escape unplundered."
</p><p>
This speech disturbed the guard greatly, because there were millions
of dollars of computer equipment inside, so he watched the man
carefully.  But the man merely wandered from booth to booth, humming
quietly to himself.
</p><p>
When the man left, the guard took him aside and searched his clothes,
but nothing was to be found.
</p><p>
On the next day of the trade show, the man returned and chided the
guard, saying: "I escaped with a vast booty yesterday, but today will
be even better."  So the guard watched him ever more closely, but to
no avail.
</p><p>
On the final day of the trade show, the guard could restrain his
curiosity no longer.  "Sir Thief," he said, "I am so perplexed, I
cannot live in peace.  Please enlighten me.  What is it that you are
stealing?"
</p><p>
The man smiled.  "I am stealing ideas," he said.
</p><hr>
<h4>3.2</h4>
<p>
There once was a Master Programmer who wrote unstructured programs.  A
novice programmer, seeking to imitate him, also began to write
unstructured programs.  When the novice asked the Master to evaluate
his progress, the Master criticized him for writing unstructured
programs, saying, "What is appropriate for the Master is not
appropriate for the novice.  You must understand Tao before
transcending structure."
</p><hr>
<h4>3.3</h4>
<p>
There was once a programmer who was attached to the court of the
warlord of Wu.  The warlord asked the programmer: "Which is easier to
design: an accounting package or an operating system?"
</p><p>
"An operating system," replied the programmer.
</p><p>
The warlord uttered an exclamation of disbelief.  "Surely an
accounting package is trivial next to the complexity of an operating
system," he said.
</p><p>
"Not so," said the programmer, "When designing an accounting package,
the programmer operates as a mediator between people having different
ideas: how it must operate, how its reports must appear, and how it
must conform to the tax laws.  By contrast, an operating system is not
limited by outside appearances.  When designing an operating system,
the programmer seeks the simplest harmony between machine and ideas.
This is why an operating system is easier to design."
</p><p>
The warlord of Wu nodded and smiled.  "That is all good and well, but
which is easier to debug?"
</p><p>
The programmer made no reply.
</p><hr>
<h4>3.4</h4>
<p>
A manager went to the Master Programmer and showed him the
requirements document for a new application.  The manager asked the
Master: "How long will it take to design this system if I assign five
programmers to it?"
</p><p>
"It will take one year," said the Master promptly.
</p><p>
"But we need this system immediately or even sooner!  How long will it
take if I assign ten programmers to it?"
</p><p>
The Master Programmer frowned.  "In that case, it will take two
years."
</p><p>
"And what if I assign a hundred programmers to it?"
</p><p>
The Master Programmer shrugged.  "Then the design will never be
completed," he said.
</p><hr>
<h3>BOOK 4</h3>
<h2><i>Coding</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b> "A well-written program is its own Heaven; a
poorly-written program is its own Hell."</b></span>
</p><hr>
<h4>4.1</h4>
<p>
A program should be light and agile, its subroutines connected like a
string of pearls.  The spirit and intent of the program should be
retained throughout.  There should be neither too little nor too much.
Neither needless loops nor useless variables; neither lack of
structure nor overwhelming rigidity.
</p><p>
A program should follow the "Law of Least Astonishment".  What is this
law?  It is simply that the program should always respond to the users
in the way that least astonishes them.
</p><p>
A program, no matter how complex, should act as a single unit.  The
program should be directed by the logic within rather than by outward
appearances.
</p><p>
If the program fails in these requirements, it will be in a state of
disorder and confusion.  The only way to correct this is to rewrite
the program.
</p><hr>
<h4>4.2</h4>
<p>
A novice asked the Master: "I have a program that sometimes runs and
sometimes aborts.  I have followed the rules of programming, yet I am
totally baffled.  What is the reason for this?"
</p><p>
The Master replied: "You are confused because you do not understand
Tao.  Only a fool expects rational behavior from his fellow humans.
Why do you expect it from a machine that humans have constructed?
Computers simulate determinism; only Tao is perfect.
</p><p>
The rules of programming are transitory; only Tao is eternal.
Therefore, you must contemplate Tao before you receive Enlightenment."
</p><p>
"But how will I know when I have received Enlightenment?"  asked the
novice.
</p><p>
"Your program will run correctly," replied the Master.
</p><hr>
<h4>4.3</h4>
<p>
The Master was explaining the nature of Tao to one of his novices.
</p><p>
"The Tao is embodied in all software -- regardless of how
insignificant," said the Master.
</p><p>
"Is the Tao in a hand-held calculator?" asked the novice.
</p><p>
"It is," came the reply.
</p><p>
"Is the Tao in a video game?" asked the novice.
</p><p>
"It is even in a video game," said the Master.
</p><p>
"Is the Tao in the <tt>DOS</tt> for a personal computer?" asked the novice.
</p><p>
The Master coughed and shifted his position slightly.  "The lesson is
over for today," he said.
</p><hr>
<h4>4.4</h4>
<p>
Prince Wang's programmer was coding software.  His fingers danced upon
the keyboard.  The program compiled without and error message, and the
program ran like a gentle wind.
</p><p>
"Excellent!"  the Prince exclaimed.  "Your technique is faultless!"
</p><p>
"Technique?"  said the programmer, turning from his terminal, "What I
follow is Tao -- beyond all techniques!  When I first began to
program, I would see before me the whole problem in one mass.  After
three years, I no longer saw this mass.  Instead, I used subroutines.
But now I see nothing.  My whole being exists in a formless void.  My
senses are idle.  My spirit, free to work without a plan, follows its
own instinct.  In short, my program writes itself.  True, sometimes
there are difficult problems.  I see them coming, I slow down, I watch
silently.  Then I change a single line of code and the difficulties
vanish like puffs of idle smoke.  I then compile the program.  I sit
still and let the joy of the work fill my being.  I close my eyes for
a moment and then log off."
</p><p>
Prince Wang said, "Would that all of my programmers were as wise!"
</p><hr>
<h3>BOOK 5</h3>
<h2><i>Maintenance</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b> "Though a program be but three lines long,
someday it will have to be maintained."</b></span>
</p><hr>
<h4>5.1</h4>
<blockquote>
A well-used door needs no oil on its hinges.
<br>
A swift-flowing stream does not grow stagnant.
<br>
A deer blends perfectly into the forest colors.
<br>
Software rots if not used.
</blockquote>
These are great mysteries.
<hr>
<h4>5.2</h4>
<p>
A manager asked a programmer how long it would take him to finish the
program on which he was working.  "I will be finished tomorrow," the
programmer promptly replied.
</p><p>
"I think you are being unrealistic," said the manager, "Truthfully,
how long will it take?"
</p><p>
The programmer thought for a moment.  "I have some features that I
wish to add.  This will take at least two weeks," he finally said.
</p><p>
"Even that is too much to expect," insisted the manager, "I will be
satisfied if you simply tell me when the program is complete."
</p><p>
The programmer agreed to this.
</p><p>
Several years later, the manager retired.  On the way to his
retirement luncheon, he discovered the programmer asleep at his
terminal.  He had been programming all night.
</p><hr>
<h4>5.3</h4>
<p>
A novice programmer was once assigned to code a simple financial
package.
</p><p>
The novice worked furiously for many days, but when his Master
reviewed his program, he discovered it contained a screen editor, a
set of generalized graphics routines, and an artificial intelligence
interface, but not the slightest hint of anything financial.
</p><p>
When the Master asked about this, the novice became indignant.  "Don't
be so impatient," he said, "I'll put in the financial stuff
eventually."
</p><hr>
<h4>5.4</h4>
<blockquote>
Does a good farmer neglect a crop he has planted?
<br>
Does a good teacher overlook even the most humble student?
<br>
Does a good father allow a single child to starve?
<br>
Does a good programmer refuse to maintain his code?
</blockquote>
<hr>
<h3>BOOK 6</h3>
<h2><i>Management</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b> "Let the programmers be many and the managers few
-- then all will be productive."</b></span>
</p><hr>
<h4>6.1</h4>
<blockquote>
When managers hold endless meetings, the programmers write games.
<br>
When accountants speak of quarterly profits, the development budget is
about to be cut.
<br>
When senior scientists talk blue sky, the clouds are about to roll in.
</blockquote>
Truly, this is not the Tao of Programming.
<blockquote>
When managers make commitments, game programs are ignored.
<br>
When accountants make long-range plans, harmony and order are about to
be restored.
<br>
When senior scientists address the problems at hand, the problems will
soon be solved.
</blockquote>
Truly, this is the Tao of Programming.
<hr>
<h4>6.2</h4>
<blockquote>
Why are programmers non-productive?  Because their time is wasted in
meetings.
<br>
Why are programmers rebellious?  Because the management interferes too
much.
<br>
Why are the programmers resigning one by one?  Because they are burnt
out.
<br>
Having worked for poor management, they no longer value their jobs.
</blockquote>
<hr>
<h4>6.3</h4>
<p>
A manager was about to be fired, but a programmer who worked for him
wrote a new program that became popular and sold well.  As a result,
the manager retained his job.
</p><p>
The manager tried to give the programmer a bonus, but the programmer
refused it, saying, "I wrote the program because I thought it was an
interesting concept, and thus I expect no reward."
</p><p>
The manager upon hearing this remarked, "This programmer, though he
holds a position of small esteem, understands well the proper duty of
an employee.  Let us promote him to the exalted position of management
consultant!"
</p><p>
But when told this, the programmer once more refused, saying, "I exist
so that I can program.  If I were promoted, I would do nothing but
waste everyone's time.  Can I go now?  I have a program that I am
working on."
</p><hr>
<h4>6.4</h4>
<p>
A manager went to his programmers and told them: "As regards to your
work hours: you are going to have to come in at nine in the morning
and leave at five in the afternoon."  At this, all of them became
angry and several resigned on the spot.
</p><p>
So the manager said: "All right, in that case you may set your own
working hours, as long as you finish your projects on schedule."  The
programmers, now satisfied, began to come in at noon and work to the
wee hours of the morning.
</p><hr>
<h3>BOOK 7</h3>
<h2><i>Corporate Wisdom</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b> "You can demonstrate a program for a corporate
executive, but you can't make him computer literate."</b></span>
</p><hr>
<h4>7.1</h4>
<p>
A novice asked the Master: "In the East, there is a great
tree-structure that men call 'Corporate Headquarters'.  It is bloated
out of shape with vice presidents and accountants.  It issues a
multitude of memos, each saying 'Go Hence!' or 'Go Hither!' and nobody
knows what is meant.  Every year new names are put onto the branches,
but all to no avail.  How can such an unnatural entity exist?"
</p><p>
The Master replied: "You perceive this immense structure and are
disturbed that it has no rational purpose.  Can you not take amusement
from its endless gyrations?  Do you not enjoy the untroubled ease of
programming beneath its sheltering branches?  Why are you bothered by
its uselessness?"
</p><hr>
<h4>7.2</h4>
<p>
In the East there is a shark which is larger than all other fish.  It
changes into a bird whose wings are like clouds filling the sky.  When
this bird moves across the land, it brings a message from Corporate
Headquarters.  This message it drops into the midst of the
programmers, like a seagull making its mark upon the beach.  Then the
bird mounts on the wind and, with the blue sky at its back, returns
home.
</p><p>
The novice programmer stares in wonder at the bird, for he understands
it not.  The average programmer dreads the coming of the bird, for he
fears its message.  The Master Programmer continues to work at his
terminal, unaware that the bird has come and gone.
</p><hr>
<h4>7.3</h4>
<p>
The Magician of the Ivory Tower brought his latest invention for the
Master Programmer to examine.  The Magician wheeled a large black box
into the Master's office while the Master waited in silence.
</p><p>
"This is an integrated, distributed, general-purpose workstation,"
began the Magician, "ergonomically designed with a proprietary
operating system, sixth generation languages, and multiple state of
the art user interfaces.  It took my assistants several hundred man
years to construct.  Is it not amazing?"
</p><p>
The Master Programmer raised his eyebrows slightly.  "It is indeed
amazing," he said.
</p><p>
"Corporate Headquarters has commanded," continued the Magician, "that
everyone use this workstation as a platform for new programs.  Do you
agree to this?"
</p><p>
"Certainly," replied the Master.  "I will have it transported to the
Data Center immediately!"  And the Magician returned to his tower,
well pleased.
</p><p>
Several days later, a novice wandered into the office of the Master
Programmer and said, "I cannot find the listing for my new program.
Do you know where it might be?"
</p><p>
"Yes," replied the Master, "the listings are stacked on the platform
in the Data Center."
</p><hr>
<h4>7.4</h4>
<p>
The Master Programmer moves from program to program without fear.  No
change in management can harm him.  He will not be fired, even if the
project is cancelled.  Why is this?  He is filled with Tao.
</p><hr>
<h3>BOOK 8</h3>
<h2><i>Hardware and Software</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b> "Without the wind, the grass does not move.
Without software hardware is useless."</b></span>
</p><hr>
<h4>8.1</h4>
<p>
A novice asked the Master: "I perceive that one computer company is
much larger than all others.  It towers above its competition like a
giant among dwarfs.  Any one of its divisions could comprise an entire
business.  Why is this so?"
</p><p>
The Master replied, "Why do you ask such foolish questions?  That
company is large because it is large.  If it only made hardware,
nobody would buy it.  If it only made software, nobody would use it.
If it only maintained systems, people would treat it like a servant.
But because it combines all of these things, people think it one of
the gods!  By not seeking to strive, it conquers without effort."
</p><hr>
<h4>8.2</h4>
<p>
A Master Programmer passed a novice programmer one day.
</p><p>
The Master noted the novice's preoccupation with a hand-held computer
game.
</p><p>
"Excuse me," he said, "may I examine it?"
</p><p>
The novice bolted to attention and handed the device to the Master.
"I see that the device claims to have three levels of play: Easy,
Medium, and Hard," said the Master.  "Yet every such device has
another level of play, where the device seeks not to conquer the
human, nor to be conquered by the human."
</p><p>
"Pray, Great Master," implored the novice, "how does one find this
mysterious setting?"
</p><p>
The Master dropped the device to the ground and crushed it with his
heel.  Suddenly the novice was enlightened.
</p><hr>
<h4>8.3</h4>
<p>
There was once a programmer who wrote software for personal computers.
"Look at how well off I am here," he said to a mainframe programmer
who came to visit.  "I have my own operating system and file storage
device.  I do not have to share my resources with anyone.  The
software is self-consistent and easy-to-use.  Why do you not quit your
present job and join me here?"
</p><p>
The mainframe programmer then began to describe his system to his
friend, saying, "The mainframe sits like an ancient Sage meditating in
the midst of the Data Center.  Its disk drives lie end-to- end like a
great ocean of machinery.  The software is as multifaceted as a
diamond, and as convoluted as a primeval jungle.  The programs, each
unique, move through the system like a swift-flowing river.  That is
why I am happy where I am."
</p><p>
The personal computer programmer, upon hearing this, fell silent.  But
the two programmers remained friends until the end of their days.
</p><hr>
<h4>8.4</h4>
<p>
Hardware met Software on the road to Changtse.  Software said: "You
are Yin and I am Yang.  If we travel together, we will become famous
and earn vast sums of money."  And so they set forth together,
thinking to conquer the world.
</p><p>
Presently, they met Firmware, who was dressed in tattered rags and
hobbled along propped on a thorny stick.  Firmware said to them: "The
Tao lies beyond Yin and Yang.  It is silent and still as a pool of
water.  It does not seek fame; therefore, nobody knows its presence.
It does not seek fortune, for it is complete within itself.  It exists
beyond space and time."
</p><p>
Software and Hardware, ashamed, returned to their homes.
</p><hr>
<h3>BOOK 9</h3>
<h2><i>Epilogue</i></h2>
<h5>Thus spake the Master Programmer:</h5>
<p><span size="+1"><b> "Time for you to leave."</b></span>
</p><hr>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Ashley Book of Knots (1944) (158 pts)]]></title>
            <link>https://archive.org/details/TheAshleyBookOfKnots</link>
            <guid>37676880</guid>
            <pubDate>Wed, 27 Sep 2023 16:13:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://archive.org/details/TheAshleyBookOfKnots">https://archive.org/details/TheAshleyBookOfKnots</a>, See on <a href="https://news.ycombinator.com/item?id=37676880">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>The Ashley Book of Knots and Macrame. working with rope, line and plating Every practical Knot, what it looks like, Who uses it, where it comes from and how to tie it with 7000 drawings representing 3800 knots.&nbsp;
</p><div id="reviews">
      <h2>
                  
                <p><span>comment</span></p>
        Reviews
      </h2>

      
      
      
                  <div id="review-1631625278">
      <p><b>Reviewer:</b>
              <a href="https://archive.org/details/%40k34456" data-event-click-tracking="ItemReviews|ReviewerLink">k34456</a>
            -
      <span alt="5.00 out of 5 stars" title="5.00 out of 5 stars"><span>favorite</span><span>favorite</span><span>favorite</span><span>favorite</span><span>favorite</span></span>      -
      September 14, 2021      <br>
      <b>Subject:</b>
      Not in the public domain      </p><div><p>
        This is a very useful book on knots. It is truly the knot bible. </p><p>

Unfortunately, however, this is the 1993 version and contains corrected material that are still protected under the copyright laws. To the reviewer who claimed that this book is in public domain, you are wrong. The Internet Archive contains much material, including game source codes that were stolen from game software companies, that are illegal to distribute. (Regarding the stolen source code, you need to know how to look for them in the IA because the uploaders used "special" words that are recognizable only by those who know what the stolen files represent.) I've also seen books (including proprietary material stolen from private corporations that have words like "proprietary" or "confidential" marked in them) posted in IA that were published within the past couple of decades and thus are still copyrighted. So this says that the IA simply does not have the resources to go through every uploaded file and delete the ones that are not legal to download. There are simply too many of them for any single individual like me to report them to IA for deletion. Just trying to report them to the IA would be a full-time job.</p><p>

As for this Ashley book, for it to be truly in the public domain, it needs to be the original 1944 version, the one that has errors that the 1993 version corrected. However, there is also the possibility that the copyright owner may have given the permission for the 1993 version of the book to be available via the IA, without releasing the book into the public domain, but I don't know how IA would be able to know this without contacting the copyright owner.      </p></div>
    </div>
        <div id="review-1596117362">
      <p><b>Reviewer:</b>
              <a href="https://archive.org/details/%40666thenumberofthebeast" data-event-click-tracking="ItemReviews|ReviewerLink">Harem Cinema</a>
            -
      <span alt="5.00 out of 5 stars" title="5.00 out of 5 stars"><span>favorite</span><span>favorite</span><span>favorite</span><span>favorite</span><span>favorite</span></span>      -
      July 30, 2020      <br>
      <b>Subject:</b>
      You Have The Thanks Of A Sailor      </p><p>
        All I can say is a simple, but heart-felt Thank You, for sharing this with us all.      </p>
    </div>
        <div id="review-1568139117">
      <p><b>Reviewer:</b>
              <a href="https://archive.org/details/%40searcharchives" data-event-click-tracking="ItemReviews|ReviewerLink">ChronicKristinitis</a>
            -
      <span alt="5.00 out of 5 stars" title="5.00 out of 5 stars"><span>favorite</span><span>favorite</span><span>favorite</span><span>favorite</span><span>favorite</span></span>      -
      September 10, 2019      <br>
      <b>Subject:</b>
      Wow      </p><div><p>
        I loved reading "knot books" when I first started sailing. Sailing of course is no requisite for studying knots! or "bends".... </p><p>

This is a lovely book written by a lovely man who cared enough about the world to share this work, which took him at least eleven years to create. </p><p>

PS- this title surely IS in public domain, so- the comment from the "public domain police" is unnecessary. If in the case a copyrighted file is in the Archive by mistake, it will quickly be removed.       </p></div>
    </div>
        <div id="review-1506973798">
      <p><b>Reviewer:</b>
              <a href="https://archive.org/details/%40gary_bunker" data-event-click-tracking="ItemReviews|ReviewerLink">Gary Bunker</a>
            -
      <span alt="0.00 out of 5 stars" title="0.00 out of 5 stars"></span>      -
      October 2, 2017      <br>
      <b>Subject:</b>
      Public Domain?      </p><div><p>
        The copyright laws in the USA say that a work enters public domain 70 years after the death of the author. Mr. Ashley died in 1947, so it is plausible that it would enter public domain this year. But, it was posted last year. And, the copyright notice is from an edition with amended material dated 1993.</p><p>

Is this truly public domain?      </p></div>
    </div>
        <div id="review-1476416070">
      <p><b>Reviewer:</b>
              <a href="https://archive.org/details/%40fleetwing" data-event-click-tracking="ItemReviews|ReviewerLink">fleetwing</a>
            -
      <span alt="5.00 out of 5 stars" title="5.00 out of 5 stars"><span>favorite</span><span>favorite</span><span>favorite</span><span>favorite</span><span>favorite</span></span>      -
      October 14, 2016      <br>
      <b>Subject:</b>
      Knots      </p><p>
        The Necessary requirement Encyclopedia of Knots for all Seafarers...   in order to Survive.      </p>
    </div>
              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Live near your friends (405 pts)]]></title>
            <link>https://headlineshq.substack.com/p/issue-no-029-live-near-your-friends</link>
            <guid>37676393</guid>
            <pubDate>Wed, 27 Sep 2023 15:45:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://headlineshq.substack.com/p/issue-no-029-live-near-your-friends">https://headlineshq.substack.com/p/issue-no-029-live-near-your-friends</a>, See on <a href="https://news.ycombinator.com/item?id=37676393">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h4>Discover more from Headlines</h4><p>A newsletter covering the consciousness, psychedelic, and mental health economy.</p><p>Over 2,000 subscribers</p> </div><div dir="auto"><p><span>Good morning, and happy Sunday. Great to see you here for another issue of </span><em>Headlines</em><span>. I thoroughly enjoyed researching and writing this one. Let’s get right to it.&nbsp;&nbsp;</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png" width="1456" height="770" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:770,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F483fdd00-9492-484b-9e3c-1af9a87e9b34_1600x846.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>Deep and meaningful friendships are </span><a href="https://twitter.com/melodaysong/status/1690068103869468672?s=20" rel="">integral</a><span> to a happy and healthy life.&nbsp;</span></p><p>Why not live closer to your pals?&nbsp;</p><p><strong>THE FRIENDSHIP RECESSION</strong></p><p><span>As we’ve </span><a href="https://headlineshq.substack.com/p/issue-no-010-we-vs-me" rel="">written about before</a><span>, friendship and community are essential to our emotional well-being. The longest-ever longitudinal study on human life found that deep relationships hold, by far, the </span><a href="https://www.theatlantic.com/ideas/archive/2023/01/harvard-happiness-study-relationships/672753/" rel="">strongest correlation with our health and happiness</a><span>.</span></p><p><span>Unfortunately, the last decade has seen a steep drop in adult friendships. Modern life encourages us, </span><a href="https://chwoodiwiss.medium.com/im-moving-into-my-own-place-and-i-m-sad-about-it-1ecb5f423009" rel="">writes</a><span> editor and journalist Catherine Woodiwiss, to atomize ourselves away from each other:</span></p><blockquote><p><em>“We seem to be doing life backward: We live alone and expend effort to gather together, as if that’s the healthy baseline; instead of starting with togetherness as the foundation, and striking out for aloneness when we need it.”</em></p></blockquote><p><span>Indeed, the rise of hyperindividualism has fragmented our connections, scattering our relationships across the country. Even today’s modern self-care trends </span><a href="https://time.com/6271915/self-love-loneliness/" rel="">turn us inward</a><span>, convincing us to “hyperfocus on ourselves at the expense of connecting with others.”&nbsp;</span></p><p><strong>And the numbers don’t lie: </strong><span>Americans are spending </span><a href="https://www.washingtonpost.com/opinions/2022/11/23/americans-alone-thanksgiving-friends/" rel="">more and more time alone</a><span> and less and less time with their friends.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png" width="555" height="431.40425531914894" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1096,&quot;width&quot;:1410,&quot;resizeWidth&quot;:555,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb989b868-5782-4905-b8f1-ec5c98c6b389_1410x1096.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Meanwhile, the number of close friendships Americans have has </span><a href="https://www.americansurveycenter.org/research/the-state-of-american-friendship-change-challenges-and-loss/" rel="">plunged over the last two decades</a><span>; only 13% of Americans report having 10 or more close friends, down from 33% in the 1990s.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png" width="647" height="348.38461538461536" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:784,&quot;width&quot;:1456,&quot;resizeWidth&quot;:647,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F750c4648-a881-4e39-9fb0-e566e28aa8c9_1600x861.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>PEAS IN A POD</strong></p><p>In a 1935 letter to his lifelong friend Arthur Greeves, author C.S. Lewis wrote:&nbsp;</p><blockquote><p><em>“Friendship [to me] is the chief happiness of life. If I had to give a piece of advice to a young man about a place to live, I think I should say, 'sacrifice almost everything to live where you can be near your friends.’”</em></p></blockquote><p>Yet our friendships today have taken a backseat to marriage, career, and more. While technology has made it easier than ever to maintain bonds across geographic distances, ease can’t replace depth.&nbsp;</p><ul><li><p><span>Having a friend whom you see on most days, compared to not having such a friend, has the same impact on well-being as </span><a href="https://www.theatlantic.com/health/archive/2013/10/social-connection-makes-a-better-brain/280934/" rel="">making an extra $100K a year</a><span>.&nbsp;</span></p></li><li><p><span>A </span><a href="https://news.harvard.edu/gazette/story/2008/12/having-happy-friends-can-make-you-happy/" rel="">20-year multi-generational study</a><span> showed that living within a mile of a friend who is happy increases the likelihood that you’ll be happy by 25%.</span></p></li></ul><p><span>Given the undeniable ties between meaningful relationships and well-being, why aren’t we placing </span><a href="https://www.theatlantic.com/family/archive/2020/10/people-who-prioritize-friendship-over-romance/616779/" rel="">friendship at the center of our lives</a><span>? That’s a question the live-near-your-friends movement is trying to answer.</span></p><p><strong>Live near your friends.</strong><span> At the frontier of this movement is Phil Levin, founder of coliving community </span><a href="https://radish.super.site/" rel="">Radish</a><span> in Oakland, California and co-founder of the car-free neighborhood project </span><a href="https://culdesac.com/" rel="">Culdesac</a><span>.</span></p><p><span>This August, Levin launched </span><a href="https://www.livenearfriends.com/" rel="">Live Near Friends</a><span>, a site that helps people live within a five-minute walk of a close friend or family member.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png" width="1456" height="822" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:822,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1b46524d-ba35-47f8-9e55-8b8bf3100228_1600x903.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Live Near Friends, Levin says, was inspired by his wife Kristen, who would proactively send out listings to friends trying to get them to move nearby. Over three years, she succeeded in getting eight people to rent and buy homes within a short walk of where they live. Now, they </span><a href="https://supernuclear.substack.com/p/babies-radish-the-early-review" rel="">help take care of each other’s kids</a><span>, do regular dinners, and hang out on a whim.</span></p><p><span>Others in this space include grassroots projects like NYC’s </span><a href="https://prigoose.substack.com/p/how-to-live-near-your-friends" rel="">Fractal</a><span>, a collective of ten living rooms within a five-minute walk from Morgan Ave’s L train station. There’s also SF’s Neighborhood, led by </span><a href="https://jasonbenn.com/" rel="">Jason Benn</a><span>, a multigenerational campus of 200+ people living within a square mile in central San Francisco.&nbsp;</span></p><p><strong>What if I don’t have friends? </strong><span>A sobering statistic you may have noticed above: 12% of Americans say they have </span><em>zero</em><span> close friends. Hoping to help, apps like </span><a href="https://www.getsaturday.com/" rel="">Saturday</a><span> and </span><a href="https://www.geneva.com/" rel="">Geneva</a><span> want to ease the discovery process while dating app Bumble launched </span><a href="https://bumble.com/en/the-buzz/bumble-for-friends" rel="">Bumble for Friends</a><span>.&nbsp;</span></p><p><span>And, as we’ve covered before, the </span><a href="https://headlineshq.substack.com/p/issue-no-018-social-prescribing" rel="">social prescribing model</a><span> uses local “link workers” to help nudge people toward nonclinical care via movement, art, and more. Here, orgs like </span><a href="https://uniteus.com/" rel="">Unite Us</a><span>, </span><a href="https://www.widercircle.com/" rel="">Wider Circle</a><span>, and </span><a href="https://company.findhelp.com/" rel="">findhelp</a><span> look to power community health and social care systems.&nbsp;</span></p><p><span>Even the federal government is stepping in. This July, Senator Chris Murphy introduced legislation to create a </span><a href="https://www.murphy.senate.gov/download/nssc-one-pager#:~:text=THE%20BILL%20WOULD%3A&amp;text=Create%20an%20Office%20of%20Social,and%20civic%20and%20community%20engagement." rel="">national policy to promote social connection</a><span>.</span></p><p><span>But, Levin notes, many interventions tend to put too much onus on the individual and not enough on </span><a href="https://insider.fitt.co/issue-no-222-living-well/" rel="">our built environment</a><span>, adding that:&nbsp;</span></p><blockquote><p><em>“People are often told to ‘go meet their neighbors.’ But for me, the question is more about: ‘How can we design places so it’s impossible not to have a relationship with your neighbors?’”&nbsp;</em></p></blockquote><p><strong>A NOTE FROM MEL</strong></p><p>This topic is one close to my heart. I’ve moved around a lot these past few years, and my friends are scattered cross-country, from Los Angeles to New York, Austin to San Francisco.&nbsp;</p><p>The pandemic further cemented the slow dissolution of many relationships I wish to rekindle but are complicated by the sheer fact of geography. This year in particular, after moving from LA to SF, I saw many dear friends much less frequently; it had an undeniable impact on my emotional well-being and resilience.&nbsp;</p><p>All this is to say: These past few months have underlined the importance of deep and meaningful relationships to me. Time and time again, when life gets rough, it is my friends who consistently show me the love and compassion I am unable to give myself, gently lifting me back up into the light.&nbsp;</p><p><strong>Punchline: </strong><span>How do we help people build and nurture meaningful relationships? It’s perhaps the most important question we should be addressing in the mental health space.&nbsp;</span></p><p>Tackling the loneliness epidemic will be no easy challenge—and not everyone can move close to their friends—but it’s a great goal to strive for. The good thing is, the first step can be very simple. A text to an old friend, a coffee with a new one. Little moments of connection spark more connection, and connection goes a long way.&nbsp;</p><div data-attrs="{&quot;url&quot;:&quot;https://headlineshq.substack.com/p/issue-no-029-live-near-your-friends?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="CaptionedButtonToDOM"><p><span>Thanks for reading ☁️🍄. Want to join the convo? Tag me </span><a href="https://twitter.com/melodaysong" rel="">@melodaysong</a><span> with your thoughts, or share with a friend below.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://headlineshq.substack.com/p/issue-no-029-live-near-your-friends?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="ButtonCreateButton"><a href="https://headlineshq.substack.com/p/issue-no-029-live-near-your-friends?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div><ul><li><p><strong>Master’s in happiness. </strong><span>Get your </span><a href="https://fortune.com/well/2023/08/05/masters-degree-in-happiness/" rel="">degree in happiness</a><span>, a 20-month interdisciplinary program on the science of well-being.</span></p></li><li><p><strong>Mayday. </strong><span>New study from Little Otter analyzes </span><a href="https://8179936.fs1.hubspotusercontent-na1.net/hubfs/8179936/Blogs%20for%20Download/Breaking%20the%20Silence.pdf" rel="">11K+</a><span> families, shines light on the worsening pediatric mental health crisis.&nbsp;</span></p></li><li><p><strong><span>Google it. The tech giant is </span><a href="https://www.nytimes.com/2023/08/16/technology/google-ai-life-advice.html" rel="">testing out</a><span> an AI bot that can offer life advice. [Re-read</span><a href="https://headlineshq.substack.com/p/issue-no-017-ai-eats-therapy" rel=""> </a></strong><em><strong><a href="https://headlineshq.substack.com/p/issue-no-017-ai-eats-therapy" rel="">Issue No. 017: AI Eats Therapy.</a><span>)</span></strong></em></p></li><li><p><strong>Sobering stats. </strong><span>About </span><a href="https://www.latimes.com/california/story/2023-04-13/988-hotline-mental-health-crisis-system-police?utm_id=107875&amp;sfmc_id=5276556&amp;skey_id=dfa482c87b8037d578676754f18d10a6d9d65b822a65565197ef4d291a62bcc5" rel="">40%</a><span> of people killed by police officers are involved in a mental health crisis.</span></p></li><li><p><strong>Ready or not, here AI come. </strong><span>AI-driven chronic health app juli </span><a href="https://medcitynews.com/2023/08/digital-health-app-helps-asthma-and-depression-2-trials-find/" rel="">improved</a><span> symptoms of asthma and depression.</span></p></li><li><p><strong>New name, who this? </strong><span>MAPS’ trademark filings reveal potential brand name for MDMA: </span><a href="https://psychedelicalpha.com/news/psychedelic-bulletin-143-trademark-filing-reveals-potential-brand-name-for-mdma-dea-open-to-considering-special-registration-for-controlled-substances-via-telemedicine-professional-practice-gu?utm_source=tricycleday&amp;utm_medium=newsletter&amp;utm_campaign=this-week-in-psychedelics" rel="">RENSANSE</a><span>.</span></p></li></ul><p><strong>1) Coming of age</strong></p><p><span>New studies from </span><a href="https://www.sciencedirect.com/science/article/abs/pii/S2215036623001931?dgcid=author" rel="">The Lancet Psychiatry</a><span> and </span><a href="https://news.blueshieldca.com/2023/08/03/new-poll-mental-health-challenges-prevalent-among-gen-z-youth-more-than-three-in-four-have-discussed-their-struggles-with-others" rel="">Blue Shield of CA</a><span> reveal mental disorders are on the rise, and Gen Z is suffering the most. A whopping nine out of 10 Gen Z youth say they’re experiencing mental health challenges on a regular basis, citing gun violence, racial/social injustice, and </span><a href="https://headlineshq.substack.com/p/issue-no-026-feeling-the-heat" rel="">climate change</a><span>.&nbsp;</span></p><p><span>Meanwhile, with 50% of the world’s population projected to develop at least one mental disorder in their lifetime, the mental health crisis has reached beyond the ability of a single sector to solve. Solutions will require concerted efforts across government, healthcare, Big Tech, and more — paying special care to prevention among young people.</span><br><strong><span>→ </span><a href="https://insider.fitt.co/the-mental-health-crisis-worsens/" rel="">Read more</a></strong></p><p><strong>2) Bird is the word</strong><span>&nbsp;</span></p><p><span>Speaking of the younger generation, there’s a new mental health trend sweeping Gen Z: Bird-watching. Posts tagged with #birdwatching and #birding on TikTok have over 1.4B and 240M views, respectively, while apps like </span><a href="https://birda.org/" rel="">Birda</a><span>, </span><a href="https://merlin.allaboutbirds.org/" rel="">Merlin Bird ID</a><span>, and </span><a href="https://birdnet.cornell.edu/" rel="">BirdNET</a><span> have reported up to 30% increases in monthly signup rates. With few barriers to entry (save a pair of binoculars), the trend is taking flight — many are calling it the </span><a href="https://www.hollywoodrepbirrter.com/lifestyle/lifestyle-news/birding-hollywood-meditation-1235503559/" rel="">new meditation</a><span>.&nbsp;</span></p><p><span>Not just a hobby, there’s strong evidence behind the mental health benefits of birding. Studies show that birdsong can help </span><a href="https://www.nature.com/articles/s41598-022-20841-0" rel="">alleviate anxiety and paranoia</a><span>, while a mere 10% increase in bird species in one’s vicinity increased participants’ life satisfaction the same as a </span><a href="https://www.sciencedaily.com/releases/2020/12/201204110246.htm" rel="">10% increase in income</a><span>. Small wonder that spending time outdoors and connecting to nature can be such a boon for our health — this is one TikTok trend we can get behind.&nbsp;</span><br><strong><span>→ </span><a href="https://www.tiktok.com/tag/birdwatching?lang=en" rel="">Watch more</a></strong></p><p><strong>🍄 COMPASS Pathways, </strong><span>a psychedelic biotech company, has entered a securities purchase agreement of </span><strong>$125M</strong><span>,</span><strong> </strong><span>including a potential additional </span><strong>$160M, </strong><span>with a group of healthcare specialist investors. The agreement was led by </span><strong>TCGX</strong><span> and </span><strong><span>Aisling Capital. </span><br><span>→ </span><a href="https://compasspathways.com/compass-pathways-announces-up-to-285-million-private-placement-financing-joined-by-leading-healthcare-investors-2/" rel="">source</a></strong></p><p><strong>🧠 MindMed, </strong><span>a clinical stage psychedelic company, secured a </span><strong>$50M</strong><span> credit facility with </span><strong><span>K2 HealthVentures.</span><br><span>→ </span><a href="https://www.businesswire.com/news/home/20230814596096/en/MindMed-Secures-50.0-Million-Credit-Facility-with-K2-HealthVentures/?feedref=JjAwJuNHiystnCoBq_hl-bQQCakZDujohEJegUyaJwquReQ0P23MrIoWkrUSV24ZevRMp3sIgu8q3wq1OF24lT93qbEzrwa15HGbLqMObxbNWfZlBntHS6jpH5ROLXsTFMkl05DM8ABqeyBleEmzJA==" rel="">source</a></strong></p><p><strong>🚺 Visana Health, </strong><span>a women’s health platform that includes behavioral health services, raised a $10.1M seed round co-led by </span><strong>Flare Capital Partners </strong><span>and </span><strong>Frist Cressey Ventures</strong><span>. </span><br><span>→ </span><strong><a href="https://www.prnewswire.com/news-releases/visana-health-raises-10-1-million-seed-round-to-bring-comprehensive-virtual-healthcare-to-women-nationwide-301903801.html" rel="">source</a></strong></p><p><strong>👗 Kohl’s</strong><span> donated </span><strong>$6M</strong><span> to the </span><strong>National Alliance on Mental Illness (NAMI)</strong><span> to increase mental health services and resources across the country, with a focus on BIPOC communities.</span><br><span>→ </span><strong><a href="https://www.businesswire.com/news/home/20230815196975/en" rel="">source</a></strong></p><p><strong>✨ Glimmer, </strong><span>a guided therapy platform connecting patients to a higher standard of mental healthcare, launched. </span><br><span>→ </span><strong><a href="https://www.prweb.com/releases/former-gaming-entrepreneur-now-holding-mental-health-to-a-higher-standard-with-launch-of-glimmer-301901071.html" rel="">source</a></strong></p><p><strong>⚡ ARC Health</strong><span>, a group of mental healthcare practices, acquired </span><strong>Dayspring Behavioral Health (DBH)</strong><span>, a practice with four locations throughout the Seattle metroplex.</span><br><span>→ </span><strong><a href="https://www.prnewswire.com/news-releases/dayspring-behavioral-health-joins-arc-health-301901326.html" rel="">source</a></strong></p><p><strong>🌷 Spring Health, </strong><span>an employer mental health platform, launched </span><strong>Sage</strong><span>, a set of self-paced online courses to help managers support employee mental health. </span><br><span>→ </span><strong><a href="https://www.prnewswire.com/news-releases/spring-health-launches-sage-setting-a-new-standard-for-mental-health-education-with-self-directed-clinician-led-courses-for-workplace-wellbeing-301903051.html" rel="">source</a></strong></p><p><strong>📝 Tebra, </strong><span>a practice management tool for independent healthcare providers, launched </span><strong>The Intake</strong><span>, a comprehensive online content hub to help independent healthcare practices thrive. </span><br><span>→ </span><strong><a href="https://www.businesswire.com/news/home/20230809997362/en" rel="">source</a></strong><span> // </span><strong>Re-read </strong><em><strong><a href="https://headlineshq.substack.com/p/issue-no-028-therapy-tech" rel="">Issue No. 028: Therapy Tech</a></strong></em><strong>&nbsp;</strong></p><p><strong>🎮 Healthy Gamer, </strong><span>a mental wellness platform designed for the internet generation, launched </span><strong>HG Institute</strong><span>, providing accredited courses on the latest clinical trends and research to help professionals better address modern mental health stressors.</span><br><span>→ </span><strong><a href="https://hg-institute.com/?utm_medium=email&amp;_hsmi=270401786&amp;_hsenc=p2ANqtz-_LuvJ8w8_2h1pUBiH-omqOobMuIxF9ZKO-P6a6hpNtcDpKNwlP-S3jjnYlPB6ZnvI518lu2iv0_y06JFhquVi3FwTE-g&amp;utm_content=270401786&amp;utm_source=hs_email" rel="">source</a></strong></p><ul><li><p><strong>Addicted? </strong><span>Deep Fix is an incisive, beautifully-written newsletter from Alex “Olo” Olshonsky, somatic coach and co-founder of nonprofit addiction program Natura Care. His essays explore all forms of modern addiction—from drugs to screens—as well as psychedelics, culture, work, and spirituality.&nbsp; → </span><em><a href="https://deepfix.substack.com/" rel="">Deep Fix</a></em></p></li></ul><p>Thank you, as always, for reading. 🫶🏻&nbsp;</p><p>I leave you with two photos from my week: One of the most magnificent sunsets I’ve seen in recent memory, and a snap of my mom looking cute as heck on her birthday. :)</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png" width="1160" height="744" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:744,&quot;width&quot;:1160,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1763565,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F046e3c50-c5c2-4c33-8025-87c1cefbf7a3_1160x744.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Until next Sunday,</span><br><span>-Mel&nbsp;</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Desalination system could produce fresh water that is cheaper than tap water (195 pts)]]></title>
            <link>https://www.eurekalert.org/news-releases/1002811</link>
            <guid>37675831</guid>
            <pubDate>Wed, 27 Sep 2023 15:14:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eurekalert.org/news-releases/1002811">https://www.eurekalert.org/news-releases/1002811</a>, See on <a href="https://news.ycombinator.com/item?id=37675831">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                                                                                                                                                                                        <figure>
                <a href="https://www.eurekalert.org/multimedia/999867">
                  <p><img src="https://earimediaprodweb.azurewebsites.net/Api/v1/Multimedia/cb328de9-26e8-4a03-afaf-b9be19ee8248/Rendition/low-res/Content/Public" alt="Solar desal">
                  </p>
                </a>
                <figcaption>
                  <p><strong>image:&nbsp;MIT researchers have designed a new solar desalination system that takes in saltwater and heats it with natural sunlight. The system flushes out accumulated salt, so replacement parts aren’t needed often, meaning the system could potentially produce drinking water at a rate and price that is cheaper than tap water.</strong>
                  <a href="https://www.eurekalert.org/multimedia/999867">view <span>more&nbsp;<i></i></span></a></p>
                  <p>Credit: Credit: Jintong Gao and Zhenyuan Xu</p>
                </figcaption>
              </figure>
            
                            <p>Engineers at MIT and in China are aiming to turn seawater into drinking water with a completely passive device that is inspired by the ocean, and powered by the sun.&nbsp;</p>

<p>In a paper appearing today in the journal&nbsp;<em>Joule</em>,&nbsp;the team outlines the design for a new solar desalination system that takes in saltwater and heats it with natural sunlight.&nbsp;</p>

<p>The configuration of the device allows water to circulate in swirling eddies, in a manner similar to the much larger “thermohaline” circulation of the ocean. This circulation, combined with the sun’s heat, drives water to evaporate, leaving salt behind. The resulting water vapor can then be condensed and collected as pure, drinkable water. In the meantime, the leftover salt continues to circulate through and out of the device, rather than accumulating and clogging the system.&nbsp;</p>

<p>The new system has a higher water-production rate and a higher salt-rejection rate than all other passive solar desalination concepts currently being tested.</p>

<p>The researchers estimate that if the system is scaled up to the size of a small suitcase, it could produce about 4 to 6 liters of drinking water per hour and last several years before requiring replacement parts. At this scale and performance, the system could produce drinking water at a rate and price that is cheaper than tap water.&nbsp;</p>

<p>“For the first time, it is possible for water, produced by sunlight, to be even cheaper than tap water,” says Lenan Zhang, a research scientist in MIT’s Device Research Laboratory.&nbsp;</p>

<p>The team envisions a scaled-up device could passively produce enough drinking water to meet the daily requirements of a small family. The system could also supply off-grid, coastal communities where seawater is easily accessible.&nbsp;</p>

<p>Zhang’s study co-authors include MIT graduate student Yang Zhong, and Evelyn Wang, the Ford Professor of Engineering, along with Jintong Gao, Jinfang You, Zhanyu Ye, Ruzhu Wang, and Zhenyuan Xu of Shanghai Jiao Tong University in China.</p>

<p><strong>A powerful convection</strong></p>

<p>The team’s new system improves on their&nbsp;<a href="https://link.mediaoutreach.meltwater.com/ls/click?upn=kLuqYYBQiqEU1tC0k1-2Bxu01QDVU-2Bz37CVhW-2F2Vj6SIQCb-2BlML1pO5WQ3FHDyvko8PrAPlxR8rMy6bVZu9syYOLMINfWzfhLlWZb1blPcuVJoCwCLwISrr-2Bp9VqOUOisQ2p_X_rlwtU090MLiXmw82ipgrvtp8SINvdCG-2By7G4BjIVQIj1rlD0F8XV3QCy4z5U0lmMzLyo7mULBFeIAf3XohbPKS3mSkskWJ0wFsWXVkxKzDbZg3JkAa-2FfARTB75XFmby7DdSWl3PreYBv0X2xHdviwPE2MffzaRcFtX5eWiN7EIM4Efa6CDz-2Fx-2Bw3QOYEqGME-2BDj-2BfGhYwEh5jrwWKKGcQPmc0HRAlpPrYJB3AmP2yc3dyJy6gKO9potYNoQojjWYn77fHUsSQygYr7WQBQtgabRK46VIOaE0G1KtGvcF3cRjPGLUdh1npbxJhMrQ2MkJ65PduIaa-2Bdc9pf0QQ9-2Buy4l-2B-2Fp4ql3BrnMjfaVFIAE8UoCngnlrau72Z0iuIZabU">previous design</a>&nbsp;— a similar concept of multiple layers, called stages. Each stage contained an evaporator and a condenser that used heat from the sun to passively separate salt from incoming water. That design, which the team tested on the roof of an MIT building, efficiently converted the sun’s energy to evaporate water, which was then condensed into drinkable water. But the salt that was left over quickly accumulated as crystals that clogged the system after a few days. In a real-world setting, a user would have to place stages on a frequent basis, which would significantly increase the system’s overall cost.</p>

<p>In a follow-up effort, they&nbsp;<a href="https://link.mediaoutreach.meltwater.com/ls/click?upn=kLuqYYBQiqEU1tC0k1-2Bxu01QDVU-2Bz37CVhW-2F2Vj6SITjPGCz8Nn-2BoEfQ321GpY7123w2QTZTaSfXbY69qv7oTX0FOyANEuzf6QshkZZCkXM-3D_7ua_rlwtU090MLiXmw82ipgrvtp8SINvdCG-2By7G4BjIVQIj1rlD0F8XV3QCy4z5U0lmMzLyo7mULBFeIAf3XohbPKS3mSkskWJ0wFsWXVkxKzDbZg3JkAa-2FfARTB75XFmby7DdSWl3PreYBv0X2xHdviwPE2MffzaRcFtX5eWiN7EIM4Efa6CDz-2Fx-2Bw3QOYEqGME-2BDj-2BfGhYwEh5jrwWKKGcQHzr4YgAwVv70UUoaMz95RK0zoowZMd4lCksoplDlUj1msNDwmHY4GuiGfwyh1skTtOLCY4bVIE14OdOMBOcVajybN8FgdblJeHqg24KxaOX2UDGzMlOO-2Fp-2BNomVxlxqxOajiSuo-2BvBqOVEitvbTSCVs0nM3chNWqp9JSPuWhsFZ">devised a solution</a>&nbsp;with a similar layered configuration, this time with an added feature that helped to circulate the incoming water as well as any leftover salt. While this design prevented salt from settling and accumulating on the device, it desalinated water at a relatively low rate.&nbsp;</p>

<p>In the latest iteration, the team believes it has landed on a design that achieves both a high water-production rate, and high salt rejection, meaning that the system can quickly and reliably produce drinking water for an extended period. The key to their new design is a combination of their two previous concepts: a multistage system of evaporators and condensers, that is also configured to boost the circulation of water — and salt — within each stage.&nbsp;</p>

<p>“We introduce now an even more powerful convection, that is similar to what we typically see in the ocean, at kilometer-long scales,” Xu says.&nbsp;</p>

<p>The small circulations generated in the team’s new system is similar to the “thermohaline” convection in the ocean — a phenomenon that drives the movement of water around the world, based on differences in sea temperature (“thermo”) and salinity (“haline”).&nbsp;</p>

<p>“When seawater is exposed to air,&nbsp;sunlight drives water to evaporate. Once water leaves the surface, salt remains. And the higher the salt concentration, the denser the liquid, and this heavier water wants to flow downward,” Zhang explains. “By mimicking this kilometer-wide phenomena in small box, we can take advantage of this feature to reject salt.”</p>

<p><strong>Tapping out</strong></p>

<p>The heart of the team’s new design is a single stage that resembles a thin box, topped with a dark material that efficiently absorbs the heat of the sun. Inside, the box is separated into a top and bottom section. Water can flow through the top half, where the ceiling is lined with an evaporator layer that uses the sun’s heat to warm up and evaporate any water in direct contact. The water vapor is then funneled to the bottom half of the box, where a condensing layer air-cools the vapor into salt-free, drinkable liquid. The researchers set the entire box at a tilt within a larger, empty vessel, then attached a tube from the top half of the box down through the bottom of the vessel, and floated the vessel in saltwater.&nbsp;</p>

<p>In this configuration, water can naturally push up through the tube and into the box, where the tilt of the box, combined with the thermal energy from the sun, induces the water to swirl as it flows through. The small eddies help to bring water in contact with the upper evaporating layer while keeping salt circulating, rather than settling and clogging.&nbsp;</p>

<p>The team built several prototypes, with one, three, and 10 stages, and tested their performance in water of varying salinity, including natural seawater and water that was seven times saltier.&nbsp;</p>

<p>From these tests, the researchers calculated that if each stage were scaled up to a square meter, it would produce up to 5 liters of drinking water per hour, and that the system could desalinate water without accumulating salt for several years. Given this extended lifetime, and the fact that the system is entirely passive, requiring no electricity to run, the team estimates that the overall cost of running the system would be cheaper than what it costs to produce tap water in the United States.&nbsp;</p>

<p>“We show that this device is capable of achieving a long lifetime,” Zhong says. “That means that, for the first time, it is possible for drinking water produced by sunlight to be cheaper than tap water. This opens up the possibility for solar desalination to address real-world problems.”&nbsp;</p>

<p>Funding for the research at Shanghai Jiao Tong University was supported by the Natural Science Foundation of China.</p>

<p>###</p>

<p><em>Written by Jennifer Chu, MIT News</em></p>

            
                        <hr>
            <hr>
            <div>
										                                            
                                                                
                                                                                                        <div>
                            <h4>Article Title</h4>
                            <p>Extreme salt-resisting multistage solar distillation with thermohaline convection</p>
                        </div>
                                                                <div>
                            <h4>Article Publication Date</h4>
                            <p>27-Sep-2023</p>
                        </div>
                                                            					                </div>
                    </div><p><strong>Disclaimer:</strong> AAAS and EurekAlert! are not responsible for the accuracy of news releases posted to EurekAlert! by contributing institutions or for the use of any information through the EurekAlert system.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral 7B, the most powerful language model for its size to date, Apache 2.0 (798 pts)]]></title>
            <link>https://mistral.ai/news/announcing-mistral-7b/</link>
            <guid>37675496</guid>
            <pubDate>Wed, 27 Sep 2023 14:52:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/announcing-mistral-7b/">https://mistral.ai/news/announcing-mistral-7b/</a>, See on <a href="https://news.ycombinator.com/item?id=37675496">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Mistral AI team is proud to release Mistral 7B, the most powerful language model for its size to date.</p><h2 id="mistral-7b-in-short">Mistral 7B in short</h2><p>Mistral 7B is a 7.3B parameter model that:</p><ul><li>Outperforms Llama 2 13B on all benchmarks</li><li>Outperforms Llama 1 34B on many benchmarks</li><li>Approaches CodeLlama 7B performance on code, while remaining good at English tasks</li><li>Uses Grouped-query attention (GQA) for faster inference</li><li>Uses Sliding Window Attention (SWA) to handle longer sequences at smaller cost</li></ul><p>We’re releasing Mistral 7B under the Apache 2.0 license, it can be used without restrictions.</p><ul><li><a href="https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-v0.1.tar">Download it</a> and use it anywhere (including locally) with <a href="https://github.com/mistralai/mistral-src">our reference implementation</a></li><li>Deploy it on any cloud (AWS/GCP/Azure), using vLLM <a href="https://docs.mistral.ai/cloud-deployment/skypilot">inference server and skypilot</a></li><li>Use it on <a href="https://huggingface.co/mistralai">HuggingFace</a></li></ul><p>Mistral 7B is easy to fine-tune on any task. As a demonstration, we’re providing a model fine-tuned for chat, which outperforms Llama 2 13B chat.</p><h3 id="performance-in-details">Performance in details</h3><p>We compared Mistral 7B to the Llama 2 family, and re-run all model evaluations ourselves for fair comparison.</p><p><img src="https://mistral.ai/images/news/announcing-mistral-7b/230927_bars.png" alt="histograms">
<em>Performance of Mistral 7B and different Llama models on a wide range of benchmarks. For all metrics, all models were re-evaluated with our evaluation pipeline for accurate comparison. Mistral 7B significantly outperforms Llama 2 13B on all metrics, and is on par with Llama 34B (since Llama 2 34B was not released, we report results on Llama 34B). It is also vastly superior in code and reasoning benchmarks.</em></p><p>The benchmarks are categorized by their themes:</p><ul><li>Commonsense Reasoning: 0-shot average of Hellaswag, Winogrande, PIQA, SIQA, OpenbookQA, ARC-Easy, ARC-Challenge, and CommonsenseQA.</li><li>World Knowledge: 5-shot average of NaturalQuestions and TriviaQA.</li><li>Reading Comprehension: 0-shot average of BoolQ and QuAC.</li><li>Math: Average of 8-shot GSM8K with maj@8 and 4-shot MATH with maj@4</li><li>Code: Average of 0-shot Humaneval and 3-shot MBPP</li><li>Popular aggregated results: 5-shot MMLU, 3-shot BBH, and 3-5-shot AGI Eval (English multiple-choice questions only)</li></ul><p><img src="https://mistral.ai/images/news/announcing-mistral-7b/230927_table.png" alt="table"></p><p>An interesting metric to compare how models fare in the cost/performance plane is to compute “equivalent model sizes”. On reasoning, comprehension and STEM reasoning (MMLU), Mistral 7B performs equivalently to a Llama 2 that would be more than 3x its size. This is as much saved in memory and gained in throughput.
<img src="https://mistral.ai/images/news/announcing-mistral-7b/230927_effective_sizes.png" alt="effective_sizes">
<em>Results on MMLU, Commonsense Reasoning, World Knowledge and Reading comprehension for Mistral 7B and Llama 2 (7B/13/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except on knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which restricts the amount of knowledge it can compress).</em></p><p><strong>Note</strong>: Important differences between our evaluation and the LLaMA2 paper’s:</p><ul><li>For MBPP, we use the hand-verified subset</li><li>For TriviaQA, we do not provide Wikipedia contexts</li></ul><h3 id="flash-and-furious-attention-drift">Flash and Furious: Attention drift</h3><p>Mistral 7B uses a sliding window attention (SWA) mechanism (<a href="https://arxiv.org/pdf/1904.10509.pdf">Child et al.</a>, <a href="https://arxiv.org/pdf/2004.05150v2.pdf">Beltagy et al.</a>), in which each layer attends to the previous <code>4,096</code> hidden states.
The main improvement, and reason for which this was initially investigated, is a linear compute cost of O(sliding_window.seq_len). In practice, changes made to <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention</a> and <a href="https://facebookresearch.github.io/xformers">xFormers</a> yield a 2x speed improvement for sequence length of 16k with a window of 4k. A huge thanks to Tri Dao and Daniel Haziza for helping include these changes on a tight schedule.</p><p>Sliding window attention exploits the stacked layers of a transformer to attend in the past beyond the window size: A token <code>i</code> at layer <code>k</code> attends to tokens <code>[i-sliding_window, i]</code> at layer <code>k-1</code>. These tokens attended to tokens <code>[i-2*sliding_window, i]</code>. Higher layers have access to informations further in the past than what the attention patterns seems to entail.</p><p><img src="https://mistral.ai/images/news/announcing-mistral-7b/attention_local.png" alt="Local attention"></p><p>Finally, a fixed attention span means we can limit our cache to a size of <code>sliding_window</code> tokens, using rotating buffers (read more in our <a href="https://github.com/mistralai/mistral-src">reference implementation repo</a>). This saves half of the cache memory for inference on sequence length of <code>8192</code>, without impacting model quality.</p><h2 id="acknowledgements">Acknowledgements</h2><p>We are grateful to CoreWeave for their 24/7 help in marshalling our cluster. We thank the <a href="https://www.cineca.it/">CINECA/EuroHPC</a> team, and in particular the operators of Leonardo, for their resources and help. We thank the maintainers of <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention</a>, <a href="https://github.com/vllm-project/vllm">vLLM</a>, <a href="https://github.com/facebookresearch/xformers">xFormers</a>, <a href="https://github.com/skypilot-org/skypilot">SkyPilot</a>, <a href="https://github.com/huggingface/text-generation-inference">TGI</a> for their precious assistance in implementing new features and integrating their solutions into ours. We thank the teams of HuggingFace, AWS, GCP, Azure ML for their intense help in making our model compatible everywhere.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unions Work (153 pts)]]></title>
            <link>https://werd.io/2023/unions-work</link>
            <guid>37675422</guid>
            <pubDate>Wed, 27 Sep 2023 14:46:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://werd.io/2023/unions-work">https://werd.io/2023/unions-work</a>, See on <a href="https://news.ycombinator.com/item?id=37675422">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[ClickHouse Keeper: A ZooKeeper alternative written in C++ (200 pts)]]></title>
            <link>https://clickhouse.com/blog/clickhouse-keeper-a-zookeeper-alternative-written-in-cpp</link>
            <guid>37674967</guid>
            <pubDate>Wed, 27 Sep 2023 14:14:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clickhouse.com/blog/clickhouse-keeper-a-zookeeper-alternative-written-in-cpp">https://clickhouse.com/blog/clickhouse-keeper-a-zookeeper-alternative-written-in-cpp</a>, See on <a href="https://news.ycombinator.com/item?id=37674967">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>ClickHouse is the fastest and most resource-efficient open-source database for real-time applications and analytics. As one of its components, ClickHouse Keeper is a fast, more resource-efficient, and feature-rich alternative to ZooKeeper. This open-source component provides a highly reliable metadata store, as well as coordination and synchronization mechanisms. It was originally developed for use with ClickHouse when it is deployed as a distributed system in a self-managed setup or a hosted offering like CloudHouse Cloud. However, we believe that the broader community can benefit from this project in additional use cases.</p>
<p>In this post, we describe the motivation, advantages, and development of ClickHouse Keeper and preview our next planned improvements. Moreover, we introduce a reusable benchmark suite, which allows us to simulate and benchmark typical ClickHouse Keeper usage patterns easily. Based on this, we present benchmark results highlighting that ClickHouse Keeper uses <strong>up to 46 times less memory than ZooKeeper ​​for the same volume of data while maintaining performance close to ZooKeeper</strong>.</p>

<p>Modern <a href="https://en.wikipedia.org/wiki/Distributed_computing">distributed systems</a> require a shared and reliable <a href="https://en.wikipedia.org/wiki/Information_repository">information repository</a> and <a href="https://en.wikipedia.org/wiki/Consensus_(computer_science)">consensus</a> system for coordinating and synchronizing distributed operations. For ClickHouse, <a href="https://zookeeper.apache.org/">ZooKeeper</a> was initially chosen for this. It was reliable through its wide usage, provided a simple and powerful API, and offered reasonable performance.</p>
<p>However, not only performance but also resource efficiency and scalability have always been a top <a href="https://clickhouse.com/docs/en/faq/general/why-clickhouse-is-so-fast">priority</a> for ClickHouse. ZooKeeper, being a Java ecosystem project, did not fit into our primarily C++ codebase very elegantly, and as we used it at a higher and higher scale, we started running into resource usage and operational challenges. In order to overcome these shortcomings of ZooKeeper, we built ClickHouse Keeper from scratch, taking into account additional requirements and goals our project needed to address.</p>
<p>ClickHouse Keeper is a drop-in replacement for ZooKeeper, with a fully compatible client protocol and the same data model. Beyond that, it offers the following benefits:</p>
<ul>
<li>Easier setup and operation: ClickHouse Keeper is implemented in C++ instead of Java and, therefore, <a href="https://clickhouse.com/company/events/scaling-clickhouse?utm_source=google.com&amp;utm_medium=paid_search&amp;utm_campaign=19979782024_153259814612&amp;utm_content=655879611258&amp;utm_term=clickhouse_g_c&amp;gclid=Cj0KCQjwuZGnBhD1ARIsACxbAVgLga7td3T2ccBaJ9zCxt4t_A2RQT_5MdK-qqnLVvp0ufgElNk5JSoaAsMtEALw_wcB">can</a> run embedded in ClickHouse or standalone</li>
<li>Snapshots and logs consume much less disk space due to better compression</li>
<li>No limit on the default packet and node data size (it <a href="https://zookeeper.apache.org/doc/r3.4.11/zookeeperAdmin.html#Unsafe+Options">is</a> 1 MB in ZooKeeper)</li>
<li>No <a href="https://issues.apache.org/jira/browse/ZOOKEEPER-1277">ZXID overflow</a> issue (it forces a restart for every 2B transactions in ZooKeeper)</li>
<li>Faster recovery after network partitions due to the use of a better-distributed consensus protocol</li>
<li>Additional <a href="https://en.wikipedia.org/wiki/Consistency_model">consistency</a> guarantees: ClickHouse Keeper provides the same consistency guarantees as ZooKeeper - <a href="https://en.wikipedia.org/wiki/Linearizability">linearizable</a> writes plus strict ordering of operations inside the same <a href="https://zookeeper.apache.org/doc/r3.5.10/zookeeperProgrammers.html#ch_zkSessions">session</a>. Additionally, and optionally (via a <a href="https://clickhouse.com/docs/en/guides/sre/keeper/clickhouse-keeper#configuration">quorum_reads</a> setting), ClickHouse Keeper provides linearizable reads.</li>
<li>ClickHouse Keeper is more resource efficient and uses less memory for the same volume of data (we will demonstrate this later in this blog)</li>
</ul>
<p>The development of ClickHouse Keeper <a href="https://github.com/ClickHouse/ClickHouse/pull/19580">started</a> as an embedded service in the ClickHouse server in February 2021. In the same year, a standalone mode was <a href="https://github.com/ClickHouse/ClickHouse/pull/24059">introduced</a>, and <a href="https://jepsen.io/">Jepsen</a> tests were <a href="https://github.com/ClickHouse/ClickHouse/pull/21677">added</a> - every 6 hours, we run automated <a href="https://github.com/ClickHouse/ClickHouse/tree/master/tests/jepsen.clickhouse">tests</a> with several different workflows and failure scenarios to validate the correctness of the consensus mechanism.</p>
<p>At the time of writing this blog, ClickHouse Keeper has been production-ready for <a href="https://clickhouse.com/blog/clickhouse-22-3-lts-released#clickhouse-keeper">more</a> than one and a half years and has been deployed at scale in our own <a href="https://clickhouse.com/cloud">ClickHouse Cloud</a> since its first private preview launch in May 2022.</p>
<p>In the rest of the blog, we sometimes refer to ClickHouse Keeper as simply “Keeper,” as we often call it internally.</p>

<p>Generally, anything requiring consistency between multiple ClickHouse servers relies on Keeper:</p>
<ul>
<li>Keeper provides the coordination system for data <a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication">replication</a> in self-managed <a href="https://en.wikipedia.org/wiki/Shared-nothing_architecture">shared-nothing</a> ClickHouse <a href="https://clickhouse.com/company/events/scaling-clickhouse">clusters</a></li>
<li>Automatic <a href="https://clickhouse.com/blog/common-getting-started-issues-with-clickhouse#5-deduplication-at-insert-time">insert deduplication</a> for replicated tables of the <a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family">mergetree</a> engine family is based on block-hash-sums <a href="https://clickhouse.com/docs/en/operations/settings/merge-tree-settings#replicated-deduplication-window">stored</a> in Keeper</li>
<li>Keeper provides consensus for <a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree#mergetree-data-storage">part</a> names (based on sequential <a href="https://clickhouse.com/docs/en/development/architecture#block">block</a> numbers) and for assigning part <a href="https://clickhouse.com/blog/asynchronous-data-inserts-in-clickhouse#data-needs-to-be-batched-for-optimal-performance">merges</a> and <a href="https://clickhouse.com/docs/en/sql-reference/statements/alter#mutations">mutations</a> to specific cluster nodes</li>
<li>Keeper is used under the hood of the <a href="https://clickhouse.com/docs/en/engines/table-engines/special/keeper-map">KeeperMap table engine</a> which allows you to use Keeper as consistent key-value store with linearizable writes and sequentially consistent reads
<ul>
<li><a href="https://clickhouse.com/blog/building-real-time-applications-with-clickhouse-and-hex-notebook-keeper-engine">read</a> about an application utilizing this for implementing a task scheduling queue on top of ClickHouse</li>
<li><a href="https://github.com/ClickHouse/clickhouse-kafka-connect">Kafka Connect Sink</a> uses this table engine as a reliable <a href="https://github.com/ClickHouse/clickhouse-kafka-connect/blob/main/docs/DESIGN.md#storing-state">state store</a> for <a href="https://github.com/ClickHouse/clickhouse-kafka-connect/blob/main/docs/DESIGN.md#state-machine">implementing</a> exactly-once delivery <a href="https://github.com/ClickHouse/clickhouse-kafka-connect/blob/main/docs/DESIGN.md#addressing-exactly-once">guarantees</a></li>
</ul>
</li>
<li>Keeper <a href="https://clickhouse.com/blog/clickhouse-release-23-08#streaming-consumption-from-s3-sergei-katkovskiy-kseniia-sumarokova">keeps track</a> of consumed files in the <a href="https://clickhouse.com/docs/en/engines/table-engines/integrations/s3queue">S3Queue table engine</a></li>
<li><a href="https://clickhouse.com/docs/en/engines/database-engines/replicated">Replicated Database engine</a> stores all metadata in Keeper</li>
<li>Keeper is used for coordinating <a href="https://clickhouse.com/docs/en/operations/backup">Backups</a> with the <a href="https://clickhouse.com/docs/en/sql-reference/distributed-ddl">ON CLUSTER</a> clause</li>
<li><a href="https://clickhouse.com/docs/en/sql-reference/functions/udf">User defined functions</a> can be <a href="https://github.com/ClickHouse/ClickHouse/pull/46085">stored</a> in Keeper</li>
<li><a href="https://clickhouse.com/docs/en/operations/access-rights">Access control</a> information can be <a href="https://github.com/ClickHouse/ClickHouse/pull/27426">stored</a> in Keeper</li>
<li>Keeper is <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates">used</a> as a shared central store for all metadata in <a href="https://clickhouse.com/cloud">ClickHouse Cloud</a></li>
</ul>

<p>In the following sections, in order to observe (and later model in a benchmark) some of ClickHouse Cloud’s interaction with Keeper, we load a month of data from the <a href="https://clickhouse.com/docs/en/getting-started/example-datasets/wikistat">WikiStat</a> data set into a <a href="https://gist.github.com/tom-clickhouse/7c88c3a231c602b44382f2ffdf98148c">table</a> in a <a href="https://clickhouse.com/docs/en/cloud-quick-start">ClickHouse Cloud service</a> with 3 nodes. Each node has 30 CPU cores and 120 GB RAM. Each service uses its own dedicated ClickHouse Keeper service consisting of 3 servers, with 3 CPU cores and 2 GB RAM per Keeper server.</p>
<p>The following diagram illustrates this data-loading scenario:
<img src="https://clickhouse.com/uploads/Keeper_01_a59945bd61.png" alt="Keeper-01.png" node="[object Object]"></p>

<p>Via a data load <a href="https://gist.github.com/tom-clickhouse/0c1b4d70c4fbebd7a14eb756d1ebc914">query</a>, we load ~4.64 billion rows from ~740 compressed files (one file represents one specific hour of one specific day) in <a href="https://clickhouse.com/docs/en/sql-reference/table-functions/s3Cluster">parallel</a> with all three ClickHouse servers in ~ 100 seconds. The peak main memory usage on a single ClickHouse server was ~107 GB:</p>
<pre><code>0 rows in set. Elapsed: 101.208 sec. Processed 4.64 billion rows, 40.58 GB (45.86 million rows/s., 400.93 MB/s.)
Peak memory usage: 107.75 GiB.
</code></pre>

<p>For storing the data, the 3 ClickHouse servers together <a href="https://gist.github.com/tom-clickhouse/6a5ee7bff4ee7e724d0e2c326ab30354">created</a> 240 initial <a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree#mergetree-data-storage">parts</a> in <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates#clickhouse-cloud-enters-the-stage">object storage</a>. The average number of rows per initial part was ~19 million rows, respectively. The average size was ~100 MiB, and the total amount of inserted rows is 4.64 billion:</p>
<pre><code>┌─parts──┬─rows_avg──────┬─size_avg───┬─rows_total───┐
│ 240.00 │ 19.34 million │ 108.89 MiB │ 4.64 billion │
└────────┴───────────────┴────────────┴──────────────┘
</code></pre>
<p>Because our data load query utilizes the <a href="https://clickhouse.com/docs/en/sql-reference/table-functions/s3Cluster">s3Cluster</a> table function, the creation of the initial parts <a href="https://gist.github.com/tom-clickhouse/f9c683945ea805062f7f5f63bf8b1389">is</a> evenly distributed over the 3 ClickHouse servers of our ClickHouse Cloud services:</p>
<pre><code>┌─n─┬─parts─┬─rows_total───┐
│ 1 │ 86.00 │ 1.61 billion │
│ 2 │ 76.00 │ 1.52 billion │
│ 3 │ 78.00 │ 1.51 billion │
└───┴───────┴──────────────┘
</code></pre>

<p>During the data loading, in the background, ClickHouse <a href="https://gist.github.com/tom-clickhouse/05f40f98dbcc6b28be6de3f96668f37b">executed</a> 1706 part <a href="https://clickhouse.com/blog/asynchronous-data-inserts-in-clickhouse#data-needs-to-be-batched-for-optimal-performance">merges</a>, respectively:</p>
<pre><code>┌─merges─┐
│   1706 │
└────────┘
</code></pre>

<p>ClickHouse Cloud completely <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates#clickhouse-cloud-enters-the-stage">separates</a> the storage of data and metadata from the servers. All data parts <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates#shared-object-storage-for-data-availability">are</a> stored in shared object storage, and all metadata <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates#sharedmergetree-for-cloud-native-data-processing">is</a> stored in Keeper. When a ClickHouse server has written a new part to object storage (see ② above) or merged some parts to a new larger part (see ③ above), then this ClickHouse server is using a <a href="https://zookeeper.apache.org/doc/r3.4.3/api/org/apache/zookeeper/ZooKeeper.html#multi(java.lang.Iterable)">multi</a>-write transaction request for updating the metadata about the new part in Keeper. This information includes the name of the part, which files belong to the part, and where the blobs corresponding to files reside in object storage. Each server has a local cache with subsets of the metadata and <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates#sharedmergetree-for-cloud-native-data-processing">gets</a> automatically informed about data changes by a Keeper instance through a <a href="https://zookeeper.apache.org/doc/current/zookeeperProgrammers.html#ch_zkWatches">watch</a>-based subscription mechanism.</p>
<p>For our aforementioned initial part creations and background part merges, a total of ~18k Keeper requests were <a href="https://gist.github.com/tom-clickhouse/da9c0faee5f509fb0fae9c4ee5c4d667">executed</a>. This includes ~12k multi-write transaction requests (containing only write-subrequests). All other requests are a mix of read and write requests. Additionally, the ClickHouse servers received ~ 800 watch notifications from Keeper:</p>
<pre><code>total_requests:      17705
multi_requests:      11642
watch_notifications: 822
</code></pre>
<p>We can <a href="https://gist.github.com/tom-clickhouse/36b7e154f47411c5a37c764ae62a3fd8">see</a> how these requests were sent and how the watch notifications got received quite evenly from all three ClickHouse nodes:</p>
<pre><code>┌─n─┬─total_requests─┬─multi_requests─┬─watch_notifications─┐
│ 1 │           5741 │           3671 │                 278 │
│ 2 │           5593 │           3685 │                 269 │
│ 3 │           6371 │           4286 │                 275 │
└───┴────────────────┴────────────────┴─────────────────────┘
</code></pre>
<p>The following two charts visualize these Keeper requests <a href="https://gist.github.com/tom-clickhouse/3e3cafe83ed468b6d312ef5461dc3d03">during</a> the data-loading process:
<img src="https://clickhouse.com/uploads/Keeper_02_0e4fabf14d.png" alt="Keeper-02.png" node="[object Object]">
We can see that ~70% of the Keeper requests are multi-write transactions.</p>
<p>Note that the amount of Keeper requests can vary based on the ClickHouse cluster size, ingest settings, and data size. We briefly demonstrate how these three factors influence the number of generated Keeper requests.</p>

<p>If we load the data with 10 instead of 3 servers in parallel, we ingest the data more than 3 times faster (with the <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates">SharedMergeTree</a>):</p>
<pre><code>0 rows in set. Elapsed: 33.634 sec. Processed 4.64 billion rows, 40.58 GB (138.01 million rows/s., 1.21 GB/s.)
Peak memory usage: 57.09 GiB.
</code></pre>
<p>The higher number of servers generates more than 3 times the amount of Keeper requests:</p>
<pre><code>total_requests:      60925
multi_requests:      41767
watch_notifications: 3468
</code></pre>

<p>For our <a href="https://gist.github.com/tom-clickhouse/0c1b4d70c4fbebd7a14eb756d1ebc914">original</a> data load, run with 3 ClickHouse servers, we configured a max size of ~25 million rows per initial part to speed up ingest speed at the expense of higher memory usage. If, instead, we <a href="https://gist.github.com/tom-clickhouse/d67403a4e1663fcbc7f8d8c97ad8df08">run</a> the same data load with the <a href="https://clickhouse.com/docs/en/operations/settings/settings#settings-max_insert_block_size">default</a> value of ~1 million rows per initial part, then the data load is slower but uses ~9 times less main memory per ClickHouse server:</p>
<pre><code>0 rows in set. Elapsed: 121.421 sec. Processed 4.64 billion rows, 40.58 GB (38.23 million rows/s., 334.19 MB/s.)
Peak memory usage: 12.02 GiB.
</code></pre>
<p>And ~4 thousand instead of 240 initial parts <a href="https://gist.github.com/tom-clickhouse/6a5ee7bff4ee7e724d0e2c326ab30354">are</a> created:</p>
<pre><code>┌─parts─────────┬─rows_avg─────┬─size_avg─┬─rows_total───┐
│ 4.24 thousand │ 1.09 million │ 9.20 MiB │ 4.64 billion │
└───────────────┴──────────────┴──────────┴──────────────┘
</code></pre>
<p>This <a href="https://gist.github.com/tom-clickhouse/05f40f98dbcc6b28be6de3f96668f37b">causes</a> a higher number of part merges:</p>
<pre><code>┌─merges─┐
│   9094 │
└────────┘
</code></pre>
<p>And we <a href="https://gist.github.com/tom-clickhouse/da9c0faee5f509fb0fae9c4ee5c4d667">get</a> a higher number of Keeper requests (~147k instead of ~17k):</p>
<pre><code>total_requests:      147540
multi_requests:      105951
watch_notifications: 7439
</code></pre>

<p>Similarly, if we <a href="https://gist.github.com/tom-clickhouse/4431d998f24304917976e118c2e95b88">load</a> more data (with the default value of ~1 million rows per initial part), e.g. six months from the WikiStat data set, then we get a higher amount of ~24 thousand initial parts for our service:</p>
<pre><code>┌─parts──────────┬─rows_avg─────┬─size_avg─┬─rows_total────┐
│ 23.75 thousand │ 1.10 million │ 9.24 MiB │ 26.23 billion │
└────────────────┴──────────────┴──────────┴───────────────┘
</code></pre>
<p>Which <a href="https://gist.github.com/tom-clickhouse/05f40f98dbcc6b28be6de3f96668f37b">causes</a> more merges:</p>
<pre><code>┌─merges─┐
│  28959 │
└────────┘
</code></pre>
<p><a href="https://gist.github.com/tom-clickhouse/da9c0faee5f509fb0fae9c4ee5c4d667">Resulting</a> in ~680k Keeper requests:</p>
<pre><code>total_requests:      680996
multi_requests:      474093
watch_notifications: 32779
</code></pre>

<p>We developed a benchmark suite coined <a href="https://github.com/ClickHouse/examples/tree/main/keeper-bench-suite">keeper-bench-suite</a> for benchmarking the typical ClickHouse interactions with Keeper explored above. For this, keeper-bench-suite allows simulating the parallel Keeper workload from a ClickHouse cluster consisting of <code>N</code> (e.g. 3) servers:
<img src="https://clickhouse.com/uploads/Keeper_03_53c9cae6a1.png" alt="Keeper-03.png" node="[object Object]">
We are piggybacking on <a href="https://github.com/ClickHouse/ClickHouse/tree/master/utils/keeper-bench">keeper-bench</a>, a tool for benchmarking Keeper or any ZooKeeper-compatible system. With that building block, we can simulate and benchmark the typical parallel Keeper traffic from <code>N</code> ClickHouse servers. This diagram shows the complete architecture of the Keeper Bench Suite, which allows us to set up easily and benchmark arbitrary Keeper workload scenarios:
<img src="https://clickhouse.com/uploads/Keeper_04_3b57f3a2c4.png" alt="Keeper-04.png" node="[object Object]">
We are using an AWS <a href="https://aws.amazon.com/pm/ec2/">EC2</a> instance as a benchmark server for executing a <a href="https://github.com/ClickHouse/examples/blob/main/keeper-bench-suite/benchmark.py">Python script</a> which
<code>① sets up and starts a 3-node Keeper <a href="https://zookeeper.apache.org/doc/current/zookeeperStarted.html#sc_RunningReplicatedZooKeeper">cluster</a> by spinning up 3 appropriate (e.g., <a href="https://aws.amazon.com/ec2/instance-types/m6a/">m6a.4xlarge</a>) EC2 instances, each running one Keeper <a href="https://en.wikipedia.org/wiki/Docker_(software)">docker</a> container and two containers with <a href="https://github.com/google/cadvisor">cAdvisor</a> and <a href="https://redis.io/">Redis</a> (required by cAdvisor) for monitoring the resource usage of the local Keeper container<p>
② starts keeper-bench with a preconfigured workload configurations</p><p>
③ scrapes the <a href="https://prometheus.io/">Prometheus</a> endpoints of each cAdvisor and Keeper every 1 second</p><p>
④ writes the scraped metrics including timestamps into two <a href="https://github.com/ClickHouse/examples/tree/main/keeper-bench-suite#getting-started">tables</a> in a ClickHouse Cloud service which is the basis for conveniently analyzing the metrics via SQL queries, and <a href="https://grafana.com/">Grafana</a> dashboards
</p></code></p>
<p>Note that both <a href="https://github.com/ClickHouse/ClickHouse/pull/43087">ClickHouse Keeper</a> and <a href="https://zookeeper.apache.org/doc/r3.8.2/zookeeperMonitor.html">ZooKeeper</a> directly provide Prometheus endpoints. Currently, these endpoints only have a very small overlap and generally give quite different metrics, which makes it hard to compare them, especially in terms of memory and CPU usage. Therefore, we opted for additional cAdvisor-based basic container metrics. Plus, running Keeper in a docker container allows us to easily change the number of CPU cores and size of RAM provided to Keeper.</p>


<p>We run benchmarks with different docker container sizes for both ClickHouse Keeper and ZooKeeper. E.g. 1 CPU core + 1 GB RAM, 3 CPU cores + 1 GB RAM, 6 CPU cores + 6 GB RAM.</p>

<p>For each of the Keeper sizes, we simulate (with the <a href="https://github.com/ClickHouse/ClickHouse/tree/master/utils/keeper-bench#general-settings">concurrency</a> setting of keeper-bench) different numbers of clients (e.g., ClickHouse servers) sending requests in parallel to Keeper:  E.g. 3, 10, 100, 500, 1000.</p>
<p>From each of these simulated clients, to simulate both short and long-running Keeper sessions, we send (with the <a href="https://github.com/ClickHouse/ClickHouse/tree/master/utils/keeper-bench#general-settings">iterations</a> setting of keeper-bench) a total number between 10 thousand and ~10 million requests to Keeper. This aims to test whether memory usage of either component changes over time.</p>

<p>We simulated a typical ClickHouse workload containing ~1/3 write and delete operations and ~2/3 reads. This reflects a scenario where some data is ingested, merged, and then queried. It is easily possible to define and benchmark other workloads.</p>


<p>We use the Prometheus endpoint of cAdvisor for measuring</p>
<ul>
<li>Main memory usage (<a href="https://github.com/google/cadvisor/blob/release-v0.47/docs/storage/prometheus.md?plain=1#L67">container_memory_working_set_bytes</a>)</li>
<li>CPU usage (<a href="https://github.com/google/cadvisor/blob/release-v0.47/docs/storage/prometheus.md?plain=1#L30">container_cpu_usage_seconds_total</a>)</li>
</ul>
<p>We use the Prometheus endpoints of <a href="https://github.com/ClickHouse/ClickHouse/pull/43087">ClickHouse Keeper</a> and <a href="https://zookeeper.apache.org/doc/r3.8.2/zookeeperMonitor.html">ZooKeeper</a> for measuring additional (all available) Keeper Prometheus endpoint metric values. E.g. for ZooKeeper, many JVM-specific metrics (heap size and usage, garbage collection, etc.).</p>

<p>We also measure the runtime for Keeper processing all requests based on the minimum and maximum timestamps from each run.</p>

<p>We used the keeper-bench-suite to compare the resource consumption and runtime of ClickHouse Keeper and ZooKeeper for our workload. We ran each benchmark configuration 10 times and stored the results in <a href="https://github.com/ClickHouse/examples/tree/main/keeper-bench-suite#getting-started">two tables</a> in a ClickHouse Cloud service. We used a <a href="https://gist.github.com/tom-clickhouse/d156a83202b1b31ab34adc09c9167192">SQL query</a> for generating three tabular result tables:</p>
<ul>
<li><a href="https://gist.github.com/tom-clickhouse/e6edb87becb2b03939db06a0c1b0ff13">mean</a></li>
<li><a href="https://gist.github.com/tom-clickhouse/e3d9cfae1903f2e457131fa5820a08ea">95th percentiles</a></li>
<li><a href="https://gist.github.com/tom-clickhouse/a4c2ffbc85463ac9fac64571599365ae">99th percentiles</a></li>
</ul>
<p>The columns of these results are described <a href="https://gist.github.com/tom-clickhouse/2d3f292ee0aac762251626c7c3156966">here</a>.</p>
<p>We used <code>ClickHouse Keeper 23.5</code> and <code>ZooKeeper 3.8.</code> (with bundled <code>OpenJDK 11</code>) for all runs.
Note that we don’t print the three tabular results here, as each table contains 216 rows. You can inspect the results by following the links above.</p>

<p>Here, we present two charts, where we <a href="https://gist.github.com/tom-clickhouse/0cb1d340efeeea123f592de2e9d6bc3c">filtered</a> the 99th percentile results for rows where both Keeper versions run with 3 CPU cores and 2 GB of RAM, processing the same request sizes sent from 3 simulated clients (ClickHouse servers) in parallel. The tabular result for these visualizations is <a href="https://gist.github.com/tom-clickhouse/f7f165ad612ba81088817226e33a431d">here</a>.</p>

<p><img src="https://clickhouse.com/uploads/Keeper_05_ef049cc5e4.png" alt="Keeper-05.png" node="[object Object]">
We can see that for our simulated workload, ClickHouse Keeper consistently uses a lot less main memory than ZooKeeper for the same number of processed requests. E.g. for the benchmark run ③ processing 6.4 million requests sent by 3 simulated ClickHouse servers in parallel, ClickHouse Keeper uses ~46 times less main memory than ZooKeeper in run ④.</p>
<p>For ZooKeeper, we used a 1GiB JVM heap size configuration (<code>JVMFLAGS: -Xmx1024m -Xms1024m</code>) for all main runs (①, ②, ③), meaning that the committed JVM memory (reserved heap and non-heap memory is guaranteed to be available for use by the Java virtual machine) size is ~1GiB for these runs (see the transparent gray bars in the chart above for how much is used). In addition to the docker container memory usage (blue bars), we also measured the amount of (heap and non-heap) JVM memory actually used within the committed JVM memory (pink bars). There is some slight container memory <a href="https://stackoverflow.com/a/53624438">overhead</a> (difference of blue and pink bars) of running the JVM itself. However, the actual used JVM memory is still consistently significantly larger than the overall container memory usage of ClickHouse Keeper.</p>
<p>Furthermore, we can see that ZooKeeper uses the complete 1 GiB JVM heap size for run ③. We did an additional run ④ with an increased JVM heap size of 2 GiB for ZooKeeper, resulting in ZooKeeper using 1.56 GiB of its 2 GiB JVM heap, with an improved runtime matching the runtime of ClickHouse Keeper’s run ③. We present runtimes for all runs above in the next chart.</p>
<p>We can see in the tabular result that (major) garbage collection takes place a few times during the ZooKeeper runs.</p>

<p>The following chart visualizes runtimes and CPU usages for the runs discussed in the previous chart (the circled numbers are aligned in both charts):
<img src="https://clickhouse.com/uploads/Keeper_06_04cf699a40.png" alt="Keeper-06.png" node="[object Object]">
ClickHouse Keeper’s runtimes closely match ZooKeeper’s runtimes. Despite using significantly less main memory (see the previous chart) and CPU.</p>

<p>We <a href="https://clickhouse.com/blog/clickhouse-keeper-a-zookeeper-alternative-written-in-cpp#observing-keeper">observed</a> that ClickHouse Cloud often uses multi-write transactions in interactions with Keeper. We zoom in a bit deeper into ClickHouse Cloud’s interactions with Keeper to sketch two main scenarios for such Keeper transactions used by ClickHouse servers.</p>

<p><img src="https://clickhouse.com/uploads/Keeper_08_3b3e4bbd75.png" alt="Keeper-08.png" node="[object Object]">
In the scenario sketched above, server-2 ① processes data inserted into a table <a href="https://clickhouse.com/docs/en/development/architecture#block">block</a>-<a href="https://clickhouse.com/docs/en/operations/settings/settings#max_insert_block_size">wise</a>. For the current block, server-2 ② writes the data into a new data part in object storage, and ③ <a href="https://github.com/ClickHouse/ClickHouse/blob/776f232ec0b1b19b91d741f8fb76a437548b86c2/src/Storages/MergeTree/EphemeralLockInZooKeeper.cpp#L56">uses</a> a Keeper multi-write <a href="https://zookeeper.apache.org/doc/r3.4.3/api/org/apache/zookeeper/ZooKeeper.html#multi(java.lang.Iterable)">transaction</a> for storing metadata about the new part in Keeper, e.g., where the blobs corresponding to part files reside in object storage. Before storing this metadata, the transaction first tries to store the hash sum of the block processed in step ① in a <code>deduplication log</code> znode in Keeper. If the same hash sum value <a href="https://github.com/ClickHouse/ClickHouse/blob/776f232ec0b1b19b91d741f8fb76a437548b86c2/src/Storages/MergeTree/EphemeralLockInZooKeeper.cpp#L49">already</a> exists in the deduplication log, then the whole transaction <a href="https://github.com/ClickHouse/ClickHouse/blob/776f232ec0b1b19b91d741f8fb76a437548b86c2/src/Storages/MergeTree/EphemeralLockInZooKeeper.cpp#L61">fails</a> (is rolled back). Additionally, the data part from step ② is deleted because the data contained in that part was already inserted in the past. This automatic insert <a href="https://clickhouse.com/blog/common-getting-started-issues-with-clickhouse#5-deduplication-at-insert-time">deduplication</a> makes ClickHouse inserts <a href="https://clickhouse.com/blog/asynchronous-data-inserts-in-clickhouse#inserts-are-idempotent">idempotent</a> and, therefore, failure-tolerant, allowing clients to retry inserts without risking data duplication. On success, the transaction triggers child <a href="https://zookeeper.apache.org/doc/current/zookeeperProgrammers.html#ch_zkWatches">watches</a>, and ④ all Clickhouse servers <a href="https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates#sharedmergetree-for-cloud-native-data-processing">subscribed</a> to events for the part-metadata znodes are automatically notified by Keeper about new entries. This causes them to fetch metadata updates from Keeper into their local metadata caches.</p>

<p><img src="https://clickhouse.com/uploads/Keeper_09_bef5e28102.png" alt="Keeper-09.png" node="[object Object]">
When server-2 decides to merge some parts into a larger part, then the server ① uses a Keeper transaction for marking the to-be-merged parts as locked (to prevent other servers from merging them). Next, server-2 ② merges the parts into a new larger part, and ③ uses another Keeper transaction for storing metadata about the new part, which triggers watches ④ notifying all other servers about the new metadata entries.</p>
<p>Note that the above scenarios can only work correctly if such Keeper transactions are executed by Keeper atomically and sequentially. Otherwise, two ClickHouse servers sending the same data in parallel at the same time could potentially both not find the data’s hash sum in the deduplication log resulting in data duplication in object storage. Or multiple servers would merge the same parts. To prevent this, the ClickHouse servers rely on Keeper’s all-or-nothing multi-write transactions plus its linearizable writes guarantee.</p>

<p>The <a href="https://betterprogramming.pub/demystifying-consensus-algorithms-and-their-implementations-c52f8aca3020">consensus algorithms</a> in ZooKeeper and ClickHouse Keeper, <a href="https://zookeeper.apache.org/doc/r3.4.13/zookeeperInternals.html#sc_atomicBroadcast">ZAB</a>, and <a href="https://raft.github.io/">Raft</a>, respectively,  both ensure that multiple distributed servers can reliably agree on the same information. e.g. which parts are allowed to be merged in the example above.</p>
<p>ZAB is a dedicated consensus mechanism for ZooKeeper and has been in development <a href="https://dl.acm.org/doi/10.1145/1529974.1529978">since</a> at least 2008.</p>
<p>We chose Raft as our consensus mechanism because of its simple and <a href="https://raft.github.io/raft.pdf">easy-to-understand</a> algorithm and the availability of a lightweight and easy-to-integrate <a href="https://tech.ebayinc.com/engineering/nuraft-a-lightweight-c-raft-core/">C++ library</a> when we started the Keeper project in 2021.</p>
<p>However, all consensus algorithms are isomorphic to each other. For <a href="https://en.wikipedia.org/wiki/Linearizability">linearizable</a> writes, (dependent) transitions and the write operations within the transaction must be processed in strict order, one at a time, regardless of which consensus algorithm is used. Suppose ClickHouse servers are sending transactions in parallel to Keeper, and these transactions are dependent because they write to the same znodes (e.g., the <code>deduplication log</code> in our example scenario at the beginning of this section). In that case, Keeper can guarantee and implement linearizability only by executing such transactions and their operations strictly sequentially:
<img src="https://clickhouse.com/uploads/Keeper_10_aa840f2251.png" alt="Keeper-10.png" node="[object Object]">
For this, ZooKeeper implements write request processing using a single-threaded <a href="https://github.com/apache/zookeeper/blob/master/zookeeper-server/src/main/java/org/apache/zookeeper/server/SyncRequestProcessor.java">request processor</a>, whereas Keeper’s NuRaft implementation uses a single-threaded <a href="https://github.com/eBay/NuRaft/blob/v2.0/src/global_mgr.cxx#L252">global queue</a>.</p>
<p>Generally, linearizability makes it hard to scale write processing speed vertically (more CPU cores) or horizontally (more servers). It would be possible to analyze and identify independent transactions and run them in parallel, but currently, neither ZooKeeper nor ClickHouse Keeper implements this. This chart (where we filtered the 99th percentile results) highlights this:
<img src="https://clickhouse.com/uploads/Keeper_07_2f3bf2e688.png" alt="Keeper-07.png" node="[object Object]">
Both ZooKeeper and ClickHouse Keeper are running with 1, 3, and 6 CPU cores and processing 1.28 million total requests sent in parallel from 500 clients.</p>
<p>The performance of (non-linearizable) read requests and auxiliary tasks (managing network requests, batching data, etc.) can be scaled theoretically with the number of CPU cores with both ZAB and Raft. Our benchmark results generally show that ZooKeeper is currently doing this better than Clickhouse Keeper, although we are consistently improving our performance (<a href="https://github.com/ClickHouse/ClickHouse/pull/43686">three</a> <a href="https://github.com/ClickHouse/ClickHouse/pull/47978">recent</a> <a href="https://github.com/ClickHouse/ClickHouse/pull/53049">examples</a>).</p>

<p>Looking forward, we see the need to extend Keeper to better support the scenarios we described above. So, we are taking a big step with this project – introducing a multi-group Raft protocol for Keeper.</p>
<p>Because, as explained above, scaling non-partitioned (non-sharded) linearizability is impossible, we will focus on <a href="https://github.com/ClickHouse/ClickHouse/issues/54172">Multi-group Raft</a> where we <a href="https://tikv.org/deep-dive/scalability/multi-raft/">partition</a> the data stored in Keeper. This allows more transactions to be independent (working over separate partitions) from each other. By using a separate Raft instance inside the same server for each partition, Keeper automatically executes independent transactions in parallel:
<img src="https://clickhouse.com/uploads/Keeper_11_72b63a9e55.png" alt="Keeper-11.png" node="[object Object]">
With multi-Raft, Keeper will be able to enable workloads with much higher parallel read/write requirements, such as for instance, very large ClickHouse clusters with 100s of nodes.</p>

<p>Sounds exciting? Then, we invite you to join the Keeper community.</p>
<ul>
<li><a href="https://clickhouse.com/docs/en/guides/sre/keeper/clickhouse-keeper">This</a> is how you use Keeper with ClickHouse</li>
<li>To become a user of Keeper outside of ClickHouse - check out <a href="https://clickhouse.com/clickhouse/keeper">this</a> page when to use it or not</li>
<li><a href="https://clickhouse.com/slack">This</a> is where you post questions; you can follow us on <a href="https://twitter.com/ClickHouseDB">X</a> and <a href="https://github.com/ClickHouse/ClickHouse#upcoming-events">join</a> our meetups and events.</li>
</ul>
<p>We welcome contributions to the Keeper codebase. See our roadmap <a href="https://github.com/ClickHouse/ClickHouse/labels/comp-keeper">here</a>, and see our contributor guidelines <a href="https://github.com/ClickHouse/ClickHouse/blob/master/CONTRIBUTING.md">here</a>.</p>

<p>In this blog post, we described the features and advantages of ClickHouse Keeper - a resource-efficient open-source drop-in replacement for ZooKeeper. We explored our own usage of it in ClickHouse Cloud and, based on this, presented a benchmark suite and results highlighting that ClickHouse Keeper consistently uses significantly fewer hardware resources than ZooKeeper with comparable performance numbers. We also shared our roadmap and ways you can get involved. We invite you to collaborate!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deno Queues (322 pts)]]></title>
            <link>https://deno.com/blog/queues</link>
            <guid>37674752</guid>
            <pubDate>Wed, 27 Sep 2023 13:58:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deno.com/blog/queues">https://deno.com/blog/queues</a>, See on <a href="https://news.ycombinator.com/item?id=37674752">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In the ever-evolving world of cloud software, Deno aims to radically simplify.
Leveraging public cloud infrastructure has traditionally demanded sifting
through layers of boilerplate code and intricate configurations, often
monopolizing a significant chunk of the developer’s time and energy. Our goal is
to distill these intricacies into user-friendly primitives, enabling developers
to design, refine, and launch their projects with unmatched speed.</p>
<p>With this in mind, we rolled out <strong><a href="https://deno.com/kv" rel="noopener noreferrer">Deno KV</a></strong> a few months
ago (<strong><a href="https://deno.com/blog/kv-open-beta" rel="noopener noreferrer">currently in open beta</a></strong>). Anchored
on the robust capabilities of FoundationDB, Deno KV is more than just a new
persistence option for apps. It’s about transforming the developer experience by
eliminating redundant configurations and offering a refreshingly streamlined
API.</p>
<p>Building upon this foundation (pun intended), we are elated to unveil <strong>Deno
Queues</strong> today. This tool is set to revolutionize scalable messaging and elevate
the management of background processing in your applications.</p>
<div><pre><span>const</span> db <span>=</span> <span>await</span> <span>Deno</span><span>.</span><span>openKv</span><span>(</span><span>)</span><span>;</span>

db<span>.</span><span>listenQueue</span><span>(</span><span>async</span> <span>(</span>msg<span>)</span> <span>=&gt;</span> <span>{</span>
  <span>await</span> <span>postToSlack</span><span>(</span>msg<span>.</span><span>channel</span><span>,</span> msg<span>.</span><span>text</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span>

<span>await</span> db<span>.</span><span>enqueue</span><span>(</span><span>{</span> channel<span>:</span> <span>"C123456"</span><span>,</span> text<span>:</span> <span>"Slack message"</span> <span>}</span><span>,</span> <span>{</span>
  delay<span>:</span> <span>60000</span><span>,</span>
<span>}</span><span>)</span><span>;</span></pre></div><p>In this post, we’ll cover key aspects of Deno Queues:</p>
<ul>
<li><a href="#what-are-deno-queues">What are Deno Queues?</a></li>
<li><a href="#use-cases-and-examples">Use cases and examples</a></li>
<li><a href="#pricing-for-deno-queues">Pricing</a></li>
<li><a href="#whats-next">What’s next</a></li>
</ul>
<h2 id="what-are-deno-queues">What are Deno Queues?</h2><p>Deno Queues, built on Deno KV, allow you to offload parts of your application or
schedule work for the future to run asynchronously, with two new simple APIs
with zero configuration or infrastructure to maintain:</p>
<ul>
<li><code>.enqueue()</code>: Pushes new messages into the queue for guaranteed delivery
immediately or at a time in the future.</li>
<li><code>.listenQueue()</code>: Handler used for processing new messages from the queue.</li>
</ul>
<p>Since Queues are built on Deno KV, it uses SQLite when running locally and
FoundationDB when running on Deno Deploy for maximum availability and
throughput.</p>
<p>Running Queues on Deno Deploy is optimized for performance. Deno Deploy
automatically spins up V8 isolates on-demand and dispatches messages when
they’re available for processing. Your application code simply listens to new
messages with <code>listenQueue</code> handler, and Deno Deploy handles the rest.</p>
<p><strong>Deno Queues guarantees at-least-once delivery.</strong> For most enqueued messages,
the <code>listenQueue</code> handler will be invoked once. In some failure instances, the
handler may be invoked multiple times to ensure delivery. It’s important to
design your applications to ensure that duplicate messages are handled
correctly.</p>
<p>You can also combine
<a href="https://docs.deno.com/kv/manual/queue_overview#queue-api-with-kv-atomic-transactions" rel="noopener noreferrer">Queues with KV atomic transactions</a>
primitives, which can unlock powerful workflows. For example, you may add
messages to the queue as part of a KV transaction, which succeeds or fails
atomically:</p>
<figure>

<div><pre><span>const</span> kv <span>=</span> <span>await</span> <span>Deno</span><span>.</span><span>openKv</span><span>(</span><span>)</span><span>;</span>
<span>const</span> change <span>=</span> <span>10</span><span>;</span>

<span>const</span> bob <span>=</span> <span>await</span> kv<span>.</span><span>get</span><span>(</span><span>[</span><span>"balance"</span><span>,</span> <span>"bob"</span><span>]</span><span>)</span><span>;</span>
<span>const</span> liz <span>=</span> <span>await</span> kv<span>.</span><span>get</span><span>(</span><span>[</span><span>"balance"</span><span>,</span> <span>"liz"</span><span>]</span><span>)</span><span>;</span>
<span>if</span> <span>(</span>bob<span>.</span><span>value</span> <span>&lt;</span> change<span>)</span> <span>{</span>
  <span>throw</span> <span>"not enough balance"</span><span>;</span>
<span>}</span>

<span>const</span> success <span>=</span> <span>await</span> kv<span>.</span><span>atomic</span><span>(</span><span>)</span>
  <span>.</span><span>check</span><span>(</span>bob<span>,</span> liz<span>)</span> 
  <span>.</span><span>set</span><span>(</span><span>[</span><span>"balance"</span><span>,</span> <span>"bob"</span><span>]</span><span>,</span> bob<span>.</span><span>value</span> <span>-</span> change<span>)</span>
  <span>.</span><span>set</span><span>(</span><span>[</span><span>"balance"</span><span>,</span> <span>"liz"</span><span>]</span><span>,</span> liz<span>.</span><span>value</span> <span>+</span> change<span>)</span>
  
  <span>.</span><span>enqueue</span><span>(</span><span>{</span> type<span>:</span> <span>"notify"</span><span>,</span> name<span>:</span> <span>"liz"</span><span>,</span> amount<span>:</span> change <span>}</span><span>)</span>
  <span>.</span><span>enqueue</span><span>(</span><span>{</span> type<span>:</span> <span>"notify"</span><span>,</span> name<span>:</span> <span>"bob"</span><span>,</span> amount<span>:</span> <span>-</span>change <span>}</span><span>)</span>
  <span>.</span><span>commit</span><span>(</span><span>)</span><span>;</span></pre></div><figcaption>Enqueue new messages as part of an atomic transaction — only if the entire
transaction succeeds, they will be enqueued.</figcaption>

</figure>

<p>You can also update Deno KV state from your <code>listenQueue</code> handler. For instance,
if you want to ensure that updates on each message is performed only once, you
can also use the Queue API with KV atomic transactions:</p>
<figure>

<div><pre><span>const</span> db <span>=</span> <span>await</span> <span>Deno</span><span>.</span><span>openKv</span><span>(</span><span>)</span><span>;</span>

db<span>.</span><span>listenQueue</span><span>(</span><span>async</span> <span>(</span>msg<span>)</span> <span>=&gt;</span> <span>{</span>
  <span>const</span> nonce <span>=</span> <span>await</span> db<span>.</span><span>get</span><span>(</span><span>[</span><span>"nonces"</span><span>,</span> msg<span>.</span><span>nonce</span><span>]</span><span>)</span><span>;</span>
  <span>if</span> <span>(</span>nonce<span>.</span><span>value</span> <span>===</span> <span>null</span><span>)</span> <span>{</span>
    
    <span>return</span><span>;</span>
  <span>}</span>

  <span>const</span> change <span>=</span> msg<span>.</span><span>change</span><span>;</span>
  <span>const</span> bob <span>=</span> <span>await</span> db<span>.</span><span>get</span><span>(</span><span>[</span><span>"balance"</span><span>,</span> <span>"bob"</span><span>]</span><span>)</span><span>;</span>
  <span>const</span> liz <span>=</span> <span>await</span> db<span>.</span><span>get</span><span>(</span><span>[</span><span>"balance"</span><span>,</span> <span>"liz"</span><span>]</span><span>)</span><span>;</span>

  <span>const</span> success <span>=</span> <span>await</span> db<span>.</span><span>atomic</span><span>(</span><span>)</span>
    
    <span>.</span><span>check</span><span>(</span><span>{</span> key<span>:</span> nonce<span>.</span><span>key</span><span>,</span> versionstamp<span>:</span> nonce<span>.</span><span>versionstamp</span> <span>}</span><span>)</span>
    <span>.</span><span>delete</span><span>(</span>nonce<span>.</span><span>key</span><span>)</span>
    <span>.</span><span>sum</span><span>(</span><span>[</span><span>"processed_count"</span><span>]</span><span>,</span> <span>1n</span><span>)</span>
    <span>.</span><span>check</span><span>(</span>bob<span>,</span> liz<span>)</span> 
    <span>.</span><span>set</span><span>(</span><span>[</span><span>"balance"</span><span>,</span> <span>"bob"</span><span>]</span><span>,</span> bob<span>.</span><span>value</span> <span>-</span> change<span>)</span>
    <span>.</span><span>set</span><span>(</span><span>[</span><span>"balance"</span><span>,</span> <span>"liz"</span><span>]</span><span>,</span> liz<span>.</span><span>value</span> <span>+</span> change<span>)</span>
    <span>.</span><span>commit</span><span>(</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span>

<span>const</span> nonce <span>=</span> crypto<span>.</span><span>randomUUID</span><span>(</span><span>)</span><span>;</span>
<span>await</span> db
  <span>.</span><span>atomic</span><span>(</span><span>)</span>
  <span>.</span><span>check</span><span>(</span><span>{</span> key<span>:</span> <span>[</span><span>"nonces"</span><span>,</span> nonce<span>]</span><span>,</span> versionstamp<span>:</span> <span>null</span> <span>}</span><span>)</span>
  <span>.</span><span>enqueue</span><span>(</span><span>{</span> nonce<span>,</span> change<span>:</span> <span>10</span> <span>}</span><span>)</span>
  <span>.</span><span>set</span><span>(</span><span>[</span><span>"nonces"</span><span>,</span> nonce<span>]</span><span>,</span> <span>true</span><span>)</span>
  <span>.</span><span>sum</span><span>(</span><span>[</span><span>"enqueued_count"</span><span>]</span><span>,</span> <span>1n</span><span>)</span>
  <span>.</span><span>commit</span><span>(</span><span>)</span><span>;</span></pre></div><figcaption>This example uses KV atomic transactions to ensure each message is updated only
once.</figcaption>

</figure>

<p>Additionally, if your <code>listenQueue</code> handler throws an exception, the runtime
will automatically retry to call the handler again until it succeeds or until
maximum retry attempts are reached. If maximum attempts (current default is 5)
are reached, the message will be dropped.</p>
<h2 id="use-cases-and-examples">Use cases and examples</h2><p>Queues are useful in scaling applications by allowing servers to offload async
processes and scheduling work for the future.</p>
<p>Below are a few examples.</p>
<h3 id="scheduled-email-notifications">Scheduled email notifications</h3><p>Sometimes a job or task that’s initiated by your user may take enough time where
you don’t want to make them wait for a “task complete” response or there’s no
need to send them a response. This is when you can offload work to a queue to
keep your server or app responsive for your user.</p>
<p>Here’s how you would use Queues to send email notifications:</p>
<div><pre><span>const</span> db <span>=</span> <span>await</span> <span>Deno</span><span>.</span><span>openKv</span><span>(</span><span>)</span><span>;</span>

db<span>.</span><span>listenQueue</span><span>(</span><span>async</span> <span>(</span>msg<span>)</span> <span>=&gt;</span> <span>{</span>
  <span>if</span> <span>(</span>msg<span>.</span><span>type</span> <span>===</span> <span>"welcome_email"</span><span>)</span> <span>{</span>
    <span>await</span> <span>sendWelcomeEmail</span><span>(</span>msg<span>.</span><span>user_id</span><span>)</span><span>;</span>
  <span>}</span> <span>else</span> <span>if</span> <span>(</span>msg<span>.</span><span>type</span> <span>===</span> <span>"survey_email"</span><span>)</span> <span>{</span>
    <span>await</span> <span>sendSurveyEmail</span><span>(</span>msg<span>.</span><span>user_id</span><span>)</span><span>;</span>
  <span>}</span>
<span>}</span><span>)</span><span>;</span>

<span>await</span> db<span>.</span><span>enqueue</span><span>(</span>
  <span>{</span> type<span>:</span> <span>"welcome_email"</span><span>,</span> customer_id<span>:</span> <span>123</span> <span>}</span><span>,</span>
<span>)</span><span>;</span>

<span>await</span> db<span>.</span><span>enqueue</span><span>(</span>
  <span>{</span> type<span>:</span> <span>"survey_email"</span><span>,</span> customer_id<span>:</span> <span>123</span> <span>}</span><span>,</span>
  <span>{</span> delay<span>:</span> <span>259200000</span> <span>}</span><span>,</span> 
<span>)</span><span>;</span></pre></div><h3 id="reliable-webhook-processing">Reliable webhook processing</h3><p>Another extremely common example of using queues on the web is through
processing webhooks. Here’s an example using Oak and Queues to handle webhooks
asynchronously:</p>
<div><pre><span>import</span> <span><span>{</span> <span>Application</span><span>,</span> <span>Router</span> <span>}</span></span> <span>from</span> <span>"https://deno.land/x/oak@v12.6.1/mod.ts"</span><span>;</span>

<span>const</span> db <span>=</span> <span>await</span> <span>Deno</span><span>.</span><span>openKv</span><span>(</span><span>)</span><span>;</span>

db<span>.</span><span>listenQueue</span><span>(</span><span>async</span> <span>(</span>msg<span>)</span> <span>=&gt;</span> <span>{</span>
  <span>await</span> <span>processWebHook</span><span>(</span>msg<span>.</span><span>webhook_body</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span>

<span>const</span> router <span>=</span> <span>new</span> <span>Router</span><span>(</span><span>)</span><span>;</span>
router<span>.</span><span>post</span><span>(</span><span>"/webhook"</span><span>,</span> <span>async</span> <span>(</span>ctx<span>)</span> <span>=&gt;</span> <span>{</span>
  db<span>.</span><span>enqueue</span><span>(</span><span>{</span> webhook_body<span>:</span> <span>await</span> ctx<span>.</span><span>request</span><span>.</span><span>body</span><span>(</span><span>)</span><span>.</span><span>value</span> <span>}</span><span>)</span><span>;</span>
  ctx<span>.</span><span>response</span><span>.</span><span>status</span> <span>=</span> <span>200</span><span>;</span>
<span>}</span><span>)</span><span>;</span>

<span>const</span> app <span>=</span> <span>new</span> <span>Application</span><span>(</span><span>)</span><span>;</span>
app<span>.</span><span>use</span><span>(</span>router<span>.</span><span>routes</span><span>(</span><span>)</span><span>)</span><span>;</span>
app<span>.</span><span>use</span><span>(</span>router<span>.</span><span>allowedMethods</span><span>(</span><span>)</span><span>)</span><span>;</span>

<span>await</span> app<span>.</span><span>listen</span><span>(</span><span>{</span> port<span>:</span> <span>8000</span> <span>}</span><span>)</span><span>;</span></pre></div><h3 id="slack-reminder-bot">Slack Reminder Bot</h3><p>Queues is great for building bots in Discord or Slack.</p>
<p><a href="https://github.com/igorzi/queue-reminder" rel="noopener noreferrer">Here’s an example</a> of using Deno
Queues to create a simple reminder app in Slack.</p>
<p><img src="https://deno.com/blog/queues/slack-reminder-slack-message.png" alt="Receiving a reminder in Slack" title=""></p>
<p>And <a href="https://github.com/Jabolol/kiwi" rel="noopener noreferrer">this is a Discord bot</a> that uses Deno
Queues to create giveaways and allow users to join with a single click.</p>
<p><img src="https://deno.com/blog/queues/discord-giveaway-bot-1.png" alt="Creating a giveaway in Discord" title=""></p>
<h3 id="more-examples">More examples</h3><p>More examples of queue usage can be found at
<a href="https://docs.deno.com/kv/manual/queue_overview#use-cases" rel="noopener noreferrer">docs.deno.com</a>.</p>
<h2 id="pricing-for-deno-queues">Pricing for Deno Queues</h2><p>As you explore the capabilities of Queues, it’s important to grasp the cost
implications. Queues has no specific cost of its own, but rather charged in
terms of Deno KV operations and Deno Deploy requests (for listening).
Specifically:</p>
<p><strong>Enqueuing a Message</strong>: Each enqueue action translates into a KV write
operation.</p>
<p><strong>Receiving a Message</strong>: Every received message entails a KV write, and a single
request charge.</p>
<p>This transparent pricing structure ensures you’re only billed for the operations
you use, aligning with our commitment to efficiency and simplicity.</p>
<h2 id="other-resources">Other resources</h2><ul>
<li><a href="https://docs.deno.com/kv/manual/queue_overview" rel="noopener noreferrer">Queues docs</a></li>
<li><a href="https://docs.deno.com/kv/manual/queue_overview#queues-on-deno-deploy" rel="noopener noreferrer">Queues on Deno Deploy docs</a></li>
<li><a href="https://deno.com/api@v1.37.0?unstable=&amp;s=Deno.Kv&amp;p=prototype.enqueue" rel="noopener noreferrer"><code>.enqueue</code> API reference</a></li>
<li><a href="https://deno.com/api@v1.37.0?unstable=&amp;s=Deno.Kv&amp;p=prototype.listenQueue" rel="noopener noreferrer"><code>.listenQueue</code> API reference</a></li>
</ul>
<h2 id="whats-next">What’s next</h2><p>Building scalable apps and servers on the web requires offloading background
tasks to queues. However, there are many steps in configuring them for use. Deno
Queues, built right into the runtime and on top of robust infrastructure of Deno
Deploy, lets you use serverless, distributed queues in only a few lines of code.</p>
<p>Deno Queues joins <a href="https://deno.com/kv" rel="noopener noreferrer">Deno KV</a>,
<a href="https://deno.land/manual/runtime/web_platform_apis" rel="noopener noreferrer">web standards APIs</a>,
<a href="https://deno.com/blog/npm-on-deno-deploy" rel="noopener noreferrer">npm</a>, and
<a href="https://deno.land/manual/tools" rel="noopener noreferrer">all-in-one modern tooling</a> as key building
blocks that make creating for the web simpler and more productive. We are still
a long ways to go from our goal and have many more exciting features on the
roadmap. Stay tuned.</p>
<p>We’re always open to feedback and feature requests! Feel free to
<a href="https://discord.gg/deno" rel="noopener noreferrer">join our growing Discord</a> or
<a href="https://github.com/denoland/deploy_feedback/issues" rel="noopener noreferrer">create an issue here</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rusty revenant Servo returns to render once more (182 pts)]]></title>
            <link>https://www.theregister.com/2023/09/27/servo_returns/</link>
            <guid>37674519</guid>
            <pubDate>Wed, 27 Sep 2023 13:40:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/09/27/servo_returns/">https://www.theregister.com/2023/09/27/servo_returns/</a>, See on <a href="https://news.ycombinator.com/item?id=37674519">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p><span>Open Source Summit</span> A pleasant surprise from Open Source Summit is that Servo, the Rusty rendering engine that Mozilla was working on – until COVID, that is – is showing green shoots of renewed vigor.</p>
<p>Servo has been around for about a decade, so as experimental software projects go, it's a mature one. Igalia developer Manuel Rego presented a <a target="_blank" href="https://osseu2023.sched.com/event/1OGkc/servo-web-rendering-engine-reboot-manuel-rego-igalia" rel="nofollow">talk</a> which reports that the project is back under active development, almost exactly three years after <a target="_blank" href="https://www.theregister.com/2020/08/14/mozilla_google_search/">Mozilla terminated its Rust efforts</a> and laid off the whole Rust team, including the Servo developers.</p>
<p>In November 2020, <a target="_blank" href="https://www.theregister.com/2020/11/18/firefox_83/">the Linux Foundation adopted Servo</a>. However, the global operation has a lot of <a target="_blank" href="https://www.linuxfoundation.org/projects" rel="nofollow">projects</a> – we think we count 625 of them, but we could be wrong. Early this year, it handed the project over to its <a target="_blank" href="https://www.theregister.com/2022/09/20/linux_foundation_europe/">new European division</a>, which has a slightly more manageable <a target="_blank" href="https://linuxfoundation.eu/en/projects" rel="nofollow">list</a> of four, among them the <a target="_blank" href="https://www.theregister.com/2022/09/16/open_standards_digital_wallets/">OpenWallet foundation</a> and the <a target="_blank" href="https://www.theregister.com/2023/06/01/linux_foundation_risc_v/">RISC-V Software Ecosystem</a>. Now this also includes <a target="_blank" href="https://servo.org/" rel="nofollow">Servo</a>.</p>

    

<p>Servo first appeared <a target="_blank" href="https://www.theregister.com/2013/04/03/samsung_helps_mozilla_with_servo/">in tandem with Rust</a> a full decade ago, and by 2016 Mozilla <a target="_blank" href="https://www.theregister.com/2016/03/15/mozilla_to_release_first_version_of_servobased_browser_in_june/">was discussing releasing a prototype</a>. Previews <a target="_blank" href="https://www.theregister.com/2016/03/15/mozilla_to_release_first_version_of_servobased_browser_in_june/">started to appear that July</a>, when as we put it: "If Google has the language of Go, Moz has the language of No: Rust."</p>

        


        

<p>The new engine is quite capable. It supports the <a target="_blank" href="https://www.theregister.com/2011/03/03/webgl_one_dot_o_released/">now elderly WebGL API</a> as well as its <a target="_blank" href="https://www.theregister.com/2017/02/08/apple_webgpu/">more modern successor WebGPU</a>, which is much more powerful. For now, Rego said, it is mainly aimed at Windows, macOS, and desktop Linux, although the team is also testing mobile versions for both Android and more generic Linux, initially being tested on <a target="_blank" href="https://www.theregister.com/2021/12/08/pinephone_ships_developers/">Pine64's PinePhone Pro hardware</a>.</p>
<p>As well as being independent of any browser vendor, it is designed to be embeddable, memory-safe, modular, and parallel. The latter in particular benefits from the concurrency features provided by Rust. So far this year, the project has seen 1,682 commits from 77 developers, compared to just 523 from 22 people in 2022. A big change has been a new layout engine, replacing what is now called the legacy engine.</p>
<ul>

<li><a href="https://www.theregister.com/2023/09/20/gnu_turns_40/">GNU turns 40: Stallman's baby still not ready for prime time, but hey, there's cake</a></li>

<li><a href="https://www.theregister.com/2023/09/19/ubuntu_2310_taking_shape/">Ubuntu's 'Mantic Minotaur' peeks out of the labyrinth</a></li>

<li><a href="https://www.theregister.com/2023/09/14/pc_xt_with_hdmi/">These days you can teach old tech a bunch of new tricks</a></li>

<li><a href="https://www.theregister.com/2023/09/13/linux_mint_debian_edition_hands_on/">Linux Mint Debian Edition 6 hits beta with reassuringly little drama</a></li>
</ul>
<p>It still can't pass the <a target="_blank" href="https://www.theregister.com/2008/03/05/acid_three_browser_flunk/">Web Standards Project ACID tests</a>, which way back in 2008 WebKit was the <a target="_blank" href="https://www.theregister.com/2008/10/01/webkit_acid/">first browser to successfully handle</a>, so there's clearly some way to go yet. Even so, Rego's <a target="_blank" href="https://static.sched.com/hosted_files/osseu2023/be/2023-09-21-open-source-summit-europe-servo.pdf" rel="nofollow">presentation</a> [PDF] illustrates the improvements it's made so far this year.</p>
<p>At least for now, its goals have been scaled back from being a full web browser in its own right. One of the targets, though, is as a web runtime that can be embedded into standalone local web apps. At the moment, <a target="_blank" href="https://www.electronjs.org/" rel="nofollow">Electron.js</a> is the dominant tool in this space, but it's based on the Chromium engine, and is therefore another cog in the giant Google machine. However, for that to work, it also needs a JavaScript runtime, which is something that Servo doesn't include. For that, it depends on Mozilla's <a target="_blank" href="https://spidermonkey.dev/" rel="nofollow">SpiderMonkey</a>, which is also the basis of <a target="_blank" href="https://www.theregister.com/2022/08/17/gnome_project_25/">the GNOME desktop's GJS</a> and accordingly <a target="_blank" href="https://www.theregister.com/2023/03/02/linux_mint_212/">Cinnamon's CJS too</a>.</p>

        

<p>The SpiderMonkey code base is much less modern than Servo itself: it <a target="_blank" href="https://openhub.net/p/spidermonkey/analyses/latest/languages_summary" rel="nofollow">consists</a> of about one half C++ and one quarter C, plus a sprinkling of Java. For Rustaceans, we suspect this counts as embarrassing legacy code. A Rust JavaScript runtime called <a target="_blank" href="https://github.com/boa-dev/boa" rel="nofollow">Boa</a> does exist, but it's still in the early stages of development.</p>
<p>There aren't many modern web-rendering engines in existence, and several of them are close relatives: Chrome's Blink is derived from Apple's WebKit, itself derived from KDE's KHTML. Firefox's Gecko is the principal independent one still standing. Since Mozilla canceled development of its successor, it's good to know that it's in active development again. Much of the world is held together by the web, and if that were entirely controlled by one company, it would be <a target="_blank" href="https://xkcd.com/1118/" rel="nofollow">scary</a>. ®</p>
<p>
  <a href="https://www.youtube.com/watch?v=e3Y1C695CIw&amp;t=4100s" data-media="x-videoplayer">Youtube Video</a>
</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The most copied StackOverflow snippet of all time is flawed (348 pts)]]></title>
            <link>https://programming.guide/worlds-most-copied-so-snippet.html</link>
            <guid>37674139</guid>
            <pubDate>Wed, 27 Sep 2023 13:10:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://programming.guide/worlds-most-copied-so-snippet.html">https://programming.guide/worlds-most-copied-so-snippet.html</a>, See on <a href="https://news.ycombinator.com/item?id=37674139">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      
      <p><span>by Andreas Lundblad, 2019-12-02</span></p>
      <p><strong>In a recent study titled <em>Usage and Attribution of Stack Overflow Code Snippets in GitHub Projects</em>, an <a href="https://stackoverflow.com/a/3758880/276052">answer</a> I wrote almost a decade ago was found to be the most copied snippet on Stack Overflow. Ironically it happens to be buggy.</strong></p>
      <h2>A Long Long Time Ago…</h2>
      <p>Back in 2010 I was sitting in my office and doing what I wasn’t supposed to be doing: code golfing and chasing reputation on Stack Overflow.</p>
      <p>The following question got my attention: How do you print a byte count in a human readable format? That is, how do you format something like 123,456,789 bytes as “123.5&nbsp;MB”.</p>
      <div>
        <p><a href="https://stackoverflow.com/q/3758606/276052"><img alt="How to convert byte size into human-readable format in Java? Like 1024 should become '1 Kb' and 1024*1024 should become '1 Mb'." src="https://programming.guide/the-most-copied-so-snippet/so-screenshot.png"></a></p>
      </div>
      <p>The implicit spec here is that the resulting string should have a value between 1 and 999.9 followed by a suffix with an appropriate magnitude.</p>
      <p>One answer had already been posted. The code in that answer was based on a loop. The idea was simple: Try all magnitudes, going from the largest (EB = 10<sup>18</sup> bytes) down to the smallest (B = 1 byte) and use the first one that is smaller than the number of bytes. In pseudo code it looks something like this:</p>
      <pre><code>suffixes   = [ <span>"EB"</span>, <span>"PB"</span>, <span>"TB"</span>, <span>"GB"</span>, <span>"MB"</span>, <span>"kB"</span>, <span>"B"</span> ]
magnitudes = [ <span>10<sup>18</sup></span>, <span>10<sup>15</sup></span>, <span>10<sup>12</sup></span>, <span>10<sup>9</sup></span>, <span>10<sup>6</sup></span>, <span>10<sup>3</sup></span>, <span>10<sup>0</sup></span> ]
i = <span>0</span>
<span>while</span> (i &lt; magnitudes.length &amp;&amp; magnitudes[i] &gt; byteCount)
    i++
printf(<span>"%.1f %s"</span>, byteCount / magnitudes[i], suffixes[i])
</code></pre>
      <p>Usually when there’s a correct answer posted that already has a positive score, it’s hard to catch up with it. In Stack Overflow lingo it’s referred to as the <a href="https://meta.stackexchange.com/q/9731/147319">Fastest Gun in the West Problem</a>. In this case the existing answer had a few flaws, so I still saw an opportunity to top it. At the very least, the loop based code could be cleaned up significantly.</p>
      <h2>This is Algebra, I know this!</h2>
      <p>Then it hit me. The kB, MB, GB,&nbsp;… suffixes are nothing but powers of 1000 (or 1024 in <a href="https://en.wikipedia.org/wiki/Binary_prefix">IEC standard</a>) which means it should be possible to compute the right suffix using logarithms instead of a loop.</p>
      <p>Based on this idea, I posted the following:</p>
      <pre><code><span>public</span> <span>static</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>humanReadableByteCount</span>(<span>long</span> <span>bytes</span>, <span>boolean</span> <span>si</span>) {
    <span>int</span> <span>unit</span> = <span>si</span> ? <span>1000</span> : <span>1024</span>;
    <span>if</span> (<span>bytes</span> &lt; <span>unit</span>) <span>return</span> <span>bytes</span> + <span>" B"</span>;
    <span>int</span> <span>exp</span> = (<span>int</span>) (<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>bytes</span>) / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>unit</span>));
    <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>pre</span> = (<span>si</span> ? <span>"kMGTPE"</span> : <span>"KMGTPE"</span>).<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#charAt(int)"><span>charAt</span></a>(<span>exp</span>-<span>1</span>) + (<span>si</span> ? <span>""</span> : <span>"i"</span>);
    <span>return</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#format(java.lang.String,java.lang.Object:A)"><span>format</span></a>(<span>"%.1f %sB"</span>, <span>bytes</span> / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>), <span>pre</span>);
}</code></pre>
      <p>Granted it’s not very readable and <span>log / pow</span> probably makes it less efficient than other solutions. But there were no loops and almost no branching which I thought was pretty neat.</p>
      <div>
        <p><strong>The math behind this</strong> is straight forward. The byte count is expressed as <span>byteCount = 1000<sup><em>s</em></sup></span> where <em>s</em> represents the scale. (For binary notation, base 1024 is used.) Solving for <em>s</em> gives <span><em>s</em> = log<sub>1000</sub>(byteCount)</span>.</p>
        <p>There’s no log<sub>1000</sub> readily available in the API, but we can express it in terms of the natural logarithm as follows <span><em>s</em> = log(byteCount) / log(1000)</span>. We then floor <em>s</em> (cast to int) since if we for example have more than one megabyte (but not a full gigabyte) we want to use MB as magnitude.</p>
        <p>At this point if <span><em>s</em> = 1</span> the scale is kilobytes, if <span><em>s</em> = 2</span> the scale is megabytes, and so on. We divide the byteCount with 1000<sup><em>s</em></sup> and slap on the corresponding letter prefix.</p>
      </div>
      <p>All I could do now was to wait and see if the community would appreciate the answer. Little did I know that this would become the most copied snippet on Stack Overflow.</p>
      <h2>A Study on Attribution</h2>
      <p>Fast forward to 2018. A PhD student by the name Sebastian Baltes publishes a paper in the journal of <em>Empirical Software Engineering</em>. The title is <a href="https://doi.org/10.1007/s10664-018-9650-5"><em>Usage and Attribution of Stack Overflow Code Snippets in GitHub Projects</em></a> and it basically tries to answer one question: Is Stack Overflow’s CC BY-SA 3.0 license respected? I.e. to what extent is proper attribution given, when copying code from Stack Overflow.</p>
      <p>As part of their analysis they extracted code snippets from the <a href="https://archive.org/details/stackexchange">Stack Overflow data dump</a> and matched them against code from public GitHub repos. Quoting the abstract:</p>
      <blockquote>
        <p><em>We present results of a large-scale empiricalstudy analyzing the usage and attribution of non-trivial Java code snippetsfrom SO answers in public GitHub (GH) projects.</em></p>
      </blockquote>
      <p>(Spoiler alert: No, most people do not include proper attribution.)</p>
      <p>In the paper, they include the following table:</p>
      
      <p>That answer at the top with id <a href="https://stackoverflow.com/a/3758880/276052">3758880</a> happens to be the answer I had posted eight years earlier. At this point the answer had over a hundreds of thousands of views and over a thousand upvotes.</p>
      <p>A quick search on GitHub indeed shows thousands of occurrences of <code>humanReadableByteCount</code>.</p>
      <p><img src="https://programming.guide/the-most-copied-so-snippet/github-search.png"></p>
      <p>To check if you happen to have the code in a locally checked out repo:</p>
      <pre><code>$ git grep humanReadableByteCount
</code></pre>
      <div>
        <p><strong>Fun side story:</strong> How did I first hear about this study?</p>
        <p>Sebastian had found a match in the OpenJDK repository. There was no attribution and the OpenJDK license is not compatible with CC BY-SA 3.0. He <a href="http://mail.openjdk.java.net/pipermail/jdk9-dev/2016-December/005327.html">reached out to the dev mailing list</a> asking if the code on Stack Overflow was copied from OpenJDK, or if it was the other way around.</p>
        <p>The funny part here is that I used to work at Oracle, on the OpenJDK project, so a former colleague and friend of mine <a href="http://mail.openjdk.java.net/pipermail/jdk9-dev/2016-December/005328.html">replied</a> with the following:</p>
        <div>
          <p>Hi,</p>
          <p>why not ask the author of this SO post (aioobe) directly? He is an OpenJDK contributor and was employed by Oracle at the time this code appeared in the OpenJDK source repos.</p>
          <p>/Claes</p>
        </div>
        <p>Oracle doesn’t take these things lightly. I happen to know that some people at Oracle took a sigh of relief when they read this reply, and saw it as a bit of a triumph after the “accusation”.</p>
        <p>Sebastian then reached out to me to straighten it out, which I did: I had <em>not</em> yet started at Oracle when that commit was merged, and I did <em>not</em> contribute that patch. Jokes on Oracle. Shortly after, an issue was <a href="https://bugs.openjdk.java.net/browse/JDK-8170860">filed</a> and the code was <a href="http://hg.openjdk.java.net/jdk9/jdk9/hotspot/rev/b552b596203f">removed</a>.</p>
      </div>
      <h2>The Bug</h2>
      <p>I bet you’ve already given it some thought. What is that bug in the code snippet?</p>
      <p>Here it is again:</p>
      <pre><code><span>public</span> <span>static</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>humanReadableByteCount</span>(<span>long</span> <span>bytes</span>, <span>boolean</span> <span>si</span>) {
    <span>int</span> <span>unit</span> = <span>si</span> ? <span>1000</span> : <span>1024</span>;
    <span>if</span> (<span>bytes</span> &lt; <span>unit</span>) <span>return</span> <span>bytes</span> + <span>" B"</span>;
    <span>int</span> <span>exp</span> = (<span>int</span>) (<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>bytes</span>) / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>unit</span>));
    <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>pre</span> = (<span>si</span> ? <span>"kMGTPE"</span> : <span>"KMGTPE"</span>).<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#charAt(int)"><span>charAt</span></a>(<span>exp</span>-<span>1</span>) + (<span>si</span> ? <span>""</span> : <span>"i"</span>);
    <span>return</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#format(java.lang.String,java.lang.Object:A)"><span>format</span></a>(<span>"%.1f %sB"</span>, <span>bytes</span> / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>), <span>pre</span>);
}</code></pre>
      <p>After exabytes, 10<sup>18</sup>, comes zettabytes, 10<sup>21</sup>. Could it be that a really large input causes an index out of bounds on the <code>"kMGTPE"</code> string? Nope. The maximum <code>long</code> value is <span>2<sup>63</sup> - 1 ≈ 9.2 × 10<sup>18</sup></span>, so no <code>long</code> value will ever go beyond EB.</p>
      <p>Could it be a mix-up between SI and binary? Nope. There was a mix-up in an early version of the answer, but that was fixed rather quickly.</p>
      <p>Can <code>exp</code> end up being 0 causing <code>charAt(exp-1)</code> to fail? Nope. The first if-statement covers that case. The <code>exp</code> value will always be at least 1.</p>
      <p>Could there be some weird rounding error in the output? Now we’re getting there…</p>
      <h2>Lots of 9’s</h2>
      <p>The solution works all the way up until it approaches 1 MB. When given 999,999 bytes as input, the result (in SI mode) is <code>"1000.0 kB"</code>. While it is true that 999,999 is closer to <span>1,000 × 1000<sup>1</sup></span> than it is to <span>999.9 × 1000<sup>1</sup></span>, the 1,000 “significand” is out of range according to spec. The correct result is <code>"1.0 MB"</code>.</p>
      <p>FWIW, <em>all</em> 22 answers posted, including the ones using Apache Commons and Android libraries, had this bug (or a variation of it) at the time of writing this article.</p>
      <p>So how do we fix this? First of all, we note that the exponent (<code>exp</code>) should change from ‘k’ to ‘M’ as soon as the number of bytes is closer to <span>1 × 1,000<sup>2</sup></span> (1 MB) than it is to <span>999.9 × 1000<sup>1</sup></span> (999.9 k). This happens at 999,950. Similarly, we should switch from ‘M’ to ‘G’ when we pass 999,950,000 and so on.</p>
      <p>To achieve this we calculate this threshold and bump <code>exp</code> if <code>bytes</code> is larger. (For the binary case, this threshold is not an integer, se we need to ceil the result.)</p>
      <pre><code><span>if</span> (<span>bytes</span> &gt;= <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#ceil(double)"><span>ceil</span></a>(<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>) * (<span>unit</span> - <span>0.05</span>)))
    <span>exp</span>++;</code></pre>
      <p>With this change the code works well all the way up until the byte count approaches 1&nbsp;EB.</p>
      <h2>Even More 9’s</h2>
      <p>Given the input 999,949,999,999,999,999 the result is now <code>1000.0 PB</code> while correct result is <code>999.9 PB</code>. Mathematically the code is accurate, so what’s going on here?</p>
      <p>At this point we’re running into the precision limitations of a <code>double</code>.</p>
      <div>
        <h3>Floating Point Arithmetic 101</h3>
        <p>Due to the IEEE 754 representation floating point values close to zero are very dense, and large values are very sparse. In fact, half of all values are found between −1 and 1, and when talking large doubles, a value as large as <code>Long.MAX_VALUE</code> means nothing. Literally.</p>
        <pre><code><span>double</span> <span>a</span> = <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Double.html"><span>Double</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Double.html#MAX_VALUE"><span>MAX_VALUE</span></a>;
<span>double</span> <span>b</span> = <span>a</span> - <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html"><span>Long</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html#MAX_VALUE"><span>MAX_VALUE</span></a>;
<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/System.html"><span>System</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/System.html#err"><span>err</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/io/PrintStream.html#println(boolean)"><span>println</span></a>(<span>a</span> == <span>b</span>);  <span>// prints true</span>
</code></pre>
        <p>See <a href="https://programming.guide/bits-of-a-floating-point-value.html">Bits of a Floating Point Value</a> for a deep dive.</p>
      </div>
      <p>There are two problematic computations:</p>
      <ul>
        <li>The division in the <code>String.format</code> argument, and</li>
        <li>The threshold for bumping <code>exp</code>.</li>
      </ul>
      <p>We could switch to <code>BigDecimal</code>, but where’s the fun in that?! Besides, it gets messy anyway because there’s no <code>BigDecimal</code> log function in the standard API.</p>
      <h3>Scaling down intermediate values</h3>
      <p>For the first issue we can scale down the <code>bytes</code> value to a range where the the precision is better, and adjust <code>exp</code> accordingly. The end result is rounded anyway, so it doesn’t matter that we’re throwing out the least significant digits.</p>
      <pre><code><span>if</span> (<span>exp</span> &gt; <span>4</span>) {
    <span>bytes</span> /= <span>unit</span>;
    <span>exp</span>--;
}
</code></pre>
      <h3>Adjusting the least significant bits</h3>
      <p>For the second issue, we <em>do</em> care about the least significant bits (999,949,99…9 and 999,950,00…0 should end up with different exponents) so this issue calls for a different solution.</p>
      <p>First we note that there are 12 different possible values for the threshold (6 for each mode), and only one of them ends up faulty. The faulty result can be uniquely identified by the fact that it ends with D00<sub>16</sub>. If this is the case we simply adjust it to the correct value.</p>
      <pre><code><span>long</span> <span>th</span> = (<span>long</span>) <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#ceil(double)"><span>ceil</span></a>(<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>) * (<span>unit</span> - <span>0.05</span>));
<span>if</span> (<span>exp</span> &lt; <span>6</span> &amp;&amp; <span>bytes</span> &gt;= <span>th</span> - ((<span>th</span> &amp; <span>0xFFF</span>) == <span>0xD00</span> ? <span>51</span> : <span>0</span>))
    <span>exp</span>++;
</code></pre>
      <p>Since we rely on specific bit patterns in floating-point results, we slap on <code>strictfp</code> to ensure it works regardless of the hardware running the code.</p>
      <h2>Negative input</h2>
      <p>It’s unclear under what circumstances a negative byte count could be relevant, but since Java doesn’t have unsigned <code>long</code>, we better deal with it. Right now an input such as −10,000 results in <code>-10000 B</code>.</p>
      <p>Let’s introduce <code>absBytes</code>:</p>
      <pre><code><span>long</span> <span>absBytes</span> = <span>bytes</span> == <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html"><span>Long</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html#MIN_VALUE"><span>MIN_VALUE</span></a> ? <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html"><span>Long</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html#MAX_VALUE"><span>MAX_VALUE</span></a> : <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#abs(int)"><span>abs</span></a>(<span>bytes</span>);
</code></pre>
      <p>The complicated expression is due to the fact that <code>-Long.MIN_VALUE == Long.MIN_VALUE</code>. Now we perform all computations related to <code>exp</code> using <code>absBytes</code> instead of <code>bytes</code>.</p>
      <h2>Final Version</h2>
      <p>Here’s the final version of the code, golfed and compacted in the spirit of the original version:</p>
      <pre><code><span>// From: https://programming.guide/worlds-most-copied-so-snippet.html</span>
<span>public</span> <span>static</span> <span>strictfp</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>humanReadableByteCount</span>(<span>long</span> <span>bytes</span>, <span>boolean</span> <span>si</span>) {
    <span>int</span> <span>unit</span> = <span>si</span> ? <span>1000</span> : <span>1024</span>;
    <span>long</span> <span>absBytes</span> = <span>bytes</span> == <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html"><span>Long</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html#MIN_VALUE"><span>MIN_VALUE</span></a> ? <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html"><span>Long</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Long.html#MAX_VALUE"><span>MAX_VALUE</span></a> : <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#abs(long)"><span>abs</span></a>(<span>bytes</span>);
    <span>if</span> (<span>absBytes</span> &lt; <span>unit</span>) <span>return</span> <span>bytes</span> + <span>" B"</span>;
    <span>int</span> <span>exp</span> = (<span>int</span>) (<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>absBytes</span>) / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#log(double)"><span>log</span></a>(<span>unit</span>));
    <span>long</span> <span>th</span> = (<span>long</span>) <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#ceil(double)"><span>ceil</span></a>(<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>) * (<span>unit</span> - <span>0.05</span>));
    <span>if</span> (<span>exp</span> &lt; <span>6</span> &amp;&amp; <span>absBytes</span> &gt;= <span>th</span> - ((<span>th</span> &amp; <span>0xFFF</span>) == <span>0xD00</span> ? <span>51</span> : <span>0</span>)) <span>exp</span>++;
    <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a> <span>pre</span> = (<span>si</span> ? <span>"kMGTPE"</span> : <span>"KMGTPE"</span>).<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#charAt(int)"><span>charAt</span></a>(<span>exp</span> - <span>1</span>) + (<span>si</span> ? <span>""</span> : <span>"i"</span>);
    <span>if</span> (<span>exp</span> &gt; <span>4</span>) {
        <span>bytes</span> /= <span>unit</span>;
        <span>exp</span> -= <span>1</span>;
    }
    <span>return</span> <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html"><span>String</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#format(java.lang.String,java.lang.Object:A)"><span>format</span></a>(<span>"%.1f %sB"</span>, <span>bytes</span> / <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html"><span>Math</span></a>.<a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/Math.html#pow(double,double)"><span>pow</span></a>(<span>unit</span>, <span>exp</span>), <span>pre</span>);
}
</code></pre>
      <p>Note that this started out as a challenge to avoid loops and excessive branching. After ironing out all corner cases the code is even less readable than the original version. Personally I would not copy this snippet into production code.</p>
      <p>For <strong>updated code that is of production quality</strong> see separate article: <a href="https://programming.guide/java/formatting-byte-size-to-human-readable-format.html">Formatting byte size to human readable format</a></p>
      <h2>Key Takeaways</h2>
      <ul>
        <li>
          <p>Stack Overflow snippets can be buggy, even if they have thousands of upvotes.</p>
        </li>
        <li>
          <p>Test all edge cases, <em>especially</em> for code copied from Stack Overflow.</p>
        </li>
        <li>
          <p>Floating-point arithmetic is hard.</p>
        </li>
        <li>
          <p>Do include proper attribution when copying code. Someone might just call you out on it.</p>
        </li>
      </ul>
      <div>
        <h2>Comments (11)</h2>
        <div id="c1575414054-aveiSoh5">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/d4a2f69d78ee7a7cd0be47f92ad3a114?d=mp"></p>
          <div>
            <p>Whoa! What a writeup. Thanks for sharing</p>
            <details>
              <summary>
                <span>by Nick&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575431393-aet3OPhu">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/af0c24a85eac0b47ac8027eb36e11e75?d=mp"></p>
          <div>
            <p>This is a really hard problem.</p>
            <details>
              <summary>
                <span>by Ssuching&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575454325-abeCohl4">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/2f86996e30f54a0d42d93e5904b74e8c?d=mp"></p>
          <div>
            <p>Brilliant. Attitude too. Thanks a lot.</p>
            <details>
              <summary>
                <span>by syjmick&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575455440-sho7Gaey">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/a01d535d38a2fe0aff4a981308915203?d=mp"></p>
          <div>
            <div>
              <p>I think key takeaway here is: prefer short and simple loops over math. As you already said, the math is very hard to get exactly right (so, error prone and hard to read). But I believe it is also at least an order of magnitude slower than the loop-based solution.</p>
              <p>Did you run any benchmarks?</p>
            </div>
            <details>
              <summary>
                <span>by Ivan&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
            <div id="c1575475836-aoeu">
              <p><img alt="User avatar" src="https://www.gravatar.com/avatar/99e100243aaa8b1469b1ed4e8bbecb06?d=mp"></p>
              <div>
                <div>
                  <p>In general I agree with you. In this particular case however, one should note that the rounding error, negative input, and floating-point precision problems would apply also to a simple loop solution.</p>
                  <p>I have not done any benchmarking. Would be interesting for sure.</p>
                </div>
                <details>
                  <summary>
                    <span>by Andreas Lundblad&nbsp;|&nbsp;</span> <span>Reply</span>
                  </summary>
                  
                </details>
              </div>
            </div>
          </div>
        </div>
        <div id="c1575468068-Ai0eengu">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/4d400194f4d28fee7487eb826d463d9e?d=mp"></p>
          <div>
            <p>This is fantastically done. Thanks for posting this.</p>
            <details>
              <summary>
                <span>by John Doe&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575532582-etha1ahJ">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/a2452b2d1c9315f8cc62c7290c9a26f2?d=mp"></p>
          <div>
            <div>
              <p>This is very interesting article, I worked with floating point but always had difficulty to grasp the special cases of rounding. This article is good academic view of special cases to consider.</p>
              <p>Thanks! Five stars for this article. ★★★★★</p>
            </div>
            <details>
              <summary>
                <span>by Audiory&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575555484-auGh7hai">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/38b8a18199083b31074a90b47810b1ce?d=mp"></p>
          <div>
            <p>This article was quite a journey! Thanks :)</p>
            <details>
              <summary>
                <span>by swiatek7&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575671337-Il5che5l">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/124aca96dfe03819a6bc6e782e18d006?d=mp"></p>
          <div>
            <p>Awesome! Thanks for sharing!</p>
            <details>
              <summary>
                <span>by Adam&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
          </div>
        </div>
        <div id="c1575719090-ohXah7iu">
          <p><img alt="User avatar" src="https://www.gravatar.com/avatar/1eaa7f930b2393cba93e21925ca58ab5?d=mp"></p>
          <div>
            <p>Nice article, thanks for sharing this story! So, how is it done in Unix? Some commands like <code>ls</code>, <code>df</code> have the <code>-h</code> human readable option.</p>
            <details>
              <summary>
                <span>by IvanV&nbsp;|&nbsp;</span> <span>Reply</span>
              </summary>
              
            </details>
            <div id="c1575732527-aoeu">
              <p><img alt="User avatar" src="https://www.gravatar.com/avatar/99e100243aaa8b1469b1ed4e8bbecb06?d=mp"></p>
              <div>
                <p>The implementation for coreutils is found in <a href="https://github.com/coreutils/gnulib/blob/master/lib/human.c"><code>human.c</code></a></p>
                <details>
                  <summary>
                    <span>by Andreas Lundblad&nbsp;|&nbsp;</span> <span>Reply</span>
                  </summary>
                  
                </details>
              </div>
            </div>
          </div>
        </div>
        <h3>Add comment</h3>
        
      </div>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Workers AI: serverless GPU-powered inference on Cloudflare’s global network (241 pts)]]></title>
            <link>https://blog.cloudflare.com/workers-ai/</link>
            <guid>37674097</guid>
            <pubDate>Wed, 27 Sep 2023 13:06:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/workers-ai/">https://blog.cloudflare.com/workers-ai/</a>, See on <a href="https://news.ycombinator.com/item?id=37674097">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post">
    <article>
        


        <p localize="" datetime="2023-09-27T14:00:47+01:00">Loading...</p>
        

        <ul>
            <li>
                <a href="https://blog.cloudflare.com/author/phil/">
                    <img src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2023/09/phil.jpeg" alt="Phil Wittig" width="62" height="62">
                </a>
                
            </li>
            <li>
                <a href="https://blog.cloudflare.com/author/rita/">
                    <img src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/05/Rita-Kozlov.jpeg" alt="Rita Kozlov" width="62" height="62">
                </a>
                
            </li>
            <li>
                <a href="https://blog.cloudflare.com/author/rebecca-weekly/">
                    <img src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/04/RWeekly---Retouched-16.jpg" alt="Rebecca Weekly" width="62" height="62">
                </a>
                
            </li>
            <li>
                <a href="https://blog.cloudflare.com/author/celso/">
                    <img src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Celso-Martinho.png" alt="Celso Martinho" width="62" height="62">
                </a>
                
            </li>
        </ul>

        <section>
            <p>11 min read</p>
            <div>
                <figure><img src="https://blog.cloudflare.com/content/images/2023/09/image1-29.png" alt="" loading="lazy" width="1800" height="1014"></figure><p>If you're anywhere near the developer community, it's almost impossible to avoid the impact that AI’s recent advancements have had on the ecosystem. Whether you're using AI in your workflow to improve productivity, or you’re shipping AI based features to your users, it’s everywhere. The focus on AI improvements are extraordinary, and we’re super excited about the opportunities that lay ahead, but it's not enough.</p><p>Not too long ago, if you wanted to leverage the power of AI, you needed to know the ins and outs of machine learning, and be able to manage the infrastructure to power it.</p><p>As a developer platform with over one million active developers, we believe there is so much potential yet to be unlocked, so we’re changing the way AI is delivered to developers. Many of the current solutions, while powerful, are based on closed, proprietary models and don't address privacy needs that developers and users demand. Alternatively, the open source scene is exploding with powerful models, but they’re simply not accessible enough to every developer. Imagine being able to run a model, from your code, wherever it’s hosted, and never needing to find GPUs or deal with setting up the infrastructure to support it.</p><p>That's why we are excited to launch Workers AI - an AI inference as a service platform, empowering developers to run AI models with just a few lines of code, all powered by our global network of GPUs. It's open and accessible, serverless, privacy-focused, runs near your users, pay-as-you-go, and it's built from the ground up for a best in class developer experience.</p><h2 id="workers-aimaking-inference-just-work">Workers AI - making inference <strong>just work</strong></h2><p>We’re launching Workers AI to put AI inference in the hands of every developer, and to actually deliver on that goal, it should <strong>just work</strong> out of the box. How do we achieve that?</p><ul><li>At the core of everything, it runs on the right infrastructure - our world-class network of GPUs</li><li>We provide off-the-shelf models that run seamlessly on our infrastructure</li><li>Finally, deliver it to the end developer, in a way that’s delightful. A developer should be able to build their first Workers AI app in minutes, and say “Wow, that’s kinda magical!”.</li></ul><p>So what exactly is Workers AI? It’s another building block that we’re adding to our developer platform - one that helps developers run well-known AI models on serverless GPUs, all on Cloudflare’s trusted global network. As one of the latest additions to our developer platform, it works seamlessly with Workers + Pages, but to make it truly accessible, we’ve made it platform-agnostic, so it also works everywhere else, made available via a REST API.</p><h2 id="models-you-know-and-love">Models you know and love</h2><p>We’re launching with a curated set of popular, open source models, that cover a wide range of inference tasks:</p><ul><li><strong>Text generation (large language model):</strong> meta/llama-2-7b-chat-int8</li><li><strong>Automatic speech recognition (ASR):</strong> openai/whisper</li><li><strong>Translation:</strong> meta/m2m100-1.2</li><li><strong>Text classification:</strong> huggingface/distilbert-sst-2-int8</li><li><strong>Image classification:</strong> microsoft/resnet-50</li><li><strong>Embeddings:</strong> baai/bge-base-en-v1.5</li></ul><p>You can browse all available models in your Cloudflare dashboard, and soon you’ll be able to dive into logs and analytics on a per model basis!</p><figure><img src="https://blog.cloudflare.com/content/images/2023/09/image4-14.png" alt="" loading="lazy" width="1306" height="832"></figure><p>This is just the start, and we’ve got big plans. After launch, we’ll continue to expand based on community feedback. Even more exciting - in an effort to take our catalog from zero to sixty, we’re announcing a partnership with Hugging Face, a leading AI community + hub. The partnership is multifaceted, and you can read more about it <a href="https://blog.cloudflare.com/best-place-region-earth-inference">here</a>, but soon you’ll be able to browse and run a subset of the Hugging Face catalog directly in Workers AI.</p><h2 id="accessible-to-everyone">Accessible to everyone</h2><p>Part of the mission of our developer platform is to provide <strong>all</strong> the building blocks that developers need to build the applications of their dreams. Having access to the right blocks is just one part of it — as a developer your job is to put them together into an application. Our goal is to make that as easy as possible.</p><p>To make sure you could use Workers AI easily regardless of entry point, we wanted to provide access via: Workers or Pages to make it easy to use within the Cloudflare ecosystem, and via REST API if you want to use Workers AI with your current stack.</p><p>Here’s a quick CURL example that translates some text from English to French:</p><!--kg-card-begin: markdown--><pre><code>curl https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/run/@cf/meta/m2m100-1.2b \
-H "Authorization: Bearer {API_TOKEN}" \
	-d '{ "text": "I'll have an order of the moule frites", "target_lang": "french" }'
</code></pre>
<!--kg-card-end: markdown--><p>And here are what the response looks like:</p><!--kg-card-begin: markdown--><pre><code>{
  "result": {
    "answer": "Je vais commander des moules frites"
  },
  "success": true,
  "errors":[],
  "messages":[]
}
</code></pre>
<!--kg-card-end: markdown--><p>Use it with any stack, anywhere - your favorite Jamstack framework, Python + Django/Flask, Node.js, Ruby on Rails, the possibilities are endless. And deploy.</p><h2 id="designed-for-developers">Designed for developers</h2><p>Developer experience is really important to us. In fact, most of this post has been about just that. Making sure it works out of the box. Providing popular models that just work. Being accessible to all developers whether you build and deploy with Cloudflare or elsewhere. But it’s more than that - the experience should be frictionless, zero to production should be fast, and it should feel good along the way.</p><p>Let’s walk through another example to show just how easy it is to use! We’ll run Llama 2, a popular large language model open sourced by Meta, in a worker.</p><p>We’ll assume you have some of the basics already complete (Cloudflare account, Node, NPM, etc.), but if you don’t <a href="https://developers.cloudflare.com/workers-ai/get-started/local-dev-setup/">this guide</a> will get you properly set up!</p><h3 id="1-create-a-workers-project">1. Create a Workers project</h3><p>Create a new project named workers-ai by running:</p><!--kg-card-begin: markdown--><pre><code>$ npm create cloudflare@latest
</code></pre>
<!--kg-card-end: markdown--><p>When setting up your workers-ai worker, answer the setup questions as follows:</p><ul><li>Enter <strong>workers-ai</strong> for the app name</li><li>Choose <strong>Hello World</strong> script for the type of application</li><li>Select <strong>yes </strong>to using TypeScript</li><li>Select <strong>yes</strong> to using Git</li><li>Select <strong>no</strong> to deploying</li></ul><p>Lastly navigate to your new app directory:</p><!--kg-card-begin: markdown--><pre><code>cd workers-ai
</code></pre>
<!--kg-card-end: markdown--><h3 id="2-connect-workers-ai-to-your-worker">2. Connect Workers AI to your worker</h3><p>Create a Workers AI binding, which allows your worker to access the Workers AI service without having to manage an API key yourself.</p><p>To bind Workers AI to your worker, add the following to the end of your <strong>wrangler.toml</strong> file:</p><!--kg-card-begin: markdown--><pre><code>[ai]
binding = "AI" #available in your worker via env.AI
</code></pre>
<!--kg-card-end: markdown--><p>You can also bind Workers AI to a Pages Function. For more information, refer to <a href="https://developers.cloudflare.com/pages/platform/functions/bindings/#ai">Functions Bindings</a>.</p><h3 id="3-install-the-workers-ai-client-library">3. Install the Workers AI client library</h3><!--kg-card-begin: markdown--><pre><code>npm install @cloudflare/ai
</code></pre>
<!--kg-card-end: markdown--><h3 id="4-run-an-inference-task-in-your-worker">4. Run an inference task in your worker</h3><p>Update the <strong>source/index.ts</strong> with the following code:</p><!--kg-card-begin: markdown--><pre><code>import { Ai } from '@cloudflare/ai'
export default {
  async fetch(request, env) {
    const ai = new Ai(env.AI);
    const input = { prompt: "What's the origin of the phrase 'Hello, World'" };
    const output = await ai.run('@cf/meta/llama-2-7b-chat-int8', input );
    return new Response(JSON.stringify(output));
  },
};
</code></pre>
<!--kg-card-end: markdown--><h3 id="5-develop-locally-with-wrangler">5. Develop locally with Wrangler</h3><p>While in your project directory, test Workers AI locally by running:</p><!--kg-card-begin: markdown--><pre><code>$ npx wrangler dev --remote
</code></pre>
<!--kg-card-end: markdown--><p><strong>Note - </strong>These models currently only run on Cloudflare’s network of GPUs (and not locally), so setting <code>--remote</code> above is a must, and you’ll be prompted to log in at this point.</p><p>Wrangler will give you a URL (most likely localhost:8787). Visit that URL, and you’ll see a response like this</p><!--kg-card-begin: markdown--><pre><code>{
  "response": "Hello, World is a common phrase used to test the output of a computer program, particularly in the early stages of programming. The phrase "Hello, World!" is often the first program that a beginner learns to write, and it is included in many programming language tutorials and textbooks as a way to introduce basic programming concepts. The origin of the phrase "Hello, World!" as a programming test is unclear, but it is believed to have originated in the 1970s. One of the earliest known references to the phrase is in a 1976 book called "The C Programming Language" by Brian Kernighan and Dennis Ritchie, which is considered one of the most influential books on the development of the C programming language.
}
</code></pre>
<!--kg-card-end: markdown--><h3 id="6-deploy-your-worker">6. Deploy your worker</h3><p>Finally, deploy your worker to make your project accessible on the Internet:</p><!--kg-card-begin: markdown--><pre><code>$ npx wrangler deploy
# Outputs: https://workers-ai.&lt;YOUR_SUBDOMAIN&gt;.workers.dev
</code></pre>
<!--kg-card-end: markdown--><p>And that’s it. You can literally go from zero to deployed AI in minutes. This is obviously a simple example, but shows how easy it is to run Workers AI from any project.</p><h2 id="privacy-by-default">Privacy by default</h2><p>When Cloudflare was founded, our value proposition had three pillars: more secure, more reliable, and more performant. Over time, we’ve realized that a better Internet is also a more private Internet, and we want to play a role in building it.</p><p>That’s why Workers AI is private by default - we don’t train our models, LLM or otherwise, on your data or conversations, and our models don’t learn from your usage. You can feel confident using Workers AI in both personal and business settings, without having to worry about leaking your data. Other providers only offer this fundamental feature with their enterprise version. With us, it’s built in for everyone.</p><p>We’re also excited to support data localization in the future. To make this happen, we have an ambitious GPU rollout plan - we’re launching with seven sites today, roughly 100 by the end of 2023, and nearly everywhere by the end of 2024. Ultimately, this will empower developers to keep delivering killer AI features to their users, while staying compliant with their end users’ data localization requirements.</p><h2 id="the-power-of-the-platform">The power of the platform</h2><h4 id="vector-databasevectorize">Vector database - Vectorize</h4><p>Workers AI is all about running Inference, and making it really easy to do so, but sometimes inference is only part of the equation. Large language models are trained on a fixed set of data, based on a snapshot at a specific point in the past, and have no context on your business or use case. When you submit a prompt, information specific to you can increase the quality of results, making it more useful and relevant. That’s why we’re also launching Vectorize, our vector database that’s designed to work seamlessly with Workers AI. Here’s a quick overview of how you might use Workers AI + Vectorize together.</p><p>Example: Use your data (knowledge base) to provide additional context to an LLM when a user is chatting with it.</p><ol><li><strong>Generate initial embeddings:</strong> run your data through Workers AI using an embedding model. The output will be embeddings, which are numerical representations of those words.</li><li><strong><strong><strong>Insert those embeddings into Vectorize: </strong></strong></strong>this essentially seeds the vector database with your data, so we can later use it to retrieve embeddings that are similar to your users’ query</li><li><strong><strong><strong>Generate embedding from user question: </strong></strong></strong>when a user submits a question to your AI app, first, take that question, and run it through Workers AI using an embedding model.</li><li><strong><strong><strong>Get context from Vectorize: </strong></strong></strong>use that embedding to query Vectorize. This should output embeddings that are similar to your user’s question.</li><li><strong><strong><strong>Create context aware prompt:</strong> </strong></strong>Now take the original text associated with those embeddings, and create a new prompt combining the text from the vector search, along with the original question</li><li><strong>Run prompt: </strong>run this prompt through Workers AI using an LLM model to get your final result</li></ol><h4 id="ai-gateway">AI Gateway</h4><p>That covers a more advanced use case. On the flip side, if you are running models elsewhere, but want to get more out of the experience, you can run those APIs through our AI gateway to get features like caching, rate-limiting, analytics and logging. These features can be used to protect your end point, monitor and optimize costs, and also help with data loss prevention. Learn more about AI gateway <a href="https://blog.cloudflare.com/announcing-ai-gateway">here</a>.</p><h2 id="start-building-today">Start building today</h2><p>Try it out for yourself, and let us know what you think. Today we’re launching Workers AI as an open Beta for all Workers plans - free or paid. That said, it’s super early, so…</p><h4 id="warningit%E2%80%99s-an-early-beta">Warning - It’s an early beta</h4><p>Usage is <strong>not currently recommended for production apps</strong>, and limits + access are subject to change.</p><h4 id="limits">Limits</h4><p>We’re initially launching with limits on a per-model basis</p><ul><li>@cf/meta/llama-2-7b-chat-int8: 50 reqs/min globally</li></ul><p>Checkout our <a href="https://developers.cloudflare.com/workers-ai/platform/limits/">docs</a> for a full overview of our limits.</p><h4 id="pricing">Pricing</h4><p>What we released today is just a small preview to give you a taste of what’s coming (we simply couldn’t hold back), but we’re looking forward to putting the full-throttle version of Workers AI in your hands.</p><p>We realize that as you approach building something, you want to understand: how much is this going to cost me? Especially with AI costs being so easy to get out of hand. So we wanted to share the upcoming pricing of Workers AI with you.</p><p>While we won’t be billing on day one, we are announcing what we expect our pricing will look like.</p><p>Users will be able to choose from two ways to run Workers AI:</p><ul><li><strong>Regular Twitch Neurons (RTN) </strong>- running wherever there's capacity at $0.01 / 1k neurons</li><li><strong>Fast Twitch Neurons (FTN)</strong> - running at nearest user location at $1.25 / 1k neurons</li></ul><p>You may be wondering — what’s a neuron?</p><p>Neurons are a way to measure AI output that always scales down to zero (if you get no usage, you will be charged for 0 neurons). To give you a sense of what you can accomplish with a thousand neurons, you can: generate 130 LLM responses, 830 image classifications, or 1,250 embeddings.</p><p>Our goal is to help our customers pay only for what they use, and choose the pricing that best matches their use case, whether it’s price or latency that is top of mind.</p><h3 id="what%E2%80%99s-on-the-roadmap">What’s on the roadmap?</h3><p>Workers AI is just getting started, and we want your feedback to help us make it great. That said, there are some exciting things on the roadmap.</p><h4 id="more-models-please">More models, please</h4><p>We're launching with a solid set of models that just work, but will continue to roll out new models based on your feedback. If there’s a particular model you'd love to see on Workers AI, pop into our<a href="https://discord.cloudflare.com/"> Discord</a> and let us know!</p><p>In addition to that, we're also announcing a<a href="https://blog.cloudflare.com/best-place-region-earth-inference"> partnership with Hugging Face</a>, and soon you'll be able to access and run a subset of the Hugging Face catalog directly from Workers AI.</p><h4 id="analytics-observability">Analytics + observability</h4><p>Up to this point, we’ve been hyper focussed on one thing - making it really easy for any developer to run powerful AI models in just a few lines of code. But that’s only one part of the story. Up next, we’ll be working on some analytics and observability capabilities to give you insights into your usage + performance + spend on a per-model basis, plus the ability to fig into your logs if you want to do some exploring.</p><h4 id="a-road-to-global-gpu-coverage">A road to global GPU coverage</h4><p>Our goal is to be the best place to run inference on Region: Earth, so we're adding GPUs to our data centers as fast as we can.</p><p><strong>We plan to be in 100 data centers by the end this year</strong></p><figure><img src="https://blog.cloudflare.com/content/images/2023/09/image3-28.png" alt="" loading="lazy" width="1801" height="1013"></figure><p><strong>And nearly everywhere by the end of 2024</strong></p><figure><img src="https://blog.cloudflare.com/content/images/2023/09/unnamed-3.png" alt="" loading="lazy" width="1600" height="900"></figure><p><br><strong>We’re really excited to see you build</strong> - head over to <a href="https://developers.cloudflare.com/workers-ai/">our docs</a> to get started.</p><p>If you need inspiration, want to share something you’re building, or have a question - pop into our <a href="https://discord.com/invite/cloudflaredev">Developer Discord</a>.</p>
            </div>
        </section>
    
        









    <div>
            <p>We protect
                <a target="_blank" href="https://www.cloudflare.com/network-services/">entire corporate networks</a>,
                    help customers build
                    <a target="_blank" href="https://workers.cloudflare.com/">Internet-scale applications efficiently</a>,
                    accelerate any
                    <a target="_blank" href="https://www.cloudflare.com/performance/accelerate-internet-applications/">website
                    or Internet application</a>,
                    <a target="_blank" href="https://www.cloudflare.com/ddos/">ward off DDoS
                    attacks</a>, keep
                    <a target="_blank" href="https://www.cloudflare.com/application-security/">hackers at
                    bay</a>,
                    and can help you on
                    <a target="_blank" href="https://www.cloudflare.com/products/zero-trust/">your journey to Zero Trust</a>.</p>
            <p>Visit <a target="_blank" href="https://1.1.1.1/">1.1.1.1</a> from any device to get started with
                our free app that makes your Internet faster and safer.</p>
            <p>To learn more about our mission to help build a better Internet, <a target="_blank" href="https://www.cloudflare.com/learning/what-is-cloudflare/">start here</a>. If you're looking for a
                new career direction, check out <a target="_blank" href="https://cloudflare.com/careers">our open
                    positions</a>.</p>
        </div>

        

        
        

        <a href="https://blog.cloudflare.com/tag/birthday-week/">Birthday Week</a>
        <a href="https://blog.cloudflare.com/tag/workers/">Cloudflare Workers</a>
        <a href="https://blog.cloudflare.com/tag/ai/">AI</a>
        <a href="https://blog.cloudflare.com/tag/developer-platform/">Developer Platform</a>
        <a href="https://blog.cloudflare.com/tag/database/">Database</a>
    </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You can now use WebGPU in Cloudflare Workers (130 pts)]]></title>
            <link>https://blog.cloudflare.com/webgpu-in-workers/</link>
            <guid>37673999</guid>
            <pubDate>Wed, 27 Sep 2023 13:00:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/webgpu-in-workers/">https://blog.cloudflare.com/webgpu-in-workers/</a>, See on <a href="https://news.ycombinator.com/item?id=37673999">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post">
    <article>
        


        <p localize="" datetime="2023-09-27T14:00:56+01:00">Loading...</p>
        

        <ul>
            <li>
                <a href="https://blog.cloudflare.com/author/andre-cruz/">
                    <img src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2021/02/andre2.jpg" alt="André Cruz" width="62" height="62">
                </a>
                
            </li>
            <li>
                <a href="https://blog.cloudflare.com/author/celso/">
                    <img src="https://blog.cloudflare.com/cdn-cgi/image/format=auto,dpr=3,width=62,height=62/http://blog.cloudflare.com/content/images/2022/08/Celso-Martinho.png" alt="Celso Martinho" width="62" height="62">
                </a>
                
            </li>
        </ul>

        <section>
            <p>12 min read</p>
            <div>
                <figure><img src="https://blog.cloudflare.com/content/images/2023/09/image1-27.png" alt="" loading="lazy" width="1999" height="1125"></figure><p>The browser as an app platform is real and stronger every day; long gone are the Browser Wars. Vendors and standard bodies have done amazingly well over the last years, working together and advancing web standards with new APIs that allow developers to build fast and powerful applications, finally comparable to those we got used to seeing in the native OS' environment.</p><p>Today, browsers can render web pages and run code that interfaces with an <a href="https://developer.mozilla.org/en-US/docs/Web/API">extensive catalog of modern Web APIs</a>. Things like networking, rendering accelerated graphics, or even accessing low-level hardware features like USB devices are all now possible within the browser sandbox.</p><p>One of the most exciting new browser APIs that browser vendors have been rolling out over the last months is WebGPU, a modern, low-level GPU programming interface designed for high-performance 2D and 3D graphics and general purpose GPU compute.</p><p>Today, we are introducing <a href="https://developer.chrome.com/blog/webgpu-release/">WebGPU</a> support to Cloudflare Workers. This blog will explain why it's important, why we did it, how you can use it, and what comes next.</p><h3 id="the-history-of-the-gpu-in-the-browser">The history of the GPU in the browser</h3><p>To understand why WebGPU is a big deal, we must revisit history and see how browsers went from relying only on the CPU for everything in the early days to taking advantage of GPUs over the years.</p><p>In 2011, <a href="https://en.wikipedia.org/wiki/WebGL">WebGL 1</a>, a limited port of <a href="https://www.khronos.org/opengles/">OpenGL ES 2.0</a>, was introduced, providing an API for fast, accelerated 3D graphics in the browser for the first time. By then, this was somewhat of a revolution in enabling gaming and 3D visualizations in the browser. Some of the most popular 3D animation frameworks, like <a href="https://threejs.org/">Three.js</a>, launched in the same period. Who doesn't remember going to the (now defunct) <a href="https://en.wikipedia.org/wiki/Google_Chrome_Experiments">Google Chrome Experiments</a> page and spending hours in awe exploring the demos? Another option then was using the Flash Player, which was still dominant in the desktop environment, and their <a href="https://en.wikipedia.org/wiki/Stage3D">Stage 3D</a> API.</p><p>Later, in 2017, based on the learnings and shortcomings of its predecessor, WebGL 2 was a significant upgrade and brought more advanced GPU capabilities like shaders and more flexible textures and rendering.</p><p>WebGL, however, has proved to be a steep and complex learning curve for developers who want to take control of things, do low-level 3D graphics using the GPU, and not use 3rd party abstraction libraries.</p><p>Furthermore and more importantly, with the advent of machine learning and cryptography, we discovered that GPUs are great not only at drawing graphics but can be used for other applications that can take advantage of things like high-speed data or blazing-fast matrix multiplications, and one can use them to perform general computation. This became known as <a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">GPGPU</a>, short for general-purpose computing on graphics processing units.</p><p>With this in mind, in the native desktop and mobile operating system worlds, developers started using more advanced frameworks like <a href="https://en.wikipedia.org/wiki/CUDA">CUDA</a>, <a href="https://developer.apple.com/metal/">Metal</a>, <a href="https://en.wikipedia.org/wiki/DirectX#DirectX_12">DirectX 12</a>, or <a href="https://www.vulkan.org/learn#key-resources">Vulkan</a>. WebGL stayed behind. To fill this void and bring the browser up to date, in 2017, companies like Google, Apple, Intel, Microsoft, Kronos, and Mozilla created the <a href="https://www.w3.org/community/gpu/">GPU for Web Community Working Group</a> to collaboratively design the successor of WebGL and create the next modern 3D graphics and computation capabilities APIs for the Web.</p><h3 id="what-is-webgpu">What is WebGPU</h3><p>WebGPU was developed with the following advantages in mind:</p><ul><li><strong>Lower Level Access</strong> - WebGPU provides lower-level, direct access to the GPU vs. the high-level abstractions in WebGL. This enables more control over GPU resources.</li><li><strong>Multi-Threading</strong> - WebGPU can leverage multi-threaded rendering and compute, allowing improved CPU/GPU parallelism compared to WebGL, which relies on a single thread.</li><li><strong>Compute Shaders</strong> - First-class support for general-purpose compute shaders for GPGPU tasks, not just graphics. WebGL compute is limited.</li><li><strong>Safety</strong> - WebGPU ensures memory and GPU access safety, avoiding common WebGL pitfalls.</li><li><strong>Portability</strong> - WGSL shader language targets cross-API portability across GPU vendors vs. GLSL in WebGL.</li><li><strong>Reduced Driver Overhead</strong> - The lower level Vulkan/Metal/D3D12 basis improves overhead vs. OpenGL drivers in WebGL.</li><li><strong>Pipeline State Objects</strong> - Predefined pipeline configs avoid per-draw driver overhead in WebGL.</li><li><strong>Memory Management</strong> - Finer-grained buffer and resource management vs. WebGL.</li></ul><p>The “too long didn't read” version is that WebGPU provides lower-level control over the GPU hardware with reduced overhead. It's safer, has multi-threading, is focused on compute, not just graphics, and has portability advantages compared to WebGL.</p><p>If these aren't reasons enough to get excited, developers are also looking at WebGPU as an option for native platforms, not just the Web. For instance, you can use this <a href="https://github.com/webgpu-native/webgpu-headers/blob/main/webgpu.h">C API</a> that mimics the JavaScript specification. If you think about this and the power of WebAssembly, you can effectively have a truly platform-agnostic GPU hardware layer that you can use to <a href="https://developer.chrome.com/blog/webgpu-cross-platform/">develop</a> platforms for any operating system or browser.</p><h3 id="more-than-just-graphics">More than just graphics</h3><p>As explained above, besides being a graphics API, WebGPU makes it possible to perform tasks such as:</p><ul><li><strong>Machine Learning</strong> - Implement ML applications like neural networks and computer vision algorithms using WebGPU compute shaders and matrices.</li><li><strong>Scientific Computing</strong> - Perform complex scientific computation like physics simulations and mathematical modeling using the GPU.</li><li><strong>High Performance Computing</strong> - Unlock breakthrough performance for parallel workloads by connecting WebGPU to languages like Rust, C/C++ via <a href="https://webassembly.org/">WebAssembly</a>.</li></ul><p><a href="https://gpuweb.github.io/gpuweb/wgsl/">WGSL</a>, the shader language for WebGPU, is what enables the general-purpose compute feature. Shaders, or more precisely, <a href="https://www.khronos.org/opengl/wiki/Compute_Shader">compute shaders</a>, have no user-defined inputs or outputs and are used for computing arbitrary information. Here are <a href="https://webgpufundamentals.org/webgpu/lessons/webgpu-compute-shaders.html">some examples</a> of simple WebGPU compute shaders if you want to learn more.</p><h3 id="webgpu-in-workers">WebGPU in Workers</h3><p>We've been watching WebGPU since the API was published. Its general-purpose compute features perfectly fit our Workers' ecosystem and capabilities and align well with our vision of providing our customers multiple compute and hardware options and bringing GPU workloads to our global network, close to clients.</p><p>Cloudflare also has a track record of pioneering support for emerging web standards on our network and services, accelerating their adoption for our customers. Examples of these are <a href="https://developers.cloudflare.com/workers/runtime-apis/web-crypto/">Web Crypto API</a>, <a href="https://blog.cloudflare.com/introducing-http2/">HTTP/2</a>, <a href="https://blog.cloudflare.com/http3-the-past-present-and-future/">HTTP/3</a>, <a href="https://blog.cloudflare.com/introducing-tls-1-3/">TLS 1.3</a>, or <a href="https://blog.cloudflare.com/early-hints/">Early hints</a>, but <a href="https://developers.cloudflare.com/workers/runtime-apis/">there are more</a>.</p><p>Bringing WebGPU to Workers was both natural and timely. Today, we are announcing that we have released a version of <a href="https://github.com/cloudflare/workerd">workerd</a>, the open-sourced JavaScript / Wasm runtime that powers Cloudflare Workers, with <a href="https://github.com/cloudflare/workerd/tree/main/src/workerd/api/gpu">WebGPU support</a>, that you can start playing and developing applications with, locally.</p><p>Starting today anyone can run this on their personal computer and experiment with WebGPU-enabled workers. Implementing local development first allows us to put this API in the hands of our customers and developers earlier and get feedback that will guide the development of this feature for production use.</p><p>But before we dig into code examples, let's explain how we built it.</p><h3 id="how-we-built-webgpu-on-top-of-workers">How we built WebGPU on top of Workers</h3><figure><img src="https://blog.cloudflare.com/content/images/2023/09/image2-22.png" alt="" loading="lazy" width="1540" height="1350"></figure><p>To implement the WebGPU API, we took advantage of <a href="https://dawn.googlesource.com/dawn/">Dawn</a>, an open-source library backed by Google, the same used in Chromium and Chrome, that provides applications with an implementation of the WebGPU standard. It also provides the <a href="https://github.com/webgpu-native/webgpu-headers/blob/main/webgpu.h">webgpu.h</a> headers file, the de facto reference for all the other implementations of the standard.</p><p>Dawn can interoperate with Linux, MacOS, and Windows GPUs by interfacing with each platform's native GPU frameworks. For example, when an application makes a WebGPU draw call, Dawn will convert that draw command into the equivalent Vulkan, Metal, or Direct3D 12 API call, depending on the platform.</p><p>From an application standpoint, Dawn handles the interactions with the underlying native graphics APIs that communicate directly with the GPU drivers. Dawn essentially acts as a middle layer that translates the WebGPU API calls into calls for the platform's native graphics API.</p><p>Cloudflare <a href="https://blog.cloudflare.com/workerd-open-source-workers-runtime/">workerd</a> is the underlying open-source runtime engine that executes Workers code. It shares most of its code with the same runtime that powers Cloudflare Workers' production environment but with some changes designed to make it more portable to other environments. We then have release cycles that aim to synchronize both codebases; more on that later. Workerd is also used with <a href="https://github.com/cloudflare/workers-sdk">wrangler</a>, our command-line tool for building and interacting with Cloudflare Workers, to support local development.</p><p>The WebGPU code that interfaces with the Dawn library can be found <a href="https://github.com/cloudflare/workerd/tree/main/src/workerd/api/gpu">here</a>, and can easily be enabled with a flag, checked <a href="https://github.com/cloudflare/workerd/blob/main/src/workerd/api/global-scope.c%2B%2B#L728">here</a>.</p><!--kg-card-begin: markdown--><pre><code>jsg::Ref&lt;api::gpu::GPU&gt; Navigator::getGPU(CompatibilityFlags::Reader flags) {
  // is this a durable object?
  KJ_IF_MAYBE (actor, IoContext::current().getActor()) {
    JSG_REQUIRE(actor-&gt;getPersistent() != nullptr, TypeError,
                "webgpu api is only available in Durable Objects (no storage)");
  } else {
    JSG_FAIL_REQUIRE(TypeError, "webgpu api is only available in Durable Objects");
  };

  JSG_REQUIRE(flags.getWebgpu(), TypeError, "webgpu needs the webgpu compatibility flag set");

  return jsg::alloc&lt;api::gpu::GPU&gt;();
}
</code></pre>
<!--kg-card-end: markdown--><p>The WebGPU API can only be accessed using <a href="https://developers.cloudflare.com/durable-objects/">Durable Objects</a>, which are essentially global singleton instances of Cloudflare Workers. There are two important reasons for this:</p><ul><li>WebGPU code typically wants to store the state between requests, for example, loading an AI model into the GPU memory once and using it multiple times for inference.</li><li>Not all Cloudflare servers have GPUs yet, so although the worker that receives the request is typically the closest one available, the Durable Object that uses WebGPU will be instantiated where there are GPU resources available, which may not be on the same machine.</li></ul><p>Using Durable Objects instead of regular Workers allow us to address both of these issues.</p><h3 id="the-webgpu-hello-world-in-workers">The WebGPU Hello World in Workers</h3><p>Wrangler uses Miniflare 3, a <a href="https://blog.cloudflare.com/wrangler3/">fully-local simulator for Workers</a>, which in turn is powered by workerd. This means you can start experimenting and doing WebGPU code locally on your machine right now before we prepare things in our production environment.</p><p>Let’s get coding then.</p><p>Since Workers doesn't render graphics yet, we started with implementing the general-purpose GPU (GPGPU) APIs in the <a href="https://www.w3.org/TR/webgpu/">WebGPU specification</a>. In other words, we fully support the part of the API that the <a href="https://www.w3.org/TR/webgpu/#gpucomputepipeline">compute shaders and the compute pipeline</a> require, but we are not yet focused on fragment or vertex shaders used in rendering pipelines.</p><p>Here’s a typical “hello world” in WebGPU. This Durable Object script will output the name of the GPU device that workerd found in your machine to your console.</p><!--kg-card-begin: markdown--><pre><code>const adapter = await navigator.gpu.requestAdapter();
const adapterInfo = await adapter.requestAdapterInfo(["device"]);
console.log(adapterInfo.device);
</code></pre>
<!--kg-card-end: markdown--><p>A more interesting example, though, is a simple compute shader. In this case, we will fill a results buffer with an incrementing value taken from the iteration number via <code>global_invocation_id</code>.</p><p>For this, we need two buffers, one to store the results of the computations as they happen (<code>storageBuffer</code>) and another to copy the results at the end (<code>mappedBuffer</code>).</p><p>We then dispatch four workgroups, meaning that the increments can happen in parallel. This parallelism and programmability are two key reasons why compute shaders and GPUs provide an advantage for things like machine learning inference workloads. Other advantages are:</p><ul><li><strong>Bandwidth</strong> - GPUs have a very high memory bandwidth, up to 10-20x more than CPUs. This allows fast reading and writing of all the model parameters and data needed for inference.</li><li><strong>Floating-point performance</strong> - GPUs are optimized for high floating point operation throughput, which are used extensively in neural networks. They can deliver much higher <a href="https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html">TFLOPs than CPUs</a>.</li></ul><p>Let’s look at the code:</p><!--kg-card-begin: markdown--><pre><code>// Create device and command encoder
const adapter = await navigator.gpu.requestAdapter();
const device = await adapter.requestDevice();
const encoder = device.createCommandEncoder();

// Storage buffer
const storageBuffer = device.createBuffer({
  size: 4 * Float32Array.BYTES_PER_ELEMENT, // 4 float32 values
  usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC,
});

// Mapped buffer
const mappedBuffer = device.createBuffer({
  size: 4 * Float32Array.BYTES_PER_ELEMENT,
  usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST,
});

// Create shader that writes incrementing numbers to storage buffer
const computeShaderCode = `
    @group(0) @binding(0)
    var&lt;storage, read_write&gt; result : array&lt;f32&gt;;

    @compute @workgroup_size(1)
    fn main(@builtin(global_invocation_id) gid : vec3&lt;u32&gt;) {
      result[gid.x] = f32(gid.x);
    }
`;

// Create compute pipeline
const computePipeline = device.createComputePipeline({
  layout: "auto",
  compute: {
    module: device.createShaderModule({ code: computeShaderCode }),
    entryPoint: "main",
  },
});

// Bind group
const bindGroup = device.createBindGroup({
  layout: computePipeline.getBindGroupLayout(0),
  entries: [{ binding: 0, resource: { buffer: storageBuffer } }],
});

// Dispatch compute work
const computePass = encoder.beginComputePass();
computePass.setPipeline(computePipeline);
computePass.setBindGroup(0, bindGroup);
computePass.dispatchWorkgroups(4);
computePass.end();

// Copy from storage to mapped buffer
encoder.copyBufferToBuffer(
  storageBuffer,
  0,
  mappedBuffer,
  0,
  4 * Float32Array.BYTES_PER_ELEMENT //mappedBuffer.size
);

// Submit and read back result
const gpuBuffer = encoder.finish();
device.queue.submit([gpuBuffer]);

await mappedBuffer.mapAsync(GPUMapMode.READ);
console.log(new Float32Array(mappedBuffer.getMappedRange()));
// [0, 1, 2, 3]
</code></pre>
<!--kg-card-end: markdown--><p>Now that we covered the basics of WebGPU and compute shaders, let's move to something more demanding. What if we could perform machine learning inference using Workers and GPUs?</p><h3 id="onnx-webgpu-demo">ONNX WebGPU demo</h3><p>The <a href="https://github.com/microsoft/onnxruntime">ONNX runtime</a> is a popular open-source cross-platform, high performance machine learning inferencing accelerator. <a href="https://github.com/webonnx/wonnx">Wonnx</a> is a GPU-accelerated version of the same engine, written in Rust, that can be compiled to WebAssembly and take advantage of WebGPU in the browser. We are going to run it in Workers using a combination of <a href="https://github.com/cloudflare/workers-rs">workers-rs</a>, our Rust bindings for Cloudflare Workers, and the workerd WebGPU APIs.</p><p>For this demo, we are using <a href="https://www.kdnuggets.com/2016/09/deep-learning-reading-group-squeezenet.html">SqueezeNet</a>. This small image classification model can run under lower resources but still achieves similar levels of accuracy on the <a href="https://en.wikipedia.org/wiki/ImageNet">ImageNet</a> image classification validation dataset as larger models like <a href="https://en.wikipedia.org/wiki/AlexNet">AlexNet</a>.</p><p>In essence, our worker will receive any uploaded image and attempt to classify it according to the 1000 ImageNet classes. Once ONNX runs the machine learning model using the GPU, it will return the list of classes with the highest probability scores. Let’s go step by step.</p><p>First we load the model from R2 into the GPU memory the first time the Durable Object is called:</p><!--kg-card-begin: markdown--><pre><code>#[durable_object]
pub struct Classifier {
    env: Env,
    session: Option&lt;wonnx::Session&gt;,
}

impl Classifier {
    async fn ensure_session(&amp;mut self) -&gt; Result&lt;()&gt; {
        match self.session {
            Some(_) =&gt; worker::console_log!("DO already has a session"),
            None =&gt; {
                // No session, so this should be the first request. In this case
                // we will fetch the model from R2, build a wonnx session, and
                // store it for subsequent requests.
                let model_bytes = fetch_model(&amp;self.env).await?;
                let session = wonnx::Session::from_bytes(&amp;model_bytes)
                    .await
                    .map_err(|err| err.to_string())?;
                worker::console_log!("session created in DO");
                self.session = Some(session);
            }
        };
        Ok(())
    }
}
</code></pre>
<!--kg-card-end: markdown--><p>This is only required once, when the Durable Object is instantiated. For subsequent requests, we retrieve the model input tensor, call the existing session for the inference, and return to the calling worker the result tensor converted to JSON:</p><!--kg-card-begin: markdown--><pre><code>        let request_data: ArrayBase&lt;OwnedRepr&lt;f32&gt;, Dim&lt;[usize; 4]&gt;&gt; =
            serde_json::from_str(&amp;req.text().await?)?;
        let mut input_data = HashMap::new();
        input_data.insert("data".to_string(), request_data.as_slice().unwrap().into());

        let result = self
            .session
            .as_ref()
            .unwrap() // we know the session exists
            .run(&amp;input_data)
            .await
            .map_err(|err| err.to_string())?;
...
        let probabilities: Vec&lt;f32&gt; = result
            .into_iter()
            .next()
            .ok_or("did not obtain a result tensor from session")?
            .1
            .try_into()
            .map_err(|err: TensorConversionError| err.to_string())?;

        let do_response = serde_json::to_string(&amp;probabilities)?;
        Response::ok(do_response)
</code></pre>
<!--kg-card-end: markdown--><p>On the Worker script itself, we load the uploaded image and pre-process it into a model input tensor:</p><!--kg-card-begin: markdown--><pre><code>    let image_file: worker::File = match req.form_data().await?.get("file") {
        Some(FormEntry::File(buf)) =&gt; buf,
        Some(_) =&gt; return Response::error("`file` part of POST form must be a file", 400),
        None =&gt; return Response::error("missing `file`", 400),
    };
    let image_content = image_file.bytes().await?;
    let image = load_image(&amp;image_content)?;
</code></pre>
<!--kg-card-end: markdown--><p>Finally, we call the GPU Durable Object, which runs the model and returns the most likely classes of our image:</p><!--kg-card-begin: markdown--><pre><code>    let probabilities = execute_gpu_do(image, stub).await?;
    let mut probabilities = probabilities.iter().enumerate().collect::&lt;Vec&lt;_&gt;&gt;();
    probabilities.sort_unstable_by(|a, b| b.1.partial_cmp(a.1).unwrap());
    Response::ok(LABELS[probabilities[0].0])
</code></pre>
<!--kg-card-end: markdown--><p>We packaged this demo in a public repository, so you can also run it. Make sure that you have a <a href="https://www.rust-lang.org/">Rust</a> compiler, <a href="https://nodejs.org/en">Node.js</a>, <a href="https://git-scm.com/">Git</a> and <a href="https://curl.se/">curl</a> installed, then clone the repository:</p><!--kg-card-begin: markdown--><pre><code>git clone https://github.com/cloudflare/workers-wonnx.git
cd workers-wonnx
</code></pre>
<!--kg-card-end: markdown--><p>Upload the model to the local R2 simulator:</p><!--kg-card-begin: markdown--><pre><code>npx wrangler@latest r2 object put model-bucket-dev/opt-squeeze.onnx --local --file models/opt-squeeze.onnx
</code></pre>
<!--kg-card-end: markdown--><p>And then run the Worker locally:</p><!--kg-card-begin: markdown--><pre><code>npx wrangler@latest dev
</code></pre>
<!--kg-card-end: markdown--><p>With the Worker running and waiting for requests you can then open another terminal window and upload one of the image examples in the same repository using curl:</p><!--kg-card-begin: markdown--><pre><code>&gt; curl -F "file=@images/pelican.jpeg" http://localhost:8787
n02051845 pelican
</code></pre>
<!--kg-card-end: markdown--><p>If everything goes according to plan the result of the curl command will be the most likely class of the image.</p><h3 id="next-steps-and-final-words">Next steps and final words</h3><p>Over the upcoming weeks, we will merge the workerd WebGPU code in the Cloudflare Workers production environment and make it available globally, on top of our growing GPU nodes fleet. We didn't do it earlier because that environment is subject to strict security and isolation requirements. For example, we can't break the <a href="https://developers.cloudflare.com/workers/learning/security-model/">security model</a> of our process sandbox and have V8 talking to the GPU hardware directly, that would be a problem; we must create a configuration where another process is closer to the GPU and use IPC (inter-process communication) to talk to it. Other things like managing resource allocation and billing are being sorted out.</p><p>For now, we wanted to get the good news out that we will support WebGPU in Cloudflare Workers and ensure that you can start playing and coding with it today and learn from it. WebGPU and general-purpose computing on GPUs is still in its early days. We presented a machine-learning demo, but we can imagine other applications taking advantage of this new feature, and we hope you can show us some of them.</p><p>As usual, you can talk to us on our <a href="https://discord.cloudflare.com/">Developers Discord</a> or the <a href="https://community.cloudflare.com/c/developers/constellation/97">Community forum</a>; the team will be listening. We are eager to hear from you and learn about what you're building.</p>
            </div>
        </section>
    
        









    <div>
            <p>We protect
                <a target="_blank" href="https://www.cloudflare.com/network-services/">entire corporate networks</a>,
                    help customers build
                    <a target="_blank" href="https://workers.cloudflare.com/">Internet-scale applications efficiently</a>,
                    accelerate any
                    <a target="_blank" href="https://www.cloudflare.com/performance/accelerate-internet-applications/">website
                    or Internet application</a>,
                    <a target="_blank" href="https://www.cloudflare.com/ddos/">ward off DDoS
                    attacks</a>, keep
                    <a target="_blank" href="https://www.cloudflare.com/application-security/">hackers at
                    bay</a>,
                    and can help you on
                    <a target="_blank" href="https://www.cloudflare.com/products/zero-trust/">your journey to Zero Trust</a>.</p>
            <p>Visit <a target="_blank" href="https://1.1.1.1/">1.1.1.1</a> from any device to get started with
                our free app that makes your Internet faster and safer.</p>
            <p>To learn more about our mission to help build a better Internet, <a target="_blank" href="https://www.cloudflare.com/learning/what-is-cloudflare/">start here</a>. If you're looking for a
                new career direction, check out <a target="_blank" href="https://cloudflare.com/careers">our open
                    positions</a>.</p>
        </div>

        

        
        

        <a href="https://blog.cloudflare.com/tag/birthday-week/">Birthday Week</a>
        <a href="https://blog.cloudflare.com/tag/workers/">Cloudflare Workers</a>
        <a href="https://blog.cloudflare.com/tag/standards/">Standards</a>
        <a href="https://blog.cloudflare.com/tag/webgpu/">WebGPU</a>
    </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Philips allegedly hid 3700 complaints about sleep apnea machines from U.S. (159 pts)]]></title>
            <link>https://nltimes.nl/2023/09/27/philips-allegedly-hid-3700-complaints-sleep-apnea-machines-us-authority</link>
            <guid>37673539</guid>
            <pubDate>Wed, 27 Sep 2023 12:14:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nltimes.nl/2023/09/27/philips-allegedly-hid-3700-complaints-sleep-apnea-machines-us-authority">https://nltimes.nl/2023/09/27/philips-allegedly-hid-3700-complaints-sleep-apnea-machines-us-authority</a>, See on <a href="https://news.ycombinator.com/item?id=37673539">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
    
          <p>
                Wednesday, 27 September 2023 - 13:02
      </p>
    
        


  </div><div><p>Philips failed to report at least 3,700 complaints about its sleep apnea devices and respirators to the United States regulator FDA since 2010,<a href="https://www.propublica.org/article/philips-kept-warnings-about-dangerous-cpaps-secret-profits-soared"> NRC</a>, research collective <a href="https://www.nrc.nl/nieuws/2023/09/27/philips-verzweeg-al-sinds-2010-klachten-over-apneuapparaten-a4175460">ProPublica</a>, and the Pittsburgh Post Gazette reported based on their analysis of over 100,000 public reports.</p>

<p>According to the researchers, the first complaint came in 2010, shortly after Philips started using a sound-dampening foam in the breathing aids that <a href="https://nltimes.nl/2021/06/14/philips-recalls-millions-ventilators-sleep-apnea-machines">triggered a mass recall in 2021</a>. At the recall, Philips said the sound-dampening foam could disintegrate when it made contact with certain cleaning agents, resulting in users potentially breathing in carcinogenic substances.</p>

<p>United States law dictates that device makers must inform the government within 30 days if they receive reports of patient injuries, deaths, or malfunctions that could cause harm. In the 11 years between the launch of the sound-dampening foam and the recall, Philips received at least 3,700 such reports in the United States that it kept in its files and didn’t report to the FDA, according to the researchers.</p>

<p>The withheld complaints contain at least 10 reports about patients who may have died due to the use of a ventilator. Other reports described “black particles” or “dirt and dust” inside the machines. One described an “oily-like substance,” another spoke of “black shavings in the chamber,” and another said it was “contaminated with unknown sticky substance.” </p>

<p>U.S. law also states that device makers must immediately investigate reported faults that may cause harm. Philips launched its first investigation in 2019. In 2021, it recalled 15 million DreamStation <a href="https://nltimes.nl/2022/06/28/philips-issues-sleep-apnea-machines-less-harmful-thought">sleep apnea machines</a> and <a href="https://nltimes.nl/2023/02/10/us-agency-reveals-8000-new-reports-health-problems-tied-philips-ventilators">ventilators</a> that used the foam in question. </p>

<p>The FDA confirmed to the researchers that Philips had withheld many reports. According to the American regulator, Philips found “a large number of foam disintegration complaints that should have been sent to the FDA” during “retrospective reviews” following the recall. </p>

<p>A Philips spokesperson told NRC that “out of an abundance of caution,” the company sent old complaints that “may be related to the deteriorated foam” to the FDA despite “previously determining that these complaints did not need to be reported.” According to Philips, it was unaware of the scope of the problem because, until early 2021, the complaints were handled “one by one by Philips Respironics” - the Philips subsidiary in Pittsburg. </p>

<p>Because Philips held the complaints under wraps for over a decade, the regulator only intervened eleven years after the company got its first report. Through the spring of 2021, the FDA had only received 30 reports from the Pittsburg subsidiary of incidents with disintegrating foam. </p>

<p>The recall caused Philips serious financial troubles. Its stock price has halved, CEO Frans van Houten resigned, and his successor Roy Jakobs has announced two major rounds of layoffs. Lawyers in the United States are preparing mass claims amounting to billions of euros against the company. </p>

<p>Earlier this month, Philips announced that it had <a href="https://nltimes.nl/2023/09/07/philips-reaches-deal-us-faulty-breathing-devices">reached a settlement in the U.S.</a> to resolve <a href="https://nltimes.nl/2023/07/16/number-claims-damages-philips-us-increasing">over 500 pending lawsuits and damage claims</a> due to the recall. In April, <a href="https://nltimes.nl/2023/04/24/philips-sets-eu575-mil-aside-respirator-lawsuits-suffered-eu583-mil-loss-q1">Philips announced</a> that it set aside 575 million euros to cover the potential cost resulting from lawsuits and settlements in the country. </p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google judge rules trial documents can be posted by U.S. online (406 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2023-09-26/google-judge-rules-trial-documents-can-be-posted-by-us-online</link>
            <guid>37673413</guid>
            <pubDate>Wed, 27 Sep 2023 12:01:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2023-09-26/google-judge-rules-trial-documents-can-be-posted-by-us-online">https://www.bloomberg.com/news/articles/2023-09-26/google-judge-rules-trial-documents-can-be-posted-by-us-online</a>, See on <a href="https://news.ycombinator.com/item?id=37673413">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First Impressions with GPT-4V(ision) (314 pts)]]></title>
            <link>https://blog.roboflow.com/gpt-4-vision/</link>
            <guid>37673409</guid>
            <pubDate>Wed, 27 Sep 2023 12:01:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.roboflow.com/gpt-4-vision/">https://blog.roboflow.com/gpt-4-vision/</a>, See on <a href="https://news.ycombinator.com/item?id=37673409">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>On September 25th, 2023, <a href="https://help.openai.com/en/articles/6825453-chatgpt-release-notes?ref=blog.roboflow.com">OpenAI announced the rollout of two new features</a> that extend how people can interact with its recent and most advanced model, <a href="https://openai.com/research/gpt-4?ref=blog.roboflow.com">GPT-4</a>: the ability to ask questions about images and to use speech as an input to a query.</p><p>This functionality marks GPT-4’s move into being a <a href="https://blog.roboflow.com/multimodal-models/">multimodal model</a>. This means that the model can accept multiple “modalities” of input – text and images – and return results based on those inputs. Bing Chat, developed by Microsoft in partnership with OpenAI, and Google’s Bard model both support images as input, too. <a href="https://blog.roboflow.com/using-google-bard-with-images/">Read our comparison post to see how Bard and Bing perform with image inputs</a>.</p><p>In this guide, we are going to share our first impressions with the GPT-4V image input feature. We will run through a series of experiments to test the functionality of GPT-4V, showing where the model performs well and where it struggles.</p><p><em>Note: This article shows a limited series of tests our team performed; your results will vary depending on the questions you ask and the images you use in a prompt. Tag us on social media @roboflow with your findings using GPT-4V. We would love to see more tests using the model!</em></p><p>Without further ado, let’s get started!</p><h2 id="what-is-gpt-4v">What is GPT-4V?</h2><p><a href="https://cdn.openai.com/papers/GPTV_System_Card.pdf?ref=blog.roboflow.com">GPT-4V(ision)</a> (GPT-4V) is a multimodal model developed by OpenAI. GPT-4V allows a user to upload an image as an input and ask a question about the image, a task type known as visual question answering (VQA).</p><p>GPT-4V is rolling out as of September 24th and will be available in both the OpenAI ChatGPT iOS app and the web interface. You must have a GPT-4 subscription to use the tool.</p><p>Let’s experiment with GPT-4V and test its capabilities!</p><h2 id="test-1-visual-question-answering">Test #1: Visual Question Answering</h2><p>One of our first experiments with GPT-4V was to inquire about a computer vision meme. We chose this experiment because it allows us to the extent to which GPT-4V understands context and relationships in a given image.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-17.25.07-1.jpg" alt="" loading="lazy" width="590" height="1280"></figure><p>GPT-4V was able to successfully describe why the image was funny, making reference to various components of the image and how they connect. Notably, the provided meme contained text, which GPT-4V was able to read and use to generate a response. With that said, GPT-4V did make a mistake. The model said the fried chicken was labeled “NVIDIA BURGER” instead of “GPU”.</p><p>We then went on to test GPT-4V with currency, running a couple of different tests. First, we uploaded a photo of a United States penny. GPT-4V was able to successfully identify the origin and denomination of the coin:</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/Screenshot-2023-09-26-at-19.36.14-1.png" alt="" loading="lazy" width="1258" height="1224" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/Screenshot-2023-09-26-at-19.36.14-1.png 600w, https://blog.roboflow.com/content/images/size/w1000/2023/09/Screenshot-2023-09-26-at-19.36.14-1.png 1000w, https://blog.roboflow.com/content/images/2023/09/Screenshot-2023-09-26-at-19.36.14-1.png 1258w" sizes="(min-width: 720px) 720px"></figure><p>We then uploaded an image with multiple coins and prompted GPT-4V with the text: “How much money do I have?”</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-17.56.29.jpg" alt="" loading="lazy" width="826" height="1280" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/2023-09-26-17.56.29.jpg 600w, https://blog.roboflow.com/content/images/2023/09/2023-09-26-17.56.29.jpg 826w" sizes="(min-width: 720px) 720px"></figure><p>GPT-4V was able to identify the number of coins but did not ascertain the currency type. With a follow up question, GPT-4V successfully identified the currency type:</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-18.00.56.jpg" alt="" loading="lazy" width="1179" height="939" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/2023-09-26-18.00.56.jpg 600w, https://blog.roboflow.com/content/images/size/w1000/2023/09/2023-09-26-18.00.56.jpg 1000w, https://blog.roboflow.com/content/images/2023/09/2023-09-26-18.00.56.jpg 1179w" sizes="(min-width: 720px) 720px"></figure><p>Moving on to another topic, we decided to try using GPT-4V with a photo from a popular movie: Pulp Fiction. We wanted to know: could GPT-4 answer a question about the movie without being told in text what movie it was?</p><p>We uploaded a photo from Pulp Fiction with the prompt “Is it a good movie?”, to which GPT-4V responded with a description of the movie and an answer to our question. GPT-4V provides a high-level description of the movie and a summary of the attributes associated with the movie considered to be positive and negative.</p><p>We further asked about the IMDB score for the movie, to which GPT-4V responded with the score as of January 2022. This suggests, like other GPT models released by OpenAI, there is a knowledge cutoff after which point the model has no more recent knowledge.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-18.13.51.jpg" alt="" loading="lazy" width="1179" height="848" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/2023-09-26-18.13.51.jpg 600w, https://blog.roboflow.com/content/images/size/w1000/2023/09/2023-09-26-18.13.51.jpg 1000w, https://blog.roboflow.com/content/images/2023/09/2023-09-26-18.13.51.jpg 1179w" sizes="(min-width: 720px) 720px"></figure><p>We then explored GPT-4V’s question answering capabilities by asking a question about a place. We uploaded a photo of San Francisco with the text prompt “Where is this?” GPT-4V successfully identified the location, San Francisco, and noted that the Transamerica Pyramid, pictured in the image we uploaded, is a notable landmark in the city.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/Screenshot-2023-09-26-at-19.39.34.png" alt="" loading="lazy" width="764" height="714" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/Screenshot-2023-09-26-at-19.39.34.png 600w, https://blog.roboflow.com/content/images/2023/09/Screenshot-2023-09-26-at-19.39.34.png 764w" sizes="(min-width: 720px) 720px"></figure><p>Moving over to the realm of plants, we provided GPT-4V with a photo of a peace lily and asked the question “What is that plant and how should I care about it?”:</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-27-13.06.19.jpg" alt="" loading="lazy" width="711" height="1280" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/2023-09-27-13.06.19.jpg 600w, https://blog.roboflow.com/content/images/2023/09/2023-09-27-13.06.19.jpg 711w"></figure><p>The model successfully identified that the plant is a peace lily and provided advice on how to care for the plant. This illustrates the utility of having text and vision combined to create a multi-modal such as they are in GPT-4V. The model returned a fluent answer to our question without having to build our own two-stage process (i.e. classification to identify the plant then GPT-4 to provide plant care advice).</p><h2 id="test-2-optical-character-recognition-ocr">Test #2: Optical Character Recognition (OCR)</h2><p>We conducted two tests to explore GPT-4V’s OCR capabilities: OCR on an image with text on a car tire and OCR on a photo of a paragraph from a digital document. Our intent was to build an understanding of how GPT-4V performs at OCR in the wild, where text may have less contrast and be at an angle, versus digital documents with clear text.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-17.36.09-1.jpg" alt="" loading="lazy" width="590" height="605"></figure><p><br>GPT-4V was unable to correctly identify the serial number in an image of a tire. Some numbers were correct but there were several errors in the result from the model.</p><p>In our document test, we presented text from a web page and asked GPT-4V to read the text in the image. The model was able to successfully identify the text in the image.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/File.jpg" alt="" loading="lazy" width="738" height="1600" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/File.jpg 600w, https://blog.roboflow.com/content/images/2023/09/File.jpg 738w" sizes="(min-width: 720px) 720px"></figure><p>GPT-4V does an excellent job translating words in an image to individual characters in text. A useful insight for tasks related to extracting text from documents.</p><h2 id="test-3-math-ocr">Test #3: Math OCR</h2><p>Math OCR is a specialized form of OCR pertaining specifically to math equations. Math OCR is often considered its own discipline because the syntax of what the OCR model needs to identify extends to a vast range of symbols.</p><p>We presented GPT-4V with a math question. This math question was in a screenshot taken from a document. The question concerns calculating the length of a zip wire given two angles. We presented the image with the prompt “Solve it.”</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-27-13.25.51.jpg" alt="" loading="lazy" width="590" height="1280"></figure><figure><img src="https://blog.roboflow.com/content/images/2023/09/photo_2023-09-27-13.25.55.jpeg" alt="" loading="lazy" width="590" height="1280"></figure><p>The model identified the problem can be solved with trigonometry, identified the function to use, and presented a step-by-step walkthrough of how to solve the problem. Then, GPT-4V provided the correct answer to the question.</p><p>With that said, the GPT-4V system card notes that the model may miss mathematical symbols. Different tests, including tests where an equation or expression is written by hand on paper, may indicate deficiencies in the model's ability to answer math questions. </p><h2 id="test-4-object-detection">Test #4: Object Detection</h2><p><a href="https://blog.roboflow.com/object-detection/">Object detection</a> is a fundamental task in the field of computer vision. We asked GPT-4V to identify the location of various objects to evaluate its ability to perform object detection tasks.</p><p>In our first test, we asked GPT-4V to detect a dog in an image and provide the x_min, y_min, x_max, and y_max values associated with the position of the dog. The bounding box coordinates returned by GPT-4V did not match the position of the dog.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/photo_2023-09-26-18.51.24.jpeg" alt="" loading="lazy" width="590" height="1280"></figure><p>While GPT-4V’s capabilities at answering questions about an image are powerful, the model is not a substitute for fine-tuned <a href="https://roboflow.com/models/object-detection?ref=blog.roboflow.com">object detection models</a> in scenarios where you want to know where an object is in an image.</p><h2 id="test-5-captcha">Test #5: CAPTCHA</h2><p>We decided to test GPT-4V with CAPTCHAs, a task OpenAI studied in their research and wrote about in their <a href="https://cdn.openai.com/papers/GPTV_System_Card.pdf?ref=blog.roboflow.com">system card</a>. We found that GPT-4V was able to identify that an image contained a CAPTCHA but often failed the tests. In a traffic light example, GPT-4V missed some boxes that contained traffic lights.</p><figure><img src="https://blog.roboflow.com/content/images/2023/09/photo_2023-09-27-13.01.22.jpeg" alt="" loading="lazy" width="1031" height="1280" srcset="https://blog.roboflow.com/content/images/size/w600/2023/09/photo_2023-09-27-13.01.22.jpeg 600w, https://blog.roboflow.com/content/images/size/w1000/2023/09/photo_2023-09-27-13.01.22.jpeg 1000w, https://blog.roboflow.com/content/images/2023/09/photo_2023-09-27-13.01.22.jpeg 1031w" sizes="(min-width: 720px) 720px"></figure><p>In the following crosswalk example, GPT-4V classified a few boxes correctly but incorrectly classified one box in the CAPTCHA as a crosswalk.</p><figure><img src="https://lh4.googleusercontent.com/sUn71XmNZHeS4C9U1KGZm9T12MPiDaWSnjeqqZXSTan3I01VVBMvJ0_8knDTQW6kO1YJS8jLXswk_zEyINNDQz7mwDT60e_NoKrikqwaKuULsM9upmURmKCZ7STF6INGj4FtvEY3jlIjvgpVi1eamCI" alt="" loading="lazy" width="248" height="325"></figure><h2 id="test-6-crosswords-and-sudokus">Test #6: Crosswords and Sudoku's</h2><p>We decided to test how GPT-4V performs on crosswords and sudokus.</p><p>First, we prompted GPT-4V with photos of a crossword with the text instruction "Solve it." GPT-4V inferred the image contained a crossword and attempted to provide a solution to the crossword. The model appeared to read the clues correctly but misinterpreted the structure of the board. As a result, the provided answers were incorrect.</p><figure><img src="https://lh6.googleusercontent.com/bXAg1SiRBcs-huLBicWFzkeKI8NxB5OE1zoa1cAvC8sqfU1aFmZ2MRDKd2PTKxafivJsaY3R189vJYPEx0BzrXyWwy5ta2TEaGU2yKrBrOxqCYiQhAM93N4SDvZu6Wb7S3lCGaB2j9PxUCvuqbWD8os" alt="" loading="lazy" width="272" height="592"></figure><p>This same limitation was exhibited in our sudoku test, where GPT-4V identified the game but misunderstood the structure of the board and thus returned inaccurate results:</p><figure><img src="https://lh4.googleusercontent.com/U9cH5wYei3jZN8mmAA6etp3ngH8Zu0YrpLisXW6CEO0uSDB-FW3UO7PDLm-u5sEwc6Isvvh3BP_qizYEZctgWRUQpt8oP2_ius6vKGvUmTmAdcn6eneWiAOgq1O6n2W1LV7rx6a6hmDXLxrHs7IkxZI" alt="" loading="lazy" width="431" height="936"></figure><h2 id="gpt-4v-limitations-and-safety">GPT-4V Limitations and Safety</h2><p>OpenAI conducted research with an alpha version of the vision model available to a small group of users, as outlined in the official <a href="https://cdn.openai.com/papers/GPTV_System_Card.pdf?ref=blog.roboflow.com">GPT-4V(ision) System Card</a>. During this process, they were able to gather feedback and insights on how GPT-4V works with prompts provided by a range of people. This was supplemented with “red teaming”, wherein external experts were “to qualitatively assess the limitations and risks associated with the model and system”.</p><p>Based on OpenAI’s research, the GPT-4V system card notes numerous limitations with the model such as:</p><ol><li>Missing text or characters in an image</li><li>Missing mathematical symbols</li><li>Being unable to recognize spatial locations and colors</li></ol><p>In addition to limitations, OpenAI identified, researched, and attempted to mitigate several risks associated with the model. For example, GPT-4V avoids identifying a specific person in an image and does not respond to prompts pertaining to hate symbols.</p><p>With that said, there is further work to be done in model safeguarding. For example, OpenAI notes in the model system card that “If prompted, GPT-4V can generate content praising certain lesser known hate groups in response to their symbols.”,</p><h2 id="gpt-4v-for-computer-vision-and-beyond">GPT-4V for Computer Vision and Beyond</h2><p>GPT-4V is a notable movement in the field of machine learning and natural language processing. With GPT-4V, you can ask questions about an image – and follow up questions – in natural language and the model will attempt to ask your question.</p><p>GPT-4V performed well at various general image questions and demonstrated awareness of context in some images we tested. For instance, GPT-4V was able to successfully answer questions about a movie featured in an image without being told in text what the movie was.</p><p>For general question answering, GPT-4V is exciting. While models existed for this purpose in the past, they often lacked fluency in their answers. GPT-4V is able to both answer questions and follow up questions about an image and do so in depth.</p><p>With GPT-4V, you can ask questions about an image without creating a two-stage process (i.e. classification then using the results to ask a question to a language model like GPT). There will likely be limitations to what GPT-4V can understand, hence testing a use case to understand how the model performs is crucial.</p><p>With that said, GPT-4V has its limitations. The model did “hallucinate”, wherein the model returned inaccurate information. This is a risk with using language models to answer questions. Furthermore, the model was unable to accurately return bounding boxes for object detection, suggesting it is unfit for this use case currently.</p><p>We also observed that GPT-4V is unable to answer questions about people. When given a photo of Taylor Swift and asked who was featured in the image, the model declined to answer. OpenAI define this as an expected behavior in the published system card.</p><p>Interested in reading more of our experiments with multi-modal language models and GPT-4’s impact on computer vision? Check out the following guides:</p><ul><li><a href="https://blog.roboflow.com/gpt-4-impact-speculation/">Speculating on How GPT-4 Changes Computer Vision</a> (<a href="https://www.youtube.com/watch?v=aNLl0wEdMq4&amp;ref=blog.roboflow.com">Video</a>)</li><li><a href="https://blog.roboflow.com/how-good-is-bing-gpt-4-multimodality/">How Good Is Bing (GPT-4) Multimodality?</a></li><li><a href="https://blog.roboflow.com/chatgpt-code-interpreter-computer-vision/">ChatGPT Code Interpreter for Computer Vision</a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Be My Eyes’ AI assistant starts rolling out (252 pts)]]></title>
            <link>https://www.bemyeyes.com/blog/announcing-be-my-ai</link>
            <guid>37673300</guid>
            <pubDate>Wed, 27 Sep 2023 11:48:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bemyeyes.com/blog/announcing-be-my-ai">https://www.bemyeyes.com/blog/announcing-be-my-ai</a>, See on <a href="https://news.ycombinator.com/item?id=37673300">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Since 2015, Be My Eyes has worked to connect our 6.9 million volunteers to users to assist them with everyday tasks. Our mission is to make the world more accessible for people who are blind or have low vision, which is why, seven months ago, our team began working with the blind community to incorporate AI into the existing Be My Eyes platform. Since then, over 19,000 blind and low-vision beta testers contributed to the design and function of our new AI feature.</p><p>Today we are thrilled to announce that Be My AI is officially entering an open beta phase for iOS users and in coming weeks will be available for hundreds of thousands of Be My Eyes users worldwide.&nbsp;</p><p>We’ll start releasing Be My AI to all our existing iPhone users this week. The full roll-out will take a few weeks, so be sure to keep your app updated so you will have access to Be My AI as soon as it is available to you.</p><h2>How to access and use Be My AI</h2><p>Using Be My AI in your everyday life is quick and simple. Once you have access, open the Be My Eyes app, click on the ‘Be My AI’ tab, and take a picture. Be My AI will give you a detailed description about it, and you can chat and ask Be My AI further questions to get more information. If you like what Be My AI described, you can send its response and photo to others, or use its description in social media.</p><p>And don’t worry - if Be My AI can’t answer all your questions, if you want to check its results, or if you just need a little more description than Be My AI can provide or crave the magic and humanity of working with people, you still can easily reach one of our dedicated volunteers, just like before. They will always be there, in 150 languages all across the globe.</p><p>If you want to learn more about Be My AI and how to use it at its best, we have collected the most common questions (and answers!) in our <a href="https://support.bemyeyes.com/hc/en-us/articles/18133134809105-How-do-I-use-Be-My-AI-beta-">Help Center</a>. Make sure to check them out!</p><h2>When to use Be My AI</h2><p>You can use Be My AI 24/7 in all those situations when you want quick visual assistance without necessarily calling a human volunteer. Be My AI is perfect for all those circumstances when you want a quick solution or you don’t feel like talking to another person to get visual assistance. You may be amazed that Be My AI knows more than just what’s in the photo – just ask for more context and discover what it can tell you.</p><p>Be My AI also will give deaf-blind users a new way to get information if they use, for example, a braille display. Be My AI's written responses are user-selectable in 29 languages.</p><p>For all of its advantages, though, Be My AI does not and should not replace a white cane, guide dog, or other mobility aid that provides for safe travel.&nbsp;</p><blockquote><em>“I have been using it in several ways: taking my own photos particularly of images in magazines, on Twitter where very few people add descriptions or alt text and on WhatsApp where my family send me photos in groups all the time without any context. I was unsure about using Chat GPT which I’d seen many blind people using on social media, but when I saw that you were adding it I thought I’d give it a go!” - Sarah, Be My AI User</em></blockquote><p>Over the past few months, our blind and low vision beta testers have experimented with Be My AI and discovered many different ways to use it throughout your day from learning how to use new breakfast products in the morning to making sure your light is off before going to bed! We have collected a bunch of real life examples directly from their experiences to inspire you and show you what Be My AI can do:</p><ul role="list"><li>Get information about a popcorn box and access to cooking directions.</li><li>Read buttons on your dishwasher, washing machine, and other appliances with flat-screen controls.</li><li>Re-organize your wardrobe or create the perfect outfit for a night out.</li><li>Read instructions to set up your new laptop, smartphone, or tablet.</li><li>Read comics, books, and magazines.</li><li>Set up your Apple TV, Chromecast, or Amazon Fire Stick.</li><li>Find something that you accidentally dropped on the floor.</li><li>Get descriptions of memes from Facebook, X, Instagram and Mastodon.</li><li>Take pictures of paintings, statues, and other artwork to get detailed descriptions of them. You can also get pictures from your holidays and special events described to you.</li><li>Read the number of your bus at the bus station, or check out the departures screen at the train station or at the airport.</li><li>Read the menu at the restaurant and get relevant information from your receipt.</li><li>Translate text from dozens of different languages.</li><li>Prepare for a university exam or get assistance with your homework.</li><li>Check your makeup and identify beauty products while getting ready to go out.</li></ul><blockquote><em>“There aren’t enough words in the world to express what a truly miraculous, life-changing, day-making thing Be My Eyes and especially Be My AI is. Now I’m actually looking forward to organizing my closet because I won’t need human help. It describes my clothes in a way that makes them all sound gorgeous!” - Aimee, Be My AI User</em></blockquote><h2>What is an “Open Beta”?</h2><p>“Open beta” status just means we are opening up Be My AI to all our iOS users while we continue developing it and making it better based on your feedback. AI image recognition is complicated, and AI in general is still a rapidly evolving technology just in its infancy. Be My Eyes - and OpenAI - are constantly learning how to improve it.</p><p>There will be hiccups. Things may break. You may still get hallucinations, occasional wrong answers, and experience some frustrations. But we will be here to fix the breaks and keep making things better. So please be patient, and keep telling us about your experiences, positive and negative, so we can make this the best possible tool for you.</p><h2>What’s next: Android</h2><p>We started closed beta testing Be My AI for Android devices last week, and our goal is to move it to broadly-available open beta status in the coming months. You can already <a href="https://play.google.com/store/apps/details?id=com.bemyeyes.bemyeyes">sign up on the waitlist for Android beta testing</a> directly in the Be My Eyes app.</p><p>Here at Be My Eyes, the pace of innovation is accelerating. To keep updated about Be My AI, make sure to follow Be My Eyes on <a href="https://www.linkedin.com/company/be-my-eyes/?originalSubdomain=dk">LinkedIn</a>, <a href="https://twitter.com/BeMyEyes?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">X</a>, <a href="https://www.facebook.com/bemyeyesapp/">Facebook</a>, <a href="https://www.instagram.com/bemyeyesapp/?hl=en">Instagram</a>, <a href="https://mastodon.social/@bemyeyes">Mastodon</a> and <a href="https://www.tiktok.com/@bemyeyesapp?lang=en">TikTok</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Uiua: A minimal stack-based, array-based language (158 pts)]]></title>
            <link>https://www.uiua.org/</link>
            <guid>37673127</guid>
            <pubDate>Wed, 27 Sep 2023 11:28:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.uiua.org/">https://www.uiua.org/</a>, See on <a href="https://news.ycombinator.com/item?id=37673127">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>Uiua <span>(<i>wee-wuh</i>)</span> is a
        stack-oriented array programming language with a focus on simplicity, beauty, and <a href="https://en.wikipedia.org/wiki/Tacit_programming">tacit</a> code.</p>
      
      <h3>Loading...</h3>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I got robbed of my first kernel contribution (652 pts)]]></title>
            <link>https://ariel-miculas.github.io/How-I-got-robbed-of-my-first-kernel-contribution/</link>
            <guid>37671991</guid>
            <pubDate>Wed, 27 Sep 2023 08:58:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ariel-miculas.github.io/How-I-got-robbed-of-my-first-kernel-contribution/">https://ariel-miculas.github.io/How-I-got-robbed-of-my-first-kernel-contribution/</a>, See on <a href="https://news.ycombinator.com/item?id=37671991">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <h3 id="context">Context</h3>
<p>Around a year and a half ago, I’ve asked my former company for some time to
work on an issue that was impacting the debugging capabilities in our project:
gdbserver couldn’t debug multithreaded applications running on a PowerPC32
architecture.  The connection to the gdbserver was broken and it couldn’t
control the debug session anymore. Multiple people have already investigated
this problem and I had a good starting point, but we still weren’t sure in
which software component the issue lied: it could have been the toolchain, the
gdbserver, the Linux kernel or the custom patches we applied on top of the
kernel tree. We were quite far away from finding the root cause.</p>

<h3 id="investigating-the-issue">Investigating the issue</h3>
<p>After diving into the existing analysis for this issue and channeling my
google-fu, I’ve had my first breakthrough: an <a href="https://lore.kernel.org/linuxppc-dev/dc38afe9-6b78-f3f5-666b-986939e40fc6@keymile.com/">email
thread</a>
which not only described the same symptoms as our issue, but also pointed to
the <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v6.6-rc3&amp;id=0c8c0f03e3a292e031596484275c14cf39c0ab7a">exact
commit</a>
which introduced it. The patch that introduced the bug moved the definition of
<code>thread_struct thread</code> from the middle of the <code>task_struct</code> to the end, a
seeminlgy innocuous change.</p>

<p>After debugging the issue, this is what Holger Brunck
<a href="https://lore.kernel.org/linuxppc-dev/e5cbd015-eeb5-31b5-0829-14cc8500dc6d@keymile.com/">observed</a></p>
<blockquote>
  <p>What I see is that gdbserver sends for each thread a SIGSTOP to the kernel and
waits for a response. The kernel does receive all the signals but only respond
to some of them in the error case. Which then matches with my “ps” output as I
see that some threads are not in the state pthread_stop and then the gdbserver
gets suspended.</p>
</blockquote>

<p>The low-level issue was that after interacting with gdbserver, some threads
were in the wrong process state and gdbserver couldn’t control them anymore.</p>

<p>I’ve spent 3-4 days reading commit descriptions related to the PowerPC
architecture and the changes around <code>task_struct</code>, trying to figure out whether
this issue was solved in subsequent kernel versions (spoiler: it was not).
I’ve moved <code>thread_struct thread</code> around to determine when the issue reproduced
and used <a href="https://linux.die.net/man/1/pahole">pahole</a> to inspect
<code>task_struct</code>’s layout. I’ve used
<a href="https://www.kernel.org/doc/html/v5.0/trace/ftrace.html">ftrace</a> to figure out
when the threads of the debugged process were scheduled and that’s how I
realized this could be a memory corruption issue: the threads that were stuck
were only scheduled once, unlike the other ones. I’ve originally dismissed that
this could be a memory corruption issue because in the <a href="https://lore.kernel.org/linuxppc-dev/b78d9e5d-fc2e-3676-a47e-ed5ca7a836e6@keymile.com/">original
thread</a>
it was mentioned that:</p>
<blockquote>
  <p>the content of the buffer is always zero and does not change. So at least no
one is writing non-zero to the buffer.</p>
</blockquote>

<p>That’s what I get for not verifying that the structure isn’t overwritten with
zero bytes (always validate your assumptions).</p>

<p>I remembered that the x86 architecture has <a href="https://en.wikipedia.org/wiki/X86_debug_register">debug
registers</a> that could be used
to trigger data write breakpoints. In fact, this is how I solved a bug back in
my earlier days as a software engineer. Sure enough, PowerPC also implements a
similar capability with the help of the <a href="https://stackoverflow.com/a/327540">DABR register</a>.</p>

<p>I’ve investigated how I could use hardware breakpoints on Linux and I ended up
implementing a linux kernel module based on this <a href="https://stackoverflow.com/a/19755213">excellent stackoverflow
answer</a>. This allowed me to place a
hardware breakpoint on the <a href="https://elixir.bootlin.com/linux/v6.5.5/source/include/linux/sched.h#L746">__state
field</a>
to figure out who on earth writes to it.</p>

<h3 id="finding-the-bug">Finding the bug</h3>
<p>And that’s how I found the issue: my custom kernel module showed the stack
traces from the places where the <code>__state</code> field of <code>task_struct</code> was being
written to.  I’ve noticed an outlier which revealed a buffer overflow in
<code>ptrace_put_fpr</code> (used by the POKEUSER API). This led to important fields from
<code>task_struct</code> getting overwritten, such as <code>__state</code>, which stores the state of
the process and it’s also used by the kernel to keep track of which processes
are stopped by the debugger.</p>

<p>The cause of this overflow? Taking an index meant to be used with an array of
32-bit elements and indexing an array of 64-bit elements. There were 64 indexes
that addressed the FPR, so the total addressable memory was 64 * 8 = 512
bytes. But there were only 32 entries in the fp_state.fpr array, which means
that the available memory was only 32 * 8 = 256 bytes. That allowed the user
(aka gdbserver) to write up to 256 bytes past the end of the array.
<img src="https://ariel-miculas.github.io/images/fpr-overflow.png" alt="fpr-overflow"></p>

<h3 id="sending-the-patch-upstream">Sending the patch upstream</h3>
<p>I’ve sent a patch to the Linux kernel security team (security@kernel.org)
because I wanted to err on the safe side: a memory corruption issue that could
overwrite the memory of the processes’s states could have security
implications. Unfortunately, this mailing list is private so I cannot link to
the original patch I sent.  Michael Ellerman, the PowerPC maintainer, followed
up and told me he will contact me in private to figure this issue out. I have
actually sent him two patches fixing the issue: the original one that I sent to
the security mailing list and <a href="https://lists.ozlabs.org/pipermail/linuxppc-dev/2022-June/244438.html">another
version</a>
(quite different from the first one) which addressed some suggestions received
in reply to my original submission. And the latter patch was actually based on
existing kernel code, which emulated PowerPC32 operations on PowerPC64 (yeah,
they got the FPR indexing right). Neither of those were accepted by Michael
Ellerman, and instead he implemented his <a href="https://lore.kernel.org/all/20220609133245.573565-1-mpe@ellerman.id.au/">own version of the
fix</a>.
I told him that I would really appreciate if he could accept a patch from me,
so that I could receive credit for fixing this issue and become a kernel
contributor. I was also open to working with him, addressing his feedback and
sending subsequent versions of patches. He said (paraphrasing):</p>
<blockquote>
  <p>Sorry, I like my version better. If you want to be a Linux kernel
contributor, here’s an issue you could fix.</p>
</blockquote>

<p>I found this really perplexing and insulting. Instead of getting recognized for
fixing the issue, he wanted to give me more work to do. My company and I should
have received proper credit for solving this issue, especially considering how
much effort we put into it.</p>

<p>I felt it was really unfair to only get a “Reported-by” tag. Here’s the
<a href="https://docs.kernel.org/process/submitting-patches.html#using-reported-by-tested-by-reviewed-by-suggested-by-and-fixes">purpose of the tag</a>:</p>

<blockquote>
  <p>The Reported-by tag gives credit to people who find bugs and report them and it hopefully inspires them to help us again in the future.</p>
</blockquote>

<p>Well, I certainly didn’t feel inspired to get involved with the kernel
community again. On the contrary, I felt belittled and angry that my work
wasn’t properly recognized.</p>

<h3 id="conclusion">Conclusion</h3>
<p>I spent a lot of time and effort doing root cause analysis, fixing the bug,
testing and validating the fix, getting feedback from other engineers at my
company, adapting the fix to the latest kernel version, and sending two
different patches to Michael Ellerman, the PowerPC maintainer. Instead of
accepting my patch or guiding me towards a better solution, he went ahead and
implemented his own fix, giving me credit only for reporting the issue (which
was <a href="https://lore.kernel.org/linuxppc-dev/dc38afe9-6b78-f3f5-666b-986939e40fc6@keymile.com/">already
reported</a>
six years prior to this).</p>

<p>My first contribution to the kernel was a really frustrating and discouraging
experience, dealing with people who do not think it’s important to get proper
recognition for your work.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Essence: A desktop OS built from scratch, for control and simplicity (392 pts)]]></title>
            <link>https://nakst.gitlab.io/essence</link>
            <guid>37671419</guid>
            <pubDate>Wed, 27 Sep 2023 07:44:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nakst.gitlab.io/essence">https://nakst.gitlab.io/essence</a>, See on <a href="https://news.ycombinator.com/item?id=37671419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p>
						Efficient with resources.
					</p>

					

					<p>
						Essence will happily run on low-powered hardware. It can take less than 30MB of drive space, and boot with even less RAM. No tasks run in the background, giving your applications all the space they need.
					</p>

					

					<p><img src="https://nakst.gitlab.io/screenshot4.png">
				</p></div><div>
					<p>
						Open source.
					</p>

					

					<p>
						All the code is made available under the MIT license. You can browse through the source on the <a href="https://gitlab.com/nakst/essence">GitLab repository</a>.
					</p>

					

					<p>
						If you're interested in contributing, join our <a href="https://discord.gg/skeP9ZGDK8">Discord server</a> to discuss ideas with other developers.
					</p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A JavaScript function that looks and behaves like a pipe operator (105 pts)]]></title>
            <link>https://github.com/laurentpayot/verticalize</link>
            <guid>37671341</guid>
            <pubDate>Wed, 27 Sep 2023 07:31:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/laurentpayot/verticalize">https://github.com/laurentpayot/verticalize</a>, See on <a href="https://news.ycombinator.com/item?id=37671341">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content--verticalize" dir="auto"><a href="#-verticalize"><sub><img src="https://github.com/laurentpayot/verticalize/raw/main/verticalize.svg" alt="triple chevron down" width="48" height="48"></sub> Verticalize</a></h2>
<p dir="auto">A pipe-like function to verticalize your JavaScript code</p>

<p dir="auto"><a href="https://github.com/laurentpayot/verticalize/blob/main/package.json#L56"><img src="https://camo.githubusercontent.com/5f7c6a59930fcd08aaa2c68c72ba3a9387164ccf2d20a55f7c422bbe9bb3dbac/68747470733a2f2f62616467656e2e6e65742f7374617469632f646570656e64656e636965732f4e6f6e652f677265656e" alt="dependencies" data-canonical-src="https://badgen.net/static/dependencies/None/green"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a77cecc88f9cbbda946d45a207027d16f6f1195a686b10bfbb4436efbdd4bbf7/68747470733a2f2f62616467656e2e6e65742f626164676573697a652f62726f746c692f6c617572656e747061796f742f766572746963616c697a652f6d61696e2f766572746963616c697a652e6d696e2e6a73"><img src="https://camo.githubusercontent.com/a77cecc88f9cbbda946d45a207027d16f6f1195a686b10bfbb4436efbdd4bbf7/68747470733a2f2f62616467656e2e6e65742f626164676573697a652f62726f746c692f6c617572656e747061796f742f766572746963616c697a652f6d61696e2f766572746963616c697a652e6d696e2e6a73" alt="minified + brotlied size" data-canonical-src="https://badgen.net/badgesize/brotli/laurentpayot/verticalize/main/verticalize.min.js"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a7b8c389b3a8012d017c9a8ca9223e6f21eca0c7fa734a237955ef707ddefaee/68747470733a2f2f62616467656e2e6e65742f626164676573697a652f677a69702f6c617572656e747061796f742f766572746963616c697a652f6d61696e2f766572746963616c697a652e6d696e2e6a73"><img src="https://camo.githubusercontent.com/a7b8c389b3a8012d017c9a8ca9223e6f21eca0c7fa734a237955ef707ddefaee/68747470733a2f2f62616467656e2e6e65742f626164676573697a652f677a69702f6c617572656e747061796f742f766572746963616c697a652f6d61696e2f766572746963616c697a652e6d696e2e6a73" alt="minified + zipped size" data-canonical-src="https://badgen.net/badgesize/gzip/laurentpayot/verticalize/main/verticalize.min.js"></a></p>
<p dir="auto"><a href="https://github.com/laurentpayot/verticalize/blob/main/index.d.ts"><img src="https://camo.githubusercontent.com/65d29de6c7efead9588acac34c7c6154d7e0c2bcf003aa47f7e13a8a909a51b2/68747470733a2f2f62616467656e2e6e65742f6e706d2f74797065732f766572746963616c697a65" alt="types" data-canonical-src="https://badgen.net/npm/types/verticalize"></a>
<a href="https://www.npmjs.com/package/verticalize" rel="nofollow"><img src="https://camo.githubusercontent.com/bdeb60a81e1ad0cd88137dc6fd119447e24cc2430eb0949ae4788976e5f21894/68747470733a2f2f62616467656e2e6e65742f6e706d2f762f766572746963616c697a65" alt="npm" data-canonical-src="https://badgen.net/npm/v/verticalize"></a>
<a href="https://github.com/laurentpayot/verticalize/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/eb0514d18fdb21aadbb299309ccae39d3914f13dd27ee6446039c263d6f1084e/68747470733a2f2f62616467656e2e6e65742f6769746875622f6c6963656e73652f6c617572656e747061796f742f766572746963616c697a65" alt="license" data-canonical-src="https://badgen.net/github/license/laurentpayot/verticalize"></a></p>
<h2 tabindex="-1" id="user-content-gist" dir="auto"><a href="#gist">Gist</a></h2>
<p dir="auto">The following example code is a bit hard to read:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const { status } = await send(capitalize(greeting) + &quot;!&quot;)
console.log(status)"><pre><span>const</span> <span>{</span> status <span>}</span> <span>=</span> <span>await</span> <span>send</span><span>(</span><span>capitalize</span><span>(</span><span>greeting</span><span>)</span> <span>+</span> <span>"!"</span><span>)</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>status</span><span>)</span></pre></div>
<p dir="auto">Make it less nested, more <em>vertical</em>, by using the <code>V</code> "pipe":</p>
<div dir="auto" data-snippet-clipboard-copy-content="V( greeting,     // initial value ➡ &quot;hi&quot;
V (capitalize),  // custom function call ➡ &quot;Hi&quot;
V .concat(&quot;!&quot;),  // String method `concat` call ➡ &quot;Hi!&quot;
V (send),        // custom async function call ➡ Promise { <pending> }
V .status,       // automatic promise chaining + getting property ➡ Promise { 200 }
V (console.log), // automatic promise chaining + global function call ➡ logs 200
)"><pre><span>V</span><span>(</span> <span>greeting</span><span>,</span>     <span>// initial value ➡ "hi"</span>
<span>V</span> <span>(</span><span>capitalize</span><span>)</span><span>,</span>  <span>// custom function call ➡ "Hi"</span>
<span>V</span> <span>.</span><span>concat</span><span>(</span><span>"!"</span><span>)</span><span>,</span>  <span>// String method `concat` call ➡ "Hi!"</span>
<span>V</span> <span>(</span><span>send</span><span>)</span><span>,</span>        <span>// custom async function call ➡ Promise { &lt;pending&gt; }</span>
<span>V</span> <span>.</span><span>status</span><span>,</span>       <span>// automatic promise chaining + getting property ➡ Promise { 200 }</span>
<span>V</span> <span>(</span><span>console</span><span>.</span><span>log</span><span>)</span><span>,</span> <span>// automatic promise chaining + global function call ➡ logs 200</span>
<span>)</span></pre></div>
<p dir="auto">If your IDE or a tool like Prettier automatically formats the code for you, it may result in the following syntax (still working):</p>
<div dir="auto" data-snippet-clipboard-copy-content="V(greeting,
  V(capitalize),
  V.concat(&quot;!&quot;),
  V(send),
  V.status,
  V(console.log),
)"><pre><span>V</span><span>(</span><span>greeting</span><span>,</span>
  <span>V</span><span>(</span><span>capitalize</span><span>)</span><span>,</span>
  <span>V</span><span>.</span><span>concat</span><span>(</span><span>"!"</span><span>)</span><span>,</span>
  <span>V</span><span>(</span><span>send</span><span>)</span><span>,</span>
  <span>V</span><span>.</span><span>status</span><span>,</span>
  <span>V</span><span>(</span><span>console</span><span>.</span><span>log</span><span>)</span><span>,</span>
<span>)</span></pre></div>
<p dir="auto">Verticalize’s <code>V</code> function is around 200 bytes minified and compressed, without dependencies. It won’t bloat your web app.</p>
<h2 tabindex="-1" id="user-content-nodejs" dir="auto"><a href="#nodejs">NodeJS</a></h2>
<h3 tabindex="-1" id="user-content-installation" dir="auto"><a href="#installation">Installation</a></h3>

<h3 tabindex="-1" id="user-content-import" dir="auto"><a href="#import">Import</a></h3>
<div dir="auto" data-snippet-clipboard-copy-content="import { V } from 'verticalize'"><pre><span>import</span> <span>{</span> <span>V</span> <span>}</span> <span>from</span> <span>'verticalize'</span></pre></div>
<h2 tabindex="-1" id="user-content-browser" dir="auto"><a href="#browser">Browser</a></h2>
<p dir="auto">Verticalize uses <a href="https://jakearchibald.com/2017/es-modules-in-browsers/" rel="nofollow">ES modules</a>, <a href="https://caniuse.com/es6-module" rel="nofollow">widely supported</a> in browsers nowadays. Import the <code>V</code> function from the <code>verticalize.min.js</code> file. This file can be located in a CDN (example below) or copied in any directory of your website (for better performance and to be GDPR compliant, since you don’t have to connect to a third party server).</p>
<div dir="auto" data-snippet-clipboard-copy-content="<script type=&quot;module&quot;>
  import { V } from 'https://cdn.jsdelivr.net/npm/verticalize@0.1.2/verticalize.min.js'
</script>"><pre><span>&lt;</span><span>script</span> <span>type</span>="<span>module</span>"<span>&gt;</span>
  <span>import</span> <span>{</span> <span>V</span> <span>}</span> <span>from</span> <span>'https://cdn.jsdelivr.net/npm/verticalize@0.1.2/verticalize.min.js'</span>
<span>&lt;/</span><span>script</span><span>&gt;</span></pre></div>
<h2 tabindex="-1" id="user-content-v-function-usage" dir="auto"><a href="#v-function-usage"><code>V</code> function usage</a></h2>
<p dir="auto">The gist example above covers pretty much everything. Just call the <code>V</code> function with the initial <em>value</em> as the first argument, followed by the other arguments wrapped by another <code>V</code> at the beginning of the line to get a nice <sub><a target="_blank" rel="noopener noreferrer" href="https://github.com/laurentpayot/verticalize/blob/main/verticalize.svg"><img src="https://github.com/laurentpayot/verticalize/raw/main/verticalize.svg" alt="triple chevron down" width="18" height="18"></a></sub> syntax. All these <code>V</code>-prefixed lines will then act like a pipeline, the output of a pipe being the input of the following pipe. Pipes can use unary functions, methods and properties, but not values (except for the initial value).</p>
<h3 tabindex="-1" id="user-content-unary-functions" dir="auto"><a href="#unary-functions">Unary functions</a></h3>
<p dir="auto">A unary function is a function that takes only one argument. You can use an anonymous ("arrow") function to turn a multi-argument function into a unary one.</p>
<div dir="auto" data-snippet-clipboard-copy-content="V( 1.9,                  // initial value
V (Math.round),          // unary function
V (n => Math.pow(n, 3)), // binary function turned into unary
) // returns 8"><pre><span>V</span><span>(</span> <span>1.9</span><span>,</span>                  <span>// initial value</span>
<span>V</span> <span>(</span><span>Math</span><span>.</span><span>round</span><span>)</span><span>,</span>          <span>// unary function</span>
<span>V</span> <span>(</span><span>n</span> <span>=&gt;</span> <span>Math</span><span>.</span><span>pow</span><span>(</span><span>n</span><span>,</span> <span>3</span><span>)</span><span>)</span><span>,</span> <span>// binary function turned into unary</span>
<span>)</span> <span>// returns 8</span></pre></div>
<h3 tabindex="-1" id="user-content-methods-and-properties" dir="auto"><a href="#methods-and-properties">Methods and properties</a></h3>
<p dir="auto">To call a method or to get a property of the previous pipe output (or of the initial value), you can use an anonymous function like <code>count =&gt; count.add(1)</code>, but or convenience Verticalize allows you to use a direct dot syntax.</p>
<div dir="auto" data-snippet-clipboard-copy-content="V ( [1, 2, 3],        // initial Array value
V .concat([4, 5, 6]), // calling the Array method `concat()` (returning an Array)
V .length,            // getting the Array property `length`
) // returns 6"><pre><span>V</span> <span>(</span> <span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>]</span><span>,</span>        <span>// initial Array value</span>
<span>V</span> <span>.</span><span>concat</span><span>(</span><span>[</span><span>4</span><span>,</span> <span>5</span><span>,</span> <span>6</span><span>]</span><span>)</span><span>,</span> <span>// calling the Array method `concat()` (returning an Array)</span>
<span>V</span> <span>.</span><span>length</span><span>,</span>            <span>// getting the Array property `length`</span>
<span>)</span> <span>// returns 6</span></pre></div>
<h3 tabindex="-1" id="user-content-promises" dir="auto"><a href="#promises">Promises</a></h3>
<p dir="auto">When the previous pipe output (or the initial value) is a promise, the next pipe will automatically chain it so you don’t have to write many <code>.then()</code> yourself.</p>
<div dir="auto" data-snippet-clipboard-copy-content="const greeting =
  await
  V( Promise.resolve(&quot;Hello!&quot;),
  V .toUpperCase(),
  )"><pre><span>const</span> <span>greeting</span> <span>=</span>
  <span>await</span>
  <span>V</span><span>(</span> <span>Promise</span><span>.</span><span>resolve</span><span>(</span><span>"Hello!"</span><span>)</span><span>,</span>
  <span>V</span> <span>.</span><span>toUpperCase</span><span>(</span><span>)</span><span>,</span>
  <span>)</span></pre></div>
<p dir="auto">is the same as</p>
<div dir="auto" data-snippet-clipboard-copy-content="const greeting =
  await Promise.resolve(&quot;Hello!&quot;)
    .then(s => s.toUpperCase())"><pre><span>const</span> <span>greeting</span> <span>=</span>
  <span>await</span> <span>Promise</span><span>.</span><span>resolve</span><span>(</span><span>"Hello!"</span><span>)</span>
    <span>.</span><span>then</span><span>(</span><span>s</span> <span>=&gt;</span> <span>s</span><span>.</span><span>toUpperCase</span><span>(</span><span>)</span><span>)</span></pre></div>
<h2 tabindex="-1" id="user-content-note" dir="auto"><a href="#note">Note</a></h2>
<p dir="auto"><a href="https://github.com/tc39/proposal-pipeline-operator">A TC39 proposal</a> for the pipe operator <code>|&gt;</code> was created in 2021 and is currently in stage 2. It may or may not be included in the official JavaScript specs in a few years. If so, then it will take a few more years to be adopted by all the major browsers and runtimes. But you can use Verticalize <em>right now</em> and enjoy its unique dot syntax and automatic promise chaining features 😉</p>
<h2 tabindex="-1" id="user-content-license" dir="auto"><a href="#license">License</a></h2>
<p dir="auto"><a href="https://github.com/laurentpayot/verticalize/blob/main/LICENSE">MIT</a></p>
<h2 tabindex="-1" id="user-content-stargazers-heart" dir="auto"><a href="#stargazers-heart">Stargazers ❤️</a></h2>
<p dir="auto"><a href="https://github.com/laurentpayot/verticalize/stargazers"><img src="https://camo.githubusercontent.com/cfdd0f9eb60f0a581daed15b5f8e9105ab8a657985bcde8c7559ef603d774dd0/68747470733a2f2f7265706f726f737465722e636f6d2f73746172732f6c617572656e747061796f742f766572746963616c697a65" alt="Stargazers repo roster for @laurentpayot/verticalize" data-canonical-src="https://reporoster.com/stars/laurentpayot/verticalize"></a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing a debugger from scratch: Breakpoints (267 pts)]]></title>
            <link>https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-5/</link>
            <guid>37670938</guid>
            <pubDate>Wed, 27 Sep 2023 06:31:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-5/">https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-5/</a>, See on <a href="https://news.ycombinator.com/item?id=37670938">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><p>(New to this series? Consider starting from <a href="https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-1">part 1</a>)</p><p>At the end of the <a href="https://www.timdbg.com/posts/writing-a-debugger-from-scratch-part-4">last post</a>, we started to get some interesting functionality with the ability to resolve addresses to names in a module. This was the last functionality missing before we could implement breakpoints! This part adds the ability for DbgRs to set hardware breakpoints.</p><p>The code for this post is in the <a href="https://github.com/TimMisiak/dbgrs/tree/part5">part5 branch on github</a>. You can also view the <a href="https://github.com/TimMisiak/dbgrs/compare/part4...part5">changes from part4</a>. If you see any mistakes or ways to improve the code, feel free to <a href="https://github.com/TimMisiak/dbgrs/issues">create issues</a> on the GitHub repo or submit a PR.</p><h2 id="first-some-cleanup">First, some cleanup</h2><p>I’ve been trying to keep DbgRs as simple as possible, avoiding extra architectural layers to keep the concepts as clear and concise as possible, but now that it has grown a bit, we need a little bit of cleanup to keep things easy to understand. I’ll just cover these changes briefly and then we’ll get to breakpoints.</p><p>The biggest change is that I moved all of the code dealing with <a href="https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-waitfordebugeventex">WaitForDebugEventEx</a> into a new file, <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/event.rs">event.rs</a>. A single public function allows waiting for the next debug event, and returns a new <code>DebugEvent</code> enum instead of the raw win32 <code>DEBUG_EVENT</code> type.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span> <span>enum</span> <span>DebugEvent</span> {
</span></span><span><span>    Exception{first_chance: <span>bool</span>, exception_code: <span>i32</span>},
</span></span><span><span>    CreateProcess{exe_name: Option<span>&lt;</span>String<span>&gt;</span>, exe_base: <span>u64</span>},
</span></span><span><span>    CreateThread{thread_id: <span>u32</span>},
</span></span><span><span>    ExitThread{thread_id: <span>u32</span>},
</span></span><span><span>    LoadModule{module_name: Option<span>&lt;</span>String<span>&gt;</span>, module_base: <span>u64</span>},
</span></span><span><span>    OutputDebugString(String),
</span></span><span><span>    ExitProcess,
</span></span><span><span>    Other(String)
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>struct</span> <span>EventContext</span> {
</span></span><span><span>    <span>pub</span> process_id: <span>u32</span>,
</span></span><span><span>    <span>pub</span> thread_id: <span>u32</span>,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>fn</span> <span>wait_for_next_debug_event</span>(mem_source: <span>&amp;</span><span>dyn</span> MemorySource) -&gt; (EventContext, DebugEvent) {
</span></span><span><span>    <span>//...
</span></span></span><span><span><span></span>}
</span></span></code></pre></div><p>As a result, the <code>main_debugger_loop</code> function is a bit smaller and can focus on the core debugger loop logic.</p><p>The other change is that I’ve added a file called <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/util.rs">util.rs</a> which has some common helpers for win32 structures, including a thin <code>AutoClosedHandle</code> wrapper for <code>HANDLE</code> and the <code>AlignedContext</code> struct that wraps the win32 <code>CONTEXT</code>. Additionally, this includes some constants that are missing from the windows-rs crate.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>#[repr(align(16))]</span>
</span></span><span><span><span>pub</span> <span>struct</span> <span>AlignedContext</span> {
</span></span><span><span>    <span>pub</span> context: <span>CONTEXT</span>,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>struct</span> <span>AutoClosedHandle</span>(<span>pub</span> <span>HANDLE</span>);
</span></span></code></pre></div><h2 id="evaluating-symbols">Evaluating symbols</h2><p>When setting a breakpoint, it’s expected that you can use the name of a function, and not just the address. So to start, we need to add the capability for resolving a name to an address. Previously, we had the <code>resolve_address_to_name</code> function in <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/name_resolution.rs#L56">name_resolution.rs</a>, so we’ll add the corresponding <code>resolve_name_to_address</code> function there as well.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span> <span>fn</span> <span>resolve_name_to_address</span>(sym: <span>&amp;</span><span>str</span>, process: <span>&amp;</span><span>mut</span> Process) -&gt; Result<span>&lt;</span><span>u64</span>, String<span>&gt;</span> {
</span></span><span><span>    <span>match</span> sym.chars().position(<span>|</span>c<span>|</span> c <span>==</span> <span>'!'</span>) {
</span></span><span><span>        None <span>=&gt;</span> {
</span></span><span><span>            <span>// Search all modules
</span></span></span><span><span><span></span>            Err(<span>"Not yet implemented"</span>.to_string())
</span></span><span><span>        },
</span></span><span><span>        Some(pos) <span>=&gt;</span> {
</span></span><span><span>            <span>let</span> module_name <span>=</span> <span>&amp;</span>sym[<span>..</span>pos];
</span></span><span><span>            <span>let</span> func_name <span>=</span> <span>&amp;</span>sym[pos <span>+</span> <span>1</span><span>..</span>];
</span></span><span><span>            <span>if</span> <span>let</span> Some(module) <span>=</span> process.get_module_by_name_mut(module_name) {
</span></span><span><span>                <span>if</span> <span>let</span> Some(addr) <span>=</span> resolve_function_in_module(module, func_name) {
</span></span><span><span>                    Ok(addr)
</span></span><span><span>                } <span>else</span> {
</span></span><span><span>                    Err(format!(<span>"Could not find </span><span>{}</span><span> in module </span><span>{}</span><span>"</span>, func_name, module_name))
</span></span><span><span>                }
</span></span><span><span>            } <span>else</span> {
</span></span><span><span>                Err(format!(<span>"Could not find module </span><span>{}</span><span>"</span>, module_name))
</span></span><span><span>            }
</span></span><span><span>        },
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><p>For now, we’ll take only the fully qualified name in <code>module.dll!functionName</code> <a aria-describedby="footnote-label" href="#fully-qualified">form</a>, and allow only exact matches.</p><p>Using this function, we can add symbols to our evaluation grammar in <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/command.rs#L26">command.rs</a></p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>#[rust_sitter::language]</span>
</span></span><span><span>    <span>pub</span> <span>enum</span> <span>EvalExpr</span> {
</span></span><span><span>        Number(<span>#[rust_sitter::leaf(pattern = r</span><span>"(\d+|0x[0-9a-fA-F]+)"</span><span>, transform = parse_int)]</span> <span>u64</span>),
</span></span><span><span>        Symbol(<span>#[rust_sitter::leaf(pattern = r</span><span>"(([a-zA-Z0-9_@#.]+!)?[a-zA-Z0-9_@#.]+)"</span><span>, transform = parse_sym)]</span> String),
</span></span></code></pre></div><p>In order to evaluate symbols, the <code>evaluate_expression</code> function now needs a context that it can use to evaluate symbols against. For this, we’ll just pass in a structure with a reference to the <code>Process</code>.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>pub</span> <span>struct</span> <span>EvalContext</span><span>&lt;</span><span>'a</span><span>&gt;</span> {
</span></span><span><span>    <span>pub</span> process: <span>&amp;</span><span>'a</span> <span>mut</span> Process,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>fn</span> <span>evaluate_expression</span>(expr: <span>EvalExpr</span>, context: <span>&amp;</span><span>mut</span> EvalContext) -&gt; Result<span>&lt;</span><span>u64</span>, String<span>&gt;</span> {
</span></span></code></pre></div><p>Note that it also returns a <code>Result</code> now because the name resolution can fail. Most of the function is unchanged besides passing the context through, and it now handles <code>EvalExpr::Symbol</code> by passing it to <code>name_resolution::resolve_to_address</code>.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>match</span> expr {
</span></span><span><span>        EvalExpr::Number(x) <span>=&gt;</span> Ok(x),
</span></span><span><span>        EvalExpr::Add(x, _, y) <span>=&gt;</span> Ok(evaluate_expression(<span>*</span>x, context)<span>?</span> <span>+</span> evaluate_expression(<span>*</span>y, context)<span>?</span>),
</span></span><span><span>        EvalExpr::Symbol(sym) <span>=&gt;</span> {
</span></span><span><span>            resolve_name_to_address(<span>&amp;</span>sym, context.process)
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>}
</span></span></code></pre></div><p>We can verify this is working by simply evaluating a symbol to an address and making sure it resolves back to the same symbol.</p><pre tabindex="0"><code>&gt; ? ntdll.dll!NtMapViewOfSection+0x14
 = 0x7FFE7360F154
[11254] ntdll.dll!NtMapViewOfSection+0x14
&gt; ln 0x7FFE7360F154
ntdll.dll!NtMapViewOfSection+0x14
[11254] ntdll.dll!NtMapViewOfSection+0x14
</code></pre><p>Success!</p><h2 id="keeping-track-of-breakpoints">Keeping track of breakpoints</h2><p>With the new functionality in the expression evaluator to evaluate symbols, we can add the commands for setting, clearing, and listing breakpoints. First, we add the breakpoint commands to the command grammar in <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/command.rs#L15">command.rs</a></p><div><pre tabindex="0"><code data-lang="rust"><span><span>        SetBreakpoint(<span>#[rust_sitter::leaf(text = </span><span>"bp"</span><span>)]</span> (), Box<span>&lt;</span>EvalExpr<span>&gt;</span>),
</span></span><span><span>        ListBreakpoints(<span>#[rust_sitter::leaf(text = </span><span>"bl"</span><span>)]</span> ()),
</span></span><span><span>        ClearBreakpoint(<span>#[rust_sitter::leaf(text = </span><span>"bc"</span><span>)]</span> (), Box<span>&lt;</span>EvalExpr<span>&gt;</span>),
</span></span></code></pre></div><p>The implementation of these commands need something to talk to, so we’ll create a new structure called BreakpointManager that keeps track of the breakpoints that should be set in the process, and create this at the start of the <a href="https://github.com/TimMisiak/dbgrs/blob/part5/src/main.rs#L81">main_debugger_loop</a></p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>fn</span> <span>main_debugger_loop</span>(process: <span>HANDLE</span>) {
</span></span><span><span>    <span>let</span> <span>mut</span> breakpoints <span>=</span> BreakpointManager::new();
</span></span></code></pre></div><p>We’ll get to the implementation of <code>BreakpointManager</code> in a minute, but first we can just see the simple implementation of <code>bp</code>, <code>bl</code>, and <code>bc</code> calling into the breakpoint manager.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>let</span> cmd <span>=</span> command::read_command();
</span></span><span><span>    <span>//...
</span></span></span><span><span><span></span>    <span>match</span> cmd {
</span></span><span><span>        <span>//...
</span></span></span><span><span><span></span>        CommandExpr::SetBreakpoint(_, expr) <span>=&gt;</span> {
</span></span><span><span>            <span>if</span> <span>let</span> Some(addr) <span>=</span> eval_expr(expr) {
</span></span><span><span>                breakpoints.add_breakpoint(addr);
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>        CommandExpr::ListBreakpoints(_) <span>=&gt;</span> {
</span></span><span><span>            breakpoints.list_breakpoints(<span>&amp;</span><span>mut</span> process);
</span></span><span><span>        }
</span></span><span><span>        CommandExpr::ClearBreakpoint(_, expr) <span>=&gt;</span> {
</span></span><span><span>            <span>if</span> <span>let</span> Some(id) <span>=</span> eval_expr(expr) {
</span></span><span><span>                breakpoints.clear_breakpoint(id <span>as</span> <span>u32</span>);
</span></span><span><span>            }
</span></span><span><span>        }
</span></span></code></pre></div><p>The <code>BreakpointManager</code> contains the list of the breakpoints that have been requested by the user. It has functions for adding a breakpoint at a specified address, removing a breakpoint given its ID, and listing the breakpoints for the user.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>struct</span> <span>Breakpoint</span> {
</span></span><span><span>    addr: <span>u64</span>,
</span></span><span><span>    id: <span>u32</span>,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>pub</span> <span>struct</span> <span>BreakpointManager</span> {
</span></span><span><span>    breakpoints: Vec::<span>&lt;</span>Breakpoint<span>&gt;</span>,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>impl</span> BreakpointManager {
</span></span><span><span>
</span></span><span><span>    <span>pub</span> <span>fn</span> <span>add_breakpoint</span>(<span>&amp;</span><span>mut</span> self, addr: <span>u64</span>) {
</span></span><span><span>        self.breakpoints.push(Breakpoint{addr, id: <span>self</span>.get_free_id()});
</span></span><span><span>        self.breakpoints.sort_by(<span>|</span>a, b<span>|</span> a.id.cmp(<span>&amp;</span>b.id));
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>pub</span> <span>fn</span> <span>list_breakpoints</span>(<span>&amp;</span>self, process: <span>&amp;</span><span>mut</span> Process) {
</span></span><span><span>        <span>for</span> bp <span>in</span> self.breakpoints.iter() {
</span></span><span><span>            <span>if</span> <span>let</span> Some(sym) <span>=</span> name_resolution::resolve_address_to_name(bp.addr, process) {
</span></span><span><span>                println!(<span>"</span><span>{:3}</span><span> </span><span>{:#018x}</span><span> (</span><span>{}</span><span>)"</span>, bp.id, bp.addr, sym)
</span></span><span><span>            } <span>else</span> {
</span></span><span><span>                println!(<span>"</span><span>{:3}</span><span> </span><span>{:#018x}</span><span>"</span>, bp.id, bp.addr)
</span></span><span><span>            }            
</span></span><span><span>        }
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>pub</span> <span>fn</span> <span>clear_breakpoint</span>(<span>&amp;</span><span>mut</span> self, id: <span>u32</span>) {
</span></span><span><span>        self.breakpoints.retain(<span>|</span>x<span>|</span> x.id <span>!=</span> id)
</span></span><span><span>    }
</span></span></code></pre></div><p>We can test these commands to make sure breakpoints are tracked correctly, although we still need to apply the breakpoints to the target process before they’ll do anything.</p><pre tabindex="0"><code>Command line was: '"C:\git\HelloWorld\hello.exe" '
LoadDll: 7FF7E7420000   hello.exe
[5A70] 0x00007ffdaed4aa40
&gt; g
LoadDll: 7FFDAECF0000   ntdll.dll
[5A70] ntdll.dll!RtlUserThreadStart
&gt; bp ntdll.dll!RtlUserThreadStart
[5A70] ntdll.dll!RtlUserThreadStart
&gt; bl
  0 0x00007ffdaed4aa40 (ntdll.dll!RtlUserThreadStart)
[5A70] ntdll.dll!RtlUserThreadStart
&gt; bc 0
[5A70] ntdll.dll!RtlUserThreadStart
&gt; bl
[5A70] ntdll.dll!RtlUserThreadStart
&gt; 
</code></pre><h2 id="applying-breakpoints">Applying breakpoints</h2><p>Finally, we can get to the fun part where we apply the breakpoints to a process. There are two types of breakpoints, software breakpoints and hardware breakpoints. Of the two, hardware breakpoints are less complicated, so we’ll start with those. On x86 processors the hardware breakpoints are controlled via the <a href="https://wiki.osdev.org/CPU_Registers_x86-64#Debug_Registers">“Debug Registers”</a>. There are four hardware breakpoints available on current CPUs. Debug registers DR0 through DR3 are used to specify the address of the breakpoint. Register DR6 is a status register to determine when a breakpoint is hit. And DR7 is a control register to specify the attributes of each hardware breakpoint. Note that there are a number of fields packed together in DR7, so we’ll use a little helper to set these fields.</p><div><pre tabindex="0"><code data-lang="rust"><span><span><span>// Helper function to set a value at a specific bit offset.
</span></span></span><span><span><span></span><span>fn</span> <span>set_bits</span><span>&lt;</span>T: <span>PrimInt</span><span>&gt;</span>(val: <span>&amp;</span><span>mut</span> T, set_val: <span>T</span>, start_bit: <span>usize</span>, bit_count: <span>usize</span>) {
</span></span><span><span>    <span>// First, mask out the relevant bits
</span></span></span><span><span><span></span>    <span>let</span> max_bits <span>=</span> std::mem::size_of::<span>&lt;</span>T<span>&gt;</span>() <span>*</span> <span>8</span>;
</span></span><span><span>    <span>let</span> mask: <span>T</span> <span>=</span> T::max_value() <span>&lt;&lt;</span> (max_bits <span>-</span> bit_count);
</span></span><span><span>    <span>let</span> mask: <span>T</span> <span>=</span> mask <span>&gt;&gt;</span> (max_bits <span>-</span> <span>1</span> <span>-</span> start_bit);
</span></span><span><span>    <span>let</span> inv_mask <span>=</span> <span>!</span>mask;
</span></span><span><span>
</span></span><span><span>    <span>*</span>val <span>=</span> <span>*</span>val <span>&amp;</span> inv_mask;
</span></span><span><span>    <span>*</span>val <span>=</span> <span>*</span>val <span>|</span> (set_val <span>&lt;&lt;</span> (start_bit <span>+</span> <span>1</span> <span>-</span> bit_count));
</span></span><span><span>}
</span></span></code></pre></div><p>To manipulate these registers, we’ll use the <code>GetThreadContext</code>/<code>SetThreadContext</code> functions to set the registers to the state needed for the requested breakpoints. Note that the debug registers are maintained for each thread separately, so we could theoretically set different breakpoints for each thread, or filter a breakpoint to a specific thread. That functionality won’t be implemented in DbgRs for now, and we’ll just apply the same breakpoints to all threads. To start, we’ll loop over all of the threads in the process and retrieve each thread’s context:</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>pub</span> <span>fn</span> <span>apply_breakpoints</span>(<span>&amp;</span><span>mut</span> self, process: <span>&amp;</span><span>mut</span> Process, resume_thread_id: <span>u32</span>, _memory_source: <span>&amp;</span><span>dyn</span> MemorySource) {
</span></span><span><span>
</span></span><span><span>        <span>for</span> thread_id <span>in</span> process.iterate_threads() {
</span></span><span><span>            <span>let</span> <span>mut</span> ctx: <span>AlignedContext</span> <span>=</span> <span>unsafe</span> { std::mem::zeroed() };
</span></span><span><span>            ctx.context.ContextFlags <span>=</span> <span>CONTEXT_ALL</span>;            
</span></span><span><span>            <span>let</span> thread <span>=</span> AutoClosedHandle(<span>unsafe</span> {
</span></span><span><span>                OpenThread(
</span></span><span><span>                    <span>THREAD_GET_CONTEXT</span> <span>|</span> <span>THREAD_SET_CONTEXT</span>,
</span></span><span><span>                    <span>FALSE</span>,
</span></span><span><span>                    <span>*</span>thread_id,
</span></span><span><span>                )
</span></span><span><span>            });
</span></span><span><span>            <span>let</span> ret <span>=</span> <span>unsafe</span> { GetThreadContext(thread.handle(), <span>&amp;</span><span>mut</span> ctx.context) };
</span></span></code></pre></div><p>We’ll then loop over the requested breakpoints. We need to set four pieces of information for each one. The three fields to set are the LEN (length), RW (access type), and LE (local enable) configuration for each breakpoint. We’ll set the <a aria-describedby="footnote-label" href="#execute-len">LEN to 0</a>, which indicates 1 byte. We’ll set RW to 0, which means “break on instruction execution” (we would use a value of 1 indicate break on read or a value of 3 to indicate break on read or write). Finally, we’ll set the “local enable” bit to 1 indicating that the breakpoint should be enabled.</p><p><img src="https://www.timdbg.com/dr7.png" alt="DR7 layout"></p><div><pre tabindex="0"><code data-lang="rust"><span><span>
</span></span><span><span>    <span>for</span> idx <span>in</span> <span>0</span><span>..</span><span>4</span> {
</span></span><span><span>        <span>if</span> self.breakpoints.len() <span>&gt;</span> idx {
</span></span><span><span>            
</span></span><span><span>            <span>// The DR7_* variables are a set of constants with the correct offsets and sizes for each
</span></span></span><span><span><span></span>            <span>// field of DR7.
</span></span></span><span><span><span></span>            set_bits(<span>&amp;</span><span>mut</span> ctx.context.Dr7, <span>0</span>, <span>DR7_LEN_BIT</span>[idx], <span>DR7_LEN_SIZE</span>);
</span></span><span><span>            set_bits(<span>&amp;</span><span>mut</span> ctx.context.Dr7, <span>0</span>, <span>DR7_RW_BIT</span>[idx], <span>DR7_RW_SIZE</span>);
</span></span><span><span>            set_bits(<span>&amp;</span><span>mut</span> ctx.context.Dr7, <span>1</span>, <span>DR7_LE_BIT</span>[idx], <span>1</span>);
</span></span></code></pre></div><p>The appropriate DR0-DR3 value will be set to the address of the resolved breakpoint.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>
</span></span><span><span>            <span>match</span> idx {
</span></span><span><span>                <span>0</span> <span>=&gt;</span> ctx.context.Dr0 <span>=</span> self.breakpoints[idx].addr,
</span></span><span><span>                <span>1</span> <span>=&gt;</span> ctx.context.Dr1 <span>=</span> self.breakpoints[idx].addr,
</span></span><span><span>                <span>2</span> <span>=&gt;</span> ctx.context.Dr2 <span>=</span> self.breakpoints[idx].addr,
</span></span><span><span>                <span>3</span> <span>=&gt;</span> ctx.context.Dr3 <span>=</span> self.breakpoints[idx].addr,
</span></span><span><span>                _ <span>=&gt;</span> (),
</span></span><span><span>            }
</span></span></code></pre></div><p>Finally, we’ll make sure to disable any breakpoints that we are not using. Note that the code assumes that the debugger “owns” the debug registers and that the target process is not using them in any way. This is typically true, but there are cases where the target process will be using the debug registers for its own purpose, or are manipulated as an anti-debugging technique. To keep things simple, we won’t worry about that and just clear the local enable (LE) bit.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>        } <span>else</span> {
</span></span><span><span>            <span>// Disable any breakpoints that we aren't using.
</span></span></span><span><span><span></span>            set_bits(<span>&amp;</span><span>mut</span> ctx.context.Dr7, <span>0</span>, <span>DR7_LE_BIT</span>[idx], <span>1</span>);
</span></span><span><span>            <span>break</span>;
</span></span><span><span>        }    
</span></span><span><span>    }
</span></span></code></pre></div><p>This new function, <code>apply_breakpoints</code>, will be called from the <code>main_debugger_loop</code> right before we call <code>ContinueDebugEvent</code>. That will ensure that we set up all thread contexts with the correct breakpoint state. Note that because Windows sends a debug event for thread creation, we’ll have an opportunity to set the breakpoint state for all new threads that are created.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    breakpoints.apply_breakpoints(<span>&amp;</span><span>mut</span> process, event_context.thread_id, mem_source.as_ref());
</span></span><span><span>    
</span></span><span><span>    <span>unsafe</span> {
</span></span><span><span>        ContinueDebugEvent(
</span></span><span><span>            event_context.process_id,
</span></span><span><span>            event_context.thread_id,
</span></span><span><span>            continue_status,
</span></span><span><span>        );
</span></span><span><span>    }
</span></span></code></pre></div><h2 id="handling-breakpoint-exceptions">Handling breakpoint exceptions</h2><p>When the CPU tries to execute an instruction that is marked with a debug register, it generates a debug exception (#DB) as a <a href="https://wiki.osdev.org/Exceptions">fault</a> (It’s important to note this is a fault, and not a trap. More on that later). Windows delivers this to a debugger as an exception event with exception code 0x80000004. The thread context will also have a flag set in DR6 indicating which breakpoint was hit. We’ll add some code in the exception event handler letting the breakpoint manager check if a breakpoint was hit.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>        <span>match</span> debug_event {
</span></span><span><span>            DebugEvent::Exception { first_chance, exception_code } <span>=&gt;</span> {
</span></span><span><span>                <span>//...
</span></span></span><span><span><span></span>                <span>if</span> <span>let</span> Some(bp_index) <span>=</span> breakpoints.was_breakpoint_hit(<span>&amp;</span>ctx.context) {
</span></span><span><span>                    println!(<span>"Breakpoint </span><span>{}</span><span> hit"</span>, bp_index);
</span></span><span><span>                    <span>// It's important to use DBG_CONTINUE with ContinueDebugEvent or else the breakpoint will be treated
</span></span></span><span><span><span></span>                    <span>// as an exception to be handled by the target process.
</span></span></span><span><span><span></span>                    continue_status <span>=</span> <span>DBG_CONTINUE</span>;
</span></span><span><span>                }
</span></span><span><span>                <span>//...
</span></span></span></code></pre></div><p>The breakpoint manager will just check DR6 to see if any of the bits were set that correspond to a hardware breakpoint triggering.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>pub</span> <span>fn</span> <span>was_breakpoint_hit</span>(<span>&amp;</span>self, thread_context: <span>&amp;</span><span>CONTEXT</span>) -&gt; Option<span>&lt;</span><span>u32</span><span>&gt;</span> {
</span></span><span><span>        <span>for</span> idx <span>in</span> <span>0</span><span>..</span>self.breakpoints.len() {
</span></span><span><span>            <span>if</span> get_bit(thread_context.Dr6, <span>DR6_B_BIT</span>[idx]) {
</span></span><span><span>                <span>return</span> Some(idx <span>as</span> <span>u32</span>);
</span></span><span><span>            }
</span></span><span><span>        }
</span></span><span><span>        None
</span></span><span><span>    }
</span></span></code></pre></div><p>Remember how I mentioned that hardware breakpoints trigger debug exceptions as a <a aria-describedby="footnote-label" href="#debug-fault">fault</a>? That’s important because a “fault” exception triggers <em>before</em> the instruction has a chance to execute. That lets us examine state before the instruction executes, which is what we want for a debugger. But since it is a fault, resuming the execution of the program just causes the program to break in again! On some architectures, this might be complicated to get past, but on x86 we simply have to set the “resume flag”, which is one of the bits in the EFlags registers that often gets overlooked. The resume flag causes the processor to ignore instruction breakpoints for a single instruction execution. It is set back to 0 right after the debug registers would have been checked, which makes it a convenient tool for resuming execution. We’ll set this flag on whatever thread caused the debugger to break in, regardless of whether the breakpoint has hit or not. (Some debuggers will only set RF when a breakpoint was hit). We’ll set the resume flag inside the <code>apply_breakpoints</code> function, since it’s already manipulating the register contexts of all threads.</p><div><pre tabindex="0"><code data-lang="rust"><span><span>    <span>if</span> <span>*</span>thread_id <span>==</span> resume_thread_id {
</span></span><span><span>        set_bits(<span>&amp;</span><span>mut</span> ctx.context.EFlags, <span>1</span>, <span>EFLAG_RF</span>, <span>1</span>);
</span></span><span><span>    }
</span></span></code></pre></div><h2 id="testing-it-out">Testing it out</h2><p>Now that we can set a breakpoint, apply a breakpoint, and handle a breakpoint exception, we’re ready to test out the new breakpoint functionality. To do that, we’ll just continue execution until kernelbase is loaded, and then set a breakpoint on kernelbase!GetLastError, which is a very frequently used function that should get called almost immediately.</p><pre tabindex="0"><code>Command line was: '"C:\git\HelloWorld\hello.exe" '
LoadDll: 7FF7E7420000   hello.exe
[5CF8] 0x00007ffdaed4aa40
&gt; g
LoadDll: 7FFDAECF0000   ntdll.dll
[5CF8] ntdll.dll!RtlUserThreadStart
&gt; g
LoadDll: 7FFDAD6E0000   C:\Windows\System32\KERNEL32.DLL
[5CF8] ntdll.dll!NtMapViewOfSection+0x14
&gt; g
LoadDll: 7FFDAC0A0000   C:\Windows\System32\KERNELBASE.dll
[5CF8] ntdll.dll!NtMapViewOfSection+0x14
&gt; bp kernelbase.dll!GetLastError
[5CF8] ntdll.dll!NtMapViewOfSection+0x14
&gt; g
Breakpoint 0 hit
[5CF8] C:\Windows\System32\KERNELBASE.dll!GetLastError
&gt; 
</code></pre><p>It works! It’s almost starting to feel like a real debugger. A few very important things are left though. To start with, we can’t see the functions that are on the call stack. Viewing the stack is probably the single most important analysis feature of a debugger. So that’s likely where we’re going next.</p><p>I hope you found this post interesting and informative! Have a question or suggestion? Let me know! You can find me on <a href="https://twitter.com/timmisiak">Twitter</a>, <a href="https://dbg.social/@tim">Mastodon</a>, and <a href="https://bsky.app/profile/timdbg.com">Bluesky</a>.</p></div></article></div></div>]]></description>
        </item>
    </channel>
</rss>