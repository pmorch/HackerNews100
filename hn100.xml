<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 03 Jul 2024 17:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Do not taunt happy fun branch predictor (2023) (113 pts)]]></title>
            <link>https://www.mattkeeter.com/blog/2023-01-25-branch/</link>
            <guid>40866374</guid>
            <pubDate>Wed, 03 Jul 2024 14:42:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mattkeeter.com/blog/2023-01-25-branch/">https://www.mattkeeter.com/blog/2023-01-25-branch/</a>, See on <a href="https://news.ycombinator.com/item?id=40866374">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
<!-- End header -->






<p>I've been writing a lot of AArch64 assembly, for <em>reasons</em>.</p>
<p>I recently came up with a "clever" idea to eliminate one jump from an inner
loop, and was surprised to find that it slowed things down.  Allow me to explain
my terrible error, so that you don't fall victim in the future.</p>
<p>A toy model of the relevant code looks something like this:</p>
<pre><code>float run(const float* data, size_t n) {
    float g = 0.0;
    while (n) {
        n--;
        const float f = *data++;
        foo(f, &amp;g);
    }
    return g;
}

static void foo(float f, float* g) {
    // do some stuff, modifying g
}
</code></pre>
<p>(eliding headers and the forward declaration of <code>foo</code> for space)</p>
<p>A simple translation into AArch64 assembly gives something like this:</p>
<pre><code>// x0: const float* data
// x1: size_t n
// Returns a single float in s0

// Prelude: store frame and link registers
stp   x29, x30, [sp, #-16]!

// Initialize g = 0.0
fmov s0, #0.0

loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

    bl foo   // call the function
    b loop   // keep looping

foo:
    // Do some work, reading from s1 and accumulating into s0
    // ...
    ret

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>Here, <code>foo</code> is kinda like a <a href="https://github.com/rust-lang/rfcs/blob/master/text/1201-naked-fns.md">naked
function</a>:
it uses the same stack frame and registers as the parent function, reads from
<code>s1</code>, and writes to <code>s0</code>.</p>
<p>The call to <code>foo</code> uses the the <code>bl</code> instruction, which is "branch and link":
it jumps to the given label, and stores the <strong>next</strong> instruction address in the
link register (<code>lr</code> or <code>x30</code>).</p>
<p>When <code>foo</code> is done, the <code>ret</code> instruction jumps to the address in the link
register, which is the instruction following the original <code>bl</code>.</p>
<p>Looking at this code, I was struck by the fact that it does two branches,
one after the other.  Surely, it would be more efficient to only branch once.</p>
<p>I had the clever idea to do so <strong>without changing <code>foo</code></strong>:</p>
<pre><code>stp   x29, x30, [sp, #-16]!
fmov s0, #0.0

bl loop // Set up x30 to point to the loop entrance
loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

foo:
    // Do some work, accumulating into `s0`
    // ...
    ret

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>This is a little subtle:</p>
<ul>
<li>The first call to <code>bl loop</code> stores the beginning of the <code>loop</code> block in <code>x30</code></li>
<li>After checking for loop termination, we fall through into the <code>foo</code> function
(without a branch!)</li>
<li><code>foo</code> still ends with <code>ret</code>, which returns to the <code>loop</code> block (because
that's what's in <code>x30</code>).</li>
</ul>
<p>Within the body of the loop, we never change <code>x30</code>, so the repeated <code>ret</code>
instructions always return to the same place.</p>
<p>I set up a benchmark using a very simple <code>foo</code>:</p>
<pre><code>foo:
    fadd s0, s0, s1
    ret
</code></pre>
<p>With this <code>foo</code>, the function as a whole sums the incoming array of <code>float</code>
values.</p>
<p>Benchmarking with <a href="https://docs.rs/criterion/latest/criterion/"><code>criterion</code></a>
(on an M1 Max CPU),
with a 1024-element array:</p>
<table>
<tbody><tr><th>Program</th><th>Time
</th></tr><tr><td>Original </td><td>969 ns
</td></tr><tr><td>"Optimized"</td><td>3.85 µs
</td></tr></tbody></table>
<p>The "optimized" code with one jump per loop is about <strong>4x slower</strong>
than the original version with two jumps per loop!</p>
<p>I found this surprising, so I asked a few colleagues about it.</p>
<p>Between <a href="https://hachyderm.io/@cliffle">Cliff</a> and
<a href="https://discuss.systems/@cross">Dan</a>,
the consensus was that mismatched <code>bl</code> / <code>ret</code>
pairs were confusing the
<a href="https://en.wikipedia.org/wiki/Branch_predictor">branch predictor</a>.</p>
<p>The <a href="https://developer.arm.com/documentation/102374/0101/Function-calls">ARM documentation</a> agrees:</p>
<blockquote>
<p>Why do we need a special function return instruction? Functionally, BR LR
would do the same job as RET. Using RET tells the processor that this is a
function return. Most modern processors, and all Cortex-A processors, support
branch prediction. Knowing that this is a function return allows processors to
more accurately predict the branch.</p>
<p>Branch predictors guess the direction the program flow will take across
branches. The guess is used to decide what to load into a pipeline with
instructions waiting to be processed. If the branch predictor guesses
correctly, the pipeline has the correct instructions and the processor does
not have to wait for instructions to be loaded from memory.</p>
</blockquote>
<p>More specifically, the branch predictor probably keeps an internal stack of
function return addresses, which is pushed to whenever a <code>bl</code> is executed. When
the branch predictor sees a <code>ret</code> coming down the pipeline, it assumes that
you're returning to the address associated with the most recent <code>bl</code> (and begins
prefetching / speculative execution / whatever), then pops that top address from
its internal stack.</p>
<p>This works if you've got matched <code>bl</code> / <code>ret</code> pairs, but the prediction will
fail if the same address is used by multiple <code>ret</code> instructions; you'll end up
with (<em>vague handwaving</em>) useless prefetching, incorrect speculative execution,
and pipeline stalls / flushes</p>
<p>Dan made the great suggestion of replacing <code>ret</code> with <code>br x30</code> to test this
theory.  Sure enough, this fixes the performance regression:</p>
<table>
<tbody><tr><th>Program</th><th>Time
</th></tr><tr><td>Matched <code>bl</code> / <code>ret</code> </td><td>969 ns
</td></tr><tr><td>One <code>bl</code>, many <code>ret</code></td><td>3.85 µs
</td></tr><tr><td>One <code>bl</code>, many <code>br x30</code></td><td>913 ns
</td></tr></tbody></table>
<p>In fact, it's slightly faster, probably because it's only doing one branch
per loop instead of two!</p>
<p>To further test the "branch predictor" theory, I opened up Instruments and
examined performance counters for the first two programs. Picking out the worst
offenders, the results seem conclusive:</p>
<table>
<tbody><tr><th>Counter</th><th>Matched <code>bl</code> / <code>ret</code></th><th>One <code>bl</code>, many <code>ret</code>
</th></tr><tr><td><code>BRANCH_RET_INDIR_MISPRED_NONSPECIFIC</code></td><td>92</td><td>928,644,975
</td></tr><tr><td><code>FETCH_RESTART</code></td><td>61,121</td><td>987,765,276
</td></tr><tr><td><code>MAP_DISPATCH_BUBBLE</code></td><td>1,155,632</td><td>7,350,085,139
</td></tr><tr><td><code>MAP_REWIND</code></td><td>6,412,734</td><td>2,789,499,545
</td></tr></tbody></table>
<p>These measurements are captured while summing an array of 1B elements.  We see
that with mismatched <code>bl</code> / <code>ret</code> pairs, the return branch predictor fails about
93% of the time!</p>
<p>Apple doesn't fully document these counters, but I'm guessing that the other
counters are downstream effects of bad branch prediction:</p>
<ul>
<li><code>FETCH_RESTART</code> is presumably bad prefetching</li>
<li><code>MAP_DISPATCH_BUBBLE</code> probably refers to <a href="https://en.wikipedia.org/wiki/Pipeline_stall">pipeline stalls</a></li>
<li><code>MAP_REWIND</code> might be bad speculative execution that needs to be rewound</li>
</ul>
<p>In conclusion,
<a href="https://www.youtube.com/watch?v=GmqeZl8OI2M">do not taunt happy fun branch predictor</a>
with asymmetric usage of <code>bl</code> and <code>ret</code> instructions.</p>
<hr>
<h2>Appendix: Going Fast</h2>
<p>Take a second look at this program:</p>
<pre><code>stp   x29, x30, [sp, #-16]!
fmov s0, #0.0

loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

    bl foo   // call the function
    b loop   // keep looping

foo:
    fadd s0, s0, s1
    ret

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>Upon seeing this program, it's a common reaction to ask "why is <code>foo</code> a
subroutine at all?"</p>
<p>The answer is "because this is a didactic example, not code that's trying
to go as fast as possible".</p>
<p>Still, it's a fair question.  You wanna go fast?  Let's go fast.</p>
<p>If we know the contents of <code>foo</code> when building this
function (and it's shorter than the maximum jump distance), we can remove the
<code>bl</code> and <code>ret</code> entirely:</p>
<pre><code>loop:
    cmp x1, #0
    b.eq exit
    sub x1, x1, #1
    ldr s1, [x0], #4

    // foo is completely inlined here
    fadd s0, s0, s1

    b loop

exit: // Function exit
    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>This is a roughly 6% speedup: from 969 ns to 911 ns.</p>
<p>We can get faster still by trusting the compiler:</p>
<pre><code>pub fn sum_slice(f: &amp;[f32]) -&gt; f32 {
    f.iter().sum()
}
</code></pre>
<p>This brings us down to 833 ns, a significant improvement!</p>
<p><a href="https://godbolt.org/z/Kv77abW6c">Looking at the assembly</a>,
it's doing some loop unrolling.
However, even when compiled with <code>-C target-cpu=native</code>, it's not generating
<a href="https://developer.arm.com/Architectures/Neon">NEON SIMD instructions</a>.
Can we beat it?</p>
<p><strong>We sure can!</strong></p>
<pre><code>stp   x29, x30, [sp, #-16]!

fmov s0, #0.0
dup v1.4s, v0.s[0]
dup v2.4s, v0.s[0]

loop:  // 1x per loop
    ands xzr, x1, #3
    b.eq simd

    sub x1, x1, #1
    ldr s3, [x0], #4

    fadd s0, s0, s3
    b loop

simd:  // 4x SIMD per loop
    ands xzr, x1, #7
    b.eq simd2

    sub x1, x1, #4
    ldp d3, d4, [x0], #16
    mov v3.d[1], v4.d[0]

    fadd v1.4s, v1.4s, v3.4s

    b simd

simd2:  // 2 x 4x SIMD per loop
    cmp x1, #0
    b.eq exit

    sub x1, x1, #8

    ldp d3, d4, [x0], #16
    mov v3.d[1], v4.d[0]
    fadd v1.4s, v1.4s, v3.4s

    ldp d5, d6, [x0], #16
    mov v5.d[1], v6.d[0]
    fadd v2.4s, v2.4s, v5.4s

    b simd2

exit: // function exit
    fadd v2.4s, v2.4s, v1.4s
    mov s1, v2.s[0]
    fadd s0, s0, s1
    mov s1, v2.s[1]
    fadd s0, s0, s1
    mov s1, v2.s[2]
    fadd s0, s0, s1
    mov s1, v2.s[3]
    fadd s0, s0, s1

    ldp   x29, x30, [sp], #16
    ret
</code></pre>
<p>This code includes three different loops:</p>
<ul>
<li>The first loop (<code>loop</code>) sums individual values
into <code>s0</code> until we have a multiple of four values remaining</li>
<li>The second loop (<code>simd</code>) uses SIMD instructions to sum 4 values at a time
into the vector register <code>v1</code>, until we have a multiple of 8 values remaining</li>
<li>The last loop (<code>simd2</code>) is the same as <code>simd</code>, but is unrolled 2x so it
handles 8 values per loop iteration, summing into <code>v1</code> and <code>v2</code></li>
</ul>
<p>At the function exit, we accumulate the values in the vector registers <code>v1</code>/<code>v2</code>
into <code>s0</code>, which is returned.</p>
<p>The type punning here is particularly cute:</p>
<pre><code>ldp d3, d4, [x0], #16
mov v3.d[1], v4.d[0]
fadd v1.4s, v1.4s, v3.4s
</code></pre>
<p>Remember, <code>x0</code> holds a <code>float*</code>.  We pretend that it's a <code>double*</code> to load 128
bits (i.e. 4x <code>float</code> values) into <code>d3</code> and <code>d4</code>.  Then, we move the "double" in <code>d4</code>
to occupy the top 64 bits of the <code>v3</code> vector register (of which <code>d3</code> is the
<em>lower</em> 64 bits).</p>
<p>Of course, each "double" is two floats, but that doesn't matter when shuffling
them around.  When summing with <code>fadd</code>, we tell the processor to treat them as
four floats (the <code>.4s</code> suffix), and everything works out fine.</p>
<p>How fast are we now?</p>
<p>This runs in 94 ns, or about <strong>8.8x faster</strong> than our previous best.</p>
<p>Here's a summary of performance:</p>
<table>
<tbody><tr><th>Program</th><th>Time
</th></tr><tr><td>Matched <code>bl</code> / <code>ret</code> </td><td>969 ns
</td></tr><tr><td>One <code>bl</code>, many <code>ret</code></td><td>3.85 µs
</td></tr><tr><td>One <code>bl</code>, many <code>br x30</code></td><td>913 ns
</td></tr><tr><td>Plain loop with <code>b</code></td><td>911 ns
</td></tr><tr><td>Rewrite it in Rust</td><td>833 ns
</td></tr><tr><td>SIMD + manual loop unrolling</td><td>94 ns
</td></tr></tbody></table>
<p>Could we get even faster?  I'm sure it's possible; I make no claims to being
the <a href="https://www.agner.org/optimize/">Agner Fog</a> of AArch64 assembly.</p>
<p>Still, this is a reasonable point to wrap up: we've demystified the initial
performance regression, and had some fun hand-writing assembly to go very
fast indeed.</p>
<p>The SIMD code does come with one asterisk, though: because floating-point
addition is not associative, and it performs the summation in a different
order, it <strong>may not get the same result</strong> as straight-line code.  In retrospect,
this is likely why the compiler doesn't generate SIMD instructions to compute
the sum!</p>
<p>Does this matter for your use case?  Only you can know!</p>
<hr>
<p>All of the code from this post is
<a href="https://github.com/mkeeter/arm64-test">published to GitHub</a>.</p>
<p>You can reproduce benchmarks by running <code>cargo bench</code> on an ARM64 machine.</p>

<!-- Begin footer -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Proton launches its own version of Google Docs (275 pts)]]></title>
            <link>https://www.engadget.com/proton-launches-its-own-version-of-google-docs-100044471.html</link>
            <guid>40864914</guid>
            <pubDate>Wed, 03 Jul 2024 11:25:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/proton-launches-its-own-version-of-google-docs-100044471.html">https://www.engadget.com/proton-launches-its-own-version-of-google-docs-100044471.html</a>, See on <a href="https://news.ycombinator.com/item?id=40864914">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a data-i13n="cpos:1;pos:1" href="https://www.engadget.com/protons-windows-and-macos-mail-app-is-out-of-beta-and-available-now-110010822.html" data-ylk="slk:Proton;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas">Proton</a> now has its own version of Google Docs in its Drive cloud storage service, and like the company's other products, it comes with end-to-end encryption. The company says its flavor of Docs "offers a unique solution in a market where most popular products neglect privacy" and recommends it for use in the healthcare, media, finance and legal industries. <a data-i13n="elm:affiliate_link;sellerN:;elmt:;cpos:2;pos:1" href="https://shopping.yahoo.com/rdlw?siteId=us-engadget&amp;pageId=1p-autolink&amp;featureId=text-link&amp;custData=eyJzb3VyY2VOYW1lIjoiV2ViLURlc2t0b3AtVmVyaXpvbiIsImxhbmRpbmdVcmwiOiJodHRwczovL3Byb3Rvbi5tZS9ibG9nL2RvY3MtcHJvdG9uLWRyaXZlIiwiY29udGVudFV1aWQiOiJjMmYyMGEyYS05ZGEzLTQyZmMtYjhmMC03MGI3ZDRlMGFhNmYifQ&amp;signature=AQAAAWZaQ6nJO7zP4rH3JS2Y44qB7q_HbbOpt8RRVvU6nYbn&amp;gcReferrer=https%3A%2F%2Fproton.me%2Fblog%2Fdocs-proton-drive" rel="nofollow noopener" target="_blank" data-ylk="slk:Proton Docs;elm:affiliate_link;sellerN:;elmt:;cpos:2;pos:1;itc:0;sec:content-canvas">Proton Docs</a> has advanced formatting and image embed options like Google Docs has and can create, open and edit documents in multiple formats, including Microsoft .docx.</p><p>It has collaboration tools similar to Google Docs', as well. Users can invite anyone to view and edit their documents, though those without a Proton account will be prompted to create one first. The free tier of Proton Drive includes essential document features so people don't have to pay for the service if they don't want to. Participants will be able to add comments to the document, reply to them and resolve them. And users will see other participants' presence and their cursor placements in real time, so that they know who's working on which part of the document and so that their edits don't clash.</p><p>Proton didn't say whether the launch of Docs means it's going to roll out analogues of Google's other Workspace apps in the future, but the company did <a data-i13n="cpos:3;pos:1" href="https://www.engadget.com/proton-encrypted-email-vpn-calendar-rebrand-103024950.html" data-ylk="slk:expand its offerings;cpos:3;pos:1;elm:context_link;itc:0;sec:content-canvas">expand its offerings</a> with several different products over the last few years. In addition to Drive cloud storage — and, of course, its email service — the company has a VPN, an encrypted calendar and even a <a data-i13n="cpos:4;pos:1" href="https://www.engadget.com/proton-launches-its-own-password-manager-115039870.html" data-ylk="slk:password manager;cpos:4;pos:1;elm:context_link;itc:0;sec:content-canvas">password manager</a>. Docs will make its way to Proton users over the coming days.</p><p>This article contains affiliate links; if you click such a link and make a purchase, we may earn a commission.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Received an AI Email (631 pts)]]></title>
            <link>https://timharek.no/blog/i-received-an-ai-email</link>
            <guid>40862865</guid>
            <pubDate>Wed, 03 Jul 2024 05:05:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://timharek.no/blog/i-received-an-ai-email">https://timharek.no/blog/i-received-an-ai-email</a>, See on <a href="https://news.ycombinator.com/item?id=40862865">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Why AI Infrastructure Startups Are Insanely Hard to Build (172 pts)]]></title>
            <link>https://nextword.substack.com/p/why-ai-infrastructure-startups-are</link>
            <guid>40862436</guid>
            <pubDate>Wed, 03 Jul 2024 03:15:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nextword.substack.com/p/why-ai-infrastructure-startups-are">https://nextword.substack.com/p/why-ai-infrastructure-startups-are</a>, See on <a href="https://news.ycombinator.com/item?id=40862436">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Recently, </span><a href="http://adept.ai/" rel="">Adept AI</a><span> announced </span><a href="https://techcrunch.com/2024/06/28/amazon-hires-founders-away-from-ai-startup-adept/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAM7kyimVR-Nntc7w3SCp466ss9rI61B5U68ESaJExyUE65kA2h6sdH5pHKpdJ0oi6Y0SvXy7wg2OgsX1JTB-hZw9n0esnLkFCY_6JckUVqoIbgGFEy2gSjzMw4YJBYwbCFKJqPR19xgviwpcnO8cCVPa99I_tMkjrjBWHoQqPtTI" rel="">they are being acquired by Amazon</a><span>, and this solidified a somewhat controversial opinion I’ve held for a while - </span><strong>that AI infra startups are a tarpit idea</strong><span>, </span><strong><span>especially as a “venture-scale” business.</span></strong></p><p><span>The term “tarpit idea” refers to startup ideas that sound reasonable on the surface, but when put to test against reality or rigorous thought, fail to hold up.</span><br></p><div><p><span>I believe most AI infra startups will also fall into this category, where AI infra refers to the “building blocks” companies </span><strong>between the cloud layer and the application layer</strong><span> - RAG services, finetuning infrastructure, text processing services, TTS APIs, vector databases, etc. I won’t name specific names, but just think of any AI infra startup that raised sizable seed rounds off of open source or social media momentum.</span></p></div><p><span>I also believe </span><strong>many founders agree with this viewpoint</strong><span>, which explains the sale of Adept (to Amazon), </span><a href="https://openai.com/index/openai-acquires-rockset/" rel="">Rockset (to OpenAI)</a><span>, InflectionAI (to Microsoft), as well as the soon to be acquisitions of Stability (if it happens), </span><a href="http://character.ai/" rel="">Character</a><span>AI, etc. Every incumbent is looking at M&amp;A to paint an “end-to-end AI platform” story. Only a lucky few will get bought.</span><br></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;June 28 updated AI Infra market map&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="June 28 updated AI Infra market map" title="June 28 updated AI Infra market map" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddeab17f-4637-437e-baeb-cd2ffb0e846a_1600x900.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Source: Bessemer Venture Partners</figcaption></figure></div><div><p><span>So why is selling AI infrastructure </span><em>as a startup</em><span> a tarpit idea? On paper, it’s perfectly reasonable to sell picks and shovels amidst proliferation of AI startups and enterprises building Gen AI features. After all, there’s over 30K “.ai” domains registered every month.</span></p></div><div><p><strong>In a nutshell, the new AI infra startups will struggle to succeed because they lack significant differentiation and capital to crack the enterprise segment.</strong><span> It’s not the startups’ fault, the real problem is competitive dynamics. There’s simply too many entities offering the same table stakes features within 1-3 months apart from each other, which creates a collective tarpit dynamic, where only the incumbents can keep swimming.</span></p></div><p>The argument goes:</p><ul><li><p>For AI infra startups to be “venture scale”, they will eventually need to win over enterprise customers. No question. That requires the startups to have some sustainable edge that separates their products from the incumbents’ (GCP, AWS, as well as the likes of Vercel, Databricks, Datadog, etc).</p></li><li><p>Unfortunately, most cutting edge innovation either comes from the incumbents or the research / OSS community - and incumbents are in a better position to commercialize the innovations because they have more usage data than startups, as well as the relationships.</p></li><li><p>To add salt to the injury, any good ideas that originate from startups get benchmarked and copied quickly. For example, I was quite surprised how quickly Databricks and Datadog caught up to the leading LLMOps products from the startup world (e.g. Arize AI). </p></li><li><p>Furthermore, OSS community can’t help but create OSS versions of other AI infra startups’ products - perhaps a testament to how easy it has become to write software.</p></li><li><p>Thus, startups struggle to maintain a sustainable lead over the incumbents to buy them time to win enterprise contracts.</p></li><li><p>And enterprise customers are incentivized to “hold off” on onboarding new vendors, because vendor products diminish in value so quickly because AI landscape changes every few months.</p></li><li><p>This ultimately lengthens sales cycles, and increases churn, which hurts startups more than the incumbents.</p></li></ul><div><p><span>There are also some other dynamics at play (to be discussed in the next section) - but essentially the AI infrastructure space becomes a grind that favors players with the longest runways.</span></p></div><div><p><span>My intention here is not to doom-post, but to highlight some real challenges, which I’m happy to be wrong on (DM me if you disagree). Also, I will end by offering some advice to AI infra startups.</span></p></div><p><em><span>To clarify, by “AI infra startup”, I’m referring to “venture scale” AI infrastructure startups. I’m sure founders can create essentially system integration agencies targeting SMB or mid market, and call themselves an AI infra company. But that’s a completely different business with a much smaller upside.</span><br></em></p><p>There’s three other major forces that’s worsening the competitive environment:</p><ol><li><p>Builders are now conditioned to “demand” composability, a.k.a making it easy to switch out your product for others’. This is great for application layer companies, but not infrastructure companies. Developers can rip out Langchain with Llamaindex, OpenAI models with Claude 3.5 through AWS Bedrock, etc. Every layer of the LLM training and inference stack has at least 10+ viable solutions, that it becomes difficult to create any type of lock-in.</p></li><li><p>The ongoing plummeting of inference costs also plays a role. The COGS are dropping fast, so AI infra players need to constantly price-match the incumbents who have the biggest economies of scale. Models or code have little perceived differentiation, so the consumption goes to the lowest cost providers (incumbents).</p></li><li><p>Incumbents seem to all have the same business strategy of creating an “end-to-end AI platform”. Databricks is getting into AI model training and business intelligence, competing with AWS Sagemaker and Tableau. Github Workspaces is getting into AI-powered security reviews, etc.</p><ol><li><p>Everyone’s default product strategy is to own all upstream and downstream workloads from their core product, which unintentionally makes startups’ lives more difficult, since it becomes hard to compete with a point solution.</p></li></ol></li></ol><p><br><span>With all these challenges, some AI infra startups have chosen to go vertical or move to the application layer. For example, I have been tracking a “Business Intelligence with Natural Language” startup since late 2022 that has pivoted three times already from:</span></p><ul><li><p>a general purpose “chat with data” platform, to</p></li><li><p>“chat with business intelligence data” platform, to</p></li><li><p>“chat with financial data” platform.</p></li></ul><div><p><span>The AI infra darlings LlamaIndex and Langchain also took this path of focus when it comes to their enterprise-oriented products. LlamaIndex is focusing on managed document parsing / OCR, whereas Langchain is focusing on LLMOps and agent building solutions. My guess is that both are working on narrowing their focus even further, since even selling a managed document parsing service is a huge scope for a seed-stage startup, given that Google and AWS already have existing vertical text extraction services. It’s not easy.</span></p></div><div><p><span>Narrowing the scope and going vertical is a typical response for AI infra startups - but I argue that these pivots rarely work out and cause new set of problems. Most importantly, these vertical pivots underestimate the importance of deep domain expertise once you go vertical, which many AI infra founders lack. Accumulating domain knowledge is time consuming. Also, your product may need to be heavily customized for the unique needs of the vertical, which means lower margins.</span></p></div><p><span>Not to mention, these application layer ecosystems have even worse competition (e.g. VCs’ LegalTech ecosystem maps ran out of space to put new logos long time ago). There’s not just the other AI startup competition, but competition from the legacy software companies. Pivoting to a vertical does not suddenly get rid of your competitors - you will just have new ones in that vertical who have been there before you. For example, legal tech industry has existed for ages, and many Legal AI companies are now competing with </span><strong>the legacy legal tech providers plus system integrators.</strong></p><div><p><span>So what’s the solution for AI infra startups? Should we all hope to be acquihired, or is it possible for startups to also stay independent for longer and find product market fit?</span></p><p><span>Here’s a somewhat anti-climatic answer, but the solution for startups goes back to the fundamentals: </span><strong>think deeply about how to be different from the incumbents.</strong><span> Here are four ways to iterate from here:</span></p></div><ul><li><p><strong>Narrow down the scope even further:</strong><span> focus on a very tiny segment of enterprise customers, as opposed to serving all customers. Don’t build all the integrations. Be a managed RAG service for customers using Salesforce with on-prem VMWare, as opposed to a general purpose RAG service. Startups don’t have the resources to solve for every environment, at least initially.</span></p></li><li><p><strong>Focus on just one workload:</strong><span> startups shouldn’t try to solve for too many workloads. Do one thing really well. Don’t try to be a platform for finetuning any LLM - there’s already too many of those. Instead, try to be the best platform for finetuning Tagalog models. The catch: the TAM might be too small.</span></p></li><li><p><strong>Raise more VC money than you think you need:</strong><span> long runways are non-negotiable. It can take a while for enterprises to be receptive to buying startup AI infra solutions, if ever. Be prepared for the worst case scenario.</span></p></li><li><p><strong>Or, don’t raise any VC money at all:</strong><span> raising VC money kind of forces you to orient business strategy around selling to the enterprise - which might be not something you can or want to do. You want the flexibility to work on more interesting and promising problems when they arise, given there’s constantly new changes in AI landscape.</span></p></li></ul><p><span>Lastly, AI startups should be open to being acquired by a larger player, even if it’s not a prestigious destination like OpenAI or Google. </span><strong>My view is that M&amp;A landscape for AI infrastructure sector will become worse, not better, over time.</strong></p><p><span>The acquisition market will become more “efficient” as the winners/losers emerge, and the workloads and enterprise needs become more clearly defined. Thus, in order to sell your startup at an “attractive” valuation, it needs to be marketed prior to the dust settles when the market is less efficient. Don’t wait for another 18 months to shop your startup, when all AI infra startups start running out of runway at the same time.</span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Bridges Don't Sink (208 pts)]]></title>
            <link>https://practical.engineering/blog/2024/7/2/why-bridges-dont-sink</link>
            <guid>40861520</guid>
            <pubDate>Tue, 02 Jul 2024 23:43:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://practical.engineering/blog/2024/7/2/why-bridges-dont-sink">https://practical.engineering/blog/2024/7/2/why-bridges-dont-sink</a>, See on <a href="https://news.ycombinator.com/item?id=40861520">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" id="item-668414c1a012432cf6a43a4b" data-layout-label="Post Body" data-type="item" data-updated-on="1719932256684">
  <p><em>[Note that this article is a transcript of the video embedded above.]</em></p><p>The essence of a bridge is not just that it goes over something, but that there’s clear space underneath for a river, railway, or road. Maybe this is already obvious to you, but bridges present a unique structural challenge. In a regular road, the forces are transferred directly into the ground. On a bridge, all those forces on the span get concentrated into the piers or abutments on either side. Because of that, bridge substructures are among the strongest engineered systems on the planet. And yet, bridge foundations are built in some of the least ideal places for heavy loading. Rivers and oceans have soft, mucky soils that can’t hold much weight. Plus, obviously, a lot of them are underwater.</p><p>What happens when you overload soil with a weight it can’t handle? In engineering-speak, it’s called a bearing failure, but it’s as simple as stepping in the mud. The foundation just sinks into the ground. But, what if you just keep loading it and causing it to sink deeper and deeper? Congratulations! You just invented one of the most widely used structural members on earth: the humble foundation pile. How do they work, and how can you install them underwater? I’m Grady, and this is Practical Engineering. Today we’re having piles of fun talking about deep foundations.</p><p>I did a video all about the different types of foundations used in engineering, but I didn’t go too deep into piles. A pile is a fairly simple structural member, just a long pole driven or drilled into the ground. But, behind that simplicity is a lot of terrifically complex engineering. Volume 1 of the Federal Highway Administration’s manual on the Design and Construction of Driven Pile Foundations is over 500 pages long. There are 11 pages of symbols, 2 pages of acronyms, and you don’t even get to the introduction until page 46. And just a little further than that, you get some history of driven piles. Namely that the history has been lost to time. Humans have been hammering sticks into the ground since way before we knew how to write about it. And that’s pretty much all a driven pile is.</p><p>The first piles were made from timber, and wood is still used all these years around the world. Timber piles are cheap, resilient to driving forces, and easy to install. But, wood rots, it has an upper limit on length from the size of the tree, and it’s not that strong compared to the alternatives. Concrete piles solve a lot of those problems. They come in a variety of sizes and shapes, and again, are widely used for deep foundations. One disadvantage of concrete piles is that they have to be pretty big to withstand the force required to drive them into ground. Some concrete piles can be upwards of 30 inches or 75 centimeters wide. It is hard to hit something that big hard enough to drive it downward into soil, and a lot of ground has to either get out of the way or compress in place to make room. Steel piles solve that problem since they can be a lot more slender. Pipe piles are just what they sound like, and the other major alternative is an H-pile. Your guess is as good as mine why the same steel shape is an <em>I</em>-beam but an <em>H</em>-pile. But, no matter the material, all driven piles are installed in basically the same way.&nbsp;</p><p>Newton’s third law applies to piles like everything else. To push one deep into the ground creates an equal and opposite reaction. You <em>would </em>need either an enormous weight to take advantage of gravity or some other strong structure attached to the ground to react against and develop the pushing force required to drive it downward. Instead of those two options, we usually just use a hammer. By dropping a comparatively small weight from a height, we convert the potential energy of the weight at that height into kinetic energy. The force required to stop the hammer as it falls gets transferred into the pile. Hopefully this is intuitive. It’s pretty hard to push a nail into wood, but it’s pretty easy to hammer it in... well, it’s <em>a little bit</em> easier to hammer it in. There are quite a few types of pile drivers, but most of them use a large hammer or vibratory head to create the forces required.</p><p>Maybe it goes without saying, but the main goal of a foundation is to not move. When you apply a load, you want it to stay put. Luckily, piles have two ways to do that (at least for vertical loads). The first is end-bearing. The end, or toe, of a pile can be driven down to a layer of strong soil or hard rock, making it able to withstand greater loads. But there’s not always a firm stratum at a reasonable depth below the ground. Quote-unquote “bedrock” is a simple idea, but in practice, geology is more complicated than that. Luckily, piles have a second type of resistance: skin friction, also known as shaft resistance. When you drive a pile, it compacts and densifies the surrounding soil, not only adding strength to the soil itself, but creating friction along the walls of the pile that hold it in place. The deeper you go, the more friction you get. Let me show you what I mean.</p><p>I have my own pipe pile in the backyard that I’ve marked with an arbitrary scale. When I drop the hammer at a prescribed height, the pile is driven a certain distance into the ground. Do this enough times, and eventually, you reach a point where the pile kind of stops moving with each successive hammer blow. In technical terms, the pile has reached refusal. I can graph the blow count required to drive the pile to each depth, and you get a pretty nice curve. It’s easy to see how it got stronger against vertical loads the deeper I drove it in. Toward the end, it barely moved with each hit. This is a really nice aspect of driven piles, you install them in a similar way to how they’ll be loaded by the final design. Of course, bridges and buildings don’t hammer on their foundations, but they do impose vertical loads. The tagline of the Pile Driving Contractors Association is “A Driven Pile is a Tested Pile” because, just by installing them, you’ve verified that they can withstand a certain amount of force. After all, you had to overcome that force to get them in the ground. And if you’re not seeing enough resistance, in most cases, you can just keep driving downward until you do!</p><p>But piles don’t just resist downward forces. Structures experience loads in other directions too. Buildings have horizontal, or lateral, loads from wind. Bridges see lateral loads from flowing water, and even ice or boats contacting the piers. Both can experience uplift forces that counteract gravity from floods due to buoyancy or strong winds. If you’ve ever hammered in a tent stake, you know that piles can withstand loading from all kinds of directions. And then there’s scour. The soil along a bridge might look like this right after the bridge is built, but after a few floods, it can look completely different. Engineers have to try and predict how the soil around a bridge will scour over time, from natural changes in the streambed and those created by the bridge itself. Then they make sure to design foundations that can accommodate those changes and stay strong over the long term. This is why bridge foundations sometimes look kind of funny. Loads transfer from the superstructure down into the piers. The piers sit on a pile cap that transfers and distributes loads into the piles themselves. Those piles can be vertical, but if the engineer is expecting serious lateral loads, some of the piles are often inclined, also called battered piles. Inclined piles take better advantage of the shaft resistance to make the foundation stronger against horizontal loads.</p><p>As important and beneficial as they are, driven piles have some limitations too. For one, they’re noisy and disruptive to install. Just last year, I had two friends on separate trips to Seattle who sent me a video of the exact same pile-driving operation. It’s good to have friends who know how much you like construction. But my point is, this type of construction is pretty much impossible to ignore. In dense urban areas, most people are just not willing to put up with the constant banging. Plus the vibrations from installing them can disrupt surrounding infrastructure. Pile driving is crude; in many cases, the piles aren’t designed to withstand the forces of the structure they’ll support but rather the forces they’ll have to experience during installation which are much higher. They can’t easily go through hard geological layers, cobbles, or boulders; they can wander off path, since you can’t really see where you’re going, and they can cause the ground to heave because you’re not removing any soil while you force them into the subsurface. The second major category of piles solves a lot of these problems.</p><p>And, wouldn’t you know it? There’s an FHWA manual that has all the juicy details - Drilled Shafts: Construction Procedures and Design Methods. This one a whopping 747 pages long. A drilled shaft is also exactly what it sounds like. The basic process is pretty simple. Drill a long hole into the ground. Place reinforcing steel in the hole. Then fill the whole thing with concrete. But, bridge piers are often, as you probably know, installed underwater. Pouring concrete underwater is a little tricky. Imagine trying to pour a smoothie at the bottom of a pool! Let me show you what I mean.</p><p>This is my garage-special bridge foundation simulator. It has transparent soil in the form of superabsorbent polymer beads… and you know we have to add some blue water too. You can probably imagine how easy it might be to drill a hole in this soil. It’s just going to collapse in on itself. We need a way to keep the hole open so the rebar and concrete can be installed. So, drilled shafts installed in soft soils or wet conditions usually rely on a casing to support the walls. Installing a casing usually happens while the hole is drilled, following the auger downward. I tried that myself, but I only have two hands, and it was pretty unwieldy. So, just for the sake of the demo, I’m advancing the casing into the soil ahead of time. Now I can drill out the soil to open the shaft. And now I’m realizing the limitations of my soil simulant. It was still pretty hard to do, even with the casing in place. It took a few tries, but I managed to get most of it out.</p><p>So now I have an open hole, but it’s still full of water. Even if your casing runs above the water surface, and you try to pump it out, you can still have water leaking in from the bottom. In ideal conditions, you can get a nice seal between the bottom of the casing and the soil, but even then, it’s pretty hard to keep water out of the hole, and luckily it doesn’t matter.</p><p>Instead of concrete, I’m using bentonite clay as a substitute. It’s got a similar density, and it’s perfect for this demo because you can push it through a small tube… if you get the proportions right. Ask me how I know. This is me pondering the life decisions that led up to me holding a gigantic syringe full of bentonite slurry in my garage. You can’t just drop this stuff through the water. It mixes and dilutes, just turning into a mess. Same is true for concrete. The ratio of water to cement in a concrete mix is essential to its strength and performance, so you can’t do anything that would add water to the mix. The trick is a little device called a tremie. Even though it has a funny name, it’s nothing more than a pipe that runs to the bottom of the hole. As long as you keep the end of the tremie below the surface of the concrete that you’re pumping in, or concrete simulant in my case, there’s no chance for it to mix with the water and dilute. I’m just pushing the clay into the casing with a big syringe, making sure to keep the end of the tube buried. Because concrete is a lot more dense than water, it just displaces it upward, out of the hole.&nbsp;</p><p>In underwater installations, the casing is often left in place. One advantage is that you can build a floating pile cap. Instead of building a big cofferdam and drying out the work area to construct a big concrete structure, sometimes you can raise the pile cap into or above the water surface, reducing the complexity of its construction. These “high rise” pile caps are used a lot in offshore wind turbines. But, not all casings are permanent.</p><p>In some situations, it’s possible to pull the casing once the hole is full of concrete, saving the sometimes enormous cost of each gigantic steel tube. I tried to show this in my demo. It’s not beautiful, but it did work. Again, the concrete is dense, so the pressure it exerts on the walls of the hole is enough to keep the soil from collapsing. And because drilled shafts can be much larger than driven piles, sometimes you don’t even need a group of them. Lots of structures, including wind turbines, highway signs, and more, are built on mono-pile foundations. Just a single drilled shaft deep in the ground, eliminating the need for a pile cap altogether. Another interesting aspect of drilled shafts is that you can ream out the bottom, creating an enlarged base that increases the surface area at the toe. This helps reduce a pile’s tendency to sink, and it can help with uplift resistance too.</p><p>Driven piles and drilled shafts are far from the only types of deep foundation systems. There are tons of variations on the idea that have been developed over the years to solve specific challenges: Continuous flight auger piles do the drilling and concreting in essentially one step, using a hollow-stem auger to fill the hole as it’s removed. Then reinforcement is lowered into the wet concrete. You can fill a hole with compacted aggregate instead of concrete, called a stone column or tradename Geopier if you’re only worried about compressive loads. Helical or screw piles twist into the ground, instead of being hammered, reducing vibrations and disturbance. Micropiles are like tiny drilled shafts used when there are access restrictions or geologic constraints. And of course, there are sheet piles that aren’t really used for foundations but are driven piles meant to create a wall or barrier. Let me know if I forgot to mention your favorite flavor of pile.</p><p>Even though they’re usually much stronger than shallow foundations, piles can and do fail. We’ve talked about San Francisco’s famous Millennium Tower in a previous video. That’s a skyscraper on a pile foundation that sank into the ground, causing the building to tilt. It seems like they mostly have it fixed now, but it’s still in the news every so often, so only time will tell. In 2004, a bridge pier on the Lee Roy Selmon Expressway in Tampa, Florida sank 11 feet (more than 3 meters) while it was still under construction because of the complicated geology. It cost 90 million dollars to fix and delayed the project’s completion by a year. These case studies highlight the complexity of geotechnical engineering when we ask the ground to hold up heavier and heavier loads. The science and technology that goes into designing deep foundations are enough to spend an entire career studying, but hopefully, this video gives you a little insight into how they work.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Illustrated Transformer (2018) (139 pts)]]></title>
            <link>https://jalammar.github.io/illustrated-transformer/</link>
            <guid>40861148</guid>
            <pubDate>Tue, 02 Jul 2024 22:42:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a>, See on <a href="https://news.ycombinator.com/item?id=40861148">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p><span>Discussions:
<a href="https://news.ycombinator.com/item?id=18351674">Hacker News (65 points, 4 comments)</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/8uh2yz/p_the_illustrated_transformer_a_visual_look_at/">Reddit r/MachineLearning (29 points, 3 comments)</a>
</span>
<br>
<span>Translations: <a href="https://www.mundhor.site/post/post14">Arabic</a>, <a href="https://blog.csdn.net/yujianmin1990/article/details/85221271">Chinese (Simplified) 1</a>, <a href="https://blog.csdn.net/qq_36667170/article/details/124359818">Chinese (Simplified) 2</a>, <a href="https://a-coles.github.io/2020/11/15/transformer-illustre.html">French 1</a>, <a href="https://lbourdois.github.io/blog/nlp/Transformer/">French 2</a>, <a href="https://medium.com/@val.mannucci/il-transformer-illustrato-it-37a78e3e2348">Italian</a>, <a href="https://tips-memo.com/translation-jayalmmar-transformer">Japanese</a>, <a href="https://nlpinkorean.github.io/illustrated-transformer/">Korean</a>, <a href="http://dml.qom.ac.ir/2022/05/17/illustrated-transformer/">Persian</a>, <a href="https://habr.com/ru/post/486358/">Russian</a>, <a href="https://www.ibidemgroup.com/edu/transformer-ilustrado-jay-alammar/">Spanish 1</a>, <a href="https://hackernoon.com/el-transformador-ilustrado-una-traduccion-al-espanol-0y73wwp">Spanish 2</a>, <a href="https://trituenhantao.io/tin-tuc/minh-hoa-transformer/">Vietnamese</a></span>
<br>
<span>Watch: MIT’s <a href="https://youtu.be/53YvP6gdD7U?t=432">Deep Learning State of the Art</a> lecture referencing this post</span>
<br>
<span>Featured in courses at <a href="https://web.stanford.edu/class/cs224n/">Stanford</a>, <a href="https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/transformers">Harvard</a>, <a href="https://ocw.mit.edu/courses/6-s897-machine-learning-for-healthcare-spring-2019/d39a6eed387ee90b1f72a01949c35c7b_MIT6_S897S19_lec8.pdf">MIT</a>, <a href="https://www.cs.princeton.edu/courses/archive/fall22/cos597G/">Princeton</a>, <a href="https://www.cs.cmu.edu/~leili/course/mldl22w/14-Transformer.pdf">CMU</a> and others</span></p>

<p>In the <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">previous post, we looked at Attention</a> – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at <strong>The Transformer</strong> – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their <a href="https://cloud.google.com/tpu/">Cloud TPU</a> offering. So let’s try to break the model apart and look at how it functions.</p>

<p>The Transformer was proposed in the paper <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a>. A TensorFlow implementation of it is available as a part of the <a href="https://github.com/tensorflow/tensor2tensor">Tensor2Tensor</a> package. Harvard’s NLP group created a <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">guide annotating the paper with PyTorch implementation</a>. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.</p>

<p><strong>2020 Update</strong>: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:</p>

<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/-QH8fRhqFHM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p>
<h2 id="a-high-level-look">A High-Level Look</h2>
<p>Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.</p>

<p><img src="https://jalammar.github.io/images/t/the_transformer_3.png">
</p>

<!--more-->

<p>Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.</p>

<p><img src="https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png">
</p>

<p>The encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.</p>

<p><img src="https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png">
</p>

<p>The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:</p>

<p><img src="https://jalammar.github.io/images/t/Transformer_encoder.png">
</p>

<p>The encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.</p>

<p>The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.</p>

<p>The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">seq2seq models</a>).</p>

<p><img src="https://jalammar.github.io/images/t/Transformer_decoder.png">
</p>

<h2 id="bringing-the-tensors-into-the-picture">Bringing The Tensors Into The Picture</h2>

<p>Now that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.</p>

<p>As is the case in NLP applications in general, we begin by turning each input word into a vector using an <a href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca">embedding algorithm</a>.</p>



<p><img src="https://jalammar.github.io/images/t/embeddings.png">
  <br>
  Each word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes.
</p>

<p>The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.</p>

<p>After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.</p>

<p><img src="https://jalammar.github.io/images/t/encoder_with_tensors.png">
  <br>

</p>

<p>Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.</p>

<p>Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.</p>

<h2 id="now-were-encoding">Now We’re Encoding!</h2>

<p>As we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.</p>

<p><img src="https://jalammar.github.io/images/t/encoder_with_tensors_2.png">
  <br>
  The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.
</p>

<h2 id="self-attention-at-a-high-level">Self-Attention at a High Level</h2>
<p>Don’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.</p>

<p>Say the following sentence is an input sentence we want to translate:</p>

<p>”<code>The animal didn't cross the street because it was too tired</code>”</p>

<p>What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.</p>

<p>When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.</p>

<p>As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.</p>

<p>If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization.png">
  <br>
  As we are encoding the word "it" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on "The Animal", and baked a part of its representation into the encoding of "it".
</p>

<p>Be sure to check out the <a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb">Tensor2Tensor notebook</a> where you can load a Transformer model, and examine it using this interactive visualization.</p>

<h2 id="self-attention-in-detail">Self-Attention in Detail</h2>
<p>Let’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented – using matrices.</p>

<p>The <strong>first step</strong> in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.</p>

<p>Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.</p>



<p><img src="https://jalammar.github.io/images/t/transformer_self_attention_vectors.png">
  <br>
  Multiplying <span>x1</span> by the <span>WQ</span> weight matrix produces <span>q1</span>, the "query" vector associated with that word. We end up creating a "query", a "key", and a "value" projection of each word in the input sentence.
</p>



<div><p>What are the “query”, “key”, and “value” vectors?
</p><p>

They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.</p></div>

<p>The <strong>second step</strong> in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.</p>

<p>The score is calculated by taking the dot product of the <span>query vector</span> with the <span>key vector</span> of the respective word we’re scoring. So if we’re processing the self-attention for the word in position <span>#1</span>, the first score would be the dot product of <span>q1</span> and <span>k1</span>. The second score would be the dot product of <span>q1</span> and <span>k2</span>.</p>



<p><img src="https://jalammar.github.io/images/t/transformer_self_attention_score.png">
  <br>

</p>



<p>The <strong>third and fourth steps</strong> are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.</p>



<p><img src="https://jalammar.github.io/images/t/self-attention_softmax.png">
  <br>

</p>

<p>This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.</p>



<p>The <strong>fifth step</strong> is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).</p>

<p>The <strong>sixth step</strong> is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).</p>



<p><img src="https://jalammar.github.io/images/t/self-attention-output.png">
  <br>
</p>

<p>That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.</p>

<h2 id="matrix-calculation-of-self-attention">Matrix Calculation of Self-Attention</h2>
<p><strong>The first step</strong> is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix <span>X</span>, and multiplying it by the weight matrices we’ve trained (<span>WQ</span>, <span>WK</span>, <span>WV</span>).</p>

<p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png">
  <br>
  Every row in the <span>X</span> matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)
</p>



<p><strong>Finally</strong>, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.</p>

<p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png">
  <br>
  The self-attention calculation in matrix form
</p>





<h2 id="the-beast-with-many-heads">The Beast With Many Heads</h2>

<p>The paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:</p>

<ol>
  <li>
    <p>It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.</p>
  </li>
  <li>
    <p>It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.</p>
  </li>
</ol>

<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png">
   <br>
   With multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.
 </p>

<p><br>
If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices</p>

<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_z.png">
  <br>

</p>



<p>This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.</p>

<p>How do we do that? We concat the matrices then multiply them by an additional weights matrix WO.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png">
  <br>

</p>

<p>That’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place</p>



<p><img src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png">
  <br>

</p>



<p>Now that we have touched upon attention heads, let’s revisit our example from before to see where the different attention heads are focusing as we encode the word “it” in our example sentence:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png">
  <br>
  As we encode the word "it", one attention head is focusing most on "the animal", while another is focusing on "tired" -- in a sense, the model's representation of the word "it" bakes in some of the representation of both "animal" and "tired".
</p>



<p>If we add all the attention heads to the picture, however, things can be harder to interpret:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png">
  <br>
</p>

<h2 id="representing-the-order-of-the-sequence-using-positional-encoding">Representing The Order of The Sequence Using Positional Encoding</h2>
<p>One thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.</p>

<p>To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.</p>



<p><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png">
  <br>
  To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.
</p>


<p>If we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_example.png">
  <br>
  A real example of positional encoding with a toy embedding size of 4
</p>



<p>What might this pattern look like?</p>

<p>In the following figure, each row corresponds to a positional encoding of a vector. So the first row would be the vector we’d add to the embedding of the first word in an input sequence. Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png">
  <br>
  A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors.
</p>

<p>The formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in <a href="https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py"><code>get_timing_signal_1d()</code></a>. This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).</p>

<p><strong>July 2020 Update:</strong> 
The positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. <a href="https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipynb">Here’s the code to generate it</a>:</p>

<p><img src="https://jalammar.github.io/images/t/attention-is-all-you-need-positional-encoding.png">
  <br>
</p>

<h2 id="the-residuals">The Residuals</h2>
<p>One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a <a href="https://arxiv.org/abs/1607.06450">layer-normalization</a> step.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png">
  <br>
</p>

<p>If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png">
  <br>
</p>

<p>This goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png">
  <br>
</p>

<h2 id="the-decoder-side">The Decoder Side</h2>
<p>Now that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.</p>

<p>The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:</p>

<p><img src="https://jalammar.github.io/images/t/transformer_decoding_1.gif">
  <br>
  After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).
</p>

<p>The following steps repeat the process until a special <end of="" sentence=""> symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.</end></p>

<p><img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif">
  <br>

</p>

<p>The self attention layers in the decoder operate in a slightly different way than the one in the encoder:</p>

<p>In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to <code>-inf</code>) before the softmax step in the self-attention calculation.</p>

<p>The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.</p>

<h2 id="the-final-linear-and-softmax-layer">The Final Linear and Softmax Layer</h2>

<p>The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.</p>

<p>The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.</p>

<p>Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.</p>

<p>The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.</p>



<p><img src="https://jalammar.github.io/images/t/transformer_decoder_output_softmax.png">
  <br>
  This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.
</p>



<h2 id="recap-of-training">Recap Of Training</h2>
<p>Now that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.</p>

<p>During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.</p>

<p>To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “&lt;eos&gt;” (short for ‘end of sentence’)).</p>

<p><img src="https://jalammar.github.io/images/t/vocabulary.png">
   <br>
   The output vocabulary of our model is created in the preprocessing phase before we even begin training.
 </p>

<p>Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector:</p>

<p><img src="https://jalammar.github.io/images/t/one-hot-vocabulary-example.png">
  <br>
  Example: one-hot encoding of our output vocabulary
</p>

<p>Following this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.</p>

<h2 id="the-loss-function">The Loss Function</h2>
<p>Say we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.</p>

<p>What this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.</p>

<p><img src="https://jalammar.github.io/images/t/transformer_logits_output_and_label.png">
  <br>
  Since the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model's weights using backpropagation to make the output closer to the desired output.
</p>



<p>How do you compare two probability distributions? We simply subtract one from the other. For more details, look at  <a href="https://colah.github.io/posts/2015-09-Visual-Information/">cross-entropy</a> and <a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">Kullback–Leibler divergence</a>.</p>

<p>But note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:</p>

<ul>
  <li>Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)</li>
  <li>The first probability distribution has the highest probability at the cell associated with the word “i”</li>
  <li>The second probability distribution has the highest probability at the cell associated with the word “am”</li>
  <li>And so on, until the fifth output distribution indicates ‘<code>&lt;end of sentence&gt;</code>’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.</li>
</ul>

<p><img src="https://jalammar.github.io/images/t/output_target_probability_distributions.png">
   <br>
   The targeted probability distributions we'll train our model against in the training example for one sample sentence.
 </p>



<p>After training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:</p>

<p><img src="https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png">
    <br>
    Hopefully upon training, the model would output the right translation we expect. Of course it's no real indication if this phrase was part of the training dataset (see: <a href="https://www.youtube.com/watch?v=TIgfjmp-4BA">cross validation</a>). Notice that every position gets a little bit of probability even if it's unlikely to be the output of that time step -- that's a very useful property of softmax which helps the training process.
</p>

<p>Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters that you can experiment with.</p>

<h2 id="go-forth-and-transform">Go Forth And Transform</h2>

<p>I hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:</p>

<ul>
  <li>Read the <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> paper, the Transformer blog post (<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer: A Novel Neural Network Architecture for Language Understanding</a>), and the <a href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html">Tensor2Tensor announcement</a>.</li>
  <li>Watch <a href="https://www.youtube.com/watch?v=rBCqOTEfxvg">Łukasz Kaiser’s talk</a> walking through the model and its details</li>
  <li>Play with the <a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb">Jupyter Notebook provided as part of the Tensor2Tensor repo</a></li>
  <li>Explore the <a href="https://github.com/tensorflow/tensor2tensor">Tensor2Tensor repo</a>.</li>
</ul>

<p>Follow-up works:</p>

<ul>
  <li><a href="https://arxiv.org/abs/1706.03059">Depthwise Separable Convolutions for Neural Machine Translation</a></li>
  <li><a href="https://arxiv.org/abs/1706.05137">One Model To Learn Them All</a></li>
  <li><a href="https://arxiv.org/abs/1801.09797">Discrete Autoencoders for Sequence Models</a></li>
  <li><a href="https://arxiv.org/abs/1801.10198">Generating Wikipedia by Summarizing Long Sequences</a></li>
  <li><a href="https://arxiv.org/abs/1802.05751">Image Transformer</a></li>
  <li><a href="https://arxiv.org/abs/1804.00247">Training Tips for the Transformer Model</a></li>
  <li><a href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations</a></li>
  <li><a href="https://arxiv.org/abs/1803.03382">Fast Decoding in Sequence Models using Discrete Latent Variables</a></li>
  <li><a href="https://arxiv.org/abs/1804.04235">Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</a></li>
</ul>

<h2 id="acknowledgements">Acknowledgements</h2>
<p>Thanks to <a href="https://twitter.com/ilblackdragon">Illia Polosukhin</a>, <a href="http://jakob.uszkoreit.net/">Jakob Uszkoreit</a>, <a href="https://www.linkedin.com/in/llion-jones-9ab3064b">Llion Jones </a>, <a href="https://ai.google/research/people/LukaszKaiser">Lukasz Kaiser</a>, <a href="https://twitter.com/nikiparmar09">Niki Parmar</a>, and <a href="https://dblp.org/pers/hd/s/Shazeer:Noam">Noam Shazeer</a> for providing feedback on earlier versions of this post.</p>

<p>Please hit me up on <a href="https://twitter.com/JayAlammar">Twitter</a> for any corrections or feedback.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brazil data regulator bans Meta from mining data to train AI models (137 pts)]]></title>
            <link>https://apnews.com/article/brazil-tech-meta-privacy-data-93e00b2e0e26f7cc98795dd052aea8e1</link>
            <guid>40861057</guid>
            <pubDate>Tue, 02 Jul 2024 22:29:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/brazil-tech-meta-privacy-data-93e00b2e0e26f7cc98795dd052aea8e1">https://apnews.com/article/brazil-tech-meta-privacy-data-93e00b2e0e26f7cc98795dd052aea8e1</a>, See on <a href="https://news.ycombinator.com/item?id=40861057">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>RIO DE JANEIRO (AP) — Brazil’s national data protection authority determined on Tuesday that Meta, the parent company of Instagram and Facebook, cannot use data originating in the country to train its artificial intelligence. </p><p>Meta’s updated privacy policy enables the company to feed people’s public posts into its AI systems. That practice will not be permitted in Brazil, however.</p><p>The decision stems from “the imminent risk of serious and irreparable or difficult-to-repair damage to the fundamental rights of the affected data subjects,” the agency said in the nation’s official gazette. </p><p>Brazil is one of Meta’s biggest markets. Facebook alone has around 102 million active users in the country, the agency said in a statement. The nation has a population of 203 million, according to the country’s 2022 census.</p><p>A spokesperson for Meta said in a statement the company is “disappointed” and insists its method “complies with privacy laws and regulations in Brazil.”</p>
    

<p>“This is a step backwards for innovation, competition in AI development and further delays bringing the benefits of AI to people in Brazil,” the spokesperson added.</p>



<p>The social media company has also encountered resistance to its privacy policy update in Europe, where it recently put on hold its plans to start feeding people’s public posts into training AI systems — which was supposed to start last week.</p><p>In the U.S., where there’s no national law protecting online privacy, such training is already happening.</p>
    
<p>Meta said on its Brazilian blog in May that it could “use information that people have shared publicly about Meta’s products and services for some of our generative AI features,” which could include “public posts or photos and their captions.”</p><p>Refusing to partake is possible, Meta said in that statement. Despite that option, there are “excessive and unjustified obstacles to accessing the information and exercising” the right to opt out, the agency said in a statement. </p>
    

<p>Meta did not provide sufficient information to allow people to be aware of the possible consequences of using their personal data for the development of generative AI, it added.</p><p>Meta isn’t the only company that has sought to train its AI systems on data from Brazilians.</p><p>Human Rights Watch released a report last month that found that personal photos of identifiable Brazilian children sourced from a large database of online images — pulled from parent blogs, the websites of professional event photographers and video-sharing sites such as YouTube — were being used to create AI image-generator tools without families’ knowledge. In some cases, those tools have been used create AI-generated nude imagery.</p><p>Hye Jung Han, a Brazil-based researcher for the rights group, said in an email Tuesday that the regulator’s action “helps to protect children from worrying that their personal data, shared with friends and family on Meta’s platforms, might be used to inflict harm back on them in ways that are impossible to anticipate or guard against.”</p><p>But the decision regarding Meta will “very likely” encourage other companies to refrain from being transparent in the use of data in the future, said Ronaldo Lemos, of the Institute of Technology and Society of Rio de Janeiro, a think-tank. </p>
    

<p>“Meta was severely punished for being the only one among the Big Tech companies to clearly and in advance notify in its privacy policy that it would use data from its platforms to train artificial intelligence,” he said. </p><p>Compliance must be demonstrated by the company within five working days from the notification of the decision, and the agency established a daily fine of 50,000 reais ($8,820) for failure to do so.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[All I want for Christmas is a negative leap second (169 pts)]]></title>
            <link>https://qntm.org/leap</link>
            <guid>40860831</guid>
            <pubDate>Tue, 02 Jul 2024 22:01:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qntm.org/leap">https://qntm.org/leap</a>, See on <a href="https://news.ycombinator.com/item?id=40860831">Hacker News</a></p>
Couldn't get https://qntm.org/leap: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Python Modern Practices (103 pts)]]></title>
            <link>https://www.stuartellis.name/articles/python-modern-practices/</link>
            <guid>40860803</guid>
            <pubDate>Tue, 02 Jul 2024 21:56:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.stuartellis.name/articles/python-modern-practices/">https://www.stuartellis.name/articles/python-modern-practices/</a>, See on <a href="https://news.ycombinator.com/item?id=40860803">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><a href="https://www.python.org/" target="_blank" rel="noreferrer">Python</a> has a long history, and it has evolved over time. This article describes some agreed modern best practices.</p>
<h2 id="using-python">Using Python <span><a href="#using-python" aria-label="Anchor">#</a></span></h2><h3 id="install-python-with-tools-that-support-multiple-versions">Install Python With Tools That Support Multiple Versions <span><a href="#install-python-with-tools-that-support-multiple-versions" aria-label="Anchor">#</a></span></h3><p>Use a tool like <a href="https://mise.jdx.dev/" target="_blank" rel="noreferrer">mise</a> or <a href="https://github.com/pyenv/pyenv" target="_blank" rel="noreferrer">pyenv</a> to install Python on your development systems, so that you can switch between different versions of Python for your projects. This enables you to upgrade each project to a new version of Python without interfering with other tools and projects that use Python.</p>
<p>Alternatively, consider using <a href="https://containers.dev/" target="_blank" rel="noreferrer">Development Containers</a>, which enable you to define an isolated environment for a software project. This also allows you to use a separate version of Python for each project.</p>
<p>Ensure that the tool compiles Python, rather than downloading <a href="https://gregoryszorc.com/docs/python-build-standalone/main/" target="_blank" rel="noreferrer">standalone builds</a>. The standlone builds are modified versions of Python that are maintained by a third-party. Both the pyenv tool and the <a href="https://github.com/devcontainers/features/blob/main/src/python/README.md" target="_blank" rel="noreferrer">Visual Studio Code Dev Container feature</a> automatically compile Python, but you must <a href="https://mise.jdx.dev/lang/python.html#precompiled-python-binaries" target="_blank" rel="noreferrer">change the mise configuration</a> to use compilation. <a href="https://pdm-project.org/" target="_blank" rel="noreferrer">PDM</a> and <a href="https://hatch.pypa.io/" target="_blank" rel="noreferrer">Hatch</a> always download standalone builds when you use them to set up versions of Python.</p>
<p>If your operating system includes a Python installation, avoid using it. This Python installation is for operating system tools. It is likely to use an older version of Python, and may not include all of the standard features. An operating system copy of Python should be <a href="https://packaging.python.org/en/latest/specifications/externally-managed-environments/#externally-managed-environments" target="_blank" rel="noreferrer">marked</a> to prevent you from installing packages into it, but not all operating systems set the marker.</p>
<h3 id="use-the-most-recent-version-of-python-that-you-can">Use The Most Recent Version of Python That You Can <span><a href="#use-the-most-recent-version-of-python-that-you-can" aria-label="Anchor">#</a></span></h3><p>For new projects, choose the most recent stable version of Python 3. This ensures that you have the latest security fixes, as well as the fastest performance.</p>
<p>Upgrade your projects as new Python versions are released. The Python development team usually support each version for five years, but some Python libraries may only support each version of Python for a shorter period of time. If you use tools that support multiple versions of Python and automated testing, you can test your projects on new Python versions with little risk.</p>
<p><span>
    <span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M506.3 417l-213.3-364c-16.33-28-57.54-28-73.98 0l-213.2 364C-10.59 444.9 9.849 480 42.74 480h426.6C502.1 480 522.6 445 506.3 417zM232 168c0-13.25 10.75-24 24-24S280 154.8 280 168v128c0 13.25-10.75 24-23.1 24S232 309.3 232 296V168zM256 416c-17.36 0-31.44-14.08-31.44-31.44c0-17.36 14.07-31.44 31.44-31.44s31.44 14.08 31.44 31.44C287.4 401.9 273.4 416 256 416z"></path></svg>
</span>
  </span>
  <span><em>Avoid using Python 2.</em> It is not supported by the Python development team or by the developers of most popular Python libraries.</span>
</p>

<h3 id="use-pipx-to-run-developer-applications">Use pipx To Run Developer Applications <span><a href="#use-pipx-to-run-developer-applications" aria-label="Anchor">#</a></span></h3><p>Use <a href="https://pipx.pypa.io/" target="_blank" rel="noreferrer">pipx</a> to run Python applications on development systems, rather than installing the applications with <em>pip</em> or another method. This ensures that each application has the correct libraries, because <em>pipx</em> automatically puts the libraries for each application into a separate <a href="https://docs.python.org/3/tutorial/venv.html" target="_blank" rel="noreferrer">Python virtual environment</a>.</p>
<p>Consider using the <a href="https://pipx.pypa.io/stable/#walkthrough-running-an-application-in-a-temporary-virtual-environment" target="_blank" rel="noreferrer">pipx run</a> command, rather than <em>pipx install</em>. The <em>pipx run</em> command downloads and runs the application without installing it. Each application is cached for several days after the first download, which means that <em>pipx run</em> may not be slower than running a manually installed application.</p>
<p>The Python Packaging Authority maintain <em>pipx</em>, but it is not included with Python. To install <em>pipx</em>, run this command:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>python3 -m pip install --user pipx
</span></span></code></pre></div><p>This enables you to use any Python command-line application with <em>pipx</em>. For example, this command downloads and runs the latest version of <a href="https://github.com/aristocratos/bpytop" target="_blank" rel="noreferrer">bpytop</a>, a system monitoring tool:</p>
<blockquote>
<p><a href="https://peps.python.org/pep-0668/#guide-users-towards-virtual-environments" target="_blank" rel="noreferrer">PEP 668 - Marking Python base environments as “externally managed”</a> recommends that users install Python applications with pipx.</p>
</blockquote>
<h2 id="developing-python-projects">Developing Python Projects <span><a href="#developing-python-projects" aria-label="Anchor">#</a></span></h2><h3 id="avoid-using-poetry">Avoid Using Poetry <span><a href="#avoid-using-poetry" aria-label="Anchor">#</a></span></h3><p>Avoid using <a href="https://python-poetry.org/" target="_blank" rel="noreferrer">Poetry</a> for new projects. Poetry predates many standards for Python tooling. This means that it uses non-standard implementations of key features, such as the dependency resolver and configuration formats in <em>pyproject.toml</em> files.</p>
<p>If you would like to use a similar tool to develop your applications, consider using <a href="https://pdm-project.org/" target="_blank" rel="noreferrer">PDM</a>. <a href="https://hatch.pypa.io/" target="_blank" rel="noreferrer">Hatch</a> is another alternative to Poetry, but it is most useful for developing Python libraries. Both of these tools follow modern standards, which avoids compatibility issues.</p>
<h3 id="use-a-pyprojecttoml-file">Use a pyproject.toml File <span><a href="#use-a-pyprojecttoml-file" aria-label="Anchor">#</a></span></h3><p>Create a <em>pyproject.toml</em> file in the root directory of each Python project. Use this file as the central place to store configuration information about the project and the tools that it uses. The <a href="https://www.pyopensci.org/python-package-guide/package-structure-code/pyproject-toml-python-package-metadata.html" target="_blank" rel="noreferrer">pyOpenSci project documentation on pyproject.toml</a> provides an introduction to the file format.</p>
<p>Modern Python tools use the <em>pyproject.toml</em> file to store configuration. Some tools support <em>pyproject.toml</em>, but do not use it by default. Python project management tools like <a href="https://pdm-project.org/" target="_blank" rel="noreferrer">PDM</a> and <a href="https://hatch.pypa.io/" target="_blank" rel="noreferrer">Hatch</a> automatically create and use a <em>pyproject.toml</em> file.</p>
<blockquote>
<p>The various features of <em>pyproject.toml</em> files are defined these PEPs: <a href="https://peps.python.org/pep-0517/" target="_blank" rel="noreferrer">PEP 517</a>, <a href="https://peps.python.org/pep-0518/" target="_blank" rel="noreferrer">PEP 518</a>, <a href="https://peps.python.org/pep-0621/" target="_blank" rel="noreferrer">PEP 621</a> and <a href="https://peps.python.org/pep-0660/" target="_blank" rel="noreferrer">PEP 660</a>.</p>
</blockquote>
<h3 id="create-a-directory-structure-that-uses-the-src-layout">Create a Directory Structure That Uses the src Layout <span><a href="#create-a-directory-structure-that-uses-the-src-layout" aria-label="Anchor">#</a></span></h3><p>Python itself does not require a specific directory structure for your projects. The Python packaging documentation describes two popular directory structures: <a href="https://packaging.python.org/en/latest/discussions/src-layout-vs-flat-layout/" target="_blank" rel="noreferrer">the src layout and the flat layout</a>.
The <a href="https://www.pyopensci.org/python-package-guide/package-structure-code/python-package-structure.html" target="_blank" rel="noreferrer">pyOpenSci project documentation on directory structures</a> explains the practical differences between the two.</p>
<p>For modern Python projects, use the src layout. This requires you to use <a href="https://setuptools.pypa.io/en/latest/userguide/development_mode.html" target="_blank" rel="noreferrer">editable installs</a> of the packages in your project, but tools like <a href="https://pdm-project.org/" target="_blank" rel="noreferrer">PDM</a> and <a href="https://hatch.pypa.io/" target="_blank" rel="noreferrer">Hatch</a> will handle this for you.</p>
<h3 id="use-virtual-environments-for-development">Use Virtual Environments for Development <span><a href="#use-virtual-environments-for-development" aria-label="Anchor">#</a></span></h3><p>The <a href="https://docs.python.org/3/tutorial/venv.html" target="_blank" rel="noreferrer">virtual environments</a> feature enables you to define separate sets of packages for each Python project, so that the packages for a project do not conflict with any other Python packages on the system. Always use Python virtual environments for your projects.</p>
<p>Several tools automate creating and switching between virtual environments. The <a href="https://mise.jdx.dev/" target="_blank" rel="noreferrer">mise</a> version manager includes <a href="https://mise.jdx.dev/lang/python.html#automatic-virtualenv-activation" target="_blank" rel="noreferrer">support for virtual environments</a>. The <a href="https://github.com/pyenv/pyenv" target="_blank" rel="noreferrer">pyenv</a> version manager supports virtual environments with the <a href="https://github.com/pyenv/pyenv-virtualenv" target="_blank" rel="noreferrer">virtualenv plugin</a>. If you use a tool like <a href="https://pdm-project.org/" target="_blank" rel="noreferrer">PDM</a> or <a href="https://hatch.pypa.io/" target="_blank" rel="noreferrer">Hatch</a> to develop your projects, these also manage Python virtual environments for you.</p>
<p>You can set up and use virtual environments with <em>venv</em>, which is part of the Python standard library, but this is a manual process.</p>
<h3 id="use-requirementstxt-files-to-install-packages-into-environments">Use requirements.txt Files to Install Packages Into Environments <span><a href="#use-requirementstxt-files-to-install-packages-into-environments" aria-label="Anchor">#</a></span></h3><p>Avoid using <em>pip</em> commands to install packages into virtual environments. If you use <a href="https://pdm-project.org/" target="_blank" rel="noreferrer">PDM</a> or <a href="https://hatch.pypa.io/" target="_blank" rel="noreferrer">Hatch</a>, they manage packages in development and test environments. For other cases, create a <em>requirements.txt</em> file that specifies all of the packages that are required in the environment.</p>
<p>You can create <em>requirements.txt</em> files with whichever tool is appropriate. For example, PDM includes <a href="https://pdm-project.org/en/stable/usage/lockfile/#export-locked-packages-to-alternative-formats" target="_blank" rel="noreferrer">an export feature</a> that creates <em>requirements.txt</em> files. If you do not already have a tool to create <em>requirements.txt</em> files, use the <em>pip-compile</em> utility that is provided by <a href="https://github.com/jazzband/pip-tools/" target="_blank" rel="noreferrer">pip-tools</a>.</p>
<p>You can then use the <em>pip-sync</em> utility in <a href="https://github.com/jazzband/pip-tools/" target="_blank" rel="noreferrer">pip-tools</a> to add the packages that are specified in the <em>requirements.txt</em> file into a target virtual environment. The <em>pip-sync</em> utility ensures that the packages in a virtual environment match the list in the <em>requirements.txt</em> file.</p>
<p>If you need to install packages without using <em>pip-sync</em>, run <em>pip install</em> with a <em>requirements.txt</em> file. For example, these commands install the packages that are specified by the file <em>requirements-dev.txt</em> into the virtual environment <em>.venv</em>:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span><span>source</span> ./.venv/bin/activate
</span></span><span><span>python3 -m pip install -r requirements-dev.txt
</span></span></code></pre></div><blockquote>
<p>Ensure that your <em>requirements.txt</em> files include hashes for the packages. The <em>pip-compile</em> utility generates <em>requirements.txt</em> files with hashes if you specify the <em>generate-hashes</em> option.</p>
</blockquote>
<h3 id="format-your-code">Format Your Code <span><a href="#format-your-code" aria-label="Anchor">#</a></span></h3><p>Use a formatting tool with a plugin to your editor, so that your code is automatically formatted to a consistent style.</p>
<p><a href="https://black.readthedocs.io/en/stable/" target="_blank" rel="noreferrer">Black</a> is currently the most popular code formatting tool for Python, but consider using <a href="https://docs.astral.sh/ruff/" target="_blank" rel="noreferrer">Ruff</a>. Ruff provides both code formatting and quality checks for Python code.</p>
<p>Run the formatting tool with your CI system, so that it rejects any code that does not match the format for your project.</p>
<h3 id="use-a-code-linter">Use a Code Linter <span><a href="#use-a-code-linter" aria-label="Anchor">#</a></span></h3><p>Use a code linting tool with a plugin to your editor, so that your code is automatically checked for issues.</p>
<p><a href="https://flake8.pycqa.org/en/latest/" target="_blank" rel="noreferrer">flake8</a> is currently the most popular linter for Python, but consider using <a href="https://docs.astral.sh/ruff/" target="_blank" rel="noreferrer">Ruff</a>. Ruff includes the features of both flake8 itself and the most popular plugins for flake8.</p>
<p>Run the linting tool with your CI system, so that it rejects any code that does not meet the standards for your project.</p>
<h3 id="test-with-pytest">Test with pytest <span><a href="#test-with-pytest" aria-label="Anchor">#</a></span></h3><p>Use <a href="http://pytest.org/" target="_blank" rel="noreferrer">pytest</a> for testing. It has superseded <em>nose</em> as the most popular testing system for Python. Use the <em>unittest</em> module in the standard library for situations where you cannot add <em>pytest</em> to the project.</p>
<h3 id="package-your-applications">Package Your Applications <span><a href="#package-your-applications" aria-label="Anchor">#</a></span></h3><p>Use <em>wheel</em> packages for libraries, or for tools that are intended to be used with an existing installation of Python. If you publish your Python application as a <em>wheel</em>, other developers can use it with <em>pipx</em> and <em>pip-sync</em>. These packages cannot be used without a Python installation.</p>
<p>In most cases, you should package an application in a format that enables you to include your code, the dependencies and a copy of the required version of Python. This ensures that your code runs with the expected version of Python, and has the correct version of each dependency.</p>
<p>Use container images to package applications that provide a network service, such as a Web application. Use <a href="https://pyinstaller.org/" target="_blank" rel="noreferrer">PyInstaller</a> to publish desktop and command-line applications as a single executable file. Each container image and PyInstaller file includes a copy of Python, along with your code and the required dependencies.</p>
<h2 id="language-syntax">Language Syntax <span><a href="#language-syntax" aria-label="Anchor">#</a></span></h2><h3 id="use-type-hinting">Use Type Hinting <span><a href="#use-type-hinting" aria-label="Anchor">#</a></span></h3><p>Current versions of Python support type hinting. Consider using type hints in any critical application. If you develop a shared library, use type hints.</p>
<p>Once you add type hints, the <a href="http://www.mypy-lang.org/" target="_blank" rel="noreferrer">mypy</a> tool can check your code as you develop it. Code editors can also read type hints to display information about the code that you are working with.</p>
<p>If you use <a href="https://docs.pydantic.dev/" target="_blank" rel="noreferrer">Pydantic</a> in your application, it can work with type hints. Use the <a href="https://docs.pydantic.dev/latest/integrations/mypy/" target="_blank" rel="noreferrer">mypy plugin for Pydantic</a> to improve the integration between mypy and Pydantic.</p>
<blockquote>
<p><a href="https://peps.python.org/pep-0484/" target="_blank" rel="noreferrer">PEP 484 - Type Hints</a> and <a href="https://peps.python.org/pep-0526/" target="_blank" rel="noreferrer">PEP 526 – Syntax for Variable Annotations</a> define the notation for type hinting.</p>
</blockquote>
<h3 id="format-strings-with-f-strings">Format Strings with f-strings <span><a href="#format-strings-with-f-strings" aria-label="Anchor">#</a></span></h3><p>The new <a href="https://docs.python.org/3/reference/lexical_analysis.html#f-strings" target="_blank" rel="noreferrer">f-string</a> syntax is both more readable and has better performance than older methods. Use f-strings instead of <em>%</em> formatting, <em>str.format()</em> or <em>str.Template()</em>.</p>
<p>The older features for formatting strings will not be removed, to avoid breaking backward compatibility.</p>
<p>The f-strings feature was added in version 3.6 of Python. Alternate implementations of Python may include this specific feature, even when they do not support version 3.6 syntax.</p>
<blockquote>
<p><a href="https://www.python.org/dev/peps/pep-0498/" target="_blank" rel="noreferrer">PEP 498</a> explains f-strings in detail.</p>
</blockquote>
<h3 id="use-datetime-objects-with-time-zones">Use Datetime Objects with Time Zones <span><a href="#use-datetime-objects-with-time-zones" aria-label="Anchor">#</a></span></h3><p>Always use <em>datetime</em> objects that are <a href="https://docs.python.org/3/library/datetime.html?highlight=datetime#aware-and-naive-objects" target="_blank" rel="noreferrer">aware</a> of time zones. By default, Python creates <em>datetime</em> objects that do not include a time zone. The documentation refers to <em>datetime</em> objects without a time zone as <strong>naive</strong>.</p>
<p>Avoid using <em>date</em> objects, except where the time of day is completely irrelevant. The <em>date</em> objects are always <strong>naive</strong>, and do not include a time zone.</p>
<p>Use aware <em>datetime</em> objects with the UTC time zone for timestamps, logs and other internal features.</p>
<p>To get the current time and date in UTC as an aware <em>datetime</em> object, specify the UTC time zone with <em>now()</em>. For example:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> <span>datetime</span> <span>import</span> <span>datetime</span><span>,</span> <span>timezone</span>
</span></span><span><span>
</span></span><span><span><span>dt</span> <span>=</span> <span>datetime</span><span>.</span><span>now</span><span>(</span><span>timezone</span><span>.</span><span>utc</span><span>)</span>
</span></span></code></pre></div><p>Python 3.9 and above include the <strong>zoneinfo</strong> module. This provides access to the standard IANA database of time zones. Previous versions of Python require a third-party library for time zones.</p>
<blockquote>
<p><a href="https://www.python.org/dev/peps/pep-0615/" target="_blank" rel="noreferrer">PEP 615</a> describes support for the IANA time zone database with <strong>zoneinfo</strong>.</p>
</blockquote>
<h3 id="use-enum-or-named-tuples-for-immutable-sets-of-key-value-pairs">Use enum or Named Tuples for Immutable Sets of Key-Value Pairs <span><a href="#use-enum-or-named-tuples-for-immutable-sets-of-key-value-pairs" aria-label="Anchor">#</a></span></h3><p>Use the <em>enum</em> type in Python 3.4 or above for immutable collections of key-value pairs. Enums can use class inheritance.</p>
<p>Python 3 also has <em>collections.namedtuple()</em> for immutable key-value pairs. Named tuples do not use classes.</p>
<h3 id="create-data-classes-for-custom-data-objects">Create Data Classes for Custom Data Objects <span><a href="#create-data-classes-for-custom-data-objects" aria-label="Anchor">#</a></span></h3><p>The data classes feature enables you to reduce the amount of code that you need to define classes for objects that exist to store values. The new syntax for data classes does not affect the behavior of the classes that you define with it. Each data class is a standard Python class.</p>
<p>You can set a <em>frozen</em> option to make <a href="https://docs.python.org/3/library/dataclasses.html#frozen-instances" target="_blank" rel="noreferrer">frozen instances</a> of a data class.</p>
<p>Data classes were introduced in version 3.7 of Python.</p>
<blockquote>
<p><a href="https://www.python.org/dev/peps/pep-0557/" target="_blank" rel="noreferrer">PEP 557</a> describes data classes.</p>
</blockquote>
<h3 id="use-collectionsabc-for-custom-collection-types">Use collections.abc for Custom Collection Types <span><a href="#use-collectionsabc-for-custom-collection-types" aria-label="Anchor">#</a></span></h3><p>The abstract base classes in <em>collections.abc</em> provide the components for building your own custom collection types.</p>
<p>Use these classes, because they are fast and well-tested. The implementations in Python 3.7 and above are written in C, to provide better performance than Python code.</p>
<h3 id="use-breakpoint-for-debugging">Use breakpoint() for Debugging <span><a href="#use-breakpoint-for-debugging" aria-label="Anchor">#</a></span></h3><p>This function drops you into the debugger at the point where it is called. Both the <a href="https://docs.python.org/3/library/pdb.html" target="_blank" rel="noreferrer">built-in debugger</a> and external debuggers can use these breakpoints.</p>
<p>The <a href="https://docs.python.org/3/library/functions.html#breakpoint" target="_blank" rel="noreferrer">breakpoint()</a> feature was added in version 3.7 of Python.</p>
<blockquote>
<p><a href="https://www.python.org/dev/peps/pep-0553/" target="_blank" rel="noreferrer">PEP 553</a> describes the <em>breakpoint()</em> function.</p>
</blockquote>
<h2 id="application-design">Application Design <span><a href="#application-design" aria-label="Anchor">#</a></span></h2><h3 id="use-logging-for-diagnostic-messages-rather-than-print">Use Logging for Diagnostic Messages, Rather Than print() <span><a href="#use-logging-for-diagnostic-messages-rather-than-print" aria-label="Anchor">#</a></span></h3><p>The built-in <em>print()</em> statement is convenient for adding debugging information, but you should include logging in your scripts and applications. Use the <a href="https://docs.python.org/3/library/logging.html#logrecord-attributes" target="_blank" rel="noreferrer">logging</a> module in the standard library, or a third-party logging module.</p>
<h3 id="use-the-toml-format-for-configuration">Use The TOML Format for Configuration <span><a href="#use-the-toml-format-for-configuration" aria-label="Anchor">#</a></span></h3><p>Use <a href="https://toml.io/" target="_blank" rel="noreferrer">TOML</a> for data files that must be written or edited by human beings. Use the JSON format for data that is transferred between computer programs. Avoid using the INI or YAML formats.</p>
<p>Python 3.11 and above include <em>tomllib</em> to read the TOML format. Use <a href="https://pypi.org/project/tomli/" target="_blank" rel="noreferrer">tomli</a> to add support for reading TOML to applications that run on older versions of Python.</p>
<p>If your Python software needs to generate TOML, add <a href="https://pypi.org/project/tomli-w/" target="_blank" rel="noreferrer">Tomli-W</a>.</p>
<blockquote>
<p><a href="https://peps.python.org/pep-0680/" target="_blank" rel="noreferrer">PEP 680 - tomllib: Support for Parsing TOML in the Standard Library</a> explains why TOML is now included with Python.</p>
</blockquote>
<h3 id="only-use-async-where-it-makes-sense">Only Use async Where It Makes Sense <span><a href="#only-use-async-where-it-makes-sense" aria-label="Anchor">#</a></span></h3><p>The <a href="https://docs.python.org/3/library/asyncio.html" target="_blank" rel="noreferrer">asynchronous features of Python</a> enable a single process to avoid blocking on I/O operations. To achieve concurrency with Python, you must run multiple Python processes. Each of these processes may or may not use asynchronous I/O.</p>
<p>To run multiple application processes, either use a container system, with one container per process, or an application server like <a href="https://gunicorn.org/" target="_blank" rel="noreferrer">Gunicorn</a>. If you need to build a custom application that manages muliple processes, use the <a href="https://docs.python.org/3/library/multiprocessing.html" target="_blank" rel="noreferrer">multiprocessing</a> package in the Python standard library.</p>
<p>Code that uses asynchronous I/O must not call <em>any</em> function that uses synchronous I/O, such as <em>open()</em>, or the <em>logging</em> module in the standard library. Instead, you need to use either the equivalent functions from <em>asyncio</em> in the standard library or a third-party library that is designed to support asynchronous code.</p>
<p>The <a href="https://fastapi.tiangolo.com/" target="_blank" rel="noreferrer">FastAPI</a> Web framework supports <a href="https://fastapi.tiangolo.com/async/" target="_blank" rel="noreferrer">using both synchronous and asynchronous functions in the same application</a>. You must still ensure that asynchronous functions never call any synchronous function.</p>
<p>If you would like to work with <em>asyncio</em>, use Python 3.7 or above. Version 3.7 of Python introduced <a href="https://docs.python.org/3/library/contextvars.html" target="_blank" rel="noreferrer">context variables</a>, which enable you to have data that is local to a specific <em>task</em>, as well as the <em>asyncio.run()</em> function.</p>
<blockquote>
<p><a href="https://www.python.org/dev/peps/pep-0567/" target="_blank" rel="noreferrer">PEP 0567</a> describes context variables.</p>
</blockquote>
<h2 id="libraries">Libraries <span><a href="#libraries" aria-label="Anchor">#</a></span></h2><h3 id="handle-command-line-input-with-argparse">Handle Command-line Input with argparse <span><a href="#handle-command-line-input-with-argparse" aria-label="Anchor">#</a></span></h3><p>The <a href="https://docs.python.org/3/library/argparse.html" target="_blank" rel="noreferrer">argparse</a> module is now the recommended way to process command-line input. Use <em>argparse</em>, rather than the older <em>optparse</em> and <em>getopt</em>.</p>
<p>The <em>optparse</em> module is officially deprecated, so update code that uses <em>optparse</em> to use <em>argparse</em> instead.</p>
<p>Refer to <a href="https://docs.python.org/3/howto/argparse.html" target="_blank" rel="noreferrer">the argparse tutorial</a> in the official documentation for more details.</p>
<h3 id="use-pathlib-for-file-and-directory-paths">Use pathlib for File and Directory Paths <span><a href="#use-pathlib-for-file-and-directory-paths" aria-label="Anchor">#</a></span></h3><p>Use <a href="https://docs.python.org/3/library/pathlib.html" target="_blank" rel="noreferrer">pathlib</a> objects instead of strings whenever you need to work with file and directory pathnames.</p>
<p>Consider using the <a href="https://docs.python.org/3/library/pathlib.html#correspondence-to-tools-in-the-os-module" target="_blank" rel="noreferrer">the pathlib equivalents for os functions</a>.</p>
<p>The existing methods in the standard library have been updated to support Path objects.</p>
<p>To list all of the the files in a directory, use either the <em>.iterdir()</em> function of a Path object, or the <em>os.scandir()</em> function.</p>
<p>This <a href="https://realpython.com/working-with-files-in-python/#directory-listing-in-modern-python-versions" target="_blank" rel="noreferrer">RealPython article</a> provides a full explanation of the different Python functions for working with files and directories.</p>
<p>The <em>pathlib</em> module was added to the standard library in Python 3.4, and other standard library functions were updated to support Path objects in version 3.5 of Python.</p>
<h3 id="use-osscandir-instead-of-oslistdir">Use os.scandir() Instead of os.listdir() <span><a href="#use-osscandir-instead-of-oslistdir" aria-label="Anchor">#</a></span></h3><p>The <em>os.scandir()</em> function is significantly faster and more efficient than <em>os.listdir()</em>. Use <em>os.scandir()</em> wherever you previously used the <em>os.listdir()</em> function.</p>
<p>This function provides an iterator, and works with a context manager:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> <span>os</span>
</span></span><span><span>
</span></span><span><span><span>with</span> <span>os</span><span>.</span><span>scandir</span><span>(</span><span>'some_directory/'</span><span>)</span> <span>as</span> <span>entries</span><span>:</span>
</span></span><span><span>    <span>for</span> <span>entry</span> <span>in</span> <span>entries</span><span>:</span>
</span></span><span><span>        <span>print</span><span>(</span><span>entry</span><span>.</span><span>name</span><span>)</span>
</span></span></code></pre></div><p>The context manager frees resources as soon as the function completes. Use this option if you are concerned about performance or concurrency.</p>
<p>The <em>os.walk()</em> function now calls <em>os.scandir()</em>, so it automatically has the same improved performance as this function.</p>
<p>The <em>os.scandir()</em> function was added in version 3.5 of Python.</p>
<blockquote>
<p><a href="https://www.python.org/dev/peps/pep-0471/" target="_blank" rel="noreferrer">PEP 471</a> explains <em>os.scandir()</em>.</p>
</blockquote>
<h3 id="run-external-commands-with-subprocess">Run External Commands with subprocess <span><a href="#run-external-commands-with-subprocess" aria-label="Anchor">#</a></span></h3><p>The <a href="https://docs.python.org/3/library/subprocess.html" target="_blank" rel="noreferrer">subprocess</a> module provides a safe way to run external commands. Use <em>subprocess</em> rather than shell backquoting or the functions in <em>os</em>, such as <em>spawn</em>, <em>popen2</em> and <em>popen3</em>. The <em>subprocess.run()</em> function in current versions of Python is sufficient for most cases.</p>
<blockquote>
<p><a href="https://www.python.org/dev/peps/pep-0324/" target="_blank" rel="noreferrer">PEP 324</a> explains the technical details of subprocess in detail.</p>
</blockquote>
<h3 id="use-httpx-for-web-clients">Use httpx for Web Clients <span><a href="#use-httpx-for-web-clients" aria-label="Anchor">#</a></span></h3><p>Use <a href="https://www.python-httpx.org/" target="_blank" rel="noreferrer">httpx</a> for Web client applications. It <a href="https://www.python-httpx.org/http2/" target="_blank" rel="noreferrer">supports HTTP/2</a>, and <a href="https://www.python-httpx.org/async/" target="_blank" rel="noreferrer">async</a>. The httpx package supersedes <a href="https://requests.readthedocs.io/en/latest/" target="_blank" rel="noreferrer">requests</a>, which only supports HTTP 1.1.</p>
<p>Avoid using <em>urllib.request</em> from the Python standard library. It was designed as a low-level library, and lacks the features of httpx.</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple poised to get OpenAI board observer role as part of AI pact (128 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-07-02/apple-to-get-openai-board-observer-role-as-part-of-ai-agreement</link>
            <guid>40860363</guid>
            <pubDate>Tue, 02 Jul 2024 21:01:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-07-02/apple-to-get-openai-board-observer-role-as-part-of-ai-agreement">https://www.bloomberg.com/news/articles/2024-07-02/apple-to-get-openai-board-observer-role-as-part-of-ai-agreement</a>, See on <a href="https://news.ycombinator.com/item?id=40860363">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a search engine for Hacker News (198 pts)]]></title>
            <link>https://hackernews.demo.vectara.com/</link>
            <guid>40860022</guid>
            <pubDate>Tue, 02 Jul 2024 20:11:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hackernews.demo.vectara.com/">https://hackernews.demo.vectara.com/</a>, See on <a href="https://news.ycombinator.com/item?id=40860022">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Google's carbon emissions surge nearly 50% due to AI energy demand (149 pts)]]></title>
            <link>https://www.cnbc.com/2024/07/02/googles-carbon-emissions-surge-nearly-50percent-due-to-ai-energy-demand.html</link>
            <guid>40859993</guid>
            <pubDate>Tue, 02 Jul 2024 20:07:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/07/02/googles-carbon-emissions-surge-nearly-50percent-due-to-ai-energy-demand.html">https://www.cnbc.com/2024/07/02/googles-carbon-emissions-surge-nearly-50percent-due-to-ai-energy-demand.html</a>, See on <a href="https://news.ycombinator.com/item?id=40859993">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-107402399" data-test="InlineImage"><p>A view of the Google headquarters in Mountain View, California, on April 16, 2024.</p><p>Tayfun Coskun | Anadolu | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/GOOGL/">Google</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>'s emissions surged nearly 50% compared to 2019, the company said Tuesday in its <a href="https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf" target="_blank">2024 environmental report</a>, marking a notable setback in its goal to achieve net-zero emissions by 2030.&nbsp;</p><p>Google's emissions also increased 13% year over year in 2023, per the report.</p><p>The company attributed the emissions spike to an increase in data center energy consumption and supply chain emissions driven by rapid advancements in and demand for <a href="https://www.cnbc.com/ai-artificial-intelligence/">artificial intelligence</a>. The report noted that the company's total data center electricity consumption grew 17% in 2023.&nbsp;</p><p>The impact of AI on electricity demand is well documented. Electricity demand is <a href="https://www.cnbc.com/2024/05/05/ai-could-drive-natural-gas-boom-as-utilities-face-surging-electric-demand.html">forecast to grow</a> as much as 20% by 2030, with AI data centers alone expected to add about 323 terawatt hours of electricity demand in the U.S., CNBC previously reported.</p><p>While renewables will likely play an important role in meeting AI energy demands, analysts say that immediate implementation is challenging. This is due to factors such as the time required to build the power lines that transport resources to the data centers, Wells Fargo analyst Roger Read<a href="https://www.cnbc.com/2024/05/05/ai-could-drive-natural-gas-boom-as-utilities-face-surging-electric-demand.html"> previously told</a> CNBC.</p><p>Google said in the report that its data centers are 1.8 times as energy efficient as a typical data center. The company added that it remains committed to mitigating the environmental impact of AI through model optimization, efficient infrastructure and emissions reductions.&nbsp;</p><p>Google is not the only major tech company to face increased emissions due to AI demand. <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-6"><a href="https://www.cnbc.com/quotes/MSFT/">Microsoft</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> reported in May that its <a href="https://docs.google.com/document/u/0/d/1Oreyv8tjRotBp3Gl0yGcuBmJjfqTcRu2kGv3Xxet8IM/edit" target="_blank">total carbon emissions</a> rose nearly 30% since 2020 primarily due to the construction of data centers.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sonar is destroying my job and it's driving me to despair (120 pts)]]></title>
            <link>https://community.sonarsource.com/t/sonar-is-destroying-my-job-and-its-driving-me-to-despair/92438</link>
            <guid>40859937</guid>
            <pubDate>Tue, 02 Jul 2024 20:00:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://community.sonarsource.com/t/sonar-is-destroying-my-job-and-its-driving-me-to-despair/92438">https://community.sonarsource.com/t/sonar-is-destroying-my-job-and-its-driving-me-to-despair/92438</a>, See on <a href="https://news.ycombinator.com/item?id=40859937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/DiscussionForumPosting" id="main-outlet" role="main">
      <meta itemprop="headline" content="Sonar is destroying my job and it's driving me to despair">
      
      <meta itemprop="datePublished" content="2023-06-13T12:17:38Z">
        <meta itemprop="articleSection" content="SonarQube">
      <meta itemprop="keywords" content="kotlin, sonarqube">
      


          <div itemprop="text" id="post_1">
              <p>First of all, I understand that Sonar is a well intentioned product.</p>
<p>It’s right <em>most</em> of the time.  Unfortunately coding is, of course, complex and it’s challenging for a product like Sonar to keep pace with the fast changing syntax of newer languages.</p>
<p>I’m an experienced Developer and Team Lead, currently coding in Kotlin with Coroutines. I’ve also worked with Java, C++, C, Objective-C and Swift.  We can all learn more, but I’ve been around the block and know a few things about what code quality looks like and how to make pragmatic decisions around it.</p>
<p>Our ‘default’ Sonar setup is currently making me ‘butcher’ my Kotlin code to comply with it’s rules and, as you may tell, it’s really upsetting.</p>
<p>You may say <em>“The rules can be customised”</em>, or <em>“Allow an exception”</em>.  You need to understand <strong>that is not a simple option for many of your users</strong>.  In my case, I have a superior who administers Sonar and is, let’s say, completely committed to it.  For any ‘exception granted’ we would have to book time with them days in advance then white-board the reason why Sonar is wrong, or produce a sample program - who has got time for that with tight deadlines?</p>
<p>Please rethink your UX with the realisation that there are probably many professional and experienced software engineers out there who - far from appreciating your product, actually feel genuinely oppressed by it: like victims of a cold unfeeling system with no ‘right to reply’.  Unable to merge deadline code until every point is addressed, for better or worse.  It’s a terrible helpless feeling.</p>
<p>A couple of Kotlin examples:</p>
<ul>
<li>
<p>Inability to define a single suspending function interface. Sonar tells me to make it a fun interface, then tells me fun interfaces can’t have suspending functions.  I should make this a functional type instead?  Completely <em>inconsistent</em> with the rest of the code and removes the opportunity to provide information though labelling the function.</p>
</li>
<li>
<p>Sonar doesn’t respect import aliasing.  Let’s say I have Domain and Service models for some entities.  I have ‘Person’ defined in domain and service packages. In one of those packages, I want to write a mapper extension.  I use import aliasing to disambiguate each ‘Person’ as DomainPerson and ServicePerson.  This is an informative convention, but Sonar doesn’t allow it: <code>import service.Person as ServicePerson</code> is considered redundant.  I could then use a private typealias ServicePerson = Person` but this isn’t the same.</p>
</li>
</ul>
<p>These are just two examples, I could find more.</p>
<p><strong>Can you do something with your product</strong> to give users more of a ‘right to reply’ and re-empower them?<br>
There will be creative ways to achieve this without undermining Sonar’s purpose.  Just some ideas:</p>
<ul>
<li>Allow a user role where rules can be overridden but the admin gets informed and can remove the override at a later time.</li>
<li>A mode where an override can be granted by 3-4 other users all agreeing, who aren’t admins.</li>
<li>Button to fast-access (lazily create) a community thread regarding a certain rule, to report a problem with it, so you know your difficulty is being heard at least by Sonar, if not within your organisation.</li>
</ul>
<p>Thank you for listening to my part rant, part suggestions.  This is sincere.</p>
            </div>
          <div id="post_2" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://community.sonarsource.com/u/Colin"><span itemprop="name">Colin</span></a>
                (Colin)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2023-06-14T08:51:41Z">
                    June 14, 2023,  8:51am
                  </time>
                  <meta itemprop="dateModified" content="2023-06-14T09:29:39Z">
              <span itemprop="position">2</span>
              </span>
            </p>
            <div itemprop="text">
              <p>Hey there.</p>
<p>Thanks for the feedback. It’s essential in our quest to ensure developers not only find value in our products but also enjoy and feel empowered by the experience.</p>
<p>If you disagree with a rule implementation, I encourage you to post the details in the category titled <a href="https://community.sonarsource.com/c/clean-code/fp/7">Report a False-Positive/False-Negative</a>, for which we <a href="https://community.sonarsource.com/t/how-to-report-a-false-positive-false-negative/37022">have a post detailing what is required to report</a> (code sample, product versions, rule IDs…). Our teams are very reactive and enjoy engaging in these discussions.</p>

<p>There is an <a href="https://docs.sonarqube.org/latest/instance-administration/security/#:~:text=permission%C2%A0below.-,Project%20permissions,-Project%20permissions%20are"><strong>Administer Issue</strong></a> permission that in most organizations would be granted to Team leaders or experienced developers (like yourself) to be able to mark issues as False-Positive/Won’t Fix.</p>
<p>Somebody else (like your superior) has at least two options (maybe more) to discover these exceptions:</p>
<ul>
<li>Using the <strong>Issues</strong> tab of a project (or the SonarQube instance overall) to filter for issues that are marked as False-Positive / Won’t Fix if they want to do some kind of global review.</li>
<li>Configure a project-level <a href="https://docs.sonarqube.org/latest/instance-administration/notifications/">e-mail notification</a> for <strong>Issues resolved as false positive or won’t fix</strong></li>
</ul>
<p>And, to be honest, Chris, if you have a superior that trusts in Sonar 100% and trusts you and your fellow developers very little (unwilling to delegate permissions, decision making)… I think that’s the root of your problem and not one that can be fixed with product changes. We create a new user role? Enable some kind of consensus-driven issue status? Your superior could just decide not to grant it.</p>
<p>Happy to continue discussing this.</p>
            </div>

            

            

          </div>
          <div id="post_4" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://community.sonarsource.com/u/john.clifton"><span itemprop="name">john.clifton</span></a>
                (John Clifton)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2023-06-14T09:51:59Z">
                    June 14, 2023,  9:51am
                  </time>
                  <meta itemprop="dateModified" content="2023-06-14T09:51:59Z">
              <span itemprop="position">4</span>
              </span>
            </p>
            <div itemprop="text">
              
<p>Hi <a href="https://community.sonarsource.com/u/chris_hatton">@Chris_Hatton</a>, we are looking into something similar to this at the moment. If you would be willing, I’d love to have a short call to talk to you about your suggestion.</p>
            </div>

            

            

          </div>
          <div id="post_7" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://community.sonarsource.com/u/Jeremy_Cox"><span itemprop="name">Jeremy_Cox</span></a>
                (Jeremy Cox)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2023-06-28T13:24:02Z">
                    June 28, 2023,  1:24pm
                  </time>
                  <meta itemprop="dateModified" content="2023-06-28T13:30:14Z">
              <span itemprop="position">7</span>
              </span>
            </p>
            <div itemprop="text">
              <blockquote>
<p>if you have a superior that trusts in Sonar 100% and trusts you and your fellow developers very little (unwilling to delegate permissions, decision making)… I think that’s the root of your problem</p>
</blockquote>
<p>You make a fair point that anyone who uses a tool and does not understand is part of the problem.  However, if the tool could be easier to understand, that could go a long way to addressing that issue.</p>
<p>Respectfully, the root of the problem is that SonarQube is not clear to everyone who uses it.  Specifically, it offers little towards gauging what is important and and what is not.  Programmers have been engaged with a tug of war with management since the dawn of the digital era.  Managers, who don’t understand coding well enough to police it, are now offered Sonarqube as a policing tool.  They are given a bat to hit programmers over the head.  This has an easy metric for determining perfections – 0 issues found.  It’s a managers dream – a computer that sums up everything you need to know in one number.  If you give someone a bat, they’re going to use it to hit things.  It’s pretty simple.</p>
<p>The number of issues found in your project is similar to likes on social media – it’s a badge of honor to show off to all your manager friends. I have a coding library that’s tens of thousands of lines.  Military Standard data types are defined by two numbers, so my (Java) class names end with something like “_1234_567”  Like a good programmer, my fields and getters follow the same naming pattern.<br>
This violates S101, S116, S100, S101 for a total of 4.7k hits. This has caused everyone to freak out.</p>
<p>The OP point is well taken, that if I had an opportunity to add some structured comments to that report before a manager read it, the sky would not be falling.  Now I am having trouble explaining in meeting after meeting, “we violated 4 <em>MINOR</em> rules on purpose.  So if we ignore those rules like we’re supposed to, the 4,700 hits go away.”  To which they respond, “You want us to cover up 4,700 hits?”</p>
<p>Clearly, it’s a major problem that they don’t understand a minor sonarqube finding “<a href="https://docs.sonarqube.org/latest/user-guide/issues/#:~:text=MINOR%3A%20A%20quality%20flaw%20that,quality%20flaw%2C%20just%20a%20finding." rel="noopener nofollow ugc">may slightly impact developer productivity</a>” and it <strong>may also not</strong> affect quality.</p>
<p>So thanks to Sonarqube, I am going to have to make my code unreadable to a human in order to please management, which means INTENTIONALLY creating a mountain of technical debt.  But that’s okay, because Sonarqube doesn’t count it.</p>
<p>Sonarqube exacerbates the problem of communication with management; it does not help it. The problem is how the information is presented.  As OP pointed out, Programmers don’t have an opportunity to respond proactively to the issues.  It’s fair to blame managers, it also fair to blame the bad information they are given to make decisions. For example, your issue count could be broken down into “must address” and “not urgent” or the like.  It could further have something like, “4,712 issues have open discussions, here is a link.”  The idea that a single number could sum up technical debt is simply dangerous.  It encourages people to NOT investigate and learn more.  It encourages people to not listen.</p>
<p>“Dude, look, it’s really simple, your project has 4,700 sonarqube findings, everyone else has less than 100.  This is a job performance problem.”  – Overheard in a meeting</p>
<p>Finally, I have a question.  Are there articles posted by Sonarsource telling managers how to interpret sonarqube results properly?  I’ve been scouring the web for “how to handle minor sonarqube issues best practice” and found nothing useful to my case.  I’d welcome knowledge if anyone has any.</p>
            </div>

            

            

          </div>
          <div id="post_11" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://community.sonarsource.com/u/Colin"><span itemprop="name">Colin</span></a>
                (Colin)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2023-07-07T11:10:25Z">
                    July 7, 2023, 11:10am
                  </time>
                  <meta itemprop="dateModified" content="2023-07-07T11:10:25Z">
              <span itemprop="position">11</span>
              </span>
            </p>
            <div itemprop="text">
              <p>Hey there <a href="https://community.sonarsource.com/u/jeremy_cox">@Jeremy_Cox</a>.</p>
<p>Thanks for the super valuable feedback (that we’ve been discussing a lot internally). I want to provide a few preliminary notes.</p>
<ul>
<li>
<p>We’re conscious of the fact that Sonar can be misused, and we’ve made some efforts to minimize how easy that is. We once had a feature (meant to “gamify” fixing issues and not introducing new ones) which… you guessed it, resulted in some managers using it to measure individual performance. Some organizations loved this feature! We removed it because it absolutely didn’t align with our values.</p>
</li>
<li>
<p>A big shift is going to come to our products soon (this year) that will shift focus on raising <em>findings</em> about <em>facts</em> about the code (<em>this is not a coding practice that aligns with our organization’s practices</em>), rather than <em>issues</em> with a hypothesized impact (“this may slightly impact developer productivity”). This repositioning might also make it easier to explain why you want to ignore a rule. T</p>
</li>
<li>
<p>As part of this shift we are also looking at how to make sure Clean Code (and Clean as You Code) is easy to understand by governance stakeholders and de-emphasize issue count. Yes, there will be product changes that have to happen as well as education/positioning that has to change (no silver bullets, only lots of reuglar ones)</p>
</li>
</ul>

<p>I don’t think we have anything regarding this. It’s a valid point I’ll pass on.</p>
<p>Please don’t hesitate to keep giving us honest feedback as things progress, or if you’ve thought of any other points you want to share. Feedback is a gift. <img src="https://emoji.discourse-cdn.com/twitter/gift.png?v=12" title=":gift:" alt=":gift:" loading="lazy" width="20" height="20"></p>
            </div>

            

            

          </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An epigenetic editor to silence genes (116 pts)]]></title>
            <link>https://www.science.org/doi/10.1126/science.adq3334</link>
            <guid>40859876</guid>
            <pubDate>Tue, 02 Jul 2024 19:50:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/doi/10.1126/science.adq3334">https://www.science.org/doi/10.1126/science.adq3334</a>, See on <a href="https://news.ycombinator.com/item?id=40859876">Hacker News</a></p>
Couldn't get https://www.science.org/doi/10.1126/science.adq3334: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Aboriginal ritual passed down over 12,000 years, cave find shows (214 pts)]]></title>
            <link>https://phys.org/news/2024-07-aboriginal-ritual-years-cave.html</link>
            <guid>40859393</guid>
            <pubDate>Tue, 02 Jul 2024 18:48:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phys.org/news/2024-07-aboriginal-ritual-years-cave.html">https://phys.org/news/2024-07-aboriginal-ritual-years-cave.html</a>, See on <a href="https://news.ycombinator.com/item?id=40859393">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/aboriginal-ritual-pass.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2024/aboriginal-ritual-pass.jpg" data-sub-html="The two miniature fireplaces with trimmed sticks immediately after they were exposed by excavation in Cloggs Cave square R31, with the sticks’ bases not yet separated from the sediments in which they sit. Credit: <i>Nature Human Behaviour</i> (2024). DOI: 10.1038/s41562-024-01912-w">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/aboriginal-ritual-pass.jpg" alt="Aboriginal ritual passed down over 12,000 years, cave find shows" title="The two miniature fireplaces with trimmed sticks immediately after they were exposed by excavation in Cloggs Cave square R31, with the sticks’ bases not yet separated from the sediments in which they sit. Credit: Nature Human Behaviour (2024). DOI: 10.1038/s41562-024-01912-w" width="800" height="530">
             <figcaption>
                The two miniature fireplaces with trimmed sticks immediately after they were exposed by excavation in Cloggs Cave square R31, with the sticks’ bases not yet separated from the sediments in which they sit. Credit: <i>Nature Human Behaviour</i> (2024). DOI: 10.1038/s41562-024-01912-w
            </figcaption>        </figure>
    </div><p>Two slightly burnt, fat-covered sticks discovered inside an Australian cave are evidence of a healing ritual that was passed down unchanged by more than 500 generations of Indigenous people over the last 12,000 years, according to new research.</p>


										      
																																	<p>The wooden sticks, found poking out of tiny fireplaces, showed that the <a href="https://phys.org/tags/ritual/" rel="tag">ritual</a> documented in the 1880s had been shared via oral traditions since the end of the last ice age, a study in the journal <i>Nature Human Behaviour</i> said on Monday.</p>
<p>The discovery was made inside Cloggs Cave in the foothills of the Victorian Alps in Australia's southeast, in a region long inhabited by the Gunaikurnai people.</p>
<p>When the cave was first excavated in the 1970s, archaeologists discovered the remains of a long extinct giant kangaroo that had previously lived there.</p>
<p>But the Gunaikurnai people were not involved in those digs, "nor were they asked for permission to do research there", lead study author Bruno David of Monash University told AFP.</p>
<p>Further excavations starting from 2020 included members of the local Gunaikurnai Land and Waters Aboriginal Corporation (GLaWAC).</p>
<p>Carefully digging through the soil, the team found a small stick poking out—then they found another one. Both well-preserved sticks were made from the wood of casuarina trees.</p>
<p>Each one was found in a separate fireplace around the size of the palm of a hand—far too small to have been used for heat or cooking meat.</p>
<p>The slightly charred ends of the sticks had been cut specially to stick into the fire, and both were coated in human or animal fat.</p>
<p>One stick was 11,000 years old and the other 12,000 years old, radiocarbon dating found.</p>

																																						
																																			<h2>'Memoirs of our ancestors'</h2>
<p>"They've been waiting here all this time for us to learn from them," said Gunaikurnai elder Russell Mullett, a co-author of the study and head of GLaWAC.</p>
<p>Mullett spent years trying to find out what they could have been used for, before discovering the accounts of Alfred Howitt, a 19th-century Australian anthropologist who studied Aboriginal culture.</p>
<p>Some of Howitt's notes had never been published, and Mullett said he spent a long time convincing a local museum to share them.</p>
<p>In the notes, Howitt describes in the late 1880s the rituals of Gunaikurnai medicine men and women called "mulla-mullung".</p>
<p>One ritual involved tying something that belonged to a sick person to the end of a throwing stick smeared in human or kangaroo fat. The stick was thrust into the ground before a small fire was lit underneath.</p>
<p>"The mulla-mullung would then chant the name of the sick person, and once the stick fell, the charm was complete," a Monash University statement said.</p>
<p>The sticks used in the ritual were made of casuarina wood, Howitt noted.</p>
<p>Jean-Jacques Delannoy, a French geomorphologist and study co-author, told AFP that "there is no other known gesture whose symbolism has been preserved for such a long time".</p>
<p>"Australia kept the memory of its first peoples alive thanks to a powerful oral tradition that enabled it to be passed on," Delannoy said.</p>
<p>"However in our societies, memory has changed since we switched to the written word, and we have lost this sense."</p>
<p>He lamented that the ancient animal paintings found in French caves would probably "never reveal their meaning" in a similar way.</p>
<p>Indigenous Australians are one of the oldest continuous living cultures, and Mullett said the discovery was a "unique opportunity to be able to read the memoirs of our ancestors".</p>
<p>It was "a reminder that we are a living culture still connected to our ancient past," he added.</p>

																																																					
																				<div>
																						<p><strong>More information:</strong>
												Bruno David et al, Archaeological evidence of an ethnographically documented Australian Aboriginal ritual dated to the last ice age, <i>Nature Human Behaviour</i> (2024). <a data-doi="1" href="https://dx.doi.org/10.1038/s41562-024-01912-w" target="_blank">DOI: 10.1038/s41562-024-01912-w</a>
																						
																						</p>
																					</div>
                               											
																															 <p>
												  © 2024 AFP
											 </p>
										                                        
										<!-- print only -->
										<div>
											 <p><strong>Citation</strong>:
												Aboriginal ritual passed down over 12,000 years, cave find shows (2024, July 2)
												retrieved 3 July 2024
												from https://phys.org/news/2024-07-aboriginal-ritual-years-cave.html
											 </p>
											 <p>
											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 </p>
										</div>
                                        
									</div></div>]]></description>
        </item>
    </channel>
</rss>