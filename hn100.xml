<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 08 May 2024 17:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[AlphaFold 3 predicts the structure and interactions of all of life's molecules (286 pts)]]></title>
            <link>https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/</link>
            <guid>40298927</guid>
            <pubDate>Wed, 08 May 2024 15:07:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/">https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/</a>, See on <a href="https://news.ycombinator.com/item?id=40298927">Hacker News</a></p>
<div id="readability-page-1" class="page"><article ng-init="drawerToggle = {'open': true}">

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
  }">
      <div>
          <p>May 08, 2024</p>
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
      
        <p>
          Introducing AlphaFold 3, a new AI model developed by Google DeepMind and Isomorphic Labs. By accurately predicting the structure of proteins, DNA, RNA, ligands and more, and how they interact, we hope it will transform our understanding of the biological world and drug discovery.
        </p>
      
    </div>

    

    
      







<div>
    <figure>
      <div>
  <p><img srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AF_hero_2.width-600.format-webp.webp 600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AF_hero_2.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AF_hero_2.width-1600.format-webp.webp 1600w" sizes="(max-width: 599px) 100vw, (max-width: 1023px) 600px, 1024px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/AF_hero_2.width-1200.format-webp.webp" fetchpriority="high" alt="Colorful protein structure against an abstract gradient background.">
  </p>
</div>

      
    </figure>
  </div>


    

    
    <div>
        
          
            <div data-component="uni-article-jumplinks" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Jumplinks&quot;,
    &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
  }">
  <nav aria-label="Article Jumplinks">
    <p><span>In this story</span>
    </p>
    
    
    <div>
      <ul id="article-jumplinks__list">
        
        <li>
          <a aria-label="link to Revealing life’s molecules" href="#life-molecules" id="life-molecules-anchor">Revealing life’s molecules</a>
        </li>
        
        <li>
          <a aria-label="link to Leading drug discovery" href="#drug-discovery" id="drug-discovery-anchor">Leading drug discovery</a>
        </li>
        
        <li>
          <a aria-label="link to Introducing AlphaFold Server" href="#alphafold-server" id="alphafold-server-anchor">Introducing AlphaFold Server</a>
        </li>
        
        <li>
          <a aria-label="link to Sharing responsibly" href="#responsibility" id="responsibility-anchor">Sharing responsibly</a>
        </li>
        
        <li>
          <a aria-label="link to Future of AI-powered cell biology" href="#future-cell-biology" id="future-cell-biology-anchor">Future of AI-powered cell biology</a>
        </li>
        
      </ul>
    </div>
    
  </nav>
</div>
          
          
          <div data-reading-time="true" data-component="uni-drop-cap|uni-tombstone">

            
              


<google-read-aloud-player data-analytics-module="{
        &quot;event&quot;: &quot;module_impression&quot;,
        &quot;module_name&quot;: &quot;ai_audio&quot;,
        &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
    }" data-date-modified="2024-05-08T15:00:00.987420+00:00" data-progress-bar-style="half-wave" data-api-key="AIzaSyBLT6VkYe-x7sWLZI2Ep26-fNkBKgND-Ac" data-article-style="style9" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-layout-style="style1" data-highlight-mode="word-over-paragraph" data-highlight-text-color="#000000" data-highlight-word-background="#8AB4F8" data-highlight-paragraph-background="#D2E3FC" data-background="linear-gradient(180deg, #F1F3F4 0%, #F8F9FA 100%)" data-foreground-color="#202124" data-font="600 16px Google Sans, sans-serif" data-box-shadow="0px 1px 3px 1px rgba(60, 64, 67, 0.15)">
</google-read-aloud-player>




            

            
            
<!--article text-->

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
         }"><p data-block-key="t1yf2">Inside every plant, animal and human cell are billions of molecular machines. They’re made up of proteins, DNA and other molecules, but no single piece works on its own. Only by seeing how they interact together, across millions of types of combinations, can we start to truly understand life’s processes.</p><p data-block-key="37r0c">In a paper published in <a href="https://www.nature.com/articles/s41586-024-07487-w" rt-link-type="external"><i>Nature</i></a>, we introduce AlphaFold 3, a revolutionary model that can predict the structure and interactions of all life’s molecules with unprecedented accuracy. For the interactions of proteins with other molecule types we see at least a 50% improvement compared with existing prediction methods, and for some important categories of interaction we have doubled prediction accuracy.</p><p data-block-key="2am3m">We hope AlphaFold 3 will help transform our understanding of the biological world and drug discovery. Scientists can access the majority of its capabilities, for free, through our newly launched <a href="http://alphafoldserver.com/" rt-link-type="external">AlphaFold Server</a>, an easy-to-use research tool. To build on AlphaFold 3’s potential for drug design, <a href="https://www.isomorphiclabs.com/articles/rational-drug-design-with-alphafold-3" rt-link-type="external">Isomorphic Labs</a> is already collaborating with pharmaceutical companies to apply it to real-world drug design challenges and, ultimately, develop new life-changing treatments for patients.</p><p data-block-key="79nqe">Our new model builds on the foundations of AlphaFold 2, which in 2020 made a <a href="https://deepmind.google/discover/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology/" rt-link-type="external">fundamental breakthrough in protein structure prediction</a>. So far, <a href="https://deepmind.google/impact/meet-the-scientists-using-alphafold/" rt-link-type="external">millions of researchers</a> globally have used AlphaFold 2 to make discoveries in areas including malaria vaccines, cancer treatments and enzyme design. AlphaFold has been cited more than 20,000 times and its scientific impact recognized through many prizes, most recently the <a href="https://breakthroughprize.org/News/73" rt-link-type="external">Breakthrough Prize in Life Sciences</a>. AlphaFold 3 takes us beyond proteins to a broad spectrum of biomolecules. This leap could unlock more transformative science, from developing biorenewable materials and more resilient crops, to accelerating drug design and genomics research.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
        }">
  

  <p>

      
      
        
          <video tabindex="0" autoplay="" loop="" muted="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/AFS-anim-7PNM.mp4" type="video/mp4" title="GIF of a rotating spike protein structure against a white background, with ground truth shown in gray." alt="7PNM">
            Video format not supported
          </video>
        
      
    
    </p>
    
      <figcaption><p data-block-key="mv0fd">7PNM - Spike protein of a common cold virus (Coronavirus OC43): AlphaFold 3’s structural prediction for a spike protein (blue) of a cold virus as it interacts with antibodies (turquoise) and simple sugars (yellow), accurately matches the true structure (gray). The animation shows the protein interacting with an antibody, then a sugar. Advancing our knowledge of such immune-system processes helps better understand coronaviruses, including COVID-19, raising possibilities for improved treatments.</p></figcaption>
    
  
    </div>
  



  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
         }"><h2 data-block-key="t1yf2">How AlphaFold 3 reveals life’s molecules</h2><p data-block-key="9sni9">Given an input list of molecules, <a href="https://www.nature.com/articles/s41586-024-07487-w" rt-link-type="external">AlphaFold 3</a> generates their joint 3D structure, revealing how they all fit together. It models large biomolecules such as proteins, DNA and RNA, as well as small molecules, also known as ligands — a category encompassing many drugs. Furthermore, AlphaFold 3 can model chemical modifications to these molecules which control the healthy functioning of cells, that when disrupted can lead to disease.</p><p data-block-key="bkj8h">AlphaFold 3’s capabilities come from its next-generation architecture and training that now covers all of life’s molecules. At the core of the model is an improved version of our <a href="https://www.nature.com/articles/s41586-021-03819-2" rt-link-type="external">Evoformer module</a> — a deep learning architecture that underpinned AlphaFold 2’s incredible performance. After processing the inputs, AlphaFold 3 assembles its predictions using a diffusion network, akin to those found in AI image generators. The diffusion process starts with a cloud of atoms, and over many steps converges on its final, most accurate molecular structure.</p><p data-block-key="bc95f">AlphaFold 3’s predictions of molecular interactions surpass the accuracy of all existing systems. As a single model that computes entire molecular complexes in a holistic way, it’s uniquely able to unify scientific insights.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
        }">
  

  <p>

      
      
        
          <video tabindex="0" autoplay="" loop="" muted="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/AFS-anim-7R6R.mp4" type="video/mp4" title="GIF of a rotating DNA binding protein structure against a white background, with ground truth shown in gray." alt="7R6R">
            Video format not supported
          </video>
        
      
    
    </p>
    
      <figcaption><p data-block-key="hhhu1">7R6R - DNA binding protein: AlphaFold 3’s prediction for a molecular complex featuring a protein (blue) bound to a double helix of DNA (pink) is a near-perfect match to the true molecular structure discovered through painstaking experiments (gray).</p></figcaption>
    
  
    </div>
  



  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
         }"><h2 data-block-key="t1yf2">Leading drug discovery at Isomorphic Labs</h2><p data-block-key="5l6pl">AlphaFold 3 creates capabilities for drug design with predictions for molecules commonly used in drugs, such as ligands and antibodies, that bind to proteins to change how they interact in human health and disease.</p><p data-block-key="beh6j">AlphaFold 3 achieves unprecedented accuracy in predicting drug-like interactions, including the binding of proteins with ligands and antibodies with their target proteins. AlphaFold 3 is 50% more accurate than the best traditional methods on the <a href="https://pubs.rsc.org/en/content/articlehtml/2024/sc/d3sc04185a" rt-link-type="external">PoseBusters benchmark</a> without needing the input of any structural information, making AlphaFold 3 the first AI system to surpass physics-based tools for biomolecular structure prediction. The ability to predict antibody-protein binding is critical to understanding aspects of the human immune response and the design of new antibodies — a growing class of therapeutics.</p><p data-block-key="4s1o1">Using AlphaFold 3 in combination with a complementary suite of in-house AI models, <a href="https://www.isomorphiclabs.com/articles/rational-drug-design-with-alphafold-3" rt-link-type="external">Isomorphic Labs</a> is working on drug design for internal projects as well as with pharmaceutical partners. Isomorphic Labs is using AlphaFold 3 to accelerate and improve the success of drug design — by helping understand how to approach new disease targets, and developing novel ways to pursue existing ones that were previously out of reach.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
         }">
        <p><h2 data-block-key="t1yf2">AlphaFold Server: A free and easy-to-use research tool</h2></p>
      </div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
        }">
  

  <p>

      
      
        
          <video tabindex="0" autoplay="" loop="" muted="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/AFS-anim-8AW3.mp4" type="video/mp4" title="GIF of a rotating RNA modifying protein structure against a white background, with ground truth shown in gray." alt="8AW3">
            Video format not supported
          </video>
        
      
    
    </p>
    
      <figcaption><p data-block-key="mc56w">8AW3 - RNA modifying protein: AlphaFold 3’s prediction for a molecular complex featuring a protein (blue), a strand of RNA (purple), and two ions (yellow) closely matches the true structure (gray). This complex is involved with the creation of other proteins — a cellular process fundamental to life and health.</p></figcaption>
    
  
    </div>
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
         }"><p data-block-key="t1yf2">Google DeepMind’s newly launched <a href="http://alphafoldserver.com/" rt-link-type="external">AlphaFold Server</a> is the most accurate tool in the world for predicting how proteins interact with other molecules throughout the cell. It is a free platform that scientists around the world can use for non-commercial research. With just a few clicks, biologists can harness the power of AlphaFold 3 to model structures composed of proteins, DNA, RNA and a selection of ligands, ions and chemical modifications.</p><p data-block-key="di2e8">AlphaFold Server helps scientists make novel hypotheses to test in the lab, speeding up workflows and enabling further innovation. Our platform gives researchers an accessible way to generate predictions, regardless of their access to computational resources or their expertise in machine learning.</p><p data-block-key="8ca9a">Experimental protein-structure prediction can take about the length of a PhD and cost hundreds of thousands of dollars. Our previous model, AlphaFold 2, has been used to predict hundreds of millions of structures, which would have taken hundreds of millions of researcher-years at the current rate of experimental structural biology.</p></div>
  

  
    <section data-analytics-module="{
       &quot;module_name&quot;: &quot;Pull Quote&quot;,
       &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
     }">
    <p><q>With AlphaFold Server, it’s not only about predicting structures anymore, it’s about generously giving access: allowing researchers to ask daring questions and accelerate discoveries.</q>

      
        <cite>
          
          
            <span>
              
                <strong>Céline Bouchoux</strong><br>
              
              
                The Francis Crick Institute
              
            </span>
          
        </cite>
      
    </p>
  </section>


  

  
    
  
    


<div data-component="uni-article-yt-player" data-page-title="AlphaFold 3 predicts the structure and interactions of all of life’s molecules" data-video-id="9ufplEgtq8w" data-index-id="13" data-type="video" data-analytics-module="{
    &quot;module_name&quot;: &quot;Youtube Video&quot;,
    &quot;section_header&quot;: &quot;undefined&quot;
  }">

    

    <a role="video" tabindex="0">
      <div>
        
          
          <p><img alt="Demo video showing the capabilities of the server." src="https://i.ytimg.com/vi_webp/9ufplEgtq8w/default.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/9ufplEgtq8w/sddefault.webp&quot;,
                &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/9ufplEgtq8w/hqdefault.webp&quot;
              }"></p>

        
        <svg role="presentation">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20240416-1649#yt_video_play_button_no_hole"></use>
          
        </svg>
        <svg role="img">
          
          <use xlink:href="/static/blogv2/images/icons.svg?version=pr20240416-1649#yt_video_play_button"></use>
          
        </svg>

        
        
        
        
      </div>
    </a>

    

    
  </div>

  


  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
         }"><h2 data-block-key="t1yf2">Sharing the power of AlphaFold 3 responsibly</h2><p data-block-key="epbhg">With each AlphaFold release, <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphafold-3-predicts-the-structure-and-interactions-of-all-lifes-molecules/Our-approach-to-biosecurity-for-AlphaFold-3-08052024" rt-link-type="external">we’ve sought to understand the broad impact of the technology</a>, working together with the research and safety community. We take a science-led approach and have conducted extensive assessments to mitigate potential risks and share the widespread benefits to biology and humanity.</p><p data-block-key="25l72">Building on the external consultations we carried out for AlphaFold 2, we’ve now engaged with more than 50 domain experts, in addition to specialist third parties, across biosecurity, research and industry, to understand the capabilities of successive AlphaFold models and any potential risks. We also participated in community-wide forums and discussions ahead of AlphaFold 3’s launch.</p><p data-block-key="bp8tm">AlphaFold Server reflects our ongoing commitment to share the benefits of AlphaFold, including our <a href="https://alphafold.ebi.ac.uk/" rt-link-type="external">free database</a> of 200 million protein structures. We’ll also be expanding our free<a href="https://www.ebi.ac.uk/training/online/courses/alphafold/" rt-link-type="external"> AlphaFold education online course</a> with <a href="https://www.ebi.ac.uk/" rt-link-type="external">EMBL-EBI</a> and partnerships with organizations in the Global South to equip scientists with the tools they need to accelerate adoption and research, including on underfunded areas such as neglected diseases and food security. We’ll continue to work with the scientific community and policy makers to develop and deploy AI technologies responsibly.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
         }">
        <p><h2 data-block-key="t1yf2">Opening up the future of AI-powered cell biology</h2></p>
      </div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
        }">
  

  <p>

      
      
        
          <video tabindex="0" autoplay="" loop="" muted="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/AFS-anim-7BBV.mp4" type="video/mp4" title="GIF of an enzyme protein structure against a white background, with ground truth shown in gray." alt="7BBV">
            Video format not supported
          </video>
        
      
    
    </p>
    
      <figcaption><p data-block-key="2em21">7BBV - Enzyme: AlphaFold 3’s prediction for a molecular complex featuring an enzyme protein (blue), an ion (yellow sphere) and simple sugars (yellow), along with the true structure (gray). This enzyme is found in a soil-borne fungus (Verticillium dahliae) that damages a wide range of plants. Insights into how this enzyme interacts with plant cells could help researchers develop healthier, more resilient crops.</p></figcaption>
    
  
    </div>
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;AlphaFold 3 predicts the structure and interactions of all of life’s molecules&quot;
         }"><p data-block-key="t1yf2">AlphaFold 3 brings the biological world into high definition. It allows scientists to see cellular systems in all their complexity, across structures, interactions and modifications. This new window on the molecules of life reveals how they’re all connected and helps understand how those connections affect biological functions — such as the actions of drugs, the production of hormones and the health-preserving process of DNA repair.</p><p data-block-key="8bon2">The impacts of AlphaFold 3 and our free AlphaFold Server will be realized through how they empower scientists to accelerate discovery across open questions in biology and new lines of research. We’re just beginning to tap into AlphaFold 3’s potential and can’t wait to see what the future holds.</p></div>
  


            
            

            
              




            
          </div>
        
      </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla is being investigated for securities and wire fraud for self-driving claim (134 pts)]]></title>
            <link>https://www.theverge.com/2024/5/8/24151881/tesla-justice-investigation-securities-wire-fraud-self-driving</link>
            <guid>40298486</guid>
            <pubDate>Wed, 08 May 2024 14:26:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/5/8/24151881/tesla-justice-investigation-securities-wire-fraud-self-driving">https://www.theverge.com/2024/5/8/24151881/tesla-justice-investigation-securities-wire-fraud-self-driving</a>, See on <a href="https://news.ycombinator.com/item?id=40298486">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The Department of Justice is looking into whether Tesla committed securities and wire fraud around its self-driving vehicle claims, <a href="https://www.reuters.com/business/autos-transportation/tesla-autopilot-probe-us-prosecutors-focus-securities-wire-fraud-2024-05-08/"><em>Reuters </em>reports today</a>, citing three sources familiar with the matter. </p><p>The investigation, which was <a href="https://www.theverge.com/2022/10/26/23425335/tesla-autopilot-justice-department-criminal-investigation">first reported in October 2022</a> but has been going on since at least late 2021, involves federal prosecutors in Washington and San Francisco who are examining whether Tesla executives misled consumers, investors, and regulators by making unsupported claims about its autonomous capabilities. Now, it appears that investigators are zeroing in on specific charges against the company: securities and wire fraud. </p><p>According to <em>Reuters</em>, the probe is looking into statements made by Tesla CEO Elon Musk in particular. For years, Musk has been promising fully autonomous Tesla vehicles are just around the corner —&nbsp;while also admitting that he often sets overly optimistic timelines. Meanwhile, the company’s advanced driver-assist features, Autopilot and Full Self-Driving, do not make the vehicles autonomous and require drivers to keep their hands on the steering wheel and eyes on the road.</p><div><p>According to <em>Reuters</em>, the probe is looking into Tesla CEO Elon Musk’s claims in particular</p></div><p>Tesla has repeatedly pushed the boundaries of safety by allowing its customers to beta test products that may not be ready for wide release. Tesla vehicles using Autopilot have been subject to numerous recalls and involved in <a href="https://www.theverge.com/2024/5/7/24151077/tesla-autopilot-nhtsa-recall-crash-data-request">hundreds of crashes over the years</a>, dozens of which have been fatal. <a href="https://www.theverge.com/2023/12/13/23999683/tesla-autopilot-defect-software-update-recall-nhtsa">The most recent recall</a>, which applied to every single Tesla sold to date, <a href="https://www.theverge.com/2024/4/26/24141403/tesla-autopilot-nhtsa-investigation-recall-software-fix">has now come under a new investigation</a> for its failure to prevent driver misuse and correct the flaws identified in the first recall. </p><p>Wire fraud involves deceiving customers in interstate communications, whereas securities fraud relates to misleading investors. The Securities and Exchange Commission is also looking into whether Tesla lied in its communications about self-driving vehicles, <em>Reuters</em> says. </p><p>The Justice Department is said to also be <a href="https://www.theverge.com/2023/10/23/23928563/tesla-doj-ev-range-exaggerate-investigation">looking into Tesla’s vehicle range claims</a>. Tesla customers have long complained that the company’s listed vehicle ranges often don’t match up to the reality of what the cars are capable of. </p><p>In its latest securities filing, Tesla acknowledged “regularly” receiving subpoenas and requests for information from the SEC and Justice Department, some of which involve Autopilot and Full Self-Driving. </p><p>“To our knowledge no government agency in any ongoing investigation has concluded that any wrongdoing occurred,” the company said. </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TimesFM (Time Series Foundation Model) for time-series forecasting (104 pts)]]></title>
            <link>https://github.com/google-research/timesfm</link>
            <guid>40297946</guid>
            <pubDate>Wed, 08 May 2024 13:34:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/google-research/timesfm">https://github.com/google-research/timesfm</a>, See on <a href="https://news.ycombinator.com/item?id=40297946">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">TimesFM</h2><a id="user-content-timesfm" aria-label="Permalink: TimesFM" href="#timesfm"></a></p>
<p dir="auto">TimesFM  (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google
Research for time-series forecasting.</p>
<ul dir="auto">
<li>Paper: <a href="https://arxiv.org/abs/2310.10688" rel="nofollow">A decoder-only foundation model for time-series forecasting</a>, to appear in ICML 2024.</li>
<li><a href="https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/" rel="nofollow">Google Research blog</a></li>
<li><a href="https://huggingface.co/google/timesfm-1.0-200m" rel="nofollow">Hugging Face checkpoint repo</a></li>
</ul>
<p dir="auto">This repo contains the code to load public TimesFM checkpoints and run model
inference. Please visit our
<a href="https://huggingface.co/google/timesfm-1.0-200m" rel="nofollow">Hugging Face checkpoint repo</a>
to download model checkpoints.</p>
<p dir="auto">This is not an officially supported Google product.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Checkpoint timesfm-1.0-200m</h2><a id="user-content-checkpoint-timesfm-10-200m" aria-label="Permalink: Checkpoint timesfm-1.0-200m" href="#checkpoint-timesfm-10-200m"></a></p>
<p dir="auto">timesfm-1.0-200m is the first open model checkpoint:</p>
<ul dir="auto">
<li>It performs univariate time series forecasting for context lengths up tp 512 timepoints and any horizon lengths, with an optional frequency indicator.</li>
<li>It focuses on point forecasts, and does not support probabilistic forecasts. We experimentally offer quantile heads but they have not been calibrated after pretraining.</li>
<li>It requires the context to be contiguous (i.e. no "holes"), and the context and the horizon to be of the same frequency.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmarks</h2><a id="user-content-benchmarks" aria-label="Permalink: Benchmarks" href="#benchmarks"></a></p>
<p dir="auto">Please refer to our result tables on the <a href="https://github.com/google-research/timesfm/blob/master/experiments/extended_benchmarks/tfm_results.png">extended benchmarks</a> and the <a href="https://github.com/google-research/timesfm/blob/master/experiments/long_horizon_benchmarks/tfm_long_horizon.png">long horizon benchmarks</a>.</p>
<p dir="auto">Please look into the README files in the respective benchmark directories within <code>experiments/</code> for instructions for running TimesFM on the respective benchmarks.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">We have two environment files. For GPU installation (assuming CUDA 12 has been
setup), you can create a conda environment <code>tfm_env</code> from the base folder
through:</p>
<div data-snippet-clipboard-copy-content="conda env create --file=environment.yml"><pre><code>conda env create --file=environment.yml
</code></pre></div>
<p dir="auto">For a CPU setup please use,</p>
<div data-snippet-clipboard-copy-content="conda env create --file=environment_cpu.yml"><pre><code>conda env create --file=environment_cpu.yml
</code></pre></div>
<p dir="auto">to create the environment instead.</p>
<p dir="auto">Follow by</p>
<div data-snippet-clipboard-copy-content="conda activate tfm_env
pip install -e ."><pre><code>conda activate tfm_env
pip install -e .
</code></pre></div>
<p dir="auto">to install the package.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Initialize the model and load a checkpoint.</h3><a id="user-content-initialize-the-model-and-load-a-checkpoint" aria-label="Permalink: Initialize the model and load a checkpoint." href="#initialize-the-model-and-load-a-checkpoint"></a></p>
<p dir="auto">Then the base class can be loaded as,</p>
<div dir="auto" data-snippet-clipboard-copy-content="import timesfm

tfm = timesfm.TimesFm(
    context_len=<context>,
    horizon_len=<horizon>,
    input_patch_len=32,
    output_patch_len=128,
    num_layers=20,
    model_dims=1280,
    backend=<backend>,
)
tfm.load_from_checkpoint(<checkpoint_path>)"><pre><span>import</span> <span>timesfm</span>

<span>tfm</span> <span>=</span> <span>timesfm</span>.<span>TimesFm</span>(
    <span>context_len</span><span>=</span><span>&lt;</span><span>context</span><span>&gt;</span>,
    <span>horizon_len</span><span>=</span><span>&lt;</span><span>horizon</span><span>&gt;</span>,
    <span>input_patch_len</span><span>=</span><span>32</span>,
    <span>output_patch_len</span><span>=</span><span>128</span>,
    <span>num_layers</span><span>=</span><span>20</span>,
    <span>model_dims</span><span>=</span><span>1280</span>,
    <span>backend</span><span>=</span><span>&lt;</span><span>backend</span><span>&gt;</span>,
)
<span>tfm</span>.<span>load_from_checkpoint</span>(<span>&lt;</span><span>checkpoint_path</span><span>&gt;</span>)</pre></div>
<p dir="auto">Note that the four parameters are fixed to load the 200m model</p>
<div dir="auto" data-snippet-clipboard-copy-content="input_patch_len=32,
output_patch_len=128,
num_layers=20,
model_dims=1280,"><pre><span>input_patch_len</span><span>=</span><span>32</span>,
<span>output_patch_len</span><span>=</span><span>128</span>,
<span>num_layers</span><span>=</span><span>20</span>,
<span>model_dims</span><span>=</span><span>1280</span>,</pre></div>
<ol dir="auto">
<li>
<p dir="auto">The context_len here can be set as the max context length <strong>of the model</strong>. You can provide shorter series to the <code>tfm.forecast()</code> function and the model will handle it. Currently the model handles a max context length of 512, which can be increased in later releases. The input time series can have <strong>any context length</strong>. Padding / truncation will be handled by the inference code if needed.</p>
</li>
<li>
<p dir="auto">The horizon length can be set to anything. We recommend setting it to the largest horizon length you would need in the forecasting tasks for your application. We generally recommend horizon length &lt;= context length but it is not a requirement in the function call.</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Perform inference</h3><a id="user-content-perform-inference" aria-label="Permalink: Perform inference" href="#perform-inference"></a></p>
<p dir="auto">We provide APIs to forecast from either array inputs or <code>pandas</code> dataframe. Both forecast methods expect (1) the input time series contexts, (2) along with their frequencies. Please look at the documentation of the functions <code>tfm.forecast()</code> and <code>tfm.forecast_on_df()</code> for detailed instructions.</p>
<p dir="auto">In particular regarding the frequency, TimesFM expects a categorical indicator valued in {0, 1, 2}:</p>
<ul dir="auto">
<li><strong>0</strong> (default): high frequency, long horizon time series. We recommend to use this for time series up to daily granularity.</li>
<li><strong>1</strong>: medium frequency time series. We recommend to use this for weekly and monthly data.</li>
<li><strong>2</strong>: low frequency, short horizon time series. We recommend to use this for anything beyond monthly, e.g. quarterly or yearly.</li>
</ul>
<p dir="auto">This categorical value should be directly provided with the array inputs. For dataframe inputs, we convert the conventional letter coding of frequencies to our expected categories, that</p>
<ul dir="auto">
<li><strong>0</strong>: T, MIN, H, D, B, U</li>
<li><strong>1</strong>: W, M</li>
<li><strong>2</strong>: Q, Y</li>
</ul>
<p dir="auto">Notice you do <strong>NOT</strong> have to strictly follow our recommendation here. Although this is our setup during model training and we expect it to offer the best forecast result, you can also view the frequency input as a free parameter and modify it per your specific use case.</p>
<p dir="auto">Examples:</p>
<p dir="auto">Array inputs, with the frequencies set to low, medium and high respectively.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import numpy as np
forecast_input = [
    np.sin(np.linspace(0, 20, 100))
    np.sin(np.linspace(0, 20, 200)),
    np.sin(np.linspace(0, 20, 400)),
]
frequency_input = [0, 1, 2]

point_forecast, experimental_quantile_forecast = tfm.forecast(
    forecast_input,
    freq=frequency_input,
)"><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>forecast_input</span> <span>=</span> [
    <span>np</span>.<span>sin</span>(<span>np</span>.<span>linspace</span>(<span>0</span>, <span>20</span>, <span>100</span>))
    <span>np</span>.<span>sin</span>(<span>np</span>.<span>linspace</span>(<span>0</span>, <span>20</span>, <span>200</span>)),
    <span>np</span>.<span>sin</span>(<span>np</span>.<span>linspace</span>(<span>0</span>, <span>20</span>, <span>400</span>)),
]
<span>frequency_input</span> <span>=</span> [<span>0</span>, <span>1</span>, <span>2</span>]

<span>point_forecast</span>, <span>experimental_quantile_forecast</span> <span>=</span> <span>tfm</span>.<span>forecast</span>(
    <span>forecast_input</span>,
    <span>freq</span><span>=</span><span>frequency_input</span>,
)</pre></div>
<p dir="auto"><code>pandas</code> dataframe, with the frequency set to "M" monthly.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import pandas as pd

# e.g. input_df is
#       unique_id  ds          y
# 0     T1         1975-12-31  697458.0
# 1     T1         1976-01-31  1187650.0
# 2     T1         1976-02-29  1069690.0
# 3     T1         1976-03-31  1078430.0
# 4     T1         1976-04-30  1059910.0
# ...   ...        ...         ...
# 8175  T99        1986-01-31  602.0
# 8176  T99        1986-02-28  684.0
# 8177  T99        1986-03-31  818.0
# 8178  T99        1986-04-30  836.0
# 8179  T99        1986-05-31  878.0

forecast_df = tfm.forecast_on_df(
    inputs=input_df,
    freq=&quot;M&quot;,  # monthly
    value_name=&quot;y&quot;,
    num_jobs=-1,
)"><pre><span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>

<span># e.g. input_df is</span>
<span>#       unique_id  ds          y</span>
<span># 0     T1         1975-12-31  697458.0</span>
<span># 1     T1         1976-01-31  1187650.0</span>
<span># 2     T1         1976-02-29  1069690.0</span>
<span># 3     T1         1976-03-31  1078430.0</span>
<span># 4     T1         1976-04-30  1059910.0</span>
<span># ...   ...        ...         ...</span>
<span># 8175  T99        1986-01-31  602.0</span>
<span># 8176  T99        1986-02-28  684.0</span>
<span># 8177  T99        1986-03-31  818.0</span>
<span># 8178  T99        1986-04-30  836.0</span>
<span># 8179  T99        1986-05-31  878.0</span>

<span>forecast_df</span> <span>=</span> <span>tfm</span>.<span>forecast_on_df</span>(
    <span>inputs</span><span>=</span><span>input_df</span>,
    <span>freq</span><span>=</span><span>"M"</span>,  <span># monthly</span>
    <span>value_name</span><span>=</span><span>"y"</span>,
    <span>num_jobs</span><span>=</span><span>-</span><span>1</span>,
)</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Underwater bicycle' propels swimmers forward at superhuman speed (206 pts)]]></title>
            <link>https://newatlas.com/marine/seabike-swimming-propeller/</link>
            <guid>40297748</guid>
            <pubDate>Wed, 08 May 2024 13:13:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/marine/seabike-swimming-propeller/">https://newatlas.com/marine/seabike-swimming-propeller/</a>, See on <a href="https://news.ycombinator.com/item?id=40297748">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I can't say I've seen anything like this "underwater mobility device" before. The idea is simple enough; you extend the Seabike's pole to the appropriate length, then strap it to your waist with a belt. Then you find the pedals with your feet, and start turning the crank, with the waist strap to push against. </p><p>This drives what looks like about a 15-inch (38-cm) propeller. At this point, you start gliding through the water with the splendid, gracious ease of a cruising dugong with an outboard up its bum. You can swim with your arms as well, which creates a surreal visual effect somewhat akin to watching somebody walking along an airport travelator:</p><p>Or you can laze along, arms held out Superman-style. Or indeed, you can angle your nose down, go fully underwater and make like a pedal-powered fish. It's fully compatible with a SCUBA setup if you want to really go nuts down there, although you wouldn't want to take it down too deep and overexert yourself. </p><p>Propellers work both ways, too –&nbsp;so you can also flip the thing upside down, hold the propeller out in front of you, stick some handles on in place of the pedals, and drive the thing with your arms instead. Mind you, this looks a lot less fun. </p><p>Seabike says the prop turns slowly enough that you can safely use it at the local pool – although you'll certainly cop some dirty looks from the Speedo brigade in the fast lane. It's also buoyant, so you won't have to dive to find it if the thing comes off somehow. </p><p>It looks like an incredibly fun way to cover distance in open water, too. Seabike runs its own snorkeling tours out of Cannes, and also sells it with snorkel boards and spear fishing kits. Does it pack down for easy storage? You know it does. </p><div data-video-disable-history="" data-align-center="">
    
        <p><ps-youtubeplayer data-video-player="" data-player-id="f6345bff7244e45f595a795d03a1f64bc" data-video-id="6btiHaTwFKw" data-video-title="SEABIKE PRO +">

    <iframe id="YouTubeVideoPlayer-f6345bff7244e45f595a795d03a1f64bc" role="application" title="YouTube embedded video player" allowfullscreen="" loading="lazy" src="https://www.youtube.com/embed/6btiHaTwFKw?enablejsapi=1"></iframe>
</ps-youtubeplayer>
</p>
    
    
        <p>SEABIKE PRO +</p>
    
</div><p>Best of all, you can instantly charge this device by eating a hot dog. In an age where everything is going electric, something so simple and mechanical is a welcome change. </p><p>It appears Seabike has been making these things for at least a year, selling for prices starting at EU€290 (US$310). The idea doesn't seem to have received much attention yet, but that strikes us as just a matter of time; it's a simple, clever gadget that looks like a ton of fun.</p><p>Personally, I've never really known what to do with my legs on a swim. Nobody's ever properly convinced me that kicking my feet around is worth the effort, absent a set of swim fins. This jigger, according to the manufacturers, makes you handily quicker than an equivalent swimmer with fins on. Sign me up, I'd love to give one a crack!</p><p>Source: <a href="https://www.seabike.fr/" target="_blank" data-cms-ai="0">Seabike</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[40 years later, a game for the ZX Spectrum will be again broadcast over FM radio (231 pts)]]></title>
            <link>https://www.racunalniski-muzej.si/en/40-years-later-a-game-for-the-zx-spectrum-will-be-once-again-broadcast-over-fm-radio/</link>
            <guid>40296926</guid>
            <pubDate>Wed, 08 May 2024 11:49:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.racunalniski-muzej.si/en/40-years-later-a-game-for-the-zx-spectrum-will-be-once-again-broadcast-over-fm-radio/">https://www.racunalniski-muzej.si/en/40-years-later-a-game-for-the-zx-spectrum-will-be-once-again-broadcast-over-fm-radio/</a>, See on <a href="https://news.ycombinator.com/item?id=40296926">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">
	
	
	
		<article id="post-12199">
				
				
			        <p><img width="2560" height="1920" src="https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/PXL_20210626_115223453.PORTRAIT-scaled.jpg" alt="" loading="lazy" srcset="https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/PXL_20210626_115223453.PORTRAIT-scaled.jpg 2560w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/PXL_20210626_115223453.PORTRAIT-300x225.jpg 300w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/PXL_20210626_115223453.PORTRAIT-1024x768.jpg 1024w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/PXL_20210626_115223453.PORTRAIT-768x576.jpg 768w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/PXL_20210626_115223453.PORTRAIT-1536x1152.jpg 1536w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/PXL_20210626_115223453.PORTRAIT-2048x1536.jpg 2048w" sizes="(max-width: 2560px) 100vw, 2560px">			        </p>

		        

			    <div>
		
<p>There were times when Sinclair ZX Spectrum games were copied over the radio waves across Slovenia. <a href="https://radiostudent.si/" title="">Radio Študent</a> broadcast screeching, beeping and whining, which we recorded on tape and played a game a few hours later. Those times are long gone, but we can take a walk through the past today. Radio Študent, which is celebrating its 55th anniversary this week, will invite two members of the legendary Software editorial team to the microphone.</p>



<figure><img src="https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/k2-short.gif" alt=""></figure>



<p>Today at 20:30 the guests will be Žiga Turk, who we know as the co-founder of the magazine <a href="https://en.wikipedia.org/wiki/Moj_mikro" title="">Moj Mikro</a>. As one of the pioneers of the Internet in Slovenia, he wrote the Virtual Shareware Library and Wodo. Together with another guest, Matevž Kmet, he also wrote the famous “Kontrabant”, a cult Slovenian text adventure, and its successor “<a href="https://worldofspectrum.org/software?id=0021603" title="">Kontrabant 2</a>“. The talk will take place in the <a href="https://www.racunalniski-muzej.si/en/home-english/" title="">Computer Museum</a> until 21:30.</p>



<p>This will be followed by a nostalgic broadcast of the game Kontrabant 2 via radio waves at the frequency of 89.3 MHz, which will begin around 21:30. Anyone who still has a working Spectrum ZX will then be able to test the game. Those who do not have one can do so at the Computer Museum or online.</p>



<figure><ul><li><figure><img loading="lazy" width="576" height="1024" src="https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/1-576x1024.jpg" alt="" data-id="12200" data-full-url="https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/1.jpg" data-link="https://www.racunalniski-muzej.si/?attachment_id=12200" srcset="https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/1-576x1024.jpg 576w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/1-169x300.jpg 169w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/1-768x1365.jpg 768w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/1-864x1536.jpg 864w, https://www.racunalniski-muzej.si/wp-content/uploads/2024/05/1.jpg 1080w" sizes="(max-width: 576px) 100vw, 576px"></figure></li></ul></figure>
	</div>

			    
			    			    
					
							    
			    			    				    	<!-- end get_the_author_meta -->
		</article>

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Who Wants to Be a Thousandaire? (2011) (165 pts)]]></title>
            <link>https://www.damninteresting.com/who-wants-to-be-a-thousandaire/</link>
            <guid>40296744</guid>
            <pubDate>Wed, 08 May 2024 11:27:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.damninteresting.com/who-wants-to-be-a-thousandaire/">https://www.damninteresting.com/who-wants-to-be-a-thousandaire/</a>, See on <a href="https://news.ycombinator.com/item?id=40296744">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

                    

                    <p><span>

                                                	<span>Long-Form:</span>
                            <span>Michael Larson had a lot of time and TVs on his hands, and he used them to hack one of his favorite game shows.<br></span>
                            <span>Written by <a href="https://www.damninteresting.com/contributors/alan-bellows/">Alan Bellows</a></span>
                        
                                                	•
                        	<span>Non-Fiction</span>
                        
		        					        		•
			        		<span>September 2011</span>
		        		                    </span>


                </p></div><div>

		            				<article>
											<p>
			© 2011 All Rights Reserved. Do not distribute or repurpose this work without written permission from the copyright holder(s).
	</p>
<p>
	Printed from https://www.damninteresting.com/who-wants-to-be-a-thousandaire/<br>
</p>
												
		                
						
						
											
					<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.734375" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2011/09/larson-mug.jpg" alt="" title=""></p></figure>
<p>On the 19th of May 1984, at CBS Television City in Hollywood, a curious air of tension hung over the studio during the taping of the popular game show <em>Press Your Luck</em>. Ordinarily a live studio audience could be counted upon to holler and slap their hands together, but something was keeping them unusually subdued. The object of the audience’s awe was sitting at the center podium on the stage, looking rather unremarkable in his thrift-store shirt and slicked-back graying hair. His name was Michael Larson.</p>
<p>“You’re going to go again?” asked the show’s host Peter Tomarken as Larson gesticulated. Gasps and murmurs punctuated the audience’s cautious applause, and the contestants sitting on either side of Larson clapped in stunned silence. “Michael’s going <em>again</em>,” Tomarken announced incredulously. “We’ve never had anything like this before.”</p>
<p>The scoreboard on Larson’s podium read “$90,351,” an amount unheard of in the history of <em>Press Your Luck</em>. In fact, this total was far greater than any person had ever earned in one sitting on any television game show. With each spin on the randomized “Big Board” Larson took a one-in-six chance of hitting a “Whammy” space that would strip him of all his spoils, yet for 36 consecutive spins he had somehow missed the whammies, stretched the show beyond its 30-minute format, and accumulated extraordinary winnings. Such a streak was astronomically unlikely, but Larson was not yet ready to stop. He was convinced that he knew exactly what he was doing.
</p>
<p>
Michael Larson was born in the small town of Lebanon, Ohio in 1949. Although he was generally regarded as creative and intelligent, he had an inexplicable preference for shady enterprises over gainful employment. One of his earliest exploits was in middle school, where he smuggled candy bars into class and profitably peddled them on the sly. This innocuous operation was just the first in a decreasingly scrupulous series of ventures. One of his later schemes involved opening a checking account with a bank that was offering a promotional $500 to each new customer; he would withdraw the cash at the earliest opportunity, close the account, then repeat the process over and over under assumed names. </p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.740625" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2011/09/larson-dinwitty.jpg" alt="Michael Larson and Teresa Dinwitty on vinyl." title="Michael Larson and Teresa Dinwitty on vinyl."></p><figcaption>Michael Larson and Teresa Dinwitty on vinyl.</figcaption></figure>
<p>On another occasion he created a fake business under a family member’s name, hired himself as an employee, then laid himself off to collect unemployment wages.</p>
<p>By 1983 Michael Larson had been married and divorced twice and was living with his girlfriend Teresa Dinwitty. During the summers he operated a Mister Softee ice cream truck, and during the off-season he passed the time poring through piles of periodicals in search of money-making schemes. Michael also spent much of the day with his console television, scanning the airwaves for lucrative opportunities. One day it occurred to him that he could double his information intake by setting a second console TV to beside the first and tuning it to a different channel. Soon he procured a third. Eventually he added a row of smaller televisions atop the three consoles, and yet another row of tubes was later stacked atop that. Now he could watch 12 channels at once.</p>
<p>The warm, buzzing television tumor metastasized into adjacent rooms, filling the house with a goulash of infomercials, news programs, game shows, and advertisements for money-making schemes. Larson watched them in a trance-like state, sometimes throughout the night. Dinwitty would later say of her boyfriend and common-law husband, “He always thought he was smarter than everybody else,” and that he had a “constant yearning for knowledge.” But when visitors asked about the chattering mass of receivers she found it easier to just tell them that Michael was crazy.</p>
<p>One fateful November day in 1983, Peter Tomarken’s dapper countenance appeared on one of Michael’s many monitors. Tomarken was the host of a new game show called <em>Press Your Luck</em> which was giving away more money than any other game shows at the time. What most interested Michael was the game’s “Big Board,” an electronic array of prize boxes which operated by lighting up squares in a rapid and random fashion until the player pressed a big red button to stop the action. The player’s randomly selected box might contain a vacation, a prize, cash rewards, and/or extra spins. But with each spin there was also a one-in-six chance of hitting a Whammy which would cause an animated character to appear on the screen and expunge all of a player’s winnings.</p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.73125" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2011/09/pyl-big-board.jpg" alt="Michael's secret safe spots are the ones that contain $3000 and $2000 prizes at the time this picture was taken." title="Michael's secret safe spots are the ones that contain $3000 and $2000 prizes at the time this picture was taken."></p><figcaption>Michael's secret safe spots are the ones that contain $3000 and $2000 prizes at the time this picture was taken.</figcaption></figure>
<p>Larson invested in a newfangled video cassette recorder and began taping episodes of <em>Press Your Luck</em>. After weeks of painstaking scrutiny Michael realized that the bouncing prize selector did not actually move randomly; it always followed one of five lengthy sequences. This information was only moderately useful due to the rapidly shuffling positions of the prizes and penalties, but his methodical analysis led to another finding. Of the eighteen squares on the Big Board there were two that never had Whammies: #4 and #8. This meant that all a player must do to avoid Whammies⁠—and thus retain his hundreds of dollars in winnings⁠—would be to memorize five interminable series of numbers and develop superhuman reflexes. Giddy with the thrill of discovery, Larson began fine-tuning his timing using his VCR’s pause key as a surrogate big red button.</p>
<p>Six months later, in May 1984, Michael Larson sat beardily in the interview room for the <em>Press Your Luck</em> auditions in Hollywood. His story left few heartstrings unpulled: He explained that he was an unemployed ice cream truck driver. He had borrowed the bus money to get to Hollywood from Ohio because he loved <em>Press Your Luck</em>. He had stopped at a thrift store down the street to buy a 65 cent dress shirt. And he was unable to afford a gift for his six-year-old daughter’s upcoming birthday. Executive producer Bill Carruthers said of Larson’s audition, “He really impressed us. He had charisma.” Contestant coordinator Bob Edwards was uneasy about Larson, but he couldn’t quite articulate why, so Bill overruled him. “I should have listened to Bob,” Carruthers later chuckled.</p>
<p>Taping occurred the following Saturday. Returning champion Ed Long sat on Michael’s right and contestant Janie Litras sat on his left. Host Peter Tomarken made boilerplate game-showey chit-chat with each contestant, and he asked Michael about his ice cream truck. “You’ve kind of OD’d on ice cream, right?” he asked Larson, who agreed. “Well hopefully you won’t OD on money, Michael.”</p>
<p>Michael earned 3 spins on the Big Board in the first question round, giving him 3 opportunities to test the skills he had cultivated over the past six months. The board’s incandescent selector began its distinctive pseudo-random maneuvers. “Come on…big bucks,” Michael chanted, as was customary for players when up against the Big Board. “STOP!” he shouted as he slapped the button with both hands. The selector was stopped on a Whammy in slot #17. Michael shook his head and forced an embarrassed smile, but now he knew exactly how the board was timed with respect to the button. With his second and third spins Michael found his stride. He dropped all pretenses and remained silent as he concentrated on the light bouncing around the big board. Both times he successfully landed on space #4, and he ended the first half of the game with $2,500.</p>
<figure data-embiggen="true"><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.72058823529412" data-lazy-load-src="https://damn-8791.kxcdn.com/wp-content/uploads/2011/09/larson-enlarged.jpg" alt="" title=""></p></figure>
<p>In the second and more lucrative half of the game, Michael managed to acquire seven spins to use on the big board. Since he was in last place he was the first to spin. He positioned his hands over the button with interlocked fingers and impatiently interrupted the host’s banter by shouting, “I’m ready, I’m ready!” Tomarken indulged him, and the light on the big board began bouncing. Again, Larson was silent as he frowned at the board. Fellow contestant Ed Long would later say of Larson during these moments that “he went into a trance.”</p>
<p>Thus began Larson’s inconceivable procession of winning spins. His demeanor alternated between intense concentration and jubilation. The strategy worked even better than he had anticipated due to the large number of Free Spin bonuses that appeared in his safe slots. Host Peter Tomarken became increasingly flabbergasted each time Larson made the “spin again” gesture. $30,000 was considered an extraordinary payoff for one day on any game show at that time, and the likelihood of missing the whammies for more than a dozen spins was considered to be vanishingly small. By his 13th spin Michael had $32,351 and nervous giggles. By his 21st spin he had $47,601 and conspicuous anxiety. But he pressed on.</p>
<p>The <em>Press Your Luck</em> control booth had grown silent as the show’s producers began to realize that Larson was consistently winning on the same two spaces. In a panic, the booth operators called Michael Brockman, CBS’s head of daytime programming. “Something was very wrong,” Brockman said in a <em>TV Guide</em> interview. “Here was this guy from nowhere, and he was hitting the bonus box every time. It was bedlam, I can tell you.” Producers asked if they should stop the show, but Larson did not appear to be breaking any rules so they were forced to allow the episode to play out.</p>
<p>Back on the stage, Ed and Janie clapped incredulously on either side of Michael, still waiting for their turns on the board. Janie let slip a snort of disgust after Michael’s 26th successful spin. Tomarken covered his face with his hand in disbelief as Larson risked almost $75k on his 32nd spin. But Michael’s zen-like concentration was beginning to falter. He paused to set his head on the podium and let out a whimper of exhaustion. Still he motioned to continue. The studio audience worried that he’d hit a whammy and experience an unfortunate reversal of fortune, while the producers in the control booth worried that he wouldn’t.</p>
<p>On his 40th spin Larson’s scoreboard debt-clocked his dollar sign to make room for another digit; he surpassed $100,000. Larson, his shoulders slumped, passed his remaining spins to the bewildered Ed Long. Ed immediately hit a whammy.</p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.73125" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2011/09/tomarken.jpg" alt="Host Peter Tomarken failing to believe what he is seeing." title="Host Peter Tomarken failing to believe what he is seeing."></p><figcaption>Host Peter Tomarken failing to believe what he is seeing.</figcaption></figure>
<p> Michael sat in a twitchy daze as Ed and Janie went through their much more pedestrian turns at the board. But Larson was snapped back to reality when Janie passed 3 of her spins to him. According to the game rules he was obligated to use them. He did not appear pleased.</p>
<p>“I didn’t want them,” Larson joked nervously as the light began bouncing around the big board, yet almost immediately he punched the big red button and landed on $4,000 in slot #4. Janie let out a squeal. The board started again. After a longer than usual delay, Larson hit the button again, landing safely in slot #8. He had just one mandatory spin remaining. The board started flashing, and Larson let out a sigh. “STOP!” he shouted as he slapped the button, but he had pressed it a fraction of a second too soon. Slot #17 was lit, the same slot where he’d hit a whammy on his first spin. As luck would have it, however, the slot contained a trip to the Bahamas. It was over; Michael had won. Larson gave Ed an awkward embrace and offered Janie a firm handshake. In total, Larson won $110,237 in cash and prizes, including two tropical vacations and a sailboat. Reportedly this was more than triple the previous record for winnings in a single episode of a game show.</p>
<p>A clearly discombobulated Peter Tomarken engaged Larson in an impromptu interview after the show. “Why did you keep going?” he asked.</p>
<p>“Well, two things:” Michael replied. “One, it felt right. And second, I still had seven spins and if I passed them, somebody could have done what I did.”</p>
<p>Tomarken was too polite to remark on the ridiculousness of that suggestion. “What are you going to do with the money, Michael?”</p>
<p>“Invest in houses.”</p>
<p>Larson was not allowed to return as champion since he had surpassed CBS’s $25k winnings limit. As all of the perplexed parties parted ways, CBS executives were called to a meeting to dissect the episode frame-by-frame. In spite of their efforts they could find no evidence of wrongdoing or rule-breaking, so after a few weeks they grudgingly mailed Larson his check. Some people at CBS didn’t want the over-extended episode to be released to the public at all, but it was ultimately decided to air it in June as an awkwardly edited two-parter. </p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.590625" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2011/09/architect.jpg" alt="" title=""></p></figure>
<p>Executives insisted that the episode never be seen again. In the meantime <em>Press Your Luck</em> paid to add some more sequences to the Big Board to prevent future contestants from mimicking Michael’s strategy.</p>
<p>Upon his return home, neighbors were shocked to learn of “crazy” Michael Larson’s accomplishment. True to his word, he regaled his daughter with expensive birthday gifts and invested some of his spoils in real estate. But his fondness for dicey get-rich-quick deals ensnared him in a Ponzi scheme, and he lost enough money to lose his appetite for houses.</p>
<p>Some months later Michael Larson saw another opportunity to stack the odds in his favor with a dash of ingenuity. He walked into his bank one day and asked to withdraw his entire account balance, but with an unusual stipulation: He wanted as much of the cash as possible in one dollar notes. The bank complied with his unorthodox request, and from there he proceeded to another bank to trade even more of his savings for singles. Over a two week period he converted the $100,000 or so that remained of his personal savings into 100,000 one dollar bills.</p>
<p>The motivation for this aberrant behavior was a contest put on by a local radio station. Each day a disk jockey would read a serial number aloud on the air, and if any listener was able to produce the matching dollar bill they would win $30,000. Michael reasoned that 100,000 one dollar bills was 100,000 opportunities to win the prize, giving him a statistical advantage. And even if his scheme proved fruitless he would just redeposit his money, so he figured he had nothing to lose.</p>
<p>Michael and Teresa spent each day rifling through piles of cash looking for matches, pausing only for such distractions as eating, bathing, and excreting. They soon realized that it was impossible for two people to examine that much money in the allotted time, so Michael redeposited a portion of it. After a few weeks, Michael’s obsession over the contest began to put considerable strain on his relationship with Teresa, and on his relationship with reality. The cash was stashed in kitchen drawers, up the stairs, and on bedroom floors. They kept the bills in burlap sacks, grocery bags, and unkempt stacks. And though his girlfriend would scream and shout, he simply would not take the cash bags out.</p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.753125" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2011/09/larson-stare.jpg" alt="" title=""></p></figure>
<p>One evening, seeking refuge from the endless hours of cash-collating, Michael and Teresa accepted an invitation to attend a Christmas party. When they returned home at about 1:00 am, they found the back door of the house had been brutalized. Apparently the pair had unwittingly left a sizable tip for an unsolicited cleaning service: about $50,000. According to Dinwitty, Michael immediately accused her of being an accessory to the heist. She denied involvement, and police found no evidence of her guilt, but she says that Larson was never convinced. She claimed that Michael would stand and stare at her while she slept, which made her fear for her safety. One day while Michael was away she took $5,000 that he had hidden in a dresser drawer and absconded with the kids. She called him from a hotel to tell him to move out of her house. His only response was, “I want my money back.” He packed his belongings and departed, leaving one wall of the living room blemished and peeling from the heat of his once-formidable tower of televisions.</p>
<p>Police never identified the thieves. In 1994, about 10 years after his pivotal <em>Press Your Luck</em> appearance, Larson was invited to be a guest on ABC’s <em>Good Morning America</em> to discuss the movie <em>Quiz Show</em>. With a raspy voice he unbeardily reminisced about his game show exploits and expressed regret that he was never able to play on Jeopardy, because, he explained, “I think I have figured out some angles on that.” Around that same time he was also interviewed by <em>TV Guide</em> magazine. When asked about the whereabouts of his <em>Press Your Luck</em> winnings, he replied, “It didn’t work out. We had a cash-flow problem, and I lost everything.”</p>
<p>In March of the following year, Michael fled from Ohio with agents from the SEC, IRS, and FBI hot on his heels. He was implicated as one of the architects of a cash-flow solution that operated under the name Pleasure Time Incorporated. It was a pyramid scam selling shares in a fraudulent “American Indian Lottery” which had hoodwinked 20,000 investors out of 3 million dollars. The Pleasure Time flimflam was historic in that it was the first time the SEC pursued a case where the bulk of the fraud took place in newfangled “cyberspace.” Michael Larson was a fugitive from justice for four years until 1999, when he turned up in Apopka, Florida. He had succumbed to throat cancer.</p>
<figure><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAMAAAAoyzS7AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAAZQTFRFgYGBAAAAqeg2zgAAAAxJREFUeNpiYAAIMAAAAgABT21Z4QAAAABJRU5ErkJggg==" data-height-ratio="0.746875" data-lazy-load-src="//damn-8791.kxcdn.com/wp-content/uploads/2011/09/larson-gma.jpg" alt="Michael Larson's appearance on Good Morning America" title="Michael Larson's appearance on Good Morning America"></p><figcaption>Michael Larson's appearance on <em>Good Morning America</em></figcaption></figure>
<p>Michael Larson held the record for the most game-show winnings in a single day until 2006, when it was broken by Vickyann Chrobak-Sadowski on <i>The Price is Right</i>. Larson’s handiwork on <em>Press Your Luck</em> was sufficiently extraordinary that he has become a strange kind of folk hero to some. Others regard him as a cheap huckster or a likable-but-occasionally-creepy crackpot. The real Michael Larson was arguably an amalgam of these qualities. His shenanigans on <em>Press Your Luck</em> are oft described as a “scam,” “scandal,” or a “cheat,” but even the CBS executives ultimately admitted that he had broken nary a rule. In the end, his impressive performance on <em>Press Your Luck</em> may be one of the only honest days of work that Michael Larson ever did.</p>

				

										
										
						<p>
			© 2011 All Rights Reserved. Do not distribute or repurpose this work without written permission from the copyright holder(s).
	</p>
<p>
	Printed from https://www.damninteresting.com/who-wants-to-be-a-thousandaire/<br>
</p>
						<p>
							<i>Since you enjoyed our work enough to print it out, and read it clear to the end, would you consider donating a few dollars at https://www.damninteresting.com/donate</i> ?
						</p>
									</article>
			
		            	            		
	            	
		            			

				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The search for easier safe systems programming (168 pts)]]></title>
            <link>https://www.sophiajt.com/search-for-easier-safe-systems-programming/</link>
            <guid>40295624</guid>
            <pubDate>Wed, 08 May 2024 08:26:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sophiajt.com/search-for-easier-safe-systems-programming/">https://www.sophiajt.com/search-for-easier-safe-systems-programming/</a>, See on <a href="https://news.ycombinator.com/item?id=40295624">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>I've been involved in the Rust project in some form or another since 2016, and it's a language I'm very comfortable using. Many Rust programmers could <a href="https://blog.rust-lang.org/2024/02/19/2023-Rust-Annual-Survey-2023-results.html">say the same</a>. But, if we take a step back and are honest with ourselves, we'd admit that the road to getting to that level of comfort was difficult.</p>
<p>I taught Rust professionally for two years. Watching the faces of people trying to learn Rust for the first time reminded me just how hard this language is to learn.</p>
<p>After two years of that, I wanted to answer a question I wasn't entirely sure had an answer: <em>Is it possible to make an easy-to-use, easy-to-learn, and easy-to-teach safe systems language?</em> Could I put my career working on programming languages (TypeScript, Rust, Nushell, etc) to use and find a solution?</p>
<h2 id="enter-june">Enter June</h2>
<p>For the last year and a half, I and my recently-added collaborator Jane Losare-Lusby have been working in secret on a safe systems language that could be learned about as quickly as one can learn Go. I think we might have something worth exploring.</p>
<h2 id="changing-how-we-think-of-memory">Changing how we think of memory</h2>
<p>In Rust, we think of each piece of memory as having its own lifetime. Each of these lifetimes must be tracked, sometimes leading to rather complex code, complex error messages, and/or complex mental models of what is happening. The complexity of course comes with the benefit of being highly precise about each and every piece of memory and its reclamation.</p>
<p>Using a Rust example:</p>
<pre><code><span>struct </span><span>Node {
    </span><span>data1</span><span>: &amp;Data
    data2: &amp;Data
    data3: &amp;Data
}
</span></code></pre>
<p>Rust developers will spot right away that this is an incomplete example. We need two more things: Lifetime Parameters and Lifetime Annotations. Adding those, we get:</p>
<pre><code><span>struct </span><span>Node&lt;</span><span>'a</span><span>, </span><span>'b</span><span>, </span><span>'c</span><span>&gt; {
    </span><span>data1</span><span>: &amp;</span><span>'a</span><span> Data
    data2: &amp;</span><span>'b</span><span> Data
    data3: &amp;</span><span>'c</span><span> Data
}
</span></code></pre>
<p>The concept count for this example ends up being pretty substantial. Counting them off, we get:</p>
<ul>
<li>Lifetimes</li>
<li>Lifetime annotations</li>
<li>Lifetime parameters</li>
<li>Ownership and borrowing</li>
<li>Generics</li>
</ul>
<p>When I showed examples like this to my class when I taught Rust, I had to walk them through each of those concepts first before I could show the full example.</p>
<p>The question then is: can we make this easier?</p>
<h2 id="what-if-memory-was-grouped">What if memory was grouped?</h2>
<p>What if instead of having to track every piece of memory's lifetime separately, we let groups of related allocations share a lifetime?</p>
<p>Effectively, this would mean that a data structure, like a linked list, would have a pointer pointing to the head which has a lifetime, and then every node in the list you can reach from that head has the same lifetime.</p>
<p>There are some benefits to this approach, as well as a few drawbacks. Let's take a look at the benefits first.</p>
<h2 id="benefits-of-grouped-allocations">Benefits of grouped allocations</h2>
<p>Exploring grouped allocations, we noticed some immediate benefits. The first is that we could treat all user-defined values as pointers, and these pointers could also represent their own lifetimes (without needing lifetime parameters). This makes the code feel quite a bit lighter:</p>
<pre><code><span>struct </span><span>Node {
    </span><span>data</span><span>: </span><span>i64</span><span>
    next: Node?
}

</span><span>let</span><span> n = new Node(data: </span><span>123</span><span>, next: none)
</span></code></pre>
<p>Since all user data is pointers, we can use the name of the type to mean "pointer to this structured data".</p>
<p>The next thing we noticed is that both lifetimes and inference for lifetimes becomes significantly simpler.</p>
<p>Let's take a variation of the example above:</p>
<pre><code><span>struct </span><span>Node {
    </span><span>data</span><span>: </span><span>i64</span><span>
    next: Node?
}

fun </span><span>do_this</span><span>() {
    </span><span>let</span><span> n = new Node(data: </span><span>123</span><span>, next: none)
}
</span></code></pre>
<p>We can infer that the allocation that creates <code>new Node(...)</code> has a lifetime and what it is. Because this allocation never "escapes" the function - that is, it never leaves the function in any way - then we can call its lifetime "Local".</p>
<p>As we'll find out, each of the lifetime possibities is a readable name that we can show the user in error messages. It also makes things significantly easier to teach.</p>
<p>Let's look at another example to see a different lifetime.</p>
<pre><code><span>struct </span><span>Stats {
    </span><span>age</span><span>: </span><span>i64
</span><span>}

</span><span>struct </span><span>Employee {
    </span><span>name</span><span>: c_string,
    </span><span>stats</span><span>: Stats,
}

fun </span><span>set_stats</span><span>(</span><span>mut</span><span> employee: Employee) {
    employee.stats = new Stats(age: </span><span>33</span><span>)
}

fun </span><span>main</span><span>() {
    </span><span>mut</span><span> employee = new Employee(name: c"</span><span>Soph</span><span>", stats: new Stats(age: </span><span>22</span><span>))
    </span><span>set_stats</span><span>(employee)
    </span><span>println</span><span>(employee.stats.age)
}
</span></code></pre>
<p>A bit of a longer example this time, but let's focus on this function:</p>
<pre><code><span>fun </span><span>set_stats</span><span>(</span><span>mut</span><span> employee: Employee) {
    employee.stats = new Stats(age: </span><span>33</span><span>)
}
</span></code></pre>
<p>What's the lifetime of this <code>new Stats(..)</code> allocation? In this example, we do see the new pointer escape the function via a parameter. We can also give this a readable lifetime: <code>Param(employee)</code></p>
<p>In all, we have three lifetimes an allocation can have:</p>
<ul>
<li>Local</li>
<li>Param(xxxx)</li>
<li>Return</li>
</ul>
<h2 id="any-data-structure-you-want">Any data structure you want</h2>
<p>Another big advantage of grouping our allocations is that we no longer have to worry about a drop order. This means we can think of the whole thing as dropping all at once. For large structures, that can be a speed-up over languages with a required drop order.</p>
<p>Additionally, we get another major benefit. We can now create arbitrary data structures.</p>
<pre><code><span>struct </span><span>Node {
    </span><span>data</span><span>: </span><span>i64</span><span>
    next: Node?
}

</span><span>mut</span><span> node3 = new Node(data: </span><span>3</span><span>, next: none)
</span><span>let</span><span> node2 = new Node(data: </span><span>2</span><span>, next: node3)
</span><span>let</span><span> node1 = new Node(data: </span><span>1</span><span>, next: node2)

node3.next = node1
</span></code></pre>
<p>And just like that, we've made a circular linked list. Creating a similar example in Rust is certainly more of a challenge.</p>
<p>But, something fishy is going on here.</p>
<p>To make the above work, we're using shared, mutable pointers. This is explicitly forbidden in Rust. Why is it okay here?</p>

<p>Rust disallows holding two mutable references to the same memory location and for good reason. Well, multiple reasons actually.</p>
<p>First, having two copies of a mutable pointer where two separate threads each hold a copy means we have the possibility for a race condition. This can leave us with incoherent data that's difficult to debug.</p>
<p>Second, even if these two multiple pointers are limited to the same thread, we get what we might call "spooky action at a distance". The modification of one pointer is then visible to the holder of the other pointer, which might be far away from the source of the mutation.</p>
<p>For us to reasonably use shared, mutable pointers, we need to tame both of these. The first issue, the race condition, is easy enough: we prevent sending shared, mutable pointers between threads. This limits them to a single thread.</p>
<p>The second issue is decidedly harder. There have been many attempts at ways of handling this through rules enforced by the type system.</p>
<p>In June, we're trying something a bit different. We'll let developers use shared, mutable pointers, but then offer a "carrot" to opt-in to restrictions around using them. The carrot ends up pulling from a classic technique of software engineering: encapsulation.</p>
<h2 id="the-full-power-of-encapsulation">The full power of encapsulation</h2>
<p>In traditional encapsulation, programmers make a kind of "best effort" to hide implementation details from the world around them. Keeping private state private grants the benefits of better code reuse, ease of updating implementation details, and more.</p>
<p>But as often is the case, if that kind of rule isn't enforced, over time APIs get designed where internal implementation details leak out.</p>
<p>Something very interesting happens if we don't allow this to happen. If an encapsulation can be checked by the compiler, and the compiler enforces that no private details leak, we have what you might call "full encapsulation".</p>
<p>These kinds of encapsulations wouldn't allow any aliasing of pointers into them. They'd have their internal pointers fully isolated from the rest of the program.</p>
<p>Once we have this, some new capabilities start opening up:</p>
<ul>
<li>We can "fence off" our shared, mutable pointers, making it possible to create single-owner encapsulations that can be sent safely between threads.</li>
<li>We can lean people in the direction of cleaner API design, as now we have a way to truly keep private implementation details private.</li>
<li>We can handle some of the drawbacks of grouped allocations.</li>
</ul>
<p>What kind of drawbacks, you might ask? It's high time we talked about them.</p>
<h2 id="drawbacks-of-grouped-allocations">Drawbacks of grouped allocations</h2>
<p>If we go back to our earlier example and look carefully, we'll notice something:</p>
<pre><code><span>fun </span><span>set_stats</span><span>(</span><span>mut</span><span> employee: Employee) {
    employee.stats = new Stats(age: </span><span>33</span><span>)
}

fun </span><span>main</span><span>() {
    </span><span>mut</span><span> employee = new Employee(name: c"</span><span>Soph</span><span>", stats: new Stats(age: </span><span>22</span><span>))
    </span><span>set_stats</span><span>(employee)
    </span><span>println</span><span>(employee.stats.age)
}
</span></code></pre>
<p>The question is: what happened to the <code>new Stats(age: 22)</code> allocation?</p>
<p>Remembering that June is a systems language, we can't say "the garbage collector handled it" because we have no garbage collector. Nor can we say "the refcount hit zero, so we reclaimed it" as we don't use refcounting. As a systems language, we can't allow hidden or difficult-to-predict overhead to happen.</p>
<p>It's not actually leaked either, as even the memory it occupies will be reclaimed once the entire group is reclaimed. For all intents and purposes, though, it's lost to the user until the group is no longer live. It's a kind of "memory bloat" that happens if we group our allocations.</p>
<p>To handle this, we'll need some way of recycling that memory. I say "recycling" specifically because in June we can't free the memory, as the group is treated together as a single entity where all the allocations in the group are freed at once. If we instead recycle the memory, we can reuse that same memory while the group is live.</p>
<p>Techniques to do this have been around for decades, and often people use "free lists" to keep a list of nodes that have been recycled, so they can be reused when the next allocation happens.</p>
<p>The problem with free lists is that they aren't safe. If you're not careful, you'll create a security vulnerability and/or an incredibly hard bug to find.</p>
<p>Instead, we need to build in a safe way of recycling memory into the language.</p>
<h2 id="safe-memory-recycling">Safe memory recycling</h2>
<p>Using the idea of full encapsulation from earlier, we can create "fenced in" sets of pointers that we know aren't shared with the rest of the world. Once we have them, it's possible to track the pointers inside. These pointers can get a "copy count", so we know how many copies are live at any point in time (not dissimilar from a refcount, though this has no automatic reclamation).</p>
<p>Once we have a copy count for each internal pointer, we can give developers a built-in <code>recycle</code> command.</p>
<pre><code><span>let x = new Foo()
recycle x
</span></code></pre>
<p>Recycling would start at the given pointer and would check the pointers reachable from it. Each pointer it finds that it can recycle would go into the safe free list.</p>
<p>You might wonder "why not do this automatically?". There are a couple reasons:</p>
<ul>
<li>The operation is linear time based on your transitively-reachable pointers. This means you may incur a noticeable overhead when recycling</li>
<li>Because of the first point, it's important to make places where this occurs visible</li>
</ul>
<p>If this sounds like a kind of manual garbage collection, you're right. My collaborator Jane calls this "semi-automatic" memory reclamation. You ask once, and when you ask you get a kind of highly focused mark and sweep for that single pointer and the pointers reachable from it.</p>
<p><em>Note: this feature is not yet in the reference compiler. We're hoping to implement it in the coming weeks.</em></p>
<h2 id="more-work-ahead">More work ahead</h2>
<p>We have a way of simplifying lifetimes, making for readable code that people from various languages should be able to understand and use. We can also give clear, easy-to-understand lifetime errors when they arise.</p>
<p>Having safe memory recycling gives us a way to keep groups and still offer things like <code>delete</code> in a linked list abstraction. It's convenient but not so automatic that we lose the visibility into the costs of memory management.</p>
<p>That said, there are still some challenge ahead that will need to be solved in the language design and tooling. For example, how do you know when the program is bloating memory? We'll need some way of doing a memory trace when the program is running to detect this and warn the developer.</p>
<p>I see this in a way as a more incremental/prototype-friendly way of development. June is always memory safe, but the first version of a program may not be as efficient as it could be in terms of memory usage. That's a process we often go through as developers. First, we "make it work" before we "make it good".</p>
<p>In June, we keep it lightweight as we keep your programs memory safe, and then we provide tools and support for incrementally improving code.</p>
<h2 id="future-possibilities">Future possibilities</h2>
<h3 id="relationship-to-rust">Relationship to Rust</h3>
<p>June has a real opportunity to be a good complement to Rust. Rust's focus on embedded and system's development is a core strength. June, on the other hand, has a lean towards application development with a system's approach. This lets both co-exist and offer safe systems programming to a larger audience.</p>
<p>An even better end state requires Rust to have a stable ABI. Once it does, June will be able to call into Rust crates to get the benefits of Rust's substantial crate ecosystem. We're looking forward to collaborating on this in the future.</p>
<h3 id="going-beyond-oop">Going beyond OOP</h3>
<p>OOP has for decades been the way many applications are written, but it's not without its flaws. Many OOP languages allow programmers to freely break good rules of thumb, like the Liskov substitution principle, or to create a mess of interwoven code between parent and child classes that's difficult to maintain.</p>
<p>We're currently investigating other ways of making code reuse easier, more modular, and more composible. We're not quite ready to talk about this, though we hope to soon.</p>

<p>Over the years, there have been a <a href="https://verdagon.dev/grimoire/grimoire">number of memory management techniques tried</a>, including many that lie outside of the ones commonly found in languages today. We'd like to explore these more deeply to see which, if any, may help June.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>We had the help of dozens of experts in various fields as we brainstormed the initial design for June, and for their contributions, we're thankful. We'd especially like to thank the collaborators who went above and beyond with their time across multiple brainstorming sessions to help June grow to where it is.</p>
<ul>
<li>Andreas Kling</li>
<li>Doug Gregor</li>
<li>Jason Turner</li>
<li>Mads Torgersen</li>
<li>Mae Milano</li>
<li>Steve Francia</li>
</ul>
<p>Also, special thanks to our private beta testers for testing out June and giving us feedback.</p>
<h2 id="checking-it-out">Checking it out</h2>
<p>Documentation on the June language and the June reference compiler are now available via the <a href="https://github.com/sophiajt/june">June repo</a>.</p>
<p>Please note: the reference compiler is pre-alpha quality.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[XLSTM: Extended Long Short-Term Memory (145 pts)]]></title>
            <link>https://arxiv.org/abs/2405.04517</link>
            <guid>40294650</guid>
            <pubDate>Wed, 08 May 2024 05:28:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2405.04517">https://arxiv.org/abs/2405.04517</a>, See on <a href="https://news.ycombinator.com/item?id=40294650">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2405.04517">View PDF</a></p><blockquote>
            <span>Abstract:</span>In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Maximilian Beck [<a href="https://arxiv.org/show-email/ed28509f/2405.04517">view email</a>]      <br>    <strong>[v1]</strong>
        Tue, 7 May 2024 17:50:21 UTC (1,455 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. Rules Apple Illegally Interrogated Staff and Confiscated Union Flyers (473 pts)]]></title>
            <link>https://www.forbes.com/sites/antoniopequenoiv/2024/05/06/us-labor-board-rules-apple-illegally-interrogated-staff-and-confiscated-union-flyers/</link>
            <guid>40294630</guid>
            <pubDate>Wed, 08 May 2024 05:25:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.forbes.com/sites/antoniopequenoiv/2024/05/06/us-labor-board-rules-apple-illegally-interrogated-staff-and-confiscated-union-flyers/">https://www.forbes.com/sites/antoniopequenoiv/2024/05/06/us-labor-board-rules-apple-illegally-interrogated-staff-and-confiscated-union-flyers/</a>, See on <a href="https://news.ycombinator.com/item?id=40294630">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Topline</h2>
<p>The National Labor Relations Board <a href="https://www.nlrb.gov/case/02-CA-295979" rel="nofollow noopener noreferrer" target="_blank" title="https://www.nlrb.gov/case/02-CA-295979" data-ga-track="ExternalLink:https://www.nlrb.gov/case/02-CA-295979" aria-label="ruled">ruled</a> Monday that Apple illegally questioned staff of its World Trade Center store in New York City in 2022, affirming findings from a judge who determined employees were specifically questioned over <a href="https://www.bloomberg.com/news/articles/2023-06-21/apple-illegally-interrogated-staff-about-union-judge-rules" rel="nofollow noopener noreferrer" target="_blank" title="https://www.bloomberg.com/news/articles/2023-06-21/apple-illegally-interrogated-staff-about-union-judge-rules" data-ga-track="ExternalLink:https://www.bloomberg.com/news/articles/2023-06-21/apple-illegally-interrogated-staff-about-union-judge-rules" aria-label="their pro-union sympathies">their pro-union sympathies</a>.</p>
<figure role="presentation"><figcaption><fbs-accordion current="-1"><p>Apple has not received any punishment or been ordered to pay damages by the board for the <span data-ga-track="caption expand">... [+]</span><span> violations. (Photo by Gary Hershorn/Getty Images)</span></p></fbs-accordion><small>Getty Images</small></figcaption></figure> 

<h2>Key Facts</h2>
<div>
 <div>
  <p>The board affirmed the decision of administrative law Judge Lauren Esposito, who ruled last year that Apple illegally stopped workers from placing union flyers on a table in the break room of the World Trade Center store, confiscated the flyers and interrogated staff over their “protected concerted activity.”</p>
  
 </div>
 <div>
  <p>Esposito ordered Apple cease and desist from illegally questioning workers about union matters in addition to confiscating union flyers from the store’s employee break room.</p>
  
 </div>
 <div>
  <p>Monday’s ruling is the board’s first decision against Apple, according to <a href="https://www.bloomberg.com/news/articles/2024-05-06/apple-illegally-interrogated-nyc-retail-staff-us-labor-board-rules?srnd=homepage-americas" rel="nofollow noopener noreferrer" target="_blank" title="https://www.bloomberg.com/news/articles/2024-05-06/apple-illegally-interrogated-nyc-retail-staff-us-labor-board-rules?srnd=homepage-americas" data-ga-track="ExternalLink:https://www.bloomberg.com/news/articles/2024-05-06/apple-illegally-interrogated-nyc-retail-staff-us-labor-board-rules?srnd=homepage-americas" aria-label="Bloomberg">Bloomberg</a>, which first reported the ruling and cited agency spokesperson Kayla Blado.</p>
  
 </div>
 <p>The board cannot impose fines or direct punishments against Apple for its violations.</p>
 <p>Apple didn’t immediately respond to Forbes’ request for comment.</p>
</div>
<p><em>Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up </em><a href="https://joinsubtext.com/forbes" rel="nofollow noopener noreferrer" target="_blank" title="https://joinsubtext.com/forbes" data-ga-track="ExternalLink:https://joinsubtext.com/forbes" aria-label="here"><em data-ga-track="ExternalLink:https://joinsubtext.com/forbes">here</em></a><em>.</em></p>


<h2>Key Background</h2>
<p>Other cases against Apple are still pending, according to Bloomberg, which noted a case in which a National Labor Relations Board member accused the company of illegally excluding unionized workers from certain benefits. Several Apple stores have <a href="https://www.theverge.com/2024/4/10/24126657/apple-store-employees-new-jersey-unionize" rel="nofollow noopener noreferrer" target="_blank" title="https://www.theverge.com/2024/4/10/24126657/apple-store-employees-new-jersey-unionize" data-ga-track="ExternalLink:https://www.theverge.com/2024/4/10/24126657/apple-store-employees-new-jersey-unionize" aria-label="moved to unionize">moved to unionize</a> in recent years including ones in Short Hills, New Jersey, Oklahoma City and Towson, Maryland, with the latter two locations successfully establishing a union. Apple employees outside of the World Trade Center store staffers have also run into opposition while seeking to unionize. The National Labor Relations Board found in late 2022 that Apple hosted mandatory <a href="https://www.bloomberg.com/news/articles/2022-12-05/apple-s-anti-union-tactics-in-atlanta-were-illegal-us-officials-say" rel="nofollow noopener noreferrer" target="_blank" title="https://www.bloomberg.com/news/articles/2022-12-05/apple-s-anti-union-tactics-in-atlanta-were-illegal-us-officials-say" data-ga-track="ExternalLink:https://www.bloomberg.com/news/articles/2022-12-05/apple-s-anti-union-tactics-in-atlanta-were-illegal-us-officials-say" aria-label="anti-union meetings">anti-union meetings</a> at an Atlanta store where management made coercive statements against employees.</p>


<h2>Further Reading</h2>
<p><a href="https://www.bloomberg.com/news/articles/2024-05-06/apple-illegally-interrogated-nyc-retail-staff-us-labor-board-rules?srnd=homepage-americas" rel="nofollow noopener noreferrer" target="_blank" title="https://www.bloomberg.com/news/articles/2024-05-06/apple-illegally-interrogated-nyc-retail-staff-us-labor-board-rules?srnd=homepage-americas" data-ga-track="ExternalLink:https://www.bloomberg.com/news/articles/2024-05-06/apple-illegally-interrogated-nyc-retail-staff-us-labor-board-rules?srnd=homepage-americas" aria-label="Apple Illegally Interrogated NYC Retail Staff, US Labor Board Rules">Apple Illegally Interrogated NYC Retail Staff, US Labor Board Rules</a> (Bloomberg)</p>
<p><a href="https://www.theverge.com/2024/4/10/24126657/apple-store-employees-new-jersey-unionize" rel="nofollow noopener noreferrer" target="_blank" title="https://www.theverge.com/2024/4/10/24126657/apple-store-employees-new-jersey-unionize" data-ga-track="ExternalLink:https://www.theverge.com/2024/4/10/24126657/apple-store-employees-new-jersey-unionize" aria-label="Apple Store employees in New Jersey are trying to unionize">Apple Store employees in New Jersey are trying to unionize</a> (The Verge)</p>
</div><div><p><span>Follow me on&nbsp;</span><a href="https://www.twitter.com/pequeno04" rel="nofollow noopener noreferrer" target="_blank">Twitter</a>&nbsp;or&nbsp;<a href="https://www.linkedin.com/in/antonio-peque%C3%B1o-iv/" rel="nofollow noopener noreferrer" target="_blank">LinkedIn</a>.&nbsp;<span>Send me a secure&nbsp;<a href="https://www.forbes.com/tips/" rel="nofollow noopener noreferrer" target="_blank">tip</a></span>.&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The C++ Iceberg (111 pts)]]></title>
            <link>https://fouronnes.github.io/cppiceberg/</link>
            <guid>40294555</guid>
            <pubDate>Wed, 08 May 2024 05:12:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fouronnes.github.io/cppiceberg/">https://fouronnes.github.io/cppiceberg/</a>, See on <a href="https://news.ycombinator.com/item?id=40294555">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Why use ECC? (2015) (152 pts)]]></title>
            <link>https://danluu.com/why-ecc/</link>
            <guid>40293943</guid>
            <pubDate>Wed, 08 May 2024 03:02:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danluu.com/why-ecc/">https://danluu.com/why-ecc/</a>, See on <a href="https://news.ycombinator.com/item?id=40293943">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p>Jeff Atwood, perhaps the most widely read programming blogger, has a post that makes <a rel="nofollow" href="http://blog.codinghorror.com/to-ecc-or-not-to-ecc/">a case against using ECC memory</a>. My read is that his major points are:</p> <ol> <li>Google didn't use ECC when they built their servers in 1999</li> <li>Most RAM errors are hard errors and not soft errors</li> <li>RAM errors are rare because hardware has improved</li> <li>If ECC were actually important, it would be used everywhere and not just servers. Paying for optional stuff like this is "awfully enterprisey"</li> </ol>  <p>Let's take a look at these arguments one by one:</p> <h2 id="1-google-didn-t-use-ecc-in-1999">1. Google didn't use ECC in 1999</h2> <p>Not too long after Google put these non-ECC machines into production, they realized this was a serious error and not worth the cost savings. If you think cargo culting what Google does is a good idea because it's Google, here are some things you might do:</p> <h4 id="a-put-your-servers-into-shipping-containers">A. Put your servers into shipping containers.</h4> <p>Articles are still written today about what a great idea this is, even though this was an experiment at Google that was deemed unsuccessful. Turns out, even Google's experiments don't always succeed. In fact, their propensity for “moonshots” in the early days meannt that they had more failed experiments that most companies. Copying their failed experiments isn't a particularly good strategy.</p> <h4 id="b-cause-fires-in-your-own-datacenters">B. Cause fires in your own datacenters</h4> <p>Part of the post talks about how awesome these servers are:</p> <blockquote> <p>Some people might look at these early Google servers and see an amateurish fire hazard. Not me. I see a prescient understanding of how inexpensive commodity hardware would shape today's internet. I felt right at home when I saw this server; it's exactly what I would have done in the same circumstances</p> </blockquote> <p>The last part of that is true. But the first part has a grain of truth, too. When Google started designing their own boards, one generation had a regrowth<sup id="fnref:R"><a rel="footnote" href="#fn:R">1</a></sup> issue that caused a non-zero number of fires.</p> <p>BTW, if you click through to Jeff's post and look at the photo that the quote refers to, you'll see that the boards have a lot of flex in them. That caused problems and was fixed in the next generation. You can also observe that the cabling is quite messy, which also caused problems, and was also fixed in the next generation. There were other problems as well. <abbr title="When someone looks in the answer key and says, 'I would've come up with that', that's often plausible when their answer is perfect. But when they say that after seeing a specific imperfect answer, it's a bit less plausible that they'd reproduce the exact same mistakes">Jeff's argument here appears to be that, if he were there at the time, he would've seen the exact same opportunities that early Google enigneers did, and since Google did this, it must've been the right thing even if it doesn't look like it. But, a number of things that make it look like not the right thing actually made it not the right thing.</abbr></p> <h4 id="c-make-servers-that-injure-your-employees">C. Make servers that injure your employees</h4> <p>One generation of Google servers had infamously sharp edges, giving them the reputation of being made of “razor blades and hate”.</p> <h4 id="d-create-weather-in-your-datacenters">D. Create weather in your datacenters</h4> <p>From talking to folks at a lot of large tech companies, it seems that most of them have had a climate control issue resulting in clouds or fog in their datacenters. You might call this a clever plan by Google to reproduce Seattle weather so they can poach MS employees. Alternately, it might be a plan to create literal cloud computing. Or maybe not.</p> <p>Note that these are all things Google tried and then changed. Making mistakes and then fixing them is common in every successful engineering organization. If you're going to cargo cult an engineering practice, you should at least cargo cult current engineering practices, not <a href="https://danluu.com/butler-lampson-1999/">something that was done in 1999</a>.</p> <p>When Google used servers without ECC back in 1999, they found a number of symptoms that were ultimately due to memory corruption, including a search index that returned effectively random results to queries. The actual failure mode here is instructive. I often hear that it's ok to ignore ECC on these machines because it's ok to have errors in individual results. But even when you can tolerate occasional errors, ignoring errors means that you're exposing yourself to total corruption, unless you've done a very careful analysis to make sure that a single error can only contaminate a single result. In research that's been done on filesystems, it's been repeatedly shown that despite making valiant attempts at creating systems that are robust against a single error, it's extremely hard to do so and basically every heavily tested filesystem can have a massive failure from a single error (<a href="https://danluu.com/file-consistency/">see the output of Andrea and Remzi's research group at Wisconsin if you're curious about this</a>). I'm not knocking filesystem developers here. They're better at that kind of analysis than 99.9% of programmers. It's just that this problem has been repeatedly shown to be hard enough that humans cannot effectively reason about it, and automated tooling for this kind of analysis is still far from a push-button process. In their book on <a href="http://www.morganclaypool.com/doi/abs/10.2200/S00516ED2V01Y201306CAC024">warehouse scale computing</a>, Google discusses error correction and detection and ECC is cited as their slam dunk case for when it's obvious that you should use hardware error correction<sup id="fnref:P"><a rel="footnote" href="#fn:P">2</a></sup>.</p> <p>Google has great infrastructure. From what I've heard of the infra at other large tech companies, Google's sounds like the best in the world. But that doesn't mean that you should copy everything they do. Even if you look at their good ideas, it doesn't make sense for most companies to copy them. They <a href="https://danluu.com/intel-cat/">created a replacement for Linux's work stealing scheduler that uses both hardware run-time information and static traces to allow them to take advantage of new hardware in Intel's server processors that lets you dynamically partition caches between cores</a>. If used across their entire fleet, that could easily save Google more money in a week than stackexchange has spent on machines in their entire history. Does that mean you should copy Google? No, not unless you've already captured all the lower hanging fruit, which includes things like making sure that your core infrastructure is written in highly optimized C++, not Java or (god forbid) Ruby. And the thing is, for the vast majority of companies, writing in a language that imposes a 20x performance penalty is a totally reasonable decision.</p> <h2 id="2-most-ram-errors-are-hard-errors">2. Most RAM errors are hard errors</h2> <p>The case against ECC quotes <a href="http://selse.org//images/selse_2012/Papers/selse2012_submission_4.pdf">this section of a study on DRAM errors</a> (the bolding is Jeff's):</p> <blockquote> <p>Our study has several main findings. First, we find that approximately <strong>70% of DRAM faults are recurring (e.g., permanent) faults, while only 30% are transient faults.</strong> Second, we find that large multi-bit faults, such as faults that affects an entire row, column, or bank, constitute over 40% of all DRAM faults. Third, we find that almost 5% of DRAM failures affect board-level circuitry such as data (DQ) or strobe (DQS) wires. Finally, we find that chipkill functionality reduced the system failure rate from DRAM faults by 36x.</p> </blockquote> <p>This seems to betray a lack of understanding of the implications of this study, as this quote doesn't sound like an argument against ECC; it sounds like an argument for "chipkill", a particular class of ECC. Putting that aside, Jeff's post points out that hard errors are twice as common as soft errors, and then mentions that they run memtest on their machines when they get them. First, a 2:1 ratio isn't so large that you can just ignore soft errors. Second the post implies that Jeff believes that hard errors are basically immutable and can't surface after some time, which is incorrect. You can think of electronics as wearing out just the same way mechanical devices wear out. The mechanisms are different, but the effects are similar. In fact, if you compare reliability analysis of chips vs. other kinds of reliability analysis, you'll find they often use the same families of distributions to model failures. And, if hard errors were immutable, they would generally get caught in testing by the manufacturer, who can catch errors much more easily than consumers can because they have hooks into circuits that let them test memory much more efficiently than you can do in your server or home computer. Third, Jeff's line of reasoning implies that ECC can't help with detection or correction of hard errors, which is not only incorrect but directly contradicted by the quote.</p> <p>So, how often are you going to run memtest on your machines to try to catch these hard errors, and how much data corruption are you willing to live with? One of the key uses of ECC is not to correct errors, but to signal errors so that hardware can be replaced before silent corruption occurs. No one's going to consent to shutting down everything on a machine every day to run memtest (that would be more expensive than just buying ECC memory), and even if you could convince people to do that, it won't catch as many errors as ECC will.</p> <p>When I worked at a company that owned about 1000 machines, we noticed that we were getting strange consistency check failures, and after maybe half a year we realized that the failures were more likely to happen on some machines than others. The failures were quite rare, maybe a couple times a week on average, so it took a substantial amount of time to accumulate the data, and more time for someone to realize what was going on. Without knowing the cause, analyzing the logs to figure out that the errors were caused by single bit flips (with high probability) was also non-trivial. We were lucky that, as a side effect of the process we used, the checksums were calculated in a separate process, on a different machine, at a different time, so that an error couldn't corrupt the result and propagate that corruption into the checksum. If you merely try to protect yourself with in-memory checksums, there's a good chance you'll perform a checksum operation on the already corrupted data and compute a valid checksum of bad data unless you're doing some really fancy stuff with calculations that carry their own checksums (and if you're that serious about error correction, you're probably using ECC regardless). Anyway, after completing the analysis, we found that memtest couldn't detect any problems, but that replacing the RAM on the bad machines caused a one to two order of magnitude reduction in error rate. Most services don't have this kind of checksumming we had; those services will simply silently write corrupt data to persistent storage and never notice problems until a customer complains.</p> <h2 id="3-due-to-advances-in-hardware-manufacturing-errors-are-very-rare">3. Due to advances in hardware manufacturing, errors are very rare</h2> <p>Jeff says</p> <blockquote> <p>I do seriously question whether ECC is as operationally critical as we have been led to believe [for servers], and I think the data shows modern, non-ECC RAM is already extremely reliable ... Modern commodity computer parts from reputable vendors are amazingly reliable. And their trends show from 2012 onward essential PC parts have gotten more reliable, not less. (I can also vouch for the improvement in SSD reliability as we have had zero server SSD failures in 3 years across our 12 servers with 24+ drives ...</p> </blockquote> <p>and quotes a study.</p> <p>The data in the post isn't sufficient to support this assertion. Note that since RAM usage has been increasing and continues to increase at a fast exponential rate, RAM failures would have to decrease at a greater exponential rate to actually reduce the incidence of data corruption. Furthermore, as chips continue shrink, features get smaller, making the kind of wearout issues discussed in “2” more common. For example, at 20nm, a DRAM capacitor might hold something like 50 electrons, and that number will get smaller for next generation DRAM and things continue to shrink.</p> <p>The <a href="http://selse.org//images/selse_2012/Papers/selse2012_submission_4.pdf">2012 study that Atwood quoted</a> has this graph on corrected errors (a subset of all errors) on ten randomly selected failing nodes (6% of nodes had at least one failure):</p> <p><img src="https://danluu.com/images/why-ecc/one_month_ecc_errors.png"></p> <p>We're talking between 10 and 10k errors for a typical node that has a failure, and that's a cherry-picked study from a post that's arguing that you don't need ECC. Note that the nodes here only have 16GB of RAM, which is an order of magnitude less than modern servers often have, and that this was on an older process node that was less vulnerable to noise than we are now. For anyone who's used to dealing with reliability issues and just wants to know the FIT rate, the study finds a FIT rate of between 0.057 and 0.071 faults per Mbit (which, contra Atwood's assertion, is not a shockingly low number). If you take the most optimistic FIT rate, .057, and do the calculation for a server without much RAM (here, I'm using 128GB, since the servers I see nowadays typically have between 128GB and 1.5TB of RAM)., you get an expected value of .057 * 1000 * 1000 * 8760 / 1000000000 = .5 faults per year per server. Note that this is for faults, not errors. From the graph above, we can see that a fault can easily cause hundreds or thousands of errors per month. Another thing to note is that there are multiple nodes that don't have errors at the start of the study but develop errors later on. So, in fact, the cherry-picked study that Jeff links contradicts Jeff's claim about reliability.</p> <p>Sun/Oracle famously ran into this a number of decades ago. Transistors and DRAM capacitors were getting smaller, much as they are now, and memory usage and caches were growing, much as they are now. Between having smaller transistors that were less resilient to transient upset as well as more difficult to manufacture, and having more on-chip cache, the vast majority of server vendors decided to add ECC to their caches. Sun decided to save a few dollars and skip the ECC. The direct result was that a number of Sun customers reported sporadic data corruption. It took Sun multiple years to spin a new architecture with ECC cache, and Sun made customers sign an NDA to get replacement chips. Of course there's no way to cover up this sort of thing forever, and when it came up, Sun's reputation for producing reliable servers took a permanent hit, much like the time they tried to <a href="https://danluu.com/anon-benchmark/">cover up poor performance results by introducing a clause into their terms of services disallowing benchmarking</a>.</p> <p>Another thing to note here is that when you're paying for ECC, you're not just paying for ECC, you're paying for parts (CPUs, boards) that have been qual'd more thoroughly. You can easily see this with disk failure rates, and I've seen many people observe this in their own private datasets. In terms of public data, I believe Andrea and Remzi's group had a SIGMETRICS paper a few years back that showed that SATA drives were 4x more likely than SCSI drives to have disk read failures, and 10x more likely to have silent data corruption. This relationship held true even with drives from the same manufacturer. There's no particular reason to think that the SCSI interface should be more reliable than the SATA interface, but it's not about the interface. It's about buying a high-reliability server part vs. a consumer part. Maybe you don't care about disk reliability in particular because you checksum everything and can easily detect disk corruption, but there are some kinds of corruption that are harder to detect.</p> <p>[2024 update, almost a decade later]: looking at this retrospectively, we can see that Jeff's assertion that commodity parts are reliable, "modern commodity computer parts from reputable vendors are amazingly reliable" is still not true. Looking at real-world user data from Firefox, <a href="https://fosstodon.org/@gabrielesvelto/112401643131904845">Gabriele Svelto estimated that approximately 10% to 20% of all Firefox crashes were due to memory corruption</a>. Various game companies that track this kind of thing also report a significant fraction of user crashes appear to be due to data corruption, although I don't have an estimate from any of those companies handy. A more direct argument is that if you talk to folks at big companies that run a lot of ECC memory and look at the rate of ECC errors, there are quite a few errors detected by ECC memory despite ECC memory typically having a lower error rate than random non-ECC memory. This kind of argument is frequently made (here, it was detailed above a decade ago, and when I looked at this when I worked at Twitter fairly recently and there has not been a revolution in memory technology that has reduced the need for ECC over the rates discussed in papers a decade ago), but it often doesn't resontate with folks who say things like "well, those bits probably didn't matter anyway", "most memory ends up not getting read", etc. Looking at real-world crashes and noting that the amount of silent data corruption should be expected to be much higher than the rate of crashes seems to resonate with people who aren't excited by looking at raw FIT rates in datacenters.</p> <h2 id="4-if-ecc-were-actually-important-it-would-be-used-everywhere-and-not-just-servers">4. If ECC were actually important, it would be used everywhere and not just servers.</h2> <p><a href="https://danluu.com/cocktail-ideas/">One way to rephrase this is as a kind of cocktail party efficient markets hypothesis. This can't be important, because if it was, we would have it</a>. Of course this is incorrect and there are many things that would be beneficial to consumers that we don't have, such as <a href="https://danluu.com/car-safety/">cars that are designed to safe instead of just getting the maximum score in crash tests</a>. Looking at this with respect to the server and consumer markets, this argument can be rephrased as “If this feature were actually important for servers, it would be used in non-servers”, which is incorrect. A primary driver of what's available in servers vs. non-servers is what can be added that buyers of servers will pay a lot for, to allow for price discrimination between server and non-server parts. This is actually one of the more obnoxious problems facing large cloud vendors — hardware vendors are able to jack up the price on parts that have server features because the features are much more valuable in server applications than in desktop applications. Most home users don't mind, giving hardware vendors a mechanism to extract more money out of people who buy servers while still providing cheap parts for consumers.</p> <p>Cloud vendors often have enough negotiating leverage to get parts at cost, but that only works where there's more than one viable vendor. Some of the few areas where there aren't any viable competitors include CPUs and GPUs. There have been a number of attempts by CPU vendors to get into the server market, but each attempt so far has been fatally flawed in a way that made it obvious from an early stage that the attempt was doomed (and these are often 5 year projects, so that's a lot of time to spend on a doomed project). The Qualcomm effort has been getting a lot of hype, but when I talk to folks I know at Qualcomm they all tell me that the current chip is basically for practice, since Qualcomm needed to learn how to build a server chip from all the folks they poached from IBM, and that the next chip is the first chip that has any hope of being competitive. I have high hopes for Qualcomm as well an ARM effort to build good server parts, but those efforts are still a ways away from bearing fruit.</p> <p>The near total unsuitability of current ARM (and POWER) options (not including hypothetical variants of Apple's impressive ARM chip) for most server workloads in terms of performance per TCO dollar is a bit of a tangent, so I'll leave that for another post, but the point is that Intel has the market power to make people pay extra for server features, and they do so. Additionally, some features are genuinely more important for servers than for mobile devices with a few GB of RAM and a power budget of a few watts that are expected to randomly crash and reboot periodically anyway.</p> <h2 id="conclusion">Conclusion</h2> <p>Should you buy ECC RAM? That depends. For servers, it's probably a good bet considering the cost, although it's hard to really do a cost/benefit analysis because it's really hard to figure out the cost of silent data corruption, or the cost of having some risk of burning half a year of developer time tracking down intermittent failures only to find that the were caused by using non-ECC memory.</p> <p>For normal desktop use, I'm pro-ECC, but if you don't have <a href="https://www.reddit.com/r/programming/comments/adoux/coding_horror_and_blogsstackoverflowcom/">regular backups</a> set up, doing backups probably has a better ROI than ECC. But once you have the absolute basics set up, there's a fairly strong case for ECC for consumer machines. For example, if you have backups without ECC, you can easily write corrupt data into your primary store and replicate that corrupt data into backup. But speaking more generally, big companies running datacenters are probably better set up to detect data corruption and more likely to have error correction at higher levels that allow them to recover from data corruption than consumers, so the case for consumers is arguably stronger than it is for servers, where the case is strong enough that's generally considered a no brainer. A major reason consumers don't generally use ECC isn't that it isn't worth it for them, it's that they just have no idea how to attribute crashes and data corruption when they happen. Once you start doing this, as Google and other large companies do, it's immediately obvious that ECC is worth the cost even when you have multiple levels of error correction operating at higher levels.</p> <h3 id="appendix-security">Appendix: security</h3> <p>If you allow any sort of code execution, even sandboxed execution, there are attacks <a href="https://en.wikipedia.org/wiki/Row_hammer">like rowhammer</a> which can allow users to cause data corruption and there have been instances where this has allowed for privilege escalation. ECC doesn't completely mitigate the attack, but it makes it much harder.</p> <p><small> Thanks to Prabhakar Ragde, Tom Murphy, Jay Weisskopf, Leah Hanson, Joe Wilder, and Ralph Corderoy for discussion/comments/corrections. Also, thanks (or maybe anti-thanks) to Leah for convincing me that I should write up this off the cuff verbal comment as a blog post. Apologies for any errors, the lack of references, and the stilted prose; this is basically a transcription of half of a conversation and I haven't explained terms, provided references, or checked facts in the level of detail that I normally do. </small></p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Revokes Intel, Qualcomm Licenses to Sell Chips to Huawei (114 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-05-07/us-revokes-intel-qualcomm-licenses-to-sell-chips-to-huawei</link>
            <guid>40293614</guid>
            <pubDate>Wed, 08 May 2024 01:55:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-05-07/us-revokes-intel-qualcomm-licenses-to-sell-chips-to-huawei">https://www.bloomberg.com/news/articles/2024-05-07/us-revokes-intel-qualcomm-licenses-to-sell-chips-to-huawei</a>, See on <a href="https://news.ycombinator.com/item?id=40293614">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Decker: A fantastic reincarnation of HyperCard with 1-bit graphics (267 pts)]]></title>
            <link>https://www.beyondloom.com/decker/index.html</link>
            <guid>40292181</guid>
            <pubDate>Tue, 07 May 2024 22:15:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.beyondloom.com/decker/index.html">https://www.beyondloom.com/decker/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=40292181">Hacker News</a></p>
<div id="readability-page-1" class="page"><h2>Decker</h2>

<p>Decker is a multimedia platform for creating and sharing interactive documents, with sound, images, hypertext, and scripted behavior. You can try it in your web browser <b><a href="https://www.beyondloom.com/decker/tour.html">right now</a></b>.</p>

<center>
	<a href="https://www.beyondloom.com/decker/tour.html"><img src="https://www.beyondloom.com/decker/images/wings.gif"></a>
</center>

<p>Decker builds on the legacy of <a href="https://en.wikipedia.org/wiki/HyperCard">HyperCard</a> and the visual aesthetic of classic MacOS. It retains the simplicity and ease of learning that HyperCard provided, while adding many subtle and overt quality-of-life improvements, like deep undo history, support for scroll wheels and touchscreens, more modern keyboard navigation, and bulk editing operations.</p>

<p>Anyone can use Decker to create E-Zines, organize their notes, give presentations, build adventure games, or even just doodle some 1-bit pixel art. The holistic "ditherpunk" aesthetic is cozy, a bit nostalgic, and provides fun and distinctive creative constraints. As a prototyping tool, Decker encourages embracing a sketchy, imperfect approach. Finished decks can be saved as standalone <fixed>.html</fixed> documents which self-execute in a web browser and can be shared anywhere you can host or embed a web page. Decker also runs natively on MacOS, Windows, and Linux.</p>

<p>For more complex projects, Decker features a novel scripting language named <i>Lil</i> which is strongly influenced by both <a href="http://www.lua.org/">Lua</a>, an imperative language popular for embedding in tools and game engines, and <a href="https://en.wikipedia.org/wiki/Q_%28programming_language_from_Kx_Systems%29">Q</a>, a functional language in the APL family used with time-series databases. Lil is easy to learn and conventional enough not to ruffle any feathers for users with prior programming experience, but also includes pleasant surprises like implicit scalar-vector arithmetic and an integrated SQL-like query language. A few lines of Lil can go a long way.</p>

<center>
	<img src="https://www.beyondloom.com/decker/images/calc.gif">
</center>

<p>Decker provides a small collection of built-in interactive widgets for building interfaces, as well as a facility for <a href="https://www.beyondloom.com/decker/decker.html#customwidgets">defining new ones</a>. Custom widgets and their definitions can be copied and pasted using the system clipboard, which also makes it possible to share them anywhere you can share or store text. Every deck is a toolkit of reusable parts that can be harvested and repurposed for another project.</p>

<center>
	<img src="https://www.beyondloom.com/decker/images/contrap.gif">
</center>

<p>Decker is command-line friendly: when built from source, it comes with <a href="https://www.beyondloom.com/decker/lilt.html">Lilt</a>, a standalone Lil interpreter which can (among other things) read, write, manipulate, and even execute Decker documents "headlessly". Lilt has even fewer dependencies than Decker itself, so it can also be compiled as a cross-platform <a href="https://justine.lol/ape.html">APE executable</a>, ready for writing run-anywhere shell scripts. Would you believe there's a Lil interpreter <a href="https://www.beyondloom.com/blog/lila.html">that runs on POSIX AWK</a>? Decks are stored in a line-oriented text format which interoperates well with existing source control tools like Git and SVN.</p>

<p>Decker includes no advertising, telemetry, gamification, or other intrusions on user privacy and autonomy. If you like Decker, please share it with other people who might enjoy it. Build something that makes you happy.</p>

<h2>Examples</h2>
<ul>
	<li><a href="https://www.beyondloom.com/decker/tour.html">Decker: A Guided Tour</a></li>
	<li><a href="https://www.beyondloom.com/decker/guis.html">5GUIs</a></li>
	<li><a href="https://www.beyondloom.com/decker/chip8.html">A CHIP-8 Interpreter</a></li>
	<li><a href="https://www.beyondloom.com/decker/draggable.html">All About Draggable</a></li>
	<li><a href="https://www.beyondloom.com/decker/sound.html">All About Sound</a></li>
	<li><a href="https://www.beyondloom.com/decker/goofs/sokoban.html">Sokoban: A Block-Pushing Puzzle Game</a></li>
</ul>

<h2>Modules</h2>
<ul>
	<li><a href="https://www.beyondloom.com/decker/plot.html">Plot: Simple Graphs for Decker</a></li>
	<li><a href="https://www.beyondloom.com/decker/zazz.html">Zazz: Animation Helpers for Decker</a></li>
	<li><a href="https://www.beyondloom.com/decker/ease.html">Ease: Easing Functions for Decker</a></li>
	<li><a href="https://www.beyondloom.com/decker/dialog.html">Dialogizer: Visual-Novel Modals for Decker</a></li>
	<li><a href="https://www.beyondloom.com/decker/puppeteer.html">Puppeteer: Visual-Novel Sprite Animation for Decker</a></li>
</ul>

<h2>Documentation</h2>
<ul>
	<li><a href="https://www.beyondloom.com/decker/decker.html">The Decker reference manual</a></li>
	<li><a href="https://www.beyondloom.com/decker/format.html">The Decker document format</a></li>
	<li><a href="https://www.beyondloom.com/decker/lil.html">The Lil programming language</a></li>
	<li><a href="https://www.beyondloom.com/decker/learnlil.html">Learn Lil in 10 Minutes</a></li>
	<li><a href="https://www.beyondloom.com/tools/trylil.html">The Lil playground</a></li>
	<li><a href="https://www.beyondloom.com/decker/lilquickref.html">Lil Quick-reference card</a></li>
	<li><a href="https://www.beyondloom.com/decker/lilt.html">Lilt: the Lil Terminal</a></li>
	<li><a href="https://www.beyondloom.com/blog/responses.html">Decker: Responding to Responses</a></li>
</ul>

<h2>Additional Resources</h2>

<p>Browsable source code and a bug-tracker are available on <a href="https://github.com/JohnEarnest/Decker">GitHub</a>. Decker is free and open-source, under a permissive <a href="https://mit-license.org/">MIT license</a>.</p>

<p>Periodic binary releases for MacOS and Windows are available on <a href="https://internet-janitor.itch.io/decker">Itch.io</a>. The Itch page includes a <a href="https://internet-janitor.itch.io/decker/community">community forum</a> for discussing Decker and sharing projects made with Decker.</p>

<a href="https://www.beyondloom.com/index.html">back</a>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[IBM Granite: A Family of Open Foundation Models for Code Intelligence (219 pts)]]></title>
            <link>https://github.com/ibm-granite/granite-code-models</link>
            <guid>40291598</guid>
            <pubDate>Tue, 07 May 2024 21:16:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ibm-granite/granite-code-models">https://github.com/ibm-granite/granite-code-models</a>, See on <a href="https://news.ycombinator.com/item?id=40291598">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/ibm-granite/granite-code-models/blob/main/figures/granite-code-models-3x-v4.png"><img src="https://github.com/ibm-granite/granite-code-models/raw/main/figures/granite-code-models-3x-v4.png"></a>
</p>
<p dir="auto">
  📚 <a href="https://github.com/ibm-granite/granite-code-models/blob/main/paper.pdf">Paper</a>&nbsp; | 🤗 <a href="https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330" rel="nofollow">HugginFace Collection</a>&nbsp; | 
  💬 <a href="https://github.com/orgs/ibm-granite/discussions">Discussions Page</a>&nbsp; | 📰 <a href="http://" rel="nofollow">Blog (coming soon)</a>&nbsp;
<br>
</p><hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction to Granite Code Models</h2><a id="user-content-introduction-to-granite-code-models" aria-label="Permalink: Introduction to Granite Code Models" href="#introduction-to-granite-code-models"></a></p>
<p dir="auto">We introduce the Granite series of decoder-only code models for code generative tasks (e.g., fixing bugs, explaining code, documenting code), trained with code written in 116 programming languages. A comprehensive evaluation of the Granite Code model family on diverse tasks demonstrates that our models consistently reach state-of-the-art performance among available open-source code LLMs.&nbsp;</p>
<p dir="auto">The key advantages of Granite Code models include:</p>
<ul dir="auto">
<li>All-rounder Code LLM: Granite Code models achieve competitive or state-of-the-art performance on different kinds of code-related tasks, including code generation, explanation, fixing, editing, translation, and more. Demonstrating their ability to solve diverse coding tasks.</li>
<li>Trustworthy Enterprise-Grade LLM: All our models are trained on license-permissible data collected following <a href="https://www.ibm.com/impact/ai-ethics" rel="nofollow">IBM's AI Ethics principles</a> and guided by IBM’s Corporate Legal team for trustworthy enterprise usage. We release all our Granite Code models under an <a href="https://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">Apache 2.0 license</a> license for research and commercial use.</li>
</ul>
<p dir="auto">The family of <strong>Granite Code Models</strong> comes in two main variants:</p>
<ul dir="auto">
<li>Granite Code Base Models: base foundational models designed for code-related tasks (e.g., code repair, code explanation, code synthesis).</li>
<li>Granite Code Instruct Models: instruction following models finetuned using a combination of Git commits paired with human instructions and open-source synthetically generated code instruction datasets.</li>
</ul>
<p dir="auto">Both base and instruct models are available in sizes of 3B, 8B, 20B, and 34B parameters.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Data Collection</h2><a id="user-content-data-collection" aria-label="Permalink: Data Collection" href="#data-collection"></a></p>
<p dir="auto">Our process to prepare code pretraining data involves several stages. First, we collect a combination of publicly available datasets (e.g., GitHub Code Clean, Starcoder data), public code repositories, and issues from GitHub. Second, we filter the code data collected based on the programming language in which data is written (which we determined based on file extension). Then, we also filter out data with low code quality. Third, we adopt an aggressive deduplication strategy that includes both exact and fuzzy deduplication to remove documents having (near) identical code content. Finally, we apply a HAP content filter that reduces models' likelihood of generating hateful, abusive, or profane language. We also make sure to redact Personally Identifiable Information (PII) by replacing PII content (e.g., names, email addresses, keys, passwords) with corresponding tokens (e.g., ⟨NAME⟩, ⟨EMAIL⟩, ⟨KEY⟩, ⟨PASSWORD⟩). We also scan all datasets using ClamAV to identify and remove instances of malware in the source code. In addition to collecting code data for model training, we curate several publicly available high-quality natural language datasets for improving the model’s proficiency in language understanding and mathematical reasoning.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Pretraining</h2><a id="user-content-pretraining" aria-label="Permalink: Pretraining" href="#pretraining"></a></p>
<p dir="auto">The <strong>Granite Code Base</strong> models are trained on 3-4T tokens of code data and natural language datasets related to code. Data is tokenized via byte pair encoding (BPE), employing the same tokenizer as StarCoder. We utilize high-quality data with two phases of training as follows:</p>
<ul dir="auto">
<li>Phase 1 (code only training): During phase 1, 3B and 8B models are trained for 4 trillion tokens of code data comprising 116 languages. The 20B parameter model is trained on 3 trillion tokens of code. The 34B model is trained on 1.4T tokens after the depth upscaling which is done on the 1.6T checkpoint of 20B model.</li>
<li>Phase 2 (code + language training): In phase 2, we include additional high-quality publicly available data from various domains, including technical, mathematics, and web documents, to further improve the model’s performance. We train all our models for 500B tokens (80% code-20% language mixture) in phase 2 training.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Instruction Tuning</h2><a id="user-content-instruction-tuning" aria-label="Permalink: Instruction Tuning" href="#instruction-tuning"></a></p>
<p dir="auto">Granite Code Instruct models are finetuned on the following types of instruction data: 1) code commits sourced from <a href="https://huggingface.co/datasets/bigcode/commitpackft" rel="nofollow">CommitPackFT</a>, 2) high-quality math datasets, specifically we used <a href="https://huggingface.co/datasets/TIGER-Lab/MathInstruct" rel="nofollow">MathInstruct</a> and <a href="https://huggingface.co/datasets/meta-math/MetaMathQA" rel="nofollow">MetaMathQA</a>, 3) Code instruction datasets such as <a href="https://huggingface.co/datasets/glaiveai/glaive-code-assistant-v3" rel="nofollow">Glaive-Code-Assistant-v3</a>, <a href="https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k" rel="nofollow">Self-OSS-Instruct-SC2</a>, <a href="https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2" rel="nofollow">Glaive-Function-Calling-v2</a>, <a href="https://huggingface.co/datasets/bugdaryan/sql-create-context-instruction" rel="nofollow">NL2SQL11</a> and a small collection of synthetic API calling datasets, and 4) high-quality language instruction datasets such as <a href="https://huggingface.co/datasets/nvidia/HelpSteer" rel="nofollow">HelpSteer</a> and an open license-filtered version of <a href="https://huggingface.co/datasets/garage-bAInd/Open-Platypus" rel="nofollow">Platypus</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Evaluation Results</h2><a id="user-content-evaluation-results" aria-label="Permalink: Evaluation Results" href="#evaluation-results"></a></p>
<p dir="auto">We conduct an extensive evaluation of our code models on a comprehensive list of benchmarks that includes but is not limited to HumanEvalPack, MBPP, and MBPP+. This set of benchmarks encompasses different coding tasks across commonly used programming languages (e.g., Python, JavaScript, Java, Go, C++, Rust).</p>
<p dir="auto">Our findings reveal that Granite Code models outperform strong open-source models across model sizes. The figure below illustrates how <code>Granite-8B-Code-Base</code> outperforms <code>Mistral-7B</code>, <code>LLama-3-8B</code>, and other open-source models in three coding tasks. We provide further evaluation results in our paper.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ibm-granite/granite-code-models/blob/main/figures/GraniteCodeFigure1.jpg"><img src="https://github.com/ibm-granite/granite-code-models/raw/main/figures/GraniteCodeFigure1.jpg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to Use our Models?</h2><a id="user-content-how-to-use-our-models" aria-label="Permalink: How to Use our Models?" href="#how-to-use-our-models"></a></p>
<p dir="auto">To use any of our models, pick an appropriate <code>model_path</code> from:</p>
<ol dir="auto">
<li><code>ibm-granite/granite-3b-code-base</code></li>
<li><code>ibm-granite/granite-3b-code-instruct</code></li>
<li><code>ibm-granite/granite-8b-code-base</code></li>
<li><code>ibm-granite/granite-8b-code-instruct</code></li>
<li><code>ibm-granite/granite-20b-code-base</code></li>
<li><code>ibm-granite/granite-20b-code-instruct</code></li>
<li><code>ibm-granite/granite-34b-code-base</code></li>
<li><code>ibm-granite/granite-34b-code-instruct</code></li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Inference</h3><a id="user-content-inference" aria-label="Permalink: Inference" href="#inference"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import AutoModelForCausalLM, AutoTokenizer

device = &quot;cuda&quot; # or &quot;cpu&quot;
model_path = &quot;ibm-granite/granite-3b-code-base&quot; # pick anyone from above list

tokenizer = AutoTokenizer.from_pretrained(model_path)

# drop device_map if running on CPU
model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)
model.eval()

# change input text as desired
input_text = &quot;def generate():&quot;
# tokenize the text
input_tokens = tokenizer(input_text, return_tensors=&quot;pt&quot;)

# transfer tokenized inputs to the device
for i in input_tokens:
    input_tokens[i] = input_tokens[i].to(device)

# generate output tokens
output = model.generate(**input_tokens)
# decode output tokens into text
output = tokenizer.batch_decode(output)

# loop over the batch to print, in this example the batch size is 1
for i in output:
    print(i)"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>AutoModelForCausalLM</span>, <span>AutoTokenizer</span>

<span>device</span> <span>=</span> <span>"cuda"</span> <span># or "cpu"</span>
<span>model_path</span> <span>=</span> <span>"ibm-granite/granite-3b-code-base"</span> <span># pick anyone from above list</span>

<span>tokenizer</span> <span>=</span> <span>AutoTokenizer</span>.<span>from_pretrained</span>(<span>model_path</span>)

<span># drop device_map if running on CPU</span>
<span>model</span> <span>=</span> <span>AutoModelForCausalLM</span>.<span>from_pretrained</span>(<span>model_path</span>, <span>device_map</span><span>=</span><span>device</span>)
<span>model</span>.<span>eval</span>()

<span># change input text as desired</span>
<span>input_text</span> <span>=</span> <span>"def generate():"</span>
<span># tokenize the text</span>
<span>input_tokens</span> <span>=</span> <span>tokenizer</span>(<span>input_text</span>, <span>return_tensors</span><span>=</span><span>"pt"</span>)

<span># transfer tokenized inputs to the device</span>
<span>for</span> <span>i</span> <span>in</span> <span>input_tokens</span>:
    <span>input_tokens</span>[<span>i</span>] <span>=</span> <span>input_tokens</span>[<span>i</span>].<span>to</span>(<span>device</span>)

<span># generate output tokens</span>
<span>output</span> <span>=</span> <span>model</span>.<span>generate</span>(<span>**</span><span>input_tokens</span>)
<span># decode output tokens into text</span>
<span>output</span> <span>=</span> <span>tokenizer</span>.<span>batch_decode</span>(<span>output</span>)

<span># loop over the batch to print, in this example the batch size is 1</span>
<span>for</span> <span>i</span> <span>in</span> <span>output</span>:
    <span>print</span>(<span>i</span>)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Finetuning</h3><a id="user-content-finetuning" aria-label="Permalink: Finetuning" href="#finetuning"></a></p>
<p dir="auto">Codebase coming soon.</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Model Cards</h2><a id="user-content-model-cards" aria-label="Permalink: Model Cards" href="#model-cards"></a></p>
<p dir="auto">The model cards for each model variant are available in their respective HuggingFace repository. Please visit our collection <a href="https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330" rel="nofollow">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to Download our Models?</h2><a id="user-content-how-to-download-our-models" aria-label="Permalink: How to Download our Models?" href="#how-to-download-our-models"></a></p>
<p dir="auto">The model of choice (granite-3b-code-base in this example) can be cloned using:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://huggingface.co/ibm-granite/granite-3b-code-base"><pre>git clone https://huggingface.co/ibm-granite/granite-3b-code-base</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">All Granite Code Models are distributed under <a href="https://github.com/ibm-granite/granite-code-models/blob/main/LICENSE">Apache 2.0</a> license.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Would you like to provide feedback?</h2><a id="user-content-would-you-like-to-provide-feedback" aria-label="Permalink: Would you like to provide feedback?" href="#would-you-like-to-provide-feedback"></a></p>
<p dir="auto">Please let us know your comments about our family of code models by visiting our <a href="https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330" rel="nofollow">collection</a>. Select the repository of the model you would like to provide feedback about. Then, go to <em>Community</em> tab, and click on <em>New discussion</em>. Alternatively, you can also post any questions/comments on our <a href="https://github.com/orgs/ibm-granite/discussions">github discussions page</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ScrapeGraphAI: Web scraping using LLM and direct graph logic (176 pts)]]></title>
            <link>https://scrapegraph-doc.onrender.com/</link>
            <guid>40290596</guid>
            <pubDate>Tue, 07 May 2024 19:41:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scrapegraph-doc.onrender.com/">https://scrapegraph-doc.onrender.com/</a>, See on <a href="https://news.ycombinator.com/item?id=40290596">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><p><a href="#__docusaurus_skipToContent_fallback">Skip to main content</a></p></div><nav aria-label="Main"><div><div><a href="https://scrapegraph-doc.onrender.com/"><p><img src="https://scrapegraph-doc.onrender.com/img/logo.svg" alt="My Site Logo"><img src="https://scrapegraph-doc.onrender.com/img/logo.svg" alt="My Site Logo"></p><b>Scrapegraph-ai</b></a><p><a href="https://scrapegraph-doc.onrender.com/docs/category/nodes">Documentation</a><a href="https://scrapegraph-doc.onrender.com/docs/activation">Installation of the keys</a></p></div><div><a href="https://github.com/VinciGit00/Scrapegraph-ai" target="_blank" rel="noopener noreferrer">GitHub</a></div></div></nav><div id="__docusaurus_skipToContent_fallback"><header><div><h2>You only scrape once</h2></div></header><main><div><div><h3>Easy to Use</h3><p>Scrapegraph-ai is an open source library for scraping with the use of AI. You just need to activate the API keys and you can scrape thousands of web pages in seconds!</p></div><div><h3>Easy and fast to implement</h3><p>You just have to implment just some lines of code and the work is done</p></div><div><h3>Focus on What Matters</h3><p>With this library you will be able to save hours of time because you have just to setupp the project and the AI will do everything for you</p></div></div></main></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Grateful Dead's Wall of Sound (253 pts)]]></title>
            <link>https://audioacademy.in/the-grateful-deads-wall-of-sound/</link>
            <guid>40289323</guid>
            <pubDate>Tue, 07 May 2024 18:06:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://audioacademy.in/the-grateful-deads-wall-of-sound/">https://audioacademy.in/the-grateful-deads-wall-of-sound/</a>, See on <a href="https://news.ycombinator.com/item?id=40289323">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p>A vision during an LSD trip is what inspired Owsley “Bear” Stanley, the Grateful Dead’s sound engineer’s mammoth feat of technical engineering, “The Wall of Sound”, irreversibly changing live sound and engineering for the better.</p>
<p>It was a time when live sound problems plagued engineers, bands, and audiences equally. While rock concerts grew in size and scope throughout the 60s, audiences grew larger and louder, without the technical sophistication of amplification ever changing to meet this scenario. Screaming fans meant that low-wattage guitar amps could hardly be heard and without the help of monitoring systems, bands could barely hear themselves play. Things were so bad that the Beatles quit touring in 1966 because they couldn’t hear themselves over the audience. It was after this era that the band, the Grateful Dead, became obsessed with their sound, largely thanks to their eccentric and dedicated sound engineer. Though incredibly frustrated with the noisy, feedback-laden, underpowered situation, they did not want to give up playing live, and the Dead had Owsley on board to help solve the sound situation.</p>
<h4>Who was Bear Owsley?</h4>
<div id="attachment_2627"><p><img fetchpriority="high" decoding="async" aria-describedby="caption-attachment-2627" src="https://audioacademy.in/wp-content/uploads/2019/02/owsley1-300x300.jpg" alt="Bear Owsley Stanley" width="300" height="300" srcset="https://audioacademy.in/wp-content/uploads/2019/02/owsley1-300x300.jpg 300w, https://audioacademy.in/wp-content/uploads/2019/02/owsley1-150x150.jpg 150w, https://audioacademy.in/wp-content/uploads/2019/02/owsley1-768x768.jpg 768w, https://audioacademy.in/wp-content/uploads/2019/02/owsley1-350x350.jpg 350w, https://audioacademy.in/wp-content/uploads/2019/02/owsley1.jpg 1024w" sizes="(max-width: 300px) 100vw, 300px"></p><p id="caption-attachment-2627">Owsley Stanley, left, with Jerry Garcia of the Grateful Dead</p></div>
<p>The famous story goes that in 1974 the Grateful Dead drummer Mickey Hart walked on stage to find Owsley “Bear Stanley standing in front of a wall of over 600 speakers with tears streaming down his face. Whispering to the huge mass of equipment, Bear said, “I love you and you love me- how could you fail me?” This story sums up Owsley’s obsession with sound, both as a concept and as a physical thing. A former ballet dancer and craftsman from Kentucky, Owsley was also jailed twice for manufacturing and distributing LSD, the profits of which he used to finance the Grateful Dead for some time. The band which formed in the San Fransico bay area during the hippie days of the mid-1960s boasted of a huge cult following. An engineering dropout, he met the Grateful Dead through Ken Kesey, during one of his infamous ‘acid test’ parties in 1965 and became friends with them.</p>
<p>Owsley, or ‘Bear’, as he became known, began to work with the band as their sound man and financed them with his earnings cooking acid. He was also the person who along with his good friend Bob Thomas, helped design the iconic lightning bolt logo, which would sell countless t-shirts. After a hallucinatory incident, where Owsley ‘saw’ the Grateful Dead’s sound, he became obsessed with aural perfection and started on an endless path, working with the band, to achieve it.</p>
<p>Keeping a sonic journal of each performance of the band, he would use it to improve on his setup and mix for each concert and highlight issues to the band members. He insisted on sound checks and encouraged them to listen to tapes of their performances, so that they could hear how they sounded, and also correct him on what he was doing. Rumored to have taken inspiration Buffalo Springfield’s monitoring setup, Owsley was not satisfied with the standard speakers and amplifiers that were available to him and began modifying and manufacturing his own audio gear with his apprentice Tim Scully, eventually founding the company Alembic.</p>
<p>In a 1969 meeting when brainstorming ideas for musical exploration and solutions for their technical problems and smoking ‘special’ cigarettes, Owsley proposed putting the P.A behind the band. This casual, stoned suggestion would change the way audio engineers thought about concert sound. This meant that the audience and the band would hear the same thing without any delays, chaotic reverb, colliding frequencies, and minimal-to-no feedback. Soon after this, Bear began his most ambitious design to date. He, Dan Healy and Mark Raizene of the Grateful Dead’s sound team collaborated with Ron Wickersham, Rick Turner and John Curl of Alembic to create the legendary Wall of sound.</p>
<h4>The Wall of Sound</h4>
<div id="attachment_2628"><p><img decoding="async" aria-describedby="caption-attachment-2628" src="https://audioacademy.in/wp-content/uploads/2019/02/grateful_dead_wall_of_sound_01-300x200.jpg" alt="wall of sound" width="476" height="317" srcset="https://audioacademy.in/wp-content/uploads/2019/02/grateful_dead_wall_of_sound_01-300x200.jpg 300w, https://audioacademy.in/wp-content/uploads/2019/02/grateful_dead_wall_of_sound_01-768x512.jpg 768w, https://audioacademy.in/wp-content/uploads/2019/02/grateful_dead_wall_of_sound_01.jpg 960w" sizes="(max-width: 476px) 100vw, 476px"></p><p id="caption-attachment-2628">The Wall of Sound, 1973</p></div>
<p>The mammoth structure was massive, made up of over 600 hi-fidelity speakers that sat behind the band as they played. It used six separate sound systems which were able to isolate eleven separate channels with vocals, rhythm guitar, piano each having their own channel. Another channel each for the bass drum, snare, tom-toms, and cymbals. The bass was transmitted through a quadraphonic encoder, which took a signal from each string and projected it through its own set of speakers. The result of each speaker carrying only one instrument or voice at a time was crystal clear audio, free of intermodulation distortion.</p>
<p>The Wall of Sound served as its own monitoring system and solved many, if not all of the technical problems that sound engineers faced at that time. This design was also the first of its kind to eliminate the need for a sound guy, at least at the front of house. In addition to monitor controls, each of the microphones had volume dials. The idea was that each band member could adjust the sound in real time. The situation of the monitors gave the band the power to control their own sound. While the system had many trials and errors, the largest version stood three stories high and about 30 meters wide, the first line array system of this scale ever assembled. High-quality audio could be heard at 200 meters, with decent sound up to another 200 meters, at which point wind started to degrade the audio. The completed Wall of Sound made its debut in 1974.</p>
<p>Some of the early issues that surfaced was problems with feedback between the speakers and the singers’ rear-facing vocal microphones and the giant task of physically mounting the system. It was a logistical nightmare and almost made the band go bankrupt, because almost as soon as it rose the roof, it was time to pull it back down again. Eventually, the massive Wall of Sound had to be streamlined into a far more manageable and cost-effective touring rig. All the same, Owsley and the band’s willingness take ideas and execution to extreme lengths changed live sound forever.</p>
<h4>The Legacy of the Wall of Sound</h4>
<p>While parts of the wall were kept, repurposed and recycled for future touring rigs, other parts were sold off. Legend has it that friends of the Grateful Dead, Hot Tuna and Jefferson Starship quickly bought some of the highly sought-after top-of-the-range gear. While modern improvements in these technologies have the benefit of being far more powerful and lightweight than the Wall’s, in terms of experimental, unrestrained engineering, at a time where there was nothing like it ever heard, the Wall of Sound remains unparalleled.</p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Convert your Containerfile to a bootable OS (201 pts)]]></title>
            <link>https://github.com/containers/podman-desktop-extension-bootc</link>
            <guid>40289120</guid>
            <pubDate>Tue, 07 May 2024 17:50:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/containers/podman-desktop-extension-bootc">https://github.com/containers/podman-desktop-extension-bootc</a>, See on <a href="https://news.ycombinator.com/item?id=40289120">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">BootC (Bootable Container) Extension for Podman Desktop</h2><a id="user-content-bootc-bootable-container-extension-for-podman-desktop" aria-label="Permalink: BootC (Bootable Container) Extension for Podman Desktop" href="#bootc-bootable-container-extension-for-podman-desktop"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/containers/podman-desktop-extension-bootc/main/docs/img/logo.png"><img src="https://raw.githubusercontent.com/containers/podman-desktop-extension-bootc/main/docs/img/logo.png" alt=""></a></p>
<p dir="auto">Want to create a bootable operating system from a Containerfile? Download this extension!</p>
<p dir="auto">Easily go from container to VM / ISO-on-a-USB / RAW image!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Topics</h2><a id="user-content-topics" aria-label="Permalink: Topics" href="#topics"></a></p>
<ul dir="auto">
<li><a href="#technology">Technology</a></li>
<li><a href="#bootable-container-images">Bootable Container Images</a></li>
<li><a href="#read-before-using">Read Before Using</a></li>
<li><a href="#example-images">Example Images</a></li>
<li><a href="#use-case">Use Case</a></li>
<li><a href="#requirements">Requirements</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="#contributing">Contributing</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technology</h2><a id="user-content-technology" aria-label="Permalink: Technology" href="#technology"></a></p>
<p dir="auto">The <strong>Bootable Container (bootc)</strong> extension uses <a href="https://github.com/osbuild/bootc-image-builder">bootc-image-builder</a> in order to build bootable <em>container</em> disk images.</p>
<p dir="auto">Once a machine is created from the disk image, it can apply transactional updates "in place" from newly pushed container images (without creating a new disk image). For more information, see <a href="https://containers.github.io/bootc/" rel="nofollow">bootc</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Bootable Container Images</h2><a id="user-content-bootable-container-images" aria-label="Permalink: Bootable Container Images" href="#bootable-container-images"></a></p>
<p dir="auto">There are many projects at work at creating "bootc" images. Below is a non-exhaustive list of compatible images which are known to work with <a href="https://github.com/osbuild/bootc-image-builder"><code>bootc-image-builder</code></a>.</p>
<p dir="auto"><strong>CentOS:</strong></p>
<ul dir="auto">
<li>Containerfile: <code>FROM quay.io/centos-bootc/centos-bootc:stream9</code></li>
<li>Repo: <a href="https://quay.io/centos-bootc/centos-bootc" rel="nofollow"><code>quay.io/centos-bootc/centos-bootc:stream9</code></a></li>
<li>Example Images: <a href="https://gitlab.com/fedora/bootc/examples" rel="nofollow">gitlab.com/fedora/bootc/examples</a></li>
<li>Documentation: <a href="https://docs.fedoraproject.org/en-US/bootc/" rel="nofollow">fedoraproject.org</a></li>
<li>Source: <a href="https://github.com/centos/centos-bootc">github.com/centos/centos-bootc</a></li>
<li>Notes: N/A</li>
</ul>
<p dir="auto"><strong>Fedora:</strong></p>
<ul dir="auto">
<li>Containerfile: <code>FROM quay.io/fedora/fedora-bootc:40</code></li>
<li>Repo: <a href="https://quay.io/fedora/fedora-bootc" rel="nofollow"><code>quay.io/fedora/fedora-bootc:40</code></a></li>
<li>Example Images: <a href="https://gitlab.com/fedora/bootc/examples" rel="nofollow">gitlab.com/fedora/bootc/examples</a></li>
<li>Documentation: <a href="https://docs.fedoraproject.org/en-US/bootc/" rel="nofollow">fedoraproject.org</a></li>
<li>Source: <a href="https://gitlab.com/fedora/bootc/base-images" rel="nofollow">gitlab.com/fedora/bootc/base-images</a></li>
<li>Notes: Must select "XFS" or "EXT4" for the root filesystem when building in the GUI. <a href="https://docs.fedoraproject.org/en-US/bootc/default-rootfs-type/" rel="nofollow">Read more here.</a></li>
</ul>
<p dir="auto"><strong>RHEL:</strong></p>
<ul dir="auto">
<li>Containerfile: <code>FROM registry.redhat.io/rhel9/rhel-bootc:9.4</code></li>
<li>Repo: <a href="https://catalog.redhat.com/search?gs&amp;q=bootc" rel="nofollow"><code>registry.redhat.io/rhel9/rhel-bootc:9.4</code></a></li>
<li>Documentation: <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/using_image_mode_for_rhel_to_build_deploy_and_manage_operating_systems/index#doc-wrapper" rel="nofollow">Red Hat Customer Portal</a></li>
</ul>
<p dir="auto">The images can then be added to your Containerfile:</p>
<div dir="auto" data-snippet-clipboard-copy-content="FROM quay.io/centos-bootc/centos-bootc:stream9"><pre><span>FROM</span> quay.io/centos-bootc/centos-bootc:stream9</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Learning more</h3><a id="user-content-learning-more" aria-label="Permalink: Learning more" href="#learning-more"></a></p>
<ul dir="auto">
<li><a href="https://docs.fedoraproject.org/en-US/bootc/building-containers/" rel="nofollow">Fedora Building Containers Guide</a>: provides an overview on how to create Fedora/CentOS-derived bootc images.</li>
<li><a href="https://containers.github.io/bootc/building/guidance.html" rel="nofollow">Bootc General Guidance</a>: provides a general configuration overview for bootc images.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Read Before Using</h2><a id="user-content-read-before-using" aria-label="Permalink: Read Before Using" href="#read-before-using"></a></p>
<p dir="auto">Some concepts to grasp before using.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><strong>Think of it as an OS provisioning tool!</strong></h3><a id="user-content-think-of-it-as-an-os-provisioning-tool" aria-label="Permalink: Think of it as an OS provisioning tool!" href="#think-of-it-as-an-os-provisioning-tool"></a></p>
<p dir="auto">You are "creating" an OS straight from a Containerfile, isn't that awesome?</p>
<p dir="auto"><strong>FIRST</strong> realize that you are creating an OS with all your applications, developer tools, even games that you want.</p>
<p dir="auto"><strong>SECONDLY</strong> ask yourself what applications you want to have running (perhaps on boot too!).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Creating your first bootable OS Containerfile</h3><a id="user-content-creating-your-first-bootable-os-containerfile" aria-label="Permalink: Creating your first bootable OS Containerfile" href="#creating-your-first-bootable-os-containerfile"></a></p>
<p dir="auto">Want a quick straight-to-the-point Hello World Containerfile?</p>
<div dir="auto" data-snippet-clipboard-copy-content="FROM quay.io/centos-bootc/centos-bootc:stream9
# Change your root password for a &quot;test login&quot; that
# allows to log in on a virtual/physical console
# NOTE: While some base images may set `PermitRootLogin prohibit-password`
# for OpenSSH, not all will.
# This is VERY dangerous and only meant for Hello World purposes.
RUN echo &quot;root:root&quot; | chpasswd"><pre><span>FROM</span> quay.io/centos-bootc/centos-bootc:stream9
<span><span>#</span> Change your root password for a "test login" that</span>
<span><span>#</span> allows to log in on a virtual/physical console</span>
<span><span>#</span> NOTE: While some base images may set `PermitRootLogin prohibit-password`</span>
<span><span>#</span> for OpenSSH, not all will.</span>
<span><span>#</span> This is VERY dangerous and only meant for Hello World purposes.</span>
<span>RUN</span> echo <span>"root:root"</span> | chpasswd</pre></div>
<p dir="auto">After creating your image you can now login and explore your bootable OS.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Example images</h2><a id="user-content-example-images" aria-label="Permalink: Example images" href="#example-images"></a></p>
<p dir="auto">Want to view more example images Such as <a href="https://gitlab.com/bootc-org/examples/-/tree/main/httpd" rel="nofollow"><code>httpd</code></a> and <a href="https://gitlab.com/bootc-org/examples/-/tree/main/nvidia" rel="nofollow"><code>nvidia</code></a>?</p>
<p dir="auto">All of our maintained example images are on the <a href="https://gitlab.com/fedora/bootc/examples" rel="nofollow">gitlab.com/fedora/bootc/examples</a> repo.</p>
<p dir="auto">You can also pull our example image based on the <a href="https://gitlab.com/bootc-org/examples/-/tree/main/httpd" rel="nofollow"><code>httpd</code></a> example:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/containers/podman-desktop-media/bootc-extension/gifs/clicking_pull.gif"><img src="https://raw.githubusercontent.com/containers/podman-desktop-media/bootc-extension/gifs/clicking_pull.gif" alt="" data-animated-image=""></a></p>
<p dir="auto">After building, read our <a href="https://github.com/containers/podman-desktop-extension-bootc/blob/main/docs/vm_guide.md">Virtual Machine Guide</a> on how to launch your image and access your HTTP server.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Use Case</h2><a id="user-content-use-case" aria-label="Permalink: Use Case" href="#use-case"></a></p>
<p dir="auto">Go from a <a href="https://containers.github.io/bootc/" rel="nofollow">bootc</a> compatible derived container build to a disk image format:</p>
<ul dir="auto">
<li><code>qcow2</code>: QEMU Disk Images</li>
<li><code>ami</code>: Amazon Machine Images</li>
<li><code>raw</code>: RAW disk image an MBR or GPT partition table</li>
<li><code>iso</code>: Unattended installation method (USB sticks / install-on-boot)</li>
<li><code>vmdk</code>: Usable in vSphere</li>
</ul>
<p dir="auto">The list above is what is supported by the underlying <code>bootc-image-builder</code> technology. The list can <a href="https://github.com/osbuild/bootc-image-builder?tab=readme-ov-file#-image-types">be found here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Requirement 1. Software and hardware requirements</h3><a id="user-content-requirement-1-software-and-hardware-requirements" aria-label="Permalink: Requirement 1. Software and hardware requirements" href="#requirement-1-software-and-hardware-requirements"></a></p>
<p dir="auto"><strong>OS:</strong></p>
<p dir="auto">Compatible on Windows, macOS &amp; Linux</p>
<p dir="auto"><strong>Software:</strong></p>
<ul dir="auto">
<li><a href="https://github.com/containers/podman-desktop">Podman Desktop 1.10.0+</a></li>
<li><a href="https://github.com/containers/podman">Podman 5.0.1+</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Requirement 2. Rootful mode on Podman Machine</h3><a id="user-content-requirement-2-rootful-mode-on-podman-machine" aria-label="Permalink: Requirement 2. Rootful mode on Podman Machine" href="#requirement-2-rootful-mode-on-podman-machine"></a></p>
<p dir="auto">Podman Machine requirements:</p>
<ul dir="auto">
<li><strong>Rootful mode enabled</strong></li>
<li><em>At least</em> 6GB of RAM allocated in order to build the disk image</li>
</ul>
<p dir="auto">Rootful mode can be enabled through the CLI to an already deployed VM:</p>
<div dir="auto" data-snippet-clipboard-copy-content="podman machine stop
podman machine set --rootful
podman machine start"><pre>podman machine stop
podman machine <span>set</span> --rootful
podman machine start</pre></div>
<p dir="auto">Or set when initially creating a Podman Machine via Podman Desktop:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/containers/podman-desktop-extension-bootc/main/docs/img/rootful_setup.png"><img src="https://raw.githubusercontent.com/containers/podman-desktop-extension-bootc/main/docs/img/rootful_setup.png" alt="rootful setup"></a></p>
<p dir="auto"><strong>Linux users:</strong></p>
<p dir="auto">On Linux, you are unable to create a Podman Machine through the GUI of Podman Desktop, to create a rootful Podman Machine you can run the following commands:</p>
<div dir="auto" data-snippet-clipboard-copy-content="podman machine init --rootful
podman machine start"><pre>podman machine init --rootful
podman machine start</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">This extension can be installed through the <strong>Extensions</strong> section of Podman Desktop within the <strong>Catalog</strong> tab:</p>
<ol dir="auto">
<li>Go to <strong>Extensions</strong> in the navbar.</li>
<li>Click on the <strong>Catalog</strong> tab.</li>
<li>Install the extension.</li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/containers/podman-desktop-media/bootc-extension/gifs/catalog_install.gif"><img src="https://raw.githubusercontent.com/containers/podman-desktop-media/bootc-extension/gifs/catalog_install.gif" alt="" data-animated-image=""></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Nightly version</h3><a id="user-content-nightly-version" aria-label="Permalink: Nightly version" href="#nightly-version"></a></p>
<p dir="auto">A version of the extension using the latest commit changes can be installed via the <strong>Install custom...</strong> button with the following link:</p>
<div data-snippet-clipboard-copy-content="ghcr.io/containers/podman-desktop-extension-bootc:nightly"><pre><code>ghcr.io/containers/podman-desktop-extension-bootc:nightly
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<ol dir="auto">
<li><strong>Build your bootc-enabled Containerfile:</strong></li>
</ol>
<blockquote>
<p dir="auto">In the example below, we are going to change the root password for testing purposes when accessing the OS.</p>
</blockquote>
<div dir="auto" data-snippet-clipboard-copy-content="FROM quay.io/centos-bootc/centos-bootc:stream9

# Change the root password
# CAUTION: This is NOT recommended and is used only for testing / hello world purposes
RUN echo &quot;root:root&quot; | chpasswd"><pre><span>FROM</span> quay.io/centos-bootc/centos-bootc:stream9

<span><span>#</span> Change the root password</span>
<span><span>#</span> CAUTION: This is NOT recommended and is used only for testing / hello world purposes</span>
<span>RUN</span> echo <span>"root:root"</span> | chpasswd</pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/containers/podman-desktop-media/bootc-extension/gifs/build_image.gif"><img src="https://raw.githubusercontent.com/containers/podman-desktop-media/bootc-extension/gifs/build_image.gif" alt="" data-animated-image=""></a></p>
<ol start="2" dir="auto">
<li><strong>Build the disk image:</strong></li>
</ol>
<blockquote>
<p dir="auto">Build the disk image, this takes approximatley 2-5 minutes depending on the performance of your machine.</p>
</blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/containers/podman-desktop-media/bootc-extension/gifs/bootc_building.gif"><img src="https://raw.githubusercontent.com/containers/podman-desktop-media/bootc-extension/gifs/bootc_building.gif" alt="" data-animated-image=""></a></p>
<ol start="3" dir="auto">
<li><strong>Launching the VM:</strong></li>
</ol>
<blockquote>
<p dir="auto">See our <a href="https://github.com/containers/podman-desktop-extension-bootc/blob/main/docs/vm_guide.md">Virtual Machine Guide</a> on how to launch the image!</p>
</blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/containers/podman-desktop-media/bootc-extension/gifs/os_booting.gif"><img src="https://raw.githubusercontent.com/containers/podman-desktop-media/bootc-extension/gifs/os_booting.gif" alt="" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Preferences</h2><a id="user-content-preferences" aria-label="Permalink: Preferences" href="#preferences"></a></p>
<p dir="auto">Preferences such as the default <code>bootc-builder-image</code> as well as timeouts can be adjusted within the <strong>Preferences</strong> section of Podman Desktop.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/containers/podman-desktop-extension-bootc/main/docs/img/preferences.png"><img src="https://raw.githubusercontent.com/containers/podman-desktop-extension-bootc/main/docs/img/preferences.png" alt=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Want to help develop and contribute to the bootc extension? View our <a href="https://github.com/containers/podman-desktop-extension-bootc/blob/main/CONTRIBUTING.md">CONTRIBUTING</a> document.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Facebook just updated its relationship status with Web Components (128 pts)]]></title>
            <link>https://www.mux.com/blog/facebook-just-updated-it-s-relationship-status-with-web-components</link>
            <guid>40288947</guid>
            <pubDate>Tue, 07 May 2024 17:38:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mux.com/blog/facebook-just-updated-it-s-relationship-status-with-web-components">https://www.mux.com/blog/facebook-just-updated-it-s-relationship-status-with-web-components</a>, See on <a href="https://news.ycombinator.com/item?id=40288947">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>As our friend of the blog, Wes Bos, pointed out the other day, React 19 (beta) added, all the way at the bottom of the announcement, much-awaited <a href="https://react.dev/blog/2024/04/25/react-19#support-for-custom-elements">support for web components</a>.</p><div data-theme="light"><article><p><span>Tucked all the way at the bottom on the React 19 release notes is full support for web components!  🥳 </span></p></article></div><p>I say “much-awaited”, but the truth is a lot of people are not going to care at all about that. There are some, though, like us who really care and have been paying close attention.</p><p>At Mux we have not been shy about our fondness for web components over the last few years. It started with building Mux Player on our <a href="https://www.mux.com/blog/media-chrome-turns-1-0">web components-based player framework Media Chrome</a> (thanks for the <a href="https://twitter.com/jhooks/status/1785410152025543065">kind words about Media Chrome</a>, Joel).</p><p>But I want to drive home one point about our fondness for web components. It’s not about some righteous party line, which so many front-end tooling debates devolve into. If you came to this post for some juicy nerd drama, turn around because that’s not what we’re here for. <a href="https://www.mux.com/blog/slow-is-stable-stable-is-fast-building-mux-player-on-the-slow-platform-of-web-components">Our adoption of web components is purely based on practicality</a>. Nothing more, and nothing less. Is it the best tool for every job? Obviously not. But we believe it is the best tool for the job of building a framework-agnostic video player SDK. We still use React heavily. In fact, we were early adopters of <a href="https://www.mux.com/blog/what-are-react-server-components">React Server Components and wrote about it</a> in Darius’s widely circulated post.</p><h2 id="so-what-does-this-mean-for-react-actually"><a href="#so-what-does-this-mean-for-react-actually"><span><span><svg role="img" xmlns="http://www.w3.org/2000/svg" width="42" height="42" fill="none" viewBox="0 0 42 42"><title>Link</title><path d="m18.172 15.343 3.535-3.535a5 5 0 0 1 7.071 0l1.414 1.414a5 5 0 0 1 0 7.07l-3.535 3.536M23.828 26.657l-3.535 3.535a5 5 0 0 1-7.071 0l-1.414-1.414a5 5 0 0 1 0-7.07l3.535-3.536" vector-effect="non-scaling-stroke"></path><path stroke-linecap="round" d="m25.243 16.757-8.486 8.486" vector-effect="non-scaling-stroke"></path></svg></span></span>So what does this mean for React, actually?</a></h2><p>To contrast all the positives about building SDKs on web components, React support has been by far the biggest hurdle to clear for web components projects. Raw web components have always <strong>worked</strong> in React (it’s just HTML, after all), but the challenge was more about the ergonomics of using web components in a React world.</p><p>For example, React developers don't interact directly with HTML elements to do things like add event listeners and call functions. It sort of goes against the declarative nature of React itself.</p><p>So although setting up an event listener was possible, like this, it feels very un-React-y:</p><div><figure><figcaption><p>jsx</p></figcaption><pre><code><span></span><span>function</span> <span>MyComponent</span> <span>(</span><span>)</span> <span>{</span>
<span></span>  <span>const</span> ref <span>=</span> <span>useRef</span><span>(</span><span>null</span><span>)</span><span>;</span>
<span></span>  <span>const</span> <span>[</span>count<span>,</span> setCount<span>]</span> <span>=</span> <span>useState</span><span>(</span><span>0</span><span>)</span><span>;</span>
<span></span>
<span></span>  <span>useEffect</span><span>(</span><span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
<span></span>    <span>function</span> <span>handleClick</span> <span>(</span><span>)</span> <span>{</span>
<span></span>      <span>setCount</span><span>(</span>count <span>+</span> <span>1</span><span>)</span>
<span></span>    <span>}</span>
<span></span>    <span>if</span> <span>(</span>ref<span>)</span> <span>{</span>
<span></span>       ref<span>.</span><span>addEventListener</span><span>(</span><span>'click'</span><span>,</span> handleClick<span>)</span><span>;</span>
<span></span>    <span>}</span>
<span></span>    <span>return</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
<span></span>      ref<span>.</span><span>removeEventListener</span><span>(</span><span>'click'</span><span>,</span> handleClick<span>)</span><span>;</span>
<span></span>    <span>}</span>
<span></span>  <span>}</span><span>,</span> <span>[</span>ref<span>]</span><span>)</span><span>;</span>
<span></span>
<span></span>  <span>return</span> <span><span><span>&lt;</span>my-button</span> <span>ref</span><span><span>=</span><span>{</span>ref<span>}</span></span><span>&gt;</span></span><span>Hello! </span><span>{</span>count<span>}</span><span><span><span>&lt;/</span>my-button</span><span>&gt;</span></span>
<span></span><span>}</span></code></pre></figure></div><p>Because of this, most maintainers (us included), have chosen to create React-specific wrappers. That works well, but it always felt like a stop-gap. Now that React 19 supports web components directly, you can do things like pass event listeners as React props and React props passed to the element get automatically set as properties on the element. There’s some extra complexity here for web component authors to handle attributes vs. properties. Not all web components will play nice with React from day 1, but at least we have a path forward.</p><p>See here, this feels much more React-y:</p><div><figure><figcaption><p>jsx</p></figcaption><pre><code><span></span><span>function</span> <span>MyComponent</span> <span>(</span><span>)</span> <span>{</span>
<span></span>  <span>const</span> <span>[</span>count<span>,</span> setCount<span>]</span> <span>=</span> <span>useState</span><span>(</span><span>0</span><span>)</span><span>;</span>
<span></span>
<span></span>  <span>return</span> <span>(</span>
<span></span>    <span>&lt;</span>my<span>-</span>button
<span></span>      ref<span>=</span><span>{</span>ref<span>}</span>
<span></span>      onClick<span>{</span><span>(</span><span>)</span> <span>=&gt;</span> <span>setCount</span><span>(</span>count <span>+</span> <span>1</span><span>)</span><span>}</span>
<span></span>     <span>&gt;</span>
<span></span>       Hello<span>!</span> <span>{</span>count<span>}</span>
<span></span>     <span><span><span>&lt;/</span>my-button</span><span>&gt;</span></span>
<span></span>  <span>)</span>
<span></span><span>}</span></code></pre></figure></div><p>Internally, all React is doing here is taking the onClick prop and setting up/tearing down the click handler under the hood. Which is basically what React wrappers were doing anyway.</p><p>I’m hoping that in 1 or 2 short years from now your average React developer will not be weirded out or averse to using a web component in their React application. And once that feels normal perhaps we can thank those old React wrappers for bringing us joy and then do away with them.</p><h2 id="it-still-might-be-a-little-weird"><a href="#it-still-might-be-a-little-weird"><span><span><svg role="img" xmlns="http://www.w3.org/2000/svg" width="42" height="42" fill="none" viewBox="0 0 42 42"><title>Link</title><path d="m18.172 15.343 3.535-3.535a5 5 0 0 1 7.071 0l1.414 1.414a5 5 0 0 1 0 7.07l-3.535 3.536M23.828 26.657l-3.535 3.535a5 5 0 0 1-7.071 0l-1.414-1.414a5 5 0 0 1 0-7.07l3.535-3.536" vector-effect="non-scaling-stroke"></path><path stroke-linecap="round" d="m25.243 16.757-8.486 8.486" vector-effect="non-scaling-stroke"></path></svg></span></span>It still might be a little weird</a></h2><p>Event handlers and passing React props into web components is a nice improvement, but I think there’s still one weird thing about web components that doesn’t quite jive with the React way of doing things, and that’s the globally scoped element register.</p><p>In any React app, when you use a component, you have to import that component:</p><div><figure><figcaption><p>jsx</p></figcaption><pre><code><span></span><span>// when MyThing is a react component</span>
<span></span><span>import</span> MyThing <span>from</span> <span>'./components/my-thing'</span></code></pre></figure></div><p>But that doesn’t exist in the web components world (at least not yet). When you use &lt;my-button&gt; in a React component, you don’t have to explicitly import my-button from anywhere inside that file.</p><div><figure><figcaption><p>jsx</p></figcaption><pre><code><span></span><span>// when my-thing defines a web component is imported it gets registered globally</span>
<span></span><span>import</span> <span>'./components/my-thing'</span></code></pre></figure></div><p>Web components get imported and registered at the global level. This also means if two different libraries have a component called my-button, you can’t use both of them. This will certainly be a little weird for React developers to get used to. It also might change with <a href="https://github.com/WICG/webcomponents/blob/gh-pages/proposals/Scoped-Custom-Element-Registries.md">Scope Custom Element Registries in the future</a>.</p><h2 id="dont-sleep-on-web-awesome-either"><a href="#dont-sleep-on-web-awesome-either"><span><span><svg role="img" xmlns="http://www.w3.org/2000/svg" width="42" height="42" fill="none" viewBox="0 0 42 42"><title>Link</title><path d="m18.172 15.343 3.535-3.535a5 5 0 0 1 7.071 0l1.414 1.414a5 5 0 0 1 0 7.07l-3.535 3.536M23.828 26.657l-3.535 3.535a5 5 0 0 1-7.071 0l-1.414-1.414a5 5 0 0 1 0-7.07l3.535-3.536" vector-effect="non-scaling-stroke"></path><path stroke-linecap="round" d="m25.243 16.757-8.486 8.486" vector-effect="non-scaling-stroke"></path></svg></span></span>Don’t sleep on Web Awesome, either</a></h2><p>One other web components topic caught my eye last week. <a href="https://shoelace.style/">Shoelace</a>, one of the premier web components projects, launched a Kickstarter project for <a href="https://www.kickstarter.com/projects/fontawesome/web-awesome">Web Awesome</a> after being acquired by Font Awesome. With a modest goal of $30,000, the campaign gained the pledges of <a href="https://www.kickstarter.com/projects/fontawesome/web-awesome">6,725 backers for a total of $723,004</a>, nearly 25x the original goal. And I don’t think that’s all hype – I think there’s a reliable signal here that developers want a stable set of tools, built on browser standards.</p><h2 id="lets-see-how-things-pan-out"><a href="#lets-see-how-things-pan-out"><span><span><svg role="img" xmlns="http://www.w3.org/2000/svg" width="42" height="42" fill="none" viewBox="0 0 42 42"><title>Link</title><path d="m18.172 15.343 3.535-3.535a5 5 0 0 1 7.071 0l1.414 1.414a5 5 0 0 1 0 7.07l-3.535 3.536M23.828 26.657l-3.535 3.535a5 5 0 0 1-7.071 0l-1.414-1.414a5 5 0 0 1 0-7.07l3.535-3.536" vector-effect="non-scaling-stroke"></path><path stroke-linecap="round" d="m25.243 16.757-8.486 8.486" vector-effect="non-scaling-stroke"></path></svg></span></span>Let’s see how things pan out</a></h2><p>That’s all for now, we’ll see how things pan out. I, for one, am happy to see the React ecosystem moving in this direction. And I believe the general frontend ecosystem is criminally under-indexed on web components. All web application developers can benefit by moving parts of their applications away from framework-specific patterns and more towards browser-level standards. Frameworks will come and go, so as developers and maintainers we build against them at our own risk. Browser standards tend to come, albeit slowly, and then rarely disappear after that.</p><p>Right now the plan is to wait and see. In the short term, we’ll keep maintaining our React wrappers. It’s all going to depend on if React developers, as a general population, start embracing web components and start feeling comfortable using them in their applications. We want our component libraries to feel great and idiomatic wherever they’re used…and we hope one day soon that could just mean web components.</p></div><div><h2>Leave your wallet <br>where it is</h2><p>No credit card required to get started.</p></div></div>]]></description>
        </item>
    </channel>
</rss>