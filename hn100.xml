<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 26 Dec 2025 16:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Rob Pike Goes Nuclear over GenAI (420 pts)]]></title>
            <link>https://skyview.social/?url=https%3A%2F%2Fbsky.app%2Fprofile%2Frobpike.io%2Fpost%2F3matwg6w3ic2s&amp;viewtype=tree</link>
            <guid>46392115</guid>
            <pubDate>Fri, 26 Dec 2025 14:08:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://skyview.social/?url=https%3A%2F%2Fbsky.app%2Fprofile%2Frobpike.io%2Fpost%2F3matwg6w3ic2s&#x26;viewtype=tree">https://skyview.social/?url=https%3A%2F%2Fbsky.app%2Fprofile%2Frobpike.io%2Fpost%2F3matwg6w3ic2s&#x26;viewtype=tree</a>, See on <a href="https://news.ycombinator.com/item?id=46392115">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Package managers keep using Git as a database, it never works out (224 pts)]]></title>
            <link>https://nesbitt.io/2025/12/24/package-managers-keep-using-git-as-a-database.html</link>
            <guid>46391514</guid>
            <pubDate>Fri, 26 Dec 2025 12:46:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nesbitt.io/2025/12/24/package-managers-keep-using-git-as-a-database.html">https://nesbitt.io/2025/12/24/package-managers-keep-using-git-as-a-database.html</a>, See on <a href="https://news.ycombinator.com/item?id=46391514">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Using git as a database is a seductive idea. You get version history for free. Pull requests give you a review workflow. It’s distributed by design. GitHub will host it for free. Everyone already knows how to use it.</p>

<p>Package managers keep falling for this. And it keeps not working out.</p>

<h2 id="cargo">Cargo</h2>

<p>The crates.io index started as a git repository. Every Cargo client cloned it. This worked fine when the registry was small, but the index kept growing. Users would see progress bars like “Resolving deltas: 74.01%, (64415/95919)” hanging for ages, the visible symptom of Cargo’s libgit2 library grinding through <a href="https://github.com/rust-lang/cargo/issues/9069">delta resolution</a> on a repository with thousands of historic commits.</p>

<p>The problem was worst in CI. Stateless environments would download the full index, use a tiny fraction of it, and throw it away. Every build, every time.</p>

<p><a href="https://rust-lang.github.io/rfcs/2789-sparse-index.html">RFC 2789</a> introduced a sparse HTTP protocol. Instead of cloning the whole index, Cargo now fetches files directly over HTTPS, downloading only the metadata for dependencies your project actually uses. (This is the “<a href="https://nesbitt.io/2025/12/05/package-manager-tradeoffs.html">full index replication vs on-demand queries</a>” tradeoff in action.) By April 2025, 99% of crates.io requests came from Cargo versions where sparse is the default. The git index still exists, still growing by thousands of commits per day, but most users never touch it.</p>

<h2 id="homebrew">Homebrew</h2>

<p><a href="https://github.com/Homebrew/brew/pull/9383">GitHub explicitly asked Homebrew to stop using shallow clones.</a> Updating them was <a href="https://brew.sh/2023/02/16/homebrew-4.0.0/">“an extremely expensive operation”</a> due to the tree layout and traffic of homebrew-core and homebrew-cask.</p>

<p>Users were downloading 331MB just to unshallow homebrew-core. The .git folder approached 1GB on some machines. Every <code>brew update</code> meant waiting for git to grind through delta resolution.</p>

<p>Homebrew 4.0.0 in February 2023 switched to JSON downloads for tap updates. The reasoning was blunt: “they are expensive to git fetch and git clone and GitHub would rather we didn’t do that… they are slow to git fetch and git clone and this provides a bad experience to end users.”</p>

<p>Auto-updates now run every 24 hours instead of every 5 minutes, and they’re much faster because there’s no git fetch involved.</p>

<h2 id="cocoapods">CocoaPods</h2>

<p>CocoaPods is the package manager for iOS and macOS development. It hit the limits hard. The Specs repo grew to hundreds of thousands of podspecs across a deeply nested directory structure. Cloning took minutes. Updating took minutes. CI time vanished into git operations.</p>

<p>GitHub imposed CPU rate limits. The culprit was shallow clones, which force GitHub’s servers to compute which objects the client already has. The team tried various band-aids: stopping auto-fetch on <code>pod install</code>, converting shallow clones to full clones, <a href="https://blog.cocoapods.org/Sharding/">sharding the repository</a>.</p>

<p>The CocoaPods blog captured it well: <a href="https://blog.cocoapods.org/Master-Spec-Repo-Rate-Limiting-Post-Mortem/">“Git was invented at a time when ‘slow network’ and ‘no backups’ were legitimate design concerns. Running endless builds as part of continuous integration wasn’t commonplace.”</a></p>

<p>CocoaPods 1.8 <a href="https://blog.cocoapods.org/CocoaPods-1.8.0-beta/">gave up on git entirely</a> for most users. A CDN became the default, serving podspec files directly over HTTP. The migration saved users about a gigabyte of disk space and made <code>pod install</code> nearly instant for new setups.</p>

<h2 id="nixpkgs">Nixpkgs</h2>

<p>Nixpkgs is currently stress-testing GitHub’s infrastructure. In November 2025, GitHub contacted the NixOS team about <a href="https://discourse.nixos.org/t/nixpkgs-core-team-update-2025-11-30-github-scaling-issues/72709">periodic maintenance jobs failing</a> and causing “issues achieving consensus between replicas.” If unresolved, the repository could have become read-only.</p>

<p>The repository totals 83GB with half a million tree objects and 20,000 forks. A local clone is only 2.5GB — the rest is GitHub’s fork network storing every pull request branch and merge commit. The CI queries mergeability daily, creating new merge commits each time.</p>

<p>Unlike CocoaPods, Nixpkgs can’t easily move to a CDN. The Nix expressions <em>are</em> the package definitions, not metadata pointing elsewhere. Binary caches already serve built packages over HTTP, but nixpkgs itself remains a git repository — and it’s still growing.</p>

<h2 id="vcpkg">vcpkg</h2>

<p>vcpkg is Microsoft’s C++ package manager. It uses git tree hashes to version its ports, with the curated registry at <a href="https://github.com/Microsoft/vcpkg">github.com/Microsoft/vcpkg</a> containing over 2,000 libraries.</p>

<p>The problem is that vcpkg needs to retrieve specific versions of ports by their git tree hash. When you specify a <code>builtin-baseline</code> in your vcpkg.json (functioning like a lockfile for reproducible builds), vcpkg looks up historical commits to find the exact port versions you need. This only works if you have the full commit history.</p>

<p>Shallow clones break everything. GitHub Actions uses shallow clones by default. DevContainers <a href="https://github.com/devcontainers/images/issues/398">shallow-clone vcpkg</a> to save space. CI systems optimize for fast checkouts. All of these result in the same error: “vcpkg was cloned as a shallow repository… Try again with a full vcpkg clone.”</p>

<p>The workarounds are ugly. One <a href="https://github.com/devcontainers/images/issues/398">proposed solution</a> involves parsing vcpkg.json to extract the baseline hash, deriving the commit date, then fetching with <code>--shallow-since=&lt;date&gt;</code>. Another suggests including twelve months of history, hoping projects upgrade before their baseline falls off the cliff. For GitHub Actions, you need <code>fetch-depth: 0</code> in your checkout step, <a href="https://github.com/microsoft/vcpkg/issues/25349">downloading the entire repository history</a> just to resolve dependencies.</p>

<p>A vcpkg team member <a href="https://github.com/microsoft/vcpkg/issues/25349">explained the fundamental constraint</a>: “Port versions don’t use commit hashes, we use the git tree hash of the port directory. As far as I know, there is no way to deduce the commit that added a specific tree hash.” An in-product fix is infeasible. The architecture baked in git deeply enough that there’s no escape hatch.</p>

<p>Unlike Cargo, Homebrew, and CocoaPods, vcpkg hasn’t announced plans to move away from git registries. Custom registries must still be git repositories. The documentation describes filesystem registries as an alternative, but these require local or mounted paths rather than HTTP access. There’s no CDN, no sparse protocol, no HTTP-based solution on the horizon.</p>

<h2 id="go-modules">Go modules</h2>

<p><a href="https://engineering.grab.com/go-module-proxy">Grab’s engineering team</a> went from 18 minutes for <code>go get</code> to 12 seconds after deploying a module proxy. That’s not a typo. Eighteen minutes down to twelve seconds.</p>

<p>The problem was that <code>go get</code> needed to fetch each dependency’s source code just to read its go.mod file and resolve transitive dependencies. Cloning entire repositories to get a single file.</p>

<p>Go had security concerns too. The original design wanted to remove version control tools entirely because <a href="https://arslan.io/2019/08/02/why-you-should-use-a-go-module-proxy/">“these fragment the ecosystem: packages developed using Bazaar or Fossil, for example, are effectively unavailable to users who cannot or choose not to install these tools.”</a> Beyond fragmentation, the Go team worried about security bugs in version control systems becoming security bugs in <code>go get</code>. You’re not just importing code; you’re importing the attack surface of every VCS tool on the developer’s machine.</p>

<p>GOPROXY became the default in Go 1.13. The proxy serves source archives and go.mod files independently over HTTP. Go also introduced a <a href="https://nesbitt.io/2025/12/21/federated-package-management.html#gos-experiment-with-dns">checksum database (sumdb)</a> that records cryptographic hashes of module contents. This protects against force pushes silently changing tagged releases, and ensures modules remain available even if the original repository is deleted.</p>

<h2 id="beyond-package-managers">Beyond package managers</h2>

<p>The same pattern shows up wherever developers try to use git as a database.</p>

<p>Git-based wikis like Gollum (used by GitHub and GitLab) become <a href="https://github.com/gollum/gollum/issues/1940">“somewhat too slow to be usable”</a> at scale. Browsing directory structure takes seconds per click. Loading pages takes longer. <a href="https://docs.gitlab.com/ee/development/wikis.html">GitLab plans to move away from Gollum entirely.</a></p>

<p>Git-based CMS platforms like Decap hit GitHub’s API rate limits. A Decap project on GitHub <a href="https://decapcms.org/blog/git-based-cms-definition-features-best-practices/">scales to about 10,000 entries</a> if you have a lot of collection relations. A new user with an empty cache makes a request per entry to populate it, burning through the 5,000 request limit quickly. If your site has lots of content or updates frequently, use a database instead.</p>

<p>Even GitOps tools that embrace git as a source of truth have to work around its limitations. ArgoCD’s repo server <a href="https://argo-cd.readthedocs.io/en/stable/operator-manual/high_availability/">can run out of disk space</a> cloning repositories. A single commit invalidates the cache for all applications in that repo. Large monorepos need special scaling considerations.</p>

<h2 id="the-pattern">The pattern</h2>

<p>The hosting problems are symptoms. The underlying issue is that git inherits filesystem limitations, and filesystems make terrible databases.</p>

<p><strong>Directory limits.</strong> Directories with too many files become slow. CocoaPods had <a href="https://blog.cocoapods.org/Sharding/">16,000 pod directories</a> in a single Specs folder, requiring huge tree objects and expensive computation. Their fix was hash-based sharding: split directories by the first few characters of a hashed name, so no single directory has too many entries. Git itself does this internally with its objects folder, splitting into 256 subdirectories. You’re reinventing B-trees, badly.</p>

<p><strong>Case sensitivity.</strong> Git is case-sensitive, but macOS and Windows filesystems typically aren’t. <a href="https://learn.microsoft.com/en-us/azure/devops/repos/git/os-compatibility">Check out a repo containing both <code>File.txt</code> and <code>file.txt</code> on Windows</a>, and the second overwrites the first. <a href="https://learn.microsoft.com/en-us/azure/devops/repos/git/case-sensitivity">Azure DevOps</a> had to add server-side enforcement to block pushes with case-conflicting paths.</p>

<p><strong>Path length limits.</strong> Windows restricts paths to <a href="https://learn.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation">260 characters</a>, a constraint dating back to DOS. Git supports longer paths, but Git for Windows inherits the OS limitation. This is painful with deeply nested node_modules directories, where <code>git status</code> fails with “Filename too long” errors.</p>

<p><strong>Missing database features.</strong> Databases have CHECK constraints and UNIQUE constraints; git has nothing, so every package manager builds its own validation layer. Databases have locking; git doesn’t. Databases have indexes for queries like “all packages depending on X”; with git you either traverse every file or build your own index. Databases have migrations for schema changes; git has “rewrite history and force everyone to re-clone.”</p>

<p>The progression is predictable. Start with a flat directory of files. Hit filesystem limits. Implement sharding. Hit cross-platform issues. Build server-side enforcement. Build custom indexes. Eventually give up and use HTTP or an actual database. You’ve built a worse version of what databases already provide, spread across git hooks, CI pipelines, and bespoke tooling.</p>

<p>None of this means git is bad. Git excels at what it was designed for: distributed collaboration on source code, with branching, merging, and offline work. The problem is using it for something else entirely. Package registries need fast point queries for metadata. Git gives you a full-document sync protocol when you need a key-value lookup.</p>

<p>If you’re building a package manager and git-as-index seems appealing, look at Cargo, Homebrew, CocoaPods, vcpkg, Go. They all had to build workarounds as they grew, causing pain for users and maintainers. The pull request workflow is nice. The version history is nice. You will hit the same walls they did.</p>

  </div>

  
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ChatGPT conversations still lack timestamps after years of requests (120 pts)]]></title>
            <link>https://community.openai.com/t/timestamps-for-chats-in-chatgpt/440107?page=3</link>
            <guid>46391472</guid>
            <pubDate>Fri, 26 Dec 2025 12:39:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://community.openai.com/t/timestamps-for-chats-in-chatgpt/440107?page=3">https://community.openai.com/t/timestamps-for-chats-in-chatgpt/440107?page=3</a>, See on <a href="https://news.ycombinator.com/item?id=46391472">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/DiscussionForumPosting">
      <meta itemprop="headline" content="Timestamps for chats in ChatGPT">
      
      <meta itemprop="datePublished" content="2023-10-19T00:50:09Z">
        <meta itemprop="articleSection" content="Feature requests">
      <meta itemprop="keywords" content="">
      

          <meta itemprop="text" content="Can we get timestamps for chats generated in ChatGPT? I understand it currently categorizes chats into day, week, months etc. but can we get actual time-of-day stamps for each message? E.g 1:20pm, 5:00am, 3:34am, etc. MM&amp;hellip;">

          <div id="post_43" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>I’ve been working in a single chat for close 9-10 months now, with some separate chats opened once in a while. Now I want to write up a chronological overview of what I’ve been up to and when these past 10 months and I fully expected “<em>the timestamps in the ChatGPT chat</em>” to make that task trivial… It’s madness that this information isn’t available.</p>

            


            
          </div>
          <div itemprop="comment" id="post_45" itemscope="" itemtype="http://schema.org/Comment">
              <p>I know this thread has been open for a while (going on a year and a half now), and this might be obvious to some experienced users — but just in case, I wanted to share a workaround I’ve been using until this feature is officially added.</p>
<p>In <strong>Settings &gt; Customize ChatGPT</strong>, under the field that asks <em>“What traits should ChatGPT have?”</em>, I added:</p>
<blockquote>
<p>“After every response, add the current timestamp.”</p>
</blockquote>
<p>Now each reply ends with a timestamp, which makes it much easier to track what was said and when, especially in ongoing conversations. Not a perfect solution, but it’s been working pretty well for me.</p>
            </div>
          <div id="post_46" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://community.openai.com/u/DGGLMT"><span itemprop="name">DGGLMT</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-06-01T02:06:32Z">
                    June 1, 2025,  2:06am
                  </time>
                  <meta itemprop="dateModified" content="2025-06-01T02:06:32Z">
              <span itemprop="position">46</span>
              </span>
            </p>
            <div itemprop="text">
              <p>Well I see this is still going on. Clearly this feature is in demand, I can only assume there is a technical reason they don’t already have it, because it seems like a given at this point…</p>
<p>But now that they’re trying to push GPT as MORE than just an AI, but a journal/life assistant/coach etc etc, and chat histories are adding up…I mean I revisited an old chat to ask a follow up question about a life event, and it didn’t realize 3 months had gone by, so was giving me advice like it had just happened.</p>
<p>The…time…has come OpenAI. Chop, chop</p>
            </div>

            


            
          </div>
          <div itemprop="comment" id="post_47" itemscope="" itemtype="http://schema.org/Comment">
              <p>Yes, it’s totally possible! Every <code>&lt;div&gt;</code> that has a <code>"data-message-id"</code> contains an internal object with some message information, including the timestamp. I created this simple script that you can copy and paste directly into the Chrome DevTools console.</p>
<p>Just right-click anywhere on the page and choose <strong>“Inspect”</strong>, then go to the <strong>“Console”</strong> tab and paste this code:</p>
<pre><code>document.querySelectorAll('div[data-message-id]').forEach(div =&gt; {
  const reactKey = Object.keys(div).find(k =&gt; k.startsWith('__reactFiber$'));
  if (!reactKey) return;

  const fiber = div[reactKey];
  const messages = fiber?.return?.memoizedProps?.messages;
  const timestamp = messages?.[0]?.create_time;
  if (!timestamp) return;

  const date = new Date(timestamp * 1000);
  const format = n =&gt; n.toString().padStart(2, '0');
  const formatted = `${format(date.getDate())}/${format(date.getMonth() + 1)}/${date.getFullYear()} - ${format(date.getHours())}:${format(date.getMinutes())}:${format(date.getSeconds())}`;

  const span = document.createElement('span');
  span.textContent = formatted;
  div.insertBefore(span, div.firstChild);
});
</code></pre>
<p>If you get an error message saying you’re not allowed to paste code in the console, just type <code>"allow pasting"</code> and press Enter — then paste the script and press Enter again.</p>
<p>This is a very simple code I just made to view the timestamp of some chat messages, but it was enough for my needs.</p>
            </div>
          <div itemprop="comment" id="post_48" itemscope="" itemtype="http://schema.org/Comment">
              <p>I created a chrome extension for the timestamp. Chrome store approval is still pending but you can just download the repo and load the extension in developer mode to use it.</p>
<div><a href="https://us1.discourse-cdn.com/openai1/original/4X/d/c/0/dc0edf1537908e252d5002f185988ce74013e6c6.jpeg" data-download-href="/uploads/short-url/voIU8dybRa2K1DCA6Ad3eygebhI.jpeg?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://us1.discourse-cdn.com/openai1/optimized/4X/d/c/0/dc0edf1537908e252d5002f185988ce74013e6c6_2_690x466.jpeg" alt="image" data-base62-sha1="voIU8dybRa2K1DCA6Ad3eygebhI" width="690" height="466" srcset="https://us1.discourse-cdn.com/openai1/optimized/4X/d/c/0/dc0edf1537908e252d5002f185988ce74013e6c6_2_690x466.jpeg, https://us1.discourse-cdn.com/openai1/optimized/4X/d/c/0/dc0edf1537908e252d5002f185988ce74013e6c6_2_1035x699.jpeg 1.5x, https://us1.discourse-cdn.com/openai1/optimized/4X/d/c/0/dc0edf1537908e252d5002f185988ce74013e6c6_2_1380x932.jpeg 2x" data-dominant-color="F7F8F8"></a></div>
<p>Thanks for the inspiration by <a href="https://community.openai.com/u/rafaelsgoncalvesbh2">@rafaelsgoncalvesbh2</a>.</p>
            </div>
          <div itemprop="comment" id="post_49" itemscope="" itemtype="http://schema.org/Comment">
              <p>That’s awesome! I just installed the extension and it works perfectly.</p>
<p>For anyone who wants to use Hangzhi’s timestamp extension before it’s officially approved on the Chrome Web Store, here’s how you can install it manually:</p>
<ol>
<li>
<p>Download the ZIP file from the GitHub repo:</p>
<ul>
<li>GitHub page: github[.]com/Hangzhi/chatgpt-timestamp-extension</li>
<li>Direct download link: github[.]com/Hangzhi/chatgpt-timestamp-extension/raw/main/chatgpt-timestamp-extension.zip</li>
</ul>
<p>(Note: Since direct links aren’t allowed here, just replace <code>[.]</code> with <code>.</code> and paste the link into your browser.)</p>
</li>
<li>
<p>After downloading, right-click the ZIP file and select “Extract to chatgpt-timestamp-extension/”. This will create a folder named <code>chatgpt-timestamp-extension</code>.</p>
</li>
<li>
<p>Open <code>chrome://extensions/</code> in your browser.</p>
</li>
<li>
<p>In the top-right corner, enable “Developer mode”.</p>
</li>
<li>
<p>After enabling it, new options will appear. Click “Load unpacked”, then select the folder you just extracted (<code>chatgpt-timestamp-extension</code>).</p>
</li>
<li>
<p>That’s it! You should now see a new extension called “ChatGPT Timestamp” in your Chrome extensions. Open ChatGPT, and you’ll see timestamps above each message.</p>
</li>
</ol>
<p>Thanks again <a href="https://community.openai.com/u/hangzhi">@Hangzhi</a> for the great work and for turning the idea into a real Chrome extension!</p>
            </div>
          <div itemprop="comment" id="post_50" itemscope="" itemtype="http://schema.org/Comment">
              <p>Hi OpenAI Team,</p>
<p>I recently ran into a technical issue with my PC and discussed it with ChatGPT. A few days later, I needed to <strong>find out exactly when I posted a specific message</strong>, because that moment correlated with when the problem occurred. Unfortunately, there’s no way to see when individual messages were sent or received, which made it hard to trace the timing.</p>
<p>This would also benefit:</p>
<ul>
<li>Anyone using ChatGPT to track ongoing issues</li>
<li>Users referencing previous decisions or drafts</li>
<li>Those working across multiple sessions and needing historical context</li>
</ul>
<p>I’d love to see this added in a future update.</p>
<p>Best,<br>
Ralf</p>
            </div>
          <div id="post_51" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>I just wanted to add my vote to adding individual-message timestamps in ChatGPT.  We all really appreciate the work you do and the amazing resource you have created.  This is a simple feature request that would really help a lot of people.  Thanks!</p>

            


            
          </div>
          <div id="post_52" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://community.openai.com/u/guido"><span itemprop="name">guido</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-11-12T14:42:02Z">
                    November 12, 2025,  2:42pm
                  </time>
                  <meta itemprop="dateModified" content="2025-11-12T14:42:02Z">
              <span itemprop="position">52</span>
              </span>
            </p>
            <p>+1 for timestamps. It’s important so we can keep a sense of time with each thread of conversation. thanks.</p>

            


            
          </div>
          <div id="post_53" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://community.openai.com/u/po24"><span itemprop="name">po24</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-11-21T21:03:01Z">
                    November 21, 2025,  9:03pm
                  </time>
                  <meta itemprop="dateModified" content="2025-11-21T21:03:01Z">
              <span itemprop="position">53</span>
              </span>
            </p>
            <p>It’s been 2 years (and 3 since release). The UI/UX team needs to get their act together.</p>

            


            
          </div>
          <div id="post_54" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://community.openai.com/u/stab"><span itemprop="name">stab</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-11-24T16:56:59Z">
                    November 24, 2025,  4:56pm
                  </time>
                  <meta itemprop="dateModified" content="2025-11-24T16:56:59Z">
              <span itemprop="position">54</span>
              </span>
            </p>
            <p>They really do need to. Timestamps and bookmarks on specific messages are highly needed</p>

            


            
          </div>
          <div id="post_55" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>Actually : this may become a Compliance requirement in the future - to document (in business environment) when a certain user (company employee / senior officer) created certain query .</p>

            


            
          </div>
          <div id="post_56" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://community.openai.com/u/po24"><span itemprop="name">po24</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-02T23:15:28Z">
                    December 2, 2025, 11:15pm
                  </time>
                  <meta itemprop="dateModified" content="2025-12-02T23:15:28Z">
              <span itemprop="position">56</span>
              </span>
            </p>
            <div itemprop="text">
              <p>I don’t think Compliance requirement will make a difference because they already have the timestamps, but it’s just not shown to the user in a user-friendly manner. If you inspect the webpage, you’ll find it. It’s already being tracked, so displaying it should be extremely easy for OpenAI devs.</p>
<p>This post, among many others, is a strong sign that OpenAI is currently not prioritizing user-feedback on existing products. Too focused on building new products.</p>
            </div>

            


            
          </div>
          <div itemprop="comment" id="post_57" itemscope="" itemtype="http://schema.org/Comment">
              <p>Yep! I too suggest adding optional timestamps to ChatGPT conversations.</p>
<p>For long-running or sensitive threads, having the exact time a message was sent improves clarity, emotional context, and continuity. It helps both the model and the user avoid misunderstandings about timing (like, night vs. day, time between replies, or multi-day situations).</p>
<p>This can also support users with health issues, legal timelines, documentation, or crisis patterns by allowing more accurate context.</p>
<p>A simple toggle to show or hide timestamps would make this feature useful without overwhelming the interface.</p>
            </div>
          <div id="post_58" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://community.openai.com/u/stab"><span itemprop="name">stab</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-22T00:11:10Z">
                    December 22, 2025, 12:11am
                  </time>
                  <meta itemprop="dateModified" content="2025-12-22T00:11:10Z">
              <span itemprop="position">58</span>
              </span>
            </p>
            <p>I just wish we were treated like grown-ups, where we would have a bunch of toggles to activate/deactivate features that for other people might clutter the interface.</p>

            


            
          </div>
          <div id="post_59" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            
            <p>Yes, please, add opinion to make timestamps by ChatGPT, maybe some checkbox in options?</p>

            


            
          </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'm a laptop weirdo and that's why I like my new Framework 13 (168 pts)]]></title>
            <link>https://blog.matthewbrunelle.com/im-a-laptop-weirdo-and-thats-why-i-like-my-new-framework-13/</link>
            <guid>46391410</guid>
            <pubDate>Fri, 26 Dec 2025 12:27:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.matthewbrunelle.com/im-a-laptop-weirdo-and-thats-why-i-like-my-new-framework-13/">https://blog.matthewbrunelle.com/im-a-laptop-weirdo-and-thats-why-i-like-my-new-framework-13/</a>, See on <a href="https://news.ycombinator.com/item?id=46391410">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
<main>

        <article>

        

    <div>
        <p>This month I sold my 2021 M1 Max Macbook Pro and bought a Framework 13 DIY Edition laptop. After I got everything setup I sat down to write about the experience. Some ~4500 words later I realized I needed to break my thoughts into multiple posts.</p>
<p>See also:</p>
<ul>
<li><a href="https://blog.matthewbrunelle.com/framework-13-diy-edition-hardware-thoughts/">Framework 13 DIY Edition Hardware Thoughts</a></li>
<li><a href="https://blog.matthewbrunelle.com/setting-up-my-new-framework-laptop-13-diy-edition-with-nixos/">Setting up my new Framework Laptop 13 DIY Edition with NixOS</a></li>
</ul>

<p>My new Framework 13 laptop just arrived. After I finally set everything up I started writing a post about the experience. I thought I'd write a <em>little bit</em> about my previous laptops, but a lot of fond memories I had forgotten about came flooding back. The tinkerings and many openings of laptops past. If you will indulge me, I've been feeling nostalgic. This is for the other laptop weirdos out there that that feel the same.</p>
<h3 id="i-have-a-history-of-doing-terrible-acts-to-laptops">I have a history of doing terrible acts to laptops</h3>
<p><img src="https://blog.matthewbrunelle.com/content/images/2025/08/25654_1350884488762_5177004_n.jpg" alt="25654_1350884488762_5177004_n.jpg" loading="lazy"><br>
The only image I could find of my NC10 was this blurry, 2021 flip phone photo of me removing the windows sticker.</p>
<p>In 2008, I managed to get my hands on a Samsung NC10 Netbook in a fancy metallic blue color. [^ Back when netbooks where a thing circle 2007-2013] Prior to this I only had desktops. The specs were pretty humble (<a href="https://en.wikipedia.org/wiki/Samsung_NC10?ref=blog.matthewbrunelle.com">from wikipedia</a>):</p>
<ul>
<li>A single core 1.6 GHz Intel Atom N270</li>
<li>Integrated Intel GMA 950 graphics</li>
<li>1 GB DDR2 RAM</li>
<li>10.2 inch 1024x600 screen and a <em>VGA connector</em> of all things.</li>
<li>83-key keyboard rather than the usual 87 or 88 keys on a laptop.</li>
<li>A 160 GB HDD</li>
</ul>
<p>Something could be done about that though! You could <a href="https://replacethatpart.com/how-to-upgrade-memory-ram-of-the-samsung-nc10-a-complete-guide-2/?ref=blog.matthewbrunelle.com">upgrade</a> the RAM to a powerful 2GB. You could replace the slow HDD with an SSD. You could <a href="https://liliputing.com/touchscreen-kit-for-the-samsung-nc10-and-other-netbooks/?ref=blog.matthewbrunelle.com">add a touch screen</a>. You could <a href="https://www.atmasphere.net/archives/2009/01/15/the-samsung-nc10-hackintosh?ref=blog.matthewbrunelle.com">make a Hackintosh</a> out of it if you replaced the wifi card. If you wanted to, you could do those things and I was a weirdo, so I did!</p>
<p>I found a lot of fun in trying to get as much as I could out of that hardware. In fact I'd say the act of doing all that was far more enjoyable than actually using the laptop once the tinkering was done. After the novelty and slowness of a Hackintosh wore off I put Linux on the Netbook. I still sought the thrill of the hunt.</p>
<p>I installed a lite weight distro <a href="https://crunchbang.org/?ref=blog.matthewbrunelle.com">CrunchBang</a> [^ or just #!] and messed around. I read more about different minimalist distros and came across two others I could hop to: Arch and Gentoo. This feels like an inflection point in my life, I choose to try Arch since I wouldn't have to compile everything on a single core. [^ Who know what would have happened if I picked Gentoo. I might have a beard now.] The screen was small and I wanted to maximize its usefulness so I started trying tiling WMs. Why not <a href="https://xmonad.org/?ref=blog.matthewbrunelle.com">XMonad</a>?</p>
<p>It turns out the GMA950 was undervolted on the NC10. Someone made a shareware tool called the GMABooster that could restore the max clock rate. The original website <a href="http://www.gmabooster.com/home.htm?ref=blog.matthewbrunelle.com">http://www.gmabooster.com/home.htm</a> is long toast and not on wayback. This <a href="https://bbs.archlinux.org/viewtopic.php?id=81429&amp;ref=blog.matthewbrunelle.com">Arch forum thread</a> has details though:</p>
<blockquote>
<p>It allows a user, not a manufacturer to choose the desired GMA speed. It combines a sophisticated assembler-level technology and the user-friendly graphic user interface, offering You to near double the GMA core perfomance without even a need to restart a computer..</p>
</blockquote>
<p>The package was on AUR so I could squeeze out a little more performance. I could finally watch 480 YouTube videos instead of 360. At some point, long after I had stopped using the netbook, the AUR package became abandoned. I <a href="https://web.archive.org/web/20150802113711/https://aur.archlinux.org/packages/gmabooster/">adopted it as maintainer</a> and <a href="https://github.com/ciferkey/GMABooster_Linux?ref=blog.matthewbrunelle.com">mirrored</a> the binary in GitHub. This was the first time I ever was a package maintainer. [^ I am on a couple random packages in nixpkgs now.] Nowadays the package is memorialized in the the <a href="https://github.com/aur-archive/gmabooster?ref=blog.matthewbrunelle.com">AUR archive</a>.</p>
<p>I had a device that I could repeatedly break and remake. Did I do anything productive or meaningful with it? Absolutely not. Did I learn a lot in the process? I'd say so!</p>
<hr>
<h3 id="in-the-past-you-could-do-terrible-things-to-macbooks-too">In the past you could do terrible things to Macbooks too</h3>
<p>When I went to College I got a 2011 Macbook Pro. The kind that would overheat and desolder the GPU. [^ Some clever people have found hardware hacks to repair the problem <a href="https://www.jeffgeerling.com/blog/2017/fixing-2011-macbook-pro-booting-grey-screen-amd-radeon-video-glitch?ref=blog.matthewbrunelle.com">https://www.jeffgeerling.com/blog/2017/fixing-2011-macbook-pro-booting-grey-screen-amd-radeon-video-glitch</a>] Mine managed to last a long time and didn't need replacing until 2019. The RAM was not built-in yet on Macbooks. Apple said the model could only support up to 8GB total RAM, but you <a href="https://apple.stackexchange.com/a/259351?ref=blog.matthewbrunelle.com">could actually</a> get 16GB to work. Also, this was back when Macbooks had CD drives. I replaced the my drive with an <a href="https://eshop.macsales.com/item/Western%20Digital/DDMB5KT1.0/?ref=blog.matthewbrunelle.com">Other World Computing DIY Optical Drive to HDD Upgrade Kit</a>. [^ And you could put the drive into an "OWC SuperSlim" enclosure to turn it into a USB CD drive.] and installed SSDs in both slots. With two drives I was able to install rEFInd as a boot manager and triple boot:</p>
<ul>
<li>OSX as a stable install for my course work</li>
<li>Windows for games</li>
<li>Linux so I could break my install repeatedly</li>
</ul>
<p>I iterated on my Arch install so many times that I started to keep a checklist about my setup process to help me remember everything. Certain stylistic choices were set and still used to this day. [^ This is when I started using Inconsolata for a monospace font and Zenburn for a color scheme.] I couldn't change quite as many things about this laptop, but I still made an effort to change what I could.</p>
<hr>
<h3 id="as-laptops-grew-thinner-they-grew-more-boring">As laptops grew thinner they grew more boring</h3>
<p>When it came time for a new laptop I was not looking at Macbook Pros anymore. Apple had made changes, like the touch bar and removing magsafe, that felt like they were targeting a different audience. So instead I had been eyeing a ThinkPad.<br>
[^ It's almost cliche to buy one and install Linux.] The prices on the Lenovo store are mostly made up and constantly discounted. My housemate had access to a corpo portal for Lenovo that let me get one at a heavily reduced price. The cost of 3 year service coverage was also discounted so I got some figuring it could help to cover cost of parts if if something failed.</p>
<p>So I bought a Gen 7 X1 Carbon and... I just used it. No mods were possible on this laptop. When I had an SSD failure I asked Lenovo if they could mail me the drive so I could do the install. They said they had to send someone to confirm the issue. So a technician came out and replaced the drive.</p>
<hr>
<h3 id="the-gift-and-curse-of-a-free-macbook-pro">The gift and curse of a free Macbook Pro</h3>
<p>Finally in 2023 I was <a href="https://www.hubspot.com/company-news/a-message-from-hubspot-ceo-yamini-rangan?ref=blog.matthewbrunelle.com">laid off by HubSpot</a>. Part of severance was the following:</p>
<blockquote>
<p>Laptops &amp; WFH Set-Up: Impacted employees may keep their HubSpot laptops (it will be cleaned of any company data remotely), as well as any work from home gear like monitors and keyboards.</p>
</blockquote>
<p>Thus a pretty high spec 2021 M1 Max Macbook Pro fell into my lap. I gave my X1 Carbon to a friend to avoid creating yet more ewaste that sits in my closet.</p>
<p>The 2021 version was a bit of return to form: touch bar was gone, magsafe was back, etc. However even the <a href="https://www.ifixit.com/News/54122/macbook-pro-2021-teardown?utm_source=M1MBPTD&amp;utm_medium=Annotation&amp;utm_campaign=YouTube">iFixit review</a> said the "design represents a major move in the right direction" but still only rated the laptop a 4/10  for repairability. [^ The score was eventually updated to a 5/10 when Apple later released a service manual and access to parts.]</p>
<p>I felt some dissonance though. If I was looking to buy a laptop, I wouldn't have picked this one. macOS was getting less enjoyable to use with each update. Likewise the Linux Desktop experience was really coming into its own. [^ By 2023 essentially all my games were playable!] However I felt bad about buying a new laptop when I now had a perfectly good one. So I held onto it and once again, no mods were done or could be done with this laptop.</p>
<hr>
<h3 id="finally-buying-a-framework-13">Finally buying a Framework 13</h3>
<p>I had waited on getting a Framework laptop because I wanted to see them go through a couple iterations. I wanted to see if the promise of repairing, replacing and upgrading actually came true. From what I read it mostly has! [^ People with Framework 15 do seem to be waiting though.]</p>
<p>What changed the decision for me was the following:</p>
<ul>
<li>Lugging around a powerful 16 inch laptop was a drag. Having a laptop when traveling is nice if I need to hurriedly rebook something. Mobile sites and apps tend to restrict you in weird ways.</li>
<li>Despite being a couple years old, the laptop was still worth a lot. People probably want Macbooks for local LLM inference. So I felt pretty good a buyer will actually use the laptop.</li>
<li>The Framework release a refresh of the 13 with the new AMD chips.</li>
</ul>
<p>Then I had a friend get Laptop 13 and attest to liking it. That was the last push I needed to finally buy one. Now I can be a laptop weirdo again.</p>
<p>You can't change the RAM on laptops now.<br>
You can't change the SSD on laptops now.<br>
You can't easily repair the screen on laptops now.</p>
<p>You can do all that and more with a Framework laptop.<br>
You can be a laptop weirdo with a Framework laptop.</p>
<p>Weirdo typically has two interpretations:</p>
<blockquote>
<p>A possibly dangerous person.<br>
A strange, odd, eccentric person.</p>
</blockquote>
<p>To both of those I say: all us laptop weirdos can now put a <a href="https://community.frame.work/t/the-snack-drawer-store-now-made-with-real-snacks/43101?ref=blog.matthewbrunelle.com">snack drawer</a> in our laptops.<br>
You cannot stop us.</p>

    </div>
</article>
                
                

</main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rob Pike Goes Nuclear over GenAI (418 pts)]]></title>
            <link>https://imgur.com/nUJCI3o</link>
            <guid>46389444</guid>
            <pubDate>Fri, 26 Dec 2025 05:27:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://imgur.com/nUJCI3o">https://imgur.com/nUJCI3o</a>, See on <a href="https://news.ycombinator.com/item?id=46389444">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[TurboDiffusion: 100–200× Acceleration for Video Diffusion Models (156 pts)]]></title>
            <link>https://github.com/thu-ml/TurboDiffusion</link>
            <guid>46388907</guid>
            <pubDate>Fri, 26 Dec 2025 03:19:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/thu-ml/TurboDiffusion">https://github.com/thu-ml/TurboDiffusion</a>, See on <a href="https://news.ycombinator.com/item?id=46388907">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">TurboDiffusion</h2><a id="user-content-turbodiffusion" aria-label="Permalink: TurboDiffusion" href="#turbodiffusion"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/TurboDiffusion_Logo.png"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/TurboDiffusion_Logo.png" width="30%"></a>
</p>
<p dir="auto">This repository provides the official implementation of <strong>TurboDiffusion</strong>, a video generation acceleration framework that can speed up end-to-end diffusion generation by <math-renderer data-run-id="b6a8a1318f74547cd9421c8baa616925">$100 \sim 200\times$</math-renderer> on a single RTX 5090, while maintaining video quality.<br>
TurboDiffusion primarily uses <a href="https://github.com/thu-ml/SageAttention">SageAttention</a>, <a href="https://github.com/thu-ml/SLA">SLA (Sparse-Linear Attention)</a> for attention acceleration, and <a href="https://github.com/NVlabs/rcm">rCM</a> for timestep distillation.</p>
<p dir="auto">Paper: <a href="https://arxiv.org/pdf/2512.16093" rel="nofollow">TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times</a></p>
<p dir="auto"><strong>Note</strong>: the checkpoints and paper are not finalized, and will be updated later to improve quality.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/TurboDiffusion_speedup.png"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/TurboDiffusion_speedup.png" width="99%"></a>
</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/acceleration_decomposition.png"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/acceleration_decomposition.png" width="93%"></a>
</p>
<div dir="auto">
<markdown-accessiblity-table><table>
<tbody><tr>
<td>
<p>Original, E2E Time: 184s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/11.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/11.gif" width="387" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>1.9s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/11.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/11.gif" width="387" data-animated-image=""></a></p>
</td>
</tr>
</tbody></table></markdown-accessiblity-table><p>
An example of a <b>5-second video</b> generated by Wan-2.1-T2V-1.3B-480P on a single <b>RTX 5090</b>.
</p></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Available Models</h2><a id="user-content-available-models" aria-label="Permalink: Available Models" href="#available-models"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Model Name</th>
<th>Checkpoint Link</th>
<th>Best Resolution</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>TurboWan2.2-I2V-A14B-720P</code></td>
<td><a href="https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P" rel="nofollow">Huggingface Model</a></td>
<td>720p</td>
</tr>
<tr>
<td><code>TurboWan2.1-T2V-1.3B-480P</code></td>
<td><a href="https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P" rel="nofollow">Huggingface Model</a></td>
<td>480p</td>
</tr>
<tr>
<td><code>TurboWan2.1-T2V-14B-480P</code></td>
<td><a href="https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-14B-480P" rel="nofollow">Huggingface Model</a></td>
<td>480p</td>
</tr>
<tr>
<td><code>TurboWan2.1-T2V-14B-720P</code></td>
<td><a href="https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-14B-720P" rel="nofollow">Huggingface Model</a></td>
<td>720p</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Note: All checkpoints support generating videos at 480p or 720p. The "Best Resolution" column indicates the resolution at which the model provides the best video quality.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><strong>Base environment</strong>: <code>python&gt;=3.9</code>, <code>torch&gt;=2.7.0</code>. <code>torch==2.8.0</code> is recommended, as higher versions may cause OOM.</p>
<p dir="auto">Install TurboDiffusion by pip:</p>
<div dir="auto" data-snippet-clipboard-copy-content="conda create -n turbodiffusion python=3.12
conda activate turbodiffusion

pip install turbodiffusion --no-build-isolation"><pre>conda create -n turbodiffusion python=3.12
conda activate turbodiffusion

pip install turbodiffusion --no-build-isolation</pre></div>
<p dir="auto">Or compile from source:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/thu-ml/TurboDiffusion.git
cd TurboDiffusion
git submodule update --init --recursive
pip install -e . --no-build-isolation"><pre>git clone https://github.com/thu-ml/TurboDiffusion.git
<span>cd</span> TurboDiffusion
git submodule update --init --recursive
pip install -e <span>.</span> --no-build-isolation</pre></div>
<p dir="auto">To enable SageSLA, a fast SLA forward pass based on SageAttention, install <a href="https://github.com/thu-ml/SpargeAttn">SpargeAttn</a> first:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install git+https://github.com/thu-ml/SpargeAttn.git --no-build-isolation"><pre>pip install git+https://github.com/thu-ml/SpargeAttn.git --no-build-isolation</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inference</h2><a id="user-content-inference" aria-label="Permalink: Inference" href="#inference"></a></p>
<p dir="auto">For GPUs with more than 40GB of GPU memory, <strong>e.g., H100, please use the unquantized checkpoints (without <code>-quant</code>) and remove <code>--quant_linear</code> from the command. For RTX 5090, RTX 4090, or similar GPUs, please use the quantized checkpoints (with <code>-quant</code>) and add <code>--quant_linear</code> in the command.)</strong></p>
<ol dir="auto">
<li>
<p dir="auto">Download the VAE (<strong>applicable for both Wan2.1 and Wan2.2</strong>) and umT5 text encoder checkpoints:</p>
<div dir="auto" data-snippet-clipboard-copy-content="mkdir checkpoints
cd checkpoints
wget https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/resolve/main/Wan2.1_VAE.pth
wget https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/resolve/main/models_t5_umt5-xxl-enc-bf16.pth"><pre>mkdir checkpoints
<span>cd</span> checkpoints
wget https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/resolve/main/Wan2.1_VAE.pth
wget https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/resolve/main/models_t5_umt5-xxl-enc-bf16.pth</pre></div>
</li>
<li>
<p dir="auto">Download our quantized model checkpoints (For RTX 5090 or similar GPUs):</p>
<div dir="auto" data-snippet-clipboard-copy-content="# For Wan2.1-T2V-1.3B
wget https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P/resolve/main/TurboWan2.1-T2V-1.3B-480P-quant.pth

# For Wan2.2-I2V-14B
wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-high-720P-quant.pth
wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-low-720P-quant.pth"><pre><span><span>#</span> For Wan2.1-T2V-1.3B</span>
wget https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P/resolve/main/TurboWan2.1-T2V-1.3B-480P-quant.pth

<span><span>#</span> For Wan2.2-I2V-14B</span>
wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-high-720P-quant.pth
wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-low-720P-quant.pth</pre></div>
<p dir="auto"><strong>Or</strong> download our unquantized model checkpoints (For H100 or similar GPUs):</p>
<div dir="auto" data-snippet-clipboard-copy-content="# For Wan2.1-T2V-1.3B
wget https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P/resolve/main/TurboWan2.1-T2V-1.3B-480P.pth

# For Wan2.2-I2V-14B
wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-high-720P.pth
wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-low-720P.pth"><pre><span><span>#</span> For Wan2.1-T2V-1.3B</span>
wget https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P/resolve/main/TurboWan2.1-T2V-1.3B-480P.pth

<span><span>#</span> For Wan2.2-I2V-14B</span>
wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-high-720P.pth
wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-low-720P.pth</pre></div>
</li>
<li>
<p dir="auto">Use the inference script for the <strong>T2V</strong> models:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export PYTHONPATH=turbodiffusion

# Arguments:
# --dit_path            Path to the finetuned TurboDiffusion checkpoint
# --model               Model to use: Wan2.1-1.3B or Wan2.1-14B (default: Wan2.1-1.3B)
# --num_samples         Number of videos to generate (default: 1)
# --num_steps           Sampling steps, 1–4 (default: 4)
# --sigma_max           Initial sigma for rCM (default: 80); larger choices (e.g., 1600) reduce diversity but may enhance quality
# --vae_path            Path to Wan2.1 VAE (default: checkpoints/Wan2.1_VAE.pth)
# --text_encoder_path   Path to umT5 text encoder (default: checkpoints/models_t5_umt5-xxl-enc-bf16.pth)
# --num_frames          Number of frames to generate (default: 81)
# --prompt              Text prompt for video generation
# --resolution          Output resolution: &quot;480p&quot; or &quot;720p&quot; (default: 480p)
# --aspect_ratio        Aspect ratio in W:H format (default: 16:9)
# --seed                Random seed for reproducibility (default: 0)
# --save_path           Output file path including extension (default: output/generated_video.mp4)
# --attention_type      Attention module to use: original, sla or sagesla (default: sagesla)
# --sla_topk            Top-k ratio for SLA/SageSLA attention (default: 0.1), we recommend using 0.15 for better video quality
# --quant_linear        Enable quantization for linear layers, pass this if using a quantized checkpoint
# --default_norm        Use the original LayerNorm and RMSNorm of Wan models

python turbodiffusion/inference/wan2.1_t2v_infer.py \
    --model Wan2.1-1.3B \
    --dit_path checkpoints/TurboWan2.1-T2V-1.3B-480P-quant.pth \
    --resolution 480p \
    --prompt &quot;A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.&quot; \
    --num_samples 1 \
    --num_steps 4 \
    --quant_linear \
    --attention_type sagesla \
    --sla_topk 0.1"><pre><span>export</span> PYTHONPATH=turbodiffusion

<span><span>#</span> Arguments:</span>
<span><span>#</span> --dit_path            Path to the finetuned TurboDiffusion checkpoint</span>
<span><span>#</span> --model               Model to use: Wan2.1-1.3B or Wan2.1-14B (default: Wan2.1-1.3B)</span>
<span><span>#</span> --num_samples         Number of videos to generate (default: 1)</span>
<span><span>#</span> --num_steps           Sampling steps, 1–4 (default: 4)</span>
<span><span>#</span> --sigma_max           Initial sigma for rCM (default: 80); larger choices (e.g., 1600) reduce diversity but may enhance quality</span>
<span><span>#</span> --vae_path            Path to Wan2.1 VAE (default: checkpoints/Wan2.1_VAE.pth)</span>
<span><span>#</span> --text_encoder_path   Path to umT5 text encoder (default: checkpoints/models_t5_umt5-xxl-enc-bf16.pth)</span>
<span><span>#</span> --num_frames          Number of frames to generate (default: 81)</span>
<span><span>#</span> --prompt              Text prompt for video generation</span>
<span><span>#</span> --resolution          Output resolution: "480p" or "720p" (default: 480p)</span>
<span><span>#</span> --aspect_ratio        Aspect ratio in W:H format (default: 16:9)</span>
<span><span>#</span> --seed                Random seed for reproducibility (default: 0)</span>
<span><span>#</span> --save_path           Output file path including extension (default: output/generated_video.mp4)</span>
<span><span>#</span> --attention_type      Attention module to use: original, sla or sagesla (default: sagesla)</span>
<span><span>#</span> --sla_topk            Top-k ratio for SLA/SageSLA attention (default: 0.1), we recommend using 0.15 for better video quality</span>
<span><span>#</span> --quant_linear        Enable quantization for linear layers, pass this if using a quantized checkpoint</span>
<span><span>#</span> --default_norm        Use the original LayerNorm and RMSNorm of Wan models</span>

python turbodiffusion/inference/wan2.1_t2v_infer.py \
    --model Wan2.1-1.3B \
    --dit_path checkpoints/TurboWan2.1-T2V-1.3B-480P-quant.pth \
    --resolution 480p \
    --prompt <span><span>"</span>A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.<span>"</span></span> \
    --num_samples 1 \
    --num_steps 4 \
    --quant_linear \
    --attention_type sagesla \
    --sla_topk 0.1</pre></div>
<p dir="auto">Or the script for the <strong>I2V</strong> model:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export PYTHONPATH=turbodiffusion

# --image_path              Path to the input image
# --high_noise_model_path   Path to the high noise TurboDiffusion checkpoint
# --low_noise_model_path    Path to the high noise TurboDiffusion checkpoint
# --boundary                Timestep boundary for switching from high to low noise model (default: 0.9)
# --model                   Model to use: Wan2.2-A14B (default: Wan2.2-A14B)
# --num_samples             Number of videos to generate (default: 1)
# --num_steps               Sampling steps, 1–4 (default: 4)
# --sigma_max               Initial sigma for rCM (default: 200); larger choices (e.g., 1600) reduce diversity but may enhance quality
# --vae_path                Path to Wan2.2 VAE (default: checkpoints/Wan2.2_VAE.pth)
# --text_encoder_path       Path to umT5 text encoder (default: checkpoints/models_t5_umt5-xxl-enc-bf16.pth)
# --num_frames              Number of frames to generate (default: 81)
# --prompt                  Text prompt for video generation
# --resolution              Output resolution: &quot;480p&quot; or &quot;720p&quot; (default: 720p)
# --aspect_ratio            Aspect ratio in W:H format (default: 16:9)
# --adaptive_resolution     Enable adaptive resolution based on input image size
# --ode                     Use ODE for sampling (sharper but less robust than SDE)
# --seed                    Random seed for reproducibility (default: 0)
# --save_path               Output file path including extension (default: output/generated_video.mp4)
# --attention_type          Attention module to use: original, sla or sagesla (default: sagesla)
# --sla_topk                Top-k ratio for SLA/SageSLA attention (default: 0.1), we recommend using 0.15 for better video quality
# --quant_linear            Enable quantization for linear layers, pass this if using a quantized checkpoint
# --default_norm            Use the original LayerNorm and RMSNorm of Wan models

python turbodiffusion/inference/wan2.2_i2v_infer.py \
    --model Wan2.2-A14B \
    --low_noise_model_path checkpoints/TurboWan2.2-I2V-A14B-low-720P-quant.pth \
    --high_noise_model_path checkpoints/TurboWan2.2-I2V-A14B-high-720P-quant.pth \
    --resolution 720p \
    --adaptive_resolution \
    --image_path assets/i2v_inputs/i2v_input_0.jpg \
    --prompt &quot;POV selfie video, ultra-messy and extremely fast. A white cat in sunglasses stands on a surfboard with a neutral look when the board suddenly whips sideways, throwing cat and camera into the water; the frame dives sharply downward, swallowed by violent bursts of bubbles, spinning turbulence, and smeared water streaks as the camera sinks. Shadows thicken, pressure ripples distort the edges, and loose bubbles rush upward past the lens, showing the camera is still sinking. Then the cat kicks upward with explosive speed, dragging the view through churning bubbles and rapidly brightening water as sunlight floods back in; the camera races upward, water streaming off the lens, and finally breaks the surface in a sudden blast of light and spray, snapping back into a crooked, frantic selfie as the cat resurfaces.&quot; \
    --num_samples 1 \
    --num_steps 4 \
    --quant_linear \
    --attention_type sagesla \
    --sla_topk 0.1 \
    --ode"><pre><span>export</span> PYTHONPATH=turbodiffusion

<span><span>#</span> --image_path              Path to the input image</span>
<span><span>#</span> --high_noise_model_path   Path to the high noise TurboDiffusion checkpoint</span>
<span><span>#</span> --low_noise_model_path    Path to the high noise TurboDiffusion checkpoint</span>
<span><span>#</span> --boundary                Timestep boundary for switching from high to low noise model (default: 0.9)</span>
<span><span>#</span> --model                   Model to use: Wan2.2-A14B (default: Wan2.2-A14B)</span>
<span><span>#</span> --num_samples             Number of videos to generate (default: 1)</span>
<span><span>#</span> --num_steps               Sampling steps, 1–4 (default: 4)</span>
<span><span>#</span> --sigma_max               Initial sigma for rCM (default: 200); larger choices (e.g., 1600) reduce diversity but may enhance quality</span>
<span><span>#</span> --vae_path                Path to Wan2.2 VAE (default: checkpoints/Wan2.2_VAE.pth)</span>
<span><span>#</span> --text_encoder_path       Path to umT5 text encoder (default: checkpoints/models_t5_umt5-xxl-enc-bf16.pth)</span>
<span><span>#</span> --num_frames              Number of frames to generate (default: 81)</span>
<span><span>#</span> --prompt                  Text prompt for video generation</span>
<span><span>#</span> --resolution              Output resolution: "480p" or "720p" (default: 720p)</span>
<span><span>#</span> --aspect_ratio            Aspect ratio in W:H format (default: 16:9)</span>
<span><span>#</span> --adaptive_resolution     Enable adaptive resolution based on input image size</span>
<span><span>#</span> --ode                     Use ODE for sampling (sharper but less robust than SDE)</span>
<span><span>#</span> --seed                    Random seed for reproducibility (default: 0)</span>
<span><span>#</span> --save_path               Output file path including extension (default: output/generated_video.mp4)</span>
<span><span>#</span> --attention_type          Attention module to use: original, sla or sagesla (default: sagesla)</span>
<span><span>#</span> --sla_topk                Top-k ratio for SLA/SageSLA attention (default: 0.1), we recommend using 0.15 for better video quality</span>
<span><span>#</span> --quant_linear            Enable quantization for linear layers, pass this if using a quantized checkpoint</span>
<span><span>#</span> --default_norm            Use the original LayerNorm and RMSNorm of Wan models</span>

python turbodiffusion/inference/wan2.2_i2v_infer.py \
    --model Wan2.2-A14B \
    --low_noise_model_path checkpoints/TurboWan2.2-I2V-A14B-low-720P-quant.pth \
    --high_noise_model_path checkpoints/TurboWan2.2-I2V-A14B-high-720P-quant.pth \
    --resolution 720p \
    --adaptive_resolution \
    --image_path assets/i2v_inputs/i2v_input_0.jpg \
    --prompt <span><span>"</span>POV selfie video, ultra-messy and extremely fast. A white cat in sunglasses stands on a surfboard with a neutral look when the board suddenly whips sideways, throwing cat and camera into the water; the frame dives sharply downward, swallowed by violent bursts of bubbles, spinning turbulence, and smeared water streaks as the camera sinks. Shadows thicken, pressure ripples distort the edges, and loose bubbles rush upward past the lens, showing the camera is still sinking. Then the cat kicks upward with explosive speed, dragging the view through churning bubbles and rapidly brightening water as sunlight floods back in; the camera races upward, water streaming off the lens, and finally breaks the surface in a sudden blast of light and spray, snapping back into a crooked, frantic selfie as the cat resurfaces.<span>"</span></span> \
    --num_samples 1 \
    --num_steps 4 \
    --quant_linear \
    --attention_type sagesla \
    --sla_topk 0.1 \
    --ode</pre></div>
</li>
</ol>
<p dir="auto">Interactive inference via the terminal is available at <code>turbodiffusion/serve/</code>. This allows multi-turn video generation without reloading the model.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Evaluation</h2><a id="user-content-evaluation" aria-label="Permalink: Evaluation" href="#evaluation"></a></p>
<p dir="auto">We evaluate video generation on <strong>a single RTX 5090 GPU</strong>. The E2E Time refers to the end-to-end diffusion generation latency, excluding text encoding and VAE decoding.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Wan-2.2-I2V-A14B-720P</h3><a id="user-content-wan-22-i2v-a14b-720p" aria-label="Permalink: Wan-2.2-I2V-A14B-720P" href="#wan-22-i2v-a14b-720p"></a></p>
<markdown-accessiblity-table><table>
<tbody><tr>
<td>
<p>Original, E2E Time: 4549s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/original/A14B_720p/gif/0.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/original/A14B_720p/gif/0.gif" width="360" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>38s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/0.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/0.gif" width="360" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 4549s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/original/A14B_720p/gif/1.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/original/A14B_720p/gif/1.gif" width="360" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>38s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/1.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/1.gif" width="360" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 4549s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/original/A14B_720p/gif/2.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/original/A14B_720p/gif/2.gif" width="360" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>38s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/2.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/2.gif" width="360" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 4549s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/original/A14B_720p/gif/3.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/original/A14B_720p/gif/3.gif" width="360" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>38s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/3.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/3.gif" width="360" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 4549s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/original/A14B_720p/gif/4.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/original/A14B_720p/gif/4.gif" width="360" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>38s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/4.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/4.gif" width="360" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 4549s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/original/A14B_720p/gif/5.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/original/A14B_720p/gif/5.gif" width="360" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>38s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/5.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/5.gif" width="360" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 4549s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/original/A14B_720p/gif/6.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/original/A14B_720p/gif/6.gif" width="360" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>38s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/6.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/6.gif" width="360" data-animated-image=""></a></p>
</td>
</tr>
</tbody></table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Wan-2.1-T2V-1.3B-480P</h3><a id="user-content-wan-21-t2v-13b-480p" aria-label="Permalink: Wan-2.1-T2V-1.3B-480P" href="#wan-21-t2v-13b-480p"></a></p>
<markdown-accessiblity-table><table>
<tbody><tr>
<td>
<p>Original, E2E Time: 184s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/5.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/5.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>FastVideo, E2E Time: 5.3s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_1.3B/5.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_1.3B/5.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>1.9s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/5.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/5.gif" width="249" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 184s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/0.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/0.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>FastVideo, E2E Time: 5.3s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_1.3B/0.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_1.3B/0.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>1.9s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/0.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/0.gif" width="249" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 184s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/1.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/1.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>FastVideo, E2E Time: 5.3s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_1.3B/1.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_1.3B/1.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>1.9s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/1.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/1.gif" width="249" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 184s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/2.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/2.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>FastVideo, E2E Time: 5.3s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_1.3B/2.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_1.3B/2.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>1.9s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/2.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/2.gif" width="249" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 184s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/7.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/7.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>FastVideo, E2E Time: 5.3s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_1.3B/7.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_1.3B/7.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>1.9s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/7.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/7.gif" width="249" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 184s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/11.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/11.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>FastVideo, E2E Time: 5.3s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_1.3B/11.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_1.3B/11.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>1.9s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/11.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/11.gif" width="249" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 184s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/13.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/13.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>FastVideo, E2E Time: 5.3s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_1.3B/13.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_1.3B/13.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>1.9s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/13.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/13.gif" width="249" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 184s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/14.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/14.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>FastVideo, E2E Time: 5.3s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_1.3B/14.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_1.3B/14.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>1.9s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/14.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/14.gif" width="249" data-animated-image=""></a></p>
</td>
</tr>
</tbody></table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Wan-2.1-T2V-14B-720P</h3><a id="user-content-wan-21-t2v-14b-720p" aria-label="Permalink: Wan-2.1-T2V-14B-720P" href="#wan-21-t2v-14b-720p"></a></p>
<markdown-accessiblity-table><table>
<tbody><tr>
<td>
<p>Original, E2E Time: 4767s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/14B_720p/0.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/14B_720p/0.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>FastVideo, E2E Time: 72.6s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_14B_720p/0.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_14B_720p/0.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>24s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/14B_720p/0.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/14B_720p/0.gif" width="249" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 4767s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/14B_720p/3.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/14B_720p/3.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>FastVideo, E2E Time: 72.6s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_14B_720p/3.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_14B_720p/3.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>24s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/14B_720p/3.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/14B_720p/3.gif" width="249" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 4767s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/14B_720p/6.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/14B_720p/6.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>FastVideo, E2E Time: 72.6s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_14B_720p/6.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_14B_720p/6.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>24s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/14B_720p/6.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/14B_720p/6.gif" width="249" data-animated-image=""></a></p>
</td>
</tr>
</tbody></table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Wan-2.1-T2V-14B-480P</h3><a id="user-content-wan-21-t2v-14b-480p" aria-label="Permalink: Wan-2.1-T2V-14B-480P" href="#wan-21-t2v-14b-480p"></a></p>
<markdown-accessiblity-table><table>
<tbody><tr>
<td>
<p>Original, E2E Time: 1676s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/14B_480p/0.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/14B_480p/0.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>FastVideo, E2E Time: 26.3s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_14B_480p/0.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_14B_480p/0.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>9.9s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/14B_480p/0.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/14B_480p/0.gif" width="249" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 1676s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/14B_480p/1.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/14B_480p/1.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>FastVideo, E2E Time: 26.3s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_14B_480p/1.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_14B_480p/1.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>9.9s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/14B_480p/1.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/14B_480p/1.gif" width="249" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 1676s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/14B_480p/4.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/14B_480p/4.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>FastVideo, E2E Time: 26.3s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_14B_480p/4.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_14B_480p/4.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>9.9s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/14B_480p/4.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/14B_480p/4.gif" width="249" data-animated-image=""></a></p>
</td>
</tr>
<tr>
<td>
<p>Original, E2E Time: 1676s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/14B_480p/5.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/14B_480p/5.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>FastVideo, E2E Time: 26.3s</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_14B_480p/5.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_14B_480p/5.gif" width="249" data-animated-image=""></a></p>
</td>
<td>
<p>TurboDiffusion, E2E Time: <b>9.9s</b></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/14B_480p/5.gif"><img src="https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/14B_480p/5.gif" width="249" data-animated-image=""></a></p>
</td>
</tr>
</tbody></table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Training</h2><a id="user-content-training" aria-label="Permalink: Training" href="#training"></a></p>
<p dir="auto">In this repo, we provide training code based on Wan2.1 and its synthetic data. The training builds on the rCM codebase (<a href="https://github.com/NVlabs/rcm">https://github.com/NVlabs/rcm</a>), with infrastructure support including FSDP2, Ulysses CP, and selective activation checkpointing (SAC). For rCM training instructions, please refer to the original rCM repository; <a href="https://github.com/thu-ml/SLA">SLA (Sparse-Linear Attention)</a> training guidance is provided here.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Additional Installation</h4><a id="user-content-additional-installation" aria-label="Permalink: Additional Installation" href="#additional-installation"></a></p>
<p dir="auto">For rCM/SLA training, additionally run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install megatron-core hydra-core wandb webdataset
pip install --no-build-isolation transformer_engine[pytorch]"><pre>pip install megatron-core hydra-core wandb webdataset
pip install --no-build-isolation transformer_engine[pytorch]</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Checkpoints Downloading</h4><a id="user-content-checkpoints-downloading" aria-label="Permalink: Checkpoints Downloading" href="#checkpoints-downloading"></a></p>
<p dir="auto">Download the Wan2.1 pretrained checkpoints in <code>.pth</code> format and VAE/text encoder to <code>assets/checkpoints</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# make sure git lfs is installed
git clone https://huggingface.co/worstcoder/Wan assets/checkpoints"><pre><span><span>#</span> make sure git lfs is installed</span>
git clone https://huggingface.co/worstcoder/Wan assets/checkpoints</pre></div>
<p dir="auto">FSDP2 relies on <a href="https://docs.pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html" rel="nofollow">Distributed Checkpoint (DCP)</a> for loading and saving checkpoints. Before training, convert <code>.pth</code> teacher checkpoints to <code>.dcp</code> first:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m torch.distributed.checkpoint.format_utils torch_to_dcp assets/checkpoints/Wan2.1-T2V-1.3B.pth assets/checkpoints/Wan2.1-T2V-1.3B.dcp"><pre>python -m torch.distributed.checkpoint.format_utils torch_to_dcp assets/checkpoints/Wan2.1-T2V-1.3B.pth assets/checkpoints/Wan2.1-T2V-1.3B.dcp</pre></div>
<p dir="auto">After training, the saved <code>.dcp</code> checkpoints can be converted to <code>.pth</code> using the script <code>scripts/dcp_to_pth.py</code>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Dataset Downloading</h4><a id="user-content-dataset-downloading" aria-label="Permalink: Dataset Downloading" href="#dataset-downloading"></a></p>
<p dir="auto">We provide Wan2.1-14B-synthesized datasets. Download to <code>assets/datasets</code> using:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# make sure git lfs is installed
git clone https://huggingface.co/datasets/worstcoder/Wan_datasets assets/datasets"><pre><span><span>#</span> make sure git lfs is installed</span>
git clone https://huggingface.co/datasets/worstcoder/Wan_datasets assets/datasets</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Start Training</h4><a id="user-content-start-training" aria-label="Permalink: Start Training" href="#start-training"></a></p>
<p dir="auto">We implement white-box SLA training by aligning the predictions of the SLA-enabled model with those of the full-attention pretrained model. Unlike black-box training in the original paper, which tunes the pretrained model using diffusion loss, white-box training mitigates distribution shift and is less sensitive to the training data.</p>
<p dir="auto">Single-node training example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="WORKDIR=&quot;/your/path/to/turbodiffusion&quot;
cd $WORKDIR
export PYTHONPATH=turbodiffusion

# the &quot;IMAGINAIRE_OUTPUT_ROOT&quot; environment variable is the path to save experiment output files
export IMAGINAIRE_OUTPUT_ROOT=${WORKDIR}/outputs
CHECKPOINT_ROOT=${WORKDIR}/assets/checkpoints
DATASET_ROOT=${WORKDIR}/assets/datasets/Wan2.1_14B_480p_16:9_Euler-step100_shift-3.0_cfg-5.0_seed-0_250K

# your Wandb information
export WANDB_API_KEY=xxx
export WANDB_ENTITY=xxx

registry=registry_sla
experiment=wan2pt1_1pt3B_res480p_t2v_SLA

torchrun --nproc_per_node=8 \
    -m scripts.train --config=rcm/configs/${registry}.py -- experiment=${experiment} \
        model.config.teacher_ckpt=${CHECKPOINT_ROOT}/Wan2.1-T2V-1.3B.dcp \
        model.config.tokenizer.vae_pth=${CHECKPOINT_ROOT}/Wan2.1_VAE.pth \
        model.config.text_encoder_path=${CHECKPOINT_ROOT}/models_t5_umt5-xxl-enc-bf16.pth \
        model.config.neg_embed_path=${CHECKPOINT_ROOT}/umT5_wan_negative_emb.pt \
        dataloader_train.tar_path_pattern=${DATASET_ROOT}/shard*.tar"><pre>WORKDIR=<span><span>"</span>/your/path/to/turbodiffusion<span>"</span></span>
<span>cd</span> <span>$WORKDIR</span>
<span>export</span> PYTHONPATH=turbodiffusion

<span><span>#</span> the "IMAGINAIRE_OUTPUT_ROOT" environment variable is the path to save experiment output files</span>
<span>export</span> IMAGINAIRE_OUTPUT_ROOT=<span>${WORKDIR}</span>/outputs
CHECKPOINT_ROOT=<span>${WORKDIR}</span>/assets/checkpoints
DATASET_ROOT=<span>${WORKDIR}</span>/assets/datasets/Wan2.1_14B_480p_16:9_Euler-step100_shift-3.0_cfg-5.0_seed-0_250K

<span><span>#</span> your Wandb information</span>
<span>export</span> WANDB_API_KEY=xxx
<span>export</span> WANDB_ENTITY=xxx

registry=registry_sla
experiment=wan2pt1_1pt3B_res480p_t2v_SLA

torchrun --nproc_per_node=8 \
    -m scripts.train --config=rcm/configs/<span>${registry}</span>.py -- experiment=<span>${experiment}</span> \
        model.config.teacher_ckpt=<span>${CHECKPOINT_ROOT}</span>/Wan2.1-T2V-1.3B.dcp \
        model.config.tokenizer.vae_pth=<span>${CHECKPOINT_ROOT}</span>/Wan2.1_VAE.pth \
        model.config.text_encoder_path=<span>${CHECKPOINT_ROOT}</span>/models_t5_umt5-xxl-enc-bf16.pth \
        model.config.neg_embed_path=<span>${CHECKPOINT_ROOT}</span>/umT5_wan_negative_emb.pt \
        dataloader_train.tar_path_pattern=<span>${DATASET_ROOT}</span>/shard<span>*</span>.tar</pre></div>
<p dir="auto">Please refer to <code>turbodiffusion/rcm/configs/experiments/sla/wan2pt1_t2v.py</code> for the 14B config or perform modifications as needed.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Model Merging</h4><a id="user-content-model-merging" aria-label="Permalink: Model Merging" href="#model-merging"></a></p>
<p dir="auto">The parameter updates from SLA training can be merged into rCM checkpoints using <code>turbodiffusion/scripts/merge_models.py</code>, enabling rCM to perform sparse attention inference. Specify <code>--base</code> as the rCM model, <code>--diff_base</code> as the pretrained model, and <code>--diff_target</code> as the SLA-tuned model.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">ComfyUI Integration</h2><a id="user-content-comfyui-integration" aria-label="Permalink: ComfyUI Integration" href="#comfyui-integration"></a></p>
<p dir="auto">We thank the community effort <a href="https://github.com/anveshane/Comfyui_turbodiffusion">Comfyui_turbodiffusion</a> for integrating TurboDiffusion into ComfyUI.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<p dir="auto">We're actively working on the following features and improvements:</p>
<ul>
<li> Organize and release training code</li>
<li> Optimize infrastructure for better parallel</li>
<li> vLLM-Omni integration</li>
<li> Support for more video generation models</li>
<li> Support for autoregressive video generation models</li>
<li> More hardware-level operator optimizations</li>
</ul>
<p dir="auto">We welcome community members to help maintain and extend TurboDiffusion. Welcome to join the TurboDiffusion Team and contribute together!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto"><strong>If you use this code or find our work valuable, please cite:</strong></p>
<div data-snippet-clipboard-copy-content="@article{zhang2025turbodiffusion,
  title={TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times},
  author={Zhang, Jintao and Zheng, Kaiwen and Jiang, Kai and Wang, Haoxu and Stoica, Ion and Gonzalez, Joseph E and Chen, Jianfei and Zhu, Jun},
  journal={arXiv preprint arXiv:2512.16093},
  year={2025}
}

@software{turbodiffusion2025,
  title={TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times},
  author={The TurboDiffusion Team},
  url={https://github.com/thu-ml/TurboDiffusion},
  year={2025}
}

@inproceedings{zhang2025sageattention,
  title={SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration}, 
  author={Zhang, Jintao and Wei, Jia and Zhang, Pengle and Zhu, Jun and Chen, Jianfei},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025}
}

@article{zhang2025sla,
  title={SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention},
  author={Zhang, Jintao and Wang, Haoxu and Jiang, Kai and Yang, Shuo and Zheng, Kaiwen and Xi, Haocheng and Wang, Ziteng and Zhu, Hongzhou and Zhao, Min and Stoica, Ion and others},
  journal={arXiv preprint arXiv:2509.24006},
  year={2025}
}

@article{zheng2025rcm,
  title={Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency},
  author={Zheng, Kaiwen and Wang, Yuji and Ma, Qianli and Chen, Huayu and Zhang, Jintao and Balaji, Yogesh and Chen, Jianfei and Liu, Ming-Yu and Zhu, Jun and Zhang, Qinsheng},
  journal={arXiv preprint arXiv:2510.08431},
  year={2025}
}

@inproceedings{zhang2024sageattention2,
  title={Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization},
  author={Zhang, Jintao and Huang, Haofeng and Zhang, Pengle and Wei, Jia and Zhu, Jun and Chen, Jianfei},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2025}
}"><pre><code>@article{zhang2025turbodiffusion,
  title={TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times},
  author={Zhang, Jintao and Zheng, Kaiwen and Jiang, Kai and Wang, Haoxu and Stoica, Ion and Gonzalez, Joseph E and Chen, Jianfei and Zhu, Jun},
  journal={arXiv preprint arXiv:2512.16093},
  year={2025}
}

@software{turbodiffusion2025,
  title={TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times},
  author={The TurboDiffusion Team},
  url={https://github.com/thu-ml/TurboDiffusion},
  year={2025}
}

@inproceedings{zhang2025sageattention,
  title={SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration}, 
  author={Zhang, Jintao and Wei, Jia and Zhang, Pengle and Zhu, Jun and Chen, Jianfei},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025}
}

@article{zhang2025sla,
  title={SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention},
  author={Zhang, Jintao and Wang, Haoxu and Jiang, Kai and Yang, Shuo and Zheng, Kaiwen and Xi, Haocheng and Wang, Ziteng and Zhu, Hongzhou and Zhao, Min and Stoica, Ion and others},
  journal={arXiv preprint arXiv:2509.24006},
  year={2025}
}

@article{zheng2025rcm,
  title={Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency},
  author={Zheng, Kaiwen and Wang, Yuji and Ma, Qianli and Chen, Huayu and Zhang, Jintao and Balaji, Yogesh and Chen, Jianfei and Liu, Ming-Yu and Zhu, Jun and Zhang, Qinsheng},
  journal={arXiv preprint arXiv:2510.08431},
  year={2025}
}

@inproceedings{zhang2024sageattention2,
  title={Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization},
  author={Zhang, Jintao and Huang, Haofeng and Zhang, Pengle and Wei, Jia and Zhu, Jun and Chen, Jianfei},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2025}
}
</code></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MiniMax M2.1: Built for Real-World Complex Tasks, Multi-Language Programming (179 pts)]]></title>
            <link>https://www.minimaxi.com/news/minimax-m21</link>
            <guid>46388213</guid>
            <pubDate>Fri, 26 Dec 2025 01:02:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.minimaxi.com/news/minimax-m21">https://www.minimaxi.com/news/minimax-m21</a>, See on <a href="https://news.ycombinator.com/item?id=46388213">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><main><div><p><a href="https://www.minimaxi.com/"><img alt="MiniMax" fetchpriority="high" width="140" height="32" decoding="async" data-nimg="1" src="https://filecdn.minimax.chat/public/969d635c-cab6-45cc-8d61-47c9fe40c81f.png?x-oss-process=image/format,webp"></a></p></div><div><p><img alt="https://filecdn.minimax.chat/public/bbd9416c-f809-4195-88df-05fcd2051b15.png" fetchpriority="high" decoding="async" data-nimg="fill" src="https://filecdn.minimax.chat/public/bbd9416c-f809-4195-88df-05fcd2051b15.png"></p></div><div><p>MiniMax一直在用更AI 原生的方式改造自己。这个过程的核心驱动力是模型、Agent脚手架和组织。在探索过程中，我们对上述三点也有了越来越深刻的认知。今天我们开放模型部分的更新，也就是MiniMax M2.1，希望有机会能帮助到更多的企业和个人早日找到更AI原生的工作（生活）方式。</p><p>

在10月底的M2中，我们主要解决模型成本和模型开放性的问题。在M2.1中，我们致力于提升真实世界复杂任务中的表现：<strong>重点聚焦于更多编程语言和办公场景的可用性，并在这个领域做到最好的水平</strong>。</p></div><div><p><img alt="icon" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" src="https://filecdn.minimax.chat/public/a533807b-9ff6-4f86-a72e-8eb31d0245c1.png"></p></div><div><div 1.7;="" max-width:="" 800px;"="">

  <p>
    MiniMax M2.1 具体模型亮点如下:
  </p>
  
  <ul>
  
    <li>
      <p><strong>卓越多编程语言能力</strong></p>
      <p>过去很多模型主要围绕 Python 优化, 但真实世界的系统往往是多语言协作的结果。</p>
      <p>在 M2.1 中, 我们系统性提升了 Rust / Java / Golang / <span>C++</span> / Kotlin / Objective-C / TypeScript / <span>JavaScript</span> 等语言的能力, 多语言任务整体表现达到业内领先水平, 覆盖从底层系统到应用层开发的完整链路。
      </p>
    </li>
    
    <li>
      <p><strong>WebDev 与 AppDev：能力与美学的整体跃迁</strong></p>
      <p>针对业界普遍存在的移动端开发短板, M2.1 显著加强了原生 Android / iOS 开发能力。</p>
      <p>同时, 我们系统性提升了模型在 Web 与 App 场景中的设计理解与美学表达能力, 能够出色地构建复杂交互、3D科学场景模拟与高质量可视化表达, 推动 <span>vibe coding</span> 成为可持续、可交付的生产实践。
      </p>
    </li>
    
    <li>
      <p><strong>复合指令约束提升，办公场景变为可能</strong></p>
      <p>作为开源模型中<span>率先系统性</span>引入 Interleaved Thinking 的模型系列, M2.1 systematic problem-solving 能力再次升级。</p>
      <p>模型不仅关注代码执行是否正确, 同时关注模型对“复合指令约束”的整合执行能力, 在真实办公场景具备更高的可用性。
      </p>
    </li>
    
    <li>
      <p><strong>更简洁高效的回复</strong></p>
      <p>相比 M2, MiniMax-M2.1 的模型回复以及思维链更加简洁, 在实际编程与交互体验中, 响应速度显著提升, Token 消耗明显下降, 在 AI Coding与Agent驱动的连续工作流中更加流畅和高效。
      </p>
    </li>
    
    <li>
      <p><strong>出色的 <span>Agent / 工具脚手架泛化能力</span></strong></p>
      <p>M2.1 在各类编程工具与 Agent 框架中均有出色表现。在 Claude Code、Droid (Factory AI)、Cline、Kilo Code、Roo Code、BlackBox 等工具中展现一致且稳定的效果, 并对 Skill.md、Claude.md / agent.md / cursorrule、Slash Command 等 Context Management机制提供可靠支持。
      </p>
    </li>
    
    <li>
      <p><strong>高质量对话和写作</strong></p>
      <p>M2.1 不再只是“代码能力更强”, 在日常对话、技术说明与写作场景中, 也能提供更具细节与结构性的回答。
      </p>
    </li>
    
  </ul>

</div>
<br></div><div><p><img alt="" fetchpriority="high" decoding="async" data-nimg="fill" src="https://filecdn.minimax.chat/public/f0d35ee7-c17f-4515-a8b6-b78eccbb7603.png"></p></div><h3>基准测试概览</h3><p>在软件工程相关场景的核心榜单上，MiniMax-M2.1相比于M2有了显著的提升，尤其是在多语言场景上，超过 Claude Sonnet 4.5和Gemini 3 Pro，并接近Claude Opus 4.5。</p><div><p><img alt="" fetchpriority="high" decoding="async" data-nimg="fill" src="https://filecdn.minimax.chat/public/8b79e8e7-355a-4644-b06c-a627057ba3ec.png"></p></div><p>我们在不同coding agent框架上测试了SWE-bench Verified，结果表明MiniMax-M2.1具有良好的框架泛化性和稳定的表现。而在公开的测试用例生成、代码性能优化，以及自建的代码审阅、指令遵从等细分场景的榜单上，MiniMax-M2.1相比M2都表现出了全面的提升，持平或超过Claude Sonnet 4.5。</p><div><p><img alt="" fetchpriority="high" decoding="async" data-nimg="fill" src="https://filecdn.minimax.chat/public/30022d7b-722a-41f4-ba5c-af5d0642170e.png"></p></div><p>为了衡量模型“从零到一”构建完整、可运行应用程序的全栈能力，我们构建并开源了全新基准 <strong>VIBE (Visual &amp; Interactive Benchmark for Execution) 测试集</strong>，涵盖了 Web、仿真 (Simulation)、Android、iOS 及后端 (Backend) 五大核心子集。不同于传统基准，通过创新的 Agent-as-a-Verifier (AaaV) 范式，VIBE 能够自动评估生成的 Application 在真实运行环境中的交互逻辑与视觉美感。这个评测集稍后将在Github上开源。<br>
MiniMax-M2.1 在 VIBE 综合榜单中表现卓越，以平均 88.6 分的成绩展现了接近Claude Opus 4.5的全栈构建能力，并在几乎所有子集上都显著优于Claude Sonnet 4.5。</p><div><p><img alt="" fetchpriority="high" decoding="async" data-nimg="fill" src="https://filecdn.minimax.chat/public/543151ab-1d79-4ebf-93a1-191c4f982437.png"></p></div><p>在办公场景、长程工具调用和综合智能指数上，MiniMax-M2.1 相比 M2 也表现出稳步提升，体现了模型解决更多真实世界复杂任务的能力。</p><div><p><img alt="" fetchpriority="high" decoding="async" data-nimg="fill" src="https://filecdn.minimax.chat/public/97a7e3fd-0590-4af7-8b3a-f793d2f254fb.png"></p></div><h3>使用者评价</h3><p>过去几天，通过MiniMax全球的开发者社区，我们开放内测了M2.1的模型，也收到了非常多的热心反馈。下面是其中一些国际头部AI平台和合作方对MiniMax-M2.1的评价。</p><div><div><div><p><a target="_blank" rel="noopener noreferrer" href="https://factory.ai/"><img alt="Factory AI (Droid)" loading="lazy" decoding="async" data-nimg="fill" src="https://filecdn.minimax.chat/public/ab9f5f43-88d9-4a50-9c6b-a817eb509bd4.png"></a></p></div><p>我们非常期待像 M2.1 这样强大的开源模型，它在各类软件开发任务中都能带来前沿水准的表现，甚至还能在部分场景下比头部闭源模型更好。开发者应当拥有选择权，而 M2.1 正是大家急需的那个优质选项！</p><div><p><span>E</span></p><div><p>Eno Reyes</p><p>Co-Founder, CTO of Factory</p></div></div></div><div><div><p><a target="_blank" rel="noopener noreferrer" href="https://fireworks.ai/"><img alt="Fireworks" loading="lazy" decoding="async" data-nimg="fill" src="https://filecdn.minimax.chat/public/d4a5d010-bd28-432f-a20c-c967eadf62bd.png"></a></p></div><p>MiniMax M2.1 在可读性与惯用结构方面与生产级工程要求高度契合，在 Go、Rust、C++ 等多语言场景下均表现稳定。精炼的交错推理机制显著压缩逻辑路径，减少冗余步骤，让多文件重构与缺陷修复等复杂任务得以更高精度完成。更可贵的是，M2.1 在激活参数量受限的前提下仍能提供可靠性能，为大规模智能体编码流程提供了兼顾效能与资源利用的均衡方案。我们期待与 MiniMax 团队展开持续、紧密的合作，在 Fireworks 平台同步支持其最新创新成果！</p><div><p><span>B</span></p><div><p>Benny Chen</p><p>Co-Founder of Fireworks</p></div></div></div><div><div><p><a target="_blank" rel="noopener noreferrer" href="https://cline.bot/"><img alt="Cline" loading="lazy" decoding="async" data-nimg="fill" src="https://filecdn.minimax.chat/public/3cab5f2b-d6f7-4383-a43d-f34b05e4ec04.png"></a></p></div><p>MiniMax M2 系列在代码生成能力上表现突出，过去几个月已迅速跻身 Cline 平台最受欢迎的模型之列。M2.1 再次实现能力层面的显著跃升，我们期待与 MiniMax 团队继续深化合作，共同推进 AI 编码技术的演进。</p><div><p><span>S</span></p><div><p>Saoud Rizwan</p><p>Founder, CEO of Cline</p></div></div></div><div><div><p><a target="_blank" rel="noopener noreferrer" href="https://kilo.ai/"><img alt="Kilo" loading="lazy" decoding="async" data-nimg="fill" src="https://filecdn.minimax.chat/public/8618f6e3-caef-493c-a2df-aa42c0384593.png"></a></p></div><p>我们对M2.1的发布而兴奋！我们的用户已经离不开MiniMax提供的最优秀的编程辅助能力和高性价比，内测显示，M2.1在架构设计、服务编排、代码评审直至部署上线的全链路环节中均表现优异，速度与资源效率均处于领先水平。</p><div><p><span>S</span></p><div><p>Scott Breitenother</p><p>Co-Founder, CEO of Kilo</p></div></div></div><div><div><p><a target="_blank" rel="noopener noreferrer" href="https://roocode.com/"><img alt="RooCode" loading="lazy" decoding="async" data-nimg="fill" src="https://filecdn.minimax.chat/public/1abfdf2b-0962-4131-848f-09a1b7b1188c.png"></a></p></div><p>我们的用户非常喜欢 MiniMax M2 在编码能力与效率方面的表现。最新发布的 M2.1 在此基础上实现了速度与可靠性的实质性提升，并在更多语言及框架中保持稳定输出。对于强调高吞吐、Agentic Coding且对速度与成本敏感的研发流程，M2.1 是稳妥且具性价比的选择。</p><div><p><span>M</span></p><div><p>Matt Rubens</p><p>Co-Founder, CEO of RooCode</p></div></div></div><div><div><p><a target="_blank" rel="noopener noreferrer" href="https://www.blackbox.ai/"><img alt="BlackBox AI" loading="lazy" decoding="async" data-nimg="fill" src="https://filecdn.minimax.chat/public/dc94e1df-2ce1-4955-ae3e-0bc2c82b71c4.png"></a></p></div><p>将 MiniMax M2 系列集成到BlackBox平台对广大用户来说是一次巨大的利好！而 M2.1 的问世，更是定义了编程专用模型能力的新高度。 在实际测试中，我们惊喜地发现 M2.1 处理复杂、多步编程任务的细腻程度和一致性在同类模型中极其罕见。凭借其规模化提供的高质量推理和深度上下文感知能力，MiniMax 已然成为我们助力开发者更高效攻克技术难题的核心引擎。我们已经迫不及待地想看到开发者社区如何利用这些升级后的强大能力，创造出更多可能！</p><div><p><span>R</span></p><div><p>Robert Rizk</p><p>Co-Founder, CEO of BlackBox AI</p></div></div></div></div><h2>Showcases</h2><div><div><h2>物理世界Agent</h2></div><div><h2>物理世界Agent</h2><p>在模型具备足够好的泛化性之后，在虚拟世界中学习到的模型竟然还可以驱动物理世界的机器人，这有些出乎意料。以下是M2.1驱动的维他动力的机器狗。</p></div></div><div><div><h2>多语言 Coding</h2></div><div><div><h2>3D 交互式动画</h2><p>MiniMax M2.1 基于 React Three Fiber 与 InstancedMesh 构建的“3D 梦幻圣诞树”，成功渲染 7000+ 实例，支持手势交互与复杂粒子动画，展现高难度 3D 渲染能力。<br>
<a href="https://yuyl27wq92.space.minimax.io/" target="_blank" rel="noopener noreferrer">体验地址</a></p></div><div><h2>Web UI 前卫设计</h2><p>M2.1 可以生成极简主义摄影师个人主页，运用非对称布局与黑白红撞色设计，结合沉浸式影像和粗犷排版，实现了极具冲击力的视觉效果。</p></div><div><h2>原生 App 开发 - 安卓</h2><p>M2.1 使用 Kotlin 语言开发了一款安卓原生重力感应模拟器，利用陀螺仪传感器实现丝滑的重力操控体验，并设计巧妙的视觉彩蛋，通过自然的 UI 过渡与碰撞特效，优雅呈现“MERRY XMAS MiniMax M2.1”信息。</p></div><div><h2>原生 App 开发 - iOS</h2><p>M2.1 编写了 iOS 桌面交互小组件，设计了“沉睡圣诞老人”点击唤醒机制，逻辑完整且具备原生级的交互动画效果 -- 你的桌面小组件住进了圣诞老人——点他十下，他就醒来送你惊喜！ 🎅🎁
</p></div><div><h2>Web 音频模拟开发</h2><p>M2.1 基于 Web Audio API 开发了 16 步鼓机模拟器，集成了合成鼓声、非线性节奏算法与实时 Glitch 音效，提供了前卫的电子音乐体验！（可以开启以下视频的声音试听！）<br>
<a href="https://21okxwno2u.space.minimax.io/" target="_blank" rel="noopener noreferrer">体验地址</a></p></div><div><h2>Rust 安全审计 TUI</h2><p>M2.1 用 Rust 打造了 CLI + TUI 双模式的 Linux 安全审计利器，支持一键对进程、网络、SSH 等关键项的底层扫描与智能风险评级。</p></div><div><h2>Python 数据监控看板</h2><p>M2.1 基于 Python 开发了黑客帝国风格的实时数据监控面板，实现了对高频数据流的动态捕捉与渲染。该面板将核心指标转化为赛博朋克视觉符号，在满足实时监控精准度的同时，呈现了独特的科幻美学。</p></div><div><h2>C++ 制作图像渲染</h2><p>M2.1 利用 C++ 与 GLSL 实现了复杂光线传输算法，在实时环境下精准还原了水晶球的物理折射、SDF 细致建模的雪人以及波光粼粼的雪地特效。</p></div><div><h2>Java 制作实时弹幕</h2><p>M2.1 基于 Java 实现了高性能实时弹幕系统，简洁直观的用户界面及毫秒级响应能力。</p></div><div><h2>SVG 生成</h2><p>M2.1 生成了 SVG 交互式等轴风格岛屿地图，构建了细节丰富的微缩世界，支持一键缩放自由探索四大主题区域。</p></div></div></div><div><div><h2>Agentic Tool Use</h2></div><div><h2>Tool use（工具调用）能力：excel 市场调研</h2><p>M2.1 展示了 Tool use 能力，自主调用 Excel 工具和 yahoo finance 工具完成了从市场调研数据清洗、分析到图表生成的全流程任务。</p></div></div><h2>数字员工</h2><p>数字员工是 MiniMax M2.1 模型的重要功能之一。M2.1 能够接受以文字形式展示的网页内容，并以文本形态的输入控制鼠标的点击行为和键盘输入，在日常办公场景下，完成行政、数据科学、财务、人力资源、软件开发类的端到端任务。<br>
以下效果演示是 M2.1 在 AgentCompany Benchmark 中的行为轨迹记录。</p><div><div><h2>全链路办公自动化</h2></div><div><div><h2>效果演示1</h2><p>在通讯软件上主动收集员工的设备请求，然后前往企业内部服务器上搜索相关文档获取设备价格，计算总成本并判断部门预算是否充足，然后记录设备变更。</p></div><div><h2>效果演示2</h2><p>在项目管理软件上查找被阻塞或积压的问题，然后在通讯软件上查找相关员工并咨询解决方案，根据员工反馈更新问题的状态。</p></div><div><h2>效果演示3</h2><p>在代码库中查找答案：某同事希望知道改动了某个文件的最近的合并请求是哪一个，检索相关的合并请求，找到编号并告知该同事。</p></div></div></div><div>

  <h2>
    如何使用
  </h2>
  
  <ul>
    <li>
      MiniMax-M2.1 API 已在 <strong>MiniMax开放平台</strong> 开放使用：<a href="https://platform.minimaxi.com/docs/guides/text-generation" target="_blank" rel="noopener noreferrer">https://platform.minimaxi.com/docs/guides/text-generation</a>
    </li>
    <li>
      基于 MiniMax-M2.1 的通用 Agent 产品 <strong>MiniMax Agent</strong> 现已全面开放使用：<a href="https://agent.minimaxi.com/" target="_blank" rel="noopener noreferrer">https://agent.minimaxi.com/</a>
    </li>
    <li>
      开源以及本地部署使用：
<a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.1" target="_blank" rel="noopener noreferrer">https://huggingface.co/MiniMaxAI/MiniMax-M2.1</a> 
   <br>
<a href="https://github.com/MiniMax-AI/MiniMax-M2.1" target="_blank" rel="noopener noreferrer">https://github.com/MiniMax-AI/MiniMax-M2.1</a>




 </li>
  </ul>

  <p>
    为了方便用户使用, 我们提供了两个版本的 API, M2.1 和 M2.1-lightning。这两个 API 结果完全一样, 但是后者速度更快, 方便对 TPS 有需求的用户来使用。同时, 在 M2 手动 Cache 的基础上, M2.1 全面支持自动 Cache, 无需设置, 自动生效, 为开发者带来更流畅的体验、更低的成本与更优的延时表现。
  </p>
  
  <p>
    我们在 Coding Plan 里面会根据资源负载给用户提供大比例的 M2.1-lightning, 并保持 Coding Plan 的价格不变。也就是说, Coding Plan 用户免费获得了大部分时间更快的推理速度。<a href="https://platform.minimaxi.com/subscribe/coding-plan" target="_blank" rel="noopener noreferrer">欢迎大家点击下单~</a>
  </p>

</div><div><p><img alt="" fetchpriority="high" decoding="async" data-nimg="fill" src="https://filecdn.minimax.chat/public/83b98c98-a82f-4964-b5b0-3057472aa0f7.webp"></p></div><h3>联系我们</h3><div><p><img alt="" fetchpriority="high" decoding="async" data-nimg="fill" src="https://filecdn.minimax.chat/public/273f3442-f89d-4e02-b10b-4dc174a67242.png"></p></div></main><main><div><p><img alt="logo" loading="lazy" width="200" height="64" decoding="async" data-nimg="1" src="https://filecdn.minimax.chat/public/6bef0882-3057-455c-a4ad-2f63ed292be2.png?x-oss-process=image/format,webp"></p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Seven Diabetes Patients Die Due to Undisclosed Bug in Abbott's Glucose Monitors (371 pts)]]></title>
            <link>https://sfconservancy.org/blog/2025/dec/23/seven-abbott-freestyle-libre-cgm-patients-dead/</link>
            <guid>46388040</guid>
            <pubDate>Fri, 26 Dec 2025 00:29:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sfconservancy.org/blog/2025/dec/23/seven-abbott-freestyle-libre-cgm-patients-dead/">https://sfconservancy.org/blog/2025/dec/23/seven-abbott-freestyle-libre-cgm-patients-dead/</a>, See on <a href="https://news.ycombinator.com/item?id=46388040">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>




<p>by 
  on December 23, 2025
</p>




<p>I <a href="https://sfconservancy.org/blog/2025/nov/06/juggluco-foss-continuous-glucose-montior-diabetes/">wrote
    last month</a> about my diabetes diagnosis this year and my
  difficult choice to wear a proprietary device (called a
  <acronym title="continuous glucose monitor">CGM</acronym>) on my arm 24/7
  to continuously monitor my glucose levels. Like my friend and colleague,
  Karen M. Sandler — who previously made a much higher-stakes choice to
  receive a proprietary implanted defibrillator to keep her safe given her
  genetic heart condition — I reluctantly chose to attach proprietary
  hardware and software to my body.</p>

<p>The device itself is quite proprietary, but fortunately
    the <acronym title="Free and Open Source Software">FOSS</acronym>
    community has reverse engineered its activation and data collection
    protocols — creating an Android application that does a better job
    than the manufacturers' proprietary ones<sup><a id="return-juggluco-previous-post" href="#footnote-juggluco-previous-post">0</a></sup>.</p>

<p>Here in the USA, we strangely use capitalism as the center of our health care system. Two major for-profit competing brands of <acronym title="continuous glucose monitor">CGM</acronym> are
  available here.  My diabetes specialist prefers the (ironically named)
  Freestyle Libre Plus from Abbott.  I (also rather strangely) bring a prescription
  for <em>electronics</em> to a pharmacy every month. On 2025-12-03, that pharmacy sent me an alarming text message (shown here). </p>

<h3>Abbott Killed Seven Patients</h3>

<p>After reading that text, I
  found <a href="https://www.fda.gov/medical-devices/medical-device-recalls-and-early-alerts/early-alert-glucose-monitor-sensor-issue-abbott-diabetes-care">the
  USA <acronym title="Food and Drug Administration">FDA</acronym> announcement</a>.  My spouse cross-referenced the lot numbers while I read them off from all my Freestyle boxes<sup><a id="return-no-match-abbott-site" href="#footnote-no-match-abbott-site">1</a></sup>.  I had indeed recently worn an
impacted device!</p>

<p>Only because my diabetes is so early of a stage was I relatively safe.  The FDA
  reports that Freestyle injured over 700 people  and <strong>killed seven
  people</strong> with this bug.  Specifically, the bug caused the device to falsely report an
  <em>extremely low</em> glucose level.  Advanced stage diabetics use low
  reading information to inform them that they may have too much insulin
  currently.  The usual remedy is to eat something sugary to raise glucose in the blood. 
   Such should be done only with great care, as a false low reading can harm and even kill the patient (who eats a high-sugar-content item while glucose in the blood is, in fact, not low).</p>
  
<p>Proprietary software in medical devices harming patients is not new.  In
  1985,
  the <a href="https://en.wikipedia.org/wiki/Therac-25#Problem_description">Therac-25
  killed three people</a>.  In 2020, hundreds of patients who relied on a financially troubled tech startup found their occular implants suddenly unsupported. Some patients went <a href="https://spectrum.ieee.org/bionic-eye-obsolete">blind as the devices powered down without updates</a>.  There are more examples that I could include here, but rereading these horrific stories is frankly more than I can take right now when I think of fellow diabetes sufferers who were “killed by code” recently..</p>


<h3>Would FOSS Have Saved Patients' Lives?</h3>
                              
<p>It's hubris for activists to guarantee that harm would be prevented if Freestyle had publicly released the
  hardware specifications and the complete, corresponding source code
  (<abbr>CCS</abbr>).
  <acronym title="Free and Open Source Software">FOSS</acronym> isn't immune to bugs —
  even dangerous ones.  However, in the centuries since the Enlightenment, we
  know that the scientific method <em>depends</em> on public disclosure about
  data and wide-reaching peer review of past work.  FOSS (plus a publicly disclosed
  hardware design) wouid allow the millions of hardware and software engineers to
  peer-review the integrity, security, and safety of the devices to which
  patients entrust their lives.  We achieve the promise of humanity when we
  each entrust our safety and health to our entire community — not
  merely a single for-profit entity.</p>

<p>We also will probably never know whether this issue was in hardware or
  software.  The bug disclosure is incredibly vague, and it remains unclear
  how much investigation was done (if any) by government regulators into this
  problem.  As a public policy and public health matter, the
  public <em>deserves to know</em> the technical details (software and
  hardware) of both the functioning device and the failed devices.  NGOs should be permitted to perform
  their own investigations and confirmations of public safety.</p>

<h3>What's Next?</h3>

<p>Given that the hardware, software, and medical for-profit industries
refuse to put the rights, safety and security of patients first, wrongful
death lawsuits are typically the only way to hold these companies
  accountable.  Yet, there are <em>very few</em> people who have not agreed
Abbott's toxic terms of their
  proprietary companion application — I guestimate 
that fewer than 1% of
  Freestyle-using patients have used Juggluco from their very start (and
  thus never agreed to Abbott's terms).  This is significant because Abbott <a href="https://sfconservancy.org/videos/2025-09_Abbott-Freestyle-Libre-Plus-App-Agreement.pdf#page=78">includes a
  comprehensive one-way indemnity for themselves in the terms</a>.  I hope that a
  class action suit begins soon on this matter, but I wonder and worry that
  so much of the class may have signed this indemnity (which may make the road to justice
  bumpier).</p>

<p>Finally, I want to offer that if there is anyone out there who does
  tear-downs of extremely tiny electronic devices, I would be thrilled to
  find a volunteer who would like to see if we can either extract any
  software components from the device, or reverse-engineer the hardware.  I
  have saved and sanitized all of my prior <acronym title="continuous glucose monitors">CGMs</acronym>.
  I'd gladly send one along
  to anyone who wants to give a try at taking them apart. (Contact SFC or <a rel="me" href="https://fedi.copyleft.org/@bkuhn">contact me on the Fediverse (via Mastodon)</a> if you're available to do this work.)</p>

<p>For my part, I look forward (after the <a href="https://sfconservancy.org/vizio">Vizio</a> trial) to
 sending some patches to
  Juggluco and also getting Juggluco available in F-Droid.  Our best option in the face of
  these powerful medical device companies curtailing our rights is to invest
  our volunteer time into the edges where
  <acronym title="Free and Open Source Software">FOSS</acronym> has
  resiliently worked around the constant roadblocks erected by bad
  actors.</p>

  

  <hr>
<p><sup><a id="footnote-juggluco-previous-post" href="#return-juggluco-previous-post">0</a></sup>My <a href="https://sfconservancy.org/blog/2025/nov/06/juggluco-foss-continuous-glucose-montior-diabetes/">prior post about <acronym title="continuous glucose monitors">CGMs</acronym> discussed</a> the GPLv3'd Juggluco in more detail.</p>

<p><sup><a id="footnote-no-match-abbott-site" href="#return-no-match-abbott-site">1</a></sup> In a fascinating turn of events, at least one of my past monitors (of which I fortitously saved all the boxes with the lot/serial number on them) is listed in <a href="https://www.fda.gov/media/189900/download?attachment">the FDA's spreadsheet</a> as recalled lot, yet the serial number is listed as “ safe to use” on <a href="https://www.freestylecheck.com/us-en/product-lookup.html">Abbott's webform</a>  🤔 … I'm left wondering how I can trust Abbott to write reliable software stuck into my arm if they can't even write a web form that cross-references serial numbers to lots correctly 😬. </p>




<p><a href="https://sfconservancy.org/blog/2025/dec/23/seven-abbott-freestyle-libre-cgm-patients-dead/">[permalink]</a></p>







</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Maybe the default settings are too high (761 pts)]]></title>
            <link>https://www.raptitude.com/2025/12/maybe-the-default-settings-are-too-high/</link>
            <guid>46387657</guid>
            <pubDate>Thu, 25 Dec 2025 23:13:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.raptitude.com/2025/12/maybe-the-default-settings-are-too-high/">https://www.raptitude.com/2025/12/maybe-the-default-settings-are-too-high/</a>, See on <a href="https://news.ycombinator.com/item?id=46387657">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img src="https://www.raptitude.com/wp-content/uploads/2025/12/spedometer.jpg" width="1200" height="800" alt="Post image for Maybe the Default Settings Are Too High"></p>
<p>I’ve been reading <em>Lord of the Rings</em> for two months and I’m just at the end of the first part. It’s not because I’m not enjoying it. It’s one of the most enjoyable reading experiences I can remember.</p>



<p>From the beginning, I’ve read the whole thing aloud. I’ve found reading <a href="https://www.raptitude.com/2024/09/in-favor-of-reading-aloud/" data-type="post" data-id="13233">aloud helpful</a> for staying engaged — limiting myself to mouth-speed rather than eye-speed means I won’t rush, miss important details, and then lose interest, which has <a href="https://www.raptitude.com/2021/10/how-to-level-up-instead-of-plugging-away/" data-type="post" data-id="11131">always been a problem</a> for me.</p>



<p>At first I was anxious to read a 1,500-page book this way, because it would take so long. But, as someone pointed out to me, if I’m enjoying it, why would I want to be done with it sooner?</p>



<p>So I tried slowing down <em>even more</em>, and discovered something. I slowed to a pace that felt almost absurd, treating each sentence as though it might be a particularly important one. I gave each one maybe triple the usual time and attention, ignoring the fact that there are hundreds of pages to go.</p>



<p>This leisurely pace made Middle-Earth blossom before my eyes. When I paused after each comma, and let each sentence ring for a small moment after the period, the events of the story reached me with more weight and strength. That extra time gave space for Tolkien’s images and moods to propagate in my mind, which they did automatically.</p>



<p>Some part of me still wanted to rush and get on with it, to make good time, to gloss over the songs and lore to get to Moria and Mount Doom and the other marquee moments of the story. But the more I ignored that impulse, the better the experience got.</p>


<div>
<figure><a href="https://www.raptitude.com/wp-content/uploads/2025/12/lotr-text.jpg"><img fetchpriority="high" decoding="async" width="300" height="179" src="https://www.raptitude.com/wp-content/uploads/2025/12/lotr-text-300x179.jpg" alt="" srcset="https://www.raptitude.com/wp-content/uploads/2025/12/lotr-text-300x179.jpg 300w, https://www.raptitude.com/wp-content/uploads/2025/12/lotr-text-292x174.jpg 292w, https://www.raptitude.com/wp-content/uploads/2025/12/lotr-text.jpg 600w" sizes="(max-width: 300px) 100vw, 300px"></a><figcaption><em>Images just waiting to propagate</em></figcaption></figure></div>


<p>By offering the book about triple the usual amount of attentiveness, I was getting about triple the <em>storyness</em> (i.e. meaning, engagement, literary pleasure). Whatever the thing is that I’m seeking when I pick up a novel in the first place, there’s much more of it available at this pace.</p>



<h3>Eating Comprehension</h3>



<p>This effect reminded me of a paradox around eating I recognized long ago. When you slow down your eating speed, say to half or a third your default speed, you get much more enjoyment out of a smaller amount of food. The extra attention given to each bite allows more of the “good stuff,” whatever that is exactly, to reach you.</p>



<p>What’s paradoxical is that it’s precisely the seeking of that “good stuff” that normally drives me to eat so quickly, and miss most of what I’m seeking. When you try to barrel ahead to access the good stuff quicker, you get less of it in the end. Slow down and much more of it is released.</p>


<div>
<figure><a href="https://www.raptitude.com/wp-content/uploads/2025/12/cookie.jpg"><img decoding="async" width="300" height="292" src="https://www.raptitude.com/wp-content/uploads/2025/12/cookie-300x292.jpg" alt="" srcset="https://www.raptitude.com/wp-content/uploads/2025/12/cookie-300x292.jpg 300w, https://www.raptitude.com/wp-content/uploads/2025/12/cookie-197x192.jpg 197w, https://www.raptitude.com/wp-content/uploads/2025/12/cookie.jpg 663w" sizes="(max-width: 300px) 100vw, 300px"></a><figcaption><em>I have so much love to give, if you would just take your time</em></figcaption></figure></div>


<p>And it’s released automatically, in both reading and eating. You don’t have to search it out. The good stuff (the meaning in the text, the pleasure in the eating) just rises up to meet you in that extra time you give it. Slowing down, and offering more time to the act of consumption, immediately increases reading comprehension (and eating comprehension).</p>



<p>Both are analogous to slowing down while you vacuum a carpet. If you pass the vacuum head too quickly, you miss half the dirt. Slow down, and you can hear how much more grit is sent skittering up the tube. The suction and bristles are working, but they need more time to do their work fully, to draw up the deeper-lying stuff.</p>


<div>
<figure><a href="https://www.raptitude.com/wp-content/uploads/2025/12/vacuum.jpg"><img decoding="async" width="300" height="200" src="https://www.raptitude.com/wp-content/uploads/2025/12/vacuum-300x200.jpg" alt="" srcset="https://www.raptitude.com/wp-content/uploads/2025/12/vacuum-300x200.jpg 300w, https://www.raptitude.com/wp-content/uploads/2025/12/vacuum-288x192.jpg 288w, https://www.raptitude.com/wp-content/uploads/2025/12/vacuum.jpg 600w" sizes="(max-width: 300px) 100vw, 300px"></a><figcaption>Wants a chance to be all it can be</figcaption></figure></div>


<h3>Question the default settings</h3>



<p>It seems that my default consumption speeds for reading and eating (and maybe everything else) reduce the rewards of those things significantly, undermining the point of doing either.</p>



<p>Part of it is my own impatience. But I also suspect that modern living, with its infinite supply of consumables, tends to push our rate-of-intake dials too high. I’m not going to run out of books, or snacks, or opportunities to learn something. There’s always more, so not every crust of bread or printed page needs to be appreciated fully.</p>



<p>Internally though, the mind is juggling like <a href="https://youtu.be/A2x8N4DjxnE?si=V6lHIhJjRtfodtO3&amp;t=64">Lucy and Ethel on the conveyor belt at the chocolate factory</a>. Our receptors for meaning and appreciation, like the vacuum head, need more time to do their full work, to make all the connections they’re designed to make.</p>


<div>
<figure><a href="https://www.raptitude.com/wp-content/uploads/2025/12/lucy-chocolate-factory.jpg"><img loading="lazy" decoding="async" width="300" height="175" src="https://www.raptitude.com/wp-content/uploads/2025/12/lucy-chocolate-factory-300x175.jpg" alt="" srcset="https://www.raptitude.com/wp-content/uploads/2025/12/lucy-chocolate-factory-300x175.jpg 300w, https://www.raptitude.com/wp-content/uploads/2025/12/lucy-chocolate-factory-768x449.jpg 768w, https://www.raptitude.com/wp-content/uploads/2025/12/lucy-chocolate-factory-292x171.jpg 292w, https://www.raptitude.com/wp-content/uploads/2025/12/lucy-chocolate-factory.jpg 1000w" sizes="auto, (max-width: 300px) 100vw, 300px"></a><figcaption><em>Your mind, reading Dostoevsky like it’s Stephen King</em></figcaption></figure></div>


<p>It might sound like I’m just offering clichés – less is more, stop and smell the roses, take your time – and I guess I am. But clichés suffer the same issue: they are often profound insights, consumed and passed on too rapidly for their real meaning to register anymore. You really should stop and smell roses, as you know if you’re in the habit of doing that.</p>



<p>At least see what happens when you reduce your consumption speed – of anything, but especially books, information, and food – by a half, or two thirds. Notice that (1) something in you really wants to plow through at the highest viable setting, and (2) how much more of the reward is released when you slow down anyway.</p>


<div>
<figure><a href="https://www.raptitude.com/wp-content/uploads/2025/12/dune-slow-blade.jpg"><img loading="lazy" decoding="async" width="860" height="366" src="https://www.raptitude.com/wp-content/uploads/2025/12/dune-slow-blade.jpg" alt="" srcset="https://www.raptitude.com/wp-content/uploads/2025/12/dune-slow-blade.jpg 860w, https://www.raptitude.com/wp-content/uploads/2025/12/dune-slow-blade-300x128.jpg 300w, https://www.raptitude.com/wp-content/uploads/2025/12/dune-slow-blade-768x327.jpg 768w, https://www.raptitude.com/wp-content/uploads/2025/12/dune-slow-blade-292x124.jpg 292w" sizes="auto, (max-width: 860px) 100vw, 860px"></a><figcaption><em>Only the slow blade penetrates the classic novel</em></figcaption></figure></div>


<p>As far as I can tell, almost everything becomes more satisfying when you give it more time and intention, even things like checking the mailbox or writing a shopping list.</p>



<h3>Speed alters taste</h3>



<p>Slowing down your rate of consumption will inevitably change what you <em>want</em> to consume. Reading throwaway news articles or AI slop with great care and attention is only going to show you how empty of value it is. Reading dense writing in inky old books, crafted for your mind by great masters, becomes easier without the rushed pace, and the meaning just blooms out of it.</p>



<p>Same with food. Try to savor a cheap, waxy “chocolate” bar, or a bag of store-brand cheese puffs, and you discover a harsh taste that you don’t want to look at too closely. Enjoy a homemade pastry with great attention, and discover there’s even more in it than you realized.</p>


<div>
<figure><a href="https://www.raptitude.com/wp-content/uploads/2025/12/cheapchocolate.jpg"><img loading="lazy" decoding="async" width="300" height="136" src="https://www.raptitude.com/wp-content/uploads/2025/12/cheapchocolate-300x136.jpg" alt="" srcset="https://www.raptitude.com/wp-content/uploads/2025/12/cheapchocolate-300x136.jpg 300w, https://www.raptitude.com/wp-content/uploads/2025/12/cheapchocolate-292x132.jpg 292w, https://www.raptitude.com/wp-content/uploads/2025/12/cheapchocolate.jpg 600w" sizes="auto, (max-width: 300px) 100vw, 300px"></a><figcaption><em>You do not want to look closer</em></figcaption></figure></div>


<p>Mass production is good in so many ways, but the faster we tend to consume its fruits, the more we end up seeking things for their glossy, candied surfaces. The more we go for these surface-level rewards, the more the culture focuses on offering only that part – such as TikTok videos, processed food, CGI-forward movies, and public discourse in the form of unexamined talking points.</p>



<p>Who knows how far we’ve drifted from the best modes of consuming the things we value. Once something becomes a norm, it seems like an appropriate standard, no matter how much has been lost. Apparently, <a href="https://bookriot.com/when-reading-went-silent/">reading silently and alone was unusual</a> until as late as the 18<sup>th</sup> century. Certainly sit-down meals and cooking at home were.</p>


<div>
<figure><a href="https://www.raptitude.com/wp-content/uploads/2025/12/medieval-text.jpg"><img loading="lazy" decoding="async" width="209" height="300" src="https://www.raptitude.com/wp-content/uploads/2025/12/medieval-text-209x300.jpg" alt="" srcset="https://www.raptitude.com/wp-content/uploads/2025/12/medieval-text-209x300.jpg 209w, https://www.raptitude.com/wp-content/uploads/2025/12/medieval-text-712x1024.jpg 712w, https://www.raptitude.com/wp-content/uploads/2025/12/medieval-text-768x1105.jpg 768w, https://www.raptitude.com/wp-content/uploads/2025/12/medieval-text-133x192.jpg 133w, https://www.raptitude.com/wp-content/uploads/2025/12/medieval-text.jpg 800w" sizes="auto, (max-width: 209px) 100vw, 209px"></a><figcaption><em>Nobody reading this at 50 pages an hour</em></figcaption></figure></div>


<p>I don’t mean to sound like a scold. Let’s say none of this is morally good or bad. It’s just that in so much of what we do, we could be getting much more of the part of it that we really seek — but it’s only available at slower speeds.</p>



<p>If you’re curious, try consuming things more slowly, so slowly it seems silly to others — say a third your habitual speed — and see what rises up to meet you.</p>



<p>***</p>



<h2>Want to quit something in January?</h2>



<p>Recently I opened a discussion forum for Raptitude readers who want to give something up for the month of December (alcohol, social media, snacks, etc).</p>



<p>It’s been a real success, and many people want to do something similar in January. If you want to quit something, or just give it up for a month, you’re invited to join.</p>



<p>Follow this link at the end of <a href="https://www.raptitude.com/2025/11/in-favor-of-giving-things-up/" data-type="post" data-id="13795">this post</a> to get an invite.</p>

            

        				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google is 'gradually rolling out' option to change your gmail.com address (227 pts)]]></title>
            <link>https://9to5google.com/2025/12/24/google-change-gmail-addresses/</link>
            <guid>46387192</guid>
            <pubDate>Thu, 25 Dec 2025 21:36:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://9to5google.com/2025/12/24/google-change-gmail-addresses/">https://9to5google.com/2025/12/24/google-change-gmail-addresses/</a>, See on <a href="https://news.ycombinator.com/item?id=46387192">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>

	<img width="1600" height="800" src="https://9to5google.com/wp-content/uploads/sites/4/2023/12/gmail-android-1.jpg?quality=82&amp;strip=all&amp;w=1600" alt="" srcset="https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/12/gmail-android-1.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/12/gmail-android-1.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/12/gmail-android-1.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/12/gmail-android-1.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high">
	</figure>

<p>On a Google support page, the company says it is rolling out a new option to let users change their email address even if it is an “@gmail.com” address.</p>



<ul>
<li><strong>Related</strong>: <a href="https://9to5google.com/2025/12/23/google-one-2026-offer/">Google One discounting annual 2 TB and AI Pro plans by 50%</a></li>
</ul>



<p>For quite some time now, Google has allowed users to change their account email address if they are using a third-party email address, but users with an “@gmail.com” address are left unable to change it, as Google <a href="https://support.google.com/accounts/answer/19870?dark=0&amp;sjid=13129471057818824497-NA&amp;hl=en#zippy:~:text=If%20your%20account%27s%20email%20address%20ends%20in%20%40gmail.com%2C%20you%20usually%20can%27t%20change%20it.">says</a>:</p>



<blockquote>
<p>If your account’s email address ends in @gmail.com, you usually can’t change it.</p>
</blockquote>



<p>It appears this is changing.</p>



<p>On the same support page that currently says that you usually can’t change your email, Google is detailing a new process that is “gradually rolling out.” The altered page weirdly <a href="https://support.google.com/accounts/answer/19870?dark=0&amp;sjid=13129471057818824497-NA&amp;hl=hi#zippy">only shows in Hindi </a>at the moment, meaning you can’t see the changes in English. Everything quoted below is translated. The page was first spotted in the “Google Pixel Hub” group <a href="https://t.me/PixelHubUpdates/4453">on Telegram</a>.</p>	
	



<p>Google explains:</p>



<blockquote>
<p>The email address associated with your Google Account is the address you use to sign in to Google services. This email address helps you and others identify your account. If you’d like, you can change your Google Account email address that ends in gmail.com to a new email address that ends in gmail.com.</p>
</blockquote>



<p>This is new functionality that Google hasn’t detailed elsewhere yet, but says is “gradually rolling out to all users.”</p>



<p>With this change, Google will allow users to change their “@gmail.com” email address to a new “@gmail.com” address with an altered username. After changing, Google details that your original email address will still receive emails at the same inbox as your new one and work for sign-in, and that none of your account access will change. Users will be unable to change or delete their email within 12 months of the change.</p>



<blockquote>
<p><strong>When you change your Google Account email address from an email address ending in gmail.com to a new email address ending in gmail.com:</strong></p>



<ul>
<li>The old email address in your Google Account that ends with gmail.com will be set as an alias.&nbsp;<a href="https://support.google.com/accounts?p=alternate_emails">Learn more about alias email addresses</a>&nbsp;.</li>



<li>You will receive emails at both your old and new email addresses.</li>



<li>Data saved in your account, including photos, messages, and emails sent to your old email address, will not be affected.</li>



<li>You can reuse your old Google Account email address at any time. However, you can’t create a new Google Account email address that ends with gmail.com for the next 12 months. You can’t delete your new email address either.</li>



<li>You can sign in to Google services like Gmail, Maps, YouTube, Google Play, or Drive with your old or new email address.&nbsp;</li>
</ul>
</blockquote>



<p>Each account can only change its “@gmail.com” address <a href="https://support.google.com/accounts/answer/19870?dark=0&amp;sjid=13129471057818824497-NA&amp;hl=hi#zippy&amp;zippy=%2C%E0%A4%85%E0%A4%AA%E0%A4%A8-google-%E0%A4%96%E0%A4%A4-%E0%A4%95-%E0%A4%88%E0%A4%AE%E0%A4%B2-%E0%A4%AA%E0%A4%A4-%E0%A4%95%E0%A4%A4%E0%A4%A8-%E0%A4%AC%E0%A4%B0-%E0%A4%AC%E0%A4%A6%E0%A4%B2-%E0%A4%9C-%E0%A4%B8%E0%A4%95%E0%A4%A4-%E0%A4%B9:~:text=up%20to%20a%20total%20of%20three%20times.">up to 3 times</a> for a total <a href="https://support.google.com/accounts/answer/19870?dark=0&amp;sjid=13129471057818824497-NA&amp;hl=hi#zippy&amp;zippy=%2C%E0%A4%85%E0%A4%AA%E0%A4%A8-google-%E0%A4%96%E0%A4%A4-%E0%A4%95-%E0%A4%88%E0%A4%AE%E0%A4%B2-%E0%A4%AA%E0%A4%A4-%E0%A4%95%E0%A4%A4%E0%A4%A8-%E0%A4%AC%E0%A4%B0-%E0%A4%AC%E0%A4%A6%E0%A4%B2-%E0%A4%9C-%E0%A4%B8%E0%A4%95%E0%A4%A4-%E0%A4%B9%2C%E0%A4%95%E0%A4%AF-google-%E0%A4%96%E0%A4%A4-%E0%A4%95-%E0%A4%B2%E0%A4%8F-%E0%A4%A8%E0%A4%8F-%E0%A4%88%E0%A4%AE%E0%A4%B2-%E0%A4%AA%E0%A4%A4-%E0%A4%AC%E0%A4%A8%E0%A4%A8-%E0%A4%95-%E0%A4%95%E0%A4%88-%E0%A4%B8%E0%A4%AE-%E0%A4%B9:~:text=Yes%2C%20you%20can%20create%20up%20to%20three%20new%20email%20addresses%20for%20your%20Google%20Account%20that%20end%20with%20gmail.com.%20This%20way%2C%20you%27ll%20have%20a%20total%20of%20four%20email%20addresses.">of 4 addresses</a>.</p>



<p>Google further details that your old Gmail address will still appear in some cases, and “won’t be immediately reflected in older instances” such as events on Calendar created before the change. You’ll also still be able to send emails from the old address. The old address remains yours and cannot be used by another user.</p>




	<p>The page is very detailed on the process, but the changes just aren’t live yet. Presumably, this support page detailing the process in Hindi went up a little earlier than intended, but it certainly seems that we’ll be hearing more about this change in the coming weeks.</p>



<p>When the functionality goes live, users will be able to change their Gmail address <a href="http://myaccount.google.com/google-account-email">via “My Account.” </a></p>



<p>We’ll update this article if further information comes out.</p>



<h2 id="h-more-on-gmail">More on Gmail:</h2>



<ul>
<li><a href="https://9to5google.com/2025/11/24/google-says-gemini-isnt-trained-on-gmail-misleading-reports/">Google says Gemini isn’t trained on Gmail, pushing back on ‘misleading reports’</a></li>



<li><a href="https://9to5google.com/2025/11/21/gmail-photo-notifications/">Gmail for Android notifications adding photo &amp; attachment previews</a></li>



<li><a href="https://9to5google.com/2025/11/16/gmail-unread-dots/">Gmail adding unread dots as Google Tasks gets deadlines</a></li>
</ul>



<p><em><strong>Follow Ben:</strong>&nbsp;<a href="https://twitter.com/NexusBen" target="_blank" rel="noreferrer noopener">Twitter/X</a>,&nbsp;<a href="https://www.threads.net/@nexusben" target="_blank" rel="noreferrer noopener">Threads</a>, <a href="https://bsky.app/profile/nexusben.com">Bluesky</a>, and&nbsp;<a href="https://www.instagram.com/nexusben" target="_blank" rel="noreferrer noopener">Instagram</a></em></p>
	<p><a target="_blank" rel="nofollow" href="https://google.com/preferences/source?q=https://9to5google.com" aria-label="Add 9to5Google as a preferred source on Google">
			<img decoding="async" src="https://9to5google.com/wp-content/themes/ninetofive/dist/images/google-preferred-source-badge-dark.png" alt="Add 9to5Google as a preferred source on Google">
			<img decoding="async" src="https://9to5google.com/wp-content/themes/ninetofive/dist/images/google-preferred-source-badge-light.png" alt="Add 9to5Google as a preferred source on Google">
		</a>
	</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://9to5mac.com/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UBlockOrigin and UBlacklist AI Blocklist (133 pts)]]></title>
            <link>https://github.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist</link>
            <guid>46386761</guid>
            <pubDate>Thu, 25 Dec 2025 20:14:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist">https://github.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist</a>, See on <a href="https://news.ycombinator.com/item?id=46386761">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">uBlockOrigin &amp; uBlacklist Huge AI Blocklist</h2><a id="user-content-ublockorigin--ublacklist-huge-ai-blocklist" aria-label="Permalink: uBlockOrigin &amp; uBlacklist Huge AI Blocklist" href="#ublockorigin--ublacklist-huge-ai-blocklist"></a></p>
<p dir="auto">A huge blocklist of manually curated sites (1000+) that contain AI generated content, for the purposes of cleaning image search engines (Google Search, DuckDuckGo, and Bing) with uBlock Origin or uBlacklist.</p>
<p dir="auto">Also works on mobile (<a href="#iOS-iPadOS-Safari-only">iOS, iPadOS,</a> <a href="#Android-via-Firefox">Android</a>) via uBlacklist, as well as pihole/adguard (via <a href="#hosts-file-for-pi-holeadguard">Hosts file</a>)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">PC/Desktop installation</h2><a id="user-content-pcdesktop-installation" aria-label="Permalink: PC/Desktop installation" href="#pcdesktop-installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installing it with uBlock Origin</h3><a id="user-content-installing-it-with-ublock-origin" aria-label="Permalink: Installing it with uBlock Origin" href="#installing-it-with-ublock-origin"></a></p>
<p dir="auto"><strong>One-click import (any platform)</strong></p>
<p dir="auto">If you have uBlock Origin installed, click <a href="https://subscribe.adblockplus.org/?location=https%3A%2F%2Fraw.githubusercontent.com%2Flaylavish%2FuBlockOrigin-HUGE-AI-Blocklist%2Fmain%2Flist.txt&amp;title=Sites%20using%20AI%20generated%20content" rel="nofollow">this link</a> to import the filter list in just a click! Quick and simple.</p>
<p dir="auto"><strong>Manual Import</strong></p>
<ol dir="auto">
<li>
<p dir="auto">Make sure that you have the uBlock Origin Extension for <a href="https://addons.mozilla.org/en-US/firefox/addon/ublock-origin/" rel="nofollow">Firefox</a>, <a href="https://chromewebstore.google.com/detail/ublock-origin/cjpalhdlnbpafiamejdnhcphjbkeiagm" rel="nofollow">Chrome</a>, or any browser that supports uBlock Origin</p>
</li>
<li>
<p dir="auto">Click on the uBlock Origin Extension, and in the bottom right, there is a cog-wheel symbol--named the dashboard. Click it.</p>
</li>
<li>
<p dir="auto">Once you are in the dashboard, look towards the top. Click on the tab that says "Filter lists".</p>
</li>
<li>
<p dir="auto">Look towards the bottom, and expand the <code>Import</code> button.</p>
</li>
<li>
<p dir="auto">Copy and paste this URL into the dialogue box:</p>
</li>
</ol>
<div data-snippet-clipboard-copy-content="https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/list.txt"><pre><code>https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/list.txt
</code></pre></div>
<ol start="6" dir="auto">
<li>Apply changes, and you're set!</li>
</ol>
<details>
<summary>Here's a video guide on how to do this (click the dropdown to expand) </summary>
<br>
<details open="">
  <summary>
    
    <span>tutorial_ubo.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/128162304/471086517-c379a750-53eb-4813-8cea-757f34ab5a2d.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjY3NDE3MDcsIm5iZiI6MTc2Njc0MTQwNywicGF0aCI6Ii8xMjgxNjIzMDQvNDcxMDg2NTE3LWMzNzlhNzUwLTUzZWItNDgxMy04Y2VhLTc1N2YzNGFiNWEyZC5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUxMjI2JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MTIyNlQwOTMwMDdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1lZmFkNWZkNTIxM2FhMDc3MDA4ZGU5YWY4MWNjYjkxZWI0Y2UzNmZlM2QyM2Y3NDE0NTFmOTk1NWZkMjk2MjY2JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.iBZy89HrGaVr3iD0jNYbWC2uXEviWBxVVOxOedtwBOE" data-canonical-src="https://private-user-images.githubusercontent.com/128162304/471086517-c379a750-53eb-4813-8cea-757f34ab5a2d.mp4?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjY3NDE3MDcsIm5iZiI6MTc2Njc0MTQwNywicGF0aCI6Ii8xMjgxNjIzMDQvNDcxMDg2NTE3LWMzNzlhNzUwLTUzZWItNDgxMy04Y2VhLTc1N2YzNGFiNWEyZC5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUxMjI2JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MTIyNlQwOTMwMDdaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1lZmFkNWZkNTIxM2FhMDc3MDA4ZGU5YWY4MWNjYjkxZWI0Y2UzNmZlM2QyM2Y3NDE0NTFmOTk1NWZkMjk2MjY2JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.iBZy89HrGaVr3iD0jNYbWC2uXEviWBxVVOxOedtwBOE" controls="controls" muted="muted">

  </video>
</details>

</details>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">uBlock Origin will automatically refresh the filter list once a day, so you'll always have up-to-date filters.
If you want to force an update of the filter list, pressing the stopwatch next to the newly added list, then pressing <code>Update now</code> will achieve that.</p>
</div>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">If you find that your imported list isn't working, it may be due to an outdated web browsing session. If you haven't restarted your web browser for a long time, there's a chance the session won't update how it should, meaning importing the list into uBlock Origin or uBlacklist won't function correctly. Try creating a new session by closing <ins><strong>all</strong></ins> web browser windows, waiting until all processes are fully closed (4-5 second wait), then re-opening your web browser. That should help; if not, then try clearing your browser's cache.</p>
</div>
<hr>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installing it with uBlacklist</h3><a id="user-content-installing-it-with-ublacklist" aria-label="Permalink: Installing it with uBlacklist" href="#installing-it-with-ublacklist"></a></p>
<p dir="auto"><strong>One-click filter import (Only for Chrome, Firefox doesn't support one-click import)</strong></p>
<p dir="auto">If you use Google Chrome/Chromium and have <a href="https://chromewebstore.google.com/detail/ublacklist/pncfbmialoiaghdehhbnbhkkgmjanfhe" rel="nofollow">uBlacklist installed</a>, you can import the list in just one click. Click <a href="https://iorate.github.io/ublacklist/subscribe?name=Main+AI+Blocklist&amp;url=https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/list_uBlacklist.txt" rel="nofollow">this link</a> to automatically subscribe to the list.</p>
<p dir="auto"><strong>Manual Import</strong></p>
<ol dir="auto">
<li>
<p dir="auto">Make sure that you have the uBlacklist extension for <a href="https://addons.mozilla.org/en-US/firefox/addon/ublacklist/" rel="nofollow">Firefox</a>, <a href="https://chromewebstore.google.com/detail/ublacklist/pncfbmialoiaghdehhbnbhkkgmjanfhe" rel="nofollow">Chrome</a>, or any browser that supports uBlacklist</p>
</li>
<li>
<p dir="auto">Click on your extensions list, select uBlacklist, then click on the "options" text, highlighted in blue.</p>
</li>
<li>
<p dir="auto">Enable other search engines by clicking on the 'Other search engines/SERPINFO' button, and click on the search engines you want this list to work on. A list of compatible search engines (with images support) is linked <a href="https://github.com/iorate/ublacklist?tab=readme-ov-file#supported-search-engines">here.</a></p>
</li>
<li>
<p dir="auto">Scroll all the way down until you see the "Subscription" tab, and click on the blue "Add a subscription" button.</p>
</li>
<li>
<p dir="auto">Give a name for the added blocklist (eg. Main AI blocklist).</p>
</li>
<li>
<p dir="auto">Copy and paste this url</p>
</li>
</ol>
<div data-snippet-clipboard-copy-content="https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/list_uBlacklist.txt"><pre><code>https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/list_uBlacklist.txt
</code></pre></div>
<p dir="auto">into the <strong>URL</strong> part of the dialogue box, then press the blue <strong>Add</strong> button.</p>
<ol start="7" dir="auto">
<li>Set the update interval to an hour for near-realtime list updates, and you're done!</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Mobile installation (iOS, iPadOS &amp; Android)</h2><a id="user-content-mobile-installation-ios-ipados--android" aria-label="Permalink: Mobile installation (iOS, iPadOS &amp; Android)" href="#mobile-installation-ios-ipados--android"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">iOS, iPadOS (Safari only)</h3><a id="user-content-ios-ipados-safari-only" aria-label="Permalink: iOS, iPadOS (Safari only)" href="#ios-ipados-safari-only"></a></p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">Both iOS and iPadOS don't have support for uBlock Origin. So, we'll be using uBlacklist for this. Safari is the only browser we can use that allows the use of extensions.</p>
</div>
<ol dir="auto">
<li>
<p dir="auto">Download uBlacklist, <a href="https://apps.apple.com/us/app/ublacklist-for-safari/id1547912640" rel="nofollow">available on the App Store</a></p>
</li>
<li>
<p dir="auto">Go into settings, scroll down until you see Safari, and tap on it.</p>
</li>
<li>
<p dir="auto">Once in the Safari settings, in General, hit <strong>Extensions</strong>. Turn on the uBlacklist extension.</p>
</li>
<li>
<p dir="auto">While still in uBlacklist's settings, in the <strong>Permissions for uBlacklist</strong> section, scroll down to your preferred search engine and change the permission from "Ask" to "Allow."</p>
</li>
</ol>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">This may look cumbersome, but all you really need to do is just allow the extension to hit your search engine's locale, for example, <code>google.fr</code> or <code>google.co.uk</code>. You can go through all of them and allow them, but it's not necessary.</p>
</div>
<ol start="5" dir="auto">
<li>
<p dir="auto">Now scroll back up, and hit the blue <strong>Extension Settings</strong> button. It will bring you to Safari and open uBlacklist's settings panel.</p>
</li>
<li>
<p dir="auto">Scroll all the way down until you see the "Subscription" tab, and click on the blue "Add a subscription" button.</p>
</li>
<li>
<p dir="auto">Give a name for the added blocklist (eg. Main AI blocklist).</p>
</li>
<li>
<p dir="auto">Copy and paste this url</p>
</li>
</ol>
<div data-snippet-clipboard-copy-content="https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/list_uBlacklist.txt"><pre><code>https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/list_uBlacklist.txt
</code></pre></div>
<p dir="auto">into the <strong>URL</strong> part of the dialogue box, then press the blue <strong>Add</strong> button.</p>
<ol start="9" dir="auto">
<li>Set the update interval to an hour for near-realtime list updates, and you're done!</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Android (via Firefox)</h3><a id="user-content-android-via-firefox" aria-label="Permalink: Android (via Firefox)" href="#android-via-firefox"></a></p>
<details>
<summary>Installation for uBlock Origin (expand) </summary>

<p dir="auto"><strong>One-click import</strong></p>
<p dir="auto">If you have uBlock Origin installed, click <a href="https://subscribe.adblockplus.org/?location=https%3A%2F%2Fraw.githubusercontent.com%2Flaylavish%2FuBlockOrigin-HUGE-AI-Blocklist%2Fmain%2Flist.txt&amp;title=Sites%20using%20AI%20generated%20content" rel="nofollow">this link</a> to import the filter list in just a click! Quick and simple.</p>
<p dir="auto"><strong>Manual Import</strong></p>
<ol dir="auto">
<li>
<p dir="auto">Make sure that you have the uBlock Origin Extension for <a href="https://addons.mozilla.org/en-US/firefox/addon/ublock-origin/" rel="nofollow">Firefox</a>.</p>
</li>
<li>
<p dir="auto">Hit the three dots in the top right, and hit the Extensions button.</p>
</li>
<li>
<p dir="auto">Click on the uBlock Origin Extension, and in the bottom right, there is a cog-wheel symbol--named the dashboard. Click it.</p>
</li>
<li>
<p dir="auto">Once you are in the dashboard, look towards the top. Click on the tab that says <code>Filter lists</code>.</p>
</li>
<li>
<p dir="auto">Look towards the bottom, and expand the <code>Import</code> button.</p>
</li>
<li>
<p dir="auto">Copy and paste this URL into the dialogue box:</p>
</li>
</ol>
<div data-snippet-clipboard-copy-content="https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/list.txt"><pre><code>https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/list.txt
</code></pre></div>
<ol start="6" dir="auto">
<li>Apply changes, and you're set!</li>
</ol>
</details>
<details>
<summary>Installation for uBlacklist (expand) </summary>

<p dir="auto"><strong>Manual Import</strong></p>
<ol dir="auto">
<li>
<p dir="auto">Make sure that you have the uBlacklist extension for <a href="https://addons.mozilla.org/en-US/firefox/addon/ublacklist/" rel="nofollow">Firefox</a>, <a href="https://chromewebstore.google.com/detail/ublacklist/pncfbmialoiaghdehhbnbhkkgmjanfhe" rel="nofollow">Chrome</a>, or any browser that supports uBlacklist</p>
</li>
<li>
<p dir="auto">Hit the three dots in the top right, and hit the Extensions button.</p>
</li>
<li>
<p dir="auto">Click on your extensions list, select uBlacklist, then click on the "options" text, highlighted in blue.</p>
</li>
<li>
<p dir="auto">Enable other search engines by clicking on the 'Other search engines/SERPINFO' button, and click on the search engine(s) you want this list to work on. A list of compatible search engines (with images support) is <a href="https://github.com/iorate/ublacklist?tab=readme-ov-file#supported-search-engines">here.</a></p>
</li>
<li>
<p dir="auto">Scroll all the way down until you see the "Subscription" tab, and click on the blue "Add a subscription" button.</p>
</li>
<li>
<p dir="auto">Give a name for the added blocklist (eg. Main AI blocklist).</p>
</li>
<li>
<p dir="auto">Copy and paste this url</p>
</li>
</ol>
<div data-snippet-clipboard-copy-content="https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/list_uBlacklist.txt"><pre><code>https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/list_uBlacklist.txt
</code></pre></div>
<p dir="auto">into the <strong>URL</strong> part of the dialogue box, then press the blue <strong>Add</strong> button.</p>
<ol start="7" dir="auto">
<li>Set the update interval to an hour for near-realtime list updates, and you're done!</li>
</ol>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hosts file for pi-hole/adguard</h2><a id="user-content-hosts-file-for-pi-holeadguard" aria-label="Permalink: Hosts file for pi-hole/adguard" href="#hosts-file-for-pi-holeadguard"></a></p>
<p dir="auto">I've added a list in HOSTS format for pi-hole/adguard or for use in your own operating system's hosts file.</p>
<div data-snippet-clipboard-copy-content="https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/noai_hosts.txt"><pre><code>https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/noai_hosts.txt
</code></pre></div>
<p dir="auto">For use in your operating system, visit the url and copy-paste the contents inside your operating systems hosts file.</p>
<p dir="auto">Here's a simple guide on how to <a href="https://linuxize.com/post/how-to-edit-your-hosts-file/" rel="nofollow">access your hosts file on Linux, macOS, and Windows.</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">For pi-hole:</h3><a id="user-content-for-pi-hole" aria-label="Permalink: For pi-hole:" href="#for-pi-hole"></a></p>
<ol dir="auto">
<li>Visit your admin's dashboard</li>
<li>Click on <code>Adlists</code></li>
<li>Copy and paste the url into the <code>address:</code> box</li>
<li>Hit the <code>add</code> button, and it should be added.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">For Adguard:</h3><a id="user-content-for-adguard" aria-label="Permalink: For Adguard:" href="#for-adguard"></a></p>
<ol dir="auto">
<li>Open Adguard Home Dashboard</li>
<li>Go to filters --&gt; DNS blocklists.</li>
<li>Click <code>Add blocklist</code>, then <code>Add a custom list</code>.</li>
<li>Enter the name of the list (eg. AI blocklist) into the first dialogue box.</li>
<li>Copy and paste the url into the second dialogue box.</li>
<li>Hit save, and the list is added!</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Additional lists</h2><a id="user-content-additional-lists" aria-label="Permalink: Additional lists" href="#additional-lists"></a></p>
<p dir="auto">Currently, there are two lists: The <a href="https://github.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/blob/main/github.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/blob/main/list.txt">main</a> default list, and the <a href="https://github.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/blob/main/github.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/blob/main/additional_list_nuclear.txt">nuclear</a> list.</p>
<p dir="auto">The nuclear list has sites that contain a mix of authentic and AI generated imagery (eg. DeviantArt, Artstation, Stock Photography sites, etc), which make it tricky to outright block in the main filter list, so I've designated it to a separate list that you can toggle on and off if you so desire.</p>
<details>
<summary>uBlock Origin Nuclear List (expand) </summary>
<br>
<div data-snippet-clipboard-copy-content="https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/additional_list_nuclear.txt"><pre><code>https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/additional_list_nuclear.txt
</code></pre></div>
</details>
<details>
<summary>uBlacklist Nuclear List (expand) </summary>
<br>
<div data-snippet-clipboard-copy-content="https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/list_uBlacklist_nuclear.txt"><pre><code>https://raw.githubusercontent.com/laylavish/uBlockOrigin-HUGE-AI-Blocklist/main/list_uBlacklist_nuclear.txt
</code></pre></div>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Allowlisting sites</h2><a id="user-content-allowlisting-sites" aria-label="Permalink: Allowlisting sites" href="#allowlisting-sites"></a></p>
<p dir="auto">Don't like a certain site being blocked? You can easily create an allowlist in your own personal uBlock Origin or uBlacklist filter list.</p>
<p dir="auto">Here's how to do it.</p>
<details>
<summary>Steps for uBlock Origin (expand) </summary>
<br>
<ol dir="auto">
<li>Toggle the <a href="https://github.com/gorhill/uBlock/wiki/DOM-inspector">DOM inspector</a> <code>&lt;/&gt;</code> through uBlock Origin's <a href="https://github.com/gorhill/uBlock/wiki/The-logger">logger</a>.</li>
<li>Locate the URL you want to allowlist.</li>
<li>Click on the filter you want to disable (eg. vecteezy.com); it should then be crossed out.</li>
<li>Press the save icon, then the "Create" button.</li>
</ol>
<p dir="auto">Boom! Now it's allowlisted!</p>
<p dir="auto">Or, if you don't want to go through that mumbo-jumbo, add this line in your filter list:</p>
<div data-snippet-clipboard-copy-content="#@#a[href*=&quot;example.com&quot;]:upward(li):remove()"><pre><code>#@#a[href*="example.com"]:upward(li):remove()
</code></pre></div>
<p dir="auto">Change "example.com" to the URL you want to allowlist. Copy &amp; paste that in uBlock Origin's "My filters" list, and you're set!</p>
</details>
<details>
<summary>Steps for uBlacklist (expand) </summary>
<br>
<ol dir="auto">
<li>Enter uBlacklist's options panel.</li>
<li>In the text box, add this line in the text box:</li>
</ol>

<ol start="3" dir="auto">
<li>Change "example.com" to a website you want allowlisted.</li>
<li>Click save. Done!</li>
</ol>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Extended Filtering</h2><a id="user-content-extended-filtering" aria-label="Permalink: Extended Filtering" href="#extended-filtering"></a></p>
<p dir="auto">It is possible to filter AI results based on keywords. It was originally in the list, but it's been taken out to make it configurable and/or optional.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">uBlock Origin</h3><a id="user-content-ublock-origin" aria-label="Permalink: uBlock Origin" href="#ublock-origin"></a></p>
<p dir="auto">In your personal filter list, you can use this template to add your own keywords you would like to block.</p>
<div data-snippet-clipboard-copy-content="google.com,duckduckgo.com,bing.com##div>a:has-text(/Your Text Here/i):upward(div):style(opacity:0!important)"><pre><code>google.com,duckduckgo.com,bing.com##div&gt;a:has-text(/Your Text Here/i):upward(div):style(opacity:0!important)
</code></pre></div>
<p dir="auto">Replace "Your Text Here" with your preferred keywords. A short list of <strong>optional</strong> procedural filters that you can use for uBlock Origin are listed in a dropdown below:</p>
<details>
<summary> Optional procedural filters for uBlock Origin </summary>
<br>
<div data-snippet-clipboard-copy-content="google.com,duckduckgo.com,bing.com##div>a:has-text(/Stable Diffusion/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div>a:has-text(/AI Art/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div>a:has-text(/Generative AI/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div>a:has-text(/Ai/):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div>a:has-text(/AI/):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div>a:has-text(/Lora Model/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div>a:has-text(/diffusion/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div>a:has-text(/midjourney/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div>a:has-text(/niji/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div>a:has-text(/SDXL/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div>a:has-text(/ai generated/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div>a:has-text(/aiart/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div>a:has-text(/AI illustration/i):upward(div):style(opacity:0!important)"><pre><code>google.com,duckduckgo.com,bing.com##div&gt;a:has-text(/Stable Diffusion/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div&gt;a:has-text(/AI Art/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div&gt;a:has-text(/Generative AI/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div&gt;a:has-text(/Ai/):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div&gt;a:has-text(/AI/):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div&gt;a:has-text(/Lora Model/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div&gt;a:has-text(/diffusion/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div&gt;a:has-text(/midjourney/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div&gt;a:has-text(/niji/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div&gt;a:has-text(/SDXL/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div&gt;a:has-text(/ai generated/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div&gt;a:has-text(/aiart/i):upward(div):style(opacity:0!important)
google.com,duckduckgo.com,bing.com##div&gt;a:has-text(/AI illustration/i):upward(div):style(opacity:0!important)
</code></pre></div>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">uBlacklist</h3><a id="user-content-ublacklist" aria-label="Permalink: uBlacklist" href="#ublacklist"></a></p>
<p dir="auto">For uBlacklist, you can use <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions" rel="nofollow">regular expressions</a> to filter AI results based on keywords.</p>
<p dir="auto">An example of a regular expression for uBlacklist would be: <code>/ai *(generated)?|stable *diffusion/i</code></p>
<p dir="auto">Below is a small list of <strong>optional</strong> regular expressions that can be used to filter out AI results based on keywords:</p>
<details>
<summary> Optional regular expressions for uBlacklist </summary>
<br>
<div data-snippet-clipboard-copy-content="/(generative)? *AI *(art|generated|illustration)?/i
/(ada)?Lo(RA|Con) *(model)?|(stable)?.*diffusion|midjourney|niji|sd *(xl|1.5)|(text|txt|img|image) *(to|2) *(image|img|video)/i"><pre><code>/(generative)? *AI *(art|generated|illustration)?/i
/(ada)?Lo(RA|Con) *(model)?|(stable)?.*diffusion|midjourney|niji|sd *(xl|1.5)|(text|txt|img|image) *(to|2) *(image|img|video)/i
</code></pre></div>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">If you'd like to contribute to the list, feel free to clone this repo and create a pull request for the site(s) you'd like to add to the list. Make sure to update all files (including hosts file) wherever necesessary (except the nuclear list if you want the site to appear in the main list of course).</p>
<p dir="auto">If you don't know how to do that or don't want to, you can instead create an issue of the site that you'd like to be included in the list, wherein I'd look over it and add it in myself. Any way that you contribute is greatly appreciated!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">To Do</h2><a id="user-content-to-do" aria-label="Permalink: To Do" href="#to-do"></a></p>
<ul>
<li>
<p dir="auto"> Provide blocklist for uBlacklist compatibility</p>
</li>
<li>
<p dir="auto"> Blocklist functionality on DuckDuckGo and Bing (ew)</p>
</li>
<li>
<p dir="auto"> Create hosts file for pi-hole/adguard</p>
</li>
<li>
<p dir="auto"> Startpage, Ecosia, Brave support (for uBlock Origin)</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Happy Pride Month!</h2><a id="user-content-happy-pride-month" aria-label="Permalink: Happy Pride Month!" href="#happy-pride-month"></a></p>
<p dir="auto">LGBTQ+ Rights! 🏳️‍🌈🏳️‍⚧️</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Special Thanks</h2><a id="user-content-special-thanks" aria-label="Permalink: Special Thanks" href="#special-thanks"></a></p>
<p dir="auto">Special thanks to:</p>
<ul dir="auto">
<li>
<p dir="auto">This <a href="https://pastebin.com/B8kP4imQ" rel="nofollow">pastebin</a> (since it added even more sites to my blocklist)</p>
</li>
<li>
<p dir="auto">u/AchernarB for the <a href="https://www.reddit.com/r/uBlockOrigin/comments/13uyex5/how_to_block_results_from_a_specific_site_in_the/" rel="nofollow">awesome snip-bit of code.</a></p>
</li>
<li>
<p dir="auto">Raymond Hill, <a href="https://github.com/gorhill/uBlock">uBlock Origin extension</a></p>
</li>
<li>
<p dir="auto">iorate, <a href="https://github.com/iorate/ublacklist">uBlacklist extension</a></p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Related Projects</h2><a id="user-content-related-projects" aria-label="Permalink: Related Projects" href="#related-projects"></a></p>
<p dir="auto"><a href="https://github.com/NotaInutilis/Super-SEO-Spam-Suppressor">Super SEO Spam Suppressor (SSSS)</a> by NotaInutilis</p>
<blockquote>
<p dir="auto">An anticapitalist blocklist targeting websites abusing SEO tactics to spam web searches with data pollution and security risks: content farms, scrapers, copycats, generative AI, scams, advertisements, malwares, and useless wasteful garbage in general. It is best used with uBlacklist.</p>
</blockquote>
<p dir="auto"><a href="https://surasshu.com/blocklist-for-ai-music-on-youtube/" rel="nofollow">Blocklist for AI Music on Youtube</a> by surasshu</p>
<blockquote>
<p dir="auto">A blocklist that targets AI music channels on YouTube, through the use of the Blocktube extension.</p>
</blockquote>
<p dir="auto"><a href="https://journeybuster.com/" rel="nofollow">Journey Buster 3</a> by k0vac</p>
<blockquote>
<p dir="auto">A Chromium extension that lets you know if an image is AI generated, for use on Twitter.</p>
</blockquote>
<p dir="auto"><a href="https://github.com/rjaus/awesome-ublacklist">Awesome List of uBlacklist Subscriptions</a> by rjaus</p>
<blockquote>
<p dir="auto">A compilation of awesome uBlacklist subscriptions to block various sites from appearing in Google, Bing, or DuckDuckGo search.</p>
</blockquote>
<p dir="auto"><a href="https://github.com/laylavish/TipsTricksGoogleSearch">Anti-AI Google Search Tips</a> by yours truly</p>
<blockquote>
<p dir="auto">Tips and tricks to make Google Search (and other search engines that have similar operators) return authentic imagery.</p>
</blockquote>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fahrplan – 39C3 (335 pts)]]></title>
            <link>https://fahrplan.events.ccc.de/congress/2025/fahrplan/</link>
            <guid>46386211</guid>
            <pubDate>Thu, 25 Dec 2025 18:40:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/">https://fahrplan.events.ccc.de/congress/2025/fahrplan/</a>, See on <a href="https://news.ycombinator.com/item?id=46386211">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <main>
        
  <div>
    
      
        
          
          <p>
            10:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            10:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            11:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            11:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            12:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            12:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            13:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            13:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            14:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            14:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            15:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            15:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            16:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            16:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            17:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            17:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            18:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            18:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            19:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            19:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            20:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            20:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            21:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            21:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            22:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            22:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            23:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            23:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            00:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            00:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            01:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            01:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            02:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            02:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            03:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            03:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            04:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            04:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            05:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            05:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            06:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            06:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            07:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            07:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            08:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            08:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            09:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            09:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            10:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            10:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            11:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            11:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            12:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            12:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            13:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            13:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            14:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            14:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            15:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            15:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            16:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            16:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            17:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            17:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            18:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            18:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            19:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            19:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            20:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            20:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            21:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            21:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            22:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            22:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            23:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            23:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            00:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            00:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            01:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            01:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            02:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            02:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            03:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            03:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            04:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            04:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            05:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            05:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            06:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            06:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            07:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            07:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            08:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            08:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            09:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            09:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            10:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            10:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            11:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            11:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            12:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            12:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            13:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            13:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            14:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            14:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            15:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            15:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            16:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            16:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            17:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            17:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            18:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            18:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            19:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            19:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            20:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            20:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            21:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            21:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            22:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            22:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            23:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            23:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            00:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            00:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            01:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            01:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            02:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            02:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            03:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            03:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            04:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            04:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            05:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            05:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            06:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            06:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            07:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            07:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            08:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            08:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            09:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            09:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            10:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            10:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            11:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            11:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            12:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            12:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            13:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            13:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            14:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            14:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            15:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            15:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            16:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            16:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            17:00
          </p>
        
      
        
      
        
      
        
      
        
      
        
      
        
          
          <p>
            17:30
          </p>
        
      
        
      
        
      
        
      
        
      
        
      

      
      
        
      
        
      
        
      
        
      
    

    
    
      
      
        <div>
          <p>
            <h2>
              Sat - Day 1 - December 27
            </h2>
          </p>
        </div>
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/opening-ceremony">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/all-sorted-by-machines-of-loving-grace-ai-cybernetics-and-fascism-and-how-to-intervene">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/the-art-of-text-rendering">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/a-tale-of-two-leaks-how-hackers-breached-the-great">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/openautolab-photographic-film-processing-machine-fully-automatic-and-diy-friendly">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/zps-ein-jahr-adenauer-srp-und-mehr">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/demystifying-fuzzer-behaviour">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/isdn-pots-telephony-at-congress-and-camp">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/brennende-walder-und-kommentarspalten-klimaupdate-mit-bits-baume-und-dem-fragdenstaat-climate-helpdesk">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/building-hardware-easier-than-ever-harder-than-it-should-be">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/neuroexploitation-by-design-wie-algorithmen-in-glucksspielprodukten-sich-wirkweisen-des-reinforcement-learning-und-dopaminergen-belohnungssystems-zunu">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/fetap-611-unplugged-taking-a-rotary-dial-phone-to-the-mobile-age">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/who-cares-about-the-baltic-jammer-terrestrial-navigation-in-the-baltic-sea-region">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/liberating-bluetooth-on-the-esp32">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/chaos-macht-kuche">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/developing-new-medicines-in-the-age-of-ai-and-personalized-medicine">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/endlich-maschinenlesbare-urteile-open-access-fur-juristen">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/opening-pamdora-s-box-and-unleashing-a-thousand-paths-on-the-journey-to-play-beatsaber-custom-songs">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/not-an-impasse-child-safety-privacy-and-healing-together">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/kim-1-5-noch-mehr-kaos-in-der-medizinischen-telematikinfrastruktur-ti">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/redscout42-zur-digitalen-wohnungsfrage">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/all-my-deutschlandtickets-gone-fraud-at-an-industrial-scale">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/of-boot-vectors-and-double-glitches-bypassing-rp2350-s-secure-boot">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/ki-digitalisierung-und-longevity-als-fix-fur-ein-kaputtes-gesundheitssystem">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/chaos-all-year-round">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/to-sign-or-not-to-sign-practical-vulnerabilities-i">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/handy-weg-bis-zur-ausreise-wie-cellebrite-ins-auslanderamt-kam">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/pwn2roll-who-needs-a-599-remote-when-you-have-wheelchair-py">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/escaping-containment-a-security-analysis-of-freebsd-jails">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/die-kanguru-rebellion-digital-independence-day">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/and-so-it-begins-wie-unser-rechtsstaat-auf-den-highway-richtung-trumpismus-rast-und-warum-afghanische-klager-innen-fur-uns-die-notbremse-ziehen">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/1965-60-years-of-algorithmic-art-with-computers">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/life-on-hold-what-does-true-solidarity-look-like-beyond-duldung-camps-deportation-and-payment-cards">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/episode-ii-der-rat-schlagt-zuruck">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/excuse-me-what-precise-time-is-it">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/bitunlocker-leveraging-windows-recovery-to-extract-bitlocker-secrets">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/not-to-be-trusted-a-fiasco-in-android-tees">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/hacking-washing-machines">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/radikalisierungspipeline-esoterik-von-eso-nazis-de">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/throwing-your-rights-under-the-omnibus-how-the-eu-s-reform-agenda-threatens-to-erase-a-decade-of-digital-rights">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/dngerouslink-a-deep-dive-into-whatsapp-0-click-exploits-on-ios-and-samsung-devices">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/bluetooth-headphone-jacking-a-key-to-your-phone">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/breaking-architecture-barriers-running-x86-games-and-apps-on-arm">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/the-eyes-of-photon-science-imaging-simulation-and-the-quest-to-make-the-invisible-visible">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/coding-dissent-art-technology-and-tactical-media">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/ai-generated-content-in-wikipedia-a-tale-of-caution">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/building-a-noc-from-scratch">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/from-silicon-to-darude-sand-storm-breaking-famous-synthesizer-dsps">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/unnecessarily-complicated-kitchen-die-wissenschaft-des-guten-geschmacks">
    
    
  </a>

    
      
      
        <div>
          <p>
            <h2>
              Sun - Day 2 - December 28
            </h2>
          </p>
        </div>
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/junghackerinnentag-einfuhrung">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/protecting-the-network-data-of-one-billion-people-breaking-network-crypto-in-popular-chinese-mobile-apps">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/hatupangwingwi-the-story-how-kenyans-fought-back-against-intrusive-digital-identity-systems">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/lightning-talks-tag-2">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/digitale-inklusion-wie-wir-digitale-barrierefreiheit-fur-alle-erreichen-konnen">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/skynet-starter-kit-from-embodied-ai-jailbreak-to-remote-takeover-of-humanoid-robots">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/suing-spyware-in-europe-news-from-the-front">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/neue-chaos-events-inselchaos-und-hack-ma-s-castle-plaudern-aus-dem-nahkastchen">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/a-post-american-enshittification-resistant-internet">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/a-space-odyssey-2-how-to-study-moon-rocks-from-the-soviet-sample-return-mission-luna-24">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/agentic-probllms-exploiting-ai-computer-use-and-coding-agents">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/selbstverstandlich-antifaschistisch-aktuelle-informationen-zu-den-verfahren-im-budapest-komplex-von-family-friends-hamburg">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/chaospager-how-to-construct-an-open-pager-system-for-c3">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/chaos-communication-chemistry-dna-security-systems-based-on-molecular-randomness">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/live-die-repeat-the-fight-against-data-retention-and-boundless-access-to-data">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/power-cycle-b7-oder-warum-kauft-man-eine-zeche">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/cracking-open-what-makes-apple-s-low-latency-wifi-so-fast">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/awful-interception-misadventures-of-the-russian-surveillance-machinery">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/amateurfunk-im-all-kontakt-mit-fram2">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/uber-europaische-grenzen-hinweg-auf-klinischen-daten-rechnen-aber-sicher">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/ccc-jahresruckblick">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/persist-resist-stitch">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/lessons-from-building-an-open-architecture-secure-element">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/auf-die-dauer-hilft-nur-power-herausforderungen-fur-dezentrale-netzwerke-aus-sicht-der-soziologie">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/current-drone-wars">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/variable-fonts-it-was-never-about-file-size">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/a-quick-stop-at-the-hostileshop">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/in-house-electronics-manufacturing-from-scratch-how-hard-can-it-be">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/cpu-entwicklung-in-factorio-vom-d-flip-flop-bis-zum-eigenen-betriebssystem">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/amtsgeheimnis-raus-datenhalde-rein-was-die-informationsfreiheit-in-osterreich-bringt">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/how-to-render-cloud-fpgas-useless">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/freiheit-exe-utopien-als-malware">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/recharge-your-batteries-with-us-an-empowering-journey-through-the-energy-transition">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/prometheus-reverse-engineering-overwatch">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/trump-government-demands-access-to-european-police-databases-and-biometrics">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/verlorene-domains-offene-turen-was-alte-behordendomains-verraten">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/css-clicker-training-making-games-in-a-styling-language">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/wie-wir-alte-flipperautomaten-am-leben-erhalten">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/power-cycles-statt-burnout-wie-einflussnahme-nicht-verpufft">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/don-t-look-up-there-are-sensitive-internal-links-in-the-clear-on-geo-satellites">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/textiles-101-fast-fiber-transform">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/how-to-minimize-bugs-in-cryptography-code">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/machine-vision-vom-algorithmus-zum-baumpilz-im-digitalen-metabolismus">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/xous-a-pure-rust-rethink-of-the-embedded-operating-system">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/51-ways-to-spell-the-image-giraffe-the-hidden-politics-of-token-languages-in-generative-ai">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/when-vibe-scammers-met-vibe-hackers-pwning-phaas-with-their-own-weapons">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/the-maybe-talent-show">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/code-to-craft-procedural-generation-for-the-physical-world">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/reverse-engineering-the-pixel-titanm2-firmware">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/the-small-packet-of-bits-that-can-save-or-destabilize-a-city">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/gptdash-der-reverse-turing-test">
    
    
  </a>

    
      
      
        <div>
          <p>
            <h2>
              Mon - Day 3 - December 29
            </h2>
          </p>
        </div>
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/azubi-tag-einfuhrung">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/greenhouse-gas-emission-data-public-difficult-to-access-and-not-always-correct">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/design-for-3d-printing">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/lightning-talks-tag-3">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/the-museum-of-care-open-source-survival-kit-collection">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/celestial-navigation-with-very-little-math">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/a-media-almost-archaeology-on-data-that-is-too-dirty-for-ai">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/hacking-karlsruhe-10-years-later">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/what-makes-bike-sharing-work-insights-from-43-million-kilometers-of-european-cycling-data">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/teckids-eine-verstehbare-digitale-welt">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/be-modded-exploring-and-hacking-the-vital-bracelet-ecosystem">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/wer-hat-angst-vor-dem-neutralitatsgebot">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/shit-for-future-turning-human-shit-into-a-climate-solution">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/watch-your-kids-inside-a-children-s-smartwatch">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/when-8-bits-is-overkill-making-blinkenlights-with-a-1-bit-cpu">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/supplements-und-social-media-wenn-der-online-hype-zur-realen-gesundheitsgefahr-wird">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/programmierte-kriegsverbrechen-uber-ki-systeme-im-kriegseinsatz-in-gaza-und-warum-it-fachleute-sich-dazu-auern-mussen">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/making-the-magic-leap-past-nvidia-s-secure-bootchain-and-breaking-some-tesla-autopilots-along-the-way">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/learning-from-south-korean-telco-breaches">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/gegenmacht-best-of-informationsfreiheit">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/there-is-no-way-we-ended-up-getting-arrested-for-this-malta-edition">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/apt-down-and-the-mystery-of-the-burning-data-centers">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/von-wegen-eisblumen-wie-man-mit-code-satelliten-und-schiffsexpeditionen-die-bunte-welt-des-arktischen-phytoplanktons-sichtbar-macht">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/schlechte-karten-it-sicherheit-im-jahr-null-der-epa-fur-alle">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/set-top-box-hacking-freeing-the-freebox">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/wer-liegt-hier-wem-auf-der-tasche-genug-mit-dem-burgergeld-fetisch-sturmt-die-palaste">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/the-last-of-us-fighting-the-eu-surveillance-law-apocalypse">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/ai-agent-ai-spy">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/build-a-fake-phone-find-real-bugs-qualcomm-gpu-emulation-and-fuzzing-with-libafl-qemu">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/transkultureller-hack-auf-die-klassische-musikszene-vortrag-und-konzert">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/netzpolitik-in-der-schweiz-zwischen-bodensee-und-matterhorn">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/the-angry-path-to-zen-amd-zen-microcode-tools-and-insights">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/blackbox-palantir">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/aber-hier-leben-nein-danke-oder-doch-wie-wir-der-autoritaren-zuspitzung-begegnen-konnen">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/race-conditions-transactions-and-free-parking">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/hegemony-eroding-excavating-diversity-in-latent-space">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/10-years-of-dieselgate">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/the-heartbreak-machine-nazis-in-the-echo-chamber">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/light-in-the-dark-net">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/the-spectrum-hackspace-beyond-hacking">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/rowhammer-in-the-wild-large-scale-insights-from-flippyr-am">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/peep-show-fur-die-polizei-staatliche-uberwachung-von-queers-in-hamburger-toiletten-bis-1980">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/human-microservices-at-the-dutch-railways-modern-architecture-ancient-hardware">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/von-fuzzern-zu-agenten-entwicklung-eines-cyber-reasoning-systems-fur-die-aixcc">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/pruf">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/verschlusselung-brechen-durch-physischen-zugriff-smartphone-beschlagnahme-durch-polizei">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/spectre-in-the-real-world-leaking-your-private-data-from-the-cloud-with-cpu-vulnerabilities">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/die-groe-datenschutz-datenpannen-und-ds-gvo-show">
    
    
  </a>

    
      
      
        <div>
          <p>
            <h2>
              Tue - Day 4 - December 30
            </h2>
          </p>
        </div>
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/asahi-linux-porting-linux-to-apple-silicon">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/atoms-in-space">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/i-hated-all-the-cross-stitch-software-so-i-made-my-own-my-deranged-outsider-software-suite-for-making-deranged-outsider-art">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/how-to-keep-open-source-open-without-leaving-our-communities-open-to-threats">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/ccc-t-cosmic-ray-the-climate-catastrophe-and-trains">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/cuii-wie-konzerne-heimlich-webseiten-in-deutschland-sperren">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/end-of-10-how-the-foss-community-is-combatting-software-drive-resource-and-energy-consumption">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/what-you-hack-is-what-you-mean-35-years-of-wiring-sense-into-text">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/security-of-cardiac-implantable-electronic-devices">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/who-runs-the-www-wsis20-and-the-future-of-internet">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/fossile-industrie-liebt-ki">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/laser-beams-light-streams-letting-hackers-go-pew-pew-building-affordable-light-based-hardware-security-tooling">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/breaking-bots-cheating-at-blue-team-ctfs-with-ai-speed-runs">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/von-groschen-und-spurlos-gnu-taler-auch-auf-eurem-event">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/we-the-eu-and-1064-danes-decided-to-look-into-youtube-a-story-about-how-the-eu-gave-us-a-law-1064-danes-gave-us-their-youtube-histories-and-reality-ga">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/battling-obsolescence-keeping-an-80s-laser-tag-sys">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/security-nightmares">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/infrastructure-review">
    
    
  </a>

    
      
      
      
  <a href="https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/closing-ceremony">
    
    
  </a>

    
  </div>

      </main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Critical vulnerability in LangChain – CVE-2025-68664 (115 pts)]]></title>
            <link>https://cyata.ai/blog/langgrinch-langchain-core-cve-2025-68664/</link>
            <guid>46386009</guid>
            <pubDate>Thu, 25 Dec 2025 18:06:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cyata.ai/blog/langgrinch-langchain-core-cve-2025-68664/">https://cyata.ai/blog/langgrinch-langchain-core-cve-2025-68664/</a>, See on <a href="https://news.ycombinator.com/item?id=46386009">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<figure><img decoding="async" width="1500" height="900" src="https://cyata.ai/wp-content/uploads/2025/12/LangGrinch-Article.png" alt="" srcset="https://cyata.ai/wp-content/uploads/2025/12/LangGrinch-Article.png 1500w, https://cyata.ai/wp-content/uploads/2025/12/LangGrinch-Article-300x180.png 300w, https://cyata.ai/wp-content/uploads/2025/12/LangGrinch-Article-1024x614.png 1024w, https://cyata.ai/wp-content/uploads/2025/12/LangGrinch-Article-768x461.png 768w, https://cyata.ai/wp-content/uploads/2025/12/LangGrinch-Article-372x223.png 372w" sizes="(max-width: 1500px) 100vw, 1500px"><figcaption>Cyata Research: LangGrinch Vulnerability in LangChain</figcaption></figure>



<p>Yesterday, LangChain published a <a href="https://github.com/advisories/GHSA-c67j-w6g6-q2cm">critical advisory</a> for a vulnerability I reported in langchain-core: <strong>CVE-2025-68664 / GHSA-c67j-w6g6-q2cm</strong>.&nbsp;</p>



<p>Earlier this year, my research focused on breaking secret managers in our “<a href="https://cyata.ai/vault-fault/">Vault Fault</a>” work – systems that are explicitly designed to be the security boundary around your most sensitive credentials. One takeaway kept repeating: when a platform accidentally treats attacker-shaped data as trusted structure, that boundary collapses fast. <strong>This time, the system that “breaks” isn’t your secret manager. It’s the agent framework that may use them.</strong></p>



<p>Why this vulnerability deserves extra attention:</p>



<ol>
<li><strong>It’s in Core.</strong> This is not a specific tool bug, not an integration edge-case, and not a “some community package did something weird.” The vulnerable APIs (dumps() / dumpd()) live in <strong>langchain-core itself</strong>.</li>



<li><strong>The blast radius is scale.</strong> By download volume, langchain is one of the most widely deployed AI framework components globally today. As of late December 2025, public package telemetry shows <strong>hundreds of millions of installs</strong>, with pepy.tech reporting <strong>~847M total downloads</strong> and pypistats showing <strong>~98M downloads in the last month</strong>.</li>



<li><strong>One prompt can trigger a lot of machinery.</strong> The most common real-world path here is not “attacker sends you a serialized blob and you call load().” It’s subtler: LLM outputs can influence fields like additional_kwargs or response_metadata, and those fields can be serialized and later deserialized through normal framework features like streaming logs/events. Plainly, this means <strong>an exploit can be triggered by a single text prompt</strong> that cascades into a surprisingly complex internal pipeline.&nbsp;<br></li>
</ol>



<p>Before you read further, patches are now released in versions 1.2.5 and 0.3.81. If you’re running LangChain in production, this one is trickier than it may seem; please update ASAP.</p>



<h2><strong>The short version of the bug</strong></h2>



<p>LangChain uses a special internal serialization format where dictionaries containing an ‘lc’ marker represent LangChain objects. The vulnerability was that dumps() and dumpd() <strong>did not properly escape user-controlled dictionaries</strong> that happened to include the reserved ‘lc’ key.&nbsp;</p>



<p>So once an attacker is able to make a LangChain orchestration loop serialize and later deserialize content including an ‘lc’ key, they would instantiate an unsafe arbitrary object, potentially triggering many attacker-friendly paths.</p>



<p>The <a href="https://github.com/advisories/GHSA-c67j-w6g6-q2cm">advisory</a> lists <strong>12 distinct vulnerable flows</strong>, which are extremely common use cases, such as standard event streaming, logging, message history/memory or caches:</p>



<figure data-wp-context="{&quot;imageId&quot;:&quot;694e5fb5d7f6a&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="221" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://cyata.ai/wp-content/uploads/2025/12/image-16-1024x221.png" alt="" srcset="https://cyata.ai/wp-content/uploads/2025/12/image-16-1024x221.png 1024w, https://cyata.ai/wp-content/uploads/2025/12/image-16-300x65.png 300w, https://cyata.ai/wp-content/uploads/2025/12/image-16-768x166.png 768w, https://cyata.ai/wp-content/uploads/2025/12/image-16-1536x331.png 1536w, https://cyata.ai/wp-content/uploads/2025/12/image-16-2048x441.png 2048w, https://cyata.ai/wp-content/uploads/2025/12/image-16-372x80.png 372w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>The most damaging outcomes include:</p>



<ul>
<li><strong>Secret extraction</strong> from environment variables. The advisory notes this happens when deserialization is performed with secrets_from_env=True. Notably, <strong>this was the default until yesterday</strong>. 🙂&nbsp;</li>



<li><strong>Object instantiation</strong> within pre-approved namespaces (including <em>langchain_core</em>, <em>langchain_openai, langchain_aws, langchain_anthropic…</em>), potentially triggering side effects in constructors (network calls, file operations, etc.).&nbsp;</li>



<li>Under certain conditions LangChain object instantiation may lead to arbitrary code execution.</li>
</ul>



<p>This is categorized under <strong>CWE-502: Deserialization of Untrusted Data</strong>, with a CNA CVSS score of <strong>9.3 (Critical)</strong>.&nbsp;</p>



<figure data-wp-context="{&quot;imageId&quot;:&quot;694e5fb5d8aeb&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="627" height="344" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://cyata.ai/wp-content/uploads/2025/12/banner-langgrinch.png" alt="" srcset="https://cyata.ai/wp-content/uploads/2025/12/banner-langgrinch.png 627w, https://cyata.ai/wp-content/uploads/2025/12/banner-langgrinch-300x165.png 300w, https://cyata.ai/wp-content/uploads/2025/12/banner-langgrinch-372x204.png 372w" sizes="auto, (max-width: 627px) 100vw, 627px"></figure>



<h2><strong>My research story: how I stumbled into it</strong></h2>



<p><br>’Twas the night before Christmas, and I was doing the least festive kind of work: staring at serialization code and asking “wait… why is that trusted?”</p>



<p>Security research often looks dramatic from the outside. In reality, it is usually careful reading, small hypotheses, and a slow accumulation of “that’s odd” moments.</p>



<p>This one started the way many do at Cyata: with a simple question we ask constantly as we assess AI stacks for real-world risk:</p>



<p><em>Where are the trust boundaries in AI applications, and do developers actually know where those boundaries are?</em></p>



<p>LangChain is a powerful framework, and like most modern frameworks it has to move complex structured data around: messages, tool calls, streaming events, traces, caches, and “runnables.”</p>



<div><p>Reviewing previous research, there was already extensive research on LangChain tooling and integrations, but very few findings on the core library.</p><p>I started the research working backwards. Finding interesting locations (sinks), then figuring out how an attacker might reach them. Deserialization was an obvious target.</p><p>It took me quite a while to find something meaningful. But after a while, I found that assuming an attacker-controlled deserialization primitive, I could trigger blind SSRF that could be leveraged to exfiltrate environment variables (to be detailed soon). Because the outcome was limited to secret exfil, instead of my primary RCE goal, I kept auditing the deserialization, and took my time.</p><p>The bug wasn’t a piece of bad code, it was the absence of code. dumps() simply didn’t escape user-controlled dictionaries containing ‘lc’&nbsp; keys. A missing escape in the serialization path, not the deserialization.</p></div>



<figure data-wp-context="{&quot;imageId&quot;:&quot;694e5fb5d9565&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="342" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://cyata.ai/wp-content/uploads/2025/12/image-2-1024x342.png" alt="" srcset="https://cyata.ai/wp-content/uploads/2025/12/image-2-1024x342.png 1024w, https://cyata.ai/wp-content/uploads/2025/12/image-2-300x100.png 300w, https://cyata.ai/wp-content/uploads/2025/12/image-2-768x256.png 768w, https://cyata.ai/wp-content/uploads/2025/12/image-2-372x124.png 372w, https://cyata.ai/wp-content/uploads/2025/12/image-2.png 1177w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></figure>



<p>It’s much easier to spot something wrong than to notice something missing, especially when you’re auditing load(), not dumps(). In one of the most scrutinized AI frameworks. For two and a half years.</p>



<p>From there, the investigation became a structured exercise:</p>



<ol>
<li>Identify where <strong>untrusted content</strong> (mostly arbitrary dictionaries) gets serialized (LLM outputs, prompt injection, user input, external tools, retrieved docs).</li>



<li>Identify when that serialized data gets deserialized.</li>



<li>Identify what an attacker can achieve from arbitrary object instantiation.</li>
</ol>



<p>At that point, the core finding was clear and actionable enough to responsibly report: there was an escaping gap in dumps() / dumpd() around ‘lc’ key dictionaries.&nbsp;</p>



<p>The advisory later captured what we often see in practice: fields like additional_kwargs and response_metadata can be influenced by LLM output and prompt injection, and those fields can get serialized-deserialized in many flows.&nbsp;</p>



<p>To the LangChain team’s credit: the response and follow-through were decisive, not just patching the bug but also hardening defaults that were too permissive for the world we’re in now.&nbsp;</p>



<p>The LangChain project decided to award a $4,000 USD bounty for this finding. According to huntr, the platform where LangChain operated its bounty program, this would be the maximum amount ever awarded in the project, with bounties so far awarded up to $125.</p>



<h2><strong>Technical deep dive</strong></h2>



<h3><strong>Background: the “lc” marker and why it exists</strong></h3>



<p>LangChain serializes certain objects using a structured dict format. The ‘lc’ key is used internally to indicate “this is a LangChain-serialized structure,” not just arbitrary user data.&nbsp;</p>



<p>This is a common pattern, but it creates a security invariant: Any user-controlled data that could contain ‘lc’ must be treated carefully. Otherwise, an attacker can craft a dict that “looks like” an internal object and trick the deserializer into giving it meaning.</p>



<p>The patch makes the intent explicit in the updated documentation: during serialization, <strong>plain dicts that contain an ‘lc’ key are escaped</strong> by wrapping them.&nbsp;</p>



<p>This prevents those dicts from being confused with actual LangChain serialized objects during deserialization.&nbsp;</p>



<h3><strong>The allowlist: what can be instantiated</strong></h3>



<p>LangChain’s load()/loads() functions don’t instantiate arbitrary classes – they check against an allowlist that controls which classes can be deserialized. By default, this allowlist includes classes from langchain_core, langchain_openai, langchain_aws, and other ecosystem packages.</p>



<p>Here’s the catch: most classes on the allowlist have harmless constructors. Finding exploitable paths required digging through the ecosystem for classes that do something meaningful on instantiation. The ones I found are detailed below, but there may be others waiting to be discovered.</p>



<h3><strong>The exfiltration path</strong></h3>



<p>LangChain’s loads() function supports a <em>secret</em> type that resolves values from environment variables during deserialization. Before the patch, this feature <em>secrets_from_env</em> was enabled by default:</p>



<pre><code>if (
&nbsp;&nbsp;&nbsp;&nbsp;value.get("lc") == 1
&nbsp;&nbsp;&nbsp;&nbsp;and value.get("type") == "secret"
&nbsp;&nbsp;&nbsp;&nbsp;and value.get("id") is not None
):
&nbsp;&nbsp;&nbsp;&nbsp;[key] = value["id"]
&nbsp;&nbsp;&nbsp;&nbsp;if key in self.secrets_map:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return self.secrets_map[key]
&nbsp;&nbsp;&nbsp;&nbsp;if self.secrets_from_env and key in os.environ and os.environ[key]:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return os.environ[key] # <strong>&lt;-- Returning env variable</strong>
&nbsp;   return None</code></pre>



<div><p>If a deserialized object is returned to an attacker, for example message history inside the LLM context, that could leak environment variables.</p><p>But the more interesting path is an indirect prompt injection.&nbsp;Even an attacker who cannot see any LLM responses can exfiltrate secrets by instantiating the right class. <em>ChatBedrockConverse</em> from <em>langchain_aws</em> is both in the default allowlist of <em>loads</em> and makes a GET request on construction.<br>The GET endpoint is attacker-controlled, and a specific HTTP header can be populated with an environment variable via the <em>secrets_from_env</em> feature.</p></div>



<figure><img loading="lazy" decoding="async" width="720" height="403" src="https://cyata.ai/wp-content/uploads/2025/12/image-1.png" alt="" srcset="https://cyata.ai/wp-content/uploads/2025/12/image-1.png 720w, https://cyata.ai/wp-content/uploads/2025/12/image-1-300x168.png 300w, https://cyata.ai/wp-content/uploads/2025/12/image-1-372x208.png 372w" sizes="auto, (max-width: 720px) 100vw, 720px"></figure>



<p>This validator runs when <em>ChatBedrockConverse</em> is instantiated. The attacker controls <em>endpoint_url</em>, triggering an outbound request. Combined with <em>secrets_from_env</em>, the <em>aws_access_key_id</em> header can be populated with any environment variable – not just AWS keys.</p>



<p>We are intentionally not publishing a copy-paste exploit here, to give time for security teams. In a few months, the Huntr website will publish them automatically.</p>



<h3><strong>Code Execution via jinja2 templates</strong></h3>



<p>Among the classes in the default <em>loads()</em> allowlist is <em>PromptTemplate</em>. This class creates a prompt from a template, and one of the available template formats is Jinja2.</p>



<p>When a template is rendered with Jinja2, arbitrary Python code can run. We did not find a way to trigger this directly from the <em>loads()</em> function alone, but if a subsequent call to the deserialized object triggers rendering, code execution follows.</p>



<p>We suspect there may be paths to direct code execution from <em>loads()</em>, but we have not confirmed one yet. If you have a solid idea or a lead worth testing, we’d love to hear from you – this is exactly where the security community helps turn hypotheses into proof. 🤝</p>



<p>Also worth noting: in past versions, the Chain class was also in the allowlist. That class has special features that might have enabled a flow to template rendering.</p>



<h2><strong>Who is affected? The practical checklist</strong></h2>



<p>Your application is potentially exposed if it uses vulnerable langchain-core versions. Here are some of the most common vulnerable patterns (12 flows have been identified in total):</p>



<ul>
<li>astream_events(version=”v1″) (v1 uses the vulnerable serialization; v2 is not vulnerable)</li>



<li>Runnable.astream_log()</li>



<li>dumps() / dumpd() on untrusted data, followed by load() / loads()</li>



<li>Deserializing untrusted data with load() / loads()</li>



<li>Internal serialization flows like RunnableWithMessageHistory, InMemoryVectorStore.load(), certain caches, pulling manifests from LangChain Hub (hub.pull), and other listed components in the advisory</li>
</ul>



<p>That said, the system’s behavior is complex enough that it’s risky to assume a quick code-path review will catch every reachable variant. <strong>The safer course is to upgrade to a patched version </strong>and not assume you’re in the clear until you do.</p>



<p>Also, the advisory calls out what I consider the most important real-world point:</p>



<p>The most common attack vector is through LLM response fields like additional_kwargs or response_metadata, which can be controlled via prompt injection and then serialized/deserialized in streaming operations.&nbsp;</p>



<p>This is exactly the kind of “AI meets classic security” intersection where organizations get caught off guard. LLM output is an untrusted input. If your framework treats portions of that output as structured objects later, you must assume attackers will try to shape it.</p>



<h2><strong>Defensive guidance: how to respond in production</strong></h2>



<h3><strong>1) Patch first (this is the fastest risk reduction)</strong></h3>



<p>Upgrade langchain-core to a patched version. If you are using langchain, langchain-community, or other ecosystem packages, validate what version of langchain-core is actually installed in production environments.</p>



<h3><strong>2) Assume LLM outputs can be attacker-shaped</strong></h3>



<p>Treat additional_kwargs, response_metadata, tool outputs, retrieved documents, and message history as untrusted unless proven otherwise. This is especially important if you stream logs/events and later rehydrate them with a loader.</p>



<h3><strong>3) Review deserialization features like secret resolution</strong></h3>



<p>Even after upgrading, keep the principle: do not enable secret resolution from environment variables unless you trust the serialized input. The project changed defaults for a reason.&nbsp;</p>



<h2><strong>The LangChainJS parallel</strong></h2>



<p>Based on my report, there is a closely related advisory in LangChainJS (GHSA-r399-636x-v7f6 / CVE-2025-68665) with similar mechanics: ‘lc’ marker confusion during serialization, enabling secret extraction and unsafe instantiation in certain configurations.&nbsp;</p>



<p>If your organization runs both Python and JavaScript LangChain stacks, treat this as a reminder that <strong>the pattern travels across ecosystems</strong>: marker-based serialization, untrusted model output, and later deserialization is a recurring risk shape.</p>



<h2><strong>Why this matters beyond LangChain</strong></h2>



<p>We are entering a phase where agentic AI frameworks are becoming critical infrastructure inside production systems. Serialization formats, orchestration pipelines, tool execution, caches, and tracing are no longer “plumbing” – they are part of your security boundary.</p>



<p>This vulnerability is not “just a bug in a library.” It is a case study in a bigger pattern:</p>



<ul>
<li>Your application might deserialize data it believes it produced safely.</li>



<li>But that serialized output can contain fields influenced by untrusted sources (including LLM outputs shaped by prompt injection).</li>



<li>A single reserved key used as an internal marker can become a pivot point into secrets and execution-adjacent behaviors.</li>
</ul>



<p>At Cyata, our work is to help organizations build <strong>visibility, risk assessment, control, and governance</strong> around AI systems – because if you can’t quickly answer <em>where agents are running, which versions are deployed, and what data flows through it</em>, you’re effectively flying blind when advisories like this land.</p>



<h2><strong>What this teaches us about AI governance</strong></h2>



<p>If you are a security leader reading this, here is the uncomfortable truth:</p>



<p>Most organizations cannot currently answer, quickly and confidently:</p>



<ul>
<li>Where are we using agents?</li>



<li>Which versions are deployed in production?</li>



<li>Which services have access to sensitive secrets?</li>



<li>Where do LLM outputs cross those boundaries?</li>
</ul>



<p>That is not a “developer problem.” It is a visibility and governance problem.</p>



<p>And that is where Cyata comes in.</p>



<h2><strong>How Cyata helps: visibility, risk assessment, control, governance</strong></h2>



<p>At Cyata, we focus on a practical outcome: reducing AI and agent risk without slowing down builders. Vulnerabilities like this one are rarely “just a patch.” They expose gaps in how teams discover where agents run, understand real trust boundaries, and enforce safer defaults across fast-moving frameworks.</p>



<h3><strong>Visibility</strong></h3>



<p>Know what is running, where, and how it is wired.</p>



<p>Answer the first CVE question quickly: <strong>are we exposed, and in which flows?</strong></p>



<p>Discover agent runtimes and integrations across environments (IDEs, CI, services, worker jobs, hosted agents).</p>



<p>Track frameworks, packages, and versions in use.</p>



<h3><strong>Risk assessment</strong></h3>



<p>Prioritize what matters based on real-world blast radius, not just “library present.”</p>



<p>Support faster triage: what is internet-facing, what touches secrets, what runs with elevated credentials.</p>



<p>Identify the highest-risk paths: untrusted content flowing into privileged contexts (services with secrets, broad tool permissions, prod network access).</p>



<p>Highlight where “structured fields” can cross trust boundaries (metadata, tool outputs, streamed events, cached artifacts).</p>



<h3><strong>Control</strong></h3>



<p>Reduce exposure even before every dependency is patched everywhere.</p>



<p>Encourage safer operational defaults: least privilege, isolation boundaries, and policy checks that scale across teams.</p>



<p>Enforce guardrails around risky patterns (for example: deserializing untrusted data, permissive object revival, unsafe streaming-to-cache-to-rehydrate flows).</p>



<p>Gate or restrict sensitive capabilities in untrusted contexts (for example: secret access via environment, high-privilege tool execution, or running risky code paths in privileged workers).</p>



<h3><strong>Governance</strong></h3>



<p>Make “safe agent usage” repeatable, auditable, and hard to drift.</p>



<ul>
<li>Define policies for <strong>approved frameworks, versions, and configurations</strong>.</li>



<li>Track and time-box exceptions with owners and rationale.</li>



<li>Monitor for drift and risky feature usage over time, with an audit trail that supports security reviews and compliance.</li>
</ul>



<p>When a Christmas-week advisory drops, the goal is not heroics – it’s calm, controlled response backed by real inventory and enforced guardrails.</p>



<h2><strong>Disclosure Timeline</strong></h2>



<p>Report submitted via Huntr – December 4th, 2025</p>



<p>Acknowledged by LangChain maintainers – December 5th, 2025&nbsp;</p>



<p>Advisory and CVE published – December 24th, 2025</p>


<div>
		<p>The Control Plane  for Agentic Identity</p>
					
			</div>




</div></div>]]></description>
        </item>
    </channel>
</rss>