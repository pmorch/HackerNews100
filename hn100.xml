<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 04 Feb 2024 20:00:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Browser extensions are underrated: the promise of hackable software (224 pts)]]></title>
            <link>https://www.geoffreylitt.com/2019/07/29/browser-extensions</link>
            <guid>39251095</guid>
            <pubDate>Sun, 04 Feb 2024 15:43:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.geoffreylitt.com/2019/07/29/browser-extensions">https://www.geoffreylitt.com/2019/07/29/browser-extensions</a>, See on <a href="https://news.ycombinator.com/item?id=39251095">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure>
  <img src="https://www.geoffreylitt.com/images/article_images/legos.jpg?1705878965" alt="Lego bricks">
  <figcaption>Photo by <a href="https://unsplash.com/photos/2FaCKyEEtis">Rick Mason on Unsplash</a></figcaption>
</figure>

<p>Recent conversations about web browser extensions have focused on controversy: <a href="https://arstechnica.com/information-technology/2019/07/dataspii-inside-the-debacle-that-dished-private-data-from-apple-tesla-blue-origin-and-4m-people/">malicious browser extensions capturing web history</a>, and <a href="https://www.wired.com/story/google-chrome-ad-blockers-extensions-api/?verso=true">Google limiting the capabilities used by ad blockers</a>. These are important discussions, but we shouldn’t lose sight of the big picture: browser extensions are a special ecosystem worth celebrating.</p>

<p>Among major software platforms today, <strong>browser extensions are the rare exception that allow and encourage users to modify the apps that we use</strong>, in creative ways not intended by their original developers. On smartphone and desktop platforms, this sort of behavior ranges from unusual to impossible, but in the browser it’s an everyday activity.</p>

<p>Browser extensions remind us what it’s like to have deep control over how we use our computers.</p>

<h2 id="assembling-our-own-software">Assembling our own software</h2>

<p>Once a software platform reaches a certain level of openness, it can fundamentally change the way that normal users relate to their software. By installing four different Gmail extensions that modify everything from the visual design to the core functionality, in some sense, I’ve put together my own email client. <strong>Instead of being a passive user of pre-built applications, I can start assembling my own personalized way of using my computer.</strong></p>

<p>The popularity of browser extensions proves that many people are interested in customizing their software, and it’s not just  a hobby for power users. There are over 180,000 extensions on the Chrome store, and nearly half of all Chrome users have browser extensions installed.<sup id="fnref1"><a href="#fn1" rel="footnote">1</a></sup> When people have an easy way to extend their software with useful functionality, they apparently take advantage.</p>

<h2 id="hackable-platforms-not-custom-apis">Hackable platforms, not custom APIs</h2>

<p>Browser extensions have remarkably broad use cases. I personally use Chrome extensions that fill in my passwords, help me read Japanese kanji, simplify the visual design of Gmail, let me highlight and annotate articles, save articles for later reading, play videos at 2x speed, and of course, block ads.</p>

<p><strong>The key to this breadth is that most extensions modify applications in ways that the original developers didn’t specifically plan for.</strong> When Japanese newspapers publish articles, they’re not thinking about compatibility with the kanji reading extension. Extension authors gain creative freedom because they don’t need to use application-specific APIs that reflect the original developers’ view of how people might want to extend their application.</p>

<p>The web platform has a few qualities that enable this sort of unplanned extensibility. The foundational one is that the classic web deployment style is to ship all the client code to the browser in human-readable form. (Source maps are a key to preserving this advantage as we ship more code that’s minified or compiled from other languages.) The web’s layout model also promotes extensibility by encouraging standardized semantic markup—my password manager extension works because web pages reliably use form tags for password submissions instead of building their own version.</p>

<p>Even with these advantages, it can still require clever tricks to modify a site in ways that it wasn’t built for. But it’s often a reasonable amount of work, not a years-long reverse engineering effort. The sheer variety of extensions available shows that extension authors are willing to jump through a few hoops to create useful software.</p>

<p>Occasionally there are tensions between website developers and extension authors, but it seems far more common that developers are fine with their sites being extended in creative ways, as long as they don’t have to do any extra work. Extensions can even make life easier for application developers: if there’s a niche request that a small minority of users want, a motivated community member can just build an extension to support it. By building on a hackable platform, developers allow their users to get even more value out of their applications.</p>



<p>Many browser extensions are generic tools designed to enhance my use of all websites. I can use my annotation extension on every website everywhere, instead of needing a different highlighting tool for each article I read. Just like using a physical highlighter with paper articles, I can master the tool once, and get a lot of leverage by applying it in different contexts.</p>

<p>In many software platforms, we think of the operating system as providing the cross-cutting tools, and third parties as providing standalone “apps” that are used in isolation. <strong>With browser extensions, third parties are also adding tools;</strong> a single piece of software has the leverage to change my experience across all the apps I use.</p>

<p>When software is built in small units, it also changes the economics. Most extensions I use are free, and are perhaps too small in their feature set to support a full-blown business. And yet, people still choose to make them, and I benefit immensely from these little bits of software. Browsing the extension store feels more like going to a local flea market than going to a supermarket. Massive software built by huge companies isn’t the only way.</p>

<h2 id="the-origins-of-openness">The origins of openness</h2>

<p>It’s not an accident that this openness emerged on the web platform.</p>

<p>Since the beginning of personal computing, there’s been a philosophical tradition that encourages using computers as an interactive medium where people contribute their own ideas and build their own tools—authorship over consumption. This idea is reflected in systems like Smalltalk, Hypercard, and more recently, <a href="https://dynamicland.org/">Dynamicland</a>.</p>

<p>When Tim Berners-Lee created the World Wide Web, he imagined it fitting into this tradition. “My vision was a system in which sharing what you knew or thought should be as easy as learning what someone else knew.”<sup id="fnref2"><a href="#fn2" rel="footnote">2</a></sup> There were some hiccups along the way<sup id="fnref3"><a href="#fn3" rel="footnote">3</a></sup>, but eventually that vision largely won out, and the Web became a place where anyone can publish their opinions or photos through social media platforms.</p>

<p>Still, there’s a catch. When you’re using Facebook, you’re operating within a confined experience. You’re forced to publish in a certain format, and to use their app in a certain way (that includes, of course, seeing all the ads). There’s more room for authorship than just browsing a news website, but only within the strict lines the app has painted for you.</p>

<p><strong>Browser extensions offer a deeper type of control.</strong> Instead of merely typing into the provided text box, we can color outside the lines and deeply modify the way we use any application on the web. Browser extensions offer a kind of decentralization: large companies building major websites don’t get to dictate all the details of our experience.</p>

<h2 id="improving-on-extensions">Improving on extensions</h2>

<p>We clearly need to work on protecting people from malicious extensions that invade their privacy. But beyond that, here are some bigger picture opportunities I see for improving on extensions:</p>

<p><strong>Accessibility:</strong> Today, it requires a big jump to go from using browser extensions to creating them: you need to learn a fair amount of web development to get started, and you can’t easily develop extensions in the browser itself. What if there were a quick way to get started developing and sharing extensions in the browser? You could imagine smoothly transitioning from editing a website in the developer tools to publishing a small extension.</p>

<p><em>Update</em>: I’ve started working on a system called <a href="https://sdg.csail.mit.edu/projects/wildcard">Wildcard</a> to work towards this vision.</p>

<p><strong>Compatibility:</strong> Because extensions hook into websites in unsupported ways, updates to websites often result in extensions temporarily breaking, and extension authors scrambling to fix them. Can we make it easier for website developers and extension authors to form stable connections between their software, without necessarily resorting to using explicit extension APIs?</p>

<p>There are existing practices that fit into this category already—for example, using clean semantic markup, human-readable CSS, and source maps makes it easier to develop an extension.</p>

<p>A simple change that would allow for more stable extensions would be to give users more control over when they upgrade to new versions of cloud software. If I have a 3 month window to continue using an old version after the new one is released, that would give extension authors more time to upgrade their software for the new version.</p>

<p><strong>Power:</strong> Web extensions are limited in their power by the typical architecture of web applications: they have broad rights to modify the browser client, but the server is off limits. For example, if my social media app’s server only provides an endpoint for querying my posts in chronological order, no browser extension can ever search through all my posts by keyword. How could we rethink the client-server boundary to enable extensions to make even deeper modifications?</p>

<p>This raises tough questions around security and privacy. The modern browser extension API has done a good job balancing extensibility with security, and yet we’re still grappling with the consequences of browser extensions invading people’s privacy. Giving extensions more power would raise the stakes further. Still, we shouldn’t give up in the name of security—we should fight for extensibility as a value and find ways to balance these interests.</p>

<h2 id="the-next-platform">The next platform</h2>

<p>I’m intrigued by a couple projects that are rethinking the web in ways that might make it more extensible:</p>

<p>The <a href="https://beakerbrowser.com/about/">Beaker Browser</a> and the decentralized web community are exploring how the web works without centralized servers. It seems like their proposed architecture would give users fuller control over modifying the “server” side of web applications.</p>

<p>Tim Berners-Lee is working on a new project called <a href="https://inrupt.com/blog/one-small-step-for-the-web">SOLID</a>. I don’t yet understand precisely what they’re up to, but given Tim’s involvement I figure it’s worth paying attention. A key principle is giving users more ownership over their data, which would enable people to use extensions and other software to manipulate their data in flexible ways beyond what application server APIs allow.</p>

<p>Computing is still young, and platforms are changing quickly. Modern browser extensions and smartphone platforms have only been around for about a decade. These platforms will evolve, and there will be new platforms after them, and we will get to collectively decide how open they will be.</p>

<p>Browser extensions give us one example to strive for: a place where we routinely hack the software we use and make it our own. <span>▪</span></p>

<p><a href="https://news.ycombinator.com/item?id=20556382"><em>Discuss on Hacker News</em>
</a></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Plastic bans work. Billions of plastic bags were avoided in the US alone (165 pts)]]></title>
            <link>https://www.zmescience.com/science/news-science/plastic-bans-work-billions-of-plastic-bags-were-avoided-in-the-us-alone/</link>
            <guid>39250434</guid>
            <pubDate>Sun, 04 Feb 2024 14:28:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.zmescience.com/science/news-science/plastic-bans-work-billions-of-plastic-bags-were-avoided-in-the-us-alone/">https://www.zmescience.com/science/news-science/plastic-bans-work-billions-of-plastic-bags-were-avoided-in-the-us-alone/</a>, See on <a href="https://news.ycombinator.com/item?id=39250434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
								
<p>“The bottom line is that <a href="https://www.zmescience.com/ecology/environmental-issues/congo-bans-plastic-bags-321313/">plastic bag bans</a> work,” said Faye Park, president of the U.S. PIRG Education Fund, in a statement. “People realize quickly it’s easy to live without plastic bags and get used to bringing a bag from home or skipping a bag when they can.”</p>



<figure><a href="https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-scaled.jpg"><picture><source srcset="https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1024x683.webp 1024w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1536x1024.jpg 1536w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-2048x1365.jpg 2048w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-750x500.jpg 750w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1140x760.jpg 1140w" sizes="(max-width: 1024px) 100vw, 1024px" type="image/webp"><img src="https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1024x683.jpg" height="683" width="1024" srcset="https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1024x683.jpg 1024w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1536x1024.jpg 1536w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-2048x1365.jpg 2048w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-750x500.jpg 750w, https://cdn.zmescience.com/wp-content/uploads/2024/01/brian-yurasits-jVXgfjSDnJI-unsplash-1140x760.jpg 1140w" sizes="(max-width: 1024px) 100vw, 1024px" alt="plastic bag waste" fetchpriority="high" decoding="async"> </picture></a><figcaption>Image via Unsplash.</figcaption></figure>



<h2 id="plastic-ain-t-all-that-fantastic">Plastic ain’t all that fantastic</h2>



<p>Plastic bags are a victim of their own success. When they <a href="https://www.unep.org/news-and-stories/story/birth-ban-history-plastic-shopping-bag">were first patented in Europe in 1965</a>, society was shocked to see how cheap and durable they could be. Within a decade or two they became mainstream on the continent and in North America, and it wasn’t long before they started being widely used on the entire planet.</p>



<p>But plastics were just a little too durable. They didn’t go away. They started accumulating in landfills and in the oceans. The environmental impact of plastic bags gained attention with the discovery of the Great <a href="https://www.zmescience.com/ecology/pollution-ecology/great-pacific-garbage-patch-06102016/">Pacific Garbage Patch</a> in 1997. Plastic bags (and plastic in general) had left its mark on the planet in an unprecedented form of pollution.</p><!-- Tag ID: zmescience_300x250_InContent -->





<p>Fast forward a couple more decades, and countries started fighting their urge to use <a href="https://www.zmescience.com/research/materials/cheap-fabrics-from-plastic-15032021/">cheap plastics</a> and implement bans or other measures against plastic bags — and finally, there’s some good news.</p>



<p>San Francisco pioneered the movement in the U.S. by passing the <a href="https://www.zmescience.com/ecology/environmental-issues/bottled-water-ban-national-park-43423/">nation’s first plastic bag ban</a> in 2007. Several other U.S. cities and <a href="https://www.zmescience.com/ecology/hawaii-bans-plastic-bags-04062012/">states implemented plastic bag bans</a> or restrictions. By 2023, ten states had statewide bans, with similar laws proposed in others​​. To get a state of how much this of a difference this made, five studied bans resulted in an average elimination of <a href="https://www.zmescience.com/ecology/plastic-bag-tax-uk-22112016/">almost 300 plastic bags</a> per person per year​​. Overall, in the US alone, <a href="https://www.zmescience.com/research/tea-plastic-particles-ocean-234523521/">billions of plastic bags</a> were avoided with anti-plastic bag measures.</p>




<h2 id="h-the-case-against-plastic">The case against plastic</h2>



<p>The case against plastic bags is straightforward.  Plastic pollution kills at least <a href="https://wwf.org.au/blogs/plastic-in-our-oceans-is-killing-marine-mammals/">100,000 marine mammals</a> and <a href="https://sustainabledevelopment.un.org/content/documents/Ocean_Factsheet_Pollution.pdf">1 million seabirds</a> every year and entanglement in plastic and other types of litter kills roughly 1,000 turtles per year. Plastic bags aren’t responsible for all of that, but they make up an important part of the problem.</p>







<blockquote>

</blockquote>



<p>The results, which were published in a report, also highlight that imperfect measures leave loopholes or encourage buyers to opt for other single use bags.</p>



<blockquote>
<p>Well-designed <a href="https://www.zmescience.com/science/news-science/europe-single-use-plastic-24102018/">single-use plastic bag bans</a> across the country have successfully reduced single-use plastic bag consumption, cut down on plastic bag litter and driven consumers to make more sustainable bag choices. Policymakers should pursue these policies at the state and local levels,” the report says.</p>
</blockquote>



<p>The idea isn’t to shift from one type of single-use bag to another type of single-use bag. Paper bags are easier to recycle than plastic, but they take 3-4 times more energy to produce and usually generate more solid waste.</p><!-- Tag ID: zmescience_300x250_InContent_3 -->




<p>Ultimately, the report concludes that regulation is the best current way to address plastic waste and plastic pollution.</p>



<blockquote>
<p>“Grocery stores, restaurants and retail shops should not be permitted to distribute plastic film bags of any thickness at checkout. Stores should be required to charge a fee of at least 10 cents for single-use paper bags. A 10-cent paper bag fee will limit the expected increase in paper bag use after a bag ban is imposed and may even reduce paper bag consumption altogether.”</p>



<p>“Local and state governments should conduct regular enforcement to ensure compliance.”</p>
</blockquote>



<p>You can read the <a href="https://environmentamerica.org/center/resources/plastic-bag-bans-work/">report in its entirety here</a>.</p>

<!-- AI CONTENT END 1 -->
								
								
																	
																	
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Write code for the web - Apple doesn't care about you, Mr. Developer (388 pts)]]></title>
            <link>https://mrmr.io/apple/</link>
            <guid>39250406</guid>
            <pubDate>Sun, 04 Feb 2024 14:24:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mrmr.io/apple/">https://mrmr.io/apple/</a>, See on <a href="https://news.ycombinator.com/item?id=39250406">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tabindex="-1" id="___gatsby"><main><div><h3>Write code for the web</h3><p>This is a yarn of three threads, and it got a bit long. The tldr is</p>
<ol>
<li>
<p>Apple doesn’t care for me as a developer</p>
</li>
<li>
<p>I should write code for the web</p>
</li>
<li>
<p>Nothing is set in stone, really</p>
</li>
</ol>
<p>The story is personal, but I hope readers find something they can relate to in
their own life stream (I guess that's the point of blogs)</p>
<h3>Apple doesn't care for you, Mr. Developer</h3>
<p>Apple cares for me as a customer, but it doesn’t care for me as a developer.
This dynamic had been subconsciously griefing me for years, until I was able to
formulate it consciously recently.</p>
<p>The dependency goes <em>Developer -&gt; Apple</em> and <em>Apple -&gt; Consumer</em>, there is no
reverse arrow from Apple to the developers.</p>
<p>If all developers stopped building for Apple's platforms tomorrow, Apple will
still survive, almost intact, since it doesn’t hurt their core value prop (I'll
expand on this below). Apple has no dependency on individual developers. They
care for and need to collaborate with corporate dev “partners”, but that's
different.</p>
<p>Since companies, especially ginormous multinationals, behave in purely game
theoretic cost/benefit terms, and since Apple has no reason to care about
developers: indeed it doesn’t.</p>
<blockquote>
<p>Just to restate the obvious - such a stance isn't necessarily the case. There
are other multinationals who've figured that developers form a core part of
their strategy.</p>
</blockquote>
<p>This realisation has made me happier since I now know my place. I can like their
products without wanting to develop for them.</p>
<hr>
<p>Google has a bug. I have dynamic light/dark mode, and if I search on Google at
night, the first page shows up in light mode. With the rest of my machine in a
subdued state, the glaring white of the background hurts my eyes.</p>
<p>In the morning, my laptop automatically switches back to light mode. Now when I
search on Google, the results first show up in an unreadable black background.</p>
<p>One would think that of all the leetcode certified staff, there must be someone
there who would know an O(n) algorithm for fixing this bug. But no, this bug has
persisted for years (I've counted), and it is unlikely to be fixed in the future
unless it gets accidentally fixed as part of some overhaul.</p>
<p>Apologies for the snide, I know some great people who work there. My point is
that Google isn't fixing this bug not because it doesn’t know how to, but
because it doesn’t care. This bug has zero impact on its bottom line.</p>
<p>The people like me who use alternate search engines like DDG have already moved
on years ago. The rest of the (overwhelming) majority is stuck with Google. No
matter how bad their search is - UX or results - they have a captive audience.</p>
<blockquote>
<p>I think DDG etc also have a marketing blind spot - they keep pitching
themselves as a more privacy friendly alternative, they never go after the
main course. I don’t use DDG because of its privacy benefits, I use it because
it reminds me of early Google - simple low clutter UX, good quality verbatim
search results, and unobtrusive ads.</p>
</blockquote>
<p>Okay Manav, but I thought you were ranting about Apple, why bring in Google?</p>
<p>Google is an example where I never grieved much because I understood the
dynamics. I know that I, the customer, am not their game theoretic target. So
when I have to invariably use their products and face another user hostile
interaction, I try to shrug it off and move on. I know that Google has entered a
rent seeking phase, and while it is sad that the world is giving all its video
content up to it for even more of a hostage situation in the future, but that’s
for governments to deal with.</p>
<p>With Apple I didn’t understand the dynamics.</p>
<hr>
<p>2009-ish. I struggled to find a computer for my mom. Windows (at that time, I
don't know about now) was just too insecure. Linux required constant tech
support. Eventually I prepared for her an OpenBSD machine running Firefox and a
basic game (Bubbles I think).</p>
<p>Obviously, this was not ideal. It fulfilled some goals - it worked without
requiring any tech support when I wasn't around, and I was ensuring her data
safety and privacy - and she was surprisingly happy with too, but I was not
happy about how this was such a shrivelled parody of what things could be, and
how it limited the ways she could use computers to enrich her life.</p>
<p>Around this time, I joined a new job as a developer for a company that was
making iOS apps. After a week or so of using the mac at work it hit me - <em>this
is the computer I wanted for my mom!</em></p>
<p>That is Apple’s value prop.</p>
<p>As soon as I had saved enough I bought her a MacBook. And heartfeltly thanked
Steve Jobs for engendering it.</p>
<p>The earth has done many a revolutions since then, and Jobs has left earth, and
the form factor my mother uses has changed from a laptop to an iPad. But it
still satisfies that core value prop.</p>
<p>Let's consider a different context. Even if there were no apps in my phone, I
would still buy an iPhone. For myself likely, but most certainly for her.</p>
<p>The people running Apple know all this. In the deep dungeons of Menlo Park when
there are meetings of the core council, after all the sacrificial lambs have
been slain and the blood and gore washed away, out comes the elder spreadsheet
that encodes Apple’s business model, but nowhere in them is any cell, input or
output, which involves developers. Sure, Apple doesn't mind if developers are
also happy. But it knows it doesn’t need to care if they are.</p>
<p>This lack of caring is never expressed out loud. It isn’t some conspiracy, it
just is one of those things - you wouldn’t walk up to someone who you don’t care
about and tell them you don’t care.</p>
<p>Unfortunately all this results in sometimes schizophrenic behaviour on Apple’s
part, as there are many individuals working at Apple who <em>do care</em> about
developers and are trying to make things better.</p>
<hr>
<p>2016-ish. As part of my annual Apple simping I was watching WWDC when they
announced Apple Music APIs.</p>
<p>I was ecstatic. My coworkers were puzzled at my ecstasy, “All this, can’t we
already do with Spotify’s API?”. I didn’t know how to answer that, so I just
repeated how <em>this changes everything</em>.</p>
<p>Of course, and as is usual, I was wrong. It didn’t change everything. In fact,
it didn’t change <em>anything</em>.</p>
<p>Apple’s own music player was, and still is, unusably bad. Even talking about it
makes me angry. The people making it can’t be so incompetent, and it seems to be
working for the rest of the world, so I've never known what to make of this
situation.</p>
<p>With the APIs out, I'd thought maybe things will change. But nothing happened.
Apple’s own player continues to make my blood pressure high anytime I have to
use it. And whatever alternative players I have tried just all seem to go for
the same generic “Spotify” approach to music.</p>
<blockquote>
<p>I think it is because the people who're making these apps were never around in
the Justin Frankel era of Winamp, and haven't seen how a music player can
provide a fast, seamless, endless <em>yet</em> still personal approach to music.</p>
<p>That era is not coming back because it relied on piracy, but luckily we don't
need to anymore - that's the great thing about Apple's music catalog! We can
recreate that experience without needing to sail the high seas.</p>
</blockquote>
<p>So that's the backstory. Now recently I had a bunch of free time, and thought
that I’ll write a music player. Mostly for myself, but I also wanted to write a
tutorial. This is what I wrote in the README in the first commit:</p>
<blockquote>
<p>Here I'll be writing down my notes as I build Flowers. The world needs not one
music player, or two, but many: each of us has our own way of connecting with
music, and so maybe these notes will help others build the flower they want,
nay need.</p>
</blockquote>
<p>Cute, dumb, and in vain.</p>
<p>As I went about it, I realized that 8 years down the line, not only is the API
still buggy, it is also still not public!</p>
<p>Firstly you need to pay Apple. No, not for bulk usage etc, but <em>just to try out
the API</em>. So there there goes my dream of writing a walkthrough – nobody’s going
to pay Apple 100 bucks just to try things out. And it also partially explains
why there has been no innovation.</p>
<p>But that's not even it - Even if you pay them, you get a restricted API.</p>
<p>The point where I disgustedly gave up was when I found out that while I was
jumping all these kafkaesque hoops, instead you could just go to Apple's own web
music player, type <code>MusicKit.getInstance().developerToken</code> in your browser
console, and you’ll get an unrestricted root token for free! </p>
<hr>
<p>All these anecdotes Manav, what does all this mean?</p>

<p>Write code that runs on the web. We’re lucky to have a shared platform that no
single entity owns. Even benevolence can be ruined by incompetence.</p>
<p>The web platform is in a precarious place – overreaching governments, browser
duopolies, a complex developer ecosystem – so it is not a given it’ll remain
thriving. But so far it has survived. And every year longer it survives, the
more the chances that it’ll continue to thrive in the future.</p>
<p>Ironically Google is the good guy here, they’re doing great work for the web. On
the other hand, literally every single workaround I’ve had to write in the
recent past in web related code has been due to Safari's princessness.</p>
<h2>Nothing is set in stone</h2>
<p>Which brings me to the third thread of this story, how all this made me
reevalute my relationship with companies.</p>
<p>Someone I know, someone who has a better grip on living than me, told me once
that it's not useful to put people into the buckets of good and bad. People are
a mix. Bad folks can do good actions sometimes, and vice versa.</p>
<p>I don't know to what extent I've been able to internalize their message, but
that's for another day. What I realized is that the same applies to companies.</p>
<p>Companies are like people. I don't know if they're sentient, but otherwise they
share many attributes with us: they're intelligent (they were the AI before AI),
have personalities, they are born, thrive, live and die, they're even legal
persons.</p>
<p>Just like we can't live without other people, we can't live without companies.
They have many inhumane characteristics, but like them or not, such fractal
conceptions of human organizations will always be around.</p>
<p>By not permanently bucketing companies into good or bad, I can have a more fluid
interaction with them - I can reduce my dependence on them when they try to put
me in a zero sum game, or reengage more with them if they're willing to be more
symbiotic. Nothing is set in stone, really.</p>
<hr>
<blockquote><p>I think most large companies and medium-size companies, and even small
companies, are starting to look at the web as the ultimate direct-to-customer
distribution chain, bypassing all middlemen, going directly from the supplier to
the consumer.</p><p>– Steve Jobs, Make Something Wonderful</p></blockquote></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New Linux glibc flaw lets attackers get root on major distros (120 pts)]]></title>
            <link>https://www.bleepingcomputer.com/news/security/new-linux-glibc-flaw-lets-attackers-get-root-on-major-distros/</link>
            <guid>39250076</guid>
            <pubDate>Sun, 04 Feb 2024 13:35:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bleepingcomputer.com/news/security/new-linux-glibc-flaw-lets-attackers-get-root-on-major-distros/">https://www.bleepingcomputer.com/news/security/new-linux-glibc-flaw-lets-attackers-get-root-on-major-distros/</a>, See on <a href="https://news.ycombinator.com/item?id=39250076">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="Linux" height="900" src="https://www.bleepstatic.com/content/hl-images/2023/06/22/Linux.jpg" width="1600"></p>
<p>​Unprivileged attackers can get root access on multiple major Linux distributions in default configurations by exploiting a newly disclosed local privilege escalation (LPE) vulnerability in the GNU C Library (glibc).</p>
<p>Tracked as <a href="https://www.qualys.com/2024/01/30/cve-2023-6246/syslog.txt" target="_blank" rel="nofollow noopener">CVE-2023-6246</a>, this security flaw was found in glibc's __vsyslog_internal() function, called by the widely-used syslog and vsyslog functions for writing messages to the system message logger.</p>
<p>The bug is due to a <a href="https://cwe.mitre.org/data/definitions/122.html" target="_blank" rel="nofollow noopener">heap-based buffer overflow weakness</a> accidentally introduced in glibc 2.37 in August 2022 and later backported to glibc 2.36 when addressing a less severe vulnerability tracked as CVE-2022-39046.</p>
<p>"The buffer overflow issue poses a significant threat as it could allow local privilege escalation, enabling an unprivileged user to gain full root access through crafted inputs to applications that employ these logging functions," Qualys security researchers said.</p>
<p>"Although the vulnerability requires specific conditions to be exploited (such as an unusually long argv[0] or openlog() ident argument), its impact is significant due to the widespread use of the affected library."</p>
<h2>Impacts Debian, Ubuntu, and Fedora systems</h2>
<p>While testing their findings, Qualys confirmed that Debian 12 and 13, Ubuntu 23.04 and 23.10, and Fedora 37 to 39 were all vulnerable to CVE-2023-6246 exploits, allowing any unprivileged user to escalate privileges to full root access on default installations.</p>
<p>Although their tests were limited to a handful of distros, the researchers added that "other distributions are probably also exploitable."</p>
<p>While analyzing glibc for other potential security issues, the researchers also found three other vulnerabilities, two of them—harder to exploit—in the __vsyslog_internal() function (CVE-2023-6779 and CVE-2023-6780) and a third one (a <a href="https://www.qualys.com/2024/01/30/qsort.txt" target="_blank" rel="nofollow noopener">memory corruption issue</a> still waiting for a CVEID) in glibc's qsort () function.</p>
<p>"The recent discovery of these vulnerabilities is not just a technical concern but a matter of widespread security implications,"&nbsp;<a href="https://blog.qualys.com/vulnerabilities-threat-research/2024/01/30/qualys-tru-discovers-important-vulnerabilities-in-gnu-c-librarys-syslog" target="_blank" rel="nofollow noopener">said</a> Saeed Abbasi, Product Manager at Qualys' Threat Research Unit.</p>
<p>"These flaws highlight the critical need for strict security measures in software development, especially for core libraries widely used across many systems and applications."</p>
<h2>Other Linux root escalation flaws found by Qualys</h2>
<p>Over the past few years, researchers at Qualys have found several other Linux security vulnerabilities that can let attackers gain complete control over unpatched Linux systems, even in default configurations.</p>
<p>Vulnerabilities they discovered include a flaw in glibc's ld.so dynamic loader (<a href="https://www.bleepingcomputer.com/news/security/new-looney-tunables-linux-bug-gives-root-on-major-distros/" target="_blank">Looney Tunables</a>), one in Polkit's pkexec component (<a href="https://www.bleepingcomputer.com/news/security/linux-system-service-bug-gives-root-on-all-major-distros-exploit-released/" target="_blank">dubbed PwnKit</a>), another in the Kernel's filesystem layer (<a href="https://www.bleepingcomputer.com/news/security/new-linux-kernel-bug-lets-you-get-root-on-most-modern-distros/" target="_blank">dubbed Sequoia</a>), and in the Sudo Unix program (aka <a href="https://www.bleepingcomputer.com/news/security/new-linux-sudo-flaw-lets-local-users-gain-root-privileges/" target="_blank">Baron Samedit</a>).</p>
<p>Days after the Looney Tunables flaw (<a href="https://access.redhat.com/security/cve/cve-2023-4911" target="_blank" rel="nofollow noopener">CVE-2023-4911</a>) was disclosed, proof-of-concept (PoC) exploits were <a href="https://www.bleepingcomputer.com/news/security/exploits-released-for-linux-flaw-giving-root-on-major-distros/" target="_blank">published online</a>, and threat actors <a href="https://www.bleepingcomputer.com/news/security/hackers-exploit-looney-tunables-linux-bug-steal-cloud-creds/" target="_blank">started exploiting it</a> one month later to steal cloud service provider (CSP) credentials in Kinsing malware attacks.</p>
<p>The Kinsing gang is known for deploying cryptocurrency mining malware on compromised cloud-based systems, including Kubernetes, Docker APIs, Redis, and Jenkins servers.</p>
<p>CISA later <a href="https://www.bleepingcomputer.com/news/security/cisa-orders-federal-agencies-to-patch-looney-tunables-linux-bug/" target="_blank">ordered U.S. federal agencies</a> to secure their Linux systems against CVE-2023-4911 attacks after adding it to its catalog of actively exploited bugs and tagging it as posing "significant risks to the federal enterprise."</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rye: A Vision Continued (106 pts)]]></title>
            <link>https://lucumr.pocoo.org/2024/2/4/rye-a-vision/</link>
            <guid>39249005</guid>
            <pubDate>Sun, 04 Feb 2024 10:15:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lucumr.pocoo.org/2024/2/4/rye-a-vision/">https://lucumr.pocoo.org/2024/2/4/rye-a-vision/</a>, See on <a href="https://news.ycombinator.com/item?id=39249005">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
  

  
  <p>written on Sunday, February 4, 2024
  

  </p><p>In April of last year I released <a href="https://rye-up.com/">Rye</a> to the public.
Rye, both then and now, represents my very personal vision of what an improved
Python packaging and project management solution can look like.
Essentially, it's a comprehensive user experience, designed so that the
only tool a Python programmer would need to interface with is Rye itself
and it gets you from zero to one in a minute.  It is capable of
bootstrapping Python by automatically downloading different Python
versions, it creates virtualenvs, it manages dependencies, and lints and
formats.  Initially developed for my own use, I decided to release it to
the public, and the feedback has been overwhelmingly positive.</p>
<p>When I introduced it, I initiated a discussion thread titled <a href="https://github.com/mitsuhiko/rye/discussions/6">“Should Rye
Exist”</a> referencing the
well known <a href="https://xkcd.com/927/">XKCD #929</a> which humorously comments
on the proliferation of competing standards.  I did not feel well throwing
yet another Python packaging tool into the ring.</p>
<p>Yet it exists now and has user.  This standard issue however I think is
helped a bit by the fact that Rye doesn't actually do any of these things
itself.  It wraps established tools:</p>
<ul>
<li><strong>Downloading Python</strong>: it provides an automated way to get access to
the amazing <a href="https://github.com/indygreg/python-build-standalone/">Indygreg Python Builds</a>
as well as the PyPy binary distributions.</li>
<li><strong>Linting and Formatting</strong>: it bundles <a href="https://github.com/astral-sh/ruff">ruff</a>
and makes it available with <cite>rye lint</cite> and <cite>rye fmt</cite>.</li>
<li><strong>Managing Virtualenvs</strong>: it uses the well established <a href="https://virtualenv.pypa.io/en/latest/">virtualenv</a> library under the hood.</li>
<li><strong>Building Wheels</strong>: it delegates that work largely to <a href="https://pypi.org/project/build/">build</a>.</li>
<li><strong>Publishing</strong>: its publish command uses <a href="https://pypi.org/project/twine/">twine</a> to accomplish this task.</li>
<li><strong>Locking and Dependency Installation:</strong> is today implemented by
using <a href="https://pypi.org/project/unearth/">unearth</a> and
<a href="https://github.com/jazzband/pip-tools/">pip-tools</a>.</li>
</ul>
<p>As you can see, Rye is not revolutionary and it's not intended to be.  Rye
itself doesn't do all that much as it delegates all the core functionality
to other tools in the ecosystem.  Rye packages these tools together in a
user-friendly manner, significantly reducing the cognitive load for
developers.  This convenience eliminates the need to learn about various
tools, read extensive documentation, and integrate these components
independently.  Rye lets you get from no Python on a computer to a fully
functioning Python project in under a minute with linting, formatting and
everything in place.  It is sufficiently opinionated that many important
decisions are made for you.  For instance it starts you out with using
<cite>pyproject.toml</cite> and picks a wheel build system for you.  It also picks
the linter and formatter, and the preferred Python distribution and
decides on a build tool.</p>
<div id="defaults-matter">
<h2>Defaults Matter</h2>
<p>Rye is designed to select the best tools for the job — it picks winners.
Why does it do that?  This approach is inspired by my admiration for the
developer experience in the Rust ecosystem, particularly the seamless
integration of <cite>rustup</cite> and <cite>cargo</cite>.  Their functionality made me long for
a similar experience within the Python community.  Crucially the way this
works in the Rust world does not mean that <cite>cargo</cite> does everything.  When
you run <cite>cargo build</cite> it invokes <cite>rustc</cite>, when you run <cite>cargo doc</cite> it runs
<cite>rustdoc</cite>.  When you invoke <cite>cargo clippy</cite> it runs <cite>clippy</cite> for you and so
worth.  Cargo is a manager that delegates the important work to bespoke
tools that are improved by sometimes entirely different teams.  This also
means that tools can be swapped out if they are found to be not the right
choice any more.  The experience in the Rust world also showed me that
excellent Windows support is just a must have.  That's why Rye is not just
a great experience on macOS and Linux, it's also excellent on Windows.</p>
<p>I am convinced that the Python community is deserving of an excellent
developer experience, and Rye, as it stands today, offers a promising
beginning.  My belief is supported by evidence gathered from conducting
in-person user interviews and demos, where Rye was well received.  In
fact, every individual who I was able to give a guided tour of Rye was
impressed by how swiftly one could start working with Python.  Because it
was demonstrably designed to avoid interference with any pre-existing
Python configurations, Rye allows for a smooth and gradual integration and
the emotional barrier of picking it up even for people who use other tools
was shown to be low.</p>
<p>That said, Rye is a one person project and it does not address the
fundamental challenges of some of the issues we have in the Python
ecosystem.  It does not solve multi version dependencies, it does not
offer better performance for the installation of dependencies.  It does
not help with distributing executables for end user applications or
anything like this.  However I am getting multiple signals that the time
is right for a tool like Rye to not just exist, but also to rally a larger
number of the Python community embrace some of these standardization
ideas.</p>
</div>
<div id="what-s-next">
<h2>What's Next?</h2>
<p><a href="https://github.com/Kwpolska">Chris Warrick</a> recently <a href="https://chriswarrick.com/blog/2024/01/15/python-packaging-one-year-later/">wrote a blog post</a>
where he looked back at the last year of Python packaging that made the
rounds on Twitter.  It laments a bit that we did not make much of a
progress in packaging and it also talks a bit about Rye and correctly
points out that Rye does not have enough contributors (basically just me).
That's not a healthy setup.</p>
<p>I still don't really know if Rye <em>should</em> exist.  It has not yet become
established and there are plenty of rough edges.  I personally really
enjoy using it but at the same time every time I use it, I get reminded
that it would stop existing if I did not invest time into it which in some
sense is what keeps me going on it.</p>
<p>However I would love to see the community converge to a Rye like solution,
no matter where it comes from.</p>
</div>
<div id="learn-more">
<h2>Learn More</h2>
<p>Did I spark your interest?  I would really appreciate it if you give it a
try and give feedback:</p>
<p><em>a 16 minute introduction to Rye</em>
    <iframe width="782" height="441" src="https://www.youtube.com/embed/q99TYA7LnuA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p><ul>
<li><a href="https://rye-up.com/">Project Website</a></li>
<li><a href="https://rye-up.com/guide/">User Guide and Documentation</a></li>
<li><a href="https://github.com/mitsuhiko/rye">GitHub Project</a></li>
<li><a href="https://github.com/mitsuhiko/rye/discussions">Discussion Forums</a></li>
<li><a href="https://discord.gg/drbkcdtSbg">Discord</a></li>
</ul>
</div>


  
  <p>This entry was tagged
    
      <a href="https://lucumr.pocoo.org/tags/announcement/">announcement</a> and 
      <a href="https://lucumr.pocoo.org/tags/python/">python</a>
  

      </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You are what you love (103 pts)]]></title>
            <link>https://gspanos.tech/posts/facts-1/</link>
            <guid>39248931</guid>
            <pubDate>Sun, 04 Feb 2024 09:57:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gspanos.tech/posts/facts-1/">https://gspanos.tech/posts/facts-1/</a>, See on <a href="https://news.ycombinator.com/item?id=39248931">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>  <blockquote>
<p>Facts is a series of articles where I express my foldable opinions. I know, right?</p>
</blockquote>
<h2 id="working-with-emotion">Working with Emotion</h2>
<p>I find it really challenging to talk with people who have completely separated their work from their emotional being. For me, work is part of life and it should be a meaningful one.</p>
<p>I recently had a talk with a close colleague that had to do with working with emotion. Their thesis was that there is no place for emotional thinking and analysis when working. They insisted on saying that the analytical brain should be almost exclusively responsible for how we behave in a work environment.</p>
<p>Not only do I find this contradicting everything that my work ethic consists of, but I also find this mantra unsustainable.</p>
<p>Firstly, let’s make clear that it’s unlikely that you’re good at something you don’t love. If you managed to do that, you’ve probably dedicated the hours to something that does not fulfill you. The goal is not happiness. It’s about fulfillment. After all, you have to do what you love. You <em>are</em> that, how can you be doing anything else?</p>
<p>This belief steers me towards believing that there has to be a clear emotional foundation on how we work. To be great, to achieve things, and to provide, we have to convert emotions into outcomes. Rather than the analytical brain being the fuel, it seems it’s more of a catalyst.</p>
<pre tabindex="0"><code><span><span>function</span><span> ValueOfOutcome</span><span>(</span><span>emotionalDrive</span><span>) {</span></span>
<span><span>	// real life situations modifier</span></span>
<span><span>	const</span><span> situationsModifier;</span></span>
<span><span>	// Value of Outcome is the result of the Analytical Process of an emotionalDrive times the situationModifier</span></span>
<span><span>	return</span><span> AnalyticalProcess</span><span>(emotionalDrive) </span><span>*</span><span> situationsModifier;</span></span>
<span><span>}</span></span></code></pre>
<p>Emotion cannot be separate from work. It has to be a part of it. When working, you’re expressing yourself. You express beliefs, opinions, and strategies, world views. You cannot detach yourself completely from work. I doubt that you ever should.</p>
<h2 id="peer-to-peer-communications">Peer-to-peer communications</h2>
<p>People around you are fully aware when you’re doing something for the sake of doing it. No, you’re not hiding it well enough. No, you don’t convince people that you’re having a great time, when you’re not. No one who pays careful attention to what they experience really believes that you’re doing great, while not loving what you do. Again, you might do ok. And ok can be fine. You have to decide if <em>“ok”</em> is enough for you.</p>
<p>In any context, people do get it when you’re acting. Most of the time.</p>
<p>People who pay attention get it almost every single time.</p>
<h2 id="you-should-be-doing-great">You should be doing great</h2>
<p>Everyone wants to excel, right? Not on everything of course, but on what they love, sure. After all, the positive feedback loop of “love” is a mandatory part of its survival over time.</p>
<p>If you want to enjoy yourself when working, try doing something you love. It’s about extroversion. It’s about getting it out and sharing it. People do notice when you love what you do. The reimbursement is not always fair. Frankly, it usually isn’t. But at least you’re doing what you love and, for most people after a certain living standard, this is reimbursement enough.</p>
<p>If you don’t know what you love, try playing with a couple of things. Try gathering some new experiences. It’ll be fun!</p>
<p>If you know what you love and are afraid to pursue to make a living out of this, stop doing that immediately. Everything else is just a compromise. And while compromise can be mandatory, it cannot constitute a permanent state of being. Compromise is a trap for more compromise. Go out and do what you love. This is who you are.</p>
<p>If you know what you love and already do that for a living, <em>thank you</em>. Your contributions are deeply appreciated.</p>
<p>George Spanos</p>
<p><a href="https://moby-it.com/" rel="nofollow, noopener, noreferrer" target="_blank">Moby IT</a></p>    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Finance worker pays out $25M after VC with deepfake CFO (216 pts)]]></title>
            <link>https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html</link>
            <guid>39248649</guid>
            <pubDate>Sun, 04 Feb 2024 08:43:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html">https://edition.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=39248649">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-editable="main" data-track-zone="main" data-reorderable="main">  <article data-uri="cms.cnn.com/_components/article/instances/cls6vbf7p0025a9nramq6c335@published" role="main" data-unselectable="true">
      
  <section data-tabcontent="Content">
    <main>
                <div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300}" data-uri="cms.cnn.com/_components/image/instances/cls6vg79l00063b6hf65f80kr@published" data-name="GettyImages-1437811938.jpg" data-component-name="image" data-observe-resizes="" data-original-ratio="0.6666666666666666" data-original-height="2000" data-original-width="3000" data-url="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=original" data-editable="lede" data-freewheel-lede="true">
       <picture><source height="383" width="680" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=16x9&amp;q=h_383,w_680,c_fill/f_webp" type="image/webp"><source height="653" width="1160" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=16x9&amp;q=h_653,w_1160,c_fill/f_webp" type="image/webp"><source height="605" width="1075" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=16x9&amp;q=h_605,w_1075,c_fill/f_webp" type="image/webp"><source height="833" width="1480" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=16x9&amp;q=h_833,w_1480,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/gettyimages-1437811938.jpg?c=16x9&amp;q=h_833,w_1480,c_fill" alt="Authorities are increasingly concerned at the damaging potential posed by artificial intelligence technology." onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="2000" width="3000"></picture>
    </div>
        
        
            <div data-editable="content" itemprop="articleBody" data-reorderable="content">
                    <p><cite>
      <span data-editable="location"></span>
      <span data-editable="source">CNN</span>
        &nbsp;—&nbsp;
    </cite>
</p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls6vbf7p0024a9nrhngzfc7l@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            A finance worker at a multinational firm was tricked into paying out $25 million to fraudsters using deepfake technology to pose as the company’s chief financial officer in a video conference call, according to Hong Kong police.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls732ubh00063d5vtonp5jy8@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The elaborate scam saw the worker duped into attending a video call with what he thought were several other members of staff, but all of whom were in fact deepfake recreations, Hong Kong police said at a briefing on Friday.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls72x77f00043d5v9kft5o5o@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “(In the) multi-person video conference, it turns out that everyone [he saw] was fake,”  senior superintendent Baron Chan Shun-ching told the city’s public broadcaster RTHK.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls73gcs300083d5vqrgwlj90@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Chan said the worker had grown suspicious after he received a message that was purportedly from the company’s UK-based chief financial officer. Initially, the worker suspected it was a phishing email, as it talked of the need for a secret transaction to be carried out.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls73gdgl000a3d5v2mmdlyua@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            However, the worker put aside his early doubts after the video call because other people in attendance had looked and sounded just like colleagues he recognized, Chan said.
    </p>

<div data-image-variation="image" data-breakpoints="{&quot;image--eq-extra-small&quot;: 115, &quot;image--eq-small&quot;: 300}" data-uri="cms.cnn.com/_components/image/instances/cls6zz14y000w3b6h93sv6453@published" data-name="" data-component-name="image" data-observe-resizes="" data-original-ratio="0.666015625" data-original-height="682" data-original-width="1024" data-url="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=h_682,w_1024,x_0,y_0" data-editable="settings">
       <picture><source height="682" width="1024" media="(min-width: 1280px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=w_1110,c_fill/f_webp" type="image/webp"><source height="682" width="1024" media="(min-width: 960px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=w_1015,c_fill/f_webp" type="image/webp"><source height="682" width="1024" media="(min-width: 480px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=w_1160,c_fill/f_webp" type="image/webp"><source height="682" width="1024" media="(max-width: 479px)" srcset="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=w_680,c_fill/f_webp" type="image/webp"><img src="https://media.cnn.com/api/v1/images/stellar/prod/c90d6199-9933-4c4e-a054-38b43dc829d9.jpg?q=w_1110,c_fill" alt="This aerial photo taken on December 19, 2018 shows a general view of the skyline of Hong Kong. " onload="this.classList.remove('image__dam-img--loading')" onerror="imageLoadError(this)" height="682" width="1024" loading="lazy"></picture>
    </div>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls73ms2y000h3d5vq6h91nw6@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Believing everyone else on the call was real, the worker agreed to remit a total of $200 million Hong Kong dollars – about $25.6 million, the police officer added.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls73p5qm000k3d5v8dltfh94@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The case is one of several recent episodes in which fraudsters are believed to have used deepfake technology to modify publicly available video and other footage to cheat people out of money.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls74r4vs001h3d5vp6iv426g@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            At the press briefing Friday, Hong Kong police said they had made six arrests in connection with such scams.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls7406x6000o3d5v7tamfvbm@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Chan said that eight stolen Hong Kong identity cards – all of which had been reported as lost by their owners – were used to make 90 loan applications and 54 bank account registrations between July and September last year.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls73p7jl000m3d5v9dn91735@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            On at least 20 occasions, AI deepfakes had been used to trick facial recognition programs by imitating the people pictured on the identity cards, according to police.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls7466h2000w3d5vo93ao08o@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The scam involving the fake CFO was only discovered when the employee later checked with the corporation’s head office.
    </p>

  


    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls745fva000u3d5vtnlcd70u@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Hong Kong police did not reveal the name or details of the company or the worker.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls718rv600163b6hcoz7rplm@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Authorities across the world are growing increasingly concerned at the sophistication of deepfake technology and the nefarious uses it can be put to.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls6yumv1000f3b6hofxm8j8t@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            At the end of January, pornographic, AI-generated images of the American pop star <a href="https://www.cnn.com/2024/01/25/tech/taylor-swift-ai-generated-images/index.html">Taylor Swift</a> spread across social media, underscoring the damaging potential posed by artificial intelligence technology.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/cls74e05b00153d5vox49fjhu@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The photos - which show the singer in sexually suggestive and explicit positions - were viewed tens of millions of times before being removed from social platforms.
    </p>

                </div>
    </main>
  </section>
</article>

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to Replace Your CPAP in Only 666 Days (299 pts)]]></title>
            <link>https://aphyr.com/posts/368-how-to-replace-your-cpap-in-only-666-days</link>
            <guid>39248631</guid>
            <pubDate>Sun, 04 Feb 2024 08:36:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aphyr.com/posts/368-how-to-replace-your-cpap-in-only-666-days">https://aphyr.com/posts/368-how-to-replace-your-cpap-in-only-666-days</a>, See on <a href="https://news.ycombinator.com/item?id=39248631">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p><em>This story is not practical advice. For me, it’s closing the book on an almost two-year saga. For you, I hope it’s an enjoyable bit of bureaucratic schadenfreude. For Anthem, I hope it’s the subject of a series of painful but transformative meetings. This is not an isolated event. I’ve had dozens of struggles with Anthem customer support, and they all go like this.</em></p>
<p><em>If you’re looking for practical advice: it’s this. Be polite. Document everything. Keep a log. Follow the claims process. Check the laws regarding insurance claims in your state. If you pass the legally-mandated deadline for your claim, call customer service. Do not allow them to waste a year of your life, or force you to resubmit your claim from scratch. Initiate a complaint with your state regulators, and escalate directly to <a href="mailto:gail.boudreaux@elevancehealth.com">Gail Boudreaux’s team</a>–or whoever Anthem’s current CEO is.</em></p>
<p>To start, experience an equipment failure.</p>
<p>Use your CPAP daily for six years. Wake up on day zero with it making a terrible sound. Discover that the pump assembly is failing. Inquire with Anthem Ohio, your health insurer, about how to have it repaired. Allow them to refer you to a list of local durable medical equipment providers. Start calling down the list. Discover half the list are companies like hair salons. Eventually reach a company in your metro which services CPAPs. Discover they will not repair broken equipment unless a doctor tells them to.</p>
<p>Leave a message with your primary care physician. Call the original sleep center that provided your CPAP. Discover they can’t help, since you’re no longer in the same state. Return to your primary, who can’t help either, because he had nothing to do with your prescription. Put the sleep center and your primary in touch, and ask them to talk.</p>
<p>On day six, call your primary to check in. He’s received a copy of your sleep records, and has forwarded them to a local sleep center you haven’t heard of. They, in turn, will talk to Anthem for you.</p>
<p>On day 34, receive an approval letter labeled “confirmation of medical necessity” from Anthem, directed towards the durable medical equipment company. Call that company and confirm you’re waitlisted for a new CPAP. They are not repairable. Begin using your partner’s old CPAP, which is not the right class of device, but at least it helps.</p>
<p>Over the next 233 days, call that medical equipment company regularly. Every time, inquire whether there’s been any progress, and hear “we’re still out of stock”. Ask them you what the manufacturer backlog might be, how many people are ahead of you in line, how many CPAPs they <em>do</em> receive per month, or whether anyone has ever received an actual device from them. They won’t answer any questions. Realize they are never going to help you.</p>
<p>On day 267, realize there is no manufacturer delay. The exact machine you need is in stock on CPAP.com. Check to make sure there’s a claims process for getting reimbursed by Anthem. Pay over three thousand dollars for it. When it arrives, enjoy being able to breathe again.</p>
<p>On day 282, follow CPAP.com’s documentation to file a claim with Anthem online. Include your prescription, receipt, shipping information, and the confirmation of medical necessity Anthem sent you.</p>
<p>On day 309, open the mail to discover a mysterious letter from Anthem. They’ve received your appeal. You do not recall appealing anything. There is no information about what might have been appealed, but something will happen within 30-60 days. There is nothing about your claim.</p>
<p>On day 418, emerge from a haze of lead, asbestos, leaks, and a host of other home-related nightmares; remember Anthem still hasn’t said anything about your claim. Discover your claim no longer appears on Anthem’s web site. Call Anthem customer service. They have no record of your claim either. Ask about the appeal letter you received. Listen, gobsmacked, as they explain that they decided your claim was in fact an appeal, and transferred it immediately to the appeals department. The appeals department examined the appeal and looked for the claim it was appealing. Finding none, they decided the appeal was moot, and rejected it. At no point did anyone inform you of this. Explain to Anthem’s agent that you filed a claim online, not an appeal. At their instruction, resign yourself to filing the entire claim again, this time using a form via physical mail. Include a detailed letter explaining the above.</p>
<p>On day 499, retreat from the battle against home entropy to call Anthem again. Experience a sense of growing dread as the customer service agent is completely unable to locate either of your claims. After a prolonged conversation, she finds it using a different tool. There is no record of the claim from day 418. There was a claim submitted on day 282. Because the claim does not appear in her system, there is no claim. There is a claim. There is no claim. Experience the cognitive equivalent of the Poltergeist hallway shot as the agent tells you “Our members are not eligible for charges for claim submission”.</p>
<p>Hear the sentence “There is a claim”. Hear the sentence “There is no claim”. Write these down in the detailed log you’ve been keeping of this unfurling Kafkaesque debacle. Ask again if there is anyone else who can help. There is no manager you can speak to. There is no tier II support. “I’m the only one you can talk to,” she says. Write that down.</p>
<p>Call CPAP.com, which has a help line staffed by caring humans. Explain that contrary to their documentation, Anthem now says members cannot file claims for equipment directly. Ask if they are the provider. Discover the provider for the claim is probably your primary care physician, who has no idea this is happening. Leave a message with him anyway. Leave a plaintive message with your original sleep center for good measure.</p>
<p>On day 502, call your sleep center again. They don’t submit claims to insurance, but they confirm that some people <em>do</em> successfully submit claims to Anthem using the process you’ve been trying. They confirm that Anthem is, in fact, hot garbage. Call your primary, send them everything you have, and ask if they can file a claim for you.</p>
<p>On day 541, receive a letter from Anthem, responding to your inquiry. You weren’t aware you filed one.</p>
<blockquote>
<p>Please be informed that we have received your concern. Upon review we have noticed that there is no claim billed for the date of service mentioned in the submitted documents, Please provide us with a valid claim. If not submitted,provide us with a valid claim iamge to process your claim further.</p>
</blockquote>
<p>Stare at the letter, typos and all. Contemplate your insignificance in the face of the vast and uncaring universe that is Anthem.</p>
<p>On day 559, steel your resolve and call Anthem again. Wait as this representative, too, digs for evidence of a claim. Listen with delight as she finds your documents from day 282. Confirm that yes, a claim definitely exists. Have her repeat that so you can write it down. Confirm that the previous agent was lying: members can submit claims. At her instruction, fill out the claim form a third time. Write a detailed letter, this time with a Document Control Number (DCN). Submit the entire package via registered mail. Wait for USPS to confirm delivery eight days later.</p>
<p>On day 588, having received no response, call Anthem again. Explain yourself. You’re getting good at this. Let the agent find a reference number for an appeal, but not the claim. Incant the magic DCN, which unlocks your original claim.  “I was able to confirm that this was a claim submitted form for a member,” he says. He sees your claim form, your receipts, your confirmation of medical necessity. However: “We still don’t have the claim”.</p>
<p>Wait for him to try system after system. Eventually he confirms what you heard on day 418: the claims department transferred your claims to appeals. “Actually this is not an appeal, but it was denied as an appeal.” Agree as he decides to submit your claim manually again, with the help of his supervisor. Write down the call ref number: he promises you’ll receive an email confirmation, and an Explanation of Benefits in 30-40 business days.</p>
<p>“I can assure you this is the last time you are going to call us regarding this.”</p>
<p>While waiting for this process, recall insurance is a regulated industry. Check the Ohio Revised Code. Realize that section 3901.381 establishes deadlines for health insurers to respond to claims. They should have paid or denied each of your claims within 30 days–45 if supporting documentation was required. Leave a message with the Ohio Department of Insurance’s Market Conduct Division. File an insurance complaint with ODI as well.</p>
<p>Grimly wait as no confirmation email arrives.</p>
<p>On day 602, open an email from Anthem. They are “able to put the claim in the system and currenty on processed [sic] to be applied”. They’re asking for more time. Realize that Anthem is well past the 30-day deadline under the Ohio Revised Code for all three iterations of your claim.</p>
<p>On day 607, call Anthem again. She explains that the claim will be received and processed as of your benefits. She asks you to allow 30-45 days from today. Quote section 3901.381 to her. She promises to expedite the request; it should be addressed within 72 business hours. Like previous agents, she promises to call you back. Nod, knowing she won’t.</p>
<p>On day 610, email the Ohio Department of Insurance to explain that Anthem has found entirely new ways to avoid paying their claims on time. It’s been 72 hours without a callback; call Anthem again. She says “You submitted a claim and it was received” on day 282. She says the claim was expedited. Ask about the status of that expedited resolution. “Because on your plan we still haven’t received any claims,” she explains. Wonder if you’re having a stroke.</p>
<p>Explain that it has been 328 days since you submitted your claim, and ask what is going on. She says that since the first page of your mailed claim was a letter, that might have caused it to be processed as an appeal. Remind yourself Anthem told you to enclose that letter. Wait as she attempts to refer you to the subrogation department, until eventually she gives up: the subrogation department doesn’t want to help.</p>
<p>Call the subrogation department yourself. Allow Anthem’s representative to induce in you a period of brief aphasia. She wants to call a billing provider. Try to explain there is none: you purchased the machine yourself. She wants to refer you to collections. Wonder why on earth Anthem would want money from <em>you</em>. Write down “I literally can’t understand what she thinks is going on” in your log. Someone named Adrian will call you by tomorrow.</p>
<p>Contemplate alternative maneuvers. Go on a deep Google dive, searching for increasingly obscure phrases gleaned from Anthem’s bureaucracy. Trawl through internal training PDFs for Anthem’s ethics and compliance procedures. Call their compliance hotline: maybe someone cares about the law. It’s a third-party call center for Elevance Health. Fail to realize this is another name for Anthem. Begin drawing a map of Anthem’s corporate structure.</p>
<p>From a combination of publicly-available internal slide decks, LinkedIn, and obscure HR databases, discover the name, email, and phone number of Anthem’s Chief Compliance Officer. Call her, but get derailed by an internal directory that requires a 10-digit extension. Try the usual tricks with automated phone systems. No dice.</p>
<p>Receive a call from an Anthem agent. Ask her what happened to “72 hours”. She says there’s been no response from the adjustments team. She doesn’t know when a response will come. There’s no one available to talk to. Agree to speak to another representative tomorrow. It doesn’t matter: they’ll never call you.</p>
<p>Do more digging. Guess the CEO’s email from what you can glean of Anthem’s account naming scheme. Write her an email with a short executive summary and a detailed account of the endlessly-unfolding Boschian hellscape in which her company has entrapped you. A few hours later, receive an acknowledgement from an executive concierge at Elevance (Anthem). It’s polite, formal, and syntactically coherent. She and promises to look into things. Smile. Maybe this will work.</p>
<p>On day 617, receive a call from the executive concierge. 355 days after submission, she’s identified a problem with your claim. CPAP.com provided you with an invoice with a single line item (the CPAP) and two associated billing codes (a CPAP and humidifier). Explain that they are integrated components of a single machine. She understands, but insists you need a receipt with multiple line items for them anyway. Anthem has called CPAP.com, but they can’t discuss an invoice unless you call them. Explain you’ll call them right now.</p>
<p>Call CPAP.com. Their customer support continues to be excellent. Confirm that it is literally impossible to separate the CPAP and humidifier, or to produce an invoice with two line items for a single item. Nod as they ask what the hell Anthem is doing. Recall that this is the exact same machine Anthem covered for you eight years ago. Start a joint call with the CPAP.com representative and Anthem’s concierge. Explain the situation to her voicemail.</p>
<p>On day 623, receive a letter from ODI. Anthem has told ODI this was a problem with the billing codes, and ODI does not intervene in billing code issues. They have, however, initiated a secretive second investigation. There is no way to contact the second investigator.</p>
<p>Write a detailed email to the concierge and ODI explaining that it took over three hundred days for Anthem to inform you of this purported billing code issue. Explain again that it is a single device. Emphasize that Anthem has been handling claims for this device for roughly a decade.</p>
<p>Wait. On day 636, receive a letter from Anthem’s appeals department. They’ve received your request for an appeal. You never filed one. They want your doctor or facility to provide additional information to Carelon Medical Benefits Management. You have never heard of Carelon. There is no explanation of how to reach Carelon, or what information they might require. The letter concludes: “There is currently no authorization on file for the services rendered.” You need to seek authorization from a department called “Utilization Management”.</p>
<p>Call the executive concierge again. Leave a voicemail asking what on earth is going on.</p>
<p>On day 637, receive an email: she’s looking into it.</p>
<p>On day 644, Anthem calls you. It’s a new agent who is immensely polite. Someone you’ve never heard of was asked to work on another project, so she’s taking over your case. She has no updates yet, but promises to keep in touch.</p>
<p>She does so. On day 653, she informs you Anthem will pay your claim in full. On day 659, she provides a check number. On day 666, the check arrives.</p>
<p>Deposit the check. Write a thank you email to the ODI and Anthem’s concierge. Write this, too, down in your log.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why is the mouse cursor slightly tilted and not straight? (395 pts)]]></title>
            <link>https://ux.stackexchange.com/questions/52336/why-is-the-mouse-cursor-slightly-tilted-and-not-straight</link>
            <guid>39248225</guid>
            <pubDate>Sun, 04 Feb 2024 06:57:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ux.stackexchange.com/questions/52336/why-is-the-mouse-cursor-slightly-tilted-and-not-straight">https://ux.stackexchange.com/questions/52336/why-is-the-mouse-cursor-slightly-tilted-and-not-straight</a>, See on <a href="https://news.ycombinator.com/item?id=39248225">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mainbar" role="main" aria-label="question and answers">
                
<div data-questionid="52336" data-position-on-page="0" data-score="604" id="question">
        

        

<div>
    
    <div itemprop="text">
                
<p>Is this a legacy thing or does a tilted cursor serves a purpose? I can tell that, the angle provides a totally vertical left edge which helps when highlighting text but what else apart from that?</p>

<p>EDIT: When cursor is swapped by the little hand cursor when hovered over buttons, the angle seems to be smaller. Why the difference?</p>
    </div>

        

    <div>
    <div>
        <p>
            asked <span title="2014-02-17 09:41:59Z">Feb 17, 2014 at 9:41</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/7393/thanos"><p><img src="https://www.gravatar.com/avatar/aac452da1312fa333c36015a5e591f01?s=64&amp;d=identicon&amp;r=PG" alt="Thanos's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>
    
</div>




            <p><span itemprop="commentCount">8</span></p>
    </div>



                
                
                <div id="answers">
                    


                                    
<div id="answer-52338" data-answerid="52338" data-parentid="52336" data-score="729" data-position-on-page="1" data-highest-scored="1" data-question-has-accepted-highest-score="1" itemprop="acceptedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
                
<p>This is the historical reason:</p>

<p><img src="https://i.stack.imgur.com/e1zH5.png" alt="Concept drawing of the standard mouse cursor at an angle"></p>

<p>(Concept drawing taken from document: <a href="http://bitsavers.trailing-edge.com/pdf/xerox/parc/techReports/VLSI-81-1_The_Optical_Mouse.pdf">VLSI-81-1_The_Optical_Mouse.pdf</a>)</p>

<p>The mouse, and therefore the mouse cursor, was <a href="http://arstechnica.com/features/2005/05/gui/2/">invented by Douglas Engelbart</a>, and was initially <a href="http://origin.arstechnica.com/images/gui/4-NLSgui.jpg">an arrow pointing up</a>. </p>

<p>When the <a href="http://arstechnica.com/features/2005/05/gui/3/">XEROX PARC</a> machine was built, the cursor changed into a tilted arrow. It was found that, given the low resolution of the screens in those days, drawing a straight line (left edge of arrow) and a line at a 45 degree angle (right edge of arrow) was easier to do and more recognizable than the straight cursor.</p>
    </div>
    <div>
            
            <div>
        <a href="https://ux.stackexchange.com/users/40110/code-maverick"><p><img src="https://www.gravatar.com/avatar/c056c352518943b11095c83a4ef2b31f?s=64&amp;d=identicon&amp;r=PG" alt="Code Maverick's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2014-02-17 09:47:52Z">Feb 17, 2014 at 9:47</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/5657/bart-gijssens"><p><img src="https://www.gravatar.com/avatar/54b1215ffb534c90dd7ea7f480d28c51?s=64&amp;d=identicon&amp;r=PG" alt="Bart Gijssens's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/5657/bart-gijssens">Bart Gijssens</a><span itemprop="name">Bart Gijssens</span></p><p><span title="reputation score 17,317" dir="ltr">17.3k</span><span>4 gold badges</span><span>49 silver badges</span><span>62 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">9</span></p>
    </div>


                                    
<div id="answer-52370" data-answerid="52370" data-parentid="52336" data-score="393" data-position-on-page="2" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
                    

<p>Take your right hand and point to your question.</p>

<p>There, you see. </p>

<p><img src="https://i.stack.imgur.com/xqGFX.jpg" alt="finger pointing at screen"></p>
    </div>
    <div>
            
            <div>
        <a href="https://ux.stackexchange.com/users/127876/silas-reel"><p><img src="https://lh5.googleusercontent.com/-yjvPGG9oHpw/AAAAAAAAAAI/AAAAAAAAAAA/AAN31DV4PX0I1kJiPjyoOIMz70ejP2SvbA/mo/photo.jpg?sz=64" alt="Silas Reel's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2014-02-17 18:13:23Z">Feb 17, 2014 at 18:13</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/43246/jturolla"><p><img src="https://www.gravatar.com/avatar/d24c555de0ab090f0b822155f31affe4?s=64&amp;d=identicon&amp;r=PG" alt="jturolla's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/43246/jturolla">jturolla</a><span itemprop="name">jturolla</span></p><p><span title="reputation score " dir="ltr">3,511</span><span>1 gold badge</span><span>10 silver badges</span><span>7 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">27</span></p>
    </div>

                                    
<div id="answer-52349" data-answerid="52349" data-parentid="52336" data-score="189" data-position-on-page="3" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>In addition to <a href="https://ux.stackexchange.com/a/52338/43668">Bart's answer</a>, I'd like to add one more reason. </p>

<p>The reason the arrow was tilted to the left was so that the click position was easier to calculate, because the origin of the cursor's bitmap was in the upper left.  This saved the mouse tracking subroutine a calculation on every click (its not much but it helped on older machines).  </p>

<p><a href="http://www.reddit.com/r/explainlikeimfive/comments/1qhzym/" rel="noreferrer">Source</a></p>
    </div>
    <div>
            
            <div>
        <a href="https://ux.stackexchange.com/users/-1/community"><p><img src="https://www.gravatar.com/avatar/a007be5a61f6aa8f3e85ae2fc18dd66e?s=64&amp;d=identicon&amp;r=PG" alt="Community's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2014-02-17 14:40:50Z">Feb 17, 2014 at 14:40</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/21591/jameo"><p><img src="https://www.gravatar.com/avatar/939c911747739eeb05c16b6b8a922ed9?s=64&amp;d=identicon&amp;r=PG" alt="Jameo's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/21591/jameo">Jameo</a><span itemprop="name">Jameo</span></p><p><span title="reputation score " dir="ltr">1,853</span><span>1 gold badge</span><span>11 silver badges</span><span>8 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">14</span></p>
    </div>


                                    
<div id="answer-52558" data-answerid="52558" data-parentid="52336" data-score="124" data-position-on-page="4" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<h2>Low level visual cognition</h2>
<p>In addition to the various answers given, there is also sense in a tilted mouse pointer if one considers the visual processes in our brain.</p>
<p>Visual information arriving from our eyes is first processed in the primary visual cortex by the V1 area, then by the V2 area. These two areas recognise low-level visual features (hue, lightness, size, orientation, etc.).</p>
<h2>The popout effect</h2>
<p>As visual information is processed by these areas, some visual irregularities truly pop out (ie, they are highly distinguishable), which greatly helps visual search (trying to find an item in a visually busy field). The popular name for this phenomenon is <strong>the popout effect</strong>.</p>
<p>A famous research from 1988 - <a href="http://www2.psychology.uiowa.edu/faculty/hollingworth/prosem/Treisman_Gormican_88_PR_FeatureAnalysisIn.pdf" rel="noreferrer">A. Treisman, and S. Gormican: Feature analysis in early vision: Evidence from search asymmetries</a> summarises many of these popout effects, and the irregularities they involve.</p>
<h2>Orientation</h2>
<p>One such irregularity is <strong>orientation</strong>, and it is neatly explained by the following illustration:</p>
<p><img src="https://i.stack.imgur.com/4xWaH.png" alt="3 images showing many vertical lines and how a tilted line pops out"></p>
<p>You should find it next to impossible to find the search target in 1 (a straight line in a group of straight lines). But rather easy in 2 - finding a tilted line in a group of straight lines. In 3 it should be equally next to impossible to find the tilted line in a group of tilted lines (of the same angle).</p>
<p>Since vertical and horizontal orientations are the most common ones on screens (and in life in general) a tilted mouse pointer will be more easily found.</p>
<p>More information can be found in Chapter 2 (What we can easily see) of <a href="http://www.amazon.co.uk/Visual-Thinking-Kaufmann-Interactive-Technologies/dp/0123708966/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1384303964&amp;sr=1-1&amp;keywords=visual+thinking+for+design" rel="noreferrer">Visual Thinking for Design</a>, Ware 2008.</p>
    </div>
    <div>
            
            <div>
        <a href="https://ux.stackexchange.com/users/-1/community"><p><img src="https://www.gravatar.com/avatar/a007be5a61f6aa8f3e85ae2fc18dd66e?s=64&amp;d=identicon&amp;r=PG" alt="Community's user avatar" width="32" height="32"></p></a>
    </div>


            <div>
    <div>
        <p>
            answered <span title="2014-02-19 23:38:31Z">Feb 19, 2014 at 23:38</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/16924/izhaki"><p><img src="https://www.gravatar.com/avatar/35c050eac0eab06a8c3b6fec8c2bb5c0?s=64&amp;d=identicon&amp;r=PG" alt="Izhaki's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/16924/izhaki">Izhaki</a><span itemprop="name">Izhaki</span></p><p><span title="reputation score 32,465" dir="ltr">32.5k</span><span>5 gold badges</span><span>66 silver badges</span><span>99 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">13</span></p>
    </div>

                                    
<div id="answer-52355" data-answerid="52355" data-parentid="52336" data-score="80" data-position-on-page="5" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
                    

<p>I've always thought that the arrow cursor is shaped similarly to your hand if you were point (naturally) at the screen with your (as typically dominant) right hand.</p>

<p>I have no support of this other than my own subjective experience but it strikes me as a natural shape when trying to relate real world interaction into a low resolution computer screen where rendering something resembling a hand would be impossible.</p>

<p>[Edit: Someone stole the only thunder I've ever had on StackAnything. Thanks!]</p>

<p><img src="https://i.stack.imgur.com/qGzNQ.jpg" alt="Hand pointing at screen"></p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-02-17 15:23:08Z">Feb 17, 2014 at 15:23</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/43174/user43174"><p><img src="https://www.gravatar.com/avatar/a69a703c86958e2d50e517ffd86c5e01?s=64&amp;d=identicon&amp;r=PG&amp;f=y&amp;so-version=2" alt="user43174's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/43174/user43174">user43174</a><span itemprop="name">user43174</span></p><p><span title="reputation score " dir="ltr">853</span><span>5 silver badges</span><span>4 bronze badges</span>
        </p>
    </div>
</div>
    
</div>




            <p><span itemprop="commentCount">6</span></p>
    </div>

                                    
<div id="answer-52461" data-answerid="52461" data-parentid="52336" data-score="49" data-position-on-page="6" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>In case anyone wonders : some less known interfaces did use a straight arrow as pointed in <a href="http://www.reddit.com/r/explainlikeimfive/comments/1qhzym/">Reddit</a></p>

<p><img src="https://i.stack.imgur.com/rJmmW.gif" alt="enter image description here"></p>

<p><img src="https://i.stack.imgur.com/ukk6x.gif" alt="enter image description here"></p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-02-19 00:48:06Z">Feb 19, 2014 at 0:48</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/25125/gildas-fr%c3%a9mont"><p><img src="https://i.stack.imgur.com/RHbGy.jpg?s=64&amp;g=1" alt="Gildas Frémont's user avatar" width="32" height="32"></p></a>
    </div>
    
</div>
    
</div>




            <p><span itemprop="commentCount">1</span></p>
    </div>

                                    
<div id="answer-52360" data-answerid="52360" data-parentid="52336" data-score="22" data-position-on-page="7" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>Also, there is another answer to this question. As a rule, the <strong>arrow</strong> mouse cursor must have one sharp tip (vertex) - because it is an arrow :) </p>

<p>On the other hand, it is better for a mouse cursor to look good and slick. </p>

<p>But drawing sharp tip on a rectangular pixel based display is very hard, especially without anti-aliasing. </p>

<p>The 0 degrees (horizontal or vertical) and 45 degrees lines are the only possible lines that look smooth without anti-aliasing. </p>

<p>That is why almost all arrow mouse cursors are based on one straight and one 45 degrees lines. As a result, the bisector line has angle of 45/2 = 22.5 degrees.</p>

<p>The tail of the arrow is much harder to be drawn well, but it is not so important as well. </p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-02-17 16:30:01Z">Feb 17, 2014 at 16:30</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/25879/johnfound"><p><img src="https://i.stack.imgur.com/R81XM.png?s=64&amp;g=1" alt="johnfound's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/25879/johnfound">johnfound</a><span itemprop="name">johnfound</span></p><p><span title="reputation score " dir="ltr">1,116</span><span>8 silver badges</span><span>16 bronze badges</span>
        </p>
    </div>
</div>
    
</div>




            <p><span itemprop="commentCount">4</span></p>
    </div>

                                    
<div id="answer-68326" data-answerid="68326" data-parentid="52336" data-score="7" data-position-on-page="8" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p><strong>It is a right-handed world.</strong> </p>

<p>It used to be that if you switched our right/left click buttons the arrow would point towards the right (opposite of the images cited). </p>

<p>This supports that the arrow mimics a hand pointing while providing angular contrast. Without a reference, it is an extension of the <em>desktop</em> metaphor.</p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-12-03 19:19:22Z">Dec 3, 2014 at 19:19</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/10767/ken"><p><img src="https://i.stack.imgur.com/htBYy.png?s=64&amp;g=1" alt="Ken's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/10767/ken">Ken</a><span itemprop="name">Ken</span></p><p><span title="reputation score " dir="ltr">1,232</span><span>7 silver badges</span><span>10 bronze badges</span>
        </p>
    </div>
</div>
    
</div>




            <p><span itemprop="commentCount">1</span></p>
    </div>

                                    
<div id="answer-97398" data-answerid="97398" data-parentid="52336" data-score="5" data-position-on-page="9" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
        

        

<div>
    
    <div itemprop="text">
<p>The fact that the mouse cursor is slightly tilted to the left makes a lot of sense. 
A very interesting fact:</p>

<p>If it were straight, it would take a nanosecond more to place the cursor on the desired object. Human mind is generally used to perceiving elements from left to the right, that is why the cursor is designed into the opposite direction, anticipating the intent of interaction with the element you are about to click on.</p>

<p>A nanosecond of time optimization is the closest thing to the absolute idea of irrelevance. With that I agree. However, on a perception level, it makes a huge difference. </p>

<p>The tilted cursor becomes similar to an athlete who's always on the start position, ready to take off towards anything you want to click on at any time.</p>

<p>It's a sensation that gives you so much comfort without you realizing why.</p>

<p>Semiotics, Cognitive Science and Psychology are all embedded into the simple and subtle decision of keeping the tilted cursor, just to simplify by a bit your experience.</p>

<p>Why was it tilted in the first place? Well, in its history, it seems like it was only an accident determined by some technical limitations:</p>

<p><a href="http://www.fastcodesign.com/3026625/why-the-mouse-cursor-is-tilted-instead-of-vertical" rel="nofollow">Why Your Mouse Cursor Looks The Way It Does</a></p>
    </div>
    <div>
            
            <div>
    
    <div>
        <a href="https://ux.stackexchange.com/users/54669/devin"><p><img src="https://i.stack.imgur.com/egmb3.jpg?s=64&amp;g=1" alt="Devin's user avatar" width="32" height="32"></p></a>
    </div>
    <div>
        <p><a href="https://ux.stackexchange.com/users/54669/devin">Devin</a></p><p><span title="reputation score 37,762" dir="ltr">37.8k</span><span>15 gold badges</span><span>79 silver badges</span><span>140 bronze badges</span>
        </p>
    </div>
</div>


            <div>
    <div>
        <p>
            answered <span title="2016-07-29 04:24:58Z">Jul 29, 2016 at 4:24</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/86656/mircea"><p><img src="https://graph.facebook.com/10208868140194564/picture?type=large" alt="Mircea's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/86656/mircea">Mircea</a><span itemprop="name">Mircea</span></p><p><span title="reputation score " dir="ltr">522</span><span>3 silver badges</span><span>4 bronze badges</span>
        </p>
    </div>
</div>
        </div>
    
</div>




            <p><span itemprop="commentCount">1</span></p>
    </div>

                                    
<div id="answer-68302" data-answerid="68302" data-parentid="52336" data-score="3" data-position-on-page="10" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
    
    <div itemprop="text">
<p>The angle, the cursor is inclined at gives a better feeling of pointing something. A cursor straight at 90 degree would not provide a good effect.It provides  improved appearance on low resolution screens.</p>

<p>Also the position calculation would become a lot easier when done from the top left corner of the pixel.</p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2014-12-03 13:04:10Z">Dec 3, 2014 at 13:04</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/41491/ashu"><p><img src="https://i.stack.imgur.com/3KnoW.jpg?s=64&amp;g=1" alt="ashu's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/41491/ashu">ashu</a><span itemprop="name">ashu</span></p><p><span title="reputation score " dir="ltr">249</span><span>1 silver badge</span><span>9 bronze badges</span>
        </p>
    </div>
</div>
    
</div>

                                    
<div id="answer-101006" data-answerid="101006" data-parentid="52336" data-score="2" data-position-on-page="11" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
    
    <p>A straight cursor would also obscure more of the object underneath raising the same issues when designing for touch interfaces</p>
    <div>
    <div>
        <p>
            answered <span title="2016-11-01 22:07:43Z">Nov 1, 2016 at 22:07</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/47160/mark-c"><p><img src="https://graph.facebook.com/710302166/picture?type=large" alt="Mark C's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/47160/mark-c">Mark C</a><span itemprop="name">Mark C</span></p><p><span title="reputation score " dir="ltr">151</span><span>1 silver badge</span><span>4 bronze badges</span>
        </p>
    </div>
</div>
    
</div>

                                    
<div id="answer-135748" data-answerid="135748" data-parentid="52336" data-score="2" data-position-on-page="12" data-highest-scored="0" data-question-has-accepted-highest-score="1" itemprop="suggestedAnswer" itemscope="" itemtype="https://schema.org/Answer">
    
    <div itemprop="text">
<p>Well, the cursor is a pointer, and mimics pointer angles from real life (~30-45° to the vertical).</p>
<p><a href="https://i.stack.imgur.com/J8xzk.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/J8xzk.png" alt="Pointers in the real-world"></a></p>
<p>Importantly, that angle serves to <strong>guide the eye down the length of the pointer</strong>, in the direction going "into" the screen, <strong>towards a single point</strong>, in the same way as perspective drawings do:</p>
<p><a href="https://i.stack.imgur.com/mKdC5.png" rel="nofollow noreferrer"><img src="https://i.stack.imgur.com/mKdC5.png" alt="Perspective drawing"></a></p>
<p>On the contrary, a straight arrow seems to point in the general up-direction, targeting no one point in particular. Have you ever used, or seen someone use, a pointer stick vertically upwards? That is indeed awkward, and reserved for moments where the object being pointed to is high up and well beyond the height of the person and the length of the stick combined, and can be vague in conveying what is actually being pointed at.</p>
    </div>
    <div>
    <div>
        <p>
            answered <span title="2020-11-26 20:32:45Z">Nov 26, 2020 at 20:32</span>
        </p>
        
    </div>
    <div>
        <a href="https://ux.stackexchange.com/users/28743/snag"><p><img src="https://www.gravatar.com/avatar/4a6245cc648b214ad6a440bfe18d0152?s=64&amp;d=identicon&amp;r=PG&amp;f=y&amp;so-version=2" alt="SNag's user avatar" width="32" height="32"></p></a>
    </div>
    <div itemprop="author" itemscope="" itemtype="http://schema.org/Person">
        <p><a href="https://ux.stackexchange.com/users/28743/snag">SNag</a><span itemprop="name">SNag</span></p><p><span title="reputation score " dir="ltr">9,597</span><span>3 gold badges</span><span>22 silver badges</span><span>26 bronze badges</span>
        </p>
    </div>
</div>
    
</div>


                                    



                            <h2 data-loc="1">
                                
                            </h2>
                </div>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The first amateur radio station on the moon (145 pts)]]></title>
            <link>https://www.arrl.org/news/the-first-amateur-radio-station-on-the-moon-js1ymg-is-now-transmitting</link>
            <guid>39247614</guid>
            <pubDate>Sun, 04 Feb 2024 04:27:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.arrl.org/news/the-first-amateur-radio-station-on-the-moon-js1ymg-is-now-transmitting">https://www.arrl.org/news/the-first-amateur-radio-station-on-the-moon-js1ymg-is-now-transmitting</a>, See on <a href="https://news.ycombinator.com/item?id=39247614">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p><span>02/02/2024</span></p><p>The Japan Aerospace Exploration Agency (JAXA) successfully landed their Smart Lander for Investigating Moon (SLIM) on January 19, 2024. Just before touchdown, SLIM released two small lunar surface probes, LEV-1 and LEV-2.</p>
<p>LEV-2 collects data while moving on the lunar surface, and LEV-1 receives the data.</p>
<p>The JAXA Ham Radio Club (JHRC), JQ1ZVI, secured amateur radio license JS1YMG for LEV-1, which has been transmitting Morse code on 437.41 MHz since January 19. The probe uses a 1 W UHF antenna with circular polarization and is transmitting "matters related to amateur business."</p>
<p>Radio amateurs have been busy analyzing JS1YMG's signal, with&nbsp;<a href="https://destevez.net/2024/01/trying-to-decode-lev-1/" target="_blank">Daniel Estévez's, EA4GPZ, blog</a>&nbsp;introducing the method and extraction results for demodulating Morse code from the signal, as well as extracting the code string.</p>
<p>It's unclear how long signals will be heard. JAXA has said that SLIM was not designed to survive a lunar night, which lasts about 14 days, and is due to return in a few days.</p>
<p>SLIM was launched on September 6, 2023, and landed on January 19, 2024, with the mission of analyzing the composition of rocks to aid research about the origin of the moon. SLIM's landing made Japan the fifth country to achieve a soft touchdown on the moon. The landing was achieved with exceptional precision -- within 180 feet of its targeted touchdown location.</p>
		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AvaloniaUI: Create Multi-Platform Apps with .NET (160 pts)]]></title>
            <link>https://www.avaloniaui.net/</link>
            <guid>39246988</guid>
            <pubDate>Sun, 04 Feb 2024 02:34:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.avaloniaui.net/">https://www.avaloniaui.net/</a>, See on <a href="https://news.ycombinator.com/item?id=39246988">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

            <!-- Heading -->
            <h2>
              A proven path for WPF app modernization.
            </h2>

            <!-- Text -->
            <p>
                <span>Familiar.</span> Considered a spiritual successor to WPF, Avalonia UI provides a <a href="https://docs.avaloniaui.net/docs/next/get-started/wpf/">familiar developer experience</a> allowing you to leverage years of pre-existing knowledge and investments. With a <a href="https://www.avaloniaui.net/XPF#hybrid">Hybrid XPF</a> license, it's possible to use WPF controls within your Avalonia application from vendors, including Actipro, Telerik, Syncfusion and more. 
            </p>

            <!-- Text -->
            <p>
                  <span>Proven.</span> Trusted by companies including JetBrains, KLM, Canon, Schneider Electric, Unity Games and more for modernising their WPF apps. 
            </p>

          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Doom didn't kill the Amiga (156 pts)]]></title>
            <link>https://www.datagubbe.se/afb/</link>
            <guid>39246825</guid>
            <pubDate>Sun, 04 Feb 2024 02:05:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.datagubbe.se/afb/">https://www.datagubbe.se/afb/</a>, See on <a href="https://news.ycombinator.com/item?id=39246825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p><b>A detailed account of the rise and fall of an Amiga zealot</b></p>

<p><i>Early 2024</i></p>

<p>
<img src="https://www.datagubbe.se/afb/pics/dotc.png">
<br>
<i>A scene from the Amiga game Defender of the Crown, released in 1986.
For years, no other home computer came close to screens like these.</i>
</p>

<p>
Ever since I saw an Amiga 500 at a friend's house in what was probably late 1988, I wanted one for myself. Back then, computers were uncommon, especially at home. Even though I went to a school in a fairly affluent neighborhood, few kids had home computers or video games.
</p>

<p>
Gradually, that started to change.
</p>

<p>
I bought my own Amiga in February 1992. It was basically the same exact model as the one I had first seen in 1988: An Amiga 500+ with a 7 MHz 68000 CPU, 1 meg of RAM, 8-bit stereo PCM sound and many various graphics modes, of which 320x256 in 16 or 32 colors was the most common for games.
</p>

<h3>Competing Platforms</h3>

<p>
Buying any other computer was completely out of the question. There were practical reasons of course: I knew how it worked and friends had one, which meant we could copy pirated games from each other. And it ran <a href="https://www.datagubbe.se/dpaint/">Deluxe Paint</a>, an era defining graphics program and a killer application for anyone with artistic inclinations, which I'd convinced myself I had.
</p>

<p>
I had also come into contact with other types of machines. The C64 felt like a thing of the past, with blocky graphics and beepy sound. The Mac was a boring monochrome computer made for writing equally boring documents about tax deductions. I intensely remember seeing a PC for the first time, and how disappointed it made me: it was much worse than the Mac, possibly even than the C64. Downright ugly graphics, terrible sound and a mysterious operating system that required you to learn textual incantations by heart.
</p>

<p>
But the real home computer feud during those days was between the Amiga 500 and the Atari ST, in which the latter always seemed to come out losing; worse sound, worse graphics, worse OS. Such a machine was completely out of the question - it just <i>had</i> to be an Amiga.
</p>

<p>
I don't regret this decision one bit. Few gadgets have given me as much joy and positive experiences as my Amigas, and even in 1992 an Amiga 500 wasn't a bad purchase. The PC was still a fairly boring machine, having a hard time keeping up with the Amiga's sound and graphics without costing an exorbitant amount of money. The revolutionary Amiga architecture, released in 1985 and basically unchanged since then, could still hold its own: a testament to its ingenuity.
</p>

<h3>The Price is Right</h3>

<p>
Around this time, PC:s were showing signs of becoming affordable, and Amiga style platform- and arcade games had begun appearing on the platform. But even the most straightforward of these titles required a fast 386 CPU, plenty of memory and a VGA card to come close to the amount of motion and commotion the Amiga had been capable of for ages. This meant the price to play was still high: In Sweden, around Christmas 1992, a 25 MHz 386/SX with 2 megs of RAM and a 40 meg hard drive was selling for $530. To this came the additional cost of a sound card. A Sound Blaster 2.0, guaranteed to be supported by most games, was available for a whopping $129. While you could hook the Amiga to a cheap 14" TV via RGB SCART, the PC required an expensive VGA monitor, on which you couldn't also watch MacGyver. Bummer!
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/sotb.png">
<br>
<i>Shadow of the Beast, released for the Amiga in 1989. It featured
lush graphics, multiple layers of parallax scrolling, massive sprites
and tough as nails gameplay. It looked like crap on pretty much every
other system, and wasn't even ported to the then inferior PC.</i>
</p>

<p>
This put the cost of an entry level 386 system at $660 without software, compared to the Amiga 600's $499, and the A600's 40 MB hard drive came pre-loaded with Deluxe Paint, a word processor and several decent games. It's true that a 386 PC was much faster than the 7 MHz Amiga in some aspects, but most of those aspects didn't matter for people like me and my friends - at least not yet.
</p>

<h3>More Moore</h3>

<p>
It's hard to convey just how intense the effects of Moore's law were during the 90's. When it came to hardware, December 1992 was a <i>very</i> different time compared to January 1992. During this year, Commodore pulled a stunt by first introducing the A500+ and then swiftly replacing it with the A600 - a low cost model with the advantage of having an integrated IDE interface, only to finish off by making the A600 obsolete with the release of the next-generation A1200. They were widely criticized for this, but in their defense, home computing was moving at breakneck speed and nobody could really keep up. At any given moment, something that had been unfeasible just six months ago was suddenly commonplace. One such something was <b>Doom</b> - which was nowhere, and then suddenly everywhere. Some Amiga fanatics still claim that Doom was what killed the Amiga, but I don't believe that to be true. Doom was a symptom, not the disease proper.
</p>

<p>
Commodore's last sigh, the Amiga 1200, has been touted as a bad machine and an architectural mistake. Its 14 MHz 68020 CPU, 2 megabytes of RAM and <a href="https://en.wikipedia.org/wiki/Amiga_Advanced_Graphics_Architecture">AGA</a> chipset is often, in hindsight, dismissed as too little, too late. Considering this, it still sold fairly well in Europe - because as a home computer, it actually wasn't half bad. It could be equipped with a hard drive, it had an expansion slot for CPU and memory upgrades, and the new graphics modes actually rivalled VGA and even SVGA in many ways. The 1280x512 pixel mode in 18-bit color was too slow for games, but when displaying still pictures it was quite a sight to behold.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/500p.jpg">
<br>
<i>One of the few surviving photos of my Amiga 500+. Here I'm
showing my grandmother something that looks like a shell window,
and she's pretending to be interested. T-shirt sizes were very comfortable
in those days.</i>
</p>

<p>
To me, even in 1994, switching to another architecture still wasn't under consideration. I was now an <i>Amiga fanboy</i> and the platform had become my natural home. And why not? An entry level A1200 hooked to a TV set wasn't the worst of choices for a boy in his early teens and it was of course a natural step up from the A500. My peripherals and nearly all of my software still worked and I could keep using my trusty old 14" TV, on which I could also watch The X-Files without parental interference. It was a platform I was comfortable with, and I got a good second hand deal - it even came with a hard drive, which was what I wanted for my computer most of all at that point.
</p>

<p>
Still, times were changing. PCs were no longer just for very rich kids and office professionals: any serious gamer had to consider one, and not just because of Doom. PC games were perhaps not yet as colorful, zippy and funky as Amiga games, but they were often very complex and relied heavily on a fast CPU to manage that complexity. Most people didn't really have a fast enough PC to fully enjoy Doom at its time of release in 1993 - but just a year later, expectations on home computing had changed. Even though 486 machines hadn't dropped <i>that</i> drastically in price yet, hardly anyone even marketed 386 machines anymore.
</p>

<p>
Memory was cheaper, as were hard drives. Better graphics and sound fidelity was expected, and it seemed every new PC graphics card that came out offered higher resolutions and more colors than the previous. CD quality sound was suddenly a thing, as was putting a CD-ROM reader in your computer. Using a flickering 50 Hz PAL TV instead of a rock solid 60 Hz VGA monitor was no longer seen as clever frugality, but rather as a way of hurting your eyes when trying to play Sim City 2000 in its high resolution 640x480 glory - which the A1200 honestly wasn't quite fast enough to do anyway. Adventure games like Monkey Island II, Simon the Sorcerer and Indiana Jones and the Fate of Atlantis came on eleven (11!), nine (9!) and eleven (11!) floppy disks, respectively. The Amiga 500 had traditionally been a floppy-based system, but a hard drive was now more or less required, even for gamers.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/agony.png">
<br>
<i>A cutscene from the Amiga game Agony (1992). It took a while,
but the PC eventually caught up, graphics wise.</i>
</p>

<p>
In spring of 1994, around the time of Commodore's demise, a 33 MHz 486 PC with an SVGA card, 4 megs of RAM, a 200 meg hard drive <i>and</i> a 14" monitor cost $1300. For an Amiga 1200 to even begin to come close to such a system, you'd have to spend at least as much money. And while the PC still came without a sound card, a high resolution monitor for the Amiga had to be able to auto-switch between 15 kHz (PAL) and 31 kHz (VGA), pushing the price of the Amiga system higher still. Add to that the fact that the PC was a big box machine, with plenty of room for, say, a CD-ROM drive - the hot new thing.
</p>

<h3>The DOS conundrum</h3>

<p>
By 1995, Commodore was well and truly dead. It was during this time I started getting acquainted with other platforms in earnest: DOS, Windows, Linux, newer Macs, even Unix workstations. I had plenty of friends who owned PCs, but I stayed true to my Amiga, adding CPU and RAM upgrades that cost as much as the computer itself. I could have saved money and bought a PC - but I honestly didn't see the point. Linux was nice for surfing the net, but it lacked all of the fun software I craved: games, demos (as in the demo scene), graphics programs, tracker music. It didn't even have the things I wanted for school, such as a reasonable word processor with Swedish spell checking.
</p>

<p>
All of this and more was of course available on PCs, and I was frequently exposed to it when visiting my PC owning friends. Deluxe Paint, my beloved killer app for the Amiga, apparently existed on PC as well. And there were lots of other neat programs that appealed to a young demo scener: Fasttracker and Screamtracker for music, QBasic and Turbo Pascal for programming, QPEG for viewing images, TheDraw for making ANSI graphics, and droves of pictures, music, games and scene demos.
</p>

<p>
The strange thing was that all of the fun PC stuff was made for DOS. I just didn't get it. Command lines no longer put me off, having dabbled quite a bit with them on both Amiga and Linux, but DOS lacked that one crucial thing I had grown ever more accustomed to on my Amiga: Native, effortless, pre-emptive multitasking.
</p>

<p>
The PCs were indeed impressive, hardware-wise. SVGA was now commonplace, as were 486/DX processors running in 33 or even 66 MHz. Doom was not only playable, but enjoyable. PC scene demos were running impressive texture-mapped 3D effects much faster and, with the addition of modern sound cards, offering much better audio fidelity than my Amiga was capable of. Ostensibly simple things like loading JPEG images felt instant compared to my A1200, even after it was upgraded with a 28 MHz 68030 CPU.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/msdos.png">
<br>
<i>Things like these were perfectly reasonable to MS-DOS users. Expanded or extended memory? Or maybe just conventional? Choose wisely, or you won't
be able to fully maximize the incredible potential of running <b>a single program at a time</b>.</i>
</p>

<p>
What I couldn't grasp was the point of having (and paying for) all that raw power if you couldn't utilize it fully. On my Amiga, I could run Amos Professional - an advanced BASIC dialect - and Deluxe Paint <i>at the same time</i>. If I wanted to change an image used in an Amos program, switching between the two applications was instant. If I suddenly wanted to take some notes, I could just fire up a text editor. And I could listen to music at the same time! In DOS, even something as mundane as enjoying tracker music was a full time, full screen activity - not something you could keep doing while working in other programs.
</p>

<p>
This multitasking was and is a big part of my affinity for the Amiga, and it opened up the possibility for several other <a href="https://www.datagubbe.se/ltmag/">ingenious features</a>. Workflows were completely customizeable with powerful scripting languages, the modular design of the OS made it extremely adaptable to things like new file formats and file systems, and every aspect of it could be tweaked and configured to suit personal needs. It was (and still is, in many ways) like running a carefully honed environment from a professional workstation on a cheap home computer. DOS, Atari and Mac just couldn't compare.
</p>

<p>
Furthermore, DOS seemed to require inordinate amounts of tweaking and configuration. Hardware interrupts had to be configured manually, the 640 kB base memory had to be carefully guarded, and you had to select and configure your sound and graphics cards in almost every single game you wanted to play. On the Amiga, everything just worked. If you bought a new peripheral, all you had to do was plug it in and boot your computer. Drivers were needed for some things, of course, but the Amiga's <i>Autoconfig</i> took care of peripheral detection and configuration.
</p>

<p>
When Dial-up Internet became (almost) affordable for the masses, the Amiga delivered there, too. PC users had to load up Windows 3.11, with its questionable cooperative multitasking, but the Amiga felt as smooth as ever. While waiting for a slow FTP download, I could chat with friends on IRC, play (simple) games and listen to music. A fast 486 with plenty of RAM could perhaps do the same, but not yet with my Amiga's inherent elegance.
</p>

<p>
Hence, the Amiga still felt like a superior platform. Its swift multitasking, efficient resource usage and <a href="https://www.datagubbe.se/ltmag/">many clever ideas</a> made both DOS and Windows feel clunky and primitive. Even though Windows 95 had entered the scene, it was more or less unusable without at least 8 megs of RAM. And all the fun stuff on PCs was <i>still</i> being made for DOS.
</p>

<h3>Doom me once, shame on you</h3>

<p>
What about Doom, then? John Carmack himself has allegedly said that he didn't consider the Amiga as being capable of running Doom. As an Amiga zealot, I'd of course like to point out that he was wrong - it's since been ported to the Amiga and runs just fine. But in my heart of hearts, I know that in 1993, he was actually right. The only Amiga available at that time powerful enough to make Doom palatable would have been an Amiga 4000 with a 25 MHz 68040 CPU, costing somewhere around $2500 - without a monitor.
</p>

<p>
Commodore was frequently derided for not producing yet another <i>killer computer</i>, a proper new Amiga model as revolutionary as the Amiga 1000 had been in 1985.  This complaint is somewhat valid, but still, I think, misses its target. It's not that the Commodore engineers didn't have plans - and even prototypes - for much more advanced graphics hardware than the AGA chipset in the A1200. There are just plenty of <i>other</i> reasons for why things might not have gone as expected, even if they had reached market.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/rnt.png">
<br>
<i>Ruff'n'Tumble on Amiga (1994): huge bosses, beautiful graphics and frantic action.</i>
</p>

<p>
VGA was deceptively simple. A framebuffer, more or less, which provided the famous <i>chunky</i> Mode 13h - part of what made Doom possible. No hardware sprites. No blitter for fast memory copying. No <i>copper</i> (co-processor) like the Amiga had, which could change the color of a given palette index every scanline. But you <i>could</i> write once to memory for a single given pixel, and get any color from an indexed palette of 256 values. And you could run spreadsheets in a steady, 60 Hz 640x480.
</p>

<p>
The Amiga had been designed when sprite-based games were the hottest thing since sliced toast, when memory was still stupendously expensive and when the ability to display 80 column text was considered a noteworthy feature on many home computers. As opposed to VGA, the Amiga had <i>planar</i> graphics, requiring multiple writes to memory to produce a single color value - a perfectly reasonable choice for the time, and one that enabled a lot of other nifty programming tricks and visual effects. The Amiga architecture was so tuned for arcade action that even in 1994, games like Ruff'n'Tumble showed that the 7 MHz Amiga 500 could still hold its own against powerful PCs when it came to fast paced 2D shooters. The problem was that after having dominated the market since first showing up in arcade cabinets, games in that style were becoming unfashionable.
</p>

<p>
By now it should be clear that what really drove the PC boom was neither Doom nor chunky graphics: it was cheaper and faster CPUs. Chunky-to-planar conversion on the Amiga does steal a few CPU cycles, but even if the A1200 had been equipped with a chunky mode, its 14 MHz 68020 processor would've been far too slow for Doom. Motorola's 486 equivalent, the 68040, hadn't been subjected to the price drops of mass produced PC clones and competition from rival manufacturers, such as AMD's and Cyrix' 486 compatible offerings. Put simply: Commodore could have crammed an actual VGA card into the A1200, but its CPU would still have been far too slow for Doom. And even if Doom had never materialized, an affordable Motorola CPU still couldn't crunch the numbers needed for increasingly complex simulators and strategy games.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/msfsim.png">
<br>
<i>Microsoft Flight Simulator 5.0 - tremendously slow even on the recommended minimum 386 CPU.</i>
</p>

<h3>The later 90's</h3>

<p>
Some time after 1995, I and many fellow Amiga zealots had started feeling an itch that was hard to scratch. It's true that this itch came, in part, from Doom. Not so much from wanting to play it, but from the desire to show the world that yes, the Amiga <i>was</i> in fact capable of running it. With the correct, expensive upgrades, mind you - but still.
</p>

<p>
Alas, the itch was also caused by other, more pressing matters. The remnants of Commodore had been bought by Escom and then Gateway, but no new Amiga models had materialized. Despite this, we had dutifully upgraded our CPUs, expanded our RAM, and kept true to our machines - but nothing of substance had materialized from the new owners of Commodore's IP.
</p>

<p>
There had been poor attempts at Doom clones on the A1200, but they all somehow lacked the parts of Doom that made it Doom. Surfing the net was now, to be honest, quite painful even on an accelerated Amiga with an expensive dual-sync monitor. The higher horizontal refresh rate (31 kHz) congested the AGA chipset and made 256 color screens unbearably slow. And all those JPEG files took forever to decompress! Some pages didn't look right, either: they were designed for Netscape, a program not available on our platform. In short, we were desperate for the launch of new Amiga hardware.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/iout.png">
<br>
<i>The pinnacle of Amiga zealot humor in the mid-90's. Hyuk, hyuk, ackshually...</i>
</p>

<p>
We were used to walkover victories in comparisons between computer brands, but it was now painfully obvious that our beloved Amiga was lagging behind. We didn't know how to respond and many of us considered remarks or even simple questions about our platform to be personal insults. This made us completely insufferable, and we spent inordinate amounts of time jeering both on- and offline about "Micro$oft", <a href="https://en.wikipedia.org/wiki/Pentium_FDIV_bug">buggy Pentiums</a>, cooperative multitasking and "Plug'n'Pray". Littering IRC with statements like "Windows 95 = Amiga 85" achieved nothing except making us look obnoxious. But teenage frustration mixed with sunk cost is a tough mix of emotions to combat: We had invested too much time, money and prestige to give up now. The Amiga would surely rise again! Except, of course, <i>damn you</i> if you even suggested the use of cheap Intel CPU:s at the core of the platform.
</p>

<p>
In 1997, the Doom source code was released and Amiga ports started cropping up. With a very expensive CPU expansion, it was perfectly playable on an A1200 - but nobody cared about Doom anymore. It was all Duke Nukem 3D and Quake now, and MP3 files, and fast Pentium MMX CPUs or even the impressive Pentium II and dedicated 3D graphics cards.
</p>

<p>
I was now running a 40 MHz 68040 CPU in my A1200, but even that couldn't keep up with the cheapest of PCs. DOS was more or less gone by now, and the fun stuff had started, little by little, to move into Windows.
</p>

<p>
In 1998, I finally caved and bought a powerful Pentium II PC. I was running a dual-boot system with both Windows 95 and Linux, but eventually Windows NT 4 won out. I could surf the web as well as anyone, play Quake III (after a graphics card update), program web pages using Microsoft's Personal Web Server, ASP and Access databases, chat with my friends on IRC and even watch DivX movies.
</p>

<p>
But there was still something missing. The fun just wasn't as fun on Linux and Windows. I guess a lot of people like me felt the same: even in 1999, the best demo scene stuff for PC was still clinging to DOS. Personally, I missed all the clever stuff my Amiga and its OS did, and the <a href="https://www.datagubbe.se/dopus/">great</a> and <a href="https://www.datagubbe.se/mkdem/">familiar</a> software it ran. 
</p>

<p>
So I just didn't stop using my Amiga. In the early 2000's, I upgraded to an impressively fast 50 MHz 68060 CPU, the last in Motorola's 68k series. I even bought an expensive, towerized A1200 with a 24-bit ("SVGA") graphics card and an ethernet card. It was ridiculously expensive and underpowered compared to any off the shelf PC available at the time - and yet, such a beefy Amiga was a paradox. It was a boyhood dream five or ten years too late - and also less of an Amiga than I perhaps cared to admit to myself at the time. The really fun stuff like Deluxe Paint, Amos and scene demos didn't run on the graphics card. It was more of a platform for running AmigaOS than an Amiga proper, and I eventually ended up downgrading to a more modest configuration. While it couldn't keep up as a daily driver, it was still a computer I booted up regularly, just to have <i>fun</i>.
</p>

<p>
It still is, and I still do.
</p>

<p>
<a href="https://www.datagubbe.se/afb/pics/anet.gif"><img src="https://www.datagubbe.se/afb/pics/anet.gif"></a>
<br>
<i>Network configuration on a souped up Amiga 1200, circa 2003.</i>
</p>

<h3>Custom Silicon</h3>

<p>
Doom didn't kill the Amiga - it was more like a measure of the many nails in the coffin of custom hardware home computer platforms. The biggest culprit was economies of scale.
</p>

<p>
Popular 8-bit home computers had, over time and quite naturally, been replaced by 16- and 32-bit machines. Many manufacturers of unique 8-bit machines during the Cambrian explosion of home computers simply went defunct or started making PC clones. By 1995, the only remaining, popular 32-bit home computer system was the PC. Apple regularly tried to muscle in on the home market, especially during the mid-90's with their PowerPC machines, but without much success. During this time period, a Mac was more of a niche office machine than the PC had ever been, and Apple survived mostly by selling systems for magazine and print ad production. They were in fact dangerously close to folding when Steve Jobs stepped back in and managed to secure funding from Microsoft (as "anti-trust insurance") and launch the iMac just in time to ride the wave of the Internet boom.
</p>

<p>
Commodore had been able to sell their successful 8-bit machines and early Amiga models cheaply thanks to vertical integration. This basically meant that they owned the chip fab, MOS Technology, that manufactured the Amiga's custom chips (and even the C64's 6502 CPU). At the time of its release, this made the original Amiga almost bizarrely cheap compared to equivalent Mac and PC offerings with similar performance.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/amad.jpg">
<br>
<i>A Swedish Amiga 4000T advertisement from 1997. For the listed SEK 29990 (roughly $3000 in today's exchange rate), you could get a fully specced out Pentium II machine - with a decent monitor - running in circles around the Amiga. If you forked out this much for an A4000 at this point in time, I dare say you were stupid. On the inside of your brain.</i>
</p>

<p>
But in 1994, when Commodore went bust, the PC clone market had been in full swing for over a decade. Cobbling together a 486 system using mass produced parts proved, in the end, to be far more competitive than designing, prototyping and manufacturing your own complex graphics and sound hardware for a single platform. Besides being cheap, this kind of standardization had more advantages. One was that PCs worked the same all over the world. Since traditional home computers were meant to work with television sets, they had to be timed to either the PAL or NTSC signal standard, which also meant that games (and other software) were timed to this as well. Games made for the European market didn't work on US Amigas without being rewritten, and vice versa. PCs didn't have this problem, providing a global market without added development cost.
</p>

<p>
Commodore's never completed <a href="https://en.wikipedia.org/wiki/Advanced_Amiga_Architecture_chipset">AAA</a> and <a href="https://en.wikipedia.org/wiki/Amiga_Hombre_chipset">Hombre</a> architectures sound impressive on paper, but were still not finished when the A1200 and A4000 were released in 1992. Besides, It's easy to list specs for nonexistent hardware in hindsight. Even if AAA had become what was promised, it wouldn't necessarily have been competitively priced. Even with the AGA machines, several cost cutting measures had been taken and <i>they still struggled</i> when it came to pricing. Would an even more advanced architecture somehow, magically, have sold as cheap or even cheaper? I have my doubts.
</p>

<p>
It's quite possible that the Commodore engineers could've worked a chunky mode into the AGA chipset. But, as discussed above, the A1200 would have needed a much faster CPU if Doom - or any other "killer app" game - was ever going to be a possibility. A 68040 would have been far too expensive, but even a 40 or 50 MHz 68030 CPU would've put the machine at a decidedly different price point. Combined with a hypothetical new graphics architecture, we can only speculate about the cost. The Amiga was known for being <i>good and cheap</i>, and it was proving hard not just for Commodore to combine the two.
</p>

<h3>Falcon Heavy</h3>

<p>
Many of Commodore's mistakes are said to have occurred after its founder, Jack Tramiel, left the company - even though the Amiga, a great success by all accounts, was bought by Commodore after his departure. Perhaps Tramiel's hardline approach to business ("Business is war!") and cost cutting ("Computers for the masses, not the classes!") could have led Commodore and (had they still bought it) the Amiga down a different path, but history tells us otherwise.
</p>

<p>
Tramiel left Commodore for Atari Corporation, a company that abandoned the home computer market in 1993 after their last-ditch effort, the Falcon 030. Like the A1200, the Falcon wasn't really a bad computer - but it cost even more than a souped-up Amiga 1200, and was still, in many aspects, underpowered compared to 486 PCs. It didn't come with the impressive specs listed for Commodore's AAA chipset, but its capabilities were far more advanced than those of the A1200. Its graphics chip, ViDEL, could produce a wide array of impressive resolutions, including a chunky 16-bit truecolor graphics mode. It had 16-bit sound, a faster CPU (16 MHz 68030), and a 32 MHz Motorola 56001 digital signal processor. This, together with both IDE, SCSI and networking hardware made for a very capable machine indeed.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/falc2.jpg">
<br>
<i>The Atari Falcon 030 still had the classic "home computer in a keyboard"
form factor.</i>
</p>

<p>
When considering all of this, the Falcon <i>was</i> actually competitively priced - but that didn't matter. People who needed SCSI or networking in 1993 usually had other budgets and priorities than home users, such as running WordPerfect or Lotus 1-2-3 for DOS. People who wanted great sound and graphics for gaming didn't want to pay extra for stuff they'd never use.
</p>

<p>
All those bells and whistles, together with the need for keeping the BLiTTER and YM2149F chips for backwards compatibility, meant the Falcon was too curious, inflexible and expensive for something that was supposedly a home computer. In the end, all its <i>killer architecture</i> managed to kill was, sadly, Atari itself.
</p>

<h3>Early birds and worms</h3>

<p>
What if Commodore had managed to put out a new, revolutionary Amiga model <i>much earlier</i> than 1992? Perhaps this could have saved the platform, ensuring its continued longevity? Maybe - but even if the Amiga 1000 launched in 1985, Amigas didn't become popular until the release of the cheaper, stabler and more mature Amiga 500 in 1987 - the same year IBM launched VGA. Could Commodore have released a VGA killer in 1988? As discussed in the beginning of this text - at this point in time, <i>the Amiga already was a VGA killer</i>. Consumer level PC:s just weren't fast enough to do something interesting with 256 colors, and VGA cards were much too expensive anyway. To keep a truly competitive cutting edge, the Amiga would have needed not just a new graphics architecture, but probably a CPU upgrade as well - raising the total cost of the machine considerably.
</p>

<p>
Commodore could surely have produced something very impressive, but again, at what price point - and would it have mattered? Most PCs sold at this time were still turbo XT clones with crappy CGA graphics and yet, PC dominance was already well established in the business sector. This also meant it had started seeping into homes where parents saw an opportunity of running spreadsheets and letting the kids play games and do their homework on the same machine, instead of buying two expensive family computers.
</p>

<p>
Commodore was often derided for their poor attempts at marketing the Amiga - but I think that's a bit unfair, too. The Amiga was launched at a huge press event where Andy Warhol and Debbie Harry famously appeared to show off the computer's graphics abilities. This was followed by print and TV ads that clearly and vividly showcased the advanced hard- and software. In Europe, Amigas sold like hotcakes and Commodore UK were very successful in bundling hardware with popular software titles. And, due to its graphics and video capabilities, the Amiga was a well known and popular machine in broadcasting circuits - in large parts thanks to the impressive <a href="https://en.wikipedia.org/wiki/Video_Toaster">Video Toaster</a>.
</p>

<p>
Could the Amiga have muscled in on the office market in a meaningful way? I honestly don't think so - Apple couldn't, Atari couldn't, in fact <i>nothing but IBM compatibles could</i>. The Amiga 3000 - a high end model often considered to be some of Commodore's finest work - is said to have piqued the interest of Sun Microsystems, who wanted to license it as a low end UNIX workstation. The deal fell through, much to the chagrin of Amiga fanatics across the globe - another oft-cited example of Commodore's failure as a company. Even so, Sun machines catered to a different niche market than IBM PCs, and while the notion of a mass produced Amiga UNIX workstation <i>sounds cool</i>, it's questionable if it would somehow have made consumer Amigas cheaper, faster and better - or simply led Commodore onto a path of expensive high end machines competing with the likes of SGI, HP and DEC. At the very least, considering it would've been running UNIX, it probably wouldn't have resulted in more resources allocated for AmigaOS development.
</p>

<p>
The real question here is, I think, if we would actually have wanted the Amiga to become yet another boring office machine. Everything that was fun and great about it was, to me at least, also what made business execs so suspicious of it.
</p>

<h3>Last breaths</h3>

<p>
With the demise of Commodore and Atari, the traditional home computer was basically dead. Acorn stayed in the game until 1998, but their expensive Archimedes line of machines was never very popular in actual homes, surviving by being more or less subsidized by British schools. Their subsequent RiscPC models catered mainly to various niche actors in broadcasting, including the BBC. Their legacy is now carried on by the popular Raspberry Pi computer.
</p>

<p>
Even highly specialized machines, such as Unix workstations, were living precarious lives by the end of the 90's. SGI, Sun, Digital and IBM focused their efforts more and more on servers and less on desktop machines. The last new Unix workstation models were both launched in 2006, by IBM and Sun. Even in this lucrative segment, competition from cheap PC hardware ultimately proved insurmountable.
</p>

<p>
With this in mind, a new <i>killer architecture</i> from Commodore may have been impressive, but <i>it might not even have been an Amiga as we know them</i>. It could have been a completely new machine. It could have been (mostly) compatible with the "classic" Amiga by somehow incorporating the old Amiga into the new one - still synced to PAL or NTSC, with all that entails, and of course adding cost to the machine. It could have become some kind of short-lived UNIX workstation or perhaps a games console. Or, it could have become a new Voodoo style graphics card for PCs - a platform that was also manufactured by Commodore, and quite profitably at that.
</p>

<p>
Both the Atari Falcon and Amiga 1200 were already suffering from minor problems with backwards software compatibility, something that probably made a lot of consumers more open to switching to PC. It's of course impossible to say for sure, but certainly not unthinkable, that the AAA architecture would have failed miserably to run the existing, bare metal banging games (and applications) an upgrading user would have expected to work on an "Amiga".
</p>

<p>
In short: Commodore might have lasted longer as a company - in some form - had they made other decisions. Such decisions, however, may not have been ones guaranteeing the continued existence of a platform recognizable as an Amiga. Apart from the Mac - in many ways thanks to Microsoft promising continued support and providing a $150 million cash injection - <i>no other home or desktop computer platform survived the 1990's PC dominance</i> in any meaningful way. It seems highly unlikely that the Amiga would somehow have propelled itself back into a significant market position thanks to a chunky graphics mode or, considering the fate of the Falcon 030, even a reworked architecture.
</p>

<h3>All but gone</h3>

<p>
The Amiga was an amazing platform, so far ahead of its time it stayed alive for much longer than what seems reasonable. It came out during the end of the Cambrian home computer explosion and remained in production for close to ten consecutive years. Saying it wasn't successful because of Commodore's lack of business savvy is doing it a disservice: Many, many millions of Amigas were sold and it was, for several years, the dominant home machine in Europe, where it shaped a generation of curious, capable and creative computer users.
</p>

<p>
It was, in fact, so popular and successful that even long after Commodore's demise, there were drawn-out efforts to modernize the platform. Here we can glean another of the many nails in the Amiga's coffin: the Amiga fanatics themselves. Most were now identifying so deeply with their platform that, say, suggestions of porting the operating system we loved to cheap and plentiful X86 hardware was considered heresy. It had to be PowerPC or nothing, despite the failure of the <a href="https://en.wikipedia.org/wiki/BeBox">BeBox</a> - which in many ways was more of a modern Amiga than any of the officially sanctioned attempts.
</p>

<p>
This eventually resulted in the AmigaOne series of PowerPC machines launched in the early 2000's. Keen supporters of the platform can nowadays purchase a motherboard with a 2 GHz dual core CPU for a mind-boggling $2000 (Yikes!). Add to that the cost for graphics, sound, memory and everything else. Spending that kind of money will get you a computer mostly useful for finding out why you don't want to run outdated ports of Linux software on an operating system without memory protection.
</p>

<p>
<img src="https://www.datagubbe.se/afb/pics/a1.png">
<br>
<i>Pay stupid prices, win stupid hardware.</i>
</p>

<p>
It's not without irony that the once cheap, integrated, cutting edge Amiga platform has now become a ridiculously expensive PC-style kit computer, running an OS kernel that seems almost as primitive today as MS-DOS once did when we Amiga zealots smugly bragged about multitasking.
</p>

<p>
A humbling journey, to say the least.
</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vision Pro Teardown – Why those fake eyes look so weird (293 pts)]]></title>
            <link>https://www.ifixit.com/News/90137/vision-pro-teardown-why-those-fake-eyes-look-so-weird</link>
            <guid>39246664</guid>
            <pubDate>Sun, 04 Feb 2024 01:37:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ifixit.com/News/90137/vision-pro-teardown-why-those-fake-eyes-look-so-weird">https://www.ifixit.com/News/90137/vision-pro-teardown-why-those-fake-eyes-look-so-weird</a>, See on <a href="https://news.ycombinator.com/item?id=39246664">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
<figure><p>
<iframe title="Vision Pro Teardown: Behind the Complex and Creepy Tech" width="456" height="257" src="https://www.youtube-nocookie.com/embed/JVJPAYwY8Us?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>The strangest thing about the Vision Pro is also the thing that makes it most uniquely Apple: it’s got a big shiny bubble glass front, which makes it stand out from the aluminum- and plastic-shrouded competition, even when it’s off. And when it’s on, it’s even stranger—instead of being fully transparent, behind the glass, an odd lenticular screen displays a 3D-ish video of the user’s eyes, emulating their gaze. Apple calls it the EyeSight display, and when the user is looking at you, it kind of, sort of, almost looks like you can see through smokey glass.</p>



<p>Tech journalists have called EyeSight “<a href="https://www.businessinsider.com/apple-vision-pro-eyesight-feature-fans-reaction-2024-1">bizarre</a>,” “<a href="https://www.theverge.com/24054862/apple-vision-pro-review-vr-ar-headset-features-price">uncanny</a>,” and “<a href="https://daringfireball.net/2024/01/the_vision_pro">of highly dubious utility</a>.” But from a repair perspective, it seems like an achilles heel. Why introduce another screen, more connectors, and <em>so many </em>more points of failure—all for the sake of a slightly creepy feature? Of course, we had to dig in and figure out how it works.&nbsp;</p>



<p>We knew it would be tough to get inside (it was). We hoped we wouldn’t break anything (we did). But we knew it would be worth it to see all the new technology Apple squeezed into this thing, from the EyeSight display to the sensor array, the external battery back to the R1 chip. We brought in the heavy hitters for this teardown, including x-ray views of the frame and high-resolution microscope shots of the displays.&nbsp;</p>



<p>We’ve got a lot of observations, some opinions, and a couple educated guesses about <em>why</em> we got the Vision Pro we have today on the teardown table. There is a lot in this device, so we’re splitting our analysis into two, with more detail on the lens system and silicon coming in a few days.</p>



<p>Let’s go spelunking into a never-before-explored cave of glass.</p>



<figure><img decoding="async" fetchpriority="high" width="2000" height="1395" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094026/crop-AVP_TD_EDITED_32.jpeg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094026/crop-AVP_TD_EDITED_32.jpeg 2000w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094026/crop-AVP_TD_EDITED_32-1536x1071.jpeg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094026/crop-AVP_TD_EDITED_32-1290x900.jpeg 1290w" sizes="(max-width: 2000px) 100vw, 2000px"></figure>



<figure><img decoding="async" width="7162" height="4029" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094617/AVP_TD_EDITED_46-2.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094617/AVP_TD_EDITED_46-2.jpg 7162w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094617/AVP_TD_EDITED_46-2-1536x864.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094617/AVP_TD_EDITED_46-2-2048x1152.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094617/AVP_TD_EDITED_46-2-1600x900.jpg 1600w" sizes="(max-width: 7162px) 100vw, 7162px"></figure>



<p>The glass panel is glued on, of course, and it took a <em>lot</em> of heat and time, but we removed it without breakage. Granted it didn’t come out unscathed—the glass has a protective plastic film that got a little peeled up and maybe a bit melted. Apple’s retail fixers <em>might</em> have faster hands than us—but they’ll <a href="https://support.apple.com/apple-vision-pro/repair">charge you $799</a> to replace broken front glass.&nbsp;</p>



<figure><img decoding="async" loading="lazy" width="7059" height="3971" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094405/AVP_TD_EDITED_55-1.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094405/AVP_TD_EDITED_55-1.jpg 7059w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094405/AVP_TD_EDITED_55-1-1536x864.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094405/AVP_TD_EDITED_55-1-2048x1152.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094405/AVP_TD_EDITED_55-1-1600x900.jpg 1600w" sizes="(max-width: 7059px) 100vw, 7059px"></figure>



<figure><img decoding="async" loading="lazy" width="2000" height="1500" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03102917/glass-only-edited.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03102917/glass-only-edited.jpg 2000w, https://valkyrie.cdn.ifixit.com/media/2024/02/03102917/glass-only-edited-1536x1152.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03102917/glass-only-edited-1200x900.jpg 1200w" sizes="(max-width: 2000px) 100vw, 2000px"></figure>



<h3><a href="https://drive.google.com/drive/folders/1pSsMhfg7AtK_QktOMHv41PAHvhhDs7kS"><strong></strong></a><strong>Heavy Metal</strong></h3>



<p>At 34 grams, the glass may not be heavy on its own, but fully kitted out with the battery the Vision Pro weighs over a kilogram.&nbsp;</p>



<p>Here’s where Apple has a performed a bit of a sleight of hand. Carefully hidden in most publicity shots is the external battery, which rides along in your pocket rather than on your headset. As in the early days of VR, integrating the battery as it is now would make the device crazy heavy.&nbsp;And hey, we’re big fans of modular batteries, when the battery inevitably stops holding a charge in <a href="https://batteryuniversity.com/article/bu-801b-how-to-define-battery-life">a year or three</a>, you can replace it painlessly. Apple’s hardware team may also be anticipating the <a href="https://repair.eu/news/big-win-for-right-to-repair-with-new-eu-rules-for-batteries-but-legislators-must-get-the-implementation-right/">upcoming EU battery regulation</a>, which will require all electronics to have user-replaceable batteries by 2027. </p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-1536x1025.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-2048x1367.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-1349x900.jpg 1349w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03104036/AVP_TD_EDITED_63-1-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"></figure>



<p>The battery pack alone weighs 353 grams and is made of three iPhone-sized batteries, delivering a grand total of 35.9 Wh, more than double an iPhone 15 Pro’s 17.3 Wh. The cells themselves are 184 g apiece, surprisingly only about half the weight of the full battery pack. To get inside, we had to soften some perimeter adhesive and release a set of single-use metal clips—then twist open Torx screws galore.<a href="https://drive.google.com/drive/folders/1pSsMhfg7AtK_QktOMHv41PAHvhhDs7kS"></a></p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-1536x1025.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-1349x900.jpg 1349w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77-450x300.jpg 450w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035829/AVP_TD_EDITED_77.jpg 2000w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>Three batteries in the aluminum pack, at ~3.8V each in series, 3166 mAh each, supplying 11.34 Volts in total.</figcaption></figure>



<p>Add the weight of the battery pack and the headset together and you get, as mentioned above, over a kilogram—which would be a really heavy pair of glasses. For comparison, the Quest Pro weighs 722 g and the Quest 3 clocks in at 515 g.</p>



<p>But weight isn’t just about how it tips the scales. It’s about balance. The weight of the Vision Pro largely rests on your face, all the tech is at the front and even the Pro Dual Loop Band can’t overcome it all without a counterbalance. <a href="https://www.patentlyapple.com/2023/11/apple-invents-a-battery-mount-on-the-back-of-vision-pros-headband-to-help-counterbalance-the-weight-of-the-hmd-that-causes-n.html">Apple patented a design</a> for a rear-mounted battery pack, which might’ve helped balance out the heavy front—though it’s hard to imagine wanting to wear something 150% as heavy.&nbsp;</p>



<p>So if we’re just counting the weight on your face—the display module, sans battery, in the Meta Quest Pro is 522 grams. The same assembly in the Vision Pro is 532 grams, effectively the same. The key difference in these units is in the weight distribution, and a much heavier pocket battery in the Vision Pro.</p>



<figure><img decoding="async" loading="lazy" width="2158" height="1319" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03102021/ifixit-the-vision-pro-is-not-as-heavy-as-you-think.png" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03102021/ifixit-the-vision-pro-is-not-as-heavy-as-you-think.png 2158w, https://valkyrie.cdn.ifixit.com/media/2024/02/03102021/ifixit-the-vision-pro-is-not-as-heavy-as-you-think-1536x939.png 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03102021/ifixit-the-vision-pro-is-not-as-heavy-as-you-think-2048x1252.png 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03102021/ifixit-the-vision-pro-is-not-as-heavy-as-you-think-1472x900.png 1472w" sizes="(max-width: 2158px) 100vw, 2158px"></figure>



<p>First impressions, though, are pretty good. “The weight isn’t as bad as expected, although it’s definitely on my forehead/cheeks as opposed to my head which feels weird, like someone is pushing on my head to tilt it down,” says iFixit teardown veteran <a href="https://www.ifixit.com/User/524640/Sam+Goldheart">Sam Goldheart</a> from the teardown lab.</p>



<h3><strong>Headbands</strong></h3>



<p>The Vision Pro comes with both a 3D-knitted Solo Knit Band and a Dual Loop Band. These attach to the ends of the stems, just behind the speakers. The now-iconic Solo Knit Band is the one that seen in all the publicity shots, and it does look cool. It wraps around the back of your head, and you adjust the fit with a dial on the side, similar to how you might tighten a bike helmet.&nbsp;</p>



<p>So how does it feel? “The fabrics are sooo nice,” says Sam. There’s a very fine, cushy weave on the Solo Knit Band, and it is stretchy enough to accommodate a ponytail and still support the face unit.&nbsp;&nbsp;</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1.jpg"><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094447/AVP_TD_EDITED_20-1-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p>The speakers are fixed onto the two rigid bands that join to the main headset. To release these, you use our old friend, the SIM-card removal tool. The holes are inside the temples of the main headset, and the removable bands have a row of electrical contacts, just like Lighting connectors, again. Easily removable parts? Only demands tools you’ve probably already got? We love to see it. This makes us hope that opening the headset might not be as daunting as we first assumed.<a href="https://drive.google.com/drive/folders/1pSsMhfg7AtK_QktOMHv41PAHvhhDs7kS"></a></p>



<p>This modular design is similar to the <a href="https://www.ifixit.com/Teardown/AirPods+Max+Teardown/139369">AirPods Max</a>, which we quite liked. Wearables are so easy to damage that it makes good sense to have easily swappable speaker modules. We tried to go further and pry the speaker out of the silicon frame, and instantly broke the molded cables inside. That’s all right, you’re not going to need to pry the speaker modules open. </p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094451/AVP_TD_EDITED_82-2-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>The speakers—not quite as hard to get into as a pair of AirPods Pro, but almost.</figcaption></figure>



<p>The speakers themselves point back towards your ears. This is a pretty clear indication that you’re not meant to wear this anywhere noisy. You can wear your AirPods Pro if you prefer—and if you want lossless, low-latency audio, they’ll have to be the latest USB-C version.</p>



<p>On the left side is the proprietary battery cable connection, which snaps into place with a magnet and then twists to lock. We understand why Apple used a non-standard connector here, even if we don’t love it—at least it can’t be yanked out by a passing child, or when the cord inevitably catches on your chair. But the plug at the other end of the cable is unforgivable. Instead of terminating with a USB-C plug, it connects to the battery pack with what looks like a proprietary oversized Lightning connector, which you release using a paperclip or SIM-removal tool.</p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094500/AVP_TD_EDITED_11-1-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>The locking design is great, but why couldn’t it be USB-C Apple? WHY?</figcaption></figure>



<p>This connector means that you can’t just swap in the USB-C battery pack you already own. Lame.</p>


<div>
<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03134020/AVP_TD_EDITED_35-37-edited-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>Aww, such a cute family snap!</figcaption></figure></div>


<h3><strong>Light Seals and Face Cushions</strong></h3>



<p>Every face is different, and Apple is selling<a href="https://www.reddit.com/r/VisionPro/comments/19ardw5/all_light_seal_sizes/">&nbsp;28 different light-seal parts</a> to cover all the different face sizes and shapes. Your seal size also changes if you need Zeiss lens inserts. That’s because the seals and cushions are also used to make sure you have the correct eye position relative to the stereo screens and eye sensors. This is why Apple is hand-packing every Vision Pro order—there’s just no “standard” setup. </p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091739/makeup-2-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>Guess who was wearing makeup?</figcaption></figure>



<p>The seals attach to the main headset using magnets, which is Apple through-and-through—it’s either glued in place, or extremely easy to swap. This modularity is a brute force attempt to get an ideal fit on your face. It will be interesting to see if this is required long-term, or if future devices find a simpler way to accomplish this. For the time being, magnets are better than velcro because they can snap the seals into exact alignment. Think how MagSafe snatches the charger and lines it up perfectly over the iPhone’s inductive charging coil.</p>



<p>As for cleaning the seals, <a href="https://support.apple.com/en-us/HT213964">Apple recommends</a> water and unscented dish soap, which will help stop these sweat-soaking parts from getting too gross, and will be especially good for anyone wearing makeup. In her <a href="https://www.wsj.com/video/series/joanna-stern-personal-technology/vision-pro-review-24-hours-in-apples-mixed-reality-headset/05CD2E77-897D-49A9-A87E-9B8A93E3E45F">Wall Street Journal video</a> where she selflessly wore the headset for 24 hours, Joanna Stern said her makeup caked the inside of the seals. And our own Sam Goldheart had the exact same problem this morning.</p>



<p>Under the magnetic seals is a permanent seal, also wrapped in a knit fabric, but less likely to get smudged. It also happens to be the way into the interior of the headset. Removing it reveals another surprise: a thin stretchy sheet of plastic. Whether it’s to compensate for gaps in the knit, or to keep particulates out of the innner workings, we’re not to sure. But we are certain this bit looks <em>very</em> masked superhero.</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim.jpg"><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03091736/Eyetrim-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<h3><strong>EyeSight Display</strong></h3>



<p>The front-facing gogglebox is the defining feature of the Vision Pro, and, now that reviews are pouring in, one of its most controversial.</p>



<p>The <a href="https://patentimages.storage.googleapis.com/51/03/12/4b7f034c90465c/US20240012601A1.pdf">patent for the EyeSight</a> describes three display modes: “internal focus,” “external engagement,” and “do not disturb.” The patent has pages and pages of images that might be displayed on the screen—all kinds of cartoon animal eyes, biometric analysis captured by other sensors, hearts when the user is talking to a loved one. The internal camera might read emotional states and project images based on those emotional states.</p>



<figure><img decoding="async" loading="lazy" width="1906" height="766" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03085115/pasted-image-0-1.jpeg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03085115/pasted-image-0-1.jpeg 1906w, https://valkyrie.cdn.ifixit.com/media/2024/02/03085115/pasted-image-0-1-1536x617.jpeg 1536w" sizes="(max-width: 1906px) 100vw, 1906px"></figure>



<p>Cool thought. In practice, the EyeSight display is so dim and low-resolution that reviewers say it’s hard to see much on it. The WSJ’s Joanna Stern <a href="https://www.youtube.com/watch?v=8xI10SFgzQ8&amp;t=211">called it</a> “hard to see,” and Marques Brownlee (aka MKBHD)<a href="https://www.youtube.com/watch?v=dtp6b76pMak&amp;t=1826"> said</a>, “You can barely see my eyes when I’m wearing the headset.”&nbsp;</p>



<p>It turns out that when the EyeSight displays your eyes, it isn’t just displaying a single video feed of your eyes; it’s showing a <em>bunch</em> of videos of your eyes. Exploring inside the glass shell, we found three layers for the front-facing display: a widening layer, a lenticular layer, and the OLED display itself.</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103.jpg"><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03122335/AVP_TD_EDITED_103-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p><a href="https://drive.google.com/drive/folders/1pSsMhfg7AtK_QktOMHv41PAHvhhDs7kS"></a><strong>Why Does EyeSight Look So Wonky?</strong></p>



<figure><img decoding="async" loading="lazy" width="3514" height="960" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03115254/Lenticular-explainer-1.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03115254/Lenticular-explainer-1.jpg 3514w, https://valkyrie.cdn.ifixit.com/media/2024/02/03115254/Lenticular-explainer-1-1536x420.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03115254/Lenticular-explainer-1-2048x559.jpg 2048w" sizes="(max-width: 3514px) 100vw, 3514px"></figure>



<p>Apple wanted to achieve something very specific: an animated, 3D-looking face with eyes. They had to make very strategic design choices and compromises to accomplish this.</p>



<p>Human brains are very sensitive to faces and expressions, its why the uncanny valley is a thing, and part of that is depth sensing. Apple needed to create a believable 3D effect. One reason why 3D renderings don’t look truly 3D is because they lack a stereoscopic effect. For something to look 3D, we need to see subtly different images with each eye. The Vision Pro tackles this problem with lenticular lenses.</p>



<figure><video controls="" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03035750/card-movement.mp4"></video></figure>



<p>A lenticular lens displays different images when viewed from different angles. You can use this effect to simulate movement with two frames of an action. Or, you can create a stereoscopic 3D effect with images of the same subject from different angles.</p>



<p>The Vision Pro has a lenticular layer on top of the exterior OLED panel. VisionOS renders multiple face images—call them A and B—slices them up, and displays A from one angle serving your left eye, and  B from another serving your right eye. This creates a 3D face via the stereoscopic effect. And those angles are tiny, and they are legion, it takes a fancy <a href="https://www.evidentscientific.com/en/">Evident Scientific microscope</a> to really see what we mean.</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03035754/AVP_017.jpg"><img decoding="async" loading="lazy" width="2000" height="1106" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03035754/AVP_017.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03035754/AVP_017.jpg 2000w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035754/AVP_017-1536x849.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03035754/AVP_017-1627x900.jpg 1627w" sizes="(max-width: 2000px) 100vw, 2000px"></a><figcaption>The curved ridges of the lenticular lens layer.</figcaption></figure>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03120833/EyeSight-no-filter.jpg"><img decoding="async" loading="lazy" width="1880" height="1040" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03120833/EyeSight-no-filter.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03120833/EyeSight-no-filter.jpg 1880w, https://valkyrie.cdn.ifixit.com/media/2024/02/03120833/EyeSight-no-filter-1536x850.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03120833/EyeSight-no-filter-1627x900.jpg 1627w" sizes="(max-width: 1880px) 100vw, 1880px"></a><figcaption>Pixels bending and shining through the lenticular layer</figcaption></figure>



<p>There are compromises to this approach. The horizontal resolution is dramatically reduced, being divided between each of the multiple images. For example, if two images are displayed on a 2000 pixel wide display, each image only has 1000 horizontal pixels to work with. Even through we don’t know the resolution of the display, nor do we know the number of images being interwoven, the resolution is necessarily reduced. And that is a major reason why EyeSight eyes seem blurry.</p>



<p>In front of the lenticular layer is another plastic lens layer, with similarly lenticular ridges. This layer appears to stretch the projected face wide enough to fit the width of the Vision Pro. Removing this layer and booting the Pro showcases some very oddly pinched eyes.</p>



<figure><video controls="" poster="https://valkyrie.cdn.ifixit.com/media/2024/02/03044324/dsc_7495.mov.00_00_27_13.still002_720.jpg" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03103510/Display-Stack_sm.mp4"></video></figure>



<p>Additionally the lens likely limits the effective viewing angle. Limiting the effect to directly in front of the Vision Pro limits artifacting you might see at extreme angles, sort of like a privacy filter. The downside is that you’re passing an already complex, blurry image through yet another layer of lens. This makes it even blurrier and darker.</p>



<h3><strong>Lens Inserts, Stereo Displays</strong></h3>



<p>You can see the outline of the ovoid lens inserts in this x-rays from our illuminous friends at <a href="https://creativeelectron.com/">Creative Electron</a>, who spent $3,500 so you could see this photo.</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray.jpg"><img decoding="async" loading="lazy" width="3546" height="1992" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray.jpg 3546w, https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray-1536x863.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray-2048x1150.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03125441/AVP-Xray-1602x900.jpg 1602w" sizes="(max-width: 3546px) 100vw, 3546px"></a></figure>



<p>The Vision Pro itself performs an <a href="https://support.apple.com/guide/apple-vision-pro/important-safety-information-c0c84db82a44/visionos">automatic interpupillary distance adjustment</a> when you first put it on, with motors adjusting the positioning of the lenses. For everything else there’s prescription lenses. </p>



<p>Apple Stores have a machine to determine approximate prescription glasses strength when you come in for a demo.  For users with eye conditions (like strabismus) that might interfere with eye tracking, the Vision Pro offers <a href="https://support.apple.com/en-us/HT213965">alternative interaction controls</a> in the accessibility features. However, we have heard that lenses are not available for people who have astigmatism, which is <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10045990/">40% of the population</a>. If you know anything more about that, leave it in the comments.&nbsp;</p>



<p>The prescription insert lenses themselves require “pairing” with the headset. The decision has already borne poor UI, John Gruber received an <a href="https://daringfireball.net/2024/01/the_vision_pro">incorrect calibration code</a> with his review unit that made eye tracking perform poorly. <a href="https://www.ifixit.com/News/82867/iphone-15-teardown-reveals-software-lockdown">We hate parts pairing</a> on principle, and there’s got to be a way to enable calibration while still allowing third party lenses.</p>



<p>Oh, and Creative Electron was bored after one photo so they shot us a 360 spin. Sweet!</p>



<figure><video autoplay="" controls="" loop="" preload="auto" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03130232/360-color-4.mp4"></video></figure>



<h3><strong>R1 and M2 Chips</strong></h3>



<p>The headset runs on an M2 Mac chip, in tandem with the new R1 chip—which is specifically responsible for processing the input from 12 cameras, the LiDAR sensor, and the TrueDepth camera, all with a minimum of latency. With AR, you need to project the camera view of the real world into the user’s eyes as fast as possible, otherwise their perceived motions won’t match up with what they see, which is a fast ticket to Vomitsville.&nbsp;</p>



<p>To keep up, the R1 <a href="https://www.theverge.com/2023/6/5/23733874/apple-vision-pro-visionos-augmented-reality-os-specs-wwdc-2023">uses</a> a <a href="https://en.wikipedia.org/wiki/Real-time_operating_system">real-time operating system</a>. That means that tasks are always executed in a fixed amount of time. Most of our computers run on a time-sharing operating system, which schedules tasks on the fly, and can result in slowdowns. Think about jittery mouse cursors, or spinning beach balls, and you’ve got the idea. That won’t fly with something as critical as pass-through video and object rendering. Any glitch there would be like a glitch in the Matrix, and would be jarring at best, and utterly nauseating at worst. It might even cause you to stumble and fall.</p>



<figure><img decoding="async" loading="lazy" width="1200" height="800" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-1200x800.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-1200x800.jpg 1200w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-300x200.jpg 300w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-600x400.jpg 600w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-768x512.jpg 768w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-324x216.jpg 324w, https://valkyrie.cdn.ifixit.com/media/2024/02/03094812/AVP_TD_EDITED_62-1-450x300.jpg 450w" sizes="(max-width: 1200px) 100vw, 1200px"></figure>



<h3>An Incredible Feat, With One Really Weird Design Decision</h3>



<p>The original iPhone did something similar. When its underpowered chips couldn’t keep up with rendering a fast-scrolling page, it would <a href="https://daringfireball.net/2008/10/iphone_3g">switch to a gray-and-white checkerboard</a>, which kept up with all your flicks and swipes. Apple prioritized responsiveness over graphical fidelity. This time around, they have prioritized graphics fidelity <em>and</em> responsiveness, and taken the hit on battery life, weight, and heat. Given how important the experience is to Apple’s AR experience, this is probably the right choice for a first generation device.</p>



<p>The Vision Pro is insanely ambitious. Yes, it’s heavy, and the glass is fragile, and that tethered battery might get annoying. But Apple has managed to pack the power of a Mac, plus the performance of a new dedicated AR chip, into a computer that you can wear on your face.</p>



<p>Repairability-wise, it’s not great, but on the plus side, some of the connections are quite delightful. You should have seen our teardown team jump up when they realized that the side arms could be popped out using the SIM-removal tool, for example, and the magnetic cushions are yet more user-friendly.</p>



<p>So why, when this thing clearly took years and years to create—and is Apple’s latest bet on the future of computing—did Apple fail to live up to their own standards with the EyeSight screen? </p>



<p>It’s dim, it’s low-resolution, and it adds a lot of bulk, weight, complexity, and expense to the most weight-sensitive part of the headset. Did they finally hit the drop dead date and miss their targeted performance? Could it be a late-stage manufacturing error? Regardless, we’re sure bringing it to market was a difficult decision.</p>



<p>We’ve been disassembling VR headsets since the original <a href="https://www.ifixit.com/Device/Oculus_Rift">Oculus</a>, and they continue to surprise and delight. There is so much fascinating mechanical and optical design packed in here. Apple’s seamless integration of sensors for rock-solid location tracking is just phenomenal, and we’re eager to dive into how they did it.</p>



<p>We’re not done with our analysis: there’s lots more to investigate inside this device. Next time, we’ll dive into the internal displays, sensor arrays and we’ll award a repairability score. </p>



<p>What else are you excited to see? IPD calibration motors, cooling, specific chips or circuitry? Follow along on social media, or check back here in a few shakes, we’ve got plenty more coming.</p>



<figure><a href="https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1.jpg"><img decoding="async" loading="lazy" width="7360" height="4136" src="https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1.jpg" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1.jpg 7360w, https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1-1536x863.jpg 1536w, https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1-2048x1151.jpg 2048w, https://valkyrie.cdn.ifixit.com/media/2024/02/03115347/AVP-Layout-1-1602x900.jpg 1602w" sizes="(max-width: 7360px) 100vw, 7360px"></a></figure>
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nuclear power saved Armenia (127 pts)]]></title>
            <link>https://thebulletin.org/2024/01/how-nuclear-power-saved-armenia/</link>
            <guid>39246563</guid>
            <pubDate>Sun, 04 Feb 2024 01:20:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thebulletin.org/2024/01/how-nuclear-power-saved-armenia/">https://thebulletin.org/2024/01/how-nuclear-power-saved-armenia/</a>, See on <a href="https://news.ycombinator.com/item?id=39246563">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span><picture title="" decoding="async" loading="lazy">
<source type="image/webp" srcset="https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-1024x768.jpg.webp 1024w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-300x225.jpg.webp 300w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-768x576.jpg.webp 768w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-1536x1152.jpg.webp 1536w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-493x370.jpg.webp 493w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image.jpg.webp 1920w" sizes="(max-width: 1024px) 100vw, 1024px">
<img width="1024" height="768" src="https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-1024x768.jpg" alt="Armenia's Metsamor nuclear power plant cooling towers. (Credit: Adam Jones via Wikimedia Commons. CC BY-SA 2.0)" decoding="async" loading="lazy" srcset="https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-1024x768.jpg 1024w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-300x225.jpg 300w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-768x576.jpg 768w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-1536x1152.jpg 1536w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image-493x370.jpg 493w, https://thebulletin.org/wp-content/uploads/2021/03/Metsamor_nuclear_power_plant_cooling_towers_Armenia_June_2015-image.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px">
</picture>
Armenia's Metsamor nuclear power plant cooling towers. (Credit: Adam Jones via Wikimedia Commons. CC BY-SA 2.0)</span></p><div><p>The world is currently in the process of reevaluating its past rejection of nuclear power and is increasingly starting to view it as a reliable source of power that allows for greater energy security. This is at least in part due to the energy crisis that befell Europe after the Russian invasion of Ukraine in 2022, vindicating past worries that over-reliance on fossil fuels from autocratic regimes has made the Western countries vulnerable to political blackmail.</p>
<p>It is now clear that Western use of natural gas and petroleum from aggressive dictatorships—which use cash flows from oil and gas sales to reinforce and expand their hold on power—has backfired badly. In this context, the experience of Armenia—a small country that draws 40 percent of its energy from nuclear power—is instructive, showing how nuclear power can be instrumental in building societal reliance and political stability.</p>
<p><strong>Living in the dark cold.</strong> It is the winter of 1992–1993. As I climb the dark stairs in a freezing-cold Soviet apartment building in Yerevan, the capital of Armenia where my family and I live, the water from the two full buckets I carry is splashing down my legs and freezing on the stairs. My sister Shooshan and I, 14 and 15, are carrying water up to our 11<sup>th</sup>-floor apartment. The water to our apartment shut off weeks ago, and we get at most one hour of electricity each day. I estimate that we need exactly seven gallons of water, if we are careful, for our basic daily needs. So, we repeat the trip every day. During the precious hour when we do get electricity, my mother rushes to the kitchen to cook food for the next 24 hours. I run to the bakery, where I stand in a long queue to buy the half pound of bread that the state has rationed for each one of us.</p>
<p>The daily routine, which goes on for the whole winter, is exhausting. But it is also empowering. As teenagers we feel that we are stronger than the disastrous conditions inflicted on us by the combination of the Soviet collapse, the Nagorno-Karabakh war, and the ensuing severe energy crisis.</p><div id="thebu-1161222088"><p><a href="https://thebulletin.org/doomsday-clock/?utm_source=Website&amp;utm_medium=MobileMediumRectangle&amp;utm_campaign=2024DDCAnnouncementWebAds&amp;utm_content=DoomsdayClock_WebsiteAd_01222023#nav_menu" target="_blank" aria-label="2024 Doomsday Clock – Mobile Medium Rectangle"><img loading="lazy" decoding="async" src="https://thebulletin.org/wp-content/uploads/2024/01/2024-Mobile-Banner-Ads-3-1.png" alt="" srcset="https://thebulletin.org/wp-content/uploads/2024/01/2024-Mobile-Banner-Ads-3-1.png 600w, https://thebulletin.org/wp-content/uploads/2024/01/2024-Mobile-Banner-Ads-3-1-300x250.png 300w, https://thebulletin.org/wp-content/uploads/2024/01/2024-Mobile-Banner-Ads-3-1-444x370.png 444w" sizes="(max-width: 600px) 100vw, 600px" width="300" height="250"></a></p></div>
<p>The reasons that my sister and I—and the thousands of other Armenian teenagers like us—had to lug water and plan their lives around the one hour of electricity during that cruel winter go back to the turbulent events that shook Armenia during the preceding decades.</p>
<figure id="attachment_83863" aria-describedby="caption-attachment-83863"><picture loading="lazy" decoding="async">
<source type="image/webp" srcset="https://thebulletin.org/wp-content/uploads/2021/03/Armenia-map-image-279x300.jpg.webp 279w, https://thebulletin.org/wp-content/uploads/2021/03/Armenia-map-image.jpg.webp 326w" sizes="(max-width: 281px) 100vw, 281px">
<img loading="lazy" decoding="async" src="https://thebulletin.org/wp-content/uploads/2021/03/Armenia-map-image-279x300.jpg" alt="Map of Armenia. Credit: The World Factbook 2021. Central Intelligence Agency." width="281" height="302" srcset="https://thebulletin.org/wp-content/uploads/2021/03/Armenia-map-image-279x300.jpg 279w, https://thebulletin.org/wp-content/uploads/2021/03/Armenia-map-image.jpg 326w" sizes="(max-width: 281px) 100vw, 281px">
</picture>
<figcaption id="caption-attachment-83863">Map of Armenia. Credit: The World Factbook 2021. Central Intelligence Agency.</figcaption></figure>
<p>In the 1970s and 1980s, the Soviet Union—which Armenia was part of—rapidly expanded its fleet of nuclear reactors to support its growing industrial energy needs. As a result, two pressurized water reactors (PWR) of the Soviet VVER-440 type were built in the Armenian town of Metsamor, about 30 kilometers west of Yerevan. Started in 1977 and 1980, respectively, the two reactors quickly covered more than half of the energy needs of the Armenian Soviet Socialist Republic. (The remainder of the electricity was generated by Armenia’s hydroelectric stations and gas-fired power plants.) The Armenia of the 1980s was a tiny but prosperous Soviet republic that prided itself in a highly educated labor force, an array of scientific institutes, and a vibrant electronics industry that produced some of the early Soviet computer mainframe designs.</p>
<p>A series of violent events during the collapse of the Soviet Union would dramatically alter the Armenian dream.</p>
<p><strong>Chernobyl. </strong>On April 26, 1986, one of the Soviet-designed, graphite-moderated RBMK reactors at the Chernobyl nuclear power plant underwent a catastrophic power excursion that ripped the reactor open. The explosion and fire that followed propelled an enormous amount of radioactive matter into the open atmosphere leading to what is now known as the Chernobyl nuclear disaster, with widespread radioactive contamination, hundreds of deaths from acute radiation poisoning, and likely thousands of additional deaths due to radiation-induced cancers in the months and years that followed.</p>
<p>The Chernobyl accident resonated worldwide, dramatically undermining public trust in nuclear power as a safe source of energy. The public perception of danger from nuclear power was magnified by the outrageous lies that the Soviet leadership spread about the disaster, the obvious incompetence and irresponsibility of the Soviet nuclear designers who built and operated the Chernobyl reactor, and the poorly executed cleanup efforts which were compounded by miscalculations and gross mistakes.</p>
<p>Overnight, citizens across the Soviet Union and beyond went from a blissful ignorance about radiation to an understandable—yet irrational—fear of anything radiation-related. People in Armenia, despite living more than 2,000 kilometers away from Chernobyl, started perceiving radioactive threats everywhere, often attributing many of their common ailments to radiation. Physicists, like my parents, tried to explain what radiation is and how natural doses of radiation are not dangerous. But their advice was sometimes met with hostility: Weren’t the builders of Chernobyl also scientists?</p>
<p>In one chilling conversation that I witnessed at a dinner party, one of the guests told my father only half in jest, “You physicists… you should all be shot!” To paraphrase Valery Legasov’s eponymous character from HBO’s <a href="https://www.nytimes.com/2019/05/03/arts/television/review-chernobyl-hbo.html">five-part mini-series “Chernobyl”</a>: The danger of the lies is not that we mistake them for the truth, but that when enough lies are told we lose hope in the truth and start believing in stories. (Legasov was a Soviet chemist who actively worked on the causes and consequences of the Chernobyl disaster. <a href="https://vtoraya-literatura.com/pdf/radio_liberty_report_on_the_ussr_vol01_14_1989__ocr.pdf">Concerned</a> by the lack of nuclear safety in the Soviet nuclear industry, he died by suicide on April 27, 1988.)</p>
<p><strong>An earthquake, the Soviet collapse, and war.</strong> In December 1988, the devastating earthquake of Spitak killed 50,000 people—a harrowing 2 percent of Armenia’s population—and destroyed most of the country’s infrastructure. The two VVER-440 reactors at Metsamor were suddenly in the public eye. Would another earthquake rip them open and turn Armenia’s heartland, where half of Armenia’s population lived, into a Chernobyl-like radioactive wasteland?</p>
<p>To be clear, the PWRs at Metsamor are safer than the shoddily designed, graphite-moderated reactors at Chernobyl. Metsamor’s Soviet reactor design is close to the standard PWR designs that are still the most common reactor technology used in Western countries. And the buildings and the reactor structures were reinforced to account for Armenia’s seismic activity. But none of that mattered. After the Soviet government’s grotesque lies about the Chernobyl disaster, the official assurances that the Metsamor reactors were safe did not convince many. Legasov’s intuition was right: The pursuit for truth was replaced with belief in conspiratorial rumors. An environmentalist movement sprang up, calling for the shutdown of the Metsamor reactors. The authorities backed down, and the two reactors were turned off on February 25 and March 18, 1989.</p>
<p>Shortly after the shutdown, the Soviet Union started to crack, finally collapsing in 1991. In neighboring Azerbaijan, an Armenian minority living in the mountainous Nagorno-Karabakh region, feeling marginalized and discriminated against, had long been fighting to protect their civil rights. With the weakening of Soviet power, the protest movement turned into demands for secession from Azerbaijan. The response in Azerbaijan was <a href="https://armenian.usc.edu/baku-pogroms-in-context-of-the-karabakh-conflict/">a series of brutal anti-Armenian pogroms</a> in the cities of Sumgait and Baku that killed hundreds of Armenian civilians and forced about 300,000 others to flee the country. Fearing retaliation, the Azeri civilians living in Armenia fled en masse to Azerbaijan.</p>
<p>A relatively peaceful political disagreement had suddenly turned into a violent conflict, with Azerbaijan’s pogroms against Armenians escalating to a total war against the Armenian people of Nagorno-Karabakh. As the Armenian government supported the Nagorno-Karabakh secessionists, Azerbaijan retaliated by shutting off some of the natural gas pipelines that led to Armenia. In a sense, Azerbaijan’s authorities did to Armenia what Russian President Vladimir Putin is now doing to Western European countries that support Ukraine’s war effort. With its nuclear reactors and natural gas supply shut down, Armenia was left with a reduced capacity to generate electricity.</p>
<p>Then came the winter of 1992–1993. Mountain rivers froze, hydroelectric dams dried up, and suddenly hydropower too was nearly gone. Armenia was getting barely a trickle of electricity. What followed is a period now known in Armenia as “tsurt u mut tariner,” literally the cold and dark years: severe shortages of electricity, freezing concrete apartment complexes, closed schools, and many other disruptions. The economy collapsed, with Armenia’s gross domestic product contracting by an estimated 50 to 80 percent between 1990 and 1993. Then, a massive exodus followed, shrinking Armenia’s population by a quarter in just a few years.</p>
<p><strong>Nuclear power revival.</strong> The Armenian public quickly realized that, by abandoning nuclear power, it had forfeited the country’s energy independence. That vulnerability was—and still is—very effectively leveraged by its arch-enemy Azerbaijan. Was it too late to restore nuclear power?</p>
<p>Understanding their mistake, the Armenian authorities re-evaluated their past decision. The choice was stark: Either indulge in exaggerated fears of radiation and face unpredictable consequences, or sober up and accept nuclear power as a lesser evil. Ultimately the government chose the sober option. But rather than rushing headfirst to hastily restart the Metsamor nuclear power plant, the authorities decided to make significant safety improvements to the reactors.</p>
<p>One of the Metsamor reactors finally restarted on November 5, 1995, just before the winter season. The desperately needed 400 megawatts flowed again into the small country’s languishing power grid. Almost overnight, lights were turned on, water pumps worked again, and industries revved up to capacity. Children like my sister and I stopped their exhausting routine and Armenia became a net exporter of electricity.</p>
<p>Over the 13 years that followed, Armenia’s economy grew by an unprecedented 700 percent. The difficult decision to restore nuclear power had saved Armenia and had put it on a path of development. In 2020, about 35 percent of <a href="https://www.eia.gov/international/data/country/ARM/electricity/electricity-generation?pd=2&amp;p=00000000000000000000000000000fvu&amp;u=0&amp;f=A&amp;v=mapbubble&amp;a=-&amp;i=none&amp;vo=value&amp;t=C&amp;g=none&amp;l=249--7&amp;s=315532800000&amp;e=1609459200000&amp;">electricity generated in Armenia</a> came from nuclear, 25 percent came from renewables (primarily hydropower), and the remaining 40 percent from fossil fuels. (In 2021, the share of nuclear power temporarily dropped to 26 percent because the Metsamor reactor was shut down longer than usual to perform a thermal annealing of the pressure vessel, a maintenance method aimed at managing aging effects.)</p>
<p>Despite its important contribution to the electricity mix, the nuclear power plant at Metsamor is not without problems. Mainly, like most Soviet-era PWRs, the reactor does not have the external containment building that is common with Western designs. It is also an aging machine. Because of Armenia’s growing energy needs, the Metsamor reactor has been issued <a href="https://world-nuclear.org/information-library/country-profiles/countries-a-f/armenia.aspx">multiple lifetime extensions</a>. Based on current plans, Metsamor’s VVER-440 reactor will shut down permanently by 2036. Meanwhile the Armenian government has been busy exploring replacement alternatives, such as possibly <a href="https://en.armradio.am/2023/05/24/armenian-in-talks-with-several-partners-to-build-a-new-power-plant-pm-pashinyan-comments-on-us-plans-to-build-modular-nuclear-reactors/">US-built small modular reactors</a> (SMRs), seen as a viable replacement. Armenian officials have also entered in <a href="https://www.neimagazine.com/news/newsarmenia-considers-nuclear-options-10933703">discussions with Russia</a> about the possibility of replacing the Soviet-era VVER-440 reactor with the much larger and more modern Russian VVER-1200 design. While the US option is not easy—mainly because of the lack of readiness of most SMR designs—the Russian option is particularly fraught. Armenia is reluctant to further increase its energy dependence on Russia, given Putin’s campaign of neo-Soviet expansionism. This is further exacerbated by the technical and economic difficulty of hosting a 1200-megawatt electric VVER-1200 unit on a grid that on average consumes only about 1,000 megawatts.</p>
<p><strong>Survival in the shadow of petro-dictatorship.</strong> In recent years, social scientists have studied the negative impacts of nuclear power on underprivileged communities, such as the effects of uranium mining on indigenous populations. These studies are important for understanding the social cost of this resource. However very rarely have scholars studied the positive impact that nuclear power has had in helping the victims of oppression.</p>
<p>Most of the three million inhabitants in Armenia trace their lineage to the survivors of the Armenian Genocide of 1915. Most live near the border with the perpetrator state of Turkey, which to this day refuses to acknowledge its crime and in the recent past has actively helped Azerbaijan. Since 1993, Azerbaijan has been ruled by the Aliyev dynasty with an iron fist, strengthened by the cash flows from the export of the country’s large hydrocarbon reserves to Western countries. To further strengthen his hold on power, Azerbaijani President Ilham Aliyev (son of Heydar Aliyev who held power in Azerbaijan for several decades) has tapped into Azerbaijanis’ trauma from the 1990s by demonizing Armenians and blaming all of Azerbaijan’s ill on this minority.</p>
<p>Since he took power in 2003, the regime of Aliyev son has been accused of <a href="https://www.theguardian.com/commentisfree/2023/oct/09/azerbaijani-ethnic-cleansing-armenians-nagorno-karabakh-children">curtailing free speech and ethnic cleansing</a> of Armenians, whereas Azerbaijan’s armed forces have been busy mounting a campaign of <a href="https://news.cornell.edu/stories/2022/09/report-shows-near-total-erasure-armenian-heritage-sites">widespread cultural erasure</a>. These decades of threats culminated in last September with a <a href="https://www.economist.com/leaders/2023/09/28/a-humanitarian-disaster-is-under-way-in-nagorno-karabakh">swift military attack</a> on the Nagorno-Karabakh region, which in just one week brought the 3,000-year-old indigenous Armenian presence there effectively to an end. The situation currently is so severe that Luis Moreno Ocampo, a former chief prosecutor of the International Criminal Court, <a href="https://amp.cnn.com/cnn/2023/08/11/asia/nagorno-karabakh-armenians-genocide-intl-hnk/index.html">has warned</a> that a new genocide may be underway.</p>
<p>Armenians, whose newly budding democracy is under constant threat from the various authoritarian governments in the region, cannot achieve cultural and existential security if they do not have a state that ensures their security. And that includes energy security, to which nuclear power generation is key. Of course, Azerbaijan deserves to have a democratic government, too, something that is being hindered by the Western countries’ over-reliance on fossil fuel exports.</p>
<p><strong>The lessons of small nations. </strong>When it comes to understanding the value of nuclear energy, studies tend to focus on the big nuclear powers such as the United States, China, and Russia. They rarely study the experiences of small countries like Armenia. Still, the study of these “insignificant” players is important in terms of understanding the mistakes made, successes achieved, and lessons learned, which can be relevant for the “big” players as well. In a telling example, Germany is learning the hard way about the dangers of complacency when it comes to choosing between nuclear energy and fossil fuels for its energy mix: Over the last 20 years, German politicians preferred to shut down their “scary”—but nonetheless safe—nuclear power plants and increase their potentially destabilizing—but considered harmless—reliance on Russia’s natural gas. Had German policymakers studied Armenia’s experience of the 1990s, they could probably have avoided the energy crisis the country is currently experiencing.</p>
<p>Sadly, it’s hard to tell whether European leaders have learned anything from Armenia’s struggle for energy security. In a now much-criticized statement from 2022, European Commission President Ursula von der Leyen called Azerbaijan’s dictator Ilham Aliyev “<a href="https://ec.europa.eu/commission/presscorner/detail/da/statement_22_4583">a reliable partner</a>.” This gesture is now believed to have, at least partly, emboldened the Aliyev regime’s brutality toward the Armenian population of Nagorno-Karabakh. At least for now, it is as if Europe is merely switching dictators while maintaining the same dependence on fossil fuels.</p>
<p>Only a full reckoning by Western countries of their over-reliance on fossil fuels can put an end to the authoritarian regimes that exist only because of their hydrocarbon exports. Such a reckoning, along with the development of renewable energy and nuclear power, would lead to net gains for the climate and the environment. It would also help strengthen liberal democracies that are being unprecedently threatened.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The DIY Diggers Who Can't Stop Making 'Hobby Tunnels' (236 pts)]]></title>
            <link>https://www.bloomberg.com/news/features/2024-02-03/dig-if-you-will-the-underground-world-of-hobby-tunneling</link>
            <guid>39245893</guid>
            <pubDate>Sat, 03 Feb 2024 23:39:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/features/2024-02-03/dig-if-you-will-the-underground-world-of-hobby-tunneling">https://www.bloomberg.com/news/features/2024-02-03/dig-if-you-will-the-underground-world-of-hobby-tunneling</a>, See on <a href="https://news.ycombinator.com/item?id=39245893">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TCC RISC-V Compiler Runs in the Web Browser (Thanks to Zig Compiler) (162 pts)]]></title>
            <link>https://lupyuen.codeberg.page/articles/tcc.html</link>
            <guid>39245664</guid>
            <pubDate>Sat, 03 Feb 2024 23:03:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lupyuen.codeberg.page/articles/tcc.html">https://lupyuen.codeberg.page/articles/tcc.html</a>, See on <a href="https://news.ycombinator.com/item?id=39245664">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <!--[if lte IE 8]>
    <div class="warning">
        This old browser is unsupported and will most likely display funky
        things.
    </div>
    <![endif]-->

        <!-- Begin scripts/rustdoc-before.html: Pre-HTML for Custom Markdown files processed by rustdoc, like chip8.md -->

    <!-- Begin Theme Picker -->
    
    
    
    <!-- Theme Picker -->

    <!-- End scripts/rustdoc-before.html -->
    

    
    <nav id="TOC"><ul>
<li><a href="#tcc-in-the-web-browser">1 TCC in the Web Browser</a><ul></ul></li>
<li><a href="#zig-compiles-tcc-to-webassembly">2 Zig compiles TCC to WebAssembly</a><ul></ul></li>
<li><a href="#posix-for-webassembly">3 POSIX for WebAssembly</a><ul></ul></li>
<li><a href="#file-input-and-output">4 File Input and Output</a><ul></ul></li>
<li><a href="#fearsome-fprintf-and-friends">5 Fearsome fprintf and Friends</a><ul></ul></li>
<li><a href="#test-with-apache-nuttx-rtos">6 Test with Apache NuttX RTOS</a><ul></ul></li>
<li><a href="#hello-nuttx">7 Hello NuttX!</a><ul></ul></li>
<li><a href="#whats-next">8 What’s Next</a><ul></ul></li>
<li><a href="#appendix-compile-tcc-with-zig">9 Appendix: Compile TCC with Zig</a><ul></ul></li>
<li><a href="#appendix-javascript-calls-tcc">10 Appendix: JavaScript calls TCC</a><ul></ul></li>
<li><a href="#appendix-pattern-matching">11 Appendix: Pattern Matching</a><ul></ul></li>
<li><a href="#appendix-nuttx-system-call">12 Appendix: NuttX System Call</a><ul></ul></li>
<li><a href="#appendix-build-nuttx-for-qemu">13 Appendix: Build NuttX for QEMU</a><ul></ul></li>
<li><a href="#appendix-missing-functions">14 Appendix: Missing Functions</a><ul></ul></li></ul></nav><p>📝 <em>4 Feb 2024</em></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-title.png" alt="TCC RISC-V Compiler runs in the Web Browser (thanks to Zig Compiler)"></p>
<p><a href="https://lupyuen.github.io/tcc-riscv32-wasm/">(Try the <strong>Online Demo</strong>)</a></p>
<p><a href="https://youtu.be/DJMDYq52Iv8">(Watch the <strong>Demo on YouTube</strong>)</a></p>
<p><em>TCC is a Tiny C Compiler for 64-bit RISC-V (and other platforms)…</em></p>
<p><em>Can we run TCC Compiler in a Web Browser?</em></p>
<p>Let’s do it! We’ll compile <a href="https://github.com/sellicott/tcc-riscv32"><strong>TCC (Tiny C Compiler)</strong></a> from C to WebAssembly with <a href="https://ziglang.org/"><strong>Zig Compiler</strong></a>.</p>
<p>In this article, we talk about the tricky bits of our <strong>TCC ported to WebAssembly</strong>…</p>
<ul>
<li>
<p>We compiled <strong>TCC to WebAssembly</strong> with one tiny fix</p>
</li>
<li>
<p>But we hit some <strong>Missing POSIX Functions</strong></p>
</li>
<li>
<p>So we built minimal <strong>File Input and Output</strong> </p>
</li>
<li>
<p>Hacked up a simple workaround for <strong>fprintf and friends</strong></p>
</li>
<li>
<p>And TCC produces a <strong>RISC-V Binary</strong> that runs OK</p>
<p>(After some fiddling and meddling in RISC-V Assembly)</p>
</li>
</ul>
<p><em>Why are we doing this?</em></p>
<p>Today we’re running <a href="https://lupyuen.codeberg.page/articles/tinyemu2"><strong>Apache NuttX RTOS</strong></a> inside a Web Browser, with WebAssembly + Emscripten + 64-bit RISC-V.</p>
<p>(<strong>Real-Time Operating System</strong> in a Web Browser on a General-Purpose Operating System!)</p>
<p>What if we could <strong>Build and Test NuttX Apps</strong> in the Web Browser…</p>
<ol>
<li>
<p>We type a <strong>C Program</strong> into our Web Browser (pic below)</p>
</li>
<li>
<p>Compile it into an <strong>ELF Executable</strong> with TCC</p>
</li>
<li>
<p>Copy the ELF Executable to the <strong>NuttX Filesystem</strong></p>
</li>
<li>
<p>And <strong>NuttX Emulator</strong> runs our ELF Executable inside the Web Browser</p>
</li>
</ol>
<p>Learning NuttX becomes so cool! This is how we made it happen…</p>
<p><a href="https://youtu.be/DJMDYq52Iv8">(Watch the <strong>Demo on YouTube</strong>)</a></p>
<p><a href="https://research.cs.queensu.ca/home/cordy/pub/downloads/tplus/Turing_Plus_Report.pdf">(Not to be confused with <strong>TTC Compiler</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-web.png" alt="Online Demo of TCC Compiler in WebAssembly"></p>
<p><a href="https://lupyuen.github.io/tcc-riscv32-wasm/"><em>Online Demo of TCC Compiler in WebAssembly</em></a></p>
<h2 id="tcc-in-the-web-browser"><a href="#tcc-in-the-web-browser">1 TCC in the Web Browser</a></h2>
<p>Click this link to try <strong>TCC Compiler in our Web Browser</strong> (pic above)</p>
<ul>
<li>
<p><a href="https://lupyuen.github.io/tcc-riscv32-wasm/"><strong>TCC RISC-V Compiler in WebAssembly</strong></a></p>
<p><a href="https://youtu.be/DJMDYq52Iv8">(Watch the <strong>Demo on YouTube</strong>)</a></p>
</li>
</ul>
<p>This <strong>C Program</strong> appears…</p>
<div><pre><code>// Demo Program for TCC Compiler
int main(int argc, char *argv[]) {
  printf("Hello, World!!\n");
  return 0;
}
</code></pre></div>
<p>Click the “<strong>Compile</strong>” button. Our Web Browser calls TCC to compile the above program…</p>
<div><pre><code>## Compile to RISC-V ELF
tcc -c hello.c
</code></pre></div>
<p>And it downloads the compiled <a href="https://en.wikipedia.org/wiki/Executable_and_Linkable_Format"><strong>RISC-V ELF <code>a.out</code></strong></a>. We inspect the Compiled Output…</p>
<div><pre><code>## Dump the RISC-V Disassembly
## of TCC Output
$ riscv64-unknown-elf-objdump \
    --syms --source --reloc --demangle \
    --line-numbers --wide  --debugging \
    a.out

main():
   ## Prepare the Stack
   0: fe010113  addi   sp, sp, -32
   4: 00113c23  sd     ra, 24(sp)
   8: 00813823  sd     s0, 16(sp)
   c: 02010413  addi   s0, sp, 32
  10: 00000013  nop

   ## Load to Register A0: "Hello World"
  14: fea43423  sd     a0, -24(s0)
  18: feb43023  sd     a1, -32(s0)
  1c: 00000517  auipc  a0, 0x0
  1c: R_RISCV_PCREL_HI20 L.0
  20: 00050513  mv     a0, a0
  20: R_RISCV_PCREL_LO12_I .text

   ## Call printf()
  24: 00000097  auipc  ra, 0x0
  24: R_RISCV_CALL_PLT printf
  28: 000080e7  jalr   ra  ## 24 &lt;main+0x24&gt;

   ## Clean up the Stack and
   ## return 0 to Caller
  2c: 0000051b  sext.w a0, zero
  30: 01813083  ld     ra, 24(sp)
  34: 01013403  ld     s0, 16(sp)
  38: 02010113  addi   sp, sp, 32
  3c: 00008067  ret
</code></pre></div>
<p>Yep the <strong>64-bit RISC-V Code</strong> looks legit! Very similar to our <a href="https://lupyuen.codeberg.page/articles/app#inside-a-nuttx-app"><strong>NuttX App</strong></a>. (So it will probably run on NuttX)</p>
<p>What just happened? We go behind the scenes…</p>
<p><a href="https://gist.github.com/lupyuen/ab8febefa9c649ad7c242ee3f7aaf974">(See the <strong>Entire Disassembly</strong>)</a></p>
<p><a href="https://lupyuen.codeberg.page/articles/app#inside-a-nuttx-app">(About the <strong>RISC-V Instructions</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-zig.jpg" alt="Zig Compiler compiles TCC Compiler to WebAssembly"></p>
<h2 id="zig-compiles-tcc-to-webassembly"><a href="#zig-compiles-tcc-to-webassembly">2 Zig compiles TCC to WebAssembly</a></h2>
<p><em>Will Zig Compiler happily compile TCC to WebAssembly?</em></p>
<p>Amazingly, yes! (Pic above)</p>
<div><pre><code>## Zig Compiler compiles TCC Compiler
## from C to WebAssembly. Produces `tcc.o`
zig cc \
  -c \
  -target wasm32-freestanding \
  -dynamic \
  -rdynamic \
  -lc \
  -DTCC_TARGET_RISCV64 \
  -DCONFIG_TCC_CROSSPREFIX="\"riscv64-\""  \
  -DCONFIG_TCC_CRTPREFIX="\"/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_LIBPATHS="\"{B}:/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_SYSINCLUDEPATHS="\"{B}/include:/usr/riscv64-linux-gnu/include\""   \
  -DTCC_GITHASH="\"main:b3d10a35\"" \
  -Wall \
  -O2 \
  -Wdeclaration-after-statement \
  -fno-strict-aliasing \
  -Wno-pointer-sign \
  -Wno-sign-compare \
  -Wno-unused-result \
  -Wno-format-truncation \
  -Wno-stringop-truncation \
  -I. \
  tcc.c
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/tcc.c">(See the <strong>TCC Source Code</strong>)</a></p>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-compile-tcc-with-zig">(About the <strong>Zig Compiler Options</strong>)</a></p>
<p>We link the TCC WebAssembly with our <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig"><strong>Zig Wrapper</strong></a> (that exports the TCC Compiler to JavaScript)…</p>
<div><pre><code>## Compile our Zig Wrapper `tcc-wasm.zig` for WebAssembly
## and link it with TCC compiled for WebAssembly `tcc.o`
## Generates `tcc-wasm.wasm`
zig build-exe \
  -target wasm32-freestanding \
  -rdynamic \
  -lc \
  -fno-entry \
  -freference-trace \
  --verbose-cimport \
  --export=compile_program \
  zig/tcc-wasm.zig \
  tcc.o

## Test everything with Web Browser
## or Node.js
node zig/test.js
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig">(See the <strong>Zig Wrapper tcc-wasm.zig</strong>)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/test.js">(See the <strong>Test JavaScript test.js</strong>)</a></p>
<p><em>What’s inside our Zig Wrapper?</em></p>
<p>Our Zig Wrapper will…</p>
<ol>
<li>
<p>Receive the <strong>C Program</strong> from JavaScript</p>
</li>
<li>
<p>Receive the <strong>TCC Compiler Options</strong> from JavaScript</p>
</li>
<li>
<p>Call TCC Compiler to <strong>compile our program</strong></p>
</li>
<li>
<p>Return the compiled <strong>RISC-V ELF</strong> to JavaScript</p>
</li>
</ol>
<p>Like so: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L11-L76">tcc-wasm.zig</a></p>
<div><pre><code>/// Call TCC Compiler to compile a
/// C Program to RISC-V ELF
pub export fn compile_program(
  options_ptr: [*:0]const u8, // Options for TCC Compiler (Pointer to JSON Array:  ["-c", "hello.c"])
  code_ptr:    [*:0]const u8, // C Program to be compiled (Pointer to String)
) [*]const u8 { // Returns a pointer to the `a.out` Compiled Code (Size in first 4 bytes)

  // Receive the C Program from
  // JavaScript and set our Read Buffer
  // https://blog.battlefy.com/zig-made-it-easy-to-pass-strings-back-and-forth-with-webassembly
  const code: []const u8 = std.mem.span(code_ptr);
  read_buf = code;

  // Omitted: Receive the TCC Compiler
  // Options from JavaScript
  // (JSON containing String Array: ["-c", "hello.c"])
  ...

  // Call the TCC Compiler
  _ = main(@intCast(argc), &amp;args_ptrs);

  // Return pointer of `a.out` to
  // JavaScript. First 4 bytes: Size of
  // `a.out`. Followed by `a.out` data.
  const slice = std.heap.page_allocator.alloc(u8, write_buflen + 4)   
    catch @panic("Failed to allocate memory");
  const size_ptr: *u32 = @alignCast(@ptrCast(slice.ptr));
  size_ptr.* = write_buflen;
  @memcpy(slice[4 .. write_buflen + 4], write_buf[0..write_buflen]);
  return slice.ptr; // TODO: Deallocate this memory
}
</code></pre></div>
<p>Plus a couple of Magical Bits that we’ll cover in the next section.</p>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-javascript-calls-tcc">(How JavaScript calls our <strong>Zig Wrapper</strong>)</a></p>
<p><em>Zig Compiler compiles TCC without any code changes?</em></p>
<p>Inside TCC, we stubbed out the <a href="https://github.com/lupyuen/tcc-riscv32-wasm/commit/e30454a0eb9916f820d58a7c3e104eeda67988d8"><strong>setjmp / longjmp</strong></a> to make it compile with Zig Compiler.</p>
<p>Everything else compiles OK!</p>
<p><em>Is it really OK to stub them out?</em></p>
<p><a href="https://en.wikipedia.org/wiki/Setjmp.h"><strong>setjmp / longjmp</strong></a> are called to <strong>Handle Errors</strong> during TCC Compilation. Assuming everything goes hunky dory, we won’t need them.</p>
<p>Later we’ll find a better way to express our outrage. (Instead of jumping around)</p>
<p>We probe the Magical Bits inside our Zig Wrapper…</p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-posix.jpg" alt="TCC Compiler in WebAssembly needs POSIX Functions"></p>
<h2 id="posix-for-webassembly"><a href="#posix-for-webassembly">3 POSIX for WebAssembly</a></h2>
<p><em>What’s this POSIX?</em></p>
<p>TCC Compiler was created as a <strong>Command-Line App</strong>. So it calls the typical <a href="https://en.wikipedia.org/wiki/POSIX"><strong>POSIX Functions</strong></a> like <strong>fopen, fprintf, strncpy, malloc,</strong> …</p>
<p>But WebAssembly running in a Web Browser ain’t <strong>No Command Line</strong>! (Pic above)</p>
<p><a href="https://en.wikipedia.org/wiki/C_standard_library">(WebAssembly doesn’t have a <strong>C Standard Library libc</strong>)</a></p>
<p><em>Is POSIX a problem for WebAssembly?</em></p>
<p>We counted <a href="https://lupyuen.codeberg.page/articles/tcc#appendix-missing-functions"><strong>72 POSIX Functions</strong></a> needed by TCC Compiler, but missing from WebAssembly.</p>
<p>Thus we fill in the <a href="https://lupyuen.codeberg.page/articles/tcc#appendix-missing-functions"><strong>Missing Functions</strong></a> ourselves.</p>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-missing-functions">(About the <strong>Missing POSIX Functions</strong>)</a></p>
<p><em>Surely other Zig Devs will have the same problem?</em></p>
<p>Thankfully we can borrow the POSIX Code from other <strong>Zig Libraries</strong>…</p>
<ul>
<li>
<p><a href="https://github.com/marler8997/ziglibc"><strong>ziglibc</strong></a>: Zig implementation of libc</p>
</li>
<li>
<p><a href="https://github.com/ZigEmbeddedGroup/foundation-libc"><strong>foundation-libc</strong></a>: Freestanding implementation of libc</p>
</li>
<li>
<p><a href="https://lupyuen.codeberg.page/articles/lvgl3#appendix-lvgl-memory-allocation"><strong>PinePhone Simulator</strong></a>: For malloc</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L447-L774">(See the <strong>Borrowed Code</strong>)</a></p>
</li>
</ul>
<p><em>72 POSIX Functions? Sounds like a lot of work…</em></p>
<p>We might not need all 72 POSIX Functions. We stubbed out <strong>many of the functions</strong> to identify the ones that are called: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L776-L855">tcc-wasm.zig</a></p>
<div><pre><code>// Stub Out the Missing POSIX
// Functions. If TCC calls them, 
// we'll see a Zig Panic. Then we 
// implement them. The Types don't
// matter because we'll halt anyway.

pub export fn atoi(_: c_int) c_int {
  @panic("TODO: atoi");
}
pub export fn exit(_: c_int) c_int {
  @panic("TODO: exit");
}
pub export fn fopen(_: c_int) c_int {
  @panic("TODO: fopen");
}

// And many more functions...
</code></pre></div>
<p>Some of these functions are especially troubling for WebAssembly…</p>
<blockquote>
<p><img src="https://lupyuen.codeberg.page/images/tcc-posix2.jpg" alt="File Input and Output are especially troubling for WebAssembly"></p>
</blockquote>
<h2 id="file-input-and-output"><a href="#file-input-and-output">4 File Input and Output</a></h2>
<p><em>Why no #include in TCC for WebAssembly? And no C Libraries?</em></p>
<p>WebAssembly runs in a Secure Sandbox. <strong>No File Access</strong> allowed, sorry! (Like for Header and Library Files)</p>
<p>That’s why our Zig Wrapper <strong>Emulates File Access</strong> for the bare minimum 2 files…</p>
<ul>
<li>
<p>Read the <strong>C Program</strong>: <strong><code>hello.c</code></strong></p>
</li>
<li>
<p>Write the <strong>RISC-V ELF</strong>: <strong><code>a.out</code></strong></p>
</li>
</ul>
<p><strong>Reading a Source File <code>hello.c</code></strong> is extremely simplistic: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L104-L118">tcc-wasm.zig</a></p>
<div><pre><code>/// Emulate the POSIX Function `read()`
/// We copy from One Single Read Buffer
/// that contains our C Program
export fn read(fd0: c_int, buf: [*:0]u8, nbyte: size_t) isize {

  // TODO: Support more than one file
  const len = read_buf.len;
  assert(len &lt; nbyte);
  @memcpy(buf[0..len], read_buf[0..len]);
  buf[len] = 0;
  read_buf.len = 0;
  return @intCast(len);
}

/// Read Buffer for read
var read_buf: []const u8 = undefined;
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L26-L32">(<strong>read_buf</strong> is populated at startup)</a></p>
<p><strong>Writing the Compiled Output <code>a.out</code></strong> is just as barebones: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L128-L140">tcc-wasm.zig</a></p>
<div><pre><code>/// Emulate the POSIX Function `write()`
/// We write to One Single Memory
/// Buffer that will be returned to 
/// JavaScript as `a.out`
export fn fwrite(ptr: [*:0]const u8, size: usize, nmemb: usize, stream: *FILE) usize {

  // TODO: Support more than one `stream`
  const len = size * nmemb;
  @memcpy(write_buf[write_buflen .. write_buflen + len], ptr[0..]);
  write_buflen += len;
  return nmemb;
}

/// Write Buffer for fputc and fwrite
var write_buf = std.mem.zeroes([8192]u8);
var write_buflen: usize = 0;
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L62-L78">(<strong>write_buf</strong> will be returned to JavaScript)</a></p>
<p><em>Can we handle Multiple Files?</em></p>
<p>Right now we’re trying to embed the simple <a href="https://github.com/lupyuen/tcc-riscv32-wasm#rom-fs-filesystem-for-tcc-webassembly"><strong>ROM FS Filesystem</strong></a> into our Zig Wrapper.</p>
<p>The ROM FS Filesystem will be preloaded with the Header and Library Files needed by TCC.</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#rom-fs-filesystem-for-tcc-webassembly">(See the updates for <strong>ROM FS Filesystem</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-format.jpg" alt="Our Zig Wrapper uses Pattern Matching to match the C Formats and substitute the Zig Equivalent"></p>
<h2 id="fearsome-fprintf-and-friends"><a href="#fearsome-fprintf-and-friends">5 Fearsome fprintf and Friends</a></h2>
<p><em>Why is fprintf particularly problematic?</em></p>
<p>Here’s the fearsome thing about <strong>fprintf</strong> and friends: <strong>sprintf, snprintf, vsnprintf</strong>…</p>
<ul>
<li>
<p><strong>C Format Strings</strong> are difficult to parse</p>
</li>
<li>
<p><strong>Variable Number of Untyped Arguments</strong> might create Bad Pointers</p>
</li>
</ul>
<p>Hence we hacked up an implementation of <strong>String Formatting</strong> that’s safer, simpler and so-barebones-you-can-make-<em>soup-tulang</em>.</p>
<p><em>Soup tulang? Tell me more…</em></p>
<p>Our Zig Wrapper uses <a href="https://lupyuen.codeberg.page/articles/tcc#appendix-pattern-matching"><strong>Pattern Matching</strong></a> to match the <strong>C Formats</strong> and substitute the <strong>Zig Equivalent</strong> (pic above): <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L189-L207">tcc-wasm.zig</a></p>
<div><pre><code>// Format a Single `%d`
// like `#define __TINYC__ %d`
FormatPattern{

  // If the C Format String contains this...
  .c_spec = "%d",
  
  // Then we apply this Zig Format...
  .zig_spec = "{}",
  
  // And extract these Argument Types
  // from the Varargs...
  .type0 = c_int,
  .type1 = null
}
</code></pre></div>
<p>This works OK (for now) because TCC Compiler only uses <strong>5 Patterns for C Format Strings</strong>: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L189-L207">tcc-wasm.zig</a></p>
<div><pre><code>/// Pattern Matching for C String Formatting:
/// We'll match these patterns when
/// formatting strings
const format_patterns = [_]FormatPattern{

  // Format a Single `%d`, like `#define __TINYC__ %d`
  FormatPattern{
    .c_spec = "%d",  .zig_spec = "{}", 
    .type0  = c_int, .type1 = null
  },

  // Format a Single `%u`, like `L.%u`
  FormatPattern{ 
    .c_spec = "%u",  .zig_spec = "{}", 
    .type0  = c_int, .type1 = null 
  },

  // Format a Single `%s`, like `.rela%s`
  // Or `#define __BASE_FILE__ "%s"`
  FormatPattern{
    .c_spec = "%s", .zig_spec = "{s}",
    .type0  = [*:0]const u8, .type1 = null
  },

  // Format Two `%s`, like `#define %s%s\n`
  FormatPattern{
    .c_spec = "%s%s", .zig_spec = "{s}{s}",
    .type0  = [*:0]const u8, .type1 = [*:0]const u8
  },

  // Format `%s:%d`, like `%s:%d: `
  // (File Name and Line Number)
  FormatPattern{
    .c_spec = "%s:%d", .zig_spec = "{s}:{}",
    .type0  = [*:0]const u8, .type1 = c_int
  },
};
</code></pre></div>
<p>That’s our quick hack for <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L209-L447"><strong>fprintf and friends</strong></a>!</p>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-pattern-matching">(How we do <strong>Pattern Matching</strong>)</a></p>
<p><em>So simple? Unbelievable!</em></p>
<p>Actually we’ll hit more Format Patterns as TCC Compiler emits various <strong>Error and Warning Messages</strong>. But it’s a good start!</p>
<p>Later our Zig Wrapper shall cautiously and meticulously parse all kinds of C Format Strings. Or we do the <a href="https://github.com/marler8997/ziglibc/blob/main/src/printf.c#L32-L191"><strong>parsing in C</strong></a>, compiled to WebAssembly. (160 lines of C!)</p>
<p>(Funny how <strong>printf</strong> is the first thing we learn about C. Yet it’s incredibly difficult to implement!)</p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-nuttx.jpg" alt="Compile and Run NuttX Apps in the Web Browser"></p>
<h2 id="test-with-apache-nuttx-rtos"><a href="#test-with-apache-nuttx-rtos">6 Test with Apache NuttX RTOS</a></h2>
<p><em>TCC in WebAssembly has compiled our C Program to RISC-V ELF…</em></p>
<p><em>Will the ELF run on NuttX?</em></p>
<p><a href="https://nuttx.apache.org/docs/latest/"><strong>Apache NuttX RTOS</strong></a> is a tiny operating system for 64-bit RISC-V that runs on <a href="https://www.qemu.org/docs/master/system/target-riscv.html"><strong>QEMU Emulator</strong></a>. (And many other devices)</p>
<p>We build <a href="https://lupyuen.codeberg.page/articles/tcc#appendix-build-nuttx-for-qemu"><strong>NuttX for QEMU</strong></a> and copy our <a href="https://en.wikipedia.org/wiki/Executable_and_Linkable_Format"><strong>RISC-V ELF <code>a.out</code></strong></a> to the <a href="https://lupyuen.codeberg.page/articles/semihost#nuttx-apps-filesystem"><strong>NuttX Apps Filesystem</strong></a> (pic above)…</p>
<div><pre><code>## Copy RISC-V ELF `a.out`
## to NuttX Apps Filesystem
cp a.out apps/bin/
chmod +x apps/bin/a.out
</code></pre></div>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-build-nuttx-for-qemu">(How we build <strong>NuttX for QEMU</strong>)</a></p>
<p>Then we boot NuttX and run <strong><code>a.out</code></strong>…</p>
<div><pre><code>## Boot NuttX on QEMU 64-bit RISC-V
$ qemu-system-riscv64 \
  -semihosting \
  -M virt,aclint=on \
  -cpu rv64 \
  -smp 8 \
  -bios none \
  -kernel nuttx \
  -nographic

## Run `a.out` in NuttX Shell
NuttShell (NSH) NuttX-12.4.0
nsh&gt; a.out
Loading /system/bin/a.out
Exported symbol "printf" not found
Failed to load program 'a.out'
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#test-tcc-output-with-nuttx">(See the <strong>Complete Log</strong>)</a></p>
<p>NuttX politely accepts the RISC-V ELF (produced by TCC). And says that <strong>printf</strong> is missing.</p>
<p>Which makes sense: We haven’t linked our C Program with the <a href="https://github.com/lupyuen/tcc-riscv32-wasm#how-nuttx-build-links-a-nuttx-app"><strong>C Library</strong></a>!</p>
<p><a href="https://gist.github.com/lupyuen/847f7adee50499cac5212f2b95d19cd3#file-nuttx-elf-loader-log-L882-L1212">(Loading a <strong>RISC-V ELF</strong> should look like this)</a></p>
<p><em>How else can we print something in NuttX?</em></p>
<p>To print something, we can make a <a href="https://lupyuen.codeberg.page/articles/app#nuttx-app-calls-nuttx-kernel"><strong>System Call (ECALL)</strong></a> directly to NuttX Kernel (bypassing the POSIX Functions)…</p>
<div><pre><code>// NuttX System Call that prints
// something. System Call Number
// is 61 (SYS_write). Works exactly
// like POSIX `write()`
ssize_t write(
  int fd,           // File Descriptor (1 for Standard Output)
  const char *buf,  // Buffer to be printed
  size_t buflen     // Buffer Length
);

// Which makes an ECALL with these Parameters...
// Register A0 is 61 (SYS_write)
// Register A1 is the File Descriptor (1 for Standard Output)
// Register A2 points to the String Buffer to be printed
// Register A3 is the Buffer Length
</code></pre></div>
<p>That’s the same NuttX System Call that <strong>printf</strong> executes internally.</p>
<p>Final chance to say hello to NuttX…</p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-ecall.png" alt="TCC WebAssembly compiles a NuttX System Call"></p>
<h2 id="hello-nuttx"><a href="#hello-nuttx">7 Hello NuttX!</a></h2>
<p><em>We’re making a System Call (ECALL) to NuttX Kernel to print something…</em></p>
<p><em>How will we code this in C?</em></p>
<p>We execute the <a href="https://lupyuen.codeberg.page/articles/app#nuttx-app-calls-nuttx-kernel"><strong>ECALL in RISC-V Assembly</strong></a> like this: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/test-nuttx.js#L52-L105">test-nuttx.js</a></p>
<div><pre><code>int main(int argc, char *argv[]) {

  // Make NuttX System Call
  // to write(fd, buf, buflen)
  const unsigned int nbr = 61; // SYS_write
  const void *parm1 = 1;       // File Descriptor (stdout)
  const void *parm2 = "Hello, World!!\n"; // Buffer
  const void *parm3 = 15; // Buffer Length

  // Load the Parameters into
  // Registers A0 to A3
  // Note: This doesn't work with TCC,
  // so we load again below
  register long r0 asm("a0") = (long)(nbr);
  register long r1 asm("a1") = (long)(parm1);
  register long r2 asm("a2") = (long)(parm2);
  register long r3 asm("a3") = (long)(parm3);

  // Execute ECALL for System Call
  // to NuttX Kernel. Again: Load the
  // Parameters into Registers A0 to A3
  asm volatile (

    // Load 61 to Register A0 (SYS_write)
    "addi a0, zero, 61 \n"
    
    // Load 1 to Register A1 (File Descriptor)
    "addi a1, zero, 1 \n"
    
    // Load 0xc0101000 to Register A2 (Buffer)
    "lui   a2, 0xc0 \n"
    "addiw a2, a2, 257 \n"
    "slli  a2, a2, 0xc \n"
    
    // Load 15 to Register A3 (Buffer Length)
    "addi a3, zero, 15 \n"
    
    // ECALL for System Call to NuttX Kernel
    "ecall \n"
    
    // NuttX needs NOP after ECALL
    ".word 0x0001 \n"

    // Input+Output Registers: None
    // Input-Only Registers: A0 to A3
    // Clobbers the Memory
    :
    : "r"(r0), "r"(r1), "r"(r2), "r"(r3)
    : "memory"
  );

  // Loop Forever
  for(;;) {}
  return 0;
}
</code></pre></div>
<p>We copy this into our Web Browser and compile it. (Pic above)</p>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#appendix-nuttx-system-call">(Why so complicated? <strong>Explained here</strong>)</a></p>
<p><a href="https://lupyuen.codeberg.page/articles/app#nuttx-kernel-handles-system-call">(Caution: <strong>SYS_write 61</strong> may change)</a></p>
<p><em>Does it work?</em></p>
<p>TCC in WebAssembly compiles the code above to <strong>RISC-V ELF <code>a.out</code></strong>. When we copy it to NuttX and run it…</p>
<div><pre><code>NuttShell (NSH) NuttX-12.4.0-RC0
nsh&gt; a.out
...
## NuttX System Call for SYS_write (61)
riscv_swint:
  cmd: 61
  A0:  3d  ## SYS_write (61)
  A1:  01  ## File Descriptor (Standard Output)
  A2:  c0101000  ## Buffer
  A3:  0f        ## Buffer Length
...
## NuttX Kernel says hello
Hello, World!!
</code></pre></div>
<p>NuttX Kernel prints <strong>“Hello World”</strong> yay!</p>
<p>Indeed we’ve created a C Compiler in a Web Browser, that <strong>produces proper NuttX Apps</strong>!</p>
<p><em>OK so we can build NuttX Apps in a Web Browser… But can we run them in a Web Browser?</em></p>
<p>Yep, a NuttX App built in the Web Browser… Now runs OK with <strong>NuttX Emulator in the Web Browser</strong>! 🎉 (Pic below)</p>
<ul>
<li>
<p><a href="https://youtu.be/DJMDYq52Iv8">Watch the <strong>Demo on YouTube</strong></a></p>
</li>
<li>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#nuttx-app-runs-in-a-web-browser">Find out <strong>How It Works</strong></a></p>
</li>
</ul>
<p><strong>TLDR:</strong> We called <a href="https://github.com/lupyuen/tcc-riscv32-wasm#nuttx-app-runs-in-a-web-browser"><strong>JavaScript Local Storage</strong></a>
to copy the RISC-V ELF <code>a.out</code> from TCC WebAssembly to NuttX Emulator… Then we patched <code>a.out</code> into the <a href="https://github.com/lupyuen/tcc-riscv32-wasm#nuttx-app-runs-in-a-web-browser"><strong>ROM FS Filesystem</strong></a> for NuttX Emulator. Nifty!</p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-emu2.png" alt="NuttX App built in a Web Browser… Runs inside the Web Browser!"></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#nuttx-app-runs-in-a-web-browser"><em>NuttX App built in a Web Browser… Runs inside the Web Browser!</em></a></p>
<h2 id="whats-next"><a href="#whats-next">8 What’s Next</a></h2>
<p>Thanks to the <a href="https://github.com/sellicott/tcc-riscv32"><strong>TCC Team</strong></a>, we have a <strong>64-bit RISC-V Compiler</strong> that runs in the Web Browser…</p>
<ul>
<li>
<p><strong>Zig Compiler</strong> compiles TCC to WebAssembly with one tiny fix</p>
</li>
<li>
<p>But <strong>POSIX Functions</strong> are missing in WebAssembly</p>
</li>
<li>
<p>So we did the bare minimum for <strong>File Input and Output</strong> </p>
</li>
<li>
<p>And cooked up the simplest workaround for <strong>fprintf and friends</strong></p>
</li>
<li>
<p>Finally TCC produces a <strong>RISC-V Binary</strong> that runs OK on Apache NuttX RTOS</p>
</li>
<li>
<p>Now we can <strong>Build and Test NuttX Apps</strong> all within a Web Browser!</p>
</li>
</ul>
<p>How will you use <strong>TCC in a Web Browser</strong>? Please lemme know 🙏</p>
<p><em>(Build and run RISC-V Apps on iPhone?)</em></p>
<p>Many Thanks to my <a href="https://github.com/sponsors/lupyuen"><strong>GitHub Sponsors</strong></a> (and the awesome NuttX and Zig Communities) for supporting my work! This article wouldn’t have been possible without your support.</p>
<ul>
<li>
<p><a href="https://github.com/sponsors/lupyuen"><strong>Sponsor me a coffee</strong></a></p>
</li>
<li>
<p><a href="https://news.ycombinator.com/item?id=39245664"><strong>Discuss this article on Hacker News</strong></a></p>
</li>
<li>
<p><a href="https://github.com/lupyuen/nuttx-ox64"><strong>My Current Project: “Apache NuttX RTOS for Ox64 BL808”</strong></a></p>
</li>
<li>
<p><a href="https://github.com/lupyuen/nuttx-star64"><strong>My Other Project: “NuttX for Star64 JH7110”</strong></a></p>
</li>
<li>
<p><a href="https://github.com/lupyuen/pinephone-nuttx"><strong>Older Project: “NuttX for PinePhone”</strong></a></p>
</li>
<li>
<p><a href="https://lupyuen.codeberg.page/"><strong>Check out my articles</strong></a></p>
</li>
<li>
<p><a href="https://lupyuen.codeberg.page/rss.xml"><strong>RSS Feed</strong></a></p>
</li>
</ul>
<p><em>Got a question, comment or suggestion? Create an Issue or submit a Pull Request here…</em></p>
<p><a href="https://github.com/lupyuen/lupyuen.github.io/blob/master/src/tcc.md"><strong>lupyuen.github.io/src/tcc.md</strong></a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-web.png" alt="Online Demo of TCC Compiler in WebAssembly"></p>
<p><a href="https://lupyuen.github.io/tcc-riscv32-wasm/"><em>Online Demo of TCC Compiler in WebAssembly</em></a></p>
<h2 id="appendix-compile-tcc-with-zig"><a href="#appendix-compile-tcc-with-zig">9 Appendix: Compile TCC with Zig</a></h2>
<p>This is how we run <strong>Zig Compiler to compile TCC Compiler</strong> from C to WebAssembly (pic below)…</p>
<div><pre><code>## Download the (slightly) Modified TCC Source Code.
## Configure the build for 64-bit RISC-V.

git clone https://github.com/lupyuen/tcc-riscv32-wasm
cd tcc-riscv32-wasm
./configure
make cross-riscv64

## Call Zig Compiler to compile TCC Compiler
## from C to WebAssembly. Produces `tcc.o`

## Omitted: Run the `zig cc` command from earlier...
## https://lupyuen.codeberg.page/articles/tcc#zig-compiles-tcc-to-webassembly
zig cc ...

## Compile our Zig Wrapper `tcc-wasm.zig` for WebAssembly
## and link it with TCC compiled for WebAssembly `tcc.o`
## Generates `tcc-wasm.wasm`

## Omitted: Run the `zig build-exe` command from earlier...
## https://lupyuen.codeberg.page/articles/tcc#zig-compiles-tcc-to-webassembly
zig build-exe ...
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/build.sh">(See the <strong>Build Script</strong>)</a></p>
<p><em>How did we figure out the “<code>zig</code> <code>cc</code>” options?</em></p>
<p>Earlier we saw a long list of <a href="https://lupyuen.codeberg.page/articles/tcc#zig-compiles-tcc-to-webassembly"><strong>Zig Compiler Options</strong></a>…</p>
<div><pre><code>## Zig Compiler Options for TCC Compiler
zig cc \
  tcc.c \
  -DTCC_TARGET_RISCV64 \
  -DCONFIG_TCC_CROSSPREFIX="\"riscv64-\""  \
  -DCONFIG_TCC_CRTPREFIX="\"/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_LIBPATHS="\"{B}:/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_SYSINCLUDEPATHS="\"{B}/include:/usr/riscv64-linux-gnu/include\""   \
  ...
</code></pre></div>
<p>We got them from “<strong><code>make</code> <code>--trace</code></strong>”, which reveals the <strong>GCC Compiler Options</strong>…</p>
<div><pre><code>## Show the GCC Options for compiling TCC
$ make --trace cross-riscv64

gcc \
  -o riscv64-tcc.o \
  -c \
  tcc.c \
  -DTCC_TARGET_RISCV64 \
  -DCONFIG_TCC_CROSSPREFIX="\"riscv64-\""  \
  -DCONFIG_TCC_CRTPREFIX="\"/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_LIBPATHS="\"{B}:/usr/riscv64-linux-gnu/lib\"" \
  -DCONFIG_TCC_SYSINCLUDEPATHS="\"{B}/include:/usr/riscv64-linux-gnu/include\""   \
  -DTCC_GITHASH="\"main:b3d10a35\"" \
  -Wall \
  -O2 \
  -Wdeclaration-after-statement \
  -fno-strict-aliasing \
  -Wno-pointer-sign \
  -Wno-sign-compare \
  -Wno-unused-result \
  -Wno-format-truncation \
  -Wno-stringop-truncation \
  -I. 
</code></pre></div>
<p>And we copied above GCC Options to become our <a href="https://lupyuen.codeberg.page/articles/tcc#zig-compiles-tcc-to-webassembly"><strong>Zig Compiler Options</strong></a>.</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/build.sh">(See the <strong>Build Script</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-zig.jpg" alt="Zig Compiler compiles TCC Compiler to WebAssembly"></p>
<h2 id="appendix-javascript-calls-tcc"><a href="#appendix-javascript-calls-tcc">10 Appendix: JavaScript calls TCC</a></h2>
<p>Previously we saw some <strong>JavaScript (Web Browser and Node.js)</strong> calling our TCC Compiler in WebAssembly (pic above)…</p>
<ul>
<li>
<p><a href="https://lupyuen.github.io/tcc-riscv32-wasm/"><strong>TCC WebAssembly in Web Browser</strong></a></p>
</li>
<li>
<p><a href="https://lupyuen.codeberg.page/articles/tcc#zig-compiles-tcc-to-webassembly"><strong>TCC WebAssembly in Node.js</strong></a></p>
</li>
</ul>
<p>This is how we test the TCC WebAssembly in a Web Browser with a <strong>Local Web Server</strong>…</p>
<div><pre><code>## Download the (slightly) Modified TCC Source Code
git clone https://github.com/lupyuen/tcc-riscv32-wasm
cd tcc-riscv32-wasm

## Start the Web Server
cargo install simple-http-server
simple-http-server ./docs &amp;

## Whenever we rebuild TCC WebAssembly...
## Copy it to the Web Server
cp tcc-wasm.wasm docs/
</code></pre></div>
<p>Browse to this URL and our TCC WebAssembly will appear…</p>
<div><pre><code>## Test TCC WebAssembly with Web Browser
http://localhost:8000/index.html
</code></pre></div>
<p>Check the <strong>JavaScript Console</strong> for Debug Messages.</p>
<p><a href="https://gist.github.com/lupyuen/5f8191d5c63b7dba030582cbe7481572">(See the <strong>JavaScript Log</strong>)</a></p>
<p><em>How does it work?</em></p>
<p>On clicking the <strong>Compile Button</strong>, our JavaScript loads the TCC WebAssembly: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L174-L191">tcc.js</a></p>
<div><pre><code>// Load the WebAssembly Module and start the Main Function.
// Called by the Compile Button.
async function bootstrap() {

  // Load the WebAssembly Module `tcc-wasm.wasm`
  // https://developer.mozilla.org/en-US/docs/WebAssembly/JavaScript_interface/instantiateStreaming
  const result = await WebAssembly.instantiateStreaming(
    fetch("tcc-wasm.wasm"),
    importObject
  );

  // Store references to WebAssembly Functions
  // and Memory exported by Zig
  wasm.init(result);

  // Start the Main Function
  window.requestAnimationFrame(main);
}        
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L25-L48">(<strong>importObject</strong> exports our <strong>JavaScript Logger</strong> to Zig)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L6-L25">(<strong>wasm</strong> is our <strong>WebAssembly Helper</strong>)</a></p>
<p>Which triggers the <strong>Main Function</strong> and calls our Zig Function <strong>compile_program</strong>: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L48-L90">tcc.js</a></p>
<div><pre><code>// Main Function
function main() {
  // Allocate a String for passing the Compiler Options to Zig
  // `options` is a JSON Array: ["-c", "hello.c"]
  const options = read_options();
  const options_ptr = allocateString(JSON.stringify(options));
  
  // Allocate a String for passing the Program Code to Zig
  const code = document.getElementById("code").value;
  const code_ptr = allocateString(code);

  // Call TCC to compile the program
  const ptr = wasm.instance.exports
    .compile_program(options_ptr, code_ptr);

  // Get the `a.out` size from first 4 bytes returned
  const memory = wasm.instance.exports.memory;
  const data_len = new Uint8Array(memory.buffer, ptr, 4);
  const len = data_len[0] | data_len[1] &lt;&lt; 8 | data_len[2] &lt;&lt; 16 | data_len[3] &lt;&lt; 24;
  if (len &lt;= 0) { return; }

  // Encode the `a.out` data from the rest of the bytes returned
  // `encoded_data` looks like %7f%45%4c%46...
  const data = new Uint8Array(memory.buffer, ptr + 4, len);
  let encoded_data = "";
  for (const i in data) {
    const hex = Number(data[i]).toString(16).padStart(2, "0");
    encoded_data += `%${hex}`;
  }

  // Download the `a.out` data into the Web Browser
  download("a.out", encoded_data);

  // Save the ELF Data to Local Storage for loading by NuttX Emulator
  localStorage.setItem("elf_data", encoded_data);
};
</code></pre></div>
<p>Our Main Function then downloads the <strong><code>a.out</code></strong> file returned by our Zig Function.</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L90-L112">(<strong>allocateString</strong> allocates a String from Zig Memory)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/docs/tcc.js#L162-L174">(<strong>download</strong> is here)</a></p>
<p><em>What about Node.js calling TCC WebAssembly?</em></p>
<div><pre><code>## Test TCC WebAssembly with Node.js
node zig/test.js
</code></pre></div>
<p><strong>For Easier Testing</strong> (via Command-Line): We copied the JavaScript above into a Node.js Script: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/test.js#L46-L78">test.js</a></p>
<div><pre><code>// Allocate a String for passing the Compiler Options to Zig
const options = ["-c", "hello.c"];
const options_ptr = allocateString(JSON.stringify(options));

// Allocate a String for passing Program Code to Zig
const code_ptr = allocateString(`
  int main(int argc, char *argv[]) {
    printf("Hello, World!!\\n");
    return 0;
  }
`);

// Call TCC to compile a program
const ptr = wasm.instance.exports
  .compile_program(options_ptr, code_ptr);
</code></pre></div>
<p><a href="https://gist.github.com/lupyuen/795327506cad9b1ee82206e614c399cd">(See the <strong>Node.js Log</strong>)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/test-nuttx.js">(Test Script for NuttX QEMU: <strong>test-nuttx.js</strong>)</a></p>
<p><a href="https://gist.github.com/lupyuen/55a4d4cae26994aa673e6d8451716b27">(Test Log for NuttX QEMU: <strong>test-nuttx.log</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-format.jpg" alt="Our Zig Wrapper doing Pattern Matching for Formatting C Strings"></p>
<h2 id="appendix-pattern-matching"><a href="#appendix-pattern-matching">11 Appendix: Pattern Matching</a></h2>
<p>A while back we saw our Zig Wrapper doing <strong>Pattern Matching</strong> for Formatting C Strings…</p>
<ul>
<li><a href="https://lupyuen.codeberg.page/articles/tcc#fearsome-fprintf-and-friends"><strong>“Fearsome fprintf and Friends”</strong></a></li>
</ul>
<p>How It Works: We search for <strong>Format Patterns</strong> in the C Format Strings and substitute the <strong>Zig Equivalent</strong> (pic above): <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L189-L207">tcc-wasm.zig</a></p>
<div><pre><code>// Format a Single `%d`
// like `#define __TINYC__ %d`
FormatPattern{

  // If the C Format String contains this...
  .c_spec = "%d",
  
  // Then we apply this Zig Format...
  .zig_spec = "{}",
  
  // And extract these Argument Types
  // from the Varargs...
  .type0 = c_int,
  .type1 = null
}
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L438-L446">(<strong>FormatPattern</strong> is defined here)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L191-L209">(See the <strong>Format Patterns</strong>)</a></p>
<p>To implement this, we call <strong>comptime Functions</strong> in Zig: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L276-L327">tcc-wasm.zig</a></p>
<div><pre><code>/// CompTime Function to format a string by Pattern Matching.
/// Format a Single Specifier, like `#define __TINYC__ %d\n`
/// If the Spec matches the Format: Return the number of bytes written to `str`, excluding terminating null.
/// Else return 0.
fn format_string1(
  ap: *std.builtin.VaList,  // Varargs passed from C
  str:    [*]u8,            // Buffer for returning Formatted String
  size:   size_t,           // Buffer Size
  format: []const u8,       // C Format String, like `#define __TINYC__ %d\n`
  comptime c_spec:   []const u8,  // C Format Pattern, like `%d`
  comptime zig_spec: []const u8,  // Zig Equivalent, like `{}`
  comptime T0:       type,        // Type of First Vararg, like `c_int`
) usize {  // Return the number of bytes written to `str`, excluding terminating null

  // Count the Format Specifiers: `%`
  const spec_cnt   = std.mem.count(u8, c_spec, "%");
  const format_cnt = std.mem.count(u8, format, "%");

  // Check the Format Specifiers: `%`
  // Quit if the number of specifiers are different
  // Or if the specifiers are not found
  if (format_cnt != spec_cnt or
      !std.mem.containsAtLeast(u8, format, 1, c_spec)) {
    return 0;
  }

  // Fetch the First Argument from the C Varargs
  const a = @cVaArg(ap, T0);

  // Format the Argument
  var buf: [512]u8 = undefined;
  const buf_slice = std.fmt.bufPrint(&amp;buf, zig_spec, .{a}) catch {
    @panic("format_string1 error: buf too small");
  };

  // Replace the C Format Pattern by the Zig Equivalent
  var buf2 = std.mem.zeroes([512]u8);
  _ = std.mem.replace(u8, format, c_spec, buf_slice, &amp;buf2);

  // Return the Formatted String and Length
  const len = std.mem.indexOfScalar(u8, &amp;buf2, 0).?;
  assert(len &lt; size);
  @memcpy(str[0..len], buf2[0..len]);
  str[len] = 0;
  return len;
}

// Omitted: Function `format_string2` looks similar,
// but for 2 Varargs (instead of 1)
</code></pre></div>
<p>The function above is called by a <strong>comptime Inline Loop</strong> that applies all the <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L191-L209"><strong>Format Patterns</strong></a> that we saw earlier: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L207-L251">tcc-wasm.zig</a></p>
<div><pre><code>/// Runtime Function to format a string by Pattern Matching.
/// Return the number of bytes written to `str`, excluding terminating null.
fn format_string(
  ap: *std.builtin.VaList,  // Varargs passed from C
  str:    [*]u8,            // Buffer for returning Formatted String
  size:   size_t,           // Buffer Size
  format: []const u8,       // C Format String, like `#define __TINYC__ %d\n`
) usize {  // Return the number of bytes written to `str`, excluding terminating null

  // If no Format Specifiers: Return the Format, like `warning: `
  const len = format_string0(str, size, format);
  if (len &gt; 0) { return len; }

  // For every Format Pattern...
  inline for (format_patterns) |pattern| {

    // Try formatting the string with the pattern...
    const len2 =
      if (pattern.type1) |t1|
      // Pattern has 2 parameters
      format_string2(ap, str, size, format, // Output String and Format String
        pattern.c_spec, pattern.zig_spec,   // Format Specifiers for C and Zig
        pattern.type0, t1 // Types of the Parameters
      )
    else
      // Pattern has 1 parameter
      format_string1(ap, str, size, format, // Output String and Format String
        pattern.c_spec, pattern.zig_spec,   // Format Specifiers for C and Zig
        pattern.type0 // Type of the Parameter
      );

    // Loop until we find a match pattern
    if (len2 &gt; 0) { return len2; }
  }

  // Format String doesn't match any Format Pattern.
  // We return the Format String and Length.
  const len3 = format.len;
  assert(len3 &lt; size);
  @memcpy(str[0..len3], format[0..len3]);
  str[len3] = 0;
  return len3;
}
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L327-L382">(<strong>format_string2</strong> is here)</a></p>
<p>And the above function is called by <strong>fprintf and friends</strong>: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L382-L438">tcc-wasm.zig</a></p>
<div><pre><code>/// Implement the POSIX Function `fprintf`
export fn fprintf(stream: *FILE, format: [*:0]const u8, ...) c_int {

  // Prepare the varargs
  var ap = @cVaStart();
  defer @cVaEnd(&amp;ap);

  // Format the string
  var buf = std.mem.zeroes([512]u8);
  const format_slice = std.mem.span(format);
  const len = format_string(&amp;ap, &amp;buf, buf.len, format_slice);

  // TODO: Print to other File Streams.
  // Right now we assume it's stderr (File Descriptor 2)
  return @intCast(len);
}

// Do the same for sprintf, snprintf, vsnprintf
</code></pre></div>
<p><a href="https://gist.github.com/lupyuen/3e650bd6ad72b2e8ee8596858bc94f36">(See the <strong>Formatting Log</strong>)</a></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#fix-the-varargs-functions">(Without <strong>comptime</strong>: Our code gets <strong>super tedious</strong>)</a></p>
<p><img src="https://lupyuen.codeberg.page/images/app-syscall.jpg" alt="NuttX Apps make a System Call to print to the console"></p>
<h2 id="appendix-nuttx-system-call"><a href="#appendix-nuttx-system-call">12 Appendix: NuttX System Call</a></h2>
<p>Just now we saw a huge chunk of C Code that makes a <strong>NuttX System Call</strong>…</p>
<ul>
<li><a href="https://lupyuen.codeberg.page/articles/tcc#hello-nuttx"><strong>“Hello NuttX!”</strong></a></li>
</ul>
<p><em>Why so complicated?</em></p>
<p>We refer to the Sample Code for <a href="https://lupyuen.codeberg.page/articles/app#nuttx-app-calls-nuttx-kernel"><strong>NuttX System Calls (ECALL)</strong></a>. Rightfully this <strong>shorter version</strong> should work…</p>
<div><pre><code>// Make NuttX System Call to write(fd, buf, buflen)
const unsigned int nbr = 61; // SYS_write
const void *parm1 = 1;       // File Descriptor (stdout)
const void *parm2 = "Hello, World!!\n"; // Buffer
const void *parm3 = 15; // Buffer Length

// Execute ECALL for System Call to NuttX Kernel
register long r0 asm("a0") = (long)(nbr);
register long r1 asm("a1") = (long)(parm1);
register long r2 asm("a2") = (long)(parm2);
register long r3 asm("a3") = (long)(parm3);

asm volatile (
  // ECALL for System Call to NuttX Kernel
  "ecall \n"

  // NuttX needs NOP after ECALL
  ".word 0x0001 \n"

  // Input+Output Registers: None
  // Input-Only Registers: A0 to A3
  // Clobbers the Memory
  :
  : "r"(r0), "r"(r1), "r"(r2), "r"(r3)
  : "memory"
);
</code></pre></div>
<p>Strangely TCC generates <a href="https://github.com/lupyuen/tcc-riscv32-wasm#ecall-for-nuttx-system-call"><strong>mysterious RISC-V Machine Code</strong></a> that mashes up the RISC-V Registers…</p>
<div><pre><code>main():
// Prepare the Stack
   0:  fc010113  add     sp, sp, -64
   4:  02113c23  sd      ra, 56(sp)
   8:  02813823  sd      s0, 48(sp)
   c:  04010413  add     s0, sp, 64
  10:  00000013  nop
  14:  fea43423  sd      a0, -24(s0)
  18:  feb43023  sd      a1, -32(s0)

// Correct: Load Register A0 with 61 (SYS_write)
  1c:  03d0051b  addw    a0, zero, 61
  20:  fca43c23  sd      a0, -40(s0)

// Nope: Load Register A0 with 1?
// Mixed up with Register A1! (Value 1)
  24:  0010051b  addw    a0, zero, 1
  28:  fca43823  sd      a0, -48(s0)

// Nope: Load Register A0 with "Hello World"?
// Mixed up with Register A2!
  2c:  00000517  auipc   a0,0x0  2c: R_RISCV_PCREL_HI20  L.0
  30:  00050513  mv      a0,a0   30: R_RISCV_PCREL_LO12_I        .text
  34:  fca43423  sd      a0, -56(s0)

// Nope: Load Register A0 with 15?
// Mixed up with Register A3! (Value 15)
  38:  00f0051b  addw    a0, zero, 15
  3c:  fca43023  sd      a0, -64(s0)

// Execute ECALL with Register A0 set to 15.
// Nope: A0 should be 61!
  40:  00000073  ecall
  44:  0001      nop
</code></pre></div>
<p>Thus we <a href="https://github.com/lupyuen/tcc-riscv32-wasm#ecall-for-nuttx-system-call"><strong>hardcode Registers A0 to A3</strong></a> in RISC-V Assembly: <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/test-nuttx.js#L55-L97">test-nuttx.js</a></p>
<div><pre><code>// Load 61 to Register A0 (SYS_write)
addi  a0, zero, 61

// Load 1 to Register A1 (File Descriptor)
addi  a1, zero, 1

// Load 0xc0101000 to Register A2 (Buffer)
lui   a2, 0xc0
addiw a2, a2, 257
slli  a2, a2, 0xc

// Load 15 to Register A3 (Buffer Length)
addi  a3, zero, 15

// ECALL for System Call to NuttX Kernel
ecall

// NuttX needs NOP after ECALL
.word 0x0001
</code></pre></div>
<p>And it prints “Hello World”!</p>
<p><strong>TODO:</strong> Is there a workaround? Do we paste the ECALL Assembly Code ourselves? <a href="https://github.com/lupyuen/tcc-riscv32-wasm#fix-missing-printf-in-nuttx-app"><strong>NuttX Libraries</strong></a> won’t link with TCC</p>
<p><a href="https://gist.github.com/lupyuen/55a4d4cae26994aa673e6d8451716b27">(See the <strong>TCC WebAssembly Log</strong>)</a></p>
<p><em>What’s with the <code>addi</code> and <code>nop</code>?</em></p>
<p>TCC won’t assemble the “<strong><code>li</code></strong>” and “<strong><code>nop</code></strong>” instructions.</p>
<p>So we used this <a href="https://riscvasm.lucasteske.dev/#"><strong>RISC-V Online Assembler</strong></a> to assemble the code above.</p>
<p>“<strong><code>addi</code></strong>” above is the longer form of “<strong><code>li</code></strong>”, which TCC won’t assemble…</p>
<div><pre><code>// Load 61 to Register A0 (SYS_write)
// But TCC won't assemble `li a0, 61`
// So we do this...

// Add 0 to 61 and save to Register A0
addi a0, zero, 61
</code></pre></div>
<p>“<strong><code>lui / addiw / slli</code></strong>” above is our expansion of “<strong><code>li a2, 0xc0101000</code></strong>”, which TCC won’t assemble…</p>
<div><pre><code>// Load 0xC010_1000 to Register A2 (Buffer)
// But TCC won't assemble `li a2, 0xc0101000`
// So we do this...

// Load 0xC0 &lt;&lt; 12 into Register A2 (0xC0000)
lui   a2, 0xc0

// Add 257 to Register A2 (0xC0101)
addiw a2, a2, 257

// Shift Left by 12 Bits (0xC010_1000)
slli  a2, a2, 0xc
</code></pre></div>
<p><em>How did we figure out that the buffer is at 0xC010_1000?</em></p>
<p>We saw this in our <a href="https://gist.github.com/lupyuen/a715e4e77c011d610d0b418e97f8bf5d#file-nuttx-tcc-app-log-L32-L42"><strong>ELF Loader Log</strong></a>…</p>
<div><pre><code>NuttShell (NSH) NuttX-12.4.0
nsh&gt; a.out
...
Read 576 bytes from offset 512
Read 154 bytes from offset 64
1. 00000000-&gt;c0000000
Read 0 bytes from offset 224
2. 00000000-&gt;c0101000
Read 16 bytes from offset 224
3. 00000000-&gt;c0101000
4. 00000000-&gt;c0101010
</code></pre></div>
<p>Which says that the NuttX ELF Loader copied 16 bytes from our NuttX App Data Section (<strong><code>.data.ro</code></strong>) to <strong><code>0xC010_1000</code></strong>.</p>
<p>That’s all 15 bytes of <em>“Hello, World!!\n”</em>, including the terminating null.</p>
<p>Thus our buffer in NuttX QEMU should be at <strong><code>0xC010_1000</code></strong>.</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#nuttx-app-runs-in-a-web-browser">(<strong>NuttX WebAssembly Emulator</strong> uses <strong><code>0x8010_1000</code></strong> instead)</a></p>
<p><a href="https://lupyuen.codeberg.page/articles/app#kernel-starts-a-nuttx-app">(More about the <strong>NuttX ELF Loader</strong>)</a></p>
<p><em>Why do we Loop Forever?</em></p>
<div><pre><code>// Omitted: Execute ECALL for System Call to NuttX Kernel
asm volatile ( ... );

// Loop Forever
for(;;) {}
</code></pre></div>
<p>That’s because NuttX Apps are not supposed to <a href="https://github.com/lupyuen/tcc-riscv32-wasm#fix-missing-printf-in-nuttx-app"><strong>Return to NuttX Kernel</strong></a>.</p>
<p>We should call the NuttX System Call <strong><code>__exit</code></strong> to terminate peacefully.</p>
<p><img src="https://lupyuen.codeberg.page/images/tcc-demo.png" alt="Online Demo of Apache NuttX RTOS"></p>
<p><a href="https://nuttx.apache.org/demo/"><em>Online Demo of Apache NuttX RTOS</em></a></p>
<h2 id="appendix-build-nuttx-for-qemu"><a href="#appendix-build-nuttx-for-qemu">13 Appendix: Build NuttX for QEMU</a></h2>
<p>Here are the steps to build and run <strong>NuttX for QEMU 64-bit RISC-V</strong> (Kernel Mode)</p>
<ol>
<li>
<p>Install the Build Prerequisites, skip the RISC-V Toolchain…</p>
<p><a href="https://lupyuen.codeberg.page/articles/nuttx#install-prerequisites"><strong>“Install Prerequisites”</strong></a></p>
</li>
<li>
<p>Download the RISC-V Toolchain for <strong>riscv64-unknown-elf</strong>…</p>
<p><a href="https://lupyuen.codeberg.page/articles/riscv#appendix-download-toolchain-for-64-bit-risc-v"><strong>“Download Toolchain for 64-bit RISC-V”</strong></a></p>
</li>
<li>
<p>Download and configure NuttX…</p>
<div><pre><code>## Download NuttX Source Code
mkdir nuttx
cd nuttx
git clone https://github.com/apache/nuttx nuttx
git clone https://github.com/apache/nuttx-apps apps

## Configure NuttX for QEMU RISC-V 64-bit (Kernel Mode)
cd nuttx
tools/configure.sh rv-virt:knsh64
make menuconfig
</code></pre></div>
<p>We use <a href="https://lupyuen.codeberg.page/articles/semihost#nuttx-apps-filesystem"><strong>Kernel Mode</strong></a> because it allows loading of NuttX Apps as ELF Files.</p>
<p>(Instead of Statically Linking the NuttX Apps into NuttX Kernel)</p>
</li>
<li>
<p>(Optional) To enable <strong>ELF Loader Logging</strong>, select…</p>
<p>Build Setup &gt; Debug Options &gt; Binary Loader Debug Features:</p>
<ul>
<li>Enable “Binary Loader Error, Warnings and Info”</li>
</ul>
</li>
<li>
<p>(Optional) To enable <strong>System Call Logging</strong>, select…</p>
<p>Build Setup &gt; Debug Options &gt; SYSCALL  Debug Features:</p>
<ul>
<li>Enable “SYSCALL Error, Warnings and Info”</li>
</ul>
</li>
<li>
<p>Save and exit <strong>menuconfig</strong>.</p>
</li>
<li>
<p>Build the <strong>NuttX Kernel and NuttX Apps</strong>…</p>
<div><pre><code>## Build NuttX Kernel
make -j 8

## Build NuttX Apps
make -j 8 export
pushd ../apps
./tools/mkimport.sh -z -x ../nuttx/nuttx-export-*.tar.gz
make -j 8 import
popd
</code></pre></div></li>
</ol>
<p>This produces the NuttX ELF Image <strong><code>nuttx</code></strong> that we may boot on QEMU RISC-V Emulator…</p>
<div><pre><code>## For macOS: Install QEMU
brew install qemu

## For Debian and Ubuntu: Install QEMU
sudo apt install qemu-system-riscv64

## Boot NuttX on QEMU 64-bit RISC-V
qemu-system-riscv64 \
  -semihosting \
  -M virt,aclint=on \
  -cpu rv64 \
  -smp 8 \
  -bios none \
  -kernel nuttx \
  -nographic
</code></pre></div>
<p>NuttX Apps are located in <strong><code>apps/bin</code></strong>.</p>
<p>We may copy our <strong>RISC-V ELF <code>a.out</code></strong> to that folder and run it…</p>
<div><pre><code>NuttShell (NSH) NuttX-12.4.0-RC0
nsh&gt; a.out
Hello, World!!
</code></pre></div>
<p><img src="https://lupyuen.codeberg.page/images/tcc-posix.jpg" alt="POSIX Functions aren’t supported for TCC in WebAssembly"></p>
<h2 id="appendix-missing-functions"><a href="#appendix-missing-functions">14 Appendix: Missing Functions</a></h2>
<p>Remember we said that POSIX Functions aren’t supported in WebAssembly? (Pic above)</p>
<ul>
<li><a href="https://lupyuen.codeberg.page/articles/tcc#posix-for-webassembly"><strong>“POSIX for WebAssembly”</strong></a></li>
</ul>
<p>We dump the <strong>Compiled WebAssembly</strong> of TCC Compiler, and we discover that it calls <strong>72 POSIX Functions</strong>…</p>
<div><pre><code>## Dump the Compiled WebAssembly
## for TCC Compiler `tcc.o`
$ sudo apt install wabt
$ wasm-objdump -x tcc.o

Import:
 - func[0] sig=1  &lt;env.strcmp&gt; &lt;- env.strcmp
 - func[1] sig=12 &lt;env.memset&gt; &lt;- env.memset
 - func[2] sig=1  &lt;env.getcwd&gt; &lt;- env.getcwd
 ...
 - func[69] sig=2  &lt;env.localtime&gt; &lt;- env.localtime
 - func[70] sig=13 &lt;env.qsort&gt;     &lt;- env.qsort
 - func[71] sig=19 &lt;env.strtoll&gt;   &lt;- env.strtoll
</code></pre></div>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#missing-functions-in-tcc-webassembly">(See the <strong>Complete List</strong>)</a></p>
<p>Do we need all 72 POSIX Functions? We scrutinise the list…</p>
<hr>
<p><strong>Filesystem Functions</strong></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L87-L166"><em>(Implemented here)</em></a></p>
<p>We’ll simulate these functions for WebAssembly, by embedding the simple <a href="https://github.com/lupyuen/tcc-riscv32-wasm#rom-fs-filesystem-for-tcc-webassembly"><strong>ROM FS Filesystem</strong></a> into our Zig Wrapper…</p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm#rom-fs-filesystem-for-tcc-webassembly">(See the updates for <strong>ROM FS Filesystem</strong>)</a></p>
<hr>
<p><strong>Varargs Functions</strong></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L186-L445"><em>(Implemented here)</em></a></p>
<p>As discussed earlier, Varargs will be <a href="https://lupyuen.codeberg.page/articles/tcc#fearsome-fprintf-and-friends"><strong>tricky to implement</strong></a> in Zig. Probably we should do it in C.</p>
<p><a href="https://github.com/marler8997/ziglibc/blob/main/src/printf.c#L32-L191">(Similar to <strong>ziglibc</strong>)</a></p>
<p>Right now we’re doing simple <a href="https://lupyuen.codeberg.page/articles/tcc#appendix-pattern-matching"><strong>Pattern Matching</strong></a>. But it might not be sufficient when TCC compiles Real Programs…</p>
<hr>
<p><strong>String Functions</strong></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L541-L776"><em>(Implemented here)</em></a></p>
<p>We’ll borrow the String Functions from <a href="https://github.com/marler8997/ziglibc/blob/main/src/cstd.zig"><strong>ziglibc</strong></a>…</p>
<hr>
<p><strong>Semaphore Functions</strong></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L166-L186"><em>(Implemented here)</em></a></p>
<p>Not sure why TCC uses Semaphores? Maybe we’ll understand when we support <strong><code>#include</code></strong> files.</p>
<p>(Where can we borrow the Semaphore Functions?)</p>
<hr>
<p><strong>Standard Library</strong></p>
<p><strong>qsort</strong> isn’t used right now. Maybe for the Linker later?</p>
<p>(Borrow <strong>qsort</strong> from where? We can probably implement <strong>exit</strong>)</p>
<hr>
<p><strong>Time and Math Functions</strong></p>
<p>Not used right now, maybe later.</p>
<p>(Anyone can lend us <strong>ldexp</strong>? How will we do the Time Functions? Call out to JavaScript to <a href="https://lupyuen.codeberg.page/articles/lvgl4#appendix-handle-lvgl-timer"><strong>fetch the time</strong></a>?)</p>
<hr>
<p><strong>Outstanding Functions</strong></p>
<p><a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L776-L855"><em>(Implemented here)</em></a></p>
<p>We have implemented (fully or partially) <strong>48 POSIX Functions</strong> from above.</p>
<p>The ones that we haven’t implemented? These <a href="https://github.com/lupyuen/tcc-riscv32-wasm/blob/main/zig/tcc-wasm.zig#L776-L855"><strong>24 POSIX Functions will Halt</strong></a> when TCC WebAssembly calls them…</p>

    


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Researchers taking on fraudulent science (117 pts)]]></title>
            <link>https://www.analystnews.org/posts/plagiarism-paper-mills-and-profit-these-scientists-are-fighting-the-epidemic-of-fraudulent-science-research</link>
            <guid>39244601</guid>
            <pubDate>Sat, 03 Feb 2024 20:49:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.analystnews.org/posts/plagiarism-paper-mills-and-profit-these-scientists-are-fighting-the-epidemic-of-fraudulent-science-research">https://www.analystnews.org/posts/plagiarism-paper-mills-and-profit-these-scientists-are-fighting-the-epidemic-of-fraudulent-science-research</a>, See on <a href="https://news.ycombinator.com/item?id=39244601">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="tps_slideContainer_15454">

<p>Nearly two decades ago, British anesthesiologist John Carlisle published an <a href="https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD004125.pub3/information#versionTable" target="_blank" rel="noopener">article</a> on preventing postoperative nausea in the Cochrane Database of Systematic Reviews.&nbsp;</p>



<p>Afterwards, however, Carlisle wasn’t celebrating his publication. Instead, he decided to look more closely at some of the papers included in his literature review. A staggering 68 of the 737 papers reviewed were written by a researcher named Yoshitaka Fujii.</p>



<p>“Peculiar patterns were evident in his papers,” Carlisle tells Analyst News. Rates of headaches or dizziness, for instance, were often precisely the same across groups of patients in Fujii’s clinical trials. “That makes one suspicious. If you see it in not just one paper but lots of papers, then something can’t be right.”</p>



<p>Statistically, Carlisle’s investigation showed, such results were near-impossible — and the data in Fujii’s trials was much more likely fabricated. Indeed, of the 68 papers that Fujii had authored and Carlisle had included, 63 were ultimately retracted from the journals in which they were published.</p>



<p>“We need to be looking out for poor science, whether it’s fabricated or whether it’s unintentionally false,” says Carlisle, a longtime editor for the journal <em>Anaesthesia </em>who has <a href="https://www.nature.com/articles/d41586-019-02241-z" target="_blank" rel="noopener">developed statistical techniques</a> to help identify problematic medical research. His methods have been adopted by at least two top medical journals — and have exposed scientific misconduct and errors in hundreds of papers that have been corrected or retracted.</p>



<p>Experts say a rampant culture of “publish or perish,” plus the rise of <a href="https://www.nature.com/articles/d41586-023-00191-1" target="_blank" rel="noopener">AI-based writing tools</a>, <a href="https://www.newyorker.com/tech/annals-of-technology/paging-dr-fraud-the-fake-publishers-that-are-ruining-science" target="_blank" rel="noopener">predatory science journals</a> and <a href="https://www.science.org/content/article/fake-scientific-papers-are-alarmingly-common" target="_blank" rel="noopener">paper mills</a>, has tainted scientific publication. Per some estimates, <a href="https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002165" target="_blank" rel="noopener">fewer than half</a> of research studies published each year are credible. Such misconduct wastes time and money, damages trust in science, and can even <a href="https://www.science.org/doi/10.1126/science.362.6413.394-a" target="_blank" rel="noopener">endanger patients</a>.</p>



<p>Last year, more than 10,000 research papers were <a href="https://www.nature.com/articles/d41586-023-03974-8" target="_blank" rel="noopener">retracted</a>, marking an all-time high. These retractions could have occurred for a number of reasons: The results in the paper may be considered inaccurate, the paper may have contained plagiarized work, or the authors may have conflicts of interest or used unethical research practices.&nbsp;</p>



<p>The retractions themselves are a good sign, experts say, demonstrating that inaccurate or unethical research is being caught. But scientists worry the retracted articles are only a fraction of all the fraudulent work out there. And a small but growing number of researchers have devoted themselves to investigating and exposing this bad science.</p>



<h2>Meet the science sleuths</h2>



<p>Microbiologist Elisabeth Bik was a researcher at Stanford University when she stumbled across a few cases of unethical science. First her own work had been plagiarized by another scientist, and then she came across a paper reusing the same image to represent two separate experiments.&nbsp;</p>



<p>A decade later, Bik is now a full-time “science integrity consultant,” sleuthing out fraudulent work in research papers. Her speciality: identifying falsified images.&nbsp;</p>



<p>Bik says one of the biggest sources of fraudulent research is paper mills, which she describes as “networks or cartels of people who make a profit selling fake or very low-quality papers to authors who need an authorship.” Per <a href="https://www.nature.com/articles/d41586-023-03464-x" target="_blank" rel="noopener">some</a> <a href="https://publicationethics.org/node/55256" target="_blank" rel="noopener">estimates</a>, about 2% of papers are produced by paper mills.&nbsp;</p>



<blockquote>
<p>“You just can’t work in a field like that. It’s like swimming in garbage. What you have to do is get out of the pool and start trying to clean it up.”&nbsp;</p>
</blockquote>



<p>These paper mills may plagiarize PhD theses found online or offer general templates for research articles, making modifications for customers. Some researchers even pay paper mills to help boost their citation index, which indicates how often their work has been cited by other scientists (generally, more established or well-reputed researchers have high citation indices). Paper mills can generate articles that contain multiple citations to a specific scientist’s work and thus serve as “vessels for citations,” Bik says.</p>



<p>Cancer biology professor Jennifer Byrne, the director of biobanking at New South Wales Health Pathology, began researching publication integrity and research fraud after becoming overwhelmed by the amount of bogus research in her field.&nbsp;</p>



<p>“I feel like I didn’t have any choice, because now in the field that I used to work, I would estimate that there are far more paper mill papers than general papers,” says Byrne, who now works with the Association for Interdisciplinary Meta-Research and Open Science (AIMOS), an international organization promoting trustworthy research practices.</p>



<p>The sheer volume of fraud was appalling, she says, with many repetitive papers and fatal errors.</p>



<p>“You just can’t work in a field like that. It’s like swimming in garbage. What you have to do is get out of the pool and start trying to clean it up.”&nbsp;</p>



<h2>Why fraud flourishes</h2>



<p>In a community that values scientific reasoning and evidence, it may be hard to believe that paper mills and fraudulent research can thrive. But there are financial incentives at play.&nbsp;</p>



<p>For journal editors and publishers, it’s economically advantageous to publish papers, so research is sometimes published without thorough screening. The false information in fraudulent papers can be nuanced or well-disguised, making it difficult for journals to quickly distinguish fraudulent papers from authentic ones.&nbsp;</p>



<p>Experts point to a clear incentivization of quantity, rather than quality, of research output — both with the publishing industry and academic institutions. Publishing articles is critical for success and distinction in scientists’ fields. Universities often require PhD candidates or faculty members to publish a certain number of papers.&nbsp;</p>



<p>Many researchers, however, end up finding negative results —&nbsp;those that do not support their initial hypotheses. While such studies are crucial to the scientific literature, the “whole industry” of journals encourages scientists to publish positive results, Bik explains. Facing pressure to get published but with no compelling results of their own, researchers may turn to paper mills instead.</p>



<p>“The second problem is increasing commercialization of the publication enterprise, where more journals are owned by very, very large companies who are profit-driven,” says Byrne. “Paper mills … are poised to step in when all journals and publishers really care about is that bottom line. Because they will produce for money.”</p>



<p>Part of the problem also lies in the increasing popularity of open-access journals, which do not require readers to pay subscriptions. These open-access journals have helped democratize access to scientific knowledge, experts say, but some publishers are misusing this model and publishing nearly everything they are sent.</p>



<p>“Many open-access publishers publish on a ‘pay-per-paper’ model that drives a much more commercial mindset within publishing,” Bik tells Analyst News. The end result is that journals have compromised the quality of articles for the quantity that they can publish.</p>



<h2>‘A work in progress’</h2>



<p>Experts say systemic changes are needed to reduce the number of fraudulent papers.</p>



<p>Bik says that journals should move towards publishing more negative results — in fact, understanding negative results in one’s field is critical for researchers to design future experiments of their own.&nbsp;</p>



<p>Governments and regulators, too, can work to remove ads for paper mills across social media sites, she says. And open communication about better science research and research practices — such as through conferences like the ones AIMOS organizes — can enable scientists to together uphold ethical standards in their fields.&nbsp;</p>



<p>Up until recently, it was largely individuals like Carlisle, Bik and Byrne detecting fraudulent research on their own. More recently, larger organizations are playing key roles.&nbsp;</p>



<p>The analytics company Clarivate, for example, maintains a list of reputable journals in its Web of Science platform. Periodically, the list is reviewed, and journals that fail to adhere to Clarivate’s standards are removed. Back in March 2023, about 50 journals were <a href="https://www.science.org/content/article/fast-growing-open-access-journals-stripped-coveted-impact-factors" target="_blank" rel="noopener">pulled</a> from the Web of Science, including well-known journals like the <em>International Journal of Environmental Research and Public Health</em>, which had published over 17,000 articles the year prior.</p>



<blockquote>
<p>“The reality is science is constructed by humans who are doing the best that they can. And it’s a work in progress.”</p>
</blockquote>



<p>Public trust in science has <a href="https://www.bloomberg.com/opinion/articles/2023-11-26/covid-public-health-mistakes-fueled-mistrust-in-scientists" target="_blank" rel="noopener">eroded</a> in recent years with several high-profile missteps during the COVID-19 pandemic. Still, experts emphasize the need for continuing to trust the scientific process, acknowledging the limitations of scientific publishing while understanding that the majority of scientists are honest and well-intentioned.&nbsp;</p>



<p>In her own work debunking scientific misconduct, Bik treads carefully to ensure her work doesn’t promote dangerous anti-science narratives.&nbsp;</p>



<p>“It’s a double-edged sword because on one hand, I am worried about fraud in science,” she says. “On the other hand, I also think it’s a relatively small fraction, and I do not want to give the impression that all science is fraudulent. This is, I think, the danger of what I do.”</p>



<p>These scientists’ message to the public is to examine scientific information with a critical eye. Consumers of information in any field should strive to look for credible sources, cross-reference studies and consult experts.</p>



<p>The scientific method is a self-correcting system of inquiry that aims to uncover truths about the universe, experts remind. Overcoming the errors that are inevitably introduced to the scientific body of knowledge — whether from human error, bias or misconduct — demands skepticism and scrutiny from both researchers and the public.</p>



<p>“The world that we live in at the moment doesn’t have a lot of certainty — there are a lot of scary things happening,” Byrne says.&nbsp;</p>



<p>“That can lead people to look at science and think, ‘Oh, here’s some certainty at last.’ But the reality is science is constructed by humans who are doing the best that they can. And it’s a work in progress.”</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A brief history of the U.S. trying to add backdoors into encrypted data (2016) (504 pts)]]></title>
            <link>https://www.atlasobscura.com/articles/a-brief-history-of-the-nsa-attempting-to-insert-backdoors-into-encrypted-data</link>
            <guid>39244254</guid>
            <pubDate>Sat, 03 Feb 2024 20:11:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.atlasobscura.com/articles/a-brief-history-of-the-nsa-attempting-to-insert-backdoors-into-encrypted-data">https://www.atlasobscura.com/articles/a-brief-history-of-the-nsa-attempting-to-insert-backdoors-into-encrypted-data</a>, See on <a href="https://news.ycombinator.com/item?id=39244254">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="article-body">
<p><span><img src="https://img.atlasobscura.com/cTZdkTIv1E4odFfr3bXu_WpdkaWAP1xldupsnW10WHY/rs:fill:12000:12000/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy84NTc4MGYxMTY5/MGNhMGU5M2RfU3Vw/ZXJjb21wdXRlcl9O/U0EtSUJNMzYwXzg1/LmpwZw.jpg" alt="" width="auto" data-kind="article-image" id="article-image-24797" data-src="https://img.atlasobscura.com/cTZdkTIv1E4odFfr3bXu_WpdkaWAP1xldupsnW10WHY/rs:fill:12000:12000/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy84NTc4MGYxMTY5/MGNhMGU5M2RfU3Vw/ZXJjb21wdXRlcl9O/U0EtSUJNMzYwXzg1/LmpwZw.jpg"></span></p>
<p><span>A government agent uses an NSA IBM 360/85 console in 1971 (Photo: <a href="https://commons.wikimedia.org/wiki/File:Supercomputer_NSA-IBM360_85.jpg">Wikimedia Commons/NSA</a>).</span></p>
<p><span><span>It’s been a weird week </span>for America’s most valuable company—a firm whose tech products have such consumer goodwill they got away with </span><em><span>forcing us to listen to U2</span></em><span>—who is poised to&nbsp;</span><a href="http://www.apple.com/customer-letter/"><span>go to court against its own government over its users’ right to privacy</span></a><span>. The government is invoking</span><span><a href="https://motherboard.vice.com/read/writs-and-giggles">&nbsp;an obscure law</a>&nbsp;</span><span>dating back almost to the founding of the country to force the company to comply. It’d be a pretty good movie. </span></p>
<p><span>But it’s just the most dramatic flare-up in a lengthy battle between government officials, cybersecurity experts, and the tech industry over how consumer’s technical data is protected, and whether or not the government has a right to access that information. </span></p>
<p><span>In fact, the government has actually won this fight before—secretly.&nbsp;</span></p>
<p><span>Throughout 2015, U.S. politicians and law enforcement officials such as FBI director James Comey have publicly lobbied for the insertion of cryptographic “backdoors” into software and hardware to allow law enforcement agencies to bypass authentication and access a suspect’s data surreptitiously. Cybersecurity experts have unanimously condemned the idea, pointing out that such backdoors would fundamentally undermine encryption and could exploited by criminals, among other issues. While a legal mandate or public agreement would be needed to allow evidence obtained via backdoors to be admissible in court, the NSA has long attempted—and occasionally succeeded—in placing backdoors for covert activities.</span></p>
<p><span><img src="https://img.atlasobscura.com/cFMKwNJo6am6FNH8kW3cgaKHEUINs5YsdNVNFymUifE/rs:fill:12000:12000/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy84NTc4MGYxMTY5/MGNhMGU5M2RfNjY1/NTc1OTYyNV81YTA2/MzdjZTc4X28uanBn.jpg" alt="" width="auto" data-kind="article-image" id="article-image-24799" data-src="https://img.atlasobscura.com/cFMKwNJo6am6FNH8kW3cgaKHEUINs5YsdNVNFymUifE/rs:fill:12000:12000/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy84NTc4MGYxMTY5/MGNhMGU5M2RfNjY1/NTc1OTYyNV81YTA2/MzdjZTc4X28uanBn.jpg"></span></p>
<p><span>An Enigma machine at Bletchley Park, long-rumored to be one of the first backdoored devices (Photo: <a href="https://flic.kr/p/b99vsi">Flickr/Adam Foster</a>).</span></p>
<p><span>One of the most important developments in cryptography was the Enigma machine, famously used to encode Nazi communications during World War II. For years, rumors have persisted that the NSA (then SSA) and their British counterparts in the Government Communications Headquarters collaborated with the Enigma’s manufacturer, Crypto AG, to place </span><a href="https://web.archive.org/web/20080202225034/http://www.inteldaily.com/?c=169&amp;a=4686"><span>backdoors into Enigma machines provided to certain countries</span></a><span> after World War II. Crypto AG has repeatedly denied the allegations, and in 2015 the </span><a href="http://www.bbc.com/news/uk-33676028"><span><em>BBC</em> sifted through 52,000 pages of declassified NSA documents</span></a><span> to find the truth. </span></p>
<p><span>The investigation revealed that while no backdoors were placed in the machines, there was a “gentlemen’s agreement” that Crypto AG would keep American and British intelligence appraised of “the technical specifications of different machines and which countries were buying which ones,” allowing analysts to decrypt messages much more quickly. Consider it a security “doggy-door.”</span></p>
<p><span>Next, in 1993, the NSA promoted “Clipper chips,” which were intended to protect private communications while still allowing law enforcement to access them. In 1994, researcher Matt Blaze </span><a href="http://www.crypto.com/papers/eesproto.pdf"><span>uncovered significant vulnerabilities in the “key escrow” system that allowed law enforcement access</span></a><span>, essentially making the chips useless. By 1996, </span><a href="http://arstechnica.com/information-technology/2015/12/what-the-government-shouldve-learned-about-backdoors-from-the-clipper-chip/"><span>Clipper chips were defunct</span></a><span>, as the tech industry adopted more secure, open encryption standards such as </span><a href="https://www.philzimmermann.com/EN/essays/WhyIWrotePGP.html"><span>PGP</span></a><span>.</span></p>
<p><span>In more recent years, the NSA was unequivocally caught inserting a backdoor into the Dual_EC_DRBG algorithm, a cryptographic algorithm that was supposed to generate random bit keys for encrypting data. The algorithm, developed in the early aughts, was championed by the NSA and included in NIST Special Publication 800-90, the official standard for random-number generators released in 2007. Within a matter of months, researchers discovered the backdoor, and awareness that the algorithm was insecure quickly spread, although it continued to be implemented in consumer software </span><a href="https://www.schneier.com/blog/archives/2007/12/dual_ec_drbg_ad.html"><span>Windows Vista</span></a><span>. What was really odd, as crypto expert Bruce Schneier explained i</span><a href="http://www.wired.com/2007/11/securitymatters-1115/"><span>n a 2007 essay published in </span><em><span>Wired</span></em></a><span>, was that Dual_EC_DRBG wasn’t even worth the NSA’s effort:</span></p>
<blockquote>
<p><span>It makes no sense as a trap door: It’s public, and rather obvious. It makes no sense from an engineering perspective: It’s too slow for anyone to willingly use it. And it makes no sense from a backwards-compatibility perspective: Swapping one random-number generator for another is easy.</span></p>
</blockquote>
<p><span><img src="https://img.atlasobscura.com/dsjgOOuhtF-s38VN6eBE_9rEnLJPt2z7u5H8kL9QngM/rs:fill:12000:12000/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy84NTc4MGYxMTY5/MGNhMGU5M2RfTVlL/LTc4X0NsaXBwZXJf/Y2hpcF9tYXJraW5n/cy5qcGc.jpg" alt="" width="auto" data-kind="article-image" id="article-image-24798" data-src="https://img.atlasobscura.com/dsjgOOuhtF-s38VN6eBE_9rEnLJPt2z7u5H8kL9QngM/rs:fill:12000:12000/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy84NTc4MGYxMTY5/MGNhMGU5M2RfTVlL/LTc4X0NsaXBwZXJf/Y2hpcF9tYXJraW5n/cy5qcGc.jpg"><span>A Chipper clip—one of the NSA’s unsuccessful backdoor attempts (Photo: <a href="https://commons.wikimedia.org/wiki/File:MYK-78_Clipper_chip_markings.jpg">Wikimedia Commons/Travis Goodspeed</a>).</span></span></p>
<p><span>Although the NSA’s effort puzzled crypto experts, </span><a href="http://www.wired.com/2013/09/nsa-backdoor/all/"><span>documents leaked by Edward Snowden in 2013</span></a><span> proved that the NSA did indeed build a backdoor into Dual_EC_DRBG and paid RSA, a computer security company, to include the compromised algorithm in its software.</span></p>
<p><span>These are the incidents that have been proven. There are, of course, numerous theories and insinuations that the NSA has made many more efforts along these lines—from </span><a href="http://www.cypherspace.org/adam/hacks/lotus-nsa-key.html"><span>backdoors in Lotus Notes</span></a><span> to </span><span><a href="http://www.networkworld.com/article/2458706/microsoft-subnet/about-those-alleged-backdoors-in-microsoft-products.html">persistent</a>&nbsp;</span><a href="https://web.archive.org/web/20000520001558/http://www.microsoft.com/security/bulletins/backdoor.asp"><span>allegations</span></a><span> that Microsoft routinely includes backdoors in its software. Additionally, the Snowden leak </span><a href="http://www.propublica.org/article/the-nsas-secret-campaign-to-crack-undermine-internet-encryption"><span>proved that the NSA is constantly working to decrypt common encryption standards</span></a><span>. </span></p>
<p><span>As our lives become more and more dominated by the digital, security experts have become </span><a href="https://www.washingtonpost.com/news/the-switch/wp/2016/01/11/the-debate-over-government-backdoors-into-encryption-isnt-just-happening-in-the-u-s/"><span>increasingly vocal</span></a><span> in their calls for truly secure encryption, and some governments have begun to listen. </span><a href="http://www.theregister.co.uk/2016/01/04/dutch_government_says_no_to_backdoors/"><span>Holland’s government</span></a><span> has agreed not to use backdoors and support open encryption standards, and despite calls to do so in response to the Paris terrorist attacks, </span><a href="http://fortune.com/2016/01/13/france-encryption/"><span>France refuses to implement a backdoor mandate</span></a><span>. Even former NSA director </span><a href="http://fortune.com/2016/02/19/hayden-apple-fbi/"><span>Michael Hayden has said that backdoors are a bad idea</span></a><span> (and he would know). As Apple vs. FBI wends its way through the courts, we are probably far from the end of this public battle. Whatever the results of this landmark case, the NSA’s classified efforts to subvert cryptography will likely continue.</span></p>
</section></div>]]></description>
        </item>
    </channel>
</rss>