<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 05 Sep 2024 13:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Building a WoW (World of Warcraft) Server in Elixir (124 pts)]]></title>
            <link>https://pikdum.dev/posts/thistle-tea/</link>
            <guid>41454741</guid>
            <pubDate>Thu, 05 Sep 2024 08:36:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pikdum.dev/posts/thistle-tea/">https://pikdum.dev/posts/thistle-tea/</a>, See on <a href="https://news.ycombinator.com/item?id=41454741">Hacker News</a></p>
Couldn't get https://pikdum.dev/posts/thistle-tea/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Desed: Demystify and debug your sed scripts (103 pts)]]></title>
            <link>https://github.com/SoptikHa2/desed</link>
            <guid>41453557</guid>
            <pubDate>Thu, 05 Sep 2024 04:46:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/SoptikHa2/desed">https://github.com/SoptikHa2/desed</a>, See on <a href="https://news.ycombinator.com/item?id=41453557">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Desed</h2><a id="user-content-desed" aria-label="Permalink: Desed" href="#desed"></a></p>
<p dir="auto">Demystify and debug your sed scripts, from comfort of your terminal.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/SoptikHa2/desed/blob/master/img/desed.gif"><img src="https://github.com/SoptikHa2/desed/raw/master/img/desed.gif" alt="desed usage example" data-animated-image=""></a></p>
<p dir="auto">Desed is a command line tool with beautiful TUI that provides users with comfortable interface and practical debugger, used to step through complex sed scripts.</p>
<p dir="auto">Some of the notable features include:</p>
<ul dir="auto">
<li>Preview variable values, both of them!</li>
<li>See how will a substitute command affect pattern space before it runs</li>
<li>Step through sed script - both forward and backwards!</li>
<li>Place breakpoints and examine program state</li>
<li>Hot reload and see what changes as you edit source code</li>
<li>Its name is a palindrome</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Alpine Linux</h3><a id="user-content-alpine-linux" aria-label="Permalink: Alpine Linux" href="#alpine-linux"></a></p>
<p dir="auto"><code>aports/testing/desed</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Arch Linux</h3><a id="user-content-arch-linux" aria-label="Permalink: Arch Linux" href="#arch-linux"></a></p>
<p dir="auto">Via AUR: <a href="https://aur.archlinux.org/packages/desed-git/" rel="nofollow">desed-git</a> or <a href="https://aur.archlinux.org/packages/desed/" rel="nofollow">desed</a> as stable version.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">DragonFly BSD</h3><a id="user-content-dragonfly-bsd" aria-label="Permalink: DragonFly BSD" href="#dragonfly-bsd"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Fedora</h3><a id="user-content-fedora" aria-label="Permalink: Fedora" href="#fedora"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">FreeBSD</h3><a id="user-content-freebsd" aria-label="Permalink: FreeBSD" href="#freebsd"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Void Linux</h3><a id="user-content-void-linux" aria-label="Permalink: Void Linux" href="#void-linux"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Source</h3><a id="user-content-source" aria-label="Permalink: Source" href="#source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/soptikha2/desed
cd desed
cargo install --path .
cp &quot;desed.1&quot; &quot;$(manpath | cut -d':' -f1)/man1&quot;"><pre>git clone https://github.com/soptikha2/desed
<span>cd</span> desed
cargo install --path <span>.</span>
cp <span><span>"</span>desed.1<span>"</span></span> <span><span>"</span><span><span>$(</span>manpath <span>|</span> cut -d<span><span>'</span>:<span>'</span></span> -f1<span>)</span></span>/man1<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Cargo</h3><a id="user-content-cargo" aria-label="Permalink: Cargo" href="#cargo"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Precompiled binaries</h3><a id="user-content-precompiled-binaries" aria-label="Permalink: Precompiled binaries" href="#precompiled-binaries"></a></p>
<p dir="auto">See <a href="https://github.com/SoptikHa2/desed/releases">releases</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dependencies:</h3><a id="user-content-dependencies" aria-label="Permalink: Dependencies:" href="#dependencies"></a></p>
<p dir="auto">Development: <code>rust</code>, <code>cargo</code> (&gt;= 1.38.0)</p>
<p dir="auto">Runtime: <code>sed</code> (GNU version, &gt;= 4.6) (desed works on BSD if you installed <code>gsed</code>)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Controls</h2><a id="user-content-controls" aria-label="Permalink: Controls" href="#controls"></a></p>
<ul dir="auto">
<li>Mouse scroll to scroll through source code, click on line to toggle breakpoint</li>
<li><code>j</code>, <code>k</code>, <code>g</code>, <code>G</code>, just as in Vim. Prefixing with numbers works too.</li>
<li><code>b</code> to toggle breakpoint (prefix with number to toggle breakpoint on target line)</li>
<li><code>s</code> to step forward, <code>a</code> to step backwards</li>
<li><code>r</code> to run to next breakpoint or end of script, <code>R</code> to do the same but backwards</li>
<li><code>l</code> to instantly reload code and continue debugging in the exactly same place as before</li>
<li><code>q</code> to <a href="https://github.com/hakluke/how-to-exit-vim">quit</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How does it work?</h2><a id="user-content-how-does-it-work" aria-label="Permalink: How does it work?" href="#how-does-it-work"></a></p>
<p dir="auto">GNU sed actually provides pretty useful debugging interface, try it yourself with <code>--debug</code> flag. However the interface is not interactive and I wanted something closer to traditional debugger. <a href="https://soptik.tech/articles/building-desed-the-sed-debugger.html" rel="nofollow">I've written something here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Does it really work?</h2><a id="user-content-does-it-really-work" aria-label="Permalink: Does it really work?" href="#does-it-really-work"></a></p>
<p dir="auto">Depends. Sed actually doesn't tell me which line number is it currently executing, so I have to emulate parts of sed to guess that. Which might not be bulletproof. But it certainly worked good enough to debug tetris without issues.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why sed??</h2><a id="user-content-why-sed" aria-label="Permalink: Why sed??" href="#why-sed"></a></p>
<p dir="auto">Sed is the perfect programming language, <a href="https://tildes.net/~comp/b2k/programming_challenge_find_path_from_city_a_to_city_b_with_least_traffic_controls_inbetween#comment-2run" rel="nofollow">especially for graph problems</a>. It's plain and simple and doesn't clutter your screen with useless identifiers like <code>if</code>, <code>for</code>, <code>while</code>, or <code>int</code>. Furthermore since it doesn't have things like numbers, it's very simple to use.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">But why?</h2><a id="user-content-but-why" aria-label="Permalink: But why?" href="#but-why"></a></p>
<p dir="auto">I wanted to program in sed but it lacked good tooling up to this point, so I had to do something about it.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why?</h2><a id="user-content-why" aria-label="Permalink: Why?" href="#why"></a></p>
<p dir="auto">Because it's the standard stream editor for filtering and transforming text. And someone wrote <a href="https://github.com/uuner/sedtris">tetris</a> in it!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is the roadmap for future updates?</h2><a id="user-content-what-is-the-roadmap-for-future-updates" aria-label="Permalink: What is the roadmap for future updates?" href="#what-is-the-roadmap-for-future-updates"></a></p>
<p dir="auto">I would like to introduce syntax highlighting and add this tool to standard repositories of all major distributions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Is this a joke?</h2><a id="user-content-is-this-a-joke" aria-label="Permalink: Is this a joke?" href="#is-this-a-joke"></a></p>
<p dir="auto">I thought it was. But apparently it's actually useful for some people.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other projects</h2><a id="user-content-other-projects" aria-label="Permalink: Other projects" href="#other-projects"></a></p>
<ul dir="auto">
<li><a href="https://github.com/soptikha2/video-summarizer">video summarizer</a>, a tool and browser extensions that determines if people in video are currently talking or not, and speeds up the video accordingly. Great for long lecture videos for skipping time spent writing on a whiteboard.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kids who use ChatGPT as a study assistant do worse on tests (151 pts)]]></title>
            <link>https://hechingerreport.org/kids-chatgpt-worse-on-tests/</link>
            <guid>41453300</guid>
            <pubDate>Thu, 05 Sep 2024 03:50:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hechingerreport.org/kids-chatgpt-worse-on-tests/">https://hechingerreport.org/kids-chatgpt-worse-on-tests/</a>, See on <a href="https://news.ycombinator.com/item?id=41453300">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content">
		<main id="main">

								

				
				<div>

					<section id="block-2"><p><em>The Hechinger Report is a national nonprofit newsroom that reports on one topic: education. Sign up for our&nbsp;<a href="https://hechingerreport.org/newsletters" target="_blank" rel="noreferrer noopener">weekly newsletters</a>&nbsp;to get stories like this delivered directly to your inbox.&nbsp;Consider supporting our stories and becoming&nbsp;<a href="https://hechingerreport.fundjournalism.org/?campaign=701VK000003ezHZYAY" target="_blank" rel="noreferrer noopener">a member</a>&nbsp;today.</em></p></section>

<article id="post-103317">
	<div>

		
		
					<p>Does AI actually help students learn? A recent experiment in a high school provides a cautionary tale.&nbsp;</p><p>Researchers at the University of Pennsylvania found that Turkish high school students who had access to ChatGPT while doing practice math problems did worse on a math test compared with students who didn’t have access to ChatGPT. Those with ChatGPT solved 48 percent more of the practice problems correctly, but they ultimately scored 17 percent worse on a test of the topic that the students were learning.&nbsp;</p><p>A third group of students had access to a revised version of ChatGPT that functioned more like a tutor. This chatbot was programmed to provide hints without directly divulging the answer. The students who used it did spectacularly better on the practice problems, solving 127 percent more of them correctly compared with students who did their practice work without any high-tech aids. But on a test afterwards, these AI-tutored students did no better. Students who just did their practice problems the old fashioned way — on their own — matched their test scores.</p><p>The researchers titled their paper, “Generative AI Can Harm Learning,” to make clear to parents and educators that the current crop of freely available AI chatbots can “substantially inhibit learning.” Even a fine-tuned version of ChatGPT designed to mimic a tutor doesn’t necessarily help.</p><p>The researchers believe the problem is that students are using the chatbot as a “crutch.” When they analyzed the questions that students typed into ChatGPT, students often simply asked for the answer. Students were not building the skills that come from solving the problems themselves.&nbsp;</p><p>ChatGPT’s errors also may have been a contributing factor. The chatbot only answered the math problems correctly half of the time. Its arithmetic computations were wrong 8 percent of the time, but the bigger problem was that its step-by-step approach for how to solve a problem was wrong 42 percent of the time. The tutoring version of ChatGPT was directly fed the correct solutions and these errors were minimized.</p><p>A <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4895486">draft paper about the experiment</a> was posted on the website of SSRN, formerly known as the Social Science Research Network, in July 2024. The paper has not yet been published in a peer-reviewed journal and could still be revised.&nbsp;</p><p>This is just one experiment in another country, and more studies will be needed to confirm its findings. But this experiment was a large one, involving nearly a thousand students in grades nine through 11 during the fall of 2023. Teachers first reviewed a previously taught lesson with the whole classroom, and then their classrooms were randomly assigned to practice the math in one of three ways: with access to ChatGPT, with access to an AI tutor powered by ChatGPT or with no high-tech aids at all. Students in each grade were assigned the same practice problems with or without AI. Afterwards, they took a test to see how well they learned the concept. Researchers conducted four cycles of this, giving students four 90-minute sessions of practice time in four different math topics to understand whether AI tends to help, harm or do nothing.</p><p>ChatGPT also seems to produce overconfidence. In surveys that accompanied the experiment, students said they did not think that ChatGPT caused them to learn less even though they had. Students with the AI tutor thought they had done significantly better on the test even though they did not. (It’s also another good reminder to all of us that our <a href="https://hechingerreport.org/proof-points-college-students-often-dont-know-when-theyre-learning/">perceptions of how much we’ve learned are often wrong</a>.)</p><p>The authors likened the problem of learning with ChatGPT to autopilot. They recounted how an overreliance on autopilot led the Federal Aviation Administration to recommend that pilots minimize their use of this technology. Regulators wanted to make sure that pilots still know how to fly when autopilot fails to function correctly.&nbsp;</p><p>ChatGPT is not the first technology to present a tradeoff in education. Typewriters and computers reduce the need for handwriting. Calculators reduce the need for arithmetic. When students have access to ChatGPT, they might answer more problems correctly, but learn less. Getting the right result to one problem won’t help them with the next one.</p><p><em>This story about&nbsp;using <a href="https://hechingerreport.org/kids-chatgpt-worse-on-tests/">ChatGPT to practice math</a>&nbsp;was written by Jill Barshay and produced by&nbsp;<a href="https://hechingerreport.org/special-reports/higher-education/" target="_blank" rel="noreferrer noopener">The Hechinger Report</a>, a nonprofit, independent news organization focused on inequality and innovation in education. Sign up for&nbsp;<a href="https://hechingerreport.org/proofpoints/" target="_blank" rel="noreferrer noopener"><em>Proof Points</em></a>&nbsp;and other&nbsp;<a href="https://hechingerreport.org/newsletters/" target="_blank" rel="noreferrer noopener"><em>Hechinger newsletters</em></a>.</em></p>
<div id="custom_html-3">
	
<p>The Hechinger Report provides in-depth, fact-based, unbiased reporting on education that is free to all readers. But that doesn't mean it's free to produce. Our work keeps educators and the public informed about pressing issues at schools and on campuses throughout the country. We tell the whole story, even when the details are inconvenient. Help us keep doing that.</p>

<p><a href="https://checkout.fundjournalism.org/memberform?amount=15&amp;installmentPeriod=monthly&amp;org_id=hechingerreport&amp;campaign=701f4000000dsvy">Join us today.</a></p>
</div>	</div><!-- .entry-content -->

	<!-- .entry-footer -->

	
			<div>
															<p><a href="https://hechingerreport.org/author/jill-barshay/" rel="author">
											<img alt="Avatar photo" src="https://hechingerreport.org/wp-content/uploads/2015/01/Barshay-80x80.jpg" srcset="https://i0.wp.com/hechingerreport.org/wp-content/uploads/2015/01/Barshay.jpg?fit=154%2C150&amp;ssl=1 2x" height="80" width="80">											</a></p><!-- .author-bio-text -->

			</div><!-- .author-bio -->
			
</article><!-- #post-${ID} -->

<!-- #comments -->
				</div><!-- .main-content -->

			
		</main><!-- #main -->
	</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Yi-Coder: A Small but Mighty LLM for Code (128 pts)]]></title>
            <link>https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md</link>
            <guid>41453237</guid>
            <pubDate>Thu, 05 Sep 2024 03:38:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md">https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md</a>, See on <a href="https://news.ycombinator.com/item?id=41453237">Hacker News</a></p>
Couldn't get https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Canadian mega landlord using AI 'pricing scheme' as it hikes rents (127 pts)]]></title>
            <link>https://breachmedia.ca/canadian-mega-landlord-ai-pricing-scheme-hikes-rents/</link>
            <guid>41452781</guid>
            <pubDate>Thu, 05 Sep 2024 01:59:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://breachmedia.ca/canadian-mega-landlord-ai-pricing-scheme-hikes-rents/">https://breachmedia.ca/canadian-mega-landlord-ai-pricing-scheme-hikes-rents/</a>, See on <a href="https://news.ycombinator.com/item?id=41452781">Hacker News</a></p>
Couldn't get https://breachmedia.ca/canadian-mega-landlord-ai-pricing-scheme-hikes-rents/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Tinystatus: A tiny status page generated by a Python script (121 pts)]]></title>
            <link>https://github.com/harsxv/tinystatus</link>
            <guid>41452339</guid>
            <pubDate>Thu, 05 Sep 2024 00:40:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/harsxv/tinystatus">https://github.com/harsxv/tinystatus</a>, See on <a href="https://news.ycombinator.com/item?id=41452339">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">TinyStatus</h2><a id="user-content-tinystatus" aria-label="Permalink: TinyStatus" href="#tinystatus"></a></p>
<p dir="auto">TinyStatus is a simple, customizable status page generator that allows you to monitor the status of various services and display them on a clean, responsive web page. <a href="https://status.harry.id/" rel="nofollow">Check out an online demo.</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/32115753/364659939-28227221-d1e1-442e-89a4-2a0a09615514.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1MzYxMDIsIm5iZiI6MTcyNTUzNTgwMiwicGF0aCI6Ii8zMjExNTc1My8zNjQ2NTk5MzktMjgyMjcyMjEtZDFlMS00NDJlLTg5YTQtMmEwYTA5NjE1NTE0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDExMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA2Zjc0OGQxMGYxYzRjYmI0MmU0MGVkMzczMmIxMGZjNDQ4YzdkYWEyY2VmMTgxYWQ2YTMyYzA5MzFiZWJjYTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QCCL41W8KUg1xVUkqRXKeIjf9PYclCeikbDKVyKaZK4"><img src="https://private-user-images.githubusercontent.com/32115753/364659939-28227221-d1e1-442e-89a4-2a0a09615514.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1MzYxMDIsIm5iZiI6MTcyNTUzNTgwMiwicGF0aCI6Ii8zMjExNTc1My8zNjQ2NTk5MzktMjgyMjcyMjEtZDFlMS00NDJlLTg5YTQtMmEwYTA5NjE1NTE0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDExMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTA2Zjc0OGQxMGYxYzRjYmI0MmU0MGVkMzczMmIxMGZjNDQ4YzdkYWEyY2VmMTgxYWQ2YTMyYzA5MzFiZWJjYTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QCCL41W8KUg1xVUkqRXKeIjf9PYclCeikbDKVyKaZK4" alt="image"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Monitor HTTP endpoints, ping hosts, and check open ports</li>
<li>Responsive design for both status page and history page</li>
<li>Customizable service checks via YAML configuration</li>
<li>Incident history tracking</li>
<li>Automatic status updates at configurable intervals</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prerequisites</h2><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
<li>Python 3.7 or higher</li>
<li>pip (Python package manager)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Clone the repository or download the source code:</p>
<div data-snippet-clipboard-copy-content="git clone https://github.com/yourusername/tinystatus.git
cd tinystatus"><pre><code>git clone https://github.com/yourusername/tinystatus.git
cd tinystatus
</code></pre></div>
</li>
<li>
<p dir="auto">Install the required dependencies:</p>
<div data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre><code>pip install -r requirements.txt
</code></pre></div>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Create a <code>.env</code> file in the project root and customize the variables:</p>
<div data-snippet-clipboard-copy-content="CHECK_INTERVAL=30
MAX_HISTORY_ENTRIES=100
LOG_LEVEL=INFO
CHECKS_FILE=checks.yaml
INCIDENTS_FILE=incidents.md
TEMPLATE_FILE=index.html.theme
HISTORY_TEMPLATE_FILE=history.html.theme
STATUS_HISTORY_FILE=history.json"><pre><code>CHECK_INTERVAL=30
MAX_HISTORY_ENTRIES=100
LOG_LEVEL=INFO
CHECKS_FILE=checks.yaml
INCIDENTS_FILE=incidents.md
TEMPLATE_FILE=index.html.theme
HISTORY_TEMPLATE_FILE=history.html.theme
STATUS_HISTORY_FILE=history.json
</code></pre></div>
</li>
<li>
<p dir="auto">Edit the <code>checks.yaml</code> file to add or modify the services you want to monitor. Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="- name: GitHub Home
  type: http
  host: https://github.com
  expected_code: 200

- name: Google DNS
  type: ping
  host: 8.8.8.8

- name: Database
  type: port
  host: db.example.com
  port: 5432"><pre>- <span>name</span>: <span>GitHub Home</span>
  <span>type</span>: <span>http</span>
  <span>host</span>: <span>https://github.com</span>
  <span>expected_code</span>: <span>200</span>

- <span>name</span>: <span>Google DNS</span>
  <span>type</span>: <span>ping</span>
  <span>host</span>: <span>8.8.8.8</span>

- <span>name</span>: <span>Database</span>
  <span>type</span>: <span>port</span>
  <span>host</span>: <span>db.example.com</span>
  <span>port</span>: <span>5432</span></pre></div>
</li>
<li>
<p dir="auto">(Optional) Customize the <code>incidents.md</code> file to add any known incidents or maintenance schedules.</p>
</li>
<li>
<p dir="auto">(Optional) Modify the <code>index.html.theme</code> and <code>history.html.theme</code> files to customize the look and feel of your status pages.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Run the TinyStatus script:</p>

</li>
<li>
<p dir="auto">The script will generate two HTML files:</p>
<ul dir="auto">
<li><code>index.html</code>: The main status page</li>
<li><code>history.html</code>: The status history page</li>
</ul>
</li>
<li>
<p dir="auto">To keep the status page continuously updated, you can run the script in the background:</p>
<ul dir="auto">
<li>On Unix-like systems (Linux, macOS):
<div data-snippet-clipboard-copy-content="nohup python tinystatus.py &amp;"><pre><code>nohup python tinystatus.py &amp;
</code></pre></div>
</li>
<li>On Windows, you can use the Task Scheduler to run the script at startup.</li>
</ul>
</li>
<li>
<p dir="auto">Serve the generated HTML files using your preferred web server (e.g., Apache, Nginx, or a simple Python HTTP server for testing).</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Customization</h2><a id="user-content-customization" aria-label="Permalink: Customization" href="#customization"></a></p>
<ul dir="auto">
<li>Adjust the configuration variables in the <code>.env</code> file to customize the behavior of TinyStatus.</li>
<li>Customize the appearance of the status page by editing the CSS in <code>index.html.theme</code> and <code>history.html.theme</code>.</li>
<li>Add or remove services by modifying the <code>checks.yaml</code> file.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Contributions are welcome! Please feel free to submit a Pull Request.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is open source and available under the <a href="https://github.com/harsxv/tinystatus/blob/master/LICENSE">MIT License</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Laminar – Open-Source DataDog + PostHog for LLM Apps, Built in Rust (159 pts)]]></title>
            <link>https://github.com/lmnr-ai/lmnr</link>
            <guid>41451698</guid>
            <pubDate>Wed, 04 Sep 2024 22:52:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/lmnr-ai/lmnr">https://github.com/lmnr-ai/lmnr</a>, See on <a href="https://news.ycombinator.com/item?id=41451698">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a href="https://www.ycombinator.com/companies/laminar-ai" rel="nofollow"><img src="https://camo.githubusercontent.com/3bf938994198a5b1d850adab39ba81e5675045b3b2b496b9856ce7b833eae93a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f59253230436f6d62696e61746f722d5332342d6f72616e6765" alt="Static Badge" data-canonical-src="https://img.shields.io/badge/Y%20Combinator-S24-orange"></a>
<a href="https://x.com/lmnrai" rel="nofollow"><img src="https://camo.githubusercontent.com/7217689996b50018699ee10564c16fa62744b484f8ab968cfcd2b4b992212b15/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f6c6d6e726169" alt="X (formerly Twitter) Follow" data-canonical-src="https://img.shields.io/twitter/follow/lmnrai"></a>
<a href="https://discord.gg/nNFUUDAKub" rel="nofollow"> <img src="https://camo.githubusercontent.com/3c567c02e658b3bbc878a42c214883dc7706c5206ceea744dd66138c5b9e348f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4a6f696e5f446973636f72642d3436343634363f266c6f676f3d646973636f7264266c6f676f436f6c6f723d353836354632" alt="Static Badge" data-canonical-src="https://img.shields.io/badge/Join_Discord-464646?&amp;logo=discord&amp;logoColor=5865F2"> </a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Laminar - Open-Source observability, analytics, evals and prompt chains for complex LLM apps.</h2><a id="user-content-laminar---open-source-observability-analytics-evals-and-prompt-chains-for-complex-llm-apps" aria-label="Permalink: Laminar - Open-Source observability, analytics, evals and prompt chains for complex LLM apps." href="#laminar---open-source-observability-analytics-evals-and-prompt-chains-for-complex-llm-apps"></a></p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/14181915/364260202-88e1f801-1dbf-4e5b-af71-1a3923661cd1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1MTgxMDIsIm5iZiI6MTcyNTUxNzgwMiwicGF0aCI6Ii8xNDE4MTkxNS8zNjQyNjAyMDItODhlMWY4MDEtMWRiZi00ZTViLWFmNzEtMWEzOTIzNjYxY2QxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDA2MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYyYTBmYjliYjg1YjAzYWJmY2IyZTBhMGIzNDRhMGFjNTdmZmMxODQ1MDliZGRkOWE2ZDdjOWY4YzQxMzQzMmYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.7I9SrNoGGdke9Vnw-RjlOeneRQgabGyiRu8uDYQadi4"><img width="1439" alt="traces" src="https://private-user-images.githubusercontent.com/14181915/364260202-88e1f801-1dbf-4e5b-af71-1a3923661cd1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1MTgxMDIsIm5iZiI6MTcyNTUxNzgwMiwicGF0aCI6Ii8xNDE4MTkxNS8zNjQyNjAyMDItODhlMWY4MDEtMWRiZi00ZTViLWFmNzEtMWEzOTIzNjYxY2QxLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDA2MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYyYTBmYjliYjg1YjAzYWJmY2IyZTBhMGIzNDRhMGFjNTdmZmMxODQ1MDliZGRkOWE2ZDdjOWY4YzQxMzQzMmYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.7I9SrNoGGdke9Vnw-RjlOeneRQgabGyiRu8uDYQadi4"></a>
<p dir="auto">Think of it as DataDog + PostHog for LLM apps.</p>
<ul dir="auto">
<li>OpenTelemetry-based instrumentation: automatic for LLM / vector DB calls with just 2 lines of code + decorators to track functions (powered by an amazing <a href="https://github.com/traceloop/openllmetry">OpenLLMetry</a> open-source package by TraceLoop).</li>
<li>Semantic events-based analytics. Laminar hosts background job queues of LLM pipelines. Outputs of those pipelines are turned into metrics. For example, you can design a pipeline which extracts "my AI drive-through agent made an upsell" data, and track this metric in Laminar.</li>
<li>Built for scale with a modern stack: written in Rust, RabbitMQ for message queue, Postgres for data, Clickhouse for analytics</li>
<li>Insightful, fast dashboards for traces / spans / events</li>
</ul>
<p dir="auto">Read the <a href="https://docs.lmnr.ai/" rel="nofollow">docs</a>.</p>
<p dir="auto">This is a work in progress repo and it will be frequently updated.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Laminar Cloud</h3><a id="user-content-laminar-cloud" aria-label="Permalink: Laminar Cloud" href="#laminar-cloud"></a></p>
<p dir="auto">The easiest way to get started is with a generous free tier on our managed platform -&gt; <a href="https://www.lmnr.ai/" rel="nofollow">lmnr.ai</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Self-hosting with Docker compose</h3><a id="user-content-self-hosting-with-docker-compose" aria-label="Permalink: Self-hosting with Docker compose" href="#self-hosting-with-docker-compose"></a></p>
<p dir="auto">Start local version with docker compose.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:lmnr-ai/lmnr
cd lmnr
docker compose up"><pre>git clone git@github.com:lmnr-ai/lmnr
<span>cd</span> lmnr
docker compose up</pre></div>
<p dir="auto">This will spin up the following containers:</p>
<ul dir="auto">
<li>app-server – the core app logic, backend, and the LLM proxies</li>
<li>rabbitmq – message queue for sending the traces and observations reliably</li>
<li>qdrant – vector database</li>
<li>semantic-search-service – service for interacting with qdrant and embeddings</li>
<li>frontend – the visual front-end dashboard for interacting with traces</li>
<li>postgres – the database for all the application data</li>
<li>clickhouse – columnar OLAP database for more efficient event and trace analytics</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Instrumenting Python code</h3><a id="user-content-instrumenting-python-code" aria-label="Permalink: Instrumenting Python code" href="#instrumenting-python-code"></a></p>
<p dir="auto">First, create a project and generate a Project API Key. Then,</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install lmnr
echo &quot;LMNR_PROJECT_API_KEY=<YOUR_PROJECT_API_KEY>&quot; >> .env"><pre>pip install lmnr
<span>echo</span> <span><span>"</span>LMNR_PROJECT_API_KEY=&lt;YOUR_PROJECT_API_KEY&gt;<span>"</span></span> <span>&gt;&gt;</span> .env</pre></div>
<p dir="auto">To automatically instrument LLM calls of popular frameworks and LLM provider libraries just add</p>
<div dir="auto" data-snippet-clipboard-copy-content="from lmnr import Laminar as L
L.initialize(project_api_key=&quot;<LMNR_PROJECT_API_KEY>&quot;)"><pre><span>from</span> <span>lmnr</span> <span>import</span> <span>Laminar</span> <span>as</span> <span>L</span>
<span>L</span>.<span>initialize</span>(<span>project_api_key</span><span>=</span><span>"&lt;LMNR_PROJECT_API_KEY&gt;"</span>)</pre></div>
<p dir="auto">In addition to automatic instrumentation, we provide a simple <code>@observe()</code> decorator, if you want to trace inputs / outputs of functions</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Example</h4><a id="user-content-example" aria-label="Permalink: Example" href="#example"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import os
from openai import OpenAI

from lmnr import observe, Laminar as L
L.initialize(project_api_key=&quot;<LMNR_PROJECT_API_KEY>&quot;)

client = OpenAI(api_key=os.environ[&quot;OPENAI_API_KEY&quot;])

@observe()  # annotate all functions you want to trace
def poem_writer(topic=&quot;turbulence&quot;):
    prompt = f&quot;write a poem about {topic}&quot;
    response = client.chat.completions.create(
        model=&quot;gpt-4o&quot;,
        messages=[
            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt},
        ],
    )
    poem = response.choices[0].message.content
    return poem

if __name__ == &quot;__main__&quot;:
    print(poem_writer(topic=&quot;laminar flow&quot;))"><pre><span>import</span> <span>os</span>
<span>from</span> <span>openai</span> <span>import</span> <span>OpenAI</span>

<span>from</span> <span>lmnr</span> <span>import</span> <span>observe</span>, <span>Laminar</span> <span>as</span> <span>L</span>
<span>L</span>.<span>initialize</span>(<span>project_api_key</span><span>=</span><span>"&lt;LMNR_PROJECT_API_KEY&gt;"</span>)

<span>client</span> <span>=</span> <span>OpenAI</span>(<span>api_key</span><span>=</span><span>os</span>.<span>environ</span>[<span>"OPENAI_API_KEY"</span>])

<span>@<span>observe</span>()  <span># annotate all functions you want to trace</span></span>
<span>def</span> <span>poem_writer</span>(<span>topic</span><span>=</span><span>"turbulence"</span>):
    <span>prompt</span> <span>=</span> <span>f"write a poem about <span><span>{</span><span>topic</span><span>}</span></span>"</span>
    <span>response</span> <span>=</span> <span>client</span>.<span>chat</span>.<span>completions</span>.<span>create</span>(
        <span>model</span><span>=</span><span>"gpt-4o"</span>,
        <span>messages</span><span>=</span>[
            {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>"You are a helpful assistant."</span>},
            {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>prompt</span>},
        ],
    )
    <span>poem</span> <span>=</span> <span>response</span>.<span>choices</span>[<span>0</span>].<span>message</span>.<span>content</span>
    <span>return</span> <span>poem</span>

<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:
    <span>print</span>(<span>poem_writer</span>(<span>topic</span><span>=</span><span>"laminar flow"</span>))</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Sending events</h4><a id="user-content-sending-events" aria-label="Permalink: Sending events" href="#sending-events"></a></p>
<p dir="auto">You can send events in two ways:</p>
<ul dir="auto">
<li><code>.event(name, value)</code> – instant event with a value.</li>
<li><code>.evaluate_event(name, evaluator, data)</code> –  event that is evaluated by evaluator pipeline based on the data.</li>
</ul>
<p dir="auto">Note that to run an evaluate event, you need to crate an evaluator pipeline and create a target version for it.</p>
<p dir="auto">Laminar processes background job queues of pipeline processes and records outputs of pipelines as events.</p>
<p dir="auto">Read our <a href="https://docs.lmnr.ai/" rel="nofollow">docs</a> to learn more about event types and how they are created and evaluated.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from lmnr import Laminar as L
# ...
poem = response.choices[0].message.content

# this will register True or False value with Laminar
L.event(&quot;topic alignment&quot;, topic in poem)

# this will run the pipeline `check_wordy` with `poem` set as the value
# of `text_input` node, and write the result as an event with name
# &quot;excessive_wordiness&quot;
L.evaluate_event(&quot;excessive_wordiness&quot;, &quot;check_wordy&quot;, {&quot;text_input&quot;: poem})"><pre><span>from</span> <span>lmnr</span> <span>import</span> <span>Laminar</span> <span>as</span> <span>L</span>
<span># ...</span>
<span>poem</span> <span>=</span> <span>response</span>.<span>choices</span>[<span>0</span>].<span>message</span>.<span>content</span>

<span># this will register True or False value with Laminar</span>
<span>L</span>.<span>event</span>(<span>"topic alignment"</span>, <span>topic</span> <span>in</span> <span>poem</span>)

<span># this will run the pipeline `check_wordy` with `poem` set as the value</span>
<span># of `text_input` node, and write the result as an event with name</span>
<span># "excessive_wordiness"</span>
<span>L</span>.<span>evaluate_event</span>(<span>"excessive_wordiness"</span>, <span>"check_wordy"</span>, {<span>"text_input"</span>: <span>poem</span>})</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Laminar pipelines as prompt chain managers</h4><a id="user-content-laminar-pipelines-as-prompt-chain-managers" aria-label="Permalink: Laminar pipelines as prompt chain managers" href="#laminar-pipelines-as-prompt-chain-managers"></a></p>
<p dir="auto">You can create Laminar pipelines in the UI and manage chains of LLM calls there.</p>
<p dir="auto">After you are ready to use your pipeline in your code, deploy it in Laminar by selecting the target version for the pipeline.</p>
<p dir="auto">Once your pipeline target is set, you can call it from Python in just a few lines.</p>
<div dir="auto" data-snippet-clipboard-copy-content="from lmnr import Laminar as L

L.initialize('<YOUR_PROJECT_API_KEY>')

result = l.run(
    pipeline = 'my_pipeline_name',
    inputs = {'input_node_name': 'some_value'},
    # all environment variables
    env = {'OPENAI_API_KEY': 'sk-some-key'},
)"><pre><span>from</span> <span>lmnr</span> <span>import</span> <span>Laminar</span> <span>as</span> <span>L</span>

<span>L</span>.<span>initialize</span>(<span>'&lt;YOUR_PROJECT_API_KEY&gt;'</span>)

<span>result</span> <span>=</span> <span>l</span>.<span>run</span>(
    <span>pipeline</span> <span>=</span> <span>'my_pipeline_name'</span>,
    <span>inputs</span> <span>=</span> {<span>'input_node_name'</span>: <span>'some_value'</span>},
    <span># all environment variables</span>
    <span>env</span> <span>=</span> {<span>'OPENAI_API_KEY'</span>: <span>'sk-some-key'</span>},
)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Learn more</h2><a id="user-content-learn-more" aria-label="Permalink: Learn more" href="#learn-more"></a></p>
<p dir="auto">To learn more about instrumenting your code, check out our client libraries:</p>
<p dir="auto"><a href="https://www.npmjs.com/package/@lmnr-ai/lmnr" rel="nofollow"> <img src="https://camo.githubusercontent.com/6b3081997512b3addac3266d3dcaa06d9fe0cfdc9d34a7b64f56c68ee0e10398/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f2534306c6d6e722d61692532466c6d6e723f6c6162656c3d6c6d6e72266c6f676f3d6e706d266c6f676f436f6c6f723d434233383337" alt="NPM Version" data-canonical-src="https://img.shields.io/npm/v/%40lmnr-ai%2Flmnr?label=lmnr&amp;logo=npm&amp;logoColor=CB3837"> </a>
<a href="https://pypi.org/project/lmnr/" rel="nofollow"> <img src="https://camo.githubusercontent.com/667df376d1224a1681f52ee5b358b52d73094911caecd6bcfa5fd55e6622df40/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6c6d6e723f6c6162656c3d6c6d6e72266c6f676f3d70797069266c6f676f436f6c6f723d333737354139" alt="PyPI - Version" data-canonical-src="https://img.shields.io/pypi/v/lmnr?label=lmnr&amp;logo=pypi&amp;logoColor=3775A9"> </a></p>
<p dir="auto">To get deeper understanding of the concepts, follow on to the <a href="https://docs.lmnr.ai/" rel="nofollow">docs</a> and <a href="https://docs.lmnr.ai/tutorials" rel="nofollow">tutorials</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lesser known parts of Python standard library – Trickster Dev (108 pts)]]></title>
            <link>https://www.trickster.dev/post/lesser-known-parts-of-python-standard-library/</link>
            <guid>41450824</guid>
            <pubDate>Wed, 04 Sep 2024 21:07:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.trickster.dev/post/lesser-known-parts-of-python-standard-library/">https://www.trickster.dev/post/lesser-known-parts-of-python-standard-library/</a>, See on <a href="https://news.ycombinator.com/item?id=41450824">Hacker News</a></p>
Couldn't get https://www.trickster.dev/post/lesser-known-parts-of-python-standard-library/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Internet Archive loses appeal over eBook lending (197 pts)]]></title>
            <link>https://www.theverge.com/2024/9/4/24235958/internet-archive-loses-appeal-ebook-lending</link>
            <guid>41449229</guid>
            <pubDate>Wed, 04 Sep 2024 18:56:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/9/4/24235958/internet-archive-loses-appeal-ebook-lending">https://www.theverge.com/2024/9/4/24235958/internet-archive-loses-appeal-ebook-lending</a>, See on <a href="https://news.ycombinator.com/item?id=41449229">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The Internet Archive has lost its appeal in a fight to lend out scanned ebooks without the approval of publishers. In a <a href="https://www.documentcloud.org/documents/25091194-internet-archive-appeal?responsive=1&amp;title=1">decision on Wednesday</a>, the Second Circuit Court of Appeals ruled that permitting the Internet Archive’s digital library would “allow for widescale copying that deprives creators of compensation and diminishes the incentive to produce new works.”</p><p>The decision is another blow to the nonprofit in the <em>Hachette v. Internet Archive</em> case. In 2020, four major publishers — Hachette, Penguin Random House, Wiley, and HarperCollins — <a href="https://www.theverge.com/2020/6/1/21277036/internet-archive-publishers-lawsuit-open-library-ebook-lending">sued the Internet Archive</a> over claims its digital library constitutes “willful digital piracy on an industrial scale.”</p><p>The Internet Archive has long offered a system called the Open Library, where users can “check out” digital scans of physical books. The library was based on a principle called controlled digital lending, where each loan corresponds to a physically purchased book held in a library — avoiding, in theory, a piracy claim. It’s a fundamentally different system from programs like OverDrive, where publishers sell limited-time licenses to ebooks on their own terms.</p><p>However, the Internet Archive <a href="https://blog.archive.org/2020/03/30/internet-archive-responds-why-we-released-the-national-emergency-library/">expanded its library project during the covid-19 pandemic</a>. It launched the National Emergency Library, allowing an unlimited number of people to access the same copies of ebooks. That’s when the publishers banded together to file the lawsuit, targeting both online libraries.</p><p>The Second Circuit Court’s decision acknowledges the benefits and drawbacks of the Internet Archive’s digital library in its decision. But it ultimately sides with publishers:</p><div><blockquote><p>On the one hand, eBook licensing fees may impose a burden on libraries and reduce access to creative work. On the other hand, authors have a right to be compensated in connection with the copying and distribution of their original creations. Congress balanced these “competing claims upon the public interest” in the Copyright Act. We must uphold that balance here.</p></blockquote></div><div><p>Last year, <a href="https://www.theverge.com/2023/3/24/23655804/internet-archive-hatchette-publisher-ebook-library-lawsuit">a federal judge ruled that the Internet Archive</a> doesn’t have the right to scan and lend out books in the same way a library would. The Internet Archive later <a href="https://www.theverge.com/2023/9/11/23868870/internet-archive-hachette-open-library-copyright-lawsuit-appeal">appealed that decision</a>. </p></div><p>“We are disappointed in today’s opinion about the Internet Archive’s digital lending of books that are available electronically elsewhere,” Chris Freeland, the director of library services at the Internet Archive, <a href="https://blog.archive.org/2024/09/04/internet-archive-responds-to-appellate-opinion/">writes in a post on the site</a>. “We are reviewing the court’s opinion and will continue to defend the rights of libraries to own, lend, and preserve books.” Freeland also <a href="https://www.change.org/p/let-readers-read-an-open-letter-to-the-publishers-in-hachette-v-internet-archive?utm_medium=custom_url&amp;utm_source=share_petition&amp;recruited_by_id=eb10e620-2915-11ef-99de-71750e499499">points to a petition</a> you can sign to restore access to the 500,000 books publishers restricted access to.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kagi: Announcing The Assistant (437 pts)]]></title>
            <link>https://blog.kagi.com/announcing-assistant</link>
            <guid>41448985</guid>
            <pubDate>Wed, 04 Sep 2024 18:35:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.kagi.com/announcing-assistant">https://blog.kagi.com/announcing-assistant</a>, See on <a href="https://news.ycombinator.com/item?id=41448985">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
            <div>
  <p>
    <iframe width="800" height="450" src="https://www.youtube-nocookie.com/embed/0cMcOtVQUkE?si=0cMcOtVQUkE&amp;rel=0&amp;vq=hd1080" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
  </p>
</div>
<p><em>Yes, the rumours are true!</em></p>

<p>Kagi has been thoughtfully integrating AI into our search experience, creating a smarter, faster, and more intuitive search. This includes <a href="https://help.kagi.com/kagi/ai/quick-answer.html">Quick Answer</a> which delivers knowledge instantly for many searches (can be activated by appending ? to the end of your searches), <a href="https://help.kagi.com/kagi/ai/summarize-page.html">Summarize Page</a> for the quick highlights of a web page, and even the ability to <a href="https://help.kagi.com/kagi/ai/ask-questions.html">ask questions about a web page</a> in your search results. And all of these features are on-demand and ready when you need them.</p>

<p>Today we’re excited to unveil the Assistant by Kagi.  A user friendly Assistant that has everything you want and none of the things you don’t (such as user data harvesting, ads &amp; tracking).  Major features include:</p>

<ul>
<li>Integration with Kagi’s legendary quality search results<br>
</li>
<li>Choice of leading LLM models from all the leading providers (OpenAI, Anthropic, Google, Mistral, …)<br>
</li>
<li>Powerful Custom Assistants that include your own custom instructions, choice of leading models, and tools like search and internet access<br>
</li>
<li>Mid-thread editing and branching for making the most of your conversations without starting over<br>
</li>
<li>All threads are private by default, retained only as long as you want and subscriber data is not used for training models.</li>
</ul>

<h2>Powered by Kagi Search</h2>

<p>Kagi Assistant has the ability to use Kagi Search to source the highest quality information meaning that its responses are grounded in the most up-to-date factual information while disregarding most “spam” and “made for advertising” sites with our unique ranking algorithm and user search personalizations on top.</p>

<p><img src="https://kagifeedback.org/assets/files/2024-09-03/1725361807-334029-upload-5b01ebb868ee82236b769125e19d2b10.png" alt="image">
</p><center><em>Assistant with References</em></center>

<h2>Choice of Best Models</h2>

<p>Kagi Assistant provides the best in class capabilities for coding, information retrieval, problem solving, brainstorming, creative writing, and other LLM applications by leveraging the finest LLM models available. You can select from any model and switch whenever you like. The Assistant can always make use of the latest models as they become available. In addition, you can decide whether to give the model web access (via Kagi Search) or you want to use the model in ‘raw’ mode.</p>

<p><img src="https://kagifeedback.org/assets/files/2024-09-03/1725361807-99305-upload-0c181df6fd10988592f870adcabf9142.png" alt="image">
</p><center><em>Model Selection</em></center>

<h2>Powerful Custom Assistants</h2>

<p>LLMs are incredibly flexible tools you can use for many tasks.  With Kagi’s Custom Assistants you can build a tool that meets your exact needs.  For example you may be a car enthusiast and are looking for advice about your VW Bus.</p>

<p>You could create a Custom Assistant to help with the myriad of questions a owner of a classic vehicle might have.  Start by naming your Custom Assistant and select the tools and options.<br>
<img src="https://kagifeedback.org/assets/files/2024-09-03/1725389662-205507-image.png" alt="image">
</p><center><em>Custom Assistant Options</em></center>

<p><br>
Then give the Custom Assistant context and clear instructions on how it should respond.  In this case providing relevant details on your car and guidelines for the advice.</p>

<p><img src="https://kagifeedback.org/assets/files/2024-09-03/1725389665-535788-image2.png" alt="image">
<em></em></p><center><em>Custom Assistant Instructions</em></center>

<p>Use the Custom Assistant to get the answers you need with the context and instructions provided.  Here the model provides relevant advice on diagnosing a oil leak.</p>

<p><img src="https://kagifeedback.org/assets/files/2024-09-03/1725389839-290214-image3.png" alt="image">
<em></em></p><center><em>Using Custom Assistant</em></center>

<h2>Mid-Thread Editing and Branching</h2>

<p>Any LLM user has seen that sometimes they can get data wrong, hallucinate or just become confused.  Or we might want to refine our prompt as we see how a model responds. For instance if you’re interested in understanding how to handle imbalanced data sets you might ask Assistant:</p>

<p><img src="https://kagifeedback.org/assets/files/2024-09-03/1725361806-569146-upload-744debb8ceb4547fd753110a30e3c18a.png" alt="image"></p>

<center><em>Starting a Thread</em></center>


<p>The response is correct, but a little too generic; you can edit the question and add that you’re working on a binary classification problem to get a more specific answer.  You could even switch the model or turn on/off web access.<br>
<img src="https://kagifeedback.org/assets/files/2024-09-03/1725361806-385380-upload-5a8180e572dc07ebe4ac4c8be278b5fc.png" alt="image">
</p><center><em>Editing the Prompt to add Specifics</em></center>

<p><br>
Clarifying the question yields much more useful advice but if it didn’t you could just go back to the original branch and continue on.</p>

<p><img src="https://kagifeedback.org/assets/files/2024-09-03/1725361806-284938-upload-baca920305f0b650338782e0d3e109a3.png" alt="image"></p>

<center><em>Updated Answer with New Detail &amp; Branch Navigation</em></center>

<h2>Private by Default</h2>

<p>We know many of you are concerned about what AI companies may be doing with your data. According to a <a href="https://www.pewresearch.org/internet/2023/10/18/how-americans-view-data-privacy/">survey by the Pew Research Center</a> about 80% of people “familiar with AI say its use by companies will lead to people’s personal information being used in ways they won’t be comfortable with (81%) or that weren’t originally intended (80%)” and “Among those who’ve heard about AI, 70% have little to no trust in companies to make responsible decisions about how they use it in their products.”</p>

<p><img src="https://kagifeedback.org/assets/files/2024-09-03/1725386335-668027-pi-20231018-data-privacy-0-04.webp"></p>



<p>Kagi is committed to protecting your information. Your threads automatically expire and are deleted according to your settings (default is after 24 hours) and you can choose to save threads you really need for later.  This approach helps not only with protecting your data, but with managing the thread clutter as well.
<img src="https://kagifeedback.org/assets/files/2024-09-03/1725391573-898057-screenshot-2024-09-03-at-122557.png" alt="image">
</p><center><em>Thread Saving Settings</em></center>


<p>Since we don’t show ads (and never will) and don’t train on subscriber data there’s no reason for us to harvest your data, track your clicks, searches, threads, or build a profile of you.  When we use third party models via their APIs it is protected under terms of service that forbid using data for training their models (e.g. <a href="https://support.anthropic.com/en/articles/7996885-how-do-you-use-personal-data-in-model-training">Anthropic Terms</a> &amp; <a href="https://ai.google.dev/gemini-api/terms#data-use-paid">Google Terms</a>).</p>

<h2>Pricing &amp; Availability</h2>

<p>The Assistant by Kagi is available today as part of the Kagi Ultimate Plan for $25 per month, which also includes full access to Kagi Search. Discount available for annual subscriptions.  <a href="https://kagi.com/">Learn more at Kagi.com</a>.</p>

<h2>FAQ:</h2>

<p><strong>Q:</strong> Can I try out Assistant today?<br>
<strong>A:</strong> Yes, the Assistant is generally available today to all Kagi Ultimate tier members   You can create an account today to try it out and cancel at any time with no long term commitments.</p>

<p><strong>Q:</strong> Is the Assistant available in Kagi’s Starter or Professional tier?<br>
<strong>A:</strong> As of today the Assistant is only available on our Ultimate tier (and Family plan members upgraded to Ultimate tier).  We are always looking for ways to provide more value to our members and are evaluating how we can offer Assistant to the Starter and Professional tiers.</p>

<p><strong>Q:</strong> What are the LLM limitations in place?<br>
<strong>A:</strong> The Assistant currently has no hard limits on usage. We would like it to stay unlimited and will be monitoring this actively. Please do not abuse so everyone can enjoy no-limit access. Provider APIs may have limitations in place.</p>

<p><strong>Q:</strong> Does the Assistant have file upload capability?<br>
<strong>A:</strong> The Assistant will have file upload capabilities very soon (work in progress). You can still access beta version that had it using <a href="https://kagi.com/v1_assistant">this link</a>.</p>

<p><strong>Q:</strong> I found a bug in Assistant, how do I report it?<br>
<strong>A:</strong> Please report all bugs and feature suggestions using <a href="https://kagifeedback.org/">Kagi Feedback</a>.</p>

<p><strong>Q:</strong> What is Kagi’s overall strategy about using LLMs in search?<br>
<strong>A:</strong> We are continuing to relentlessly focus on the core search experience and build thoughtfully integrated features on top of it. Read more about it in our <a href="https://blog.kagi.com/what-is-next-for-kagi#8">recent blog post</a>.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon bans its drivers from moving their own lips too much at work (228 pts)]]></title>
            <link>https://www.freightwaves.com/news/should-truckers-be-allowed-to-sing-along-with-the-radio</link>
            <guid>41448866</guid>
            <pubDate>Wed, 04 Sep 2024 18:24:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.freightwaves.com/news/should-truckers-be-allowed-to-sing-along-with-the-radio">https://www.freightwaves.com/news/should-truckers-be-allowed-to-sing-along-with-the-radio</a>, See on <a href="https://news.ycombinator.com/item?id=41448866">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Welcome to the WHAT THE TRUCK?!? <a href="https://freightwaves.com/wtt">Newsletter</a> presented by <a href="https://truckstop.com/">Truckstop.</a> In this issue, inward dash cams penalize singing?; Flexport’s last supper&nbsp; and more.</p><div id="omeda-post-content">


<p><strong><a href="https://www.reddit.com/r/AmazonDSPDrivers/comments/1f2y2cp/try_to_tell_me_when_i_can_move_my_own_mouth_im_out/"><img decoding="async" width="624" height="219" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfeqfTVpWUk21h2Q1tzZ2gnQ3rEpUf21aEPBc1uLi3zgU9zmIcn6ug9SOj4wilNOJ7dt2kENHbS6yc5NHTR2cOtbfQ7s08vy-f_we-b6QaByzackminnfD7qyd1nlaprMXomAZM0zHCEveQm-VW_jBorF4?key=lGXafURkKN6DLhpG9BhIMQ"></a></strong><br><a href="https://www.reddit.com/r/AmazonDSPDrivers/comments/1f2y2cp/try_to_tell_me_when_i_can_move_my_own_mouth_im_out/">Reddit</a></p>







<p><strong>Big brother — </strong>Amazon Delivery Service Partner (DSP) drivers on <a href="https://www.reddit.com/r/AmazonDSPDrivers/comments/1f2y2cp/try_to_tell_me_when_i_can_move_my_own_mouth_im_out/">Reddit</a> are furious over a new set of standards the company is imposing on them in regards to their inward facing dash cams.&nbsp;<br></p>



<p>According to a number of Redditors as well as drivers that I’ve confirmed with, Amazon is trying to crack down on distracted driving. Now, some of those partners are being warned that singing along with the radio will trigger the inward facing camera in their cab.&nbsp;<br></p>



<div><p>Why? A new update to their Netradyne systems has advanced the scope of how it detects eye and mouth movement.&nbsp;</p><p>Amazon’s <a href="https://hiring.amazon.com/job-opportunities/delivery-driver-jobs#/">website</a> describes their DSP program as “an independent third-party business.” However, that level of independence is under question after these latest measures that are proving to be very unpopular.</p></div>



<p><img loading="lazy" decoding="async" width="624" height="160" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcw-BIe6ZT0I8oHvFr4xjaiJ4nzqwlGXKY4hjNVhTJ3sgXytyfxhs_ZGqeYM2hfKN9IoaPVSuS87AAzV7SLQRBPwgiSuO60zaMTzRF2Nh4f6XE-crvaxLWCM3btBkPtC1rdoh1j1S8zbi1LlQCKZlDooAws?key=lGXafURkKN6DLhpG9BhIMQ"><br></p>



<div><p><strong>Shake it off — </strong>Does anyone really believe that singing distracts drivers or is this a case of AI and surveillance overstepping its bounds? For me personally, hitting the high notes on a Whitney song energizes me and keeps me between the white lines.</p><p>I asked my network that is almost entirely composed of supply chain or supply chain adjacent followers what they thought about the issue. Over <a href="https://x.com/TimothyDooner/status/1829146707290169433">94% of X</a> users and <a href="https://www.linkedin.com/posts/timothydooner_should-truck-drivers-be-allowed-to-sing-along-activity-7234913956052553729-YVBg?utm_source=share&amp;utm_medium=member_desktop">97% of LinkedIn</a> users believe that drivers should be allowed to croon.</p></div>



<p><strong>Here’s what some truckers had to say:</strong><br></p>



<p><a href="https://x.com/EdMapes1/status/1829148133286388110">Ed Mapes</a> – It helps us stay focused and awake.<br></p>



<div><p><a href="https://x.com/NewTruckerMike/status/1829167209194893782">New Trucker Mike</a> – I rock, ska, and country my way all over the highways. Go ahead and try to stop me.</p></div>



<p><img loading="lazy" decoding="async" width="624" height="124" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXeosLmqqn3FS87EV7nzxRwVieeYSniqPpPaQg5LPGrgzu-vTDJfEi54zrbU3wBt_xk2ENsF5v4Oa09-c4GVgywzgYBE76f2zRZ56zUGaXCkIGbfbOsFSjoOERsWc6e1qLwP9q7tskCUk_ty7SZIEFDSqRI?key=lGXafURkKN6DLhpG9BhIMQ"></p>



<p><a href="https://x.com/TommyApples80/status/1829188060904349762">X</a>&nbsp;</p>



<p><br><a href="https://x.com/sask_trucker_/status/1829153392448672073">Sask Trucker</a> – If Shania comes on I’m singing.</p>











<p><strong>Data – </strong>Opinions may be anecdotal, but there are studies that have looked at the relationship between music and distracted driving.&nbsp;<br></p>



<p><strong><em>It was concluded that, in some indicators, listening to music has adverse effects on driving. However, in many indicators, music has a positive impact on improving driving safety. It is better to choose appropriate music for different driving conditions and to train the drivers about it. – </em></strong><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10790125/"><strong><em>National Library of Medicine&nbsp;</em></strong></a><br></p>



<p>While there were some negative factors associated with music, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10790125/">the study</a> found that, “Listening to music can enhance not only the driver’s driving quality but also their physiological performance. In particular, listening to music while driving is effective in controlling stress, calming emotions, and preventing driver drowsiness”&nbsp;<br></p>



<div><p><strong>Sound off –</strong>&nbsp; While it probably isn’t likely that Amazon is specifically targeting singing, the end result is the same. What do you think? Should drivers be allowed to sing while behind the wheel? <a href="https://www.freightwaves.com/cdn-cgi/l/email-protection#6b0f0404050e192b0d02190e0819041c0545080406">Email me</a>.&nbsp;</p><p><strong>The last shipper</strong></p></div>



<div><p><img loading="lazy" decoding="async" width="508" height="579" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcpsHkE_RgeXHvYCOaGjY03VQwGZSmBESAf3x1f7QiE0K7CQZcERCkSdVs8wjSvgLXRPg9wAexl1kBfHd2imCM3_30--Pn_Idzq5o0kg7zTJXoQ9fHoMDdkOgsrkKEAFNdJYd1ggHROm7F588fUYgWIm9s?key=lGXafURkKN6DLhpG9BhIMQ"><a href="https://x.com/typesfast/status/1828925875372593167">X</a></p><p><strong>Controversy – </strong>Flexport’s Ryan Petersen <a href="https://x.com/typesfast/status/1828925875372593167?s=46&amp;t=P3RlHpCY-GJkhcsno3878g">posted</a> his latest AI-generated artwork to X on Wednesday, and it hasn’t been well received by everyone. The image appears to show a number of Flexport executives and apostles celebrating the last supper.</p></div>







<p><strong>“This dude really looked at the bad publicity around the Olympics Opening Ceremony and said ‘I gotta get my company in on that.’ – </strong><a href="https://x.com/Logistorian/status/1828934367877468300"><strong>Logistorian on X</strong></a><br></p>



<p>In July, the Olympic opening ceremony in Paris came under fire when a performance appeared to mock da Vinci’s famous painting “The Last Supper.” The artistic director of that <a href="https://x.com/Olympics/status/1816929100532945380?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1816929100532945380%7Ctwgr%5E9664cca506ae15d2bb810c2b3b24f7734d8bd0b4%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fwww.snopes.com%2Fnews%2F2024%2F07%2F30%2Folympics-last-supper%2F">said it was a misinterpretation</a>, and instead they were referencing Dionysus, the Greek God of wine and festivity.&nbsp;<br></p>



<div><p>Not everyone accepted that answer, but Petersen’s piece is less ambiguous as it clearly shows Jesus levitating over the table.</p></div>



<div><p><img loading="lazy" decoding="async" width="624" height="145" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfH_MKLlLrWnUlT1XaQJ9e9sOwb-qGgo-ie1NfcvHnSNVDv0gZNOOxqQQlXH8Jl7ayyyVx4AHzrsk-2AJiy36niYjus8SMSNq5EfQ54h2mb-kW2BcXEy-3RIqh51IhhqRB1Gp19pC705VsX7Ywwn2k1Y-Yt?key=lGXafURkKN6DLhpG9BhIMQ"><a href="https://x.com/Girldad741/status/1829017497875496991">X</a></p><p><strong>The Judas Cradle – </strong>Many of the comments under the image have called it blasphemous or unnecessary. However, a few didn’t mind at all. Petersen may have even sold a few copies of his book, ‘<a href="https://www.amazon.com/Big-Ship-Little-Digger/dp/1667800442">The Big Ship and the Little Digger</a>’ in the process.</p></div>



<div><p><img loading="lazy" decoding="async" width="624" height="108" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfpF-_e7d-e6yiyIHQ4gnfSOMm7YD5u2L-rmZBECvJow-XmLBZNulnI3VEna7r2lK20eGjBT3jflvZW1e6ZEHtRXt9R9frss651dpF7rllUA9lQli3bsbaitpKn1nroozQrFUjSaXgoXQ_aSCoAx4nElELD?key=lGXafURkKN6DLhpG9BhIMQ"><a href="https://x.com/carsjam33/status/1828929385342083252">X</a></p><p>What do you think? Was it a&nbsp; good use of AI or a misguided attempt at marketing? <a href="https://www.freightwaves.com/cdn-cgi/l/email-protection#d0b4bfbfbeb5a290b6b9a2b5b3a2bfa7befeb3bfbd">Email me</a>.</p></div>







<p><strong>Gear for the gals</strong><br></p>



<div><p><img loading="lazy" decoding="async" width="624" height="396" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfgKhBplDWyySYxmA-jKxBx-1BiL3hQ1-Gn4_YTAj0zxePg1b_4xXCrZKzD8OXLoc5m-w_JIFOl87u7efo2hwWRnuaWA0D9qE1cC-2tzsIFK2yYYc6qLEpgZI8MMsRgSL4BncZvB0ODiXBL0_rnkdLK3uhL?key=lGXafURkKN6DLhpG9BhIMQ"><br><strong>Crop top –</strong> Head on over to<strong> </strong><a href="https://wttgear.com/products/rate-the-strap-work-t-shirt">WTTGear.com</a> to get our latest merch! Use code WTTFans for 10% off.</p><p><strong>WTT Friday</strong></p></div>



<p><strong><img loading="lazy" decoding="async" width="624" height="351" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXdHvGD4P0xxnQXu2Fb6AEP_IWXwbvF6oCKMu_a1XgKFh1FBXhi4PPM46zbOYRLUCSXSTyNo50XqZ2BG63GVGqgXBPhas8Mt6VMt-ZcJDpdcCTy71JeufG8vWq-XtXVdB_OjNDc47Yjs7QO_SK0b0YsckiVk?key=lGXafURkKN6DLhpG9BhIMQ"></strong></p>



<p><strong>How Costco leveraged its supply chain to become the 3rd biggest retailer on Earth —</strong> Friday live at 12 p.m. Eastern, we’re joined by supply chain super consultant Brittain Ladd to talk about how Costco leveraged its supply chain to become the third largest retailer in the world. Nearly one third of Americans count themselves as members and the company is now expanding into East Asia. Ladd shares how Costco is using tech to scale the business even further.<br>&nbsp;</p>



<p>MoLo co-founder and podcaster Andrew Silver returns to the show to talk about the state of the industry and lessons learned from his time in the trenches. We’ll also find out how he’s building his show The Freight Pod and what insights he’s gleaned from top leaders in freight.<br></p>



<div><p>Plus, headlines, weirdness and more.</p><p><strong>Catch new shows live at noon EDT Mondays, Wednesdays and Fridays on FreightWaves </strong><a href="https://www.linkedin.com/company/freightwaves/"><strong>LinkedIn</strong></a><strong>, </strong><a href="https://www.facebook.com/FreightWaves"><strong>Facebook</strong></a><strong>, </strong><a href="https://twitter.com/freightwaves?lang=en"><strong>X</strong></a><strong> or </strong><a href="https://www.youtube.com/c/FreightWaves"><strong>YouTube</strong></a><strong>, or on demand by looking up WHAT THE TRUCK?!? on your favorite podcast player and at 5 p.m. Eastern on SiriusXM’s Road Dog Trucking Channel 146.</strong></p></div>











<p><strong><a href="https://www.freightwaves.com/news/the-logistic-of-death-what-the-truck"><img loading="lazy" decoding="async" width="624" height="351" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXcL2Xm10RIw2oK3XpuveodDbrPQKZfdXVlNyyUbvMZumm_ka2Ts_toeotavLFbNaGryY27FsAsvDJx9EI1Z2QR0N00fkmJQ5C1ei6PFp8OTEQM32NH6BNFlBV4f8B8hJHDtoSWid1zcbaP9ch6a6puB-vU?key=lGXafURkKN6DLhpG9BhIMQ"></a></strong></p>



<p><strong><a href="https://www.freightwaves.com/news/rail-strike-decision-disastrous-trooper-bens-rules-of-the-road-freight-magic-what-the-truck">Rail strike decision “disastrous”; Trooper Ben’s rules of the road; freight magic</a></strong></p>



<p><strong><a href="https://www.freightwaves.com/news/rail-strike-decision-disastrous-trooper-bens-rules-of-the-road-freight-magic-what-the-truck"><img loading="lazy" decoding="async" width="624" height="351" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfUGgx8Bo7kWDbGgniJEjEdB2wloA2CsBwV3LB27iCDLyI9_X4rGQXaGg1SdBloaMgtZ3J3bUVHGXuSyEJNRDjiZvNtm2G1A6VVqBJDkSp8nqXroq6ehSecCeBUq3YogdVEX8pUtXVCOM1cueKl6rGU9Wwo?key=lGXafURkKN6DLhpG9BhIMQ"></a></strong></p>



<p><strong>The rest of the noise</strong></p>



<ul>
<li><a href="https://www.freightwaves.com/news/hazmat-carrier-sues-dali-shipowners-for-negligence"><strong>Hazmat carrier sues Dali shipowners for negligence</strong></a></li>



<li><a href="https://www.freightwaves.com/news/us-mexico-trade-relations-enter-uncharted-territory-expert-says"><strong>US-Mexico trade relations enter “uncharted territory,” expert says</strong></a></li>



<li><a href="https://www.freightwaves.com/news/impact-minimal-from-canada-rail-shutdown"><strong>Supply chain sees unexpected impact from Canada rail ramp-up</strong></a></li>



<li><a href="https://www.freightwaves.com/news/former-polar-air-cargo-executive-receives-18-month-jail-sentence"><strong>Former Polar Air Cargo executive receives 18-month jail sentence</strong></a></li>



<li><a href="https://www.businessinsider.com/kraft-heinz-ai-lighthouse-helps-forecast-supply-chain-demands-2024-8"><strong>Kraft Heinz is using AI to make more autonomous supply-chain decisions</strong></a></li>
</ul>







<p><strong>Thanks for reading, and feel free to forward this to a friend.</strong></p>



<p><br><a href="https://twitter.com/TimothyDooner"><strong>Tweet @ Dooner</strong></a></p>



<p><a href="https://www.freightwaves.com/cdn-cgi/l/email-protection#3753585859524577515e455254455840591954585a"><strong>Email me</strong></a></p>



<p><a href="https://freightwaves.com/communities"><strong>Subscribe to the newsletter</strong></a></p>







<p><strong>Subscribe to the show</strong></p>



<p><a href="https://podcasts.apple.com/us/podcast/what-the-truck/id1357715797"><strong>Apple Podcasts</strong></a></p>



<p><a href="https://open.spotify.com/show/5X1AlbXLIKJAwwwA059sVd?si=KxAg2SYvS3O6WjK2Ftzt9Q"><strong>Spotify</strong></a></p>



<p><a href="https://www.youtube.com/playlist?list=PLVi2PdlRdiSqmJsM01U1gwAfc_y75q_PW"><strong>YouTube</strong></a></p>



<p><a href="https://www.tiktok.com/@fwwhatthetruck"><strong>TikTok</strong></a></p>



<p><a href="https://twitter.com/FWwhatthetruck"><strong>Twitter</strong></a></p>



<p><strong>Or simply look up WHAT THE TRUCK?!? on your favorite podcast player. Or, if you have SiriusXM, tune in to the show Monday, Wednesday and Friday at 5 p.m. Eastern time on Road Dog Trucking Channel 146.</strong></p>







<p><strong>Exit through the gift shop: </strong><a href="http://wttgear.com/"><strong>WTTGear.com&nbsp;</strong></a></p>







<p><strong>Don’t be a stranger,</strong></p>



<p><strong>Dooner</strong></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CSS @property and the New Style (472 pts)]]></title>
            <link>https://ryanmulligan.dev/blog/css-property-new-style/</link>
            <guid>41448740</guid>
            <pubDate>Wed, 04 Sep 2024 18:13:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ryanmulligan.dev/blog/css-property-new-style/">https://ryanmulligan.dev/blog/css-property-new-style/</a>, See on <a href="https://news.ycombinator.com/item?id=41448740">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">




<section>
  <div>
    

    <p><span>Posted on <strong>September 2, 2024</strong></span>
  </p></div>
  <div>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="var(--color-text-accent)" viewBox="0 0 256 256"><rect width="256" height="256" fill="none"></rect><circle cx="128" cy="128" r="88" fill="var(--color-theme)"></circle><circle cx="128" cy="128" r="88" fill="none" stroke="var(--color-text-accent)" stroke-miterlimit="10" stroke-width="16"></circle><line x1="128" y1="128" x2="167.6" y2="88.4" fill="none" stroke="var(--color-text-accent)" stroke-linecap="round" stroke-linejoin="round" stroke-width="16"></line><line x1="104" y1="8" x2="152" y2="8" fill="none" stroke="var(--color-text-accent)" stroke-linecap="round" stroke-linejoin="round" stroke-width="16"></line></svg>

    <p><span> Takes about <strong>9 minutes</strong> to read </span>
  </p></div>
</section>
<p>The <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/@property"><code>@property</code></a> at-rule recently gained support across all modern browsers, unlocking the ability to explicitly define a syntax, initial value, and inheritance for CSS custom properties. It seems like forever ago that CSS Houdini and its <a href="https://developer.mozilla.org/en-US/docs/Web/API/CSS_Properties_and_Values_API">CSS Properties and Values API</a> were initially introduced. I experimented sparingly over time, reading articles that danced around the concepts, but I had barely scratched the surface of what <code>@property</code> could offer. The ensuing demo explores what's possible in the next generation of CSS.</p>
<h2 id="calls-to-action">Calls to action</h2>
<p>Ever seen those sleek, attention-seeking, shiny call-to-action webpage elements? Waves of sites across the web, especially the ones marketing services and software urging for you to "Upgrade your account" or "Sign up today," have discovered the look and latched on. I'm not here to knock it and admittedly think it's kind of fresh. I thought I'd give that style a try myself. Check out the result in the CodePen below.</p>
<p data-height="500" data-preview="false" data-default-tab="result" data-slug-hash="MWMqXbK" data-user="hexagoncircle">
  <a href="https://codepen.io/hexagoncircle/pen/MWMqXbK??editors=0100">
    
    <span>Open CodePen demo</span>
  </a>
</p>

<p>There's a ton to unpack in this demo. Let's start with that shine looping around the button. Toggle open the demo's CSS panel to find a collection of <code>@property</code> rules related to those custom properties that need to animate. Here's the one defined for the <code>--gradient-angle</code>:</p>
<pre><code><span><span>@property</span> --gradient-angle</span> <span>{</span>
  <span>syntax</span><span>:</span> <span>"&lt;angle&gt;"</span><span>;</span>
  <span>initial-value</span><span>:</span> 0deg<span>;</span>
  <span>inherits</span><span>:</span> <span>false</span><span>;</span>
<span>}</span></code></pre>
<p>The <code>@property</code> rule communicates to the browser that <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/angle"><code>&lt;angle&gt;</code></a> is the allowed syntax for this custom property and its initial value is <code>0deg</code>. This enables the browser to smoothly transition from <code>0deg</code> to <code>360deg</code> and output a rotating gradient.</p>
<pre><code><span><span>@keyframes</span> rotate-gradient</span> <span>{</span>
  <span>to </span><span>{</span> <span>--gradient-angle</span><span>:</span> 360deg<span>;</span> <span>}</span>
<span>}</span>

<span>.rotate-gradient </span><span>{</span>
  <span>background</span><span>:</span> <span>conic-gradient</span><span>(</span>from <span>var</span><span>(</span>--gradient-angle<span>)</span><span>,</span> transparent<span>,</span> black<span>)</span><span>;</span>
  <span>animation</span><span>:</span> rotate-gradient 10s linear infinite<span>;</span>
<span>}</span></code></pre>
<p>I put together a simple gradient spin demo to focus on the handful of lines necessary to render this concept.</p>
<p data-height="300" data-preview="true" data-default-tab="result" data-slug-hash="eYwLqJx" data-user="hexagoncircle">
  <a href="https://codepen.io/hexagoncircle/pen/eYwLqJx">
    
    <span>Open CodePen demo</span>
  </a>
</p>

<p>We can achieve the shiny animated border effect by evolving this code a bit. We'll introduce a <code>linear-gradient</code> as the first value of the element's <code>background</code> property and set a <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/background-origin"><code>background-origin</code></a> to each value.</p>
<ul>
<li>The origin of the <code>linear-gradient</code> is set to <code>padding-box</code>. This prevents the gradient from spilling into the border area.</li>
<li>The <code>conic-gradient</code> origin is set to <code>border-box</code>. This gradient overflows into the space created by the border width.</li>
<li>To reveal the rotating <code>conic-gradient</code>, a single-pixel transparent border is added.</li>
</ul>
<pre><code><span>.border-gradient </span><span>{</span>
  <span>background</span><span>:</span> 
    <span>linear-gradient</span><span>(</span>black<span>,</span> black<span>)</span> padding-box<span>,</span>
    <span>conic-gradient</span><span>(</span>from <span>var</span><span>(</span>--gradient-angle<span>)</span><span>,</span> transparent 25%<span>,</span> white<span>,</span> transparent 50%<span>)</span> border-box<span>;</span>
  <span>border</span><span>:</span> 1px solid transparent<span>;</span>
<span>}</span></code></pre>
<p>In the CSS panel of the <a href="#cp_embed_eYwLqJx">simple gradient spin demo</a>, uncomment the <code>.border-gradient</code> ruleset to reveal the shiny animated border. Looking pretty slick! For more examples, I've included a bunch of animated gradient border articles in the <a href="#helpful-resources">resources section</a> at the end of the post.</p>
<h2 id="silky-smooth-hover-transitions">Silky smooth hover transitions</h2>
<p>A few special ingredients help facilitate a buttery smooth gradient transition when the element is hovered. Let's dig into its <code>background</code> values:</p>
<pre><code><span>.shiny-cta </span><span>{</span>
  <span>background</span><span>:</span> 
    <span>linear-gradient</span><span>(</span><span>var</span><span>(</span>--shiny-cta-bg<span>)</span><span>,</span> <span>var</span><span>(</span>--shiny-cta-bg<span>)</span><span>)</span> padding-box<span>,</span>
    <span>conic-gradient</span><span>(</span>
        <span>from</span> <span>calc</span><span>(</span><span>var</span><span>(</span>--gradient-angle<span>)</span> <span>-</span> <span>var</span><span>(</span>--gradient-angle-offset<span>)</span><span>)</span><span>,</span>
        transparent<span>,</span>
        <span>var</span><span>(</span>--shiny-cta-highlight<span>)</span> <span>var</span><span>(</span>--gradient-percent<span>)</span><span>,</span>
        <span>var</span><span>(</span>--gradient-shine<span>)</span> <span>calc</span><span>(</span><span>var</span><span>(</span>--gradient-percent<span>)</span> <span>*</span> 2<span>)</span><span>,</span>
        <span>var</span><span>(</span>--shiny-cta-highlight<span>)</span> <span>calc</span><span>(</span><span>var</span><span>(</span>--gradient-percent<span>)</span> <span>*</span> 3<span>)</span><span>,</span>
        transparent <span>calc</span><span>(</span><span>var</span><span>(</span>--gradient-percent<span>)</span> <span>*</span> 4<span>)</span>
      <span>)</span>
      border-box<span>;</span>
<span>}</span></code></pre>
<p>Each custom property that needs to animate has a <code>syntax</code> declared in its <code>@property</code> definition so that the browser can interpolate between corresponding value changes and transition them seamlessly. The size of the shiny area is determined by the <code>--gradient-percent</code> value. On hover, a higher percentage lengthens the shine. The <code>--gradient-angle-offset</code> value is used to readjust the gradient angle so that the shine doesn't rubber band back and forth on hover.</p>
<figure>
    <video preload="metadata" loop="" muted="" playsinline="" controls="">
      <source src="https://ryanmulligan.dev/videos/shiny-cta-angle-offset.webm#t=0.001" type="video/webm">
      <source src="https://ryanmulligan.dev/videos/shiny-cta-angle-offset.mp4#t=0.001" type="video/mp4">
      <p>Your browser cannot play the provided video file.</p>
    </video>
  <figcaption>Demonstrating the transition behavior without the angle offset value</figcaption></figure>
<p>I had to fine-tune the percent and offset values until the shine length and transition felt optically aligned. Finally, the <code>--gradient-shine</code> brightness gets toned down to blend more seamlessly with the adjacent highlight colors.</p>
<h2 id="slow-it-on-down">Slow it on down</h2>
<p>This <a href="https://css-tip.com/slow-down-rotation/">CSS tip to slow down a rotation on hover</a> truly blew my mind. In the tip's example code, the same rotate animation is declared twice. The second one is reversed and paused, its duration divided in half. When the element is hovered, <code>animation-play-state: running</code> overrides the <code>paused</code> value and slows the rotation to half speed. The mind-blowing part, at least to me, is that the animation speeds back up at the current position when the element is no longer hovered. No snapping back to a start position, no extra wrapper elements necessary. That is one heck of a tip.</p>
<p>The <a href="#cp_embed_MWMqXbK">call-to-action animations</a> rely on this method to slow them down when the button is hovered. This technique keeps all the rotations and movements in sync as they change speed.</p>
<h2 id="tiny-shiny-dots">Tiny shiny dots</h2>
<p>Looking even closer, we'll discover pinhole-sized dots shimmering inside the button as the shiny border passes near them. To render this dot pattern, a <code>radial-gradient</code> background is created.</p>
<pre><code><span>.shiny-cta::before </span><span>{</span>
  <span>--position</span><span>:</span> 2px<span>;</span>
  <span>--space</span><span>:</span> <span>calc</span><span>(</span><span>var</span><span>(</span>--position<span>)</span> <span>*</span> 2<span>)</span><span>;</span>
  <span>background</span><span>:</span> <span>radial-gradient</span><span>(</span>
      circle at <span>var</span><span>(</span>--position<span>)</span> <span>var</span><span>(</span>--position<span>)</span><span>,</span>
      white <span>calc</span><span>(</span><span>var</span><span>(</span>--position<span>)</span> <span>/</span> 4<span>)</span><span>,</span>
      transparent 0
    <span>)</span>
    padding-box<span>;</span>
  <span>background-size</span><span>:</span> <span>var</span><span>(</span>--space<span>)</span> <span>var</span><span>(</span>--space<span>)</span><span>;</span>
  <span>background-repeat</span><span>:</span> space<span>;</span>
<span>}</span></code></pre>
<p>Remember that <code>--gradient-angle</code> custom property? It has returned! But this time, it's being used in a <code>conic-gradient</code> mask that reveals parts of the dot pattern as it rotates. The gradient angle is offset by 45 degrees to align it perfectly with the shiny border rotation.</p>
<pre><code><span>.shiny-cta::before </span><span>{</span>
  <span>mask-image</span><span>:</span> <span>conic-gradient</span><span>(</span>
    <span>from</span> <span>calc</span><span>(</span><span>var</span><span>(</span>--gradient-angle<span>)</span> <span>+</span> 45deg<span>)</span><span>,</span>
    black<span>,</span>
    transparent 10% 90%<span>,</span>
    black
  <span>)</span><span>;</span>
<span>}</span></code></pre>
<p>For one last touch of magic, a gradient containing the highlight color is added to the <code>::after</code> pseudo element, spinning in unison with the shine area. These highlights flowing through the button add a pleasant, welcoming ambience that was previously missing.</p>
<h2 id="enhancing-the-hover-colors">Enhancing the hover colors</h2>
<p>The hover styles looked decent. But they didn't seem totally finished. I felt the desire to enhance. Create more depth. <a href="https://ryanmulligan.dev/blog/detect-js-support-in-css/#:~:text=%22Make%20it%20pop!%22">Make it pop, as they say</a>.</p>
<p>The button's <code>::before</code> and <code>::after</code> pseudo elements were already in use so I wrapped the button text in a <code>span</code> element. A blurred <code>box-shadow</code> containing the highlight color is applied to one of its pseudo elements which is then expanded to fill the button dimensions. On hover, the pseudo element slowly scales up and down, evoking a vibe similar to relaxed breathing. Paired with the spinning highlight color inside the button, the effect finally resonated with me. This intricately designed call-to-action button felt complete.</p>
<h2 id="in-with-the-new-style">In with the new style</h2>
<p>Many of the above techniques would have been nearly impossible only a short time ago. Explicitly defining custom properties unlocks a great big world of opportunity. I'm especially eager to see how <code>@property</code> will be utilized in large-scale applications and design systems. <a href="https://moderncss.dev/providing-type-definitions-for-css-with-at-property/">Providing Type Definitions for CSS with @property</a> by Stephanie Eckles as well as Adam Argyle's <a href="https://nerdy.dev/cant-break-this-design-system">Type safe CSS design systems with @property</a> are just a couple glimpses into a really promising future for publishing our CSS.</p>
<h2 id="helpful-resources">Helpful resources</h2>
<ul>
<li><a href="https://www.learnwithjason.dev/blog/animated-css-gradient-border/">Animated CSS gradient borders (no JavaScript, no hacks)</a></li>
<li><a href="https://ibelick.com/blog/create-animated-gradient-borders-with-css">Creating an animated gradient border with CSS</a></li>
<li><a href="https://web.dev/articles/css-border-animations">CSS border animations</a></li>
<li><a href="https://www.bram.us/2021/01/29/animating-a-css-gradient-border/">Animating a CSS Gradient Border</a></li>
<li><a href="https://codepen.io/hexagoncircle/full/LYKJPjm">CSS border ripple effect</a></li>
<li><a href="https://www.smashingmagazine.com/2024/05/times-need-custom-property-instead-css-variable/">The Times You Need A Custom @property Instead Of A CSS Variable</a></li>
<li><a href="https://web.dev/blog/at-property-baseline">@property: Next-gen CSS variables now with universal browser support</a></li>
</ul>

<p>
  <a href="https://ryanmulligan.dev/blog/">Back to all blog posts</a>
</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What's functional programming all about? (2017) (147 pts)]]></title>
            <link>https://www.lihaoyi.com/post/WhatsFunctionalProgrammingAllAbout.html</link>
            <guid>41448664</guid>
            <pubDate>Wed, 04 Sep 2024 18:04:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lihaoyi.com/post/WhatsFunctionalProgrammingAllAbout.html">https://www.lihaoyi.com/post/WhatsFunctionalProgrammingAllAbout.html</a>, See on <a href="https://news.ycombinator.com/item?id=41448664">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>There are many descriptions floating around the internet, trying to explain functional programming in simple terms. Unfortunately, most discuss details only loosely related to functional programming, while others focus on topics that are completely irrelevant. So of course, I had to write my own!</p>
<p>This post is my own understanding of what is the "core" of "functional programming", how it differs from "imperative" programming, and what the main benefits of the approach are. As a worked example, we will use a kitchen recipe as a proxy for the more-abstract kind of logic you find in program source code, to try and make concrete what is normally a very abstract topic. That recipe is one of my favorite recipes available online, <a href="http://www.cookingforengineers.com/recipe/60/The-Classic-Tiramisu-original-recipe">Michael Chu's Classic Tiramisu</a>.</p><hr><p><a href="https://www.handsonscala.com/"><img src="https://www.lihaoyi.com/handsonscala-mockup.png"></a></p><p><b>About the Author: </b><i>Haoyi is a software engineer, and the author of many open-source Scala tools such as the Ammonite REPL and the Mill Build Tool. If you enjoyed the contents on this blog, you may also enjoy Haoyi's book <a href="https://www.handsonscala.com/"><b><i>Hands-on Scala Programming</i></b></a></i></p><hr>
<ul>
  <li><a href="#what-functional-programming-is-not">What Functional Programming is Not</a>
    <ul>
      <li><a href="#helper-methods">Helper Methods</a></li>
      <li><a href="#writing-things-in-haskell">Writing Things in Haskell</a></li>
      <li><a href="#compile-time-ast-macros">Compile-time AST Macros</a></li>
      <li><a href="#static-types">Static Types</a></li>
    </ul>
  </li>
  <li><a href="#step-by-step-imperative-recipes">Step by Step Imperative Recipes</a>
    <ul>
      <li><a href="#kitchen-refactoring">Kitchen Refactoring</a></li>
    </ul>
  </li>
  <li><a href="#functional-programming-recipes">Functional Programming Recipes</a>
    <ul>
      <li><a href="#tiramisu-diagram-to-functional-programming">Tiramisu Diagram to Functional Programming</a></li>
      <li><a href="#preventing-errors-with-functional-programming">Preventing Errors with Functional Programming</a></li>
      <li><a href="#refactoring-a-functional-tiramisu-recipe">Refactoring a Functional Tiramisu Recipe</a></li>
      <li><a href="#the-core-of-functional-programming">The Core of Functional Programming</a></li>
    </ul>
  </li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>
<p>A topic as broad as "Functional Programming", or "FP" has too many different interpretations and facets to be summarized in one blog post. Nevertheless, this post will discuss what <em>I</em> think is the most core, basic level of functional programming. This will hopefully be something that everyone, from FP newbies to FP "experts", should be able to empathise with and agree is a useful part of functional programming.</p>
<p>It's not surprising that many people have tried to explain functional programming using kitchen/recipe/cookbook examples: learning things "by analogy" of things you already know is one of the easiest ways of learning. However, all explanations I have seen fall short. I will begin by examining some typical, <em>incorrect</em> explanations of what functional programming is about, before discussing how <a href="http://www.cookingforengineers.com/recipe/60/The-Classic-Tiramisu-original-recipe">Michael Chu's Classic Tiramisu</a> recipe:</p>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/Tiramisu.jpg" alt="TiramisuDiagram"></p>
<p>Can provide insight into what I think are the core techniques and benefits of functional programming.</p><h2 id="what-functional-programming-is-not">What Functional Programming is Not<a href="#what-functional-programming-is-not"></a></h2>
<p>There are many poor explanations people have given for "what is functional programming". Here is a selection:</p><h3 id="helper-methods">Helper Methods<a href="#helper-methods"></a></h3>
<p>One of the most common misconceptions of what FP is is illustrated by the following example:</p>
<blockquote>
  <p>FP =&gt; I'll have a Sazerac</p>
  <p>Imperative =&gt; Excuse me sir, could you take some ice, add rye whiskey, add bitters, add absinthe, shake, strain into a glass, and add a lemon garnish before bringing it to me</p>
</blockquote>
<p>While this example was taken from the <a href="https://news.ycombinator.com/item?id=13281413">y-combinator message board</a>, I've seen this attitude in many places: the idea that functional programming is just taking imperative instructions, and wrapping them in a helper. In this case, the messy imperative code will all sit inside a single helper:</p>
<pre><code>def sazerac():
    ... 10000 lines of messy imperative code ...
</code></pre>
<p>But even in imperative programming you always end up factoring things into helper methods. Java has helper methods. Write assembly, and it ends up being organized with sub-procedures to encapsulate messes of imperative code. </p>
<p>Thus, while this is a useful technique, writing helper methods to wrap your messy code in a single method/function/subprocess/subroutine call does not count as functional programming. </p>
<p>Furthermore, <em>picking an easier/simpler problem</em>, despite making your code look neater, does not count as "Functional Programming" either. Calling a single method that executes a huge blob of code that <em>someone else</em> has written is convenient, but is not functional programming. The point of FP is to face the complexity, own it, and control it, not shove it inside some unmaintained helper function or say it's a problem for some "other department" to deal with.</p><h3 id="writing-things-in-haskell">Writing Things in Haskell<a href="#writing-things-in-haskell"></a></h3>
<pre><code>sazerac = do
    add ice
    add ryeWhisky
    add bitters
    add absinthe
    shake
    strainInto glass
    add lemonGarnish

main = serve $ makeCocktail sazerac
</code></pre>
<ul>
  <li>Also from the <a href="https://news.ycombinator.com/item?id=13281413">y-combinator message board</a></li>
</ul>
<p>It's often said that you can write COBOL in any language, that you can write Java in any language. Well, you can write any language in Haskell too: the above is basically writing Bash in Haskell</p>
<p>Just because something is implemented in Haskell with Monads, doesn't mean it's functional programming. If it looks like imperative code written in Bash, and it's semantics are like imperative code written in Bash, it's imperative code. This example certainly looks exactly like imperative code written in Bash except it's run using <code>serve $ makeCocktail</code> instead of <code>bash cocktail.sh</code>.</p><h3 id="compile-time-ast-macros">Compile-time AST Macros<a href="#compile-time-ast-macros"></a></h3>
<p>Some variant of Lisp (or <a href="https://en.wikipedia.org/wiki/Scheme_(programming_language)">Scheme</a>?) was probably one of the first implemented FP languages; and Lisps tend to have compile-time AST macros that allow you to transform sections of the program at compile-time. </p>
<p>But compile-time code-transformations are not unique to Lisp; apart from other FP languages that have them, like <a href="https://wiki.haskell.org/Template_Haskell">Template Haskell</a> or <a href="http://docs.scala-lang.org/overviews/macros/overview.html">Scala Macros</a>, many languages have some sort of compile-time code transformation. From <a href="http://hannesdorfmann.com/annotation-processing/annotationprocessing101">Java Annotation Processors</a>, to my own <a href="https://github.com/lihaoyi/macropy">MacroPy</a> project in Python, it turns out that compile-time ast macros are just as feasible in imperative languages, doing imperative programming. You can manipulate mutable ASTs using imperative Python code just as easily as you can elegantly transform immutable ASTs using Scala.</p>
<p>Furthermore, there are a large set of "obviously" functional programming languages that don't have AST-transforming macros at all. Purescript, non-Template Haskell, Scala 2.9, and many other "obviously" functional languages do not include support for compile-time AST transformations. So whatever is the core of functional programming, it's not AST macros.</p><h3 id="static-types">Static Types<a href="#static-types"></a></h3>
<p>There are a large number of people who use FP together with static types, e.g. in languages like Haskell, Scala, or Ocaml. Thus, if you spend all your time within this world, it might be tempting to think that FP is all about static types. <a href="http://stackoverflow.com/questions/6246719/what-is-a-higher-kinded-type-in-scala">Higher-kinded</a>, <a href="https://wiki.haskell.org/Rank-N_types">Rank-N</a>, <a href="https://en.wikipedia.org/wiki/Dependent_type">Dependent</a>, the fancier the types, the more functional the programming. </p>
<p>However, there are probably just as many people using FP without static types: in some parts of the Javascript community, Clojure, Scheme or one of the many other Lisps. It turns out, that all those using FP without types still get many of the benefits. And then there are all those people in static-typed languages like Java that use minimal FP in their code. </p>
<p>So static types, while present in many FP languages, are not the core of FP.</p><h2 id="step-by-step-imperative-recipes">Step by Step Imperative Recipes<a href="#step-by-step-imperative-recipes"></a></h2>
<p>Now that we've looked at a few common misconceptions of what FP is, let's look at what the core of FP <em>actually is</em> (according to me) in contrast to "imperative" programming, using <a href="http://www.cookingforengineers.com/recipe/60/The-Classic-Tiramisu-original-recipe">Michael Chu's Classic Tiramisu</a>:</p>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/Tiramisu.jpg" alt="TiramisuDiagram"></p>
<p>As an example. To begin with, we'll explore an "imperative" recipe, that is probably familiar to those you already know.</p>
<p>Michael Chu's Classic Tiramisu, like all the other recipe's on his excellent recipe site has roughly four sections on the page:</p>
<ol>
  <li>
  <p>The backstory of the recipe</p></li>
  <li>
  <p>A step-by-step guide, with photos, of how to make the Tiramisu</p></li>
  <li>
  <p>A diagram of the overall process, showing which ingredients are combined with  which others, to create the resultant Tiramisu</p></li>
  <li>
  <p>A lively and entertaining comments section</p></li>
</ol>
<p>For the purpose of this programming blog we will only be looking at parts <code>2.</code> and <code>3.</code>: the step by step guide, and the process diagram. The step by step guide details, in order, a series of steps that you can take to make a Tiramisu. At a high level, hiding many of the details, it looks like this:</p>
<ol>
  <li>
  <p>Begin by assembling four large egg yolks, 1/2 cup sweet marsala wine, 16  ounces mascarpone cheese, 12 ounces espresso, 2 tablespoons cocoa powder, 1  cup heavy cream, 1/2 cup granulated sugar, and enough lady fingers to layer a  12x8 inch pan twice (40).</p></li>
  <li>
  <p>Stir two tablespoons of granulated sugar into the espresso and put it in  the refrigerator to chill.</p></li>
  <li>
  <p>Whisk the egg yolks</p></li>
  <li>
  <p>Pour in the sugar and wine and whisked briefly until it was well blended.</p></li>
  <li>
  <p>Pour some water into a saucepan and set it over high heat until it began  to boil. </p></li>
  <li>
  <p>Lowering the heat to medium, place the heatproof bowl over the water and  stirred as the mixture began to thicken and smooth out. </p></li>
  <li>
  <p>Whip the heavy cream until soft peaks.</p></li>
  <li>
  <p>Beat the mascarpone cheese until smooth and creamy. </p></li>
  <li>
  <p>Poured the mixture onto the cheese and beat</p></li>
  <li>
  <p>Fold in the whipped cream</p></li>
  <li>
    <p>Assemble the tiramisu. </p>
    <ul>
      <li>
      <p>Give the each ladyfinger cookie  a one second soak on each side and then arrange it on the pan</p></li>
      <li>
      <p>After the first layer of ladyfingers are done, use a spatula to spread  half the cream mixture over it.</p></li>
      <li>
      <p>Cover the cream layer with another layer of soaked ladyfingers.</p></li>
      <li>
      <p>The rest of the cream is spread onto the top and cocoa powder sifted over  the surface to cover the tiramisu.</p></li>
    </ul>
  </li>
  <li>
  <p>The tiramisu was now complete and would require a four hour chill in the refrigerator.</p></li>
</ol>
<p>This is, I think, something like what most people would think of when told "imperative recipe". You start with a set of inputs (the bullet <code>1.</code>) and then perform a series of steps until you have a result at the end. (For now, I'm ignoring the pictures in the recipe, though you could think of them as a sort of <code>assert</code> function for a would-be chef to check some invariants after each step to make sure his tiramisu hasn't gone terribly wrong!)</p>
<p>A simplified Python version of this recipe (ignoring the fact that I'm overloading the same functions to work on different types/number of arguments) may look something like this:</p>
<pre><code>def make_tiramisu(eggs, sugar1, wine, cheese, cream, fingers, espresso, sugar2, cocoa):
    dissolve(sugar2, espresso)
    mixture = whisk(eggs)
    beat(mixture, sugar1, wine)
    whisk(mixture) # over steam
    whip(cream)
    beat(cheese)
    beat(mixture, cheese)
    fold(mixture, cream)
    assemble(mixture, fingers)
    sift(mixture, cocoa)
    refrigerate(mixture)
    return mixture # it's now a tiramisu
</code></pre><h3 id="kitchen-refactoring">Kitchen Refactoring<a href="#kitchen-refactoring"></a></h3>
<p>Like most imperative code, it works, but may be hard to understand deeply or difficult to refactor. For example, in cooking terms, you may ask the following questions:</p>
<ul>
  <li>
  <p>If I have two people to make this tiramisu, which parts can be done in  parallel?</p></li>
  <li>
    <p>My expresso hasn't arrived yet; can I shift that step down and do other  things first and include the expresso later when it arrives? </p>
    <ul>
      <li>
      <p>What if my eggs haven't arrived? Which steps can I do first before  the eggs turn up?</p></li>
    </ul>
  </li>
  <li>
    <p>At step 9. I screwed up and spilled the bowl onto the floor. Which steps do  I need to re-do (and which ingredients I may have to re-purchase) to recover  and continue the recipe?</p>
    <ul>
      <li>
      <p>What if I spilled the bowl at step 10? Or step 8?</p></li>
    </ul>
  </li>
  <li>
    <p>Just before step 10, you realize you forgot to do step 7. How much of  your ingredients have been ruined?</p>
    <ul>
      <li>
      <p>What if the forgotten step was step 4? Or step 2?</p></li>
    </ul>
  </li>
</ul>
<p>All four of these are things that happen regularly in a kitchen, and also happen to correspond to things you do with program code all the time: parallelizing things over the available cores to speed things up, shuffling the order of a computation around, dealing with failures and exceptions, or plain old bugs and mistakes.</p>
<p>The answers to these questions are left as an exercise to the reader; in this case, with 12 steps, it's not terribly hard to figure out. A few minutes carefully studying the recipe and you could probably figure it out, so you should definitely give it a try. </p>
<hr>
<p>In a large software project, with a codebase containing thousands or millions of lines of imperative code, that time could easily stretch to days, weeks, or months trying to figure out how to properly recover when one of those imperative steps fails, or how to make your legacy PHP monolith do something faster by using more than 1 of the 32 cores you have available on your beefy server. </p>
<p><strong>The problem in these cases often isn't that you don't know how to run stuff in a separate process in PHP - the problem is that you don't know enough about your own code to decide what to run in that other process</strong>. To move things onto a separate process, you need to know exactly what each bit of code depends on, and who depends on it, so you can pick a set with minimal dependencies to run somewhere else (since inter-process communication is expensive). That's difficult when you have a pile of imperative code and don't even understand it enough to easily move things around <em>within</em> a single process.</p>
<p>The reason that these kinds of analyses are hard on this imperative recipe is the same reason that the analyses are hard when programming in an imperative style: </p>
<ul>
  <li>
  <p>There is an ordering of the steps, but the ordering between some steps is  required, e.g. the series 9, 10, 11, while those between other steps is  entirely arbitrary: step 2 could be done anywhere before step 11, and step 7  and 8 could be swapped or done much earlier and nobody would care.</p></li>
  <li>
  <p>The instructions are based on changing the state of things, e.g. pouring  stuff into <code>mixture</code>, a term that we use repeatedly throughout the recipe  but means a different thing in each step. Even the meaning of <code>cheese</code> and  <code>cream</code> changes as the recipe progresses (e.g. after calling <code>whip(cream)</code>),  but it is entirely hidden from you and not obvious from the code.</p></li>
</ul>
<p>Overall, these factors make it hard to decide, given a single step <em>S</em>, what steps <em>S</em> depends on, and what <em>other</em> steps depend on <em>S</em>. Again, it is possible to figure it out, but what is somewhat-tedious to figure out in a 16-line tiramisu recipe becomes painful and difficult in a 1,000,000 line enterprise codebase.</p>
<p>So that's what an imperative Tiramisu recipe looks like. What does a "functional programming" Tiramisu recipe look like?</p><h2 id="functional-programming-recipes">"Functional Programming" Recipes<a href="#functional-programming-recipes"></a></h2>
<p>It turns out, there's a FP version of this recipe right underneath the imperative one! The "process diagram" mentioned above is an excellent illustration of how such a recipe would look like using "Functional Programming":</p>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/TiramisuDiagram.png" alt="TiramisuDiagram"></p>
<p>To read this, the raw ingredients are on the left, and each of the boxes represents a single operation that transforms and combines the ingredients. After all the combinations have taken place, you end up on the right with a single, complete Tiramisu. While this "2D" format is not how people write program source code, the underlying structure is not too different from how people structure "FP" programs, which I will demonstrate below.</p>
<p>This diagram leaves out some the <em>detail</em> that the full imperative recipe provides, even compared to the abridged version I transcribed above. For example, chilling the expresso or explicitly boiling the water are left out, and the details of <em>assemble</em> are not included. Nevertheless, it contains the same high-level steps of how to build the tiramisu I abridged above. We're not leaving out large numbers of operations or hiding things behind high-level instructions: all the same steps are still there, just organized slightly differently.</p>
<p>But even if this diagram has the same "content" as the imperative instruction-list I discussed earlier, what about this makes this presentation of the recipe more "functional"?</p><h3 id="tiramisu-diagram-to-functional-programming">Tiramisu Diagram to Functional Programming<a href="#tiramisu-diagram-to-functional-programming"></a></h3>
<p>While nobody actually writes their code in a 2D table-flowchart-thing like this tiramisu diagram is, it turns out underneath the 2D format the "core" of this diagram is the dependency graph between elements:</p>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/DiagramGraph.png" alt="TiramisuDiagram"></p>
<p>Where each box takes in some "inputs" from the left, and results in an "output" that can be used by more-rightward boxes. This can be straightforwardly represented in code by treating the boxes as functions, e.g. in the following Python code:</p>
<pre><code>def make_tiramisu(eggs, sugar1, wine, cheese, cream, fingers, espresso, sugar2, cocoa):
                 
    return refrigerate(
        sift(
            assemble(
                fold(
                    beat(
                        whisk( # over steam
                            beat(beat(eggs), sugar1, wine)
                        ), 
                        beat(cheese)
                    ), 
                    whip(cream)
                ), 
                soak2seconds(fingers, dissolve(sugar2, espresso))
            ), 
            cocoa
        )
    )
</code></pre>
<p>(Again, forgive the fact that I'm overloading the same functions to work on different types and numbers of arguments)</p>
<p>If it's not immediately clear how this code relates to the "functional programming dependency diagram" I discussed above, we can draw the dependency graph <em>of this code</em>: showing where the input variables go, where the return value of each function goes, all the way into the "final" result that gets returned:</p>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/CodeGraph.png" alt="TiramisuDiagram"></p>
<p>It might look like a bit of a mess, but if you look carefully, you will see that <strong>although the graphs are laid out differently, the fundamental structure of the two graphs is identical!</strong> That is what I mean when I say the 2D box-diagram is a "FP Recipe": although people don't tend to write code in 2D box-diagrams, the underlying structure that the diagram represents is totally equivalent to some "FP"-ish Python code, not too dissimilar to what people <em>do</em> write. </p>
<p>This code looks very unlike code you are likely to see in a Python project, "in the wild", but we can fix that! If you prefer to have intermediate named values instead of one big expression, it's straightforward to pull out each function call into it's own statement: </p>
<pre><code># FP         
def make_tiramisu(eggs, sugar1, wine, cheese, cream, fingers, espresso, sugar2, cocoa):
    beat_eggs = beat(eggs)
    mixture = beat(beat_eggs, sugar1, wine)
    whisked = whisk(mixture)
    beat_cheese = beat(cheese)
    cheese_mixture = beat(whisked, beat_cheese)
    whipped_cream = whip(cream)
    folded_mixture = fold(cheese_mixture, whipped_cream)
    sweet_espresso = dissolve(sugar2, espresso)
    wet_fingers = soak2seconds(fingers, sweet_espresso)
    assembled = assemble(folded_mixture, wet_fingers)
    complete = sift(assembled, cocoa)
    ready_tiramisu = refrigerate(complete)
    return ready_tiramisu
</code></pre>
<p>That makes it look entirely "pythonic", indistinguishable from the code you might find in any random project on Github</p>
<p>Moving every expression into a separate statement is a straightforward transformation, at least for FP programs, and is the kind of thing that compilers regularly do automatically. Thus, although that block-flow-chart diagram may have looked a bit foreign at first, it really isn't <em>that</em> different from the code people write day to day, all year round.</p>
<p>In fact, it looks not too unlike the "Imperative" version we came up with earlier!</p>
<pre><code># Imperative
def make_tiramisu(eggs, sugar1, wine, cheese, cream, fingers, espresso, sugar2, cocoa):
    dissolve(sugar2, espresso)
    mixture = whisk(eggs)
    beat(mixture, sugar1, wine)
    whisk(mixture) # over steam
    whip(cream)
    beat(cheese)
    beat(mixture, cheese)
    fold(mixture, cream)
    soak2seconds(fingers, espresso)
    assemble(mixture, fingers)
    sift(mixture, cocoa)
    refrigerate(mixture)
    return mixture # it's now a tiramisu
</code></pre>
<p>These two snippets of code look very similar, but the top one is "Functional Programming" while the bottom one is "Imperative Programming". The difference between them? </p>
<ul>
  <li>
  <p>In the first, you can see that <code>beat(cheese)</code> must come  before <code>beat(whisked, beat_cheese)</code>, because <code>beat_cheese</code>  is defined by the <code>beat(cheese)</code> and used by <code>beat(whisked, beat_cheese)</code>.  Even if you know nothing about <code>beat</code>, <code>cheese</code> or <code>whisked</code>, it is clear  from the code that if you tried to reverse the order - and  <code>beat(whisked, beat_cheese)</code> <em>before</em> <code>beat(cheese)</code>, it wouldn't work.</p></li>
  <li>
  <p>In the second, it's not so clear: does <code>beat(cheese)</code> <em>need</em> to come before  <code>beat(mixture, cheese)</code>? Or does <code>beat(mixture, cheese)</code> need to come before  <code>beat(cheese)</code>? In this case, we have a link to the "docs" (the original  recipe) so we can look it up, but which one depends on the other - and  whether they are currently in the right order - is not clear from the code.</p></li>
</ul>
<p>But how does this seemingly-trivial difference affect the way you build software?</p><h3 id="preventing-errors-with-functional-programming">Preventing Errors with Functional Programming<a href="#preventing-errors-with-functional-programming"></a></h3>
<p>The difference between the two Python snippets, the <code># FP</code> and <code># Imperative</code> snippets, will become clear with the following thought experiment: what if we try to make changes to the code?</p>
<p>Changing code is something we do all day, and sometimes we do it incorrectly. It would be a nice property of a codebase if changes tended to be easier to make correctly, and incorrect changes were easier to spot. We'll discuss the latter first.</p>
<p>If I try to tidy things up and accidentally move the statement</p>
<pre><code>beat_cheese = beat(cheese)
</code></pre>
<p>below</p>
<pre><code>cheese_mixture = beat(whisked, beat_cheese)
</code></pre>
<p>It should be clear to me that something is wrong, because there will be no <code>beat_cheese</code> in scope to create the <code>cheese_mixture</code>. Even if it's not clear to me, it's probably clear to my linter and editor:</p>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/FunctionalError.png" alt="TiramisuDiagram"></p>
<p>As you can see, not only does the usage of <code>beat_cheese</code> raise an error because no such variable is defined, the <em>definition</em> of <code>beat_cheese</code> <em>also</em> raises a visual warning: it is greyed out since it is dead code! This makes it very hard to miss when you make such trivial error, and saves you time: rather than waiting 10s for your test suite to run, within less than 1s your linter would have lit up and flagged the lines as invalid. Over the days, months and years, this adds up to a significant productivity boost</p>
<p>However, in the <em>Imperative</em> case, it's not clear how </p>
<pre><code>beat(mixture, cheese)
</code></pre>
<p>Relates to the things before or after it. If I remove the <code>beat(cheese)</code> earlier, I still have a <code>cheese</code> to pass in. If I remove the <code>beat(mixture, cheese)</code> entirely, I still have a <code>mixture</code> I can use in later steps of the recipe. So how do I know, from looking at the code, that removing a step or re-ordering them so that <code>beat(cheese)</code> comes after <code>beat(mixture, cheese)</code> is a problem? </p>
<p>The answer is, you often don't, and neither does your computer, or your editor and linter, who aren't going to help you spot the fact that you accidentally swapped two of the imperative statements:</p>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/ImperativeError.png" alt="TiramisuDiagram"></p>
<p>Fundamentally, in the "FP" example, the code is laid out in a way that the "correct" usage is obvious: each function, e.g. <code>beat</code>, only depends on the things that are passed into it, and it's output is only depended upon by whoever uses it's return value. In the "Imperative" example, it's not clear who depends on who: you have to memorize the fact that <code>beat(cheese)</code> must come before <code>beat(mixture, cheese)</code>, and not the other way around. </p>
<p>While this is not difficult assuming we are looking at already-correct code (the current order is the correct order!), when mistakes are made, and code happens to be incorrect, "FP" code makes the mistakes much easier for you (or your linter) to spot so you can correct them.</p>
<hr>
<p>While this example may seem contrived, the basic problem exists in all large codebases I've worked with. For example, maybe you've bumped into code similar to the following three functions:</p>
<pre><code>def initialize():
   ... 1000 lines of messy code, no return value...
   
def make_app():
   ... 2000 lines of messy code, no return value...

def start_server():
   ... 4000 lines of messy code, no return value...
</code></pre>
<p>Which transitively depend on a 1 million line codebase ("The App"). How could I know that <code>start_server()</code> needs to be called before <code>make_app()</code>, which itself needs to be called before <code>initialize()</code>, when all of them are global functions which don't take arguments or return anything? I have certainly spent countless days of my career puzzling over such mysteries in large codebases, and I am sure others have too. If <code>start_server</code> returned something I needed to pass to <code>make_app</code>, which returned something I needed to pass to <code>initialize</code>, that would make it clear from the outset which one needs to come before the other.</p>
<p>Re-ordering or shuffling around statements is not uncommon. When you are refactoring a piece of code to let you re-use it in a different place, a lot of time is spent shifting bits of code up and down small amounts, just like the example I showed above, so that the code you want to re-use is all in one place and you can extract it into a helper. </p>
<p>Perhaps you just want to tidy up what was previous a messy function to organize the code a bit better than it already is, grouping related lines so they can be read together easily, without changing any behavior at all.</p>
<p>Or perhaps, as mentioned earlier, someone made a mistake and the code <em>that already exists</em> is incorrect, and your job is to figure out which of the statements is out of order so you can fix it.</p>
<p>All of these are things that software engineers do day in, day out. And often, we make mistakes when doing so. With functional programming, whether in a typed language or not, it tends to be much more clear when you've made a trivial, dumb error. That means you get feedback quicker: you get corrected quietly by your linter in the privacy of your own laptop, and can quickly fix it and make progress, rather than waiting a long time only to be loudly yelled at by Jenkins CI in front of your entire team.</p><h3 id="refactoring-a-functional-tiramisu-recipe">Refactoring a Functional Tiramisu Recipe<a href="#refactoring-a-functional-tiramisu-recipe"></a></h3>
<p>Even if you haven't already-made a mistake, and are just <em>thinking</em> of making a change to a codebase, the <code># FP</code> version of the code is a lot easier to think about than the <code># Imperative</code> version. The same often applies whether you're writing dealing with Python, Javascript, Scala, or a Tiramisu recipe!</p>
<p>I have already shown above how the 2D-block-diagram version of this recipe is exactly equivalent in semantics to a "FP" Python function. For this section I will use the 2D-block-diagrams to illustrate my points, as it is much clearer visually, but the same kind of reasoning applies to "FP" code in Python or any other programming language. While working with an FP style, you quickly get used to performing the same analyses in your head, just as quickly, but on lines of source code rather than 2D-block-diagrams.</p>
<p>What is interesting is that this structure lets us very easily answer some of the questions we asked above:</p>
<blockquote>
  <ul>
    <li>If I have two people to make this tiramisu, which parts can be done in parallel?</li>
  </ul>
</blockquote>
<p>This one is easy: </p>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/Parallel.png" alt="TiramisuDiagram"></p>
<p>Anything vertically separated can be done in parallel. For example, preparing the ladies fingers and preparing the eggs/sugar/wine are separate and can be done independently, as can whipping the cream and mascarpone cheese. Thus, if you have three people, you might assign: </p>
<ul>
  <li>one person to be the egg/wine/sugar mixture czar,</li>
  <li>one to be the mascarpone/cream czar, and</li>
  <li>one to be the expresso/ladyfingers czar.</li>
</ul>
<p>On the other hand, anything horizontally separated has to be done sequentially:</p>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/Sequential.png" alt="TiramisuDiagram"></p>
<p>Thus even if you parallelize the early bits, the later beat-fold-assembly-refrigerate steps all have to be done sequentially, and how much time you can save on your Tiramisu is limited by the length of the <a href="https://en.wikipedia.org/wiki/Critical_path_method">Critical Path</a>. </p>
<p>Working with the "FP" representation of the recipe doesn't shorten the critical path, and thus doesn't affect how much you can "theoretically" speed up your recipe with parallelism. What it does do is make clear exactly which parts of the recipe can be parallelized and which can't, so you can more quickly organize your work to get maximum parallelism given the constraints of the recipe, and then move on to other things.</p>
<p>Again, while we're looking at a 2D-block-diagram, the same applies to FP-style code in Python, Javascript, Scala, or any other programming language.</p>
<blockquote>
  <ul>
    <li>My expresso hasn't arrived yet; can I shift that step down and do other things first and include the expresso later when it arrives?</li>
  </ul>
</blockquote>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/MissingExpresso.png" alt="TiramisuDiagram"></p>
<p>If you expresso hasn't arrived, anything depending on it can't be done, but anything else involving eggs/sugar/wine/cheese/cream can be prepared: the sections marked in red make it clear which parts of the recipe depend on expresso; the rest can be done while waiting for the expresso to arrive</p>
<blockquote>
  <ul>
    <li>What if my eggs haven't arrived? Which steps can I do first before  the eggs turn up?</li>
  </ul>
</blockquote>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/MissingEggs.png" alt="TiramisuDiagram"></p>
<p>In this case, the top block can't be done but you can prepare the bottom and middle blocks: preparing the expresso, beating the cream and mascarpone cheese. Again, this is obvious from looking at the diagram</p>
<blockquote>
  <ul>
    <li>At step 9. I screwed up and spilled the bowl onto the floor. Which steps do I need to re-do (and which ingredients I may have to re-purchase) to recover and continue the recipe?</li>
  </ul>
</blockquote>
<p>Step 9 is when you beat the Mascarpone cheese into the egg mixture. Once we find it on the diagram, it's clear what we need to do:</p>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/FailBeatCheese.png" alt="TiramisuDiagram"></p>
<p>You will need to get some new eggs/sugar/wine/cheese and beat/beat/whisk/beat them all over again </p>
<blockquote>
  <ul>
    <li>What if I spilled the bowl at step 10? Or step 8?</li>
  </ul>
</blockquote>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/FailFoldCream.png" alt="TiramisuDiagram"></p>
<p>Spilling the bowl at step 10 (folding the whipped cream into the main mixture) is the same as spilling the bowl at step 9, except you need to get new cream too.</p>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/FailBeatCream.png" alt="TiramisuDiagram"></p>
<p>Spilling the bowl at step 8 (beating Mascarpone cheese) and you just need to get new mascarpone cheese and beat it. The rest of your ingredients are fine. </p>
<blockquote>
  <ul>
    <li>Just before step 10, you realize you forgot to do step 7. How much of your ingredients have been ruined?</li>
  </ul>
</blockquote>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/ForgotWhipCream.png" alt="TiramisuDiagram"></p>
<p>In the diagram above, the red boxes represent the steps we've already done, up to step 10 (folding in the whipped cream). As you can see, not having done step 7 (whipping the heavy cream) is no big deal; we haven't needed to done it up to now, so we can do it and continue with step 10 </p>
<blockquote>
  <ul>
    <li>What if the forgotten step was step 4? Or step 2?</li>
  </ul>
</blockquote>
<p>If you forgot step 4 (whisking in wine and sugar to the beaten eggs) you've ruined your eggs/sugar/wine/cheese:</p>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/ForgotWineSugar.png" alt="TiramisuDiagram"></p>
<p>As you can see, the stuff we've been whisking and beating was not prepared properly before being whisked and beaten, since we forgot to mix in the wine and sugar. Assuming we don't know enough kitchen chemistry to incorporate the wine/sugar in at this stage (Our eggs may well have turned into omelettes by now without the additional liquid from the wine...) we will need to re-do all the steps in the upper red box.</p>
<p>If you forgot step 2 (dissolving sugar into expresso) you're fine. The expresso hasn't been needed yet:</p>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/ForgotExpresso.png" alt="TiramisuDiagram"></p>
<p>According to the imperative recipe above, we <em>should</em> have done the expresso mixing first before starting on the egg/wine/cheese. But even though we didn't do it, it is trivial to see from the FP-style recipe that there really isn't any loss: no other steps so far depended on that, no other ingredients were ruined.</p>
<hr>
<p>As you can see, many of the questions that were non-trivial to answer when dealing with the imperative code back in <a href="#kitchen-refactoring">Kitchen Refactoring</a> are now trivial to answer when working with the FP-style 2D-block-diagrams.</p>
<p>Again, while nobody actually codes in 2D-block-diagrams (except skilled engineers running recipe blogs) the 2D-block-diagrams are equivalent to a relatively straightforward snippet as shown above. With some experience dealing with FP code, you can often perform the same analyses just as easily when working directly with the equivalent Python code we showed earlier. And it's not just about programmers: automated tools linters or IDEs often perform the same analysis on the fly, as <a href="#preventing-errors-with-functional-programming">shown earlier</a>, quickly alerting you if you make a mistake that means the recipe can no longer be completed successfully:</p>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/FunctionalError.png" alt="TiramisuDiagram"></p><h3 id="the-core-of-functional-programming">The Core of Functional Programming<a href="#the-core-of-functional-programming"></a></h3>
<p><strong>The core of Functional Programming is thinking about data-flow rather than control-flow</strong>. Although, by virtue of editing plain text, you are forced to order your code in a linear sequence of statements, those statements are a thin skin over what you really care about: the shape and structure of the data-flow graph within your program.</p>
<pre><code>def make_tiramisu(eggs, sugar1, wine, cheese, cream, fingers, espresso, sugar2, cocoa):
    beat_eggs = beat(eggs)
    mixture = beat(beat_eggs, sugar1, wine)
    whisked = whisk(mixture)
    beat_cheese = beat(cheese)
    cheese_mixture = beat(whisked, beat_cheese)
    whipped_cream = whip(cream)
    folded_mixture = fold(cheese_mixture, whipped_cream)
    sweet_espresso = dissolve(sugar2, espresso)
    wet_fingers = soak2seconds(fingers, sweet_espresso)
    assembled = assemble(folded_mixture, wet_fingers)
    complete = sift(assembled, cocoa)
    ready_tiramisu = refrigerate(complete)
    return ready_tiramisu
</code></pre>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/CodeGraph.png" alt="TiramisuDiagram"></p> <p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/DiagramGraph.png" alt="TiramisuDiagram"></p>
<p>Similarly, when <em>executing</em> a "functional program" in a single thread, you are forced to pick a linear order in which you execute each individual instruction, which e.g. might be the same as the order in which it is written down in the code. But since we know what <em>really</em> matters is the shape of the data-flow graph, we can freely re-arrange the statements in the code, and the order of execution, as long as the graph shape is preserved. Since the data-flow graph matches the graph of definitions and usages, even your editors and linters understand it enough to warn you if you re-arrange things in an invalid order. In fact, if you have multiple cores (or multiple cooks!) you can execute parts of it in parallel, not in any linear order at all! Exactly in what order the <a href="https://en.wikipedia.org/wiki/Program_counter">program-counter</a> proceeds from instruction to instruction is irrelevant.</p>
<p>This is in contrast to an imperative program, where the exact <em>order</em> in which the program-counter executes each statement, going in and out of loops, in and out of sub-routines, is the key to understanding the program. In an imperative program, you tend to think in terms of steps that must happen "before" and "after", and make sure that the control-flow of the program executes the commands in the right order for the program to work.</p>
<p><strong>Note that none of the FP examples here are "less complex" than the "imperative" recipe we discussed above</strong>. It's about the same number of lines:</p>
<pre><code>def make_tiramisu(eggs, sugar1, wine, cheese, cream, fingers, espresso, sugar2, cocoa):
    dissolve(sugar2, espresso)
    mixture = whisk(eggs)
    beat(mixture, sugar1, wine)
    whisk(mixture) # over steam
    whip(cream)
    beat(cheese)
    beat(mixture, cheese)
    fold(mixture, cream)
    assemble(mixture, fingers)
    sift(mixture, cocoa)
    refrigerate(mixture)
    return mixture # it's now a tiramisu
</code></pre>
<pre><code>def make_tiramisu(eggs, sugar1, wine, cheese, cream, fingers, espresso, sugar2, cocoa):
    beat_eggs = beat(eggs)
    mixture = beat(beat_eggs, sugar1, wine)
    whisked = whisk(mixture)
    beat_cheese = beat(cheese)
    cheese_mixture = beat(whisked, beat_cheese)
    whipped_cream = whip(cream)
    folded_mixture = fold(cheese_mixture, whipped_cream)
    sweet_espresso = dissolve(sugar2, espresso)
    wet_fingers = soak2seconds(fingers, sweet_espresso)
    assembled = assemble(folded_mixture, wet_fingers)
    complete = sift(assembled, cocoa)
    ready_tiramisu = refrigerate(complete)
    return ready_tiramisu
</code></pre>
<p>whether as multiple statements, one big expression, or as a 2D block diagram. All the same operations are present: <code>beat</code>ing, <code>whip</code>ing, <code>fold</code>ing, etc.. Functional Programming is not about hiding ugly code in helper methods and hoping nobody notices: it's about managing the same complexity in a way that makes the dependencies between each piece of code obvious, by following the graph of where function arguments come from and where return values end up.</p>
<p>When you have a working program, having the dependency graph of function return values being passed into other functions as arguments makes it really easy to analyze code. For example, if we were curious what <em>exactly</em> is required to get our <code>wet_fingers_mixture</code>, we can see:</p>
<ul>
  <li><code>wet_fingers</code> comes from <code>soak2seconds(fingers, sweet_espresso)</code></li>
  <li><code>sweet_espresso</code> comes from <code>dissolve(sugar2, espresso)</code></li>
  <li><code>sugar2</code>, <code>fingers</code>, <code>espresso</code> are the initial ingredients of the recipe</li>
</ul>
<p>An there you have it: just a few steps, entirely mechanical, and we can see exactly what <code>wet_fingers</code> needs. We need no understanding of what <code>dissolve</code> does, or what a <code>sugar2</code> is: just from the structure of the code we can already see what <code>wet_fingers</code> requires. Just as importantly, we can also see that it does <em>not</em> depend on <code>folded_mixture</code>, <code>whipped_cream</code>, or any of the other steps that are above it in the code: while those steps "come before" the operations that give us a <code>wet_fingers</code>, it's clear from this analysis that their ordering is entirely accidental, and that we could e.g. prepare the <code>wet_fingers</code> before the other steps if we so desired.</p>
<p>It's not hard to do this yourself, but any IDE with jump-to-definition should be able to do this for you, and so can automated linters and code analysis tools. And understanding the code is the first step in changing it, without bugs.</p>
<p>When you have a broken program, having the dependencies be easy to analyze means it's easier to spot when you make a mistake or do something out of order: even in a dynamic language like python, a subtly bad copy-paste job can get called out by your editor so you can fix it before needing to run any code:</p>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/FunctionalError.png" alt="TiramisuDiagram"></p>
<p>Whether you're working in a dynamic language like Python or a static language like Scala, whether your code is currently working or broken, Functional Programming's data-flow-centric approach helps you understand your code faster, easier and with more tooling help than a Imperative, mutation-heavy approach.</p><h2 id="conclusion">Conclusion<a href="#conclusion"></a></h2>
<p><strong>The core of Functional Programming is thinking about data-flow rather than control-flow</strong></p>
<p><img src="https://www.lihaoyi.com/post/BasicFunctionalProgramming/TiramisuDiagram.png" alt="TiramisuDiagram"></p>
<p>While this may seem a trivial definition of "Functional Programming", I think it is really the core of the idea. While there are many further steps, from the simple (immutability, referential transparency, ...) to the more advanced (monads, lenses, ...) this core of should be something that everyone, from newbies to old hands, whether using Scala or Clojure or Haskell or React.js, should be able to empathise with. Even in a language like Python, as I have used for the examples, it is possible to program in a more "Functional" style, and reap some of the benefits of functional programming.</p>
<p>Those more advanced topics don't really fit this worked example anyway: kitchen ingredients tend to be very, very mutable (and perishable!).</p>
<p>Though it's growing, this baseline-level of FP is not yet widespread in industry. </p>
<ul>
  <li>
  <p>Whole languages, such as Bash, make it a pain in the neck to take  non-trivial function arguments or return non-trivial results, resulting in  people's code writing things to the filesystem, hopefully "before" someone  else needs to read them.</p></li>
  <li>
  <p>Languages like Java encourage  patterns where you instantiate a half-baked object and then set the fields  later, praying that nobody accidentally tries to use it "too early" it in  it's half-baked state while it's internal variables are garbage. </p></li>
</ul>
<p>In all of these cases, the <em>order</em> in which things run - exactly how the program-counter progresses from statement to statement, in and out of for-loops, in and out of sub-routines - is critical. </p>
<p>Even in the kitchen, having a "FP-style" recipe like the block diagram I showed above is helpful, because when the person bringing your Marsala Wine is stuck in traffic, it makes it easier to re-organize your recipe so you can get as much work done immediately. When that person arrives, it helps you figure out how to parallelize the work over the people you have available. When someone screws up, it helps you figure out exactly which ingredients you need to re-purchase and steps you need to re-do. </p>
<p>This widespread applicability, even to fields outside the software world, and to every "FP" language <em>within</em> the software world, is why I think this is truly what functional programming is all about.</p><hr><p><a href="https://www.handsonscala.com/"><img src="https://www.lihaoyi.com/handsonscala-mockup.png"></a></p><p><b>About the Author: </b><i>Haoyi is a software engineer, and the author of many open-source Scala tools such as the Ammonite REPL and the Mill Build Tool. If you enjoyed the contents on this blog, you may also enjoy Haoyi's book <a href="https://www.handsonscala.com/"><b><i>Hands-on Scala Programming</i></b></a></i></p><hr></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: An open-source implementation of AlphaFold3 (251 pts)]]></title>
            <link>https://github.com/Ligo-Biosciences/AlphaFold3</link>
            <guid>41448439</guid>
            <pubDate>Wed, 04 Sep 2024 17:44:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Ligo-Biosciences/AlphaFold3">https://github.com/Ligo-Biosciences/AlphaFold3</a>, See on <a href="https://news.ycombinator.com/item?id=41448439">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">AlphaFold3 Open-Source Implementation</h2><a id="user-content-alphafold3-open-source-implementation" aria-label="Permalink: AlphaFold3 Open-Source Implementation" href="#alphafold3-open-source-implementation"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">This is Ligo's open-source implementation of AlphaFold3, an ongoing research project aimed at advancing open-source biomolecular structure prediction. This release implements the full AlphaFold3 model along with the training code. We are releasing the single chain prediction capability first and we will add ligand, multimer, and nucleic acid prediction capabilities once they are trained. <a href="https://form.fillout.com/t/ct1BWM5QWqus" rel="nofollow">Sign up for beta testing here</a>.</p>
<p dir="auto">This repository is intended to accelerate progress towards a faithful, fully open-source implementation of AlphaFold3 for the entire biotech community to use freely.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo Video</h2><a id="user-content-demo-video" aria-label="Permalink: Demo Video" href="#demo-video"></a></p>
<p dir="auto">We find that the model training dynamics are quite fast. The following video is a sample from a model trained for 4,000 steps on 8 A100 GPUs for 10 hours without templates.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Ligo-Biosciences/AlphaFold3/blob/main/media/AlphaFold3-sample-4_000-steps-training.gif"><img src="https://github.com/Ligo-Biosciences/AlphaFold3/raw/main/media/AlphaFold3-sample-4_000-steps-training.gif" alt="AlphaFold3 Sample" data-animated-image=""></a></p>
<p dir="auto">Animation credits: <a href="https://batisio.co.uk/" rel="nofollow">Matthew Clark</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgments</h2><a id="user-content-acknowledgments" aria-label="Permalink: Acknowledgments" href="#acknowledgments"></a></p>
<p dir="auto">This project would not have been possible without the contributions of the following projects and individuals:</p>
<ul dir="auto">
<li>
<p dir="auto">The AlphaFold3 team at Google DeepMind for their groundbreaking work and publishing the core algorithms.</p>
</li>
<li>
<p dir="auto">The OpenFold project (<a href="https://github.com/aqlaboratory/openfold">https://github.com/aqlaboratory/openfold</a>), which laid the foundation for open-source protein structure prediction. We reuse many of their core modules, such as triangular attention and multiplicative update, as well as their data processing pipelines.</p>
</li>
<li>
<p dir="auto">The ProteinFlow library (<a href="https://github.com/adaptyvbio/ProteinFlow">https://github.com/adaptyvbio/ProteinFlow</a>), especially the architect of ProteinFlow, Liza Kozlova (<a href="https://github.com/elkoz">@elkoz</a>), who has been an absolute hero throughout this process. We trained most of our prototype models on ProteinFlow, since it provides a clean and well-documented data pipeline for working with protein data. We have partnered with AdaptyvBio to build the data pipeline of AlphaFold3 based on ProteinFlow that includes full ligand and nucleic acid support. <a href="https://github.com/elkoz">@elkoz</a> and <a href="https://github.com/igor-krawczuk">@igor-krawczuk</a> are building the next release of ProteinFlow to include full support for these data modalities.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Project Status</h2><a id="user-content-project-status" aria-label="Permalink: Project Status" href="#project-status"></a></p>
<p dir="auto">This is an active research project in its early phases. We are working to prepare a stable release for the community. While we are excited about the potential of this work, we want to emphasise that this is not yet a production-ready tool.
We trained a version of AlphaFold3 on single-chain proteins to test the implementation -- the next release will include full ligand and nucleic acid support.
We are accepting a small number of beta testers to help us test the implementation and provide feedback. If you are interested in beta testing, please <a href="https://form.fillout.com/t/ct1BWM5QWqus" rel="nofollow">join our waitlist</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Discrepancies from AlphaFold3's Pseudocode</h2><a id="user-content-discrepancies-from-alphafold3s-pseudocode" aria-label="Permalink: Discrepancies from AlphaFold3's Pseudocode" href="#discrepancies-from-alphafold3s-pseudocode"></a></p>
<p dir="auto">While working on this project, we discovered a few properties of the algorithms described in the AlphaFold3 supplementary information that were not consistent with surrounding deep learning literature.
We discuss these below:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>MSA Module Order</strong>: In the Supplementary Information, the MSA module communication step occurs before the MSA stack. This results in the MSA stack of the last block not contributing to the structure prediction, since all information flows out through the pair representation. With the order in the pseudocode, the MSA stack in the last block does not have an opportunity to update the pair representation. We swap the OuterProductMean operation and the MSA stack to ensure all blocks contribute to the structure prediction. It is important to note this correction is consistent with the order of operations in the ExtraMSAStack of AlphaFold2. DeepMind mentions these MSA module blocks are "homogeneous". It is unclear whether this means shared weights or same architecture across blocks. If the layers are shared, then gradients will flow through all of them but the final calculation of the MSA stack is idle - this can be safely skipped (not mentioned in the pseudocode). We will resolve this ambiguity in light of DeepMind's response.</p>
</li>
<li>
<p dir="auto"><strong>Loss scaling</strong>: The loss scaling factor described in the Supplementary Information does not give unit-loss at initialization. Unit-loss at initialization is one of the properties that Karras et al. (2022) set as a desirable property of the loss function when training diffusion models, and Max Jaderberg mentions this as one of the properties for why they chose the framework of Karras et al. in this talk <a href="https://youtu.be/AE35XCN5NuU?si=S_9-i3hupk3i9GDR" rel="nofollow">here</a>. We think this is a simple typo in the Supplementary info that is due to an addition being typed as a multiplication -- in our implementation, we use the loss scaling factor consistent with Karras et al. (2022). Our measurements show that this gives unit MSE loss at initialization, whilst the scaling in the Supplementary Information is two to three orders of magnitude larger at initialization. Additionally, the loss scaling factor in the paper has a local minimum at t = 16.0, but then it increases with increasing noise level. This is not in line with the properties of the loss function that Karras et al. (2022) proposed, which emphasises the importance of downweighting the loss at higher noise levels. We add a Jupyter notebook to the repository showing our experiments.</p>
</li>
<li>
<p dir="auto"><strong>DiT block design</strong>: The design of the AttentionPairBias and the DiffusionTransformer blocks seem to closely follow the DiT block design introduced by Peebles &amp; Xie (2022) <a href="https://arxiv.org/abs/2212.09748" rel="nofollow">here</a>. However, the residual connections are missing. It is not explained in the paper why DeepMind chose to omit them. We experiment with both and find that (within the range of steps we trained our models on) the DiT block with residual connections gives much faster convergence and better gradient flow through the network. Note that this is the discrepancy we are the least sure about, and it can be changed in a couple lines in our code if the original implementation does not use the residual connections.</p>
</li>
</ul>
<p dir="auto">These are noted here for transparency and to invite community input on the best approaches to resolve them.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Model Efficiency</h2><a id="user-content-model-efficiency" aria-label="Permalink: Model Efficiency" href="#model-efficiency"></a></p>
<p dir="auto">A significant focus of this implementation has been on optimising the model components for speed and memory efficiency. AlphaFold3 has many transformer-like components, but efficient hardware-aware attention implementations like FlashAttention2 do not integrate out-of-the-box with these modules due to pair biasing in AlphaFold3. All of the attention operations project a pair bias from the pairwise representation that is added after the key-query dot product, and the bias requires a gradient to be backpropagated. This is not out of scope for FlashAttention2, since the bias gradient would have the same gradient as the scaled QK^T dot product, but the current implementation does not support this. More recent attention implementations like <a href="https://pytorch.org/blog/flexattention/" rel="nofollow">FlexAttention</a> are very promising, but they also do not support a bias gradient for now since broadcasting operations of the bias tensor during the forward pass become reductions in the backward pass, and this functionality is not implemented in the first release of FlexAttention.</p>
<ul dir="auto">
<li>
<p dir="auto">We reuse battle-tested components such as TriangularAttention and TriangularMultiplicativeUpdate from the OpenFold project wherever we can. The modular design of the OpenFold project allows us to easily import these modules into our codebase. We are working on improving the efficiency of these modules with Triton, fusing operations to increase performance and reduce intermediate tensor allocation.</p>
</li>
<li>
<p dir="auto">We observed that a naive implementation of the Diffusion Module in PyTorch frequently ran out of memory since the Diffusion Module is replicated 48 times per batch. To solve this issue, we re-purpose the MSARowAttentionWithPairBias kernel from Deepspeed4Science to implement a memory-efficient version of the Diffusion Module, treating the batch replicas with different noise levels as an additional batch dimension. For the AtomAttentionEncoder and AtomAttentionDecoder modules, we experimented with a custom PyTorch-native implementation to reduce the memory footprint from quadratic to linear, but the benefits were not that significant compared to a naive re-purposing of the AttentionPairBias kernel. We include both implementations in the repository, but use the naive implementation for the sake of reducing clutter.
Despite these optimisations, our profiling experiments show that over 60% of the model's operations are memory-bound. We are working on a far more efficient and scalable implementation using the ideas of <a href="https://paperswithcode.com/paper/scalefold-reducing-alphafold-initial-training" rel="nofollow">ScaleFold</a>, which will allow us to reach the training scale of the original AlphaFold3.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">We do not yet provide sampling code since the ligand-protein and nucleic acid prediction capabilities are yet to be trained. The checkpoint weights can be loaded with PyTorch Lightning's checkpoint loading for experimentation and model surgery. The current model only predicts single-chain proteins, which is the same functionality as the original AlphaFold2. The model components are written to be reusable and modular so that researchers can easily incorporate them into their own projects.
For beta testing of ligand-protein and nucleic acid prediction: <a href="https://form.fillout.com/t/ct1BWM5QWqus" rel="nofollow">Join our Waitlist</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">For now, the primary use of this repository is for research and development. We will include more user-facing functionality in the future once the ligand-protein and nucleic acid prediction capabilities are ready.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We welcome contributions from the community! There are likely numerous bugs and subtle implementation errors in our code. Deep learning training often fails silently, where the errors still allow the network to converge but make it work slightly worse. If you're interested in contributing, you can raise a Github issue with a bug description or fork the repository, create a new branch with your corrections and submit a pull request with a clear description of your changes.</p>
<p dir="auto">For any other comments or suggestions please contact us via email at <a href="mailto:alphafold3@ligo.bio">alphafold3@ligo.bio</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citations</h2><a id="user-content-citations" aria-label="Permalink: Citations" href="#citations"></a></p>
<p dir="auto">If you use this code in your research, please cite the following papers:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@article{Abramson2024-fj,
  title    = &quot;Accurate structure prediction of biomolecular interactions with
              {AlphaFold} 3&quot;,
  author   = &quot;Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans,
              Richard and Green, Tim and Pritzel, Alexander and Ronneberger,
              Olaf and Willmore, Lindsay and Ballard, Andrew J and Bambrick,
              Joshua and Bodenstein, Sebastian W and Evans, David A and Hung,
              Chia-Chun and O'Neill, Michael and Reiman, David and
              Tunyasuvunakool, Kathryn and Wu, Zachary and {\v Z}emgulyt{\.e},
              Akvil{\.e} and Arvaniti, Eirini and Beattie, Charles and
              Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and
              Congreve, Miles and Cowen-Rivers, Alexander I and Cowie, Andrew
              and Figurnov, Michael and Fuchs, Fabian B and Gladman, Hannah and
              Jain, Rishub and Khan, Yousuf A and Low, Caroline M R and Perlin,
              Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and
              Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine
              and Yakneen, Sergei and Zhong, Ellen D and Zielinski, Michal and
              {\v Z}{\'\i}dek, Augustin and Bapst, Victor and Kohli, Pushmeet
              and Jaderberg, Max and Hassabis, Demis and Jumper, John M&quot;,
  journal  = &quot;Nature&quot;,
  month    = &quot;May&quot;,
  year     =  2024
}"><pre><span>@article</span>{<span>Abramson2024-fj</span>,
  <span>title</span>    = <span><span>"</span>Accurate structure prediction of biomolecular interactions with</span>
<span>              {AlphaFold} 3<span>"</span></span>,
  <span>author</span>   = <span><span>"</span>Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans,</span>
<span>              Richard and Green, Tim and Pritzel, Alexander and Ronneberger,</span>
<span>              Olaf and Willmore, Lindsay and Ballard, Andrew J and Bambrick,</span>
<span>              Joshua and Bodenstein, Sebastian W and Evans, David A and Hung,</span>
<span>              Chia-Chun and O'Neill, Michael and Reiman, David and</span>
<span>              Tunyasuvunakool, Kathryn and Wu, Zachary and {\v Z}emgulyt{\.e},</span>
<span>              Akvil{\.e} and Arvaniti, Eirini and Beattie, Charles and</span>
<span>              Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and</span>
<span>              Congreve, Miles and Cowen-Rivers, Alexander I and Cowie, Andrew</span>
<span>              and Figurnov, Michael and Fuchs, Fabian B and Gladman, Hannah and</span>
<span>              Jain, Rishub and Khan, Yousuf A and Low, Caroline M R and Perlin,</span>
<span>              Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and</span>
<span>              Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine</span>
<span>              and Yakneen, Sergei and Zhong, Ellen D and Zielinski, Michal and</span>
<span>              {\v Z}{\'\i}dek, Augustin and Bapst, Victor and Kohli, Pushmeet</span>
<span>              and Jaderberg, Max and Hassabis, Demis and Jumper, John M<span>"</span></span>,
  <span>journal</span>  = <span><span>"</span>Nature<span>"</span></span>,
  <span>month</span>    = <span><span>"</span>May<span>"</span></span>,
  <span>year</span>     =  <span>2024</span>
}</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="@article {Ahdritz2022.11.20.517210,
	author = {Ahdritz, Gustaf and Bouatta, Nazim and Floristean, Christina and Kadyan, Sachin and Xia, Qinghui and Gerecke, William and O{\textquoteright}Donnell, Timothy J and Berenberg, Daniel and Fisk, Ian and Zanichelli, Niccolò and Zhang, Bo and Nowaczynski, Arkadiusz and Wang, Bei and Stepniewska-Dziubinska, Marta M and Zhang, Shang and Ojewole, Adegoke and Guney, Murat Efe and Biderman, Stella and Watkins, Andrew M and Ra, Stephen and Lorenzo, Pablo Ribalta and Nivon, Lucas and Weitzner, Brian and Ban, Yih-En Andrew and Sorger, Peter K and Mostaque, Emad and Zhang, Zhao and Bonneau, Richard and AlQuraishi, Mohammed},
	title = {{O}pen{F}old: {R}etraining {A}lpha{F}old2 yields new insights into its learning mechanisms and capacity for generalization},
	elocation-id = {2022.11.20.517210},
	year = {2022},
	doi = {10.1101/2022.11.20.517210},
	publisher = {Cold Spring Harbor Laboratory},
	URL = {https://www.biorxiv.org/content/10.1101/2022.11.20.517210},
	eprint = {https://www.biorxiv.org/content/early/2022/11/22/2022.11.20.517210.full.pdf},
	journal = {bioRxiv}
}"><pre><span>@article</span> {<span>Ahdritz2022.11.20.517210</span>,
	<span>author</span> = <span><span>{</span>Ahdritz, Gustaf and Bouatta, Nazim and Floristean, Christina and Kadyan, Sachin and Xia, Qinghui and Gerecke, William and O{\textquoteright}Donnell, Timothy J and Berenberg, Daniel and Fisk, Ian and Zanichelli, Niccolò and Zhang, Bo and Nowaczynski, Arkadiusz and Wang, Bei and Stepniewska-Dziubinska, Marta M and Zhang, Shang and Ojewole, Adegoke and Guney, Murat Efe and Biderman, Stella and Watkins, Andrew M and Ra, Stephen and Lorenzo, Pablo Ribalta and Nivon, Lucas and Weitzner, Brian and Ban, Yih-En Andrew and Sorger, Peter K and Mostaque, Emad and Zhang, Zhao and Bonneau, Richard and AlQuraishi, Mohammed<span>}</span></span>,
	<span>title</span> = <span><span>{</span>{O}pen{F}old: {R}etraining {A}lpha{F}old2 yields new insights into its learning mechanisms and capacity for generalization<span>}</span></span>,
	<span>elocation-id</span> = <span><span>{</span>2022.11.20.517210<span>}</span></span>,
	<span>year</span> = <span><span>{</span>2022<span>}</span></span>,
	<span>doi</span> = <span><span>{</span>10.1101/2022.11.20.517210<span>}</span></span>,
	<span>publisher</span> = <span><span>{</span>Cold Spring Harbor Laboratory<span>}</span></span>,
	<span>URL</span> = <span><span>{</span>https://www.biorxiv.org/content/10.1101/2022.11.20.517210<span>}</span></span>,
	<span>eprint</span> = <span><span>{</span>https://www.biorxiv.org/content/early/2022/11/22/2022.11.20.517210.full.pdf<span>}</span></span>,
	<span>journal</span> = <span><span>{</span>bioRxiv<span>}</span></span>
}</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="@article{kozlova_2023_proteinflow,
  author = {Kozlova, Elizaveta and Valentin, Arthur and Khadhraoui, Aous and Gutierrez, Daniel Nakhaee-Zadeh},
  month = {09},
  title = {ProteinFlow: a Python Library to Pre-Process Protein Structure Data for Deep Learning Applications},
  doi = {https://doi.org/10.1101/2023.09.25.559346},
  year = {2023},
  journal = {bioRxiv}
}"><pre><span>@article</span>{<span>kozlova_2023_proteinflow</span>,
  <span>author</span> = <span><span>{</span>Kozlova, Elizaveta and Valentin, Arthur and Khadhraoui, Aous and Gutierrez, Daniel Nakhaee-Zadeh<span>}</span></span>,
  <span>month</span> = <span><span>{</span>09<span>}</span></span>,
  <span>title</span> = <span><span>{</span>ProteinFlow: a Python Library to Pre-Process Protein Structure Data for Deep Learning Applications<span>}</span></span>,
  <span>doi</span> = <span><span>{</span>https://doi.org/10.1101/2023.09.25.559346<span>}</span></span>,
  <span>year</span> = <span><span>{</span>2023<span>}</span></span>,
  <span>journal</span> = <span><span>{</span>bioRxiv<span>}</span></span>
}</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{ahdritz2023openproteinset,
      title={{O}pen{P}rotein{S}et: {T}raining data for structural biology at scale}, 
      author={Gustaf Ahdritz and Nazim Bouatta and Sachin Kadyan and Lukas Jarosch and Daniel Berenberg and Ian Fisk and Andrew M. Watkins and Stephen Ra and Richard Bonneau and Mohammed AlQuraishi},
      year={2023},
      eprint={2308.05326},
      archivePrefix={arXiv},
      primaryClass={q-bio.BM}
}"><pre><span>@misc</span>{<span>ahdritz2023openproteinset</span>,
      <span>title</span>=<span><span>{</span>{O}pen{P}rotein{S}et: {T}raining data for structural biology at scale<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Gustaf Ahdritz and Nazim Bouatta and Sachin Kadyan and Lukas Jarosch and Daniel Berenberg and Ian Fisk and Andrew M. Watkins and Stephen Ra and Richard Bonneau and Mohammed AlQuraishi<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2023<span>}</span></span>,
      <span>eprint</span>=<span><span>{</span>2308.05326<span>}</span></span>,
      <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>,
      <span>primaryClass</span>=<span><span>{</span>q-bio.BM<span>}</span></span>
}</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="@article{Peebles2022DiT,
  title={Scalable Diffusion Models with Transformers},
  author={William Peebles and Saining Xie},
  year={2022},
  journal={arXiv preprint arXiv:2212.09748},
}"><pre><span>@article</span>{<span>Peebles2022DiT</span>,
  <span>title</span>=<span><span>{</span>Scalable Diffusion Models with Transformers<span>}</span></span>,
  <span>author</span>=<span><span>{</span>William Peebles and Saining Xie<span>}</span></span>,
  <span>year</span>=<span><span>{</span>2022<span>}</span></span>,
  <span>journal</span>=<span><span>{</span>arXiv preprint arXiv:2212.09748<span>}</span></span>,
}</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{Karras2022edm,
  author    = {Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},
  title     = {Elucidating the Design Space of Diffusion-Based Generative Models},
  booktitle = {Proc. NeurIPS},
  year      = {2022}
}"><pre><span>@inproceedings</span>{<span>Karras2022edm</span>,
  <span>author</span>    = <span><span>{</span>Tero Karras and Miika Aittala and Timo Aila and Samuli Laine<span>}</span></span>,
  <span>title</span>     = <span><span>{</span>Elucidating the Design Space of Diffusion-Based Generative Models<span>}</span></span>,
  <span>booktitle</span> = <span><span>{</span>Proc. NeurIPS<span>}</span></span>,
  <span>year</span>      = <span><span>{</span>2022<span>}</span></span>
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the Apache License 2.0 - see the <a href="https://github.com/Ligo-Biosciences/AlphaFold3/blob/main/LICENSE.txt">LICENSE</a> file for details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The coming long-run slowdown in corporate profit growth and stock returns [pdf] (2023) (125 pts)]]></title>
            <link>https://www.federalreserve.gov/econres/feds/files/2023041pap.pdf</link>
            <guid>41448139</guid>
            <pubDate>Wed, 04 Sep 2024 17:13:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.federalreserve.gov/econres/feds/files/2023041pap.pdf">https://www.federalreserve.gov/econres/feds/files/2023041pap.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=41448139">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Dynamicland 2024 (497 pts)]]></title>
            <link>https://dynamicland.org/</link>
            <guid>41448022</guid>
            <pubDate>Wed, 04 Sep 2024 17:02:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dynamicland.org/">https://dynamicland.org/</a>, See on <a href="https://news.ycombinator.com/item?id=41448022">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Internet Archive has lost its appeal in Hachette vs. Internet Archive (723 pts)]]></title>
            <link>https://storage.courtlistener.com/recap/gov.uscourts.ca2.60988/gov.uscourts.ca2.60988.306.1.pdf</link>
            <guid>41447758</guid>
            <pubDate>Wed, 04 Sep 2024 16:41:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://storage.courtlistener.com/recap/gov.uscourts.ca2.60988/gov.uscourts.ca2.60988.306.1.pdf">https://storage.courtlistener.com/recap/gov.uscourts.ca2.60988/gov.uscourts.ca2.60988.306.1.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=41447758">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The first nuclear clock will test if fundamental constants change (233 pts)]]></title>
            <link>https://www.quantamagazine.org/the-first-nuclear-clock-will-test-if-fundamental-constants-change-20240904/</link>
            <guid>41447515</guid>
            <pubDate>Wed, 04 Sep 2024 16:23:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/the-first-nuclear-clock-will-test-if-fundamental-constants-change-20240904/">https://www.quantamagazine.org/the-first-nuclear-clock-will-test-if-fundamental-constants-change-20240904/</a>, See on <a href="https://news.ycombinator.com/item?id=41447515">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="postContent">
            
            <div id="postBody">
                <div>
        <p>
            An ultra-precise measurement of a transition in the hearts of thorium atoms gives physicists a tool to probe the forces that bind the universe.        </p>
        
    </div>
    <figure>
        <div>
                            <p><img width="2560" height="1440" src="https://www.quantamagazine.org/wp-content/uploads/2024/09/NuclearClock-crNashWeerasekera-Lede-scaled.webp" alt="Illustration of a deconstructed clock where one metal layer is imprinted to look like a thorium atom." decoding="async" loading="lazy" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/09/NuclearClock-crNashWeerasekera-Lede-scaled.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2024/09/NuclearClock-crNashWeerasekera-Lede-1720x968.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2024/09/NuclearClock-crNashWeerasekera-Lede-520x293.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2024/09/NuclearClock-crNashWeerasekera-Lede-768x432.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2024/09/NuclearClock-crNashWeerasekera-Lede-1536x864.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2024/09/NuclearClock-crNashWeerasekera-Lede-2048x1152.webp 2048w" sizes="(max-width: 2560px) 100vw, 2560px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>The discovery of a laser-controllable transition in the atomic nucleus of thorium-229 marks the dawn of the “nuclear clock.”</p>
            <p>Nash Weerasekera for&nbsp;<em>Quanta Magazine</em></p>
        </div>
</figcaption>
    </figure>
<div>
            <h2>Introduction</h2>
            <div data-role="selectable">
    <p>At 11:30 one night in May 2024, a graduate student, Chuankun Zhang, saw a signal that physicists have sought for 50 years. As a peak rose from the static on his monitor at the research institute JILA in Boulder, Colorado, Zhang dropped a screenshot in a group chat with his three lab mates. One by one they hopped out of bed and trickled in. After several sanity checks to make sure that what they were looking at was real — a signal from a thorium-229 nucleus switching between two states, known as the “nuclear clock” transition — the young researchers took a selfie to commemorate the moment. Time stamp: 3:42 a.m.</p>
<p>At their weekly meeting later that morning with their group leader, <a href="https://www.colorado.edu/physics/jun-ye">Jun Ye</a>, builder of the world’s most precise atomic clock, they decided to play it cool. “They were all poker-faced,” Ye said, until Zhang shared a slide displaying the long-sought peak. Tears flooded Ye’s eyes as the group clinked glasses of champagne at 9:30 a.m.</p>
<p>The group’s measurement, <a href="https://www.nature.com/articles/s41586-024-07839-6">reported</a> on September 4, 2024, in the journal <em>Nature</em>, is the third observation of the thorium-229 transition published within the last four months, coming on the heels of results from Germany and California. But the new measurement is millions of times more precise than the others, and it marks the end of a marathon search for the exact laser frequency needed to induce the nuclear clock transition. “This paper is an incredible technical achievement,” said <a href="https://www.durham.ac.uk/staff/hannah-williams4/">Hannah Williams</a>, a physicist at Durham University in the United Kingdom who was not involved in the work.</p>
<p>More importantly, it launches a new effort: Researchers will now try to use the transition to observe whether the laws of physics vary over time, as predicted by many theories of fundamental physics. Thanks to an apparently accidental, nearly exact cancellation of two of nature’s four forces in the thorium-229 nucleus, the nuclear clock transition is extremely sensitive to changes in these forces. Measuring this thorium-229 transition at different times could therefore reveal any variability in the fundamental constants of physics.</p>
<p>“I see it as the beginning of a beautiful journey,” said <a href="https://perimeterinstitute.ca/people/asimina-arvanitaki">Asimina Arvanitaki</a>, a theoretical physicist at the Perimeter Institute for Theoretical Physics in Canada who was also not involved. “Now we’ve measured this freak of nature. But in order to use its freakiness, a lot of work needs to be done.”</p>
<h2><strong>Freak of Nature</strong></h2>
<p>Scientists realized there’s something special about the isotope thorium-229 back in 1976, when they first studied this byproduct of Cold War nuclear weapons research.</p>
<p>Atoms are ordinarily in what’s called the ground state, in which all the electrons orbit the nucleus in a stable way. But an electron can also absorb energy from the outside world in the form of a photon and become excited, zip about the atom more quickly for a moment, then reemit the photon and return to the ground state. The photon has to have just the right amount — or “quantum” — of energy to excite the electron.</p>
<p>The modern notion of time is actually defined by this process. Scientists use a laser to bathe a cesium atom with photons. Then they vary the laser’s wavelength until its photons each have just the right energy to excite an electron. This ultra-precise wavelength then defines the international standard for a second, which is the time it takes for 9,192,631,770 of those wavelengths to pass a given point in space.</p>
</div>
    </div>
    <figure>
        <div>
                    <p><img width="1354" height="2560" src="https://www.quantamagazine.org/wp-content/uploads/2024/09/Ye-Zhang-crGeoffreyWheeler_KennaHughes-Castleberry_JILA-3-scaled.webp" alt="" decoding="async" loading="lazy" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/09/Ye-Zhang-crGeoffreyWheeler_KennaHughes-Castleberry_JILA-3-scaled.webp 1354w, https://www.quantamagazine.org/wp-content/uploads/2024/09/Ye-Zhang-crGeoffreyWheeler_KennaHughes-Castleberry_JILA-3-910x1720.webp 910w, https://www.quantamagazine.org/wp-content/uploads/2024/09/Ye-Zhang-crGeoffreyWheeler_KennaHughes-Castleberry_JILA-3-275x520.webp 275w, https://www.quantamagazine.org/wp-content/uploads/2024/09/Ye-Zhang-crGeoffreyWheeler_KennaHughes-Castleberry_JILA-3-768x1452.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2024/09/Ye-Zhang-crGeoffreyWheeler_KennaHughes-Castleberry_JILA-3-812x1536.webp 812w, https://www.quantamagazine.org/wp-content/uploads/2024/09/Ye-Zhang-crGeoffreyWheeler_KennaHughes-Castleberry_JILA-3-1083x2048.webp 1083w" sizes="(max-width: 1354px) 100vw, 1354px"><img width="2560" height="1096" src="https://www.quantamagazine.org/wp-content/uploads/2024/09/Ye-Zhang-cr.Geoffrey-Wheeler-Kenna-Hughes-Castleberry_JILA-Mobile-1-scaled.webp" alt="" decoding="async" loading="lazy" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/09/Ye-Zhang-cr.Geoffrey-Wheeler-Kenna-Hughes-Castleberry_JILA-Mobile-1-scaled.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2024/09/Ye-Zhang-cr.Geoffrey-Wheeler-Kenna-Hughes-Castleberry_JILA-Mobile-1-1720x736.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2024/09/Ye-Zhang-cr.Geoffrey-Wheeler-Kenna-Hughes-Castleberry_JILA-Mobile-1-520x223.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2024/09/Ye-Zhang-cr.Geoffrey-Wheeler-Kenna-Hughes-Castleberry_JILA-Mobile-1-768x329.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2024/09/Ye-Zhang-cr.Geoffrey-Wheeler-Kenna-Hughes-Castleberry_JILA-Mobile-1-1536x658.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2024/09/Ye-Zhang-cr.Geoffrey-Wheeler-Kenna-Hughes-Castleberry_JILA-Mobile-1-2048x877.webp 2048w" sizes="(max-width: 2560px) 100vw, 2560px"></p><figcaption>
    <div>
                            <p>A team in the lab of Jun Ye (top), led by graduate student Chuankun Zhang (bottom), published an ultra-precise measurement of the nuclear clock transition in the journal <i data-stringify-type="italic">Nature</i>.</p>
                    </div>
</figcaption>
                </div>
        <figcaption>
    <div>
                            <p>A team in the lab of Jun Ye (left), led by graduate student Chuankun Zhang (right), published an ultra-precise measurement of the nuclear clock transition in the journal <i data-stringify-type="italic">Nature</i>.</p>
            <p>From left: Geoffrey Wheeler; Kenna Hughes-Castleberry/JILA</p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>Nuclei, the tight balls of neutrons and protons at every atom’s core, also have ground and excited states, in which one of their constituent protons or neutrons absorbs a photon and briefly swirls about more energetically. But these particles are packed much more tightly than electrons, so it takes much more energetic photons — gamma rays — to excite them. Those are much harder to produce in large quantities or with a precise energy.</p>
<p>The thorium-229 nucleus, however, is different.</p>
<p>From the 1950s to the 1970s, the United States produced about two tons of uranium-233, a weapons-grade fissile material that was being investigated as a possible alternative to uranium-235 and plutonium-239 in atomic weapons research. The program was eventually scrapped, leaving only some tanks of radioactive liquid behind. But when the nuclear physicists Larry Kroger and Charles Reich at Idaho National Laboratory <a href="https://www.sciencedirect.com/science/article/abs/pii/0375947476904942">studied the radiation</a> emanating from that liquid in 1976, they found indirect evidence that uranium-233’s “daughter” nucleus (the product of its radioactive decay), thorium-229, had a mysterious excited nuclear state that involved far less energy than expected.</p>
<p>Every nucleus lives in a tense tug-of-war between two of nature’s forces. The electromagnetic force between its positively charged protons tries to rip it apart, while the strong force holds the bundle together. Exciting a neutron or proton causes the nucleus to settle into a new, more energetic equilibrium between the two forces.</p>
<p>The Idaho researchers observed that reversing the intrinsic angular momentum, or “spin,” of thorium-229’s outermost neutron seemed to take 10,000 times less energy than a typical nuclear excitation. The neutron’s altered spin slightly changes both the electromagnetic and strong forces, but those changes happen to cancel each other out almost exactly. Consequently, the excited nuclear state barely differs from the ground state. Lots of nuclei have similar spin transitions, but only in thorium-229 is this cancellation so nearly perfect.</p>
<p>“It’s accidental,” said <a href="https://www.unsw.edu.au/staff/victor-flambaum">Victor Flambaum</a>, a theoretical physicist at the University of New South Wales in Sydney. “A priori, there is no special reason for thorium. It’s just experimental fact.” But this accident of forces and energy has big consequences.</p>
<h2><strong>Clocking the Constants</strong></h2>
<p>It took decades for scientists to realize just how special thorium-229 is, and what to do with it.</p>
<p>Kroger and Reich’s 1976 measurement had been imprecise, because it was carried out in the noisy bath of radiation that uranium-233 waste produces. They couldn’t see the actual low-energy photons released when nuclei decayed to the ground state; they only inferred the energy indirectly from the pattern of more powerful gamma radiation emitted by more excited nuclei.</p>
<p>In 1990, Reich and a colleague <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.64.271">redid this measurement</a> more carefully and found that the excited state’s energy was even smaller — more than 10 times smaller — than they initially thought. Whereas nuclear transitions often take millions of electron-volts, thorium-229 takes less than 10. This was a true game changer: No other isotope has a nuclear transition in the energy range of conventional lasers, which can deliver the energy to trigger the transition reliably and precisely. “In the whole chart of all the nuclei, it’s the only one,” said <a href="https://hudsongroup.physics.ucla.edu/content/people">Eric Hudson</a>, a physicist at the University of California, Los Angeles.</p>
</div>
    <figure>
        <div>
                            <p><img width="1600" height="1003" src="https://www.quantamagazine.org/wp-content/uploads/2024/09/EricHudson_crDavidEsquivel_UCLA.webp" alt="Portrait of a man with green lasers shining behind him." decoding="async" loading="lazy" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/09/EricHudson_crDavidEsquivel_UCLA.webp 1600w, https://www.quantamagazine.org/wp-content/uploads/2024/09/EricHudson_crDavidEsquivel_UCLA-520x326.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2024/09/EricHudson_crDavidEsquivel_UCLA-768x481.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2024/09/EricHudson_crDavidEsquivel_UCLA-1536x963.webp 1536w" sizes="(max-width: 1600px) 100vw, 1600px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>Eric Hudson’s team at the University of California, Los Angeles reported a measurement of the nuclear clock transition over the summer.</p>
            <p>David Esquivel/UCLA</p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>If someone could isolate these nuclei from that radioactive environment and match an ultraviolet laser’s energy to that excited state, they could trigger it at will, just as they can with an electron.</p>
<p>The vast majority of the government’s uranium-233 “waste” still sat in guarded rooms at the Idaho and Oak Ridge labs. “Their budget was something like $20 million a year to just sit there and watch the stuff so nobody comes in and steals it,” said <a href="https://www.ornl.gov/our-people/saed-mirzadeh">Saed Mirzadeh</a>, a radiochemist who worked for 31 years at Oak Ridge. “They’d just sit there and smoke their cigarettes with guns around their necks.”</p>
<p>In 1994, Mirzadeh, who knew about the Idaho team’s work, convinced the lab to give him access to the languishing vats of dangerous liquid. He developed a method to separate the uranium atoms that had already decayed into thorium-229 from those that hadn’t. “The first time we actually did it, there were guards with machine guns outside the lab,” he said. Most of the world’s existing stock of thorium-229, he noted, comes from his efforts.</p>
<p>Ideas began to arise for how to utilize such a unique nucleus. In 2003, Ekkehard Peik and Christian Tamm at the Federal Physical and Technical Institute (PTB), Germany’s metrology institute, <a href="https://iopscience.iop.org/article/10.1209/epl/i2003-00210-x/meta">proposed</a> using it to build a nuclear clock. Since nuclei are shielded from the outside world by their clouds of electrons, they realized, a clock based on thorium-229 atoms would be immune to much of the background interference that plagued the best atomic clocks at the time.</p>
<p>Then Flambaum <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.97.092502">showed</a> that such a sensitive, isolated clock could be used to test the constancy of nature itself.</p>
</div>
    <figure>
        <div>
                            <p><img width="1614" height="1170" src="https://www.quantamagazine.org/wp-content/uploads/2024/09/EkkehardPeik_crPhysikalisch-TechnischeBundesanstalt.webp" alt="A man standing in front of an optical setup with his arms crossed smiles at the camera." decoding="async" loading="lazy" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/09/EkkehardPeik_crPhysikalisch-TechnischeBundesanstalt.webp 1614w, https://www.quantamagazine.org/wp-content/uploads/2024/09/EkkehardPeik_crPhysikalisch-TechnischeBundesanstalt-520x377.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2024/09/EkkehardPeik_crPhysikalisch-TechnischeBundesanstalt-768x557.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2024/09/EkkehardPeik_crPhysikalisch-TechnischeBundesanstalt-1536x1113.webp 1536w" sizes="(max-width: 1614px) 100vw, 1614px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>Ekkehard Peik at the Federal Physical and Technical Institute in Germany and his collaborators were the first to excite the nuclear clock transition with a laser earlier this year.</p>
            <p>Physikalisch-Technische Bundesanstalt</p>
        </div>
</figcaption>
    </figure>
<div data-role="selectable">
    <p>Physicists have developed equations to characterize the forces that bind the universe, and these equations are fitted with some 26 numbers called fundamental constants. These numbers, such as the speed of light or the gravitational constant, define how everything works in our universe. But lots of physicists think the numbers might not actually be constant.</p>
<p>Theoretical ideas like string theory that try to build a deeper, more complete understanding of where forces come from often predict that these numbers, even the speed of light, change ever so slightly over time. In other words, the constants may result from underlying phenomena or processes that are themselves dynamic. This is also predicted by one of the most popular theories of dark matter, the invisible substance that floats in and around galaxies. If dark matter is made up of wavelike particles called axions, then the varying density of axions from place to place should cause the strength of some of the forces to wiggle up and down.</p>
<p>These small tweaks to nature’s laws could slightly disrupt the delicate balancing act that takes place inside every atom’s nucleus, altering the energies of its states. The energies of nuclear states come from adding and subtracting the huge electromagnetic and strong forces acting on all the protons and neutrons. Even a relatively small change in the strength of one of these forces would result in a substantial shift in energy. The shift would be especially noticeable when applied to the thorium-229 transition’s remarkably tiny energy.</p>
<p>Over the 2000s and 2010s, several teams entered the race to build the first nuclear clock. To win, they needed to figure out the exact energy a laser would need to excite the nuclear state in question, now called the nuclear clock transition.</p>
<h2><strong>Photo Finish</strong></h2>
<p>The existing estimate of the energy required for the nuclear clock transition was a thousand times less precise than the wavelengths of the lasers that researchers were trying to probe it with. So there were thousands of laser wavelengths to rule out. After tuning a laser to one of these wavelengths, researchers had to trap a few thorium-229 atoms, hit them with the laser, then wait for photons showing they’d excited the state. This process of elimination was simply going to take too long.</p>

<p>Following Hudson’s lead, groups <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.104.200802">began building</a> solid crystal compounds with the thorium embedded inside — an approach mentioned in Peik and Tamm’s original proposal. The crystals can hold quadrillions of atoms instead of just a few, so a laser could rule out wavelengths at a rapid clip.</p>
<p>A <a href="https://www.nature.com/articles/s41586-023-05894-z">breakthrough</a> last year at CERN kicked the race into overdrive. As in the older Idaho studies, the CERN team produced excited thorium-229 through radioactive decay, then looked at the photons coming out. But they found a way to do so in a much quieter environment, which enabled them to directly measure the faint rays of ultraviolet light coming from the nuclear clock transition and put a tighter estimate on the transition energy.</p>
<p>The CERN team’s updated estimate narrowed the wavelength hunters’ search from an entire forest to a small copse of trees, which they immediately began scouring. In April of this year, a European team became the <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.132.182501">first to report</a> that they had probed the state with a laser. Peik contributed his laser expertise, and the collaboration made use of a crystal-growing powerhouse built by the physicist Thorsten Schumm at the University of Vienna.</p>
<p>Hudson’s group was right on their heels — <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.133.013201">a paper</a> reporting their discovery ran in <em>Physical Review Letters</em> in July.</p>
<p>Ye’s group at JILA had also obtained one of Schumm’s crystals and was racing to excite the thorium-229 transition as well. For years, the group has been using its clock-building acumen to engineer a special ultraviolet laser with the sole purpose of turning thorium-229 into a nuclear clock. The laser allows Ye and his group to test many wavelengths at once to close in on any transition he seeks. His team’s new paper caps this trio of parallel discoveries with what will likely be the most precise measurement of the state’s energy for years to come.</p>
<p>“These results have all come out in a very short period of time,” Williams said, “so that is very exciting as to what they’re going to do next.”</p>
        
        
<p>The result starts the clock on thorium’s test of nature’s forces. “Now the fun starts,” Hudson said, excited to put the new tool to use studying fundamental constants. “We can actually do this stuff.”</p>
<p>The thorium nuclear state’s energy is far more sensitive to variations in the fundamental constants than that of any atomic state. But scientists will need to improve the precision of their measurements even further to notice changes more subtle than those already ruled out by conventional atomic clocks. Currently, Ye can measure the nuclear clock transition with a precision of one part in a trillion, but possible variations would be as small as one part in 10 trillion. “It’s many years down the road,” he said.</p>
<p>Eventually, though, some old Cold War byproducts could yield the first evidence for deeper, still undiscovered physics that underlies the universe we see. “We call them constants, but why?” Hudson asked. “Nothing is ever that simple when you zoom in and look at it.”</p>
</div>
                
                
            </div>
                <div id="newsletter">
            <p>
                The Quanta Newsletter            </p>
                            <p>
                    <em>Get highlights of the most important news delivered to your email inbox</em>
                </p>
                        
                            
                    </div>
    <div>
            <h2>Also in <span>Physics</span></h2>
            
        </div>
<section data-function="toggle" data-name="show-comments" id="comments">
    <h2>Comment on this article</h2>
    
    
</section>
    <div>
        <div data-name="next-post__image-wrapper">
    <p><img width="1720" height="729" src="https://www.quantamagazine.org/wp-content/uploads/2024/09/HiggsExplainer-crMicheleSclafani-HP-1720x729.jpg" alt="" decoding="async" loading="lazy" srcset="https://www.quantamagazine.org/wp-content/uploads/2024/09/HiggsExplainer-crMicheleSclafani-HP-1720x729.jpg 1720w, https://www.quantamagazine.org/wp-content/uploads/2024/09/HiggsExplainer-crMicheleSclafani-HP-520x220.jpg 520w, https://www.quantamagazine.org/wp-content/uploads/2024/09/HiggsExplainer-crMicheleSclafani-HP-768x325.jpg 768w, https://www.quantamagazine.org/wp-content/uploads/2024/09/HiggsExplainer-crMicheleSclafani-HP-1536x651.jpg 1536w, https://www.quantamagazine.org/wp-content/uploads/2024/09/HiggsExplainer-crMicheleSclafani-HP-2048x868.jpg 2048w" sizes="(max-width: 1720px) 100vw, 1720px">    </p>
</div>
        
        <div>
                <h2>Next article</h2>
                <p>How the Higgs Field (Actually) Gives Mass to Elementary Particles</p>
            </div>
        </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Mem0 – open-source Memory Layer for AI apps (172 pts)]]></title>
            <link>https://github.com/mem0ai/mem0</link>
            <guid>41447317</guid>
            <pubDate>Wed, 04 Sep 2024 16:01:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mem0ai/mem0">https://github.com/mem0ai/mem0</a>, See on <a href="https://news.ycombinator.com/item?id=41447317">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a href="https://github.com/mem0ai/mem0">
  <img src="https://github.com/mem0ai/mem0/raw/main/docs/images/banner-sm.png" width="800px" alt="Mem0 - The Memory Layer for Personalized AI">
  </a>
  </p><p dir="auto">
    <a href="https://mem0.ai/" rel="nofollow">Learn more</a>
    ·
    <a href="https://mem0.ai/discord" rel="nofollow">Join Discord</a>
  </p>

<p dir="auto">
  <a href="https://mem0.ai/discord" rel="nofollow">
    <img src="https://camo.githubusercontent.com/77710b3fddffb8b35ad4dfdca1724a4c58daf3778c42d494f3e98f57def1c4a3/68747470733a2f2f646362616467652e76657263656c2e6170702f6170692f7365727665722f36507a584467456a47353f7374796c653d666c6174" alt="Mem0 Discord" data-canonical-src="https://dcbadge.vercel.app/api/server/6PzXDgEjG5?style=flat">
  </a>
  <a href="https://pepy.tech/project/mem0ai" rel="nofollow">
    <img src="https://camo.githubusercontent.com/94088672ebb9e674fd4a0f250561da38b6df08d22087233d0165f3b64045822e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6d656d306169" alt="Mem0 PyPI - Downloads" data-canonical-src="https://img.shields.io/pypi/dm/mem0ai">
  </a>
  <a href="https://pypi.org/project/mem0ai" rel="nofollow">
        <img src="https://camo.githubusercontent.com/5b7f5ca05c54e143f9f52a7aa9ece4c188cf2241e98427a3731611dcb91b5b4c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6d656d3061693f636f6c6f723d253233333444303538266c6162656c3d707970692532307061636b616765" alt="Package version" data-canonical-src="https://img.shields.io/pypi/v/mem0ai?color=%2334D058&amp;label=pypi%20package">
    </a>
    <a href="https://pypi.org/project/mem0ai" rel="nofollow">
        <img src="https://camo.githubusercontent.com/0ce3fb4bc2581c5befd06e9d35e8fd87cd01217a4e65194b184947f53be7fdb2/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6d656d3061692e7376673f636f6c6f723d253233333444303538" alt="Supported Python versions" data-canonical-src="https://img.shields.io/pypi/pyversions/mem0ai.svg?color=%2334D058">
    </a>
  <a href="https://www.ycombinator.com/companies/mem0" rel="nofollow">
    <img src="https://camo.githubusercontent.com/cbd5d8722017c9a1eefa0e3620ecdb179dac21fbba84ec4aab7711adfa42c4ea/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f59253230436f6d62696e61746f722d5332342d6f72616e67653f7374796c653d666c61742d737175617265" alt="Y Combinator S24" data-canonical-src="https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square">
  </a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto"><a href="https://mem0.ai/" rel="nofollow">Mem0</a> (pronounced as "mem-zero") enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. Mem0 remembers user preferences, adapts to individual needs, and continuously improves over time, making it ideal for customer support chatbots, AI assistants, and autonomous systems.</p>

<p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/fa2f1cb7ec103daa40989bd997dc4e97d1f648e3ee788d18463e5db2fe85ce53/68747470733a2f2f6d656469612e74656e6f722e636f6d2f4b336a397077576c4d453041414141692f666972652d666c616d652e676966"><img src="https://camo.githubusercontent.com/fa2f1cb7ec103daa40989bd997dc4e97d1f648e3ee788d18463e5db2fe85ce53/68747470733a2f2f6d656469612e74656e6f722e636f6d2f4b336a397077576c4d453041414141692f666972652d666c616d652e676966" alt="Graph Memory Integration" data-animated-image="" data-canonical-src="https://media.tenor.com/K3j9pwWlME0AAAAi/fire-flame.gif"></a>
  <span>New Feature: Introducing Graph Memory. Check out our <a href="https://docs.mem0.ai/open-source/graph-memory" rel="nofollow">documentation</a>.</span>
</p>

<p dir="auto"><h3 tabindex="-1" dir="auto">Core Features</h3><a id="user-content-core-features" aria-label="Permalink: Core Features" href="#core-features"></a></p>
<ul dir="auto">
<li><strong>Multi-Level Memory</strong>: User, Session, and AI Agent memory retention</li>
<li><strong>Adaptive Personalization</strong>: Continuous improvement based on interactions</li>
<li><strong>Developer-Friendly API</strong>: Simple integration into various applications</li>
<li><strong>Cross-Platform Consistency</strong>: Uniform behavior across devices</li>
<li><strong>Managed Service</strong>: Hassle-free hosted solution</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">How Mem0 works?</h3><a id="user-content-how-mem0-works" aria-label="Permalink: How Mem0 works?" href="#how-mem0-works"></a></p>
<p dir="auto">Mem0 leverages a hybrid database approach to manage and retrieve long-term memories for AI agents and assistants. Each memory is associated with a unique identifier, such as a user ID or agent ID, allowing Mem0 to organize and access memories specific to an individual or context.</p>
<p dir="auto">When a message is added to the Mem0 using add()  method, the system extracts relevant facts and preferences and stores it across data stores: a vector database, a key-value database, and a graph database. This hybrid approach ensures that different types of information are stored in the most efficient manner, making subsequent searches quick and effective.</p>
<p dir="auto">When an AI agent or LLM needs to recall memories, it uses the search() method. Mem0 then performs search across these data stores, retrieving relevant information from each source. This information is then passed through a scoring layer, which evaluates their importance based on relevance, importance, and recency. This ensures that only the most personalized and useful context is surfaced.</p>
<p dir="auto">The retrieved memories can then be appended to the LLM's prompt as needed, enhancing the personalization and relevance of its responses.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Use Cases</h3><a id="user-content-use-cases" aria-label="Permalink: Use Cases" href="#use-cases"></a></p>
<p dir="auto">Mem0 empowers organizations and individuals to enhance:</p>
<ul dir="auto">
<li><strong>AI Assistants and agents</strong>: Seamless conversations with a touch of déjà vu</li>
<li><strong>Personalized Learning</strong>: Tailored content recommendations and progress tracking</li>
<li><strong>Customer Support</strong>: Context-aware assistance with user preference memory</li>
<li><strong>Healthcare</strong>: Patient history and treatment plan management</li>
<li><strong>Virtual Companions</strong>: Deeper user relationships through conversation memory</li>
<li><strong>Productivity</strong>: Streamlined workflows based on user habits and task history</li>
<li><strong>Gaming</strong>: Adaptive environments reflecting player choices and progress</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get Started</h2><a id="user-content-get-started" aria-label="Permalink: Get Started" href="#get-started"></a></p>
<p dir="auto">The easiest way to set up Mem0 is through the managed <a href="https://app.mem0.ai/" rel="nofollow">Mem0 Platform</a>. This hosted solution offers automatic updates, advanced analytics, and dedicated support. <a href="https://app.mem0.ai/" rel="nofollow">Sign up</a> to get started.</p>
<p dir="auto">If you prefer to self-host, use the open-source Mem0 package. Follow the <a href="#install">installation instructions</a> to get started.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation Instructions <a name="user-content-install"></a></h2><a id="user-content-installation-instructions-" aria-label="Permalink: Installation Instructions " href="#installation-instructions-"></a></p>
<p dir="auto">Install the Mem0 package via pip:</p>

<p dir="auto">Alternatively, you can use Mem0 with one click on the hosted platform <a href="https://app.mem0.ai/" rel="nofollow">here</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Usage</h3><a id="user-content-basic-usage" aria-label="Permalink: Basic Usage" href="#basic-usage"></a></p>
<p dir="auto">Mem0 requires an LLM to function, with <code>gpt-4o</code> from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our <a href="https://docs.mem0.ai/llms" rel="nofollow">Supported LLMs documentation</a>.</p>
<p dir="auto">First step is to instantiate the memory:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from mem0 import Memory

m = Memory()"><pre><span>from</span> <span>mem0</span> <span>import</span> <span>Memory</span>

<span>m</span> <span>=</span> <span>Memory</span>()</pre></div>
<details>
<summary>How to set OPENAI_API_KEY</summary>
<div dir="auto" data-snippet-clipboard-copy-content="import os
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;sk-xxx&quot;"><pre><span>import</span> <span>os</span>
<span>os</span>.<span>environ</span>[<span>"OPENAI_API_KEY"</span>] <span>=</span> <span>"sk-xxx"</span></pre></div>
</details>
<p dir="auto">You can perform the following task on the memory:</p>
<ol dir="auto">
<li>Add: Store a memory from any unstructured text</li>
<li>Update: Update memory of a given memory_id</li>
<li>Search: Fetch memories based on a query</li>
<li>Get: Return memories for a certain user/agent/session</li>
<li>History: Describe how a memory has changed over time for a specific memory ID</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# 1. Add: Store a memory from any unstructured text
result = m.add(&quot;I am working on improving my tennis skills. Suggest some online courses.&quot;, user_id=&quot;alice&quot;, metadata={&quot;category&quot;: &quot;hobbies&quot;})

# Created memory --> 'Improving her tennis skills.' and 'Looking for online suggestions.'"><pre><span># 1. Add: Store a memory from any unstructured text</span>
<span>result</span> <span>=</span> <span>m</span>.<span>add</span>(<span>"I am working on improving my tennis skills. Suggest some online courses."</span>, <span>user_id</span><span>=</span><span>"alice"</span>, <span>metadata</span><span>=</span>{<span>"category"</span>: <span>"hobbies"</span>})

<span># Created memory --&gt; 'Improving her tennis skills.' and 'Looking for online suggestions.'</span></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# 2. Update: update the memory
result = m.update(memory_id=<memory_id_1>, data=&quot;Likes to play tennis on weekends&quot;)

# Updated memory --> 'Likes to play tennis on weekends.' and 'Looking for online suggestions.'"><pre><span># 2. Update: update the memory</span>
<span>result</span> <span>=</span> <span>m</span>.<span>update</span>(<span>memory_id</span><span>=</span><span>&lt;</span><span>memory_id_1</span><span>&gt;</span>, <span>data</span><span>=</span><span>"Likes to play tennis on weekends"</span>)

<span># Updated memory --&gt; 'Likes to play tennis on weekends.' and 'Looking for online suggestions.'</span></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# 3. Search: search related memories
related_memories = m.search(query=&quot;What are Alice's hobbies?&quot;, user_id=&quot;alice&quot;)

# Retrieved memory --> 'Likes to play tennis on weekends'"><pre><span># 3. Search: search related memories</span>
<span>related_memories</span> <span>=</span> <span>m</span>.<span>search</span>(<span>query</span><span>=</span><span>"What are Alice's hobbies?"</span>, <span>user_id</span><span>=</span><span>"alice"</span>)

<span># Retrieved memory --&gt; 'Likes to play tennis on weekends'</span></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# 4. Get all memories
all_memories = m.get_all()
memory_id = all_memories[&quot;memories&quot;][0] [&quot;id&quot;] # get a memory_id

# All memory items --> 'Likes to play tennis on weekends.' and 'Looking for online suggestions.'"><pre><span># 4. Get all memories</span>
<span>all_memories</span> <span>=</span> <span>m</span>.<span>get_all</span>()
<span>memory_id</span> <span>=</span> <span>all_memories</span>[<span>"memories"</span>][<span>0</span>] [<span>"id"</span>] <span># get a memory_id</span>

<span># All memory items --&gt; 'Likes to play tennis on weekends.' and 'Looking for online suggestions.'</span></pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="# 5. Get memory history for a particular memory_id
history = m.history(memory_id=<memory_id_1>)

# Logs corresponding to memory_id_1 --> {'prev_value': 'Working on improving tennis skills and interested in online courses for tennis.', 'new_value': 'Likes to play tennis on weekends' }"><pre><span># 5. Get memory history for a particular memory_id</span>
<span>history</span> <span>=</span> <span>m</span>.<span>history</span>(<span>memory_id</span><span>=</span><span>&lt;</span><span>memory_id_1</span><span>&gt;</span>)

<span># Logs corresponding to memory_id_1 --&gt; {'prev_value': 'Working on improving tennis skills and interested in online courses for tennis.', 'new_value': 'Likes to play tennis on weekends' }</span></pre></div>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">If you prefer a hosted version without the need to set up infrastructure yourself, check out the <a href="https://app.mem0.ai/" rel="nofollow">Mem0 Platform</a> to get started in minutes.</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Graph Memory</h3><a id="user-content-graph-memory" aria-label="Permalink: Graph Memory" href="#graph-memory"></a></p>
<p dir="auto">To initialize Graph Memory you'll need to set up your configuration with graph store providers.
Currently, we support Neo4j as a graph store provider. You can setup <a href="https://neo4j.com/" rel="nofollow">Neo4j</a> locally or use the hosted <a href="https://neo4j.com/product/auradb/" rel="nofollow">Neo4j AuraDB</a>.
Moreover, you also need to set the version to <code>v1.1</code> (<em>prior versions are not supported</em>).
Here's how you can do it:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from mem0 import Memory

config = {
    &quot;graph_store&quot;: {
        &quot;provider&quot;: &quot;neo4j&quot;,
        &quot;config&quot;: {
            &quot;url&quot;: &quot;neo4j+s://xxx&quot;,
            &quot;username&quot;: &quot;neo4j&quot;,
            &quot;password&quot;: &quot;xxx&quot;
        }
    },
    &quot;version&quot;: &quot;v1.1&quot;
}

m = Memory.from_config(config_dict=config)
"><pre><span>from</span> <span>mem0</span> <span>import</span> <span>Memory</span>

<span>config</span> <span>=</span> {
    <span>"graph_store"</span>: {
        <span>"provider"</span>: <span>"neo4j"</span>,
        <span>"config"</span>: {
            <span>"url"</span>: <span>"neo4j+s://xxx"</span>,
            <span>"username"</span>: <span>"neo4j"</span>,
            <span>"password"</span>: <span>"xxx"</span>
        }
    },
    <span>"version"</span>: <span>"v1.1"</span>
}

<span>m</span> <span>=</span> <span>Memory</span>.<span>from_config</span>(<span>config_dict</span><span>=</span><span>config</span>)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">For detailed usage instructions and API reference, visit our documentation at <a href="https://docs.mem0.ai/" rel="nofollow">docs.mem0.ai</a>. Here, you can find more information on both the open-source version and the hosted <a href="https://app.mem0.ai/" rel="nofollow">Mem0 Platform</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Star History</h2><a id="user-content-star-history" aria-label="Permalink: Star History" href="#star-history"></a></p>
<p dir="auto"><a href="https://star-history.com/#mem0ai/mem0&amp;Date" rel="nofollow"><img src="https://camo.githubusercontent.com/32407dd18b36d02377972982a21bb54f481c3cfaf5b5368263538fb896ef2bde/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6d656d3061692f6d656d3026747970653d44617465" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=mem0ai/mem0&amp;type=Date"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Support</h2><a id="user-content-support" aria-label="Permalink: Support" href="#support"></a></p>
<p dir="auto">Join our community for support and discussions. If you have any questions, feel free to reach out to us using one of the following methods:</p>
<ul dir="auto">
<li><a href="https://mem0.ai/discord" rel="nofollow">Join our Discord</a></li>
<li><a href="https://x.com/mem0ai" rel="nofollow">Follow us on Twitter</a></li>
<li><a href="mailto:founders@mem0.ai">Email founders</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributors</h2><a id="user-content-contributors" aria-label="Permalink: Contributors" href="#contributors"></a></p>
<p dir="auto">Join our <a href="https://mem0.ai/discord" rel="nofollow">Discord community</a> to learn about memory management for AI agents and LLMs, and connect with Mem0 users and contributors. Share your ideas, questions, or feedback in our <a href="https://github.com/mem0ai/mem0/issues">GitHub Issues</a>.</p>
<p dir="auto">We value and appreciate the contributions of our community. Special thanks to our contributors for helping us improve Mem0.</p>
<a href="https://github.com/mem0ai/mem0/graphs/contributors">
  <img src="https://camo.githubusercontent.com/f107c6afd1adb1112f86fe17c427bb28c458965ef70874be78fac0e424a87a68/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d6d656d3061692f6d656d30" data-canonical-src="https://contrib.rocks/image?repo=mem0ai/mem0">
</a>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the Apache 2.0 License - see the <a href="https://github.com/mem0ai/mem0/blob/main/LICENSE">LICENSE</a> file for details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intel Honesty (342 pts)]]></title>
            <link>https://stratechery.com/2024/intel-honesty/</link>
            <guid>41446766</guid>
            <pubDate>Wed, 04 Sep 2024 15:15:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stratechery.com/2024/intel-honesty/">https://stratechery.com/2024/intel-honesty/</a>, See on <a href="https://news.ycombinator.com/item?id=41446766">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-13412">
	<!-- .entry-header -->

	<div>
		<p>It really is a valley:</p>
<p><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-1.png?resize=640%2C645&amp;ssl=1" alt="The topography of Silicon Valley" width="640" height="645" srcset="https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-1.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-1.png?resize=298%2C300&amp;ssl=1 298w, https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-1.png?resize=1016%2C1024&amp;ssl=1 1016w, https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-1.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-1.png?resize=768%2C774&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-1.png?resize=625%2C630&amp;ssl=1 625w" sizes="(max-width: 640px) 100vw, 640px"></p>
<p>There, right in the middle of the Santa Clara Valley, formed by the Santa Cruz Mountains on the west and the Diablo Range on the east, lies the once-sleepy city of Mountain View. Mountain View was dominated by the U.S. Navy’s Moffett Field in 1955, when William Shockley, one of the inventors of the transistor at Bell Labs, returned to neighboring Palo Alto to care for his ailing mother.</p>
<p>Convinced that silicon was a superior material for transistors — Bell Labs was focused on germanium — Shockley, unable to hire many Bell Labs co-workers both because of the distance from New Jersey and also his abusive management style, set up the Shockley Semiconductor Laboratory in 1956 in Mountain View with a collection of young scientists. Only a year later eight of those scientists, led by Robert Noyce and Gordon Moore, fled Shockley —&nbsp;he really was a terrible manager — and set up Fairchild Semiconductor, a new division of Fairchild Camera and Instrument, in neighboring Sunnyvale.</p>
<p>It was Fairchild Semiconductor that gave the tech industry’s home the other half of its name: yes, we talk about “The Valley”, but at least when it comes to tech, we mean <em>Silicon</em> Valley. From <a href="https://techcrunch.com/2014/07/26/the-first-trillion-dollar-startup/">TechCrunch in 2014</a>:</p>
<blockquote><p>
  As Fairchild started to grow, employees began to leave the firm to launch new spin-off businesses. Many of these firms also grew quickly, inspiring other employees still working at the company…The growth of these new companies started to reshape the region. In just 12 years, the co-founders and former employees of Fairchild generated more than 30 spin-off companies and funded many more. By 1970, chip businesses in the San Francisco area employed a total of 12,000 people…</p>
<p>  The achievements of these companies eventually attracted attention. In 1971, a journalist named Don Hoefler wrote an article about the success of computer chip companies in the Bay Area. The firms he profiled all produced chips using silicon and were located in a large valley south of San Francisco. Hoefler put these two facts together to create a new name for the region: Silicon Valley.</p>
<p>  Hoefler’s article and the name he coined have become quite famous, but there’s a critical part of his analysis that is often overlooked: Almost all of the silicon chip companies he profiled can be traced back to Fairchild and its co-founders.</p>
<p>  <a href="https://techcrunch.com/2014/07/26/the-first-trillion-dollar-startup/"><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-3.png?resize=640%2C646&amp;ssl=1" alt="Companies formed downstream from Fairchild Semiconductor" width="640" height="646" srcset="https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-3.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-3.png?resize=297%2C300&amp;ssl=1 297w, https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-3.png?resize=1014%2C1024&amp;ssl=1 1014w, https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-3.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-3.png?resize=768%2C775&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-3.png?resize=624%2C630&amp;ssl=1 624w" sizes="(max-width: 640px) 100vw, 640px"></a>
</p></blockquote>
<p>Still, for all of the massive success downstream from Fairchild Semiconductor, none mattered more, or came to define Silicon Valley in every respect, than Intel. Arthur Rock, who had helped the so-called “Traitorous Eight” find Fairchild Camera and Instrument, funded Intel, and in the process <a href="https://law.stanford.edu/stanford-lawyer/articles/legal-matters-arthur-rock-on-the-early-venture-capital-decisions-that-sparked-decades-of-innovation/">created the compensation structure that came to define Silicon Valley</a>. Gordon Moore wrote the roadmap for Intel — nor more commonly known as <a href="http://cva.stanford.edu/classes/cs99s/papers/moore-crammingmorecomponents.pdf">Moore’s Law</a> — which “predicted” that the number of transistors would double at a set rate, both increasing compute speed and driving down prices for that compute; “predict” is in quotes because Moore’s Law was not a physical law, but an economic one, downstream of Intel’s inexorable push for continued improvement. That, by extension, meant that Intel set the pace of innovation for all of technology, not just by making the processors for the PC — and, in an underrated wave of disruption in the early part of this century, the cloud — but also by defining the expectations of every software engineer in the entire world.</p>
<h3>Intel’s Long Decline</h3>
<p>Stratechery has, from the beginning, operated with a great degree of reverence for tech history; perhaps that’s why I’ve always been a part of the camp cheering for Intel to succeed. The unfortunate fact of the matter is that the need for cheerleading has been clear for as long as I have written this blog: in <a href="https://stratechery.com/2013/the-intel-opportunity/">May 2013</a> I wrote that Intel needed to build out a foundry business, as the economics of their IDM business, given their mobile miss, faced long-term challenges.</p>
<p>Unfortunately not only did Intel not listen, but their business got a lot worse: in the late 2010’s Intel got stuck trying to move to 10nm, thanks in part to their reluctance to embrace the vastly more expensive EUV lithography process, handing the performance crown to TSMC. Meanwhile Intel’s chip design team, increasingly fat and lazy thanks to the fact they could leverage Intel’s once-industry-leading processes, had started to fall behind AMD; today AMD has both better designs and, thanks to the fact they fab their chips at TSMC, better processes. Meanwhile, the rise of hyperscalers meant there were entities that both had the scale to justify overcoming whatever software advantages Intel had, and the resources to do so; the result is that AMD has been taking data center share for years, and is on the verge of passing 50%:</p>
<p><img data-recalc-dims="1" loading="lazy" decoding="async" src="https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-2.png?resize=640%2C348&amp;ssl=1" alt="Intel vs AMD in the data center" width="640" height="348" srcset="https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-2.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-2.png?resize=300%2C163&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-2.png?resize=1024%2C556&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-2.png?resize=768%2C417&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2024/09/intel-2.png?resize=1160%2C630&amp;ssl=1 1160w" sizes="(max-width: 640px) 100vw, 640px"></p>
<p>[<em>Editor’s Note: these two paragraphs are technically incorrect, in that AMD’s data center revenue includes their AI chips; the directionally point remains, but I regret the erros</em>]</p>
<p><strike>This chart actually understates the problem, because it only includes x86 processors;</strike> in fact, those capabilities that have allowed the hyperscalers to take advantage of AMD’s increasingly superior total cost of ownership have also been devoted to building ARM-based server chips. Amazon in particular has invested heavily in its Graviton line of chips, taking advantage of ARM’s theoretically better efficiency and lower licensing fees (as compared to Intel’s margins).</p>
<p>Beyond that, what is especially problematic — and why Intel’s datacenter revenue is actually down year-over-year — is that an increasing amount of data center spend is going towards AI, the latest paradigm where Intel missed the boat.</p>
<p>[<em>End Editor’s Note</em>]</p>
<p>The story Intel — or at least <a href="https://www.theatlantic.com/technology/archive/2013/05/paul-otellinis-intel-can-the-company-that-built-the-future-survive-it/275825/">its past management</a> — wants you to believe about mobile is that they foolishly passed up the opportunity to supply Apple’s iPhone, not realizing that the volume would more than make up for the margin hit; in fact, <a href="https://stratechery.com/2022/an-interview-with-father-of-the-ipod-tony-fadell/">Tony Fadell told me</a> that while Steve Jobs wanted Intel — Apple had just switched to using Intel chips for Macs —&nbsp;Intel chips weren’t competitive:</p>
<blockquote><p>
  For me, when it came to Intel at the time, back in the mid-2000s, they were always about, “Well, we’ll just repackage what we have on the desktop for the laptop and then we’ll repackage that again for embedding.” It reminded me of Windows saying, “I’m going to do Windows and then I’m going to do Windows Mobile and I’m going to do Windows embedded.” It was using those same cores and kernels and trying to slim them down…</p>
<p>  The mindset at Intel was never about — when they went through that CISC-RISC duality of “Which one are we going to be?”, and they chose CISC, which was the right thing at the time, if you fast forward, they also made that decision, they threw away architectural and they went to more manufacturing. That was the time when they said “We don’t have to worry about all these different product lines to meet all these architectural needs. We’re just going to have Moore’s Law take over” and so in a way that locks you into a path and that’s why Intel, not under the Pat days but previous to the Pat days, was all driven by manufacturing capability and legal. It wasn’t driven by architectural decisions, it was like, “Here’s what we got and we’re going to spread it around and we’re going to keep reusing it”.
</p></blockquote>
<p>In fact, it does go back to the Pat days, specifically CEO Pat Gelsinger’s initial stint at Intel. He was the one that <a href="https://stratechery.com/2022/mr-cisc-vs-mr-risc-arm-and-amd-threats-gelsingers-three-tenets/">pushed CISC over RISC</a>, arguing that Intel’s CISC software advantage, supported by the company’s superior manufacturing, would ensure that the company dominated microprocessors. And, as Fadell noted, it worked, at least in PCs and servers.</p>
<p>Where it didn’t work was mobile: Intel couldn’t leverage its manufacturing to make x86 competitive with ARM, particularly since the latter had a head start on software; it also didn’t work in GPUs, where Intel spent years trying to build x86-based gaming chips that — you guessed it — were meant to rely on Intel’s manufacturing prowess. GPUs, of course, are the foundation of today’s AI boom, and while Intel bought Gaudi to offer AI chips, they haven’t made a dent in the market — and oh, by the way, Gaudi chips are manufactured by TSMC.</p>
<h3>IDM 2.0</h3>
<p>None of this story is new; I recounted it in 2021’s <a href="https://stratechery.com/2021/intel-problems/">Intel Problems</a>. My solution then — written shortly after Gelsinger came back to Intel, fifteen years after being passed over for the CEO job — was that the company needed to split up.</p>
<blockquote><p>
  Integrating design and manufacturing was the foundation of Intel’s moat for decades, but that integration has become a strait-jacket for both sides of the business. Intel’s designs are held back by the company’s struggles in manufacturing, while its manufacturing has an incentive problem.</p>
<p>  The key thing to understand about chips is that design has much higher margins; Nvidia, for example, has gross margins between 60~65%, while TSMC, which makes Nvidia’s chips, has gross margins closer to 50%. Intel has, as I noted above, traditionally had margins closer to Nvidia, thanks to its integration, which is why Intel’s own chips will always be a priority for its manufacturing arm. That will mean worse service for prospective customers, and less willingness to change its manufacturing approach to both accommodate customers and incorporate best-of-breed suppliers (lowering margins even further). There is also the matter of trust: would companies that compete with Intel be willing to share their designs with their competitor, particularly if that competitor is incentivized to prioritize its own business?</p>
<p>  The only way to fix this incentive problem is to spin off Intel’s manufacturing business. Yes, it will take time to build out the customer service components necessary to work with third parties, not to mention the huge library of IP building blocks that make working with a company like TSMC (relatively) easy. But a standalone manufacturing business will have the most powerful incentive possible to make this transformation happen: the need to survive.
</p></blockquote>
<p>Two months later and Gelsinger announced his turnaround plan: <a href="https://stratechery.com/2021/intel-unleashed-gelsinger-on-intel-idm-2-0/">IDM 2.0</a>. Intel would separate out its manufacturing into a separate division that would serve third parties, but still under the Intel banner. Gelsinger told me <a href="https://stratechery.com/2022/an-interview-with-intel-ceo-pat-gelsinger/">in an interview</a> that this was the only way Intel could both be competitive in chips and keep investing in the leading edge; after all, AMD’s spin-off of Global Foundries resulted in the former floundering until <a href="https://stratechery.com/2024/an-interview-with-amd-ceo-lisa-su-about-solving-hard-problems/">they could break their purchase agreements with Global Foundries and go to TSMC</a>, and <a href="https://stratechery.com/2018/uber-follow-up-globalfoundries-abandons-7nm-pricing-power-differentiation-and-integration/">the latter giving up on the leading edge</a>.</p>
<p>Gelsinger is persuasive and optimistic, and for the last three years I’ve given him the benefit of the doubt. Suddenly, though, a split is back on the table; from <a href="https://www.bloomberg.com/news/articles/2024-08-30/intel-is-said-to-explore-options-to-cope-with-historic-slump">Bloomberg</a>:</p>
<blockquote><p>
  Intel Corp. is working with investment bankers to help navigate the most difficult period in its 56-year history, according to people familiar with the matter. The company is discussing various scenarios, including a split of its product-design and manufacturing businesses, as well as which factory projects might potentially be scrapped, said the people, who asked not to be identified because the deliberations are private…</p>
<p>  A potential separation or sale of Intel’s foundry division, which is aimed at manufacturing chips for outside customers, would be an about-face for Chief Executive Officer Pat Gelsinger. Gelsinger has viewed the business as key to restoring Intel’s standing among chipmakers and had hoped it would eventually compete with the likes of Taiwan Semiconductor Manufacturing Co., which pioneered the foundry industry.
</p></blockquote>
<p>As the article notes, Intel is likely to consider less drastic steps first; <a href="https://www.reuters.com/technology/intel-ceo-pitch-board-plans-shed-assets-cut-costs-source-says-2024-09-01/">Reuters reported</a> that ideas include selling businesses like its Altera programmable chip business and reducing capital expeditures, including axing a proposed foundry in Germany. The company also finally killed its dividend, and is cutting 15,000 jobs, which frankly, isn’t enough; I noted <a href="https://stratechery.com/2024/intel-concerns-fubo-blocks-venu/">in an Update last week</a>:</p>
<blockquote><p>
  Intel ended last year with 124,800 people; to put that in context, TSMC had 76,478 employees and AMD 26,000, which is to say that the two companies combined had fewer employees than Intel while making better x86 chips, an actually competitive GPU, and oh yeah, making chips for everyone else on earth, including Apple and Nvidia. A 15,000 employee cut is both too small and too late.
</p></blockquote>
<p>The fundamental problem facing the company is encapsulated in that paragraph:</p>
<ul>
<li>Intel doesn’t have the best manufacturing</li>
<li>Intel doesn’t design the best chips</li>
<li>Intel is out of the game in AI</li>
</ul>
<p>Moreover, the future does not look bright; the problem with Intel’s most recent earnings call was threefold:</p>
<ul>
<li>Intel’s is technically on pace to achieve the five nodes in four years Gelsinger promised (in truth two of those nodes were iterations), but they haven’t truly scaled any of them; the first attempt to do so, with Intel 3, destroyed their margins. This isn’t a surprise: the reason why it is hard to skip steps is not just because technology advances, but because you have to actually learn on the line how to implement new technology at scale, with sustainable yield. Go back to Intel’s 10nm failure: the company could technically make a 10nm chip, they just couldn’t do so economically; there are now open questions about Intel 3, much less next year’s promised 18A.</li>
<li>Intel is dramatically ramping up its Lunar Lake architecture as it is the only design the company has that is competitive with the Qualcomm ARM architecture undergirding Microsoft’s CoPilot+ PC initiative; the problem is that Lunar Lake’s tiles — including its CPU — are made by TSMC, which is both embarrassing and also terrible for margins.</li>
<li>The third problem is that the goal Gelsinger has been pushing for is the aforementioned 18A, yet Intel has yet to announce a truly committed at-scale partner. Yes, the company is in talks with lots of folks and claims some number of secret agreements, but at this point the foundry strategy needs real proof points; unfortunately Intel itself ramping up on TSMC, even as it loses control of its costs, isn’t exactly a selling point as to why any third-party should put their fortunes in Intel’s hands.</li>
</ul>
<p>All that noted, my initial response to the meltdown over Intel’s earnings <a href="https://stratechery.com/2024/intel-earnings-intels-nadir/">was to defend Gelsinger</a>; what is happening to Intel now is downstream of mistakes that happened years before Gelsinger came back to the company. That remains true, but Gelsinger does have one fatal flaw: he still believes in Intel, and I no longer do.</p>
<h3>Market Realities</h3>
<p>Here is the fundamental problem facing Intel, and by extension, U.S. dreams of controlling leading edge capacity: there is no reason for Intel Foundry to exist. Apple, Nvidia, AMD, and other leading edge fabless chip companies rely on TSMC, and why wouldn’t they? TSMC invested in EUV, surpassed Intel, and are spending tens of billions of dollars a year to continue pushing forward to 2nm and beyond. Yes, <a href="https://stratechery.com/2024/tsmc-earnings-tsmcs-pricing-mistake-intel-v-tsmc/">TSMC priced 3nm too low</a>, but even if the company raises prices for future nodes, as I expect them to, the relative cost matters much less than TSMC’s superior customer services and demonstrated reliability.</p>
<p>The kicker is that the smartest decision for Intel’s own chip unit is to — as they are with Lunar Lake — rely on TSMC’s manufacturing as well. Intel still has advantages in PCs and a dominant position in on-premises and government data centers, but the best way to leverage those remaining areas of strength is to have TSMC make their chips.</p>
<p>This was, for the record, why Gelsinger did have a point in keeping the company together; Intel Foundry needs volume, and the easiest way to get that volume is from Intel itself. However, that by definition is a decision that is not driven by what is best for a theoretical Intel fabless business, but rather the impetus to restore Intel’s manufacturing capability, even as that manufacturing capability is heavily incentivized to cater to Intel’s chip business at the expense of external customers.</p>
<p>Gelsinger’s trump card has been the fact that <a href="https://stratechery.com/2020/chips-and-geopolitics/">TSMC is based in Taiwan</a>, which is under continuous threat from China. Indeed, Gelsinger has been <a href="https://stratechery.com/2021/intel-vs-tsmc-how-samsung-and-tsmc-won-mad-chips/">quite explicit on this point</a>; from <a href="https://focustaiwan.tw/business/202112030015">CNA English News in 2021</a>:</p>
<blockquote><p>
  Intel CEO Pat Gelsinger said at the Fortune Brainstorm Tech summit in California on Wednesday that the United States government should support a sustainable semiconductor supply chain in the U.S., in part because “Taiwan is not a stable place”…</p>
<p>  Asked about the comment, TSMC Chairman Mark Liu (劉德音) said, “there’s nothing that needs to be addressed. TSMC does not speak ill of other companies in the industry,” and added there were probably not many people who believed Gelsinger’s argument. Geopolitical tensions, Liu said, may have a short-term impact, but he believed Taiwan could help create a brilliant decade for the global semiconductor industry, with the best technology and the best manufacturing ecosystem.
</p></blockquote>
<p>Gelsinger made the same point to me in that interview while explaining why Intel needed to stay together:</p>
<blockquote><p>
  As we look at this, to me, there is almost a global national perspective to this, in that I deeply believe the West needs a world class technology provider, and I don’t think that splitting Intel in two, that it could survive for many, many, many years till that would become the case, that you could stand that up. Remember, given cash flows, R&amp;D streams, products that enable us to drive that, and I’m committed to go fix it, and I think we’re on a good path to go fix it since I’ve been here as well. So for those three different reasons, we chose the IDM 2.0 path, but it’s not because we didn’t look at the alternative, it’s partially because we did.
</p></blockquote>
<p>This is where everyone who is invested in American manufacturing — or perhaps more accurately, concerned about China’s threat to Taiwan — has to get brutally honest. If the U.S. government and U.S. tech companies want to have a non-Taiwan option, they are going to have to pay for it directly. Yes, the CHIPS Act passed, but while Intel is getting a lot of funds, it’s going to take a lot more — and the price of those funds needs to be a much smarter incentive structure that drives Intel apart.</p>
<p>My proposal back in 2021 was purchase guarantees instead of subsidies, and I am back to thinking that is the only viable path.</p>
<blockquote><p>
  That is why a federal subsidy program should operate as a purchase guarantee: the U.S. will buy A amount of U.S.-produced 5nm processors for B price; C amount of U.S. produced 3nm processors for D price; E amount of U.S. produced 2nm processors for F price; etc. This will not only give the new Intel manufacturing spin-off something to strive for, but also incentivize other companies to invest; perhaps Global Foundries will get back in the game, or TSMC will build more fabs in the U.S. And, in a world of nearly free capital, perhaps there will finally be a startup willing to take the leap.
</p></blockquote>
<p>That free capital world is gone, and it’s probably not realistic for a startup to figure out how to manufacture the most complex devices humans have ever produced; the best idea at this point is a new company that has the expertise and starting position of Intel Foundry. Critically, though, it shouldn’t be at all beholden to x86 chips, have hundreds of thousands of employees, or the cultural overhang of having once led the computing world. The best we can do is purchase guarantees — on the order of hundreds of billions of dollars over the next decade — and a prayer that someone can make such an entity stand on its own.</p>
<p>To summarize, there is no market-based reason for Intel Foundry to exist; that’s not a market failure in a purely economic sense, but to the extent the U.S. national security apparatus sees it as a failure is the extent to which the U.S. is going to have to pay to make it happen. And, if the U.S. is going to pay up, that means giving that foundry the best possible chance to stand on its own two feet in the long run. That means actually earning business from Apple, Nvidia, AMD, and yes, even the fabless Intel company that will remain. The tech world has moved on from Intel; the only chance for U.S. leading edge manufacturing is to do the same.</p>
<p><em>I wrote a follow-up to this Article in <a href="https://stratechery.com/2024/broadcom-and-intel-nvidia-earnings-doj-investigating-nvidia/">this Daily Update</a>.</em></p>

	</div><!-- .entry-content -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Insecurity of Debian (226 pts)]]></title>
            <link>https://unix.foo/posts/insecurity-of-debian/</link>
            <guid>41446428</guid>
            <pubDate>Wed, 04 Sep 2024 14:46:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://unix.foo/posts/insecurity-of-debian/">https://unix.foo/posts/insecurity-of-debian/</a>, See on <a href="https://news.ycombinator.com/item?id=41446428">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>In June of 2023 Red Hat made a controversial decision to change how they distribute the source code behind Red Hat Enterprise Linux (RHEL). There have been a lot of keyboards tapped angrily across social media that left many uncertain about the ramifications of the decision. There were many questions about the future viability of downstream rebuilds of RHEL affecting distributions like Rocky Linux, AlmaLinux, Oracle Linux, and others. Each have since made announcements to try and calm their communities.</p>
<p>Still. Many in the open source community have interpreted Red Hat’s decision for what it really was: A dick move.</p>
<p>There has been a steady uptick of people stating that they will migrate (or already have) to Debian – seeking refuge from what they see as greedy corporate influence. I understand the sentiment fully. However, there’s a problem here that I want to talk about: security.</p>
<p>The ugly truth is that security is hard. It’s tedious. Unpleasant. And requires a lot of work to get right.</p>
<p>Debian does not do enough here to protect users.</p>
<p>Long ago, Red Hat embraced the usage of <a href="https://en.wikipedia.org/wiki/Security-Enhanced_Linux">SELinux</a>. And they took it beyond just enabling the feature in their kernel. They put in the arduous work of crafting default SELinux policies for their distribution.</p>
<p>These <a href="https://www.redhat.com/sysadmin/selinux-kata-containers">policies ship enabled by default</a> in their distribution. The policies help protect a variety of daemons that run by default on RHEL, as well as many of the most popular daemons folks tend to use on the server.</p>
<p>Apache, nginx, MariaDB, PostgreSQL, OpenSSH, etc. are all covered by SELinux policies that ship on RHEL distributions.</p>
<p>The protection even extends to containers. Containers are increasingly the preferred method for developers to deploy their software – myself included. A common misconception is that if you run something in a container, it’s inherently secure. This is absolutely not true. Containers by themselves do not solve a security problem. They solve a software distribution problem. They give a false impression of security to those that run them.</p>
<p>On Red Hat based distributions, you can use a drop-in Docker alternative named podman which allows you to run containers without needing a daemon (unlike Docker) and it provides other benefits like being able to run fully root-less. But Red Hat takes it a step further here and applies strong default SELinux policies that separate the container from the host OS and even from other containers!</p>
<p>There have been numerous <a href="https://www.redhat.com/en/blog/latest-container-exploit-runc-can-be-blocked-selinux?intcmp=701f20000012ngPAAQ">examples</a> of being able to escape from a container and touch the host OS or other containers. This is where tools like SELinux step in. The application of SELinux policies on a container allows for a hardened sarcophagus to place your application in which mitigates the risk of unknown future exploits. And it’s nearly effortless to use on RHEL.</p>
<p>Red Hat was aware that unless they put in the work on these default policies, their users would simply not embrace the technology and millions of servers would remain vulnerable. Because let’s be real here. SELinux is hard. The policy language and tooling is cumbersome, obtuse, and is about as appealing as filling out tax forms. It frankly sucks to use – if you are manually creating your own policies that is.</p>
<p>But due to the work Red Hat has put in, the usage of SELinux on RHEL is mostly transparent and provides real security benefits to their users.</p>
<h2 id="debians-approach">Debian’s Approach</h2>
<p>Debian, a stalwart of the open-source community, is revered for its stability and extensive software library. I am a fan and <a href="https://www.debian.org/donations">donate</a> to the project every year (you should too!) even though I don’t run it in production environments.</p>
<p>However, its default security framework leaves much to be desired. Debian’s decision to enable <a href="https://en.wikipedia.org/wiki/AppArmor">AppArmor</a> by default starting with version 10 signifies a positive step towards improved security, yet it falls short due to the half-baked implementation across the system.</p>
<p>Debian’s reliance on AppArmor and its default configurations reveals a systemic issue with its approach to security. While AppArmor is capable of providing robust security when properly configured, Debian’s out-of-the-box settings fail to leverage its full potential:</p>
<p><strong>Limited Default Profiles:</strong> Debian ships with a minimal set of AppArmor profiles, leaving many critical services unprotected.</p>
<p><strong>Reactive vs. Proactive Stance:</strong> Debian’s security model often relies on users to implement stricter policies, rather than providing a secure-by-default configuration.</p>
<p><strong>Inconsistent Application:</strong> Unlike SELinux in Red Hat systems, which applies to the entire system consistently, AppArmor in Debian is applied piecemeal, leading to potential security gaps.</p>
<p><strong>Lack of Resources:</strong> Debian as a community-driven project lacks the resources to develop and maintain comprehensive security policies comparable to those provided by Red Hat.</p>
<p>It’s very common for folks to run container workloads on Debian via Docker – which <a href="https://docs.docker.com/engine/security/apparmor/">does automatically generate</a> and load a default AppArmor profile for containers named <code>docker-default</code>. Unfortunately, it’s not a very strong profile for security and is overly permissive.</p>
<p>This profile, while providing some protection, <a href="https://github.com/moby/moby/blob/master/profiles/apparmor/template.go">leaves significant attack surfaces</a> exposed. For instance:</p>
<div><pre tabindex="0"><code data-lang="gdscript3"><span><span>  <span>network</span><span>,</span>
</span></span><span><span>  <span>capability</span><span>,</span>
</span></span><span><span>  <span>file</span><span>,</span>
</span></span><span><span>  <span>umount</span><span>,</span>
</span></span><span><span>  <span># Host (privileged) processes may send signals to container processes.</span>
</span></span><span><span>  <span>signal</span> <span>(</span><span>receive</span><span>)</span> <span>peer</span><span>=</span><span>unconfined</span><span>,</span>
</span></span><span><span>  <span># runc may send signals to container processes (for "docker stop").</span>
</span></span><span><span>  <span>signal</span> <span>(</span><span>receive</span><span>)</span> <span>peer</span><span>=</span><span>runc</span><span>,</span>
</span></span><span><span>  <span># crun may send signals to container processes (for "docker stop" when used with crun OCI runtime).</span>
</span></span><span><span>  <span>signal</span> <span>(</span><span>receive</span><span>)</span> <span>peer</span><span>=</span><span>crun</span><span>,</span>
</span></span><span><span>  <span># dockerd may send signals to container processes (for "docker kill").</span>
</span></span><span><span>  <span>signal</span> <span>(</span><span>receive</span><span>)</span> <span>peer</span><span>=</span><span>{{</span><span>.</span><span>DaemonProfile</span><span>}},</span>
</span></span><span><span>  <span># Container processes may send signals amongst themselves.</span>
</span></span><span><span>  <span>signal</span> <span>(</span><span>send</span><span>,</span><span>receive</span><span>)</span> <span>peer</span><span>=</span><span>{{</span><span>.</span><span>Name</span><span>}},</span>
</span></span><span><span>  <span>deny</span> <span>@</span><span>{</span><span>PROC</span><span>}</span><span>/*</span> <span>w</span><span>,</span>   <span># deny write for all files directly in /proc (not in a subdir)</span>
</span></span><span><span>  <span># deny write to files not in /proc/&lt;number&gt;/** or /proc/sys/**</span>
</span></span><span><span>  <span>deny</span> <span>@</span><span>{</span><span>PROC</span><span>}</span><span>/</span><span>{[</span><span>^</span><span>1</span><span>-</span><span>9</span><span>],[</span><span>^</span><span>1</span><span>-</span><span>9</span><span>][</span><span>^</span><span>0</span><span>-</span><span>9</span><span>],[</span><span>^</span><span>1</span><span>-</span><span>9</span><span>s</span><span>][</span><span>^</span><span>0</span><span>-</span><span>9</span><span>y</span><span>][</span><span>^</span><span>0</span><span>-</span><span>9</span><span>s</span><span>],[</span><span>^</span><span>1</span><span>-</span><span>9</span><span>][</span><span>^</span><span>0</span><span>-</span><span>9</span><span>][</span><span>^</span><span>0</span><span>-</span><span>9</span><span>][</span><span>^</span><span>0</span><span>-</span><span>9</span><span>/</span><span>]</span><span>*</span><span>}</span><span>/**</span> <span>w</span><span>,</span>
</span></span><span><span>  <span>deny</span> <span>@</span><span>{</span><span>PROC</span><span>}</span><span>/</span><span>sys</span><span>/</span><span>[</span><span>^</span><span>k</span><span>]</span><span>**</span> <span>w</span><span>,</span>  <span># deny /proc/sys except /proc/sys/k* (effectively /proc/sys/kernel)</span>
</span></span><span><span>  <span>deny</span> <span>@</span><span>{</span><span>PROC</span><span>}</span><span>/</span><span>sys</span><span>/</span><span>kernel</span><span>/</span><span>{</span><span>?</span><span>,</span><span>??</span><span>,[</span><span>^</span><span>s</span><span>][</span><span>^</span><span>h</span><span>][</span><span>^</span><span>m</span><span>]</span><span>**</span><span>}</span> <span>w</span><span>,</span>  <span># deny everything except shm* in /proc/sys/kernel/</span>
</span></span><span><span>  <span>deny</span> <span>@</span><span>{</span><span>PROC</span><span>}</span><span>/</span><span>sysrq</span><span>-</span><span>trigger</span> <span>rwklx</span><span>,</span>
</span></span><span><span>  <span>deny</span> <span>@</span><span>{</span><span>PROC</span><span>}</span><span>/</span><span>kcore</span> <span>rwklx</span><span>,</span>
</span></span><span><span>  <span>deny</span> <span>mount</span><span>,</span>
</span></span><span><span>  <span>deny</span> <span>/</span><span>sys</span><span>/</span><span>[</span><span>^</span><span>f</span><span>]</span><span>*/**</span> <span>wklx</span><span>,</span>
</span></span><span><span>  <span>deny</span> <span>/</span><span>sys</span><span>/</span><span>f</span><span>[</span><span>^</span><span>s</span><span>]</span><span>*/**</span> <span>wklx</span><span>,</span>
</span></span><span><span>  <span>deny</span> <span>/</span><span>sys</span><span>/</span><span>fs</span><span>/</span><span>[</span><span>^</span><span>c</span><span>]</span><span>*/**</span> <span>wklx</span><span>,</span>
</span></span><span><span>  <span>deny</span> <span>/</span><span>sys</span><span>/</span><span>fs</span><span>/</span><span>c</span><span>[</span><span>^</span><span>g</span><span>]</span><span>*/**</span> <span>wklx</span><span>,</span>
</span></span><span><span>  <span>deny</span> <span>/</span><span>sys</span><span>/</span><span>fs</span><span>/</span><span>cg</span><span>[</span><span>^</span><span>r</span><span>]</span><span>*/**</span> <span>wklx</span><span>,</span>
</span></span><span><span>  <span>deny</span> <span>/</span><span>sys</span><span>/</span><span>firmware</span><span>/**</span> <span>rwklx</span><span>,</span>
</span></span><span><span>  <span>deny</span> <span>/</span><span>sys</span><span>/</span><span>devices</span><span>/</span><span>virtual</span><span>/</span><span>powercap</span><span>/**</span> <span>rwklx</span><span>,</span>
</span></span><span><span>  <span>deny</span> <span>/</span><span>sys</span><span>/</span><span>kernel</span><span>/</span><span>security</span><span>/**</span> <span>rwklx</span><span>,</span>
</span></span></code></pre></div><p>The <strong>network</strong> rule allows all network-related syscalls without restriction.</p>
<p>The <strong>capability</strong> rule, without specific denials, permits most capabilities by default.</p>
<p>The <strong>file</strong> rule grants broad file access permissions, relying on specific deny rules for protection.</p>
<h2 id="apparmor-vs-selinux">AppArmor vs. SELinux</h2>
<p>The fundamental difference between AppArmor and SELinux lies in their approach to Mandatory Access Control (MAC). AppArmor operates on a path-based model, while SELinux employs a significantly more complex type enforcement system. This distinction becomes particularly evident in container environments.</p>
<p>SELinux applies a type to every object in the system - files, processes, ports, you name it. When you launch a container on a SELinux-enabled RHEL system, it’s immediately assigned the <strong>container_t</strong> type – a strict access control mechanism. The <strong>container_t</strong> type effectively cordons off the container, preventing it from interacting with any object not explicitly labeled for container use.</p>
<p>But SELinux doesn’t stop at type enforcement. It takes container isolation a step further with <a href="https://en.wikipedia.org/wiki/Multi_categories_security">Multi-Category Security (MCS)</a> labels. These labels function as an additional layer of segregation, ensuring that even containers of the same type (<strong>container_t</strong>) remain isolated from each other. Each container gets its own unique MCS label, creating what amounts to a private sandbox within the broader <strong>container_t</strong> environment.</p>
<p>AppArmor, in contrast, doesn’t concern itself with types or categories. It focuses on limiting the capabilities of specific programs based on pre-defined profiles. These profiles specify which files a program can access and which operations it can perform. While this approach is more straightforward to implement and understand, it lacks the granularity and system-wide consistency of SELinux’s type enforcement. Almost no mainstream Linux distribution distributes comprehensive AppArmor profiles for all common network-facing daemons by default.</p>
<p>The practical implications of these differences are significant. In a SELinux environment, a compromised container faces substantial hurdles in accessing or affecting the host system or other containers, thanks to the dual barriers of type enforcement and MCS labels.</p>
<p>This isn’t to say one is universally superior to the other. SELinux offers more robust isolation but at the cost of increased complexity and potential performance overhead. AppArmor provides a simpler, more approachable security model that can still be quite effective when configured properly. The root of my point though is that Red Hat has put in the work here to make the use of SELinux and containers seamless and easy for its users. You aren’t left to fend for yourself.</p>
<p>In the end, the choice between Debian and Red Hat isn’t just about corporate influence versus community-driven development. It’s also a choice between a system that assumes the best and one that prepares for the worst. Unfortunately in today’s highly connected world, pessimism is a necessity.</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boom Supersonic's XB-1 prototype aces 2nd test flight (117 pts)]]></title>
            <link>https://www.space.com/boom-supersonic-xb-1-second-test-flight-photos</link>
            <guid>41446337</guid>
            <pubDate>Wed, 04 Sep 2024 14:38:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.space.com/boom-supersonic-xb-1-second-test-flight-photos">https://www.space.com/boom-supersonic-xb-1-second-test-flight-photos</a>, See on <a href="https://news.ycombinator.com/item?id=41446337">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div itemprop="image" itemscope="" itemtype="https://schema.org/ImageObject">
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ-1920-80.jpg.webp 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ-320-80.jpg" alt="a needle-nosed white and silver plane flies over a desert-mountain landscape" srcset="https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ-1920-80.jpg 1920w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<meta itemprop="url" content="https://cdn.mos.cms.futurecdn.net/4rUC9BLjmW7yNbCoMWJVJ.jpg">
<meta itemprop="height" content="600">
<meta itemprop="width" content="338">
<figcaption itemprop="caption description">
<span>Boom Supersonic's XB-1 prototype conducts its second-ever test flight on Aug. 26, 2024.</span>
<span itemprop="copyrightHolder">(Image credit: Boom Supersonic)</span>
</figcaption>
</div>

<div id="article-body">
<p>Colorado company Boom Supersonic's XB-1 supersonic demonstrator aircraft flew for the second time ever on Monday (Aug. 26).</p><p>The flight took place from California's Mojave Air and Space Port and lasted about 15 minutes, seeing the <a data-analytics-id="inline-link" href="https://www.space.com/boom-xb-1-faa-approval-supersonic-test-flight" data-before-rewrite-localise="https://www.space.com/boom-xb-1-faa-approval-supersonic-test-flight"><u>XB-1</u></a> reach an altitude of 10,400 feet (3,170 meters) and a speed of 277 mph (446 kph).</p><p>The flight demonstrated landing gear being retracted and extended for the first time, and a new digital stability augmentation system was tested to improve handling.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/kcrxVLha8QLCBh88tXXAUB-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/kcrxVLha8QLCBh88tXXAUB-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/kcrxVLha8QLCBh88tXXAUB-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/kcrxVLha8QLCBh88tXXAUB-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/kcrxVLha8QLCBh88tXXAUB-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/kcrxVLha8QLCBh88tXXAUB-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/kcrxVLha8QLCBh88tXXAUB-320-80.jpg" alt="a needle-nosed white and silver plane comes in for a landing at a desert airstrip" srcset="https://cdn.mos.cms.futurecdn.net/kcrxVLha8QLCBh88tXXAUB-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/kcrxVLha8QLCBh88tXXAUB-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/kcrxVLha8QLCBh88tXXAUB-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/kcrxVLha8QLCBh88tXXAUB-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/kcrxVLha8QLCBh88tXXAUB-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/kcrxVLha8QLCBh88tXXAUB-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/kcrxVLha8QLCBh88tXXAUB.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/kcrxVLha8QLCBh88tXXAUB.jpg"></picture></p></div><figcaption itemprop="caption description"><span>The flight took place from California's Mojave Air and Space Port. </span><span itemprop="copyrightHolder">(Image credit: Boom Supersonic)</span></figcaption></figure><p>The test marks another step toward achieving supersonic flight, expected later in the year, according to a company <a data-analytics-id="inline-link" href="https://boomsupersonic.com/flyby/xb-1-completes-second-flight" target="_blank" data-url="https://boomsupersonic.com/flyby/xb-1-completes-second-flight" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><u>statement</u></a>.</p><p>"XB-1 had a fantastic second flight this morning. Initial results indicate we've successfully resolved the findings from Flight One and are excited to continue flight testing on the path to supersonic flight," said Blake Scholl, founder and CEO of Boom Supersonic. "I'm proud of the team. Today's flight is another step toward the return of supersonic passenger travel."</p><p><strong>Related:</strong> <a data-analytics-id="inline-link" href="https://www.space.com/nasa-mars-rover-perseverance-speed-of-sound" data-before-rewrite-localise="https://www.space.com/nasa-mars-rover-perseverance-speed-of-sound">The speed of sound on Mars is different from Earth, Perseverance rover finds</a></p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/7HhEJAPVarGBDBVhDHWzbM-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/7HhEJAPVarGBDBVhDHWzbM-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/7HhEJAPVarGBDBVhDHWzbM-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/7HhEJAPVarGBDBVhDHWzbM-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/7HhEJAPVarGBDBVhDHWzbM-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/7HhEJAPVarGBDBVhDHWzbM-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/7HhEJAPVarGBDBVhDHWzbM-320-80.jpg" alt="closeup photo of a needle-nosed white and silver plane on a runway" srcset="https://cdn.mos.cms.futurecdn.net/7HhEJAPVarGBDBVhDHWzbM-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/7HhEJAPVarGBDBVhDHWzbM-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/7HhEJAPVarGBDBVhDHWzbM-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/7HhEJAPVarGBDBVhDHWzbM-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/7HhEJAPVarGBDBVhDHWzbM-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/7HhEJAPVarGBDBVhDHWzbM-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/7HhEJAPVarGBDBVhDHWzbM.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/7HhEJAPVarGBDBVhDHWzbM.jpg"></picture></p></div><figcaption itemprop="caption description"><span>Boom Supersonic plans to ramp up its flight-test campaign with the XB-1 soon.&nbsp; </span><span itemprop="copyrightHolder">(Image credit: Boom Supersonic)</span></figcaption></figure><p>The XB-1 test plane had its <a data-analytics-id="inline-link" href="https://www.space.com/boom-xb-1-faa-approval-supersonic-test-flight" data-before-rewrite-localise="https://www.space.com/boom-xb-1-faa-approval-supersonic-test-flight">first flight</a> in March this year. The company now intends to ramp up its flight rate and plans around 10 tests before reaching for supersonic speeds.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-6ZtMezCJiku2rwsHDsW9UD"><section><p>Breaking space news, the latest updates on rocket launches, skywatching events and more!</p></section></div><p>The XB-1 program is part of the design and development process for Boom's flagship project, Overture, a planned supersonic airliner. Boom Supersonic aims to revolutionize air travel by making it much faster and more efficient.</p>
</div>
<p><em><a href="https://forums.space.com/">Join our Space Forums</a> to keep talking space on the latest missions, night sky and more! And if you have a news tip, correction or comment, let us know at: <a href="mailto:community@space.com">community@space.com.</a></em></p>
<div id="slice-container-authorBio-6ZtMezCJiku2rwsHDsW9UD"><p>Andrew&nbsp;is a freelance space journalist with a focus on reporting on China's rapidly growing space sector. He began writing&nbsp;for Space.com in 2019 and writes for SpaceNews, IEEE Spectrum, National Geographic, Sky &amp; Telescope, New Scientist and others.&nbsp;Andrew&nbsp;first caught the space bug when, as a youngster,&nbsp;he&nbsp;saw Voyager images of other worlds in our solar system for the first&nbsp;time.&nbsp;Away from space,&nbsp;Andrew&nbsp;enjoys trail running in the forests of Finland.&nbsp;You can follow him on Twitter&nbsp;<a href="https://twitter.com/AJ_FI" target="_blank">@AJ_FI</a>.</p></div>


</section>





<div id="slice-container-relatedArticles"><p><h5>Most Popular</h5></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Physics is unreasonably good at creating new math (197 pts)]]></title>
            <link>https://nautil.us/why-physics-is-unreasonably-good-at-creating-new-math-797056/</link>
            <guid>41445790</guid>
            <pubDate>Wed, 04 Sep 2024 13:43:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nautil.us/why-physics-is-unreasonably-good-at-creating-new-math-797056/">https://nautil.us/why-physics-is-unreasonably-good-at-creating-new-math-797056/</a>, See on <a href="https://news.ycombinator.com/item?id=41445790">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                        
            <p><span>M</span>athematics has long been the basis for advances in physics. Albert Einstein hailed general relativity as “a real triumph” for mathematics in 1915, when he discovered that purely mathematical work, more than a half-century old, perfectly described the fabric of spacetime in his theory of gravity. How could math conceived with no application in mind, he wondered later, prove “so admirably appropriate to the objects of reality?”</p><p>Math’s service to physics, now often taken for granted, is rooted in its origins. Mathematics was, after all, invented for surveying, quantifying, and understanding the physical world. In Mesopotamia, the Sumerians developed a counting system, leaving behind clay tablets inscribed with multiplication tables. Their purpose? To tally goods and property. In the intervening millennia, what began as a tool to grease the wheels of government and commerce took on a life of its own. But, while expanding into areas of abstraction so obscure they can only be grasped after years of training, mathematics has continued to underpin the great breakthroughs in physics.</p>
      
    <p>Recently, though, the tables have turned. Now insights and intuitions from physics are unexpectedly leading to breakthroughs in mathematics. After going their own way for much of the 20th century, mathematicians are increasingly turning to the laws and patterns of the natural world for inspiration. Fields stuck for decades are being unstuck. And even philosophers have started to delve into the mystery of why physics is proving “unreasonably effective” in mathematics, as one has boldly declared. That question hinges on a largely unappreciated, perplexing, and profound link between the rules that govern the behavior of the cosmos and the most abstract musings of the human mind.</p><blockquote>
<p>The experience of mathematical beauty excites the same parts of the brain as beautiful music, art, or poetry.&nbsp;</p>
</blockquote>
          <p>Why <em>should</em> physics—rooted in making sense of real things in the world like apples and electrons—provide such good leads for solving some of the toughest problems in mathematics, which deals with intangible stuff, like functions and equations?</p><p>“Physicists are much less concerned than mathematicians about rigorous proofs,” says Timothy Gowers, a mathematician at the Collège de France and a Fields Medal winner. Sometimes, he says, that “allows physicists to explore mathematical terrain more quickly than mathematicians.” If mathematicians tend to survey—in great depth—small parcels of this landscape, physicists are more likely to skim rapidly over vast tracts of this largely uncharted territory. With this perspective, physicists can happen across new, powerful mathematical concepts and associations, to which mathematicians can return, to try and justify (or disprove) them.</p><p><span>T</span>he process of physics inspiring mathematics is, in fact, as old as science itself. The ancient Greek mathematician and inventor Archimedes described how the laws of mechanics had spurred some of his most important mathematical discoveries. Then there’s Isaac Newton, who (alongside his contemporary, the German polymath Gottfried Wilhelm Leibniz) famously developed an entirely new kind of math—calculus—while trying to understand the motion of falling objects.</p><p>But in the middle of the 20th century, the flow of new math from physics all but dried up. Neither physicists nor mathematicians were much interested in what was happening on the other side of the fence. In mathematics, an influential set of young French mathematicians called the Bourbaki group sought to make mathematics as precise as possible, rebuilding whole fields from scratch and publishing their collaborative work in an effort—they hoped—to facilitate future discoveries. Physicists, meanwhile, were excitedly developing path-breaking ideas such as the Standard Model—still physicists’ best theory of the atomic and subatomic world. For many of them, math was just a handy tool, and they had no interest in the austere vision of mathematics championed by the Bourbakis.</p>
          <p>Yet a reconciliation was afoot, spearheaded by the late British-Lebanese geometer Michael Atiyah. With rare intuition, and a little luck, Atiyah, also a Fields medalist, often alighted on areas that would later be of interest to theoretical physicists.</p><p>“In the mid 1970s, he became convinced that theoretical physics was by far the most promising source of new ideas,” Nigel Hitchin, a mathematician and emeritus professor at the University of Oxford who collaborated with Atiyah <a href="https://ncatlab.org/nlab/files/HitchinOnAtiyahGeometryPhysics.pdf" target="_blank" rel="noreferrer noopener">wrote</a> in 2020 of his former colleague. “From that point on, he became a facilitator of interactions between mathematicians and physicists, attacking mathematical challenges posed by physicists, using physical ideas to prove pure mathematical results, and feeding the physicist community with the parts of modern mathematics he regarded as important but were unfamiliar to them.”&nbsp;</p><figure><img width="800" height="725" alt="In Body Image" src="https://assets.nautil.us/sites/3/nautilus/Bhattacharya_BREAKER.png?auto=compress&amp;fit=scale&amp;fm=png&amp;h=928&amp;ixlib=php-3.3.1&amp;w=1024&amp;wpsize=large" srcset="https://assets.nautil.us/sites/3/nautilus/Bhattacharya_BREAKER.png?q=65&amp;auto=format&amp;w=1600 800w,https://assets.nautil.us/sites/3/nautilus/Bhattacharya_BREAKER.png?q=65&amp;auto=format&amp;w=1200 600w,https://assets.nautil.us/sites/3/nautilus/Bhattacharya_BREAKER.png?q=65&amp;auto=format&amp;w=800 400w" loading="lazy"><figcaption><strong>STICKY PROBLEM: </strong>Physicist Philip Candelas and his collaborators used tools from string theory to solve a sticky problem in enumerative geometry: counting the number of certain kinds of curves in a Calabi-Yau manifold (pictured here), strange six-dimensional shapes central to string theory. <em>Credit: Wikimedia Commons.</em></figcaption></figure><p>One of Atiyah’s long-term collaborators, who he first met in 1977, was the mathematical physicist Edward Witten. More than 20 years Atiyah’s junior, Witten later became a pioneer of string theory, an idea that posits that tiny one-dimensional vibrating strings are the fundamental building blocks of the universe, rather than the particles of the Standard Model.</p>
          <p>Initially hailed as a possible “theory of everything” that would unite quantum theory with Einstein’s theory of gravity, string theory has to date arguably had a bigger impact on some of the most abstract fields of mathematics, such as <a href="https://nautil.us/the-math-that-takes-newton-into-the-quantum-world-237339/" target="_blank" rel="noreferrer noopener">algebraic geometry</a> and differential topology, than in physics. In these areas, Witten and other string theorists have been able to produce precise conjectures that mathematicians have later proved.</p><p>In 1991, for example, physicists Philip Candelas, Xenia de la Ossa, and their colleagues <a href="https://webspace.science.uu.nl/~beuke106/HypergeometricFunctions/COGP.pdf" target="_blank" rel="noreferrer noopener">applied</a> string theory to a decades-old puzzle in enumerative geometry, an ancient branch of mathematics dedicated to counting the number of solutions to geometrical problems. At its simplest, this asks questions such as “How many lines can pass through two points on a plane?” (One.) Or Apollonius’ famous problem, “How many circles can be drawn that touch (are tangent to) three given circles?” (Eight.)</p><p>Candelas and his collaborators were able to use tools from string theory to solve a particularly sticky problem in enumerative geometry: counting the number of certain kinds of curves in Calabi-Yau manifolds, strange six-dimensional shapes that are central to string theory. Their result connected two kinds of geometry, “symplectic” and “complex,” that mathematicians had studied in isolation from each other for decades, thinking they were unrelated. This sort of advance—one that connects two fields that were thought to be unrelated—is considered a “deep” result in math: You can suddenly use tools from one to solve problems in the other, enabling and accelerating advances.</p><p>Just a few years later, in 1995, Witten proposed that five different versions of string theory, each requiring 10 dimensions, were all different aspects of a single 11-dimensional conceptualization he called “M-theory.” Though M-theory remains unproven, mapping the correspondences between the different theories has led to startling mathematical discoveries. “It feels like every month string theory is giving new structures to mathematicians in an unprecedented way,” says mathematical physicist Yang-Hui He of the London Institute for Mathematical Sciences.</p>
          <blockquote>
<p>The sort of math that emerges from studying reality is the sort our brains tend to like.</p>
</blockquote><p>That string theory is a rich source of such unexpected relationships, or “dualities,” between two mathematical worlds, continues to excite mathematicians today. Physicist He and his collaborator string theorist Federico Carta, also of the London Institute, were studying the simplest type of Calabi-Yau manifold, called a K3 surface, when they stumbled across a relationship between the surface’s “homotopy groups,” which are used to classify shapes in topology, and a symmetry group, called “Matthieu 24.” The pair’s discovery reveals an unanticipated connection between two disparate fields of pure mathematics—topology, the study of shapes, and an area of modern algebra called group theory, which concerns the types of symmetry that objects possess.</p><p>Why physics should give rise to such interesting mathematics, says He, is a “deep question.” There are an infinite number of patterns and structures that mathematicians could study, he says. “But the ones which come from reality are ones which we have an intuition about at some level.”</p><p>Hitchin agrees. “Mathematical research doesn’t operate in a vacuum,” he says. “You don’t sit down and invent a new theory for its own sake. You need to believe that there is something there to be investigated. New ideas have to condense around some notion of reality, or someone’s notion, maybe.”</p>
          <p>This raises the question of whether physics feeds math merely by providing a keener motivation for exploring it and a focus for mathematicians’ energies. Guided by intuitions about how the world should work, and a plausible endpoint, mathematicians can sometimes make speedier progress on a problem than they otherwise would.&nbsp;</p><p>It would also explain a curious fact: “Bad” physics can sometimes lead to good math.&nbsp;</p><p>Vortex theory, for instance, was an early attempt by British mathematical physicist William Thomson (Lord Kelvin) to explain why atoms came in a relatively small number of varieties. He pictured atoms as spinning rings which could be tied in intricate knots, with each knot corresponding to a different chemical element. The theory was abandoned after the discovery of the electron—but the mathematics led to the development of knot theory, which has since both become a fecund area for pure mathematicians to explore and found surprising applications in fluid dynamics and for understanding tangled molecules like DNA.</p><p><span>F</span>or Atiyah, the enigmatic relationship between physics and math all came down to the human brain. “Humans are a product of long evolution, in which powerful brains were an advantage. Such brains evolved in the physical world, so evolutionary success was measured by physical success,” he explained in a 2018 <a href="https://www.uv.es/~azcarrag/pdf/2018%20REF%20Conversation%20Atiyah%20English.pdf" target="_blank" rel="noreferrer noopener">interview</a>. “Hence human brains evolved to solve physical problems and this required the brain to develop the right kind of mathematics.” To do so, the brain must also have adapted to recognizing and appreciating mathematical patterns in nature. Atiyah even co-authored a brain-imaging <a href="https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2014.00068/full" target="_blank" rel="noreferrer noopener">study</a> in 2014 that concluded the experience of mathematical beauty excites the same parts of the brain as beautiful music, art, or poetry. That might explain why physics can be a lodestar for mathematicians: The sort of math that emerges from studying reality is the sort our brains tend to like.</p>
          <p>In a 2010 <a href="https://royalsocietypublishing.org/doi/full/10.1098/rsta.2009.0227" target="_blank" rel="noreferrer noopener">paper</a> with Hitchin and Dutch theoretical physicist <a href="https://nautil.us/are-there-barbarians-at-the-gates-of-science-235904/" target="_blank" rel="noreferrer noopener">Robbert Dijkgraaf</a>, then of Princeton University, Atiyah went on to further highlight the successful use of physics in mathematics. Since then, however, there has been scant work to try to understand the phenomenon.</p><p>One philosopher who has recently re-examined the issue is Daniele Molinini, at the University of Bologna. His 2023 <a href="https://www.journals.uchicago.edu/doi/abs/10.1086/715104" target="_blank" rel="noreferrer noopener">paper</a>, published in <em>The British Journal for the Philosophy of Science</em>, responded to an oft-cited 1960 essay entitled “The Unreasonable Effectiveness of Mathematics in the Natural Sciences” written by Nobel laureate physicist Eugene Wigner. Molinini’s cheeky response instead explored “The Unreasonable Effectiveness of Physics in Mathematics.” His surprising answer is that some laws of physics may be as incontrovertible as a mathematical theorem. “There are some principles about the world that we have to take as fundamental,” he says.</p><p>Philosophers broadly agree that mathematical truths are “necessary,” in that they have to be true in all possible worlds. Truths about nature, empirical facts, are different—they’re contingent. Light travels at a certain speed, but arguably it could have been otherwise in a differently set up universe. That is, mathematical truths have been, and will always be, true, no matter what.&nbsp;</p><p>Might there be certain laws of physics that are also “necessary” in the same way? In his paper, Molinini argues that the principle of conservation may be one such law. In physics, some properties of a system, such as energy or momentum, can’t change. A bicyclist freewheeling down a hill, for example, is converting her gravitational potential energy into movement energy, but the total amount of energy she and her bike have stays the same.</p>
          <blockquote>
<p>The universe itself is not just described by mathematics but is made of mathematics.</p>
</blockquote><p>If such conservation is “necessary,” Molinini contends, that may explain how Archimedes was able to successfully infer the truth of geometric proofs by mechanical considerations, a feat which is otherwise puzzling. The physics and the math, in this case, are two sides of the same coin: Both are true because they draw on the same fundamental principle.</p><p>Another view, famously articulated in the early 17th century by Galileo and often championed by mathematicians, is that the universe is written in the language of mathematics. That idea has ancient origins, going back to at least Pythagoras and his followers, but a more recent, and extreme, version is <a href="https://nautil.us/ingenious-max-tegmark-234731/" target="_blank" rel="noreferrer noopener">Max Tegmark</a>’s mathematical universe hypothesis, in which the universe itself is not just described by mathematics but is <a href="https://nautil.us/life-is-a-braid-in-spacetime-234729/" target="_blank" rel="noreferrer noopener"><em>made</em> of mathematics</a>.</p><p>In Tegmark’s telling, our universe is just one of an infinite number of parallel universes and all the infinite possibilities of mathematics—every theorem, every proof—are realized somewhere in this multiverse. So, no wonder physics inspires new discoveries in math—the reality physics describes is, at bottom, mathematical anyway. “There’s an intimate connection between empirical science and mathematics,” says Mark Colyvan, a philosopher at the University of Sydney who has studied the relationship between math and physics. “One conclusion one could draw is that somehow the world itself is mathematical.”</p>
          <p>In both cases, however, the mathematics of known physics is just a tiny fraction of all the mathematics out there (nearly all of which is likely to be far less interesting), so this view doesn’t really explain why math emerging from physics should be unusually rich.&nbsp;</p><p>Molinini is now challenging a popular philosophical explanation for math’s applicability, “<a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0068.2010.00772.x" target="_blank" rel="noreferrer noopener">mapping</a>,” which he believes can’t account for why good math can flow from physics. Mapping suggests that math is applied to physics by turning physical concepts, like mass or separation, into mathematical entities, such as the equation for Newton’s law of gravitation, which can then be used to calculate something that is then mapped back into a physical property—for instance, the attraction between two objects. But Molinini argues that that process of mapping breaks down when one tries to reverse it to explain how math can emerge from physics.&nbsp;</p><p>There is burgeoning interest in this question from philosophers, he says, who have until now focused on the converse problem of why math can be applied to the empirical sciences.</p><p>“Modern physics is providing mathematicians with a whole host of new tools and unexpected leads,” says the London Institute’s He. “And in future, physics and math will need to work together even more closely to solve some of the biggest problems in pure math.”</p>
          <p>The Langlands program, conceived by Robert Langlands in the 1960s and often called “the grand unified theory of mathematics,” is one such area, says He. One arm of the program, the geometric Langlands, was purportedly recently settled by a team of mathematicians who presented a proof spanning five papers and 800 pages. A kernel of that proof rests on insights originally drawn from conformal field theory, a branch of physics that is a cornerstone of string theory, among other areas. He believes that mathematicians will need to draw on more physics to explore the implications of that proof, as well as to make progress on the other arms of the program.</p><p>Similarly, mathematicians have already tapped physics in their attempts to make progress on the Riemann hypothesis and Birch and Swinnerton-Dyer conjecture—two of the most challenging open problems in math. An alliance between the two fields, suspects He, will be key to finally unlocking these behemoths.</p><p>“Physics and math are starting to become one again, like they were in Newton and Gauss’ day,” says He, who trained as a theoretical physicist but is increasingly drawn to applying physical ideas to problems in pure math.</p><p>It’s an intriguing thought. The story of the universe might be written in the language of mathematics. But beautiful as the tale seems to be, the signs are that understanding more than physicists already do will demand increasingly exotic and sophisticated mathematical tools, some of which are yet to be invented. Breaking down the barriers between the two fields could open new worlds of understanding in both.&nbsp; <img decoding="async" src="https://assets.nautil.us/sites/3/nautilus/nautilus-favicon-14.png?fm=png" alt=""></p>
          <p><em>Lead image: Art Furnace / Shutterstock</em></p>              
                            <ul>
                                      <li>
                      <div>
                        <h6>
                          Ananyo Bhattacharya                        </h6>
                        <p>
                          Posted on <time datetime="2024-09-03T10:38:54-05:00">September 3, 2024</time>
                        </p>
                      </div>
                                                <p>
                            Ananyo Bhattacharya is chief science writer at the London Institute for Mathematical Sciences. During a 15-year career in journalism, he has worked as a senior editor at <i>Nature</i> and as a science correspondent for <i>The Economist</i>. He is the author of <i><a href="https://wwnorton.com/books/the-man-from-the-future">The Man from the Future</a></i>, an intellectual biography of mathematician John von Neumann.                          </p>
                                            </li>
                                  </ul>
            <div>
  <p><img src="https://nautil.us/wp-content/themes/nautilus-block-theme/images/icons/logo-icon.svg" alt="new_letter"></p><div>
    <h4>Get the Nautilus newsletter</h4>
    <p>Cutting-edge science, unraveled by the very brightest living thinkers.</p>
  </div>

  
</div>        </div></div>]]></description>
        </item>
    </channel>
</rss>