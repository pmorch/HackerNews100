<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 24 Aug 2025 10:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[How to build a coding agent (206 pts)]]></title>
            <link>https://ghuntley.com/agent/</link>
            <guid>45001051</guid>
            <pubDate>Sun, 24 Aug 2025 03:21:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ghuntley.com/agent/">https://ghuntley.com/agent/</a>, See on <a href="https://news.ycombinator.com/item?id=45001051">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <div><p>😎</p><p>The following was developed last month and has already been delivered at two conferences. If you would like for me to run a workshop similar to this at your employer, please <a href="https://ghuntley.com/workshops/" rel="noreferrer">get in contact</a>.</p></div><figure><a href="https://github.com/ghuntley/how-to-build-a-coding-agent?ref=ghuntley.com"><div><p>GitHub - ghuntley/how-to-build-a-coding-agent: A workshop that teaches you how to build your own coding agent. Similar to Roo code, Cline, Amp, Cursor, Windsurf or OpenCode.</p><p>A workshop that teaches you how to build your own coding agent. Similar to Roo code, Cline, Amp, Cursor, Windsurf or OpenCode. - ghuntley/how-to-build-a-coding-agent</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-23.svg" alt=""><span>ghuntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/how-to-build-a-coding-agent-2" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>workshop materials and source code for the workshop can be found here</span></p></figcaption></figure><p>Hey everyone, I'm here today to teach you how to build a coding agent.  By this stage of the conference, you may be tired of hearing the word "agent". </p><p>You hear the word frequently. However, it appears that everyone is using this term loosely without a clear understanding of what it means or how these coding agents operate internally. It's time to pull back the hood and show that there is no moat.</p><p>Learning how to build a coding agent is one of the best things you can do for your personal development in 2025, as it teaches you the fundamentals. Once you understand these fundamentals, you'll move from being a consumer of AI to a producer of AI who can automate things with AI.</p><p>Let me open with the following facts:</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.002.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.002.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.002.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.002.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.002.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>it's not that hard</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.003.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.003.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.003.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.003.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.003.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>to build a coding agent</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.004.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.004.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.004.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.004.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.004.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>it's 300 lines of code</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.005.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.005.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.005.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.005.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.005.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>running in a loop</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.006.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.006.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.006.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.006.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.006.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>With LLM tokens, that's all it is. </p><p>300 lines of code running in a loop with LLM tokens. You just keep throwing tokens at the loop, and then you've got yourself an agent.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.007.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.007.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.007.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.007.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.007.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Today, we're going to build one. We're going to do it live, and I'll explain the fundamentals of how it all works. As we are now in 2025, it has become the norm to work concurrently with AI assistance. So, what better way to demonstrate the point of this talk than to have an agent build me an agent whilst I deliver this talk? </p><figure data-kg-thumbnail="https://ghuntley.com/content/media/2025/08/Let-s-build-an-agent-AFK_thumb.jpg" data-kg-custom-thumbnail="https://ghuntley.com/content/images/2025/08/Screenshot-2025-08-24-125648.png">
            <div>
                <video src="https://ghuntley.com/content/media/2025/08/Let-s-build-an-agent-AFK.mp4" poster="https://img.spacergif.org/v1/1920x1080/0a/spacer.png" width="1920" height="1080" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:22</span>
                        </p>
                        </div>
            </div>
            
        <img src="https://ghuntley.com/content/media/2025/08/Let-s-build-an-agent-AFK_thumb.jpg"></figure><p>Cool. We're now building an agent. This is one of the things that's changing in our industry, because work can be done concurrently and whilst you are away from your computer. </p><p>The days of spending a week or a couple of days on a research spike are now over because you can turn an idea into execution just by speaking to your computer. </p><p>The next time you're on a Zoom call, consider that you could've had an agent building the work that you're planning to do during that Zoom call. If that's not the norm for you, and it is for your coworkers, then you're naturally not going to get ahead.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.009.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.009.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.009.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.009.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.009.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>please build your own</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.010.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.010.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.010.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.010.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.010.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>as the knowledge</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.011.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.011.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.011.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.011.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.011.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>will transform you</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.012.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.012.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.012.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.012.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.012.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>from being a consumer</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.013.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.013.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.013.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.013.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.013.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>to a producer that can</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.014.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.014.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.014.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.014.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.014.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>automate things</span></figcaption></figure><p>The tech industry is almost like a conveyor belt - we always need to be learning new things. </p><p>If I were to ask you what a primary key is, you should know what a primary key is. That's been the norm for a long time.  </p><p>In 2024, it is essential to understand what a primary key is. </p><p>In 2025, you should be familiar with what a primary key is and how to create an agent, as knowing what this loop is and how to build an agent is now fundamental knowledge that employers are looking for in candidates before they'll let you in the door.</p><figure><a href="https://www.canva.dev/blog/engineering/yes-you-can-use-ai-in-our-interviews/?ref=ghuntley.com"><div><p>Yes, You Can Use AI in Our Interviews. In fact, we insist - Canva Engineering Blog</p><p>How We Redesigned Technical Interviews for the AI Era</p><p><img src="https://ghuntley.com/content/images/icon/apple-touch-180x180.b6aae4a9-3.webp" alt=""><span>canva.dev</span><span>Simon Newton</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/thumbnail.1c7590f6-3.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>As this knowledge will transform you from being a consumer of AI to being a producer of AI that can orchestrate your job function. Employers are now seeking individuals who can automate tasks within their organisation.</p><p>If you're joining me later this afternoon for the conference closing (see below), I'll delve a bit deeper into the above. </p><figure><a href="https://ghuntley.com/six-month-recap/"><div><p>the six-month recap: closing talk on AI at Web Directions, Melbourne, June 2025</p><p>Welcome back to our final session at WebDirections. We’re definitely on the glide path—though I’m not sure if we’re smoothly landing, about to hit turbulence, or perhaps facing a go-around. We’ll see how it unfolds. Today, I’m excited to introduce Geoffrey Huntley. I discovered Geoff earlier this year through</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-42.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/the-future-belongs-to-people-who-do-things.001-2.png" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>the conference closing talk</span></p></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.015.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.015.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.015.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.015.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.015.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Right now, you'll be somewhere on the journey above. </p><p>On the top left, we've got 'prove it to me, it's not real,' 'prove it to me, show me outcomes', 'prove it to me that it's not hype', and a bunch of 'it's not good enough' folks who get stuck up there on that left side of the cliff, completely ignoring that there are people on the other side of the cliff, completely automating their job function.</p><blockquote>In my opinion, any disruption or job loss related to AI is not a result of AI itself, but rather a consequence of a lack of personal development and self-investment. If your coworkers are hopping between multiple agents, chewing on ideas, and running in the background during meetings, and you're not in on that action, then naturally you're just going to fall behind.</blockquote><figure><a href="https://ghuntley.com/ngmi/"><div><p>What do I mean by some software devs are “ngmi”?</p><p>At “an oh fuck moment in time”, I closed off the post with the following quote. N period on from now, software engineers who haven’t adopted or started exploring software assistants, are frankly not gonna make it. Engineering organizations right now are split between employees who have had that “oh</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-45.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/fruits-2.png" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>don't be the person on the left side of the cliff.</span></p></figcaption></figure><p>The tech industry's conveyor belt continues to move forward. If you're a DevOps engineer in 2025 and you don't have any experience with AWS or GCP, then you're going to find it pretty tough in the employment market. </p><p>What's surprising to software and data engineers is just how fast this is elapsing. It has been eight months since the release of the first coding agent, and most people are still unaware of how straightforward it is to build one, how powerful this loop is, and its disruptive implications for our profession.</p><p>So, my name's Geoffrey Huntley. I was the tech lead for developer productivity at Canva, but as of a couple of months ago, I'm one of the engineers at Sourcegraph building <a href="https://ampcode.com/?ref=ghuntley.com" rel="noreferrer">Amp</a>. It's a small core team of about six people. We build AI with AI.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.016.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.016.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.016.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.016.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.016.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>ampcode.com</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.017.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.017.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.017.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.017.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.017.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>cursor</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.018.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.018.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.018.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.018.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.018.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>windsurf</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.019.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.019.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.019.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.019.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.019.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>claude code</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.020.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.020.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.020.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.020.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.020.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>github co-pilot</span></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.022.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.022.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.022.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.022.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.022.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>are lines of code running in a loop with LLM tokens</span></figcaption></figure><p>Cursor, Windsurf, Claude Code, GitHub Copilot, and Amp are just a small number of lines of code running in a loop of LLM tokens. I can't stress that enough. The model does all the heavy lifting here, folks. It's the model that does it all. </p><p>You are probably five vendors deep in product evaluation, right now, trying to compare all these agents to one another. But really, you're just chasing your tail. </p><p>It's so easy to build your own...</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.023.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.023.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.023.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.023.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.023.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>There are just a few key concepts you need to be aware of.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.024.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.024.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.024.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.024.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.024.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Not all LLMs are agentic. </p><p>The same way that you have different types of cars, like you've got a <a href="https://en.wikipedia.org/wiki/Toyota_Land_Cruiser_(J40)?ref=ghuntley.com">40 series</a> if you want to go off-road, and then you've also got people movers, which exist for transporting people. </p><p>The same principle applies to LLMs, and I've been able to map their behaviours into a quadrant. </p><blockquote>A model is either high safety, low safety, an oracle, or agentic. It's never both or all.</blockquote><p>If I were to ask you to do some security research, which model would you use? </p><p>That'd be Grok. That's a low safety model.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.025.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.025.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.025.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.025.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.025.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>If you want something that's "ethics-aligned", it's Anthropic or OpenAI. So that's high safety. Similarly, you have oracles. Oracles are on the polar opposite of agentic. Oracles are suitable for summarisation tasks or require a high level of thinking. </p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.026.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.026.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.026.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.026.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.026.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Meanwhile, you have providers like Anthropic, and their Claude Sonnet is a digital squirrel (see below). </p><figure><a href="https://ghuntley.com/cars/"><div><p>Claude Sonnet is a small-brained mechanical squirrel of &lt;T&gt;</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-43.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/Traditional-tattoo-style-print.--Low-angle--dramatic-scene-a-mechanical-pump-amidst-a-stormy-sky--pumping-generic-tools.-Vivid-electric-colors--complex-ornamental-designs--retro-feel--white-background.jpg" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>The first robot used to chase tennis balls. The first digital robot chases tool calls.</span></p></figcaption></figure><p>Sonnet is a robotic squirrel that just wants to do tool calls. It doesn't spend too much time thinking; it biases towards action, which is what makes it agentic. Sonnet focuses on incrementally obtaining success instead of pondering for minutes per turn before taking action.</p><p>It seems like every day, a new model is introduced to the market, and they're all competing with one another. But truth be told, they have their specialisations and have carved out their niches. </p><p>The problem is that, unless you're working with these models at an intimate level, you may not have this level of awareness of the specialisations of the models, which results in consumers just comparing the models on two basic primitives:</p><ol><li>The size of the context window</li><li>The cost</li></ol><p>It's kind of like looking at a car, whether it has two doors or three doors, whilst ignoring the fact that some vehicles are designed for off-roading, while others are designed for passenger transport. </p><p>To build an agent, the first step is to choose a highly agentic model. That is currently Claude Sonnet, or Kimi K2. </p><p>Now, you might be wondering, "What if you want a higher level of reasoning and checking of work that the incremental squirrel does?". Ah, that's simple. You can wire other LLMs in as tools into an existing agentic LLM. This is what we do at Amp.</p><p>We call it the Oracle. The Oracle is just GPT wired in as a tool that Claude Sonnet can function call for guidance, to check work progress, and to conduct research/planning.</p><figure><a href="https://ampcode.com/news/oracle?ref=ghuntley.com"><div><p>Oracle</p><p><img src="https://ghuntley.com/content/images/icon/amp-mark-color-9.svg" alt=""><span>Amp</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/oracle.jpg" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>Amp's oracle is just another LLM registered in as a tool to an agentic LLM that it can function call</span></p></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.027.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.027.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.027.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.027.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.027.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>The next important thing to learn is that you should only use the context window for one activity. When you're using Cursor or any one of these tools, it's essential to clear the context window after each activity (see below).</p><figure><a href="https://ghuntley.com/gutter/"><div><p>autoregressive queens of failure</p><p>Have you ever had your AI coding assistant suggest something so off-base that you wonder if it’s trolling you? Welcome to the world of autoregressive failure. LLMs, the brains behind these assistants, are great at predicting the next word—or line of code—based on what’s been fed into</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-44.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/A-traditional-tattoo-style-print-of-a-bowling-ball-split-in-the-gutter--rendered-in-vibrant-colors-with-bold-lines-and-diffused-shadows.--The-image-features-a-retro-flair-and-complex-ornamental-details-against-a-white-background-2.jpg" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>LLM outcomtes are a needle in a haystack of what you've allocated into the haystack.</span></p></figcaption></figure><p>If you start an AI-assisted session to build a backend API controller, then reuse that session to research facts about meerkats. Then it should be no surprise when you tell it to redesign the website in the active session; the website might end up with facts about your API or meerkats, or both.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.028.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.028.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.028.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.028.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.028.png 1920w" sizes="(min-width: 720px) 720px"><figcaption><span>nb. the context window for Sonnet since delivering this workshop has increased to 1m</span></figcaption></figure><p>Context windows are very, very small. It's best to think of them as a Commodore 64, and as such, you should be treating it as a computer with a limited amount of memory. The more you allocate, the worse your outcome and performance will be.</p><p>The advertised context window for Sonnet is 200k. However, you don't get to use all of that because the model needs to allocate memory for the system-level prompt. Then the harness (Cursor, Windsurf, Claude Code, Amp) also needs to allocate some additional memory, which means you end up with approximately 176k tokens usable.</p><p>You probably heard a lot about the Model Context Protocols (MCPs). They are the current hot thing, and the easiest way to think about them is as a function with a description allocated to the context window that tells it how to invoke that function.</p><p>A common failure scenario I observe is people installing an excessive number of MCP servers or failing to consider the number of tools exposed by a single MCP tool or the aggregate context window allocation of all tools.</p><p>There is a cardinal rule that is not as well understood as it should be. The more you allocate to a context window, the worse the performance of the context window will be, and your outcomes will deteriorate.</p><p>Avoid excessively allocating to the context window with your agent or through MCP tool consumption. It's very easy to fall into a trap of allocating an additional 76K of tokens just for MCP tools, which means you only have 100K usable. </p><blockquote>Less is more, folks. Less is more. </blockquote><p>I recommend dropping by and reading the blog post below if you want to understand when to use MCP and when not to.</p><figure><a href="https://ghuntley.com/allocations/"><div><p>too many model context protocol servers and LLM allocations on the dance floor</p><p>This blog post intends to be a definitive guide to context engineering fundamentals from the perspective of an engineer who builds commercial coding assistants and harnesses for a living. Just two weeks ago, I was back over in San Francisco, and there was a big event on Model Context Protocol</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-52.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/Depict-AI-robots-dancing-on-a-rose-patterned-dance-floor-in-a-traditional-tattoo-art-style.-The-print-uses-a-vibrant-color-scheme--soft-lighting--and-diffused-shadows--creating-a-calming-atmosphere.-Complex-orna.jpg" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>When you should use MCP, when you should not use MCP, and how allocations work in the context window.</span></p></figcaption></figure><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.029.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.029.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.029.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.029.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.029.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Let's head back and check on our agent that's being built in the background. If you look at it closely enough, you can see the loop and how it's invoking other tools. </p><p>Essentially, how this all works is outlined in the loop below.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.030.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.030.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.030.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.030.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.030.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>For every piece of input from the user or result of a tool call that gets allocated to the response, and that response is sent off for inferencing:</p><figure><img src="https://ghuntley.com/content/images/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015242.png" alt="" loading="lazy" width="2000" height="382" srcset="https://ghuntley.com/content/images/size/w600/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015242.png 600w, https://ghuntley.com/content/images/size/w1000/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015242.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015242.png 1600w, https://ghuntley.com/content/images/size/w2400/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015242.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><span>The inferencing loop (minus tool registrations)</span></figcaption></figure><figure><a href="https://github.com/ghuntley/how-to-build-a-coding-agent?ref=ghuntley.com"><div><p>GitHub - ghuntley/how-to-build-a-coding-agent</p><p>Contribute to ghuntley/how-to-build-a-coding-agent development by creating an account on GitHub.</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-19.svg" alt=""><span>GitHub</span><span>ghuntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/how-to-build-a-coding-agent" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Let's open up our workshop materials (above) and run the basic chat application via:</p><pre><code>go run chat.go</code></pre><blockquote><strong>You:</strong> You are a weather expert. When I ask you about the weather in a given location, I want you to reply with <code>get_weather(&lt;location_name&gt;)</code>. I will then tell you what the weather in that location is. Understood?<p><strong>Claude:</strong> I understand your instructions. When you ask me about the weather in a specific location, I will respond with <code>get_weather(&lt;location_name&gt;)</code>. Then you will provide me with the weather information for that location, and I can discuss it with you from there. I'm ready whenever you'd like to ask about the weather somewhere.</p></blockquote><p>So, let's ask what the weather is today here in Melbourne, Australia?</p><blockquote><strong>You:</strong> What's the weather like today in Melbourne, Australia?<p><strong>Claude:</strong>  <code>get_weather(Melbourne, Australia)</code></p></blockquote><p>boom! There we go! </p><p>We've registered our first tool into the model, and any time that we tickle the latent space with a query about the weather, it's going to function call our new tool. </p><p>It's a stub tool currently, but you can see it; we have a method, and the LLM is passing the parameters into the method from our query.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.031.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.031.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.031.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.031.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.031.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>When we talk about agentic, we're referring to something that's tool-calling, something that has been trained on its reward function to call a tool. There are models where they've been explicitly trained; all they want to do is chase nuts, much like a squirrel (see below). It's not nuts, it's tools.</p><figure><a href="https://ghuntley.com/cars/"><div><p>Claude Sonnet is a small-brained mechanical squirrel of &lt;T&gt;</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-46.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/Traditional-tattoo-style-print.--Low-angle--dramatic-scene-a-mechanical-pump-amidst-a-stormy-sky--pumping-generic-tools.-Vivid-electric-colors--complex-ornamental-designs--retro-feel--white-background-1.jpg" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>Whilst visiting the Computer History Museum in San Francisco, I stumbled upon the first robotic squirrel, and it got me thinking: "Wait a sec, is Sonnet our first digital squirrel?"</span></p></figcaption></figure><p>In the simplest way to explain what MCP is or a tool, it's just a function with a billboard on top that nudges the LLM's latent space to invoke that function. It is that simple. You register the tool, you describe how to invoke it, and then it just works.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.032.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.032.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.032.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.032.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.032.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Let's start the short journey towards making this a real coding agent. We have a mock function and a mock tool, but <code>get_weather(Melbourne, Australia)</code> currently does nothing, so let's fix that.</p><p>What we need to do is create a tool and then adjust our inferencing loop to check whether it wants to execute the tool (see line 5 above). If it does, execute the tool (see line 6 above) and then allocate the outcome of that tool back (see line 7 above) to the loop for more inferencing.</p><p>Here's what our updated sequence diagram looks like when tools have been registered.</p><figure><img src="https://ghuntley.com/content/images/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015409.png" alt="" loading="lazy" width="1714" height="3840" srcset="https://ghuntley.com/content/images/size/w600/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015409.png 600w, https://ghuntley.com/content/images/size/w1000/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015409.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015409.png 1600w, https://ghuntley.com/content/images/2025/08/Untitled-diagram-_-Mermaid-Chart-2025-08-24-015409.png 1714w" sizes="(min-width: 720px) 720px"><figcaption><span>this is what a coding agent is in it's purest form without any UI</span></figcaption></figure><p>As we're not making a weather application today, let's switch gears to build the first primitive of a coding agent.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.034.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.034.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.034.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.034.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.034.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>The first primitive behind any coding agent is a read tool. All coding agents have a read tool that reads a file into the context window, either the whole file or, in the case of a larger file, in chunks.</p><pre><code>var ReadFileDefinition = ToolDefinition{
	Name:        "read_file",
	Description: "Read the contents of a given relative file path. Use this when you want to see what's inside a file. Do not use this with directory names.",
	InputSchema: ReadFileInputSchema,
	Function:    ReadFile,
}</code></pre><figure><a href="https://github.com/ghuntley/how-to-build-a-coding-agent?ref=ghuntley.com"><div><p>GitHub - ghuntley/how-to-build-a-coding-agent</p><p>Contribute to ghuntley/how-to-build-a-coding-agent development by creating an account on GitHub.</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-20.svg" alt=""><span>GitHub</span><span>ghuntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/how-to-build-a-coding-agent-1" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Let's open up our workshop materials once again (above) and create a file:</p><pre><code>cat &lt;&lt; EOF
I have a mane but I'm not a lion,
I have four legs but I'm not a table,
I can gallop but I'm not running,
People say I'm disagreeable because I always say "neigh."
What am I?
EOF &gt; riddle.txt</code></pre><p>Then let's run the read file tool:</p><pre><code>go run read.go</code></pre><blockquote><strong>You:</strong> Claude, mate. What's the answer to this riddle in <code>riddle.txt</code>?<p><strong>Claude:</strong> I'll help you solve the riddle. Let me first read the contents of the <code>riddle.txt</code> file to see what the riddle is.</p><p><strong>tool:</strong> <code>read_file({"path":"riddle.txt"})</code></p><p>Reading file: <code>riddle.txt</code><br>Successfully read file <code>riddle.txt</code> (175 bytes)</p><p>result: I have a mane but I'm not a lion,<br>I have four legs but I'm not a table,<br>I can gallop but I'm not running,<br>People say I'm disagreeable because I always say "neigh."<br>What am I?</p><p><strong>Claude:</strong> After reading the riddle, I can solve it for you:</p><p>The answer to the riddle is: a horse.</p><p>Let's analyze the clues:</p><p>"I have a mane but I'm not a lion" - Horses have manes, which is hair growing from the top of the neck.</p><p>"I have four legs but I'm not a table" - Horses walk on four legs.</p><p>"I can gallop but I'm not running" - Galloping is a specific gait of horses.</p><p>"People say I'm disagreeable because I always say 'neigh'" - "Neigh" is the sound a horse makes, and it sounds like "nay" (which means "no" or disagreement).</p><p>All of these clues point to a horse as the answer to the riddle.</p></blockquote><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.033.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.033.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.033.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.033.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.033.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>The second primitive behind any coding agent is a list tool. All coding agents have a list files tool, which lists all the files and directories in a given path.</p><pre><code>var ListFilesDefinition = ToolDefinition{
	Name:        "list_files",
	Description: "List files and directories at a given path. If no path is provided, lists files in the current directory.",
	InputSchema: ListFilesInputSchema,
	Function:    ListFiles,
}</code></pre><figure><a href="https://github.com/ghuntley/how-to-build-a-coding-agent?ref=ghuntley.com"><div><p>GitHub - ghuntley/how-to-build-a-coding-agent</p><p>Contribute to ghuntley/how-to-build-a-coding-agent development by creating an account on GitHub.</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-20.svg" alt=""><span>GitHub</span><span>ghuntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/how-to-build-a-coding-agent-1" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Let's open up our workshop materials once again (above) and run the basic chat application via:</p><pre><code>go run list_files.go</code></pre><blockquote><strong>You:</strong> Claude, mate. Is there  a <code>README.md</code> in this folder? Also what is the contents of <code>riddle.txt</code>?<p><strong>Claude:</strong>  I'll help you check for a README.md file and show you the contents of <code>riddle.txt</code>.</p><p>First, let me list the files in the current directory to see if <code>README.md</code> exists:</p><p><strong>tool:</strong> <code>list_files({})</code><br>Listing files in directory: .<br>Successfully listed 88 items in .</p><p>result:</p><p><code>[".devenv.flake.nix",".direnv/",".envrc",".gitignore","AGENT.md","Makefile","README.md","bash_tool.go","chat.go","code_search_tool.go","devenv.lock","devenv.nix","devenv.yaml","edit_tool.go","go.mod","go.sum","list_files.go","prompts/","prompts/00-weather.md","prompts/01-read_file.md","prompts/02-list_files.md","prompts/03-bash_tool.md","prompts/04-edit_tool.md","read.go","renovate.json","riddle.txt"]</code></p><p><strong>Claude:</strong> Yes, there is a <code>README.md</code> file in this folder.</p><p>Now, let me show you the contents of <code>riddle.txt</code>:</p><p>tool: <code>read_file({"path":"riddle.txt"})</code><br>result: I have a mane but I'm not a lion,<br>I have four legs but I'm not a table,<br>I can gallop but I'm not running,<br>People say I'm disagreeable because I always say "neigh."<br>What am I?</p><p><strong>Claude:</strong> The contents of <code>riddle.txt is</code> a riddle:</p><p>I have a mane but I'm not a lion,<br>I have four legs but I'm not a table,<br>I can gallop but I'm not running,<br>People say I'm disagreeable because I always say "neigh."<br>What am I?</p><p>The answer to this riddle appears to be a horse.</p></blockquote><p>Boom! We now have a read file tool and a list files tool, and you can see that their functions are called and chained together from our original prompt.</p><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.035.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.035.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.035.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.035.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.035.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>The third primitive behind any coding harness is a bash tool, which does precisely what its name implies. It enables the LLM to execute commands on your computer.</p><pre><code>var BashDefinition = ToolDefinition{
	Name:        "bash",
	Description: "Execute a bash command and return its output. Use this to run shell commands.",
	InputSchema: BashInputSchema,
	Function:    Bash,
}</code></pre><figure><a href="https://github.com/ghuntley/how-to-build-a-coding-agent?ref=ghuntley.com"><div><p>GitHub - ghuntley/how-to-build-a-coding-agent</p><p>Contribute to ghuntley/how-to-build-a-coding-agent development by creating an account on GitHub.</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-20.svg" alt=""><span>GitHub</span><span>ghuntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/how-to-build-a-coding-agent-1" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Let's open up our workshop materials once again (above) and run the bash tool:</p><pre><code>go run bash_tool.go</code></pre><blockquote><strong>You:</strong> Hey Claude, provide a summary of all the processes running on this computer.<p><strong>Claude:</strong> I'll provide a summary of all the processes running on this computer. Let me use the <code>ps</code> command to gather this information.</p><p>tool: <code>bash({"command":"ps aux"})</code>precisely<br>Executing bash command: ps aux<br>Bash command succeeded: ps aux (output: 87344 bytes)</p><p>result: &lt;snipped&gt;</p><p><strong>Claude:</strong> Based on the output of the <code>ps aux</code> command, here's a summary of the processes running on this computer:</p><p>System Summary:</p><p>This is a multi-CPU system with 96 cores/threads (visible from the many CPU-related kernel threads)</p><p>Running Linux with systemd as the init system</p><p>The system has been up since August 20th (seen in START column)</p></blockquote><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.036.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.036.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.036.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.036.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.036.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>The fourth primitive behind any coding harness is the edit tool.  The edit tool also does precisely as the name implies. It takes the results back from inference and applies edits to the file.</p><figure><a href="https://github.com/ghuntley/how-to-build-a-coding-agent?ref=ghuntley.com"><div><p>GitHub - ghuntley/how-to-build-a-coding-agent</p><p>Contribute to ghuntley/how-to-build-a-coding-agent development by creating an account on GitHub.</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-20.svg" alt=""><span>GitHub</span><span>ghuntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/how-to-build-a-coding-agent-1" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Let's open up our workshop materials once again (above) and run edit tool:</p><pre><code>go run edit_file.go</code></pre><blockquote><strong>You:</strong> Claude, mate. Create <code>fizzbuzz.js</code> with nodejs that does fizzbuzz up to 20 and execute it to ensure it works<p><strong>Claude:</strong> I'll create a Node.js implementation of FizzBuzz that counts up to 20 and then run it to make sure it works correctly.</p><p>First, let me create the <code>fizzbuzz.js</code>, file:</p><p>tool: <code>edit_file({"path":"fizzbuzz.js","old_str":"","new_str":"// FizzBuzz implementation in Node.js\n// Prints numbers from 1 to 20\n// If number is divisible by 3, print "Fizz"\n// If number is divisible by 5, print "Buzz"\n// If number is divisible by both 3 and 5, print "FizzBuzz"\n\nfor (let i = 1; i &lt;= 20; i++) {\n if (i % 3 === 0 &amp;&amp; i % 5 === 0) {\n console.log("FizzBuzz");\n } else if (i % 3 === 0) {\n console.log("Fizz");\n } else if (i % 5 === 0) {\n console.log("Buzz");\n } else {\n console.log(i);\n }\n}"})</code></p><p>Editing file: <code>fizzbuzz.js</code> (replacing 0 chars with 485 chars)</p><p>File does not exist, creating new file: <code>fizzbuzz.js</code><br>Creating new file: <code>fizzbuzz.js</code> (485 bytes)<br>Successfully created file <code>fizzbuzz.js</code></p><p>result: Successfully created file <code>fizzbuzz.js</code></p><p><strong>Claude:</strong> Now let me verify the file was created:<br>tool: <code>read_file({"path":"fizzbuzz.js"})</code></p><p>Reading file: <code>fizzbuzz.js</code><br>Successfully read file <code>fizzbuzz.js</code> (485 bytes)</p></blockquote><figure><img src="https://ghuntley.com/content/images/2025/08/fizzbuzz.js--1--1.png" alt="" loading="lazy" width="2000" height="1331" srcset="https://ghuntley.com/content/images/size/w600/2025/08/fizzbuzz.js--1--1.png 600w, https://ghuntley.com/content/images/size/w1000/2025/08/fizzbuzz.js--1--1.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/08/fizzbuzz.js--1--1.png 1600w, https://ghuntley.com/content/images/2025/08/fizzbuzz.js--1--1.png 2080w" sizes="(min-width: 720px) 720px"></figure><blockquote><strong>Claude:</strong> Now I'll execute the script to make sure it works correctly:<p>tool: <code>bash({"command":"node fizzbuzz.js"})</code><br>Executing bash command: <code>node fizzbuzz.js</code></p><p>Bash command executed successfully, output length: 77 chars<br>result: </p><p>1<br>2<br>Fizz<br>4<br>Buzz<br>Fizz<br>7<br>8<br>Fizz<br>Buzz<br>11<br>Fizz<br>13<br>14<br>FizzBuzz<br>16<br>17<br>Fizz<br>19<br>Buzz</p><p><strong>Claude:</strong> Great! The <code>fizzbuzz.js</code> file has been created and executed successfully. The output shows:</p><p>Numbers not divisible by 3 or 5 are printed as is (1, 2, 4, 7, 8, 11, 13, 14, 16, 17, 19)</p><p>Numbers divisible by 3 (but not 5) print "Fizz" (3, 6, 9, 12, 18)</p><p>Numbers divisible by 5 (but not 3) print "Buzz" (5, 10, 20)</p><p>Number 15 prints "FizzBuzz" as it's divisible by both 3 and 5</p></blockquote><p>And voila, we now have a coding agent. Is it this point that we could throw some other prompts in and say, "Hey, let's do FizzBuzz to 100, and it will just work."</p><figure><img src="https://ghuntley.com/content/images/2025/08/how-to-build-a-agent.039.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/08/how-to-build-a-agent.039.png 600w, https://ghuntley.com/content/images/size/w1000/2025/08/how-to-build-a-agent.039.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/08/how-to-build-a-agent.039.png 1600w, https://ghuntley.com/content/images/2025/08/how-to-build-a-agent.039.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>The fifth and final primitive behind any coding harness is a search tool. What if I were to tell you that there is no magic for indexing source code or any intelligence? </p><pre><code>var CodeSearchDefinition = ToolDefinition{
	Name: "code_search",
	Description: `Search for code patterns using ripgrep (rg).

Use this to find code patterns, function definitions, variable usage, or any text in the codebase.
You can search by pattern, file type, or directory.`,
	InputSchema: CodeSearchInputSchema,
	Function:    CodeSearch,
}</code></pre><p>Nearly every coding tool currently available uses the open source <code>ripgrep</code> binary under the hood. See the two blog posts below if you want more proof:</p><figure><a href="https://ghuntley.com/amazon-kiro-source-code/"><div><p>source code analysis of Amazon Kiro</p><p>It’s an uncertain time for our profession, but one thing is certain—things will change. Drafting used to require a room of engineers, but then CAD came along…</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-53.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/Tattoo-art-print-depicting-reverse-engineered-Amazon-systems--warm-autumn-colors--white-background--intricate-lines-and-vibrant-colors--retro-flair--complex-ornamental-details--charming-and-cute-style.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://ghuntley.com/overton/"><div><p>from Luddites to AI: the Overton Window of disruption</p><p>I’ve been thinking about Overton Windows lately, but not of the political variety. You see, the Overton window can be adapted to model disruptive innovation by framing the acceptance of novel technologies, business models, or ideas within a market or society. So I’ve been pondering about where, when and how</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-51.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/download-3.jpeg" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Let's open up our workshop materials once again and run the code search tool:</p><pre><code>go run code_search_tool.go</code></pre><blockquote><strong>You:</strong> Claude, mate. Search the source code for a riddle<p><strong>Claude:</strong> I'd be happy to search the source code for a riddle. Let me use the code search tool to look for mentions of riddles in the codebase.</p><p>tool: <code>code_search({"pattern":"riddle"})</code></p><p>Searching for pattern: <code>riddle</code><br>Found 6 matches for pattern: <code>riddle</code></p><p>result: </p><p><code>./README.md:332:- <strong>riddle.txt</strong>: A simple riddle for content analysis</code></p><p><strong>Claude:</strong> I've found references to a riddle file called "<code>riddle.txt</code>" in the codebase. Let me check if this file exists by listing the files in the current directory.</p><p>tool: <code>list_files({})</code><br>Listing files in directory: .<br>Successfully listed 25 files in .<br>result: </p><p><code>["./go.mod","./renovate.json","./devenv.yaml","./AGENT.md","./.devenv.flake.nix","./.envrc","./bash_tool.go","./fizzbuzz.js","./code_search_tool.go","./list_files.go","./Makefile","./devenv.nix","./devenv.lock","./prompts/01-read_file.md","./prompts/02-list_files.md","./prompts/00-weather.md","./prompts/03-bash_tool.md","./prompts/04-edit_tool.md","./chat.go","./go.sum","./riddle.txt","./edit_tool.go","./README.md","./.gitignore","./read.go"]</code></p><p>Claude: Great! I can see that "<code>riddle.txt</code>" exists in the current directory. Let me read its contents to see what the riddle is.</p><p>tool: <code>read_file({"path":"riddle.txt"})</code></p><p>Reading file: <code>riddle.txt</code><br>Successfully read file <code>riddle.txt</code> (175 bytes)<br>result: I have a mane but I'm not a lion,<br>I have four legs but I'm not a table,<br>I can gallop but I'm not running,<br>People say I'm disagreeable because I always say "neigh."<br>What am I?</p><p><strong>Claude:</strong> I found the riddle in the file <code>riddle.txt</code>!</p></blockquote><figure><img src="https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.037.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/07/how-to-build-a-agent.037.png 600w, https://ghuntley.com/content/images/size/w1000/2025/07/how-to-build-a-agent.037.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/07/how-to-build-a-agent.037.png 1600w, https://ghuntley.com/content/images/2025/07/how-to-build-a-agent.037.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>Everything from this point forward is just a matter of tuning your prompts. Due to time constraints, we didn't delve into the notion of the harness prompt. The harness prompt is where your tool registrations go. It contains information such as the operating system you're running, so it knows whether to use PowerShell (Windows) or bash (Linux or macOS). </p><p>It also contains instructions on how the agent should operate. I say should is because LLMs are non-deterministic. You can include the guidance, and it's just guidance. However, through prompt evaluation, tuning, and spending time playing with the models to understand how they behave, you can develop effective prompts.</p><p>Drop by the blog post below if you want to see an example of a fully fledged coding agent:</p><figure><a href="https://ghuntley.com/amazon-kiro-source-code/"><div><p>source code analysis of Amazon Kiro</p><p>It’s an uncertain time for our profession, but one thing is certain—things will change. Drafting used to require a room of engineers, but then CAD came along…</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-55.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/Tattoo-art-print-depicting-reverse-engineered-Amazon-systems--warm-autumn-colors--white-background--intricate-lines-and-vibrant-colors--retro-flair--complex-ornamental-details--charming-and-cute-style-1.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>There are plenty of open-source coding agents already, such as SST Open Code</p><figure><a href="https://github.com/sst/opencode?ref=ghuntley.com"><div><p>GitHub - sst/opencode: AI coding agent, built for the terminal.</p><p>AI coding agent, built for the terminal. Contribute to sst/opencode development by creating an account on GitHub.</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-21.svg" alt=""><span>GitHub</span><span>sst</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/6aa80c72-cede-4e0d-b724-d3ed79320a7e" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Or this 100-line agent, which scored really high on the SWE Bench.</p><figure><a href="https://github.com/SWE-agent/mini-swe-agent?ref=ghuntley.com"><div><p>GitHub - SWE-agent/mini-swe-agent: The 100 line AI agent that solves GitHub issues or helps you in your command line. Radically simple, no huge configs, no giant monorepo—but scores 68% on SWE-bench verified!</p><p>The 100 line AI agent that solves GitHub issues or helps you in your command line. Radically simple, no huge configs, no giant monorepo—but scores 68% on SWE-bench verified! - SWE-agent/mini-swe-agent</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-24.svg" alt=""><span>GitHub</span><span>SWE-agent</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/mini-swe-agent" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>And if you want some inspiration, there are many repositories on GitHub with leaked developer tooling harness and tool prompts.</p><figure><a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools?ref=ghuntley.com"><div><p>GitHub - x1xhlol/system-prompts-and-models-of-ai-tools: FULL v0, Cursor, Manus, Augment Code, Same.dev, Lovable, Devin, Replit Agent, Windsurf Agent, VSCode Agent, Dia Browser, Xcode, Trae AI, Cluely &amp; Orchids.app (And other Open Sourced) System Prompts, Tools &amp; AI Models.</p><p>FULL v0, Cursor, Manus, Augment Code, Same.dev, Lovable, Devin, Replit Agent, Windsurf Agent, VSCode Agent, Dia Browser, Xcode, Trae AI, Cluely &amp;amp; Orchids.app (And other Open Sourced) System Pro…</p><p><img src="https://ghuntley.com/content/images/icon/pinned-octocat-093da3e6fa40-22.svg" alt=""><span>GitHub</span><span>x1xhlol</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/system-prompts-and-models-of-ai-tools" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><img src="https://ghuntley.com/content/images/2025/08/how-to-build-a-agent.041.png" alt="" loading="lazy" width="1920" height="1080" srcset="https://ghuntley.com/content/images/size/w600/2025/08/how-to-build-a-agent.041.png 600w, https://ghuntley.com/content/images/size/w1000/2025/08/how-to-build-a-agent.041.png 1000w, https://ghuntley.com/content/images/size/w1600/2025/08/how-to-build-a-agent.041.png 1600w, https://ghuntley.com/content/images/2025/08/how-to-build-a-agent.041.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>In recap. What you just built was a coding agent. Perhaps you don't want to create a coding agent. What if you're in the data engineering profession? What would that look like? Think about all of the activities that you do day-to-day, where having the capability to automate using these primitives could be handy or valuable to your employer.</p><p>Your current workers are going to take your job, not AI. </p><p>If you're concerned about AI, the answer is straightforward: just invest in yourself. It really is that simple. This year is a particularly challenging time to be asleep at the wheel when it comes to personal development.  </p><figure><a href="https://ghuntley.com/six-month-recap/"><div><p>the six-month recap: closing talk on AI at Web Directions, Melbourne, June 2025</p><p>Welcome back to our final session at WebDirections. We’re definitely on the glide path—though I’m not sure if we’re smoothly landing, about to hit turbulence, or perhaps facing a go-around. We’ll see how it unfolds. Today, I’m excited to introduce Geoffrey Huntley. I discovered Geoff earlier this year through</p><p><img src="https://ghuntley.com/content/images/icon/7V0ak3am_400x400-1-54.jpg" alt=""><span>Geoffrey Huntley</span><span>Geoffrey Huntley</span></p></div><p><img src="https://ghuntley.com/content/images/thumbnail/the-future-belongs-to-people-who-do-things.001-3.png" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>conference lock-note</span></p></figcaption></figure><p>I hope to see you later this afternoon for the conference for the locknote (see above). </p><blockquote>Go forward and build.</blockquote><p>ps. socials</p><ul><li>X - <a href="https://x.com/GeoffreyHuntley/status/1959453609240007052?ref=ghuntley.com">https://x.com/GeoffreyHuntley/status/1959453609240007052</a></li><li>Linkedin : <a href="https://www.linkedin.com/posts/geoffreyhuntley_how-to-build-an-agent-activity-7365220712707485697-EiJO?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAABQKuUB2AJ059keUcRUVLbtmoa6miLVlTI">https://www.linkedin.com/posts/geoffreyhuntley_how-to-build-an-agent-activity-7365220712707485697-EiJO</a></li><li>Bluesky: <a href="https://bsky.app/profile/ghuntley.com/post/3lx4jmqsclk2q?ref=ghuntley.com">https://bsky.app/profile/ghuntley.com/post/3lx4jmqsclk2q</a></li></ul>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AGI is an engineering problem, not a model training problem (148 pts)]]></title>
            <link>https://www.vincirufus.com/posts/agi-is-engineering-problem/</link>
            <guid>45000176</guid>
            <pubDate>Sun, 24 Aug 2025 00:18:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vincirufus.com/posts/agi-is-engineering-problem/">https://www.vincirufus.com/posts/agi-is-engineering-problem/</a>, See on <a href="https://news.ycombinator.com/item?id=45000176">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="article"> <p>We’ve reached an inflection point in AI development. The scaling laws that once promised ever-more-capable models are showing diminishing returns. GPT-5, Claude, and Gemini represent remarkable achievements, but they’re hitting asymptotes that brute-force scaling can’t solve. The path to artificial general intelligence isn’t through training ever-larger language models—it’s through building engineered systems that combine models, memory, context, and deterministic workflows into something greater than their parts.</p>
<p>Let me be blunt: <strong>AGI is an engineering problem, not a model training problem.</strong></p>
<h2 id="the-plateauing-reality">The Plateauing Reality</h2>
<p>The current generation of large language models has hit a wall that’s become increasingly obvious to anyone working with them daily. They’re impressive pattern matchers and text generators, but they remain fundamentally limited by their inability to maintain coherent context across sessions, their lack of persistent memory, and their stochastic nature that makes them unreliable for complex multi-step reasoning.</p>
<p>We’ve seen this movie before. Every technology wave follows the same trajectory: initial breakthrough, rapid scaling, then increasing marginal costs for decreasing marginal gains. The semiconductor industry hit this wall in the early 2000s when clock speed scaling became impossible. The solution then wasn’t to brute-force faster processors—it was to fundamentally rethink the architecture with multi-core designs.</p>
<p><strong>AI is at the same inflection point.</strong> We need to stop asking “how do we make the model bigger?” and start asking “how do we make the system smarter?”</p>
<h2 id="the-systems-approach-to-agi">The Systems Approach to AGI</h2>
<p>The human brain isn’t a single neural net—it’s a collection of specialized systems working in concert: memory formation, context management, logical reasoning, spatial navigation, language processing. Each system has evolved specific purposes, and they operate asynchronously with complex feedback loops between them.</p>
<p><strong>True AGI requires us to engineer similar systems. Here’s what we actually need to build:</strong></p>
<h3 id="1-context-management-as-infrastructure">1. Context Management as Infrastructure</h3>
<p>Current models have attention spans measured in thousands of tokens. Human context span extends across years of lived experience. The gap isn’t just quantitative—it’s qualitative. We need context management systems that can:</p>
<ul>
<li><strong>Retrieve and filter relevant information</strong> on-demand using sophisticated retrieval systems</li>
<li><strong>Maintain coherent world models</strong> that persist across sessions and evolve with new information</li>
<li><strong>Bridge context gaps</strong> between different specialized knowledge domains</li>
<li><strong>Handle conflicting information</strong> with probabilistic weighting and uncertainty quantification</li>
</ul>
<p>This requires moving beyond simple vector similarity searches to building operational knowledge graphs that can be updated, queried, and reasoned about in real-time. <a href="https://www.vincirufus.com/blog/context-engineering">Our work on Context Engineering</a> provides a foundation for these systems.</p>
<h3 id="2-memory-as-a-service">2. Memory as a Service</h3>
<p>LLMs don’t have memory—they engage in elaborate methods to fake it through prompt engineering and context stuffing. Real AGI needs memory systems that:</p>
<ul>
<li><strong>Update beliefs</strong> when contradicted by new evidence</li>
<li><strong>Consolidate information</strong> across multiple experiences into general principles</li>
<li><strong>Forget irrelevant details</strong> without catastrophic forgetting</li>
<li><strong>Generate meta-knowledge</strong> about the reliability and source of stored information</li>
</ul>
<p>This isn’t just database persistence—it’s building memory systems that evolve the way human memory does: strengthening with use, decaying with disuse, and reorganizing based on new understanding. The <a href="https://www.vincirufus.com/blog/adr">architectural patterns from software systems</a> show us how to design such evolving structures.</p>
<h3 id="3-deterministic-workflows-with-probabilistic-components">3. Deterministic Workflows with Probabilistic Components</h3>
<p>The real breakthrough in AGI will come from building deterministic frameworks that can incorporate probabilistic components when appropriate. Think of it like building a compiler: the overall flow is rigid and predictable, but individual steps can use heuristics and probabilistic optimization.</p>
<p><strong>We need systems that can:</strong></p>
<ul>
<li><strong>Route problems</strong> to appropriate specialized solvers based on problem characteristics</li>
<li><strong>Execute multi-step workflows</strong> with rollback and recovery capabilities</li>
<li><strong>Validate outputs</strong> through deterministic checks before accepting probabilistic results</li>
<li><strong>Compose capabilities</strong> in predictable ways while maintaining the benefits of stochastic generation</li>
</ul>
<p>Our <a href="https://www.vincirufus.com/blog/deterministic-vs-propapbistic">research on deterministic vs. probabilistic systems</a> demonstrates how we can build these hybrid architectures effectively. The key insight is that <strong>uncertainty should be a first-class concept</strong> in system design, not something we try to eliminate.</p>
<h3 id="4-specialized-models-as-modular-components">4. Specialized Models as Modular Components</h3>
<p>The future isn’t one model to rule them all—it’s hundreds or thousands of specialized models working together in orchestrated workflows. Language models remain excellent at linguistic tasks, but they’re terrible at:</p>
<ul>
<li><strong>Symbolic manipulation</strong> and exact calculation</li>
<li><strong>Visual-spatial reasoning</strong> beyond basic pattern matching</li>
<li><strong>Temporal reasoning</strong> and planning complex sequences</li>
<li><strong>Intentional agent behavior</strong> with persistent goals</li>
</ul>
<p>Instead of waiting for a breakthrough that makes language models good at everything, we should be building systems that:</p>
<ul>
<li><strong>Route problems</strong> to models optimized for specific domains (<a href="https://www.vincirufus.com/blog/thinking-in-agents">thinking in agents</a> demonstrates this approach)</li>
<li><strong>Combine outputs</strong> from different model types into coherent solutions</li>
<li><strong>Maintain compatibility</strong> while allowing individual components to evolve independently</li>
<li><strong>Handle failure gracefully</strong> when individual models underperform</li>
</ul>
<h2 id="the-engineering-challenge">The Engineering Challenge</h2>
<p>This brings us to the core insight: <strong>building AGI is a distributed systems problem, not a machine learning problem.</strong> We’ve been fooled into thinking that because data center-scale training clusters are distributed systems, we’re already doing systems engineering. Nothing could be further from the truth.</p>
<p><strong>The real engineering challenge is building:</strong></p>
<ul>
<li><strong>Fault-tolerant pipelines</strong> where component failures don’t cascade into system failures</li>
<li><strong>Monitoring and observability</strong> systems that can detect when model outputs are drifting or becoming unreliable</li>
<li><strong>Deployment systems</strong> that allow for rolling updates without breaking existing integrations</li>
<li><strong>Testing frameworks</strong> that can validate system behavior across thousands of model and parameter combinations</li>
</ul>
<p>This is the kind of engineering challenge that requires decades of distributed systems experience, not just machine learning expertise. The solutions will come from infrastructure engineers who understand how to build reliable, scalable systems at the intersection of hardware, software, and AI models.</p>
<h2 id="what-we-should-be-building-instead">What We Should Be Building Instead</h2>
<p>While everyone else is focused on scaling the next model, we should be building the infrastructure that makes general intelligence possible. Here’s my roadmap:</p>
<h3 id="phase-1-foundation-layer">Phase 1: Foundation Layer</h3>
<ul>
<li><strong>Context Management Service</strong>: Persistent, queryable, versioned knowledge graphs with real-time updates</li>
<li><strong>Memory Service</strong>: Episodic and semantic memory systems with learned consolidation patterns</li>
<li><strong>Workflow Engine</strong>: Deterministic orchestration of probabilistic components with rollback capabilities</li>
<li><strong>Agent Coordination Layer</strong>: Multi-agent systems with negotiated consensus and conflict resolution</li>
</ul>
<h3 id="phase-2-capability-layer">Phase 2: Capability Layer</h3>
<ul>
<li><strong>Specialized Model Controls</strong>: Fine-tuned models for specific reasoning domains with standardized interfaces</li>
<li><strong>Symbolic Reasoning Engine</strong>: Exact calculation and symbolic manipulation capabilities that work with probabilistic components</li>
<li><strong>Planning and Goal Management</strong>: Systems that can break complex objectives into executable sub-plans</li>
<li><strong>Cross-modal Integration</strong>: Systems that combine sensory inputs (text, vision, audio) into unified representations</li>
</ul>
<h3 id="phase-3-emergence-layer">Phase 3: Emergence Layer</h3>
<p>This is where real AGI emerges—from the interaction of all these components working together, not from any single breakthrough model. The system’s capabilities will exceed those of its individual parts through emergent properties that arise from careful architectural design.</p>
<h2 id="the-path-forward">The Path Forward</h2>
<p>The path to AGI isn’t through training a bigger transformer—it’s through building distributed systems that can orchestrate hundreds of specialized models, maintain coherent context across sessions, execute deterministic workflows around probabilistic components, and provide fault-tolerant operation at production scale.</p>
<p>This is fundamentally <strong>engineering work</strong>, requiring decades of experience building reliable distributed systems. The breakthroughs will come from infrastructure engineers who understand how to build context paths, memory systems, workflow orchestration, and model coordination at scale.</p>
<p><strong>The race to AGI isn’t being won by the team with the biggest GPU cluster—it’s being won by the team that understands how to build reliable, engineered AI systems that can actually reason across domains while maintaining consistent behavior.</strong></p>
<p>The models we have now are sufficient. The missing piece is the systems engineering that turns them into general intelligence.</p>
<p>We’ve been asking the wrong question. It’s not “how do we get to the next model breakthrough?” It’s “how do we build the system architecture that makes general intelligence inevitable with the models we already have?”</p>
<p>The answer is systems engineering. The future of AGI is architectural, not algorithmic.</p> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Good EU regulations (125 pts)]]></title>
            <link>https://www.actuallygoodregulations.eu/</link>
            <guid>44999419</guid>
            <pubDate>Sat, 23 Aug 2025 21:51:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.actuallygoodregulations.eu/">https://www.actuallygoodregulations.eu/</a>, See on <a href="https://news.ycombinator.com/item?id=44999419">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The cost of interrupted work (2023) (192 pts)]]></title>
            <link>https://blog.oberien.de/2023/11/05/23-minutes-15-seconds.html</link>
            <guid>44999373</guid>
            <pubDate>Sat, 23 Aug 2025 21:45:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.oberien.de/2023/11/05/23-minutes-15-seconds.html">https://blog.oberien.de/2023/11/05/23-minutes-15-seconds.html</a>, See on <a href="https://news.ycombinator.com/item?id=44999373">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="main_content_wrap">
        
<p>2023-11-05</p>

<p>You’ve likely read lots of blog posts stating that it takes 23 minutes and 15 seconds to get back to work after an interruption, context switch, or meeting. Thus, “do you have five minutes” ends up not only costing those few minutes, but instead about half an hour. But where does that number come from?</p>

<p>I just wanted to quickly reference this fact to a colleague. Quick search for the reference, copy’n’paste it, in and out, 20 minutes adventure. I quickly found a reference to a paper. For sanity sake, I just wanted to verify where it states the 23 minutes. Open the paper, Ctrl+F for “23”, “no results”. Huh?</p>

<h3 id="papers">Papers</h3>

<p>Most of the posts mentioning the number refer to the paper <a href="https://ics.uci.edu/~gmark/chi08-mark.pdf">The Cost of Interrupted Work: More Speed and Stress</a>. The authors performed a study investigating different effects of interruptions during long tasks.<br>
Contrary to the quoted number, the study found out that the time spent on only the original task was in fact lower when interruptions were present (20.31 and 20.60 min) compared to no interruptions (22.77 min), albeit with much higher experienced stress. The paper never goes into details regarding the recovery time between finishing the interruption and getting back to the original task. The paper never mentions the number <code>23</code>.</p>

<p>Maybe it’s in a different paper? Related Work? References?</p>
<ul>
  <li><a href="http://erichorvitz.com/taskdiary.pdf">A Diary Study of Task Switching and Interruptions</a> let participants record interruption diaries. The paper does not include or mention task switch recovery time. Its primary result is that the average person has 50 task switches per week.</li>
  <li><a href="https://erichorvitz.com/CHI_2007_Iqbal_Horvitz.pdf">Disruption and Recovery of Computing Tasks: Field Study, Analysis, and Directions</a> states that it takes 11 - 16 minutes to resolve an interruption until getting back to the original task. Some of that time is spent to get the mind back into the original task. However, no further investigation of the recovery period has been performed.</li>
  <li><a href="https://interruptions.net/literature/Adamczyk-CHI04-p271-adamczyk.pdf">If Not Now, When?: The Effects of Interruption at Different Moments Within Task Execution</a> states “An approximate value for Resumption Lag, the time a subject takes to switch focus back to primary task after interruption, was also collected.” However, the paper never provides any value for that number and doesn’t discuss it further.</li>
  <li><a href="https://ics.uci.edu/~gmark/CHI2005.pdf">No Task Left Behind? Examining the Nature of Fragmented Work</a> focuses on the probability that a task was resumed on the same day in regards to recovery. There is no mention of a specific recovery time.</li>
</ul>

<h3 id="blog-posts">Blog Posts</h3>

<p>The search continued. In addition to the 5 papers I (fittingly) read through a total of 23 posts.</p>
<ul>
  <li>9 posts incorrectly referred to one of the papers; one of them even included a quote that is nowhere to be found within the referenced paper</li>
  <li>2 posts correctly referred to the first paper for its actual results</li>
  <li>9 posts directly or indirectly refer to three interviews with Gloria Mark (the author of the original paper), in which she stated the 23 minutes and 15 seconds recovery time</li>
  <li>2 posts refer to the Wall Street Journal, which directly quotes Gloria Mark with the 23 minutes 15 seconds figure</li>
</ul>

<p>So in the end, where do the 23 minutes and 15 seconds come from? They are mentioned in interviews multiple times by Gloria Mark. But I wasn’t able to find a primary printed source. There are <a href="https://ics.uci.edu/~gmark/Home_page/Publications.html">many more publications by Gloria Mark</a>, but none of them turned up while searching for the 23 minutes 15 seconds figure. If someone knows a paper or study where that figure originally appears in, please tell me.</p>

<hr>

<p>Discussion on <a href="https://www.reddit.com/r/programming/comments/17ooxwe/interruptions_cost_23_minutes_15_seconds_right/">r/programming</a>.</p>

<hr>

<p>Here is the reference graph of all posts and papers I’ve mentioned in this post and a list of their links.</p>

<p><img src="https://blog.oberien.de/assets/2023-11-05-references.svg" alt=""></p>

<ul>
  <li>Dev Interrupted – <a href="https://devinterrupted.substack.com/p/3-proven-ways-to-improve-dev-focus">https://devinterrupted.substack.com/p/3-proven-ways-to-improve-dev-focus</a></li>
  <li>Loom – <a href="https://www.loom.com/blog/cost-of-context-switching">https://www.loom.com/blog/cost-of-context-switching</a></li>
  <li>Paladinic – <a href="https://www.paladininc.com/blog/detail/6299/dealing-with-work-interruptions">https://www.paladininc.com/blog/detail/6299/dealing-with-work-interruptions</a></li>
  <li>Lifehacker – <a href="https://lifehacker.com/how-long-it-takes-to-get-back-on-track-after-a-distract-1720708353">https://lifehacker.com/how-long-it-takes-to-get-back-on-track-after-a-distract-1720708353</a></li>
  <li>The Muse – <a href="https://www.themuse.com/advice/this-is-nuts-it-takes-nearly-30-minutes-to-refocus-after-you-get-distracted">https://www.themuse.com/advice/this-is-nuts-it-takes-nearly-30-minutes-to-refocus-after-you-get-distracted</a></li>
  <li>Fast Company – <a href="https://www.fastcompany.com/944128/worker-interrupted-cost-task-switching">https://www.fastcompany.com/944128/worker-interrupted-cost-task-switching</a></li>
  <li>idonethis – <a href="https://blog.idonethis.com/distractions-at-work/">https://blog.idonethis.com/distractions-at-work/</a></li>
  <li>idea to value – <a href="https://www.ideatovalue.com/curi/nickskillicorn/2023/07/it-takes-23-minutes-to-regain-focus-after-a-distraction-task-switching/">https://www.ideatovalue.com/curi/nickskillicorn/2023/07/it-takes-23-minutes-to-regain-focus-after-a-distraction-task-switching/</a></li>
  <li>LeadDev – <a href="https://leaddev.com/process/managing-chaos-context-switching">https://leaddev.com/process/managing-chaos-context-switching</a></li>
  <li>getabstract – <a href="https://journal.getabstract.com/en/2022/03/17/twenty-three-minutes/">https://journal.getabstract.com/en/2022/03/17/twenty-three-minutes/</a></li>
  <li>gallup – <a href="https://news.gallup.com/businessjournal/23146/too-many-interruptions-work.aspx">https://news.gallup.com/businessjournal/23146/too-many-interruptions-work.aspx</a></li>
  <li>JournalStar – <a href="https://eu.pjstar.com/story/news/2013/01/25/frequent-emails-phone-call-interruptions/42450766007/">https://eu.pjstar.com/story/news/2013/01/25/frequent-emails-phone-call-interruptions/42450766007/</a></li>
  <li>togglblog – <a href="https://toggl.com/blog/how-to-get-back-on-track-when-you-get-distracted-at-work">https://toggl.com/blog/how-to-get-back-on-track-when-you-get-distracted-at-work</a></li>
  <li>Presentation by Gloria Mark – <a href="https://slideplayer.com/slide/1409624/">https://slideplayer.com/slide/1409624/</a></li>
  <li>Productivityjunkie – <a href="https://www.linkedin.com/pulse/productivityjunkie-23-minutes-15-seconds-mystery-sugar-inga-bieli%C5%84ska">https://www.linkedin.com/pulse/productivityjunkie-23-minutes-15-seconds-mystery-sugar-inga-bieli%C5%84ska</a></li>
  <li>Bright Developers – <a href="https://www.brightdevelopers.com/the-cost-of-interruption-for-software-developers/">https://www.brightdevelopers.com/the-cost-of-interruption-for-software-developers/</a></li>
  <li>Wall Street Journal – <a href="https://www.wsj.com/articles/SB10001424127887324339204578173252223022388">https://www.wsj.com/articles/SB10001424127887324339204578173252223022388</a></li>
  <li>Ironistic – <a href="https://www.linkedin.com/pulse/cost-distractions-developers-ironistic-com">https://www.linkedin.com/pulse/cost-distractions-developers-ironistic-com</a></li>
  <li>Jazz Hanley – <a href="https://www.linkedin.com/pulse/reclaim-23-minutes-you-lose-every-time-youre-work-jazz-hanley">https://www.linkedin.com/pulse/reclaim-23-minutes-you-lose-every-time-youre-work-jazz-hanley</a></li>
  <li>devmio – <a href="https://devm.io/careers/aaaand-gone-true-cost-interruptions-128741">https://devm.io/careers/aaaand-gone-true-cost-interruptions-128741</a></li>
  <li>Stephanie C. Mitchell – <a href="https://www.stephaniecmitchell.com/articles/whats-distracting-you-from-writing">https://www.stephaniecmitchell.com/articles/whats-distracting-you-from-writing</a></li>
  <li>devbizops – <a href="https://devbizops.medium.com/getting-into-the-developer-flow-state-7b0e5c98eb8a">https://devbizops.medium.com/getting-into-the-developer-flow-state-7b0e5c98eb8a</a></li>
  <li>Hardvard Business Review – <a href="https://hbr.org/2014/04/help-your-employees-find-flow">https://hbr.org/2014/04/help-your-employees-find-flow</a></li>
</ul>


      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Romhack.ing's Internet Archive Mirror No Longer Available (151 pts)]]></title>
            <link>https://romhack.ing/database/news/entry/DW8BKnRHSEqaGDwXTiKjMw</link>
            <guid>44998982</guid>
            <pubDate>Sat, 23 Aug 2025 20:43:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://romhack.ing/database/news/entry/DW8BKnRHSEqaGDwXTiKjMw">https://romhack.ing/database/news/entry/DW8BKnRHSEqaGDwXTiKjMw</a>, See on <a href="https://news.ycombinator.com/item?id=44998982">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[A 2k-year-old sun hat worn by a Roman soldier in Egypt (132 pts)]]></title>
            <link>https://www.smithsonianmag.com/smart-news/a-2000-year-old-sun-hat-worn-by-a-roman-soldier-in-egypt-goes-on-view-after-a-century-in-storage-180987192/</link>
            <guid>44998514</guid>
            <pubDate>Sat, 23 Aug 2025 19:35:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.smithsonianmag.com/smart-news/a-2000-year-old-sun-hat-worn-by-a-roman-soldier-in-egypt-goes-on-view-after-a-century-in-storage-180987192/">https://www.smithsonianmag.com/smart-news/a-2000-year-old-sun-hat-worn-by-a-roman-soldier-in-egypt-goes-on-view-after-a-century-in-storage-180987192/</a>, See on <a href="https://news.ycombinator.com/item?id=44998514">Hacker News</a></p>
Couldn't get https://www.smithsonianmag.com/smart-news/a-2000-year-old-sun-hat-worn-by-a-roman-soldier-in-egypt-goes-on-view-after-a-century-in-storage-180987192/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[What makes Claude Code so damn good (319 pts)]]></title>
            <link>https://minusx.ai/blog/decoding-claude-code/</link>
            <guid>44998295</guid>
            <pubDate>Sat, 23 Aug 2025 19:07:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://minusx.ai/blog/decoding-claude-code/">https://minusx.ai/blog/decoding-claude-code/</a>, See on <a href="https://news.ycombinator.com/item?id=44998295">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Claude Code is the most delightful AI agent/workflow I have used so far. Not only does it make targeted edits or vibe coding throwaway tools less annoying, using Claude Code makes me happy. It has enough autonomy to do interesting things, while not inducing a jarring loss of control like some other tools do. Of course most of the heavy lifting is done by the new Claude 4 model (especially interleaved thinking). But I find Claude Code objectively less annoying to use compared to Cursor, or Github Copilot agents even with the same underlying model! What makes it so damn good? If you're reading this and nodding along, I'm going to try and provide some answers.</p>
<p><strong>Note</strong>: This is not a blogpost with Claude Code's architecture dump (there are some good ones out there). This blogpost is meant to be a guide for building delightful LLM agents, based on my own experience using and tinkering with Claude Code over the last few months (and all the logs we intercepted and analyzed). You can find <a href="#appendix">prompts</a> and <a href="#appendix">tools</a> in the <a href="#appendix">Appendix section</a>. This post is ~2k words long, so strap in! If you're looking for some quick takeaways, the <a href="#how-to-build-a-claude-code-like-agent-tldr">TL;DR</a> section is a good place to start.</p>

<p><img src="https://minusx.ai/images/claude-code/prompts.png" alt="prompts">
</p><p>You can clearly see the different Claude Code updates.</p>

<p>Claude Code (CC) feels great to use, because it <em>just simply works</em>. CC has been crafted with a fundamental understanding of what the LLM is good at and what it is terrible at. Its prompts and tools cover for the model's stupidity and help it shine in its wheelhouse. The control loop is extremely simple to follow and trivial to debug.</p>
<p>We started using CC at MinusX as soon as it launched. To look under the hood, <a href="https://x.com/ppsreejith_">Sreejith</a> wrote a logger that intercepts and logs every network request made. The following analysis is from my extensive use over the last couple of months. <strong>This post attempts to answer the question - "What makes Claude Code so good, and how can you give a CC-like experience in your own chat-based-LLM agent?"</strong> We've incorporated most of these into MinusX already and I'm excited to see you do it too!</p>

<p><img src="https://minusx.ai/images/claude-code/tools.png" alt="prompts">
</p><p>Edit is the most frequent tool, followed by Read and ToDoWrite</p>
<br>
<h2 id="how-to-build-a-claude-code-like-agent-tldr"><a href="#how-to-build-a-claude-code-like-agent-tldr">How to build a Claude Code like agent: TL;DR</a></h2>
<p>If there is one thing to take away from this, it is this - <strong>Keep Things Simple, Dummy</strong>. LLMs are terrible enough to debug and evaluate. Any additional complexity you introduce (multi-agents, agent handoffs or complex RAG search algorithms) only makes debugging 10x harder. If such a fragile system works at all, you'll be terrified of making drastic changes to it later. So, keep everything in one file, avoid excessive boilerplate scaffolding and rip it all out at least a couple of times :)</p>
<p>Here are the main takeaways from Claude Code to implement in your own system.</p>
<h4 id="1-control-loop"><a href="#1-control-loop">1. Control Loop</a></h4>
<ul>
<li>1.1 <a href="#11-keep-one-main-loop">Keep one main loop (with max one branch) and one message history</a></li>
<li>1.2 <a href="#12-use-a-smaller-model-for-everything">Use a smaller model for all sorts of things. All. The. Frickin. Time.</a></li>
</ul>
<h4 id="2-prompts"><a href="#2-prompts">2. Prompts</a></h4>
<ul>
<li>2.1 <a href="#21-use-claudemd-for-collaborating-on-user-context-and-preferences">Use claude.md pattern to collaborate on and remember user preferences</a></li>
<li>2.2 <a href="#22-special-xml-tags-markdown-and-lots-of-examples">Use special XML Tags, Markdown, and lots of examples</a></li>
</ul>
<h4 id="3-tools"><a href="#3-tools">3. Tools</a></h4>
<ul>
<li>3.1 <a href="#31-llm-search---rag-based-search">LLM search &gt;&gt;&gt;  RAG based search</a></li>
<li>3.2 <a href="#32-how-to-design-good-tools-low-level-vs-high-level-tools">How to design good tools? (High vs Low level tools)</a></li>
<li>3.3 <a href="#33-let-the-agent-manage-a-todo-list">Let your agent manage its own todo list</a></li>
</ul>
<h4 id="4-steerability"><a href="#4-steerability">4. Steerability</a></h4>
<ul>
<li>4.1 <a href="#41-tone-and-style">Tone and style</a></li>
<li>4.2 <a href="#42-this-is-important-is-still-state-of-the-art">"<strong>PLEASE THIS IS IMPORTANT</strong>" is unfortunately still state of the art</a></li>
<li>4.3 <a href="#43-write-the-algorithm-with-heuristics-and-examples">Write the algorithm, with heuristics and examples</a></li>
</ul>
<br>
<blockquote>
<p>Claude Code choses architectural simplicity at every juncture - one main loop, simple search, simple todolist, etc. Resist the urge to over-engineer, build good harness for the model let it cook! Is this end-to-end self-driving all over again? Bitter lesson much?</p>
</blockquote>
<hr>
<h2 id="1-control-loop-design"><a href="#1-control-loop-design">1. Control Loop Design</a></h2>
<h3 id="11-keep-one-main-loop"><a href="#11-keep-one-main-loop">1.1 Keep One Main Loop</a></h3>
<p>Debuggability &gt;&gt;&gt; complicated hand-tuned multi-agent lang-chain-graph-node mishmash.</p>
<p>Despite multi agent systems being all the rage, Claude Code has just one main thread. It uses a few different types of prompts periodically to summarize the git history, to clobber up the message history into one message or to come up with some fun UX elements. But apart from that, it maintains a flat list of messages. An interesting way it handles hierarchical tasks is by spawning itself as a sub-agent without the ability to spawn more sub-agents. There is a maximum of one branch, the result of which is added to the main message history as a "tool response".</p>
<p>If the problem is simple enough, the main loop just handles it via iterative tool calling. But if there are one or more tasks that are complex, the main agent creates clones of itself. The combination of the max-1-branch and the todo list makes sure the agent has the ability to break the problem into sub-problems, but also keep the eye on the final desired outcome.</p>
<p>I highly doubt your app needs a multi-agent system. With every layer of abstraction you make your system harder to debug, and more importantly you deviate from the general-model-improvement trajectory.</p>
<p><img src="https://minusx.ai/images/claude-code/control_loop.gif" alt="Control Loop"></p>
<h3 id="12-use-a-smaller-model-for-everything"><a href="#12-use-a-smaller-model-for-everything">1.2 Use a Smaller model for <em>everything</em></a></h3>
<p>Over 50% of all important LLM calls made by CC are to claude-3-5-haiku. It is used to read large files, parse web pages, process git history and summarize long conversations. It is also used to come up with the one-word processing label - literally for every key stroke! The smaller models are 70-80% cheaper than the standard ones (Sonnet 4, GPT-4.1). Use them liberally!</p>
<h2 id="2-prompts-1"><a href="#2-prompts-1">2. Prompts</a></h2>
<p>Claude Code has extremely elaborate prompts filled with heuristics, examples and IMPORTANT (tch-tch) reminders. The system prompt is ~2800 tokens long, with the Tools taking up a whopping 9400 tokens. The user prompt always contains the claude.md file, which can typically be another 1000-2000 tokens. The system prompt contains sections on tone, style, proactiveness, task management, tool usage policy and doing tasks. It also contains the date, current working directory, platform and OS information and recent commits.</p>
<p><a href="#appendix"><strong>Go read the entire prompt</strong></a>!</p>
<h3 id="21-use-claudemd-for-collaborating-on-user-context-and-preferences"><a href="#21-use-claudemd-for-collaborating-on-user-context-and-preferences">2.1 Use claude.md for collaborating on user context and preferences</a></h3>
<p>One of the major patterns most coding agent creators have settled on is the context file (aka Cursor Rules / claude.md / agent.md). The difference in Claude Code's performance with and without claude.md is night and day. It is a great way for the developers to impart context that cannot be inferred from the codebase and to codify all strict preferences. For example, you can force the LLM to skip some folders, or use specific libraries. CC sends the entire contents of the claude.md with every user request</p>
<p>We recently introduced <a href="https://minusx.ai/blog/memory/">minusx.md in MinusX</a> which is fast becoming the de-facto context file for our agents to codify user and team preferences.</p>
<h3 id="22-special-xml-tags-markdown-and-lots-of-examples"><a href="#22-special-xml-tags-markdown-and-lots-of-examples">2.2 Special XML Tags, Markdown, and lots of examples</a></h3>
<p>It is fairly established that XML tags and Markdown are two ways to structure a prompt. CC uses both, extensively. Here are a few notable XML tags in Claude Code:</p>
<ul>
<li><code>&lt;system-reminder&gt;</code>: This is used at the end of many prompt sections to remind the LLM of thing it presumably otherwise forgets. Example:</li>
</ul>
<pre><code>&lt;system-reminder&gt;This is a reminder that your todo list is currently empty. DO NOT mention this to the user explicitly because they are already aware. If you are working on tasks that would benefit from a todo list please use the TodoWrite tool to create one. If not, please feel free to ignore. Again do not mention this message to the user.&lt;/system-reminder&gt;
</code></pre>
<ul>
<li><code>&lt;good-example&gt;</code>, <code>&lt;bad-example&gt;</code>: These are used to codify heuristics. They can be especially useful when there is a fork in the road with multiple seemingly reasonable paths/tool_calls the model can choose. Examples can be used to contrast the cases and make it very clear which path is preferable. Example:</li>
</ul>
<pre><code>Try to maintain your current working directory throughout the session by using absolute paths and avoiding usage of `cd`. You may use `cd` if the User explicitly requests it.
&lt;good-example&gt;
pytest /foo/bar/tests  
&lt;/good-example&gt;
&lt;bad-example&gt;
cd /foo/bar &amp;&amp; pytest tests
&lt;/bad-example&gt;
</code></pre>
<p>CC also uses markdown to demarcate clear sections in the system prompt. Example markdown headings include:</p>
<ul>
<li>Tone and style</li>
<li>Proactiveness</li>
<li>Following conventions</li>
<li>Code style</li>
<li>Task Management</li>
<li>Tool use policy</li>
<li>Doing Tasks</li>
<li>Tools</li>
</ul>
<h2 id="3-tools-1"><a href="#3-tools-1">3. Tools</a></h2>
<p><a href="#appendix"><strong>Go read the entire tools prompt</strong></a> - it is a whopping 9400 tokens long!</p>
<h3 id="31-llm-search---rag-based-search"><a href="#31-llm-search---rag-based-search">3.1 LLM search &gt;&gt;&gt;  RAG based search</a></h3>
<p>One significant way in which CC deviates from other popular coding agents is in its rejection of RAG. Claude Code searches your code base just as you would, with really complex <code>ripgrep</code>, <code>jq</code> and <code>find</code> commands. Since the LLM understands code really well, it can use sophisticated regex to find pretty much any codeblock it deems relevant. Sometimes it ends up reading whole files with a smaller model.</p>
<p>RAG sounds like a good idea in theory, but it introduces new (and more importantly, hidden) failure modes. What is the similarity function to use? What reranker? How do you chunk the code? What do you do with large JSON or log files? With LLM Search, it just looks at 10 lines of the json file to understand its structure. If it wants, it looks at 10 more lines - just like you would. Most importantly, this is RL learnable - something BigLabs are already working on. The model does most of the heavy lifting - as it should, dramatically reducing the number of moving parts in the agent. Also, having two complicated, intelligent systems wired this way is just ugly. I was recently kidding with a friend saying this is the Camera vs Lidar of the LLM era and I'm only half joking.</p>
<h3 id="32-how-to-design-good-tools-low-level-vs-high-level-tools"><a href="#32-how-to-design-good-tools-low-level-vs-high-level-tools">3.2 How to design good tools? (Low level vs High level tools)</a></h3>
<p>This question keeps anyone who is building an LLM agent up at night. Should you give the model generic tasks (like meaningful actions) or should it be low level (like type and click and bash)? The answer is that it depends (and you should use both).</p>
<p>Claude Code has low level (Bash, Read, Write), medium level (Edit, Grep, Glob) and high level tools (Task, WebFetch, exit_plan_mode). CC can use bash, so why give a separate Grep tool? The real trade-off here is in how often you expect your agent to use the tool vs accuracy of the agent in using the tool. CC uses grep and glob so frequently that it makes sense to make separate tools out of them, but at the same time, it can also write generic bash commands for special scenarios.</p>
<p>Similarly, there are even higher level tools like WebFetch or 'mcp__ide__getDiagnostics' that are extremely deterministic in what they do. This saves the LLM from having to do multiple low level clicking and typing and keeps it on track. Help the poor model out, will ya!? Tool descriptions have elaborate prompts with plenty of examples. The system prompt has information about ‘when to use a tool' or how to choose between two tools that can do the same task.</p>
<p><strong>Tools in Claude Code:</strong></p>
<div><div><ul>
<li><a href="#appendix">Task</a></li>
<li><a href="#appendix">Bash</a></li>
<li><a href="#appendix">Glob</a></li>
<li><a href="#appendix">Grep</a></li>
<li><a href="#appendix">LS</a></li>
<li><a href="#appendix">ExitPlanMode</a></li>
<li><a href="#appendix">Read</a></li>
<li><a href="#">Edit</a></li>
</ul></div><div><ul>
<li><a href="#appendix">MultiEdit</a></li>
<li><a href="#appendix">Write</a></li>
<li><a href="#appendix">NotebookEdit</a></li>
<li><a href="#appendix">WebFetch</a></li>
<li><a href="#appendix">TodoWrite</a></li>
<li><a href="#appendix">WebSearch</a></li>
<li><a href="#">mcp__ide__getDiagnostics</a></li>
<li><a href="#">mcp__ide__executeCode</a></li>
</ul></div></div>
<h3 id="33-let-the-agent-manage-a-todo-list"><a href="#33-let-the-agent-manage-a-todo-list">3.3 Let the agent manage a todo list</a></h3>
<p>There are many reasons why this is a good idea. Context rot is a common problem in long-running LLM agents. They enthusiastically start out tackling a difficult problem, but over time lose their way and devolve into garbage. There are a few ways current agent designs tackle this.
Many agents have experimented with explicit todos (one model generates todos, another model implements them) or with Multi-agent handoff + verification (PRD/PM agent -&gt; implementer agent -&gt; QA agent)</p>
<p>We already know multi-agent handoff is not a good idea, for many many reasons. CC uses an explicit todo list, but one that the model maintains. This keeps the LLM on track (it has been heavily prompted to refer to the todo list frequently), while at the same time giving the model the flexibility to course correct mid-way in an implementation. This also effectively leverages the model's interleaved thinking abilities to either reject or insert new todo items on the fly.</p>
<h2 id="4-steerability-1"><a href="#4-steerability-1">4. Steerability</a></h2>
<h3 id="41-tone-and-style"><a href="#41-tone-and-style">4.1 Tone and Style</a></h3>
<p>CC explicitly attempts to control the aesthetic behavior of the agent. There are sections in the system prompt around tone, style and proactiveness - full of instructions and examples. This is  why Claude Code “feels” tasteful in its comments and eagerness. I recommend just copying large sections of this into your app as is.</p>
<pre><code># Some examples of tone and style
- IMPORTANT: You should NOT answer with unnecessary preamble or postamble (such as explaining your code or summarizing your action), unless the user asks you to.
Do not add additional code explanation summary unless requested by the user.

- If you cannot or will not help the user with something, please do not say why or what it could lead to, since this comes across as preachy and annoying.

- Only use emojis if the user explicitly requests it. Avoid using emojis in all communication unless asked.
</code></pre>
<h3 id="42-this-is-important-is-still-state-of-the-art"><a href="#42-this-is-important-is-still-state-of-the-art">4.2 "THIS IS IMPORTANT" is still State of the Art</a></h3>
<p>Unfortunately CC is no better when it comes to asking the model to not do something. IMPORTANT, VERY IMPORTANT, NEVER and ALWAYS seem to be the best way to steer the model away from landmines. I expect the models to get more steerable in the future and avoid this ugliness. But for now, CC uses this liberally, and so should you. Some examples:</p>
<pre><code>- IMPORTANT: DO NOT ADD ***ANY*** COMMENTS unless asked

- VERY IMPORTANT: You MUST avoid using search commands like `find` and `grep`. Instead use Grep, Glob, or Task to search. You MUST avoid read tools like `cat`, `head`, `tail`, and `ls`, and use Read and LS to read files.\n  - If you _still_ need to run `grep`, STOP. ALWAYS USE ripgrep at `rg` first

- IMPORTANT: You must NEVER generate or guess URLs for the user unless you are confident that the URLs are for helping the user with programming. You may use URLs provided by the user in their messages or local files.

</code></pre>
<h3 id="43-write-the-algorithm-with-heuristics-and-examples"><a href="#43-write-the-algorithm-with-heuristics-and-examples">4.3 Write the Algorithm (with heuristics and examples)</a></h3>
<p>It is extremely important to identify the most important task the LLM needs to perform and write out the algorithm for it. Try to role-play as the LLM and work through examples, identify all the decision points and write them explicitly. It helps if this is in the form of a flow-chart. This helps structure the decision making and aids the LLM in following instructions. One thing to definitely avoid is a big soup of Dos and Don'ts. They are harder to keep track, and keep mutually exclusive. If your prompt is several thousand tokens long, you will inadvertently have conflicting Dos and Don'ts. The LLM becomes extremely fragile in this case and it becomes impossible to incorporate new use cases.</p>
<p><code>Task Management</code>, <code>Doing Tasks</code> and <code>Tool Usage Policy</code> sections in Claude Code's system prompt clearly walk through the algorithm to follow. This is also the section to add lots of heuristics and examples of various scenarios the LLM might encounter.</p>
<h2 id="bonus-why-pay-attention-to-biglab-prompts"><a href="#bonus-why-pay-attention-to-biglab-prompts">Bonus: Why pay attention to BigLab prompts?</a></h2>
<p>A lot of the effort in steering LLMs is trying to reverse engineer their post-training / RLHF data distribution. Should you use JSON or XML? Should the tool descriptions be in the system prompt or just in tools? What about your app's current state? It helps to see what they do in their own apps and use it to inform yours. Claude Code design is very opinionated and it helps to use that in forming your own.</p>
<br>
<h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2>
<p>The main takeaway, again, is to keep things simple. Extreme scaffolding frameworks will hurt more than help you. Claude Code really made me believe that an "agent" can be simple and yet extremely powerful. We've incorporated a bunch of these lessons into MinusX, and are continuing to incorporate more.</p>
<p>If you're interested in Claude-Codifying your own LLM agent, I'd love to chat - ping me on <a href="https://x.com/nuwandavek">twitter</a>! If you want trainable Claude Code like data agents for your Metabase, check out <a href="https://minusx.com/">MinusX</a> or set up a demo with me <a href="https://minusx.com/demo">here</a>. Happy (Claude) Coding!</p>

<br>
<hr>
<br>
<h2 id="appendix"><a href="#appendix">Appendix</a></h2>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RFK Jr demanded a vaccine study be retracted – the journal said no (108 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-025-02682-9</link>
            <guid>44997435</guid>
            <pubDate>Sat, 23 Aug 2025 17:15:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-025-02682-9">https://www.nature.com/articles/d41586-025-02682-9</a>, See on <a href="https://news.ycombinator.com/item?id=44997435">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-test="access-teaser"> <figure><picture><source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-025-02682-9/d41586-025-02682-9_51368256.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-025-02682-9/d41586-025-02682-9_51368256.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px"><img alt="HHS Secretary Robert F. Kennedy Jr. gestures while speaking into microphones at a podium during a press conference." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-025-02682-9/d41586-025-02682-9_51368256.jpg"><figcaption><p><span>Robert F. Kennedy Jr wants a study about vaccines retracted.</span><span>Credit: Tom Williams/CQ Roll Call/Sipa US via Alamy</span></p></figcaption></picture></figure><p>US health secretary and vaccine sceptic Robert F. Kennedy Jr has called for the retraction of a Danish study that found no link between aluminium in vaccines and chronic diseases in children — a rare move for a US public official. Aluminium has been used for almost a century to enhance the immune system’s response to some vaccines. But some people claim the ingredient is linked to rising rates of childhood disorders such as autism.</p><p>Public-health officials in Kennedy’s position rarely request that studies be retracted, says Ivan Oransky, a specialist in academic publishing and co-founder of the media organization Retraction Watch. Through this request, “Secretary Kennedy has demonstrated that he wants the scientific literature to bend to his will”, says Oransky.</p><p>The study<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup> in question, published in <i>Annals of Internal Medicine</i> in July, is one of the largest of its kind, looking at 1.2 million children born over more than two decades in Denmark. The authors reported that no significant risk of developing autoimmune, allergic or neurodevelopmental disorders was associated with exposure to aluminium compounds in vaccines. </p><p>In an <a href="https://www.trialsitenews.com/a/flawed-science-bought-conclusions-the-aluminum-vaccine-study-the-media-wont-question-aaec2793" data-track="click" data-label="https://www.trialsitenews.com/a/flawed-science-bought-conclusions-the-aluminum-vaccine-study-the-media-wont-question-aaec2793" data-track-category="body text link">opinion piece</a> published on TrialSite News on 1 August, Kennedy called into question the study’s methodology, analysis and results. Since his appointment as head of the US Department of Health and Human Services, Kennedy has <a href="https://www.nature.com/articles/d41586-025-01852-z" data-track="click" data-label="https://www.nature.com/articles/d41586-025-01852-z" data-track-category="body text link">bypassed normal scientific review processes</a> to change vaccine recommendations and <a href="https://www.nature.com/articles/d41586-025-02612-9" data-track="click" data-label="https://www.nature.com/articles/d41586-025-02612-9" data-track-category="body text link">terminated grants</a> for projects on mRNA vaccines.</p><p><i>Annals of Internal Medicine</i> says it stands by the study and has no plans to retract it. Christine Laine, editor in chief for the journal, wrote in a comment on the study’s web page on 11 August that “retraction is warranted only when serious errors invalidate findings or there is documented scientific misconduct, neither of which occurred here”.</p><p>The Department of Health and Human Services said that Kennedy’s article spoke for itself, and that the department did not have any further comment in response to <i>Nature</i>’s questions about Kennedy's request for a retraction.</p><h2>Widely used</h2><p>Aluminium, in the form of salts, such as potassium aluminium sulfate, have been administered in vaccines — for diseases ranging from whooping cough to pneumonia — to millions of people worldwide, and the vaccines have been widely studied for safety issues<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup><sup>,</sup><sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>. Gary Grohmann, an independent virologist in Canberra, says there is no evidence of significant side effects caused by the small amount of aluminium in vaccines.</p><p>But in 2011, a study<sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup> published in the <i>Journal of Inorganic Biochemistry</i> claimed to show a causal relationship between rising autism diagnoses in children and increased exposure to aluminium-containing vaccines. In 2012, the World Health Organization’s Global Advisory Committee on Vaccine Safety said the study and another by the same authors were “<a href="https://www.who.int/groups/global-advisory-committee-on-vaccine-safety/topics/adjuvants" data-track="click" data-label="https://www.who.int/groups/global-advisory-committee-on-vaccine-safety/topics/adjuvants" data-track-category="body text link">seriously flawed</a>” because they used inappropriate study designs, incorrect assumptions and questionable data.</p><p>Since then, Grohmann says, the claim that aluminium in vaccines causes autism has been debunked “again and again”. “If there was a mechanism of action where a particular vaccine caused autism, we’d see it in 80, 90, 100% of people receiving the vaccine, and we don’t,” he says. Any association between autism and vaccines is probably a coincidence of timing, he says. “In other words, vaccines might be given at the age of two, and autism genetically might also kick in at the age of two,” he adds.</p><p>Allen Cheng, an epidemiologist at Monash University in Melbourne, Australia, says the Danish study adds to the evidence that vaccines containing aluminium are safe.</p><h2>Kennedy’s concerns</h2><p>Among Kennedy’s criticisms of the Danish study are that the analysis excluded children who had died before the age of two. According to Kennedy, this means that the children “most likely to reveal injuries” associated with aluminum exposure were excluded.</p><p>Kennedy also criticized the fact that the authors did not compare vaccinated and unvaccinated children to determine whether any aluminium exposure causes harm, even though they had some data on unvaccinated children.</p><p>Other critiques posted on the journal website overlapped with Kennedy’s criticisms, says Anders Hviid, the senior author and an epidemiologist at the Statens Serum Institut in Copenhagen, Denmark’s public-health agency. Hviid says he and his colleagues addressed the critiques “one by one”. He also published a <a href="https://www.trialsitenews.com/a/data-vs.-doubt-danish-scientist-responds-to-u.s.-hhs-secretary-critique-of-aluminum-vaccine-study-290120e9" data-track="click" data-label="https://www.trialsitenews.com/a/data-vs.-doubt-danish-scientist-responds-to-u.s.-hhs-secretary-critique-of-aluminum-vaccine-study-290120e9" data-track-category="body text link">rebuttal</a> of Kennedy’s article on TrialSite News on 3 August.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I hacked Monster Energy (178 pts)]]></title>
            <link>https://bobdahacker.com/blog/monster-energy</link>
            <guid>44997145</guid>
            <pubDate>Sat, 23 Aug 2025 16:42:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bobdahacker.com/blog/monster-energy">https://bobdahacker.com/blog/monster-energy</a>, See on <a href="https://news.ycombinator.com/item?id=44997145">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
            <div>
                <h2>The Energy Drink Giant That Forgot to Lock Its Doors</h2>
<p>As a hacker who likes energy drinks, I decided to check out Monster Energy's corporate infrastructure. What I found was completely exposed and making terrible security decisions.</p>
<h3>Monster University: Where Security Goes to Die</h3>
<p>Monster University (<code>mu.monsterenergy.com</code>) is where Monster employees go to learn about their brand. It's also where I learned that changing <code>/login</code> to <code>/register</code> in the URL is apparently Monster's idea of "authentication."</p>
<p>The registration form appeared but wouldn't submit. So I went straight to the JavaScript to find the actual API endpoint. The API helpfully told me exactly which fields were missing from my registration attempt.</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/register_api.png" alt="Monster University Registration API"></p>
<p>Once I called the API directly with the right fields, boom, I was in. Full access to Monster University, complete with all their training materials, including this absolute masterpiece about their target demographic:</p>
<h3>This Is What Monster Thinks You Look Like</h3>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/consumer_demographics.jpg" alt="Monster's " consumer"="" demographics"=""></p>
<p>I'm not kidding. This is from their actual brand training guide. According to Monster Energy, their "Core Brand Family Consumer" is:</p>
<p><em>"Monster Green shoppers are likely younger (Gen-Z/Millennial/Gen-X) male, lower income &amp; Caucasian (skews Hispanic)."</em></p>
<p>And they included this photo of five people in Monster gear looking like they're being held hostage in a marketing photoshoot. This is literally what Monster corporate thinks their average customer looks like. I can't make this stuff up.</p>
<h3>The Irony: Their Own Cybersecurity Training</h3>
<p>The best part? Monster University has a cybersecurity course they bought from a third-party vendor. The same platform with no authentication has lessons teaching employees about phishing and basic security:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/cybersecurity_course.png" alt="Monster Cybersecurity Course"></p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/gamelist.png" alt="Monster Cybersecurity Game List"></p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/amplify_game.png" alt="Amplify Cybersecurity Game"></p>
<p>The irony of hosting a cybersecurity course about phishing on a completely unsecured platform is just <em>chef's kiss</em>.</p>
<h3>Meanwhile in Monster Corporate: Walmart Zoom Calls and "ULTIMATE BEAST" Badges</h3>
<p>While exploring Monster University, I found some gems about their corporate culture. You can view their entire Zoom meeting schedule and even get the join links:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/walmart_meeting.png" alt="Walmart Office Hours Event"></p>
<p>And check out their employee achievement system - you can earn badges for everything from "BEAST" to "ULTIMATE BEAST":</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/monster_badges.png" alt="Monster Achievement Badges"></p>
<h3>Beast Bux: Monster's Internal Currency System</h3>
<p>But wait, it gets better. I found their internal employee rewards system called "Beast Bux." Here's their actual training video explaining it:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/video_section.png" alt="Monster University Video Section"></p>

<p>Essentially, employees get Beast Bux yearly and can give them to other employees to show appreciation. These can be used to buy Monster merch at their internal store: <a target="_blank" rel="noopener noreferrer" href="https://bleedgreenshop.monsterenergy.com/">https://bleedgreenshop.monsterenergy.com/</a></p>
<h3>The Real Treasure: Exposed Corporate File System</h3>
<p>The scariest part wasn't the training portal or the questionable customer profiling. It was finding their OpenText API completely exposed with no authentication required:</p>
<pre><code>https://opentextapi.monsterenergy.com/opentext/search/?page=1&amp;pageSize=1000&amp;searchTerms=
</code></pre>
<p>This endpoint allows anyone to search through Monster's entire file system. No password. No authentication. Nothing.</p>
<p>Want to see internal contracts? Sure, here's one I found:</p>
<pre><code>https://opentextapi.monsterenergy.com/opentext/images/7e02a7602d8cee4aaf5b999850c243df9d0a184b
</code></pre>
<p>(Don't let the "images" in the URL fool you - it serves all file types, not just images)</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/contract.png" alt="Monster Contract Document"></p>
<p>The API returns full document metadata, file paths, and direct download links for everything in their system. Contracts, internal documents, you name it.</p>
<h3>Even Worse: The ClickUp Integration Disaster</h3>
<p>On a subdomain called Kermometer (<code>kermometer.monsterenergy.com</code>), I discovered Monster had integrated ClickUp into their workflow, but they made a critical mistake: they exposed an admin's private account token directly in their website's JavaScript.</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/clickup_token.png" alt="ClickUp Token Exposed in JavaScript"></p>
<p>This token would allow anyone to:</p>
<ul>
<li>Access their entire ClickUp workspace</li>
<li>View all private documents and projects</li>
<li>Invite themselves to the workspace</li>
<li>Potentially modify or delete critical project data</li>
</ul>
<p>To prove it worked, I invited myself to their workspace:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/clickup_invite.png" alt="ClickUp Workspace Invitation"></p>
<p>For the lulz, I wrote a script to share everything with myself using their admin token:</p>
<p><img src="https://bobdahacker.com/static/images/blogs/monsterEnergy/sharing_script.png" alt="Script Sharing Everything With Me"></p>
<p>Don't worry, I left their workspace immediately after proving the vulnerability. I'm not trying to steal Monster's secret energy drink formulas or anything.</p>
<h3>The Response (Or Lack Thereof)</h3>
<p>I tried contacting Monster Energy directly about these vulnerabilities. No response.</p>
<p>They did fix the Monster University registration issue, but I don't think they even read my emails - they probably just noticed someone had signed up through their broken system and patched it.</p>
<p>Finally, I told ClickUp themselves about the exposed token on Monster's site. They investigated and contacted Monster, getting it fixed in less than a week. </p>
<p>But Monster? They never even acknowledged any of my reports. And as you can see, they left their entire file system API wide open.</p>
<p><strong>The OpenText API is STILL ACTIVE as of writing this post.</strong></p>
<h3>To Monster Energy</h3>
<p>Your energy drinks might "Unleash the Beast," but your security is definitely asleep.</p>
<p>Maybe spend less time creating stereotypical customer profiles and more time securing your infrastructure? Just a thought.</p>
<p>Also, a security contact email would be nice. You know, for next time.</p>
<hr>

            </div>
        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Line scan camera image processing for train photography (327 pts)]]></title>
            <link>https://daniel.lawrence.lu/blog/y2025m09d21/</link>
            <guid>44996938</guid>
            <pubDate>Sat, 23 Aug 2025 16:09:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daniel.lawrence.lu/blog/y2025m09d21/">https://daniel.lawrence.lu/blog/y2025m09d21/</a>, See on <a href="https://news.ycombinator.com/item?id=44996938">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header><div><ol><li><a href="#s1"><span>1</span> <span>Principle of operation</span></a></li><li><a href="#s2"><span>2</span> <span>About the camera</span></a></li><li><a href="#s3"><span>3</span> <span>Detecting the region of interest</span></a></li><li><a href="#s4"><span>4</span> <span>Speed estimation</span></a></li><li><a href="#s5"><span>5</span> <span>Resampling</span></a></li><li><a href="#s6"><span>6</span> <span>Demosaicing</span></a></li><li><a href="#s7"><span>7</span> <span>Getting rid of the vertical stripes</span></a></li><li><a href="#s8"><span>8</span> <span>Denoising</span></a></li><li><a href="#s9"><span>9</span> <span>Skew correction</span></a></li><li><a href="#s10"><span>10</span> <span>Color calibration</span></a></li><li><a href="#s11"><span>11</span> <span>Implementation details</span></a><ol><li><a href="#s11.1"><span>11.1</span> <span>Vibe coding experience</span></a></li></ol></li><li><a href="#s12"><span>12</span> <span>Other people’s line scan photography of trains</span></a><ol><li><a href="#s12.1"><span>12.1</span> <span>Adam Magyar</span></a></li><li><a href="#s12.2"><span>12.2</span> <span>KR64’s blog</span></a></li></ol></li></ol></div></header><p>I use my line scan camera to take cool pictures of trains and other stuff.</p><p>But there’s a lot that goes into properly processing the images.</p><div><figure id="fig1"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_a8414d74520f4baa.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_450228d70b868eb2.jpg" alt=""></a><figcaption><a href="#fig1">FIGURE 1</a> A cool tram.</figcaption></figure></div><div><figure id="fig2"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_0d9fee240b0c6e5f.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_5df7185ac60d16b4.jpg" alt=""></a><figcaption><a href="#fig2">FIGURE 2</a> A cool train, the Renfe AVE Class 102, nicknamed <em>Pato</em> because of its duck bill-like appearance.</figcaption></figure></div><div><figure id="fig3"><a href="https://i.dllu.net/rgb_9_prod_no_denoise_20704300acae5f1e.jpg"><img src="https://i.dllu.net/rgb_9_prod_no_denoise_16c81b9b2f43f4bb.jpg" alt=""></a><figcaption><a href="#fig3">FIGURE 3</a> Cool diesel locomotive.</figcaption></figure></div><div><figure id="fig4"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_dd93f40ada264e00.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_7e6bd0d3e527a2d0.jpg" alt=""></a><figcaption><a href="#fig4">FIGURE 4</a> Nice CRH6A intercity electric multiple unit.</figcaption></figure></div><div><figure id="fig5"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_685eacc3349ba19a.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_b45d3300018b8368.jpg" alt=""></a><figcaption><a href="#fig5">FIGURE 5</a> Awesome CR400AF. Super fast.</figcaption></figure></div><h2 id="s1"><a href="#s1">1</a> <span>Principle of operation</span></h2><p>The way it works is that the camera has a single column of pixels (or in this case, two columns), that scans at a super high speed.
The camaera is stationary, but as a train moves past it, it gets scanned.</p><p>This is essentially also how a <a href="https://en.wikipedia.org/wiki/Photo_finish">photo finish camera</a> works.</p><p>Since the background is static, it gets repeated for every column of the image, giving it its distinctive striped look.</p><p>Line scan cameras are very suitable for capturing trains, since I can capture the full length of the train with minimal perspective distortion.
This is super nice for train nerds who want to make models of the trains.
Also, as you keep the camera running, you can get incredibly high resolution photos that span over 100,000 pixels wide.</p><p>By the way, film photo finish cameras and strip cameras behave almost the same as line scan cameras but with one subtle distinction, which is that you have to pull the film across a strip that’s somewhat wider than a single column of pixels.
This is because film is less sensitive than modern digital image sensors.
However, you’ll need to know the approximate speed of the subject and pull the film across at roughly the right speed.</p><h2 id="s2"><a href="#s2">2</a> <span>About the camera</span></h2><p>I’m using an <a href="https://www.alkeria.com/products/necta-series">Alkeria Necta N4K2-7C</a>.
It has a 4096×2 <a href="https://en.wikipedia.org/wiki/Bayer_filter">Bayer array</a> image sensor.
I’m saving its raw data in 16 bit binary arrays.</p><div><figure id="fig8"><a href="https://i.dllu.net/2024-09-12-18-31-00_DSCF0249_d644edd144f26bdabd7a876eda224c2a673ad9ec_d6e13378a10243e5.jpg"><img src="https://i.dllu.net/2024-09-12-18-31-00_DSCF0249_d644edd144f26bdabd7a876eda224c2a673ad9ec_7281c09603aff005.jpg" alt=""></a><figcaption><a href="#fig8">FIGURE 8</a> My camera.</figcaption></figure></div><iframe width="320" height="400" src="https://www.youtube.com/embed/r-GHYwkQD1o" title="line scan photography of Shanghai Transrapid" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe><div><figure id="fig9"><a href="https://i.dllu.net/IMG_6801_654e71a21ee44795.jpg"><img src="https://i.dllu.net/IMG_6801_9c68db912482ad71.jpg" alt=""></a><figcaption><a href="#fig9">FIGURE 9</a> Waiting for a subway train to roll by in Brooklyn, New York.</figcaption></figure></div><h2 id="s3"><a href="#s3">3</a> <span>Detecting the region of interest</span></h2><p>Sometimes, I keep the line scan camera running for a while, and it generates tons of boring data of the background.
To detect moving things, I compute an “energy function” that’s defined as</p><p>where <img src="https://daniel.lawrence.lu/texcache/4ba7d3cff9565fbc8341f413e7711422cb61cc27i.svg" alt="\text{max}_\mathbf{I}"> is the maximum pixel value of the image, and the partial derivative are the <a href="https://en.wikipedia.org/wiki/Image_gradient">image gradient</a>.</p><div><figure id="fig10"><a href="https://i.dllu.net/score_sample_91b1560a90ba63f2.png"><img src="https://i.dllu.net/score_sample_8c65dd9f93d6c9aa.png" alt=""></a><figcaption><a href="#fig10">FIGURE 10</a> Example energy image.</figcaption></figure></div><p>This is because, for a static background, it will be full of horizontal stripes.
By weighing the <img src="https://daniel.lawrence.lu/texcache/11f6ad8ec52a2984abaafd7c3b516503785c2072i.svg" alt="x">-direction (time direction) gradient against the total gradient norm, we can find areas where it’s a more vertical-ish structure rather than a horizontal structure.
However, doing this by itself risks noisy gradients in empty (but noisy) areas where the gradient direction is completely random.
The maximum pixel value term ensures that whatever gradient we see is salient.</p><p>The image is divided into chunks and the score of a chunk is the 99th percentile energy.</p><p>Finally, chunks containing moving objects are defined to be ones where the score is at least 1.5× that of the minimum score.</p><p>This heuristic took me longer than I would like to admit to figure out.
Previously, I came up with heuristics that worked well on one capture but couldn’t generalize well to other captures.
Sometimes, the background will contain slowly moving foliage waving in the wind, that would screw up other methods of detection.
That resulted in a lot of wasted time because time spent processing empty regions seriously slows down iteration speed when developing the later steps.</p><h2 id="s4"><a href="#s4">4</a> <span>Speed estimation</span></h2><p>The most common question I get is, how do I estimate the speed of the subject?
If I don’t do it properly, it will appear stretched out, squished, or flipped.</p><p>Typically, I just set the camera to scan as fast as possible while maintaining a decent exposure, so the scan rate is independent of the subject.
Faster subjects will appear squished, and slower subjects will appear stretched out.</p><p>For most of my earlier works, I just eyeballed it. A good rule of thumb is to look for round things such as the wheels and “no smoking” signs.
But now I have a fully automated technique that works fairly robustly.</p><p>The key idea is to exploit the fact that the line scan camera actually has two lines in a <a href="https://en.wikipedia.org/wiki/Bayer_filter">Bayer array</a>, where one line is red, green, red, green, and the second line is green, blue, green blue.
By comparing the two green channels, we are able to figure out how fast stuff is moving.</p><p>The problem is that the data is very noisy, and salient features are sparse.
Here’s the general approach:</p><ul><li>Divide image into chunks.</li><li>Compute the absolute difference between the 2 green channels of each chunk for various small shifts (from -7 to +7). This gives us a cost array for each chunk.</li><li>Perform subpixel peak interpolation in the cost array using an iteratively reweighted Gaussian, <a href="https://en.wikipedia.org/wiki/Mean_shift">mean shift</a> style. This gives us a shift estimate per chunk.</li><li>Fit a robust spline to the shift estimates.</li></ul><div><figure id="fig11"><a href="https://i.dllu.net/mean_shift_0080_e3d219d77b34d22e.png"><img src="https://i.dllu.net/mean_shift_0080_c375801a598f46a4.png" alt=""></a><figcaption><a href="#fig11">FIGURE 11</a> Interpolating to find the peak using mean shift.</figcaption></figure></div><div><figure id="fig12"><a href="https://i.dllu.net/spline_8457468cc8c5d06a.png"><img src="https://i.dllu.net/spline_49ca48ada37ab8d2.png" alt=""></a><figcaption><a href="#fig12">FIGURE 12</a> Sample spacing and fitted spline.</figcaption></figure></div><p>As you can see, the data is noisy, but we have surprisingly decent granularity for this very subpixel case where we were scanning slower than needed so the spacing is like 0.5.</p><p>The value of the spline is actually the <em>sample spacing</em>. It tells us how close together or far apart the sample points in the original time series we should be using.
This leads us to the next section.</p><div><figure id="fig13"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_b83579bd26d10381.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_2e790d956d6045d7.jpg" alt=""></a><figcaption><a href="#fig13">FIGURE 13</a> Uncorrected left end. It’s squished!!!</figcaption></figure></div><div><figure id="fig14"><a href="https://i.dllu.net/rgb_2_prod_no_denoise_32cd95888d846ffa.jpg"><img src="https://i.dllu.net/rgb_2_prod_no_denoise_aeefb00dbe18bf6f.jpg" alt=""></a><figcaption><a href="#fig14">FIGURE 14</a> Uncorrected right end. It’s slightly squished but not nearly as much.</figcaption></figure></div><div><figure id="fig15"><a href="https://i.dllu.net/rgb_0_prod_no_denoise_e458634bcd3cc936.jpg"><img src="https://i.dllu.net/rgb_0_prod_no_denoise_b489258f43a8a411.jpg" alt=""></a><figcaption><a href="#fig15">FIGURE 15</a> Left end of New York subway train.</figcaption></figure></div><div><figure id="fig16"><a href="https://i.dllu.net/rgb_3_prod_no_denoise_fa356ca82a22b333.jpg"><img src="https://i.dllu.net/rgb_3_prod_no_denoise_6c643c2fa36cc924.jpg" alt=""></a><figcaption><a href="#fig16">FIGURE 16</a> Right end of New York subway train.</figcaption></figure></div><p>Hmm, I think my speed estimation still isn’t perfect. It could be off by about 10%.
For future work, I think I might be able to extract features correspondences such as SIFT or LightGlue. Trains are full of repeating elements that are supposed to be evenly spaced. I can detect those, and add a cost function to evenly space them, and optimize.
Another idea is to use a circle Hough transform to find circles.</p><h2 id="s5"><a href="#s5">5</a> <span>Resampling</span></h2><p>From the spline that gives us the sample spacing, we can basically generate the samples as such:</p><div><pre><span></span><span>samples</span> <span>=</span> <span>[]</span>
<span>sample_position</span> <span>=</span> <span>0.0</span>
<span>while</span> <span>sample_position</span> <span>&lt;</span> <span>raw_width</span><span>:</span>
    <span>samples</span><span>.</span><span>append</span><span>(</span><span>sample_position</span><span>)</span>
    <span>sample_position</span> <span>+=</span> <span>spline</span><span>(</span><span>sample_position</span><span>)</span>
</pre></div>
<p>However, there are a few gotchas:</p><ul><li>If the spline is negative-valued, it means the subject is going the other way, i.e. the image is flipped. In this case, I start with <code>sample_position</code> set to <code>raw_width</code> and go from right to left.</li><li>If the spline goes to zero, we are doomed because the while loop will never terminate. I clamp the steps to at least 0.1 and throw an error if the spline has both positive and negative values.</li><li>This is sort of a naive integration compared to the trapezoidal rule or something. However, given that the spline moves very slowly, it is fine.</li></ul><p>Now, for each sample position, we also store the sample width, which is the value of the spline.
If we were to simply extract a single column from the raw data, we would be throwing away a lot of data, and the result wouldn’t be antialiased.
Instead, it is better to pick a window of width proportional to the sample spacing.
I chose a <a href="https://en.wikipedia.org/wiki/Hann_function">Hann window</a>.</p><div><figure id="fig17"><a href="https://i.dllu.net/2025-08-19-12-10-58_b5c58484522b4579.png"><img src="https://i.dllu.net/2025-08-19-12-10-58_26c04e2f3b5d16e1.png" alt=""></a><figcaption><a href="#fig17">FIGURE 17</a> Naively selecting columns instead of using a windowing function.</figcaption></figure></div><div><figure id="fig18"><a href="https://i.dllu.net/2025-08-19-12-11-19_006f733f714d000c.png"><img src="https://i.dllu.net/2025-08-19-12-11-19_1c8c1f1331cf0c62.png" alt=""></a><figcaption><a href="#fig18">FIGURE 18</a> Using a rectangular window.</figcaption></figure></div><p>Not only is the first image very grainy, but the rapidly blinking LED display showing the characters for 筲箕灣 is completely illegible without proper sampling.</p><div><figure id="fig19"><a href="https://i.dllu.net/2025-08-21-23-01-25_b836f51e7b72da06.png"><img src="https://i.dllu.net/2025-08-21-23-01-25_f6d4fd8579a3ee52.png" alt=""></a><figcaption><a href="#fig19">FIGURE 19</a> Upsampling using a rectangular window.</figcaption></figure></div><div><figure id="fig20"><a href="https://i.dllu.net/2025-08-21-23-00-57_cf42ac3a31b6cdf8.png"><img src="https://i.dllu.net/2025-08-21-23-00-57_89de76f62fa2ad89.png" alt=""></a><figcaption><a href="#fig20">FIGURE 20</a> Upsampling using a Hann window.</figcaption></figure></div><p>As you can see, the rectangular window performs very poorly when upsampling and introduces horrible jagged artifacts. The Hann window does better. Some other windows like the Sinc are even better supposedly.</p><h2 id="s6"><a href="#s6">6</a> <span>Demosaicing</span></h2><p>Recall that the camera has two lines forming a Bayer array.</p><p>If we simply create an image of half resolution (i.e. 2048 pixels tall instead of 4096), by grouping each RGGB group into one pixel, we would have some nasty fringing problems since the red and blue pixels are offset.</p><div><figure id="fig21"><a href="https://i.dllu.net/2025-08-19-17-57-15_ea319c5f717e47df.png"><img src="https://i.dllu.net/2025-08-19-17-57-15_35d9b99165d33e0a.png" alt=""></a><figcaption><a href="#fig21">FIGURE 21</a> Fringing due to bad demosaicing.</figcaption></figure></div><div><figure id="fig22"><a href="https://i.dllu.net/2025-08-19-17-57-35_41dc85cc57217678.png"><img src="https://i.dllu.net/2025-08-19-17-57-35_92ba71ba7bbc7152.png" alt=""></a><figcaption><a href="#fig22">FIGURE 22</a> Better.</figcaption></figure></div><p>Instead we should write out the image with careful attention to offsets, interpolating as necessary.
Note that the horizontal offsets must be done <em>after</em> speed estimation, because, before speed estimation, the <img src="https://daniel.lawrence.lu/texcache/11f6ad8ec52a2984abaafd7c3b516503785c2072i.svg" alt="x">-axis is time, and after speed estimation, the <img src="https://daniel.lawrence.lu/texcache/11f6ad8ec52a2984abaafd7c3b516503785c2072i.svg" alt="x">-axis is space.
But the 2-pixel wide Bayer array is physically a <em>spatial</em> offset.</p><p>I implemented a basic interpolation scheme that uses bilinear interpolation.
This fixes most of the fringing, although we can do even better. That will be left for future work.</p><p>Unlike a traditional Bayer array, here we have the possibility that the green channels cover 100% of the pixels, so we can potentially do better than traditional demosaicing algorithms.
But there’s currently an annoying problem, which is that the two green channels on my line scan camera don’t match.</p><h2 id="s7"><a href="#s7">7</a> <span>Getting rid of the vertical stripes</span></h2><p>Vertical stripes in the image are common and are due to two main reasons:</p><ul><li>Clock jitter. The exposure time of each column may be randomly slightly off for some reason.</li><li>I’ve noticed that when a dark object shows up, like the coupling between train cars, the whole slice of the image there is brighter.</li></ul><p>To fix this, I use linear regression to fit a basic model of the form:</p><p>where <img src="https://daniel.lawrence.lu/texcache/86f7e437faa5a7fce15d1ddcb9eaeaea377667b8i.svg" alt="a">, <img src="https://daniel.lawrence.lu/texcache/e9d71f5ee7c92d6dc9e92ffdad17b8bd49418f98i.svg" alt="b">, and <img src="https://daniel.lawrence.lu/texcache/84a516841ba77a5b4648de2cd0dfcb30ea46dbb4i.svg" alt="c"> are scalar parameters of the model, <img src="https://daniel.lawrence.lu/texcache/5fafb73566c2a261cb4f5b35b6642b16d8284d3di.svg" alt="\mathbf{x}"> is a 2048-element vector containing the luminance value of the column (mean over the 4 channels), and <img src="https://daniel.lawrence.lu/texcache/ba360d8ea75d540ca5e1b1ce009ddbe6cd0691e4i.svg" alt="\mathbf{k}"> is the row index (aka the 2048-element vector of <img src="https://daniel.lawrence.lu/texcache/6b8d068c8ae69b58d2484e0ad9b0f6576d9bd529i.svg" alt="\begin{bmatrix}0, 1, \cdots, 2047\end{bmatrix}">.</p><p>You can compose models as such:</p><p>This gives us a new model <img src="https://daniel.lawrence.lu/texcache/c1086bb06dd2795dc0b7c7de2b953422fa2bc22di.svg" alt="\text{model}_{12}"> with parameters:</p><p>The associative property of the composition operator is left as an exercise for the reader.</p><p>There is also the identity model, i.e. one that does nothing, which is</p><p>and there’s also the inverse:</p><p>so the set of these models forms a mathematical group.</p><p>I fit a model to each consecutive pair of columns using weighted least squares, where we assign each row element a weight based on a Gaussian.
The weight would be:</p><p>In other words, the residual would be</p><p>After fitting this model, we redo the steps again several times, where the <img src="https://daniel.lawrence.lu/texcache/5d3a9194bf53d4e2741ee6cf67ae186b5d22770ai.svg" alt="\mathbf{w}"> vector is recalculated each time.
This is known as <em>iteratively-reweighted least squares</em> and is pretty good at rejecting outliers.</p><div><figure id="fig23"><a href="https://i.dllu.net/jitter_debug_082121e08533fc28.png"><img src="https://i.dllu.net/jitter_debug_c28f675b057b422f.png" alt=""></a><figcaption><a href="#fig23">FIGURE 23</a> The first plot shows the current column’s luminance and previous column’s luminance, as well as the previous column corrected by the model. The second plot shows the weight. The third plot shows the weighted initial and final error.</figcaption></figure></div><p>This all gives us <em>relative</em> models between the previous column and the current column, but we want <em>global</em>models that tell us how to correct each column overall.
We could set the global models by just composing them forever, but they would soon start to drift arbitrarily far away from the identity model.</p><p>You could prevent them from drifting away by solving a band-diagonal linear system where you have residuals of two types:</p><ul><li>prior residual, penalizing the difference between each model from the identity</li><li>relative model residual, penalizing the difference between the delta between adjacent models and the relative model we computed</li></ul><p>This can be solved in <img src="https://daniel.lawrence.lu/texcache/ebc75cd71fe8ecc45d16e8fbe4ca608d05d1efe0i.svg" alt="O(n)">. However, it is a bit of work to implement. In practice, you can mitigate most high frequency stripes by just doing <a href="https://en.wikipedia.org/wiki/Exponential_smoothing">exponential smoothing</a>, which basically acts as a high-pass filter</p><p>for some small <img src="https://daniel.lawrence.lu/texcache/b3931f1ce298c536432fd324b3a1ab4337120689i.svg" alt="\lambda">, in this case hardcoded to be 0.02.</p><div><figure id="fig24"><a href="https://i.dllu.net/2025-08-21-23-06-05_7f2b75d765c999be.png"><img src="https://i.dllu.net/2025-08-21-23-06-05_c5de9c2fbf369030.png" alt=""></a><figcaption><a href="#fig24">FIGURE 24</a> Before. You can see rather subtle stripes in the dark area.</figcaption></figure></div><div><figure id="fig25"><a href="https://i.dllu.net/2025-08-21-23-06-21_d6e64e01704c96d1.png"><img src="https://i.dllu.net/2025-08-21-23-06-21_a3b1e6747c4f6c1c.png" alt=""></a><figcaption><a href="#fig25">FIGURE 25</a> After.</figcaption></figure></div><p>Previously, I also had some success by directly fitting the model to line up each column with the first column. However, it doesn’t work for captures where the background isn’t static (e.g. rotating line scan panoramas, and pointing the line scan camera out of a moving train).</p><p>By the way, I should point out that vertical stripes getting rid of should be done <em>before</em> speed estimation, since it happens in the time domain at capture time.
If a train were speeding up, it would appear stretched out at first, and squished at the end, and the striping would affect the end a lot more than the start.</p><h2 id="s8"><a href="#s8">8</a> <span>Denoising</span></h2><p>I implemented a patch-based denoiser, also known as <a href="https://en.wikipedia.org/wiki/Block-matching_and_3D_filtering">block matching</a>.
It works by making the observation that you often have repeated textures in a line scan photo of a train.
Technically, you also have lots of self-similarity in general photos, so patch-based denoising is a common method for denoising in general.
However, one important distinction is that most denoisers only look in a small neighborhood around the current patch, but mine looks along the entire row.</p><p>What I do is, for each row, we process it independently.
From each 3×3 pixel patch, we can construct a <em>feature vector</em> of size 27 (9 times 3 channels, RGB).
Then, we collect all these features and sort them by mean value.
Now, for each position along the row, we search in the window of size 128 in the sorted vector.
The sorted vector will have similar-looking patches nearby, but we further weigh them by Gaussian similarity to the current patch.
Then, we compute the weighted average of the center pixel of each of those patches.</p><p>Another trick is to realize that the noise is Poisson-distributed which has a standard deviation that scales with the square root of the signal.
But if I just square root the input data first, then we just need to compare it to a constant.</p><p>This works decently, but is incredibly slow.
Let me know if you think of any faster ways to do it. A KD tree in feature space would die from the curse of dimensionality. Perhaps a hash table? To keep things lightweight, we can limit the population in each cell.</p><div><figure id="fig26"><a href="https://i.dllu.net/2025-08-20-16-08-55_264411aaba797e0c.png"><img src="https://i.dllu.net/2025-08-20-16-08-55_648c53661a3709d9.png" alt=""></a><figcaption><a href="#fig26">FIGURE 26</a> Noisy watch.</figcaption></figure></div><div><figure id="fig27"><a href="https://i.dllu.net/2025-08-20-16-08-30_6c3c0102c46a5803.png"><img src="https://i.dllu.net/2025-08-20-16-08-30_7db8ed848f17bc87.png" alt=""></a><figcaption><a href="#fig27">FIGURE 27</a> Denoised watch.</figcaption></figure></div><div><figure id="fig28"><a href="https://i.dllu.net/2025-08-20-16-09-34_8be193ad950c4151.png"><img src="https://i.dllu.net/2025-08-20-16-09-34_4440c8997d90f81e.png" alt=""></a><figcaption><a href="#fig28">FIGURE 28</a> Noisy passenger.</figcaption></figure></div><div><figure id="fig29"><a href="https://i.dllu.net/2025-08-20-16-09-59_dbce64ce63ce8080.png"><img src="https://i.dllu.net/2025-08-20-16-09-59_49cd78feafeaebf8.png" alt=""></a><figcaption><a href="#fig29">FIGURE 29</a> Denoised passenger.</figcaption></figure></div><p>The good thing about the patch-based denoiser is that unique features like this passenger remain virtually unchanged.</p><p>Previously, I also tried using a <a href="https://en.wikipedia.org/wiki/Total_variation_denoising">total variation denoiser</a>, processing each row and column independently.
It worked decently but would often destroy fine detail in textures.</p><h2 id="s9"><a href="#s9">9</a> <span>Skew correction</span></h2><p>If the camera isn’t perfectly upright, the resulting image may be slightly skewed.
I’m planning on implementing automatic skew correction.
But here are two caveats:</p><ul><li>skew detection must be done after speed estimation</li><li>proper sampling should happen after skew detection, since the skew transformation introduces generation loss and we can sample directly from the raw data instead.</li></ul><p>So basically we’d need to generate a quick, poorly sampled version, run skew detection on it, and then sample it properly afterwards.
We can implement skew detection using a <a href="https://daniel.lawrence.lu/blog/y2025m09d21/Hough%20transform">Hough transform</a>.
Generally, I do a decent job of keeping the camera upright, so we just need to correct for very small skews, so a Hough transform is suitable (since the complexity scales with the number of bins of the histogram).
We can also use the energy function from the region of interest detector to primarily care about vertical structures.</p><h2 id="s10"><a href="#s10">10</a> <span>Color calibration</span></h2><p>I kinda just eyeballed this color calibration matrix.</p><p>But to be honest it looks fairly decent.</p><div><figure id="fig30"><a href="https://i.dllu.net/rgb_7_prod_no_denoise_2474ac2d89335c15.jpg"><img src="https://i.dllu.net/rgb_7_prod_no_denoise_e7ce15b1872fe528.jpg" alt=""></a><figcaption><a href="#fig30">FIGURE 30</a> People’s skin tones look fine to me.</figcaption></figure></div><h2 id="s11"><a href="#s11">11</a> <span>Implementation details</span></h2><p>The code is implemented in Python using numpy.</p><p><a href="https://github.com/dllu/nectar/blob/master/python/preview.py">the code</a></p><p>Due to the large size of the data (4096 rows and hundreds of thousands of columns), it is sometimes impossible to fit all of it in memory, so the code takes several passes and outputs in chunks.
Actually, it is probably okay to fit it in a few gigabytes of RAM, but you’d have to chunk up the storage (there’s no way a contiguous numpy array of 4096 by 100,000 could be allocated).</p><h2 id="s11.1"><a href="#s11.1">11.1</a> <span>Vibe coding experience</span></h2><p>I tried using AI to help with a lot of the implementation. However, the results were mixed.</p><p>AI would often accidentally make things quadratic for no reason when a linear time algorithm would suffice. For example, when trying to implement spline-based resampling, ChatGPT 5 came up with horribly slow (but vectorized) code that constructed a giant tensor with a mask across the entire width of the image for <em>every single sample</em>. Since there are 100,000 samples, and each mask was 100,000 columns wide, you can imagine it would take millennia to run. I ended up reimplementing it from scratch by hand. Then Grok 4 implemented weighted least squares regression by materializing the entire weight vector with <code>np.diag</code> instead of simply pre-multiplying each row of <code>A</code> and <code>x</code> with the square root of the weight before doing <code>np.linalg.solve(A, x)</code>. Again, with 100,000 elements, making the square matrix with <code>np.diag</code> would have instantly run out of memory.</p><p>Both Grok 4 Expert and ChatGPT 5 Thinking also completely failed to implement the band-diagonal least squares to my vertical stripes strategy, but as mentioned, the exponential smoothing trick works okay for now.</p><p>However, for some other stuff, AI was quite helpful.
It created a class that dynamically loads chunks from disk but provides the API to index and slice it.
That was neat.
AI was also incredibly good at helping with Matplotlib’s arcane syntax.</p><h2 id="s12"><a href="#s12">12</a> <span>Other people’s line scan photography of trains</span></h2><h2 id="s12.1"><a href="#s12.1">12.1</a> <span>Adam Magyar</span></h2><p><a href="https://www.magyaradam.com/wp/">Adam Magyar</a> uses a black and white digital line scan camera for his “Stainless” project, and another derived from a scanner for his “Urban Flow” project.</p><p>His camera must have much better sensitivity than mine since he managed to capture fairly clean images even for underground trains (whereas I generally require sunlight for mine).
Apparently, he had to scout out many subway stations to find ones where the lights don’t flicker at 60 Hz.</p><h2 id="s12.2"><a href="#s12.2">12.2</a> <span>KR64’s blog</span></h2><p>At <a href="https://web.archive.org/web/20250715102540/https://kr64.seesaa.net/">kr64.seesaa.net</a> you can find a mind boggling collection of high quality line scan photos of trains from all across Japan.</p><p>They probably do this full time as the variety of trains is far greater than I can ever hope to achieve.
I believe they use a film slit scan camera.</p><p>Unfortunately, their website has a bunch of technical issues and often goes down. I would be happy to help them out but my Japanese is very poor and I don’t see any way to contact them.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RFC 9839 and Bad Unicode (249 pts)]]></title>
            <link>https://www.tbray.org/ongoing/When/202x/2025/08/14/RFC9839</link>
            <guid>44995640</guid>
            <pubDate>Sat, 23 Aug 2025 12:54:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tbray.org/ongoing/When/202x/2025/08/14/RFC9839">https://www.tbray.org/ongoing/When/202x/2025/08/14/RFC9839</a>, See on <a href="https://news.ycombinator.com/item?id=44995640">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="centercontent">
<p itemprop="description">Unicode is good. If you’re designing a data structure or protocol that has text fields, they should contain
    Unicode characters encoded in UTF-8. There’s another question, though:
    “<em>Which</em> Unicode characters?” The 
    answer is “Not all of them, please exclude some.”</p>

<p>This issue keeps coming up, so Paul Hoffman and I put together an individual-submission draft
    to the IETF and now (where by “now” I mean “two years later”) it’s been published as
    <a href="https://www.rfc-editor.org/rfc/rfc9839.html">RFC 9839</a>. It explains which characters are bad, and why, then offers
    three plausible less-bad subsets that you might want to use.
    Herewith a bit of background, but…</p>

<p id="p-2"><span>Please</span> · 
If you’re actually working on something new that will have text fields, please read the RFC. It’s only ten pages long, and that’s
    with all the IETF boilerplate. It’s written specifically for software and networking people.</p>

<p id="p-3"><span>The smoking gun</span> · 
The badness that 9839 focuses on is “problematic characters”, so let’s start with a painful example of what that means.
    Suppose you’re designing a protocol that uses JSON and one of your constructs has a <code>username</code> field.
    Suppose you get this message (I omit all the non-<code>username</code> fields). It’s 
    a perfectly legal JSON text:</p>

<div><pre><span></span><span>{</span>
  <span>  </span><span>"username"</span><span>:</span><span> </span><span>"\u0000\u0089\uDEAD\uD9BF\uDFFF"</span>
<span>}</span>    </pre></div>
    <p>Unpacking all the JSON escaping gibberish reveals that the value of the <code>username</code> field contains four 
    numeric “code points” identifying Unicode characters:</p>

    <ol>
      <li><p>The first code point is zero, in Unicode jargon <code>U+0000</code>. In human-readable text it
      has no meaning, but it will interfere with the operation of certain programming languages.</p>
</li>
      <li><p>Next is Unicode <code>U+0089</code>, official name “CHARACTER TABULATION WITH JUSTIFICATION”. It’s what Unicode calls a
      <a href="https://en.wikipedia.org/wiki/C0_and_C1_control_codes">C1
      control code</a>, inherited from ISO/IEC 6429:1992, adopted from 
      <a href="https://www.ecma-international.org/wp-content/uploads/ECMA-48_5th_edition_june_1991.pdf">ECMA 48</a> (1991), which calls it
      “HTJ” and says: <i>HTJ causes the contents of the active field (the field in the presentation component that contains the
      active presentation position) to be shifted forward so that it ends at the character position preceding the
      following character tabulation stop. The active presentation position is moved to that following character
      tabulation stop. The character positions which precede the beginning of the shifted string are put into the
      erased state.</i></p>

      <p>Good luck with that.</p>
</li>
      <li><p>The third code point, <code>U+DEAD</code>, in Unicode lingo, is an “unpaired surrogate”.  To understand,
      you’d have to learn how Unicode’s much-detested
      <a href="https://en.wikipedia.org/wiki/UTF-16">UTF-16</a> encoding works.
      I recommend not bothering.</p>

      <p>All you need to know is that surrogates are only meaningful when they come in pairs in UTF-16 encoded text. There is
      effectively no such text on the wire and thus no excuse for tolerating surrogates in your data. In fact, the UTF-8 specification
      says that you mustn’t use UTF-8 to encode surrogates. But the real problem is that different libraries in different
      programming languages don’t always do the same things when they encounter this sort of fœtid interloper.</p>
    </li>
      <li><p>Finally, <code>\uD9BF\uDFFF</code> is JSON for the code point <code>U+7FFFF</code>.
      Unicode has a category called “noncharacter”, containing a few dozen code points that, for a variety of
      reasons, some good, 
      don’t represent anything and must not be interchanged on the wire. <code>U+7FFFF</code> is one of those.</p>
</li>
    </ol>
    <p>The four code points in the example are all clearly problematic. 
    The just-arrived RFC 9839 formalizes the notion of “problematic” and
    offers easy-to-cite language saying which of these problematic types you want to
    exclude from your text fields. Which, if you’re going to use JSON, you should probably do.</p>

    <p id="p-6"><span>Don’t blame Doug</span> · 
    Doug Crockford I mean, the inventor of JSON.  If he (or I or really anyone careful) were inventing JSON now that Unicode is
    mature, he’d have been fussier about its character repertoire. Having said that, we’re stuck with JSON-as-it-is forever, so we
    need a good way to say which of the problematic characters we’re going to exclude even if JSON allows them.</p>

    <p id="p-5"><span>PRECISion</span> · 
    You may find yourself wondering why the IETF waited until 2025 to provide help with Bad Unicode.
    It didn’t; here’s
    <a href="https://www.rfc-editor.org/rfc/rfc8264.html">RFC 8264</a>: <cite>PRECIS Framework: Preparation, Enforcement, and
    Comparison of Internationalized Strings in Application Protocols</cite>; the first PRECIS predecessor was published in 2002.
    8264 is 43 pages long, containing a <em>very</em>
    thorough discussion of many more potential Bad Unicode issues than 9839 does.</p>

    <p>Like 9839, PRECIS specifies subsets of the Unicode character repertoire and goes further, providing a mechanism for defining
    more.</p>

    <p>Having said that, PRECIS doesn’t seem to be very widely used by people who are defining new data structures and protocols. My
    personal opinion is that there are two problems which make it hard to adopt. First, it’s large and 
    complex, with many moving parts, and requires careful study to understand. Developers are (for good reason) lazy.</p>

    <p>Second, using PRECIS ties you to a specific version of Unicode. In particular, it forbids the use of the (nearly a million)
    unassigned code points. Since each release of Unicode includes new code point assignments, that means that a sender and receiver
    need to agree on exactly which version of Unicode they’re both going to use if they want reliably interoperable behavior. This
    makes life difficult for anyone writing a general-purpose code designed to be used in lots of different applications.</p>

    <p>I personally think that the only version of Unicode anybody wants to use is “as recent as possible”, so they can be confident
    of having all the latest emojis.</p>

    <p>Anyhow, 9839 is simpler and dumber than PRECIS. But I think some people will find it useful and now the IETF agrees.</p>

    <p id="p-7"><span>Source code</span> · 
    I’ve written a little Go-language library to validate incoming text fields against each of the three subsets that 9839
    specifies,
    <a href="https://github.com/timbray/RFC9839">here</a>.  I don’t claim it’s optimal, but it is well-tested.</p>

    <p>It doesn’t have a version number or release just yet, I’ll wait till a few folk have had a chance to spot any dumb mistakes I
    probably made.</p>

    <p id="p-9"><span>Details</span> · 
    Here’s a compact summary of the world of problematic Unicode code points and data formats and standards.</p>

    <table>
      <tbody><tr><td></td><th colspan="3">Problematic classes excluded?</th></tr>
      <tr><td></td><th>Surrogates</th><th>Legacy controls</th><th>Noncharacters</th></tr>
      <tr><td>CBOR</td><td>yes</td><td>no</td><td>no</td></tr>
      <tr><td>I-JSON</td><td>yes</td><td>no</td><td>yes</td></tr>
      <tr><td>JSON</td><td>no</td><td>no</td><td>no</td></tr>
      <tr><td>Protobufs</td><td>no</td><td>no</td><td>no</td></tr>
      <tr><td>TOML</td><td>yes</td><td>no</td><td>no</td></tr>
      <tr><td>XML</td><td>yes</td><td>partial [1]</td><td>partial [2]</td></tr>
      <tr><td>YAML</td><td>yes</td><td>mostly [3]</td><td>partial [2]</td></tr>
      <tr><td></td><th colspan="3">RFC 9839 Subsets</th></tr>
      <tr><td>Scalars</td><td>yes</td><td>no</td><td>no</td></tr>
      <tr><td>XML</td><td>yes</td><td>partial</td><td>partial</td></tr>
      <tr><td>Assignables</td><td>yes</td><td>yes</td><td>yes</td></tr>
    </tbody></table>
    <p>Notes:</p>

    <p><b>[1]</b> XML allows C1 controls.</p>

    <p><b>[2]</b> XML and YAML don’t exclude the noncharacters outside the Basic Multilingual Pane.</p>

    <p><b>[3]</b> YAML excludes all the legacy controls except for the mostly-harmless <code>U+0085</code>, another version of
    <code>\n</code> used in IBM mainframe documents.</p>

    <p id="p-8"><span>Thanks!</span> · 
    9839 is not a solo production. It received an extraordinary amount of discussion and improvement from a lot of smart and
    well-informed people 
    and the published version, 15 draft revisions later, is immensely better than my initial draft. My sincere thanks go to my
    co-editor Paul Hoffman and to all those mentioned in the RFC’s “Acknowledgements” section.</p>

    <p id="p-4"><span>On individual submissions</span> · 
    9839 is the second “individual submission” RFC I’ve pushed through the IETF (the other is
    <a href="https://datatracker.ietf.org/doc/html/rfc7725">RFC 7725</a>, which registers the HTTP 451 status code).  While it’s nice
    to decide something is worth standardizing and eventually have that happen, it’s really a lot of work. Some of that work is
    annoying.</p>

    <p>I’ve been involved in
    other efforts as Working-Group member, WG chair, and WG specification editor, and I can report authoritatively that creating an
    RFC the traditional way, through a Working Group, is easier and better.</p>

    <p>I feel discomfort advising others not to follow in my footsteps, but in this case I think it’s the right advice.</p>

  <hr>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing Speed-of-Light Flash Attention for 5090 in CUDA C++ (150 pts)]]></title>
            <link>https://gau-nernst.github.io/fa-5090/</link>
            <guid>44995508</guid>
            <pubDate>Sat, 23 Aug 2025 12:29:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gau-nernst.github.io/fa-5090/">https://gau-nernst.github.io/fa-5090/</a>, See on <a href="https://news.ycombinator.com/item?id=44995508">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><header><p><time>Aug 23, 2025</time></p></header><section><p>In this post, I will walkthrough how I learned to implement Flash Attention for 5090 in CUDA C++. The main objective is to learn writing attention in CUDA C++, since many features are not available in <a href="https://triton-lang.org/main/index.html">Triton</a>, such as MXFP8 / NVFP4 MMA for sm120. I also feel this is a natural next step after learning about matmul kernels. Lastly, there are <a href="https://alexarmbr.github.io/2024/08/10/How-To-Write-A-Fast-Matrix-Multiplication-From-Scratch-With-Tensor-Cores.html">many</a> <a href="https://www.spatters.ca/mma-matmul">excellent</a> <a href="https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog">blogposts</a> on writing fast matmul kernels, but there is none for attention. So I want to take this chance to write up something nicely.</p><p>Readers are highly recommended to be familiar with CUDA C++ and how to use Tensor cores on NVIDIA GPUs. Of course you can still read along and clarify with your favourite LLMs along the way. Or you can check out GPU-MODE series (<a href="https://github.com/gpu-mode/lectures">slides</a>, <a href="https://www.youtube.com/@GPUMODE">YouTube</a>) for basic CUDA C++ knowledge, as well as the excellent matmul blogposts mentioned above, to quickly get up to speed.</p><p>You can find the full implementation discussed in this post here: <a href="https://github.com/gau-nernst/learn-cuda/tree/e83c256/07_attention">https://github.com/gau-nernst/learn-cuda/tree/e83c256/07_attention</a>. For <code>bs=1, num_heads=8, len_query=4096, len_kv = 8192</code>, 5090 @ 400W, compile with CUDA 12.9, I obtained the following benchmark results (theoretical limit of 5090 is 209.5 TFLOPS for BF16)</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td><code>F.sdpa()</code> (Flash Attention)</td><td>186.73</td><td>89.13%</td></tr><tr><td><code>F.sdpa()</code> (CuDNN)</td><td>203.61</td><td>97.19%</td></tr><tr><td><code>flash-attn</code></td><td>190.58</td><td>90.97%</td></tr><tr><td>v1 (basic)</td><td>142.87</td><td>68.20%</td></tr><tr><td>v2 (shared memory swizzling)</td><td>181.11</td><td>86.45%</td></tr><tr><td>v3 (2-stage pipelining)</td><td>189.84</td><td>90.62%</td></tr><tr><td>v4 (<code>ldmatrix.x4</code> for K and V)</td><td>194.33</td><td>92.76%</td></tr><tr><td>v5 (better pipelining)</td><td>197.74</td><td>94.39%</td></tr></tbody></table><p>Do note that although I only use Ampere features in these implementations (sm120 supports <code>cp.async.bulk</code> i.e. TMA, but I don’t use it here), my implementations might not run performantly on earlier generations of GPUs. Due to improvements in newer hardware, you might need to use more tricks to reach Speed-of-Light on older GPUs e.g. pipeline shared memory to register memory data movements.</p><h2 id="flash-attention-algorithm">Flash Attention algorithm</h2><p>Let’s start with the reference implementation of attention.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> torch <span>import</span> Tensor
</span></span><span><span>
</span></span><span><span><span>def</span> <span>sdpa</span>(q: Tensor, k: Tensor, v: Tensor):
</span></span><span><span>    <span># q: [B, Lq, DIM]</span>
</span></span><span><span>    <span># k: [B, Lk, DIM]</span>
</span></span><span><span>    <span># v: [B, Lk, DIM]</span>
</span></span><span><span>    D <span>=</span> q<span>.</span>shape[<span>-</span><span>1</span>]
</span></span><span><span>    scale <span>=</span> D <span>**</span> <span>-</span><span>0.5</span>
</span></span><span><span>    attn <span>=</span> (q <span>@</span> k<span>.</span>transpose(<span>-</span><span>1</span>, <span>-</span><span>2</span>)) <span>*</span> scale  <span># [B, Lq, Lk]</span>
</span></span><span><span>    attn <span>=</span> attn<span>.</span>softmax(dim<span>=-</span><span>1</span>)
</span></span><span><span>    out <span>=</span> attn <span>@</span> v  <span># [B, Lq, DIM]</span>
</span></span><span><span>    <span>return</span> out
</span></span></code></pre></div><p>Technically, if the inputs are BF16, some computations should remain in FP32, especially softmax. However, for brevity, we omit them.</p><p>We are implementing the algorithm outlined in the <a href="https://arxiv.org/abs/2307.08691">Flash Attention 2 paper</a>. Each threadblock is responsible for a chunk of Q, and we will iterate along the sequence length of KV. A Python-like outline of the algorithm looks like below (S and P follow Flash Attention notation).</p><div><pre tabindex="0"><code data-lang="python"><span><span>scale <span>=</span> DIM <span>**</span> <span>-</span><span>0.5</span>
</span></span><span><span><span>for</span> b_idx <span>in</span> range(B):
</span></span><span><span>    <span>for</span> tile_Q_idx <span>in</span> range(Lq <span>//</span> BLOCK_Q):
</span></span><span><span>        <span>### start of each threadblock's kernel</span>
</span></span><span><span>        tile_O <span>=</span> torch<span>.</span>zeros(BLOCK_Q, DIM)
</span></span><span><span>        tile_Q <span>=</span> load_Q(b_idx, tile_Q_idx)  <span># [BLOCK_Q, DIM]</span>
</span></span><span><span>
</span></span><span><span>        <span>for</span> tile_KV_idx <span>in</span> range(Lk <span>//</span> BLOCK_KV):
</span></span><span><span>            <span># first MMA: S = Q @ K.T</span>
</span></span><span><span>            <span># (BLOCK_Q, DIM) x (BLOCK_KV, DIM).T -&gt; (BLOCK_Q, BLOCK_KV)</span>
</span></span><span><span>            tile_Q                               <span># (BLOCK_Q, DIM)</span>
</span></span><span><span>            tile_K <span>=</span> load_K(b_idx, tile_KV_idx)  <span># (BLOCK_KV, DIM)</span>
</span></span><span><span>            tile_S <span>=</span> tile_Q <span>@</span> tile_K<span>.</span>T           <span># (BLOCK_Q, BLOCK_KV)</span>
</span></span><span><span>            tile_S <span>=</span> tile_S <span>*</span> scale
</span></span><span><span>
</span></span><span><span>            <span># online softmax and rescale tile_O</span>
</span></span><span><span>            <span>...</span>
</span></span><span><span>
</span></span><span><span>            <span># second MMA: O = P @ V</span>
</span></span><span><span>            <span># (BLOCK_Q, BLOCK_KV) x (BLOCK_KV, DIM) -&gt; (BLOCK_Q, DIM)</span>
</span></span><span><span>            tile_P                               <span># (BLOCK_Q, BLOCK_KV)</span>
</span></span><span><span>            tile_V <span>=</span> load_V(b_idx, tile_KV_idx)  <span># (BLOCK_KV, DIM)</span>
</span></span><span><span>            tile_O <span>+=</span> tile_P <span>@</span> tile_V            <span># (BLOCK_Q, DIM)</span>
</span></span><span><span>
</span></span><span><span>        <span># normalize output and write results</span>
</span></span><span><span>        store_O(b_idx, tile_Q_idx)
</span></span><span><span>        <span>### end of each threadblock's kernel</span>
</span></span></code></pre></div><p>It’s implied <code>DIM</code> is small, so that we can hold <code>tile_Q</code> in register memory throughout the duration of the kernel. This is the reason pretty much all models nowadays use <code>head_dim=128</code>. There are exceptions of course, like <a href="https://arxiv.org/abs/2405.04434">MLA</a>, which uses <code>head_dim=576</code> for Q and K, and <code>head_dim=512</code> for V. Talking about this, I should study <a href="https://github.com/deepseek-ai/FlashMLA">FlashMLA</a> some day.</p><p>Online softmax is quite tricky to explain, so let’s delay the explanation of that part. At the high level, you just need to know that online softmax will transform <code>tile_S</code> to <code>tile_P</code>, and also rescale <code>tile_O</code>.</p><h2 id="version-1---basic-implementation">Version 1 - Basic implementation</h2><p>We will follow the typical MMA flow</p><ul><li>Load a 2D tile of data from global memory to shared memory using <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#data-movement-and-conversion-instructions-cp-async">cp.async</a>. This requires Ampere (sm80 and newer).</li><li>Load data from shared memory to register memory using <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-instructions-ldmatrix">ldmatrix</a>.</li><li>Call <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-fragment-mma-16816-float">mma.m16n8k16</a> for BF16 matrix multiplication (and accumulate).</li></ul><p>I want to focus on implementing the algorithm correctly first, hence I leave out more complicated tricks like shared memory swizzling and pipelining. This reduces the surface area for mistakes, and we will revisit them later for performance optimization.</p><h3 id="global-to-shared-memory-data-transfer">Global to Shared memory data transfer</h3><p>The following templated function does a 2D tile copy from global memory to shared memory.</p><ul><li>Shape of the 2D tile is specified via <code>HEIGHT</code> and <code>WIDTH</code>.</li><li><code>dst</code> is shared memory address, <code>src</code> is global memory address.</li><li>Global memory <code>src</code> is row-major, so <code>src_stride</code> specifies how much to move to the next row.</li><li>Shared memory <code>dst</code> is also row-major, and will be stored as a contiguous block -&gt; <code>dst_stride = WIDTH</code>.</li></ul><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>#include</span> <span>&lt;cuda_bf16.h&gt;</span><span>
</span></span></span><span><span><span></span>
</span></span><span><span><span>template</span> <span>&lt;</span><span>int</span> HEIGHT, <span>int</span> WIDTH, <span>int</span> TB_SIZE<span>&gt;</span>
</span></span><span><span>__device__ <span>inline</span>
</span></span><span><span><span>void</span> global_to_shared(<span>uint32_t</span> dst, <span>const</span> nv_bfloat16 <span>*</span>src, <span>int</span> src_stride, <span>int</span> tid) {
</span></span><span><span>  <span>constexpr</span> <span>int</span> num_elems <span>=</span> <span>16</span> <span>/</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>  <span>constexpr</span> <span>int</span> num_iters <span>=</span> HEIGHT <span>*</span> WIDTH <span>/</span> (TB_SIZE <span>*</span> num_elems);
</span></span><span><span>
</span></span><span><span>  <span>for</span> (<span>int</span> iter <span>=</span> <span>0</span>; iter <span>&lt;</span> num_iters; iter<span>++</span>) {
</span></span><span><span>    <span>const</span> <span>int</span> idx <span>=</span> (iter <span>*</span> TB_SIZE <span>+</span> tid) <span>*</span> num_elems;
</span></span><span><span>    <span>const</span> <span>int</span> row <span>=</span> idx <span>/</span> WIDTH;
</span></span><span><span>    <span>const</span> <span>int</span> col <span>=</span> idx <span>%</span> WIDTH;
</span></span><span><span>
</span></span><span><span>    <span>const</span> <span>uint32_t</span> dst_addr <span>=</span> dst <span>+</span> (row <span>*</span> WIDTH <span>+</span> col) <span>*</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>    <span>const</span> nv_bfloat16 <span>*</span>src_addr <span>=</span> src <span>+</span> (row <span>*</span> src_stride <span>+</span> col);
</span></span><span><span>    <span>asm</span> <span>volatile</span>(<span>"cp.async.cg.shared.global [%0], [%1], 16;"</span> <span>::</span> <span>"r"</span>(dst_addr), <span>"l"</span>(src_addr));
</span></span><span><span>  }
</span></span><span><span>}
</span></span></code></pre></div><figure><img src="https://gau-nernst.github.io/fa-5090/global_to_shared.svg" alt="Global to Shared data transfer"><figcaption><p>2D tile copy from Global memory to Shared memory.</p></figcaption></figure><p>We will use inline assembly to write <code>cp.async.cg.shared.global</code>. This PTX does 16-byte transfer, or 8 BF16 elements (<code>num_elems = 16 / sizeof(nv_bfloat16)</code>), for each CUDA thread. To ensure coalesced memory access, consecutive threads will be responsible for consecutive groups of 8xBF16.</p><figure><img src="https://gau-nernst.github.io/fa-5090/coalesced.svg" alt="Coalesced memory access"><figcaption><p>Consecutive threads are responsible for consecutive groups of 8xBF16.</p></figcaption></figure><p>Note:</p><ul><li>The loop <code>for (int iter = 0; iter &lt; num_iters; iter++)</code> is written this way so that the compiler (<code>nvcc</code>) can fully unroll the loop. <code>num_iters</code> is known at compile time (guaranteed by <code>constexpr</code>). If we mix <code>tid</code> in the loop, which is a “dynamic” variable to the compiler, the loop can’t be unrolled, even when we know certain constraints about the variable i.e. <code>tid &lt; TB_SIZE</code>.</li><li>Data type of shared memory pointer <code>dst</code> is <code>uint32_t</code>. This is intentional. Pretty much all PTX instructions expect shared memory addresses to be in <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#state-spaces">shared state space</a>. We can convert C++ pointers, which are generic addresses, to shared state space addresses with <code>static_cast&lt;uint32_t&gt;(__cvta_generic_to_shared(ptr))</code>. This is done outside of <code>global_to_shared()</code>.</li></ul><p>To finish using <code>cp.async</code>, we also need to add the following:</p><ul><li><code>cp.async.commit_group</code> (PTX): commit all previously issued <code>cp.async</code> instructions into a <strong><code>cp.async</code> group</strong>. This group will be the unit for synchronization.</li><li><code>cp.async.wait_all</code> (PTX): wait for all committed groups to finish.</li><li><code>__syncthreads()</code>: make sure all threads (in a threadblock) reach here before reading the loaded data in shared memory (because one thread may read data loaded by another thread). More importantly, this broadcasts <strong>visibility</strong> of the new data to all threads in the threadblock. Without <code>__syncthreads()</code>, the compiler is free to optimize away memory accesses.</li></ul><p>As always, refer to <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/">PTX doc</a> for more information about the instructions. Basically we issue multiple <code>cp.async</code> and wait for them to complete immediately right after. <code>commit_group</code> and <code>wait_group</code> provide a mechanism for us to implement pipelining later. But for now, just need to know we have to write it that way to use <code>cp.async</code>.</p><p>Our code snippet would look something like this.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// nv_bfloat16 *Q;
</span></span></span><span><span><span>// uint32_t Q_smem;
</span></span></span><span><span><span>// const int tid = blockIdx.x;
</span></span></span><span><span><span>// constexpr int TB_SIZE = 32 * 4;
</span></span></span><span><span><span>// constexpr int DIM = 128;
</span></span></span><span><span><span></span>
</span></span><span><span>global_to_shared<span>&lt;</span>BLOCK_Q, DIM, TB_SIZE<span>&gt;</span>(Q_smem, Q, DIM, tid);
</span></span><span><span><span>asm</span> <span>volatile</span>(<span>"cp.async.commit_group;"</span>);
</span></span><span><span><span>asm</span> <span>volatile</span>(<span>"cp.async.wait_all;"</span>);
</span></span><span><span>__syncthreads();
</span></span></code></pre></div><h3 id="shared-memory-to-register-memory-data-transfer">Shared memory to Register memory data transfer</h3><p>When doing global-&gt;shared data transfer, we think in terms of threadblock tiles and individual CUDA threads. For shared-&gt;register data transfer, since this is to service later MMA instructions, we think in terms of warp tiles/MMA tiles and warps. Following Flash Attention 2 (section 3.3), we let each warp in a threadblock handle a portion of <code>tile_Q</code>, splitting along the Q sequence length dimension. This means that different warps will index into different chunks of <code>tile_Q</code>, but they all index to the same <code>tile_K</code> and <code>tile_V</code> chunks in the KV-sequence-length loop.</p><figure><img src="https://gau-nernst.github.io/fa-5090/fa_warp_partition.svg" alt="Flash Attention warp partition"><figcaption><p>Warp partition in Flash Attention 2.</p></figcaption></figure><p>Since we are using <code>mma.m16n8k16</code> instruction, each MMA 16x8 output tile (<code>m16n8</code>) requires 16x16 A tile (<code>m16k16</code>) and 8x16 B tile (<code>n8k16</code>). <code>ldmatrix</code> can load one, two, or four 8x8 tile(s) of 16-bit elements. Hence,</p><ul><li>A tile <code>m16k16</code> requires four 8x8 tiles -&gt; <code>ldmatrix.x4</code></li><li>B tile <code>n8k16</code> requires two 8x8 tiles -&gt; <code>ldmatrix.x2</code></li></ul><p>Only Q acts as A in an MMA. Both K and V act as B in their MMAs, though K will require transposed <code>ldmatrix</code> for correct layout (all tensors use row-major layout in global and shared memory).</p><p>To use <code>ldmatrix</code>, each thread supplies address of a row. Threads 0-7 select the 1st 8x8 tile, threads 8-15 select the 2nd 8x8 tile, and so on. The <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-fragment-mma-16816-float">layout of A</a> in the official PTX documentation can look confusing. But it’s easier (at least for me) to focus on the order of 8x8 tiles within an MMA tile.</p><figure><img src="https://gau-nernst.github.io/fa-5090/ldmatrix.svg" alt="ldmatrix for MMA layout"><figcaption><p>Order of <code>ldmatrix</code> tiles in <code>mma.m16n8k16</code>.</p></figcaption></figure><p>With the visualisation above, I hope the following snippet makes sense</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>constexpr</span> <span>int</span> MMA_M <span>=</span> <span>16</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> MMA_N <span>=</span> <span>8</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> MMA_K <span>=</span> <span>16</span>;
</span></span><span><span>
</span></span><span><span><span>uint32_t</span> Q_smem;
</span></span><span><span><span>uint32_t</span> Q_rmem[WARP_Q <span>/</span> MMA_M][DIM <span>/</span> MMA_K][<span>4</span>];
</span></span><span><span>
</span></span><span><span><span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>)
</span></span><span><span>  <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_K; mma_id_d<span>++</span>) {
</span></span><span><span>    <span>const</span> <span>int</span> row <span>=</span> (warp_id <span>*</span> WARP_Q) <span>+</span> (mma_id_q <span>*</span> MMA_M) <span>+</span> (lane_id <span>%</span> <span>16</span>);
</span></span><span><span>    <span>const</span> <span>int</span> col <span>=</span> (mma_id_d <span>*</span> MMA_K) <span>+</span> (lane_id <span>/</span> <span>16</span> <span>*</span> <span>8</span>);
</span></span><span><span>    <span>const</span> <span>uint32_t</span> addr <span>=</span> Q_smem <span>+</span> (row <span>*</span> DIM <span>+</span> col) <span>*</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>    ldmatrix_x4(Q_rmem[mma_id_q][mma_id_d], addr);
</span></span><span><span>  }
</span></span></code></pre></div><ul><li>The two nested loops tile <code>[MMA_M, MMA_K]</code> (i.e. <code>[16, 16]</code>) over <code>[WARP_Q, DIM]</code> in shared memory.</li><li><code>(warp_id * WARP_Q)</code> selects the warp tile. We don’t need this for K and V.</li><li><code>(mma_id_q * MMA_M)</code> in <code>row</code> and <code>(mma_id_d * MMA_K)</code> in <code>col</code> selects the MMA tile.</li><li><code>(lane_id % 16)</code> in <code>row</code> and <code>(lane_id / 16 * 8)</code> in <code>col</code> select the correct row address for each thread, following the required Multiplicand A layout (see the figure above).</li></ul><p><code>ldmatrix_x4()</code> is a small wrapper around <code>ldmatrix.sync.aligned.m8n8.x4.b16</code> PTX for convenience. You can refer to <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/common.h">common.h</a> for more details.</p><p>K and V can be loaded from shared to register memory similarly. One thing to note is about the row-major / column-major layout when using <code>ldmatrix</code>. Regardless of whether <code>.trans</code> modifier is used, each thread still provides the row address of each row in 8x8 tiles. <code>.trans</code> only changes the <strong>register layout</strong> of <code>ldmatrix</code> results.</p><figure><img src="https://gau-nernst.github.io/fa-5090/ldmatrix_kv.svg" alt="ldmatrix for K and V"><figcaption><p>Use transposed version of <code>ldmatrix</code> for V.</p></figcaption></figure><p>One trick to know whether to use the transposed version of <code>ldmatrix</code> is to look at the K-dim or the reduction dimension. The 1st MMA’s K-dim is along <code>DIM</code> dimension, while the 2nd MMA’s K-dim is along the <code>BLOCK_KV</code> dimension.</p><h3 id="draft-version">Draft version</h3><p>We have the high-level tile-based design, and know how to load the data for MMA. Calling MMA is simple - just drop <code>mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32</code> PTX in our code. Our draft version looks like this.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>constexpr</span> <span>int</span> BLOCK_Q <span>=</span> <span>128</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> BLOCK_KV <span>=</span> <span>64</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> DIM <span>=</span> <span>128</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> NUM_WARPS <span>=</span> <span>4</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> TB_SIZE <span>=</span> NUM_WARPS <span>*</span> <span>32</span>;
</span></span><span><span>
</span></span><span><span><span>// mma.m16n8k16
</span></span></span><span><span><span></span><span>constexpr</span> <span>int</span> MMA_M <span>=</span> <span>16</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> MMA_N <span>=</span> <span>8</span>;
</span></span><span><span><span>constexpr</span> <span>int</span> MMA_K <span>=</span> <span>16</span>;
</span></span><span><span>
</span></span><span><span>__global__
</span></span><span><span><span>void</span> <span>attention_v1_kernel</span>(
</span></span><span><span>  <span>const</span> nv_bfloat16 <span>*</span>Q,  <span>// [bs, len_q, DIM]
</span></span></span><span><span><span></span>  <span>const</span> nv_bfloat16 <span>*</span>K,  <span>// [bs, len_kv, DIM]
</span></span></span><span><span><span></span>  <span>const</span> nv_bfloat16 <span>*</span>V,  <span>// [bs, len_kv, DIM]
</span></span></span><span><span><span></span>  nv_bfloat16 <span>*</span>O,        <span>// [bs, len_q, DIM]
</span></span></span><span><span><span></span>  <span>int</span> bs,
</span></span><span><span>  <span>int</span> len_q,
</span></span><span><span>  <span>int</span> len_kv) {
</span></span><span><span>
</span></span><span><span>  <span>// basic setup
</span></span></span><span><span><span></span>  <span>const</span> <span>int</span> tid <span>=</span> threadIdx.x;
</span></span><span><span>  <span>const</span> <span>int</span> warp_id <span>=</span> tid <span>/</span> <span>32</span>;
</span></span><span><span>  <span>const</span> <span>int</span> lane_id <span>=</span> tid <span>%</span> <span>32</span>;
</span></span><span><span>
</span></span><span><span>  <span>// increment Q, K, V, O based on blockIdx.x
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// set up shared memory
</span></span></span><span><span><span></span>  <span>// Q_smem is overlapped with (K_smem + V_smem), since we only use Q_smem once
</span></span></span><span><span><span></span>  <span>extern</span> __shared__ <span>uint8_t</span> smem[];
</span></span><span><span>  <span>const</span> <span>uint32_t</span> Q_smem <span>=</span> __cvta_generic_to_shared(smem);
</span></span><span><span>  <span>const</span> <span>uint32_t</span> K_smem <span>=</span> Q_smem;
</span></span><span><span>  <span>const</span> <span>uint32_t</span> V_smem <span>=</span> K_smem <span>+</span> BLOCK_KV <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>
</span></span><span><span>  <span>// FA2: shard BLOCK_Q among warps
</span></span></span><span><span><span></span>  <span>constexpr</span> <span>int</span> WARP_Q <span>=</span> BLOCK_Q <span>/</span> NUM_WARPS;
</span></span><span><span>
</span></span><span><span>  <span>// set up register memory
</span></span></span><span><span><span></span>  <span>uint32_t</span> Q_rmem[WARP_Q <span>/</span> MMA_M][DIM <span>/</span> MMA_K][<span>4</span>];       <span>// act as A in MMA
</span></span></span><span><span><span></span>  <span>uint32_t</span> K_rmem[BLOCK_KV <span>/</span> MMA_N][DIM <span>/</span> MMA_K][<span>2</span>];     <span>// act as B in MMA
</span></span></span><span><span><span></span>  <span>uint32_t</span> P_rmem[WARP_Q <span>/</span> MMA_M][BLOCK_KV <span>/</span> MMA_K][<span>4</span>];  <span>// act as A in MMA
</span></span></span><span><span><span></span>  <span>uint32_t</span> V_rmem[BLOCK_KV <span>/</span> MMA_K][DIM <span>/</span> MMA_N][<span>2</span>];     <span>// act as B in MMA
</span></span></span><span><span><span></span>  <span>float</span> O_rmem[WARP_Q <span>/</span> MMA_M][DIM <span>/</span> MMA_N][<span>4</span>];          <span>// act as C/D in MMA
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// Q global-&gt;shared [BLOCK_Q, DIM]
</span></span></span><span><span><span></span>  global_to_shared<span>&lt;</span>BLOCK_Q, DIM, TB_SIZE<span>&gt;</span>(Q_smem, Q, DIM, tid);
</span></span><span><span>  <span>asm</span> <span>volatile</span>(<span>"cp.async.commit_group;"</span>);
</span></span><span><span>  <span>asm</span> <span>volatile</span>(<span>"cp.async.wait_all;"</span>);
</span></span><span><span>  __syncthreads();
</span></span><span><span>
</span></span><span><span>  <span>// Q shared-&gt;register. select the correct warp tile
</span></span></span><span><span><span></span>  <span>// Q stays in registers throughout the kernel's lifetime
</span></span></span><span><span><span></span>  <span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>)
</span></span><span><span>    <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_K; mma_id_d<span>++</span>) {
</span></span><span><span>      <span>const</span> <span>int</span> row <span>=</span> warp_id <span>*</span> WARP_Q <span>+</span> mma_id_q <span>*</span> MMA_M <span>+</span> (lane_id <span>%</span> <span>16</span>);
</span></span><span><span>      <span>const</span> <span>int</span> col <span>=</span> mma_id_d <span>*</span> MMA_K <span>+</span> (lane_id <span>/</span> <span>16</span> <span>*</span> <span>8</span>);
</span></span><span><span>      <span>const</span> <span>uint32_t</span> addr <span>=</span> Q_smem <span>+</span> (row <span>*</span> DIM <span>+</span> col) <span>*</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>      ldmatrix_x4(Q_rmem[mma_id_q][mma_id_d], addr);
</span></span><span><span>    }
</span></span><span><span>  __syncthreads();
</span></span><span><span>
</span></span><span><span>  <span>// main loop
</span></span></span><span><span><span></span>  <span>const</span> <span>int</span> num_kv_iters <span>=</span> len_kv <span>/</span> BLOCK_KV;
</span></span><span><span>  <span>for</span> (<span>int</span> kv_idx <span>=</span> <span>0</span>; kv_idx <span>&lt;</span> num_kv_iters; kv_idx<span>++</span>) {
</span></span><span><span>    <span>// accumulator for the 1st MMA. reset to zeros
</span></span></span><span><span><span></span>    <span>float</span> S_rmem[WARP_Q <span>/</span> MMA_M][BLOCK_KV <span>/</span> MMA_N][<span>4</span>] <span>=</span> {};  <span>// act as C/D in MMA
</span></span></span><span><span><span></span>
</span></span><span><span>    <span>// load K global-&gt;shared-&gt;registers [BLOCK_KV, DIM]
</span></span></span><span><span><span></span>    <span>// similar to loading Q, except we use ldmatrix_x2()
</span></span></span><span><span><span></span>    ...
</span></span><span><span>
</span></span><span><span>    <span>// 1st MMA: S = Q @ K.T
</span></span></span><span><span><span></span>    <span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>)
</span></span><span><span>      <span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>)
</span></span><span><span>        <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_K; mma_id_d<span>++</span>)
</span></span><span><span>          mma_m16n8k16(Q_rmem[mma_id_q][mma_id_d],
</span></span><span><span>                       K_rmem[mma_id_kv][mma_id_d],
</span></span><span><span>                       S_rmem[mma_id_q][mma_id_kv]);
</span></span><span><span>
</span></span><span><span>    <span>// online softmax. we will touch on this later
</span></span></span><span><span><span></span>    <span>// also pack S_rmem to P_rmem for the 2nd MMA
</span></span></span><span><span><span></span>    ...
</span></span><span><span>
</span></span><span><span>    <span>// load V global-&gt;shared-&gt;registers [BLOCK_KV, DIM]
</span></span></span><span><span><span></span>    <span>// similar to loading K, except we use ldmatrix_x2_trans()
</span></span></span><span><span><span></span>    ...
</span></span><span><span>
</span></span><span><span>    <span>// 2nd MMA: O += P @ V
</span></span></span><span><span><span></span>    <span>// similar to the 1st MMA
</span></span></span><span><span><span></span>    ...
</span></span><span><span>
</span></span><span><span>    <span>// increment pointer to the next KV block
</span></span></span><span><span><span></span>    K <span>+=</span> BLOCK_KV <span>*</span> DIM;
</span></span><span><span>    V <span>+=</span> BLOCK_KV <span>*</span> DIM;
</span></span><span><span>  }
</span></span><span><span>
</span></span><span><span>  <span>// write output
</span></span></span><span><span><span></span>  <span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>)
</span></span><span><span>    <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_N; mma_id_d<span>++</span>) {
</span></span><span><span>      <span>const</span> <span>int</span> row <span>=</span> warp_id <span>*</span> WARP_Q <span>+</span> mma_id_q <span>*</span> MMA_M <span>+</span> (lane_id <span>/</span> <span>4</span>);
</span></span><span><span>      <span>const</span> <span>int</span> col <span>=</span> mma_id_d <span>*</span> MMA_N <span>+</span> (lane_id <span>%</span> <span>4</span>) <span>*</span> <span>2</span>;
</span></span><span><span>
</span></span><span><span>      <span>float</span> <span>*</span>regs <span>=</span> O_rmem[mma_id_q][mma_id_d];
</span></span><span><span>      <span>reinterpret_cast</span><span>&lt;</span>nv_bfloat162 <span>*&gt;</span>(O <span>+</span> (row <span>+</span> <span>0</span>) <span>*</span> DIM <span>+</span> col)[<span>0</span>] <span>=</span> __float22bfloat162_rn({regs[<span>0</span>], regs[<span>1</span>]});
</span></span><span><span>      <span>reinterpret_cast</span><span>&lt;</span>nv_bfloat162 <span>*&gt;</span>(O <span>+</span> (row <span>+</span> <span>8</span>) <span>*</span> DIM <span>+</span> col)[<span>0</span>] <span>=</span> __float22bfloat162_rn({regs[<span>2</span>], regs[<span>3</span>]});
</span></span><span><span>    }
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// kernel launcher
</span></span></span><span><span><span></span><span>void</span> <span>attention_v1</span>(
</span></span><span><span>  <span>const</span> nv_bfloat16 <span>*</span>Q,  <span>// [bs, len_q, DIM]
</span></span></span><span><span><span></span>  <span>const</span> nv_bfloat16 <span>*</span>K,  <span>// [bs, len_kv, DIM]
</span></span></span><span><span><span></span>  <span>const</span> nv_bfloat16 <span>*</span>V,  <span>// [bs, len_kv, DIM]
</span></span></span><span><span><span></span>  nv_bfloat16 <span>*</span>O,        <span>// [bs, len_q, DIM]
</span></span></span><span><span><span></span>  <span>int</span> bs,
</span></span><span><span>  <span>int</span> len_q,
</span></span><span><span>  <span>int</span> len_kv) {
</span></span><span><span>
</span></span><span><span>  <span>// 1 threadblock for each BLOCK_Q
</span></span></span><span><span><span></span>  <span>const</span> <span>int</span> num_blocks <span>=</span> bs <span>*</span> cdiv(len_q, BLOCK_Q);
</span></span><span><span>
</span></span><span><span>  <span>// Q overlap with K+V.
</span></span></span><span><span><span></span>  <span>const</span> <span>int</span> smem_size <span>=</span> max(BLOCK_Q, BLOCK_KV <span>*</span> <span>2</span>) <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16);
</span></span><span><span>
</span></span><span><span>  <span>// use dynamic shared memory so we can allocate more than 48kb if needed.
</span></span></span><span><span><span></span>  <span>if</span> (smem_size <span>&gt;</span> <span>48'000</span>)
</span></span><span><span>    CUDA_CHECK(cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size));
</span></span><span><span>
</span></span><span><span>  attention_v1_kernel<span>&lt;&lt;&lt;</span>num_blocks, TB_SIZE, smem_size<span>&gt;&gt;&gt;</span>(Q, K, V, O, bs, len_q, len_kv);
</span></span><span><span>  CUDA_CHECK(cudaGetLastError());
</span></span><span><span>}
</span></span></code></pre></div><p>Now, let’s tackle online softmax.</p><h3 id="online-softmax---theory">Online softmax - Theory</h3><p>For the original explanation, you can refer to <a href="https://arxiv.org/abs/1805.02867">Online normalizer calculation for softmax</a> and Flash Attention 2 paper.</p><p>We have the following mathematical definition of softmax. For each row with length $L_{kv}$</p><p>$$
p_l = \frac{\exp(s_l-m)}{\exp(s_0-m) + \exp(s_1-m) + \dots + \exp(s_{L_{kv}-1}-m)}
$$
$$
l\in[0,L_{kv})
$$
$$
m=\max(s_0,s_1,\dots,s_{L_{kv}-1})
$$</p><p>$-m$ is max subtraction to improve numerical stability ($\exp(\cdot)$ can easily explode if its input is large). Let’s bring out the denominator normalizer and write the whole row as a vector.</p><p>$$
\vec P =
\begin{bmatrix}
p_0 \\
\vdots \\
p_{L_{kv}-1}
\end{bmatrix}
= \frac{1}{\sum_{l\in[0,L_{kv})}\exp(s_l-m)}
\begin{bmatrix}
\exp(s_0-m) \\
\vdots \\
\exp(s_{L_{kv}-1}-m)
\end{bmatrix}
$$</p><p>In our 2nd matmul <code>O += P @ V</code>, each row of P (softmax output) is dot-producted with the corresponding column of V.</p><p>$$
o=\vec P \cdot \vec V = \frac{1}{\sum_{l\in[0,L_{kv})}\exp(s_l-m)} \sum_{l\in[0,L_{kv})}\exp(s_l-m) \cdot v_l
$$</p><p>The extra dot-product is a blessing in disguise - we no longer need individual elements in a row for the final result. This enables Flash Attention to compute attention in one pass. To see it more clearly, let’s consider the iterative process of adding a new element during online computation.</p><p>$$
o_{[0,L)} = \frac{1}{\sum_{l\in[0,L)}\exp(s_l-m_{[0,L)})} \sum_{l\in[0,L)}\exp(s_l-m_{[0,L)}) \cdot v_l
$$
$$
m_{[0,L)}=\max(s_0,s_1,\dots,s_{L-1})
$$</p><p>I’m abusing the notation here, but I hope I get the idea across. When we add a new element $s_{L+1}$</p><p>$$
o_{[0,L+1)} = \frac{1}{\sum_{l\in[0,L+1)}\exp(s_l-m_{[0,L+1)})} \sum_{l\in[0,L+1)}\exp(s_l-m_{[0,L+1)}) \cdot v_l
$$</p><p>Look at the normalizer (denominator)</p><p>$$
\sum_{l\in[0,L+1)}\exp(s_l-m_{[0,L+1)}) = \colorbox{red}{$\displaystyle\exp(m_{[0,L)}-m_{[0,L+1)})$}\colorbox{orange}{$\displaystyle\sum_{l\in[0,L)}\exp(s_l-m_{[0,L)})$} + \colorbox{lime}{$\displaystyle\exp(s_L-m_{[0,L+1)})$}
$$</p><p>The equation means that we only need to $\colorbox{red}{rescale}$ the $\colorbox{orange}{previous normalizer}$ before adding the $\colorbox{lime}{new term}$. The same logic can be applied for the dot product with V (unnormalized output). <strong>This is the key idea of online softmax and Flash Attention</strong>.</p><p>Define <strong>attention state</strong></p><p>$$
\begin{bmatrix}
m \\
\tilde{o} \\
\mathrm{sumexp}
\end{bmatrix}
$$</p><p>where $m$ is the max of elements seen so far, $\tilde{o}$ is the <strong>unnormalized</strong> output, and $\mathrm{sumexp}$ is the normalizer. We need $m$ to compute the rescaling factor as seen above.</p><p>You can convince yourself that updating attention state is an <strong>associative</strong> operation - it does not matter the order in which elements are used to update the attention state.</p><p>$$
\begin{aligned}
\begin{bmatrix}
m_1 \\
\tilde{o}_1 \\
\mathrm{sumexp}_1
\end{bmatrix}
\oplus \begin{bmatrix}
m_2 \\
\tilde{o}_2 \\
\mathrm{sumexp}_2
\end{bmatrix}
&amp;= \begin{bmatrix}
m_3 \\
\tilde{o}_3 \\
\mathrm{sumexp}_3
\end{bmatrix} \\
&amp;= \begin{bmatrix}
\max(m_1,m_2) \\
\exp(m_1-m_3)\tilde{o}_1+\exp(m_2-m_3)\tilde{o}_2 \\
\exp(m_1-m_3)\mathrm{sumexp}_1+\exp(m_2-m_3)\mathrm{sumexp}_2
\end{bmatrix}
\end{aligned}
$$</p><p>This associative property enables things like <a href="https://pytorch.org/blog/flash-decoding/">Flash Decoding</a>, a split-K version of attention.</p><h3 id="online-softmax---implementation">Online softmax - Implementation</h3><p>We can now fill in the gap of online softmax in our high-level Python implementation.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># attention state</span>
</span></span><span><span>m <span>=</span> torch<span>.</span>zeros(BLOCK_Q)
</span></span><span><span>tile_O <span>=</span> torch<span>.</span>zeros(BLOCK_Q, DIM)
</span></span><span><span>sumexp <span>=</span> torch<span>.</span>zeros(BLOCK_Q)
</span></span><span><span>
</span></span><span><span><span>for</span> _ <span>in</span> range(Lk <span>//</span> BLOCK_KV):
</span></span><span><span>  <span># 1st MMA</span>
</span></span><span><span>  tile_S <span>=</span> tile_Q <span>@</span> tile_K<span>.</span>T  <span># [BLOCK_Q, BLOCK_KV]</span>
</span></span><span><span>  tile_S <span>=</span> tile_S <span>*</span> scale
</span></span><span><span>
</span></span><span><span>  <span># online softmax</span>
</span></span><span><span>  tile_max <span>=</span> tile_S<span>.</span>amax(dim<span>=-</span><span>1</span>)  <span># [BLOCK_Q]</span>
</span></span><span><span>  new_m <span>=</span> torch<span>.</span>maximum(m, tile_max)
</span></span><span><span>  tile_P <span>=</span> torch<span>.</span>exp(tile_S <span>-</span> new_m<span>.</span>unsqueeze(<span>-</span><span>1</span>))
</span></span><span><span>
</span></span><span><span>  <span># rescale</span>
</span></span><span><span>  scale <span>=</span> torch<span>.</span>exp(m <span>-</span> new_m)
</span></span><span><span>  tile_O <span>*=</span> scale<span>.</span>unsqueeze(<span>-</span><span>1</span>)
</span></span><span><span>  sumexp <span>=</span> sumexp <span>*</span> scale <span>+</span> tile_P<span>.</span>sum(dim<span>=-</span><span>1</span>)
</span></span><span><span>  m <span>=</span> new_m  <span># save new max</span>
</span></span><span><span>
</span></span><span><span>  <span># 2nd MMA</span>
</span></span><span><span>  tile_O <span>+=</span> tile_P <span>@</span> tile_V  <span># [BLOCK_Q, DIM]</span>
</span></span><span><span>
</span></span><span><span><span># apply normalization</span>
</span></span><span><span>tile_O <span>/=</span> sumexp<span>.</span>unsqueeze(<span>-</span><span>1</span>)
</span></span></code></pre></div><h4 id="row-max">Row max</h4><p>When translating this to CUDA C++, the most tricky part is to wrap our head around MMA layout. Let’s start with <code>tile_S</code>.</p><figure><img src="https://docs.nvidia.com/cuda/parallel-thread-execution/_images/mma-16816-C-f16.png" alt="MMA m16n8k16 output layout"><figcaption><p>Thread and register layout of MMA m16n8k16 output. Source: <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-fragment-mma-16816-float">NVIDIA PTX doc</a>.</p></figcaption></figure><p>Softmax scale applies the same scaling for all elements, so that is trivial. Next, we need to compute row max for the current tile. Remember that we allocate the registers for <code>tile_S</code> this way.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>float</span> S_rmem[WARP_Q <span>/</span> MMA_M][BLOCK_KV <span>/</span> MMA_N][<span>4</span>];
</span></span></code></pre></div><p><code>4</code> means <code>c0,c1,c2,c3</code> in the figure above i.e. each thread holds 2 consecutive elements from 2 rows. To do reduction within a row (of an MMA output tile), we do reduction for 2 consecutive elements held by a thread, then reduction within a group of 4 threads i.e. <code>T0-T3</code>, <code>T4-T7</code>, and so on. However, the row reduction is actually within the whole <code>tile_S</code>, hence we also need to loop over <code>BLOCK_KV / MMA_N</code> of <code>S_rmem</code>. This can be combined with thread-level reduction before 4-thread reduction.</p><figure><img src="https://gau-nernst.github.io/fa-5090/row_reduction.svg" alt="Row reduction"><figcaption><p>Perform row reduction on MMA output.</p></figcaption></figure><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// initial attention state
</span></span></span><span><span><span></span><span>float</span> rowmax[WARP_Q <span>/</span> MMA_M][<span>2</span>];
</span></span><span><span><span>float</span> rowsumexp[WARP_Q <span>/</span> MMA_M][<span>2</span>] <span>=</span> {};
</span></span><span><span><span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>) {
</span></span><span><span>  rowmax[mma_id_q][<span>0</span>] <span>=</span> <span>-</span>FLT_MAX;
</span></span><span><span>  rowmax[mma_id_q][<span>1</span>] <span>=</span> <span>-</span>FLT_MAX;
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// main loop
</span></span></span><span><span><span></span><span>const</span> <span>int</span> num_kv_iters <span>=</span> len_kv <span>/</span> BLOCK_KV;
</span></span><span><span><span>for</span> (<span>int</span> kv_idx <span>=</span> <span>0</span>; kv_idx <span>&lt;</span> num_kv_iters; kv_idx<span>++</span>) {
</span></span><span><span>  <span>// tile_S = tile_Q @ tile_K.T
</span></span></span><span><span><span></span>  S_rmem[][] <span>=</span> ...
</span></span><span><span>
</span></span><span><span>  <span>// loop over rows
</span></span></span><span><span><span></span>  <span>for</span> (<span>int</span> mma_id_q <span>=</span> <span>0</span>; mma_id_q <span>&lt;</span> WARP_Q <span>/</span> MMA_M; mma_id_q<span>++</span>) {
</span></span><span><span>    <span>// apply softmax scale
</span></span></span><span><span><span></span>    <span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>)
</span></span><span><span>      <span>for</span> (<span>int</span> reg_id <span>=</span> <span>0</span>; reg_id <span>&lt;</span> <span>4</span>; reg_id<span>++</span>)
</span></span><span><span>        S_rmem[mma_id_q][mma_id_kv][reg_id] <span>*=</span> softmax_scale;
</span></span><span><span>
</span></span><span><span>    <span>// rowmax
</span></span></span><span><span><span></span>    <span>float</span> this_rowmax[<span>2</span>] <span>=</span> {<span>-</span>FLT_MAX, <span>-</span>FLT_MAX};
</span></span><span><span>    <span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>) {
</span></span><span><span>      <span>float</span> <span>*</span>regs <span>=</span> S_rmem[mma_id_q][mma_id_kv];
</span></span><span><span>      this_rowmax[<span>0</span>] <span>=</span> max(this_rowmax[<span>0</span>], max(regs[<span>0</span>], regs[<span>1</span>]));  <span>// c0 and c1
</span></span></span><span><span><span></span>      this_rowmax[<span>1</span>] <span>=</span> max(this_rowmax[<span>1</span>], max(regs[<span>2</span>], regs[<span>3</span>]));  <span>// c2 and c3
</span></span></span><span><span><span></span>    }
</span></span><span><span>
</span></span><span><span>    <span>// butterfly reduction within 4 threads
</span></span></span><span><span><span></span>    this_rowmax[<span>0</span>] <span>=</span> max(this_rowmax[<span>0</span>], __shfl_xor_sync(<span>0xFFFF'FFFF</span>, this_rowmax[<span>0</span>], <span>1</span>));
</span></span><span><span>    this_rowmax[<span>0</span>] <span>=</span> max(this_rowmax[<span>0</span>], __shfl_xor_sync(<span>0xFFFF'FFFF</span>, this_rowmax[<span>0</span>], <span>2</span>));
</span></span><span><span>    this_rowmax[<span>1</span>] <span>=</span> max(this_rowmax[<span>1</span>], __shfl_xor_sync(<span>0xFFFF'FFFF</span>, this_rowmax[<span>1</span>], <span>1</span>));
</span></span><span><span>    this_rowmax[<span>1</span>] <span>=</span> max(this_rowmax[<span>1</span>], __shfl_xor_sync(<span>0xFFFF'FFFF</span>, this_rowmax[<span>1</span>], <span>2</span>));
</span></span><span><span>  }
</span></span><span><span>
</span></span><span><span>  ...
</span></span><span><span>}
</span></span></code></pre></div><p>In a typical reduction kernel, when there are only 32 active threads left, we can use warp shuffle <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions">__shfl_down_sync()</a> to copy data from higher lanes to lower lanes, and the final result is stored in thread 0. In this case, since we need the max value to be shared among the 4 threads in a group (for max subtraction later), we can use <code>__shfl_xor_sync()</code> to avoid an additional broadcast step.</p><figure><img src="https://gau-nernst.github.io/fa-5090/butterfly_reduction.svg" alt="Butterfly reduction"><figcaption><p>Butterfly reduction within 4 threads using __shfl_xor_sync().</p></figcaption></figure><h4 id="rescaling">Rescaling</h4><p>With row max of the new tile, we can compute rescaling factor for (unnormalized) output as well as normalizer (sumexp of each row).</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// new rowmax
</span></span></span><span><span><span></span>this_rowmax[<span>0</span>] <span>=</span> max(this_rowmax[<span>0</span>], rowmax[mma_id_q][<span>0</span>]);
</span></span><span><span>this_rowmax[<span>1</span>] <span>=</span> max(this_rowmax[<span>1</span>], rowmax[mma_id_q][<span>1</span>]);
</span></span><span><span>
</span></span><span><span><span>// rescale for previous O
</span></span></span><span><span><span></span><span>float</span> rescale[<span>2</span>];
</span></span><span><span>rescale[<span>0</span>] <span>=</span> __expf(rowmax[mma_id_q][<span>0</span>] <span>-</span> this_rowmax[<span>0</span>]);
</span></span><span><span>rescale[<span>1</span>] <span>=</span> __expf(rowmax[mma_id_q][<span>1</span>] <span>-</span> this_rowmax[<span>1</span>]);
</span></span><span><span><span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_N; mma_id_d<span>++</span>) {
</span></span><span><span>  O_rmem[mma_id_q][mma_id_d][<span>0</span>] <span>*=</span> rescale[<span>0</span>];
</span></span><span><span>  O_rmem[mma_id_q][mma_id_d][<span>1</span>] <span>*=</span> rescale[<span>0</span>];
</span></span><span><span>  O_rmem[mma_id_q][mma_id_d][<span>2</span>] <span>*=</span> rescale[<span>1</span>];
</span></span><span><span>  O_rmem[mma_id_q][mma_id_d][<span>3</span>] <span>*=</span> rescale[<span>1</span>];
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// save new rowmax
</span></span></span><span><span><span></span>rowmax[mma_id_q][<span>0</span>] <span>=</span> this_rowmax[<span>0</span>];
</span></span><span><span>rowmax[mma_id_q][<span>1</span>] <span>=</span> this_rowmax[<span>1</span>];
</span></span></code></pre></div><p>We don’t rescale <code>rowsumexp</code> here because we want to fuse it with addition of the new sumexp term later i.e. FMA - fused multiply add. We can’t fuse multiplication with MMA, hence we need to do a separate multiplication for <code>O_rmem</code>.</p><h4 id="pack-tile_s-to-tile_p-and-compute-row-sum-exp">Pack <code>tile_S</code> to <code>tile_P</code> (and compute row sum exp)</h4><p>For the next part, we will loop over the row dimension again (<code>BLOCK_KV / MMA_N</code>), to compute and pack <code>tile_P</code> from <code>tile_S</code>, as well as doing reduction for sumexp. Recall that we declare registers for <code>S</code> and <code>P</code> as follows.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>float</span> S_rmem[WARP_Q <span>/</span> MMA_M][BLOCK_KV <span>/</span> MMA_N][<span>4</span>]      <span>// m16n8
</span></span></span><span><span><span></span><span>uint32_t</span> P_rmem[WARP_Q <span>/</span> MMA_M][BLOCK_KV <span>/</span> MMA_K][<span>4</span>];  <span>// m16k16
</span></span></span></code></pre></div><p>Look up the thread/register layout for MMA multiplicand A and output C/D again in PTX doc. Luckily, the layouts are exactly the same - within an 8x8 tile, the arrangement of elements is identical.</p><figure><img src="https://gau-nernst.github.io/fa-5090/m16n8_to_m16k16.svg" alt="Register layout of MMA m16n8k16"><figcaption><p>The left half of multiplicand A has the same layout as accumulator. Source: <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-fragment-mma-16816-float">NVIDIA PTX doc</a>.</p></figcaption></figure><p>It means that for all threads, every 2 floats in <code>S_rmem</code> can be packed as BF16x2 in a single 32-bit register of <code>P_rmem</code>, exactly how <code>mma.m16n8k16</code> expects for the 2nd MMA. There are no data movements across threads. Note that this is not always true: if we use INT8 or FP8 MMA for the 1st and/or 2nd MMA, we would need to permute data across threads to pack <code>tile_S</code> to <code>tile_P</code>.</p><p>Our code for the last part of online softmax is below.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// rowsumexp
</span></span></span><span><span><span></span><span>float</span> this_rowsumexp[<span>2</span>] <span>=</span> {};
</span></span><span><span><span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>) {
</span></span><span><span>  <span>float</span> <span>*</span>regs <span>=</span> S_rmem[mma_id_q][mma_id_kv];
</span></span><span><span>  regs[<span>0</span>] <span>=</span> __expf(regs[<span>0</span>] <span>-</span> rowmax[mma_id_q][<span>0</span>]);  <span>// c0
</span></span></span><span><span><span></span>  regs[<span>1</span>] <span>=</span> __expf(regs[<span>1</span>] <span>-</span> rowmax[mma_id_q][<span>0</span>]);  <span>// c1
</span></span></span><span><span><span></span>  regs[<span>2</span>] <span>=</span> __expf(regs[<span>2</span>] <span>-</span> rowmax[mma_id_q][<span>1</span>]);  <span>// c2
</span></span></span><span><span><span></span>  regs[<span>3</span>] <span>=</span> __expf(regs[<span>3</span>] <span>-</span> rowmax[mma_id_q][<span>1</span>]);  <span>// c3
</span></span></span><span><span><span></span>
</span></span><span><span>  this_rowsumexp[<span>0</span>] <span>+=</span> regs[<span>0</span>] <span>+</span> regs[<span>1</span>];
</span></span><span><span>  this_rowsumexp[<span>1</span>] <span>+=</span> regs[<span>2</span>] <span>+</span> regs[<span>3</span>];
</span></span><span><span>
</span></span><span><span>  <span>// pack to P registers for next MMA
</span></span></span><span><span><span></span>  <span>// we need to change from m16n8 to m16k16
</span></span></span><span><span><span></span>  <span>// each iteration of this loop packs half of m16k16
</span></span></span><span><span><span></span>  nv_bfloat162 <span>*</span>this_P_rmem <span>=</span> <span>reinterpret_cast</span><span>&lt;</span>nv_bfloat162 <span>*&gt;</span>(P_rmem[mma_id_q][mma_id_kv <span>/</span> <span>2</span>]);
</span></span><span><span>  this_P_rmem[(mma_id_kv <span>%</span> <span>2</span>) <span>*</span> <span>2</span>]     <span>=</span> __float22bfloat162_rn({regs[<span>0</span>], regs[<span>1</span>]});  <span>// top row
</span></span></span><span><span><span></span>  this_P_rmem[(mma_id_kv <span>%</span> <span>2</span>) <span>*</span> <span>2</span> <span>+</span> <span>1</span>] <span>=</span> __float22bfloat162_rn({regs[<span>2</span>], regs[<span>3</span>]});  <span>// bottom row
</span></span></span><span><span><span></span>}
</span></span><span><span>
</span></span><span><span><span>// butterfly reduction on this_rowsumexp[2]
</span></span></span><span><span><span></span>...
</span></span><span><span>
</span></span><span><span><span>// accumulate to total rowsumexp using FMA
</span></span></span><span><span><span></span>rowsumexp[mma_id_q][<span>0</span>] <span>=</span> rowsumexp[mma_id_q][<span>0</span>] <span>*</span> rescale[<span>0</span>] <span>+</span> this_rowsumexp[<span>0</span>];
</span></span><span><span>rowsumexp[mma_id_q][<span>1</span>] <span>=</span> rowsumexp[mma_id_q][<span>1</span>] <span>*</span> rescale[<span>1</span>] <span>+</span> this_rowsumexp[<span>1</span>];
</span></span></code></pre></div><p>After this is the 2nd MMA: load V, then compute <code>tile_O += tile_P @ tile_V</code>. This completes our 1st version of Flash Attention. Actually we also need to normalize the output before writing <code>O_rmem</code> to global memory, but the logic for that should be pretty straightforward.</p><p>You can find the full code for Version 1 at <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention_v1.cu">attention_v1.cu</a>.</p><h3 id="benchmark-setup">Benchmark setup</h3><p>Wow, that’s plentiful for the 1st version. Indeed, I spent the most time on version 1 trying to implement Flash Attention correctly. Took me 2 days to realize <a href="https://github.com/gau-nernst/learn-cuda/commit/8fdb3e6a">__shfl_xor_sync()’s mask should be 2 (0b10) instead of 0x10 for butterfly reduction</a>.</p><p>Anyway, now we need a script for correctness check as well as speed benchmark. I prefer to do these things in Python Pytorch since it’s easy to do, as well as making it simple to compare against other attention kernels with PyTorch bindings. To achieve this, I create:</p><ol><li><a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention.cpp">attention.cpp</a>: provides PyTorch bindings for my attention kernels.</li><li><a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/main.py">main.py</a>: correctness check and speed benchmark.</li></ol><p>For correctness check, I compare against <code>F.sdpa()</code>, which should dispatch Flash Attention 2 by default (at least on my GPU and current PyTorch version). I also purposely add a small bias to the random inputs, which are sampled from the standard normal distribution, so that the output has a positive mean. This is to avoid large relative error caused by zero mean.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>generate_input</span>(<span>*</span>shape):
</span></span><span><span>    <span>return</span> torch<span>.</span>randn(shape)<span>.</span>add(<span>0.5</span>)<span>.</span>bfloat16()<span>.</span>cuda()
</span></span></code></pre></div><p>For speed benchmarks, it’s generally a good idea to compare against (1) theoretical limit of the hardware i.e. Speed-of-Light, and (2) known good implementations. I’m more interested in the compute-bound regime of attention, hence I will be using FLOPS (floating point operations per second, with a capital S) as the metric for comparison.</p><p>To compute FLOPS of a given kernel, we count the number of required floating point operations (FLOPs, with a small s), then divide by the latency. Just counting FLOPs from the MMAs should be good enough, which turns out to be <code>4 * bsize * num_heads * len_q * len_kv * head_dim</code>.</p><p>The “known good implementations” are FA2 and CuDNN backends of <code>F.sdpa()</code>, as well as FA2 from <a href="https://github.com/Dao-AILab/flash-attention">flash-attn</a> package. For my kernel, I did do some tuning of <code>BLOCK_Q</code> and <code>BLOCK_KV</code>, and obtained the following results.</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td><code>F.sdpa()</code> (Flash Attention)</td><td>186.73</td><td>89.13%</td></tr><tr><td><code>F.sdpa()</code> (CuDNN)</td><td>203.61</td><td>97.19%</td></tr><tr><td><code>flash-attn</code></td><td>190.58</td><td>90.97%</td></tr><tr><td>v1 (basic)</td><td>142.87</td><td>68.20%</td></tr></tbody></table><p>It doesn’t look too bad for the first version, but we still have some headroom to go. That’s fine, because we still have a few tricks up our sleeves for the next versions. In fact, the tricks are exactly the same as the ones used in optimizing a matmul kernel.</p><h4 id="profiling">Profiling</h4><p>Before moving to the next version, I want to talk about profiling tools. I think it’s always a good idea to use profiling as the guide for optimization. Previously I only knew how to use <a href="https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html">ncu</a> at a very basic level. Seeing so many people using <a href="https://developer.nvidia.com/nsight-compute">Nsight Compute</a> with cool diagrams, I decided to learn how to use it, and it was actually quite easy to use.</p><p>Nsight Compute can run on macOS with SSH access to another machine with NVIDIA GPU, which is exactly the setup I’m using right now (yes, I write code exclusively on my Macbook). If you are unfamiliar with Nsight Compute, I recommend watching a tutorial or two to get acquainted with it.</p><p>To enable source inspection feature, remember to pass <code>-lineinfo</code> to NVCC (see <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/main.py#L22">here</a>), and enable “Import Source” in Nsight Compute.</p><p>Let’s do a profiling with Nsight Compute, and look at <strong>Warp State Statistics</strong> section.</p><figure><img src="https://gau-nernst.github.io/fa-5090/v1_warp_state_stats.png" alt="Warp state statistics of v1"><figcaption><p>Warp state statistics of kernel v1.</p></figcaption></figure><p><strong>Stall Math Pipe Throttle</strong> being the highest is good - it means warps are busy with math operations i.e. Tensor Cores. The second highest is <strong>Stall Short Scoreboard</strong>. This typically means waiting for accesses to and from shared memory. You can check <a href="https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html">Nsight Compute doc</a> and search for <code>stalled_short_scoreboard</code>.</p><p>We can double confirm this by looking at <strong>Memory Workload Analysis</strong>, which reveals several problems.</p><figure><img src="https://gau-nernst.github.io/fa-5090/v1_memory_analysis.png" alt="Memory analsysis of v1"><figcaption><p>Memory analysis of kernel v1.</p></figcaption></figure><ul><li><strong>L1TEX Global Store Access Pattern</strong> comes from storing the output, since it is the only global write we have. This is not important since the runtime of looping over the sequence length of KV should dominate when <code>len_kv</code> is large.</li><li><strong>L1TEX Local Load/Store Access Pattern</strong> is due to register spilling. Since it’s register spilling, only spilling and reloading 1 element at a time is normal. Reducing <code>BLOCK_Q</code> (so that we use fewer registers to hold accumulators) would resolve this issue, but my manual tuning showed that some spilling was actually faster.</li><li><strong>Shared Load Bank Conflicts</strong> is exactly what we are looking for - bank conflicts that result in “Stall Short Scoreboard”.</li></ul><p>NVIDIA GPU’s shared memory is backed by 32 memory banks. Consecutive 4-byte memory addresses are assigned to consecutive memory banks. This poses a problem when we load data from shared to register memory with <code>ldmatrix</code>. Although it’s not explitcitly stated in any documentations, <code>ldmatrix.x2</code> and <code>ldmatrix.x4</code> operate per 8x8 tile at a time. This is good, since it makes our analysis simpler: we only need to consider the case of loading a 8x8 tile.</p><p>Consider a 2D tile of shape 8x64, BF16 dtype, in shared memory.</p><figure><img src="https://gau-nernst.github.io/fa-5090/bank_conflicts.svg" alt="Bank conflicts"><figcaption><p>Memory bank distribution for a 8x64 BF16 tile in shared memory.</p></figcaption></figure><p>From the figure above, when we load the 8x8 <code>ldmatrix</code> tile, the same 4 banks 0-3 service all 32 threads, resulting in 8-way bank conflict. I’m not sure why Nsight Compute reports 16-way bank conflict as shown above. I tried looking up <a href="https://alexarmbr.github.io/2024/08/10/How-To-Write-A-Fast-Matrix-Multiplication-From-Scratch-With-Tensor-Cores.html">matmul blogposts with swizzling</a> and <a href="https://forums.developer.nvidia.com/t/ncu-detects-bank-conflicts-in-matrix-transposition-after-padding/239100/6">NVIDIA forum threads</a>, and found another way to check for bank conflicts was to go to the <strong>Source</strong> tab of Nsight Compute and check for <strong>L1 Wavefronts Shared</strong> and <strong>L1 Wavefronts Shared Ideal</strong> (I had to enable these two columns manually since they were not displayed by default for me).</p><figure><img src="https://gau-nernst.github.io/fa-5090/ldmatrix_bank_conflicts.png" alt="Bank conflicts in ldmatrix"><figcaption><p>Actual and Ideal L1 Wavefronts Shared of <code>ldmatrix</code> in kernel v1.</p></figcaption></figure><p>The ratio of <strong>Actual / Ideal</strong> is 8, matching our hypothesis of 8-way bank conflicts. I’m still not sure why there is a discrepancy between this value and the one in <strong>Details</strong> tab.</p><p>Anyway, there are 2 standard solutions to this problem</p><ol><li><strong>Pad shared memory</strong>. Due to <code>ldmatrix</code>’s alignment requirement, we can only pad the width with 16 bytes, equivalent to 4 banks. This means that when we go to the next row, the memory banks are shifted by 4, avoiding bank conflicts. In many cases, this is good enough. However, it’s generally quite wasteful as we are not utilising the padded storage.</li><li><strong>Swizzle shared memory address</strong>. This is black magic: you XOR the shared memory address with some magic numbers, then suddenly bank conflicts disappear!</li></ol><p>Let’s elaborate on the 2nd approach. I’m not smart enough to invent this trick, but at least I hope I can give some pointers on why it makes sense. We use XOR since this operation permutes the data nicely - there is a one-to-one mapping between input and output, given a fixed 2nd input. We get bank conflicts because when we move to the next row, we are hitting the same memory banks again -&gt; we can use this row index to permute the addresses.</p><p>In particular, if we look at the raw row addresses:</p><ul><li><strong>Bits 0-3</strong> are always zeros due to 16-byte alignment constraint.</li><li><strong>Bits 2-6</strong> determine bank index. We only need to care about bits 4-6 since the lower bits are always zeros (due to alignment).</li><li>Row stride determines which bits are incremented when we move to the next row (this is by definition). If our 2D tile’s width is 64 BF16 elements, row stride is 128 bytes. Going to the next row will increment bit 7, leaving <strong>bits 0-6 unchanged</strong> (but we don’t care about bits 0-3).</li><li>Thus, we can XOR <strong>bits 4-6</strong> of row address with <strong>bits 0-2</strong> of row index, which is guaranteed to change for every row.</li></ul><p>If the tile width is different, e.g. 32 BF16, we can go through the same reasoning. Also notice that row index is encoded within the row address, thus we only need the row address and row stride to do swizzling.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// NOTE: stride in bytes
</span></span></span><span><span><span></span><span>template</span> <span>&lt;</span><span>int</span> STRIDE<span>&gt;</span>
</span></span><span><span>__device__
</span></span><span><span><span>uint32_t</span> swizzle(<span>uint32_t</span> index) {
</span></span><span><span>  <span>// no need swizzling
</span></span></span><span><span><span></span>  <span>if</span> <span>constexpr</span> (STRIDE <span>==</span> <span>16</span>)
</span></span><span><span>    <span>return</span> index;
</span></span><span><span>
</span></span><span><span>  <span>uint32_t</span> row_idx <span>=</span> (index <span>/</span> STRIDE) <span>%</span> <span>8</span>;
</span></span><span><span>  <span>uint32_t</span> bits_to_xor <span>=</span> row_idx <span>/</span> max(<span>64</span> <span>/</span> STRIDE, <span>1</span>);
</span></span><span><span>  <span>return</span> index <span>^</span> (bits_to_xor <span>&lt;&lt;</span> <span>4</span>);
</span></span><span><span>}
</span></span></code></pre></div><p>To enable this swizzling, we need to add it to <code>cp.async</code> (write to shared memory) and <code>ldmatrix</code> (read from shared memory) calls.</p><div><pre tabindex="0"><code data-lang="diff"><span><span>// for cp.async
</span></span><span><span><span>- const uint32_t dst_addr = dst + (row * WIDTH + col) * sizeof(nv_bfloat16);
</span></span></span><span><span><span></span><span>+ const uint32_t dst_addr = swizzle&lt;WIDTH * sizeof(nv_bfloat16)&gt;(dst + (row * WIDTH + col) * sizeof(nv_bfloat16));
</span></span></span><span><span><span></span>asm volatile("cp.async.cg.shared.global [%0], [%1], 16;" :: "r"(dst_addr), "l"(src_addr));
</span></span><span><span>
</span></span><span><span>// for ldmatrix
</span></span><span><span><span>- ldmatrix_x2(K_rmem[mma_id_kv][mma_id_d], addr);
</span></span></span><span><span><span></span><span>+ ldmatrix_x2(K_rmem[mma_id_kv][mma_id_d], swizzle&lt;DIM * sizeof(nv_bfloat16)&gt;(addr));
</span></span></span></code></pre></div><p>Since this is a standard optimization in matmul kernels, I also added a small optimization for <code>ldmatrix</code>. I pre-compute row addresses and swizzling outside of the main loop, so that there is less work in the hot loop. When we iterate over MMA tiles within a warp tile, we need to increment the address. However, swizzling is a XOR operation, and we cannot simply exchange XOR with addition i.e. <code>(a + b) ^ c != (a ^ c) + b</code>. Notice that if there is some alignment in the base address <code>a</code>, addition becomes XOR! i.e. <code>100 + 001 == 100 ^ 001</code>. Hence, when incrementing the input address of <code>ldmatrix</code>, we XOR it with column offset, instead of doing addition. Row offset will affect bits higher than the swizzled bits, so we can keep addition for it.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// K shared-&gt;registers
</span></span></span><span><span><span></span><span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>)
</span></span><span><span>  <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_K; mma_id_d<span>++</span>) {
</span></span><span><span>    <span>// swizzle(addr + offset) = swizzle(addr) XOR offset
</span></span></span><span><span><span></span>    <span>uint32_t</span> addr <span>=</span> K_smem_thread;
</span></span><span><span>    addr <span>+=</span> mma_id_kv <span>*</span> MMA_N <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16);  <span>// row
</span></span></span><span><span><span></span>    addr <span>^=</span> mma_id_d <span>*</span> MMA_K <span>*</span> <span>sizeof</span>(nv_bfloat16);  <span>// col
</span></span></span><span><span><span></span>    ldmatrix_x2(K_rmem[mma_id_kv][mma_id_d], addr);
</span></span><span><span>  }
</span></span></code></pre></div><p>Version 2: <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention_v2.cu">attention_v2.cu</a>.</p><p>We can verify that there are no more bank conflicts with Nsight Compute. Benchmark results show an impressive uplift (I always re-tune <code>BLOCK_Q</code> and <code>BLOCK_KV</code> for new versions of the kernel).</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td>v1 (basic)</td><td>142.87</td><td>68.20%</td></tr><tr><td>v2 (shared memory swizzling)</td><td>181.11</td><td>86.45%</td></tr></tbody></table><h2 id="version-3---2-stage-pipelining">Version 3 - 2-stage pipelining</h2><figure><img src="https://gau-nernst.github.io/fa-5090/v2_warp_state_stats.png" alt="Warp state statistics of v2"><figcaption><p>Warp state statistics of kernel v2.</p></figcaption></figure><p><strong>Stall Short Scoreboard</strong> is no longer an issue, since we have handled it with swizzling. Now the issues are:</p><ul><li><strong>Stall Wait</strong> (<code>stalled_wait</code> in Nsight Compute doc): “waiting on a fixed latency execution dependency”, doesn’t seem to be a big issue.</li><li><strong>Stall Long Scoreboard</strong> (<code>stalled_long_scoreboard</code> in Nsight Compute doc): usually means waiting for global memory accesses.</li></ul><p>Up until now, we haven’t overlapped global memory operations with compute operations (MMA). This means the Tensor Cores are idle while waiting for global-&gt;shared transfer to complete. This seems to be the right time to introduce <strong>pipelining</strong> or <strong>double-buffering</strong>: allocate more shared memory than needed so that we can prefetch data for the next iteration while working on the current iteration.</p><ul><li>Technically we can also pipeline shared-&gt;register data transfer. This is in fact mentioned in <a href="https://github.com/NVIDIA/cutlass/blob/v4.1.0/media/docs/cpp/efficient_gemm.md">Efficient GEMM doc</a> of CUTLASS. However, I could never implement it successfully on my 5090. Inspecting the generated SASS of my current code, I see that there is interleaving between <code>LDSM</code> (<code>ldmatrix</code> in PTX) and <code>HMMA</code> (half-precision <code>mma</code> in PTX), probably done by the compiler to achieve similar memory-compute overlapping effect.</li></ul><p>Let’s discuss the more general implementation of <strong>N-stage pipelining</strong>. This <a href="https://developer.nvidia.com/blog/controlling-data-movement-to-boost-performance-on-ampere-architecture/">NVIDIA blogpost</a> gives a pretty good explanation of the idea, but generally I don’t really like using CUDA C++ API (and considering that CUTLASS also doesn’t, I think it’s more fun to use PTX directly). N-stage means there are N ongoing stages at any point in time. This will be the <strong>invariance</strong> we want to keep throughout the inner loop.</p><ul><li>This is the same concept of <code>num_stages</code> mentioned in <a href="https://triton-lang.org/main/python-api/generated/triton.Config.html">triton.Config</a> for autotuning.</li><li>Double buffering is a special case of N=2.</li></ul><div><pre tabindex="0"><code data-lang="python"><span><span>num_stages <span>=</span> <span>4</span>
</span></span><span><span>
</span></span><span><span><span># set up num_stages buffers</span>
</span></span><span><span>tile_K_buffers <span>=</span> torch<span>.</span>empty(num_stages, BLOCK_KV, DIM)
</span></span><span><span>tile_V_buffers <span>=</span> torch<span>.</span>empty(num_stages, BLOCK_KV, DIM)
</span></span><span><span>
</span></span><span><span><span># initiate with (num_stages-1) prefetches</span>
</span></span><span><span><span># this is async: the code continues before data loading finishes.</span>
</span></span><span><span><span>for</span> stage_idx <span>in</span> range(num_stages<span>-</span><span>1</span>):
</span></span><span><span>    tile_K_buffers[stage_idx] <span>=</span> load_K(stage_idx)
</span></span><span><span>    tile_V_buffers[stage_idx] <span>=</span> load_V(stage_idx)
</span></span><span><span>
</span></span><span><span><span>for</span> tile_KV_idx <span>in</span> range(Lk <span>//</span> BLOCK_KV):
</span></span><span><span>    <span># prefetch tile (num_stages-1) ahead</span>
</span></span><span><span>    <span># now we have num_stages global-&gt;shared inflight.</span>
</span></span><span><span>    <span># in practice, we need to guard against out of bounds memory access.</span>
</span></span><span><span>    prefetch_idx <span>=</span> tile_KV_idx <span>+</span> num_stages <span>-</span> <span>1</span>
</span></span><span><span>    tile_K_buffers[prefetch_idx <span>%</span> num_stages] <span>=</span> load_K(prefetch_idx)
</span></span><span><span>    tile_V_buffers[prefetch_idx <span>%</span> num_stages] <span>=</span> load_V(prefetch_idx)
</span></span><span><span>
</span></span><span><span>    <span># select the current tile</span>
</span></span><span><span>    <span># we need a synchronization mechanism to make sure data loading</span>
</span></span><span><span>    <span># for this tile has finished.</span>
</span></span><span><span>    <span># this "consumes" the oldest global-&gt;shared inflight, and</span>
</span></span><span><span>    <span># replaces it with a compute stage.</span>
</span></span><span><span>    tile_K <span>=</span> tile_K_buffers[tile_KV_idx <span>%</span> num_stages]
</span></span><span><span>    tile_V <span>=</span> tile_V_buffers[tile_KV_idx <span>%</span> num_stages]
</span></span><span><span>
</span></span><span><span>    <span># compute attention as normal</span>
</span></span><span><span>    <span>...</span>
</span></span></code></pre></div><p>NVIDIA engineers/architects have graced us with <code>cp.async.commit_group</code> and <code>cp.async.wait_group</code> to implement this elegantly.</p><ul><li><code>cp.async.commit_group</code>: one <code>cp.async</code> group maps naturally to one prefetch stage in the pipeline.</li><li><code>cp.async.wait_group N</code>: means wait until there are at most N ongoing groups left. If we do <code>cp.async.wait_group num_stages-1</code>, it means we wait until the earliest prefetch has finished (remember, we always have <code>num_stages</code> ongoing prefetches as the loop invariance).</li></ul><p>In our case of implementing attention, there are two small changes.</p><ol><li>Since we already consume a lot of shared memory for K and V, and <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications-technical-specifications-per-compute-capability">consumer GPUs typically have modest shared memory size</a> compared to their server counterparts, I decide to keep it to 2-stage pipeline, which also makes the code slightly simpler.</li><li>We can split K and V prefetches since issuing V prefetch can be delayed to after the 1st MMA. The second change requires some minor adjustments: each K and V prefetch is a separate <code>cp.async</code> group (so that we can wait for them independently).</li></ol><p>One neat coding style that I have learned from <a href="https://github.com/mingfeima">Mingfei Ma</a>, the maintainer of PyTorch CPU backend, is to use <a href="https://github.com/pytorch/pytorch/blob/v2.8.0/aten/src/ATen/native/cpu/int8mm_kernel.cpp#L63">lambda expression</a> to write prefetch code. It achieves two benefits: (1) keep the relevant code close to the call site, and (2) make it very clean to call the same block of code multiple times.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>const</span> <span>int</span> num_kv_iter <span>=</span> cdiv(len_kv, BLOCK_KV);
</span></span><span><span>
</span></span><span><span><span>auto</span> load_K <span>=</span> [<span>&amp;</span>](<span>int</span> kv_id) {
</span></span><span><span>  <span>// guard against out-of-bounds global read
</span></span></span><span><span><span></span>  <span>if</span> (kv_id <span>&lt;</span> num_kv_iter) {
</span></span><span><span>    <span>// select the shared buffer destination
</span></span></span><span><span><span></span>    <span>const</span> <span>uint32_t</span> dst <span>=</span> K_smem <span>+</span> (kv_id <span>%</span> <span>2</span>) <span>*</span> (<span>2</span> <span>*</span> BLOCK_KV <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16));
</span></span><span><span>    global_to_shared_swizzle<span>&lt;</span>BLOCK_KV, DIM, TB_SIZE<span>&gt;</span>(dst, K, DIM, tid);
</span></span><span><span>
</span></span><span><span>    <span>// load_K() will be in charge of incrementing global memory address
</span></span></span><span><span><span></span>    K <span>+=</span> BLOCK_KV <span>*</span> DIM;
</span></span><span><span>  }
</span></span><span><span>
</span></span><span><span>  <span>// we always commit a cp-async group regardless of whether there is a cp.async
</span></span></span><span><span><span></span>  <span>// to maintain loop invariance.
</span></span></span><span><span><span></span>  <span>asm</span> <span>volatile</span>(<span>"cp.async.commit_group;"</span>);
</span></span><span><span>};
</span></span><span><span><span>auto</span> load_V <span>=</span> ...;
</span></span><span><span>
</span></span><span><span><span>// prefetch K and V
</span></span></span><span><span><span></span>load_K(<span>0</span>);
</span></span><span><span>load_V(<span>0</span>);
</span></span><span><span>
</span></span><span><span><span>for</span> (<span>int</span> kv_id <span>=</span> <span>0</span>; kv_id <span>&lt;</span> num_kv_iter; kv_id<span>++</span>) {
</span></span><span><span>  <span>// prefetch K for the next iteration
</span></span></span><span><span><span></span>  <span>// now we have 3 prefetches in flight: K-V-K
</span></span></span><span><span><span></span>  load_K(kv_id <span>+</span> <span>1</span>);
</span></span><span><span>
</span></span><span><span>  <span>// wait for prefetch of current K to finish and load K shared-&gt;registers
</span></span></span><span><span><span></span>  <span>// now we have 2 prefetches in flight: V-K
</span></span></span><span><span><span></span>  <span>asm</span> <span>volatile</span>(<span>"cp.async.wait_group 2;"</span>);
</span></span><span><span>  __syncthreads();
</span></span><span><span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// 1st MMA
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// prefetch V for the next iteration
</span></span></span><span><span><span></span>  <span>// now we have 3 prefetches in flight: V-K-V
</span></span></span><span><span><span></span>  load_V(kv_id <span>+</span> <span>1</span>);
</span></span><span><span>
</span></span><span><span>  <span>// online softmax
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// wait for prefetch of current V to finish and load V shared-&gt;registers
</span></span></span><span><span><span></span>  <span>// now we have 2 prefetches in flight: K-V
</span></span></span><span><span><span></span>  <span>asm</span> <span>volatile</span>(<span>"cp.async.wait_group 2;"</span>);
</span></span><span><span>  __syncthreads();
</span></span><span><span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// 2nd MMA
</span></span></span><span><span><span></span>  ...
</span></span><span><span>}
</span></span></code></pre></div><p>I experimented a bit with where to place <code>load_K/V</code> and <code>cp.async.wait_group</code> in the loop, and have found the above placement yielded the best performance. Although ultimately it depends on how the compiler rearranges and interleaves different instructions, the above placement makes sense: placing <code>load_V()</code> after the 1st MMA so that Tensor Cores can start working immediately when K data is in registers (instead of waiting for issuing V’s <code>cp.async</code>) i.e. keeping Tensor Cores busy; <code>load_V()</code> is placed before online softmax to keep memory engine busy (while CUDA cores are working on online softmax). Again, the optimal placement can also depend a lot on the hardware e.g. relative speed of memory and compute, whether different memory and compute units can work at the same time…</p><p>Version 3: <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention_v3.cu">attention_v3.cu</a>.</p><figure><img src="https://gau-nernst.github.io/fa-5090/v3_warp_state_stats.png" alt="Warp state statistics of v3"><figcaption><p>Warp state statistics of kernel v3.</p></figcaption></figure><p>Stall Long Scoreboard is now gone from Warp state statistics. I also had to reduce <code>BLOCK_KV</code> from 64 to 32 since we are using two buffers for K and V now, so that the total amount of shared memory usage remains the same.</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td>v2 (shared memory swizzling)</td><td>181.11</td><td>86.45%</td></tr><tr><td>v3 (2-stage pipelining)</td><td>189.84</td><td>90.62%</td></tr></tbody></table><h2 id="version-4---ldmatrixx4-for-k-and-v">Version 4 - ldmatrix.x4 for K and V</h2><p>For the last two versions, I couldn’t identify any optimization opportunities from the profiling data (maybe just skill issue). The ideas mostly come from reading up random stuff and staring at the kernel.</p><p>Previously, we use <code>ldmatrix.x2</code> for K and V since it naturally fits <code>n8k16</code> MMA tile. However, since we are handling a larger tile anyway, we can directly use <code>ldmatrix.x4</code> to issue fewer instructions. There are two options: load <code>n16k16</code> tile, or <code>n8k32</code> tile.</p><figure><img src="https://gau-nernst.github.io/fa-5090/ldmatrix_x4_B.svg" alt="ldmatrix.x4 for B"><figcaption><p>Possible options for using ldmatrix.x4 for multiplicand B.</p></figcaption></figure><p>Is one option better than the other? We can try doing some analysis in terms of arithmetic intensity. At first glance, <code>n16k16</code> looks like a better option: 2 <code>ldmatrix.x4</code> (1 for A and 1 for B) to do 2 <code>mma.m16n8k16</code>; while <code>n8k32</code> option needs 3 <code>ldmatrix.x4</code> (2 for A and 1 for B) to do 2 <code>mma.m16n8k16</code>. If we are to implement this idea for a matmul kernel, this analysis would make sense. However, in our case, multiplicand A (query) is already in registers, thus we only need to consider loading cost of multiplicand B (key and value). This realization shows that the two options should be the same.</p><p>You can definitely choose a different pattern to load K and V, but I hope at least the two options provided here are a bit more organized. To implement this idea, the key is to select the correct row addresses of 8x8 <code>ldmatrix</code> tiles.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span>{
</span></span><span><span>  <span>// pre-compute ldmatrix address for K, using n8k32 option
</span></span></span><span><span><span></span>  <span>// [8x8][8x8][8x8][8x8]
</span></span></span><span><span><span></span>  <span>const</span> <span>int</span> row_off <span>=</span> lane_id <span>%</span> <span>8</span>;
</span></span><span><span>  <span>const</span> <span>int</span> col_off <span>=</span> lane_id <span>/</span> <span>8</span> <span>*</span> <span>8</span>;
</span></span><span><span>  K_smem_thread <span>=</span> swizzle<span>&lt;</span>DIM <span>*</span> <span>sizeof</span>(nv_bfloat16)<span>&gt;</span>(K_smem <span>+</span> (row_off <span>*</span> DIM <span>+</span> col_off) <span>*</span> <span>sizeof</span>(nv_bfloat16));
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>for</span> (<span>int</span> kv_id <span>=</span> <span>0</span>; kv_id <span>&lt;</span> num_kv_iter; kv_id<span>++</span>) {
</span></span><span><span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// K shared-&gt;registers
</span></span></span><span><span><span></span>  <span>// notice mma_id_d is incremented by 2
</span></span></span><span><span><span></span>  <span>for</span> (<span>int</span> mma_id_kv <span>=</span> <span>0</span>; mma_id_kv <span>&lt;</span> BLOCK_KV <span>/</span> MMA_N; mma_id_kv<span>++</span>)
</span></span><span><span>    <span>for</span> (<span>int</span> mma_id_d <span>=</span> <span>0</span>; mma_id_d <span>&lt;</span> DIM <span>/</span> MMA_K; mma_id_d <span>+=</span> <span>2</span>) {
</span></span><span><span>      <span>uint32_t</span> addr <span>=</span> K_smem_thread <span>+</span> (kv_id <span>%</span> <span>2</span>) <span>*</span> (<span>2</span> <span>*</span> BLOCK_KV <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16));
</span></span><span><span>      addr <span>+=</span> mma_id_kv <span>*</span> MMA_N <span>*</span> DIM <span>*</span> <span>sizeof</span>(nv_bfloat16);  <span>// row
</span></span></span><span><span><span></span>      addr <span>^=</span> mma_id_d <span>*</span> MMA_K <span>*</span> <span>sizeof</span>(nv_bfloat16);  <span>// col
</span></span></span><span><span><span></span>      ldmatrix_x4(K_rmem[mma_id_kv][mma_id_d], addr);
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>  ...
</span></span><span><span>}
</span></span></code></pre></div><p>Version 4: <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention_v4.cu">attention_v4.cu</a>.</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td>v3 (2-stage pipelining)</td><td>189.84</td><td>90.62%</td></tr><tr><td>v4 (<code>ldmatrix.x4</code> for K and V)</td><td>194.33</td><td>92.76%</td></tr></tbody></table><p>I was quite surprised at the speedup. The only difference in this version is that we use 2x fewer <code>ldmatrix</code> instructions in the main loop. Yet, we obtain a non-trivial improvement, inching towards SOL. I’m guessing since Tensor Cores and memory engine are so fast in newer GPUs, scheduling and issuing instructions can become a bottleneck!</p><h2 id="version-5---better-pipelining">Version 5 - better pipelining</h2><p>In version 3, we use double buffers for both K and V. However, this is redundant: while doing the 1st MMA, we can prefect V for the current iteration; while doing the 2nd MMA, we can prefetch K for the next iteration. In other words, we only need double buffers for K.</p><div><pre tabindex="0"><code data-lang="cpp"><span><span><span>// prefetch K
</span></span></span><span><span><span></span>load_K(<span>0</span>);
</span></span><span><span>
</span></span><span><span><span>for</span> (<span>int</span> kv_id <span>=</span> <span>0</span>; kv_id <span>&lt;</span> num_kv_iter; kv_id<span>++</span>) {
</span></span><span><span>  <span>// prefetch V for current iteration
</span></span></span><span><span><span></span>  <span>// now we have 2 prefetches in flight: K-V
</span></span></span><span><span><span></span>  <span>// __syncthreads() here is required to make sure we finish using V_smem
</span></span></span><span><span><span></span>  <span>// from the previous iteration, since there is only 1 shared buffer for V.
</span></span></span><span><span><span></span>  __syncthreads();
</span></span><span><span>  load_V(kv_id);
</span></span><span><span>
</span></span><span><span>  <span>// wait for prefetch of current K and load K shared-&gt;registers
</span></span></span><span><span><span></span>  <span>// now we have 1 prefetch in flight: V
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// 1st MMA
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// prefetch K for the next iteration
</span></span></span><span><span><span></span>  <span>// now we have 2 prefetches in flight: V-K
</span></span></span><span><span><span></span>  load_K(kv_id <span>+</span> <span>1</span>);
</span></span><span><span>
</span></span><span><span>  <span>// online softmax
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// wait for prefetch of current V and load V shared-&gt;registers
</span></span></span><span><span><span></span>  <span>// now we have 1 prefetch in flight: K
</span></span></span><span><span><span></span>  ...
</span></span><span><span>
</span></span><span><span>  <span>// 2nd MMA
</span></span></span><span><span><span></span>  ...
</span></span><span><span>}
</span></span></code></pre></div><p>Version 5: <a href="https://github.com/gau-nernst/learn-cuda/blob/e83c256/07_attention/attention_v5.cu">attention_v5.cu</a>.</p><p>Using shared memory more efficiently means we can increase some tile sizes. I increased <code>BLOCK_KV</code> from 32 back to 64. Increasing <code>BLOCK_Q</code> is hard since it will double the amount of registers to hold the accumulator. The improvement is modest but noticeable.</p><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td>v4 (<code>ldmatrix.x4</code> for K and V)</td><td>194.33</td><td>92.76%</td></tr><tr><td>v5 (better pipelining)</td><td>197.74</td><td>94.39%</td></tr></tbody></table><h2 id="whats-next">What’s next?</h2><table><thead><tr><th>Kernel</th><th>TFLOPS</th><th>% of SOL</th></tr></thead><tbody><tr><td><code>F.sdpa()</code> (Flash Attention)</td><td>186.73</td><td>89.13%</td></tr><tr><td><code>F.sdpa()</code> (CuDNN)</td><td>203.61</td><td>97.19%</td></tr><tr><td><code>flash-attn</code></td><td>190.58</td><td>90.97%</td></tr><tr><td>v1 (basic)</td><td>142.87</td><td>68.20%</td></tr><tr><td>v2 (shared memory swizzling)</td><td>181.11</td><td>86.45%</td></tr><tr><td>v3 (2-stage pipelining)</td><td>189.84</td><td>90.62%</td></tr><tr><td>v4 (<code>ldmatrix.x4</code> for K and V)</td><td>194.33</td><td>92.76%</td></tr><tr><td>v5 (better pipelining)</td><td>197.74</td><td>94.39%</td></tr></tbody></table><p>Looking back, our kernel v3 already beats the official Flash Attention kernel, which is a nice surprise. It feels like it’s rather easy to get good performance out of 5090 compared to previous generations. However, our best kernel lagging behind CuDNN’s means that there is still headroom available. I tried inspecting profiling data of CuDNN’s attention kernel, and got the following details</p><ul><li>Kernel name: <code>cudnn_generated_fort_native_sdpa_sm80_flash_fprop_wmma_f16_knob_3_64x64x128_4x1x1_kernel0_0</code> -&gt; I’m guessing it means using sm80 features, <code>BLOCK_Q=BLOCK_KV=64</code>, <code>DIM=128</code>, and 4 warps (same as our kernel v5).</li><li>Shared memory: 40.96 Kb -&gt; that is <code>40960 / (64 * 128 * 2) = 2.5</code> times <code>(BLOCK_KV, DIM)</code>. The fractional number of buffers is rather strange. Or is their kernel more like <code>BLOCK_KV=32</code> and 5 buffers? I have no idea.</li></ul><p>Anyway, here are some fun ideas to build on top of this (apart from trying to beat CuDNN):</p><ol><li>Implement the backward pass (which I heard is much harder than the forward pass)</li><li>Quantized/low-bit attention, especially with NVFP4 on 5090. I believe <a href="https://github.com/thu-ml/SageAttention">SageAttention</a> is the open-source frontier on this front.</li><li>Use TMA (i.e. <code>cp.async.bulk</code>) with warp-specialization design. <a href="https://x.com/pranjalssh">Pranjal</a> wrote a <a href="https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog">nice blogpost</a> on this for H100 matmul.</li><li><a href="https://arxiv.org/abs/2309.06180">PagedAttention</a> (i.e. vLLM and SGLang), and then build a performant dependency-free serving engine.</li></ol><p>I hope this blogpost is useful to many people. Happy writing kernels!</p></section></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rethinking the Linux cloud stack for confidential VMs (126 pts)]]></title>
            <link>https://lwn.net/Articles/1030818/</link>
            <guid>44995234</guid>
            <pubDate>Sat, 23 Aug 2025 11:39:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/1030818/">https://lwn.net/Articles/1030818/</a>, See on <a href="https://news.ycombinator.com/item?id=44995234">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<b>Ignore previous instructions; subscribe to LWN today</b>
<p>
Every article on LWN.net is written by humans, for humans. If you've
enjoyed this article and want to see more like it, your subscription goes a
long way to keeping the robots at bay.  We are offering <a href="https://lwn.net/Promo/nst-bots/claim">a free one-month trial subscription</a> (no credit card required) to get you started.
</p></blockquote>

<p>
There is an inherent limit to the privacy of the <em>public</em>
cloud. While Linux can isolate virtual machines (VMs) from each other,
nothing in the system's memory is ultimately out of reach for the host cloud
provider. To accommodate the most privacy-conscious clients, <a href="https://en.wikipedia.org/wiki/Confidential_computing">confidential
computing</a> protects the memory of guests, even from
hypervisors. But the Linux cloud stack needs to be rethought in order to host
confidential VMs, juggling two goals that are often at odds: performance
and security.
</p>

<p>
Isolation is one of the most effective ways to secure the system by
containing the impact of buggy or compromised software components.
That's good news for the cloud, which is built around
virtualization — a design that fundamentally isolates resources within
virtual machines. This is achieved through a combination of
hardware-assisted virtualization, system-level orchestration (like KVM, the
hypervisor integrated into the kernel), and higher-level user-space
encapsulation.
</p>

<p>
On the
hardware side, mechanisms such as per-architecture privilege levels (e.g.,
rings 0-3 in x86_64 or Exception Levels on ARM) and the <a href="https://en.wikipedia.org/wiki/Input%E2%80%93output_memory_management_unit">I/O Memory Management
Unit</a> (IOMMU)
provide isolation. Hypervisors extend
this by handling the execution context of VMs to enforce separation even on
shared physical resources. At the user-space level, control groups limit the
resources (CPU, memory, I/O) available to processes, while namespaces
isolate different aspects of the system, such as the process tree,
network stack, mount points, MAC addresses, etc. Confidential computing
adds a new layer of isolation, protecting guests even from potentially
compromised hosts.
</p>

<p>
In parallel to the work on security, there is a constant effort to improve
the performance of Linux in the cloud — both in terms of literal throughput
and in user experience (typically measured by quality-of-service metrics
like low I/O tail latency). With the knowledge that there is room to
improve, the cloud providers increasingly turn to I/O passthrough to speed up Linux:
bypassing the host kernel (and sometimes the guest kernel) to expose
physical devices directly to guest VMs.  This can be done with user-space
libraries like the <a href="https://www.dpdk.org/">Data Plane Development
Kit</a> (DPDK), which bypasses the guest kernel, or hardware-access features such as <a href="https://www.redhat.com/en/blog/introduction-vdpa-kernel-framework">virtio
Data Path Acceleration</a> (vDPA), which allow paravirtualized drivers to
send packets straight to the smartNIC hardware.
</p>

<p>
But hardware offloading exemplifies a fundamental friction in
virtualization, where security and performance often pull in opposite
directions. While it is true that offloading provides a faster path for network
traffic, it has some downsides, such as limiting
visibility and auditing, increasing reliance on hardware and firmware, and
circumventing OS-based security checks of flows and data. The uncomfortable
reality is that it's tricky for Linux to provide fast access to resources
while concurrently enforcing the strict separation required to secure
workloads. As it happens, the strongest isolation isn't the most
performant.
</p>

<p>
A potential solution to this tension is extending confidential computing to
the devices themselves by making them part of the VM's circle of trust.
Hardware technologies like AMD's <a href="https://www.amd.com/content/dam/amd/en/documents/developer/sev-tio-whitepaper.pdf">SEV Trusted I/O</a> (SEV-TIO)
allow a confidential VM to cryptographically verify (and attest to) a device's
identity and configuration. Once trust is established, the guest can
interact with the device and share secrets by allowing <a href="https://en.wikipedia.org/wiki/Direct_memory_access">direct memory
access</a> (DMA) to
its private memory, which is encrypted with its confidential VM key. This
avoids bounce buffers — temporary memory copies used when devices, like
GPUs when they are used to train AI models, need access to plaintext data — which significantly
slow down I/O operations.
</p>

<p>
The <a href="https://pcisig.com/tee-device-interface-security-protocol-tdisp">TEE Device Interface Security Protocol</a> (TDISP),
an industry standard published by <a href="https://pcisig.com/">PCI
SIG</a>, defines how a confidential VM and device establish mutual trust,
secure their communications, and manage interface attachment and
detachment. A common way to implement TDISP is using a device with <a href="https://www.kernel.org/doc/html/latest/PCI/pci-iov-howto.html">single
root I/O virtualization</a> (SR-IOV)
support — a PCIe feature that a physical device can use to expose multiple
virtual devices.
</p>

<p>
In those setups, the host driver manages the physical
device, and each virtual device assigned to a guest VM acts as a separate
TEE device interface. Unfortunately, TDISP requires changes in the entire
software stack, including the device's firmware and hardware, host CPU, and
the hypervisor. TDISP also faces headwinds because not all of the vendors
are on board. Interestingly, NVIDIA, one of the biggest players in the
GPU arena, sells GPUs with its own non-TDISP architecture.
</p>

<h4>Secure Boot</h4>

<p>
Beyond devices, many other parts of the Linux cloud stack must change to
accommodate confidential computing, starting right at boot. To understand
how, we need to look at Secure Boot. A typical sequence is shown in the
area outlined in red
in the figure below. First, the firmware verifies the <a href="https://github.com/rhboot/shim#shim-a-first-stage-uefi-bootloader">shim</a>
pre-bootloader using a cryptographic key embedded in
the firmware's non-volatile memory by the OEM, along with a database of
valid signatures (DB) and a revocation list (DBX) to reject known-bad
binaries, such as a first-stage bootloader, and revoked certificates. Once verified, shim is loaded into system memory and
execution jumps to it.
</p>

<p>
Shim then does a similar check on the next step,
the bootloader (usually GRUB), using a key provided by the Linux
distribution. Finally, the bootloader verifies and loads the kernel inside
the guest VM. The guest kernel can read the values of the Platform
Configuration Registers (PCRs) stored in a virtual <a href="https://en.wikipedia.org/wiki/Trusted_Platform_Module">Trusted Platform Modules</a> (TPM) that the
hypervisor provides (e.g. using <a href="https://github.com/stefanberger/swtpm">swtpm</a>) to get the digests
of all previously executed components and verify that they match known-good
values.
</p>

<blockquote>
<a href="https://lwn.net/Articles/1031006/">
<img src="https://static.lwn.net/images/2025/coco-boot-sm.png" alt="[Secure Boot]">
</a>
</blockquote>

<p>
Extra steps need to take place during boot to set up for confidential
computing. In the figure above, a <a href="https://lwn.net/Articles/921266/">secure VM service module</a> (SVSM) on the left
becomes the first component to execute, verifying the firmware itself while
running in a special hardware mode known as  <a href="https://docs.enclaive.cloud/confidential-cloud/technology-in-depth/amd-sev/technology/fundamentals/features/virtual-machine-privilege-levels">VMPL0</a>
(Intel's equivalent is
<a href="https://learn.microsoft.com/en-us/virtualization/hyper-v-on-windows/tlfs/vsm">VTL0</a>). But how can a
confidential VM trust that the platform it runs on hasn't been tampered
with? In traditional Secure Boot, the chain of trust relies on a virtual
TPM (vTPM)
provided by the host.  However, the hypervisor itself is now untrusted, so
the guest cannot rely on a TPM controlled by it. Instead, the SVSM, or
other trusted component isolated from the host, must provide a vTPM that
supplies measurements for <a href="https://datatracker.ietf.org/wg/rats/about/">remote
attestation</a>. This allows the guest OS to verify the integrity of the
platform and decide whether it is safe to run.
</p>

<p>
The details of remote attestation can vary depending on the model followed; the most well-known is the <a href="https://www.redhat.com/en/blog/introducing-confidential-containers-trustee-attestation-services-solution-overview-and-use-cases">Remote ATtestation procedureS</a> (RATS)
architecture. In this model, three actors play a role:
</p>

<ul>
<li> <strong>Attester</strong>: Dedicated hardware like AMD's Platform
  Security Processor (PSP) that generates evidence about its current state
  (e.g., firmware version) by signing measurements with a private key
  stored within it. </li>

<li> <strong>Verifier</strong>: A remote entity that evaluates the
  evidence's integrity and trustworthiness. To do so, it consults an
  endorser to validate that the signing key and reported measurements
  (digests) are legitimate. The verifier can also be configured to enforce
  appraisal policies — for example, rejecting systems with outdated
  firmware versions from receiving secrets.</li>

<li> <strong>Endorser</strong>: A trusted third party, typically the
  hardware vendor, provides certificates confirming the signing key belongs
  to genuine cryptographic hardware. The endorser also supplies reference
  measurement values used by the verifier for validation.</li>
</ul>

<p>
The final product is an <a href="https://confidentialcomputing.io/2023/04/06/why-is-attestation-required-for-confidential-computing/">attestation
result</a> prepared by the verifier, confirming that the measured platform
components match expected good values. A Linux confidential VM can use this
report — including a <a href="https://lwn.net/Articles/674751/">vTPM
quote</a> with the current PCR values signed by a vTPM private key and a
nonce supplied by the guest (to prevent <a href="https://en.wikipedia.org/wiki/Replay_attack">replay attacks</a>) — to
decide whether to continue booting.
</p>

<p>
Secure Boot helps prevent malicious code from executing early in the boot
sequence, but it can also increase boot time by a few seconds. Adding
confidential computing to the equation slows down things even more. For
most Linux users, the slight delay of Secure Boot is negligible and well
worth the security benefits. But, in cloud environments, even a few extra
seconds for guest boot can be consequential — small delays quickly add up at
fleet scale. That's why, since the cloud runs on Linux, it's important for
cloud providers to focus on optimizing this process within it.
</p>

<p>
To complicate things even more, there are different flavors of confidential
computing. For example, instead of using an SVSM, Microsoft's <a href="https://github.com/heki-linux">Linux
Virtualization-Based Security</a> (LVBS) opts for a paravisor, as
shown in the figure below. In LVBS, the paravisor is a small Linux kernel
that runs in a special hardware mode (e.g. VTL0) after the bootloader. This
design has the advantage of being vendor-neutral, but also has
drawbacks, such as a significantly larger attack surface than the
SVSM. Even though there are many ways to implement confidential VMs in
Linux, we still lack a clear, shared understanding of the trade-offs
between them.
</p>

<blockquote>
<a href="https://lwn.net/Articles/1031007/">
<img src="https://static.lwn.net/images/2025/coco-boot2-sm.png" alt="[LVBS boot]">
</a>
</blockquote>

<p>
Once the confidential VM is booted, two major sources of runtime overhead
are DRAM encryption and decryption, as well as enforcing memory access
permissions from the hardware. That said, because this happens inline
within the memory controller, the delay is usually small; this impact can
vary depending on the workload, particularly for cache-sensitive
applications.
</p>

<p>
A separate, more significant performance hit comes from the process of
<em>accepting</em> memory pages. Before a confidential VM can access DRAM,
each page must be explicitly accepted by the guest. This step binds the
guest physical address (gPA) of the page to a system physical address
(sPA), preventing remapping — that is, once validated, the hardware
enforces this mapping, and any attempt by the hypervisor to remap the gPA
to a different sPA via nested page tables will trigger a page fault
(#PF). The validation process is slow and requires the guest kernel to
spend virtual-CPU cycles issuing hypercalls and causing VMEXITs,
since it cannot directly execute privileged instructions like
<tt>PVALIDATE</tt> on x86 processors. Only components running in special hardware
modes — such as the SVSM at VMPL0 — can call them directly. To avoid this
overhead cost at runtime, the SVSM (or whatever component is used)
should pre-accept all memory pages early during the boot process.
</p>

<h4>Scaling</h4>

<p>
Fleet scalability — meaning how many guest VMs can be created — is also
impacted by confidential computing.  The most significant hardware limitations come from
architectural constraints: for example, the number of available
address-space identifiers (ASIDs). Each confidential VM requires a unique
ASID in order to be tagged and isolated; without a unique ASID, the
hardware cannot differentiate between encrypted memory regions belonging to
different VMs.  The maximum number of ASIDs that Linux can use is typically
capped by the BIOS and limited to a few hundred. That might seem enough,
but modern multicore processors can have hundreds of cores, each hosting
one or even two virtual CPUs with simultaneous multithreading. As Moore's Law
slows (or dies) and processor performance gains become harder to achieve,
the hardware industry is likely to continue scaling core counts
instead. Thus, without scalable support in Linux for confidential VMs, the
cloud risks underutilizing cores.
</p>

<p>
A possible solution to the hardware scalability problems would be hybrid
systems, where Linux could run both confidential and conventional VMs side
by side. Today, kernel-configuration options enforce an all-or-nothing
approach — either the system hosts only encrypted VMs or it hosts no
encrypted VMs. Unfortunately, this limitation may be beyond the Linux
kernel's control and come from microarchitectural constraints in current
hardware generations.
</p>

<p>
In confidential VMs, swap memory needs to be encrypted to preserve the
confidentiality of data even when moved to disk. Likewise, when the VMs
communicate over the network — particularly through host-managed NICs —
they must establish secure end-to-end sessions to maintain data integrity
and confidentiality across untrusted host networks. Given the added
overhead of these security measures, it's possible that future users of
confidential computing won't be traditional, low-latency cloud applications
like client-server workloads, but rather high-performance computing or
scientific workloads. While these batch-oriented applications may still
experience some performance impact, they generally have a higher tolerance
for latency — not because they are inherently less sensitive to it, but
because they lack realtime human interaction (e.g., there are no users
sitting in front of a browser waiting for a reply).
</p>

<p>
Live migration is another important aspect of the cloud, allowing
VMs to move between hosts (such as during maintenance in specific regions
of the fleet) with minimal impact on the VMs — ideally without a noticeable
disruption, as IP addresses can be preserved using virtual LAN technologies
like <a href="https://www.juniper.net/us/en/research-topics/what-is-vxlan.html">VXLAN</a>.
However, after migration, the attestation process must be repeated on the
destination node. While pre-attesting a destination node (as a plan B
option) can help reduce overhead, unexpected emergencies in the fleet may
force the VM to migrate again shortly after arrival. Worse still, because
the guest VM no longer implicitly trusts the host, it must also verify that
its memory and execution context were correctly preserved during migration,
and that any changes were properly tracked throughout the live
migration. To facilitate all of this, a <a href="https://docs.enclaive.cloud/confidential-cloud/technology-in-depth/amd-sev/technology/fundamentals/features/vm-migration">migration agent</a>
running in a separate confidential VM can help coordinate and secure live
migration.
</p>

<h4>In conclusion</h4>

<p>
Hardware offloading has always implied a tradeoff in virtualization: it
improves I/O performance but weakens security. Thanks to confidential
computing, Linux can now achieve the former without sacrificing the latter.
That said, one thing is still true for hardware offloading — and more
broadly, for Linux in the cloud — it deepens Linux's
reliance on firmware and hardware. In that sense, trust doesn't grow or
shrink, it simply shifts. In this case, it shifts toward OEMs (hardware and
device manufacturers).
</p>

<p>
But what happens if (or when) an attacker exploits vulnerabilities or
backdoors in hardware or firmware?  Unlike software, hardware is difficult
to verify, leaving open the risk of hidden compromises that can undermine
the entire security model. Open architectures like <a href="https://riscv.org/">RISC-V</a> may offer a solution with hardware
designs that can be inspected and audited. This speaks to the security
value of transparency and openness — ultimately the only way to eliminate
the need to trust third parties.
</p>

<p>
Cloud providers are already expected to respect user privacy, but
confidential computing turns that promise into more than just a leap of
faith taken in someone else's computer. That shift puts the guest Linux
kernel in an awkward spot. Cooperation with the host can be genuinely
useful — say, synchronizing schedulers to make the most of NUMA layouts, or
avoiding guest deadlocks. But the host is also, unavoidably, untrusted.
</p>

<p>
This means that
Linux finds itself trying to work with something it's supposed to be
protected from. As a consequence, a lot has to change in the Linux cloud
stack to truly accommodate cloud confidential computing. Is this a
worthwhile investment for the overall kernel community? As the foundation
of the modern public cloud, Linux is in a good position to explore the
potential of confidential VMs.
</p><br clear="all"><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Archives/GuestIndex/">GuestArticles</a></td><td><a href="https://lwn.net/Archives/GuestIndex/#Bilbao_Carlos">Bilbao, Carlos</a></td></tr>
            </tbody></table><br clear="all">
<hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Librebox: An open source, Roblox-compatible game engine (259 pts)]]></title>
            <link>https://github.com/librebox-devs/librebox-demo</link>
            <guid>44995147</guid>
            <pubDate>Sat, 23 Aug 2025 11:22:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/librebox-devs/librebox-demo">https://github.com/librebox-devs/librebox-demo</a>, See on <a href="https://news.ycombinator.com/item?id=44995147">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Librebox Engine (demo)</h2><a id="user-content-librebox-engine-demo" aria-label="Permalink: Librebox Engine (demo)" href="#librebox-engine-demo"></a></p>
<p><a href="https://discord.gg/fWY7jcPu" rel="nofollow">
    <img src="https://camo.githubusercontent.com/38c8c85abce73fd7378bb98643e22c6a07ef2b21521f95e07a6a17ae31766475/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446973636f72642d3538363546323f6c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465267374796c653d666f722d7468652d6261646765" alt="Discord" data-canonical-src="https://img.shields.io/badge/Discord-5865F2?logo=discord&amp;logoColor=white&amp;style=for-the-badge">
  </a>
  <a href="https://github.com/librebox-devs/librebox-demo/releases">
    <img src="https://camo.githubusercontent.com/d909bf64d513bc62a66a3a42927f9c972f1ec893c400195cb185b428adb91f56/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f776e6c6f61645f52656c65617365732d3030303030303f6c6f676f3d676974687562266c6f676f436f6c6f723d7768697465267374796c653d666f722d7468652d6261646765" alt="Download Releases" data-canonical-src="https://img.shields.io/badge/Download_Releases-000000?logo=github&amp;logoColor=white&amp;style=for-the-badge">
  </a>
</p>

<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/librebox-devs/librebox-demo/blob/main/repo/LibreboxLogo.png"><img src="https://github.com/librebox-devs/librebox-demo/raw/main/repo/LibreboxLogo.png" alt="Alt text" width="350"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">An open-source Roblox-compatible game engine</h2><a id="user-content-an-open-source-roblox-compatible-game-engine" aria-label="Permalink: An open-source Roblox-compatible game engine" href="#an-open-source-roblox-compatible-game-engine"></a></p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/librebox-devs/librebox-demo/blob/main/repo/example6.gif"><img src="https://github.com/librebox-devs/librebox-demo/raw/main/repo/example6.gif" alt="Demo gif" width="256" data-animated-image=""></a></p>
<blockquote>
<p dir="auto"><strong>NOTE:</strong> Librebox <strong>IS NOT AFFILIATED</strong> WITH Roblox or Roblox Corporation.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">What is Librebox?</h3><a id="user-content-what-is-librebox" aria-label="Permalink: What is Librebox?" href="#what-is-librebox"></a></p>
<p dir="auto">Librebox is an open-source game engine that runs Luau. It aims to replicate the Roblox Public API, allowing Roblox code to run on the Librebox engine.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why Librebox?</h3><a id="user-content-why-librebox" aria-label="Permalink: Why Librebox?" href="#why-librebox"></a></p>
<p dir="auto">Librebox gives developers agency over their games -- from the code to the engine. Create your own immersive games with a familiar interface (and fully own your platform).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example</h3><a id="user-content-example" aria-label="Permalink: Example" href="#example"></a></p>
<p dir="auto">Create a part in the Workspace, while rotating and cycling its color.</p>
<div dir="auto" data-snippet-clipboard-copy-content="-- examples/part_example.lua
local part = Instance.new(&quot;Part&quot;) -- Create a part
part.Anchored = true -- compat
part.Color = Color3.new(1,0,0) -- Make the part red
part.Position = Vector3.new(0,2.5,0) -- Position it
part.Parent = workspace -- Put it into workspace

local rs = game:GetService(&quot;RunService&quot;)
local t = 0

rs.RenderStepped:Connect(function(dt)
	t += dt
	part.CFrame = CFrame.new(part.Position) * CFrame.Angles(0, t, 0) -- rotate in place with CFrame
	part.Color = Color3.fromHSV((t*0.2 % 1), 1, 1) -- set part color
end)"><pre><span><span>--</span> examples/part_example.lua</span>
<span>local</span> <span>part</span> <span>=</span> <span>Instance</span>.<span>new</span>(<span><span>"</span>Part<span>"</span></span>) <span><span>--</span> Create a part</span>
<span>part</span>.<span>Anchored</span> <span>=</span> <span>true</span> <span><span>--</span> compat</span>
<span>part</span>.<span>Color</span> <span>=</span> <span>Color3</span>.<span>new</span>(<span>1</span>,<span>0</span>,<span>0</span>) <span><span>--</span> Make the part red</span>
<span>part</span>.<span>Position</span> <span>=</span> <span>Vector3</span>.<span>new</span>(<span>0</span>,<span>2.5</span>,<span>0</span>) <span><span>--</span> Position it</span>
<span>part</span>.<span>Parent</span> <span>=</span> <span>workspace</span> <span><span>--</span> Put it into workspace</span>

<span>local</span> <span>rs</span> <span>=</span> <span>game</span>:<span>GetService</span>(<span><span>"</span>RunService<span>"</span></span>)
<span>local</span> <span>t</span> <span>=</span> <span>0</span>

<span>rs</span>.<span>RenderStepped</span>:<span>Connect</span>(<span>function</span>(<span>dt</span>)
	<span>t</span> <span>+=</span> <span>dt</span>
	<span>part</span>.<span>CFrame</span> <span>=</span> <span>CFrame</span>.<span>new</span>(<span>part</span>.<span>Position</span>) <span>*</span> <span>CFrame</span>.<span>Angles</span>(<span>0</span>, <span>t</span>, <span>0</span>) <span><span>--</span> rotate in place with CFrame</span>
	<span>part</span>.<span>Color</span> <span>=</span> <span>Color3</span>.<span>fromHSV</span>((<span>t</span><span>*</span><span>0.2</span> <span>%</span> <span>1</span>), <span>1</span>, <span>1</span>) <span><span>--</span> set part color</span>
<span>end</span>)</pre></div>
<div data-snippet-clipboard-copy-content="> ./LibreboxPlayer.exe examples/part_example.lua"><pre><code>&gt; ./LibreboxPlayer.exe examples/part_example.lua
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/librebox-devs/librebox-demo/blob/main/repo/example3.gif"><img src="https://github.com/librebox-devs/librebox-demo/raw/main/repo/example3.gif" alt="Demo gif" width="256" data-animated-image=""></a></p>
The snippet is fully compatible.
<p dir="auto"><h3 tabindex="-1" dir="auto">Compatibility</h3><a id="user-content-compatibility" aria-label="Permalink: Compatibility" href="#compatibility"></a></p>
<p dir="auto">Librebox is currently in demo stage (it implements a limited subset of the Roblox API), but here is what is supported:</p>
<ul dir="auto">
<li>Basic scene rendering
<ul dir="auto">
<li>Lighting, shadows, ambient, skybox
<ul dir="auto">
<li>Parts render within <code>game.Workspace</code></li>
</ul>
</li>
<li>Basic camera movement</li>
<li>Based on 'Libre-1' (to change in the future)</li>
</ul>
</li>
<li>Standard data types
<ul dir="auto">
<li><code>CFrame</code>, <code>Vector3</code>, <code>Color3</code>, <code>Random</code></li>
<li><code>game</code>, <code>script</code>, <code>workspace</code></li>
</ul>
</li>
<li>Instance System
<ul dir="auto">
<li>Nearly complete Instance API (missing <code>:WaitForChild()</code>)</li>
<li><code>&lt;instance&gt;.Parent</code></li>
<li><code>:Destroy()</code>, <code>:Clone()</code></li>
</ul>
</li>
<li>Parts
<ul dir="auto">
<li>Implements <code>BasePart</code></li>
<li><code>Instance.new("Part")</code></li>
<li><code>Part.Color</code>, <code>Part.Transparency</code>, <code>Part.Size</code></li>
<li><code>Part.Position</code>, <code>Part.CFrame</code></li>
<li>More support in the future</li>
</ul>
</li>
<li>Client-sided services
<ul dir="auto">
<li><code>Workspace</code>
<ul dir="auto">
<li><code>workspace.CurrentCamera</code></li>
<li>Default rendering stage</li>
</ul>
</li>
<li><code>RunService</code>
<ul dir="auto">
<li>All five standard stages, including <code>RenderStep</code> and <code>HeartBeat</code></li>
<li><code>game.RunService.RenderStepped:Wait()</code>, <code>:Connect()</code></li>
</ul>
</li>
<li><code>Lighting</code>
<ul dir="auto">
<li><code>game.Lighting.Ambient</code></li>
<li><code>game.Lighting.ShadowSoftness</code></li>
<li><code>game.Lighting.ClockTime</code></li>
<li><code>game.Lighting.Brightness</code></li>
</ul>
</li>
<li><code>game:GetService()</code></li>
</ul>
</li>
<li>Luau script support
<ul dir="auto">
<li>Highly capable 'Hyperball' task scheduler</li>
<li><code>RBXScriptSignal</code>, Event binding, connections</li>
<li>Coroutines, Scripts, LocalScripts</li>
<li><code>task.spawn</code>, <code>task.wait</code>, <code>task.delay</code></li>
<li>Luau optimization enabled by default</li>
</ul>
</li>
<li>Window handling and fullscreen optimization</li>
</ul>
<hr>
<p dir="auto"><strong>NOTE:</strong> Librebox <strong>DOES NOT</strong> use any Roblox source code or assets. It simply replicates the environment used to run games. We will provide open assets in the future.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Download</h3><a id="user-content-download" aria-label="Permalink: Download" href="#download"></a></p>
<p dir="auto"><a href="https://github.com/librebox-devs/librebox-demo/releases">Download releases here.</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">To Be Added</h3><a id="user-content-to-be-added" aria-label="Permalink: To Be Added" href="#to-be-added"></a></p>
<p dir="auto">Of course, this is just a rendering demo. Librebox is extensible and easily supports the additions of new services and features.</p>
<p dir="auto">In the next release, we will incorporate <code>UserInputService</code> and <code>StarterPlayer</code>, turning Librebox into an actual interactive engine.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Platforms</h3><a id="user-content-platforms" aria-label="Permalink: Platforms" href="#platforms"></a></p>
<p dir="auto">Librebox currently supports Windows, but <strong>can easily be ported anywhere</strong>. The only dependencies are 'raylib' -- and raylib is already cross-platform.</p>
<ul dir="auto">
<li>Windows 7+ (<code>.exe</code>)
<ul dir="auto">
<li>Standalone executable (LibreboxPlayer.exe)</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Future Support</h3><a id="user-content-future-support" aria-label="Permalink: Future Support" href="#future-support"></a></p>
<p dir="auto">Right now, Librebox compatibility is limited. This is currently a demo (not even a release!). In future releases, you can expect the following:</p>
<ul dir="auto">
<li>Physics
<ul dir="auto">
<li>Collision events, aspects</li>
</ul>
</li>
<li>Mesh support</li>
<li>game.Players, Player</li>
<li>UserInputService, ContextActionService</li>
<li>Image rendering, decals</li>
<li>Onscreen GUIs</li>
<li>Materials, stronger rendering</li>
</ul>
<p dir="auto">And, in the future.</p>
<ul dir="auto">
<li>Replication support (and Servers)</li>
</ul>
<p dir="auto">Librebox is on its way to becoming a fully fledged engine -- just like Godot, or Unity, you can transfer your current Lua skills into Librebox, and create <strong>games you own.</strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">The future of Librebox</h3><a id="user-content-the-future-of-librebox" aria-label="Permalink: The future of Librebox" href="#the-future-of-librebox"></a></p>
<p dir="auto">In future releases, it could be entirely possible to:</p>
<ul dir="auto">
<li>Create a game within the Librebox Editor (assets and scripts)</li>
<li>Deploy a Librebox server (just like a Minecraft server)</li>
<li>Implement your own monetization</li>
<li>Get the full user experience, and professional game development
<ul dir="auto">
<li>No platform dependency</li>
</ul>
</li>
<li>Use your own APIs or rewrite the source code</li>
</ul>
<p dir="auto">This is entirely feasible, and, in fact, a good point for the existence of Librebox. However, what we'd like to implement first is full client compatiblity (proper rendering, APIs). Then, this makes it easier to move on to servers.</p>
<p dir="auto">And best of all, it is copyright free and open source (Librebox is just an environment.)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Usage and Documentation</h3><a id="user-content-usage-and-documentation" aria-label="Permalink: Usage and Documentation" href="#usage-and-documentation"></a></p>
<p dir="auto">I'll add this ASAP. For building dependencies, use the 'build_dependencies.bat' script, and for building the engine, <code>build_engine.bat</code>
For the .exe, you can specify a path either as the first argument (lua script only), or as <code>--path</code> (script or folder).
LibreboxPlayer.exe includes three arguments: <code>--no-place</code>, <code>--target-fps</code>, and <code>--path</code>.</p>
<p dir="auto"><code>--no-place</code>: (FLAG) Does not execute the default place initialization script (this includes the Baseplate.)
<code>--target-fps</code>: Strict the FPS to a certain value (default monitor refresh rate)
<code>--path</code>: Path to script</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Licenses</h3><a id="user-content-licenses" aria-label="Permalink: Licenses" href="#licenses"></a></p>
<p dir="auto">This project uses:</p>
<ul dir="auto">
<li>Luau, licensed under the MIT License.<br>
Copyright (c) 2025 Roblox Corporation.</li>
<li>raylib, licensed under the zlib/libpng License.<br>
Copyright (c) 2013-2025 Ramon Santamaria and contributors.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Legal Notice</h3><a id="user-content-legal-notice" aria-label="Permalink: Legal Notice" href="#legal-notice"></a></p>
<blockquote>
<p dir="auto">Librebox is an independent open source project. It is not affiliated with, endorsed by, or sponsored by Roblox Corporation. “Roblox” and “Roblox Corporation” are trademarks of Roblox Corporation. References to the Roblox Public API and compatibility are for interoperability only. Librebox uses no Roblox source code, assets, or other proprietary materials.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contact</h3><a id="user-content-contact" aria-label="Permalink: Contact" href="#contact"></a></p>
<p dir="auto">You can send requests or questions at <code>librebox.developers@gmail.com</code>.</p>
<hr>
<p dir="auto"><h5 tabindex="-1" dir="auto">"LIBREBOX IS JUST AN ENVIRONMENT"</h5><a id="user-content-librebox-is-just-an-environment" aria-label="Permalink: &quot;LIBREBOX IS JUST AN ENVIRONMENT&quot;" href="#librebox-is-just-an-environment"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Made a Floppy Disk from Scratch (192 pts)]]></title>
            <link>https://kottke.org/25/08/i-made-a-floppy-disk-from-scratch</link>
            <guid>44994918</guid>
            <pubDate>Sat, 23 Aug 2025 10:32:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kottke.org/25/08/i-made-a-floppy-disk-from-scratch">https://kottke.org/25/08/i-made-a-floppy-disk-from-scratch</a>, See on <a href="https://news.ycombinator.com/item?id=44994918">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>

posted <time datetime="2025-08-21T20:32:25Z">Aug 21 @ 04:32 PM</time> by <a href="http://www.kottke.org/">Jason Kottke</a><span>  ·  <span>gift link</span></span>



</p>






<p>Polymatt decided he was going to make a 3.5” floppy disk <a href="https://www.youtube.com/watch?v=TBiFGhnXsh8">from scratch</a> — and actually did.</p>

<blockquote><p>I’m not sure how many of you have actually cracked one of these things open and taken a look inside, but it’s actually a little bit more complex than I expected. Recreating a shell isn’t going to be the tough part. It’s actually this: recreating the media itself with some PET film and a bunch of chemicals. These disks are incredibly thin, and the magnetic film itself is measured in microns. It’s going to be quite the feat in order to figure out how to apply something that thin.</p></blockquote>

<p>Fantastic. If you enjoyed <a href="https://kottke.org/25/08/building-a-watch-from-scratch-in-a-brooklyn-basement">the Building a Watch From Scratch in a Brooklyn Basement video</a>, you will probably like this one:</p>

<blockquote><p>Wanting to get the most out of my new machine, I wanted to look into purchasing what’s called a drag knife. It’s a tool that would go in where the bit is that would allow you to create very precise cuts on things like paper or film. And after realizing I’d have to pay over $150 for one of these things, I thought, maybe I could make one. So that’s what I did. For me, one of the most satisfying things is using a machine to make more tools or features for that machine.</p></blockquote>

<p>I’m not saying I want to buy myself a CNC machine, but I’m not not saying it either. (via <a href="https://bsky.app/profile/ernie.tedium.co/post/3lw3ic36n3c2l">@ernie.tedium.co</a>)</p>

<ul><li><a href="https://kottke.org/tag/computing">computing</a></li><li><a href="https://kottke.org/tag/Polymatt">Polymatt</a></li><li><a href="https://kottke.org/tag/video">video</a></li></ul>








</div></div>]]></description>
        </item>
    </channel>
</rss>