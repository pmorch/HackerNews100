<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 10 Aug 2024 18:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[OpenSnitch is a GNU/Linux interactive application firewall (136 pts)]]></title>
            <link>https://github.com/evilsocket/opensnitch</link>
            <guid>41209688</guid>
            <pubDate>Sat, 10 Aug 2024 14:15:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/evilsocket/opensnitch">https://github.com/evilsocket/opensnitch</a>, See on <a href="https://news.ycombinator.com/item?id=41209688">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><p>
  Join the project community on our server!
  </p><p>
  <a href="https://discord.gg/https://discord.gg/btZpkp45gQ" title="Join our community!" rel="nofollow">
    <img src="https://camo.githubusercontent.com/549e93886be3c89143d30b3a80f7b34e8fedee957710c2481953ddde669193c6/68747470733a2f2f646362616467652e6c696d65732e70696e6b2f6170692f7365727665722f68747470733a2f2f646973636f72642e67672f62745a706b7034356751" data-canonical-src="https://dcbadge.limes.pink/api/server/https://discord.gg/btZpkp45gQ">
  </a></p></div>
<hr>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/evilsocket/opensnitch/master/ui/opensnitch/res/icon.png"><img alt="opensnitch" src="https://raw.githubusercontent.com/evilsocket/opensnitch/master/ui/opensnitch/res/icon.png" height="160"></a>
  </p><p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/evilsocket/opensnitch/workflows/Build%20status/badge.svg"><img src="https://github.com/evilsocket/opensnitch/workflows/Build%20status/badge.svg"></a>
    <a href="https://github.com/evilsocket/opensnitch/releases/latest"><img alt="Release" src="https://camo.githubusercontent.com/219cfed611036fcac4f1c190954d3d0af9f7d489d2e5d69e10b2415a86f18de2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6576696c736f636b65742f6f70656e736e697463682e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/github/release/evilsocket/opensnitch.svg?style=flat-square"></a>
    <a href="https://github.com/evilsocket/opensnitch/blob/master/LICENSE.md"><img alt="Software License" src="https://camo.githubusercontent.com/d74ecd3c454461cffea50e16ee633e212ab258222b06e5fd630d34c5429c2fa5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d47504c332d627269676874677265656e2e7376673f7374796c653d666c61742d737175617265" data-canonical-src="https://img.shields.io/badge/license-GPL3-brightgreen.svg?style=flat-square"></a>
    <a href="https://goreportcard.com/report/github.com/evilsocket/opensnitch/daemon" rel="nofollow"><img alt="Go Report Card" src="https://camo.githubusercontent.com/6d9060c6e28f36e61ee8e0f59335b109a23f9ce97e9554da72cd553a3efca3dd/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f6576696c736f636b65742f6f70656e736e697463682f6461656d6f6e3f7374796c653d666c61742d737175617265" data-canonical-src="https://goreportcard.com/badge/github.com/evilsocket/opensnitch/daemon?style=flat-square"></a>
    <a href="https://repology.org/project/opensnitch/versions" rel="nofollow"><img src="https://camo.githubusercontent.com/24dbb94e706fb18f6b34697db56522fcbe2f6172f058b05710822bd39a45a367/68747470733a2f2f7265706f6c6f67792e6f72672f62616467652f74696e792d7265706f732f6f70656e736e697463682e737667" alt="Packaging status" data-canonical-src="https://repology.org/badge/tiny-repos/opensnitch.svg"></a>
  </p>

<p dir="auto"><strong>OpenSnitch</strong> is a GNU/Linux application firewall.</p>
<p dir="auto">•• <a href="#key-features">Key Features</a> • <a href="#download">Download</a> • <a href="#installation">Installation</a> • <a href="#opensnitch-in-action">Usage examples</a> • <a href="#in-the-press">In the press</a> ••</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/2742953/85205382-6ba9cb00-b31b-11ea-8e9a-bd4b8b05a236.png"><img src="https://user-images.githubusercontent.com/2742953/85205382-6ba9cb00-b31b-11ea-8e9a-bd4b8b05a236.png" alt="OpenSnitch"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key features</h2><a id="user-content-key-features" aria-label="Permalink: Key features" href="#key-features"></a></p>
<ul dir="auto">
<li>Interactive outbound connections filtering.</li>
<li><a href="https://github.com/evilsocket/opensnitch/wiki/block-lists">Block ads, trackers or malware domains</a> system wide.</li>
<li>Ability to <a href="https://github.com/evilsocket/opensnitch/wiki/System-rules">configure system firewall</a> from the GUI (nftables).
<ul dir="auto">
<li>Configure input policy, allow inbound services, etc.</li>
</ul>
</li>
<li>Manage <a href="https://github.com/evilsocket/opensnitch/wiki/Nodes">multiple nodes</a> from a centralized GUI.</li>
<li><a href="https://github.com/evilsocket/opensnitch/wiki/SIEM-integration">SIEM integration</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Download</h2><a id="user-content-download" aria-label="Permalink: Download" href="#download"></a></p>
<p dir="auto">Download deb/rpm packages for your system from <a href="https://github.com/evilsocket/opensnitch/releases">https://github.com/evilsocket/opensnitch/releases</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">deb</h4><a id="user-content-deb" aria-label="Permalink: deb" href="#deb"></a></p>
<blockquote>
<p dir="auto">$ sudo apt install ./opensnitch*.deb ./python3-opensnitch-ui*.deb</p>
</blockquote>
<p dir="auto"><h4 tabindex="-1" dir="auto">rpm</h4><a id="user-content-rpm" aria-label="Permalink: rpm" href="#rpm"></a></p>
<blockquote>
<p dir="auto">$ sudo yum localinstall opensnitch-1*.rpm; sudo yum localinstall opensnitch-ui*.rpm</p>
</blockquote>
<p dir="auto">Then run: <code>$ opensnitch-ui</code> or launch the GUI from the Applications menu.</p>
<p dir="auto">Please, refer to <a href="https://github.com/evilsocket/opensnitch/wiki/Installation">the documentation</a> for detailed information.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">OpenSnitch in action</h2><a id="user-content-opensnitch-in-action" aria-label="Permalink: OpenSnitch in action" href="#opensnitch-in-action"></a></p>
<p dir="auto">Examples of OpenSnitch intercepting unexpected connections:</p>
<p dir="auto"><a href="https://github.com/evilsocket/opensnitch/discussions/categories/show-and-tell">https://github.com/evilsocket/opensnitch/discussions/categories/show-and-tell</a></p>
<p dir="auto">Have you seen a connection you didn't expect? <a href="https://github.com/evilsocket/opensnitch/discussions/new?category=show-and-tell">submit it!</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">In the press</h2><a id="user-content-in-the-press" aria-label="Permalink: In the press" href="#in-the-press"></a></p>
<ul dir="auto">
<li>2017 <a href="https://twitter.com/pentestmag/status/857321886807605248" rel="nofollow">PenTest Magazine</a></li>
<li>11/2019 <a href="https://itsfoss.com/opensnitch-firewall-linux/" rel="nofollow">It's Foss</a></li>
<li>03/2020 <a href="https://www.linux-magazine.com/Issues/2020/232/Firewalld-and-OpenSnitch" rel="nofollow">Linux Format #232</a></li>
<li>08/2020 <a href="https://linux-magazine.pl/archiwum/wydanie/387" rel="nofollow">Linux Magazine Polska #194</a></li>
<li>08/2021 <a href="https://github.com/evilsocket/opensnitch/discussions/631" data-hovercard-type="discussion" data-hovercard-url="/evilsocket/opensnitch/discussions/631/hovercard">Linux Format #280</a></li>
<li>02/2022 <a href="https://www.linux-community.de/magazine/linuxuser/2022/03/" rel="nofollow">Linux User</a></li>
<li>06/2022 <a href="https://www.linux-magazine.com/Issues/2022/259/OpenSnitch" rel="nofollow">Linux Magazine #259</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Donations</h2><a id="user-content-donations" aria-label="Permalink: Donations" href="#donations"></a></p>
<p dir="auto">If you find OpenSnitch useful and want to donate to the dedicated developers, you can do it from the <strong>Sponsor this project</strong> section on the right side of this repository.</p>
<p dir="auto">You can see here who are the current maintainers of OpenSnitch:
<a href="https://github.com/evilsocket/opensnitch/commits/master">https://github.com/evilsocket/opensnitch/commits/master</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributors</h2><a id="user-content-contributors" aria-label="Permalink: Contributors" href="#contributors"></a></p>
<p dir="auto"><a href="https://github.com/evilsocket/opensnitch/graphs/contributors">See the list</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Translating</h2><a id="user-content-translating" aria-label="Permalink: Translating" href="#translating"></a></p>
<a href="https://hosted.weblate.org/engage/opensnitch/" rel="nofollow">
<img src="https://camo.githubusercontent.com/75c6d0a4c406c1a7802bb3b7283238c3df71084751e7aa00f9822d203876e903/68747470733a2f2f686f737465642e7765626c6174652e6f72672f776964676574732f6f70656e736e697463682f2d2f676c6f73736172792f6d756c74692d6175746f2e737667" alt="Translation status" data-canonical-src="https://hosted.weblate.org/widgets/opensnitch/-/glossary/multi-auto.svg">
</a>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deep Live Cam: Real-time face swapping and one-click video deepfake tool (132 pts)]]></title>
            <link>https://deeplive.cam</link>
            <guid>41209181</guid>
            <pubDate>Sat, 10 Aug 2024 13:05:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deeplive.cam">https://deeplive.cam</a>, See on <a href="https://news.ycombinator.com/item?id=41209181">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="hero"><h2>Deep Live Cam<br> <!-- -->The Next Leap in Real-Time Face Swapping and Video Deepfake Technology</h2><p>Deep Live Cam harnesses cutting-edge AI to push the boundaries of real-time face swapping and video deepfakes.<br> <!-- -->Achieve high-quality face replacement with just a single image.</p></section><div><div id="features"><p><h4>editions</h4><h2>Deep Live Cam Supports Multiple Execution Platforms</h2></p></div><div><h2>Deep Live Cam: Bringing Your Ideas to Life</h2><p>Deep Live Cam is a state-of-the-art AI tool that delivers astonishingly accurate real-time face swapping and video deepfakes. Here's what sets it apart:</p><div><div><p>Swap faces in real-time using a single image, with instant preview capabilities.</p></div><div><div><p><span>One-Click Video Deepfakes</span></p></div><p>Generate high-quality deepfake videos quickly and easily with simple operations.</p></div><div><p>Run on various platforms including CPU, NVIDIA CUDA, and Apple Silicon, adapting to different hardware setups.</p></div><div><p>Built-in checks prevent processing of inappropriate content, ensuring legal and ethical use.</p></div><div><p>Leverages optimized algorithms for significantly faster processing, especially on CUDA-enabled NVIDIA GPUs.</p></div><div><p>Benefit from an active community providing ongoing support and improvements, keeping the tool at the cutting edge.</p></div></div></div><div><h2>How Deep Live Cam Works</h2><p>Deep Live Cam employs advanced AI algorithms to achieve real-time face swapping and video deepfakes.</p></div><section><h2>What Users Are Saying About Deep Live Cam on X</h2><p>Explore real experiences and creations shared by developers and users on X. See how Deep Live Cam is inspiring creativity and solving practical problems across various fields, from stunning face-swap effects to innovative applications.</p></section><div id="faq"><div><h2>Frequently Asked Questions About Deep Live Cam</h2><p>Get answers to common questions about Deep Live Cam</p></div><div><div><h3>What is Deep Live Cam?</h3><p>Deep Live Cam is an open-source tool for real-time face swapping and one-click video deepfakes. It can replace faces in videos or images using a single photo, ideal for video production, animation, and various creative projects.</p><hr></div><div><h3>What are the main features of Deep Live Cam?</h3><p>Deep Live Cam's key features include: 1) Real-time face swapping; 2) One-click video deepfakes; 3) Multi-platform support; 4) Ethical use safeguards.</p><hr></div><div><h3>How do I use Deep Live Cam?</h3><p>To use Deep Live Cam: 1) Set up the required environment; 2) Clone the GitHub repository; 3) Download necessary models; 4) Install dependencies; 5) Run the program; 6) Select source image and target; 7) Start the face-swapping process.</p><hr></div><div><h3>Which platforms does Deep Live Cam support?</h3><p>Deep Live Cam supports various execution platforms, including CPU, NVIDIA CUDA, Apple Silicon (CoreML), DirectML (Windows), and OpenVINO (Intel). Users can choose the optimal platform based on their hardware configuration.</p><hr></div><div><h3>How does Deep Live Cam prevent misuse?</h3><p>Deep Live Cam incorporates built-in checks to prevent processing of inappropriate content (e.g., nudity, violence, sensitive material). The developers are committed to evolving the project within legal and ethical frameworks, implementing measures like watermarking outputs when necessary to prevent abuse.</p><hr></div><div><h3>Is Deep Live Cam free to use?</h3><p>Yes, Deep Live Cam is an open-source project and completely free to use. You can access the source code on GitHub and use it freely.</p><hr></div><div><h3>Can I use Deep Live Cam for commercial purposes?</h3><p>While Deep Live Cam is open-source, for commercial use, you should carefully review the project's license terms. Additionally, using deepfake technology may involve legal and ethical considerations. We recommend consulting with legal professionals before any commercial application.</p><hr></div><div><h3>What are the hardware requirements for Deep Live Cam?</h3><p>Deep Live Cam's performance varies with hardware configuration. Basic functionality runs on standard CPUs, but for optimal performance and results, we recommend using CUDA-enabled NVIDIA GPUs or devices with Apple Silicon chips.</p><hr></div><div><h3>Does Deep Live Cam support real-time video stream processing?</h3><p>Yes, Deep Live Cam supports real-time video stream processing. You can use a webcam for real-time face swapping, with the program providing live preview functionality.</p><hr></div><div><h3>How can I improve the face-swapping results in Deep Live Cam?</h3><p>To enhance face-swapping results, try: 1) Using high-quality, clear source images; 2) Choosing source and target images with similar angles and lighting; 3) Adjusting program parameters; 4) Running the program on more powerful hardware.</p><hr></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A wonderful coincidence or an expected connection: why π² ≈ g (254 pts)]]></title>
            <link>https://roitman.io/blog/91</link>
            <guid>41208988</guid>
            <pubDate>Sat, 10 Aug 2024 12:24:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://roitman.io/blog/91">https://roitman.io/blog/91</a>, See on <a href="https://news.ycombinator.com/item?id=41208988">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Let’s take a brief trip back to our school years and recall some lessons in mathematics and physics. Do you remember what the number π equals? And what is π squared? That’s a strange question too. Of course, it’s 9.87. And do you remember the value of the acceleration due to gravity, g? Of course, that number was drilled into our memory so thoroughly that it’s impossible to forget: 9.81 m/s². Naturally, it can vary, but for solving basic school problems, we typically used this value.</p><p>August 9, 2024</p></div><p><span><h2><strong>Mysterious equality</strong></h2><p>And now, here’s the next question: how on earth is it that π² is approximately equal to g? You might say that such questions aren’t asked in polite society. First of all, they aren’t exactly equal. There’s already a difference in the second decimal place. Secondly, π is a dimensionless number, while g is a physical quantity with its own units.</p><p>And yet, no matter how you look at it, this can’t just be a simple coincidence.</p><h2><strong>Not as simple as it seems</strong></h2><p>Let's start by taking a close look at the right side. The value 9.81 is in m/s². But these are far from the only units of measurement. If you express this value in any other units, the magic immediately disappears. So, this is no coincidence—let's dig deeper into the meters and seconds.</p><p>What exactly is a "meter," and how could it be related to π? At first glance, not at all. According to Wikipedia, a "meter is the distance light travels in a vacuum during a time interval of 1/299,792,458 seconds." Great, now we have seconds involved—good! But there's still nothing about π.</p><p>Wait a minute, why exactly 1/299,792,458? Why not, for example, 1/300? Where did this number come from in the first place? It seems we need to delve into the history of the unit of length itself to understand this better.</p><h2>A standard for every honest merchant</h2><p>In the past, people didn't bother much with standards: they only cared about what was convenient for measurement. For example, why not measure length in human cubits? It might not be precise, but it was cheap, reliable, and practical. And the fact that everyone's cubits were of different lengths? Sometimes that was even useful. If you needed to buy more cloth, you'd call the tallest person in the village and have them measure the fabric with their cubits.</p><p>Later on, of course, people began thinking about standardization. They started creating various standards. But this turned out to be inconvenient and cumbersome: you couldn't always run to a single standard for measurement. So, copies of the standards began to appear. And then copies of the copies...</p><p>Serious people decided that such chaos was hindering serious business, so they set a goal: to come up with a definition of a unit of length that wouldn't depend on any arbitrary standards. It should only depend on natural constants, so that anyone with some basic tools could reproduce and measure it.</p><h2>Bright dreams of standardization and insidious gravity</h2><p>A "standard-free" definition for the meter was actually proposed back in the 17th century. The Dutch mechanic, physicist, mathematician, astronomer, and inventor Christiaan Huygens suggested using a simple pendulum for this purpose. You take a small object and suspend it on a string. The length of the string should be such that the pendulum completes a full oscillation (returns to its original position) in exactly two seconds. This length of string was called the "universal measure" or the "Catholic meter." This length differed from the modern meter by about half a centimeter.</p><p>The proposal was well-received and adopted. However, problems soon arose. First, Huygens was dealing with what he called a "mathematical pendulum." This is a "material point suspended on a weightless, inextensible string." A material point and a weightless string are hardly the simple tools that every merchant would have on hand.</p><p>Second, it was quickly discovered that the length of the pendulum's string varied in different parts of the Earth. Gravity cunningly decreased as one approached the equator and did not cooperate with humanity's bright dream of standardization.</p><h2>An astonishing equation</h2><p>But let’s return to our mysterious equation. To find the period of small oscillations of a mathematical pendulum as a function of the length of the suspension, the following formula is used:</p><figure><img src="https://nkjhvudpdnbuifryqtzj.supabase.co/storage/v1/object/public/pictures/public/e6232f8b-513a-46c4-a056-a0537b26f421"><figcaption></figcaption></figure><p>And here it is—our π! Let's substitute the parameters of Huygens’ pendulum into this formula. The length of the string l in Huygens' pendulum equals 1. The T - oscillation equals 2. Plugging these values into the formula, we get π²=g.</p><p>So, have we found the answer to our question? Well, not quite. We already saw that the equality is only approximate. It doesn’t feel right to equate 9.87 and 9.81 exactly. Does this mean that the meter has changed since then?</p><h2>With revolutionary greetings from France</h2><p>Yes, indeed, it did change! This occurred during the reform of the units of measurement initiated by the French Academy of Sciences in 1791. Intelligent people suggested maintaining the definition of the meter through the pendulum, but with the clarification that it should specifically be a French pendulum—at the latitude of 45° N (approximately between Bordeaux and Grenoble).</p><p>However, this did not sit well with the commission in charge of the reform. The problem was that the head of the commission, Jean-Charles de Borda, was a fervent supporter of transitioning to a new (revolutionary) system of angle measurement—using grads (a grad being one-hundredth of a right angle). Each grad was divided into 100 minutes, and each minute into 100 seconds. The method of the seconds pendulum did not fit into this neat concept.</p><h2>The true and final meter</h2><p>In the end, they successfully got rid of the seconds and defined the meter as one forty-millionth of the Paris meridian. Or, alternatively, as one ten-millionth of the distance from the North Pole to the equator along the surface of the Earth’s ellipsoid at the longitude of Paris. This measurement slightly differed from the "pendulum" meter. The commission, without false modesty, dubbed the resulting value as the "true and final meter."</p><p>The idea of a universal standard accessible to everyone waved goodbye and faded into the sunset. Need an accurate standard for the meter? No problem! All you have to do is measure the length of a meridian and divide it by a few million. By the way, the French actually did this—they physically measured a portion of the Paris meridian, the arc from Dunkirk to Barcelona. They laid out a chain of 115 triangles across France and part of Spain. Based on these measurements, they created a brass standard. Incidentally, they made a mistake—they didn't account for the Earth's polar flattening.</p><h2><strong>Conclusion</strong></h2><p>Let's return to our equation once again. Now we know where the inaccuracy comes from: π² and g differ by about 0.06. If it weren't for yet another attempt to reform and improve everything, we would now have a slightly different value for the meter and the elegant equation π² = g. Later, scientists did return to defining the meter through unchanging and reproducible natural constants, but the meter standard was no longer the same.</p></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ladybird browser to start using Swift language this fall (169 pts)]]></title>
            <link>https://twitter.com/awesomekling/status/1822236888188498031</link>
            <guid>41208836</guid>
            <pubDate>Sat, 10 Aug 2024 11:52:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/awesomekling/status/1822236888188498031">https://twitter.com/awesomekling/status/1822236888188498031</a>, See on <a href="https://news.ycombinator.com/item?id=41208836">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Susan Wojcicki has died (150 pts)]]></title>
            <link>https://www.facebook.com/share/p/qe2ZMcs9Bz4K1SPt/</link>
            <guid>41207446</guid>
            <pubDate>Sat, 10 Aug 2024 05:07:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.facebook.com/share/p/qe2ZMcs9Bz4K1SPt/">https://www.facebook.com/share/p/qe2ZMcs9Bz4K1SPt/</a>, See on <a href="https://news.ycombinator.com/item?id=41207446">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Susan Wojcicki has died (589 pts)]]></title>
            <link>https://twitter.com/sundarpichai/status/1822132667959386588</link>
            <guid>41207415</guid>
            <pubDate>Sat, 10 Aug 2024 04:58:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/sundarpichai/status/1822132667959386588">https://twitter.com/sundarpichai/status/1822132667959386588</a>, See on <a href="https://news.ycombinator.com/item?id=41207415">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Defcon stiffs badge HW vendor, drags FW author offstage during talk (440 pts)]]></title>
            <link>https://twitter.com/mightymogomra/status/1822119942281650278</link>
            <guid>41207221</guid>
            <pubDate>Sat, 10 Aug 2024 03:59:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/mightymogomra/status/1822119942281650278">https://twitter.com/mightymogomra/status/1822119942281650278</a>, See on <a href="https://news.ycombinator.com/item?id=41207221">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Caltech Develops First Noninvasive Method to Continually Measure Blood Pressure (118 pts)]]></title>
            <link>https://www.caltech.edu/about/news/caltech-team-develops-first-noninvasive-method-to-continually-measure-true-blood-pressure</link>
            <guid>41207182</guid>
            <pubDate>Sat, 10 Aug 2024 03:53:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.caltech.edu/about/news/caltech-team-develops-first-noninvasive-method-to-continually-measure-true-blood-pressure">https://www.caltech.edu/about/news/caltech-team-develops-first-noninvasive-method-to-continually-measure-true-blood-pressure</a>, See on <a href="https://news.ycombinator.com/item?id=41207182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p data-block-key="qqzwe">Solving a decades-old problem, a multidisciplinary team of Caltech researchers has figured out a method to noninvasively and continually measure blood pressure anywhere on the body with next to no disruption to the patient. A device based on the new technique holds the promise to enable better vital-sign monitoring at home, in hospitals, and possibly even in remote locations where resources are limited.</p><p data-block-key="dk5fr">The new patented technique, called resonance sonomanometry, uses sound waves to gently stimulate resonance in an artery and then uses ultrasound imaging to measure the artery's resonance frequency, arriving at a true measurement of blood pressure. In a small clinical study, the device, which gives patients a gentle buzzing sensation on the skin, produced results akin to those obtained using the standard-of-care blood pressure cuff.</p><p data-block-key="970vo">"We ended up with a device that is able to measure the absolute blood pressure—not only the systolic and diastolic numbers that we are used to getting from blood pressure cuffs—but the full waveform," says <a href="https://www.eas.caltech.edu/people/yaser">Yaser Abu-Mostafa</a> (PhD '83), professor of electrical engineering and computer science and one of the authors of a <a href="https://academic.oup.com/pnasnexus/article-lookup/doi/10.1093/pnasnexus/pgae252">new paper</a> describing the technique and device in the journal <i>PNAS Nexus</i>. "With this device you can measure blood pressure continuously and in different sites on the body, giving you much more information about the blood pressure of a person."</p><p data-block-key="2mnhc">"This team has been working for almost a decade, trying to build something that makes a difference, that is good enough to solve a real clinical problem," says Aditya Rajagopal (BS '08, PhD '14), visiting associate in electrical engineering at Caltech, research adjunct assistant professor of biomedical engineering at USC, and a co-author of the new paper. "Many groups, including tech giants like Apple and Google, have been working toward a solution like this, because it enables a spectrum of patient-monitoring possibilities from the hospital to the home. Our method broadens access to hospital-grade monitoring of blood pressure and cardiac health metrics."</p><p data-block-key="dj1o6"><b>Blood pressure 101</b></p><p data-block-key="7dki3">Blood pressure is simply the force of blood pushing on the walls of the body's blood vessels as it gets pumped around the body. High blood pressure, or hypertension, is related to risk of heart attack, stroke, chronic kidney disease, and other health problems. Low blood pressure, or hypotension, can also be a serious problem because it means the blood is not carrying enough oxygen to the organs. Taking regular measurements of blood pressure is considered one of the best ways to monitor overall health and to identify potential problems.</p><p data-block-key="62t65">Most of us have experienced the cuff-style measurement of blood pressure. A nurse, doctor, or machine inflates a cuff that fits around the upper arm until blood can no longer flow, and then slowly releases the air from the cuff while listening for the sound that blood makes as it once again begins to flow. The pressure in the cuff at that point corresponds to the blood pressure in the patient's arteries. But this technique has limitations: It can only be performed periodically, as it involves occluding a blood vessel, and can only collect data from the arm.</p><p data-block-key="a2ou5">Physicians would very much like to have continuous readings that provide full waveforms of a patient's blood pressure, and not only peripheral measurements from an arm but also central measurements from the chest and other parts of the body. To get the full information they need, intensive care physicians and surgeons sometimes resort to inserting a catheter directly into the artery of critical patients (a practice known as placing an arterial line, or "a-line"). This is invasive and can be risky, but, until now, it has been the only way to get a continuous readout of true blood pressure. In some cases, such as problems with heart valves, full blood-pressure waveforms can provide physicians with diagnostic information that they cannot get any other way.</p><p data-block-key="6gtjj">"There's a lot of information in that waveform that is really valuable," says Alaina Brinley Rajagopal, a visiting associate in electrical engineering at Caltech, an emergency medicine physician, and a co-author of the paper. And other blood pressure devices developed over the last decade or two require a calibration step that emergency physicians simply do not have time for, she says. "I need to be able to put something on a patient and have it work immediately."</p><p data-block-key="camvc">The new device fits the bill. The current prototype, built and tested by a spin-off company called Esperto Medical, is housed in a transducer case smaller than a deck of cards and is mounted on an armband, though the researchers say it could eventually fit within a package the size of a watch or adhesive patch. The team aims for the device to first be used in hospitals, where it would connect via wire to existing hospital monitors. It could mean that doctors would no longer have to weigh the risks of placing an a-line in order to get the continuous monitoring of real blood pressure for any patient.</p><p data-block-key="fqe92">Eventually, Brinley says their device could replace blood pressure cuffs as well. "Blood pressure cuffs only take one measurement as often as you run the cuff, so if you're asking patients to monitor their blood pressure at home, they have to know how to use the device, they have to put it on, and they have to be motivated to record the information, and I would say a majority of patients do not do that," says Brinley Rajagopal. "Having a device like ours, where it is just place and forget, you can wear it all day, and it can take however many measurements your provider wants, that would allow for better, precision dosing of medication."</p><p data-block-key="6c36e"><b>Developing a game changer</b></p><p data-block-key="eib58">Rajagopal recalls the long road it has been getting to this point with the blood pressure device. About a decade ago, Brinley Rajagopal returned from a global health trip particularly frustrated by the standard of care she could provide patients in remote locations. Talking with Rajagopal, the two wished they could invent something like a medical tricorder, a handheld device seen in <i>Star Trek</i> that helped the fictional doctors of the future scan patients, gather medical information, and diagnose. "That got us thinking about technologies we could adapt to get us closer to a goal like that," says Brinley Rajagopal. Those initial sci-fi–inspired discussions eventually led them down the path to try to develop a better blood pressure monitor.</p><p data-block-key="1cqst">But their first efforts did not pan out. After years of work on a possible solution using blood velocity to derive blood pressure, the team decided that they had reached a dead end. As with many other current blood pressure monitoring devices, that approach could only provide the <i>relative</i> blood pressure—the difference between the high and low measurements without the absolute number. It also required calibration.</p><p data-block-key="dlbkg"><b>Back to the drawing board</b></p><p data-block-key="8gfok">Rajagopal decided it was time to reevaluate and determine if they had any chance of solving this problem. "It was this moment of desperation that actually led to the key insight," says Rajagopal.</p><p data-block-key="1si80">Thinking back to his first-year physics course at Caltech, he began scribbling on a nearby wall. He remembered that his Physics1 textbook presented a canonical problem: You have a string under tension. How can you determine how taut the line is? If you tweeze the string, you can relate the velocity at which vibration waves travel back and forth on the string to the resonance frequency in the string, which could give you your answer. "I thought if I could stretch an artery in one direction and magically tweeze it and let it go, the ringing would give us the resonance frequency, which would get us to blood pressure," says Rajagopal. After six years of failures and returning to first principles, they finally had their guiding insight.</p><p data-block-key="dort6">And indeed, that is the underlying idea behind the new device: Like a guitar changing pitch as it is plucked while being tightened, the frequency at which an artery resonates when struck by sound waves changes depending on the pressure of the blood it contains.</p><p data-block-key="4803d">This resonance frequency can be measured with ultrasound, providing a measure of blood pressure. This measurement requires three parameters—a measurement of the artery's radius, the thickness of the artery's walls, and the tension or energy in the skin of the artery.</p><p data-block-key="8tptm">With the physics worked out, there were still a lot of other details to be resolved—identifying the sound waves that would make arteries resonate, understanding how to measure that resonance, and then determining how to efficiently map that back to blood pressure, and, significantly, how to build a working system.</p><p data-block-key="6pmif">"Building that system required some extraordinarily bespoke technologies," says Rajagopal. Caltech alumnus Raymond Jimenez (BS '13) was instrumental in building out that first system. "The art form, which involved a lot of other Caltech alumni, was to put the physics answer into a very simple, practical instrument."</p><p data-block-key="e1p0o">The resulting Esperto device is small, noninvasive, relatively inexpensive, and it has an automated method for locating the patient's blood vessel without needing to be physically repositioned. It also does not suffer from the problems that some blood pressure monitoring devices have, such as not being accurate for patients with low blood pressure or getting varying results depending on a patient's skin tone.</p><p data-block-key="ck0n8">It might not be a medical tricorder, but the team says the device solves the longstanding blood pressure monitoring problem. And Rajagopal says it is the product of a million small leaps. "Everything we've done is a product of the exact mistakes we've made over time," he says, "and all the work that others have done too."</p><p data-block-key="81uf3">"This work is emblematic of what makes Caltech so remarkable: solving a very hard problem by going back to first principles and understanding a physical phenomenon at the fundamental level," says Fred Farina, Caltech's Chief Innovation and Corporate Partnerships Officer. "This approach, combined with the tenacity and entrepreneurial drive of the team, is our homemade recipe for societal impact and improving people's lives."</p><p data-block-key="3vl0c">The paper describing the new technique is titled "Resonance sonomanometry for noninvasive, continuous monitoring of blood pressure." Additional authors on the paper include Raymond Jimenez (BS '13), Steven Dell, Austin C. Rutledge, Matt K. Fu (BS '13), and William P. Dempsey (PhD '12) of Esperto Medical, and Dominic Yurk (BS '17, PhD '23), a current member of Abu-Mostafa's group at Caltech. The work at Caltech was supported by Caltech trustee Charles Trimble (BS '63, MS '64), the Carver Mead Innovation Fund, and the Grubstake Fund.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a highly-available web service without a database (213 pts)]]></title>
            <link>https://blog.screenshotbot.io/2024/08/10/building-a-highly-available-web-service-without-a-database/</link>
            <guid>41206908</guid>
            <pubDate>Sat, 10 Aug 2024 02:37:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.screenshotbot.io/2024/08/10/building-a-highly-available-web-service-without-a-database/">https://blog.screenshotbot.io/2024/08/10/building-a-highly-available-web-service-without-a-database/</a>, See on <a href="https://news.ycombinator.com/item?id=41206908">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>If you’ve ever built a web service or a web app, you know the drill: pick a database, pick a web service framework (and in today’s day and age, pick a front-end framework, but let’s not get into that).</p>



<p>This has been the case for several decades now, and people don’t stop to question if this is still the best way to build a web app. Many things have changed in the last decade:</p>



<ul>
<li>Disk is a lot faster (NVMe)</li>



<li>Disk is a lot more robust (EBS/EFS etc.)</li>



<li>RAM is super cheap, for most startups you could probably fit all your data in RAM</li>



<li>You can rent a machine with hundreds of cores if your heart desires.</li>
</ul>



<p>This was not the case when I first worked at a Rails startup in 2010. But most importantly, there’s one new very important change that’s happened in the last decade:</p>



<ul>
<li>The <a href="https://en.wikipedia.org/wiki/Raft_(algorithm)">Raft Consensus algorithm</a> was published in 2014 with many robust implementations easily available.</li>
</ul>



<p>In this blog post, we’re going to break down a new architecture for web development. We use it successfully for <a href="https://screenshotbot.io/">Screenshotbot</a>, and we hope you’ll use it too.</p>



<p>I’ll break this blog post into three parts: Explore, Expand and Extract, obviously referencing <a href="https://tidyfirst.substack.com/">Kent Beck</a>‘s 3X. Your needs are going to vary in each of these stages of your startup, and I’m going to demonstrate how you use the architecture in all three phases.</p>



<h2>Explore</h2>



<p>So you’re a new startup. You’re iterating on a product, you have no idea how people are going to use it, or even <em>if</em> they’re going to use it.</p>



<p>For most startups today, this would mean you’ll pick Rails or Django or Node or some such, backed with a MySQL or PostgreSQL or MongoDB or some such.</p>



<p>“<em>Keep it simple silly</em>,” you say, and this seems simple enough.</p>



<p>But is this as simple as possibly can be? Could we make it simpler? What if the web service and the database instance were exactly one and the same? I’m not talking about using something like SQLite where your data is still serialized, I’m saying what if all the memory in your RAM <em>is</em> your database.</p>



<p>Imagine all the wonderful things you could build if you never had to serialize data into SQL queries. First, you don’t need multiple front-end servers talking to a single DB, just get a bigger server with more RAM and more CPU if you need it. What about indices? Well, you can use in-memory indices, effectively just hash-tables to lookup objects. You don’t need clever indices like B-tree that are optimized for disk latency. (In fact, you can use some indices that were probably not possible with traditional databases. <a href="https://blog.screenshotbot.io/2023/10/29/scaling-screenshotbot/">One such index</a> using functional collections was critical to the scalability of Screenshotbot.)</p>



<p>You also won’t need special architectures to reduce round-trips to your database.  In particular, you won’t need any of that Async-IO business, because your threads are no longer IO bound. Retrieving data is just a matter of reading RAM. Suddenly debugging code has become a lot easier too.</p>



<p>You don’t need any services to run background jobs, because background jobs are just threads running in this large process.</p>



<p>You don’t need crazy concurrency protocols, because most of your concurrency requirements can be satisfied with simple in-memory mutexes and condition variables.</p>



<p>But then comes the important part: how do you recover when your process crashes? It turns out that answer is easy, periodically just take a snapshot of everything in RAM.</p>



<p>Hold on, what if you’ve made changes since the last snapshot? And this is the clever bit: you ensure that every time you change parts of RAM, we write a transaction to disk. So if you have a line like <code>foo.setBar(2)</code>, this will first write a transaction that says we’ve changed the <code>bar</code> field of <code>foo</code> to 2, and then actually set the field to 2. An operation like <code>new Foo()</code> writes a transaction to disk to say that a Foo object was created, and then returns the new object.</p>



<p>And so, if your process crashes and restarts, it first reloads the snapshot, and replays the transaction logs to fully recover the state. (Notice that index changes don’t need to be part of the transaction log. For instance if there’s an index on field <code>bar</code> from <code>Foo</code>, then <code>setBar</code> should just update the index, which will get updated whether it’s read from a snapshot, or from a transaction.)</p>



<p>Finally, this architecture enables some new kind of code that couldn’t be written before. Since all requests are being served by the same process, which <em>usually</em> doesn’t get killed, it means you can store closures in memory that can be used to serve pages. For example on Screenshotbot, if you ever see a “<a href="https://screenshotbot.io/n/" rel="nofollow">https://screenshotbot.io/n/</a><em>nnnnnnn</em>” URL, it’s actually a closure on the server, where <em>nnnnnnn</em> maps to an internal closure. But amazingly, this simple change means we don’t need to serialize objects across page transitions. The closure has references to the objects, so we don’t need to pass around object-ids across every single request. In Javascript, this might hypothetically look like:</p>



<pre><code>function renderMyObject(obj) {
   return &lt;html&gt;...
            &lt;a href=(() =&gt; obj.delete()) &gt;Delete&lt;/a&gt;
            ...
          &lt;/html&gt;
} </code></pre>



<p>What this all means is that you can iterate quickly. If you have to debug, there’s exactly one service that you need to debug. If you need to profile code, there’s exactly one service you need to profile (no more MySQL slow query logs). There’s exactly one service to monitor: if that one service goes down the site certainly goes down, but since there’s only one service and one server, the probability of failure is also much lower. If the server dies, AWS will automatically bring up a new server to replace it within a few minutes.</p>



<p>It’s also a lot easier to write test code, since you no longer have to mock out databases.</p>



<h2>Expand</h2>



<p>So you’re moving fast, iterating, and building out ideas, and slowly getting customers along the way.</p>



<p>Then one day, you get a high-profile customer. Bingo, you’re now in the <em>Expand</em> phase of your startup.</p>



<p>But there’s a catch: this high-profile customer requires 99.999% availability. </p>



<p>Surely, the architecture we just described cannot handle this. If the server goes down, we would need to wait several minutes for AWS to bring it back up. Once it’s back up, we might wait several minutes for our process to even restore the snapshot from disk. Even re-deploys are tricky: restarting the service can bring down the server for multiple minutes.</p>



<p>And this is where the Raft Consensus Protocol comes in to place. </p>



<p>Raft is a wonderful algorithm and protocol. It takes your <em>finite state machine</em> (your web server/database), and essentially replicates the transaction log. So now, we can take our very simple architecture and replicate it across three machines. If the leader goes down, then a new leader is elected within seconds and will continue to serve requests.</p>



<p>We’ve just made our simple little service into a full-fledged highly-available database, without fundamentally changing how developers write code.</p>



<p>With this mechanism, you can also do a rolling deploy without ever bringing the server down. (Although we rarely restart our server processes, more on that in a moment.) Because there’s just one service, it’s also easy to calculate your availability guarantees.</p>



<h2>Extract</h2>



<p>So your startup is doing well, and you have thousands of large customers.</p>



<p>To be honest, Screenshotbot is not at this stage, I’ll talk about where we are in a moment. But we’re preparing for this possibility, with monitoring in place for predicted bottlenecks. </p>



<p>The solution here is something large companies already do with their databases: sharding. You can break up your web services into shards, each shard being its own cluster. In particular, at Screenshotbot we already do this: each of our enterprise customers get their own dedicated cluster. (Fun story: Meta <a href="https://engineering.fb.com/2023/05/16/data-infrastructure/mysql-raft-meta/">switched to Raft</a> to handle replication for each of its MySQL clusters, so we’re essentially doing the same thing but without using a separate database.)</p>



<p>I don’t know what else to expect, since I’m more of a solve-today’s-problem kind of person. The main bottleneck I expect to see is scaling the commit-thread. The read threads parallelize beautifully. There’s one commit-thread that’s applying each transaction one at a time. It turns out the disk latency is irrelevant for this, since the Raft algorithm will just commit multiple transactions together to disk. My main concern is that the CPU cost for applying the transactions will exceed the single core performance. I highly doubt that I would ever see this, but it’s a possibility. At this point we could profile the cost of commits and improve it (for instance, move some of the work out of the transaction thread),  or we could just figure out sharding. I’ll probably write another blog post when that happens.</p>



<h2>Our Stack</h2>



<p>Now that I’ve described the idea to you, let me tell you about our stack, and why it turned out to be so suitable for this architecture.</p>



<p>We use Common Lisp. My initial implementation of Screenshotbot did use MySQL, but I quickly swapped it out for <a href="https://github.com/bknr-datastore/bknr-datastore">bknr.datastore</a> exactly because handling concurrency with MySQL was hard and Screenshotbot is a highly concurrent app. BKNR Datastore is a library that handles the architecture described in the <em>Explore</em> section, but built for Common Lisp. (There are similar libraries for other languages, but not a whole lot of them.)</p>



<p>Common Lisp is also heavily multi-threaded, and this is going to be crucial for this architecture since your web requests are being handled by threads in a single process. Ruby or Python would be disqualified by this requirement.</p>



<p>We also use the idea of closures that I mentioned earlier. But this means we can’t keep restarting the server frequently (if you restart the server, you lose the closures). So reloading code is just hot-reloading code in the running process. It turns out Common Lisp is excellent at this: so much so that a large part of the standard is all about handling reloading code. (For instance, if the class definition changes, how do you update objects of that class? <a href="http://clhs.lisp.se/Body/f_reinit.htm#reinitialize-instance">There’s a standard for it</a>.)</p>



<p>Occasionally, we do restart the servers. Currently, it looks like we only restart the servers about once every month or two months. When we need to do this, we just do a rolling restart with our Raft cluster. We use a cluster of 3 servers per installation, which allows for one server to go down. We don’t use Kubernetes, we don’t need it (at least, not yet).</p>



<p>For the Raft implementation, we wrote our own custom library built on top of bknr.datastore. We built and open-sourced <a href="https://github.com/tdrhq/bknr.cluster">bknr.cluster</a>, that under the hood uses the fantastic <a href="https://github.com/baidu/braft">Braft</a> library from Baidu. Braft is super solid, and I can highly recommend it. Braft also handles background snapshots, which means while we’re taking snapshots, the server can still continue serving requests.</p>



<p>To store image files, or blobs that shouldn’t be part of the datastore, we use EFS (a highly available NFS) that is shared between the three servers. EFS is easier to work with than S3, because we don’t have to handle error conditions. EFS also makes our code more testable, since we aren’t interacting with an external server, and just writing to disk.</p>



<p><strong>How well does this scale?</strong> We have a couple of big enterprise customers, but one especially well-known customer. Screenshotbot runs on their CI, so we get API requests 100s of times for every single commit and Pull Request. Despite this, we only need a 4-core 16GB machine to serve their requests. (And similar machines for the replicas, mostly running idle.) Even with this, the CPU usage maxes out at 20%, but even then most of that comes from image processing, so we have a lot of room to scale before we need to bump up the number of cores.</p>



<h2>Summary</h2>



<p>I think this architecture is excellent for new startups, and I’m hoping more companies will adopt it. Obviously, you’ll need to build out some of the tooling we’ve built out for the language of your choice. (Although, if you choose to use Common Lisp, it’s all here for you to use, and all open-source.)</p>



<p>We’re super grateful to the folk behind bknr.datastore, Braft and Raft, because without their work we wouldn’t be able to do any of this.</p>



<p>If you think this was useful or interesting, I would appreciate it if you could share it on social media. Please reach out to me at <a href="mailto:arnold@screenshotbot.io">arnold@screenshotbot.io</a> if you have any questions.</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rivian reduced electrical wiring by 1.6 miles and 44 pounds (147 pts)]]></title>
            <link>https://www.popsci.com/technology/rivian-zonal-electrical-architecture/</link>
            <guid>41206443</guid>
            <pubDate>Sat, 10 Aug 2024 00:12:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.popsci.com/technology/rivian-zonal-electrical-architecture/">https://www.popsci.com/technology/rivian-zonal-electrical-architecture/</a>, See on <a href="https://news.ycombinator.com/item?id=41206443">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-toc-container="">
			
<p>Unveiled just a couple of months ago, Rivian’s second-generation R1T pickup and R1S SUV will maintain their distinctive look, with playful headlamps and a sleek exterior shape. Underneath the surface is where the magic is taking place, specifically a wholly new electrical architecture the brand says is less costly and easier to service.</p>



<p><a rel="nofollow noreferrer" href="https://www.thedrive.com/news/the-rivian-r3s-retro-hatch-was-inspired-by-group-b-rally-legends" target="_blank">Rivian</a> senior vice president of electrical hardware Vidya Rajagopalan says the new electrical system offers more features, as well as an increase in sensing and computing capability. In the process of making the transition from Gen 1 to Gen 2 R1 vehicles, Rivian switched to a zonal architecture.</p>



<p>Ultimately, that results in a more sustainable option. The zonal approach reduces the total wiring length by a whopping 1.6 miles and enables each R1 vehicle to shed 44 pounds. Weight is a big deal for <a href="https://www.popsci.com/category/electric-vehicles/" target="_blank">EVs</a>, as it has a direct correlation to battery performance. Plus, the company claims a 20 percent savings in material costs and 15 percent reduction in its carbon footprint between Gen 1 and Gen 2.</p>



<p>All of it was developed in-house by Rivian’s hardware and software team, an impressive feat. Software complexity is a big deal, and Rivian is finding ways to simplify and streamline its golden goose as a software-defined automaker.</p>



<h2 id="h-zone-versus-domain-based-architecture"><strong>Zone versus domain-based architecture</strong></h2>



<p>In basketball, kids learn one-to-one defense early on. Each player is assigned to a competitor, and they stick to that person like glue for effective coverage. Zone defense requires each player to guard an area (zone) and any offensive player within those parameters. It’s a more elegant option, and one that requires more knowledge of the game.</p>



<p>Using a zonal approach, Rivian is showing off its technological prowess.</p>



<p>When Rivian engineers started building R1s, they designed a platform based on what’s called domain-based architecture, Rajagopalan explains. With this setup, each category of software is paired with a piece of hardware. Every time you open a door, raise the cabin temperature, slide your seats, turn on the lights, change the mode, and more, you could be connecting to different hardware. Those electrical control units, or ECUs, can multiply like Gremlins after dark.</p>



<figure><img decoding="async" width="2736" height="1324" loading="lazy" src="https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?strip=all&amp;quality=95" alt="x-ray-like image of SUV with ECU locations highlighted" srcset="https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png 2736w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1536&amp;h=743 1536w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=2048&amp;h=991 2048w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=930&amp;h=450 930w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=413&amp;h=200 413w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1364&amp;h=660 1364w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=827&amp;h=400 827w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1728&amp;h=836 1728w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1426&amp;h=690 1426w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=446&amp;h=216 446w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=835&amp;h=404 835w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1847&amp;h=894 1847w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1554&amp;h=752 1554w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1434&amp;h=694 1434w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=280&amp;h=135 280w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=1440&amp;h=697 1440w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=289&amp;h=140 289w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=370&amp;h=179 370w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=308&amp;h=149 308w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-3.png?w=50&amp;h=24 50w" sizes="(max-width: 2736px) 100vw, 2736px"><figcaption>Each R1T and R1S is lighter in its second generation due to a vast reduction in electrical control units and wiring. <em>Image: Rivian</em> </figcaption></figure>



<p>“We had 17 ECUs [in Gen 1], each dedicated to a category,” Rajagopalan says. “Other manufacturers can have between 40-150 per vehicle, depending on how they work.”</p>



<p>Even though Rivian was using significantly fewer pieces of hardware than their competitors, they wanted to improve the system. More ECUs means more parts overall; consequently, that leads to increased opportunities for failure.</p>



<p>For the second generation of vehicles, four categories get their own ECUs: infotainment, autonomy, vehicle access, drive units, and its battery management system. Every other vehicle function is controlled by just three ECUs Rivian refers to as West, East, and South. The infotainment ECU alone is as powerful as a laptop and has the capabilities of a smartphone.</p>



<figure><img decoding="async" width="2573" height="1357" loading="lazy" src="https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?strip=all&amp;quality=95" alt="x-ray-like image of SUV with wiring locations highlighted" srcset="https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg 2573w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1536&amp;h=810 1536w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=2048&amp;h=1080 2048w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=853&amp;h=450 853w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=379&amp;h=200 379w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1251&amp;h=660 1251w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=758&amp;h=400 758w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1585&amp;h=836 1585w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1308&amp;h=690 1308w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=410&amp;h=216 410w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=766&amp;h=404 766w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1695&amp;h=894 1695w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1426&amp;h=752 1426w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1316&amp;h=694 1316w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=280&amp;h=148 280w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=1440&amp;h=759 1440w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=289&amp;h=152 289w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=370&amp;h=195 370w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=308&amp;h=162 308w, https://www.popsci.com/wp-content/uploads/2024/08/Rivian-ECUs.jpeg?w=50&amp;h=26 50w" sizes="(max-width: 2573px) 100vw, 2573px"><figcaption>Switching from domain-based to zonal architecture allowed the company to reduce its complexity and improve scalability.&nbsp;<em>Image: Rivian</em> </figcaption></figure>



<p>However, the ECU that runs the vehicle’s autonomy platform is the most powerful computer in the R1S and R1T. The system includes an array of 11 internally developed cameras and five radars performing over 250 trillion operations per second, which<a rel="nofollow noreferrer" href="https://stories.rivian.com/meet-the-new-r1" target="_blank"> Rivian says is an industry-leading statistic</a>. As such, it connects to artificial intelligence, which helps identify and perceive the world in front of you to detect street signs, lanes, pedestrians, and more.</p>



<h2 id="h-reducing-parts-improves-scalability-and-reduces-cost"><strong>Reducing parts improves scalability and reduces cost</strong></h2>



<p>Fewer ECUs also results in less wiring, and another upshot of casting off ECUs is reduced weight.</p>



<p>“If your ECUs control function by function, you’ll have long wires running all over the car,” Rajagopalan says. “When you switch to a zone mindset, it’s more like a wheel-and-spoke function. Consolidation always wins; having fewer pieces reduces cost and makes manufacturing easier.”</p>



<p>It also makes room for scalability, something Rivian will need in order to grow.</p>



<p>Rajagopalan was working for Tesla as a senior director of engineering before joining Rivian at the end of 2020. Now she manages 400+ people at Rivian and has witnessed the evolution of the company. She says when Rivian first launched the first generation of its R1T and R1S, buyers loved the adventure-focused brand and the vehicles’ <a href="https://www.popsci.com/technology/rivian-r1t-r1s-handling/" target="_blank">off-road capabilities</a>. Since then, the automaker has proven it can ramp up its manufacturing capacity and make improvements.</p>



<p>With a new joint venture with Volkswagen in progress, Rivian will have the opportunity to build a common platform on a much larger scale. To start,<a rel="nofollow noreferrer" href="https://rivian.com/en-GB/newsroom/article/rivian-and-volkswagen-group-announce-plans-for-joint-venture" target="_blank"> VW will invest $1 billion in Rivian and invest another $4 billion over time</a>. In turn, VW will benefit from Rivian’s software-defined vehicle expertise and maverick approach.Meanwhile, we’ll be waiting for the <a href="https://www.popsci.com/technology/rivian-r3x-r2-2024/" target="_blank">smaller R2 and R3</a> to debut as Rivian ramps up.</p>
			
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grace Hopper, Nvidia's Halfway APU (106 pts)]]></title>
            <link>https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/</link>
            <guid>41206025</guid>
            <pubDate>Fri, 09 Aug 2024 22:52:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/">https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/</a>, See on <a href="https://news.ycombinator.com/item?id=41206025">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Nvidia and AMD are the biggest players in the high performance GPU space. But while Nvidia has a huge GPU market share advantage over AMD, the latter’s CPU prowess makes it a strong competitor. AMD can sell both a CPU and GPU as one unit, and that capability has gotten the company wins in consoles and supercomputers. Oak Ridge National Laboratory’s Frontier supercomputer is one such example. There, AMD MI250X GPUs interface with a custom EPYC server CPU via Infinity Fabric.</p>
<p>Of course, Nvidia is not blind to this situation. They too have a high speed in-house interconnect, called NVLink. Nvidia has also dabbled with bundling CPUs alongside their GPUs. The Nintendo Switch’s Tegra X1 is a prominent example. But Tegra used relatively small CPUs and GPUs to target low power mobile applications. Grace Hopper is an attempt to get Nvidia’s CPU efforts into high performance territory. Beyond providing server-level CPU core counts and memory bandwidth, Grace Hopper comes with Nvidia’s top-of-the-line H100 datacenter GPU.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30408"><img decoding="async" width="688" height="387" data-attachment-id="30408" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_press_image/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?fit=1077%2C606&amp;ssl=1" data-orig-size="1077,606" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_press_image" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?fit=1077%2C606&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?fit=688%2C387&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?resize=688%2C387&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?w=1077&amp;ssl=1 1077w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_press_image.jpg?resize=768%2C432&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>GH200 rendering from Nvidia, showing the CPU on the left and GPU on the right</figcaption></figure></div>
<p>I’ll be looking at GH200 as hosted on <a href="https://brokkr.hydrahost.com/">Hydra</a>. GH200 has several variants. The one I’m looking at has 480 GB of LPDDR5X memory on the CPU side, and 96 GB of HBM3 on the GPU side. I’ve already covered Neoverse V2 in Graviton 4, so I’ll focus on implementation differences rather than going over the core architecture again.</p>
<h2>System Architecture</h2>
<p>GH200 bundles a CPU and GPU together. The Grace CPU consists of 72 Neoverse V2 cores running at up to 3.44 GHz, supported by 114 MB of L3 cache. Cores and L3 cache sit on top of Nvidia’s Scalable Coherency Fabric (SCF). SCF is a mesh interconnect, with cores and L3 cache slices attached to mesh stops.</p>

<p>SCF’s responsibilities include ensuring cache coherency and proper memory ordering. From a core to core latency test, those requirements are satisfied with reasonably consistent latency across the mesh. Latency is generally comparable to Graviton 4’s, which uses Arm’s CMN-700 mesh interconnect.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30413"><img decoding="async" width="688" height="346" data-attachment-id="30413" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_c2c/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?fit=2945%2C1481&amp;ssl=1" data-orig-size="2945,1481" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_c2c" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?fit=2560%2C1287&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?fit=688%2C346&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=688%2C346&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?w=2945&amp;ssl=1 2945w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=768%2C386&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=1536%2C772&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=2048%2C1030&amp;ssl=1 2048w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=1200%2C603&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=1600%2C805&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?resize=1320%2C664&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_c2c.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Core to core latency test on Grace’s CPU</figcaption></figure></div>
<p>DRAM access is handled by a 480-bit LPDDR5X-6400 setup, which provides 480 GB of capacity and 384 GB/s of theoretical bandwidth. Graviton 4 opts for a 768-bit DDR5-5200 setup for 500 GB/s of theoretical bandwidth and 768 GB of capacity. Nvidia may be betting on LPDDR5X providing lower power consumption, as DRAM power can count for a significant part of a server’s power budget.</p>
<p>GH200’s H100 GPU sits next to the Grace CPU. Even though both are sold as a single unit, it’s not an integrated GPU setup because the two chips have separate memory pools. Opting against an integrated GPU is a sensible decision because CPUs and GPUs have different memory subsystem requirements. CPUs are sensitive to memory latency and want a lot of DRAM capacity. GPUs require high memory bandwidth, but are less latency sensitive. A memory setup that excels in all of those areas will be very costly. GH200 avoids trying to square the circle, and its H100 GPU comes with 96 GB of dedicated HBM3 memory. That’s good for 4 TB/s of theoretical bandwidth, far more than what the LPDDR5X setup can provide.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30418"><img loading="lazy" decoding="async" width="688" height="386" data-attachment-id="30418" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/kbl_vega_m_hotchips/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?fit=954%2C535&amp;ssl=1" data-orig-size="954,535" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="kbl_vega_m_hotchips" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?fit=954%2C535&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?fit=688%2C386&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?resize=688%2C386&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?w=954&amp;ssl=1 954w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/kbl_vega_m_hotchips.png?resize=768%2C431&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>From Intel’s Hot Chips presentation</figcaption></figure></div>
<p>Conceptually, GH200’s design is similar to Intel’s Kaby Lake CPU with AMD’s Radeon RX Vega M graphics. That design also packages the CPU and GPU together as one unit. A 4 GB pool of HBM2 memory gives the GPU high memory bandwidth, while regular DDR4 memory gives the CPU high memory capacity and low latency. GH200 of course does this on a much larger scale on both the CPU and GPU side.</p>

<p>But an integrated GPU design has benefits too, mainly allowing for faster communication between the CPU and GPU. Games don’t require much bandwidth between the CPU and GPU, as long as there’s enough VRAM to handle the game in question. But compute applications are different, and can involve frequent data exchange between the CPU and GPU. Therefore, Nvidia connects the two dies with a high bandwidth proprietary interconnect called NVLink C2C. NVLink C2C offers 900 GB/s of cross-die bandwidth, or 450 GB/s in each direction. That’s an order of magnitude faster than a PCIe Gen 5 x16 link. </p>
<p>Besides higher bandwidth, NVLink C2C has hardware coherency support. The CPU can access HBM3 memory without explicitly copying it to LPDDR5X first, and the underlying hardware can ensure correct memory ordering without special barriers. Nvidia is confident enough in their NVLink C2C implementation that HBM3 memory is directly exposed to the CPU side as a NUMA node.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30500"><img loading="lazy" decoding="async" width="688" height="505" data-attachment-id="30500" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_numa_bw-2/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?fit=789%2C579&amp;ssl=1" data-orig-size="789,579" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_numa_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?fit=789%2C579&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?fit=688%2C505&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?resize=688%2C505&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?w=789&amp;ssl=1 789w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_bw-1.png?resize=768%2C564&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Note that remote bandwidth for Grace Hopper is for CPU-owned memory allocated through standard Linux interfaces, which uses the CPU cores for all data transfer and does not make use of CUDA Copy Engines or other acceleration</figcaption></figure></div>
<p>Accessing the HBM3 memory pool across NVLink C2C provides comparable bandwidth to AMD’s current generation Zen 4 servers in a dual socket configuration. It’s a good performance, if a bit short compared to the theoretical figures. Bandwidth is still significantly higher than what AWS can achieve between two Graviton 4 chips, and shows the value of Nvidia’s proprietary interconnect. Bandwidth from Grace’s local LPDDR5X pool is also solid, and on par with AMD’s Bergamo with DDR5-4800.</p>
<p>Latency however is poor at nearly 800 ns, even when using 2 MB pages to minimize address translation penalties. That’s a difference of 592 ns compared to accessing directly attached LPDDR5X, which itself doesn’t offer particularly good latency.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30429"><img loading="lazy" decoding="async" width="688" height="520" data-attachment-id="30429" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_numa_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?fit=862%2C652&amp;ssl=1" data-orig-size="862,652" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_numa_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?fit=862%2C652&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?fit=688%2C520&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?resize=688%2C520&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?w=862&amp;ssl=1 862w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?resize=768%2C581&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_numa_latency.png?resize=200%2C150&amp;ssl=1 200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Part of this is undoubtedly because HBM isn’t a technology designed to offer good latency characteristics. But testing from the H100 GPU shows about 300 ns of DRAM latency, suggesting HBM3 latency is only a minor factor. NVLink C2C therefore appears to have much higher latency than AMD’s Infinity Fabric, or whatever Graviton 4 is using. Intel’s QPI also offers better latency.</p>
<p>To make things worse, the system became unresponsive during that latency test run. The first signs of trouble appeared when vi, a simple text editor, took more than several seconds to load. Even weak systems like a Cortex A73 SBC usually load vi instantly. Then, the system stopped responding to all keystrokes over SSH. When I tried to establish another SSH session, it probably got past the TCP handshake stage because it didn’t time out. But the shell never loaded, and the system remained unusable. I eventually managed to recover it by initiating a reboot through the cloud provider, but that sort of behavior is non-ideal.</p>
<p>Since GH200 is a discrete GPU, it’s insightful to compare link latency against other discrete GPU setups. Here, I’m using Nemes’s Vulkan uplink latency test, which uses <code>vkMapMemory</code> to map a portion of GPU VRAM into the test program’s address space. Latency is then measured using pointer chasing accesses, just like above.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30434"><img loading="lazy" decoding="async" width="688" height="328" data-attachment-id="30434" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_vs_pcie_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?fit=802%2C382&amp;ssl=1" data-orig-size="802,382" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_vs_pcie_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?fit=802%2C382&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?fit=688%2C328&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?resize=688%2C328&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?w=802&amp;ssl=1 802w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?resize=768%2C366&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vs_pcie_latency.png?resize=800%2C382&amp;ssl=1 800w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>This comparison is more reasonable, and NVLink C2C offers better latency than some discrete GPU configurations. It lands right between setups with AMD’s RX 5700 XT and HD 7950. Latency with that in mind is quite reasonable. However, CPU code will need to be careful about treating HBM3 memory simply as another NUMA node because of its high latency.</p>
<h2>Grace’s Neoverse V2 Implementation</h2>
<p>A CPU core architecture’s performance can vary depending on implementation. Zen 4 for example behaves very differently depending on whether it’s in a server, desktop, or mobile CPU. Neoverse V2’s situation is no different, and can vary even more because Arm wants to give implementers as much flexibility as possible.</p>
<figure><table><tbody><tr><td></td><td>Nvidia Grace</td><td>Amazon Graviton 4</td><td>Arm Neoverse V2 Emulation Environment</td></tr><tr><td>Clock Speed</td><td>3.44 GHz<br>3.1 GHz base<br>3.0 GHz all-core SIMD</td><td>2.7-2.8 GHz</td><td>3 GHz</td></tr><tr><td>Core Count</td><td>72</td><td>96</td><td>32</td></tr><tr><td>L2 Cache Capacity</td><td>1 MB</td><td>2 MB</td><td>2 MB</td></tr><tr><td>Interconnect</td><td>Nvidia SCF</td><td>Arm CMN-700</td><td>ARM CMN-700</td></tr><tr><td>L3 Cache</td><td>114 MB</td><td>36 MB</td><td>32 MB</td></tr><tr><td>Main Memory</td><td>480 GB LPDDR5X-6400, 480-bit bus</td><td>768 GB DDR5-5200, 768-bit bus</td><td>DDR5-5600, 128-bit bus</td></tr></tbody></table></figure>
<p>Grace targets parallel compute applications. To that end, Nvidia opted for a large shared L3 cache and higher clock speeds. Less parallel parts of a workload can benefit from a flexible boost policy, giving individual threads more performance when power and thermal conditions allow. A large L3 might handle better when threads from the same process share data.</p>
<p>Graviton 4 on the other hand has to server a lot of customers while maintaining consistent performance. Neoverse V2 cores on Graviton 4 get a larger L2, helping reduce noisy neighbor effects. Low clock speeds minimize workload-dependent thermal or power throttling, Finally, a higher core count lets Amazon fit more of their smallest instances on a single server. A latency test shows the memory hierarchy differences well.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30439"><img loading="lazy" decoding="async" width="688" height="343" data-attachment-id="30439" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_latency_cycles/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?fit=1054%2C525&amp;ssl=1" data-orig-size="1054,525" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_latency_cycles" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?fit=1054%2C525&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?fit=688%2C343&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?resize=688%2C343&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?w=1054&amp;ssl=1 1054w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_cycles.png?resize=768%2C383&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Latency in cycles is identical up to L2, because that’s part of the Neoverse V2 core design. Differences start to appear at L3, and do so in dramatic fashion. Large mesh interconnects tend to suffer high latency, and high capacity caches tend to come with a latency cost too. L3 load-to-use latency is north of 125 cycles on Grace. With such a high L2 miss cost, I would have preferred to see 2 MB of L2 cache. Graviton 4 and Intel’s Sapphire Rapids both use 2 MB of L2 cache to counter L3 latency. AMD’s Zen 4 does have a 1 MB L2, but has much lower L2 miss costs.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30442"><img loading="lazy" decoding="async" width="688" height="343" data-attachment-id="30442" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_latency_ns/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?fit=1054%2C525&amp;ssl=1" data-orig-size="1054,525" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_latency_ns" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?fit=1054%2C525&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?fit=688%2C343&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?resize=688%2C343&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?w=1054&amp;ssl=1 1054w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_latency_ns.png?resize=768%2C383&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Higher clock speeds do hand Grace an advantage over Graviton 4 when accesses hit L1 or L2. But L3 latency is still sky-high at over 38 ns. Even <a href="https://chipsandcheese.com/2023/03/12/a-peek-at-sapphire-rapids/">Intel’s Sapphire Rapids</a>, which also accesses a giant L3 over a giant mesh, does slightly better with 33 ns of L3 latency.</p>
<p>L3 cache misses head to Grace’s LPDDR5X controllers. Latency at that point is over 200 ns. Graviton 4’s DDR5 is better at 114.08 ns, putting it in the same ballpark as other server CPUs.</p>
<h3>Bandwidth</h3>
<p>Higher clocks mean higher bandwidth, so a Neoverse V2 core in Grace is comfortably ahead of its counterpart in Graviton 4. Cache bandwidth isn’t quite as high as AMD’s, which can be a disadvantage because Nvidia positions Grace as a CPU for highly parallel workloads. Such workloads are likely to be vectorized, and Zen 4 is very well optimized for those cases. Even when both are pulling data from large L3 caches, a Zen 4 core has more bandwidth on tap.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30446"><img loading="lazy" decoding="async" width="688" height="296" data-attachment-id="30446" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_st_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?fit=1258%2C541&amp;ssl=1" data-orig-size="1258,541" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_st_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?fit=1258%2C541&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?fit=688%2C296&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?resize=688%2C296&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?w=1258&amp;ssl=1 1258w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?resize=768%2C330&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_st_bw.png?resize=1200%2C516&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>To Nvidia’s credit, a single Grace core can pull more bandwidth from L3 than a Graviton 4 core can. This test uses a prefetcher-friendly linear access pattern. I suspect Grace has a very aggressive prefetcher willing to queue up a ton of outstanding requests from a single core. Single core bandwidth is usually <a href="https://en.wikipedia.org/wiki/Little%27s_law">latency limited</a>, and a L2 prefetcher can create more in-flight requests even when the core’s out-of-order execution engine reaches its reordering limits. But even the prefetcher can only go so far, and cannot cope with LPDDR5X latency. DRAM bandwidth from a single core is only 21 GB/s compared to Graviton 4’s 28 GB/s.</p>
<p>When all cores are loaded, Grace can achieve a cool 10.7 TB/s of L1 bandwidth. L2 bandwidth is around 5 TB/s. Both figures are lower than Graviton 4’s, which makes up for lower clock speeds by having more cores. AMD’s Genoa-X has the best of both worlds, with high per-cycle cache bandwidth, higher clock speeds, and 96 cores.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30449"><img loading="lazy" decoding="async" width="688" height="312" data-attachment-id="30449" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_mt_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?fit=1346%2C611&amp;ssl=1" data-orig-size="1346,611" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_mt_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?fit=1346%2C611&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?fit=688%2C312&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?resize=688%2C312&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?w=1346&amp;ssl=1 1346w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?resize=768%2C349&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?resize=1200%2C545&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_mt_bw.png?resize=1320%2C599&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>L3 bandwidth is hard to see from this test because Grace and Graviton 4 have a lot of L2 capacity compared to L3. I usually split the test array across threads because testing with a shared array tends to overestimate DRAM bandwidth. Requests from different cores to the same cacheline may get combined at some level. But testing with a shared array does help to estimate Graviton 4 and Grace’s L3 bandwidth.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30451"><img loading="lazy" decoding="async" width="688" height="312" data-attachment-id="30451" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_shared_arr_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?fit=1346%2C611&amp;ssl=1" data-orig-size="1346,611" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_shared_arr_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?fit=1346%2C611&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?fit=688%2C312&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?resize=688%2C312&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?w=1346&amp;ssl=1 1346w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?resize=768%2C349&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?resize=1200%2C545&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_shared_arr_bw.png?resize=1320%2C599&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Grace has over 2 TB/s of L3 bandwidth, putting it ahead of Graviton 4’s 750 GB/s. Nvidia wants Grace to serve bandwidth hungry parallel compute applications, and having that much L3 bandwidth on tap is a good thing. But AMD is still ahead. Genoa-X dodges the problem of servicing all cores from a unified cache. Instead, each octa-core cluster gets its own L3 instance. That keeps data closer to the cores, giving better L3 bandwidth scaling and lower latency. The downside is Genoa-X has more than 1 GB of last level cache, and a single core only allocates into 96 MB of it.</p>
<h3>Some Light Benchmarking</h3>
<p>In-depth benchmarking is best left to mainstream tech news sites with deeper budgets and full time employees. But I did dig briefly into Grace’s performance.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30455"><img loading="lazy" decoding="async" width="688" height="370" data-attachment-id="30455" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_libx264/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264.png?fit=704%2C379&amp;ssl=1" data-orig-size="704,379" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_libx264" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264.png?fit=704%2C379&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264.png?fit=688%2C370&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264.png?resize=688%2C370&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>libx264 uses plenty of vector instructions, and can demand a lot of bandwidth. It’s the kind of thing I’d expect Grace to do well at, especially with the test locked to matching core counts. But despite clocking higher, Grace’s Neoverse V2 cores fail to beat Graviton 4’s.</p>
<p>7-Zip is a file compression program that only uses scalar integer instructions. The situation is no better there, and I ran the test several times despite the clock running on cloud instance time.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30458"><img loading="lazy" decoding="async" width="688" height="377" data-attachment-id="30458" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_7z/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z.png?fit=702%2C385&amp;ssl=1" data-orig-size="702,385" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_7z" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z.png?fit=702%2C385&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z.png?fit=688%2C377&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z.png?resize=688%2C377&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>Despite using the same command line parameters, 7-Zip wound up executing 2.58 trillion instructions to finish compressing the test file on GH200. On Graviton 4, the same work took a mere 1.86 trillion instructions. libx264’s instruction counts were similar on both Neoverse V2 implementations, at approximately 19.8 trillion instructions. That makes the 7-Zip situation a bit suspect, so I’ll focus on libx264.</p>
<blockquote>
<p>…counts cycles in which the core is unable to dispatch instructions from the front end to the back end due to a back end stall caused by a miss in the last level of cache within the core clock domain</p>
<p>Arm Neoverse V2 Technical Reference Manual</p>
</blockquote>
<p>Neoverse V2 has a STALL_BACKEND_MEM performance monitoring event. The description for this event is clear if a bit wordy. Let’s unpack it. L2 is the last level of cache that runs at core clock. Therefore, STALL_BACKEND_MEM only considers stalls caused by L3 and DRAM latency. Dispatching instructions from the frontend to the backend is what the renamer does, and we know the renamer is the narrowest part of Neoverse V2’s pipeline. Therefore, the event is counting L2 miss latency that the out-of-order engine couldn’t absorb. And, throughput lost from those events can’t be recovered by racing ahead later elsewhere in the pipeline.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30463"><img loading="lazy" decoding="async" width="686" height="370" data-attachment-id="30463" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_libx264_be_bound/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264_be_bound.png?fit=686%2C370&amp;ssl=1" data-orig-size="686,370" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_libx264_be_bound" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264_be_bound.png?fit=686%2C370&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264_be_bound.png?fit=686%2C370&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_libx264_be_bound.png?resize=686%2C370&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>libx264 sees a massive increase in those stalls. Grace’s smaller L2 combined with worse L3 and DRAM latency isn’t a winning combination. Overall lost throughput measured at the rename stage only increased by a few percent. It’s a good demonstration of how Neoverse V2’s large backend can cope with extra latency. But it can’t cope hard enough, nullifying Grace’s clock speed advantage.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30466"><img loading="lazy" decoding="async" width="684" height="370" data-attachment-id="30466" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_7z_be_bound/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z_be_bound.png?fit=684%2C370&amp;ssl=1" data-orig-size="684,370" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_7z_be_bound" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z_be_bound.png?fit=684%2C370&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z_be_bound.png?fit=684%2C370&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_7z_be_bound.png?resize=684%2C370&amp;ssl=1" alt="" data-recalc-dims="1"></a></figure></div>
<p>The same counters in 7-Zip don’t show such a huge discrepancy, though Grace again suffers more from L2 miss latency. Grace’s poor showing in this workload is largely due to 7-Zip somehow executing more instructions to do the same work.</p>
<p>7-Zip and libx264 don’t benefit from Nvidia’s implementation choices, but that doesn’t mean Grace’s design is without merit. The large 114 MB L3 cache looks great for cache blocking techniques, and higher clocks can help speed up less parallel parts of a program. Some throughput bound programs may have prefetcher-friendly sections, which can be aided by Grace’s prefetcher. Specific workloads may do better on Grace than on Graviton 4, particularly if they receive optimizations to fit Grace’s memory subsystem. But that’s beyond the scope of this brief article.</p>
<h2>H100 On-Package GPU</h2>
<p>The GPU on GH200 is similar to the H100 SXM variant, since 132 Streaming Multiprocessors (SMs) are enabled out of 144 on the die. VRAM capacity is 96 GB compared to the 80 GB on separately sold H100 cards, indicating that all 12 HBM controllers are enabled. Each HBM controller has a 512-bit interface, so the GH200’s GPU has a 6144-bit memory bus. Even though GH200’s GPU is connected using a higher bandwidth NVLink C2C interface, it’s exposed to software as a regular PCIe device. </p>
<p><code>nvidia-smi</code> indicates GH200 has a 900W power limit. For comparison, H100’s SXM variant has a 700W power limit, while the H100 PCIe makes do with 350-400W. GH200 obviously has to share power between the CPU and GPU, but the GPU may have more room to breathe than its discrete counterparts when CPU load is low.</p>
<p>Compared to the PCIe version of the H100, GH200’s H100 runs at higher clocks, reducing cache latency. Otherwise, the H100 here looks a lot like other H100 variants. There’s large L1 cache backed by a medium capacity L2. H100 doesn’t have a gigantic last level cache like RDNA 2, CDNA 3, or Nvidia’s own Ada Lovelace client architecture. But it’s not a tiny cache either like on Ampere or other older GPUs.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30468"><img loading="lazy" decoding="async" width="688" height="321" data-attachment-id="30468" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_vk_latency/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?fit=1201%2C561&amp;ssl=1" data-orig-size="1201,561" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_vk_latency" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?fit=1201%2C561&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?fit=688%2C321&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?resize=688%2C321&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?w=1201&amp;ssl=1 1201w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_latency.png?resize=768%2C359&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>VRAM latency sees a very substantial improvement, going down from 330 ns to under 300. It’s impossible to tell how much of this comes from higher clock speeds reducing time taken to traverse H100’s on-chip network, and how much comes from HBM3 offering better latency.</p>
<p>Bandwidth also goes up, thanks to more enabled SMs and higher clock speeds. Unfortunately, the test couldn’t get past 384 MB. That makes VRAM bandwidth difficult to determine. If things worked though, I assume GH200 would have higher GPU memory bandwidth than discrete H100 cards.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30472"><img loading="lazy" decoding="async" width="688" height="320" data-attachment-id="30472" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_vk_bw/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?fit=1196%2C556&amp;ssl=1" data-orig-size="1196,556" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_vk_bw" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?fit=1196%2C556&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?fit=688%2C320&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?resize=688%2C320&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?w=1196&amp;ssl=1 1196w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_vk_bw.png?resize=768%2C357&amp;ssl=1 768w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Further tests would have been interesting. I wanted to test CPU to GPU bandwidth using the GPU’s copy engine. DMA engines can queue up memory accesses independently of CPU (or GPU) cores, and are generally more latency tolerant. Nemes does have a test that uses <code>vkCmdCopyBuffer</code> to test exactly that. Unfortunately, that test hung and never completed.</p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30489"><img loading="lazy" decoding="async" width="688" height="138" data-attachment-id="30489" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/gh200_pcie_errors/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?fit=1358%2C272&amp;ssl=1" data-orig-size="1358,272" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gh200_pcie_errors" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?fit=1358%2C272&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?fit=688%2C138&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?resize=688%2C138&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?w=1358&amp;ssl=1 1358w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?resize=768%2C154&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?resize=1200%2C240&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/gh200_pcie_errors.png?resize=1320%2C264&amp;ssl=1 1320w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p>Checking <code>dmesg</code> showed the kernel complaining about PCIe errors and graphics exceptions. I tried looking up some of those messages in Linux source code, but couldn’t find anything. They probably come from a closed source Nvidia kernel module. Overall, I had a frustrating experience exercising NVLink C2C. At least the Vulkan test didn’t hang the system, unlike running a plain memory latency test targeting the HBM3 memory pool. I also couldn’t use any OpenCL tests. <code>clinfo</code> could detect the GPU, but <code>clpeak</code> or any other application was unable to create an OpenCL context. I didn’t have the same frustrating experience with H100 PCIe cloud instances, where the GPU pretty much behaved as expected with Vulkan or OpenCL code. It’s a good reminder that designing and validating a custom platform like GH200 can be an incredibly difficult task.</p>
<h2>Final Words</h2>
<p>Nvidia’s GH200 and Grace CPU is an interesting Neoverse V2 implementation. With fewer cores and a higher power budget, Grace can clock higher than Graviton 4. But rather than providing better per-core performance as specifications might suggest, Grace is likely optimized for specific applications. General consumer workloads may not be the best fit, even well vectorized ones.</p>
<p>Previously I thought Arm’s Neoverse V2 had an advantage over Zen 4, because Arm can focus on a narrower range of power and performance targets. But after looking at Grace, I don’t think that captures the full picture. Rather, Arm faces a different set of challenges thanks to their business model. They don’t see chip designs through to completion like AMD and Intel. Those x86 vendors can design cores with a comparatively narrow set of platform characteristics in mind. Arm has to attract as many implementers as possible to get licensing revenue. Their engineers will have a harder time anticipating what the final platform looks like. </p>
<div>
<figure><a href="https://chipsandcheese.com/?attachment_id=30486"><img loading="lazy" decoding="async" width="688" height="383" data-attachment-id="30486" data-permalink="https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/arm_v2_hotchips_reference_platform/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?fit=1276%2C711&amp;ssl=1" data-orig-size="1276,711" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="arm_v2_hotchips_reference_platform" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?fit=1276%2C711&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?fit=688%2C383&amp;ssl=1" tabindex="0" role="button" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?resize=688%2C383&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?w=1276&amp;ssl=1 1276w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?resize=768%2C428&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2024/07/arm_v2_hotchips_reference_platform.png?resize=1200%2C669&amp;ssl=1 1200w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a><figcaption>Arm evaluated Neoverse V2 in an emulation environment that drastically differs from Grace and Graviton 4. Slide from Arm’s Hot Chips 2023 presentation</figcaption></figure></div>
<p>So, Neoverse V2 can find itself having to perform in an environment that doesn’t play nice with its core architecture. Nvidia’s selection of a 1 MB L2, high latency L3, and very high latency LPDDR5X present Neoverse V2 with a spicy challenge. As covered in the <a href="https://chipsandcheese.com/2024/07/22/arms-neoverse-v2-in-awss-graviton-4/">Graviton 4</a> article, Neoverse V2 has similar reordering capacity to Zen 4. I think Zen 4 would also trip over itself with 125 cycles of L3 latency and over 200 ns of memory latency. I don’t think it’s a coincidence that every Zen 4 implementation has a fast L3. Intel is another example, Golden Cove can see 11.8 ns of L3 latency in a Core i7-12700K, or 33.3 ns in a Xeon Platinum 8480+. Golden Cove has much higher reordering capacity, making it more latency tolerant. In a server environment, Golden Cove gets a 2 MB L2 cache as well.</p>
<p>GH200’s GPU implementation deserves comment too. It should be the most powerful H100 variant on the market, with a fully enabled memory bus and higher power limits. NVLink C2C should provide higher bandwidth CPU to GPU communication than conventional PCIe setups too.</p>
<p>But it’s not perfect. NVLink C2C’s theoretical 450 GB/s is difficult to utilize because of high latency. Link errors and system hangs are a concerning problem, and point to the difficulty of validating a custom interconnect. Exposing VRAM to software as a simple NUMA node is a good north star goal, because it makes VRAM access very easy and transparent from a software point of view. But with current technology, it might be a bridge too far.</p>

<p>Even though it’s not an iGPU, Grace Hopper might be Nvidia’s strongest shot at competing with AMD’s iGPU prowess. Nvidia has already scored a win with <a href="https://nvidianews.nvidia.com/news/aws-nvidia-strategic-collaboration-for-generative-ai">Amazon</a> and the <a href="https://www.bristol.ac.uk/news/2023/november/supercomputer-announcement.html">UK’s Isambard-AI supercomputer</a>. AMD’s MI300A is shaping up to be tough competition, with a win in Lawrence Livermore National Laboratory’s upcoming <a href="https://asc.llnl.gov/exascale/el-capitan">El Capitan</a> supercomputer. MI300A uses an integrated GPU setup, which speeds up CPU to GPU communication. However, it limits memory capacity to 128 GB, a compromise that Nvidia’s discrete GPU setup doesn’t need to make. It’s good to see Nvidia and AMD competing so fiercely in the CPU/GPU integration space, and it should be exciting to see how things play out.</p>
<p>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p>

<div data-post_id="10949" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-10949 box-instance-id-1">
<p><span>
<ul>
<li>
<div>
<p><img alt="clamchowder" src="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/7c39d2e6d35e77c8fd15c4b2d9ce4e64?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80"> </p>
</div>

</li>
</ul>
</span>
</p></div>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DARPA wants to bypass the thermal middleman in nuclear power systems (178 pts)]]></title>
            <link>https://www.ans.org/news/article-6276/darpa-wants-to-bypass-the-thermal-middleman-in-nuclear-power-systems/</link>
            <guid>41205439</guid>
            <pubDate>Fri, 09 Aug 2024 21:12:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ans.org/news/article-6276/darpa-wants-to-bypass-the-thermal-middleman-in-nuclear-power-systems/">https://www.ans.org/news/article-6276/darpa-wants-to-bypass-the-thermal-middleman-in-nuclear-power-systems/</a>, See on <a href="https://news.ycombinator.com/item?id=41205439">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="expand_6276"><p>DARPA is interested in ideas to take alpha, beta, gamma, and neutron radiation from “any type of reactor,” including fission and fusion, and from nuclear processes and radioisotope decay, and directly convert those radioactive emissions to electricity to meet the specific power and lifetime requirements of a range of nuclear power systems. Respondents have until Aug. 30 to share their ideas.</p><p><strong>There’s got to be a better way:</strong> “Methods to convert the energy of nuclear fission reactions and the decay of radioisotopes into electricity have not evolved since the invention of radioisotope power systems and fission reactors over 70 years ago and remain unoptimized,” the RFI says. They rely on thermal heat transfer, and “in each step of this indirect conversion method neutrons, heat, and energy are lost to the shielding material, working fluid, and other system materials.”</p><p>Advanced reactor designs that use alternative coolants, including helium, sodium, and salts, would still use what DARPA calls “heritage nuclear power conversion technology” with water and steam as the working fluids, as would the fusion power plants being planned today.</p><p><strong>Why now? </strong>Tabitha Dodson, the program manager for DARPA DSO, which is launching the RFI, told <em>Nuclear News</em> that “two big things” are driving the interest.</p><p>“One is the extreme surge of investment in small and advanced nuclear technologies, such as in fusion and space reactors, which do not have a concurrent pairing of advanced power generation methods that doesn’t involve liquid-based heat transfer,” she said. “Next, there has been an order of magnitude improvement in radiation tolerance and efficiency for voltaics in recent years with encouraging performance that indicates radiovoltaics could scale up as an array usable in nuclear reactors.”</p><p><strong>Exploring radiovoltaics:</strong> The RFI points to research in direct energy conversion based on radiation, or radiovoltaics, which includes semiconductor-based neutron, gamma, beta, and alpha voltaics. In radiovoltaics, radiation indirectly excites electron-hole pairs in a semiconductor lattice—a process that resembles but is distinct from how photons accomplish energy conversion in photovoltaics.</p><p>Radiovoltaics has been used to generate electricity from small amounts of decaying radioisotopes to produce commercially available microelectronics and small batteries at the nanowatt to milliwatt levels. “At least two challenges, however, [prevent] these from being viable solutions today for simultaneous high-power, long-duration applications,” according to the RFI. Those challenges are efficiency—"in most cases the efficiencies of radiovoltaics are 1–3 percent per radiation emission”—and the lifespan of candidate semiconductor materials, which is limited by their ability to withstand excess radiation energy over time and maintain performance.</p><p>That’s where the order of magnitude improvement in performance that Dodson noted comes in. Recent years have seen “the discovery of radiation-resilient materials that are still able to carry electrons into the conduction band to drive current despite the incursion of radiation-driven defects,” she said. Now, DARPA is interested in the possibility of applying direct energy conversion beyond the milliwatt power levels that have been demonstrated, and in solutions optimized for neutron and gamma radiation from a fission or fusion reactor.</p><p><strong>Scaled to fit:</strong> If durable, efficient radiovoltaics can be developed, Dodson sees the potential for them to fit a range of electricity needs. “If we could make an array of cells that could be scaled to any broad area, this power generation method could be paired with any sized nuclear reactor or even nuclear radioisotope power system. It could even be scaled up to a commercial-sized power plant or down to tiny-sized microelectronics. What’s appealing about voltaics versus thermoelectrics or working-fluid methods are how thin, light, pliable, and scalable they could be to any size or form factor,” Dodson said.</p><p>Dodson’s work at DARPA has included <a href="https://www.ans.org/news/article-4842/leading-draco-to-launch-an-interview-with-darpas-tabitha-dodson/">a key role</a> in establishing and then serving as program manager of the nuclear thermal propulsion rocket program DRACO—the Demonstration Rocket for Agile Cislunar Operations. Space missions and deployments stand to gain from a scalable high power direct energy conversion technology.</p><p>“The issues that come with outdated power generation equipment on the ground are magnified for space nuclear technologies since space reactors are size constrained and have to carry all of that equipment with them,” according to Dodson.</p><p>Here on Earth, progress in neutron- and gamma-based radiovoltaics could lead to greater availability of small neutron and gamma detectors for reactor instrumentation and control, she noted. The RFI suggests that, with systems scaled for large-scale electricity generation and with improved material lifetimes and energy conversion efficiency, “one could create energy-generating ‘smart shields’ for nuclear systems that simultaneously cut down on nuclear waste.” In such shields, “neutron radiation from a fusion or fission reactor could transmute and decay radiovoltaic lattice materials doped or layered with isotopes that are tuned to absorb the reactor’s neutrons, thereafter generating secondary emission alpha or beta particles to further energize radiovoltaics.”</p><p><strong>What is the ask?</strong> The RFI asks: “Is it possible to achieve [a] direct energy conversion nuclear power system, ranging in power from 10s of watts electric (We) to 100s of kWe?” DARPA wants information “on the potential to improve specific power greater than 1 We/kg conversion from watts-thermal per radiation emission product,” and information on the potential to improve damage tolerance of the voltaic to nuclear radiation to reach an operating lifetime comparable to the life of its nuclear source, on the scale of decades.</p><p>“We will learn what our boundary conditions are when respondents tell us what technologies in the field of voltaics are possible, and we’ll use that to see if there is sufficient scientific rationale make a case to present for further DARPA investment,” Dodson said. “I also hope people are going to start thinking about nuclear systems that use electromagnetic versus thermal-kinetic methods to harvest nuclear energetic reactions.”</p><p>Responses can be submitted until 4 p.m. (EDT) on August 30. Questions can be directed to Dodson at <a href="mailto:DARPA-SS-24-01@darpa.mil">DARPA-SS-24-01@darpa.mil</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Urchin Software Corp: The unlikely origin story of Google Analytics (2016) (232 pts)]]></title>
            <link>https://urchin.biz/urchin-software-corp-89a1f5292999</link>
            <guid>41205176</guid>
            <pubDate>Fri, 09 Aug 2024 20:32:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://urchin.biz/urchin-software-corp-89a1f5292999">https://urchin.biz/urchin-software-corp-89a1f5292999</a>, See on <a href="https://news.ycombinator.com/item?id=41205176">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><div><div><div><h2 id="ddd7">The unlikely origin story of Google Analytics, 1996–2005-ish</h2><div><a href="https://medium.com/@impunity?source=post_page-----89a1f5292999--------------------------------" rel="noopener follow"><div aria-hidden="false"><p><img alt="Scott Crosby" src="https://miro.medium.com/v2/resize:fill:88:88/1*3uQ_Wu_tc6tzDRqN6pYeAA.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a><a href="https://urchin.biz/?source=post_page-----89a1f5292999--------------------------------" rel="noopener  ugc nofollow"><div aria-hidden="false"><p><img alt="Urchin Software Corp. Vault" src="https://miro.medium.com/v2/da:true/resize:fill:48:48/1*oH65EY-qH6myOMQxSt3Teg.gif" width="24" height="24" loading="lazy" data-testid="publicationPhoto"></p></div></a></div></div><figure><figcaption>The first Urchin logo (Jason Collins)</figcaption></figure><p id="48ce"><em>TL, DR: Urchin Software Corporation was a web analytics company based in San Diego, CA. The founders of the company were Paul Muret, Jack Ancone, Brett Crosby, and Scott Crosby (yours truly). In April 2005 the company was acquired by Google, and the Urchin product became “Urchin from Google,” then later simply Google Analytics. As the 10th anniversary of the acquisition has recently passed, I thought it was a good time to get the history of the company down for posterity. I don’t expect this to be a riveting read for those not directly involved; it’s more of my attempt to close the book on that era.</em></p><p id="4f3b"><em>And maybe, if nothing else, I guess it suggests that despite the soup du jour — huge seed/A rounds, massive valuations, binary outcomes— you can sometimes do alright by just taking less money and more time.</em></p><figure><figcaption>Our first day at Google, April 21, 2005. Brett was on his honeymoon!</figcaption></figure><p id="70ef"><span>T</span>he predecessor to Urchin Software Corp. was originally started by Paul Muret and Scott Crosby (me) in late 1995. Prior to then, Paul had been working in the Space Physics dept. at UCSD, where he was exposed to HTML 1.0 after being tasked with putting the department’s syllabus online. Paul and I were post-college roommates at the time, living in the Bay Park neighborhood of San Diego. One night Paul came home from work and announced that he saw a business opportunity in building websites for businesses. To prove the point, he showed me his bright-blue-text-on-grey-background site for UCSD. Maybe some of the text even &lt;BLINK&gt;ed. I agreed and we started work on a business plan. This was presented to my proverbial rich uncle (Chuck Scott), who agreed to invest $10,000 in the “company” and provide a desk in a corner of his office at <a href="http://www.cbsscientific.com/" rel="noopener ugc nofollow" target="_blank">C.B.S. Scientific</a>. It would be quite awhile until he saw a return on that money.</p><figure><figcaption>Our first webserver, running at 50 mhz, was ~$3200 in 1995 money. That was about 1/3 of our total raised capital to-date.</figcaption></figure><p id="e296">Armed with Chuck’s cash, the new company bought a Sun SPARC 20 for webserving duty and procured a then-very-expensive ISDN line. Ever heard of “<a href="https://en.wikipedia.org/wiki/10BASE2" rel="noopener ugc nofollow" target="_blank">10base2</a>”? That’s how our office computers were networked — coaxial cable with fun twist-lock fittings, kinda like TV cable. Antiquated. Anyway, Paul and I then set about acquiring customers, which we slowly accomplished. Most were small businesses paying a modest monthly fee, like Cinemagic, a vintage movie-poster company run by a couple named Herb and Roberta. Or ReVest, a financial startup whose owner didn’t “do” email, so all edits to his site were communicated via thermal-transfer fax, which spooled out onto our floor for 6 or 8 pages every morning. Another was an obscure division of Pioneer Electronics that specialized in the even-then old-timey format known as <a href="https://en.wikipedia.org/wiki/LaserDisc" rel="noopener ugc nofollow" target="_blank">LaserDisc</a> [1] I know I know, the quality is better.</p><p id="2127">These “wins” had us feeling optimistic enough to lease some office space in a squat brownish-green building in San Diego’s faux-historic Old Town theme park-y part of town, not far from Rockin’ Baja Lobster. Our office had room for 4 desks, or 5 if you counted the vestibule (for a secretary?) In 1997 Brett Crosby (my younger brother) joined the company, and things started improving. We managed to sign two of the larger local employers, Sharp Healthcare, a hospital system, and Solar Turbines*, the power generation subsidiary of Caterpillar. But we still had a bunch of small customers, most of whom we hosted on our lone webserver, for a recurring fee. To accurately bill for bandwidth consumed (weird right? bandwidth used to be expensive), Paul wrote a simple log analyzer to tally bytes transferred, and gave it a nice web interface. He added referrers, “<a href="http://www.kaushik.net/" rel="noopener ugc nofollow" target="_blank">hits</a>”, pageviews, etc., and voilá, the first version of Urchin was born. After some further development to add date-range features, user authentication, etc., the product was demonstrated to customers, to generally favorable reviews.</p><p id="25bb">[*this deal paid $10k/month and kept us alive for at least a year; thanks Steve!]</p></div><div><figure></figure><figure><figcaption>Our first tradeshow ever, circa 1997. We borrowed these giant blue light boxes from an underwear startup, as I recall. They were 1" thick particle board and extremely heavy. And we had booth babes! Not really. They were friends who thought an internet tradeshow would be fun, so they hung out all day for free. Suffice to say they never volunteered again.</figcaption></figure></div><div><p id="17b0">Brett’s girlfriend at the time (Julie, now his wife) also worked in “the industry”, for <a href="http://www.rpa.com/" rel="noopener ugc nofollow" target="_blank">Rubin Postaer</a> Interactive (“RPI”, a subsidiary of RPA; A=Associates). RPA was and is a prominent advertising &amp; web development shop in LA, and they managed the Honda.com account. Sometime in late 1997, it was learned via our RPA spy (Julie) that Honda.com, which was using WebTrends at the time, was unable to process each day’s Apache access log before the end of the current day, dooming them to fall ever more behind. After some effort, we obtained a few days’ server logs to process as a demo, and the task was completed in ~30 minutes. We became the web analytics solution for American Honda from that point forward, and it became clear a business could be built on this log processing technology.</p><figure><figcaption>One of the earlier Urchin t-shirt designs, before we corporate-ized the logo. Cabel from <a href="http://panic.com/" rel="noopener ugc nofollow" target="_blank">Panic</a> was mad at us for that.</figcaption></figure><p id="d819">Around this time Jack Ancone joined the company as (initially) CFO and moved to San Diego. We moved into our office at 2165 India St. around the same time (Note: The <a href="http://ballastpoint.com/" rel="noopener ugc nofollow" target="_blank">Ballast Point</a> tasting room is now located directly across the street from our old office; suffice to say we would have never achieved anything had they been there in the late 1990s.) The company was then known as Quantified Systems, Inc., and work was divided into web dev, hosting, and software dev. Be divided and conquer yourself, to paraphrase Caesar.</p><figure><figcaption>Classy awnings right? The big Ballast Point tasting room/restaurant is now in the building barely visible to the left. They have since become a <a href="http://insidescoopsf.sfgate.com/blog/2015/11/16/san-diegos-ballast-point-sold/" rel="noopener ugc nofollow" target="_blank">legit unicorn</a>. So much for software being the way to get rich.</figcaption></figure><p id="72ee">In January 1998 we received our very first order for the “Pro” version of Urchin, for $199. [Side note: why does “Pro” always mean “lame” in the software world?] Anyway, shortly after, we made the decision to jettison the non-software parts of the business, and all hosting/webdev customers were rather abruptly “sold” ($0) to another local webdev shop. We were now a software company (high-five!).</p><p id="6d27">As such, we needed to raise some money. Tapping our family “networks” and one boutique VC (Green Thumb Capital, of NYC, who Jack brought on board*), we raised $1m, bringing our total outside capital to about $1.25m. We would never raise any further money (with the exception of ~$400k in debt, which was repaid with interest and warrants). Not for lack of trying… more on that later.</p><p id="7859">[*to their credit, Green Thumb never once hassled us nor apparently had they any thought of recovering their investment; I can only imagine their dumbfounded shock when they found out Google had agreed to acquire us.]</p><p id="2732">As we struggled to figure out how to sell “enterprise” software in the late 1990s, we decided to try an advertising-based approach to grab market share. For some reason, we were always more concerned with popularity than money… go figure. Internet companies of the era were often valued most highly if measured by “eyeballs” and we thought we could get lots of those by giving away the software for free and showing banner ads at the top of each page. So we released Urchin ASAP, the free counterpart to Urchin ISP. Both were designed to be used by hosting operations. We thought we could make some significant fraction of a cent per click on these ads, on top of some infinitesimal CPM… (reminds me of a classic SNL skit… Q: “how do you make money at the Citiwide Change Bank? A: <a href="https://video.yahoo.com/first-citywide-change-bank-2-000000534.html" rel="noopener ugc nofollow" target="_blank">Volume!</a>”) We never made anything on those banner ads, but we did get exposure. And the software was pretty good, for the era. Good enough to make our first real breakthrough.</p><figure><figcaption>One of the Urchin ASAP banner ads, which advertised itself when no one else wanted the space. Meta.</figcaption></figure><p id="94db">Before Tumblr, before Blogger, and contemporary with <a href="https://techcrunch.com/2009/04/23/yahoo-quietly-pulls-the-plug-on-geocities/" rel="noopener ugc nofollow" target="_blank">Geocities</a>, there was something called <a href="http://www.forbes.com/1999/02/01/mu3.html" rel="noopener ugc nofollow" target="_blank">Nettaxi</a>. What-Taxi? Right. But at the time they claimed something like 100,000 “sites”, and we viewed them as a fantastic source of eyeballs for our advertising-supported version of Urchin. They claimed to have no money for such a luxury as web statistics (glad that term got retired), so we “negotiated” a deal: Urchin would be 100% free for them in exchange for the anticipated ad revenue we’d get from all those eyeballs. How much money did we make? 4¢ or so maybe, I don’t recall ever actually getting a check. But that’s beside the point. From then on we claimed 100,000 “sites” were using Urchin, and that got us places.</p><p id="c6d9">Another “innovation” we came up with mirrored what Google does with its logo on special days — we created a dynamic Urchin guy in the upper-left of the interface called “the Urchin of the Day.” This was straight silly, but we thought it would endear us to customers. Maybe it did. It definitely occupied our design guy Jason Collins for the better part of year, as he got so into it he forgot about his more important work. The images he made still make me laugh. So good! We even had our then-friend, the pre-famous <a href="https://obeygiant.com/" rel="noopener ugc nofollow" target="_blank">Shepard Fairey</a>, do one, the “Power to the People” version.</p><figure><figcaption>7'4", 520 lb.</figcaption></figure><p id="81ef">He also did several promo posters and ads for us in exchange for free web hosting. Really nice, humble guy. Of course, now he gets presidents elected and such.</p><figure><figcaption>My favorite Urchin of the Day — Jason Collins (graphics guy) was a car nut and master of animated GIFs.</figcaption></figure><p id="589d">In 1999, Brett Crosby, VP of Sales and Marketing, was casting about trying to get Urchin 2.0 noticed. He had zeroed-in on Earthlink as our dream customer, mostly due to their vast reach and name recognition. Heck, they were almost as big as AOL! Of course, we had no idea how to reach anyone important at a place like that, so Brett did the natural thing and filled out a web form. Again, and again, and again. He must have submitted that thing 20 or 30 times. Finally, he got a response. Rob Maupin, VP of hosting (or something similar) agreed to a meeting. We were stunned. The all-powerful Earthlink would meet with a bunch of idiots like us? We could hardly believe it. So we piled into the fanciest car we had, Brett’s old Mercedes 420 SEL, which he had bought for $4,000, and drove up to Pasadena. I stayed at the office to “manage” and because I was scared of Earthlink.</p><p id="fc7c">Rob didn’t seem very impressed. He immediately dismissed the Urchin 2.0 interface as (and I quote) “too blue blue blue blue blue,” which I had to admit was true.</p><figure><figcaption><a href="https://www.linkedin.com/in/rmaupin" rel="noopener ugc nofollow" target="_blank">Rob Maupin</a>: We’ll try it if you make it less blue.</figcaption></figure><p id="ae67">But he agreed to try it, and the tests went well. Urchin was perhaps not the most full-featured web reporting tool in the world at the time, but it was fast, and that’s mainly what web hosts cared about. It also managed log files and helped sysadmins run things more easily, so the ops guys liked it. After we made some changes as Earthlink requested, we negotiated a pretty lame deal that ensured our eventual success: $4,000/month for unlimited Urchin software across all Earthlink-hosted websites. We were ecstatic.</p><figure><figcaption>Jack, Brett, and Jason Senn (head of Channel and carpentry), looking tough.</figcaption></figure><figure><figcaption>This photo (and the above) were for an article in Fortune in 2000 (“Traffic Aficionados”). Thanks <a href="https://www.linkedin.com/in/suzanne-koudsi-b28349ab" rel="noopener ugc nofollow" target="_blank">Suzi Koudsi</a>!</figcaption></figure><p id="fe28">In 2001, the company was rechristened Urchin Software Corporation. Product and sales were sufficiently far along that we thought it would be a good idea to raise more money. Anyone who’s pitched VCs knows it’s a long, distracting slog, and the business really suffers during the months the leadership is preoccupied. But things went pretty well, and after dozens of meetings and plenty of travel, we finally secured term sheets from two respectable VCs — Ampersand and <a href="https://en.wikipedia.org/wiki/John_Moores_%28baseball%29" rel="noopener ugc nofollow" target="_blank">JMI</a>. The closing was scheduled for late August, but then Labor Day rolled around, and a few more days passed, and finally the capital call to the LPs was supposed to happen on… September 12th (2001). Uhh, no. Suffice to say, the world had bigger things to worry about that day.</p><p id="72e3">By that point, having anticipated a capital infusion of approximately $7 million, we had ramped up hiring and infrastructure spending, as one does. We had leased two additional office spaces in the same building, and built out the interiors. So without the funding, we had little choice but to pare back down. On a Friday not long after, we laid off 12 people and shortly afterward relinquished one of our office spaces. We called it, somewhat unimaginatively, Black Friday. Our financial situation was bleak (like, we couldn’t make payroll in two weeks), and we saw no alternative but to borrow money from our rich uncles — Chuck Scott and <a href="http://jeromes.com/" rel="noopener ugc nofollow" target="_blank">Jerry Navarra</a>. They saved us, and got interest and warrants for their trouble. But Thanksgiving was uncomfortable for a couple years.</p><p id="a18b">2001 and 2002 were very difficult years for Urchin Software Corp. — I remember walking down our hallway, praying to the acoustic ceiling tiles on more than one occasion, “please, just let it die” — but it wouldn’t. Costs were cut way back, and some employees took voluntary pay cuts of up to 60% to help with cashflow (this “back pay” was eventually repatriated, thank goodness).</p><figure><figcaption>We always had stickers, because we were juvenile and liked sticking them on competitors’ booths and airplanes. Surfer Urchin is the rarest of the lot. Shepard Fairey designed a few bits of schwag in exchange for hosting <a href="http://obeygiant.com/" rel="noopener ugc nofollow" target="_blank">obeygiant.com</a>.</figcaption></figure><p id="3e9f">Tech spending was slow in the early 2000s post-bubble, and while things were gradually improving, revenue was uneven and not growing as hoped. Until 2002, our major source of inflow had been large annual licensing deals that were complex and long negotiations. Our biggest deal, at over $1 million, was negotiated by Jack Ancone with Cable &amp; Wireless, a major global telco/host/ISP.</p><figure><figcaption>Urchin 3.0’s interface, cobranded with the now-defunct Worldport, of Dublin. Urchin 3.x still runs on more than a few old servers in dusty corners of the internet.</figcaption></figure><p id="d893">Similar deals were negotiated with Winstar, KeyBridge, and Ireland-based Worldport, all VC-backed hosts with seemingly limitless resources. As it turned out, they were indeed limited. All of them expired before we got paid.</p><p id="227b">So to jumpstart sales, the decision was made to radically simplify our enterprise deals to hosting companies, even though it meant less money in the short term. Essentially, we made the financially-puny Earthlink deal the standard, but included some potential upside, at least. The Site License Model (“SLM”) was about as simple as it gets: $5,000/month per physical datacenter, all the Urchin software you want, 1-page contract, no legalese, and nothing really to negotiate.</p><figure><figcaption>The accurséd Winstar blimp flew lazy circles around Jack’s house in Bird Rock while he negotiated a $400,000 deal with them, which was finally executed; we never saw a dime. Beware of customers with blimps!</figcaption></figure><p id="762c">The SLM was immediately a hit, and we signed deals with many of the largest US and European hosting companies in the ensuing year. Rackspace (now part of IBM), Everyone’s Internet (aka EV1 Servers), The Planet, mediatemple, and many others signed on, with several agreeing to multi-datacenter deals. By around fall 2003, we were cashflow-positive on these alone, and we were also selling more and more individual licenses to self-hosted organizations including much of the Fortune 500 and many university systems.</p><p id="1fa8">We had released most of the sales team during our near-death experience in 2001, but the few who remained — Paul Botto, Nikki Morrissey, and Megan Cash— worked for nothing and slowly, fitfully started selling our way out of the doldrums. Once we finally figured out a workable commission model — low base, high commission with fully retroactive kickers — these three positively rocked. Paul and Megan went on to work for Google for years (Nikki declined to move north in 2005, in favor of having kids.) Paul is still the best sales/BD guy with whom I’ve ever worked.</p><figure><figcaption>Paul Botto accepts the Employee of the Month plaque, March 3rd 2002; note the drum kit in the background.</figcaption></figure><p id="829c">After a failed attempt at running an international office in Tokyo (the moneys did not come[2]), we launched a channel program, which ran in parallel to direct sales. I still think for certain markets where English isn’t the primary language, it makes sense to have a local partner. Japan in particular generated strong sales for many years, and for this we can thank Jason Senn, our channel guy and chief office builder (we were too cheap to hire contractors.) Japan is also the most fun place ever to party it up in the name of business. On-sens and fire festivals? Yes please.</p><p id="2dbc">If Urchin 2 got us in the door, and Urchin 3 didn’t suck, Urchin 4 was actually pretty respectable. It had the then-Apple-esque brushed-aluminum look, some fancy-for-the-time interface elements, and most importantly, it had the UTM. The UTM, or Urchin Traffic Monitor, was an early method for augmenting Apache (or IIS, etc.) log files with cookies, such that unique visitors could be established. This method entailed a line of javascript in the &lt;HEAD&gt; of each page on the site, and a small modification to the webserver’s logging behavior. Most of our competitors at the time used either logs only (old school) or javascript/cookies only (WebSideStory, etc.), and both necessarily missed out on a lot of available information. Urchin was the first to use both data sources in one unified collection method, neatly contained in augmented access-log files. Nowadays pretty much everything you’d want can be had via the cookie method (á la GA), but analyzing logs still has its advantages.</p><figure><figcaption>Urchin 4 had an easter egg that no one ever found, to my knowledge. If you clicked a random “rivet” in the sexy brushed aluminum interface, you’d be treated to a photo of the illustrious Urchin dev team: Doug Silver, Nathan Moon, Paul, Jonathon Vance, Rolf Schreiber, and Jim Napier. Most of these guys are still at Google (as of Aug. 2016).</figcaption></figure><p id="8a26">Urchin 4 continued our tradition of supporting way, way too many random platforms (Google still has Urchin 4 help: <a href="https://support.google.com/urchin/answer/28276?hl=en&amp;ref_topic=7389" rel="noopener ugc nofollow" target="_blank">check out the OS support</a>… ever heard of Yellow Dog Linux?). I had this idea that platform-carpet-bombing might get us into some big corporations or universities running AIX or HP-UX, but no, everyone bought the Linux or Windows IIS versions. I guess I just liked buying random servers off ebay and getting Apache and a compiler running. We even compiled a NeXT version at one point, if I’m not mistaken. But no DEC, at least (I couldn’t get the machine to boot up).</p><figure><figcaption>Paul, looking here like a cartel drug lord, withdrew something like $53,000 in cash for our xmas bonuses in 2004. Funny thing is, Google also gave out actual cash money bonuses for years after we joined — millions of dollars in currency. Great minds think alike I guess. Anyway, it was fun and no one got robbed. This was December 17, 2004.</figcaption></figure><p id="7e7a">Urchin 4 was the first release I really felt could compete against anyone, and not just with regard to back-end performance. But Urchin 5 was superior in every way, and I’m sure thousands of instances still run to this day. If anything, Urchin 5 was just too much of a good thing. Almost every menu item had submenus upon submenus. It was pretty overwhelming and dry, but analytics nerds dug it.</p><figure><figcaption>I’m embarrassed to see that “hits” was still part of Urchin at the time. <a href="http://www.kaushik.net/avinash/" rel="noopener ugc nofollow" target="_blank">Avinash</a> is rolling his eyes right now.</figcaption></figure><p id="5158">Urchin 5 had e-commerce/”ROI” tracking, the Campaign Tracking Module, and multiserver versions that could all conspire to get the price pretty high. But the real killer feature IMHO, which was released in Urchin 6, was individual visitor history drill-down. If this sounds potentially, um, sensitive, that’s because it is. Google wouldn’t touch this feature and it was summarily axed, never to return.</p><figure><figcaption>Individual visitor history drilldown — potentially controversial I guess. But at least there wasn’t a “composite sketch” of the visitor. That would have been SO COOL. This is Urchin 6.</figcaption></figure><p id="7c7b">Up through Urchin 5, we’d been a traditional licensed-software outfit — you pay us money, you own the software. But by 2004 it was obvious we needed a hosted version (“hosted” would later become “cloud”, but we didn’t know that yet.) So we bought a bunch of servers, upgraded our T1, and released Urchin 6, which was available in on-premises (you run it yourself) or hosted by us, for $500/month(!) We didn’t have much time left as an independent concern, but businesses were surprisingly willing to pay up for the privilege of not having to run Urchin themselves. That business was a winner from day 1.</p><figure><figcaption>Paul Botto, me, and Brett at <a href="https://en.wikipedia.org/wiki/Search_Engine_Strategies" rel="noopener ugc nofollow" target="_blank">Search Engine Strategies</a> 2004, San Jose, where we first met the Google people.</figcaption></figure><p id="8c04">By Summer 2004, Urchin had the largest installed base among web analytics vendors on a number-of-websites basis[3]. Tradeshows had become fun again, and we planned our biggest spread yet for Search Engine Strategies 2004, in San Jose. It was there that two Google people, <a href="https://www.felicis.com/team/wesley-chan/" rel="noopener ugc nofollow" target="_blank">Wesley Chan</a> (PM) and David Friedberg (Corp. Dev.), went “shopping” as they put it, for a web analytics company. I guess they weren’t overly put-off by what they saw.</p><figure><figcaption>The Google Dance was a big party thrown in conjunction with the Search Engine Strategies tradeshow for a few years. During the “Dance” in ’04, Paul, Jack, and Brett, iirc, were off doing some sneaky corp. dev. work.</figcaption></figure><p id="0fb0">A few weeks later Google had made an offer for the company. By then we had interest from other quarters too — remember WebSideStory? They were actually a public company at the time and offered us more. But I think we made the right choice.</p><figure><figcaption>Brett, some dude named Al, and David Friedberg, in 2006</figcaption></figure><p id="fe31">Sidebar: Friedberg left Google in 2006 to start what became The Climate Corporation, which was <a href="http://techcrunch.com/2013/10/02/monsanto-acquires-weather-big-data-company-climate-corporation-for-930m/" rel="noopener ugc nofollow" target="_blank">acquired</a> for over $1 billion by Monsanto. Now that’s an exit. He also started <a href="http://metromile.com/" rel="noopener ugc nofollow" target="_blank">Metromile</a> and <a href="http://eatsa.com/" rel="noopener ugc nofollow" target="_blank">Eatsa</a>. Legit!</p><figure><figcaption>By 2004, we were feeling pretty smug about ourselves, despite still being puny. Design by <a href="https://about.me/mayorrock" rel="noopener ugc nofollow" target="_blank">Merrick</a>.</figcaption></figure><p id="e952">Selling the company was a needlessly rough process. It should have wrapped up just after the Google IPO in late 2004, but frankly the Google lawyers were prickly CYA types, demanding all kinds of IP-related protection — the four founders were personally on the hook if we were later found to be violating anyone’s patents etc. As we were now part of the big G, it seemed distinctly possible that WebTrends or someone would see fit to sue us. In hindsight, probably all boilerplate, but it was scary to risk it all like that. By the time we finally got it done, it was April 2005, and Google’s stock had doubled (half our payout was in stock). Oh well.</p><figure><figcaption>Brett prepares to “fax” the signed acquisition agreement back to Google. By this time we were sufficiently profitable that it was a tough decision to sell. Brett signed the actual, final paperwork in a tuxedo about 30 seconds before walking down the aisle at his wedding.</figcaption></figure><p id="688b">The still-young Google of 2005 was, I think, a lot more fun than the Mature Google/Alphabet of today. It was small enough (~3,000 employees) that everyone could get together for one (awesome, incredible) holiday party. And MC Hammer was always around. That was cool.</p><figure><figcaption>Jack, MC Hammer, and <a href="https://lowercasecapital.com/proprietor/" rel="noopener ugc nofollow" target="_blank">Chris Sacca</a>, circa 2005.</figcaption></figure><p id="feef">The day we joined, Eric Schmidt took time out of his day to hang out with us and learn about our flavor of web analytics. He immediately saw the potential with regard to Adwords spend, and was ever after helpful and available. I really liked the guy. Years later, Brett (then Sr. Director of Marketing) had an office right next to Eric, and they were bros. Kinda. I mean, billionaires are not like the rest of us.</p><figure><figcaption>Eric Schmidt gets to know the Urchin sales team, plus some others. L-R: Nick Mihailovski, Mike Chipman, Jim Napier, Megan Cash, Eric Schmidt, Paul Botto, Rolf Schreiber, Jason Senn, and Jack Ancone.</figcaption></figure><p id="94fa">Our first office at Google (for some of us) was the “fishbowl” of Building 42, in the nucleus of the Mountain View campus. We were actually really close to Larry &amp; Sergey for awhile. Sergey had this laser engraver in his office with a long air duct snaking down the hall to vent the gases. He’s a nut, that Sergey. The fishbowl was also home to a new Google engineer named Mike Stoppelman, whose brother would soon start a company called Yelp. Mike is SVP of engineering there as of this writing.</p><figure><figcaption>Urchin stuff would plague the Google campus for years after we joined. I’m sure some of it still lurks in supply cabinets here and there.</figcaption></figure><p id="0b35">The pervasiveness of Google Analytics now seems kind of a given, but in the spring of 2005, we were fairly panicked that its Google-fied release would be greeted with a shrug. So Wesley Chan, the Google PM who spearheaded the integration effort, commenced a daily “war room” and everyone was given milestones (<a href="https://medium.com/@brettc/implementing-okrs-a-tale-from-the-trenches-f4c42059fa17" rel="noopener">OKRs</a>, in Google land) with pretty tight timelines. Paul got a bunch of engineering tasks, I got a bunch of sales-ish/adoption tasks around penetrating the Fortune-500, Brett got marketing/PR/branding stuff, and Jack got BD/partnerships.</p><figure><figcaption>Camo’ was hot in 2005… this was the last shirt we made pre-Google, and the first we gave away at TGIF once we joined.</figcaption></figure><p id="61d5">It was a fun and hectic time, with much of the effort centered around bringing the rest of Google up to speed — some of us toured the nation visiting the various remote Google offices, small outposts that appreciated visitors from the mothership. By the time we thought we were ready for a public launch in November of 2005, we were sweating it. Would anyone care? Turns out they did. After we announced “Urchin from Google” was now free for any website in the world, the demand was sufficiently high that even Google’s infrastructure (well, the part allocated to us) was groaning, and the SRE team made us shut down signups until we could arrange the server resources etc. we needed to reopen.</p><figure><figcaption>Google Analytics was once knows as “Urchin from Google,” catchy right?</figcaption></figure><p id="434e">This is one of those nice-to-have problems, but a lot of people were still pissed off. Several months later, signups were reopened via the old invitation model, and the GA saturation we know today started happening in earnest.</p><figure><figcaption>Alden DeSoto (chief GA tech writer), Jeff Veen, Brett, Greg Veen, Ryan Carver, and Jeff Gillis (GA marketing) sporting the new (2006) Google Analytics track jackets, celebrating the re-launch. A few years hence the Veens and Ryan would start TypeKit, which was later acquired by Adobe. 2nd exit for those guys, nice.</figcaption></figure><p id="c4cf">Now, Google buys a lot of companies. Some of these are huge successes, like YouTube and Keyhole (Google Earth), but many just kind of scatter to the four winds, despite being worthy. <a href="https://en.wikipedia.org/wiki/Dodgeball_%28service%29" rel="noopener ugc nofollow" target="_blank">Dodgeball</a>, for example. I think this happens partially because of the way Google buys companies and partially because of big-company inertia/fog. Under some dollar amount (I’ve heard $50 million) all it took was one VP to say “buy them!” and it was done. Once they get onboarded, that VP maybe had left, or may have just been distracted. Turns out no one else cares, and the corporate soup subsumes the employees and the product melts into a shapeless saltine. In reality, no one at Google much cared about a product if it didn’t generate at least something like $100 million per year. Urchin of course didn’t either, but we were lucky enough to have some powerful allies: Wesley Chan and Eric Schmidt. Wesley was a PM (product manager) who understood Google needed effective analytics to drive Adwords spend. He was also dead determined not to let “his” acquisition be a failure, and he never let up until it could motor under its own power. But of course nothing can compare to having the CEO on your team, and lucky for us, Eric immediately got how web traffic analysis could positively affect Adwords. A few years later, Google did a big internal study with a bunch of “quants” running various models, and they pretty well proved an XX% increase in ad spending across the broad swath of customers studied. That was big money, with a “B.”</p><figure><figcaption>Up and to the right! Now that I’ve moved out of San Francisco, I wear Google shirts a lot more often.</figcaption></figure><p id="4484">In 2006, the Urchin people started drifting around Google, and some left. Today, my guess is about 12–15 original Urchin types are still there, and some still work on GA. Most notably, Paul is a senior VP of engineering, with hundreds of engineers reporting to him. He owns not just GA, but display ads too. Smart guy, that <a href="http://adage.com/article/digital/google-s-neal-mohan-heads-youtube-product-boss/301409/" rel="noopener ugc nofollow" target="_blank">Paul</a>.</p><figure><figcaption>The last Urchin-specIfic shirt we ever made, in about 2009(?). Little-known fact: Urchin was sold as standalone software until 2012, since many educational, government, and corporate customers wanted on-premises analytics software. Some still do, and former Urchinite Mike Chipman started a company to serve them — the product is compatible with Urchin databases and is called <a href="http://actualmetrics.com/" rel="noopener ugc nofollow" target="_blank">Angelfish</a>.</figcaption></figure></div></div><div><p id="daae">Urchin/GA have touched a lot of people, and that’s probably the most satisfying thing about this whole adventure. It’s also great to see Urchin people like <a href="https://www.linkedin.com/in/nick-mihailovski-1199264" rel="noopener ugc nofollow" target="_blank">Nick Mihailovski</a> and Nathan Moon do so well at Google. And of course Paul Muret, who is now a top exec in the engineering org. I’m also happy and very relieved that all our investors made money and got Google shares at 2005 prices. Thank you again for making this bumbling comedy of errors work out in the end. We (obviously) couldn’t have done it without you.</p><figure><figcaption>We were based in San Diego, after all</figcaption></figure></div><div><p id="5f21">Footnotes</p><ol><li id="a948">Laserdiscs: The 12" ones, like a shiny record. I’m sure some people still prefer them.</li><li id="7334"><em>The moneys will come</em> was the mantra of one Chi Kwan, president of our Japan division. Quite possibly the stupidest thing we ever did was open an office in Tokyo. It absolutely incinerated cash, and didn’t sell anything. We must have blown close to a million dollars on that poorly considered idea. That said, we did finally figure out how to make money in Japan — partner with a distributor. They took a 70% cut, but they did everything, including put it on a CD in an actual physical box. Hats off to <a href="https://www.runexy.co.jp/en/" rel="noopener ugc nofollow" target="_blank">Runexy</a>! And we have a fun phrase to say all these years later. Anytime I’m short on cash, I just repeat “the moneys will come,” and they always do.</li><li id="4b4d">Most-sites marketshare — this did not translate to most revenue. Kind of a vanity metric, but we got a lot of mileage out of it.</li></ol><figure><figcaption>Viking Urchin? Seriously?</figcaption></figure></div><div><p id="1005">Epilogue: Some lessons learned</p><p id="3e5a">If I had to do it over again, I’d do a few things differently, of course. Here’s a distillation of my/our painfully learnt truths, along with some reinvention of the wheel wisdom we had to live to believe. Dispensing generalized advice is always a dicey business, but I think I can defend these. Read with a grain of salt.</p><ol><li id="bd7d">Be willing to leave money on the table. I can’t emphasize this enough. Deal velocity is far more important than getting every dollar you “should” from a customer. Doing two deals for $1 each is better than one deal for $2.</li><li id="c05e">Reduce legalese, reduce text, shrink your contracts. One page is best. Somebody, probably an underling concerned with CYA, is actually going to have to read that 40-pages of lawyerspeak and it’s going to take that poor sap a long time. Make it so your counterpart on the deal reads it instead and execute the deal that day. Shun terms that you need a lawyer to interpret. Warren Buffett lent Goldman Sachs $5 BILLION on a hand-written note. It’s a legally binding agreement. Simple=less room for wiggling, and no excuse to delay.</li><li id="f2dd">Time kills all deals (just to recap 1 and 2).</li><li id="245b">Specialize sooner. Narrow your market. No one believes you if you say you serve all customers. Legitimacy/credibility are contingent on tight focus. No one can do it all, but even if you could, no one will believe it.</li><li id="681f">Put more thought into the right sales incentives and don’t limit the earnings of your sales people. Make commissions bigger as targets are hit, retroactively to all dollars — eg., maybe the commission is 8% up to $100k/month, then it becomes 10%, which applies to the entire $100k, not just the marginal amount above that. If your sales people make more than the CEO from time to time, you’re on the right track.</li><li id="622d">Pay commissions at least monthly — quarterly is too long to wait, and will hurt motivation. Sales people think short term, as they should.</li><li id="e47c">Accounting transparency — be more transparent with employees, sooner, until you are really uncomfortable. All employees should have access to a sales dashboard showing daily/monthly sales in real time, along with a “here’s our nut” indicator so everyone knows what to shoot for — profitability. The nut itself can be just a simple number, but the more transparent the better, even with salaries. They should be defensible, after all.</li><li id="7d91"><a href="https://play.google.com/store/books/details?id=HHJVIpbpSgsC&amp;utm_source=HA_Desktop_US&amp;utm_medium=SEM&amp;utm_campaign=PLA&amp;pcampaignid=MKTAD0930BO1&amp;gl=US&amp;gclid=CNmb7uj4-MwCFVRtfgod5PwLoQ&amp;gclsrc=ds" rel="noopener ugc nofollow" target="_blank">Overcommunicate organizational clarity</a>.</li><li id="544d">Hiring is a bitch, firing is harder. Everyone talks about hiring “A” players and sh*t like that. Of course, that implies you can divine which ones they are without seeing their work <em>at your company</em>. You’re better at hiring than I, if so. The reality is that you need to have the backbone to fire people that don’t work out. Do it as soon as it’s clear it’s the right move. Be generous with people you offload — if they speak positively of your company after the experience, that means you did it right. Do your best to hire the best (smarter than you is a good thing to strive for), but let’s admit the truth: it’s a crapshoot. Try hard, then cut loose your mistakes. Then there’s no fault, no guilt.</li><li id="33bc">More on hiring: Degrees are for working at Google. Also take a good look at highschool/college dropouts that are awesome. They usually can’t get a job at Google, so they might actually meet with you.</li><li id="a5c6">Money will burn a hole in your pocket. Consider only raising as much as you need to get to break-even, plus some % to account for how slow things are in the real world. Every $1 you raise is another $10 you have to sell for to make VCs happy. Moving the goalposts farther away makes it harder to score (sorry, I hoped to avoid sports metaphors).</li><li id="184f">And finally, the most important “rule” of all: if you want someone to read your email, make it short.</li></ol><figure><figcaption>At lease two of the UoDs were sporting cocktails</figcaption></figure></div><div><p id="6c2e">Thanks for reading! You rule.</p></div></div></article></div>]]></description>
        </item>
    </channel>
</rss>