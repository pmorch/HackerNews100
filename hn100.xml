<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 12 Sep 2023 12:00:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[CVE-2023-4863: Heap buffer overflow in WebP (Chrome) (101 pts)]]></title>
            <link>https://chromereleases.googleblog.com/2023/09/stable-channel-update-for-desktop_11.html</link>
            <guid>37478403</guid>
            <pubDate>Tue, 12 Sep 2023 08:58:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chromereleases.googleblog.com/2023/09/stable-channel-update-for-desktop_11.html">https://chromereleases.googleblog.com/2023/09/stable-channel-update-for-desktop_11.html</a>, See on <a href="https://news.ycombinator.com/item?id=37478403">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><span itemprop="datePublished">
Monday, September 11, 2023
</span>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open source P2P alternative to Slack and Discord built on Tor and IPFS (201 pts)]]></title>
            <link>https://github.com/TryQuiet/quiet</link>
            <guid>37477512</guid>
            <pubDate>Tue, 12 Sep 2023 06:29:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/TryQuiet/quiet">https://github.com/TryQuiet/quiet</a>, See on <a href="https://news.ycombinator.com/item?id=37477512">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto">
   <a target="_blank" rel="noopener noreferrer nofollow" href="https://avatars.githubusercontent.com/u/59660937?s=200&amp;v=4"><img width="150" height="150" src="https://avatars.githubusercontent.com/u/59660937?s=200&amp;v=4" alt="Logo"></a>
  </p>
  <h2 tabindex="-1" dir="auto"><b>Quiet</b></h2>
  

Quiet is an alternative to team chat apps like Slack, Discord, and Element that does not require trusting a central server or running one's own. In Quiet, all data syncs directly between a team's devices over <a href="https://torproject.org/" rel="nofollow">Tor</a> with no server required.
<blockquote>
<p dir="auto">NOTE: Quiet is not audited and should not be used when privacy and security are critical. It lacks basic features and probably won't replace your Slack or Discord yet. That said, it works surprisingly well and we use it daily as a Slack replacement.</p>
</blockquote>
<p dir="auto">Quiet is for fans of software freedom, decentralization and privacy tech, and for anyone craving a future where humanity can collaborate effectively online without trusting our communities, networks, and data to giant corporations.</p>
<p dir="auto"><strong>Quiet is written (mostly) in TypeScript, with Electron and React Native frontends, and welcomes outside contributions! See: <a href="#contributing-to-quiet">Contributing to Quiet</a></strong></p>
<div dir="auto">
  <p><a target="_blank" rel="noopener noreferrer nofollow" href="https://user-images.githubusercontent.com/213678/177447638-29d6805c-5458-4f5e-b4ed-7a5d6cb51f6e.png"><img src="https://user-images.githubusercontent.com/213678/177447638-29d6805c-5458-4f5e-b4ed-7a5d6cb51f6e.png" alt="Screenshot"></a></p></div>
<h2 tabindex="-1" dir="auto">How it works</h2>
<p dir="auto">While apps like Slack, Discord, and Signal use central servers, Quiet syncs messages directly between a team's devices, over Tor, with no server required.</p>
<p dir="auto">Each group of people (Quiet calls them "communities") gets their own insular network, so that data from one community never touches the devices of Quiet users in <em>other</em> communities. Not even in encrypted form!</p>
<p dir="auto">Message syncing is taken care of by a project called <a href="https://orbitdb.org/" rel="nofollow">OrbitDB</a>, which works like a mashup of Git, a <a href="https://en.wikipedia.org/wiki/Gossip_protocol" rel="nofollow">gossip protocol</a>, and <a href="https://en.wikipedia.org/wiki/BitTorrent" rel="nofollow">BitTorrent</a>; it broadcasts new messages, syncs the latest messages, and fetches files. Syncing means that users typically receive all messages sent while they were offline.</p>
<p dir="auto">Invites, access, and usernames are granted by a community owner, i.e. whoever creates the community. The owner hands out an "invitation code" which invitees use to connect to the owner's device, register a username, and get a standard cryptographic certificate so they can prove to other peers they're part of the community.</p>
<p dir="auto">See our <a href="https://github.com/TryQuiet/monorepo/wiki/Quiet-FAQ">FAQ</a> for answers to common questions and a comparison of Quiet with similar apps.</p>
<h2 tabindex="-1" dir="auto">Getting started</h2>
<p dir="auto">To try Quiet, download the <a href="https://github.com/TryQuiet/quiet/releases/tag/quiet%401.8.0">latest release</a> for your platform (.dmg for macOS, .exe for Windows, etc.) and install it in the normal way. Then create a community and open the community's settings to invite members.</p>
<p dir="auto">If you'd like to help develop Quiet, see <a href="#contributing-to-quiet">Contributing to Quiet</a>.</p>
<h2 tabindex="-1" dir="auto">Features</h2>
<ul dir="auto">
<li><strong>Team Chat</strong> - Create a "community" for your team or organization and invite members.</li>
<li><strong>End-to-end Encryption</strong> - All data is encrypted end-to-end between member devices.</li>
<li><strong>Channels</strong> - Organize chats in Slack-like channels.</li>
<li><strong>Images</strong> - Send and receive images, with copy/paste, drag &amp; drop, and image previews.</li>
<li><strong>Files</strong> - Send and receive giant files without arbitrary limits.</li>
<li><strong>Notifications</strong> - Get desktop notifications for new messages, with optional sounds.</li>
<li><strong>Invite links</strong> - Share invite links, just like in WhatsApp, Signal, or Discord.</li>
<li><strong>Keyboard Controls</strong> - Navigate channels without using the mouse.</li>
<li><strong>Desktop Apps</strong> - Desktop apps for Mac, Windows, and Linux.</li>
<li><strong>Android App</strong> - A fully peer-to-peer Android app with working notifications.</li>
<li><strong>No email or phone number required</strong> - Unlike Slack, Discord, WhatsApp, Telegram, and Signal, no email or phone number is required to create or join a community.</li>
</ul>
<h2 tabindex="-1" dir="auto">Planned (but still-missing) features</h2>
<ul dir="auto">
<li><strong>iOS App</strong> - Join communities and sync messages on iOS, with no central server.</li>
<li><strong>Direct Messages</strong> - Send and receive direct messages that are encrypted to the recipient and unreadable by other community members.</li>
<li><strong>Mentions</strong> - Send @ mentions that notify other users.</li>
<li><strong>Removal</strong> - Remove users from your community.</li>
<li><strong>User Profiles</strong> - Add an avatar or bio.</li>
<li><strong>Message Deletion</strong> - Delete individual messages and set timed deletion rules ("disappearing messages") for the community.</li>
<li><strong>Status</strong> - See your own connection status and the online status of other users.</li>
<li><strong>Reactions</strong> - React with emojis.</li>
<li><strong>Multiple Communities</strong> - Join multiple communities, as you would in Slack or Discord.</li>
<li><strong>Account Recovery</strong> - Recover owner accounts from a backup phrase.</li>
<li><strong>Private channels</strong> - Create private channels with multiple members that are unreadable to the community at large.</li>
</ul>
<h2 tabindex="-1" dir="auto">Post-1.0 Features</h2>
<ul dir="auto">
<li><strong>Large Communities</strong> - Create a community with 1000 members or more (right now ~30-100 members is the limit.)</li>
<li><strong>Moderation</strong> - Appoint moderators who can hide messages and shadowban or remove users.</li>
<li><strong>Spam and Denial-of-Service Protection</strong> - Settings to automatically remove users who send disruptive messages.</li>
<li><strong>Search</strong> - Robust message search.</li>
<li><strong>Threads</strong> - Reply to messages in threads.</li>
<li><strong>Tor Bridges</strong> - Connect via public or private bridges to avoid Internet censorship.</li>
<li><strong>Tor Browser Support</strong> - Join communities as a full member with Tor Browser, without downloading an app.</li>
<li><strong>Browser Support</strong> - Join communities with <em>any</em> modern browser via <a href="https://gitlab.torproject.org/tpo/core/arti/-/issues/103" rel="nofollow">Arti-in-WASM</a>.</li>
<li><strong>Publishing</strong> - Share files (or entire websites) from your community to the web, via Tor, <a href="https://github.com/asn-d6/onionbalance">OnionBalance</a>, and <a href="https://www.tor2web.org/" rel="nofollow">Tor2web</a> + IPFS.</li>
</ul>
<h2 tabindex="-1" dir="auto">Technical overview</h2>
<p dir="auto">This is a concise technical summary of the main points.</p>
<ol dir="auto">
<li><strong>Granting access:</strong> community owners use standard PKI (<a href="https://pkijs.org/" rel="nofollow">PKI.js</a>) to grant access, with each community owner serving as the community's <a href="https://en.wikipedia.org/wiki/Certificate_authority" rel="nofollow">certificate authority</a>; this is handled by Quiet and transparent to users.</li>
<li><strong>Authentication:</strong> a valid signed certificate from the community owner is required to connect to peers, receive connections from peers, and for messages to be visible to other peers.</li>
<li><strong>Networking:</strong> peers connect via <a href="https://en.wikipedia.org/wiki/Tor_(network)#Onion_services" rel="nofollow">Tor onion services</a>, exclusively with their fellow community members.</li>
<li><strong>Privacy:</strong> Tor encrypts all data in transit, and a Quiet user's device connects only to the devices of their fellow community members, so all messages are encrypted to recipients.</li>
<li><strong>Syncing:</strong> IPFS and <a href="https://orbitdb.org/" rel="nofollow">OrbitDB</a>, an <a href="https://ipfs.io/" rel="nofollow">IPFS</a>-based <a href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type" rel="nofollow">CRDT</a>, ensure that all data (messages, user data, etc) syncs between peers with <a href="https://arxiv.org/abs/2012.00472" rel="nofollow">eventual consistency</a>.</li>
<li><strong>Asynchronous messaging:</strong> because messages sync to all members, members can communicate without being contemporaneously online, provided that there is "continuous liveness", a continuous chain of online peers who each sync the latest updates, between the sender and the recipient.</li>
<li><strong>Identity:</strong> a valid certificate from the community owner on account creation establishes a username, which the owner attests is unique; in future versions, Quiet will warn all members if community owners are caught issuing non-unique usernames, to protect against impersonation by malicious or compromised owners. (See: <a href="https://github.com/TryQuiet/monorepo/issues/119" data-hovercard-type="issue" data-hovercard-url="/TryQuiet/quiet/issues/119/hovercard">#119</a>)</li>
<li><strong>Invitation:</strong> to invite new members, community owners provide (via some other secure channel) an onion address that points to a registration API which accepts a certificate signing request, responds with a signed certificate, and provides sufficient peer information to connect to other peers; in future versions this onion address will expire. (See: <a href="https://github.com/TryQuiet/monorepo/issues/536" data-hovercard-type="issue" data-hovercard-url="/TryQuiet/quiet/issues/536/hovercard">#536</a>)</li>
<li><strong>Account recovery:</strong> owners must back up their data (e.g. by copying a folder, or someday with a wallet-style passphrase) and members request new accounts from owners.</li>
<li><strong>User removal:</strong> TBD, but likely a combination of expiring invitation onion addresses, certificate revocation, and message-layer encryption with updated keys.</li>
<li><strong>Multiple device support:</strong> TBD, but most likely based on <a href="https://github.com/local-first-web/auth">local-first-web/auth</a></li>
<li><strong>Mobile push notifications:</strong> barring a major victory for consumer rights, iOS notifications require using a centralized push notification service that connects to Apple, but message data can still be encrypted; in proof-of-concept, Quiet works well as an always-on background app on Android, so Android versions will likely not require a push notification server.</li>
<li><strong>Stack:</strong> Our backend is in Node.js (on iOS/Android we use <a href="https://github.com/nodejs-mobile">nodejs-mobile</a>); we use Electron on desktop and React Native on mobile.</li>
</ol>
<h2 tabindex="-1" dir="auto">Our Mission</h2>
<p dir="auto">We are building Quiet to sharpen the tools that <a href="https://en.wikipedia.org/wiki/Open_society" rel="nofollow">open societies</a> use to hold power accountable. Each year, movements use the Internet to hold power accountable in breathtaking new ways. But the rise of big tech has made the Internet <em>itself</em> seem like <em>yet another</em> unaccountable power. The medium that brought us <em>Occupy</em> Wall Street now looks like regular old Wall Street. We believe this happened because software became too dependent on company-run infrastructure, which undermined the role <a href="https://en.wikipedia.org/wiki/Free_software" rel="nofollow">free software</a> has historically played in holding the software industry accountable. Our goal is to fix that.</p>
<p dir="auto">In the 2000s, when key dominant tech products had viable free software competitors that were radically pro-user (products like <a href="https://en.wikipedia.org/wiki/Firefox" rel="nofollow">Firefox</a>, <a href="https://en.wikipedia.org/wiki/BitTorrent" rel="nofollow">BitTorrent</a>, <a href="https://www.videolan.org/" rel="nofollow">VLC</a>, <a href="https://en.wikipedia.org/wiki/HandBrake" rel="nofollow">Handbrake</a>, or <a href="https://en.wikipedia.org/wiki/Linux" rel="nofollow">Linux</a>) there was a limit to how much big tech could abuse users before users fled.</p>
<p dir="auto">But software for communication and collaboration seemed to require servers, whose cost grew with the software's popularity, so the question "who runs the server?" became a dilemma for free software projects. Should the project itself run the server? What about when costs grew too high? Should users run the server? But only a small niche of hobbyists have servers! Should an organization run the server? If so, then that organization now controls the data and relationships that make the product useful, limiting the freedom to <a href="https://en.wikipedia.org/wiki/Fork_(software_development)#Forking_of_free_and_open-source_software" rel="nofollow">fork</a> and flee that makes free software so accountable and desirable. Reddit, for example, <a href="https://www.reddit.com/r/changelog/comments/6xfyfg/an_update_on_the_state_of_the_redditreddit_and/" rel="nofollow">was once free software</a>, but because forking Reddit's <em>code</em> would never have resulted in anything more than an empty website (since all the conversations and relationships that make Reddit what it is sit on <em>company-run servers</em>) Reddit being free software never gave Reddit's users any real power to hold it accountable.</p>
<p dir="auto"><a href="https://en.wikipedia.org/wiki/Federation_(information_technology)" rel="nofollow">Federation</a> is a proposed solution to this dilemma, but Gmail shows its limits. After all, email is the most well-known federated product, but Google can still build must-have features like spam filtering on the server side, and Gmail controls a user's email address, so exiting Gmail means updating dozens or hundreds of accounts created with that address. Exiting Gmail might be easier than exiting Facebook or Instagram, but no Gmail competitor can make exiting Gmail as easy and delightful an experience as Firefox made exiting Internet Explorer, because Gmail controls infrastructure, where Internet Explorer never did. So while federation does help, we must do better if we want to hold big tech accountable.</p>
<p dir="auto">Regulation is an even weaker proposed solution. Even when regulation works—and a quick look at the media, telecom, energy, or banking industries will illustrate its limits—regulation tends to create a cozy relationship between industry and regulators that makes industries easy targets for government subversion. For example, the highly-regulated telecom industry <a href="https://www.theguardian.com/world/2013/jun/06/nsa-phone-records-verizon-court-order" rel="nofollow">bends</a> <a href="https://www.vice.com/en/article/wx8jax/researchers-find-powerful-ss7-cellphone-location-surveillance-in-europe-middle-east-australia" rel="nofollow">over</a> <a href="https://en.wikipedia.org/wiki/Room_641A" rel="nofollow">backwards</a> every time governments want help carrying out unpopular mass surveillance. Is this what we want from big tech?</p>
<p dir="auto">We're building Quiet because we believe that, for a broad and growing class of software, the best answer to the "who runs the server?" dilemma is "no one." Eliminate the server; in terms of accountability, it is a burden and a weakness. By eliminating servers from software's <a href="https://en.wikipedia.org/wiki/Attack_surface" rel="nofollow">attack surface</a>, software can be more private and secure. By eliminating exponentially growing server costs and the expertise-intensive work of scaling servers, software can be built by smaller teams under less financial pressure to betray users. Most importantly, by eliminating the server operator's control of relationships and data, users will be free to fork and exit, so they will once again have real power to hold software accountable.</p>
<p dir="auto">We're building Quiet to spark a new phase of the free software movement where it is easy and normal to build apps this way. We want to make a private alternative to Slack &amp; Discord that people love, to figure out the best and easiest technical approach along the way, and—by doing all this—to blaze a trail that other free software teams building other products can follow. Once one team (us, we hope!) can build a good alternative to Slack that doesn't use servers, other teams can build alternatives to Google Docs, Figma, Asana, Trello, 1Password, and so on, until someday—and this is technically much more difficult—humanity can build fully-forkable alternatives to things like Facebook, Twitter, Instagram, or even more complex applications. Big tech's users will be free to flee, and the Internet can stop being yet another unaccountable power, and keep being the breathtaking medium for holding power accountable that open societies need.</p>
<p dir="auto">Join us, and let's figure this out.</p>
<h2 tabindex="-1" dir="auto">Contributing to Quiet</h2>
<p dir="auto">Even though Quiet is completely peer-to-peer, it is mostly written in TypeScript and will be familiar to anyone accustomed to Node.js web development. Desktop and mobile versions share a common Node.js <a href="https://github.com/TryQuiet/monorepo/tree/develop/packages/backend">backend</a> and React <a href="https://github.com/TryQuiet/monorepo/tree/develop/packages/state-manager">state manager</a>, with <a href="https://torproject.org/" rel="nofollow">Tor</a> binaries for each platform and architecture, using Electron and React Native and for their respective frontends.</p>
<p dir="auto">To get started hacking on Quiet, follow the instructions for <a href="https://github.com/TryQuiet/quiet/blob/develop/packages/desktop/README.md">Quiet Desktop</a> or <a href="https://github.com/TryQuiet/monorepo/tree/develop/packages/mobile#readme">Quiet Mobile</a>. (If you're new to the project, start with Quiet Desktop, as it's more stable and vastly easier to start hacking on.) Here are some <a href="https://github.com/orgs/TryQuiet/projects/3/views/1?filterQuery=label%3A%22good+first+issue%22">good first issues</a>, and you can see upcoming priorities in our <a href="https://github.com/orgs/TryQuiet/projects/3/views/1">project board</a>.</p>
<p dir="auto">Most of all, if you're interested in contributing, be in touch! Drop us a line at <a href="mailto:h@quiet.chat">h@quiet.chat</a> and we'll add you to the project's Quiet community and (if you like) plan an onboarding session.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Death by a Thousand Microservices (377 pts)]]></title>
            <link>https://renegadeotter.com/2023/09/10/death-by-a-thousand-microservices.html</link>
            <guid>37477095</guid>
            <pubDate>Tue, 12 Sep 2023 05:05:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://renegadeotter.com/2023/09/10/death-by-a-thousand-microservices.html">https://renegadeotter.com/2023/09/10/death-by-a-thousand-microservices.html</a>, See on <a href="https://news.ycombinator.com/item?id=37477095">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="postBody">
    <h3 id="the-church-of-complexity">The Church of Complexity</h3>

<p>There is a pretty well-known sketch in which an engineer is explaining to the project manager how an overly complicated maze of 
microservices works in order to get a user’s birthday - and fails to do so anyway. The scene accurately describes the 
absurdity of the state of the current tech culture. We laugh, and yet bringing this up in a 
serious conversation is tantamount to professional heresy, rendering you borderline un-hirable.</p>

<p>
    <iframe src="https://www.youtube.com/embed/y8OnoxKotPQ?si=7qBEqGaN7ATD-Gex" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; 
        picture-in-picture; web-share" allowfullscreen="">
    </iframe>
</p>

<p>How did we get here? How did our aim become not addressing the task at hand but instead setting a pile of 
cash on fire by <strong>solving problems we don’t have</strong>?</p>

<h3 id="the-perfect-storm">The perfect storm</h3>

<p>There are a few things in recent history that may have contributed to the current state of things. First, a whole army of 
developers 
writing Javascript for the browser started self-identifying as 
“full-stack”, diving into server development and asynchronous code. Javascript is Javascript, right? What difference does it make 
what you create using it - user interfaces, servers, games, or embedded systems. <em>Right</em>? 
Node was still kind of a <a href="https://www.youtube.com/watch?v=M3BM9TB-8yA" target="_blank">learning project of one person</a>, and 
Javascript back then was a deeply problematic choice for server development.
<a href="https://notes.ericjiang.com/posts/751" target="_blank">Pointing this out</a> to still green server-side developers usually 
resulted in a lot of huffing and puffing. This is all they knew, after all. The world outside of Node 
effectively did not exist, the Node way was the only known way, and so this was the genesis of the 
stubborn, dogmatic thinking 
that we deal with to this day.</p>

<photo-element source="complexity/theway.webp" aspect="landscape" alt="Performance"></photo-element>

<p><strong>And then</strong>, a steady stream of 
<a href="https://en.wikipedia.org/wiki/Big_Tech" target="_blank">FAANG</a> veterans started merging into the river of startups, 
mentoring the newly-minted and highly impressionable young Javascript server-side engineers. The apostles of the Church of Complexity would 
assertively claim that “how they did things over at Google” was unquestionable and correct - even if it made no sense under 
current context and size. What 
do you <em>mean</em> you don’t have a separate <em>User Preferences Service</em>? That just <em>will not scale, bro!</em></p>

<p>But, it’s easy to blame the veterans and the newcomers for all of this. What else was happening? Oh yeah - easy money.</p>

<p>What do you do when you are flush with venture capital? You don’t 
<a href="https://www.youtube.com/embed/BzAdXyPYKQo?si=3lOVi1rhtaPC-nv4" target="_blank">go for revenue</a>, surely! On more than one 
occasion I received an email from management, asking everyone to be in the office, tidy up their desks and look busy, as a 
clouder of Patagonia vests was about to be paraded through the office. Investors needed to see explosive growth, but not in 
profitability, 
no. They just needed to see how quickly the company could hire ultra-expensive software engineers to do … <em>something</em>.</p>

<p>And now that you have these developers, what do you do with them? Well, they could build a simpler system that is easier to 
grow and
maintain, or they could conjure up a monstrous constellation of “microservices” that no one really 
understands. Microservices 
- the new way of writing scalable software! Are we just going to pretend that the concept of “distributed systems” never existed?
(Let’s skip the whole parsing of the nuances that microservices are not real distributed systems).</p>

<p>Back in the days when the tech industry was not such a bloated farce, distributed systems were respected, feared, and generally 
avoided - reserved only as the weapon of last resort for particularly gnarly problems. Everything with a distributed system 
becomes more challenging and time-consuming - development, debugging, deployment, testing, resilience. But I don’t know - maybe 
it’s all 
super easy now because 
<em>toooollling</em>.</p>

<p>There is no standard tooling for microservices-based development - there is no common 
framework. Working on distributed systems has gotten only marginally easier in 2020s. The Dockers and the 
Kuberneteses of the world did not magically take away the inherent complexity of a distributed setup.</p>

<p>I love referring to this
<a href="https://kenkantzer.com/learnings-from-5-years-of-tech-startup-code-audits/" target="_blank">summary of 5 years of startup audits</a>, 
as it is packed with common-sense conclusions based on hard evidence (and paid insights):</p>

<blockquote>
  <p>… the startups we audited that are now doing the best usually had an almost brazenly ‘Keep It Simple’ approach to
engineering. Cleverness for cleverness sake was abhorred. On the flip side, the companies where we were like ”woah,
these folks are smart as hell” for the most part kind of faded.</p>
</blockquote>

<p>Literally - “complexity kills”.</p>

<p>The 
<a href="https://podcasts.apple.com/mt/podcast/lessons-from-5-years-of-startup-code-audits/id341623264?i=1000567623452" target="_blank">audit</a>
revealed an interesting pattern, where many startups experienced a sort of collective imposter syndrome while building 
straight-forward, simple,
performant systems. There is a dogma attached to not starting out with microservices on day one - 
no matter the problem. “Everyone is doing microservices, yet we have a single Django monolith maintained by just a few engineers, 
and a MySQL instance - what are we doing wrong?”. The answer is almost always “nothing”.</p>

<p>Likewise, it’s common for seasoned engineers to experience 
hesitation and inadequacy in today’s tech world, and the good news is 
that, 
no - it’s probably not you. It’s common for teams to pretend like they are doing “webs cale”, hiding behind libraries, ORMs, and 
cache - 
confident in their expertise (they crushed that Leetcode!), yet they may not even be
<a href="https://www.reddit.com/r/programming/comments/f46f5a/comment/fhp26k8/?context=3" target="_blank">aware of database indexing basics</a>. 
You are operating in a sea of unjustified overconfidence, waste, and Dunning-Kruger, so who is really the imposter here?</p>

<h3 id="there-is-nothing-wrong-with-a-monolith">There is nothing wrong with a monolith</h3>

<p>The idea that you cannot grow without a system that looks like the infamous diagram of Afghanistan war 
strategy is largely a myth.</p>

<photo-element source="complexity/af.png" aspect="landscape"></photo-element>

<p>Dropbox, Twitter, Facebook, Instagram, Shopify, Stack Overflow - these companies and others started out as monolithic 
code bases. Many have a monolith at their core to this day. Stack Overflow makes it a 
<a href="https://stackexchange.com/performance" target="_blank">point of pride</a> how little hardware they need to run the massive site. 
Shopify is still a <a href="https://blog.quastor.org/p/shopify-ensures-consistent-reads">Rails monolith</a>, leveraging the tried and true 
<a href="https://twitter.com/ShopifyEng/status/1597983928018948096" target="_blank">Resque</a> to proces billions of tasks.</p>

<p>WhatsApp went supernova with their 
<a href="https://blog.quastor.org/p/whatsapp-scaled-1-billion-users-50-engineers" target="_blank">Erlang monolith and 50 engineers</a>.
How?</p>

<blockquote>
  <p>WhatsApp consciously keeps the engineering staff small to only about 50 engineers.</p>

  <p>Individual engineering teams are also small, consisting of 1 - 3 engineers and teams are each given a great deal of autonomy.</p>

  <p>In terms of servers, WhatsApp prefers to use a smaller number of servers and vertically scale each server to the highest 
extent possible.</p>
</blockquote>

<p>Instagram was acquired for billions - with a crew of 12.</p>

<p>And do you imagine Threads as an effort involving a whole Meta campus? Nope. They followed the
<a href="https://instagram-engineering.com/static-analysis-at-scale-an-instagram-story-8f498ab71a0c" target="_blank">Instagram model</a>, 
and this is the entire Threads team:</p>

<photo-element source="complexity/threads-team.webp" aspect="landscape"></photo-element>

<p>Perhaps claiming that <em>your</em> particular problem domain requires a massively complicated distributed system and an open office 
stuffed 
to the gills with turbo-geniuses is just crossing over into 
arrogance rather than brilliance?</p>

<h3 id="dont-solve-problems-you-dont-have">Don’t solve problems you don’t have</h3>

<p>It’s a simple question - what problem are you solving? Is it scale? How do you know how to break it all up for scale and performance? 
Do you have enough data to show what needs to be a separate service and why? Distributed systems are built for size and 
resilience. Can your system scale and be resilient at the same time? What happens if one of the services goes down or comes to a crawl? 
Just scale it up, yes? What about the <em>other</em> services that will get flooded with load? Did you war-game the endless permutations 
of things that can and will go wrong? Is there back pressure? Circuit breakers? Queues? Jitter? Sensible timeouts on every 
endpoint? Are there fool-proof guards to make sure a simple change does not bring everything down? 
The knobs you need to be aware of and tune are endless, and they are all specific to your system’s particular signature of 
usage and traffic.</p>

<p>The truth is that most companies will never reach the massive size that will actually require building a true distribute 
system. Your cos playing Amazon and Google - without 
their scale, expertise, and endless resources - is very likely just an egregious waste of money and time.</p>

<p><em>The only thing harder than a distributed system is a BAD distributed system</em>.</p>

<photo-element source="complexity/twitter3.png" aspect="landscape" alt="Performance"></photo-element>

<h3 id="but-each-teambut-separate-but-api">“But each team…but separate… but API”</h3>

<p>Trying to shove a distributed topology into your company’s structure is a noble effort, but it almost always backfires. It’s a 
common approach 
to break up a problem into smaller pieces and then solve those one by one. So, the thinking goes, if you break up one service 
into multiple ones, everything becomes easier, right?</p>

<p>The theory is sweet and elegant - each microservice is being maintained rigorously by a dedicated team, 
walled off behind a beautiful, backward-compatible, versioned API. In fact, this is all so steely that you rarely even have to 
communicate with that team - as if the microservice was maintained by a 3rd party vendor. It’s <em>simple</em>!</p>

<p>If that doesn’t sound familiar, that’s because this rarely happens. In reality, our Slack channels are <em>flooded</em> with 
messages from teams communicating about releases, bugs, configuration updates, breaking changes, and PSAs. Everyone 
needs to be on top of everything, all the time. And if that wasn’t great, it’s normal for one 
already-slammed team to half-ass multiple 
microservices instead of doing a great job on a single one, often changing ownership as people come and go.</p>

<p>In order to win the race, we don’t build <em>one</em> good race car - we build a fleet of shitty golf carts.</p>

<photo-element source="complexity/twitter2.png" aspect="landscape" alt="Performance"></photo-element>

<h3 id="what-you-lose">What you lose</h3>

<p>There are multiple pitfalls to building with microservices, and often that minefield is either not fully appreciated or simply 
ignored. Teams spend months writing highly customized tooling and learning lessons not 
related at all to the core product. Here are just some often overlooked aspects…</p>

<h4 id="say-goodbye-to-dry">Say goodbye to DRY</h4>

<p>After decades of teaching developers to write Don’t Repeat Yourself code, it seems we just stopped talking about it altogether.
Microservices by default are not DRY, with every service stuffed with redundant boilerplate. Very often the overhead of such 
“plumbing” is so heavy, and the size of the microservices is so small, that the average instance of a service has more “service” 
than 
“product”. So what about the common code that <em>can</em> be factored out?</p>

<ul>
  <li>Have a common library?</li>
  <li>How does the common library get updated? Keep different versions everywhere?</li>
  <li>Force updates regularly, creating dozens of pull requests across all repositories?</li>
  <li>Keep it all in a monorepo? That comes with its <em>own</em> set of problems.</li>
  <li>Allow for some code duplication?</li>
  <li>Forget it, each team gets to reinvent the wheel every time.</li>
</ul>

<p>Each company going this route faces these choices, and there are no good “ergonomic” options - you <em>have</em> to 
choose your version of the pain.</p>

<h4 id="developer-ergonomics-will-crater">Developer ergonomics will crater</h4>

<p>“Developer ergonomics” is the friction, the amount of effort a developer
must go through in order to get something done, be it working on a new feature or resolving a bug.</p>

<p>With microservices, an engineer has to have a mental map of the entire system in order to know what services  to bring up for any 
particular task, what teams to talk to, whom to talk to, and what about. The “you have to know everything before 
doing anything” 
principle. How 
do you keep on top of it? Spotify, a 
multi-billion dollar company, spent probably not negligible internal resources to build 
<a href="https://backstage.spotify.com/" target="_blank">Backstage</a>, software for cataloging its endless systems and services.</p>

<p>This should at least give you a clue that this game is not for everyone, and the price of the ride is <em>high</em>. So what about 
the <em>tooooling</em>? The Not Spotifies of the world are left with McGivering their own solutions, robustness and portability of 
which you can probably guess.</p>

<p>And how many teams actually streamline the process of starting a <em>YASS</em> - “yet another stupid service”? 
This includes:</p>

<ul>
  <li>Developer privileges in GitHub/GitLab</li>
  <li>Default environment variables and configuration</li>
  <li>CI/CD</li>
  <li>Code quality checkers</li>
  <li>Code review settings</li>
  <li>Branch rules and protections</li>
  <li>Monitoring and observability</li>
  <li>Test harness</li>
  <li>Infrastructure-as-code</li>
</ul>

<p>And of course, multiply this list by the number of programming languages used throughout the company. Maybe you have a 
usable 
template or a runbook? Maybe a frictionless, one-click system to 
launch a new service from scratch? It takes months to iron out all the kinks with this kind of automation. So, you can either 
work on your product, or you can be working on <em>toooooling</em>.</p>

<h4 id="integration-tests---lol">Integration tests - LOL</h4>

<p>As if the everyday microservices grind was not enough, you also forfeit the peace of mind offered by solid integration tests. 
Your single-service and unit tests are passing, but are your critical paths still intact after 
each commit? Who is in charge of the overall integration test suite, in Postman or wherever else? Is there one?</p>

<photo-element source="complexity/unit.gif" aspect="original-size" alt="Service tests"></photo-element>

<p>Integration testing a distributed setup is a nearly-impossible problem, so we pretty much gave up on that and replaced it with 
another one - Observability. Just like “microservices” are the new “distributed systems”, “observability” is the new 
“debugging in production”. Surely, you are not writing real software if you are not doing…. observability!</p>

<p>Observability has become its own sector, and you will pay in both pretty penny and in developer 
time for it. It doesn’t come as plug-and-pay either - you need to understand and implement canary releases, feature flags, etc. 
Who is doing that? One already 
<a href="https://renegadeotter.com/2023/07/26/i-am-not-your-cloud-person.html">overwhelmed</a> engineer?</p>

<p>As you can see, breaking up your problem does not make solving it easier - all you get is another set of <em>even 
harder problems</em>.</p>

<h3 id="what-about-just-services">What about just “services”?</h3>

<p>Why do your services need to be “micro”? What’s wrong with
<a href="https://leeatchison.com/app-architectures/moving-beyond-microservices-hype/" target="_blank">just services</a>? Some
startups have gone as far as create <em>a service for
each function</em>, and yes, “isn’t that just like Lambda” is a valid question. This 
gives you an idea of how far gone this unchecked cargo cult is.</p>

<p>So what do we do? 
<a href="https://www.fearofoblivion.com/build-a-modular-monolith-first" target="_blank">Starting with a monolith</a>
is one obvious choice. A pattern that could also  work in many instances is “trunk &amp; 
branches”, where the main “meat and potatoes” monolith is helped by “branch” services. A branch service can be a service that 
takes care of a clearly-identifiable and
separately-scalable load. A CPU-hungry <em>Image-Resizing Service</em> makes way more sense than a <em>User Registration Service</em>. Or do you
get so many registrations per second that it requires independent horizontal scaling?</p>

<photo-element source="complexity/really.gif" aspect="original-size" alt="Vertical"></photo-element>



<h3 id="the-pendulum-is-swinging-back">The pendulum is swinging back</h3>
<p>The hype, however, seems to be dying down. The VC cash faucet is tightening, and so the
businesses have been market-corrected into exercising common-sense decisions, recognizing that perhaps splurging on 
web-scale architectures when they don’t have web-scale problems is not sustainable.</p>

<photo-element source="complexity/twitter1.png" aspect="landscape" alt="Vertical"></photo-element>

<photo-element source="complexity/twitter4.png" aspect="landscape" alt="AWS"></photo-element>

<p>Ultimately, when faced with the need to travel from New York to Philadelphia, you have two options. 
You can either attempt to construct a highly intricate spaceship for an orbital descent to your destination, or you can simply 
purchase an Amtrak train ticket for a 90-minute ride. <em>That</em> is the problem at hand.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hacking the Book8088 for better accuracy (111 pts)]]></title>
            <link>https://martypc.blogspot.com/2023/09/hacking-book8088-for-better-accuracy.html</link>
            <guid>37476588</guid>
            <pubDate>Tue, 12 Sep 2023 03:05:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://martypc.blogspot.com/2023/09/hacking-book8088-for-better-accuracy.html">https://martypc.blogspot.com/2023/09/hacking-book8088-for-better-accuracy.html</a>, See on <a href="https://news.ycombinator.com/item?id=37476588">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-1388372834381468609">
<p>The <a href="https://arstechnica.com/gadgets/2023/07/going-deep-with-the-book-8088-the-brand-new-laptop-that-runs-like-its-1981/" target="_blank">Book8088 </a>is an interesting little machine. It is essentially a 1980's computer in a laptop form factor.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicW3ytc0vuyQCaJm6SbVo5Rcfjz6bkeRZx42BMrd3qOhE1TtlsiX1j5moJM5LJ-7Jh3xDrYxLRgae2ZG9eogUMbvngsUOtMEcvKAW2SEe8zmC0fcIOUPK2mNI7yD7RoCU362oKkGhXofdqB4J3RgOdwg7fs7mlA5smWoZt2cJv-l_W69xRNToQvJJx9c2B/s1348/book8088.JPG" imageanchor="1"><img data-original-height="1158" data-original-width="1348" height="344" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicW3ytc0vuyQCaJm6SbVo5Rcfjz6bkeRZx42BMrd3qOhE1TtlsiX1j5moJM5LJ-7Jh3xDrYxLRgae2ZG9eogUMbvngsUOtMEcvKAW2SEe8zmC0fcIOUPK2mNI7yD7RoCU362oKkGhXofdqB4J3RgOdwg7fs7mlA5smWoZt2cJv-l_W69xRNToQvJJx9c2B/w400-h344/book8088.JPG" width="400"></a></p><p>With apologies to <a href="https://github.com/skiselev" target="_blank">Sergey Kiselev</a>, whom the manufacturers of the Book8088 <a href="https://forum.vcfed.org/index.php?threads/chinese-8088-based-laptop-system-with-pirated-8088-bios.1243152/" target="_blank">did a little dirty</a>, I couldn't resist ordering one myself to tinker with.&nbsp;</p><p>The Book8088 is trying hard to basically be compatible with the original IBM PC, containing some of the same or equivalent chips. It's natural to want to put it through its paces, and one of the best tests for IBM PC compatibility has to be the <a href="https://www.pouet.net/prod.php?which=65371" target="_blank">8088MPH demo</a>.&nbsp;&nbsp;If 8088MPH will run we must be operating pretty darn close to the original.&nbsp;</p><p>Looking at the specs of the Book8088, there's good reason to be optimistic.&nbsp; The machine supports a real 8088 CPU.&nbsp; Even better, there's a socketed CRTC chip, so the complex CRTC abuse the demo performs doesn't need to be emulated in any fashion.</p><p>My Book8088 came with an NEC V20 CPU.&nbsp; This CPU is a lot faster than the 8088, we'll need to replace it:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjalO2Z-FXpF5Dx_XFwmjDfH6H4GXGnvfw0dDWNT4ssXg-Eg1zlFqoCU0wIviGS0bZAOV6YJtXL1KaVdbkCB0FRNxUx_8JZjsTNpS60pERli_DFHKSQ5kppJGYl_0daJ5ILqaXkWLJkuRgA8HSwHw54mmXl4zCOGKpULU9OnW7FmJtgqFnnAfpwtc6XvDpG/s1175/cpu_swap.jpg" imageanchor="1"><img data-original-height="998" data-original-width="1175" height="340" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjalO2Z-FXpF5Dx_XFwmjDfH6H4GXGnvfw0dDWNT4ssXg-Eg1zlFqoCU0wIviGS0bZAOV6YJtXL1KaVdbkCB0FRNxUx_8JZjsTNpS60pERli_DFHKSQ5kppJGYl_0daJ5ILqaXkWLJkuRgA8HSwHw54mmXl4zCOGKpULU9OnW7FmJtgqFnnAfpwtc6XvDpG/w400-h340/cpu_swap.jpg" width="400"></a></p><p>Additionally, the CRTC chip I received is not the same as you would find in an original IBM CGA card. Instead there's a Hitachi CRTC - let's replace it with a CGA-accurate Motorola MC6845:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiOXlRQicpK5W-gUlwdURecUC6O99l_7Xe2E8-EBanRWo5eOWEmNZYF_cC1qxLX2xiRcM2Y6u3mLuC0pWbU9TbKDxS1J-traLp_OpRvAhe2mcnJhBkRvKRL72wQ0j6BVNdgKKpSugH7TQUAncE1W7dsPdHbS1PzYE5GR_NgeM8h4JCsaZfT72Ci9LemMwWa/s962/crtc_swap.jpg" imageanchor="1"><img data-original-height="805" data-original-width="962" height="335" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiOXlRQicpK5W-gUlwdURecUC6O99l_7Xe2E8-EBanRWo5eOWEmNZYF_cC1qxLX2xiRcM2Y6u3mLuC0pWbU9TbKDxS1J-traLp_OpRvAhe2mcnJhBkRvKRL72wQ0j6BVNdgKKpSugH7TQUAncE1W7dsPdHbS1PzYE5GR_NgeM8h4JCsaZfT72Ci9LemMwWa/w400-h335/crtc_swap.jpg" width="400"></a></p><p>Now, as it turns out, most of the demo <i>does</i> run, albeit in RGBI mode which loses out on all the cool composite artifact color effects. But most notably the famous Kefrens Bars effect does not display - the screen just goes blank.&nbsp; What's going wrong on the Book8088 vs a real IBM PC 5150?</p><p><a href="https://medium.com/@davidly_33504/the-book-8088-is-too-fast-c0a2f01a1cc8" target="_blank">As others have noted</a>, even with an 8088 swap, there's something a little fishy going on.</p><h2>DRAM Refresh DMA</h2><p>The original IBM PC used DRAM - 'dynamic' memory that had to be periodically refreshed, or it would lose charge and subsequently its memory contents. IBM was trying to keep the costs of the IBM PC down, so decided to utilize some of the accessory chips that the PC had to perform the task of DRAM refresh rather than add dedicated circuitry to handle it. Therefore one of three channels of the system's timer chip and one of four channels of the system's DMA controller are dedicated solely to the task of refreshing memory.&nbsp;</p><p>By default, every 72 CPU cycles, the timer chip counts down to 0 and sends an output pulse to the DMA controller, triggering a DMA request which eventually pulls the READY line to the CPU low, potentially stalling the CPU if it is in the middle of a bus transaction. During this time certain address lines are strobed, which is all that is needed to refresh the memory cells.</p><p>This has a variable impact on system performance. If the CPU is primarily performing long, arithmetic instructions such as division or multiplication, it won't really slow things down at all. If the CPU is instead copying memory with a string instruction, or executing a sequence of very short instructions&nbsp; requiring rapid fetching, then the impact can be quite substantial.&nbsp; Averaged out, the 8088 CPU in the IBM PC is about 5% slower than it otherwise would be without DRAM refresh.</p><p>If you're curious to know more about how DMA works on the IBM PC, I wrote a <a href="https://martypc.blogspot.com/2023/05/exploring-dma-on-ibm-pc.html" target="_blank">previous article on it</a>.</p><p>The Book8088 and many modern hobby PC clones like it usually forgo DRAM and its complications and instead use SRAM chips. The S stands for Static, and like the name suggests it does not require refreshing. Therefore we don't need to have this process going on at all, so we don't need to have timer channel #1 ticking away and we don't need DMA channel #0 configured for DRAM refresh, and our CPU can run 5% faster. Isn't that just a bonus?</p><p>Well, yes and no. When talking about compatibility with the original IBM PC, there were several software titles that made assumptions about the exact speed of the 8088 CPU, and they made those assumptions with the DRAM refresh performance impact baked-in. If we don't simulate DRAM refresh, we open ourselves up to compatibility issues.</p><p>The Kefrens effect in the 8088MPH demo is a good example. It is a perfectly cycle-counted effect. This effect does not tightly poll the CGA status register to determine when the CGA card is in hblank or vblank, or really where it is on the screen at all. It does not set up a normal screen resolution - instead, it draws a scanline at a time, 'racing the beam'. This is possible due to the fact that the CGA clock is an exact multiple of the CPU clock, so that a single CPU cycle equates to 3 pixels or 'hdots' on screen. Knowing that a single scanline takes 912 hdots to display means if we execute an effect somehow utilizing exactly 304 CPU cycles per scanline, that effect will run in perfect synchronization with the CGA card. And so that's exactly what the Kefrens effect does - it's some impressive coding.</p><table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimrhVOWMuCJZzaX5gLtvs-K7mybXE9YXpMZ5_BYG9sNgFeLLjw5pR_Yckxgdy-vfLzbsHLObw2_06bTn50GQri-yJ250FNnBEB3WSiU_2x-1M4xUhxJzTr0KGeSty4udEsyvrPL-0fzdcrV-lPSGCUxuIw8pQhEolIyqa-4WAZjR_94lgqp8-BllKI8zmV/s658/kefrens_02.png"><img data-original-height="547" data-original-width="658" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimrhVOWMuCJZzaX5gLtvs-K7mybXE9YXpMZ5_BYG9sNgFeLLjw5pR_Yckxgdy-vfLzbsHLObw2_06bTn50GQri-yJ250FNnBEB3WSiU_2x-1M4xUhxJzTr0KGeSty4udEsyvrPL-0fzdcrV-lPSGCUxuIw8pQhEolIyqa-4WAZjR_94lgqp8-BllKI8zmV/s16000/kefrens_02.png"></a></td></tr><tr><td>Kefrens Bars in 8088MPH - it looks better in motion!</td></tr></tbody></table><p>The reason that this effect does not work on a stock Book8088 is that the painstaking cycle counting done for the effect takes into account the wait state cycles incurred by DRAM refresh DMA on the IBM PC.</p><p>Since the Book8088 isn't doing DRAM refresh, the corresponding wait states don't occur, the effect runs too fast, and quickly is out of sync with the CGA card. Vsync signals happen at the wrong times, and I imagine the LCD controller on the Book8088 gets very upset and refuses to display an image at all.</p><h2>Can we just turn DMA on?</h2><p>It would be nice if we could just program the timer channel #1 to the appropriate value, and set up DMA channel #0 like the BIOS does, and then DRAM refresh would operate as it does on the IBM PC, just harmlessly addressing the system's SRAM but making our 8088 CPU operate at the correct speed.</p><p>We can write a short assembly program that attempts to do just that, and assemble it to 'startdma.com':</p>

<!--HTML generated using hilite.me--><div><pre><span>; startdma.asm</span>
<span>; begin DRAM refresh DMA if for some reason your BIOS didn't</span>

<span>cpu</span> <span>8086</span>
<span>org</span> <span>100h</span>

<span>%include "macros.asm"</span>
<span>%include "library.asm"</span>

<span>main:</span>

<span>begin:</span>  <span>jmp</span> <span>start</span>

        <span>e_init_dmac</span><span>         equ</span> <span>03h</span>     <span>; Initialize DMAC initialized</span>
        <span>dmac_ch0_addr_reg</span><span>   equ</span> <span>00h</span>     <span>; DMAC channel 0 base addres (W)</span>
        <span>dmac_ch0_count_reg</span><span>  equ</span> <span>01h</span>     <span>; DMAC channel 0 word count (W)</span>
        <span>dmac_mask_reg</span><span>       equ</span> <span>0Ah</span>     <span>; DMAC single mask bit register (W)</span>
        <span>dmac_mode_reg</span><span>       equ</span> <span>0Bh</span>     <span>; DMAC mode register (R/W)</span>
        <span>dmac_cmd_reg</span><span>        equ</span> <span>08h</span>     <span>; DMAC command register</span>
        
<span>start:</span>
        <span>; set up DRAM refresh on DMA channel 0</span>
        
        <span>mov</span>     <span>al</span>, <span>0ffh</span>                <span>; 16-bit memory refresh counter = 0FFFFh</span>
        <span>out</span>     <span>dmac_ch0_count_reg</span>, <span>al</span>  <span>; write low byte</span>
        <span>nop</span>
        <span>out</span>     <span>dmac_ch0_count_reg</span>, <span>al</span>  <span>; write high byte</span>
        <span>inc</span>     <span>ax</span>                      <span>; al = 0</span>
        <span>out</span>     <span>dmac_mask_reg</span>,<span>al</span>        <span>; unmask all DMA channels</span>
        <span>mov</span>     <span>al</span>, <span>58h</span>                 <span>; single mode, auto-init, read, channel 0</span>
        <span>out</span>     <span>dmac_mode_reg</span>,<span>al</span>        <span>; DMA Mode register</span>
        <span>mov</span>     <span>al</span>,<span>0</span>       
        <span>out</span>     <span>dmac_cmd_reg</span>, <span>al</span>        <span>; DMA Command register</span>
        
        <span>; set up pit channel #1 DMA timer</span>
        
                <span>pit_set_mode</span> <span>1</span>, <span>PIT_RWM_LSB</span>, <span>2</span>, <span>0</span>   <span>; Pit channel 1, LSB, RateGenerator, binary</span>
        <span>mov</span>     <span>al</span>, <span>12h</span>                             <span>; Default refresh value of 18</span>
                <span>pit_write_byte</span> <span>1</span>                    <span>; Write reload value to start timer</span>
                
                <span>dos_exit</span> <span>0</span>
        <span>ret</span>
</pre></div>


<p>We can run startdma.com and try 8088MPH, and... it makes no difference whatsoever.</p><p>If the designers of the Book8088 didn't really consider this to be an issue, it's quite possible they didn't bother to connect the output of the timer channel #1.&nbsp; Helpfully, ArsTechnica released <a href="https://cdn.arstechnica.net/wp-content/uploads/2023/07/Book8088-manual-and-original-BIOS.zip" target="_blank">schematics for this system</a>. Let's take a look:</p><table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcZnFJ92XPOM_IbJW9EosKSk8gpz9sc1Hzi9m_zYd6HmrDm18u1S2p5gD0HjCkr6neRc3M9Pf_vhfysVD8QK1nr5NdUZBrvi9DooYuSyarnk6lSgvZZeYGLC7sei4fOD5dt8WK_UECjuOn7ntGWcV4diDm7ezqNduIJY_djtVBs9-wwPFyMdIYd63f_42z/s478/pit_schematic.PNG"><img data-original-height="363" data-original-width="478" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjcZnFJ92XPOM_IbJW9EosKSk8gpz9sc1Hzi9m_zYd6HmrDm18u1S2p5gD0HjCkr6neRc3M9Pf_vhfysVD8QK1nr5NdUZBrvi9DooYuSyarnk6lSgvZZeYGLC7sei4fOD5dt8WK_UECjuOn7ntGWcV4diDm7ezqNduIJY_djtVBs9-wwPFyMdIYd63f_42z/s16000/pit_schematic.PNG"></a></td></tr><tr><td>The Book8088 timer schematic</td></tr></tbody></table><p>Well, drat. Just as we feared, the output of timer channel #1, the "OUT1" pin, isn't connected to anything. At least they were kind enough to connect the channel #1 clock input and the gate pin, so the timer channel is usable and can be programmed, but it won't ever trigger the DMA controller.</p><p>Over on the DMA controller, we see a similar situation:</p><table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh3UB2CCuEWnzJ3HFA4OcdFtWIRlT_mS_kwuVrBSww-E4xP7-aeZPC5XA1Zrero9YzTKsZ45_5TWKqzTWFvTusu-z-pUA08EiiXGDl64AckaA0Mg1yK7kk1C1mAgwKCg91MKD0HfAscIvv6XG2tt2Knte0WnIAdq5OWAMK6gI4vQ8t0MZ2BsKeYE6NFek1i/s515/dma_schematic.png"><img data-original-height="515" data-original-width="454" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh3UB2CCuEWnzJ3HFA4OcdFtWIRlT_mS_kwuVrBSww-E4xP7-aeZPC5XA1Zrero9YzTKsZ45_5TWKqzTWFvTusu-z-pUA08EiiXGDl64AckaA0Mg1yK7kk1C1mAgwKCg91MKD0HfAscIvv6XG2tt2Knte0WnIAdq5OWAMK6gI4vQ8t0MZ2BsKeYE6NFek1i/s16000/dma_schematic.png"></a></td></tr><tr><td>The Book8088 DMA schematic</td></tr></tbody></table><p>The "DREQ0" pin that would normally connect to the timer's OUT1 pin is tied to ground.&nbsp;</p><p>We can physically fix this by reconnecting the timer and DMA controller, although that ground connection is a bit of a pain - we'll need to cut that trace.</p><h2>Fixing the Book8088</h2><p>The DMA controller is a Renesas CS82C37A , a CMOS variant of the original Intel 8237A Programmable Interrupt Controller. It is in a 44-PLCC package. Taking a peek at the white paper shows us the pinout:</p><div><table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhWqPz8AUg6dgwcJOeUB12BOy3TGYW-k4Fwi54t4zzW2aQAcKvhGSchbJNSEPak3_6K9c770Pt4IyyUDJLuszpo5V5SlZZZRDDRCEEf3IVNUI1_6ZtwJn8uy4E6_SYZYmMdFcpb_67TvbSyqIhFYW6ZBEq2oOmKsVoTfRUAeiFrSzRQnpsv9OEvZRjbECSe"><img alt="" data-original-height="909" data-original-width="807" height="400" src="https://blogger.googleusercontent.com/img/a/AVvXsEhWqPz8AUg6dgwcJOeUB12BOy3TGYW-k4Fwi54t4zzW2aQAcKvhGSchbJNSEPak3_6K9c770Pt4IyyUDJLuszpo5V5SlZZZRDDRCEEf3IVNUI1_6ZtwJn8uy4E6_SYZYmMdFcpb_67TvbSyqIhFYW6ZBEq2oOmKsVoTfRUAeiFrSzRQnpsv9OEvZRjbECSe=w355-h400" width="355"></a></td></tr><tr><td>DMA controller pinout</td></tr></tbody></table><p>DREQ0 is Pin22. We just need to look at the helpfully supplied PCB diagram:</p></div><table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0J1uJ1v1GmwmtPvZ-k0a_XAMMFxbkuTE_At9IWG7b5fZAKdw0kQIcxxCRDSNX9gGTyHI4NNL4L5qf2vPmiAS5W5GkTVqL-UqfrCouk3vjj16rGnRVJTEj7BsvFP7bza8f39jErwF7XknuSeF10BudKGDJHPmXUgISKRjy9TQqmxCpAKp4N3T7R6xGSJl0/s611/dma_pinout.png"><img data-original-height="605" data-original-width="611" height="317" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi0J1uJ1v1GmwmtPvZ-k0a_XAMMFxbkuTE_At9IWG7b5fZAKdw0kQIcxxCRDSNX9gGTyHI4NNL4L5qf2vPmiAS5W5GkTVqL-UqfrCouk3vjj16rGnRVJTEj7BsvFP7bza8f39jErwF7XknuSeF10BudKGDJHPmXUgISKRjy9TQqmxCpAKp4N3T7R6xGSJl0/s320/dma_pinout.png" width="320"></a></td></tr><tr><td>DREQ0 pin</td></tr></tbody></table><p>Unfortunately, this pin is connected to a rather wide trace with a via to ground, making it a bit of a pain to cut, but thankfully there is enough access to the side of the socket so is fairly straightforward, as long as we are careful.</p><table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjVvELCuETtPYBNfwyAjydMGUJFbrmvvfb43p3kJgMnA3pIBnptEvhP9a5HIbOUwPwumCuPlRkQNHQhYZYX70UUCqLMZCdYTIbCm0Dn_YXaziahx_0BjfvhQeLy9oFdTay96hmcOZ94kuWzMmVM3yRVPdqYQmQuTMHZu01pZJ_-QoUaG_uUFKcZxKz3l9J/s1367/cutme2.jpg" imageanchor="1"><img data-original-height="769" data-original-width="1367" height="225" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgjVvELCuETtPYBNfwyAjydMGUJFbrmvvfb43p3kJgMnA3pIBnptEvhP9a5HIbOUwPwumCuPlRkQNHQhYZYX70UUCqLMZCdYTIbCm0Dn_YXaziahx_0BjfvhQeLy9oFdTay96hmcOZ94kuWzMmVM3yRVPdqYQmQuTMHZu01pZJ_-QoUaG_uUFKcZxKz3l9J/w400-h225/cutme2.jpg" width="400"></a></td></tr><tr><td>Scrape the trace off before the via here, carefully.</td></tr></tbody></table><p>Use a continuity checker to verify that this pin is no longer connected to ground. Even a tiny sliver of trace can cause issues.</p><p>We will also need the DACK0 line, pin #28, for part of our DRAM refresh logic. This pin runs out to the ISA connector. This is an odd choice, since with DREQ0 connected to ground, the DACK0 line isn't very useful.&nbsp; Even more puzzling is DACK1 is tied to ground, but DREQ1 is not.</p><p>I am not sure what contributed to these design decisions, did they get their lines crossed?</p><p>This being a surface-mount socket, our options for attaching our wires are a little limited. If we take stranded, 28 gauge wire, we can effectively drape them across the contacts of the empty socket, then push the chip back in on top of them. A gentle tug proves these connections reasonably secure. Be sure to check there is no connectivity between adjacent pins - the stranded wires can splay out and potentially short.</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3yq9pM-fvtCa6mBoJowWsO91kZ-U-J3YT5SQR4PAZkooyfuTGcVM9a3eQJKPlwK-CDq678gtF28o5rjULzEwrAqjHsSMtffG-qZ6QVqbrGTl8IdoQ0oPEyuWMf6CnYYiMS4RL_NjltLhniWUOKo97n5kc3wbljC353GYw4awyto6ZA6EUxEtFlFk1jzlH/s1575/dmac_wires01.JPG"><img data-original-height="1160" data-original-width="1575" height="295" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi3yq9pM-fvtCa6mBoJowWsO91kZ-U-J3YT5SQR4PAZkooyfuTGcVM9a3eQJKPlwK-CDq678gtF28o5rjULzEwrAqjHsSMtffG-qZ6QVqbrGTl8IdoQ0oPEyuWMf6CnYYiMS4RL_NjltLhniWUOKo97n5kc3wbljC353GYw4awyto6ZA6EUxEtFlFk1jzlH/w400-h295/dmac_wires01.JPG" width="400"></a></p><p>DREQ0 will go over to the timer chip, but it can't connect to the output of timer channel #1 directly.&nbsp; The timer is driven in Rate Generator mode, which stays high most of the time, dropping low on a 1 count for one tick.&nbsp; DREQ0 is level-triggered; if directly connected, DRAM refresh DMA would run constantly, and our PC would be much slower than intended.</p><table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdLHZyNdc308_1Mj5wmXKFtBvTF2Kfls2S2r2OQsgxkRutENe7DCfYeiEto2ciKm59oYIuPyp0aALTK5ywY_capRquzNPgyZjdc7FrEwQZTGzXpmgy2ydNZyJ511XZrb6izyJdtASx7qfiI-uYGhI5PP24dqqie_rZt64K_mxqLnWk5Wq0m5A9WD0EcpPk/s779/dma_circuit02.PNG"><img data-original-height="266" data-original-width="779" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdLHZyNdc308_1Mj5wmXKFtBvTF2Kfls2S2r2OQsgxkRutENe7DCfYeiEto2ciKm59oYIuPyp0aALTK5ywY_capRquzNPgyZjdc7FrEwQZTGzXpmgy2ydNZyJ511XZrb6izyJdtASx7qfiI-uYGhI5PP24dqqie_rZt64K_mxqLnWk5Wq0m5A9WD0EcpPk/s16000/dma_circuit02.PNG"></a></td></tr><tr><td>The IBM 5150 DREQ0 logic circuit</td></tr></tbody></table><p>Instead, DREQ0 is driven by the output of a 74LS74 flip-flop, which is clocked by the channel #1 output. When there is a low to high transition of the timer output, the flipflop will take its input - tied to 5V, and output it on Q, sending our DREQ0 signal to the DMA controller. Notice the reset line is tied to DACK0; once the DMA controller acknowledges the DMA operation, the DREQ0 line will drop low. This ensures we only perform one DMA operation per timer terminal count.</p><p>This is easy enough to wire up if you happen to have a 74LS74 lying around:</p><table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwFBP9WfDKKrKarlIYWeeldBp7g4XlaPsXmVwonZNIPcLCu_PvxD9Z9ECKadvOwysz7P3r6K80Pyt99jF3NcIohFfuQL5fkITSsFQBQ32T_2BEjWWTyg6yNMpu8BA7Irm8tgBZ6zQxOVg0FW93LoOqu4oPcqCGE0iXxAXINmS50pZqd2RVrIJOc0w3nWNO/s1022/74ls74.JPG"><img data-original-height="685" data-original-width="1022" height="428" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiwFBP9WfDKKrKarlIYWeeldBp7g4XlaPsXmVwonZNIPcLCu_PvxD9Z9ECKadvOwysz7P3r6K80Pyt99jF3NcIohFfuQL5fkITSsFQBQ32T_2BEjWWTyg6yNMpu8BA7Irm8tgBZ6zQxOVg0FW93LoOqu4oPcqCGE0iXxAXINmS50pZqd2RVrIJOc0w3nWNO/w640-h428/74ls74.JPG" width="640"></a></td></tr><tr><td>The DREQ0 74LS74 flipflop</td></tr></tbody></table><p>The red wire is our 5V line, tying the D and PR lines to VCC; DACK0 comes in on our periwinkle wire and DREQ0 on the seafoam green wire. The timer channel 1 comes in on the yellow wire, and of course, black is our ground.&nbsp; If we trim the little breadboard, it fits pretty nicely in the bare spot beside the CGA ROM. I stuck it on with a square of double-stick gorilla tape:</p><table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6GG2T9HuIZTFa2ie1YVxlO97NZnv4WrDL3Ri_6F72KsDKmnaYbHHmxotlnySiHVnKbvxzFVwHhm5__3K3GR-yox4L8zraIFTxAoBNI0FR2EMMLoSR3sm2w3edgty9sXjSyulr7Z50KT9bZiLG6ddLwvuTdmp8ZJmOa8J8setYxyLYzCxuyHvAWxwXrH2F/s3224/dram_circuit_01.jpg"><img data-original-height="1960" data-original-width="3224" height="390" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi6GG2T9HuIZTFa2ie1YVxlO97NZnv4WrDL3Ri_6F72KsDKmnaYbHHmxotlnySiHVnKbvxzFVwHhm5__3K3GR-yox4L8zraIFTxAoBNI0FR2EMMLoSR3sm2w3edgty9sXjSyulr7Z50KT9bZiLG6ddLwvuTdmp8ZJmOa8J8setYxyLYzCxuyHvAWxwXrH2F/w640-h390/dram_circuit_01.jpg" width="640"></a></td></tr><tr><td>The final assembled DRAM refresh DMA circuit<span>&nbsp;</span></td></tr></tbody></table><h2>Testing&nbsp;</h2><p>Let's try the MIPS 1.20 benchmark first, to see how we square up against the baseline of an IBM PC.</p><p>Before running startdma, we can see we are about 5% faster than baseline:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFKSw0fHfg28HhJp2lbiI2FynZ2TEUGZC70AtlADm7nouvbreoogoGQTLrpXsqxBnLmYyFw9Q0hkn5qBWVUOtTWbxHsmhtxRa8sHKRZ3HRl3ElTQIPrIQ2Nc14Inp7tEacvytW8-bhE6_bz4VzvplaOLTXHdpj6BevYu-t0Yz_Mv_ZtVFqrfjr9QBntmUk/s1626/mips_toofast.JPG"><img data-original-height="762" data-original-width="1626" height="301" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFKSw0fHfg28HhJp2lbiI2FynZ2TEUGZC70AtlADm7nouvbreoogoGQTLrpXsqxBnLmYyFw9Q0hkn5qBWVUOtTWbxHsmhtxRa8sHKRZ3HRl3ElTQIPrIQ2Nc14Inp7tEacvytW8-bhE6_bz4VzvplaOLTXHdpj6BevYu-t0Yz_Mv_ZtVFqrfjr9QBntmUk/w640-h301/mips_toofast.JPG" width="640"></a></p><p>After, we're right on the money:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinTGAaq5mSyDQSQWHhFlfKw3S8hGerj7Cw6iThX3NIhCl33IAzYRbZ4AfPLOL2dEpLPVK65qRc1eGNFqxoAhZ2c2qsofECgTBhvYXm-W5zdh0CzCp8KMwUJL_5VVzQ2bzuLvjlTv6SdQZgYUCEVn4D-aLSjXVxXz2_dGK4RU143iqozsodVNyD-pC2iO2-/s1360/mips_justright.jpg"><img data-original-height="629" data-original-width="1360" height="296" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEinTGAaq5mSyDQSQWHhFlfKw3S8hGerj7Cw6iThX3NIhCl33IAzYRbZ4AfPLOL2dEpLPVK65qRc1eGNFqxoAhZ2c2qsofECgTBhvYXm-W5zdh0CzCp8KMwUJL_5VVzQ2bzuLvjlTv6SdQZgYUCEVn4D-aLSjXVxXz2_dGK4RU143iqozsodVNyD-pC2iO2-/w640-h296/mips_justright.jpg" width="640"></a></p><p>Let's try the 8088MPH CPU test. It will certainly complain if we aren't running at the right speed, and without startdma, it does:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsAkn3oxiU14szV-pTvuj4BORyGtCj_Q8BTDwejRrZzRKcplWl5snShfX0lpx2LDjiO6kmuhpWaeEoZRjx9Qn86wr1h47FYCiBv1Tc0RMR12SK6dYQUzwAMIG1BZqPeUjK-7P1If2ROvvLZwcNEZ39AEapJ85b18QIB46uRkLORfW_ronyC6fCjYesSSTg/s1633/8088_toofast.jpg"><img data-original-height="568" data-original-width="1633" height="222" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsAkn3oxiU14szV-pTvuj4BORyGtCj_Q8BTDwejRrZzRKcplWl5snShfX0lpx2LDjiO6kmuhpWaeEoZRjx9Qn86wr1h47FYCiBv1Tc0RMR12SK6dYQUzwAMIG1BZqPeUjK-7P1If2ROvvLZwcNEZ39AEapJ85b18QIB46uRkLORfW_ronyC6fCjYesSSTg/w640-h222/8088_toofast.jpg" width="640"></a></p><p>After, 8088MPH is content with our CPU speed:</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMv7OpEKIj3So9RLkvvZPrJ6DAULyUXvpip8gGe09yhLcanC0X1o6iurjg-nmRKiePaTLb4Sqv-GUXwbpwijkGx_Gok9Ywu-LWx89oa-4kd-XiqyWDY6fb1bsS2YcLDRXfzJiVQka6xs4rMN6WTluvG3xNJ536srpGw7MIrqoSaa6hMY6hCxHxP92-PcV9/s1242/8088_justright.jpg"><img data-original-height="438" data-original-width="1242" height="226" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMv7OpEKIj3So9RLkvvZPrJ6DAULyUXvpip8gGe09yhLcanC0X1o6iurjg-nmRKiePaTLb4Sqv-GUXwbpwijkGx_Gok9Ywu-LWx89oa-4kd-XiqyWDY6fb1bsS2YcLDRXfzJiVQka6xs4rMN6WTluvG3xNJ536srpGw7MIrqoSaa6hMY6hCxHxP92-PcV9/w640-h226/8088_justright.jpg" width="640"></a></p><p>The real test however, is to see if the cycle-counted Kefrens Bars effect works.&nbsp;</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgXXw5uDLUuU58IVMUl7Z4VPoOnerFTr7ktkGTFEmoFUxnfS-jkJWce7K4bRhOVbsZCG0uSABVxhWHcPjQO51lB_f_MjDa26C7qTAG4t_2dkMHufdr5qyxAqG51g7F1kuclc9oXkJ99-ho4gkR3V3SDk9k88RWgyjfPNS2OK38QF2tAV3PzMrM4DYrvZcKQ/s1277/kefrens_success.jpg"><img data-original-height="909" data-original-width="1277" height="456" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgXXw5uDLUuU58IVMUl7Z4VPoOnerFTr7ktkGTFEmoFUxnfS-jkJWce7K4bRhOVbsZCG0uSABVxhWHcPjQO51lB_f_MjDa26C7qTAG4t_2dkMHufdr5qyxAqG51g7F1kuclc9oXkJ99-ho4gkR3V3SDk9k88RWgyjfPNS2OK38QF2tAV3PzMrM4DYrvZcKQ/w640-h456/kefrens_success.jpg" width="640"></a></p><p>And it does! It's a shame we can't enjoy 8088MPH properly without a composite display, and the Book8088 doesn't have any sort of video-out, composite or otherwise. Maybe we can add one?&nbsp;</p><p>You might be curious about the 'sequel' to 8088MPH as well - <a href="https://www.pouet.net/prod.php?which=91938" target="_blank">Area 5150</a>.&nbsp; As it turns out, the final two effects are a bit trickier. Even with our DMA fix, they still show a black screen. It's possible that the Book8088's CPLD-implemented CGA logic isn't up to snuff - but I haven't given up hope.&nbsp;</p><p>Stay tuned :)</p><br>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Caddy is the first and only web server to use HTTPS automatically and by default (122 pts)]]></title>
            <link>https://caddyserver.com/docs/automatic-https</link>
            <guid>37476218</guid>
            <pubDate>Tue, 12 Sep 2023 02:02:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://caddyserver.com/docs/automatic-https">https://caddyserver.com/docs/automatic-https</a>, See on <a href="https://news.ycombinator.com/item?id=37476218">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<article>
<p><strong>Caddy is the first and only web server to use HTTPS automatically <em>and by default</em>.</strong></p>
<p>Automatic HTTPS provisions TLS certificates for all your sites and keeps them renewed. It also redirects HTTP to HTTPS for you! Caddy uses safe and modern defaults -- no downtime, extra configuration, or separate tooling is required.</p>

<p>Here's a 28-second video showing how it works:</p>
<iframe width="100%" height="480" src="https://www.youtube-nocookie.com/embed/nk4EWHvvZtI?rel=0" frameborder="0" allowfullscreen=""></iframe>
<p><strong>Menu:</strong></p>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#activation">Activation</a></li>
<li><a href="#effects">Effects</a></li>
<li><a href="#hostname-requirements">Hostname requirements</a></li>
<li><a href="#local-https">Local HTTPS</a></li>
<li><a href="#testing">Testing</a></li>
<li><a href="#acme-challenges">ACME Challenges</a></li>
<li><a href="#on-demand-tls">On-Demand TLS</a></li>
<li><a href="#errors">Errors</a></li>
<li><a href="#storage">Storage</a></li>
<li><a href="#wildcard-certificates">Wildcard certificates</a></li>
</ul>
<h2 id="overview">Overview</h2>
<p><strong>By default, Caddy serves all sites over HTTPS.</strong></p>

<p>Caddy keeps all managed certificates renewed and redirects HTTP (default port <code>80</code>) to HTTPS (default port <code>443</code>) automatically.</p>
<p><strong>For local HTTPS:</strong></p>
<ul>
<li>Caddy may prompt for a password to install its unique root certificate into your trust store. This happens only once per root; and you can remove it at any time.</li>
<li>Any client accessing the site without trusting Caddy's root CA certificate will show security errors.</li>
</ul>
<p><strong>For public domain names:</strong></p>

<ul>
<li>If your domain's A/AAAA records point to your server,</li>
<li>ports <code>80</code> and <code>443</code> are open externally,</li>
<li>Caddy can bind to those ports (<em>or</em> those ports are forwarded to Caddy),</li>
<li>your <a href="https://caddyserver.com/docs/conventions#data-directory">data directory</a> is writeable and persistent,</li>
<li>and your domain name appears somewhere relevant in the config,</li>
</ul>
<p>then sites will be served over HTTPS automatically. You won't have to do anything else about it. It just works!</p>
<p>Because HTTPS utilizes a shared, public infrastructure, you as the server admin should understand the rest of the information on this page so that you can avoid unnecessary problems, troubleshoot them when they occur, and properly configure advanced deployments.</p>
<h2 id="activation">Activation</h2>
<p>Caddy implicitly activates automatic HTTPS when it knows a domain name (i.e. hostname) or IP address it is serving. There are various ways to tell Caddy your domain/IP, depending on how you run or configure Caddy:</p>
<ul>
<li>A <a href="https://caddyserver.com/docs/caddyfile/concepts#addresses">site address</a> in the <a href="https://caddyserver.com/docs/caddyfile">Caddyfile</a></li>
<li>A <a href="https://caddyserver.com/docs/json/apps/http/servers/routes/match/host/">host matcher</a> in a <a href="https://caddyserver.com/docs/modules/http#servers/routes">JSON route</a></li>
<li>Command line flags like <a href="https://caddyserver.com/docs/command-line#caddy-file-server">--domain</a> or <a href="https://caddyserver.com/docs/command-line#caddy-reverse-proxy">--from</a></li>
<li>The <a href="https://caddyserver.com/docs/json/apps/tls/certificates/automate/">automate</a> certificate loader</li>
</ul>
<p>Any of the following will prevent automatic HTTPS from being activated, either in whole or in part:</p>
<ul>
<li>Explicitly disabling it <a href="https://caddyserver.com/docs/json/apps/http/servers/automatic_https/">via JSON</a> or <a href="https://caddyserver.com/docs/caddyfile/options#auto-https">via Caddyfile</a></li>
<li>Not providing any hostnames or IP addresses in the config</li>
<li>Listening exclusively on the HTTP port</li>
<li>Prefixing the <a href="https://caddyserver.com/docs/caddyfile/concepts#addresses">site address</a> with <code>http://</code> in the Caddyfile</li>
<li>Manually loading certificates (unless <a href="https://caddyserver.com/docs/json/apps/http/servers/automatic_https/ignore_loaded_certificates/"><code>ignore_loaded_certificates</code></a> is set)</li>
</ul>
<p><strong>Special cases:</strong></p>

<h2 id="effects">Effects</h2>
<p>When automatic HTTPS is activated, the following occurs:</p>
<ul>
<li>Certificates are obtained and renewed for <a href="#hostname-requirements">all domain names</a></li>
<li>The default port (if any) is changed to the <a href="https://caddyserver.com/docs/modules/http#https_port">HTTPS port</a> <code>443</code></li>
<li>HTTP is redirected to HTTPS (this uses <a href="https://caddyserver.com/docs/modules/http#http_port">HTTP port</a> <code>80</code>)</li>
</ul>
<p>Automatic HTTPS never overrides explicit configuration.</p>
<p>You can <a href="https://caddyserver.com/docs/json/apps/http/servers/automatic_https/">customize or disable automatic HTTPS</a> if necessary; for example, you can skip certain domain names or disable redirects (for Caddyfile, do this with <a href="https://caddyserver.com/docs/caddyfile/options">global options</a>).</p>
<h2 id="hostname-requirements">Hostname requirements</h2>
<p>All hostnames (domain names) qualify for fully-managed certificates if they:</p>
<ul>
<li>are non-empty</li>
<li>consist only of alphanumerics, hyphens, dots, and wildcard (<code>*</code>)</li>
<li>do not start or end with a dot (<a href="https://tools.ietf.org/html/rfc1034#section-3.5">RFC 1034</a>)</li>
</ul>
<p>In addition, hostnames qualify for publicly-trusted certificates if they:</p>
<ul>
<li>are not localhost (including <code>.localhost</code> and <code>.local</code> TLDs)</li>
<li>are not an IP address</li>
<li>have only a single wildcard <code>*</code> as the left-most label</li>
</ul>
<h2 id="local-https">Local HTTPS</h2>
<p>Caddy uses HTTPS automatically for all sites with a host (domain, IP, or hostname) specified, including internal and local hosts. Some hosts are either not public (e.g. <code>127.0.0.1</code>, <code>localhost</code>) or do not generally qualify for publicly-trusted certificates (e.g. IP addresses -- you can get certificates for them, but only from some CAs). These are still served over HTTPS unless disabled.</p>
<p>To serve non-public sites over HTTPS, Caddy generates its own certificate authority (CA) and uses it to sign certificates. The trust chain consists of a root and intermediate certificate. Leaf certificates are signed by the intermediate. They are stored in <a href="https://caddyserver.com/docs/conventions#data-directory">Caddy's data directory</a> at <code>pki/authorities/local</code>.</p>
<p>Caddy's local CA is powered by <a href="https://smallstep.com/certificates/">Smallstep libraries <img src="https://caddyserver.com/resources/images/external-link.svg"></a>.</p>
<p>Local HTTPS does not use ACME nor does it perform any DNS validation. It works only on the local machine and is trusted only where the CA's root certificate is installed.</p>
<h3 id="ca-root">CA Root</h3>
<p>The root's private key is uniquely generated using a cryptographically-secure pseudorandom source and persisted to storage with limited permissions. It is loaded into memory only to perform signing tasks, after which it leaves scope to be garbage-collected.</p>
<p>Although Caddy can be configured to sign with the root directly (to support non-compliant clients), this is disabled by default, and the root key is only used to sign intermediates.</p>
<p>The first time a root key is used, Caddy will try to install it into the system's local trust store(s). If it does not have permission to do so, it will prompt for a password. This behavior can be disabled in the configuration if it is not desired. If this fails due to being run as an unprivileged user, you may run <a href="https://caddyserver.com/docs/command-line#caddy-trust"><code>caddy trust</code></a> to retry installation as a privileged user.</p>

<p>After Caddy's root CA is installed, you will see it in your local trust store as "Caddy Local Authority" (unless you've configured a different name). You can uninstall it any time if you wish (the <a href="https://caddyserver.com/docs/command-line#caddy-untrust"><code>caddy untrust</code></a> command makes this easy).</p>
<p>Note that automatically installing the certificate into the local trust stores is for convenience only and isn't guaranteed to work, especially if containers are being used or if Caddy is being run as an unprivileged system service. Ultimately, if you are relying on internal PKI, it is the system administrator's responsibility to ensure Caddy's root CA is properly added to the necessary trust stores (this is outside the scope of the web server).</p>
<h3 id="ca-intermediates">CA Intermediates</h3>
<p>An intermediate certificate and key will also be generated, which will be used for signing leaf (individual site) certificates.</p>
<p>Unlike the root certificate, intermediate certificates have a much shorter lifetime and will automatically be renewed as needed.</p>
<h2 id="testing">Testing</h2>
<p>To test or experiment with your Caddy configuration, make sure you <a href="https://caddyserver.com/docs/modules/tls.issuance.acme#ca">change the ACME endpoint</a> to a staging or development URL, otherwise you are likely to hit rate limits which can block your access to HTTPS for up to a week, depending on which rate limit you hit.</p>
<p>One of Caddy's default CAs is <a href="https://letsencrypt.org/">Let's Encrypt <img src="https://caddyserver.com/resources/images/external-link.svg"></a>, which has a <a href="https://letsencrypt.org/docs/staging-environment/">staging endpoint <img src="https://caddyserver.com/resources/images/external-link.svg"></a> that is not subject to the same <a href="https://letsencrypt.org/docs/rate-limits/">rate limits <img src="https://caddyserver.com/resources/images/external-link.svg"></a>:</p>
<pre><code>https://acme-staging-v02.api.letsencrypt.org/directory
</code></pre>
<h2 id="acme-challenges">ACME challenges</h2>
<p>Obtaining a publicly-trusted TLS certificate requires validation from a publicly-trusted, third-party authority. These days, this validation process is automated with the <a href="https://tools.ietf.org/html/rfc8555">ACME protocol <img src="https://caddyserver.com/resources/images/external-link.svg"></a>, and can be performed one of three ways ("challenge types"), described below.</p>
<p>The first two challenge types are enabled by default. If multiple challenges are enabled, Caddy chooses one at random to avoid accidental dependence on a particular challenge. Over time, it learns which challenge type is most successful and will begin to prefer it first, but will fall back to other available challenge types if necessary.</p>
<h3 id="http-challenge">HTTP challenge</h3>
<p>The HTTP challenge performs an authoritative DNS lookup for the candidate hostname's A/AAAA record, then requests a temporary cryptographic resource over port <code>80</code> using HTTP. If the CA sees the expected resource, a certificate is issued.</p>
<p>This challenge requires port <code>80</code> to be externally accessible. If Caddy cannot listen on port 80, packets from port <code>80</code> must be forwarded to Caddy's <a href="https://caddyserver.com/docs/json/apps/http/http_port/">HTTP port</a>.</p>
<p>This challenge is enabled by default and does not require explicit configuration.</p>
<h3 id="tls-alpn-challenge">TLS-ALPN challenge</h3>
<p>The TLS-ALPN challenge performs an authoritative DNS lookup for the candidate hostname's A/AAAA record, then requests a temporary cryptographic resource over port <code>443</code> using a TLS handshake containing special ServerName and ALPN values. If the CA sees the expected resource, a certificate is issued.</p>
<p>This challenge requires port <code>443</code> to be externally accessible. If Caddy cannot listen on port 443, packets from port <code>443</code> must be forwarded to Caddy's <a href="https://caddyserver.com/docs/json/apps/http/https_port/">HTTPS port</a>.</p>
<p>This challenge is enabled by default and does not require explicit configuration.</p>
<h3 id="dns-challenge">DNS challenge</h3>
<p>The DNS challenge performs an authoritative DNS lookup for the candidate hostname's <code>TXT</code> records, and looks for a special <code>TXT</code> record with a certain value. If the CA sees the expected value, a certificate is issued.</p>
<p>This challenge does not require any open ports, and the server requesting a certificate does not need to be externally accessible. However, the DNS challenge requires configuration. Caddy needs to know the credentials to access your domain's DNS provider so it can set (and clear) the special <code>TXT</code> records. If the DNS challenge is enabled, other challenges are disabled by default.</p>
<p>Since ACME CAs follow DNS standards when looking up <code>TXT</code> records for challenge verification, you can use CNAME records to delegate answering the challenge to other DNS zones. This can be used to delegate the <code>_acme-challenge</code> subdomain to another zone. This is particularly useful if your DNS provider doesn't provide an API, or isn't supported by one of the DNS plugins for Caddy.</p>
<p>DNS provider support is a community effort. <a href="https://caddy.community/t/how-to-use-dns-provider-modules-in-caddy-2/8148">Learn how to enable the DNS challenge for your provider at our wiki.</a></p>
<h2 id="on-demand-tls">On-Demand TLS</h2>
<p>Caddy pioneered a new technology we call <strong>On-Demand TLS</strong>, which dynamically obtains a new certificate during the first TLS handshake that requires it, rather than at config load. Crucially, this does <strong>not</strong> require hard-coding the domain names in your configuration ahead of time.</p>
<p>Many businesses rely on this unique feature to scale their TLS deployments at lower cost and without operational headaches when serving tens of thousands of sites.</p>
<p>On-demand TLS is useful if:</p>
<ul>
<li>you do not know all the domain names when you start or reload your server,</li>
<li>domain names might not be properly configured right away (DNS records not yet set),</li>
<li>you are not in control of the domain names (e.g. they are customer domains).</li>
</ul>
<p>When on-demand TLS is enabled, you do not need to specify the domain names in your config in order to get certificates for them. Instead, when a TLS handshake is received for a server name (SNI) that Caddy does not yet have a certificate for, the handshake is held while Caddy obtains a certificate to use to complete the handshake. The delay is usually only a few seconds, and only that initial handshake is slow. All future handshakes are fast because certificates are cached and reused, and renewals happen in the background. Future handshakes may trigger maintenance for the certificate to keep it renewed, but this maintenance happens in the background if the certificate hasn't expired yet.</p>
<h3 id="using-on-demand-tls">Using On-Demand TLS</h3>
<p><strong>On-demand TLS must be both enabled and restricted to prevent abuse.</strong></p>
<p>Enabling on-demand TLS happens in <a href="https://caddyserver.com/docs/json/apps/tls/automation/policies/">TLS automation policies</a> if using the JSON config, or <a href="https://caddyserver.com/docs/caddyfile/directives/tls">in site blocks with the <code>tls</code> directive</a> if using the Caddyfile.</p>
<p>To prevent abuse of this feature, you must configure restrictions. This is done in the <a href="https://caddyserver.com/docs/json/apps/tls/automation/on_demand/"><code>automation</code> object of the JSON config</a>, or the <a href="https://caddyserver.com/docs/caddyfile/options#on-demand-tls"><code>on_demand_tls</code> global option</a> of the Caddyfile. Restrictions are "global" and aren't configurable per-site or per-domain. The primary restriction is an "ask" endpoint to which Caddy will send an HTTP request to ask if it has permission to obtain and manage a certificate for the domain in the handshake. This means you will need some internal backend that can, for example, query the accounts table of your database and see if a customer has signed up with that domain name.</p>
<p>Be mindful of how quickly your CA is able to issue certificates. If it takes more than a few seconds, this will negatively impact the user experience (for the first client only).</p>
<p>Due to its deferred nature and the extra configuration required to prevent abuse, we recommend enabling on-demand TLS only when your actual use case is described above.</p>
<p><a href="https://caddy.community/t/serving-tens-of-thousands-of-domains-over-https-with-caddy/11179">See our wiki article for more information about using on-demand TLS effectively.</a></p>
<h2 id="errors">Errors</h2>
<p>Caddy does its best to continue if errors occur with certificate management.</p>
<p>By default, certificate management is performed in the background. This means it will not block startup or slow down your sites. However, it also means that the server will be running even before all certificates are available. Running in the background allows Caddy to retry with exponential backoff over a long period of time.</p>
<p>Here's what happens if there's an error obtaining or renewing a certificate:</p>
<ol>
<li>Caddy retries once after a brief pause just in case it was a fluke</li>
<li>Caddy pauses briefly, then switches to the next enabled challenge type</li>
<li>After all enabled challenge types have been tried, <a href="#issuer-fallback">it tries the next configured issuer</a>
<ul>
<li>Let's Encrypt</li>
<li>ZeroSSL</li>
</ul>
</li>
<li>After all issuers have been tried, it backs off exponentially
<ul>
<li>Maximum of 1 day between attempts</li>
<li>For up to 30 days</li>
</ul>
</li>
</ol>
<p>During retries with Let's Encrypt, Caddy switches to their <a href="https://letsencrypt.org/docs/staging-environment/">staging environment <img src="https://caddyserver.com/resources/images/external-link.svg"></a> to avoid rate limit concerns. This isn't a perfect strategy, but in general it's helpful.</p>
<p>ACME challenges take at least a few seconds, and internal rate limiting helps mitigate accidental abuse. Caddy uses internal rate limiting in addition to what you or the CA configure so that you can hand Caddy a platter with a million domain names and it will gradually -- but as fast as it can -- obtain certificates for all of them. Caddy's internal rate limit is currently 10 attempts per ACME account per 10 seconds.</p>
<p>To avoid leaking resources, Caddy aborts in-flight tasks (including ACME transactions) when config is changed. While Caddy is capable of handling frequent config reloads, be mindful of operational considerations such as this, and consider batching config changes to reduce reloads and give Caddy a chance to actually finish obtaining certificates in the background.</p>
<h3 id="issuer-fallback">Issuer fallback</h3>
<p>Caddy is the first (and so far only) server to support fully-redundant, automatic failover to other CAs in the event it cannot successfully get a certificate.</p>
<p>By default, Caddy enables two ACME-compatible CAs: <a href="https://letsencrypt.org/"><strong>Let's Encrypt</strong> <img src="https://caddyserver.com/resources/images/external-link.svg"></a> and <a href="https://zerossl.com/"><strong>ZeroSSL</strong> <img src="https://caddyserver.com/resources/images/external-link.svg"></a>. If Caddy cannot get a certificate from Let's Encrypt, it will try with ZeroSSL; if both fail, it will backoff and retry again later. In your config, you can customize which issuers Caddy uses to obtain certificates, either universally or for specific names.</p>
<h2 id="storage">Storage</h2>
<p>Caddy will store public certificates, private keys, and other assets in its <a href="https://caddyserver.com/docs/json/storage/">configured storage facility</a> (or the default one, if not configured -- see link for details).</p>
<p><strong>The main thing you need to know using the default config is that the <code>$HOME</code> folder must be writeable and persistent.</strong> To help you troubleshoot, Caddy prints its environment variables at startup if the <code>--environ</code> flag is specified.</p>
<p>Any Caddy instances that are configured to use the same storage will automatically share those resources and coordinate certificate management as a cluster.</p>
<p>Before attempting any ACME transactions, Caddy will test the configured storage to ensure it is writeable and has sufficient capacity. This helps reduce unnecessary lock contention.</p>
<h2 id="wildcard-certificates">Wildcard certificates</h2>
<p>Caddy can obtain and manage wildcard certificates when it is configured to serve a site with a qualifying wildcard name. A site name qualifies for a wildcard if only its left-most domain label is a wildcard. For example, <code>*.example.com</code> qualifies, but these do not: <code>sub.*.example.com</code>, <code>foo*.example.com</code>, <code>*bar.example.com</code>, and <code>*.*.example.com</code>.</p>
<p>If using the Caddyfile, Caddy takes site names literally with regards to the certificate subject names. In other words, a site defined as <code>sub.example.com</code> will cause Caddy to manage a certificate for <code>sub.example.com</code>, and a site defined as <code>*.example.com</code> will cause Caddy to manage a wildcard certificate for <code>*.example.com</code>. You can see this demonstrated on our <a href="https://caddyserver.com/docs/caddyfile/patterns#wildcard-certificates">Common Caddyfile Patterns</a> page. If you need different behavior, the <a href="https://caddyserver.com/docs/json/">JSON config</a> gives you more precise control over certificate subjects and site names ("host matchers").</p>
<p>Wildcard certificates represent a wide degree of authority and should only be used when you have so many subdomains that managing individual certificates for them would strain the PKI or cause you to hit CA-enforced rate limits.</p>
<p><strong>Note:</strong> <a href="https://letsencrypt.org/docs/challenge-types/">Let's Encrypt requires <img src="https://caddyserver.com/resources/images/external-link.svg"></a> the <a href="#dns-challenge">DNS challenge</a> to obtain wildcard certificates.</p>
</article>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Neopets is still around (113 pts)]]></title>
            <link>https://www.dualshockers.com/neopets-is-still-active-2023/</link>
            <guid>37474780</guid>
            <pubDate>Mon, 11 Sep 2023 22:52:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dualshockers.com/neopets-is-still-active-2023/">https://www.dualshockers.com/neopets-is-still-active-2023/</a>, See on <a href="https://news.ycombinator.com/item?id=37474780">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body" itemprop="articleBody">
<p>Having grown up in the '90s, gaming on PC was nothing like it is today. I can still recall the excitement on my dad's face when he brought home our second family computer, excitedly telling me about the 1GB of storage space we'd now have. A whole Gig! We'd <em>never </em>fill that up! That was also the first computer we ever took online, and back in those early glory days when whatever was on AOL made up the bulk of my online experience, there were still a few times I'd stray from the AOL message board and chatrooms and take trepid step into the dreaded URL bar, typing in a secret code starting with www that I'd read in a book. (Yes, I used to find websites through books; it as a different time). I don't know that you could truly consider them all games by today's standard — they were more like little animated communities where minigames were present, but community and website exploration made up a good deal of the action.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":0,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":0,"nbrPlacementsScanned":0,"ruleCount":10,"degradationStartingPoint":2,"actualCount":965} -->
<!-- Zone: below first paragraph. -->
<!-- No ads allowed! -->
<!-- No winning ad found for zone: mid intro! -->
<p>So when I saw our own Jack Coleman stepping outside the box to revisit his <a href="https://www.dualshockers.com/wizard-101-still-active/">childhood love for Wizard 101</a> a while back, it made me nostalgic for a simpler time when I was but a bored and curious teen on the Internet (not like that, ya pervs. Get your minds out of the gutter). In the last couple of weeks, I've re-embraced my childhood love of Neopets, a splendiferous online virtual pet playground of games and merriment launched in 1999. To my surprise, I've discovered that you're still free to explore Neopets today, though it's safe to say it's not quite the online tour-de-force it once was.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":0,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":0,"nbrPlacementsScanned":1,"ruleCount":10,"degradationStartingPoint":2,"actualCount":616} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":1,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":1,"nbrPlacementsScanned":1,"ruleCount":10,"degradationStartingPoint":2,"actualCount":0} -->
<p>Oh, and Neopets was dependent on Flash, and, well, Flash is dead, with the end result being that the site doesn't function the way that it should. Choose-your-own-adveture-style stories are abruptly cut off as soon as a movie is encountered, and about 80 percent of the minigames scattered throughout the site crash as soon as I click the start button. I found out through Reddit that you <em>can</em> get everything (or almost everything) to function properly if you download and install an off-the-mainstram web browser that still supports flash, but I'm already middle-aged and stalking around a website designed for kids. Installing a special web browser just to hang around the Neopets community? That feels like the sort of thing that'd get me put on a government watchlist — and I think I'm in the majority of people who aren't going to add an extra Internet browser for the sake of one nostalgic website — so I think I'll just keep poking around in good ol' Chrome.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":1,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":1,"nbrPlacementsScanned":2,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1004} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":2,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":2,"nbrPlacementsScanned":2,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<div data-img-url="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/untitled-cropped.jpg" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="null"> <figure> <picture><!--[if IE 9]> <video style="display: none;"><![endif]--> <source media="(min-width: 1024px)" sizes="740px" data-srcset="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/untitled-cropped.jpg?q=50&amp;fit=crop&amp;w=740&amp;dpr=1.5"> <source media="(min-width: 768px)" sizes="963px" data-srcset="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/untitled-cropped.jpg?q=50&amp;fit=crop&amp;w=963&amp;dpr=1.5"> <source media="(min-width: 481px)" sizes="737px" data-srcset="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/untitled-cropped.jpg?q=50&amp;fit=crop&amp;w=737&amp;dpr=1.5"> <source media="(min-width: 0px)" sizes="450px" data-srcset="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/untitled-cropped.jpg?q=50&amp;fit=crop&amp;w=450&amp;dpr=1.5"><!--[if IE 9]></video><![endif]--><img width="1330" height="665" alt="Neopets plugin error" data-img-url="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/untitled-cropped.jpg" src="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/untitled-cropped.jpg"> </picture> </figure> </div>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":2,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":2,"nbrPlacementsScanned":3,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":240} -->
<p>The site is still filled with brightly-colored maps beckoning me into a wonderful array of shops and games and other attractions. There's the dual medieval-themed kingdoms of Brightvale and Meridell, the under-the-sea realm of Maraqua, the orbital Virtupets Space Station, and more than a dozen others. It's like that feeling you get on a glorious summer morning when your parents have driven you out to an amusement park; a place where enjoyment is king, and you can really just run wild and be a kid.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":2,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":2,"nbrPlacementsScanned":3,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":750} -->

<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":2,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":2,"nbrPlacementsScanned":3,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":830} -->
<p>But the more and more I clicked around this world of wonder and merriment, the more it lost that magical glow, and it started to feel like those eerie photo essays of abandoned theme parks, where nature has reclaimed the empty walkways and rides sit idly in a dilapidated state of inoperable decay. Most of the games scattered throughout the map are inaccessible by conventional means (more on that later), simply popping up a plugin error whenever I try to play.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":2,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":2,"nbrPlacementsScanned":3,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1293} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":3,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":3,"nbrPlacementsScanned":3,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<p>You're still allowed to spend your hard-won Neopoints on things like scratch cards, but good luck actually trying to scratch them off and cash them in once the irritatingly smiling shopkeepers have taken your virtual currency. Even the tutorial just displays an endless screen of blank white, and creating a NeoHome, one of the boxes on my new user checklist, is staying permanently unchecked, as the website lets me pick what region of the virtual world I'd like to live in but won't let me close on the deal. To their credit, the devs had announced a long time ago that they'd been working on porting the game to mobile, but it's been that way for years and the whole thing's still in a semi-broken beta state on my phone too.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":3,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":3,"nbrPlacementsScanned":4,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":752} -->
<div data-img-url="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/npbrightdale-cropped.jpg" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="null"> <figure> <picture><!--[if IE 9]> <video style="display: none;"><![endif]--> <source media="(min-width: 1024px)" sizes="740px" data-srcset="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/npbrightdale-cropped.jpg?q=50&amp;fit=crop&amp;w=740&amp;dpr=1.5"> <source media="(min-width: 768px)" sizes="963px" data-srcset="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/npbrightdale-cropped.jpg?q=50&amp;fit=crop&amp;w=963&amp;dpr=1.5"> <source media="(min-width: 481px)" sizes="737px" data-srcset="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/npbrightdale-cropped.jpg?q=50&amp;fit=crop&amp;w=737&amp;dpr=1.5"> <source media="(min-width: 0px)" sizes="450px" data-srcset="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/npbrightdale-cropped.jpg?q=50&amp;fit=crop&amp;w=450&amp;dpr=1.5"><!--[if IE 9]></video><![endif]--><img width="1353" height="677" alt="Neopets Brightvale map" data-img-url="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/npbrightdale-cropped.jpg" src="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/npbrightdale-cropped.jpg"> </picture> </figure> </div>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":3,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":3,"nbrPlacementsScanned":4,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":992} -->
<p>Despite feeling like some sort of spiritually tortuous purgatory where everything that you used to love is right in front of you and you just can't touch it, the NeoBoards forums are still packed to the Flotsam gills with a dedicated player base. And to make me feel less like an old creeper on a website designed with kids in mind, it kindly displays the number of months since each user that posts there has had an account, letting me know that, yes, there are plenty of players who have been around for 200+ months, maintaining that sweet virtual escape from their own days of youth.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":3,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":3,"nbrPlacementsScanned":4,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1582} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":4,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":4,"nbrPlacementsScanned":4,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->

<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":4,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":4,"nbrPlacementsScanned":5,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":71} -->
<p>There's still some charming stuff here. You can find a few card games that still work scattered throughout Neopia, and there's a handy games page that lists 14 other games that are currently still functioning. Just don't play for too long in one session, because most of them seem to randomly change your score to NAN or decide after a while that you don't get your NeoPoints rewards because you're a "suspicious user." I mean, I already admitted to being too old to be here, but you don't have to make me feel bad about it; this ain't Chuck E. Cheese!</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":4,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":4,"nbrPlacementsScanned":5,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":661} -->
<p>My old favorite, the hot potato-like Gormball, is sadly unavailable, but there's still some mindless, lighthearted fun to be had with the games that are fully functional, like Hasee Bounce, in which you control two adorable little something-or-others as they catapult each other on a seesaw to catch doughnut-fruits, culminating in either a scene of them happily munching on their treasures or sobbing their pwecious widdle eyes out.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":4,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":4,"nbrPlacementsScanned":5,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1098} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":5,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":5,"nbrPlacementsScanned":5,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<div data-img-url="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/nphaseewin-cropped.jpg" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="null"> <figure> <picture><!--[if IE 9]> <video style="display: none;"><![endif]--> <source media="(min-width: 1024px)" sizes="740px" data-srcset="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/nphaseewin-cropped.jpg?q=50&amp;fit=crop&amp;w=740&amp;dpr=1.5"> <source media="(min-width: 768px)" sizes="963px" data-srcset="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/nphaseewin-cropped.jpg?q=50&amp;fit=crop&amp;w=963&amp;dpr=1.5"> <source media="(min-width: 481px)" sizes="737px" data-srcset="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/nphaseewin-cropped.jpg?q=50&amp;fit=crop&amp;w=737&amp;dpr=1.5"> <source media="(min-width: 0px)" sizes="450px" data-srcset="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/nphaseewin-cropped.jpg?q=50&amp;fit=crop&amp;w=450&amp;dpr=1.5"><!--[if IE 9]></video><![endif]--><img width="1430" height="715" alt="Neopets Hasee Bounce Victory Screen" data-img-url="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/nphaseewin-cropped.jpg" src="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/nphaseewin-cropped.jpg"> </picture> </figure> </div>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":5,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":5,"nbrPlacementsScanned":6,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":240} -->
<p>And of course, the core of the game — the feeding, grooming, and training of virtual pets that was a defining feature of '90s kids everywhere — is still intact. It really doesn't matter what you feed them, but there's a lot of fun to be had just stocking up on food for your little buds, from berry picking at Meridell Farms to grabbing a slice on the massive omelette on Tyranian Plateau. The arena still makes no sense to me, but reflecting on it, I don't think it made sense to teenage me either, like it was tacked on to cash in on the Pokemon hype and still hasn't been developed in the nearly two-and-a-half decades that it's been around.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":5,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":5,"nbrPlacementsScanned":6,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":920} -->
<p>All in all, it's been a nice little nostalgic visit to Neopia, but I don't think I'll be settling down here — and not just because my pedestrian, mainstream web browser won't let me create a NeoHome. My old account and pets are gone, lost to the ages among website maintenance and increased security measures of the past. Neopets now belongs to the diehards; the people who've stayed around through the thick and thin of things. In my hubris, I mistakenly thought I'd grown too cool for it, and it passed me by.</p>
<!-- Repeatable debug data: {"injection":"before","adPosition":5,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":5,"nbrPlacementsScanned":6,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":1461} --><!-- Zone: character count repeatable. --><!-- No ads allowed! --><!-- Repeatable debug data: {"injection":"after","adPosition":6,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":6,"nbrPlacementsScanned":6,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":0} -->
<div data-img-url="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/npsomethingishappening-cropped.jpg" data-modal-id="single-image-modal" data-modal-container-id="single-image-modal-container" data-img-caption="null"> <figure> <picture><!--[if IE 9]> <video style="display: none;"><![endif]--> <source media="(min-width: 1024px)" sizes="740px" data-srcset="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/npsomethingishappening-cropped.jpg?q=50&amp;fit=crop&amp;w=740&amp;dpr=1.5"> <source media="(min-width: 768px)" sizes="963px" data-srcset="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/npsomethingishappening-cropped.jpg?q=50&amp;fit=crop&amp;w=963&amp;dpr=1.5"> <source media="(min-width: 481px)" sizes="737px" data-srcset="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/npsomethingishappening-cropped.jpg?q=50&amp;fit=crop&amp;w=737&amp;dpr=1.5"> <source media="(min-width: 0px)" sizes="450px" data-srcset="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/npsomethingishappening-cropped.jpg?q=50&amp;fit=crop&amp;w=450&amp;dpr=1.5"><!--[if IE 9]></video><![endif]--><img width="1748" height="874" alt="Neopets Something Is Happening Neggfest 2023 Petpet" data-img-url="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/npsomethingishappening-cropped.jpg" src="https://static1.dualshockersimages.com/wordpress/wp-content/uploads/2023/05/npsomethingishappening-cropped.jpg"> </picture> </figure> </div>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":6,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":6,"nbrPlacementsScanned":7,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":240} -->
<p>But that doesn't mean I won't be popping in every few days — my Korbat and Grundo would be starving otherwise — and if you've ever been a Neopets player, I'd encourage you to pop in and have a look around too. And while the mobile-friendly version seems entirely stalled out, Neopets Metaverse <a href="https://www.augustman.com/my/gear/tech/neopets-enters-metaverse-alpha-release/#:~:text=That's%20right%20%E2%80%94%20the%20original%20Neopets,an%20exciting%20new%20web3%20format" rel="noopener noreferrer" target="_blank">released in alpha last fall</a>, and if they can get all the classic stuff working for the mainstream again, I'd happily challenge you to a game of Gormball.</p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":6,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":6,"nbrPlacementsScanned":7,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":719} -->
<p><span>NEXT: <a href="https://www.dualshockers.com/lego-dimensions-better-warner-bros-crossover-game/">Lego Dimensions Is A Better Warner Bros. Crossover Game Than Multiversus</a></span></p>
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":6,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":6,"nbrPlacementsScanned":7,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":797} -->
 
<!-- No repeatable ad for zone: character count repeatable. --><!-- Repeatable debug data: {"injection":"none","adPosition":6,"startingPoint":1,"skipEvery":null,"nbrPlacementFilledEachSkip":6,"nbrPlacementsScanned":7,"ruleCount":1000,"degradationStartingPoint":2,"actualCount":798} --></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenSSL 1.1.1 End of Life (132 pts)]]></title>
            <link>https://www.openssl.org/blog/blog/2023/09/11/eol-111/</link>
            <guid>37474601</guid>
            <pubDate>Mon, 11 Sep 2023 22:33:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openssl.org/blog/blog/2023/09/11/eol-111/">https://www.openssl.org/blog/blog/2023/09/11/eol-111/</a>, See on <a href="https://news.ycombinator.com/item?id=37474601">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>OpenSSL 1.1.1 series has reached its End of Life (EOL).
As such it will no longer receive publicly available security fixes.</p>

<!-- more -->

<p>OpenSSL 1.1.1 was released on 11th September 2018 as a <a href="https://www.openssl.org/policies/general/versioning-policy.html#supported-releases">Long Term Support</a> (LTS)
release.  LTS lasts five years and consequently OpenSSL 1.1.1 has reached
its EOL as of today, 11th September 2023.</p>

<p>If your copy of OpenSSL 1.1.1 is from an Operating System vendor (e.g. via
<code>.rpm</code> or <code>.deb</code> packages) or another third party then the support periods
for them may differ to those provided by the OpenSSL Project itself.
Check with the OS vendor/other third party on what support for OpenSSL
they provide.</p>

<p>If you downloaded your copy of OpenSSL 1.1.1 direct from the OpenSSL
project then it is time to upgrade to a more recent version.  Our most
recent version is OpenSSL 3.1 which will be supported until 14th March 2025.
Also available is OpenSSL 3.0 which is an LTS release and will
be supported until 7th September 2026.  Our <a href="https://www.openssl.org/docs/man3.1/man7/migration_guide.html">migration guide</a> provides
useful information on the issues to consider when upgrading.</p>

<p>Another option is to purchase a <a href="https://www.openssl.org/support/contracts.html#premium">premium support contract</a> which offers
extended support (i.e. ongoing access to security fixes) for 1.1.1 beyond
its public EOL date.  There is no defined end date for this extended
support and we intend to continue to provide it for as long as it remains
commercially viable for us to do so (i.e. for the foreseeable future).
Further information is available on our <a href="https://www.openssl.org/support/contracts.html">support contracts</a> page. Email
osf-contact@openssl.org for further information.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YouTube-dl fork with additional features and fixes (173 pts)]]></title>
            <link>https://github.com/yt-dlp/yt-dlp</link>
            <guid>37474066</guid>
            <pubDate>Mon, 11 Sep 2023 21:37:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/yt-dlp/yt-dlp">https://github.com/yt-dlp/yt-dlp</a>, See on <a href="https://news.ycombinator.com/item?id=37474066">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text">


<p dir="auto">yt-dlp is a <a href="https://github.com/ytdl-org/youtube-dl">youtube-dl</a> fork based on the now inactive <a href="https://github.com/blackjack4494/yt-dlc">youtube-dlc</a>. The main focus of this project is adding new features and patches while also keeping up to date with the original project</p>


<ul dir="auto">
<li><a href="#new-features">NEW FEATURES</a>
<ul dir="auto">
<li><a href="#differences-in-default-behavior">Differences in default behavior</a></li>
</ul>
</li>
<li><a href="#installation">INSTALLATION</a>
<ul dir="auto">
<li><a href="https://github.com/yt-dlp/yt-dlp/wiki/Installation">Detailed instructions</a></li>
<li><a href="#update">Update</a></li>
<li><a href="#release-files">Release Files</a></li>
<li><a href="#dependencies">Dependencies</a></li>
<li><a href="#compile">Compile</a></li>
</ul>
</li>
<li><a href="#usage-and-options">USAGE AND OPTIONS</a>
<ul dir="auto">
<li><a href="#general-options">General Options</a></li>
<li><a href="#network-options">Network Options</a></li>
<li><a href="#geo-restriction">Geo-restriction</a></li>
<li><a href="#video-selection">Video Selection</a></li>
<li><a href="#download-options">Download Options</a></li>
<li><a href="#filesystem-options">Filesystem Options</a></li>
<li><a href="#thumbnail-options">Thumbnail Options</a></li>
<li><a href="#internet-shortcut-options">Internet Shortcut Options</a></li>
<li><a href="#verbosity-and-simulation-options">Verbosity and Simulation Options</a></li>
<li><a href="#workarounds">Workarounds</a></li>
<li><a href="#video-format-options">Video Format Options</a></li>
<li><a href="#subtitle-options">Subtitle Options</a></li>
<li><a href="#authentication-options">Authentication Options</a></li>
<li><a href="#post-processing-options">Post-processing Options</a></li>
<li><a href="#sponsorblock-options">SponsorBlock Options</a></li>
<li><a href="#extractor-options">Extractor Options</a></li>
</ul>
</li>
<li><a href="#configuration">CONFIGURATION</a>
<ul dir="auto">
<li><a href="#configuration-file-encoding">Configuration file encoding</a></li>
<li><a href="#authentication-with-netrc">Authentication with netrc</a></li>
<li><a href="#notes-about-environment-variables">Notes about environment variables</a></li>
</ul>
</li>
<li><a href="#output-template">OUTPUT TEMPLATE</a>
<ul dir="auto">
<li><a href="#output-template-examples">Output template examples</a></li>
</ul>
</li>
<li><a href="#format-selection">FORMAT SELECTION</a>
<ul dir="auto">
<li><a href="#filtering-formats">Filtering Formats</a></li>
<li><a href="#sorting-formats">Sorting Formats</a></li>
<li><a href="#format-selection-examples">Format Selection examples</a></li>
</ul>
</li>
<li><a href="#modifying-metadata">MODIFYING METADATA</a>
<ul dir="auto">
<li><a href="#modifying-metadata-examples">Modifying metadata examples</a></li>
</ul>
</li>
<li><a href="#extractor-arguments">EXTRACTOR ARGUMENTS</a></li>
<li><a href="#plugins">PLUGINS</a>
<ul dir="auto">
<li><a href="#installing-plugins">Installing Plugins</a></li>
<li><a href="#developing-plugins">Developing Plugins</a></li>
</ul>
</li>
<li><a href="#embedding-yt-dlp">EMBEDDING YT-DLP</a>
<ul dir="auto">
<li><a href="#embedding-examples">Embedding examples</a></li>
</ul>
</li>
<li><a href="#deprecated-options">DEPRECATED OPTIONS</a></li>
<li><a href="https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#contributing-to-yt-dlp">CONTRIBUTING</a>
<ul dir="auto">
<li><a href="https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#opening-an-issue">Opening an Issue</a></li>
<li><a href="https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions">Developer Instructions</a></li>
</ul>
</li>
<li><a href="https://github.com/yt-dlp/yt-dlp/wiki">WIKI</a>
<ul dir="auto">
<li><a href="https://github.com/yt-dlp/yt-dlp/wiki/FAQ">FAQ</a></li>
</ul>
</li>
</ul>

<h2 tabindex="-1" dir="auto">NEW FEATURES</h2>
<ul dir="auto">
<li>
<p dir="auto">Forked from <a href="https://github.com/blackjack4494/yt-dlc/commit/f9401f2a91987068139c5f757b12fc711d4c0cee"><strong>yt-dlc@f9401f2</strong></a> and merged with <a href="https://github.com/ytdl-org/youtube-dl/commit/07af47960f3bb262ead02490ce65c8c45c01741e"><strong>youtube-dl@42f2d4</strong></a> (<a href="https://github.com/yt-dlp/yt-dlp/issues/21" data-hovercard-type="issue" data-hovercard-url="/yt-dlp/yt-dlp/issues/21/hovercard">exceptions</a>)</p>
</li>
<li>
<p dir="auto"><strong><a href="#sponsorblock-options">SponsorBlock Integration</a></strong>: You can mark/remove sponsor sections in YouTube videos by utilizing the <a href="https://sponsor.ajay.app/" rel="nofollow">SponsorBlock</a> API</p>
</li>
<li>
<p dir="auto"><strong><a href="#sorting-formats">Format Sorting</a></strong>: The default format sorting options have been changed so that higher resolution and better codecs will be now preferred instead of simply using larger bitrate. Furthermore, you can now specify the sort order using <code>-S</code>. This allows for much easier format selection than what is possible by simply using <code>--format</code> (<a href="#format-selection-examples">examples</a>)</p>
</li>
<li>
<p dir="auto"><strong>Merged with animelover1984/youtube-dl</strong>: You get most of the features and improvements from <a href="https://github.com/animelover1984/youtube-dl">animelover1984/youtube-dl</a> including <code>--write-comments</code>, <code>BiliBiliSearch</code>, <code>BilibiliChannel</code>, Embedding thumbnail in mp4/ogg/opus, playlist infojson etc. Note that NicoNico livestreams are not available. See <a href="https://github.com/yt-dlp/yt-dlp/pull/31" data-hovercard-type="pull_request" data-hovercard-url="/yt-dlp/yt-dlp/pull/31/hovercard">#31</a> for details.</p>
</li>
<li>
<p dir="auto"><strong>YouTube improvements</strong>:</p>
<ul dir="auto">
<li>Supports Clips, Stories (<code>ytstories:&lt;channel UCID&gt;</code>), Search (including filters)<strong>*</strong>, YouTube Music Search, Channel-specific search, Search prefixes (<code>ytsearch:</code>, <code>ytsearchdate:</code>)<strong>*</strong>, Mixes, and Feeds (<code>:ytfav</code>, <code>:ytwatchlater</code>, <code>:ytsubs</code>, <code>:ythistory</code>, <code>:ytrec</code>, <code>:ytnotif</code>)</li>
<li>Fix for <a href="https://github.com/ytdl-org/youtube-dl/issues/29326" data-hovercard-type="issue" data-hovercard-url="/ytdl-org/youtube-dl/issues/29326/hovercard">n-sig based throttling</a> <strong>*</strong></li>
<li>Supports some (but not all) age-gated content without cookies</li>
<li>Download livestreams from the start using <code>--live-from-start</code> (<em>experimental</em>)</li>
<li><code>255kbps</code> audio is extracted (if available) from YouTube Music when premium cookies are given</li>
<li>Channel URLs download all uploads of the channel, including shorts and live</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Cookies from browser</strong>: Cookies can be automatically extracted from all major web browsers using <code>--cookies-from-browser BROWSER[+KEYRING][:PROFILE][::CONTAINER]</code></p>
</li>
<li>
<p dir="auto"><strong>Download time range</strong>: Videos can be downloaded partially based on either timestamps or chapters using <code>--download-sections</code></p>
</li>
<li>
<p dir="auto"><strong>Split video by chapters</strong>: Videos can be split into multiple files based on chapters using <code>--split-chapters</code></p>
</li>
<li>
<p dir="auto"><strong>Multi-threaded fragment downloads</strong>: Download multiple fragments of m3u8/mpd videos in parallel. Use <code>--concurrent-fragments</code> (<code>-N</code>) option to set the number of threads used</p>
</li>
<li>
<p dir="auto"><strong>Aria2c with HLS/DASH</strong>: You can use <code>aria2c</code> as the external downloader for DASH(mpd) and HLS(m3u8) formats</p>
</li>
<li>
<p dir="auto"><strong>New and fixed extractors</strong>: Many new extractors have been added and a lot of existing ones have been fixed. See the <a href="https://github.com/yt-dlp/yt-dlp/blob/master/Changelog.md">changelog</a> or the <a href="https://github.com/yt-dlp/yt-dlp/blob/master/supportedsites.md">list of supported sites</a></p>
</li>
<li>
<p dir="auto"><strong>New MSOs</strong>: Philo, Spectrum, SlingTV, Cablevision, RCN etc.</p>
</li>
<li>
<p dir="auto"><strong>Subtitle extraction from manifests</strong>: Subtitles can be extracted from streaming media manifests. See <a href="https://github.com/yt-dlp/yt-dlp/commit/be6202f12b97858b9d716e608394b51065d0419f">commit/be6202f</a> for details</p>
</li>
<li>
<p dir="auto"><strong>Multiple paths and output templates</strong>: You can give different <a href="#output-template">output templates</a> and download paths for different types of files. You can also set a temporary path where intermediary files are downloaded to using <code>--paths</code> (<code>-P</code>)</p>
</li>
<li>
<p dir="auto"><strong>Portable Configuration</strong>: Configuration files are automatically loaded from the home and root directories. See <a href="#configuration">CONFIGURATION</a> for details</p>
</li>
<li>
<p dir="auto"><strong>Output template improvements</strong>: Output templates can now have date-time formatting, numeric offsets, object traversal etc. See <a href="#output-template">output template</a> for details. Even more advanced operations can also be done with the help of <code>--parse-metadata</code> and <code>--replace-in-metadata</code></p>
</li>
<li>
<p dir="auto"><strong>Other new options</strong>: Many new options have been added such as <code>--alias</code>, <code>--print</code>, <code>--concat-playlist</code>, <code>--wait-for-video</code>, <code>--retry-sleep</code>, <code>--sleep-requests</code>, <code>--convert-thumbnails</code>, <code>--force-download-archive</code>, <code>--force-overwrites</code>, <code>--break-match-filter</code> etc</p>
</li>
<li>
<p dir="auto"><strong>Improvements</strong>: Regex and other operators in <code>--format</code>/<code>--match-filter</code>, multiple <code>--postprocessor-args</code> and <code>--downloader-args</code>, faster archive checking, more <a href="#format-selection">format selection options</a>, merge multi-video/audio, multiple <code>--config-locations</code>, <code>--exec</code> at different stages, etc</p>
</li>
<li>
<p dir="auto"><strong>Plugins</strong>: Extractors and PostProcessors can be loaded from an external file. See <a href="#plugins">plugins</a> for details</p>
</li>
<li>
<p dir="auto"><strong>Self updater</strong>: The releases can be updated using <code>yt-dlp -U</code>, and downgraded using <code>--update-to</code> if required</p>
</li>
<li>
<p dir="auto"><strong>Nightly builds</strong>: <a href="#update-channels">Automated nightly builds</a> can be used with <code>--update-to nightly</code></p>
</li>
</ul>
<p dir="auto">See <a href="https://github.com/yt-dlp/yt-dlp/blob/master/Changelog.md">changelog</a> or <a href="https://github.com/yt-dlp/yt-dlp/commits">commits</a> for the full list of changes</p>
<p dir="auto">Features marked with a <strong>*</strong> have been back-ported to youtube-dl</p>
<h3 tabindex="-1" dir="auto">Differences in default behavior</h3>
<p dir="auto">Some of yt-dlp's default options are different from that of youtube-dl and youtube-dlc:</p>
<ul dir="auto">
<li>yt-dlp supports only <a href="##" title="Windows 7">Python 3.7+</a>, and <em>may</em> remove support for more versions as they <a href="https://devguide.python.org/versions/#python-release-cycle" rel="nofollow">become EOL</a>; while <a href="https://github.com/ytdl-org/youtube-dl/issues/30568#issue-1118238743" data-hovercard-type="issue" data-hovercard-url="/ytdl-org/youtube-dl/issues/30568/hovercard">youtube-dl still supports Python 2.6+ and 3.2+</a></li>
<li>The options <code>--auto-number</code> (<code>-A</code>), <code>--title</code> (<code>-t</code>) and <code>--literal</code> (<code>-l</code>), no longer work. See <a href="#Removed">removed options</a> for details</li>
<li><code>avconv</code> is not supported as an alternative to <code>ffmpeg</code></li>
<li>yt-dlp stores config files in slightly different locations to youtube-dl. See <a href="#configuration">CONFIGURATION</a> for a list of correct locations</li>
<li>The default <a href="#output-template">output template</a> is <code>%(title)s [%(id)s].%(ext)s</code>. There is no real reason for this change. This was changed before yt-dlp was ever made public and now there are no plans to change it back to <code>%(title)s-%(id)s.%(ext)s</code>. Instead, you may use <code>--compat-options filename</code></li>
<li>The default <a href="#sorting-formats">format sorting</a> is different from youtube-dl and prefers higher resolution and better codecs rather than higher bitrates. You can use the <code>--format-sort</code> option to change this to any order you prefer, or use <code>--compat-options format-sort</code> to use youtube-dl's sorting order</li>
<li>The default format selector is <code>bv*+ba/b</code>. This means that if a combined video + audio format that is better than the best video-only format is found, the former will be preferred. Use <code>-f bv+ba/b</code> or <code>--compat-options format-spec</code> to revert this</li>
<li>Unlike youtube-dlc, yt-dlp does not allow merging multiple audio/video streams into one file by default (since this conflicts with the use of <code>-f bv*+ba</code>). If needed, this feature must be enabled using <code>--audio-multistreams</code> and <code>--video-multistreams</code>. You can also use <code>--compat-options multistreams</code> to enable both</li>
<li><code>--no-abort-on-error</code> is enabled by default. Use <code>--abort-on-error</code> or <code>--compat-options abort-on-error</code> to abort on errors instead</li>
<li>When writing metadata files such as thumbnails, description or infojson, the same information (if available) is also written for playlists. Use <code>--no-write-playlist-metafiles</code> or <code>--compat-options no-playlist-metafiles</code> to not write these files</li>
<li><code>--add-metadata</code> attaches the <code>infojson</code> to <code>mkv</code> files in addition to writing the metadata when used with <code>--write-info-json</code>. Use <code>--no-embed-info-json</code> or <code>--compat-options no-attach-info-json</code> to revert this</li>
<li>Some metadata are embedded into different fields when using <code>--add-metadata</code> as compared to youtube-dl. Most notably, <code>comment</code> field contains the <code>webpage_url</code> and <code>synopsis</code> contains the <code>description</code>. You can <a href="#modifying-metadata">use <code>--parse-metadata</code></a> to modify this to your liking or use <code>--compat-options embed-metadata</code> to revert this</li>
<li><code>playlist_index</code> behaves differently when used with options like <code>--playlist-reverse</code> and <code>--playlist-items</code>. See <a href="https://github.com/yt-dlp/yt-dlp/issues/302" data-hovercard-type="pull_request" data-hovercard-url="/yt-dlp/yt-dlp/pull/302/hovercard">#302</a> for details. You can use <code>--compat-options playlist-index</code> if you want to keep the earlier behavior</li>
<li>The output of <code>-F</code> is listed in a new format. Use <code>--compat-options list-formats</code> to revert this</li>
<li>Live chats (if available) are considered as subtitles. Use <code>--sub-langs all,-live_chat</code> to download all subtitles except live chat. You can also use <code>--compat-options no-live-chat</code> to prevent any live chat/danmaku from downloading</li>
<li>YouTube channel URLs download all uploads of the channel. To download only the videos in a specific tab, pass the tab's URL. If the channel does not show the requested tab, an error will be raised. Also, <code>/live</code> URLs raise an error if there are no live videos instead of silently downloading the entire channel. You may use <code>--compat-options no-youtube-channel-redirect</code> to revert all these redirections</li>
<li>Unavailable videos are also listed for YouTube playlists. Use <code>--compat-options no-youtube-unavailable-videos</code> to remove this</li>
<li>The upload dates extracted from YouTube are in UTC <a href="https://github.com/yt-dlp/yt-dlp/blob/89e4d86171c7b7c997c77d4714542e0383bf0db0/yt_dlp/extractor/youtube.py#L3898-L3900">when available</a>. Use <code>--compat-options no-youtube-prefer-utc-upload-date</code> to prefer the non-UTC upload date.</li>
<li>If <code>ffmpeg</code> is used as the downloader, the downloading and merging of formats happen in a single step when possible. Use <code>--compat-options no-direct-merge</code> to revert this</li>
<li>Thumbnail embedding in <code>mp4</code> is done with mutagen if possible. Use <code>--compat-options embed-thumbnail-atomicparsley</code> to force the use of AtomicParsley instead</li>
<li>Some internal metadata such as filenames are removed by default from the infojson. Use <code>--no-clean-infojson</code> or <code>--compat-options no-clean-infojson</code> to revert this</li>
<li>When <code>--embed-subs</code> and <code>--write-subs</code> are used together, the subtitles are written to disk and also embedded in the media file. You can use just <code>--embed-subs</code> to embed the subs and automatically delete the separate file. See <a href="https://github.com/yt-dlp/yt-dlp/issues/630#issuecomment-893659460" data-hovercard-type="issue" data-hovercard-url="/yt-dlp/yt-dlp/issues/630/hovercard">#630 (comment)</a> for more info. <code>--compat-options no-keep-subs</code> can be used to revert this</li>
<li><code>certifi</code> will be used for SSL root certificates, if installed. If you want to use system certificates (e.g. self-signed), use <code>--compat-options no-certifi</code></li>
<li>yt-dlp's sanitization of invalid characters in filenames is different/smarter than in youtube-dl. You can use <code>--compat-options filename-sanitization</code> to revert to youtube-dl's behavior</li>
<li>yt-dlp tries to parse the external downloader outputs into the standard progress output if possible (Currently implemented: <a href="https://github.com/yt-dlp/yt-dlp/issues/5931" data-hovercard-type="issue" data-hovercard-url="/yt-dlp/yt-dlp/issues/5931/hovercard"><del>aria2c</del></a>). You can use <code>--compat-options no-external-downloader-progress</code> to get the downloader output as-is</li>
<li>yt-dlp versions between 2021.09.01 and 2023.01.02 applies <code>--match-filter</code> to nested playlists. This was an unintentional side-effect of <a href="https://github.com/yt-dlp/yt-dlp/commit/8f18aca8717bb0dd49054555af8d386e5eda3a88">8f18ac</a> and is fixed in <a href="https://github.com/yt-dlp/yt-dlp/commit/d7b460d0e5fc710950582baed2e3fc616ed98a80">d7b460</a>. Use <code>--compat-options playlist-match-filter</code> to revert this</li>
</ul>
<p dir="auto">For ease of use, a few more compat options are available:</p>
<ul dir="auto">
<li><code>--compat-options all</code>: Use all compat options (Do NOT use)</li>
<li><code>--compat-options youtube-dl</code>: Same as <code>--compat-options all,-multistreams,-playlist-match-filter</code></li>
<li><code>--compat-options youtube-dlc</code>: Same as <code>--compat-options all,-no-live-chat,-no-youtube-channel-redirect,-playlist-match-filter</code></li>
<li><code>--compat-options 2021</code>: Same as <code>--compat-options 2022,no-certifi,filename-sanitization,no-youtube-prefer-utc-upload-date</code></li>
<li><code>--compat-options 2022</code>: Same as <code>--compat-options playlist-match-filter,no-external-downloader-progress</code>. Use this to enable all future compat options</li>
</ul>
<h2 tabindex="-1" dir="auto">INSTALLATION</h2>

<p dir="auto"><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe"><img src="https://camo.githubusercontent.com/5e7d03f7f5cc1dc4cd6797a5ede9af299143001f2fc89a7386b87f3d4828c5d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d57696e646f77735f7836342d626c75652e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d77696e646f7773" alt="Windows" data-canonical-src="https://img.shields.io/badge/-Windows_x64-blue.svg?style=for-the-badge&amp;logo=windows"></a>
<a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp"><img src="https://camo.githubusercontent.com/5461aa20146a9fe60de1cc47ac0de9a070a05e73dec54a829d1cf21d85324974/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4c696e75782f4253442d7265642e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d6c696e7578" alt="Unix" data-canonical-src="https://img.shields.io/badge/-Linux/BSD-red.svg?style=for-the-badge&amp;logo=linux"></a>
<a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos"><img src="https://camo.githubusercontent.com/65d1ed3107ea8b6ab3c6b06766904cbc6fdd9e7fd961929f81349e38e3229767/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4d61634f532d6c69676874626c75652e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d6170706c65" alt="MacOS" data-canonical-src="https://img.shields.io/badge/-MacOS-lightblue.svg?style=for-the-badge&amp;logo=apple"></a>
<a href="https://pypi.org/project/yt-dlp" rel="nofollow"><img src="https://camo.githubusercontent.com/8360967fa65c453c411c016c2cfab78837823a995bd4a7510df6fa80f15085f6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d507950692d626c75652e7376673f6c6f676f3d70797069266c6162656c436f6c6f723d353535353535267374796c653d666f722d7468652d6261646765" alt="PyPi" data-canonical-src="https://img.shields.io/badge/-PyPi-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge"></a>
<a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz"><img src="https://camo.githubusercontent.com/d1a58041d43ca05b59ad01cc301b4282ffa507f12e87a7a1bb51d6e82080c312/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d536f757263655f7461722d677265656e2e7376673f7374796c653d666f722d7468652d6261646765" alt="Source Tarball" data-canonical-src="https://img.shields.io/badge/-Source_tar-green.svg?style=for-the-badge"></a>
<a href="#release-files"><img src="https://camo.githubusercontent.com/b38bcbc7dbeb210434768a9f20c9fcebf7d25fe6a3438334d910aae7ee277008/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4f746865722d677265792e7376673f7374796c653d666f722d7468652d6261646765" alt="Other variants" data-canonical-src="https://img.shields.io/badge/-Other-grey.svg?style=for-the-badge"></a>
<a href="https://github.com/yt-dlp/yt-dlp/releases"><img src="https://camo.githubusercontent.com/139754d1ce29070f1f33c96733a649a3fb009d6f2cf91edad2e86560d53d70ee/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d416c6c5f56657273696f6e732d6c69676874677265792e7376673f7374796c653d666f722d7468652d6261646765" alt="All versions" data-canonical-src="https://img.shields.io/badge/-All_Versions-lightgrey.svg?style=for-the-badge"></a></p>

<p dir="auto">You can install yt-dlp using <a href="#release-files">the binaries</a>, <a href="https://pypi.org/project/yt-dlp" rel="nofollow">pip</a> or one using a third-party package manager. See <a href="https://github.com/yt-dlp/yt-dlp/wiki/Installation">the wiki</a> for detailed instructions</p>
<h2 tabindex="-1" dir="auto">UPDATE</h2>
<p dir="auto">You can use <code>yt-dlp -U</code> to update if you are using the <a href="#release-files">release binaries</a></p>
<p dir="auto">If you <a href="https://github.com/yt-dlp/yt-dlp/wiki/Installation#with-pip">installed with pip</a>, simply re-run the same command that was used to install the program</p>
<p dir="auto">For other third-party package managers, see <a href="https://github.com/yt-dlp/yt-dlp/wiki/Installation#third-party-package-managers">the wiki</a> or refer their documentation</p>
<a id="user-content-update-channels">
</a><p dir="auto">There are currently two release channels for binaries, <code>stable</code> and <code>nightly</code>.
<code>stable</code> is the default channel, and many of its changes have been tested by users of the nightly channel.
The <code>nightly</code> channel has releases built after each push to the master branch, and will have the most recent fixes and additions, but also have more risk of regressions. They are available in <a href="https://github.com/yt-dlp/yt-dlp-nightly-builds/releases">their own repo</a>.</p>
<p dir="auto">When using <code>--update</code>/<code>-U</code>, a release binary will only update to its current channel.
<code>--update-to CHANNEL</code> can be used to switch to a different channel when a newer version is available. <code>--update-to [CHANNEL@]TAG</code> can also be used to upgrade or downgrade to specific tags from a channel.</p>
<p dir="auto">You may also use <code>--update-to &lt;repository&gt;</code> (<code>&lt;owner&gt;/&lt;repository&gt;</code>) to update to a channel on a completely different repository. Be careful with what repository you are updating to though, there is no verification done for binaries from different repositories.</p>
<p dir="auto">Example usage:</p>
<ul dir="auto">
<li><code>yt-dlp --update-to nightly</code> change to <code>nightly</code> channel and update to its latest release</li>
<li><code>yt-dlp --update-to stable@2023.02.17</code> upgrade/downgrade to release to <code>stable</code> channel tag <code>2023.02.17</code></li>
<li><code>yt-dlp --update-to 2023.01.06</code> upgrade/downgrade to tag <code>2023.01.06</code> if it exists on the current channel</li>
<li><code>yt-dlp --update-to example/yt-dlp@2023.03.01</code> upgrade/downgrade to the release from the <code>example/yt-dlp</code> repository, tag <code>2023.03.01</code></li>
</ul>

<h2 tabindex="-1" dir="auto">RELEASE FILES</h2>
<h4 tabindex="-1" dir="auto">Recommended</h4>
<table>
<thead>
<tr>
<th>File</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp">yt-dlp</a></td>
<td>Platform-independent <a href="https://docs.python.org/3/library/zipimport.html" rel="nofollow">zipimport</a> binary. Needs Python (recommended for <strong>Linux/BSD</strong>)</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe">yt-dlp.exe</a></td>
<td>Windows (Win7 SP1+) standalone x64 binary (recommended for <strong>Windows</strong>)</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos">yt-dlp_macos</a></td>
<td>Universal MacOS (10.15+) standalone executable (recommended for <strong>MacOS</strong>)</td>
</tr>
</tbody>
</table>
<h4 tabindex="-1" dir="auto">Alternatives</h4>
<table>
<thead>
<tr>
<th>File</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_x86.exe">yt-dlp_x86.exe</a></td>
<td>Windows (Vista SP2+) standalone x86 (32-bit) binary</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_min.exe">yt-dlp_min.exe</a></td>
<td>Windows (Win7 SP1+) standalone x64 binary built with <code>py2exe</code><br> (<a href="#standalone-py2exe-builds-windows">Not recommended</a>)</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux">yt-dlp_linux</a></td>
<td>Linux standalone x64 binary</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux.zip">yt-dlp_linux.zip</a></td>
<td>Unpackaged Linux executable (no auto-update)</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_armv7l">yt-dlp_linux_armv7l</a></td>
<td>Linux standalone armv7l (32-bit) binary</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64">yt-dlp_linux_aarch64</a></td>
<td>Linux standalone aarch64 (64-bit) binary</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win.zip">yt-dlp_win.zip</a></td>
<td>Unpackaged Windows executable (no auto-update)</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos.zip">yt-dlp_macos.zip</a></td>
<td>Unpackaged MacOS (10.15+) executable (no auto-update)</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos_legacy">yt-dlp_macos_legacy</a></td>
<td>MacOS (10.9+) standalone x64 executable</td>
</tr>
</tbody>
</table>
<h4 tabindex="-1" dir="auto">Misc</h4>
<table>
<thead>
<tr>
<th>File</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz">yt-dlp.tar.gz</a></td>
<td>Source tarball</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS">SHA2-512SUMS</a></td>
<td>GNU-style SHA512 sums</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS.sig">SHA2-512SUMS.sig</a></td>
<td>GPG signature file for SHA512 sums</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS">SHA2-256SUMS</a></td>
<td>GNU-style SHA256 sums</td>
</tr>
<tr>
<td><a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS.sig">SHA2-256SUMS.sig</a></td>
<td>GPG signature file for SHA256 sums</td>
</tr>
</tbody>
</table>
<p dir="auto">The public key that can be used to verify the GPG signatures is <a href="https://github.com/yt-dlp/yt-dlp/blob/master/public.key">available here</a>
Example usage:</p>
<div data-snippet-clipboard-copy-content="curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS"><pre><code>curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS
</code></pre></div>

<p dir="auto"><strong>Note</strong>: The manpages, shell completion (autocomplete) files etc. are available inside the <a href="https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz">source tarball</a></p>
<h2 tabindex="-1" dir="auto">DEPENDENCIES</h2>
<p dir="auto">Python versions 3.7+ (CPython and PyPy) are supported. Other versions and implementations may or may not work correctly.</p>

<p dir="auto">While all the other dependencies are optional, <code>ffmpeg</code> and <code>ffprobe</code> are highly recommended</p>
<h3 tabindex="-1" dir="auto">Strongly recommended</h3>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://www.ffmpeg.org/" rel="nofollow"><strong>ffmpeg</strong> and <strong>ffprobe</strong></a> - Required for <a href="#format-selection">merging separate video and audio files</a> as well as for various <a href="#post-processing-options">post-processing</a> tasks. License <a href="https://www.ffmpeg.org/legal.html" rel="nofollow">depends on the build</a></p>
<p dir="auto">There are bugs in ffmpeg that causes various issues when used alongside yt-dlp. Since ffmpeg is such an important dependency, we provide <a href="https://github.com/yt-dlp/FFmpeg-Builds#ffmpeg-static-auto-builds">custom builds</a> with patches for some of these issues at <a href="https://github.com/yt-dlp/FFmpeg-Builds">yt-dlp/FFmpeg-Builds</a>. See <a href="https://github.com/yt-dlp/FFmpeg-Builds#patches-applied">the readme</a> for details on the specific issues solved by these builds</p>
<p dir="auto"><strong>Important</strong>: What you need is ffmpeg <em>binary</em>, <strong>NOT</strong> <a href="https://pypi.org/project/ffmpeg" rel="nofollow">the python package of the same name</a></p>
</li>
</ul>
<h3 tabindex="-1" dir="auto">Networking</h3>
<ul dir="auto">
<li><a href="https://github.com/certifi/python-certifi"><strong>certifi</strong></a>* - Provides Mozilla's root certificate bundle. Licensed under <a href="https://github.com/certifi/python-certifi/blob/master/LICENSE">MPLv2</a></li>
<li><a href="https://github.com/google/brotli"><strong>brotli</strong></a>* or <a href="https://github.com/python-hyper/brotlicffi"><strong>brotlicffi</strong></a> - <a href="https://en.wikipedia.org/wiki/Brotli" rel="nofollow">Brotli</a> content encoding support. Both licensed under MIT <sup><a href="https://github.com/google/brotli/blob/master/LICENSE">1</a> <a href="https://github.com/python-hyper/brotlicffi/blob/master/LICENSE">2</a> </sup></li>
<li><a href="https://github.com/aaugustin/websockets"><strong>websockets</strong></a>* - For downloading over websocket. Licensed under <a href="https://github.com/aaugustin/websockets/blob/main/LICENSE">BSD-3-Clause</a></li>
</ul>
<h3 tabindex="-1" dir="auto">Metadata</h3>
<ul dir="auto">
<li><a href="https://github.com/quodlibet/mutagen"><strong>mutagen</strong></a>* - For <code>--embed-thumbnail</code> in certain formats. Licensed under <a href="https://github.com/quodlibet/mutagen/blob/master/COPYING">GPLv2+</a></li>
<li><a href="https://github.com/wez/atomicparsley"><strong>AtomicParsley</strong></a> - For <code>--embed-thumbnail</code> in <code>mp4</code>/<code>m4a</code> files when <code>mutagen</code>/<code>ffmpeg</code> cannot. Licensed under <a href="https://github.com/wez/atomicparsley/blob/master/COPYING">GPLv2+</a></li>
<li><a href="https://github.com/xattr/xattr"><strong>xattr</strong></a>, <a href="https://github.com/iustin/pyxattr"><strong>pyxattr</strong></a> or <a href="http://savannah.nongnu.org/projects/attr" rel="nofollow"><strong>setfattr</strong></a> - For writing xattr metadata (<code>--xattr</code>) on <strong>Linux</strong>. Licensed under <a href="https://github.com/xattr/xattr/blob/master/LICENSE.txt">MIT</a>, <a href="https://github.com/iustin/pyxattr/blob/master/COPYING">LGPL2.1</a> and <a href="http://git.savannah.nongnu.org/cgit/attr.git/tree/doc/COPYING" rel="nofollow">GPLv2+</a> respectively</li>
</ul>
<h3 tabindex="-1" dir="auto">Misc</h3>
<ul dir="auto">
<li><a href="https://github.com/Legrandin/pycryptodome"><strong>pycryptodomex</strong></a>* - For decrypting AES-128 HLS streams and various other data. Licensed under <a href="https://github.com/Legrandin/pycryptodome/blob/master/LICENSE.rst">BSD-2-Clause</a></li>
<li><a href="https://github.com/ariya/phantomjs"><strong>phantomjs</strong></a> - Used in extractors where javascript needs to be run. Licensed under <a href="https://github.com/ariya/phantomjs/blob/master/LICENSE.BSD">BSD-3-Clause</a></li>
<li><a href="https://github.com/mitya57/secretstorage"><strong>secretstorage</strong></a> - For <code>--cookies-from-browser</code> to access the <strong>Gnome</strong> keyring while decrypting cookies of <strong>Chromium</strong>-based browsers on <strong>Linux</strong>. Licensed under <a href="https://github.com/mitya57/secretstorage/blob/master/LICENSE">BSD-3-Clause</a></li>
<li>Any external downloader that you want to use with <code>--downloader</code></li>
</ul>
<h3 tabindex="-1" dir="auto">Deprecated</h3>
<ul dir="auto">
<li><a href="https://www.libav.org/" rel="nofollow"><strong>avconv</strong> and <strong>avprobe</strong></a> - Now <strong>deprecated</strong> alternative to ffmpeg. License <a href="https://libav.org/legal" rel="nofollow">depends on the build</a></li>
<li><a href="https://github.com/faissaloo/SponSkrub"><strong>sponskrub</strong></a> - For using the now <strong>deprecated</strong> <a href="#sponskrub-options">sponskrub options</a>. Licensed under <a href="https://github.com/faissaloo/SponSkrub/blob/master/LICENCE.md">GPLv3+</a></li>
<li><a href="http://rtmpdump.mplayerhq.hu/" rel="nofollow"><strong>rtmpdump</strong></a> - For downloading <code>rtmp</code> streams. ffmpeg can be used instead with <code>--downloader ffmpeg</code>. Licensed under <a href="http://rtmpdump.mplayerhq.hu/" rel="nofollow">GPLv2+</a></li>
<li><a href="http://mplayerhq.hu/design7/info.html" rel="nofollow"><strong>mplayer</strong></a> or <a href="https://mpv.io/" rel="nofollow"><strong>mpv</strong></a> - For downloading <code>rstp</code>/<code>mms</code> streams. ffmpeg can be used instead with <code>--downloader ffmpeg</code>. Licensed under <a href="https://github.com/mpv-player/mpv/blob/master/Copyright">GPLv2+</a></li>
</ul>
<p dir="auto">To use or redistribute the dependencies, you must agree to their respective licensing terms.</p>
<p dir="auto">The standalone release binaries are built with the Python interpreter and the packages marked with <strong>*</strong> included.</p>
<p dir="auto">If you do not have the necessary dependencies for a task you are attempting, yt-dlp will warn you. All the currently available dependencies are visible at the top of the <code>--verbose</code> output</p>
<h2 tabindex="-1" dir="auto">COMPILE</h2>
<h3 tabindex="-1" dir="auto">Standalone PyInstaller Builds</h3>
<p dir="auto">To build the standalone executable, you must have Python and <code>pyinstaller</code> (plus any of yt-dlp's <a href="#dependencies">optional dependencies</a> if needed). Once you have all the necessary dependencies installed, simply run <code>pyinst.py</code>. The executable will be built for the same architecture (x86/ARM, 32/64 bit) as the Python used.</p>
<div data-snippet-clipboard-copy-content="python3 -m pip install -U pyinstaller -r requirements.txt
python3 devscripts/make_lazy_extractors.py
python3 pyinst.py"><pre><code>python3 -m pip install -U pyinstaller -r requirements.txt
python3 devscripts/make_lazy_extractors.py
python3 pyinst.py
</code></pre></div>
<p dir="auto">On some systems, you may need to use <code>py</code> or <code>python</code> instead of <code>python3</code>.</p>
<p dir="auto"><code>pyinst.py</code> accepts any arguments that can be passed to <code>pyinstaller</code>, such as <code>--onefile/-F</code> or <code>--onedir/-D</code>, which is further <a href="https://pyinstaller.org/en/stable/usage.html#what-to-generate" rel="nofollow">documented here</a>.</p>
<p dir="auto"><strong>Note</strong>: Pyinstaller versions below 4.4 <a href="https://github.com/pyinstaller/pyinstaller#requirements-and-tested-platforms">do not support</a> Python installed from the Windows store without using a virtual environment.</p>
<p dir="auto"><strong>Important</strong>: Running <code>pyinstaller</code> directly <strong>without</strong> using <code>pyinst.py</code> is <strong>not</strong> officially supported. This may or may not work correctly.</p>
<h3 tabindex="-1" dir="auto">Platform-independent Binary (UNIX)</h3>
<p dir="auto">You will need the build tools <code>python</code> (3.7+), <code>zip</code>, <code>make</code> (GNU), <code>pandoc</code>* and <code>pytest</code>*.</p>
<p dir="auto">After installing these, simply run <code>make</code>.</p>
<p dir="auto">You can also run <code>make yt-dlp</code> instead to compile only the binary without updating any of the additional files. (The build tools marked with <strong>*</strong> are not needed for this)</p>
<h3 tabindex="-1" dir="auto">Standalone Py2Exe Builds (Windows)</h3>
<p dir="auto">While we provide the option to build with <a href="https://www.py2exe.org/" rel="nofollow">py2exe</a>, it is recommended to build <a href="#standalone-pyinstaller-builds">using PyInstaller</a> instead since the py2exe builds <strong>cannot contain <code>pycryptodomex</code>/<code>certifi</code> and needs VC++14</strong> on the target computer to run.</p>
<p dir="auto">If you wish to build it anyway, install Python and py2exe, and then simply run <code>setup.py py2exe</code></p>
<div data-snippet-clipboard-copy-content="py -m pip install -U py2exe -r requirements.txt
py devscripts/make_lazy_extractors.py
py setup.py py2exe"><pre><code>py -m pip install -U py2exe -r requirements.txt
py devscripts/make_lazy_extractors.py
py setup.py py2exe
</code></pre></div>
<h3 tabindex="-1" dir="auto">Related scripts</h3>
<ul dir="auto">
<li><strong><code>devscripts/update-version.py</code></strong> - Update the version number based on current date.</li>
<li><strong><code>devscripts/set-variant.py</code></strong> - Set the build variant of the executable.</li>
<li><strong><code>devscripts/make_changelog.py</code></strong> - Create a markdown changelog using short commit messages and update <code>CONTRIBUTORS</code> file.</li>
<li><strong><code>devscripts/make_lazy_extractors.py</code></strong> - Create lazy extractors. Running this before building the binaries (any variant) will improve their startup performance. Set the environment variable <code>YTDLP_NO_LAZY_EXTRACTORS=1</code> if you wish to forcefully disable lazy extractor loading.</li>
</ul>
<p dir="auto">Note: See their <code>--help</code> for more info.</p>
<h3 tabindex="-1" dir="auto">Forking the project</h3>
<p dir="auto">If you fork the project on GitHub, you can run your fork's <a href="https://github.com/yt-dlp/yt-dlp/blob/master/.github/workflows/build.yml">build workflow</a> to automatically build the selected version(s) as artifacts. Alternatively, you can run the <a href="https://github.com/yt-dlp/yt-dlp/blob/master/.github/workflows/release.yml">release workflow</a> or enable the <a href="https://github.com/yt-dlp/yt-dlp/blob/master/.github/workflows/release-nightly.yml">nightly workflow</a> to create full (pre-)releases.</p>
<h2 tabindex="-1" dir="auto">USAGE AND OPTIONS</h2>

<div data-snippet-clipboard-copy-content="yt-dlp [OPTIONS] [--] URL [URL...]"><pre><code>yt-dlp [OPTIONS] [--] URL [URL...]
</code></pre></div>
<p dir="auto"><code>Ctrl+F</code> is your friend :D</p>


<h2 tabindex="-1" dir="auto">General Options:</h2>
<div data-snippet-clipboard-copy-content="-h, --help                      Print this help text and exit
--version                       Print program version and exit
-U, --update                    Update this program to the latest version
--no-update                     Do not check for updates (default)
--update-to [CHANNEL]@[TAG]     Upgrade/downgrade to a specific version.
                                CHANNEL can be a repository as well. CHANNEL
                                and TAG default to &quot;stable&quot; and &quot;latest&quot;
                                respectively if omitted; See &quot;UPDATE&quot; for
                                details. Supported channels: stable, nightly
-i, --ignore-errors             Ignore download and postprocessing errors.
                                The download will be considered successful
                                even if the postprocessing fails
--no-abort-on-error             Continue with next video on download errors;
                                e.g. to skip unavailable videos in a
                                playlist (default)
--abort-on-error                Abort downloading of further videos if an
                                error occurs (Alias: --no-ignore-errors)
--dump-user-agent               Display the current user-agent and exit
--list-extractors               List all supported extractors and exit
--extractor-descriptions        Output descriptions of all supported
                                extractors and exit
--use-extractors NAMES          Extractor names to use separated by commas.
                                You can also use regexes, &quot;all&quot;, &quot;default&quot;
                                and &quot;end&quot; (end URL matching); e.g. --ies
                                &quot;holodex.*,end,youtube&quot;. Prefix the name
                                with a &quot;-&quot; to exclude it, e.g. --ies
                                default,-generic. Use --list-extractors for
                                a list of extractor names. (Alias: --ies)
--default-search PREFIX         Use this prefix for unqualified URLs. E.g.
                                &quot;gvsearch2:python&quot; downloads two videos from
                                google videos for the search term &quot;python&quot;.
                                Use the value &quot;auto&quot; to let yt-dlp guess
                                (&quot;auto_warning&quot; to emit a warning when
                                guessing). &quot;error&quot; just throws an error. The
                                default value &quot;fixup_error&quot; repairs broken
                                URLs, but emits an error if this is not
                                possible instead of searching
--ignore-config                 Don't load any more configuration files
                                except those given by --config-locations.
                                For backward compatibility, if this option
                                is found inside the system configuration
                                file, the user configuration is not loaded.
                                (Alias: --no-config)
--no-config-locations           Do not load any custom configuration files
                                (default). When given inside a configuration
                                file, ignore all previous --config-locations
                                defined in the current file
--config-locations PATH         Location of the main configuration file;
                                either the path to the config or its
                                containing directory (&quot;-&quot; for stdin). Can be
                                used multiple times and inside other
                                configuration files
--flat-playlist                 Do not extract the videos of a playlist,
                                only list them
--no-flat-playlist              Fully extract the videos of a playlist
                                (default)
--live-from-start               Download livestreams from the start.
                                Currently only supported for YouTube
                                (Experimental)
--no-live-from-start            Download livestreams from the current time
                                (default)
--wait-for-video MIN[-MAX]      Wait for scheduled streams to become
                                available. Pass the minimum number of
                                seconds (or range) to wait between retries
--no-wait-for-video             Do not wait for scheduled streams (default)
--mark-watched                  Mark videos watched (even with --simulate)
--no-mark-watched               Do not mark videos watched (default)
--color [STREAM:]POLICY         Whether to emit color codes in output,
                                optionally prefixed by the STREAM (stdout or
                                stderr) to apply the setting to. Can be one
                                of &quot;always&quot;, &quot;auto&quot; (default), &quot;never&quot;, or
                                &quot;no_color&quot; (use non color terminal
                                sequences). Can be used multiple times
--compat-options OPTS           Options that can help keep compatibility
                                with youtube-dl or youtube-dlc
                                configurations by reverting some of the
                                changes made in yt-dlp. See &quot;Differences in
                                default behavior&quot; for details
--alias ALIASES OPTIONS         Create aliases for an option string. Unless
                                an alias starts with a dash &quot;-&quot;, it is
                                prefixed with &quot;--&quot;. Arguments are parsed
                                according to the Python string formatting
                                mini-language. E.g. --alias get-audio,-X
                                &quot;-S=aext:{0},abr -x --audio-format {0}&quot;
                                creates options &quot;--get-audio&quot; and &quot;-X&quot; that
                                takes an argument (ARG0) and expands to
                                &quot;-S=aext:ARG0,abr -x --audio-format ARG0&quot;.
                                All defined aliases are listed in the --help
                                output. Alias options can trigger more
                                aliases; so be careful to avoid defining
                                recursive options. As a safety measure, each
                                alias may be triggered a maximum of 100
                                times. This option can be used multiple times"><pre><code>-h, --help                      Print this help text and exit
--version                       Print program version and exit
-U, --update                    Update this program to the latest version
--no-update                     Do not check for updates (default)
--update-to [CHANNEL]@[TAG]     Upgrade/downgrade to a specific version.
                                CHANNEL can be a repository as well. CHANNEL
                                and TAG default to "stable" and "latest"
                                respectively if omitted; See "UPDATE" for
                                details. Supported channels: stable, nightly
-i, --ignore-errors             Ignore download and postprocessing errors.
                                The download will be considered successful
                                even if the postprocessing fails
--no-abort-on-error             Continue with next video on download errors;
                                e.g. to skip unavailable videos in a
                                playlist (default)
--abort-on-error                Abort downloading of further videos if an
                                error occurs (Alias: --no-ignore-errors)
--dump-user-agent               Display the current user-agent and exit
--list-extractors               List all supported extractors and exit
--extractor-descriptions        Output descriptions of all supported
                                extractors and exit
--use-extractors NAMES          Extractor names to use separated by commas.
                                You can also use regexes, "all", "default"
                                and "end" (end URL matching); e.g. --ies
                                "holodex.*,end,youtube". Prefix the name
                                with a "-" to exclude it, e.g. --ies
                                default,-generic. Use --list-extractors for
                                a list of extractor names. (Alias: --ies)
--default-search PREFIX         Use this prefix for unqualified URLs. E.g.
                                "gvsearch2:python" downloads two videos from
                                google videos for the search term "python".
                                Use the value "auto" to let yt-dlp guess
                                ("auto_warning" to emit a warning when
                                guessing). "error" just throws an error. The
                                default value "fixup_error" repairs broken
                                URLs, but emits an error if this is not
                                possible instead of searching
--ignore-config                 Don't load any more configuration files
                                except those given by --config-locations.
                                For backward compatibility, if this option
                                is found inside the system configuration
                                file, the user configuration is not loaded.
                                (Alias: --no-config)
--no-config-locations           Do not load any custom configuration files
                                (default). When given inside a configuration
                                file, ignore all previous --config-locations
                                defined in the current file
--config-locations PATH         Location of the main configuration file;
                                either the path to the config or its
                                containing directory ("-" for stdin). Can be
                                used multiple times and inside other
                                configuration files
--flat-playlist                 Do not extract the videos of a playlist,
                                only list them
--no-flat-playlist              Fully extract the videos of a playlist
                                (default)
--live-from-start               Download livestreams from the start.
                                Currently only supported for YouTube
                                (Experimental)
--no-live-from-start            Download livestreams from the current time
                                (default)
--wait-for-video MIN[-MAX]      Wait for scheduled streams to become
                                available. Pass the minimum number of
                                seconds (or range) to wait between retries
--no-wait-for-video             Do not wait for scheduled streams (default)
--mark-watched                  Mark videos watched (even with --simulate)
--no-mark-watched               Do not mark videos watched (default)
--color [STREAM:]POLICY         Whether to emit color codes in output,
                                optionally prefixed by the STREAM (stdout or
                                stderr) to apply the setting to. Can be one
                                of "always", "auto" (default), "never", or
                                "no_color" (use non color terminal
                                sequences). Can be used multiple times
--compat-options OPTS           Options that can help keep compatibility
                                with youtube-dl or youtube-dlc
                                configurations by reverting some of the
                                changes made in yt-dlp. See "Differences in
                                default behavior" for details
--alias ALIASES OPTIONS         Create aliases for an option string. Unless
                                an alias starts with a dash "-", it is
                                prefixed with "--". Arguments are parsed
                                according to the Python string formatting
                                mini-language. E.g. --alias get-audio,-X
                                "-S=aext:{0},abr -x --audio-format {0}"
                                creates options "--get-audio" and "-X" that
                                takes an argument (ARG0) and expands to
                                "-S=aext:ARG0,abr -x --audio-format ARG0".
                                All defined aliases are listed in the --help
                                output. Alias options can trigger more
                                aliases; so be careful to avoid defining
                                recursive options. As a safety measure, each
                                alias may be triggered a maximum of 100
                                times. This option can be used multiple times
</code></pre></div>
<h2 tabindex="-1" dir="auto">Network Options:</h2>
<div data-snippet-clipboard-copy-content="--proxy URL                     Use the specified HTTP/HTTPS/SOCKS proxy. To
                                enable SOCKS proxy, specify a proper scheme,
                                e.g. socks5://user:pass@127.0.0.1:1080/.
                                Pass in an empty string (--proxy &quot;&quot;) for
                                direct connection
--socket-timeout SECONDS        Time to wait before giving up, in seconds
--source-address IP             Client-side IP address to bind to
-4, --force-ipv4                Make all connections via IPv4
-6, --force-ipv6                Make all connections via IPv6
--enable-file-urls              Enable file:// URLs. This is disabled by
                                default for security reasons."><pre><code>--proxy URL                     Use the specified HTTP/HTTPS/SOCKS proxy. To
                                enable SOCKS proxy, specify a proper scheme,
                                e.g. socks5://user:pass@127.0.0.1:1080/.
                                Pass in an empty string (--proxy "") for
                                direct connection
--socket-timeout SECONDS        Time to wait before giving up, in seconds
--source-address IP             Client-side IP address to bind to
-4, --force-ipv4                Make all connections via IPv4
-6, --force-ipv6                Make all connections via IPv6
--enable-file-urls              Enable file:// URLs. This is disabled by
                                default for security reasons.
</code></pre></div>
<h2 tabindex="-1" dir="auto">Geo-restriction:</h2>
<div data-snippet-clipboard-copy-content="--geo-verification-proxy URL    Use this proxy to verify the IP address for
                                some geo-restricted sites. The default proxy
                                specified by --proxy (or none, if the option
                                is not present) is used for the actual
                                downloading
--xff VALUE                     How to fake X-Forwarded-For HTTP header to
                                try bypassing geographic restriction. One of
                                &quot;default&quot; (only when known to be useful),
                                &quot;never&quot;, an IP block in CIDR notation, or a
                                two-letter ISO 3166-2 country code"><pre><code>--geo-verification-proxy URL    Use this proxy to verify the IP address for
                                some geo-restricted sites. The default proxy
                                specified by --proxy (or none, if the option
                                is not present) is used for the actual
                                downloading
--xff VALUE                     How to fake X-Forwarded-For HTTP header to
                                try bypassing geographic restriction. One of
                                "default" (only when known to be useful),
                                "never", an IP block in CIDR notation, or a
                                two-letter ISO 3166-2 country code
</code></pre></div>
<h2 tabindex="-1" dir="auto">Video Selection:</h2>
<div data-snippet-clipboard-copy-content="-I, --playlist-items ITEM_SPEC  Comma separated playlist_index of the items
                                to download. You can specify a range using
                                &quot;[START]:[STOP][:STEP]&quot;. For backward
                                compatibility, START-STOP is also supported.
                                Use negative indices to count from the right
                                and negative STEP to download in reverse
                                order. E.g. &quot;-I 1:3,7,-5::2&quot; used on a
                                playlist of size 15 will download the items
                                at index 1,2,3,7,11,13,15
--min-filesize SIZE             Abort download if filesize is smaller than
                                SIZE, e.g. 50k or 44.6M
--max-filesize SIZE             Abort download if filesize is larger than
                                SIZE, e.g. 50k or 44.6M
--date DATE                     Download only videos uploaded on this date.
                                The date can be &quot;YYYYMMDD&quot; or in the format 
                                [now|today|yesterday][-N[day|week|month|year]].
                                E.g. &quot;--date today-2weeks&quot; downloads only
                                videos uploaded on the same day two weeks ago
--datebefore DATE               Download only videos uploaded on or before
                                this date. The date formats accepted is the
                                same as --date
--dateafter DATE                Download only videos uploaded on or after
                                this date. The date formats accepted is the
                                same as --date
--match-filters FILTER          Generic video filter. Any &quot;OUTPUT TEMPLATE&quot;
                                field can be compared with a number or a
                                string using the operators defined in
                                &quot;Filtering Formats&quot;. You can also simply
                                specify a field to match if the field is
                                present, use &quot;!field&quot; to check if the field
                                is not present, and &quot;&amp;&quot; to check multiple
                                conditions. Use a &quot;\&quot; to escape &quot;&amp;&quot; or
                                quotes if needed. If used multiple times,
                                the filter matches if atleast one of the
                                conditions are met. E.g. --match-filter
                                !is_live --match-filter &quot;like_count>?100 &amp;
                                description~='(?i)\bcats \&amp; dogs\b'&quot; matches
                                only videos that are not live OR those that
                                have a like count more than 100 (or the like
                                field is not available) and also has a
                                description that contains the phrase &quot;cats &amp;
                                dogs&quot; (caseless). Use &quot;--match-filter -&quot; to
                                interactively ask whether to download each
                                video
--no-match-filters              Do not use any --match-filter (default)
--break-match-filters FILTER    Same as &quot;--match-filters&quot; but stops the
                                download process when a video is rejected
--no-break-match-filters        Do not use any --break-match-filters (default)
--no-playlist                   Download only the video, if the URL refers
                                to a video and a playlist
--yes-playlist                  Download the playlist, if the URL refers to
                                a video and a playlist
--age-limit YEARS               Download only videos suitable for the given
                                age
--download-archive FILE         Download only videos not listed in the
                                archive file. Record the IDs of all
                                downloaded videos in it
--no-download-archive           Do not use archive file (default)
--max-downloads NUMBER          Abort after downloading NUMBER files
--break-on-existing             Stop the download process when encountering
                                a file that is in the archive
--break-per-input               Alters --max-downloads, --break-on-existing,
                                --break-match-filter, and autonumber to
                                reset per input URL
--no-break-per-input            --break-on-existing and similar options
                                terminates the entire download queue
--skip-playlist-after-errors N  Number of allowed failures until the rest of
                                the playlist is skipped"><pre><code>-I, --playlist-items ITEM_SPEC  Comma separated playlist_index of the items
                                to download. You can specify a range using
                                "[START]:[STOP][:STEP]". For backward
                                compatibility, START-STOP is also supported.
                                Use negative indices to count from the right
                                and negative STEP to download in reverse
                                order. E.g. "-I 1:3,7,-5::2" used on a
                                playlist of size 15 will download the items
                                at index 1,2,3,7,11,13,15
--min-filesize SIZE             Abort download if filesize is smaller than
                                SIZE, e.g. 50k or 44.6M
--max-filesize SIZE             Abort download if filesize is larger than
                                SIZE, e.g. 50k or 44.6M
--date DATE                     Download only videos uploaded on this date.
                                The date can be "YYYYMMDD" or in the format 
                                [now|today|yesterday][-N[day|week|month|year]].
                                E.g. "--date today-2weeks" downloads only
                                videos uploaded on the same day two weeks ago
--datebefore DATE               Download only videos uploaded on or before
                                this date. The date formats accepted is the
                                same as --date
--dateafter DATE                Download only videos uploaded on or after
                                this date. The date formats accepted is the
                                same as --date
--match-filters FILTER          Generic video filter. Any "OUTPUT TEMPLATE"
                                field can be compared with a number or a
                                string using the operators defined in
                                "Filtering Formats". You can also simply
                                specify a field to match if the field is
                                present, use "!field" to check if the field
                                is not present, and "&amp;" to check multiple
                                conditions. Use a "\" to escape "&amp;" or
                                quotes if needed. If used multiple times,
                                the filter matches if atleast one of the
                                conditions are met. E.g. --match-filter
                                !is_live --match-filter "like_count&gt;?100 &amp;
                                description~='(?i)\bcats \&amp; dogs\b'" matches
                                only videos that are not live OR those that
                                have a like count more than 100 (or the like
                                field is not available) and also has a
                                description that contains the phrase "cats &amp;
                                dogs" (caseless). Use "--match-filter -" to
                                interactively ask whether to download each
                                video
--no-match-filters              Do not use any --match-filter (default)
--break-match-filters FILTER    Same as "--match-filters" but stops the
                                download process when a video is rejected
--no-break-match-filters        Do not use any --break-match-filters (default)
--no-playlist                   Download only the video, if the URL refers
                                to a video and a playlist
--yes-playlist                  Download the playlist, if the URL refers to
                                a video and a playlist
--age-limit YEARS               Download only videos suitable for the given
                                age
--download-archive FILE         Download only videos not listed in the
                                archive file. Record the IDs of all
                                downloaded videos in it
--no-download-archive           Do not use archive file (default)
--max-downloads NUMBER          Abort after downloading NUMBER files
--break-on-existing             Stop the download process when encountering
                                a file that is in the archive
--break-per-input               Alters --max-downloads, --break-on-existing,
                                --break-match-filter, and autonumber to
                                reset per input URL
--no-break-per-input            --break-on-existing and similar options
                                terminates the entire download queue
--skip-playlist-after-errors N  Number of allowed failures until the rest of
                                the playlist is skipped
</code></pre></div>
<h2 tabindex="-1" dir="auto">Download Options:</h2>
<div data-snippet-clipboard-copy-content="-N, --concurrent-fragments N    Number of fragments of a dash/hlsnative
                                video that should be downloaded concurrently
                                (default is 1)
-r, --limit-rate RATE           Maximum download rate in bytes per second,
                                e.g. 50K or 4.2M
--throttled-rate RATE           Minimum download rate in bytes per second
                                below which throttling is assumed and the
                                video data is re-extracted, e.g. 100K
-R, --retries RETRIES           Number of retries (default is 10), or
                                &quot;infinite&quot;
--file-access-retries RETRIES   Number of times to retry on file access
                                error (default is 3), or &quot;infinite&quot;
--fragment-retries RETRIES      Number of retries for a fragment (default is
                                10), or &quot;infinite&quot; (DASH, hlsnative and ISM)
--retry-sleep [TYPE:]EXPR       Time to sleep between retries in seconds
                                (optionally) prefixed by the type of retry
                                (http (default), fragment, file_access,
                                extractor) to apply the sleep to. EXPR can
                                be a number, linear=START[:END[:STEP=1]] or
                                exp=START[:END[:BASE=2]]. This option can be
                                used multiple times to set the sleep for the
                                different retry types, e.g. --retry-sleep
                                linear=1::2 --retry-sleep fragment:exp=1:20
--skip-unavailable-fragments    Skip unavailable fragments for DASH,
                                hlsnative and ISM downloads (default)
                                (Alias: --no-abort-on-unavailable-fragments)
--abort-on-unavailable-fragments
                                Abort download if a fragment is unavailable
                                (Alias: --no-skip-unavailable-fragments)
--keep-fragments                Keep downloaded fragments on disk after
                                downloading is finished
--no-keep-fragments             Delete downloaded fragments after
                                downloading is finished (default)
--buffer-size SIZE              Size of download buffer, e.g. 1024 or 16K
                                (default is 1024)
--resize-buffer                 The buffer size is automatically resized
                                from an initial value of --buffer-size
                                (default)
--no-resize-buffer              Do not automatically adjust the buffer size
--http-chunk-size SIZE          Size of a chunk for chunk-based HTTP
                                downloading, e.g. 10485760 or 10M (default
                                is disabled). May be useful for bypassing
                                bandwidth throttling imposed by a webserver
                                (experimental)
--playlist-random               Download playlist videos in random order
--lazy-playlist                 Process entries in the playlist as they are
                                received. This disables n_entries,
                                --playlist-random and --playlist-reverse
--no-lazy-playlist              Process videos in the playlist only after
                                the entire playlist is parsed (default)
--xattr-set-filesize            Set file xattribute ytdl.filesize with
                                expected file size
--hls-use-mpegts                Use the mpegts container for HLS videos;
                                allowing some players to play the video
                                while downloading, and reducing the chance
                                of file corruption if download is
                                interrupted. This is enabled by default for
                                live streams
--no-hls-use-mpegts             Do not use the mpegts container for HLS
                                videos. This is default when not downloading
                                live streams
--download-sections REGEX       Download only chapters that match the
                                regular expression. A &quot;*&quot; prefix denotes
                                time-range instead of chapter. Negative
                                timestamps are calculated from the end.
                                &quot;*from-url&quot; can be used to download between
                                the &quot;start_time&quot; and &quot;end_time&quot; extracted
                                from the URL. Needs ffmpeg. This option can
                                be used multiple times to download multiple
                                sections, e.g. --download-sections
                                &quot;*10:15-inf&quot; --download-sections &quot;intro&quot;
--downloader [PROTO:]NAME       Name or path of the external downloader to
                                use (optionally) prefixed by the protocols
                                (http, ftp, m3u8, dash, rstp, rtmp, mms) to
                                use it for. Currently supports native,
                                aria2c, avconv, axel, curl, ffmpeg, httpie,
                                wget. You can use this option multiple times
                                to set different downloaders for different
                                protocols. E.g. --downloader aria2c
                                --downloader &quot;dash,m3u8:native&quot; will use
                                aria2c for http/ftp downloads, and the
                                native downloader for dash/m3u8 downloads
                                (Alias: --external-downloader)
--downloader-args NAME:ARGS     Give these arguments to the external
                                downloader. Specify the downloader name and
                                the arguments separated by a colon &quot;:&quot;. For
                                ffmpeg, arguments can be passed to different
                                positions using the same syntax as
                                --postprocessor-args. You can use this
                                option multiple times to give different
                                arguments to different downloaders (Alias:
                                --external-downloader-args)"><pre><code>-N, --concurrent-fragments N    Number of fragments of a dash/hlsnative
                                video that should be downloaded concurrently
                                (default is 1)
-r, --limit-rate RATE           Maximum download rate in bytes per second,
                                e.g. 50K or 4.2M
--throttled-rate RATE           Minimum download rate in bytes per second
                                below which throttling is assumed and the
                                video data is re-extracted, e.g. 100K
-R, --retries RETRIES           Number of retries (default is 10), or
                                "infinite"
--file-access-retries RETRIES   Number of times to retry on file access
                                error (default is 3), or "infinite"
--fragment-retries RETRIES      Number of retries for a fragment (default is
                                10), or "infinite" (DASH, hlsnative and ISM)
--retry-sleep [TYPE:]EXPR       Time to sleep between retries in seconds
                                (optionally) prefixed by the type of retry
                                (http (default), fragment, file_access,
                                extractor) to apply the sleep to. EXPR can
                                be a number, linear=START[:END[:STEP=1]] or
                                exp=START[:END[:BASE=2]]. This option can be
                                used multiple times to set the sleep for the
                                different retry types, e.g. --retry-sleep
                                linear=1::2 --retry-sleep fragment:exp=1:20
--skip-unavailable-fragments    Skip unavailable fragments for DASH,
                                hlsnative and ISM downloads (default)
                                (Alias: --no-abort-on-unavailable-fragments)
--abort-on-unavailable-fragments
                                Abort download if a fragment is unavailable
                                (Alias: --no-skip-unavailable-fragments)
--keep-fragments                Keep downloaded fragments on disk after
                                downloading is finished
--no-keep-fragments             Delete downloaded fragments after
                                downloading is finished (default)
--buffer-size SIZE              Size of download buffer, e.g. 1024 or 16K
                                (default is 1024)
--resize-buffer                 The buffer size is automatically resized
                                from an initial value of --buffer-size
                                (default)
--no-resize-buffer              Do not automatically adjust the buffer size
--http-chunk-size SIZE          Size of a chunk for chunk-based HTTP
                                downloading, e.g. 10485760 or 10M (default
                                is disabled). May be useful for bypassing
                                bandwidth throttling imposed by a webserver
                                (experimental)
--playlist-random               Download playlist videos in random order
--lazy-playlist                 Process entries in the playlist as they are
                                received. This disables n_entries,
                                --playlist-random and --playlist-reverse
--no-lazy-playlist              Process videos in the playlist only after
                                the entire playlist is parsed (default)
--xattr-set-filesize            Set file xattribute ytdl.filesize with
                                expected file size
--hls-use-mpegts                Use the mpegts container for HLS videos;
                                allowing some players to play the video
                                while downloading, and reducing the chance
                                of file corruption if download is
                                interrupted. This is enabled by default for
                                live streams
--no-hls-use-mpegts             Do not use the mpegts container for HLS
                                videos. This is default when not downloading
                                live streams
--download-sections REGEX       Download only chapters that match the
                                regular expression. A "*" prefix denotes
                                time-range instead of chapter. Negative
                                timestamps are calculated from the end.
                                "*from-url" can be used to download between
                                the "start_time" and "end_time" extracted
                                from the URL. Needs ffmpeg. This option can
                                be used multiple times to download multiple
                                sections, e.g. --download-sections
                                "*10:15-inf" --download-sections "intro"
--downloader [PROTO:]NAME       Name or path of the external downloader to
                                use (optionally) prefixed by the protocols
                                (http, ftp, m3u8, dash, rstp, rtmp, mms) to
                                use it for. Currently supports native,
                                aria2c, avconv, axel, curl, ffmpeg, httpie,
                                wget. You can use this option multiple times
                                to set different downloaders for different
                                protocols. E.g. --downloader aria2c
                                --downloader "dash,m3u8:native" will use
                                aria2c for http/ftp downloads, and the
                                native downloader for dash/m3u8 downloads
                                (Alias: --external-downloader)
--downloader-args NAME:ARGS     Give these arguments to the external
                                downloader. Specify the downloader name and
                                the arguments separated by a colon ":". For
                                ffmpeg, arguments can be passed to different
                                positions using the same syntax as
                                --postprocessor-args. You can use this
                                option multiple times to give different
                                arguments to different downloaders (Alias:
                                --external-downloader-args)
</code></pre></div>
<h2 tabindex="-1" dir="auto">Filesystem Options:</h2>
<div data-snippet-clipboard-copy-content="-a, --batch-file FILE           File containing URLs to download (&quot;-&quot; for
                                stdin), one URL per line. Lines starting
                                with &quot;#&quot;, &quot;;&quot; or &quot;]&quot; are considered as
                                comments and ignored
--no-batch-file                 Do not read URLs from batch file (default)
-P, --paths [TYPES:]PATH        The paths where the files should be
                                downloaded. Specify the type of file and the
                                path separated by a colon &quot;:&quot;. All the same
                                TYPES as --output are supported.
                                Additionally, you can also provide &quot;home&quot;
                                (default) and &quot;temp&quot; paths. All intermediary
                                files are first downloaded to the temp path
                                and then the final files are moved over to
                                the home path after download is finished.
                                This option is ignored if --output is an
                                absolute path
-o, --output [TYPES:]TEMPLATE   Output filename template; see &quot;OUTPUT
                                TEMPLATE&quot; for details
--output-na-placeholder TEXT    Placeholder for unavailable fields in
                                &quot;OUTPUT TEMPLATE&quot; (default: &quot;NA&quot;)
--restrict-filenames            Restrict filenames to only ASCII characters,
                                and avoid &quot;&amp;&quot; and spaces in filenames
--no-restrict-filenames         Allow Unicode characters, &quot;&amp;&quot; and spaces in
                                filenames (default)
--windows-filenames             Force filenames to be Windows-compatible
--no-windows-filenames          Make filenames Windows-compatible only if
                                using Windows (default)
--trim-filenames LENGTH         Limit the filename length (excluding
                                extension) to the specified number of
                                characters
-w, --no-overwrites             Do not overwrite any files
--force-overwrites              Overwrite all video and metadata files. This
                                option includes --no-continue
--no-force-overwrites           Do not overwrite the video, but overwrite
                                related files (default)
-c, --continue                  Resume partially downloaded files/fragments
                                (default)
--no-continue                   Do not resume partially downloaded
                                fragments. If the file is not fragmented,
                                restart download of the entire file
--part                          Use .part files instead of writing directly
                                into output file (default)
--no-part                       Do not use .part files - write directly into
                                output file
--mtime                         Use the Last-modified header to set the file
                                modification time (default)
--no-mtime                      Do not use the Last-modified header to set
                                the file modification time
--write-description             Write video description to a .description file
--no-write-description          Do not write video description (default)
--write-info-json               Write video metadata to a .info.json file
                                (this may contain personal information)
--no-write-info-json            Do not write video metadata (default)
--write-playlist-metafiles      Write playlist metadata in addition to the
                                video metadata when using --write-info-json,
                                --write-description etc. (default)
--no-write-playlist-metafiles   Do not write playlist metadata when using
                                --write-info-json, --write-description etc.
--clean-info-json               Remove some internal metadata such as
                                filenames from the infojson (default)
--no-clean-info-json            Write all fields to the infojson
--write-comments                Retrieve video comments to be placed in the
                                infojson. The comments are fetched even
                                without this option if the extraction is
                                known to be quick (Alias: --get-comments)
--no-write-comments             Do not retrieve video comments unless the
                                extraction is known to be quick (Alias:
                                --no-get-comments)
--load-info-json FILE           JSON file containing the video information
                                (created with the &quot;--write-info-json&quot; option)
--cookies FILE                  Netscape formatted file to read cookies from
                                and dump cookie jar in
--no-cookies                    Do not read/dump cookies from/to file
                                (default)
--cookies-from-browser BROWSER[+KEYRING][:PROFILE][::CONTAINER]
                                The name of the browser to load cookies
                                from. Currently supported browsers are:
                                brave, chrome, chromium, edge, firefox,
                                opera, safari, vivaldi. Optionally, the
                                KEYRING used for decrypting Chromium cookies
                                on Linux, the name/path of the PROFILE to
                                load cookies from, and the CONTAINER name
                                (if Firefox) (&quot;none&quot; for no container) can
                                be given with their respective seperators.
                                By default, all containers of the most
                                recently accessed profile are used.
                                Currently supported keyrings are: basictext,
                                gnomekeyring, kwallet, kwallet5, kwallet6
--no-cookies-from-browser       Do not load cookies from browser (default)
--cache-dir DIR                 Location in the filesystem where yt-dlp can
                                store some downloaded information (such as
                                client ids and signatures) permanently. By
                                default ${XDG_CACHE_HOME}/yt-dlp
--no-cache-dir                  Disable filesystem caching
--rm-cache-dir                  Delete all filesystem cache files"><pre><code>-a, --batch-file FILE           File containing URLs to download ("-" for
                                stdin), one URL per line. Lines starting
                                with "#", ";" or "]" are considered as
                                comments and ignored
--no-batch-file                 Do not read URLs from batch file (default)
-P, --paths [TYPES:]PATH        The paths where the files should be
                                downloaded. Specify the type of file and the
                                path separated by a colon ":". All the same
                                TYPES as --output are supported.
                                Additionally, you can also provide "home"
                                (default) and "temp" paths. All intermediary
                                files are first downloaded to the temp path
                                and then the final files are moved over to
                                the home path after download is finished.
                                This option is ignored if --output is an
                                absolute path
-o, --output [TYPES:]TEMPLATE   Output filename template; see "OUTPUT
                                TEMPLATE" for details
--output-na-placeholder TEXT    Placeholder for unavailable fields in
                                "OUTPUT TEMPLATE" (default: "NA")
--restrict-filenames            Restrict filenames to only ASCII characters,
                                and avoid "&amp;" and spaces in filenames
--no-restrict-filenames         Allow Unicode characters, "&amp;" and spaces in
                                filenames (default)
--windows-filenames             Force filenames to be Windows-compatible
--no-windows-filenames          Make filenames Windows-compatible only if
                                using Windows (default)
--trim-filenames LENGTH         Limit the filename length (excluding
                                extension) to the specified number of
                                characters
-w, --no-overwrites             Do not overwrite any files
--force-overwrites              Overwrite all video and metadata files. This
                                option includes --no-continue
--no-force-overwrites           Do not overwrite the video, but overwrite
                                related files (default)
-c, --continue                  Resume partially downloaded files/fragments
                                (default)
--no-continue                   Do not resume partially downloaded
                                fragments. If the file is not fragmented,
                                restart download of the entire file
--part                          Use .part files instead of writing directly
                                into output file (default)
--no-part                       Do not use .part files - write directly into
                                output file
--mtime                         Use the Last-modified header to set the file
                                modification time (default)
--no-mtime                      Do not use the Last-modified header to set
                                the file modification time
--write-description             Write video description to a .description file
--no-write-description          Do not write video description (default)
--write-info-json               Write video metadata to a .info.json file
                                (this may contain personal information)
--no-write-info-json            Do not write video metadata (default)
--write-playlist-metafiles      Write playlist metadata in addition to the
                                video metadata when using --write-info-json,
                                --write-description etc. (default)
--no-write-playlist-metafiles   Do not write playlist metadata when using
                                --write-info-json, --write-description etc.
--clean-info-json               Remove some internal metadata such as
                                filenames from the infojson (default)
--no-clean-info-json            Write all fields to the infojson
--write-comments                Retrieve video comments to be placed in the
                                infojson. The comments are fetched even
                                without this option if the extraction is
                                known to be quick (Alias: --get-comments)
--no-write-comments             Do not retrieve video comments unless the
                                extraction is known to be quick (Alias:
                                --no-get-comments)
--load-info-json FILE           JSON file containing the video information
                                (created with the "--write-info-json" option)
--cookies FILE                  Netscape formatted file to read cookies from
                                and dump cookie jar in
--no-cookies                    Do not read/dump cookies from/to file
                                (default)
--cookies-from-browser BROWSER[+KEYRING][:PROFILE][::CONTAINER]
                                The name of the browser to load cookies
                                from. Currently supported browsers are:
                                brave, chrome, chromium, edge, firefox,
                                opera, safari, vivaldi. Optionally, the
                                KEYRING used for decrypting Chromium cookies
                                on Linux, the name/path of the PROFILE to
                                load cookies from, and the CONTAINER name
                                (if Firefox) ("none" for no container) can
                                be given with their respective seperators.
                                By default, all containers of the most
                                recently accessed profile are used.
                                Currently supported keyrings are: basictext,
                                gnomekeyring, kwallet, kwallet5, kwallet6
--no-cookies-from-browser       Do not load cookies from browser (default)
--cache-dir DIR                 Location in the filesystem where yt-dlp can
                                store some downloaded information (such as
                                client ids and signatures) permanently. By
                                default ${XDG_CACHE_HOME}/yt-dlp
--no-cache-dir                  Disable filesystem caching
--rm-cache-dir                  Delete all filesystem cache files
</code></pre></div>
<h2 tabindex="-1" dir="auto">Thumbnail Options:</h2>
<div data-snippet-clipboard-copy-content="--write-thumbnail               Write thumbnail image to disk
--no-write-thumbnail            Do not write thumbnail image to disk (default)
--write-all-thumbnails          Write all thumbnail image formats to disk
--list-thumbnails               List available thumbnails of each video.
                                Simulate unless --no-simulate is used"><pre><code>--write-thumbnail               Write thumbnail image to disk
--no-write-thumbnail            Do not write thumbnail image to disk (default)
--write-all-thumbnails          Write all thumbnail image formats to disk
--list-thumbnails               List available thumbnails of each video.
                                Simulate unless --no-simulate is used
</code></pre></div>
<h2 tabindex="-1" dir="auto">Internet Shortcut Options:</h2>
<div data-snippet-clipboard-copy-content="--write-link                    Write an internet shortcut file, depending
                                on the current platform (.url, .webloc or
                                .desktop). The URL may be cached by the OS
--write-url-link                Write a .url Windows internet shortcut. The
                                OS caches the URL based on the file path
--write-webloc-link             Write a .webloc macOS internet shortcut
--write-desktop-link            Write a .desktop Linux internet shortcut"><pre><code>--write-link                    Write an internet shortcut file, depending
                                on the current platform (.url, .webloc or
                                .desktop). The URL may be cached by the OS
--write-url-link                Write a .url Windows internet shortcut. The
                                OS caches the URL based on the file path
--write-webloc-link             Write a .webloc macOS internet shortcut
--write-desktop-link            Write a .desktop Linux internet shortcut
</code></pre></div>
<h2 tabindex="-1" dir="auto">Verbosity and Simulation Options:</h2>
<div data-snippet-clipboard-copy-content="-q, --quiet                     Activate quiet mode. If used with --verbose,
                                print the log to stderr
--no-quiet                      Deactivate quiet mode. (Default)
--no-warnings                   Ignore warnings
-s, --simulate                  Do not download the video and do not write
                                anything to disk
--no-simulate                   Download the video even if printing/listing
                                options are used
--ignore-no-formats-error       Ignore &quot;No video formats&quot; error. Useful for
                                extracting metadata even if the videos are
                                not actually available for download
                                (experimental)
--no-ignore-no-formats-error    Throw error when no downloadable video
                                formats are found (default)
--skip-download                 Do not download the video but write all
                                related files (Alias: --no-download)
-O, --print [WHEN:]TEMPLATE     Field name or output template to print to
                                screen, optionally prefixed with when to
                                print it, separated by a &quot;:&quot;. Supported
                                values of &quot;WHEN&quot; are the same as that of
                                --use-postprocessor (default: video).
                                Implies --quiet. Implies --simulate unless
                                --no-simulate or later stages of WHEN are
                                used. This option can be used multiple times
--print-to-file [WHEN:]TEMPLATE FILE
                                Append given template to the file. The
                                values of WHEN and TEMPLATE are same as that
                                of --print. FILE uses the same syntax as the
                                output template. This option can be used
                                multiple times
-j, --dump-json                 Quiet, but print JSON information for each
                                video. Simulate unless --no-simulate is
                                used. See &quot;OUTPUT TEMPLATE&quot; for a
                                description of available keys
-J, --dump-single-json          Quiet, but print JSON information for each
                                url or infojson passed. Simulate unless
                                --no-simulate is used. If the URL refers to
                                a playlist, the whole playlist information
                                is dumped in a single line
--force-write-archive           Force download archive entries to be written
                                as far as no errors occur, even if -s or
                                another simulation option is used (Alias:
                                --force-download-archive)
--newline                       Output progress bar as new lines
--no-progress                   Do not print progress bar
--progress                      Show progress bar, even if in quiet mode
--console-title                 Display progress in console titlebar
--progress-template [TYPES:]TEMPLATE
                                Template for progress outputs, optionally
                                prefixed with one of &quot;download:&quot; (default),
                                &quot;download-title:&quot; (the console title),
                                &quot;postprocess:&quot;,  or &quot;postprocess-title:&quot;.
                                The video's fields are accessible under the
                                &quot;info&quot; key and the progress attributes are
                                accessible under &quot;progress&quot; key. E.g.
                                --console-title --progress-template
                                &quot;download-title:%(info.id)s-%(progress.eta)s&quot;
-v, --verbose                   Print various debugging information
--dump-pages                    Print downloaded pages encoded using base64
                                to debug problems (very verbose)
--write-pages                   Write downloaded intermediary pages to files
                                in the current directory to debug problems
--print-traffic                 Display sent and read HTTP traffic"><pre><code>-q, --quiet                     Activate quiet mode. If used with --verbose,
                                print the log to stderr
--no-quiet                      Deactivate quiet mode. (Default)
--no-warnings                   Ignore warnings
-s, --simulate                  Do not download the video and do not write
                                anything to disk
--no-simulate                   Download the video even if printing/listing
                                options are used
--ignore-no-formats-error       Ignore "No video formats" error. Useful for
                                extracting metadata even if the videos are
                                not actually available for download
                                (experimental)
--no-ignore-no-formats-error    Throw error when no downloadable video
                                formats are found (default)
--skip-download                 Do not download the video but write all
                                related files (Alias: --no-download)
-O, --print [WHEN:]TEMPLATE     Field name or output template to print to
                                screen, optionally prefixed with when to
                                print it, separated by a ":". Supported
                                values of "WHEN" are the same as that of
                                --use-postprocessor (default: video).
                                Implies --quiet. Implies --simulate unless
                                --no-simulate or later stages of WHEN are
                                used. This option can be used multiple times
--print-to-file [WHEN:]TEMPLATE FILE
                                Append given template to the file. The
                                values of WHEN and TEMPLATE are same as that
                                of --print. FILE uses the same syntax as the
                                output template. This option can be used
                                multiple times
-j, --dump-json                 Quiet, but print JSON information for each
                                video. Simulate unless --no-simulate is
                                used. See "OUTPUT TEMPLATE" for a
                                description of available keys
-J, --dump-single-json          Quiet, but print JSON information for each
                                url or infojson passed. Simulate unless
                                --no-simulate is used. If the URL refers to
                                a playlist, the whole playlist information
                                is dumped in a single line
--force-write-archive           Force download archive entries to be written
                                as far as no errors occur, even if -s or
                                another simulation option is used (Alias:
                                --force-download-archive)
--newline                       Output progress bar as new lines
--no-progress                   Do not print progress bar
--progress                      Show progress bar, even if in quiet mode
--console-title                 Display progress in console titlebar
--progress-template [TYPES:]TEMPLATE
                                Template for progress outputs, optionally
                                prefixed with one of "download:" (default),
                                "download-title:" (the console title),
                                "postprocess:",  or "postprocess-title:".
                                The video's fields are accessible under the
                                "info" key and the progress attributes are
                                accessible under "progress" key. E.g.
                                --console-title --progress-template
                                "download-title:%(info.id)s-%(progress.eta)s"
-v, --verbose                   Print various debugging information
--dump-pages                    Print downloaded pages encoded using base64
                                to debug problems (very verbose)
--write-pages                   Write downloaded intermediary pages to files
                                in the current directory to debug problems
--print-traffic                 Display sent and read HTTP traffic
</code></pre></div>
<h2 tabindex="-1" dir="auto">Workarounds:</h2>
<div data-snippet-clipboard-copy-content="--encoding ENCODING             Force the specified encoding (experimental)
--legacy-server-connect         Explicitly allow HTTPS connection to servers
                                that do not support RFC 5746 secure
                                renegotiation
--no-check-certificates         Suppress HTTPS certificate validation
--prefer-insecure               Use an unencrypted connection to retrieve
                                information about the video (Currently
                                supported only for YouTube)
--add-headers FIELD:VALUE       Specify a custom HTTP header and its value,
                                separated by a colon &quot;:&quot;. You can use this
                                option multiple times
--bidi-workaround               Work around terminals that lack
                                bidirectional text support. Requires bidiv
                                or fribidi executable in PATH
--sleep-requests SECONDS        Number of seconds to sleep between requests
                                during data extraction
--sleep-interval SECONDS        Number of seconds to sleep before each
                                download. This is the minimum time to sleep
                                when used along with --max-sleep-interval
                                (Alias: --min-sleep-interval)
--max-sleep-interval SECONDS    Maximum number of seconds to sleep. Can only
                                be used along with --min-sleep-interval
--sleep-subtitles SECONDS       Number of seconds to sleep before each
                                subtitle download"><pre><code>--encoding ENCODING             Force the specified encoding (experimental)
--legacy-server-connect         Explicitly allow HTTPS connection to servers
                                that do not support RFC 5746 secure
                                renegotiation
--no-check-certificates         Suppress HTTPS certificate validation
--prefer-insecure               Use an unencrypted connection to retrieve
                                information about the video (Currently
                                supported only for YouTube)
--add-headers FIELD:VALUE       Specify a custom HTTP header and its value,
                                separated by a colon ":". You can use this
                                option multiple times
--bidi-workaround               Work around terminals that lack
                                bidirectional text support. Requires bidiv
                                or fribidi executable in PATH
--sleep-requests SECONDS        Number of seconds to sleep between requests
                                during data extraction
--sleep-interval SECONDS        Number of seconds to sleep before each
                                download. This is the minimum time to sleep
                                when used along with --max-sleep-interval
                                (Alias: --min-sleep-interval)
--max-sleep-interval SECONDS    Maximum number of seconds to sleep. Can only
                                be used along with --min-sleep-interval
--sleep-subtitles SECONDS       Number of seconds to sleep before each
                                subtitle download
</code></pre></div>
<h2 tabindex="-1" dir="auto">Video Format Options:</h2>
<div data-snippet-clipboard-copy-content="-f, --format FORMAT             Video format code, see &quot;FORMAT SELECTION&quot;
                                for more details
-S, --format-sort SORTORDER     Sort the formats by the fields given, see
                                &quot;Sorting Formats&quot; for more details
--format-sort-force             Force user specified sort order to have
                                precedence over all fields, see &quot;Sorting
                                Formats&quot; for more details (Alias: --S-force)
--no-format-sort-force          Some fields have precedence over the user
                                specified sort order (default)
--video-multistreams            Allow multiple video streams to be merged
                                into a single file
--no-video-multistreams         Only one video stream is downloaded for each
                                output file (default)
--audio-multistreams            Allow multiple audio streams to be merged
                                into a single file
--no-audio-multistreams         Only one audio stream is downloaded for each
                                output file (default)
--prefer-free-formats           Prefer video formats with free containers
                                over non-free ones of same quality. Use with
                                &quot;-S ext&quot; to strictly prefer free containers
                                irrespective of quality
--no-prefer-free-formats        Don't give any special preference to free
                                containers (default)
--check-formats                 Make sure formats are selected only from
                                those that are actually downloadable
--check-all-formats             Check all formats for whether they are
                                actually downloadable
--no-check-formats              Do not check that the formats are actually
                                downloadable
-F, --list-formats              List available formats of each video.
                                Simulate unless --no-simulate is used
--merge-output-format FORMAT    Containers that may be used when merging
                                formats, separated by &quot;/&quot;, e.g. &quot;mp4/mkv&quot;.
                                Ignored if no merge is required. (currently
                                supported: avi, flv, mkv, mov, mp4, webm)"><pre><code>-f, --format FORMAT             Video format code, see "FORMAT SELECTION"
                                for more details
-S, --format-sort SORTORDER     Sort the formats by the fields given, see
                                "Sorting Formats" for more details
--format-sort-force             Force user specified sort order to have
                                precedence over all fields, see "Sorting
                                Formats" for more details (Alias: --S-force)
--no-format-sort-force          Some fields have precedence over the user
                                specified sort order (default)
--video-multistreams            Allow multiple video streams to be merged
                                into a single file
--no-video-multistreams         Only one video stream is downloaded for each
                                output file (default)
--audio-multistreams            Allow multiple audio streams to be merged
                                into a single file
--no-audio-multistreams         Only one audio stream is downloaded for each
                                output file (default)
--prefer-free-formats           Prefer video formats with free containers
                                over non-free ones of same quality. Use with
                                "-S ext" to strictly prefer free containers
                                irrespective of quality
--no-prefer-free-formats        Don't give any special preference to free
                                containers (default)
--check-formats                 Make sure formats are selected only from
                                those that are actually downloadable
--check-all-formats             Check all formats for whether they are
                                actually downloadable
--no-check-formats              Do not check that the formats are actually
                                downloadable
-F, --list-formats              List available formats of each video.
                                Simulate unless --no-simulate is used
--merge-output-format FORMAT    Containers that may be used when merging
                                formats, separated by "/", e.g. "mp4/mkv".
                                Ignored if no merge is required. (currently
                                supported: avi, flv, mkv, mov, mp4, webm)
</code></pre></div>
<h2 tabindex="-1" dir="auto">Subtitle Options:</h2>
<div data-snippet-clipboard-copy-content="--write-subs                    Write subtitle file
--no-write-subs                 Do not write subtitle file (default)
--write-auto-subs               Write automatically generated subtitle file
                                (Alias: --write-automatic-subs)
--no-write-auto-subs            Do not write auto-generated subtitles
                                (default) (Alias: --no-write-automatic-subs)
--list-subs                     List available subtitles of each video.
                                Simulate unless --no-simulate is used
--sub-format FORMAT             Subtitle format; accepts formats preference,
                                e.g. &quot;srt&quot; or &quot;ass/srt/best&quot;
--sub-langs LANGS               Languages of the subtitles to download (can
                                be regex) or &quot;all&quot; separated by commas, e.g.
                                --sub-langs &quot;en.*,ja&quot;. You can prefix the
                                language code with a &quot;-&quot; to exclude it from
                                the requested languages, e.g. --sub-langs
                                all,-live_chat. Use --list-subs for a list
                                of available language tags"><pre><code>--write-subs                    Write subtitle file
--no-write-subs                 Do not write subtitle file (default)
--write-auto-subs               Write automatically generated subtitle file
                                (Alias: --write-automatic-subs)
--no-write-auto-subs            Do not write auto-generated subtitles
                                (default) (Alias: --no-write-automatic-subs)
--list-subs                     List available subtitles of each video.
                                Simulate unless --no-simulate is used
--sub-format FORMAT             Subtitle format; accepts formats preference,
                                e.g. "srt" or "ass/srt/best"
--sub-langs LANGS               Languages of the subtitles to download (can
                                be regex) or "all" separated by commas, e.g.
                                --sub-langs "en.*,ja". You can prefix the
                                language code with a "-" to exclude it from
                                the requested languages, e.g. --sub-langs
                                all,-live_chat. Use --list-subs for a list
                                of available language tags
</code></pre></div>
<h2 tabindex="-1" dir="auto">Authentication Options:</h2>
<div data-snippet-clipboard-copy-content="-u, --username USERNAME         Login with this account ID
-p, --password PASSWORD         Account password. If this option is left
                                out, yt-dlp will ask interactively
-2, --twofactor TWOFACTOR       Two-factor authentication code
-n, --netrc                     Use .netrc authentication data
--netrc-location PATH           Location of .netrc authentication data;
                                either the path or its containing directory.
                                Defaults to ~/.netrc
--netrc-cmd NETRC_CMD           Command to execute to get the credentials
                                for an extractor.
--video-password PASSWORD       Video password (vimeo, youku)
--ap-mso MSO                    Adobe Pass multiple-system operator (TV
                                provider) identifier, use --ap-list-mso for
                                a list of available MSOs
--ap-username USERNAME          Multiple-system operator account login
--ap-password PASSWORD          Multiple-system operator account password.
                                If this option is left out, yt-dlp will ask
                                interactively
--ap-list-mso                   List all supported multiple-system operators
--client-certificate CERTFILE   Path to client certificate file in PEM
                                format. May include the private key
--client-certificate-key KEYFILE
                                Path to private key file for client
                                certificate
--client-certificate-password PASSWORD
                                Password for client certificate private key,
                                if encrypted. If not provided, and the key
                                is encrypted, yt-dlp will ask interactively"><pre><code>-u, --username USERNAME         Login with this account ID
-p, --password PASSWORD         Account password. If this option is left
                                out, yt-dlp will ask interactively
-2, --twofactor TWOFACTOR       Two-factor authentication code
-n, --netrc                     Use .netrc authentication data
--netrc-location PATH           Location of .netrc authentication data;
                                either the path or its containing directory.
                                Defaults to ~/.netrc
--netrc-cmd NETRC_CMD           Command to execute to get the credentials
                                for an extractor.
--video-password PASSWORD       Video password (vimeo, youku)
--ap-mso MSO                    Adobe Pass multiple-system operator (TV
                                provider) identifier, use --ap-list-mso for
                                a list of available MSOs
--ap-username USERNAME          Multiple-system operator account login
--ap-password PASSWORD          Multiple-system operator account password.
                                If this option is left out, yt-dlp will ask
                                interactively
--ap-list-mso                   List all supported multiple-system operators
--client-certificate CERTFILE   Path to client certificate file in PEM
                                format. May include the private key
--client-certificate-key KEYFILE
                                Path to private key file for client
                                certificate
--client-certificate-password PASSWORD
                                Password for client certificate private key,
                                if encrypted. If not provided, and the key
                                is encrypted, yt-dlp will ask interactively
</code></pre></div>
<h2 tabindex="-1" dir="auto">Post-Processing Options:</h2>
<div data-snippet-clipboard-copy-content="-x, --extract-audio             Convert video files to audio-only files
                                (requires ffmpeg and ffprobe)
--audio-format FORMAT           Format to convert the audio to when -x is
                                used. (currently supported: best (default),
                                aac, alac, flac, m4a, mp3, opus, vorbis,
                                wav). You can specify multiple rules using
                                similar syntax as --remux-video
--audio-quality QUALITY         Specify ffmpeg audio quality to use when
                                converting the audio with -x. Insert a value
                                between 0 (best) and 10 (worst) for VBR or a
                                specific bitrate like 128K (default 5)
--remux-video FORMAT            Remux the video into another container if
                                necessary (currently supported: avi, flv,
                                gif, mkv, mov, mp4, webm, aac, aiff, alac,
                                flac, m4a, mka, mp3, ogg, opus, vorbis,
                                wav). If target container does not support
                                the video/audio codec, remuxing will fail.
                                You can specify multiple rules; e.g.
                                &quot;aac>m4a/mov>mp4/mkv&quot; will remux aac to m4a,
                                mov to mp4 and anything else to mkv
--recode-video FORMAT           Re-encode the video into another format if
                                necessary. The syntax and supported formats
                                are the same as --remux-video
--postprocessor-args NAME:ARGS  Give these arguments to the postprocessors.
                                Specify the postprocessor/executable name
                                and the arguments separated by a colon &quot;:&quot;
                                to give the argument to the specified
                                postprocessor/executable. Supported PP are:
                                Merger, ModifyChapters, SplitChapters,
                                ExtractAudio, VideoRemuxer, VideoConvertor,
                                Metadata, EmbedSubtitle, EmbedThumbnail,
                                SubtitlesConvertor, ThumbnailsConvertor,
                                FixupStretched, FixupM4a, FixupM3u8,
                                FixupTimestamp and FixupDuration. The
                                supported executables are: AtomicParsley,
                                FFmpeg and FFprobe. You can also specify
                                &quot;PP+EXE:ARGS&quot; to give the arguments to the
                                specified executable only when being used by
                                the specified postprocessor. Additionally,
                                for ffmpeg/ffprobe, &quot;_i&quot;/&quot;_o&quot; can be
                                appended to the prefix optionally followed
                                by a number to pass the argument before the
                                specified input/output file, e.g. --ppa
                                &quot;Merger+ffmpeg_i1:-v quiet&quot;. You can use
                                this option multiple times to give different
                                arguments to different postprocessors.
                                (Alias: --ppa)
-k, --keep-video                Keep the intermediate video file on disk
                                after post-processing
--no-keep-video                 Delete the intermediate video file after
                                post-processing (default)
--post-overwrites               Overwrite post-processed files (default)
--no-post-overwrites            Do not overwrite post-processed files
--embed-subs                    Embed subtitles in the video (only for mp4,
                                webm and mkv videos)
--no-embed-subs                 Do not embed subtitles (default)
--embed-thumbnail               Embed thumbnail in the video as cover art
--no-embed-thumbnail            Do not embed thumbnail (default)
--embed-metadata                Embed metadata to the video file. Also
                                embeds chapters/infojson if present unless
                                --no-embed-chapters/--no-embed-info-json are
                                used (Alias: --add-metadata)
--no-embed-metadata             Do not add metadata to file (default)
                                (Alias: --no-add-metadata)
--embed-chapters                Add chapter markers to the video file
                                (Alias: --add-chapters)
--no-embed-chapters             Do not add chapter markers (default) (Alias:
                                --no-add-chapters)
--embed-info-json               Embed the infojson as an attachment to
                                mkv/mka video files
--no-embed-info-json            Do not embed the infojson as an attachment
                                to the video file
--parse-metadata [WHEN:]FROM:TO
                                Parse additional metadata like title/artist
                                from other fields; see &quot;MODIFYING METADATA&quot;
                                for details. Supported values of &quot;WHEN&quot; are
                                the same as that of --use-postprocessor
                                (default: pre_process)
--replace-in-metadata [WHEN:]FIELDS REGEX REPLACE
                                Replace text in a metadata field using the
                                given regex. This option can be used
                                multiple times. Supported values of &quot;WHEN&quot;
                                are the same as that of --use-postprocessor
                                (default: pre_process)
--xattrs                        Write metadata to the video file's xattrs
                                (using dublin core and xdg standards)
--concat-playlist POLICY        Concatenate videos in a playlist. One of
                                &quot;never&quot;, &quot;always&quot;, or &quot;multi_video&quot;
                                (default; only when the videos form a single
                                show). All the video files must have same
                                codecs and number of streams to be
                                concatable. The &quot;pl_video:&quot; prefix can be
                                used with &quot;--paths&quot; and &quot;--output&quot; to set
                                the output filename for the concatenated
                                files. See &quot;OUTPUT TEMPLATE&quot; for details
--fixup POLICY                  Automatically correct known faults of the
                                file. One of never (do nothing), warn (only
                                emit a warning), detect_or_warn (the
                                default; fix file if we can, warn
                                otherwise), force (try fixing even if file
                                already exists)
--ffmpeg-location PATH          Location of the ffmpeg binary; either the
                                path to the binary or its containing directory
--exec [WHEN:]CMD               Execute a command, optionally prefixed with
                                when to execute it, separated by a &quot;:&quot;.
                                Supported values of &quot;WHEN&quot; are the same as
                                that of --use-postprocessor (default:
                                after_move). Same syntax as the output
                                template can be used to pass any field as
                                arguments to the command. If no fields are
                                passed, %(filepath,_filename|)q is appended
                                to the end of the command. This option can
                                be used multiple times
--no-exec                       Remove any previously defined --exec
--convert-subs FORMAT           Convert the subtitles to another format
                                (currently supported: ass, lrc, srt, vtt)
                                (Alias: --convert-subtitles)
--convert-thumbnails FORMAT     Convert the thumbnails to another format
                                (currently supported: jpg, png, webp). You
                                can specify multiple rules using similar
                                syntax as --remux-video
--split-chapters                Split video into multiple files based on
                                internal chapters. The &quot;chapter:&quot; prefix can
                                be used with &quot;--paths&quot; and &quot;--output&quot; to set
                                the output filename for the split files. See
                                &quot;OUTPUT TEMPLATE&quot; for details
--no-split-chapters             Do not split video based on chapters (default)
--remove-chapters REGEX         Remove chapters whose title matches the
                                given regular expression. The syntax is the
                                same as --download-sections. This option can
                                be used multiple times
--no-remove-chapters            Do not remove any chapters from the file
                                (default)
--force-keyframes-at-cuts       Force keyframes at cuts when
                                downloading/splitting/removing sections.
                                This is slow due to needing a re-encode, but
                                the resulting video may have fewer artifacts
                                around the cuts
--no-force-keyframes-at-cuts    Do not force keyframes around the chapters
                                when cutting/splitting (default)
--use-postprocessor NAME[:ARGS]
                                The (case sensitive) name of plugin
                                postprocessors to be enabled, and
                                (optionally) arguments to be passed to it,
                                separated by a colon &quot;:&quot;. ARGS are a
                                semicolon &quot;;&quot; delimited list of NAME=VALUE.
                                The &quot;when&quot; argument determines when the
                                postprocessor is invoked. It can be one of
                                &quot;pre_process&quot; (after video extraction),
                                &quot;after_filter&quot; (after video passes filter),
                                &quot;video&quot; (after --format; before
                                --print/--output), &quot;before_dl&quot; (before each
                                video download), &quot;post_process&quot; (after each
                                video download; default), &quot;after_move&quot;
                                (after moving video file to it's final
                                locations), &quot;after_video&quot; (after downloading
                                and processing all formats of a video), or
                                &quot;playlist&quot; (at end of playlist). This option
                                can be used multiple times to add different
                                postprocessors"><pre><code>-x, --extract-audio             Convert video files to audio-only files
                                (requires ffmpeg and ffprobe)
--audio-format FORMAT           Format to convert the audio to when -x is
                                used. (currently supported: best (default),
                                aac, alac, flac, m4a, mp3, opus, vorbis,
                                wav). You can specify multiple rules using
                                similar syntax as --remux-video
--audio-quality QUALITY         Specify ffmpeg audio quality to use when
                                converting the audio with -x. Insert a value
                                between 0 (best) and 10 (worst) for VBR or a
                                specific bitrate like 128K (default 5)
--remux-video FORMAT            Remux the video into another container if
                                necessary (currently supported: avi, flv,
                                gif, mkv, mov, mp4, webm, aac, aiff, alac,
                                flac, m4a, mka, mp3, ogg, opus, vorbis,
                                wav). If target container does not support
                                the video/audio codec, remuxing will fail.
                                You can specify multiple rules; e.g.
                                "aac&gt;m4a/mov&gt;mp4/mkv" will remux aac to m4a,
                                mov to mp4 and anything else to mkv
--recode-video FORMAT           Re-encode the video into another format if
                                necessary. The syntax and supported formats
                                are the same as --remux-video
--postprocessor-args NAME:ARGS  Give these arguments to the postprocessors.
                                Specify the postprocessor/executable name
                                and the arguments separated by a colon ":"
                                to give the argument to the specified
                                postprocessor/executable. Supported PP are:
                                Merger, ModifyChapters, SplitChapters,
                                ExtractAudio, VideoRemuxer, VideoConvertor,
                                Metadata, EmbedSubtitle, EmbedThumbnail,
                                SubtitlesConvertor, ThumbnailsConvertor,
                                FixupStretched, FixupM4a, FixupM3u8,
                                FixupTimestamp and FixupDuration. The
                                supported executables are: AtomicParsley,
                                FFmpeg and FFprobe. You can also specify
                                "PP+EXE:ARGS" to give the arguments to the
                                specified executable only when being used by
                                the specified postprocessor. Additionally,
                                for ffmpeg/ffprobe, "_i"/"_o" can be
                                appended to the prefix optionally followed
                                by a number to pass the argument before the
                                specified input/output file, e.g. --ppa
                                "Merger+ffmpeg_i1:-v quiet". You can use
                                this option multiple times to give different
                                arguments to different postprocessors.
                                (Alias: --ppa)
-k, --keep-video                Keep the intermediate video file on disk
                                after post-processing
--no-keep-video                 Delete the intermediate video file after
                                post-processing (default)
--post-overwrites               Overwrite post-processed files (default)
--no-post-overwrites            Do not overwrite post-processed files
--embed-subs                    Embed subtitles in the video (only for mp4,
                                webm and mkv videos)
--no-embed-subs                 Do not embed subtitles (default)
--embed-thumbnail               Embed thumbnail in the video as cover art
--no-embed-thumbnail            Do not embed thumbnail (default)
--embed-metadata                Embed metadata to the video file. Also
                                embeds chapters/infojson if present unless
                                --no-embed-chapters/--no-embed-info-json are
                                used (Alias: --add-metadata)
--no-embed-metadata             Do not add metadata to file (default)
                                (Alias: --no-add-metadata)
--embed-chapters                Add chapter markers to the video file
                                (Alias: --add-chapters)
--no-embed-chapters             Do not add chapter markers (default) (Alias:
                                --no-add-chapters)
--embed-info-json               Embed the infojson as an attachment to
                                mkv/mka video files
--no-embed-info-json            Do not embed the infojson as an attachment
                                to the video file
--parse-metadata [WHEN:]FROM:TO
                                Parse additional metadata like title/artist
                                from other fields; see "MODIFYING METADATA"
                                for details. Supported values of "WHEN" are
                                the same as that of --use-postprocessor
                                (default: pre_process)
--replace-in-metadata [WHEN:]FIELDS REGEX REPLACE
                                Replace text in a metadata field using the
                                given regex. This option can be used
                                multiple times. Supported values of "WHEN"
                                are the same as that of --use-postprocessor
                                (default: pre_process)
--xattrs                        Write metadata to the video file's xattrs
                                (using dublin core and xdg standards)
--concat-playlist POLICY        Concatenate videos in a playlist. One of
                                "never", "always", or "multi_video"
                                (default; only when the videos form a single
                                show). All the video files must have same
                                codecs and number of streams to be
                                concatable. The "pl_video:" prefix can be
                                used with "--paths" and "--output" to set
                                the output filename for the concatenated
                                files. See "OUTPUT TEMPLATE" for details
--fixup POLICY                  Automatically correct known faults of the
                                file. One of never (do nothing), warn (only
                                emit a warning), detect_or_warn (the
                                default; fix file if we can, warn
                                otherwise), force (try fixing even if file
                                already exists)
--ffmpeg-location PATH          Location of the ffmpeg binary; either the
                                path to the binary or its containing directory
--exec [WHEN:]CMD               Execute a command, optionally prefixed with
                                when to execute it, separated by a ":".
                                Supported values of "WHEN" are the same as
                                that of --use-postprocessor (default:
                                after_move). Same syntax as the output
                                template can be used to pass any field as
                                arguments to the command. If no fields are
                                passed, %(filepath,_filename|)q is appended
                                to the end of the command. This option can
                                be used multiple times
--no-exec                       Remove any previously defined --exec
--convert-subs FORMAT           Convert the subtitles to another format
                                (currently supported: ass, lrc, srt, vtt)
                                (Alias: --convert-subtitles)
--convert-thumbnails FORMAT     Convert the thumbnails to another format
                                (currently supported: jpg, png, webp). You
                                can specify multiple rules using similar
                                syntax as --remux-video
--split-chapters                Split video into multiple files based on
                                internal chapters. The "chapter:" prefix can
                                be used with "--paths" and "--output" to set
                                the output filename for the split files. See
                                "OUTPUT TEMPLATE" for details
--no-split-chapters             Do not split video based on chapters (default)
--remove-chapters REGEX         Remove chapters whose title matches the
                                given regular expression. The syntax is the
                                same as --download-sections. This option can
                                be used multiple times
--no-remove-chapters            Do not remove any chapters from the file
                                (default)
--force-keyframes-at-cuts       Force keyframes at cuts when
                                downloading/splitting/removing sections.
                                This is slow due to needing a re-encode, but
                                the resulting video may have fewer artifacts
                                around the cuts
--no-force-keyframes-at-cuts    Do not force keyframes around the chapters
                                when cutting/splitting (default)
--use-postprocessor NAME[:ARGS]
                                The (case sensitive) name of plugin
                                postprocessors to be enabled, and
                                (optionally) arguments to be passed to it,
                                separated by a colon ":". ARGS are a
                                semicolon ";" delimited list of NAME=VALUE.
                                The "when" argument determines when the
                                postprocessor is invoked. It can be one of
                                "pre_process" (after video extraction),
                                "after_filter" (after video passes filter),
                                "video" (after --format; before
                                --print/--output), "before_dl" (before each
                                video download), "post_process" (after each
                                video download; default), "after_move"
                                (after moving video file to it's final
                                locations), "after_video" (after downloading
                                and processing all formats of a video), or
                                "playlist" (at end of playlist). This option
                                can be used multiple times to add different
                                postprocessors
</code></pre></div>
<h2 tabindex="-1" dir="auto">SponsorBlock Options:</h2>
<p dir="auto">Make chapter entries for, or remove various segments (sponsor,
introductions, etc.) from downloaded YouTube videos using the
<a href="https://sponsor.ajay.app/" rel="nofollow">SponsorBlock API</a></p>
<div data-snippet-clipboard-copy-content="--sponsorblock-mark CATS        SponsorBlock categories to create chapters
                                for, separated by commas. Available
                                categories are sponsor, intro, outro,
                                selfpromo, preview, filler, interaction,
                                music_offtopic, poi_highlight, chapter, all
                                and default (=all). You can prefix the
                                category with a &quot;-&quot; to exclude it. See [1]
                                for description of the categories. E.g.
                                --sponsorblock-mark all,-preview
                                [1] https://wiki.sponsor.ajay.app/w/Segment_Categories
--sponsorblock-remove CATS      SponsorBlock categories to be removed from
                                the video file, separated by commas. If a
                                category is present in both mark and remove,
                                remove takes precedence. The syntax and
                                available categories are the same as for
                                --sponsorblock-mark except that &quot;default&quot;
                                refers to &quot;all,-filler&quot; and poi_highlight,
                                chapter are not available
--sponsorblock-chapter-title TEMPLATE
                                An output template for the title of the
                                SponsorBlock chapters created by
                                --sponsorblock-mark. The only available
                                fields are start_time, end_time, category,
                                categories, name, category_names. Defaults
                                to &quot;[SponsorBlock]: %(category_names)l&quot;
--no-sponsorblock               Disable both --sponsorblock-mark and
                                --sponsorblock-remove
--sponsorblock-api URL          SponsorBlock API location, defaults to
                                https://sponsor.ajay.app"><pre><code>--sponsorblock-mark CATS        SponsorBlock categories to create chapters
                                for, separated by commas. Available
                                categories are sponsor, intro, outro,
                                selfpromo, preview, filler, interaction,
                                music_offtopic, poi_highlight, chapter, all
                                and default (=all). You can prefix the
                                category with a "-" to exclude it. See [1]
                                for description of the categories. E.g.
                                --sponsorblock-mark all,-preview
                                [1] https://wiki.sponsor.ajay.app/w/Segment_Categories
--sponsorblock-remove CATS      SponsorBlock categories to be removed from
                                the video file, separated by commas. If a
                                category is present in both mark and remove,
                                remove takes precedence. The syntax and
                                available categories are the same as for
                                --sponsorblock-mark except that "default"
                                refers to "all,-filler" and poi_highlight,
                                chapter are not available
--sponsorblock-chapter-title TEMPLATE
                                An output template for the title of the
                                SponsorBlock chapters created by
                                --sponsorblock-mark. The only available
                                fields are start_time, end_time, category,
                                categories, name, category_names. Defaults
                                to "[SponsorBlock]: %(category_names)l"
--no-sponsorblock               Disable both --sponsorblock-mark and
                                --sponsorblock-remove
--sponsorblock-api URL          SponsorBlock API location, defaults to
                                https://sponsor.ajay.app
</code></pre></div>
<h2 tabindex="-1" dir="auto">Extractor Options:</h2>
<div data-snippet-clipboard-copy-content="--extractor-retries RETRIES     Number of retries for known extractor errors
                                (default is 3), or &quot;infinite&quot;
--allow-dynamic-mpd             Process dynamic DASH manifests (default)
                                (Alias: --no-ignore-dynamic-mpd)
--ignore-dynamic-mpd            Do not process dynamic DASH manifests
                                (Alias: --no-allow-dynamic-mpd)
--hls-split-discontinuity       Split HLS playlists to different formats at
                                discontinuities such as ad breaks
--no-hls-split-discontinuity    Do not split HLS playlists to different
                                formats at discontinuities such as ad breaks
                                (default)
--extractor-args IE_KEY:ARGS    Pass ARGS arguments to the IE_KEY extractor.
                                See &quot;EXTRACTOR ARGUMENTS&quot; for details. You
                                can use this option multiple times to give
                                arguments for different extractors"><pre><code>--extractor-retries RETRIES     Number of retries for known extractor errors
                                (default is 3), or "infinite"
--allow-dynamic-mpd             Process dynamic DASH manifests (default)
                                (Alias: --no-ignore-dynamic-mpd)
--ignore-dynamic-mpd            Do not process dynamic DASH manifests
                                (Alias: --no-allow-dynamic-mpd)
--hls-split-discontinuity       Split HLS playlists to different formats at
                                discontinuities such as ad breaks
--no-hls-split-discontinuity    Do not split HLS playlists to different
                                formats at discontinuities such as ad breaks
                                (default)
--extractor-args IE_KEY:ARGS    Pass ARGS arguments to the IE_KEY extractor.
                                See "EXTRACTOR ARGUMENTS" for details. You
                                can use this option multiple times to give
                                arguments for different extractors
</code></pre></div>
<h2 tabindex="-1" dir="auto">CONFIGURATION</h2>
<p dir="auto">You can configure yt-dlp by placing any supported command line option to a configuration file. The configuration is loaded from the following locations:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Main Configuration</strong>:</p>
<ul dir="auto">
<li>The file given by <code>--config-location</code></li>
</ul>
</li>
<li>
<p dir="auto"><strong>Portable Configuration</strong>: (Recommended for portable installations)</p>
<ul dir="auto">
<li>If using a binary, <code>yt-dlp.conf</code> in the same directory as the binary</li>
<li>If running from source-code, <code>yt-dlp.conf</code> in the parent directory of <code>yt_dlp</code></li>
</ul>
</li>
<li>
<p dir="auto"><strong>Home Configuration</strong>:</p>
<ul dir="auto">
<li><code>yt-dlp.conf</code> in the home path given by <code>-P</code></li>
<li>If <code>-P</code> is not given, the current directory is searched</li>
</ul>
</li>
<li>
<p dir="auto"><strong>User Configuration</strong>:</p>
<ul dir="auto">
<li><code>${XDG_CONFIG_HOME}/yt-dlp.conf</code></li>
<li><code>${XDG_CONFIG_HOME}/yt-dlp/config</code> (recommended on Linux/macOS)</li>
<li><code>${XDG_CONFIG_HOME}/yt-dlp/config.txt</code></li>
<li><code>${APPDATA}/yt-dlp.conf</code></li>
<li><code>${APPDATA}/yt-dlp/config</code> (recommended on Windows)</li>
<li><code>${APPDATA}/yt-dlp/config.txt</code></li>
<li><code>~/yt-dlp.conf</code></li>
<li><code>~/yt-dlp.conf.txt</code></li>
<li><code>~/.yt-dlp/config</code></li>
<li><code>~/.yt-dlp/config.txt</code></li>
</ul>
<p dir="auto">See also: <a href="#notes-about-environment-variables">Notes about environment variables</a></p>
</li>
<li>
<p dir="auto"><strong>System Configuration</strong>:</p>
<ul dir="auto">
<li><code>/etc/yt-dlp.conf</code></li>
<li><code>/etc/yt-dlp/config</code></li>
<li><code>/etc/yt-dlp/config.txt</code></li>
</ul>
</li>
</ol>
<p dir="auto">E.g. with the following configuration file yt-dlp will always extract the audio, not copy the mtime, use a proxy and save all videos under <code>YouTube</code> directory in your home directory:</p>
<div data-snippet-clipboard-copy-content="# Lines starting with # are comments

# Always extract audio
-x

# Do not copy the mtime
--no-mtime

# Use this proxy
--proxy 127.0.0.1:3128

# Save all videos under YouTube directory in your home directory
-o ~/YouTube/%(title)s.%(ext)s"><pre><code># Lines starting with # are comments

# Always extract audio
-x

# Do not copy the mtime
--no-mtime

# Use this proxy
--proxy 127.0.0.1:3128

# Save all videos under YouTube directory in your home directory
-o ~/YouTube/%(title)s.%(ext)s
</code></pre></div>
<p dir="auto"><strong>Note</strong>: Options in configuration file are just the same options aka switches used in regular command line calls; thus there <strong>must be no whitespace</strong> after <code>-</code> or <code>--</code>, e.g. <code>-o</code> or <code>--proxy</code> but not <code>- o</code> or <code>-- proxy</code>. They must also be quoted when necessary as-if it were a UNIX shell.</p>
<p dir="auto">You can use <code>--ignore-config</code> if you want to disable all configuration files for a particular yt-dlp run. If <code>--ignore-config</code> is found inside any configuration file, no further configuration will be loaded. For example, having the option in the portable configuration file prevents loading of home, user, and system configurations. Additionally, (for backward compatibility) if <code>--ignore-config</code> is found inside the system configuration file, the user configuration is not loaded.</p>
<h3 tabindex="-1" dir="auto">Configuration file encoding</h3>
<p dir="auto">The configuration files are decoded according to the UTF BOM if present, and in the encoding from system locale otherwise.</p>
<p dir="auto">If you want your file to be decoded differently, add <code># coding: ENCODING</code> to the beginning of the file (e.g. <code># coding: shift-jis</code>). There must be no characters before that, even spaces or BOM.</p>
<h3 tabindex="-1" dir="auto">Authentication with netrc</h3>
<p dir="auto">You may also want to configure automatic credentials storage for extractors that support authentication (by providing login and password with <code>--username</code> and <code>--password</code>) in order not to pass credentials as command line arguments on every yt-dlp execution and prevent tracking plain text passwords in the shell command history. You can achieve this using a <a href="https://stackoverflow.com/tags/.netrc/info" rel="nofollow"><code>.netrc</code> file</a> on a per-extractor basis. For that you will need to create a <code>.netrc</code> file in <code>--netrc-location</code> and restrict permissions to read/write by only you:</p>
<div data-snippet-clipboard-copy-content="touch ${HOME}/.netrc
chmod a-rwx,u+rw ${HOME}/.netrc"><pre><code>touch ${HOME}/.netrc
chmod a-rwx,u+rw ${HOME}/.netrc
</code></pre></div>
<p dir="auto">After that you can add credentials for an extractor in the following format, where <em>extractor</em> is the name of the extractor in lowercase:</p>
<div data-snippet-clipboard-copy-content="machine <extractor> login <username> password <password>"><pre><code>machine &lt;extractor&gt; login &lt;username&gt; password &lt;password&gt;
</code></pre></div>
<p dir="auto">E.g.</p>
<div data-snippet-clipboard-copy-content="machine youtube login myaccount@gmail.com password my_youtube_password
machine twitch login my_twitch_account_name password my_twitch_password"><pre><code>machine youtube login myaccount@gmail.com password my_youtube_password
machine twitch login my_twitch_account_name password my_twitch_password
</code></pre></div>
<p dir="auto">To activate authentication with the <code>.netrc</code> file you should pass <code>--netrc</code> to yt-dlp or place it in the <a href="#configuration">configuration file</a>.</p>
<p dir="auto">The default location of the .netrc file is <code>~</code> (see below).</p>
<p dir="auto">As an alternative to using the <code>.netrc</code> file, which has the disadvantage of keeping your passwords in a plain text file, you can configure a custom shell command to provide the credentials for an extractor. This is done by providing the <code>--netrc-cmd</code> parameter, it shall output the credentials in the netrc format and return <code>0</code> on success, other values will be treated as an error. <code>{}</code> in the command will be replaced by the name of the extractor to make it possible to select the credentials for the right extractor.</p>
<p dir="auto">E.g. To use an encrypted <code>.netrc</code> file stored as <code>.authinfo.gpg</code></p>
<div data-snippet-clipboard-copy-content="yt-dlp --netrc-cmd 'gpg --decrypt ~/.authinfo.gpg' https://www.youtube.com/watch?v=BaW_jenozKc"><pre><code>yt-dlp --netrc-cmd 'gpg --decrypt ~/.authinfo.gpg' https://www.youtube.com/watch?v=BaW_jenozKc
</code></pre></div>
<h3 tabindex="-1" dir="auto">Notes about environment variables</h3>
<ul dir="auto">
<li>Environment variables are normally specified as <code>${VARIABLE}</code>/<code>$VARIABLE</code> on UNIX and <code>%VARIABLE%</code> on Windows; but is always shown as <code>${VARIABLE}</code> in this documentation</li>
<li>yt-dlp also allow using UNIX-style variables on Windows for path-like options; e.g. <code>--output</code>, <code>--config-location</code></li>
<li>If unset, <code>${XDG_CONFIG_HOME}</code> defaults to <code>~/.config</code> and <code>${XDG_CACHE_HOME}</code> to <code>~/.cache</code></li>
<li>On Windows, <code>~</code> points to <code>${HOME}</code> if present; or, <code>${USERPROFILE}</code> or <code>${HOMEDRIVE}${HOMEPATH}</code> otherwise</li>
<li>On Windows, <code>${USERPROFILE}</code> generally points to <code>C:\Users\&lt;user name&gt;</code> and <code>${APPDATA}</code> to <code>${USERPROFILE}\AppData\Roaming</code></li>
</ul>
<h2 tabindex="-1" dir="auto">OUTPUT TEMPLATE</h2>
<p dir="auto">The <code>-o</code> option is used to indicate a template for the output file names while <code>-P</code> option is used to specify the path each type of file should be saved to.</p>

<p dir="auto"><strong>tl;dr:</strong> <a href="#output-template-examples">navigate me to examples</a>.</p>

<p dir="auto">The simplest usage of <code>-o</code> is not to set any template arguments when downloading a single file, like in <code>yt-dlp -o funny_video.flv "https://some/video"</code> (hard-coding file extension like this is <em>not</em> recommended and could break some post-processing).</p>
<p dir="auto">It may however also contain special sequences that will be replaced when downloading each video. The special sequences may be formatted according to <a href="https://docs.python.org/3/library/stdtypes.html#printf-style-string-formatting" rel="nofollow">Python string formatting operations</a>, e.g. <code>%(NAME)s</code> or <code>%(NAME)05d</code>. To clarify, that is a percent symbol followed by a name in parentheses, followed by formatting operations.</p>
<p dir="auto">The field names themselves (the part inside the parenthesis) can also have some special formatting:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Object traversal</strong>: The dictionaries and lists available in metadata can be traversed by using a dot <code>.</code> separator; e.g. <code>%(tags.0)s</code>, <code>%(subtitles.en.-1.ext)s</code>. You can do Python slicing with colon <code>:</code>; E.g. <code>%(id.3:7:-1)s</code>, <code>%(formats.:.format_id)s</code>. Curly braces <code>{}</code> can be used to build dictionaries with only specific keys; e.g. <code>%(formats.:.{format_id,height})#j</code>. An empty field name <code>%()s</code> refers to the entire infodict; e.g. <code>%(.{id,title})s</code>. Note that all the fields that become available using this method are not listed below. Use <code>-j</code> to see such fields</p>
</li>
<li>
<p dir="auto"><strong>Addition</strong>: Addition and subtraction of numeric fields can be done using <code>+</code> and <code>-</code> respectively. E.g. <code>%(playlist_index+10)03d</code>, <code>%(n_entries+1-playlist_index)d</code></p>
</li>
<li>
<p dir="auto"><strong>Date/time Formatting</strong>: Date/time fields can be formatted according to <a href="https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes" rel="nofollow">strftime formatting</a> by specifying it separated from the field name using a <code>&gt;</code>. E.g. <code>%(duration&gt;%H-%M-%S)s</code>, <code>%(upload_date&gt;%Y-%m-%d)s</code>, <code>%(epoch-3600&gt;%H-%M-%S)s</code></p>
</li>
<li>
<p dir="auto"><strong>Alternatives</strong>: Alternate fields can be specified separated with a <code>,</code>. E.g. <code>%(release_date&gt;%Y,upload_date&gt;%Y|Unknown)s</code></p>
</li>
<li>
<p dir="auto"><strong>Replacement</strong>: A replacement value can be specified using a <code>&amp;</code> separator according to the <a href="https://docs.python.org/3/library/string.html#format-specification-mini-language" rel="nofollow"><code>str.format</code> mini-language</a>. If the field is <em>not</em> empty, this replacement value will be used instead of the actual field content. This is done after alternate fields are considered; thus the replacement is used if <em>any</em> of the alternative fields is <em>not</em> empty. E.g. <code>%(chapters&amp;has chapters|no chapters)s</code>, <code>%(title&amp;TITLE={:&gt;20}|NO TITLE)s</code></p>
</li>
<li>
<p dir="auto"><strong>Default</strong>: A literal default value can be specified for when the field is empty using a <code>|</code> separator. This overrides <code>--output-na-placeholder</code>. E.g. <code>%(uploader|Unknown)s</code></p>
</li>
<li>
<p dir="auto"><strong>More Conversions</strong>: In addition to the normal format types <code>diouxXeEfFgGcrs</code>, yt-dlp additionally supports converting to <code>B</code> = <strong>B</strong>ytes, <code>j</code> = <strong>j</strong>son (flag <code>#</code> for pretty-printing, <code>+</code> for Unicode), <code>h</code> = HTML escaping, <code>l</code> = a comma separated <strong>l</strong>ist (flag <code>#</code> for <code>\n</code> newline-separated), <code>q</code> = a string <strong>q</strong>uoted for the terminal (flag <code>#</code> to split a list into different arguments), <code>D</code> = add <strong>D</strong>ecimal suffixes (e.g. 10M) (flag <code>#</code> to use 1024 as factor), and <code>S</code> = <strong>S</strong>anitize as filename (flag <code>#</code> for restricted)</p>
</li>
<li>
<p dir="auto"><strong>Unicode normalization</strong>: The format type <code>U</code> can be used for NFC <a href="https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize" rel="nofollow">Unicode normalization</a>. The alternate form flag (<code>#</code>) changes the normalization to NFD and the conversion flag <code>+</code> can be used for NFKC/NFKD compatibility equivalence normalization. E.g. <code>%(title)+.100U</code> is NFKC</p>
</li>
</ol>
<p dir="auto">To summarize, the general syntax for a field is:</p>
<div data-snippet-clipboard-copy-content="%(name[.keys][addition][>strf][,alternate][&amp;replacement][|default])[flags][width][.precision][length]type"><pre><code>%(name[.keys][addition][&gt;strf][,alternate][&amp;replacement][|default])[flags][width][.precision][length]type
</code></pre></div>
<p dir="auto">Additionally, you can set different output templates for the various metadata files separately from the general output template by specifying the type of file followed by the template separated by a colon <code>:</code>. The different file types supported are <code>subtitle</code>, <code>thumbnail</code>, <code>description</code>, <code>annotation</code> (deprecated), <code>infojson</code>, <code>link</code>, <code>pl_thumbnail</code>, <code>pl_description</code>, <code>pl_infojson</code>, <code>chapter</code>, <code>pl_video</code>. E.g. <code>-o "%(title)s.%(ext)s" -o "thumbnail:%(title)s\%(title)s.%(ext)s"</code>  will put the thumbnails in a folder with the same name as the video. If any of the templates is empty, that type of file will not be written. E.g. <code>--write-thumbnail -o "thumbnail:"</code> will write thumbnails only for playlists and not for video.</p>
<a id="user-content-outtmpl-postprocess-note">
<p dir="auto"><strong>Note</strong>: Due to post-processing (i.e. merging etc.), the actual output filename might differ. Use <code>--print after_move:filepath</code> to get the name after all post-processing is complete.</p>
<p dir="auto">The available fields are:</p>
<ul dir="auto">
<li><code>id</code> (string): Video identifier</li>
<li><code>title</code> (string): Video title</li>
<li><code>fulltitle</code> (string): Video title ignoring live timestamp and generic title</li>
<li><code>ext</code> (string): Video filename extension</li>
<li><code>alt_title</code> (string): A secondary title of the video</li>
<li><code>description</code> (string): The description of the video</li>
<li><code>display_id</code> (string): An alternative identifier for the video</li>
<li><code>uploader</code> (string): Full name of the video uploader</li>
<li><code>license</code> (string): License name the video is licensed under</li>
<li><code>creator</code> (string): The creator of the video</li>
<li><code>timestamp</code> (numeric): UNIX timestamp of the moment the video became available</li>
<li><code>upload_date</code> (string): Video upload date in UTC (YYYYMMDD)</li>
<li><code>release_timestamp</code> (numeric): UNIX timestamp of the moment the video was released</li>
<li><code>release_date</code> (string): The date (YYYYMMDD) when the video was released in UTC</li>
<li><code>modified_timestamp</code> (numeric): UNIX timestamp of the moment the video was last modified</li>
<li><code>modified_date</code> (string): The date (YYYYMMDD) when the video was last modified in UTC</li>
<li><code>uploader_id</code> (string): Nickname or id of the video uploader</li>
<li><code>channel</code> (string): Full name of the channel the video is uploaded on</li>
<li><code>channel_id</code> (string): Id of the channel</li>
<li><code>channel_follower_count</code> (numeric): Number of followers of the channel</li>
<li><code>channel_is_verified</code> (boolean): Whether the channel is verified on the platform</li>
<li><code>location</code> (string): Physical location where the video was filmed</li>
<li><code>duration</code> (numeric): Length of the video in seconds</li>
<li><code>duration_string</code> (string): Length of the video (HH:mm:ss)</li>
<li><code>view_count</code> (numeric): How many users have watched the video on the platform</li>
<li><code>concurrent_view_count</code> (numeric): How many users are currently watching the video on the platform.</li>
<li><code>like_count</code> (numeric): Number of positive ratings of the video</li>
<li><code>dislike_count</code> (numeric): Number of negative ratings of the video</li>
<li><code>repost_count</code> (numeric): Number of reposts of the video</li>
<li><code>average_rating</code> (numeric): Average rating give by users, the scale used depends on the webpage</li>
<li><code>comment_count</code> (numeric): Number of comments on the video (For some extractors, comments are only downloaded at the end, and so this field cannot be used)</li>
<li><code>age_limit</code> (numeric): Age restriction for the video (years)</li>
<li><code>live_status</code> (string): One of "not_live", "is_live", "is_upcoming", "was_live", "post_live" (was live, but VOD is not yet processed)</li>
<li><code>is_live</code> (boolean): Whether this video is a live stream or a fixed-length video</li>
<li><code>was_live</code> (boolean): Whether this video was originally a live stream</li>
<li><code>playable_in_embed</code> (string): Whether this video is allowed to play in embedded players on other sites</li>
<li><code>availability</code> (string): Whether the video is "private", "premium_only", "subscriber_only", "needs_auth", "unlisted" or "public"</li>
<li><code>start_time</code> (numeric): Time in seconds where the reproduction should start, as specified in the URL</li>
<li><code>end_time</code> (numeric): Time in seconds where the reproduction should end, as specified in the URL</li>
<li><code>extractor</code> (string): Name of the extractor</li>
<li><code>extractor_key</code> (string): Key name of the extractor</li>
<li><code>epoch</code> (numeric): Unix epoch of when the information extraction was completed</li>
<li><code>autonumber</code> (numeric): Number that will be increased with each download, starting at <code>--autonumber-start</code>, padded with leading zeros to 5 digits</li>
<li><code>video_autonumber</code> (numeric): Number that will be increased with each video</li>
<li><code>n_entries</code> (numeric): Total number of extracted items in the playlist</li>
<li><code>playlist_id</code> (string): Identifier of the playlist that contains the video</li>
<li><code>playlist_title</code> (string): Name of the playlist that contains the video</li>
<li><code>playlist</code> (string): <code>playlist_id</code> or <code>playlist_title</code></li>
<li><code>playlist_count</code> (numeric): Total number of items in the playlist. May not be known if entire playlist is not extracted</li>
<li><code>playlist_index</code> (numeric): Index of the video in the playlist padded with leading zeros according the final index</li>
<li><code>playlist_autonumber</code> (numeric): Position of the video in the playlist download queue padded with leading zeros according to the total length of the playlist</li>
<li><code>playlist_uploader</code> (string): Full name of the playlist uploader</li>
<li><code>playlist_uploader_id</code> (string): Nickname or id of the playlist uploader</li>
<li><code>webpage_url</code> (string): A URL to the video webpage which if given to yt-dlp should allow to get the same result again</li>
<li><code>webpage_url_basename</code> (string): The basename of the webpage URL</li>
<li><code>webpage_url_domain</code> (string): The domain of the webpage URL</li>
<li><code>original_url</code> (string): The URL given by the user (or same as <code>webpage_url</code> for playlist entries)</li>
</ul>
</a><p dir="auto">All the fields in <a href="#filtering-formats">Filtering Formats</a> can also be used</p>
<p dir="auto">Available for the video that belongs to some logical chapter or section:</p>
<ul dir="auto">
<li><code>chapter</code> (string): Name or title of the chapter the video belongs to</li>
<li><code>chapter_number</code> (numeric): Number of the chapter the video belongs to</li>
<li><code>chapter_id</code> (string): Id of the chapter the video belongs to</li>
</ul>
<p dir="auto">Available for the video that is an episode of some series or programme:</p>
<ul dir="auto">
<li><code>series</code> (string): Title of the series or programme the video episode belongs to</li>
<li><code>season</code> (string): Title of the season the video episode belongs to</li>
<li><code>season_number</code> (numeric): Number of the season the video episode belongs to</li>
<li><code>season_id</code> (string): Id of the season the video episode belongs to</li>
<li><code>episode</code> (string): Title of the video episode</li>
<li><code>episode_number</code> (numeric): Number of the video episode within a season</li>
<li><code>episode_id</code> (string): Id of the video episode</li>
</ul>
<p dir="auto">Available for the media that is a track or a part of a music album:</p>
<ul dir="auto">
<li><code>track</code> (string): Title of the track</li>
<li><code>track_number</code> (numeric): Number of the track within an album or a disc</li>
<li><code>track_id</code> (string): Id of the track</li>
<li><code>artist</code> (string): Artist(s) of the track</li>
<li><code>genre</code> (string): Genre(s) of the track</li>
<li><code>album</code> (string): Title of the album the track belongs to</li>
<li><code>album_type</code> (string): Type of the album</li>
<li><code>album_artist</code> (string): List of all artists appeared on the album</li>
<li><code>disc_number</code> (numeric): Number of the disc or other physical medium the track belongs to</li>
<li><code>release_year</code> (numeric): Year (YYYY) when the album was released</li>
</ul>
<p dir="auto">Available only when using <code>--download-sections</code> and for <code>chapter:</code> prefix when using <code>--split-chapters</code> for videos with internal chapters:</p>
<ul dir="auto">
<li><code>section_title</code> (string): Title of the chapter</li>
<li><code>section_number</code> (numeric): Number of the chapter within the file</li>
<li><code>section_start</code> (numeric): Start time of the chapter in seconds</li>
<li><code>section_end</code> (numeric): End time of the chapter in seconds</li>
</ul>
<p dir="auto">Available only when used in <code>--print</code>:</p>
<ul dir="auto">
<li><code>urls</code> (string): The URLs of all requested formats, one in each line</li>
<li><code>filename</code> (string): Name of the video file. Note that the <a href="#outtmpl-postprocess-note">actual filename may differ</a></li>
<li><code>formats_table</code> (table): The video format table as printed by <code>--list-formats</code></li>
<li><code>thumbnails_table</code> (table): The thumbnail format table as printed by <code>--list-thumbnails</code></li>
<li><code>subtitles_table</code> (table): The subtitle format table as printed by <code>--list-subs</code></li>
<li><code>automatic_captions_table</code> (table): The automatic subtitle format table as printed by <code>--list-subs</code></li>
</ul>
<p dir="auto">Available only after the video is downloaded (<code>post_process</code>/<code>after_move</code>):</p>
<ul dir="auto">
<li><code>filepath</code>: Actual path of downloaded video file</li>
</ul>
<p dir="auto">Available only in <code>--sponsorblock-chapter-title</code>:</p>
<ul dir="auto">
<li><code>start_time</code> (numeric): Start time of the chapter in seconds</li>
<li><code>end_time</code> (numeric): End time of the chapter in seconds</li>
<li><code>categories</code> (list): The <a href="https://wiki.sponsor.ajay.app/w/Types#Category" rel="nofollow">SponsorBlock categories</a> the chapter belongs to</li>
<li><code>category</code> (string): The smallest SponsorBlock category the chapter belongs to</li>
<li><code>category_names</code> (list): Friendly names of the categories</li>
<li><code>name</code> (string): Friendly name of the smallest category</li>
<li><code>type</code> (string): The <a href="https://wiki.sponsor.ajay.app/w/Types#Action_Type" rel="nofollow">SponsorBlock action type</a> of the chapter</li>
</ul>
<p dir="auto">Each aforementioned sequence when referenced in an output template will be replaced by the actual value corresponding to the sequence name. E.g. for <code>-o %(title)s-%(id)s.%(ext)s</code> and an mp4 video with title <code>yt-dlp test video</code> and id <code>BaW_jenozKc</code>, this will result in a <code>yt-dlp test video-BaW_jenozKc.mp4</code> file created in the current directory.</p>
<p dir="auto"><strong>Note</strong>: Some of the sequences are not guaranteed to be present since they depend on the metadata obtained by a particular extractor. Such sequences will be replaced with placeholder value provided with <code>--output-na-placeholder</code> (<code>NA</code> by default).</p>
<p dir="auto"><strong>Tip</strong>: Look at the <code>-j</code> output to identify which fields are available for the particular URL</p>
<p dir="auto">For numeric sequences you can use <a href="https://docs.python.org/3/library/stdtypes.html#printf-style-string-formatting" rel="nofollow">numeric related formatting</a>; e.g. <code>%(view_count)05d</code> will result in a string with view count padded with zeros up to 5 characters, like in <code>00042</code>.</p>
<p dir="auto">Output templates can also contain arbitrary hierarchical path, e.g. <code>-o "%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s"</code> which will result in downloading each video in a directory corresponding to this path template. Any missing directory will be automatically created for you.</p>
<p dir="auto">To use percent literals in an output template use <code>%%</code>. To output to stdout use <code>-o -</code>.</p>
<p dir="auto">The current default template is <code>%(title)s [%(id)s].%(ext)s</code>.</p>
<p dir="auto">In some cases, you don't want special characters such as 中, spaces, or &amp;, such as when transferring the downloaded filename to a Windows system or the filename through an 8bit-unsafe channel. In these cases, add the <code>--restrict-filenames</code> flag to get a shorter title.</p>
<h4 tabindex="-1" dir="auto">Output template examples</h4>
<div dir="auto" data-snippet-clipboard-copy-content="$ yt-dlp --print filename -o &quot;test video.%(ext)s&quot; BaW_jenozKc
test video.webm    # Literal name with correct extension

$ yt-dlp --print filename -o &quot;%(title)s.%(ext)s&quot; BaW_jenozKc
youtube-dl test video ''_ä↭𝕐.webm    # All kinds of weird characters

$ yt-dlp --print filename -o &quot;%(title)s.%(ext)s&quot; BaW_jenozKc --restrict-filenames
youtube-dl_test_video_.webm    # Restricted file name

# Download YouTube playlist videos in separate directory indexed by video order in a playlist
$ yt-dlp -o &quot;%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s&quot; &quot;https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re&quot;

# Download YouTube playlist videos in separate directories according to their uploaded year
$ yt-dlp -o &quot;%(upload_date>%Y)s/%(title)s.%(ext)s&quot; &quot;https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re&quot;

# Prefix playlist index with &quot; - &quot; separator, but only if it is available
$ yt-dlp -o &quot;%(playlist_index&amp;{} - |)s%(title)s.%(ext)s&quot; BaW_jenozKc &quot;https://www.youtube.com/user/TheLinuxFoundation/playlists&quot;

# Download all playlists of YouTube channel/user keeping each playlist in separate directory:
$ yt-dlp -o &quot;%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s&quot; &quot;https://www.youtube.com/user/TheLinuxFoundation/playlists&quot;

# Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home
$ yt-dlp -u user -p password -P &quot;~/MyVideos&quot; -o &quot;%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s&quot; &quot;https://www.udemy.com/java-tutorial&quot;

# Download entire series season keeping each series and each season in separate directory under C:/MyVideos
$ yt-dlp -P &quot;C:/MyVideos&quot; -o &quot;%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s&quot; &quot;https://videomore.ru/kino_v_detalayah/5_sezon/367617&quot;

# Download video as &quot;C:\MyVideos\uploader\title.ext&quot;, subtitles as &quot;C:\MyVideos\subs\uploader\title.ext&quot;
# and put all temporary files in &quot;C:\MyVideos\tmp&quot;
$ yt-dlp -P &quot;C:/MyVideos&quot; -P &quot;temp:tmp&quot; -P &quot;subtitle:subs&quot; -o &quot;%(uploader)s/%(title)s.%(ext)s&quot; BaW_jenoz --write-subs

# Download video as &quot;C:\MyVideos\uploader\title.ext&quot; and subtitles as &quot;C:\MyVideos\uploader\subs\title.ext&quot;
$ yt-dlp -P &quot;C:/MyVideos&quot; -o &quot;%(uploader)s/%(title)s.%(ext)s&quot; -o &quot;subtitle:%(uploader)s/subs/%(title)s.%(ext)s&quot; BaW_jenozKc --write-subs

# Stream the video being downloaded to stdout
$ yt-dlp -o - BaW_jenozKc"><pre>$ yt-dlp --print filename -o <span><span>"</span>test video.%(ext)s<span>"</span></span> BaW_jenozKc
<span>test</span> video.webm    <span><span>#</span> Literal name with correct extension</span>

$ yt-dlp --print filename -o <span><span>"</span>%(title)s.%(ext)s<span>"</span></span> BaW_jenozKc
youtube-dl <span>test</span> video <span><span>'</span><span>'</span></span>_ä↭𝕐.webm    <span><span>#</span> All kinds of weird characters</span>

$ yt-dlp --print filename -o <span><span>"</span>%(title)s.%(ext)s<span>"</span></span> BaW_jenozKc --restrict-filenames
youtube-dl_test_video_.webm    <span><span>#</span> Restricted file name</span>

<span><span>#</span> Download YouTube playlist videos in separate directory indexed by video order in a playlist</span>
$ yt-dlp -o <span><span>"</span>%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s<span>"</span></span> <span><span>"</span>https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re<span>"</span></span>

<span><span>#</span> Download YouTube playlist videos in separate directories according to their uploaded year</span>
$ yt-dlp -o <span><span>"</span>%(upload_date&gt;%Y)s/%(title)s.%(ext)s<span>"</span></span> <span><span>"</span>https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re<span>"</span></span>

<span><span>#</span> Prefix playlist index with " - " separator, but only if it is available</span>
$ yt-dlp -o <span><span>"</span>%(playlist_index&amp;{} - |)s%(title)s.%(ext)s<span>"</span></span> BaW_jenozKc <span><span>"</span>https://www.youtube.com/user/TheLinuxFoundation/playlists<span>"</span></span>

<span><span>#</span> Download all playlists of YouTube channel/user keeping each playlist in separate directory:</span>
$ yt-dlp -o <span><span>"</span>%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s<span>"</span></span> <span><span>"</span>https://www.youtube.com/user/TheLinuxFoundation/playlists<span>"</span></span>

<span><span>#</span> Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home</span>
$ yt-dlp -u user -p password -P <span><span>"</span>~/MyVideos<span>"</span></span> -o <span><span>"</span>%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s<span>"</span></span> <span><span>"</span>https://www.udemy.com/java-tutorial<span>"</span></span>

<span><span>#</span> Download entire series season keeping each series and each season in separate directory under C:/MyVideos</span>
$ yt-dlp -P <span><span>"</span>C:/MyVideos<span>"</span></span> -o <span><span>"</span>%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s<span>"</span></span> <span><span>"</span>https://videomore.ru/kino_v_detalayah/5_sezon/367617<span>"</span></span>

<span><span>#</span> Download video as "C:\MyVideos\uploader\title.ext", subtitles as "C:\MyVideos\subs\uploader\title.ext"</span>
<span><span>#</span> and put all temporary files in "C:\MyVideos\tmp"</span>
$ yt-dlp -P <span><span>"</span>C:/MyVideos<span>"</span></span> -P <span><span>"</span>temp:tmp<span>"</span></span> -P <span><span>"</span>subtitle:subs<span>"</span></span> -o <span><span>"</span>%(uploader)s/%(title)s.%(ext)s<span>"</span></span> BaW_jenoz --write-subs

<span><span>#</span> Download video as "C:\MyVideos\uploader\title.ext" and subtitles as "C:\MyVideos\uploader\subs\title.ext"</span>
$ yt-dlp -P <span><span>"</span>C:/MyVideos<span>"</span></span> -o <span><span>"</span>%(uploader)s/%(title)s.%(ext)s<span>"</span></span> -o <span><span>"</span>subtitle:%(uploader)s/subs/%(title)s.%(ext)s<span>"</span></span> BaW_jenozKc --write-subs

<span><span>#</span> Stream the video being downloaded to stdout</span>
$ yt-dlp -o - BaW_jenozKc</pre></div>
<h2 tabindex="-1" dir="auto">FORMAT SELECTION</h2>
<p dir="auto">By default, yt-dlp tries to download the best available quality if you <strong>don't</strong> pass any options.
This is generally equivalent to using <code>-f bestvideo*+bestaudio/best</code>. However, if multiple audiostreams is enabled (<code>--audio-multistreams</code>), the default format changes to <code>-f bestvideo+bestaudio/best</code>. Similarly, if ffmpeg is unavailable, or if you use yt-dlp to stream to <code>stdout</code> (<code>-o -</code>), the default becomes <code>-f best/bestvideo+bestaudio</code>.</p>
<p dir="auto"><strong>Deprecation warning</strong>: Latest versions of yt-dlp can stream multiple formats to the stdout simultaneously using ffmpeg. So, in future versions, the default for this will be set to <code>-f bv*+ba/b</code> similar to normal downloads. If you want to preserve the <code>-f b/bv+ba</code> setting, it is recommended to explicitly specify it in the configuration options.</p>
<p dir="auto">The general syntax for format selection is <code>-f FORMAT</code> (or <code>--format FORMAT</code>) where <code>FORMAT</code> is a <em>selector expression</em>, i.e. an expression that describes format or formats you would like to download.</p>

<p dir="auto"><strong>tl;dr:</strong> <a href="#format-selection-examples">navigate me to examples</a>.</p>

<p dir="auto">The simplest case is requesting a specific format; e.g. with <code>-f 22</code> you can download the format with format code equal to 22. You can get the list of available format codes for particular video using <code>--list-formats</code> or <code>-F</code>. Note that these format codes are extractor specific.</p>
<p dir="auto">You can also use a file extension (currently <code>3gp</code>, <code>aac</code>, <code>flv</code>, <code>m4a</code>, <code>mp3</code>, <code>mp4</code>, <code>ogg</code>, <code>wav</code>, <code>webm</code> are supported) to download the best quality format of a particular file extension served as a single file, e.g. <code>-f webm</code> will download the best quality format with the <code>webm</code> extension served as a single file.</p>
<p dir="auto">You can use <code>-f -</code> to interactively provide the format selector <em>for each video</em></p>
<p dir="auto">You can also use special names to select particular edge case formats:</p>
<ul dir="auto">
<li><code>all</code>: Select <strong>all formats</strong> separately</li>
<li><code>mergeall</code>: Select and <strong>merge all formats</strong> (Must be used with <code>--audio-multistreams</code>, <code>--video-multistreams</code> or both)</li>
<li><code>b*</code>, <code>best*</code>: Select the best quality format that <strong>contains either</strong> a video or an audio or both (ie; <code>vcodec!=none or acodec!=none</code>)</li>
<li><code>b</code>, <code>best</code>: Select the best quality format that <strong>contains both</strong> video and audio. Equivalent to <code>best*[vcodec!=none][acodec!=none]</code></li>
<li><code>bv</code>, <code>bestvideo</code>: Select the best quality <strong>video-only</strong> format. Equivalent to <code>best*[acodec=none]</code></li>
<li><code>bv*</code>, <code>bestvideo*</code>: Select the best quality format that <strong>contains video</strong>. It may also contain audio. Equivalent to <code>best*[vcodec!=none]</code></li>
<li><code>ba</code>, <code>bestaudio</code>: Select the best quality <strong>audio-only</strong> format. Equivalent to <code>best*[vcodec=none]</code></li>
<li><code>ba*</code>, <code>bestaudio*</code>: Select the best quality format that <strong>contains audio</strong>. It may also contain video. Equivalent to <code>best*[acodec!=none]</code> (<a href="https://github.com/yt-dlp/yt-dlp/issues/979#issuecomment-919629354" data-hovercard-type="issue" data-hovercard-url="/yt-dlp/yt-dlp/issues/979/hovercard">Do not use!</a>)</li>
<li><code>w*</code>, <code>worst*</code>: Select the worst quality format that contains either a video or an audio</li>
<li><code>w</code>, <code>worst</code>: Select the worst quality format that contains both video and audio. Equivalent to <code>worst*[vcodec!=none][acodec!=none]</code></li>
<li><code>wv</code>, <code>worstvideo</code>: Select the worst quality video-only format. Equivalent to <code>worst*[acodec=none]</code></li>
<li><code>wv*</code>, <code>worstvideo*</code>: Select the worst quality format that contains video. It may also contain audio. Equivalent to <code>worst*[vcodec!=none]</code></li>
<li><code>wa</code>, <code>worstaudio</code>: Select the worst quality audio-only format. Equivalent to <code>worst*[vcodec=none]</code></li>
<li><code>wa*</code>, <code>worstaudio*</code>: Select the worst quality format that contains audio. It may also contain video. Equivalent to <code>worst*[acodec!=none]</code></li>
</ul>
<p dir="auto">For example, to download the worst quality video-only format you can use <code>-f worstvideo</code>. It is however recommended not to use <code>worst</code> and related options. When your format selector is <code>worst</code>, the format which is worst in all respects is selected. Most of the time, what you actually want is the video with the smallest filesize instead. So it is generally better to use <code>-S +size</code> or more rigorously, <code>-S +size,+br,+res,+fps</code> instead of <code>-f worst</code>. See <a href="#sorting-formats">Sorting Formats</a> for more details.</p>
<p dir="auto">You can select the n'th best format of a type by using <code>best&lt;type&gt;.&lt;n&gt;</code>. For example, <code>best.2</code> will select the 2nd best combined format. Similarly, <code>bv*.3</code> will select the 3rd best format that contains a video stream.</p>
<p dir="auto">If you want to download multiple videos, and they don't have the same formats available, you can specify the order of preference using slashes. Note that formats on the left hand side are preferred; e.g. <code>-f 22/17/18</code> will download format 22 if it's available, otherwise it will download format 17 if it's available, otherwise it will download format 18 if it's available, otherwise it will complain that no suitable formats are available for download.</p>
<p dir="auto">If you want to download several formats of the same video use a comma as a separator, e.g. <code>-f 22,17,18</code> will download all these three formats, of course if they are available. Or a more sophisticated example combined with the precedence feature: <code>-f 136/137/mp4/bestvideo,140/m4a/bestaudio</code>.</p>
<p dir="auto">You can merge the video and audio of multiple formats into a single file using <code>-f &lt;format1&gt;+&lt;format2&gt;+...</code> (requires ffmpeg installed); e.g. <code>-f bestvideo+bestaudio</code> will download the best video-only format, the best audio-only format and mux them together with ffmpeg.</p>
<p dir="auto"><strong>Deprecation warning</strong>: Since the <em>below</em> described behavior is complex and counter-intuitive, this will be removed and multistreams will be enabled by default in the future. A new operator will be instead added to limit formats to single audio/video</p>
<p dir="auto">Unless <code>--video-multistreams</code> is used, all formats with a video stream except the first one are ignored. Similarly, unless <code>--audio-multistreams</code> is used, all formats with an audio stream except the first one are ignored. E.g. <code>-f bestvideo+best+bestaudio --video-multistreams --audio-multistreams</code> will download and merge all 3 given formats. The resulting file will have 2 video streams and 2 audio streams. But <code>-f bestvideo+best+bestaudio --no-video-multistreams</code> will download and merge only <code>bestvideo</code> and <code>bestaudio</code>. <code>best</code> is ignored since another format containing a video stream (<code>bestvideo</code>) has already been selected. The order of the formats is therefore important. <code>-f best+bestaudio --no-audio-multistreams</code> will download only <code>best</code> while <code>-f bestaudio+best --no-audio-multistreams</code> will ignore <code>best</code> and download only <code>bestaudio</code>.</p>
<h2 tabindex="-1" dir="auto">Filtering Formats</h2>
<p dir="auto">You can also filter the video formats by putting a condition in brackets, as in <code>-f "best[height=720]"</code> (or <code>-f "[filesize&gt;10M]"</code> since filters without a selector are interpreted as <code>best</code>).</p>
<p dir="auto">The following numeric meta fields can be used with comparisons <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>=</code> (equals), <code>!=</code> (not equals):</p>
<ul dir="auto">
<li><code>filesize</code>: The number of bytes, if known in advance</li>
<li><code>filesize_approx</code>: An estimate for the number of bytes</li>
<li><code>width</code>: Width of the video, if known</li>
<li><code>height</code>: Height of the video, if known</li>
<li><code>aspect_ratio</code>: Aspect ratio of the video, if known</li>
<li><code>tbr</code>: Average bitrate of audio and video in KBit/s</li>
<li><code>abr</code>: Average audio bitrate in KBit/s</li>
<li><code>vbr</code>: Average video bitrate in KBit/s</li>
<li><code>asr</code>: Audio sampling rate in Hertz</li>
<li><code>fps</code>: Frame rate</li>
<li><code>audio_channels</code>: The number of audio channels</li>
<li><code>stretched_ratio</code>: <code>width:height</code> of the video's pixels, if not square</li>
</ul>
<p dir="auto">Also filtering work for comparisons <code>=</code> (equals), <code>^=</code> (starts with), <code>$=</code> (ends with), <code>*=</code> (contains), <code>~=</code> (matches regex) and following string meta fields:</p>
<ul dir="auto">
<li><code>url</code>: Video URL</li>
<li><code>ext</code>: File extension</li>
<li><code>acodec</code>: Name of the audio codec in use</li>
<li><code>vcodec</code>: Name of the video codec in use</li>
<li><code>container</code>: Name of the container format</li>
<li><code>protocol</code>: The protocol that will be used for the actual download, lower-case (<code>http</code>, <code>https</code>, <code>rtsp</code>, <code>rtmp</code>, <code>rtmpe</code>, <code>mms</code>, <code>f4m</code>, <code>ism</code>, <code>http_dash_segments</code>, <code>m3u8</code>, or <code>m3u8_native</code>)</li>
<li><code>language</code>: Language code</li>
<li><code>dynamic_range</code>: The dynamic range of the video</li>
<li><code>format_id</code>: A short description of the format</li>
<li><code>format</code>: A human-readable description of the format</li>
<li><code>format_note</code>: Additional info about the format</li>
<li><code>resolution</code>: Textual description of width and height</li>
</ul>
<p dir="auto">Any string comparison may be prefixed with negation <code>!</code> in order to produce an opposite comparison, e.g. <code>!*=</code> (does not contain). The comparand of a string comparison needs to be quoted with either double or single quotes if it contains spaces or special characters other than <code>._-</code>.</p>
<p dir="auto"><strong>Note</strong>: None of the aforementioned meta fields are guaranteed to be present since this solely depends on the metadata obtained by particular extractor, i.e. the metadata offered by the website. Any other field made available by the extractor can also be used for filtering.</p>
<p dir="auto">Formats for which the value is not known are excluded unless you put a question mark (<code>?</code>) after the operator. You can combine format filters, so <code>-f "bv[height&lt;=?720][tbr&gt;500]"</code> selects up to 720p videos (or videos where the height is not known) with a bitrate of at least 500 KBit/s. You can also use the filters with <code>all</code> to download all formats that satisfy the filter, e.g. <code>-f "all[vcodec=none]"</code> selects all audio-only formats.</p>
<p dir="auto">Format selectors can also be grouped using parentheses; e.g. <code>-f "(mp4,webm)[height&lt;480]"</code> will download the best pre-merged mp4 and webm formats with a height lower than 480.</p>
<h2 tabindex="-1" dir="auto">Sorting Formats</h2>
<p dir="auto">You can change the criteria for being considered the <code>best</code> by using <code>-S</code> (<code>--format-sort</code>). The general format for this is <code>--format-sort field1,field2...</code>.</p>
<p dir="auto">The available fields are:</p>
<ul dir="auto">
<li><code>hasvid</code>: Gives priority to formats that have a video stream</li>
<li><code>hasaud</code>: Gives priority to formats that have an audio stream</li>
<li><code>ie_pref</code>: The format preference</li>
<li><code>lang</code>: The language preference</li>
<li><code>quality</code>: The quality of the format</li>
<li><code>source</code>: The preference of the source</li>
<li><code>proto</code>: Protocol used for download (<code>https</code>/<code>ftps</code> &gt; <code>http</code>/<code>ftp</code> &gt; <code>m3u8_native</code>/<code>m3u8</code> &gt; <code>http_dash_segments</code>&gt; <code>websocket_frag</code> &gt; <code>mms</code>/<code>rtsp</code> &gt; <code>f4f</code>/<code>f4m</code>)</li>
<li><code>vcodec</code>: Video Codec (<code>av01</code> &gt; <code>vp9.2</code> &gt; <code>vp9</code> &gt; <code>h265</code> &gt; <code>h264</code> &gt; <code>vp8</code> &gt; <code>h263</code> &gt; <code>theora</code> &gt; other)</li>
<li><code>acodec</code>: Audio Codec (<code>flac</code>/<code>alac</code> &gt; <code>wav</code>/<code>aiff</code> &gt; <code>opus</code> &gt; <code>vorbis</code> &gt; <code>aac</code> &gt; <code>mp4a</code> &gt; <code>mp3</code> &gt; <code>ac4</code> &gt; <code>eac3</code> &gt; <code>ac3</code> &gt; <code>dts</code> &gt; other)</li>
<li><code>codec</code>: Equivalent to <code>vcodec,acodec</code></li>
<li><code>vext</code>: Video Extension (<code>mp4</code> &gt; <code>mov</code> &gt; <code>webm</code> &gt; <code>flv</code> &gt; other). If <code>--prefer-free-formats</code> is used, <code>webm</code> is preferred.</li>
<li><code>aext</code>: Audio Extension (<code>m4a</code> &gt; <code>aac</code> &gt; <code>mp3</code> &gt; <code>ogg</code> &gt; <code>opus</code> &gt; <code>webm</code> &gt; other). If <code>--prefer-free-formats</code> is used, the order changes to <code>ogg</code> &gt; <code>opus</code> &gt; <code>webm</code> &gt; <code>mp3</code> &gt; <code>m4a</code> &gt; <code>aac</code></li>
<li><code>ext</code>: Equivalent to <code>vext,aext</code></li>
<li><code>filesize</code>: Exact filesize, if known in advance</li>
<li><code>fs_approx</code>: Approximate filesize</li>
<li><code>size</code>: Exact filesize if available, otherwise approximate filesize</li>
<li><code>height</code>: Height of video</li>
<li><code>width</code>: Width of video</li>
<li><code>res</code>: Video resolution, calculated as the smallest dimension.</li>
<li><code>fps</code>: Framerate of video</li>
<li><code>hdr</code>: The dynamic range of the video (<code>DV</code> &gt; <code>HDR12</code> &gt; <code>HDR10+</code> &gt; <code>HDR10</code> &gt; <code>HLG</code> &gt; <code>SDR</code>)</li>
<li><code>channels</code>: The number of audio channels</li>
<li><code>tbr</code>: Total average bitrate in KBit/s</li>
<li><code>vbr</code>: Average video bitrate in KBit/s</li>
<li><code>abr</code>: Average audio bitrate in KBit/s</li>
<li><code>br</code>: Average bitrate in KBit/s, <code>tbr</code>/<code>vbr</code>/<code>abr</code></li>
<li><code>asr</code>: Audio sample rate in Hz</li>
</ul>
<p dir="auto"><strong>Deprecation warning</strong>: Many of these fields have (currently undocumented) aliases, that may be removed in a future version. It is recommended to use only the documented field names.</p>
<p dir="auto">All fields, unless specified otherwise, are sorted in descending order. To reverse this, prefix the field with a <code>+</code>. E.g. <code>+res</code> prefers format with the smallest resolution. Additionally, you can suffix a preferred value for the fields, separated by a <code>:</code>. E.g. <code>res:720</code> prefers larger videos, but no larger than 720p and the smallest video if there are no videos less than 720p. For <code>codec</code> and <code>ext</code>, you can provide two preferred values, the first for video and the second for audio. E.g. <code>+codec:avc:m4a</code> (equivalent to <code>+vcodec:avc,+acodec:m4a</code>) sets the video codec preference to <code>h264</code> &gt; <code>h265</code> &gt; <code>vp9</code> &gt; <code>vp9.2</code> &gt; <code>av01</code> &gt; <code>vp8</code> &gt; <code>h263</code> &gt; <code>theora</code> and audio codec preference to <code>mp4a</code> &gt; <code>aac</code> &gt; <code>vorbis</code> &gt; <code>opus</code> &gt; <code>mp3</code> &gt; <code>ac3</code> &gt; <code>dts</code>. You can also make the sorting prefer the nearest values to the provided by using <code>~</code> as the delimiter. E.g. <code>filesize~1G</code> prefers the format with filesize closest to 1 GiB.</p>
<p dir="auto">The fields <code>hasvid</code> and <code>ie_pref</code> are always given highest priority in sorting, irrespective of the user-defined order. This behaviour can be changed by using <code>--format-sort-force</code>. Apart from these, the default order used is: <code>lang,quality,res,fps,hdr:12,vcodec:vp9.2,channels,acodec,size,br,asr,proto,ext,hasaud,source,id</code>. The extractors may override this default order, but they cannot override the user-provided order.</p>
<p dir="auto">Note that the default has <code>vcodec:vp9.2</code>; i.e. <code>av1</code> is not preferred. Similarly, the default for hdr is <code>hdr:12</code>; i.e. dolby vision is not preferred. These choices are made since DV and AV1 formats are not yet fully compatible with most devices. This may be changed in the future as more devices become capable of smoothly playing back these formats.</p>
<p dir="auto">If your format selector is <code>worst</code>, the last item is selected after sorting. This means it will select the format that is worst in all respects. Most of the time, what you actually want is the video with the smallest filesize instead. So it is generally better to use <code>-f best -S +size,+br,+res,+fps</code>.</p>
<p dir="auto"><strong>Tip</strong>: You can use the <code>-v -F</code> to see how the formats have been sorted (worst to best).</p>
<h2 tabindex="-1" dir="auto">Format Selection examples</h2>
<div dir="auto" data-snippet-clipboard-copy-content="# Download and merge the best video-only format and the best audio-only format,
# or download the best combined format if video-only format is not available
$ yt-dlp -f &quot;bv+ba/b&quot;

# Download best format that contains video,
# and if it doesn't already have an audio stream, merge it with best audio-only format
$ yt-dlp -f &quot;bv*+ba/b&quot;

# Same as above
$ yt-dlp

# Download the best video-only format and the best audio-only format without merging them
# For this case, an output template should be used since
# by default, bestvideo and bestaudio will have the same file name.
$ yt-dlp -f &quot;bv,ba&quot; -o &quot;%(title)s.f%(format_id)s.%(ext)s&quot;

# Download and merge the best format that has a video stream,
# and all audio-only formats into one file
$ yt-dlp -f &quot;bv*+mergeall[vcodec=none]&quot; --audio-multistreams

# Download and merge the best format that has a video stream,
# and the best 2 audio-only formats into one file
$ yt-dlp -f &quot;bv*+ba+ba.2&quot; --audio-multistreams


# The following examples show the old method (without -S) of format selection
# and how to use -S to achieve a similar but (generally) better result

# Download the worst video available (old method)
$ yt-dlp -f &quot;wv*+wa/w&quot;

# Download the best video available but with the smallest resolution
$ yt-dlp -S &quot;+res&quot;

# Download the smallest video available
$ yt-dlp -S &quot;+size,+br&quot;



# Download the best mp4 video available, or the best video if no mp4 available
$ yt-dlp -f &quot;bv*[ext=mp4]+ba[ext=m4a]/b[ext=mp4] / bv*+ba/b&quot;

# Download the best video with the best extension
# (For video, mp4 > mov > webm > flv. For audio, m4a > aac > mp3 ...)
$ yt-dlp -S &quot;ext&quot;



# Download the best video available but no better than 480p,
# or the worst video if there is no video under 480p
$ yt-dlp -f &quot;bv*[height<=480]+ba/b[height<=480] / wv*+ba/w&quot;

# Download the best video available with the largest height but no better than 480p,
# or the best video with the smallest resolution if there is no video under 480p
$ yt-dlp -S &quot;height:480&quot;

# Download the best video available with the largest resolution but no better than 480p,
# or the best video with the smallest resolution if there is no video under 480p
# Resolution is determined by using the smallest dimension.
# So this works correctly for vertical videos as well
$ yt-dlp -S &quot;res:480&quot;



# Download the best video (that also has audio) but no bigger than 50 MB,
# or the worst video (that also has audio) if there is no video under 50 MB
$ yt-dlp -f &quot;b[filesize<50M] / w&quot;

# Download largest video (that also has audio) but no bigger than 50 MB,
# or the smallest video (that also has audio) if there is no video under 50 MB
$ yt-dlp -f &quot;b&quot; -S &quot;filesize:50M&quot;

# Download best video (that also has audio) that is closest in size to 50 MB
$ yt-dlp -f &quot;b&quot; -S &quot;filesize~50M&quot;



# Download best video available via direct link over HTTP/HTTPS protocol,
# or the best video available via any protocol if there is no such video
$ yt-dlp -f &quot;(bv*+ba/b)[protocol^=http][protocol!*=dash] / (bv*+ba/b)&quot;

# Download best video available via the best protocol
# (https/ftps > http/ftp > m3u8_native > m3u8 > http_dash_segments ...)
$ yt-dlp -S &quot;proto&quot;



# Download the best video with either h264 or h265 codec,
# or the best video if there is no such video
$ yt-dlp -f &quot;(bv*[vcodec~='^((he|a)vc|h26[45])']+ba) / (bv*+ba/b)&quot;

# Download the best video with best codec no better than h264,
# or the best video with worst codec if there is no such video
$ yt-dlp -S &quot;codec:h264&quot;

# Download the best video with worst codec no worse than h264,
# or the best video with best codec if there is no such video
$ yt-dlp -S &quot;+codec:h264&quot;



# More complex examples

# Download the best video no better than 720p preferring framerate greater than 30,
# or the worst video (still preferring framerate greater than 30) if there is no such video
$ yt-dlp -f &quot;((bv*[fps>30]/bv*)[height<=720]/(wv*[fps>30]/wv*)) + ba / (b[fps>30]/b)[height<=720]/(w[fps>30]/w)&quot;

# Download the video with the largest resolution no better than 720p,
# or the video with the smallest resolution available if there is no such video,
# preferring larger framerate for formats with the same resolution
$ yt-dlp -S &quot;res:720,fps&quot;



# Download the video with smallest resolution no worse than 480p,
# or the video with the largest resolution available if there is no such video,
# preferring better codec and then larger total bitrate for the same resolution
$ yt-dlp -S &quot;+res:480,codec,br&quot;"><pre><span><span>#</span> Download and merge the best video-only format and the best audio-only format,</span>
<span><span>#</span> or download the best combined format if video-only format is not available</span>
$ yt-dlp -f <span><span>"</span>bv+ba/b<span>"</span></span>

<span><span>#</span> Download best format that contains video,</span>
<span><span>#</span> and if it doesn't already have an audio stream, merge it with best audio-only format</span>
$ yt-dlp -f <span><span>"</span>bv*+ba/b<span>"</span></span>

<span><span>#</span> Same as above</span>
$ yt-dlp

<span><span>#</span> Download the best video-only format and the best audio-only format without merging them</span>
<span><span>#</span> For this case, an output template should be used since</span>
<span><span>#</span> by default, bestvideo and bestaudio will have the same file name.</span>
$ yt-dlp -f <span><span>"</span>bv,ba<span>"</span></span> -o <span><span>"</span>%(title)s.f%(format_id)s.%(ext)s<span>"</span></span>

<span><span>#</span> Download and merge the best format that has a video stream,</span>
<span><span>#</span> and all audio-only formats into one file</span>
$ yt-dlp -f <span><span>"</span>bv*+mergeall[vcodec=none]<span>"</span></span> --audio-multistreams

<span><span>#</span> Download and merge the best format that has a video stream,</span>
<span><span>#</span> and the best 2 audio-only formats into one file</span>
$ yt-dlp -f <span><span>"</span>bv*+ba+ba.2<span>"</span></span> --audio-multistreams


<span><span>#</span> The following examples show the old method (without -S) of format selection</span>
<span><span>#</span> and how to use -S to achieve a similar but (generally) better result</span>

<span><span>#</span> Download the worst video available (old method)</span>
$ yt-dlp -f <span><span>"</span>wv*+wa/w<span>"</span></span>

<span><span>#</span> Download the best video available but with the smallest resolution</span>
$ yt-dlp -S <span><span>"</span>+res<span>"</span></span>

<span><span>#</span> Download the smallest video available</span>
$ yt-dlp -S <span><span>"</span>+size,+br<span>"</span></span>



<span><span>#</span> Download the best mp4 video available, or the best video if no mp4 available</span>
$ yt-dlp -f <span><span>"</span>bv*[ext=mp4]+ba[ext=m4a]/b[ext=mp4] / bv*+ba/b<span>"</span></span>

<span><span>#</span> Download the best video with the best extension</span>
<span><span>#</span> (For video, mp4 &gt; mov &gt; webm &gt; flv. For audio, m4a &gt; aac &gt; mp3 ...)</span>
$ yt-dlp -S <span><span>"</span>ext<span>"</span></span>



<span><span>#</span> Download the best video available but no better than 480p,</span>
<span><span>#</span> or the worst video if there is no video under 480p</span>
$ yt-dlp -f <span><span>"</span>bv*[height&lt;=480]+ba/b[height&lt;=480] / wv*+ba/w<span>"</span></span>

<span><span>#</span> Download the best video available with the largest height but no better than 480p,</span>
<span><span>#</span> or the best video with the smallest resolution if there is no video under 480p</span>
$ yt-dlp -S <span><span>"</span>height:480<span>"</span></span>

<span><span>#</span> Download the best video available with the largest resolution but no better than 480p,</span>
<span><span>#</span> or the best video with the smallest resolution if there is no video under 480p</span>
<span><span>#</span> Resolution is determined by using the smallest dimension.</span>
<span><span>#</span> So this works correctly for vertical videos as well</span>
$ yt-dlp -S <span><span>"</span>res:480<span>"</span></span>



<span><span>#</span> Download the best video (that also has audio) but no bigger than 50 MB,</span>
<span><span>#</span> or the worst video (that also has audio) if there is no video under 50 MB</span>
$ yt-dlp -f <span><span>"</span>b[filesize&lt;50M] / w<span>"</span></span>

<span><span>#</span> Download largest video (that also has audio) but no bigger than 50 MB,</span>
<span><span>#</span> or the smallest video (that also has audio) if there is no video under 50 MB</span>
$ yt-dlp -f <span><span>"</span>b<span>"</span></span> -S <span><span>"</span>filesize:50M<span>"</span></span>

<span><span>#</span> Download best video (that also has audio) that is closest in size to 50 MB</span>
$ yt-dlp -f <span><span>"</span>b<span>"</span></span> -S <span><span>"</span>filesize~50M<span>"</span></span>



<span><span>#</span> Download best video available via direct link over HTTP/HTTPS protocol,</span>
<span><span>#</span> or the best video available via any protocol if there is no such video</span>
$ yt-dlp -f <span><span>"</span>(bv*+ba/b)[protocol^=http][protocol!*=dash] / (bv*+ba/b)<span>"</span></span>

<span><span>#</span> Download best video available via the best protocol</span>
<span><span>#</span> (https/ftps &gt; http/ftp &gt; m3u8_native &gt; m3u8 &gt; http_dash_segments ...)</span>
$ yt-dlp -S <span><span>"</span>proto<span>"</span></span>



<span><span>#</span> Download the best video with either h264 or h265 codec,</span>
<span><span>#</span> or the best video if there is no such video</span>
$ yt-dlp -f <span><span>"</span>(bv*[vcodec~='^((he|a)vc|h26[45])']+ba) / (bv*+ba/b)<span>"</span></span>

<span><span>#</span> Download the best video with best codec no better than h264,</span>
<span><span>#</span> or the best video with worst codec if there is no such video</span>
$ yt-dlp -S <span><span>"</span>codec:h264<span>"</span></span>

<span><span>#</span> Download the best video with worst codec no worse than h264,</span>
<span><span>#</span> or the best video with best codec if there is no such video</span>
$ yt-dlp -S <span><span>"</span>+codec:h264<span>"</span></span>



<span><span>#</span> More complex examples</span>

<span><span>#</span> Download the best video no better than 720p preferring framerate greater than 30,</span>
<span><span>#</span> or the worst video (still preferring framerate greater than 30) if there is no such video</span>
$ yt-dlp -f <span><span>"</span>((bv*[fps&gt;30]/bv*)[height&lt;=720]/(wv*[fps&gt;30]/wv*)) + ba / (b[fps&gt;30]/b)[height&lt;=720]/(w[fps&gt;30]/w)<span>"</span></span>

<span><span>#</span> Download the video with the largest resolution no better than 720p,</span>
<span><span>#</span> or the video with the smallest resolution available if there is no such video,</span>
<span><span>#</span> preferring larger framerate for formats with the same resolution</span>
$ yt-dlp -S <span><span>"</span>res:720,fps<span>"</span></span>



<span><span>#</span> Download the video with smallest resolution no worse than 480p,</span>
<span><span>#</span> or the video with the largest resolution available if there is no such video,</span>
<span><span>#</span> preferring better codec and then larger total bitrate for the same resolution</span>
$ yt-dlp -S <span><span>"</span>+res:480,codec,br<span>"</span></span></pre></div>
<h2 tabindex="-1" dir="auto">MODIFYING METADATA</h2>
<p dir="auto">The metadata obtained by the extractors can be modified by using <code>--parse-metadata</code> and <code>--replace-in-metadata</code></p>
<p dir="auto"><code>--replace-in-metadata FIELDS REGEX REPLACE</code> is used to replace text in any metadata field using <a href="https://docs.python.org/3/library/re.html#regular-expression-syntax" rel="nofollow">python regular expression</a>. <a href="https://docs.python.org/3/library/re.html?highlight=backreferences#re.sub" rel="nofollow">Backreferences</a> can be used in the replace string for advanced use.</p>
<p dir="auto">The general syntax of <code>--parse-metadata FROM:TO</code> is to give the name of a field or an <a href="#output-template">output template</a> to extract data from, and the format to interpret it as, separated by a colon <code>:</code>. Either a <a href="https://docs.python.org/3/library/re.html#regular-expression-syntax" rel="nofollow">python regular expression</a> with named capture groups, a single field name, or a similar syntax to the <a href="#output-template">output template</a> (only <code>%(field)s</code> formatting is supported) can be used for <code>TO</code>. The option can be used multiple times to parse and modify various fields.</p>
<p dir="auto">Note that these options preserve their relative order, allowing replacements to be made in parsed fields and viceversa. Also, any field thus created can be used in the <a href="#output-template">output template</a> and will also affect the media file's metadata added when using <code>--embed-metadata</code>.</p>
<p dir="auto">This option also has a few special uses:</p>
<ul dir="auto">
<li>
<p dir="auto">You can download an additional URL based on the metadata of the currently downloaded video. To do this, set the field <code>additional_urls</code> to the URL that you want to download. E.g. <code>--parse-metadata "description:(?P&lt;additional_urls&gt;https?://www\.vimeo\.com/\d+)"</code> will download the first vimeo video found in the description</p>
</li>
<li>
<p dir="auto">You can use this to change the metadata that is embedded in the media file. To do this, set the value of the corresponding field with a <code>meta_</code> prefix. For example, any value you set to <code>meta_description</code> field will be added to the <code>description</code> field in the file - you can use this to set a different "description" and "synopsis". To modify the metadata of individual streams, use the <code>meta&lt;n&gt;_</code> prefix (e.g. <code>meta1_language</code>). Any value set to the <code>meta_</code> field will overwrite all default values.</p>
</li>
</ul>
<p dir="auto"><strong>Note</strong>: Metadata modification happens before format selection, post-extraction and other post-processing operations. Some fields may be added or changed during these steps, overriding your changes.</p>
<p dir="auto">For reference, these are the fields yt-dlp adds by default to the file metadata:</p>
<table>
<thead>
<tr>
<th>Metadata fields</th>
<th>From</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>title</code></td>
<td><code>track</code> or <code>title</code></td>
</tr>
<tr>
<td><code>date</code></td>
<td><code>upload_date</code></td>
</tr>
<tr>
<td><code>description</code>,  <code>synopsis</code></td>
<td><code>description</code></td>
</tr>
<tr>
<td><code>purl</code>, <code>comment</code></td>
<td><code>webpage_url</code></td>
</tr>
<tr>
<td><code>track</code></td>
<td><code>track_number</code></td>
</tr>
<tr>
<td><code>artist</code></td>
<td><code>artist</code>, <code>creator</code>, <code>uploader</code> or <code>uploader_id</code></td>
</tr>
<tr>
<td><code>genre</code></td>
<td><code>genre</code></td>
</tr>
<tr>
<td><code>album</code></td>
<td><code>album</code></td>
</tr>
<tr>
<td><code>album_artist</code></td>
<td><code>album_artist</code></td>
</tr>
<tr>
<td><code>disc</code></td>
<td><code>disc_number</code></td>
</tr>
<tr>
<td><code>show</code></td>
<td><code>series</code></td>
</tr>
<tr>
<td><code>season_number</code></td>
<td><code>season_number</code></td>
</tr>
<tr>
<td><code>episode_id</code></td>
<td><code>episode</code> or <code>episode_id</code></td>
</tr>
<tr>
<td><code>episode_sort</code></td>
<td><code>episode_number</code></td>
</tr>
<tr>
<td><code>language</code> of each stream</td>
<td>the format's <code>language</code></td>
</tr>
</tbody>
</table>
<p dir="auto"><strong>Note</strong>: The file format may not support some of these fields</p>
<h2 tabindex="-1" dir="auto">Modifying metadata examples</h2>
<div dir="auto" data-snippet-clipboard-copy-content="# Interpret the title as &quot;Artist - Title&quot;
$ yt-dlp --parse-metadata &quot;title:%(artist)s - %(title)s&quot;

# Regex example
$ yt-dlp --parse-metadata &quot;description:Artist - (?P<artist>.+)&quot;

# Set title as &quot;Series name S01E05&quot;
$ yt-dlp --parse-metadata &quot;%(series)s S%(season_number)02dE%(episode_number)02d:%(title)s&quot;

# Prioritize uploader as the &quot;artist&quot; field in video metadata
$ yt-dlp --parse-metadata &quot;%(uploader|)s:%(meta_artist)s&quot; --embed-metadata

# Set &quot;comment&quot; field in video metadata using description instead of webpage_url,
# handling multiple lines correctly
$ yt-dlp --parse-metadata &quot;description:(?s)(?P<meta_comment>.+)&quot; --embed-metadata

# Do not set any &quot;synopsis&quot; in the video metadata
$ yt-dlp --parse-metadata &quot;:(?P<meta_synopsis>)&quot;

# Remove &quot;formats&quot; field from the infojson by setting it to an empty string
$ yt-dlp --parse-metadata &quot;video::(?P<formats>)&quot; --write-info-json

# Replace all spaces and &quot;_&quot; in title and uploader with a `-`
$ yt-dlp --replace-in-metadata &quot;title,uploader&quot; &quot;[ _]&quot; &quot;-&quot;
"><pre><span><span>#</span> Interpret the title as "Artist - Title"</span>
$ yt-dlp --parse-metadata <span><span>"</span>title:%(artist)s - %(title)s<span>"</span></span>

<span><span>#</span> Regex example</span>
$ yt-dlp --parse-metadata <span><span>"</span>description:Artist - (?P&lt;artist&gt;.+)<span>"</span></span>

<span><span>#</span> Set title as "Series name S01E05"</span>
$ yt-dlp --parse-metadata <span><span>"</span>%(series)s S%(season_number)02dE%(episode_number)02d:%(title)s<span>"</span></span>

<span><span>#</span> Prioritize uploader as the "artist" field in video metadata</span>
$ yt-dlp --parse-metadata <span><span>"</span>%(uploader|)s:%(meta_artist)s<span>"</span></span> --embed-metadata

<span><span>#</span> Set "comment" field in video metadata using description instead of webpage_url,</span>
<span><span>#</span> handling multiple lines correctly</span>
$ yt-dlp --parse-metadata <span><span>"</span>description:(?s)(?P&lt;meta_comment&gt;.+)<span>"</span></span> --embed-metadata

<span><span>#</span> Do not set any "synopsis" in the video metadata</span>
$ yt-dlp --parse-metadata <span><span>"</span>:(?P&lt;meta_synopsis&gt;)<span>"</span></span>

<span><span>#</span> Remove "formats" field from the infojson by setting it to an empty string</span>
$ yt-dlp --parse-metadata <span><span>"</span>video::(?P&lt;formats&gt;)<span>"</span></span> --write-info-json

<span><span>#</span> Replace all spaces and "_" in title and uploader with a `-`</span>
$ yt-dlp --replace-in-metadata <span><span>"</span>title,uploader<span>"</span></span> <span><span>"</span>[ _]<span>"</span></span> <span><span>"</span>-<span>"</span></span>
</pre></div>
<h2 tabindex="-1" dir="auto">EXTRACTOR ARGUMENTS</h2>
<p dir="auto">Some extractors accept additional arguments which can be passed using <code>--extractor-args KEY:ARGS</code>. <code>ARGS</code> is a <code>;</code> (semicolon) separated string of <code>ARG=VAL1,VAL2</code>. E.g. <code>--extractor-args "youtube:player-client=android_embedded,web;include_live_dash" --extractor-args "funimation:version=uncut"</code></p>
<p dir="auto">Note: In CLI, <code>ARG</code> can use <code>-</code> instead of <code>_</code>; e.g. <code>youtube:player-client"</code> becomes <code>youtube:player_client"</code></p>
<p dir="auto">The following extractors use this feature:</p>
<h4 tabindex="-1" dir="auto">youtube</h4>
<ul dir="auto">
<li><code>lang</code>: Prefer translated metadata (<code>title</code>, <code>description</code> etc) of this language code (case-sensitive). By default, the video primary language metadata is preferred, with a fallback to <code>en</code> translated. See <a href="https://github.com/yt-dlp/yt-dlp/blob/c26f9b991a0681fd3ea548d535919cec1fbbd430/yt_dlp/extractor/youtube.py#L381-L390">youtube.py</a> for list of supported content language codes</li>
<li><code>skip</code>: One or more of <code>hls</code>, <code>dash</code> or <code>translated_subs</code> to skip extraction of the m3u8 manifests, dash manifests and <a href="https://github.com/yt-dlp/yt-dlp/issues/4090#issuecomment-1158102032" data-hovercard-type="issue" data-hovercard-url="/yt-dlp/yt-dlp/issues/4090/hovercard">auto-translated subtitles</a> respectively</li>
<li><code>player_client</code>: Clients to extract video data from. The main clients are <code>web</code>, <code>android</code> and <code>ios</code> with variants <code>_music</code>, <code>_embedded</code>, <code>_embedscreen</code>, <code>_creator</code> (e.g. <code>web_embedded</code>); and <code>mweb</code> and <code>tv_embedded</code> (agegate bypass) with no variants. By default, <code>ios,android,web</code> is used, but <code>tv_embedded</code> and <code>creator</code> variants are added as required for age-gated videos. Similarly, the music variants are added for <code>music.youtube.com</code> urls. You can use <code>all</code> to use all the clients, and <code>default</code> for the default clients.</li>
<li><code>player_skip</code>: Skip some network requests that are generally needed for robust extraction. One or more of <code>configs</code> (skip client configs), <code>webpage</code> (skip initial webpage), <code>js</code> (skip js player). While these options can help reduce the number of requests needed or avoid some rate-limiting, they could cause some issues. See <a href="https://github.com/yt-dlp/yt-dlp/pull/860" data-hovercard-type="pull_request" data-hovercard-url="/yt-dlp/yt-dlp/pull/860/hovercard">#860</a> for more details</li>
<li><code>player_params</code>: YouTube player parameters to use for player requests. Will overwrite any default ones set by yt-dlp.</li>
<li><code>comment_sort</code>: <code>top</code> or <code>new</code> (default) - choose comment sorting mode (on YouTube's side)</li>
<li><code>max_comments</code>: Limit the amount of comments to gather. Comma-separated list of integers representing <code>max-comments,max-parents,max-replies,max-replies-per-thread</code>. Default is <code>all,all,all,all</code>
<ul dir="auto">
<li>E.g. <code>all,all,1000,10</code> will get a maximum of 1000 replies total, with up to 10 replies per thread. <code>1000,all,100</code> will get a maximum of 1000 comments, with a maximum of 100 replies total</li>
</ul>
</li>
<li><code>formats</code>: Change the types of formats to return. <code>dashy</code> (convert HTTP to DASH), <code>duplicate</code> (identical content but different URLs or protocol; includes <code>dashy</code>), <code>incomplete</code> (cannot be downloaded completely - live dash and post-live m3u8)</li>
<li><code>innertube_host</code>: Innertube API host to use for all API requests; e.g. <code>studio.youtube.com</code>, <code>youtubei.googleapis.com</code>. Note that cookies exported from one subdomain will not work on others</li>
<li><code>innertube_key</code>: Innertube API key to use for all API requests</li>
</ul>
<h4 tabindex="-1" dir="auto">youtubetab (YouTube playlists, channels, feeds, etc.)</h4>
<ul dir="auto">
<li><code>skip</code>: One or more of <code>webpage</code> (skip initial webpage download), <code>authcheck</code> (allow the download of playlists requiring authentication when no initial webpage is downloaded. This may cause unwanted behavior, see <a href="https://github.com/yt-dlp/yt-dlp/pull/1122" data-hovercard-type="pull_request" data-hovercard-url="/yt-dlp/yt-dlp/pull/1122/hovercard">#1122</a> for more details)</li>
<li><code>approximate_date</code>: Extract approximate <code>upload_date</code> and <code>timestamp</code> in flat-playlist. This may cause date-based filters to be slightly off</li>
</ul>
<h4 tabindex="-1" dir="auto">generic</h4>
<ul dir="auto">
<li><code>fragment_query</code>: Passthrough any query in mpd/m3u8 manifest URLs to their fragments if no value is provided, or else apply the query string given as <code>fragment_query=VALUE</code>. Does not apply to ffmpeg</li>
<li><code>variant_query</code>: Passthrough the master m3u8 URL query to its variant playlist URLs if no value is provided, or else apply the query string given as <code>variant_query=VALUE</code></li>
<li><code>hls_key</code>: An HLS AES-128 key URI <em>or</em> key (as hex), and optionally the IV (as hex), in the form of <code>(URI|KEY)[,IV]</code>; e.g. <code>generic:hls_key=ABCDEF1234567980,0xFEDCBA0987654321</code>. Passing any of these values will force usage of the native HLS downloader and override the corresponding values found in the m3u8 playlist</li>
<li><code>is_live</code>: Bypass live HLS detection and manually set <code>live_status</code> - a value of <code>false</code> will set <code>not_live</code>, any other value (or no value) will set <code>is_live</code></li>
</ul>
<h4 tabindex="-1" dir="auto">funimation</h4>
<ul dir="auto">
<li><code>language</code>: Audio languages to extract, e.g. <code>funimation:language=english,japanese</code></li>
<li><code>version</code>: The video version to extract - <code>uncut</code> or <code>simulcast</code></li>
</ul>
<h4 tabindex="-1" dir="auto">crunchyrollbeta (Crunchyroll)</h4>
<ul dir="auto">
<li><code>format</code>: Which stream type(s) to extract (default: <code>adaptive_hls</code>). Potentially useful values include <code>adaptive_hls</code>, <code>adaptive_dash</code>, <code>vo_adaptive_hls</code>, <code>vo_adaptive_dash</code>, <code>download_hls</code>, <code>download_dash</code>, <code>multitrack_adaptive_hls_v2</code></li>
<li><code>hardsub</code>: Preference order for which hardsub versions to extract, or <code>all</code> (default: <code>None</code> = no hardsubs), e.g. <code>crunchyrollbeta:hardsub=en-US,None</code></li>
</ul>
<h4 tabindex="-1" dir="auto">vikichannel</h4>
<ul dir="auto">
<li><code>video_types</code>: Types of videos to download - one or more of <code>episodes</code>, <code>movies</code>, <code>clips</code>, <code>trailers</code></li>
</ul>
<h4 tabindex="-1" dir="auto">niconico</h4>
<ul dir="auto">
<li><code>segment_duration</code>: Segment duration in milliseconds for HLS-DMC formats. Use it at your own risk since this feature <strong>may result in your account termination.</strong></li>
</ul>
<h4 tabindex="-1" dir="auto">youtubewebarchive</h4>
<ul dir="auto">
<li><code>check_all</code>: Try to check more at the cost of more requests. One or more of <code>thumbnails</code>, <code>captures</code></li>
</ul>
<h4 tabindex="-1" dir="auto">gamejolt</h4>
<ul dir="auto">
<li><code>comment_sort</code>: <code>hot</code> (default), <code>you</code> (cookies needed), <code>top</code>, <code>new</code> - choose comment sorting mode (on GameJolt's side)</li>
</ul>
<h4 tabindex="-1" dir="auto">hotstar</h4>
<ul dir="auto">
<li><code>res</code>: resolution to ignore - one or more of <code>sd</code>, <code>hd</code>, <code>fhd</code></li>
<li><code>vcodec</code>: vcodec to ignore - one or more of <code>h264</code>, <code>h265</code>, <code>dvh265</code></li>
<li><code>dr</code>: dynamic range to ignore - one or more of <code>sdr</code>, <code>hdr10</code>, <code>dv</code></li>
</ul>
<h4 tabindex="-1" dir="auto">tiktok</h4>
<ul dir="auto">
<li><code>api_hostname</code>: Hostname to use for mobile API requests, e.g. <code>api-h2.tiktokv.com</code></li>
<li><code>app_version</code>: App version to call mobile APIs with - should be set along with <code>manifest_app_version</code>, e.g. <code>20.2.1</code></li>
<li><code>manifest_app_version</code>: Numeric app version to call mobile APIs with, e.g. <code>221</code></li>
</ul>
<h4 tabindex="-1" dir="auto">rokfinchannel</h4>
<ul dir="auto">
<li><code>tab</code>: Which tab to download - one of <code>new</code>, <code>top</code>, <code>videos</code>, <code>podcasts</code>, <code>streams</code>, <code>stacks</code></li>
</ul>
<h4 tabindex="-1" dir="auto">twitter</h4>
<ul dir="auto">
<li><code>api</code>: Select one of <code>graphql</code> (default), <code>legacy</code> or <code>syndication</code> as the API for tweet extraction. Has no effect if logged in</li>
</ul>
<h4 tabindex="-1" dir="auto">stacommu, wrestleuniverse</h4>
<ul dir="auto">
<li><code>device_id</code>: UUID value assigned by the website and used to enforce device limits for paid livestream content. Can be found in browser local storage</li>
</ul>
<h4 tabindex="-1" dir="auto">twitch</h4>
<ul dir="auto">
<li><code>client_id</code>: Client ID value to be sent with GraphQL requests, e.g. <code>twitch:client_id=kimne78kx3ncx6brgo4mv6wki5h1ko</code></li>
</ul>
<h4 tabindex="-1" dir="auto">nhkradirulive (NHK らじる★らじる LIVE)</h4>
<ul dir="auto">
<li><code>area</code>: Which regional variation to extract. Valid areas are: <code>sapporo</code>, <code>sendai</code>, <code>tokyo</code>, <code>nagoya</code>, <code>osaka</code>, <code>hiroshima</code>, <code>matsuyama</code>, <code>fukuoka</code>. Defaults to <code>tokyo</code></li>
</ul>
<p dir="auto"><strong>Note</strong>: These options may be changed/removed in the future without concern for backward compatibility</p>

<h2 tabindex="-1" dir="auto">PLUGINS</h2>
<p dir="auto">Note that <strong>all</strong> plugins are imported even if not invoked, and that <strong>there are no checks</strong> performed on plugin code. <strong>Use plugins at your own risk and only if you trust the code!</strong></p>
<p dir="auto">Plugins can be of <code>&lt;type&gt;</code>s <code>extractor</code> or <code>postprocessor</code>.</p>
<ul dir="auto">
<li>Extractor plugins do not need to be enabled from the CLI and are automatically invoked when the input URL is suitable for it.</li>
<li>Extractor plugins take priority over builtin extractors.</li>
<li>Postprocessor plugins can be invoked using <code>--use-postprocessor NAME</code>.</li>
</ul>
<p dir="auto">Plugins are loaded from the namespace packages <code>yt_dlp_plugins.extractor</code> and <code>yt_dlp_plugins.postprocessor</code>.</p>
<p dir="auto">In other words, the file structure on the disk looks something like:</p>
<div data-snippet-clipboard-copy-content="    yt_dlp_plugins/
        extractor/
            myplugin.py
        postprocessor/
            myplugin.py"><pre><code>    yt_dlp_plugins/
        extractor/
            myplugin.py
        postprocessor/
            myplugin.py
</code></pre></div>
<p dir="auto">yt-dlp looks for these <code>yt_dlp_plugins</code> namespace folders in many locations (see below) and loads in plugins from <strong>all</strong> of them.</p>
<p dir="auto">See the <a href="https://github.com/yt-dlp/yt-dlp/wiki/Plugins">wiki for some known plugins</a></p>
<h2 tabindex="-1" dir="auto">Installing Plugins</h2>
<p dir="auto">Plugins can be installed using various methods and locations.</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Configuration directories</strong>:
Plugin packages (containing a <code>yt_dlp_plugins</code> namespace folder) can be dropped into the following standard <a href="#configuration">configuration locations</a>:</p>
<ul dir="auto">
<li><strong>User Plugins</strong>
<ul dir="auto">
<li><code>${XDG_CONFIG_HOME}/yt-dlp/plugins/&lt;package name&gt;/yt_dlp_plugins/</code> (recommended on Linux/macOS)</li>
<li><code>${XDG_CONFIG_HOME}/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li>
<li><code>${APPDATA}/yt-dlp/plugins/&lt;package name&gt;/yt_dlp_plugins/</code> (recommended on Windows)</li>
<li><code>${APPDATA}/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li>
<li><code>~/.yt-dlp/plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li>
<li><code>~/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li>
</ul>
</li>
<li><strong>System Plugins</strong>
<ul dir="auto">
<li><code>/etc/yt-dlp/plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li>
<li><code>/etc/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Executable location</strong>: Plugin packages can similarly be installed in a <code>yt-dlp-plugins</code> directory under the executable location (recommended for portable installations):</p>
<ul dir="auto">
<li>Binary: where <code>&lt;root-dir&gt;/yt-dlp.exe</code>, <code>&lt;root-dir&gt;/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li>
<li>Source: where <code>&lt;root-dir&gt;/yt_dlp/__main__.py</code>, <code>&lt;root-dir&gt;/yt-dlp-plugins/&lt;package name&gt;/yt_dlp_plugins/</code></li>
</ul>
</li>
<li>
<p dir="auto"><strong>pip and other locations in <code>PYTHONPATH</code></strong></p>
<ul dir="auto">
<li>Plugin packages can be installed and managed using <code>pip</code>. See <a href="https://github.com/yt-dlp/yt-dlp-sample-plugins">yt-dlp-sample-plugins</a> for an example.
<ul dir="auto">
<li>Note: plugin files between plugin packages installed with pip must have unique filenames.</li>
</ul>
</li>
<li>Any path in <code>PYTHONPATH</code> is searched in for the <code>yt_dlp_plugins</code> namespace folder.
<ul dir="auto">
<li>Note: This does not apply for Pyinstaller/py2exe builds.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p dir="auto"><code>.zip</code>, <code>.egg</code> and <code>.whl</code> archives containing a <code>yt_dlp_plugins</code> namespace folder in their root are also supported as plugin packages.</p>
<ul dir="auto">
<li>e.g. <code>${XDG_CONFIG_HOME}/yt-dlp/plugins/mypluginpkg.zip</code> where <code>mypluginpkg.zip</code> contains <code>yt_dlp_plugins/&lt;type&gt;/myplugin.py</code></li>
</ul>
<p dir="auto">Run yt-dlp with <code>--verbose</code> to check if the plugin has been loaded.</p>
<h2 tabindex="-1" dir="auto">Developing Plugins</h2>
<p dir="auto">See the <a href="https://github.com/yt-dlp/yt-dlp-sample-plugins">yt-dlp-sample-plugins</a> repo for a template plugin package and the <a href="https://github.com/yt-dlp/yt-dlp/wiki/Plugin-Development">Plugin Development</a> section of the wiki for a plugin development guide.</p>
<p dir="auto">All public classes with a name ending in <code>IE</code>/<code>PP</code> are imported from each file for extractors and postprocessors repectively. This respects underscore prefix (e.g. <code>_MyBasePluginIE</code> is private) and <code>__all__</code>. Modules can similarly be excluded by prefixing the module name with an underscore (e.g. <code>_myplugin.py</code>).</p>
<p dir="auto">To replace an existing extractor with a subclass of one, set the <code>plugin_name</code> class keyword argument (e.g. <code>class MyPluginIE(ABuiltInIE, plugin_name='myplugin')</code> will replace <code>ABuiltInIE</code> with <code>MyPluginIE</code>). Since the extractor replaces the parent, you should exclude the subclass extractor from being imported separately by making it private using one of the methods described above.</p>
<p dir="auto">If you are a plugin author, add <a href="https://github.com/topics/yt-dlp-plugins">yt-dlp-plugins</a> as a topic to your repository for discoverability.</p>
<p dir="auto">See the <a href="https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions">Developer Instructions</a> on how to write and test an extractor.</p>
<h2 tabindex="-1" dir="auto">EMBEDDING YT-DLP</h2>
<p dir="auto">yt-dlp makes the best effort to be a good command-line program, and thus should be callable from any programming language.</p>
<p dir="auto">Your program should avoid parsing the normal stdout since they may change in future versions. Instead they should use options such as <code>-J</code>, <code>--print</code>, <code>--progress-template</code>, <code>--exec</code> etc to create console output that you can reliably reproduce and parse.</p>
<p dir="auto">From a Python program, you can embed yt-dlp in a more powerful fashion, like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from yt_dlp import YoutubeDL

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']
with YoutubeDL() as ydl:
    ydl.download(URLS)"><pre><span>from</span> <span>yt_dlp</span> <span>import</span> <span>YoutubeDL</span>

<span>URLS</span> <span>=</span> [<span>'https://www.youtube.com/watch?v=BaW_jenozKc'</span>]
<span>with</span> <span>YoutubeDL</span>() <span>as</span> <span>ydl</span>:
    <span>ydl</span>.<span>download</span>(<span>URLS</span>)</pre></div>
<p dir="auto">Most likely, you'll want to use various options. For a list of options available, have a look at <a href="https://github.com/yt-dlp/yt-dlp/blob/master/yt_dlp/YoutubeDL.py#L183"><code>yt_dlp/YoutubeDL.py</code></a> or <code>help(yt_dlp.YoutubeDL)</code> in a Python shell. If you are already familiar with the CLI, you can use <a href="https://github.com/yt-dlp/yt-dlp/blob/master/devscripts/cli_to_api.py"><code>devscripts/cli_to_api.py</code></a> to translate any CLI switches to <code>YoutubeDL</code> params.</p>
<p dir="auto"><strong>Tip</strong>: If you are porting your code from youtube-dl to yt-dlp, one important point to look out for is that we do not guarantee the return value of <code>YoutubeDL.extract_info</code> to be json serializable, or even be a dictionary. It will be dictionary-like, but if you want to ensure it is a serializable dictionary, pass it through <code>YoutubeDL.sanitize_info</code> as shown in the <a href="#extracting-information">example below</a></p>
<h2 tabindex="-1" dir="auto">Embedding examples</h2>
<h4 tabindex="-1" dir="auto">Extracting information</h4>
<div dir="auto" data-snippet-clipboard-copy-content="import json
import yt_dlp

URL = 'https://www.youtube.com/watch?v=BaW_jenozKc'

# ℹ️ See help(yt_dlp.YoutubeDL) for a list of available options and public functions
ydl_opts = {}
with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    info = ydl.extract_info(URL, download=False)

    # ℹ️ ydl.sanitize_info makes the info json-serializable
    print(json.dumps(ydl.sanitize_info(info)))"><pre><span>import</span> <span>json</span>
<span>import</span> <span>yt_dlp</span>

<span>URL</span> <span>=</span> <span>'https://www.youtube.com/watch?v=BaW_jenozKc'</span>

<span># ℹ️ See help(yt_dlp.YoutubeDL) for a list of available options and public functions</span>
<span>ydl_opts</span> <span>=</span> {}
<span>with</span> <span>yt_dlp</span>.<span>YoutubeDL</span>(<span>ydl_opts</span>) <span>as</span> <span>ydl</span>:
    <span>info</span> <span>=</span> <span>ydl</span>.<span>extract_info</span>(<span>URL</span>, <span>download</span><span>=</span><span>False</span>)

    <span># ℹ️ ydl.sanitize_info makes the info json-serializable</span>
    <span>print</span>(<span>json</span>.<span>dumps</span>(<span>ydl</span>.<span>sanitize_info</span>(<span>info</span>)))</pre></div>
<h4 tabindex="-1" dir="auto">Download using an info-json</h4>
<div dir="auto" data-snippet-clipboard-copy-content="import yt_dlp

INFO_FILE = 'path/to/video.info.json'

with yt_dlp.YoutubeDL() as ydl:
    error_code = ydl.download_with_info_file(INFO_FILE)

print('Some videos failed to download' if error_code
      else 'All videos successfully downloaded')"><pre><span>import</span> <span>yt_dlp</span>

<span>INFO_FILE</span> <span>=</span> <span>'path/to/video.info.json'</span>

<span>with</span> <span>yt_dlp</span>.<span>YoutubeDL</span>() <span>as</span> <span>ydl</span>:
    <span>error_code</span> <span>=</span> <span>ydl</span>.<span>download_with_info_file</span>(<span>INFO_FILE</span>)

<span>print</span>(<span>'Some videos failed to download'</span> <span>if</span> <span>error_code</span>
      <span>else</span> <span>'All videos successfully downloaded'</span>)</pre></div>
<h4 tabindex="-1" dir="auto">Extract audio</h4>
<div dir="auto" data-snippet-clipboard-copy-content="import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

ydl_opts = {
    'format': 'm4a/bestaudio/best',
    # ℹ️ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments
    'postprocessors': [{  # Extract audio using ffmpeg
        'key': 'FFmpegExtractAudio',
        'preferredcodec': 'm4a',
    }]
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    error_code = ydl.download(URLS)"><pre><span>import</span> <span>yt_dlp</span>

<span>URLS</span> <span>=</span> [<span>'https://www.youtube.com/watch?v=BaW_jenozKc'</span>]

<span>ydl_opts</span> <span>=</span> {
    <span>'format'</span>: <span>'m4a/bestaudio/best'</span>,
    <span># ℹ️ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments</span>
    <span>'postprocessors'</span>: [{  <span># Extract audio using ffmpeg</span>
        <span>'key'</span>: <span>'FFmpegExtractAudio'</span>,
        <span>'preferredcodec'</span>: <span>'m4a'</span>,
    }]
}

<span>with</span> <span>yt_dlp</span>.<span>YoutubeDL</span>(<span>ydl_opts</span>) <span>as</span> <span>ydl</span>:
    <span>error_code</span> <span>=</span> <span>ydl</span>.<span>download</span>(<span>URLS</span>)</pre></div>
<h4 tabindex="-1" dir="auto">Filter videos</h4>
<div dir="auto" data-snippet-clipboard-copy-content="import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

def longer_than_a_minute(info, *, incomplete):
    &quot;&quot;&quot;Download only videos longer than a minute (or with unknown duration)&quot;&quot;&quot;
    duration = info.get('duration')
    if duration and duration < 60:
        return 'The video is too short'

ydl_opts = {
    'match_filter': longer_than_a_minute,
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    error_code = ydl.download(URLS)"><pre><span>import</span> <span>yt_dlp</span>

<span>URLS</span> <span>=</span> [<span>'https://www.youtube.com/watch?v=BaW_jenozKc'</span>]

<span>def</span> <span>longer_than_a_minute</span>(<span>info</span>, <span>*</span>, <span>incomplete</span>):
    <span>"""Download only videos longer than a minute (or with unknown duration)"""</span>
    <span>duration</span> <span>=</span> <span>info</span>.<span>get</span>(<span>'duration'</span>)
    <span>if</span> <span>duration</span> <span>and</span> <span>duration</span> <span>&lt;</span> <span>60</span>:
        <span>return</span> <span>'The video is too short'</span>

<span>ydl_opts</span> <span>=</span> {
    <span>'match_filter'</span>: <span>longer_than_a_minute</span>,
}

<span>with</span> <span>yt_dlp</span>.<span>YoutubeDL</span>(<span>ydl_opts</span>) <span>as</span> <span>ydl</span>:
    <span>error_code</span> <span>=</span> <span>ydl</span>.<span>download</span>(<span>URLS</span>)</pre></div>
<h4 tabindex="-1" dir="auto">Adding logger and progress hook</h4>
<div dir="auto" data-snippet-clipboard-copy-content="import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

class MyLogger:
    def debug(self, msg):
        # For compatibility with youtube-dl, both debug and info are passed into debug
        # You can distinguish them by the prefix '[debug] '
        if msg.startswith('[debug] '):
            pass
        else:
            self.info(msg)

    def info(self, msg):
        pass

    def warning(self, msg):
        pass

    def error(self, msg):
        print(msg)


# ℹ️ See &quot;progress_hooks&quot; in help(yt_dlp.YoutubeDL)
def my_hook(d):
    if d['status'] == 'finished':
        print('Done downloading, now post-processing ...')


ydl_opts = {
    'logger': MyLogger(),
    'progress_hooks': [my_hook],
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    ydl.download(URLS)"><pre><span>import</span> <span>yt_dlp</span>

<span>URLS</span> <span>=</span> [<span>'https://www.youtube.com/watch?v=BaW_jenozKc'</span>]

<span>class</span> <span>MyLogger</span>:
    <span>def</span> <span>debug</span>(<span>self</span>, <span>msg</span>):
        <span># For compatibility with youtube-dl, both debug and info are passed into debug</span>
        <span># You can distinguish them by the prefix '[debug] '</span>
        <span>if</span> <span>msg</span>.<span>startswith</span>(<span>'[debug] '</span>):
            <span>pass</span>
        <span>else</span>:
            <span>self</span>.<span>info</span>(<span>msg</span>)

    <span>def</span> <span>info</span>(<span>self</span>, <span>msg</span>):
        <span>pass</span>

    <span>def</span> <span>warning</span>(<span>self</span>, <span>msg</span>):
        <span>pass</span>

    <span>def</span> <span>error</span>(<span>self</span>, <span>msg</span>):
        <span>print</span>(<span>msg</span>)


<span># ℹ️ See "progress_hooks" in help(yt_dlp.YoutubeDL)</span>
<span>def</span> <span>my_hook</span>(<span>d</span>):
    <span>if</span> <span>d</span>[<span>'status'</span>] <span>==</span> <span>'finished'</span>:
        <span>print</span>(<span>'Done downloading, now post-processing ...'</span>)


<span>ydl_opts</span> <span>=</span> {
    <span>'logger'</span>: <span>MyLogger</span>(),
    <span>'progress_hooks'</span>: [<span>my_hook</span>],
}

<span>with</span> <span>yt_dlp</span>.<span>YoutubeDL</span>(<span>ydl_opts</span>) <span>as</span> <span>ydl</span>:
    <span>ydl</span>.<span>download</span>(<span>URLS</span>)</pre></div>
<h4 tabindex="-1" dir="auto">Add a custom PostProcessor</h4>
<div dir="auto" data-snippet-clipboard-copy-content="import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

# ℹ️ See help(yt_dlp.postprocessor.PostProcessor)
class MyCustomPP(yt_dlp.postprocessor.PostProcessor):
    def run(self, info):
        self.to_screen('Doing stuff')
        return [], info


with yt_dlp.YoutubeDL() as ydl:
    # ℹ️ &quot;when&quot; can take any value in yt_dlp.utils.POSTPROCESS_WHEN
    ydl.add_post_processor(MyCustomPP(), when='pre_process')
    ydl.download(URLS)"><pre><span>import</span> <span>yt_dlp</span>

<span>URLS</span> <span>=</span> [<span>'https://www.youtube.com/watch?v=BaW_jenozKc'</span>]

<span># ℹ️ See help(yt_dlp.postprocessor.PostProcessor)</span>
<span>class</span> <span>MyCustomPP</span>(<span>yt_dlp</span>.<span>postprocessor</span>.<span>PostProcessor</span>):
    <span>def</span> <span>run</span>(<span>self</span>, <span>info</span>):
        <span>self</span>.<span>to_screen</span>(<span>'Doing stuff'</span>)
        <span>return</span> [], <span>info</span>


<span>with</span> <span>yt_dlp</span>.<span>YoutubeDL</span>() <span>as</span> <span>ydl</span>:
    <span># ℹ️ "when" can take any value in yt_dlp.utils.POSTPROCESS_WHEN</span>
    <span>ydl</span>.<span>add_post_processor</span>(<span>MyCustomPP</span>(), <span>when</span><span>=</span><span>'pre_process'</span>)
    <span>ydl</span>.<span>download</span>(<span>URLS</span>)</pre></div>
<h4 tabindex="-1" dir="auto">Use a custom format selector</h4>
<div dir="auto" data-snippet-clipboard-copy-content="import yt_dlp

URLS = ['https://www.youtube.com/watch?v=BaW_jenozKc']

def format_selector(ctx):
    &quot;&quot;&quot; Select the best video and the best audio that won't result in an mkv.
    NOTE: This is just an example and does not handle all cases &quot;&quot;&quot;

    # formats are already sorted worst to best
    formats = ctx.get('formats')[::-1]

    # acodec='none' means there is no audio
    best_video = next(f for f in formats
                      if f['vcodec'] != 'none' and f['acodec'] == 'none')

    # find compatible audio extension
    audio_ext = {'mp4': 'm4a', 'webm': 'webm'}[best_video['ext']]
    # vcodec='none' means there is no video
    best_audio = next(f for f in formats if (
        f['acodec'] != 'none' and f['vcodec'] == 'none' and f['ext'] == audio_ext))

    # These are the minimum required fields for a merged format
    yield {
        'format_id': f'{best_video[&quot;format_id&quot;]}+{best_audio[&quot;format_id&quot;]}',
        'ext': best_video['ext'],
        'requested_formats': [best_video, best_audio],
        # Must be + separated list of protocols
        'protocol': f'{best_video[&quot;protocol&quot;]}+{best_audio[&quot;protocol&quot;]}'
    }


ydl_opts = {
    'format': format_selector,
}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
    ydl.download(URLS)"><pre><span>import</span> <span>yt_dlp</span>

<span>URLS</span> <span>=</span> [<span>'https://www.youtube.com/watch?v=BaW_jenozKc'</span>]

<span>def</span> <span>format_selector</span>(<span>ctx</span>):
    <span>""" Select the best video and the best audio that won't result in an mkv.</span>
<span>    NOTE: This is just an example and does not handle all cases """</span>

    <span># formats are already sorted worst to best</span>
    <span>formats</span> <span>=</span> <span>ctx</span>.<span>get</span>(<span>'formats'</span>)[::<span>-</span><span>1</span>]

    <span># acodec='none' means there is no audio</span>
    <span>best_video</span> <span>=</span> <span>next</span>(<span>f</span> <span>for</span> <span>f</span> <span>in</span> <span>formats</span>
                      <span>if</span> <span>f</span>[<span>'vcodec'</span>] <span>!=</span> <span>'none'</span> <span>and</span> <span>f</span>[<span>'acodec'</span>] <span>==</span> <span>'none'</span>)

    <span># find compatible audio extension</span>
    <span>audio_ext</span> <span>=</span> {<span>'mp4'</span>: <span>'m4a'</span>, <span>'webm'</span>: <span>'webm'</span>}[<span>best_video</span>[<span>'ext'</span>]]
    <span># vcodec='none' means there is no video</span>
    <span>best_audio</span> <span>=</span> <span>next</span>(<span>f</span> <span>for</span> <span>f</span> <span>in</span> <span>formats</span> <span>if</span> (
        <span>f</span>[<span>'acodec'</span>] <span>!=</span> <span>'none'</span> <span>and</span> <span>f</span>[<span>'vcodec'</span>] <span>==</span> <span>'none'</span> <span>and</span> <span>f</span>[<span>'ext'</span>] <span>==</span> <span>audio_ext</span>))

    <span># These are the minimum required fields for a merged format</span>
    <span>yield</span> {
        <span>'format_id'</span>: <span>f'<span><span>{</span><span>best_video</span>[<span>"format_id"</span>]<span>}</span></span>+<span><span>{</span><span>best_audio</span>[<span>"format_id"</span>]<span>}</span></span>'</span>,
        <span>'ext'</span>: <span>best_video</span>[<span>'ext'</span>],
        <span>'requested_formats'</span>: [<span>best_video</span>, <span>best_audio</span>],
        <span># Must be + separated list of protocols</span>
        <span>'protocol'</span>: <span>f'<span><span>{</span><span>best_video</span>[<span>"protocol"</span>]<span>}</span></span>+<span><span>{</span><span>best_audio</span>[<span>"protocol"</span>]<span>}</span></span>'</span>
    }


<span>ydl_opts</span> <span>=</span> {
    <span>'format'</span>: <span>format_selector</span>,
}

<span>with</span> <span>yt_dlp</span>.<span>YoutubeDL</span>(<span>ydl_opts</span>) <span>as</span> <span>ydl</span>:
    <span>ydl</span>.<span>download</span>(<span>URLS</span>)</pre></div>

<h2 tabindex="-1" dir="auto">DEPRECATED OPTIONS</h2>
<p dir="auto">These are all the deprecated options and the current alternative to achieve the same effect</p>
<h4 tabindex="-1" dir="auto">Almost redundant options</h4>
<p dir="auto">While these options are almost the same as their new counterparts, there are some differences that prevents them being redundant</p>
<div data-snippet-clipboard-copy-content="-j, --dump-json                  --print &quot;%()j&quot;
-F, --list-formats               --print formats_table
--list-thumbnails                --print thumbnails_table --print playlist:thumbnails_table
--list-subs                      --print automatic_captions_table --print subtitles_table"><pre><code>-j, --dump-json                  --print "%()j"
-F, --list-formats               --print formats_table
--list-thumbnails                --print thumbnails_table --print playlist:thumbnails_table
--list-subs                      --print automatic_captions_table --print subtitles_table
</code></pre></div>
<h4 tabindex="-1" dir="auto">Redundant options</h4>
<p dir="auto">While these options are redundant, they are still expected to be used due to their ease of use</p>
<div data-snippet-clipboard-copy-content="--get-description                --print description
--get-duration                   --print duration_string
--get-filename                   --print filename
--get-format                     --print format
--get-id                         --print id
--get-thumbnail                  --print thumbnail
-e, --get-title                  --print title
-g, --get-url                    --print urls
--match-title REGEX              --match-filter &quot;title ~= (?i)REGEX&quot;
--reject-title REGEX             --match-filter &quot;title !~= (?i)REGEX&quot;
--min-views COUNT                --match-filter &quot;view_count >=? COUNT&quot;
--max-views COUNT                --match-filter &quot;view_count <=? COUNT&quot;
--break-on-reject                Use --break-match-filter
--user-agent UA                  --add-header &quot;User-Agent:UA&quot;
--referer URL                    --add-header &quot;Referer:URL&quot;
--playlist-start NUMBER          -I NUMBER:
--playlist-end NUMBER            -I :NUMBER
--playlist-reverse               -I ::-1
--no-playlist-reverse            Default
--no-colors                      --color no_color"><pre><code>--get-description                --print description
--get-duration                   --print duration_string
--get-filename                   --print filename
--get-format                     --print format
--get-id                         --print id
--get-thumbnail                  --print thumbnail
-e, --get-title                  --print title
-g, --get-url                    --print urls
--match-title REGEX              --match-filter "title ~= (?i)REGEX"
--reject-title REGEX             --match-filter "title !~= (?i)REGEX"
--min-views COUNT                --match-filter "view_count &gt;=? COUNT"
--max-views COUNT                --match-filter "view_count &lt;=? COUNT"
--break-on-reject                Use --break-match-filter
--user-agent UA                  --add-header "User-Agent:UA"
--referer URL                    --add-header "Referer:URL"
--playlist-start NUMBER          -I NUMBER:
--playlist-end NUMBER            -I :NUMBER
--playlist-reverse               -I ::-1
--no-playlist-reverse            Default
--no-colors                      --color no_color
</code></pre></div>
<h4 tabindex="-1" dir="auto">Not recommended</h4>
<p dir="auto">While these options still work, their use is not recommended since there are other alternatives to achieve the same</p>
<div data-snippet-clipboard-copy-content="--force-generic-extractor        --ies generic,default
--exec-before-download CMD       --exec &quot;before_dl:CMD&quot;
--no-exec-before-download        --no-exec
--all-formats                    -f all
--all-subs                       --sub-langs all --write-subs
--print-json                     -j --no-simulate
--autonumber-size NUMBER         Use string formatting, e.g. %(autonumber)03d
--autonumber-start NUMBER        Use internal field formatting like %(autonumber+NUMBER)s
--id                             -o &quot;%(id)s.%(ext)s&quot;
--metadata-from-title FORMAT     --parse-metadata &quot;%(title)s:FORMAT&quot;
--hls-prefer-native              --downloader &quot;m3u8:native&quot;
--hls-prefer-ffmpeg              --downloader &quot;m3u8:ffmpeg&quot;
--list-formats-old               --compat-options list-formats (Alias: --no-list-formats-as-table)
--list-formats-as-table          --compat-options -list-formats [Default] (Alias: --no-list-formats-old)
--youtube-skip-dash-manifest     --extractor-args &quot;youtube:skip=dash&quot; (Alias: --no-youtube-include-dash-manifest)
--youtube-skip-hls-manifest      --extractor-args &quot;youtube:skip=hls&quot; (Alias: --no-youtube-include-hls-manifest)
--youtube-include-dash-manifest  Default (Alias: --no-youtube-skip-dash-manifest)
--youtube-include-hls-manifest   Default (Alias: --no-youtube-skip-hls-manifest)
--geo-bypass                     --xff &quot;default&quot;
--no-geo-bypass                  --xff &quot;never&quot;
--geo-bypass-country CODE        --xff CODE
--geo-bypass-ip-block IP_BLOCK   --xff IP_BLOCK"><pre><code>--force-generic-extractor        --ies generic,default
--exec-before-download CMD       --exec "before_dl:CMD"
--no-exec-before-download        --no-exec
--all-formats                    -f all
--all-subs                       --sub-langs all --write-subs
--print-json                     -j --no-simulate
--autonumber-size NUMBER         Use string formatting, e.g. %(autonumber)03d
--autonumber-start NUMBER        Use internal field formatting like %(autonumber+NUMBER)s
--id                             -o "%(id)s.%(ext)s"
--metadata-from-title FORMAT     --parse-metadata "%(title)s:FORMAT"
--hls-prefer-native              --downloader "m3u8:native"
--hls-prefer-ffmpeg              --downloader "m3u8:ffmpeg"
--list-formats-old               --compat-options list-formats (Alias: --no-list-formats-as-table)
--list-formats-as-table          --compat-options -list-formats [Default] (Alias: --no-list-formats-old)
--youtube-skip-dash-manifest     --extractor-args "youtube:skip=dash" (Alias: --no-youtube-include-dash-manifest)
--youtube-skip-hls-manifest      --extractor-args "youtube:skip=hls" (Alias: --no-youtube-include-hls-manifest)
--youtube-include-dash-manifest  Default (Alias: --no-youtube-skip-dash-manifest)
--youtube-include-hls-manifest   Default (Alias: --no-youtube-skip-hls-manifest)
--geo-bypass                     --xff "default"
--no-geo-bypass                  --xff "never"
--geo-bypass-country CODE        --xff CODE
--geo-bypass-ip-block IP_BLOCK   --xff IP_BLOCK
</code></pre></div>
<h4 tabindex="-1" dir="auto">Developer options</h4>
<p dir="auto">These options are not intended to be used by the end-user</p>
<div data-snippet-clipboard-copy-content="--test                           Download only part of video for testing extractors
--load-pages                     Load pages dumped by --write-pages
--youtube-print-sig-code         For testing youtube signatures
--allow-unplayable-formats       List unplayable formats also
--no-allow-unplayable-formats    Default"><pre><code>--test                           Download only part of video for testing extractors
--load-pages                     Load pages dumped by --write-pages
--youtube-print-sig-code         For testing youtube signatures
--allow-unplayable-formats       List unplayable formats also
--no-allow-unplayable-formats    Default
</code></pre></div>
<h4 tabindex="-1" dir="auto">Old aliases</h4>
<p dir="auto">These are aliases that are no longer documented for various reasons</p>
<div data-snippet-clipboard-copy-content="--avconv-location                --ffmpeg-location
--clean-infojson                 --clean-info-json
--cn-verification-proxy URL      --geo-verification-proxy URL
--dump-headers                   --print-traffic
--dump-intermediate-pages        --dump-pages
--force-write-download-archive   --force-write-archive
--load-info                      --load-info-json
--no-clean-infojson              --no-clean-info-json
--no-split-tracks                --no-split-chapters
--no-write-srt                   --no-write-subs
--prefer-unsecure                --prefer-insecure
--rate-limit RATE                --limit-rate RATE
--split-tracks                   --split-chapters
--srt-lang LANGS                 --sub-langs LANGS
--trim-file-names LENGTH         --trim-filenames LENGTH
--write-srt                      --write-subs
--yes-overwrites                 --force-overwrites"><pre><code>--avconv-location                --ffmpeg-location
--clean-infojson                 --clean-info-json
--cn-verification-proxy URL      --geo-verification-proxy URL
--dump-headers                   --print-traffic
--dump-intermediate-pages        --dump-pages
--force-write-download-archive   --force-write-archive
--load-info                      --load-info-json
--no-clean-infojson              --no-clean-info-json
--no-split-tracks                --no-split-chapters
--no-write-srt                   --no-write-subs
--prefer-unsecure                --prefer-insecure
--rate-limit RATE                --limit-rate RATE
--split-tracks                   --split-chapters
--srt-lang LANGS                 --sub-langs LANGS
--trim-file-names LENGTH         --trim-filenames LENGTH
--write-srt                      --write-subs
--yes-overwrites                 --force-overwrites
</code></pre></div>
<h4 tabindex="-1" dir="auto">Sponskrub Options</h4>
<p dir="auto">Support for <a href="https://github.com/faissaloo/SponSkrub">SponSkrub</a> has been deprecated in favor of the <code>--sponsorblock</code> options</p>
<div data-snippet-clipboard-copy-content="--sponskrub                      --sponsorblock-mark all
--no-sponskrub                   --no-sponsorblock
--sponskrub-cut                  --sponsorblock-remove all
--no-sponskrub-cut               --sponsorblock-remove -all
--sponskrub-force                Not applicable
--no-sponskrub-force             Not applicable
--sponskrub-location             Not applicable
--sponskrub-args                 Not applicable"><pre><code>--sponskrub                      --sponsorblock-mark all
--no-sponskrub                   --no-sponsorblock
--sponskrub-cut                  --sponsorblock-remove all
--no-sponskrub-cut               --sponsorblock-remove -all
--sponskrub-force                Not applicable
--no-sponskrub-force             Not applicable
--sponskrub-location             Not applicable
--sponskrub-args                 Not applicable
</code></pre></div>
<h4 tabindex="-1" dir="auto">No longer supported</h4>
<p dir="auto">These options may no longer work as intended</p>
<div data-snippet-clipboard-copy-content="--prefer-avconv                  avconv is not officially supported by yt-dlp (Alias: --no-prefer-ffmpeg)
--prefer-ffmpeg                  Default (Alias: --no-prefer-avconv)
-C, --call-home                  Not implemented
--no-call-home                   Default
--include-ads                    No longer supported
--no-include-ads                 Default
--write-annotations              No supported site has annotations now
--no-write-annotations           Default
--compat-options seperate-video-versions  No longer needed"><pre><code>--prefer-avconv                  avconv is not officially supported by yt-dlp (Alias: --no-prefer-ffmpeg)
--prefer-ffmpeg                  Default (Alias: --no-prefer-avconv)
-C, --call-home                  Not implemented
--no-call-home                   Default
--include-ads                    No longer supported
--no-include-ads                 Default
--write-annotations              No supported site has annotations now
--no-write-annotations           Default
--compat-options seperate-video-versions  No longer needed
</code></pre></div>
<h4 tabindex="-1" dir="auto">Removed</h4>
<p dir="auto">These options were deprecated since 2014 and have now been entirely removed</p>
<div data-snippet-clipboard-copy-content="-A, --auto-number                -o &quot;%(autonumber)s-%(id)s.%(ext)s&quot;
-t, -l, --title, --literal       -o &quot;%(title)s-%(id)s.%(ext)s&quot;"><pre><code>-A, --auto-number                -o "%(autonumber)s-%(id)s.%(ext)s"
-t, -l, --title, --literal       -o "%(title)s-%(id)s.%(ext)s"
</code></pre></div>
<h2 tabindex="-1" dir="auto">CONTRIBUTING</h2>
<p dir="auto">See <a href="https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#contributing-to-yt-dlp">CONTRIBUTING.md</a> for instructions on <a href="https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#opening-an-issue">Opening an Issue</a> and <a href="https://github.com/yt-dlp/yt-dlp/blob/master/CONTRIBUTING.md#developer-instructions">Contributing code to the project</a></p>
<h2 tabindex="-1" dir="auto">WIKI</h2>
<p dir="auto">See the <a href="https://github.com/yt-dlp/yt-dlp/wiki">Wiki</a> for more information</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Power of Prolog (159 pts)]]></title>
            <link>https://www.metalevel.at/prolog/facets</link>
            <guid>37473933</guid>
            <pubDate>Mon, 11 Sep 2023 21:25:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.metalevel.at/prolog/facets">https://www.metalevel.at/prolog/facets</a>, See on <a href="https://news.ycombinator.com/item?id=37473933">Hacker News</a></p>
<div id="readability-page-1" class="page">

    <center></center>
    <table>
      <tbody><tr>
        <td><i>Video</i>:</td>
        <td><a href="https://www.metalevel.at/prolog/videos/tour"><img src="https://www.metalevel.at/prolog/videos/t_tour.png" alt="A Tour of Prolog"></a>
        </td>
      </tr>
    </tbody></table>

    <h2>Prolog is ...</h2>

    <ul>
      <li>... a very <a href="#simple"><b>simple</b></a> language</li>
      <li>... a <a href="#declarative"><b>declarative</b></a> language</li>
      <li>... a <a href="#logic"><b>logic programming</b></a> language</li>
      <li>... a <a href="#homoiconic"><b>homoiconic</b></a> language</li>
      <li>... a very <a href="#dynamic"><b>dynamic</b></a> language</li>
      <li>... a very <a href="#versatile"><b>versatile</b></a> language.</li>
    </ul>
    

    <h2 id="simple"><center>Prolog is a very simple language</center></h2>

    It is easy to describe Prolog <b>syntax</b> in sufficient detail
    to start working with Prolog immediately.

    <p>

    All <a href="https://www.metalevel.at/prolog/data">data</a> are represented by
    Prolog&nbsp;<a href="https://www.metalevel.at/prolog/data#term"><b>terms</b></a>.

    </p><p>

    There is a single language element, called
    a <a href="https://www.metalevel.at/prolog/concepts#clause"><i>clause</i></a>. A clause is of the
    form:
    </p><p><b><tt>Head :- Body.</tt></b>
    </p>

    <br>

    This means that <b>if</b> <tt>Body</tt>
    holds, <b>then</b> <tt>Head</tt> holds. The infix
    operator <tt>(:-)/2</tt> represents an arrow from right to
    left:&nbsp;←.

    <p>

    If </p><tt>Head</tt> <i>always</i>
    holds, then <tt>:-&nbsp;Body</tt> can be&nbsp;<i>omitted</i>.

    <p>

    <i>The above is enough to write useful first Prolog programs.</i></p><p>

    <b>You may not believe this</b>, so witness the evidence: All
    programs presented in the following consist <i>only</i> of such
    clauses.

    </p><p>

    In fact, <i>all known computations</i> can be described in terms
    of such&nbsp;clauses, making Prolog
    a <a href="https://en.wikipedia.org/wiki/Turing_completeness">Turing&nbsp;complete</a>
    programming&nbsp;language. One way to implement a
    Turing&nbsp;machine in Prolog is to describe the <i>relation</i>
    between different&nbsp;states of the machine with clauses of the
    form "<i>If</i> the current state is&nbsp;</p><tt>S0</tt> <i>and</i>
    the symbol under the tape&nbsp;head is&nbsp;<tt>T</tt>, <i>and</i>
    ... <i>then</i> the next state is&nbsp;<tt>S</tt>".
    See <a href="https://www.metalevel.at/prolog/showcases/turing.pl"><tt>turing.pl</tt></a> for one
    implementation, and <a href="https://www.metalevel.at/tist/"><i>Thinking
    in&nbsp;States</i></a> for more information.

    <h2 id="declarative"><center>Prolog is a declarative language</center></h2>

    Prolog is a <i>declarative language</i>. This means that we focus
    on stating <i>what</i> we are interested in. We <a href="https://www.metalevel.at/prolog/writing">express
      what <i>holds</i></a> about solutions we want to find. We are less
    concerned about <i>how</i> the Prolog implementation finds these
    solutions.

    <p>

    This declarative nature often allows for very concise, clear and
    general specifications. It is unlikely that shorter formalisms
    that are equally clear and expressive exist.

    </p><p>

    For example, let us describe the <i>relation</i> between
    a <a href="https://www.metalevel.at/prolog/data#list">list</a> and its length,
    using <a href="https://www.metalevel.at/prolog/clpz">integer&nbsp;arithmetic</a>:

    </p><p><b>Note</b>: In <i>some</i> Prolog systems, you currently need to
      include a dedicated library to use declarative integer
      arithmetic. <a href="https://www.metalevel.at/prolog/clpz">More...</a>
    </p>

    <pre>list_length([], 0).
list_length([_|Ls], N) :-
        N #&gt; 0,
        N #= N0 + 1,
        list_length(Ls, N0).
    </pre>

    We can <a href="https://www.metalevel.at/prolog/reading#declarative"><b>read this declaratively</b></a> as follows:

    <ol>
      <li>The length of the <i>empty list</i>&nbsp;<tt>[]</tt> is &nbsp;0.</li>
      <li><b>If</b> the length of the list&nbsp;<tt>Ls</tt>
        is&nbsp;<tt>N0</tt> <b>and</b> <tt>N</tt>
        is <tt>N0+1</tt>, <b>then</b> the length
        of&nbsp;<tt>[_|Ls]</tt> is&nbsp;<tt>N</tt>. Further, this only
        holds <b>if</b> <tt>N</tt> is <i>greater than</i>&nbsp;0.
    </li></ol>

    When programming in Prolog, think in terms of <i>relations</i>
    between entities. Your programs will become very general with this
    approach. In the above example, it is tempting to think and say
    "We are computing the length of a list". And yes, it is true: We
    can indeed use the above definition to compute the length of
    a&nbsp;list:

    <pre>?- list_length([a,b,c], L).
   L = 3.
    </pre>

    However, this <i>imperative</i> reading does not do justice to
    what we have actually implemented, because the definition also
    covers several additional usage&nbsp;patterns. For
    example, <i>given</i> a specific length, we can ask <i>whether</i>
    there are lists of that length:

    <pre>?- list_length(Ls, 3).
   Ls = [_A,_B,_C]
;  false.
    </pre>

    Using the <b>most general query</b>, we can even ask for all
    answers that Prolog finds <i>in&nbsp;general</i>:

    <pre>?- list_length(Ls, L).
   Ls = [], L = 0
;  Ls = [_A], L = 1
;  Ls = [_A,_B], L = 2
;  Ls = [_A,_B,_C], L = 3
;  ... .
    </pre>

    We say that the relation is usable in different <i>modes</i>.
    Characteristically, Prolog reports all answers
    via&nbsp;<i>backtracking</i>.

    <p>

    The predicate </p><tt>length/2</tt> is part of
    the <a href="https://www.complang.tuwien.ac.at/ulrich/iso-prolog/prologue">Prologue
    for Prolog</a> draft, and already available as
    a <a href="https://www.metalevel.at/prolog/concepts#builtin">built-in</a> predicate in almost all
    Prolog implementations with the above semantics.

    <h2 id="logic"><center>Prolog is a logic programming language</center></h2>

    In the category of <i>declarative</i> languages, we
    find <i>functional</i> programming languages and <i>logic</i>
    programming languages. A function is a special case of a relation,
    and functional programming can be regarded as a restricted form of
    logic&nbsp;programming.

    <p>

    Prolog is firmly rooted in <a href="https://www.metalevel.at/prolog/logic"><b>logic</b></a>. </p><p>

    A <a href="https://www.metalevel.at/prolog/purity">pure</a> Prolog program consists of a set
    of <a href="https://en.wikipedia.org/wiki/Horn_clause">Horn&nbsp;clauses</a>.

    Its execution can be regarded as a special case
    of <a href="https://en.wikipedia.org/wiki/Resolution_(logic)"><i>resolution</i></a>.

    </p><p>

    This connection to formal logic allows us to apply powerful
    <a href="https://www.metalevel.at/prolog/debugging"><b>declarative&nbsp;debugging</b></a> techniques that
    are based on logical properties of the program.  For example,
    adding a <i>constraint</i> can at most <i>reduce</i> the set of
    solutions, and adding a&nbsp;<i>clause</i> can at most extend
    it. This property of pure Prolog programs is
    called <a href="https://en.wikipedia.org/wiki/Monotonicity_of_entailment">monotonicity</a>.

    </p><p>

    <i>See
      the <a href="https://www.complang.tuwien.ac.at/ulrich/gupu/"><b>GUPU
      system</b></a> by Ulrich Neumerkel for an impressive application
      of these&nbsp;ideas.</i></p><h2 id="homoiconic"><center>Prolog is a homoiconic language</center></h2>

    <p><i>homoiconic</i>: from ὁμός = "same" and εικών = "image"
    </p>
    <p>

    Prolog <i>programs</i> are also valid Prolog <i>terms</i>! This
    has many great advantages: It is easy to write Prolog programs
    that analyze, transform and interpret <i>other</i> Prolog
    programs. You can use the built-in predicate </p><tt>read/1</tt> to
    read a Prolog&nbsp;term, and thus also a Prolog&nbsp;clause.

    <p>

    There is a powerful <a href="https://www.metalevel.at/prolog/macros">mechanism</a> to rewrite
    Prolog programs at compilation time, so that you can easily
    implement domain-specific languages that help you solve your tasks
    more naturally.

    </p><p>
    <b>You may not believe this</b>, because some goals—such
    as </p><tt>list_length(Ls,&nbsp;N)</tt>—look like
    Prolog&nbsp;terms as defined above, whereas other goals—such
    as&nbsp;<tt>N&nbsp;#&gt;&nbsp;0</tt>—look quite different.
    The reason for this is that Prolog provides prefix, infix and
    postfix&nbsp;<i>operators</i> that let you write Prolog&nbsp;terms
    in a more readable way. For example, the Prolog
    term&nbsp;<tt>+(a,b)</tt> can also be written using
    operator&nbsp;notation as&nbsp;<tt>a+b</tt>.
    The <i>abstract&nbsp;syntax</i> remains completely uniform, and
    you can read and process all Prolog terms independent of how they
    are written&nbsp;down.

    <p>
    <i>You can dynamically define custom operators for specific use cases.</i></p><h2 id="dynamic"><center>Prolog is a very dynamic language</center></h2>

    Prolog programs can be easily created, called and modified <i>at
    run time</i>. This further increases the expressiveness of Prolog
    and lets you implement <a href="https://www.metalevel.at/prolog/metapredicates">higher-order
    predicates</a> which have other predicates as arguments. It also
    allows the implementation of very dynamic techniques
    like <i>adaptive&nbsp;parsing</i>.

    <p>

    The dynamic nature of Prolog also makes the language ideally
    suited for writing programs that are <i>extensible</i> by
    custom&nbsp;rules that other programmers and even regular users
    provide. <a href="https://www.metalevel.at/proloxy/"><b>Proloxy</b></a>
    and <a href="https://www.gerritcodereview.com/">Gerrit
    Code&nbsp;Review</a> are examples of this approach: You configure
    these programs by supplying
    Prolog&nbsp;<a href="https://www.metalevel.at/prolog/concepts#rule">rules</a> that express your
    requirements in a very readable and flexible&nbsp;way.

    </p><p>
    See <a href="https://www.metalevel.at/acomip/"><b>A Couple of
        Meta-interpreters in&nbsp;Prolog</b></a> for more information.

    </p><p>

    <i>You can define an interpreter for pure Prolog in two lines of
      Prolog&nbsp;code.</i></p><h2 id="versatile"><center>Prolog is a very versatile language</center></h2>

    Prolog is an extremely versatile language. Its relational nature
    makes Prolog programs very flexible and general. This plays an
    important role in <a href="https://www.metalevel.at/prolog/dcg">language processing</a> and
    knowledge representation
    in&nbsp;<a href="https://www.metalevel.at/prolog/business#database">databases</a>. Modern Prolog
    systems provide everything that is needed for solving
    simple <a href="https://www.metalevel.at/prolog/puzzles">logic&nbsp;puzzles</a> to building huge
    applications, ranging from
    <a href="https://www.metalevel.at/prolog/web">web&nbsp;hosting</a>
    to <a href="https://www.metalevel.at/prolog/theoremproving">verification</a>
    and <a href="https://www.metalevel.at/prolog/optimization">optimization</a> tasks.

    <p>

    Prolog's versatility and power are rooted in <i>implicit</i>
    mechanisms that include search, unification, argument indexing and
    constraint propagation. You can use these mechanisms to your
    advantage, and <i>delegate</i> many tasks to the
    Prolog&nbsp;engine.

    </p><table>
      <tbody><tr>
        <td><i>Video</i>:</td>
        <td><a href="https://www.metalevel.at/prolog/videos/sparrows_on_eagles"><img src="https://www.metalevel.at/prolog/videos/t_sparrows_on_eagles.png" alt="Sparrows on Eagles"></a>
        </td>
      </tr>
    </tbody></table>

    <p>

    <i>This page was sent to you by
      a <a href="https://www.metalevel.at/letswicrypt/"><b>Prolog&nbsp;HTTPS
      server</b></a>.</i></p><p>
    <b><a href="https://www.metalevel.at/prolog">More about Prolog</a></b></p><p>

    <b><a href="https://www.metalevel.at/">Main page</a></b></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft to kill off third-party printer drivers in Windows (130 pts)]]></title>
            <link>https://www.theregister.com/2023/09/11/go_native_or_go_home/</link>
            <guid>37473628</guid>
            <pubDate>Mon, 11 Sep 2023 21:00:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/09/11/go_native_or_go_home/">https://www.theregister.com/2023/09/11/go_native_or_go_home/</a>, See on <a href="https://news.ycombinator.com/item?id=37473628">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>Microsoft has made it clear: it will ax third-party printer drivers in Windows.</p>
<p>The death rattle will be lengthy, as the timeline for the end of servicing stretches into 2027 – although Microsoft noted that the dates will be subject to change. There is, after all, always that important customer with a strange old printer lacking Mopria support.</p>
<p><a target="_blank" rel="nofollow" href="https://mopria.org/mopria-alliance">Mopria</a> is part of the Windows' teams justification for removing support. Founded in 2013 by Canon, HP, Samsung and Xerox, the <a target="_blank" rel="nofollow" href="https://mopria.org/">Mopria Alliance</a>'s mission is to provide universal standards for printing and scanning. Epson, Lexmark, Adobe and Microsoft have also joined the gang since then.</p>

    

<p>Since Windows 10 21H2, Microsoft has baked Mopria support into the flagship operating system, with support for devices connected via the network or USB, thanks to the Microsoft IPP Class driver. Microsoft said: "This removes the need for print device manufacturers to provide their own installers, drivers, utilities, and so on."</p>

        


        

<p>The software giant also said that customization can be performed via Print Support Apps from the Windows Store. It added: "This framework improves reliability and performance by moving customization from the Win32 framework to the UWP software development framework."</p>
<p>While some wags have dubbed the framework the <a target="_blank" href="https://www.theregister.com/2021/10/26/microsofts_uwp_unwanted_windows_platform/">"Unwanted Windows Platform"</a>, it's always good to see legacy tech being retired in favor of something with a bright future ahead of it.</p>

        

<p>Microsoft's timeline for the end of servicing will be staged. The next milestone will occur in 2025 when no new printer drivers will be published to Windows Update – although existing drivers can still be updated. In 2026, driver ranking will be tweaked to bring the IPP inbox class driver to the top, and by 2027 – except for security-related fixes – no printer driver updates will be allowed.</p>
<ul>

<li><a href="https://www.theregister.com/2023/07/11/microsoft_patch_tuesday/">Miscreants exploit five Microsoft bugs as Windows giant addresses 130 flaws</a></li>

<li><a href="https://www.theregister.com/2022/12/05/opinion_column_printing/">Killing trees with lasers isn't cool, says Epson. So why are inkjets any better?</a></li>

<li><a href="https://www.theregister.com/2022/03/04/on_call/">Saving a loved one from a document disaster</a></li>

<li><a href="https://www.theregister.com/2021/10/18/windows_printing/">Microsoft admits to yet more printing problems in Windows as back-at-the-office folks asked for admin credentials</a></li>
</ul>
<p>To be clear, the end of servicing applies to drivers provided via Windows Update. Manufacturers will, according to Microsoft, "need to provide customers with an alternative means to download and install those printer drivers." Legacy v3 and v4 Windows printer drivers are facing the end of servicing ax.</p>
<p>Microsoft added that multi-function devices – print, scan and fax – will work via the inbox drivers.</p>
<p>Printing and Windows have <a target="_blank" href="https://www.theregister.com/2022/07/26/windows_10_printer_bork/">long been uneasy bedfellows</a>. While Microsoft hopes the end-of-servicing will take away some legacy driver headaches, there are plenty of other components within the Windows printing subsystem that can <a target="_blank" href="https://www.theregister.com/2021/10/18/windows_printing/">occasionally topple</a> when poked the wrong way by a patch. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[uBlock-Origin – 1.52.0 (215 pts)]]></title>
            <link>https://github.com/gorhill/uBlock/releases/tag/1.52.0</link>
            <guid>37472994</guid>
            <pubDate>Mon, 11 Sep 2023 20:13:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/gorhill/uBlock/releases/tag/1.52.0">https://github.com/gorhill/uBlock/releases/tag/1.52.0</a>, See on <a href="https://news.ycombinator.com/item?id=37472994">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:gorhill/uBlock" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="x8C6R9GJSwffziApgP8Nx1Hb5B4IUBkEuexf3bwqIz2-7Feg9FkhJtAX9lTquRZnH9DjGw5zPjEi5JI5gpqigg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="gorhill/uBlock" data-current-org="" data-current-owner="gorhill" data-logged-in="false">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Freleases%2Fshow&amp;source=header-repo&amp;source_repo=gorhill%2FuBlock" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/gorhill/uBlock/releases/tag/1.52.0&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="2f3c5dde8d1530c1097543858fcd6094e612f73a18c39160cb2954947a81a077" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/releases/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Blood pressure should be measured lying down: study (231 pts)]]></title>
            <link>https://newsroom.heart.org/news/high-blood-pressure-while-lying-down-linked-to-higher-risk-of-heart-health-complications</link>
            <guid>37471354</guid>
            <pubDate>Mon, 11 Sep 2023 18:20:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsroom.heart.org/news/high-blood-pressure-while-lying-down-linked-to-higher-risk-of-heart-health-complications">https://newsroom.heart.org/news/high-blood-pressure-while-lying-down-linked-to-higher-risk-of-heart-health-complications</a>, See on <a href="https://news.ycombinator.com/item?id=37471354">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body-container">
          <p>Research Highlights:</p>

<ul>
	<li>An analysis of data from a long-running study of more than 11,000 adults from four diverse communities in the United States has found that adults who had high blood pressure while both seated upright and lying supine (flat on their backs) had a higher risk of heart disease, stroke, heart failure or premature death compared to adults without high blood pressure while upright and supine.&nbsp;</li>
	<li>Adults who had high blood pressure while lying supine but not while seated upright had similar elevated risks of heart attack, stroke, heart failure or premature death as adults who had high blood pressure in both supine and upright positions.</li>
	<li>The increased risk of heart disease, stroke, heart failure or premature death did not differ by the type of blood pressure medication used among participants.</li>
</ul>

<p><strong>Embargoed until 6:30a.m. CT/7:30 a.m. ET Thursday, Sept. 7, 2023</strong></p>

<p>BOSTON, Sept. 7, 2023 — People who had high blood pressure while lying flat on their backs had a higher risk of heart attack, stroke, heart failure or premature death, according to new research to be presented at the American Heart Association’s <a href="https://professional.heart.org/en/meetings/hypertension" rel="" target="_blank" title="">Hypertension Scientific Sessions 2023</a>, to be held Sept. 7-10, 2023, in Boston. The meeting is the premier scientific exchange focused on recent advances in basic and clinical research on high blood pressure and its relationship to cardiac and kidney disease, stroke, obesity and genetics.</p>

<p>The autonomic nervous system regulates blood pressure in different body positions; however, gravity may cause blood to pool when seated or upright, and the body is sometimes unable to properly regulate blood pressure during lying, seated and standing positions, the authors noted.</p>

<p>“If blood pressure is only measured while people are seated upright, cardiovascular disease risk may be missed if not measured also while they are lying supine on their backs,” said lead study author Duc M. Giao, a researcher and a 4<sup>th</sup>-year M.D. student at Harvard Medical School in Boston.</p>

<p>To examine body position, blood pressure and heart health risk, the researchers examined health data for 11,369 adults from the longitudinal Atherosclerosis Risk in Communities (ARIC) study. The data on supine and seated blood pressure was gathered during the enrollment period, ARIC visit 1, which took place between 1987–1989. Participants had their blood pressure taken while briefly lying down at a clinic. The average age of participants at that time was 54 years old; 56% of the group self-identified as female; and 25% of participants self-identified as Black race. Participants in this analysis were followed for an average of 25 to 28 years, up through ARIC visit 5, which includes health data collected from 2011-2013.</p>

<p>The researcher’s findings included:</p>

<ul>
	<li>16% percent of participants who did not have high blood pressure — defined in this study as having top and bottom blood pressure measures greater than or equal to 130/80 mm Hg — while seated had high blood pressure while lying supine (flat on their backs), compared to 74% of those with seated high blood pressure who also had supine high blood pressure.</li>
	<li>In comparison to participants who did not have high blood pressure while seated and supine, participants who had high blood pressure while seated and supine had a 1.6 times higher risk of developing coronary heart disease; a 1.83 times higher risk of developing heart failure; a 1.86 times higher risk of stroke; a 1.43 times higher risk of overall premature death; and a 2.18 times higher risk of dying from coronary heart disease</li>
	<li>Participants who had high blood pressure while supine but not while seated had similar elevated risks as participants who had high blood pressure while both seated and supine.</li>
	<li>Differences in blood pressure medication use did not affect these elevated risks in either group.</li>
</ul>

<p>“Our findings suggest people with known risk factors for heart disease and stroke may benefit from having their blood pressure checked while lying flat on their backs,” Giao said.</p>

<p>“Efforts to manage blood pressure during daily life may help lower blood pressure while sleeping. Future research should compare supine blood pressure measurements in the clinic with overnight measurements.”</p>

<p>The study’s limitations included that it focused on adults who were middle-aged at the time of enrollment, meaning the results might not be as generalizable to older populations, Giao said.</p>

<p><strong>Note: Giao presents <em>Seated And Supine Blood Pressure And Risk Of Cardiovascular Disease And Mortality From The Atherosclerosis Risk In Communities Study </em>at 2:15 p.m. ET on Saturday, Sept. 9, 2023, Presentation #071; Abstract #452</strong></p>

<p>Background:</p>

<ul>
	<li>The Atherosclerosis Risk in Communities (ARIC) study is an ongoing, community-based cohort of 15,792 adults in the United States enrolled from 1987-1989 to investigate the causes for atherosclerotic disease (plaque or fatty buildup in the arteries). ARIC study participants were ages 45–65 years at the start of the study and from rural areas in the U.S. (Forsyth County, North Carolina, and Washington County, Maryland) and urban areas: Minneapolis and Jackson, Mississippi. The research and data from the ARIC clinical visits — including hospital record abstraction, ECG tracings, and physician and coroner questionnaires, as well as death certificate data — have led to discoveries and guidelines surrounding atherosclerosis, heart disease, kidney disease, diabetes, stroke and cognitive decline.</li>
	<li>The <a href="https://www.ahajournals.org/doi/full/10.1161/HYP.0000000000000065" target="_blank">2017 ACC/AHA Guideline for the Prevention, Detection, Evaluation, and Management of High Blood Pressure in Adults</a> classifies hypertension as having top and bottom numbers greater than or equal to 130/80 mm Hg, which was the definition of hypertension used in this study.</li>
</ul>

<p>Co-authors and their disclosures are listed in the abstract. The study was funded by the National Institutes of Health.</p>

<p>Statements and conclusions of studies that are presented at the American Heart Association’s scientific meetings are solely those of the study authors and do not necessarily reflect the Association’s policy or position. The Association makes no representation or guarantee as to their accuracy or reliability. The Association receives funding primarily from individuals; foundations and corporations (including pharmaceutical, device manufacturers and other companies) also make donations and fund specific Association programs and events. The Association has strict policies to prevent these relationships from influencing the science content. Revenues from pharmaceutical and biotech companies, device manufacturers and health insurance providers and the Association’s overall financial information are available <a href="https://www.heart.org/en/about-us/aha-financial-information">here</a>.</p>

<p><strong>Additional Resources:</strong></p>

<ul>
	<li>Available multimedia is on right column of release link&nbsp;<a href="https://newsroom.heart.org/news/high-blood-pressure-while-lying-down-linked-to-higher-risk-of-heart-health-complications?preview=3a007402d06b4cd7cbc3c53acb93b5f5" rel="" target="_blank" title="">https://newsroom.heart.org/news/high-blood-pressure-while-lying-down-linked-to-higher-risk-of-heart-health-complications?preview=3a007402d06b4cd7cbc3c53acb93b5f5</a></li>
	<li><a href="https://www.abstractsonline.com/pp8/?_ga=2.68132181.553845914.1693780001-1860925560.1690312372#!/10947" rel="" target="_blank" title="">Program abstracts online at embargo</a></li>
	<li>AHA news release: <a href="https://newsroom.heart.org/news/if-blood-pressure-rises-upon-standing-so-may-risk-for-heart-attack" target="_blank">If blood pressure rises upon standing, so may risk for heart attack</a> (March 2022)</li>
	<li>AHA&nbsp; news release: <a href="https://newsroom.heart.org/news/blood-pressure-rising-at-night-linked-to-doubling-risk-of-death-in-adults-with-diabetes" target="_blank">Blood pressure rising at night linked to doubling risk of death in adults with diabetes</a> (Sept. 2021)</li>
	<li>AHA news release: <a href="https://newsroom.heart.org/news/abnormal-blood-pressure-levels-while-sleeping-increase-risk-of-heart-disease-stroke" target="_blank">Abnormal blood pressure levels while sleeping increase risk of heart disease</a> (November 2020)</li>
	<li>Follow AHA/ASA news on X (formerly known as Twitter) <a href="https://twitter.com/HeartNews" target="_blank">@HeartNews</a> #Hypertension23</li>
</ul>

<p><strong>###</strong></p>

<p><strong>About the American Heart Association </strong></p>

<p>The American Heart Association is a relentless force for a world of longer, healthier lives. We are dedicated to ensuring equitable health in all communities. Through collaboration with numerous organizations, and powered by millions of volunteers, we fund innovative research, advocate for the public’s health and share lifesaving resources. The Dallas-based organization has been a leading source of health information for nearly a century. Connect with us on <a href="http://www.heart.org/en" target="_blank">heart.org</a>, <a href="http://facebook.com/AmericanHeart" target="_blank">Facebook</a>, <a href="https://twitter.com/American_Heart" rel="" target="_blank" title="">X</a>&nbsp;or by calling 1-800-AHA-USA1.</p>

<p><strong>For Media Inquiries and AHA Expert Perspective: </strong></p>

<p>AHA Communications &amp; Media Relations&nbsp;in Dallas: 214-706-1173; <a href="mailto:ahacommunications@heart.org">ahacommunications@heart.org</a></p>

<p>John Arnst: 214-706-1060; <a href="mailto:John.Arnst@heart.org">John.Arnst@heart.org</a></p>

<p>For Public Inquiries: 1-800-AHA-USA1 (242-8721)</p>

<p><a href="https://www.heart.org/en" target="_blank">heart.org</a> and <a href="https://www.stroke.org/en" target="_blank">stroke.org</a></p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In Germany, 27 are in 'preventive detention' b/c they might do climate protests (258 pts)]]></title>
            <link>https://mastodon.energy/@Sustainable2050/111039159882536261</link>
            <guid>37471048</guid>
            <pubDate>Mon, 11 Sep 2023 18:00:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.energy/@Sustainable2050/111039159882536261">https://mastodon.energy/@Sustainable2050/111039159882536261</a>, See on <a href="https://news.ycombinator.com/item?id=37471048">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[MGM is down, cybersecurity attack ongoing (187 pts)]]></title>
            <link>https://www.casino.org/news/mgm-resorts-suffers-cybersecurity-attack-system-outage-reported/</link>
            <guid>37470801</guid>
            <pubDate>Mon, 11 Sep 2023 17:44:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.casino.org/news/mgm-resorts-suffers-cybersecurity-attack-system-outage-reported/">https://www.casino.org/news/mgm-resorts-suffers-cybersecurity-attack-system-outage-reported/</a>, See on <a href="https://news.ycombinator.com/item?id=37470801">Hacker News</a></p>
Couldn't get https://www.casino.org/news/mgm-resorts-suffers-cybersecurity-attack-system-outage-reported/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Real-Time 3D Gaussian Splatting in WebGL (286 pts)]]></title>
            <link>https://antimatter15.com/splat/</link>
            <guid>37470611</guid>
            <pubDate>Mon, 11 Sep 2023 17:31:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://antimatter15.com/splat/">https://antimatter15.com/splat/</a>, See on <a href="https://news.ycombinator.com/item?id=37470611">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="info">
			<h3>WebGL 3D Gaussian Splat Viewer</h3>
			<p>Use mouse or arrow keys to navigate.</p>
			<p><small>
				By <a href="https://twitter.com/antimatter15">Kevin Kwok</a>.
				Code on
				<a href="https://github.com/antimatter15/splat">Github</a>.
			</small>
		</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Why did Visual Basic die? (378 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37470318</link>
            <guid>37470318</guid>
            <pubDate>Mon, 11 Sep 2023 17:12:56 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37470318">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="37471507"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471507" href="https://news.ycombinator.com/vote?id=37471507&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Visual Basic (both of them) still exist, but their use has dropped dramatically through some big changes:<p>* "Visual .NET" (aka "Visual Fred" <a href="http://catb.org/jargon/html/V/Visual-Fred.html" rel="nofollow noreferrer">http://catb.org/jargon/html/V/Visual-Fred.html</a> ) was released by Microsoft. This was an incompatible language confusingly <i>also</i> called Visual Basic. I don't think Microsoft realized how angry this made developers and businesses, who were being asked to spend hundreds of billions of dollars (USD) to rewrite code just to keep the same functionality. Before that time, many thought that Visual Basic's wide use gave it a kind of "herd immunity". I don't have numbers with me, but I remember that years later that a study found that some were sticking to the original Visual Basic (even though it was no longer supported), a few had moved to Visual .NET, and many other had abandoned Visual Basic entirely (some to C#, others beyond). In short, the Visual Basic community was split into multiple communities, and anyone using Visual Basic would have to worry about either lack of support or yet another harmful change.</p><p>* The rise of the web and of platforms other than Windows (including Android, iOS, MacOS, Linux). Visual Basic is fine when you send files via sneakernet to another Windows user. Now people want to access through their web browser, smartphone, etc. If you have a website, anything can access it (as long as they have the permissions), and you don't have to worry about synchronizing data changes the way you do if people make changes on their local device. Most of the simple "fill in a form" kinds of applications that Visual Basic was used for are more sensibly web applications (server side or client side).</p><p>Visual Basic is still used. And yes, I think there could be better tools for developing software. But as best as I recall, that's how we ended up here.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472277"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472277" href="https://news.ycombinator.com/vote?id=37472277&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Visual Basic is one of the best arguments for open source and community ownership in the history of computing, IMO. Microsoft's decision to tank it was hugely painful for companies that had made major investments in it -- no company should make that kind of investment in a proprietary platform that can be killed off by a single company and not forked and maintained by others.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472386"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472386" href="https://news.ycombinator.com/vote?id=37472386&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>As someone who was writing Visual Basic.NET back when it came out, there was no upside to it over writing in C#. VB's original sweet spot was for writing small scripts and apps in Windows, and it was the only language available. When the .NET line came out, you could do the same things in whichever language you wanted. When new tasks came in, I started defaulting to C# for that reason. I don't think anyone actually prefers VB syntax. C# has a pretty robust community around it now.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37472754"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472754" href="https://news.ycombinator.com/vote?id=37472754&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt; When new tasks came in, I started defaulting to C# for that reason. I don't think anyone actually prefers VB syntax. C# has a pretty robust community around it now.<p>Which was your experience because you knew C. VB appealed to people who were not programmers (or not very good ones like me). Microsoft effectively tossed an easy to learn procedural language in the trash and said "go learn all these advanced CS concepts or stop writing stuff" to which most hardcore VB users chose the latter.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472059"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472059" href="https://news.ycombinator.com/vote?id=37472059&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Lol, my first programming gig as a teenager was performing a VB6 -&gt; VB.NET "upgrade" of a 200K sloc legacy desktop application, which obviously ended up being a total rewrite. Everything in my career since then has seemed easy in comparison.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472298"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472298" href="https://news.ycombinator.com/vote?id=37472298&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I was a hobby VB developer at this time and I abandoned it shortly after VB.net without really realizing why. I also abandoned Windows completely shortly thereafter.<p>The main thing that killed me was the size of the files that you had to distribute when you used VB.net. No one had the .net runtime early on and it was absolutely massive. I was paying for outgoing data by the gigabyte back then and our connections were much slower.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472597"><td></td></tr>
            <tr id="37472322"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472322" href="https://news.ycombinator.com/vote?id=37472322&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Yeah, as someone who worked somewhere that had a VB6 project: VB.net was only at all useful as a stop gap between VB and C# and a barely useful one at that.<p>The language as it stands is fine and interop with .NET means it is a decent choice to use, outside of C# just having a bigger user base from a "how easy can I hire devs" standpoint.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472377"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472377" href="https://news.ycombinator.com/vote?id=37472377&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Great points. And in a tongue in cheek way, having reactive components you can attach handlers to fetch / redraw you UI .. is still there, it's just labelled vue or react ;)</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472779"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472779" href="https://news.ycombinator.com/vote?id=37472779&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span><i>Yet I can't help wondering what problems it had that caused them to abandon it?</i><p>Tech comes and goes, there’s nothing to vb specifically. As a language it’s pretty limited, tedious and quirky.</p><p><i>Moreover, why hasn't someone come out with a solid replacement?</i></p><p>Because webdev at its core is a community of stubborn smart guys who <i>love</i> the complexity and hate dull business code. They will present absurd arguments like I can do this and that, as if it couldn’t be packed into a vb component and drag-dropped onto a form from a palette without accompanying 1kloc boilerplate and pages of configuration documentation with no sane defaults. VB GUI model may be obsolete, gray and non-responsive, but no one prevents from building responsive interfaces wysiwyg way. My peer web designer does it without bothering with html/css much and it works for her for decades. All the tech is there, it’s just nobody’s collective interest to combine it into a business RAD instead of an intermediate haskell-level mindfuck starter kit. You can’t burn hundreds of millions doing actual work on a platform that everyone could start using solo in just a few days and deliver a working solution next week, even if raw and clumsy as it usually goes with non-pros.</p><p>I’d like to get a better and less bitter explanation, but there is none.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472327"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472327" href="https://news.ycombinator.com/vote?id=37472327&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>The reason is even bigger than just VB.<p>MS at the time just decide to <i>fully</i> kill the "enthusiast" developer and the "single/truly small" team developer. This is mostly covered under the "RAD" umbrella.</p><p>It kills VB, FoxPro, and now more evidently, Access (more like let it slowly die).</p><p>.NET + Visual Studio + Sql Server are <i>not</i> a substitute in this market. Them are for "professional developer"/"a small cog in a big machine". The worst part is that this move somehow kill the other tools in this space (because somehow others follow suit or whatever) and without somebody leading the charge to see how adapt this tool for the web. MS not getting the Web, Borland doing Hara-kiri and others getting annihilated by "free" open source and all that not helps.</p><p>Ironically, this market have rebound in the myriad of tools like "low code, notebooks, etc" that fill (badly!) the gap.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472473"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472473" href="https://news.ycombinator.com/vote?id=37472473&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Serious question: what is a good alternative to Access? The database design tools and basic forms were incredibly easy to use, and there were very good tutorials for everything else. LibreOffice Base is different and not even close in comprehensiveness, and there seems to be nothing replacing it that isn't a super expensive SaaS.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37472653"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472653" href="https://news.ycombinator.com/vote?id=37472653&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>SQLite. It's not close in terms of ease-of-use of the GUI administration and forms, but as a single-server database solution to embed into a line-of-business app it's fantastic.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37472752"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37472752" href="https://news.ycombinator.com/vote?id=37472752&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>The GUI and forms is what made Access, though. Back in the day I actually made a system with Access that connected to a SQL Server database on the backend rather than Access's file-based engine.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472728"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472728" href="https://news.ycombinator.com/vote?id=37472728&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Airtable is the closest I have found for ease of use - users that don’t know SQL can put together basic queries and views quickly.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472748"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472748" href="https://news.ycombinator.com/vote?id=37472748&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Equally serious question: what is the use case for Access?<p>It's been installed on every corp workstation I've had and it's never been useful.  In my experience either Excel can do it or you need a real programming language/database.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472621"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472621" href="https://news.ycombinator.com/vote?id=37472621&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>The parents point is that there is no alternative, because everyone left the market chasing Microsoft.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472594"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472594" href="https://news.ycombinator.com/vote?id=37472594&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>For a long time I would have said FileMaker, but considering the strategic moves of Claris in the last few years probably not anymore</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472615"><td></td></tr>
                        <tr id="37470603"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470603" href="https://news.ycombinator.com/vote?id=37470603&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Does VBA for Excel count? Because if it does then VBA for Excel has reached the"nuclear resistant cockroach" level in finance.<p>You wouldn't believe what sort of processes in very big banks/financial institutions are built using 10 year old VBA macros. In fact, VBA consulting for finance is a very juicy cottage industry at least in Europe to this very day.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37470877"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470877" href="https://news.ycombinator.com/vote?id=37470877&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Excel is literally 2D programming. Us mortal developers who can only put lines below one another are incapable of comprehending it, so we only get to ask the wise finance people how their enigma works.<p>On a serious note, I dread excel. If your PC is set to german, excel will translate the VBA keywords to german. But if you want to type them, you have to do that in english and then have excel translate them.</p><p>I don't want to accept that crap like this is the standard.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472516"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472516" href="https://news.ycombinator.com/vote?id=37472516&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>The 2D aspect is the part of Excel I don't understand.  Why does it have to be a grid?<p>It's great for laying out things meant to print, and making invoices and stuff... But why didn't we have code files and proper fixed layout DB-style tables as "pages" that can go in a workbook?</p><p>Maybe keeping everything as 2D as possible is a necessary compromise for the spatial thinkers out there, and they just wouldn't want it if it was full of boring linear stuff.</p><p>I love the reactivity and the concept that anywhere you put a value, you can put an =expression. But the 2D stuff seems like it's for the people who always have a sense of where things are in space.</p><p>They've done a good job of convincing people that it's not programming and they can do it, I can't really complain, because if Excel didn't exist we might all still have to use paper on a regular basis, or completely unstructured text files.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471538"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471538" href="https://news.ycombinator.com/vote?id=37471538&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt; Us mortal developers who can only put lines below one another<p>At least my spaghetti code goes into one direction only...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472328"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472328" href="https://news.ycombinator.com/vote?id=37472328&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>"2D programming" is normally called array programming, and it's common in scientific computing, ML, and finance. It does require a different mindset, kind of similar to SQL but not exactly. See APL, K, q, NumPy, matlab, Julia, and friends for languages that embrace this paradigm</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472138"><td></td></tr>
            <tr id="37472216"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472216" href="https://news.ycombinator.com/vote?id=37472216&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt; But if you want to type them, you have to do that in english and then have excel translate them.<p>Huh? At least in the versions I've used I've always needed to type the commands in German.</p><p>There's even an online German - English Excel dictionary...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37471107"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471107" href="https://news.ycombinator.com/vote?id=37471107&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Healthcare and insurance too! I transform into some glorious magical elf when I volunteer to do the VBA tasks nobody else understands or can lower themselves to do.<p>You can make Excel do some real wacky stuff. I have a spreadsheet that actually calls out to exec() to run a curl POST on commandline and consume REST API endpoints, parse the results, and update the spreadsheet -- why on earth?? because the API was ready but the web app was delayed. I was the fix. :D
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471164"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471164" href="https://news.ycombinator.com/vote?id=37471164&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Pretty sure you can access a REST API from VBA without resorting to exec()</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37471214"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37471214" href="https://news.ycombinator.com/vote?id=37471214&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>It only works with toy examples and then stops working. For reasons unknown, as it should work.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37471459"><td></td></tr>
                              <tr id="37472447"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472447" href="https://news.ycombinator.com/vote?id=37472447&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Pre-face: I write a lot of VBA<p>VBA is kind of the result of people only - ONLY - wanting to use Excel for everything. I work with those people. They have mastered excel, but have little to zero interest in learning anything else, and would rather see the world be built around excel.</p><p>So you (like me) get tasked with building applications and forms in VBA.</p><p>I was STOKED when MS announced Python for excel, but alas, turned out to not be what I (and many other) wanted.</p><p>What's the medicine? Dunno, hire analysts that are more open to using other tools . Don't get me wrong, I love using excel for many tasks - but damnit, it's not the only tool.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470889"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470889" href="https://news.ycombinator.com/vote?id=37470889&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I know of a restaurant franchisee with 170+ locations that uses a home grown ERP system built in VBA on top of Access by an accountant about 20 years ago.<p>I once had to update it to optimize (minimize) front-line staff working hours so the company didn't have to pay health insurance for those employees. A real nightmare of a task in more ways than one!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471254"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471254" href="https://news.ycombinator.com/vote?id=37471254&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>My first programming job in the 90s while I was still in college was building systems exactly like you describe (and they were as poorly built as you think hah). I worked for a small IT programming/consulting shop. We did small jobs like this in town in addition to installing networks, IVRs, etc... while working on larger software to sell (which is an entirely different/crazy story that involved burning CD demos and using a hand 'stomper' to label and then mail them out).</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472418"><td></td></tr>
                  <tr id="37470909"><td></td></tr>
                <tr id="37471364"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471364" href="https://news.ycombinator.com/vote?id=37471364&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I can confirm this: I know several mechanical engineers that do mission critical-type systems (think "power plants"), and they routinely use Excel VBA for calculations.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37471506"><td></td></tr>
                        <tr id="37472385"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472385" href="https://news.ycombinator.com/vote?id=37472385&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Before VBA for Excel, I wrote numerous macros, including ATAN2(x,y) before there was an ATAN2.<p>Afterwards, forget it. Maybe one.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470767"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470767" href="https://news.ycombinator.com/vote?id=37470767&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>There is a book called: Professional Excel Development. If you want to get into it. You could probably use that book to build an OS in Excel. I'm not joking.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37471265"><td></td></tr>
                  <tr id="37472520"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472520" href="https://news.ycombinator.com/vote?id=37472520&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I used it as a coding layman in 15-buck-an-hour "admin" (clerk/secretary) roles to automate some processes that my predecessors had done by hand. Mostly just copying entries from a spreadsheet to company Excel/Powerpoint templates and printing them without killing myself copy/pasting or going into the save/print dialog 50 times. It did its work as something any old schmuck could harness to save themselves from carpal tunnel.<p>To that point, I imagine that there are a LOT of admin jobs (or, at least, a lot of tasks) that could be almost completely automated away. It's probably not even a capability issue, but one of job security on the employee side and a lack of *waves hands vaguely* on the employer side.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470953"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470953" href="https://news.ycombinator.com/vote?id=37470953&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>So true!!! :D :D :D<p>If Excel stops working, financial institutions around the world would collapse.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37470812"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470812" href="https://news.ycombinator.com/vote?id=37470812&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I think for a lot of purposes, the internet kind of took over.<p>VB was great if you needed to do something limited to a single machine.</p><p>These days, we want data to be available across machines which requires using a network, and the default network is the internet.</p><p>If I'm going to be using the internet anyway, I can knock up something in HTML + JS + firebase/whatever data store, and have an application that works on any platform, and is accessible from anywhere in the world. You <i>might</i> need slightly more technical knowledge, but not so much that you can't have a simple CRUD app running in a day or so of work.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472722"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472722" href="https://news.ycombinator.com/vote?id=37472722&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span><i>You might need slightly more technical knowledge, but not so much that you can't have a simple CRUD app running in a day or so of work.</i><p>Well that’s quite a professional bubble you live in. Web dev is truly a frog in a boiling water.</p><p>If my VB/Delphi/Access/PIC buddy who made various apps and hardware back in the day asked me for a platform and I advised him to use what HN praises as “simple”, then pretty sure he’ll never contact me with it again.</p><p>I mean, yeah, <i>CRUD</i> is not hard to do by tutorial. But he will laugh at my CRUD explanation, because he never ever thought about <i>implementing</i> input &lt;&gt; data channels. It’s akin to positioning heads above a cylinder to fetch a database record.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470920"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470920" href="https://news.ycombinator.com/vote?id=37470920&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I can write HTML/CSS/JS and a few back-end languages like PHP in my sleep but there's no way I could hand-code a web app as fast as I could a desktop app in 1997 using VB.<p>I would LOVE it if I could, though.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472214"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472214" href="https://news.ycombinator.com/vote?id=37472214&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Have you tried <a href="https://anvil.works/" rel="nofollow noreferrer">https://anvil.works</a>? (I'm a founder!)<p>It's quite explicitly VB-esque (only using Python), and having a single paradigm rather than stitching together several different programs speeds things up even for those who can write HTML/JS/CSS in their sleep. (And of course, you can drop out to JS/CSS/HTML if you want.) Overall I think the development speed is comparable to VB6.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472470"><td></td></tr>
                  <tr id="37472543"><td></td></tr>
            <tr id="37472104"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472104" href="https://news.ycombinator.com/vote?id=37472104&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Same. I’m very good at cranking out Tailwind + React UIs, but nothing touches the productivity I had with VB6 or even early C# and Winforms.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37471885"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471885" href="https://news.ycombinator.com/vote?id=37471885&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I've never written VB, but I can bang out a React UI in jsxstyle like nobody's business.<p>You've piqued my curiosity.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472159"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37472159" href="https://news.ycombinator.com/vote?id=37472159&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Development in VB6 was basically dragging and dropping controls onto a window or dialog box, setting properties on them, and then filling in snippets of code to tie things together.<p>And "control" here means anything from simple labels and buttons up to database connections and embedded COM objects.</p><p>Literally anybody could bang out a simple Windows .exe like nobody's business.</p><p>It would have been really cool if Microsoft included it in the OS like they used to do with QBasic.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472012"><td></td></tr>
                        <tr id="37471167"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471167" href="https://news.ycombinator.com/vote?id=37471167&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Yep, this is the right answer. I was there, 23 years ago programming VB6, making apps that were wrapped up in InstallShield, burnt to a CD, and then mailed to our customers. We did do some web things with Apache and some C++ apps interfaced via CGI. But it seemed that overnight Java and Servlets came about and made CGI-bin obsolete and slow as molasses. Add in JSPs and Struts and you had fully functional web apps that were fast and scalable.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472419"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472419" href="https://news.ycombinator.com/vote?id=37472419&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Does no-one remember VBScript and Active Server Pages (ASP). It was Visual basic as a server-side language, positioned to challenge PHP.<p>VB made it to the web :-)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472759"><td></td></tr>
                  <tr id="37472229"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472229" href="https://news.ycombinator.com/vote?id=37472229&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Yup, that was also my take. I used to write 'on box' and you communicated between components using Com+, and when you switched to http, there was no reason to stick with just VB6.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37470944"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470944" href="https://news.ycombinator.com/vote?id=37470944&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt; VB was great if you needed to do something limited to a single machine.<p>VB6 used to work with oracle across network.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472555"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472555" href="https://news.ycombinator.com/vote?id=37472555&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Same here, we did a VB6 project that allowed the customer to chat with their remote mortgage advisor using a webcam, around '98.<p>The client was supposed to go to a local branch of the bank and then connect to the banks HQ.</p><p>I have also wondered why the software industry with the arrival of Internet went away from all these excellent tools. Not just VB6, but remember all the 4GL and model driven development tools. All gone and never really replaced.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471108"><td></td></tr>
                <tr id="37472129"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37472129" href="https://news.ycombinator.com/vote?id=37472129&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>More common, in my experience, was the 2 tier app where a (very) fat client directly talked to the db, and all the businesses and data access logic was intermixed with UI event handlers and some stored procedures.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37471386"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471386" href="https://news.ycombinator.com/vote?id=37471386&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Don't forget that VB.NET was a thing.  I worked on a web-based client management system that used VB.NET on the back-end.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37472245"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472245" href="https://news.ycombinator.com/vote?id=37472245&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>It still is a thing. VB.net is a CLR language just like C#. You can use a tool to translate from one to the other and back.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37471291"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471291" href="https://news.ycombinator.com/vote?id=37471291&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I think the answer is: because Microsoft let it. I'm a big fan of modern .NET, but my biggest complaint is that Microsoft views, and always has, the CLR as the C# Language Runtime and not the Common Language Runtime.<p>For example, see the relationship between F# and C#. The CLR is constantly getting features that are only to support features in C#, leaving F# in a position where they either don't get the feature, can't add the feature, or begrudgingly add the feature to keep up compatibility with C#, which is something it does take seriously. But this has the effect of "dirtying up" the F# language by either adding features that don't really belong in the language or keeping features out.</p><p>The other thing is that C# consistently adds features to itself that are inspired by F#, since F# already implements these features on the CLR, thus showing their viability. So what happens is that C# continually approaches a more bloated language with a subset of it being a poor copy of F#. But then F# gets dragged along towards having a small subset of C# in it for compatibility purposes. So it's simultaneously making both languages worse.</p><p>Even the iron languages project that lead to IronPython, IronRuby, etc. was a bit of a Trojan horse to test out and exercise the CLR and .NET with no intention of ever providing long-term support for those projects. The DLR, which was implemented to support those, appears to be just maintained by a skeleton crew of people invested in it, probably by those interested in keeping IronPython up and running.</p><p>I do not understand why Microsoft takes this approach. It is myopic, shows a misunderstanding of their own technology in the CLR, and ultimately turns C# into another C++, leave dead languages and projects in the wake.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472399"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472399" href="https://news.ycombinator.com/vote?id=37472399&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>To be fair VB.net originally had dynamic capabilities that C# lacked so at least it possessed features that made its existence justifiable.<p>However the extreme changes required to go from VB6 -&gt; VB.net made it all kind of moot, might as well rewrite.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472295"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472295" href="https://news.ycombinator.com/vote?id=37472295&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Interestingly enough there are a small handful of things VB.NET and C++/CLI can do which C# can't, which enable the former languages to have better interoperability with COM interfaces.<p>That was with .NET Framework and I'm not sure what the story is today with .NET Core aka .NET 5+.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472371"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472371" href="https://news.ycombinator.com/vote?id=37472371&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>They eventually added optional names parameters which made the interop problem much better for C# compared to VB.<p>C# wasn't as good still but you no longer had to put earlier params at their default value (which is what made it unusable).</p><p>C++/CLI will probably remain king of interop though being a Frankenstein combination of the .NET runtime and C++.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472493"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37472493" href="https://news.ycombinator.com/vote?id=37472493&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>The other headache was indexed properties, but I don't think that's changed.<p>C++/CLI is no man's land and I tried my best to avoid it; and so far, I've managed; sometimes after-the-fact.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37471934"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471934" href="https://news.ycombinator.com/vote?id=37471934&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>The F# situation is not comparable to VB.NET.<p>VB.NET was just C# semantics with a VB like syntax. From the beginning it didn't serve any purpose except to make VB Fans feel slightly more at home.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472734"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472734" href="https://news.ycombinator.com/vote?id=37472734&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Related but off-topic:  Nothing I see today compares to the productivity that I saw with Lotus Notes, Dbase 3 and 4, Paradox, Microsoft Access, and a few other things from that era.  There are some really good SAAS offerings that target this space, but none of them seem as dominant as I would expect.  There was a time that if you wanted a simple application that could be covered with 3-4 tables and 5-6 views, any of the things I mentioned above could handle it.  You could explain the business problem and hand a developer a book on any of the above technologies and expect to have a working product a month later.  Today, this isn't really true.  It does seem we've gone backwards a bit...</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37470852"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470852" href="https://news.ycombinator.com/vote?id=37470852&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I used VB all the way from 1.0 to 6.0.<p>And when VB.NET came out came out in 2002, that was exactly when all the types of GUI-database projects VB6 was used for professionally, started being built in PHP/MySQL/HTML/CSS instead. The switch would have happened anyways, but the fact that VB.NET wasn't backwards-compatible made it really easy to switch since you were going to have to learn/build something new anyway -- otherwise there probably would have been a somewhat longer transition period. Microsoft really shot themselves in the foot (but the web benefited).</p><p>And then on the hobbyist/personal side, that's also basically when casual developers switched from building fun Windows apps to building fun websites.</p><p>So I'd mark it up entirely to web programming replacing it on both sides.</p><p>As for what a replacement might look like, Google had created App Maker (2016-2020) that got replaced by AppSheet (2020-present), which is the closest I've found for the drag-and-drop GUI/database aspect of VB6. But those have been very much geared towards business development, not kids learning programming. Maybe some parents here can chime in on what their kids are learning to program in?</p><p>[1] <a href="https://en.wikipedia.org/wiki/Google_App_Maker" rel="nofollow noreferrer">https://en.wikipedia.org/wiki/Google_App_Maker</a></p><p>[2] <a href="https://en.wikipedia.org/wiki/AppSheet" rel="nofollow noreferrer">https://en.wikipedia.org/wiki/AppSheet</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471053"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471053" href="https://news.ycombinator.com/vote?id=37471053&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Good explanation. Hobbiest switched. Because IIS and Asp.net run very well with VB.NET. but who could afford that compared to PHP.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37471209"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471209" href="https://news.ycombinator.com/vote?id=37471209&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>The bigger pain point, from memory, was having an ODBC-compliant driver so the OS
 could actually talk to your DB. That basically meant MSSQL if you wanted to co-exist happily with early dotnet.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37470764"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470764" href="https://news.ycombinator.com/vote?id=37470764&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Microsoft accidentally killed it moving to .NET and the increasingly stupid GUI libraries they offered up.  Silverlight, 32 bit native, winforms etc and etc.<p>Web development meanwhile went bonkers and VB was a poor cousin to C# very suddenly.</p><p>Its a shame, VB was not for purists but it was very productive.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471028"><td></td></tr>
                <tr id="37471366"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471366" href="https://news.ycombinator.com/vote?id=37471366&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I don't think they did. They said increasingly so. They're just getting at Microsoft continually re-inventing the wheel and then abandoning that wheel when it comes to their GUI libraries. There's WinForms -&gt; WPF -&gt; UDP -&gt; WinUI, Xamarin Forms -&gt; .NET MAUI, and then the evolution of Avalonia and Uno as third-parties trying to step in. And all of those options still exist! You literally have a minimum of 8 GUI options, at least on Windows. There are of course more by external parties.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37471978"><td></td></tr>
                        <tr id="37471790"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471790" href="https://news.ycombinator.com/vote?id=37471790&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Agreed, .NET (well, C# specifically) was my go to for anything with a GUI until they tried moving on from Winforms/WPF and the way forward seemed to become a large undefined mess.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37471424"><td></td></tr>
                  <tr id="37472685"><td></td></tr>
            <tr id="37472598"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472598" href="https://news.ycombinator.com/vote?id=37472598&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Visual Basic was mostly used for LOB applications.  That stuff mostly migrated to the web.  When businesses decided to move those applications to the web, they had a choice between C# and VB.NET (if they stuck to the MS stack).  C# ultimately won.  Most of the old VB developers that are still around have converted over to being C# developers now.<p>When everything was a desktop app then the choice was C++ or VB and there were a lot of situations where VB "won".  Today C# does everything VB used to do but better on the desktop and non-performance sensitive applications are increasingly using Electron anyway.</p><p>Performance sensitive desktop app = C++
Windows focused non-performace sensitive desktop app = C#
Cross platform non-performace sensitive desktop app = JS/TS in Electron
Web app = anything but VB
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472350"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472350" href="https://news.ycombinator.com/vote?id=37472350&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>The FOSS community could do SO much better if they wanted to, with reactive web tech, typescript instead of basic, a project file format that's easy to work with in Git, Android support, etc.<p>Despite all this talk about no-code, it seems like all we have now is like, a CMS that lets you embed Google maps, but if you want to do anything more you have to use code.</p><p>Maybe it's just that end users usually don't need to build anything anymore, there's almost always an professionally made app for everything.</p><p>I've tried many times to "Build the app you want to see in the world" and almost every time the result is I decide that living with and working around the imperfections of what's out there is more practical than building and maintaining anything by myself in hopes of the the very small chance people notice and it becomes A Thing.</p><p>Perhaps better dev tools like VB aimed at one off software like that could change the equation, and if the tools existed we'd all find uses for them?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472445"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472445" href="https://news.ycombinator.com/vote?id=37472445&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt;  with reactive web tech, typescript<p>I very much doubt. Actually reactive frameworks such as react and languages that transpile in other languages such as trypescript are the reason they couldn't. These overcomplicated, seemingly well architected, tools are in fact just a fractal of poor design.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472591"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472591" href="https://news.ycombinator.com/vote?id=37472591&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Not just VB, but also things like Borland Delphi.<p>25 years ago, you could visually compose a UI using standardized components, including advanced concepts like a layout manager. You could do data-binding visually by navigating a linked database. You can write logic/events just by double-clicking a button and the event is created. Here you'd write your code which would typically be pretty easy because all contextual objects are readily available.</p><p>Sure enough, I understand that the above development model also has its limitations and doesn't serve all common modern needs. But still, it's pretty pathetic what we ended up with. Our tool chains are much more complicated and we program at a lower abstraction level whilst requiring a laundry list of skills.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471664"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471664" href="https://news.ycombinator.com/vote?id=37471664&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>The web happened. VB was great for builing stand alone desktop applications, possibly with database integration. But they never managed to deliver a similar slick and self-contained solution for building web-based solutions. It didn’t help that the migration from vb6 to vb.net was too painful, but the root cause was the demand for business desktop apps disappeared.<p>The infamous WebForms API was supposed to bring the same gui builder paradigm to the web, but the developer experience was not as great.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471277"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471277" href="https://news.ycombinator.com/vote?id=37471277&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I started my career with VB/VBA/Access, but got burned by one of their non-backward compatible upgrades (I don't remember which versions) that derailed an important project.  This was around the time that Java was what the cool kids were using, and I was very amenable to trying open source after that experience, and I never looked back.<p>It was a great experience though, especially for a self-taught beginner long before code academies and YouTube.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471948"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471948" href="https://news.ycombinator.com/vote?id=37471948&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>People want more these days. With VB you could write a CRUD application that saved data to a database and ran some reports. But now people want the database to pull in multiple feeds from other businesses via API, people want it to be available on the web, it needs to have a self service component so the customer can also log in, at the back end the database is connected to several other systems with daily feeds coming in and out. Shit is just much more complicated these days.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37472586"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472586" href="https://news.ycombinator.com/vote?id=37472586&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>VB6 could do all these things and more, in the 90's I built just the right small parts in C++ to glue things together, and we could do multi-processing, resilient multi-server setups, and much much more. 
Serving HTTP for some always-limiting browser was an obvious easy sidenote, ie. what your mantra "available on the web" wraps in silver, but leveraging the full power of the desktop and OS was what power users actually wanted, and would still want now if the apps were there.
I think some people at Microsoft realized VB had the potential to canibalize a lot of what they wanted as their proprietary backyard, and they killed the baby before it would grow into more of a mass movement. 
Now they succeeded, many that would have been educated into owning their computers now mindlessly click on "Yes" and are lost in the adds panopticon, but then MS had to non-compete some of the pie off to Google and co...</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472566"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472566" href="https://news.ycombinator.com/vote?id=37472566&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Microsoft effectively killed VB because they decided everyone should be using .NET. While they created VB .NET in an attempt to make the transition palatable to VB programmers, this was a second-class citizen of the CLR, and more importantly too different from VB, more akin to an inferior C# with VB-like syntax than actual VB. It was widely decried by VB developers [0], and nick-named "Visual Fred" due to really being a different language than VB. Microsoft ignored that, and also didn't bring a VB-like experience to .NET (maybe with the exception of WinForms, I'm not too familiar). It didn't help that mainstream software development started drifting to the web, and later to mobile apps.<p>[0] <a href="https://classicvb.net/vfred/breaks.asp" rel="nofollow noreferrer">https://classicvb.net/vfred/breaks.asp</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470632"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470632" href="https://news.ycombinator.com/vote?id=37470632&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Are you talking about VB the language, or Visual Studio the IDE with its awesome GUI builder for WinForms (and not so awesome ones for the subsequent Windows GUI layers, forget what they're called).<p>VB is still around but desktop apps in general (and thus VS's GUI builder) largely gave way to web technologies invented outside Microsoft. The dev experience is definitely worse though. Visual Studio was sooooo nice and integrated.</p><p>I don't think this is really VB or NET's fault, Microsoft just kinda missed (or failed the fight against) the web transition. They were busy trying to make it coexist with Windows with seamless downloads like ClickOnce but ultimately simple web pages won out for their reach and ease of use, then mobile app stores came along, and now desktop apps are petty much dead except for niches.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471390"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471390" href="https://news.ycombinator.com/vote?id=37471390&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I remember, in some sense, Google (and also a group of anti Microsoft developers) encouraged web development as a strategy to break the Windows software monopoly.<p>So it's not that Microsoft missed it, it is that a big shift happened and it was specifically targeted against them. There's nothing they could have done, except to embrace it, and they did. Visual Basic Script existed and was very popular for a while. IE4 ruled the web.</p><p>The shift to web based technologies is a Google and an open source win. And it is also an inferior experience. That was the price we paid =)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471522"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471522" href="https://news.ycombinator.com/vote?id=37471522&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Even in the IE4 days, Microsoft was still very much "we'll only do as much web as necessary, but we can't lose our Windows fort... how can we sabotage this effort?". They were in full-on EEE mode[1] then and wanted to subvert, not join, the web. JScript, ActiveX, ClickOnce, IE's idiosyncracies, etc. were all Microsoft's own efforts to get around the webification of everything. They lost, not just because of Google but also Netscape, Mozilla/Phoenix, eBay, Craigslist, Amazon, Match, MapQuest, etc. Nobody wanted to build desktop apps anymore once they could effortlessly reach everyone via the web without having to be a Microsoft vassal.<p>Even ASP and IIS etc. insisted on having its own stack -- superior in some ways, but way less compatible and more expensive. Again their own doing. Free/cheap won out, I guess :)</p><p>[1] <a href="https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguish#Examples_by_Microsoft" rel="nofollow noreferrer">https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguis...</a></p><p>(edit: how the heck do you actually properly make hyperlinks on HN? I always struggle with this)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37470935"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470935" href="https://news.ycombinator.com/vote?id=37470935&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt; web pages won out for their reach and ease of use,<p>Your definition of "ease of use" is amazing. /s
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471078"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471078" href="https://news.ycombinator.com/vote?id=37471078&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Heh, point taken. But I'd say overall it's easier for grandma to go to Gmail to check her email than have to figure out what this "Netscrape Communicator" is, install it, set up her "pop three" from her "eyesp" and then deal with a million viruses.<p>Yeah, the web ain't perfect, but it did win... ads got worse, though =/
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37470741"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470741" href="https://news.ycombinator.com/vote?id=37470741&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Visual Basic 6 was very productive - yes.  But it was considered by many to be a toy language (it didn't support class inheritance for one).<p>VB.NET - at least initial versions - was about productive as C#, thus there was no incentive to use VB.</p><p>The thing that made VB6 super productive was it's form designer.  The .NET successor - WinForms designer - wasn't nearly as fast and capable (to this day, really).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472196"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472196" href="https://news.ycombinator.com/vote?id=37472196&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>From my perspective, Microsoft killed it.  It did not die on its own.  The upgrade path from VB6 to VB.NET was basically unusable.  You either stayed on VB6 or you rewrote the application.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37470684"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470684" href="https://news.ycombinator.com/vote?id=37470684&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt;&gt; I have yet to find a tool that can allow me to be as productive in so short a time as Visual Basic.<p>For me: Delphi or Lazarus
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472358"><td></td></tr>
                <tr id="37472474"><td></td></tr>
                  <tr id="37470688"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470688" href="https://news.ycombinator.com/vote?id=37470688&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>VB died because of .NET and VB.NET. The syntaxes were similar, but VB.NET was much closer to a "real programming language" in feeling and complexity than VB was, and that's not what anyone who used VB actually wanted.<p>Microsoft was more interested in developing .NET and C# to battle Java, and less interested in developing and promoting VB, their own successful and original product.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                      <tr id="37472465"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472465" href="https://news.ycombinator.com/vote?id=37472465&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>What made VB6 super-productive and quick to get up and running also made it terrible for long-term maintenance, because it hid too much of the details.<p>Then there were language warts like set versus let (strong vs weak pointers), for example, which was way above the paygrade of the average VB6 coders; and having to rely on the Win32 API anyway in order to start doing actual work and work around all of its limitaitons.</p><p>One thing VB6 did do right was forcing interface-based inheritance: Composition over class inheritance was seen as a weakness back then but it was proven as the right concept.</p><p>This is just my opinion!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471131"><td></td></tr>
            <tr id="37471134"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471134" href="https://news.ycombinator.com/vote?id=37471134&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span><i>&gt;nothing I have found compares to that development experience today. I would go so far as to say we've gone backwards in a big way.</i><p>I did Visual Basic 3.0 through VB 6.0 corporate development for a few years back in the 1990s.  The closest equivalent today for desktop apps <i>that still has Microsoft's focus on future innovation</i> is C# with Windows Forms.  (I downplay the "obvious" comparison of VB.NET to VB 6.0 because Microsoft already said they will "stop evolving" Visual Basic .NET -- so that's a technology dead end and will fall further and further behind the latest C# as the years go by.)</p><p>I personally don't experience that C#/Winforms has gone backwards from VB 6.0.  Workflow feels much the same as VB6:  Drag some GUI components like text boxes and buttons onto a form, code the controls' event handlers, build the exe.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471178"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471178" href="https://news.ycombinator.com/vote?id=37471178&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I've only done a small amount of VB programming, but from what I remember of it, WinForms + C# is lightyears ahead of VB.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37470665"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470665" href="https://news.ycombinator.com/vote?id=37470665&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>It’s simple: Corporate internal app development (mostly CRUD stuff, as you’d expect) was the bread-and-butter of VB, and it moved wholesale to HTML &amp; JavaScript.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472149"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472149" href="https://news.ycombinator.com/vote?id=37472149&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I recently wondered:<p>Why is there no VB-Like tool for building Electron - Apps using eg. React-Widgets &amp; Javascript as the scripting - language?</p><p>I'm not married to Electron vs. another similar, possibly more modern / less ressource hungry alternative; or another Frontend Framework.</p><p>But it seems to be it should be possible to do a VB-Style thing using these kinds of tools, Drag+Drop, and Javascript, most of the components should be available already...</p><p>And it could be a fun environment for prototyping, having fun, creating really bad games &amp; greeting card apps again like in the 90s/2000s etc etc.</p><p>It could be a fun learning environment for newbies while a powerful GUI builder for multiple platforms for experts..</p><p>Why isn't there such a thing? Would somebody please build this? :D
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470873"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470873" href="https://news.ycombinator.com/vote?id=37470873&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>VB (and BASIC in general) has a legacy of being a procedural language. In the late 90’s and early 2000’s, there was a massive push to leverage object oriented programming and design. The births of Java in the 90’s and then C# in the 2000’s were clear catalysts to abandon procedural programming.<p>Of course in .NET VB has nearly all of the OO capabilities that C# has, but I think most developers just decided to go “all-in” on object oriented programming and learn C# and graduate from their procedural past.</p><p>A lot of colleges taught Java and moving to it or C# in the workplace was a much more natural process.</p><p>There are BASIC alternatives and they are fairly strong offerings, but I think OO and functional programming are the standard today.</p><p>That means python, Rust, Golang, Ruby, C#, and Java are the mainstream languages.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37470891"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470891" href="https://news.ycombinator.com/vote?id=37470891&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Honestly, I was never a fan of Basic but I still don't understand why there isn't a GUI app builder as productive as the VB6-era tools were for any language.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37470969"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37470969" href="https://news.ycombinator.com/vote?id=37470969&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Devs don’t design user interfaces anymore. There are separate disciplines for user experience, graphic design, front-end development, API development, and data storage choices.<p>There’s no reason to have a drag and drop UI builder anymore.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472436"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37472436" href="https://news.ycombinator.com/vote?id=37472436&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>not everything is a team effort, and the desire espoused all over this thread for an equivalency clearly demonstrates that there is a 'market' there.<p>when some small developer is making a silly one-off app for a mom&amp;pop local store to facilitate a one-off kind of task they aren't interested in handing off work and splitting meager profits. not every company has the whole "front-end/back-end/devops/ux/design/management" paradigm going on.</p><p>the reality is that microsoft , a fairly litigious group of people, abandoned a concept for their own reasons; and the rest of the market doesn't exactly know where they can step in that minefield of offering equivalent features to a piece of software that is still on life support by a very very large/valuable/litigious company.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="37470421"><td></td></tr>
                <tr id="37470493"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470493" href="https://news.ycombinator.com/vote?id=37470493&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I think that’s the sense in which it died in that if you were going to switch to .NET you would probably also switch to C#.  The original VB had A GUI builder better than almost anything (even today!) but the UI builders in today’s visual studio support C# as well if not better than VB.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37470752"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37470752" href="https://news.ycombinator.com/vote?id=37470752&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>You bring up a good point, which is that I remember most the GUI builder.  Basic isn't really that great a language, all things considered.  But being able to quickly design and deploy GUI apps with VB was better than anything I've encountered since.<p>There are things like WebFlow and FlutterFlow but those tools feel clunky by comparison.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37470811"><td></td></tr>
                <tr id="37470924"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37470924" href="https://news.ycombinator.com/vote?id=37470924&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Netbeans java gui builder is the only builder I’ve seen come close to the vb gui builder. In fact I was a little disappointed moving from net beans to a “real Java ide” since their gui builders don’t exist or aren’t as good. But vb as a language is a lot better than Java for getting to a productive state for a novice.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37470710"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37470710" href="https://news.ycombinator.com/vote?id=37470710&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>And everything in that IDE was fast and super responsive. Today's IDE are more complex than ever and yet they lack in functionality that was present in VB6.0</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37470868"><td></td></tr>
                  <tr id="37470663"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37470663" href="https://news.ycombinator.com/vote?id=37470663&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I once had a job offer from a big company that developed exclusively in VB.NET, after transitioning away from VB6. Both for desktop and web. So it's still out there.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37470985"><td></td></tr>
            <tr id="37470848"><td></td></tr>
                <tr id="37470992"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37470992" href="https://news.ycombinator.com/vote?id=37470992&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Well Basic evolved a lot. Comparing 80s Basic on whatever hardware you imagine (think goto + Line Numbers) to 90s Microsoft VisualBasic (think event Händlers for UIs) is a similar jump than VisualBasic to VisualBasic.NET (writing OO code).<p>VB.NET is the same but so much more evolved. Also the runtime below was switched (surely for the better).</p><p>However, like outlined in other comments: VB.NET is a OOP language which handles procedural/functional code as a subset while VB of the 90s was purely procedural.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37471812"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471812" href="https://news.ycombinator.com/vote?id=37471812&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>With VB.NET, VB basically turned into C# with different syntax. I think a lot of people in that space might as well have been writing C#, so a lot of them switched. Then separately, perhaps almost simultaneously, the Microsoft ecosystem lost relevance.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37470442"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470442" href="https://news.ycombinator.com/vote?id=37470442&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>IMO, it didn't really make a smooth transition to the web. Webforms was the attempt but it was a leaky abstraction (iirc, it kept state for UI components but not variables so things got... weird) and kind of worked against the paradigms at the time (everything was a POST).</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37470795"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470795" href="https://news.ycombinator.com/vote?id=37470795&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>Yeah exactly, VB dying is sort of the CONVERSE of programmers adopting platforms, not languages - <a href="https://old.reddit.com/r/ProgrammingLanguages/comments/sk6w1c/programming_languages_as_biological_strategies/hvkxp4n/" rel="nofollow noreferrer">https://old.reddit.com/r/ProgrammingLanguages/comments/sk6w1...</a><p>- Why is JS popular?  Not because it's the best language, but because it's attached to the browser, and people want to deploy apps to browsers (Figma, etc.)</p><p>- Why is shell the 8th most popular language on Github, and the 6th fastest growing? [1]  Not because it's the best language, but because it's attached to the Unix kernel (specifically Linux, which has a ton of features).  Software in containers and virtual machines must talk to kernels.</p><p>So then the converse is</p><p>- Why is VB no longer popular?  Not because it's a worse language than it used to be (though maybe that's true), but the platform that it supported isn't as popular.</p><p>Like others said, it's probably popular for Excel and app automation, etc.  But today more apps are targeting web and mobile, not Windows desktop.</p><p>- Same answer with  Objective C and Swift -- people are using them to write apps for a platform.  And Kotlin/Android, etc.</p><p>[1] <a href="https://octoverse.github.com/2022/top-programming-languages" rel="nofollow noreferrer">https://octoverse.github.com/2022/top-programming-languages</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472264"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472264" href="https://news.ycombinator.com/vote?id=37472264&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I used VB 3.0 thru VB 6.0 and I echo the sentiment: those were fun and productive times.  Really solid integrated development environment centered around the desktop UI / form design / UI controls like buttons, textboxes, combo boxes, etc.<p>I also agree that the web changed everything and that is the major reason for the shift away.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472501"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472501" href="https://news.ycombinator.com/vote?id=37472501&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Popularity of a language is like a popularity of a soda - it's never about the inherent quality or features, but always about the company that stands behind it.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472369"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472369" href="https://news.ycombinator.com/vote?id=37472369&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I just recently wanted to spin up a simple CRUD UI over a simple DB schema and also thought of Visual Basic for the first time.  It seems so well suited to something like building operational tooling for the endless parade of internal APIs.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472093"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472093" href="https://news.ycombinator.com/vote?id=37472093&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt; despite all the advances in technology since then, nothing I have found compares to that development experience today<p>You are going to have to qualify this a lot more, because it is absolutely not true. Coding VB was...fine. Language features were primitive, even compared to what was available in the mainstream back then. The tooling was fully proprietary and expensive. Languages and IDEs today are 1000x better in every way.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472260"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472260" href="https://news.ycombinator.com/vote?id=37472260&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>It's the Python of yesteryear. Nothing about the language was great but the ecosystem was amazing.<p>CRUD app creation was almost completely point and click, VBA was easy and you could take data straight from a document and process it in complex ways, the database integrations were great and easy to use.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472224"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472224" href="https://news.ycombinator.com/vote?id=37472224&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Honestly it's because Microsoft sucked at marketing. There were so many confusing name choices and unclear development guidelines that people simply stopped using it. Even now developing for windows is a pain and that's because there is like 5 different ways to build an app there.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37471824"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471824" href="https://news.ycombinator.com/vote?id=37471824&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>You're probably speaking to VB plus Visual Studio RAD (Rapid Application Development aka "drag and drop" GUI programming) and you more or less still have that today with VB.NET and Windows Forms, with minor differences.<p>What killed the older VB6 and it's APIs, whose name I can't recall even though I developed for it in the 1990s, was .net coming along.</p><p>Be warned, it might just be easier to learn C# as it's more well-supported/documented, and VB.net seems like effectively a language dialect of C#.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37472421"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472421" href="https://news.ycombinator.com/vote?id=37472421&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Because you could only use it if you paid for it. If they had released it for free, as in beer, lots of people would have made wi does apps with it.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37470966"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470966" href="https://news.ycombinator.com/vote?id=37470966&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>vb didn't die, it evolved into vb.net, but the market place has shifted to C# for the most part.<p>What has died, is the maturity of microsofts tool set, they keep changing their concept/design/platform.</p><p>silverlight/wpf/uwp/winui2|3/etc..</p><p>vb was around for nearly 2 decades and had a very mature tool set, everything since then hasn't gotten nearly that sort of life span or dedication to tool sets.</p><p>Developing in visual studio now, is more like web dev in the 2000s, I can't tell you how often you have to go to the xaml and make correction or adjustments that the UI just can't get right, or just goes bonkers and can't render the UI at all until something is fixed.</p><p>It is really sad, because the power of those old drag and drop builders that just worked meant that prototyping and mocking up applications was much much faster.</p><p>now standing up a UI based project takes ages, I'll usually do a console application now, and are dumping results to a API or console.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471206"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471206" href="https://news.ycombinator.com/vote?id=37471206&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>People always worry that Microsoft is silverlighting (that is a verb now) MAUI. I think that would be the end of VisualStudio. The word Visual had a meaning. When they would give up MAUI in favor of React Native (like Office) or Blazor (like the popular opinion), why the hack someone would buy a VS license. And when the think they could again commercialize .NET itself, then .NET would be dead. Modern Java, Flutter and TypeScript would easily swallow their market shares. MAUI, Blazor and .NET are an awesome set of products if they would just put some more concentration in MAUI.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472396"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472396" href="https://news.ycombinator.com/vote?id=37472396&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>The development paradigm was great. I wish three were something similar for Python, having to switch to TCL/TK for UI stuff is extremely annoying.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37470692"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470692" href="https://news.ycombinator.com/vote?id=37470692&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I wonder the same thing about Apple's HyperCard. I used HyperCard a ton at school and side-projects. Visual Basic was a firmer and better step-up from that imho, less so as VBScript when it came to browsers.<p>As per other comments VBA is alive and well and I did many consulting gigs using that in Ireland in the late 1990s. I hope, in the name of all that is holy, that code isn't running still.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37470763"><td></td></tr>
            <tr id="37470719"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470719" href="https://news.ycombinator.com/vote?id=37470719&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I wonder about HyperCard as well.  I never used it but, from everything I've ever heard about it, it was also an amazing developer experience.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37470776"><td></td></tr>
                        <tr id="37470872"><td></td></tr>
            <tr id="37472092"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472092" href="https://news.ycombinator.com/vote?id=37472092&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I started my career with FoxPro for DOS, dabbled a bit with Visual FoxPro and then I found Visual Basic. Spent a lot of fun years with it before moving to ASP and web development in general. Not exactly sure what was the reason for its demise. I guess .NET killed it.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472720"><td></td></tr>
            <tr id="37470777"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470777" href="https://news.ycombinator.com/vote?id=37470777&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>There's still no better GUI toolkit out there than VB6 that I have used. It was amazing.<p>Problem is a lot of apps that would have been traditional LOB apps written in VB/C# have moved to the web so demand isn't there's clear advantages to C# as a language over VB.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37470858"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470858" href="https://news.ycombinator.com/vote?id=37470858&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Netbeans Java Swing GUI builder is far better. That's without factoring in the enormous third party component landscape that is available.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37471325"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471325" href="https://news.ycombinator.com/vote?id=37471325&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Oh man, I remember building GUI apps with it in college.  My professor only let me use it on the condition that I could fully explain what every component was doing.  I could.  I took the generated swing code and added a <i>massive</i> amount of comments, but even having to do that I was still an order of magnitude faster than every other CS student without.  A lot of my peers were super jealous that I had so much free time, but they weren't willing to invest time in their tools (gdb, perl, regex, sql, etc).  It was quite the force multiplier.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37472378"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472378" href="https://news.ycombinator.com/vote?id=37472378&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>I was a VB6 developer in the 1990s, and I actually wrote a pretty well known product with it (I bet most people didn't know it was VB6 underneath!).<p>When I got my first job in software engineering, it was when .Net was in beta.  Since I only knew VB, I chose to go with VB.Net and Windows Forms.</p><p>Well, that was quite a jarring introduction to actual object oriented programming!</p><p>This is the point when a lot of people surrendered.  The change was too much, there was no path forward for VB, and Microsoft wanted everyone on .Net.</p><p>I eventually switched to C# and we had a hybrid application for a while before it was all ported to C#.  It actually went on to become industry leading software in its space.</p><p>20+ years later, that doesn't seem to have been a bad choice.  Windows Forms is very simple, with a powerful drag-and-drop designer, double-click to hook up events, it's all very similar.  Yes, you have to understand object oriented programming much more than you would have under VB, but that's as close as you are going to get from Microsoft on the desktop.  They try to hide it as best they can.</p><p>I <i>still</i> use Windows Forms if I need to put together something that "just works" on Windows as a desktop app quickly.  It's unmatched for that.</p><p>There are newer technologies, such as WPF, WinUI3, Avalonia and others for .Net but they come with a complexity that would be above your typical use-case for VB6 back in the day.  The VB6 user who just wanted to get stuff done probably doesn't want to have to understand how to implement the MVVM pattern just to get a UI.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470978"><td></td></tr>
            <tr id="37471197"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471197" href="https://news.ycombinator.com/vote?id=37471197&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>The web is cross-platform and easier to deploy. The (non-?)existence of RAD tools for it vs VB pales in comparison to that advantage.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37470940"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470940" href="https://news.ycombinator.com/vote?id=37470940&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>We had plenty of amazing paradigms/development environments/holistic experiences which we've regressed from:<p>- LISP environments</p><p>- Smalltalk environments</p><p>- Symbolics genera</p><p>- Mesa and Cedar</p><p>- Apple's Newton</p><p>Besides things like Oberon...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471828"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471828" href="https://news.ycombinator.com/vote?id=37471828&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>New version of VB was VB.net.<p>People still used VB.net primarily as they were familiar with VB.</p><p>But the new projects gradually took over C#.net, as it sounded more cool.</p><p>VB.net died after a while.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471717"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471717" href="https://news.ycombinator.com/vote?id=37471717&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>How about some love for Quick BASIC? That was my big Christmas present once upon a time when the dinosaurs still roamed the earth.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37472167"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472167" href="https://news.ycombinator.com/vote?id=37472167&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>Oh you spoiled little children with your integrated development environment and in place editing. Real programmers used GW-Basic with consecutive line numbers.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37472287"><td></td></tr>
            <tr id="37470729"><td></td></tr>
                <tr id="37470922"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470922" href="https://news.ycombinator.com/vote?id=37470922&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>How many incompatible languages called VB-something did Microsoft create? There's at least the classic Visual Basic, VBA, VBScript, and VB.NET.<p>Although to be fair, the distinction between the language and libraries has never been very clear in BASIC variants.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37471884"><td></td></tr>
                <tr id="37472155"><td></td></tr>
                  <tr id="37471150"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37471150" href="https://news.ycombinator.com/vote?id=37471150&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>While I was never a big fan of VB, I agree that the developer experience has been getting worse for most languages since the early 1990s.<p>Do you have a top 3 list of things you miss?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471958"><td></td></tr>
            <tr id="37471439"><td></td></tr>
            <tr id="37471938"><td></td></tr>
            <tr id="37470717"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470717" href="https://news.ycombinator.com/vote?id=37470717&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>My personal reason, which is probably unique:<p>VB 1.0 was extremely buggy, making it unusable for my use cases.</p><p>VB 1.1, which fixed the bugs I was encountering, was a paid upgrade.</p><p>I switched to Linux and never looked back.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470644"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470644" href="https://news.ycombinator.com/vote?id=37470644&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I got the feeling that a lot of the corp/small business apps that used to be developed in VB and deployed to PCs have been replaced by Web-based apps.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37471070"><td></td></tr>
                <tr id="37471317"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37471317" href="https://news.ycombinator.com/vote?id=37471317&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>VB was first released in 1991. If you're certain that you remember it from the late 80's, then you may be thinking of QBASIC. I first learned to write code in QBASIC running on MS-DOS.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37471083"><td></td></tr>
            <tr id="37470859"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470859" href="https://news.ycombinator.com/vote?id=37470859&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>DotNet is meant to be the replacement.  The big problem though is that MS keeps failing to make a GUI framework that is quick-and-easy as VB forms were.  Winforms still exists, and it's only a <i>bit</i> clumsier than VB forms, but it's very old and not modern.  The more modern .NET gui-frameworks are much less user-friendly.<p>Linguistically, I think the successor to VB is Powershell.  It's the same mashup of inconsistent flags that let you swap between "this is a serious language" and "I'm smashing crap together" with tons of unexpected weird behavior, but instead of being a quick-and-dirty GUI app maker, it's a Shell.  Hardcore focus on being easy and productive but unforgivably warty.</p><p>As for VB itself, VB.Net just didn't offer much value distinct from C#, so most people who were coding in VB switched to C#.</p><p>So if you're an old longbearded MS LOB programmer who started before .NET, and you're still working in Microsoft LOB shops, you're probably doing similar stuff but with C#.  But realistically, you've probably also switched to Web.</p><p>And the lack of the VB-level ease-of-use in <i>web</i> technologies is a whole other story.  All the hoary mess of using a document-engine for a cross-platform application server makes it pretty untameable.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470656"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470656" href="https://news.ycombinator.com/vote?id=37470656&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>VB6 morphed into VB.Net but then the web took over and anything that could be accomplished as a desktop app was replaced by a web page.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37472097"><td></td></tr>
            <tr id="37470372"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470372" href="https://news.ycombinator.com/vote?id=37470372&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>There's still Visual Basic .NET, is it much different productivity wise? I assume the language has changed enough from classic VB.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37470403"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470403" href="https://news.ycombinator.com/vote?id=37470403&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><br><div>
                  <p><span>I honestly haven't used it because I thought they had retired it altogether.  The last time I did use it was about 20 years ago and found that they had changed it enough that I wasn't nearly as productive with it as, say, VB6.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37470474"><td></td></tr>
                        <tr id="37472294"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472294" href="https://news.ycombinator.com/vote?id=37472294&amp;how=up&amp;goto=item%3Fid%3D37470318"></a></center>    </td><td><p><span>&gt;"nothing I have found compares to that development experience today. I would go so far as to say we've gone backwards in a big way"<p>&gt;"I have yet to find a tool that can allow me to be as productive in so short a time as Visual Basic"</p><p>I have and do program in many languages. From my perspective - for type the of applications usually done in VB Delphi / Lazarus would run circles around it. Both productivity and performance wise. It is also possible to do things one simply can not accomplish in VB.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471211"><td></td></tr>
                <tr id="37472193"><td></td></tr>
                  <tr id="37472114"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Calculate the difference and intersection of any two regexes (322 pts)]]></title>
            <link>http://phylactery.org/antimirov/</link>
            <guid>37470285</guid>
            <pubDate>Mon, 11 Sep 2023 17:10:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://phylactery.org/antimirov/">http://phylactery.org/antimirov/</a>, See on <a href="https://news.ycombinator.com/item?id=37470285">Hacker News</a></p>
<div id="readability-page-1" class="page">
<pre><h2>enter some regular expressions!</h2>
<b>α</b> := 
<b>β</b> := 
<hr>
<b>   ~α</b> = <span id="not-alpha">ϕ</span>
<b>   ~β</b> = <span id="not-beta">ϕ</span>
<b>α &lt; β</b> = <span id="alpha-lt-beta">false</span>
<b>α = β</b> = <span id="alpha-eq-beta">true</span>
<b>α &gt; β</b> = <span id="alpha-gt-beta">false</span>
<b>α &amp; β</b> = <span id="alpha-and-beta">.*</span>
<b>α ^ β</b> = <span id="alpha-xor-beta">ϕ</span>
<b>α - β</b> = <span id="alpha-minus-beta">ϕ</span>
<hr>
<b>s</b> := 

<b>s ∈ α</b> = <span id="str-in-alpha">true</span>
<b>s ∈ β</b> = <span id="str-in-beta">true</span>
<hr>
<b>|α|</b> = <span id="alpha-card">0</span>
<b>|β|</b> = <span id="beta-card">0</span>
<hr>
<b>dfa(α)</b> has <span id="alpha-dfa">0</span> states

<b>dfa(β)</b> has <span id="beta-dfa">0</span> states

<hr>
<b>regex syntax</b>

  <b>.</b>         match any single character
  <b>xy</b>        concatenation: match <b>x</b> and then <b>y</b>
  <b>x|y</b>       alternation: match <b>x</b> or <b>y</b>
  <b>x*</b>        kleene star: match <b>x</b> zero-or-more times
  <b>(xyz)</b>     grouping: treat <b>xyz</b> as a single item (e.g. <b>(xyz)*</b>)
  <b>()</b>        an empty regex matches the empty string
  <b>x+</b>        kleene plus: match <b>x</b> one-or-more times (equivalent to <b>xx*</b>)
  <b>x?</b>        optional: optionally match <b>x</b> (equivalent to <b>(x|)</b>)
  <b>x{n}</b>      exponentiation: concatenate <b>x</b> to itself <b>n</b> times
  <b>x{m,n}</b>    repetition: concatenate <b>x</b> to itself between <b>m</b> and <b>n</b> times
  <b>[a-z0-9]</b>  grouping: match any single character in the group
  <b>[^a-z0-9]</b> negative grouping: match any single character <b>not</b> in the group
  <b>\c</b>        escaping: match the special character <b>c</b>
  <b>\u001a</b>    unicode escaping: match the corresponding UTF-16 character
  a, b, c   all other characters match themselves

<b>unsupported features</b>

  - anchors (e.g. <b>^</b>, <b>$</b>), <i>although <b>^</b> and <b>$</b> must still be escaped!</i>
  - zero-width assertions (e.g. <b>(?=...)</b>, <b>(?&lt;=...)</b>)
  - back references (e.g. <b>\1</b>, <b>\2</b>)
  - subgroup extraction
  - searching or partial matching
  - other flags that change behavior (e.g. case-insensitivity)

see <a href="https://github.com/non/antimirov">https://github.com/non/antimirov</a> for more information

by <a href="http://plastic-idolatry.com/erik/">eiríkr åsheim</a> (@d6 on <a href="https://twitter.com/d6">twitter</a> and <a href="https://mastodon.social/@d6">mastodon</a>)

    </pre>

    
    
    

    

    

    <!-- <script type="text/javascript" src="antimirov-web-fastopt.js"></script> -->

  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[WiFi can read through walls (234 pts)]]></title>
            <link>https://news.ucsb.edu/2023/021198/wifi-can-read-through-walls</link>
            <guid>37469920</guid>
            <pubDate>Mon, 11 Sep 2023 16:45:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.ucsb.edu/2023/021198/wifi-can-read-through-walls">https://news.ucsb.edu/2023/021198/wifi-can-read-through-walls</a>, See on <a href="https://news.ycombinator.com/item?id=37469920">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-history-node-id="21198">
      

    
                    <div><p><span><span><span><span lang="EN" xml:lang="EN"><span><span>Researchers in UC Santa Barbara professor Yasamin Mostofi’s lab have proposed a new foundation that can enable high-quality imaging of still objects with only WiFi signals. Their method uses the Geometrical Theory of Diffraction and the corresponding Keller cones to trace edges of the objects. The technique has also enabled, for the first time, imaging, or reading, the English alphabet through walls with WiFi, a task deemed too difficult for WiFi due to the complex details of the letters.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span></span></span></span></span></p>
<p><span><span><span><span lang="EN" xml:lang="EN"><span><span>For more details on this technology, check their video at <span><a href="https://www.youtube.com/watch?v=pvqL3gqGDeM">https://www.youtube.com/watch?v=pvqL3gqGDeM</a></span></span></span></span></span></span></span></p>
<p><span><span><span><span lang="EN" xml:lang="EN"><span><span><span>“Imaging still scenery with WiFi is considerably challenging due to the lack of motion,” said Mostofi, a professor of electrical and computer engineering. “We have then taken a completely different approach to tackle this challenging problem by focusing on tracing the edges of the objects instead.”&nbsp; The proposed methodology and experimental results appeared in the Proceedings of the 2023 IEEE </span></span></span></span><span lang="EN" xml:lang="EN"><span><span><span>National Conference on Radar</span></span></span></span><span lang="EN" xml:lang="EN"><span><span><span> (RadarConf) on June 21, 2023.</span></span></span></span></span></span></span></p>
</div>

                  

                  <div><p><span><span><span><span lang="EN" xml:lang="EN"><span><span>This innovation builds on previous work in the Mostofi Lab, which since 2009 has pioneered sensing with everyday radio frequency signals such as WiFi for several different applications, including crowd analytics, person identification, smart health and smart spaces. </span></span></span></span></span></span></p>
<p><span><span><span><span lang="EN" xml:lang="EN"><span><span>“When a given wave is incident on an edge point, a cone of outgoing rays emerges according to the Keller’s Geometrical Theory of Diffraction (GTD), referred to as a Keller cone,” Mostofi explained. The researchers note that this interaction is not limited to visibly sharp edges but applies to a broader set of surfaces with a small enough curvature.</span></span></span></span></span></span></p>
<p><span lang="EN" xml:lang="EN"><span><span>“Depending on the edge orientation, the cone then leaves different footprints (i.e., conic sections) on a given receiver grid. We then develop a mathematical framework that uses these conic footprints as signatures to infer the orientation of the edges, thus creating an edge map of the scene,”&nbsp; Mostofi continued. </span></span></span></p>
</div>

                  <div><p><span><span><span><span lang="EN" xml:lang="EN"><span><span>More specifically, the team proposed a Keller cone-based imaging projection kernel. This kernel is implicitly a function of the edge orientations, a relationship that is then exploited to infer the existence/orientation of the edges via hypothesis testing over a small set of possible edge orientations. In other words, if existence of an edge is determined, the edge orientation that best matches the resulting Keller cone-based signature is chosen for a given point that they are interested in imaging. </span></span></span></span></span></span></p>
<p><span><span><span><span lang="EN" xml:lang="EN"><span><span>“Edges of real-life objects have local dependencies,” said Anurag Pallaprolu, the lead Ph.D. student on the project. “Thus, once we find the high-confidence edge points via the proposed imaging kernel, we then propagate their information to the rest of the points using Bayesian information propagation. This step can further help improve the image, since some of the edges may be in a blind region, or <span>can be </span>overpowered by other edges that are closer to the transmitters.” Finally, once an image is formed, the researchers can further improve the image by using image completion tools from the area of vision.</span></span></span></span></span></span></p>
<p><span><span><span><span lang="EN" xml:lang="EN"><span><span><span>“It is worth noting that traditional imaging techniques result in poor imaging quality when deployed with commodity WiFi transceivers,” added Pallaprolu, “as the surfaces can appear near-specular at lower frequencies, thus not leaving enough signature on the receiver grid.” &nbsp;</span></span></span></span></span></span></span></p>
<p><span><span><span><span lang="EN" xml:lang="EN"><span><span><span>The researchers have also extensively studied the impact of several different parameters, such as curvature of a surface, edge orientation, distance to the receiver grid, and transmitter location on the Keller cones and their proposed edge-based imaging system, thereby developing a foundation for a methodical imaging system design. </span></span></span></span></span></span></span></p>
</div>

                  <div><p><span><span><span><span lang="EN" xml:lang="EN"><span><span><span>In the team’s experiments, three off-the-shelf WiFi transmitters send wireless waves in the area. WiFi receivers are then mounted on an unmanned vehicle that emulates a WiFi receiver grid as it moves.&nbsp; The receiver measures the received signal power which it then uses for imaging, based on the proposed methodology. </span></span></span></span></span></span></span></p>
<p><span lang="EN" xml:lang="EN"><span><span><span>The researchers have extensively tested this technology with several experiments in three different areas, including through-wall scenarios. In one example application, they developed a WiFi Reader to showcase the capabilities of the proposed pipeline. </span></span></span></span></p>
</div>

                  <div><p><span><span><span><span lang="EN" xml:lang="EN"><span><span><span>This application is particularly informative as the English alphabet presents complex details which can be used to test the performance of the imaging system. Along this line, the group has shown how they can successfully image several alphabet-shaped objects. In addition to imaging, they can further classify the letters. Finally, they have shown how their approach enables WiFi to image and read through walls by imaging the details and further reading the letters of the word “BELIEVE” through walls. They have furthermore imaged a number of other objects as well, showing that they can capture details previously not possible with WiFi. </span></span></span></span></span></span></span></p>
<p><span><span><span><span lang="EN" xml:lang="EN"><span><span>Overall, the proposed approach can open up new directions for RF imaging.</span></span></span></span></span></span></p>
</div>

                  

                  

      
  




            
      
            
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The meeting of the minds that launched AI (160 pts)]]></title>
            <link>https://spectrum.ieee.org/dartmouth-ai-workshop</link>
            <guid>37469849</guid>
            <pubDate>Mon, 11 Sep 2023 16:40:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/dartmouth-ai-workshop">https://spectrum.ieee.org/dartmouth-ai-workshop</a>, See on <a href="https://news.ycombinator.com/item?id=37469849">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-elid="2659955531" data-post-url="https://spectrum.ieee.org/dartmouth-ai-workshop" data-authors="Grace Solomonoff" data-headline="The Meeting of the Minds That Launched AI" data-page-title="The Meeting of the Minds That Launched AI - IEEE Spectrum"><p>The <a href="http://jmc.stanford.edu/articles/dartmouth/dartmouth.pdf" rel="noopener noreferrer" target="_blank">Dartmouth Summer Research Project on Artificial Intelligence</a>, held from 18 June through 17 August of 1956, is widely considered the event that kicked off AI as a research discipline. Organized by <a href="https://amturing.acm.org/award_winners/mccarthy_1118322.cfm" target="_blank">John McCarthy</a>, <a href="https://web.media.mit.edu/~minsky/" target="_blank">Marvin Minsky</a>, <a href="https://spectrum.ieee.org/claude-shannon-tinkerer-prankster-and-father-of-information-theory" target="_blank">Claude Shannon</a>, and <a href="https://en.wikipedia.org/wiki/Nathaniel_Rochester_(computer_scientist)" target="_blank">Nathaniel Rochester</a>, it brought together a few dozen of the leading thinkers in AI, computer science, and information theory to map out future paths for investigation.</p><p>A group photo [shown above] captured seven of the main participants. When the photo was <a href="https://spectrum.ieee.org/history-of-ai" target="_self">reprinted</a> in Eliza Strickland’s October 2021 article “The Turbulent Past and Uncertain Future of Artificial Intelligence” in <em><a href="https://spectrum.ieee.org/">IEEE Spectrum</a></em>, the caption identified six people, plus one “unknown.” So who was this unknown person?</p><h2>Who is in the photo?</h2><p>Six of the people in the photo are easy to identify. In the back row, from left to right, we see <a href="https://www.nytimes.com/2008/12/04/us/04selfridge.html" rel="noopener noreferrer" target="_blank">Oliver Selfridge</a>, Nathaniel Rochester, Marvin Minsky, and John McCarthy. Sitting in front on the left is <a href="http://raysolomonoff.com/index.html" rel="noopener noreferrer" target="_blank">Ray Solomonoff</a>, and on the right, Claude Shannon. All six contributed to AI, computer science, or related fields in the decades following the Dartmouth workshop.</p><p><img alt="Close up of a black and white photo of seven smiling men, sitting on a lawn." data-rm-shortcode-id="4c395bd647a036dd5677d8158e7eeb05" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/close-up-of-a-black-and-white-photo-of-seven-smiling-men-sitting-on-a-lawn.jpg?id=33603729&amp;width=980" height="2250" id="6557b" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/close-up-of-a-black-and-white-photo-of-seven-smiling-men-sitting-on-a-lawn.jpg?id=33603729&amp;width=980" width="3000"><small placeholder="Add Photo Caption...">In the back row from left to right are Oliver Selfridge, Nathaniel Rochester, Marvin Minsky, and John McCarthy. In front on the left is Ray Solomonoff; on the right, Claude Shannon. The identity of the person between Solomonoff and Shannon remained a mystery for some time.</small><small placeholder="Add Photo Credit...">The Minsky Family</small></p><p>Between Solomonoff and Shannon is the unknown person. Over the years, some people <a href="https://pemey.medium.com/whos-that-kid-laughing-with-high-socks-in-the-middle-of-summer-7801a34feeef" rel="noopener noreferrer" target="_blank">suggested</a> that this was <a href="https://en.wikipedia.org/wiki/Trenchard_More" target="_blank">Trenchard More</a>, another AI expert who attended the workshop.</p><p>I first ran across the Dartmouth group photo in 2018, when I was gathering material for <a href="https://raysolomonoff.com/" rel="noopener noreferrer" target="_blank">Ray’s memorial website</a>. Ray and I had met in 1969, and we got married in 1989; he <a href="https://www.nytimes.com/2010/01/10/science/10solomonoff.html" rel="noopener noreferrer" target="_blank">passed away</a> in late 2009. Over the years, I had attended a number of his talks, and I had met many of Ray’s peers and colleagues in AI, so I was curious about the photo.</p><p>I thought, “Gee, that guy in the middle doesn’t look like my memory of Trenchard.” So I called up Trenchard’s son Paul More. He assured me that the unknown person was not his father.</p><p>More recently, I discovered a letter among Ray’s papers. On 8 November 1956, Nat Rochester sent a short note and a copy of the photo to some colleagues: “Enclosed is a print of the photograph I took of the <a href="https://spectrum.ieee.org/topic/artificial-intelligence/">Artificial Intelligence</a> group.” He sent his note to McCarthy, Minsky, Selfridge, Shannon, Solomonoff—and Peter Milner.</p><p data-rm-resized-container="25%"><img alt="A typed letter with the photograph of the six men,  is addressed to six names from Nathaniel Rochester." data-rm-shortcode-id="29a714f8f61d2228ac93efd838df9fb6" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-typed-letter-with-the-photograph-of-the-six-men-is-addressed-to-six-names-from-nathaniel-rochester.jpg?id=33603732&amp;width=980" height="1375" id="1b532" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-typed-letter-with-the-photograph-of-the-six-men-is-addressed-to-six-names-from-nathaniel-rochester.jpg?id=33603732&amp;width=980" width="1338"><small placeholder="Add Photo Caption...">Several months after the workshop, Nathaniel Rochester sent a copy of the photo, along with this note, to six people.</small><small placeholder="Add Photo Credit...">Grace Solomonoff</small></p><p>So the unknown person must be Milner! This makes perfect sense. <a href="https://www.mcgill.ca/psychology/article/remembering-peter-m-milner-1919-2018" target="_blank">Milner</a> was working on neuropsychology at <a href="https://www.mcgill.ca/" target="_blank">McGill University</a>, in Montreal, although he had trained as an electrical engineer. He’s not generally lumped in with the other AI pioneers because his research interests diverged from theirs. Even at Dartmouth, he felt he was in over his head, as he wrote in his 1999 autobiography: “I was invited to a meeting of computer scientists and information theorists at Dartmouth College…. Most of the time I had no idea what they were talking about.”</p><p>In his fascinating autobiography, Milner writes about his work in radar development during World War II, and his switch after the war from nuclear-reactor design to psychology. His doctoral thesis in 1954, “<a href="https://escholarship.mcgill.ca/concern/theses/zc77sv01t" target="_blank">Effects of Intracranial Stimulation on Rat Behaviour</a>,” examined the effects of electrical stimulation on certain rat neurons, which became widely and enthusiastically known as “pleasure centers.”</p><p>This work led to one of Milner’s most famous papers, “<a href="https://psycnet.apa.org/record/1959-00249-001" target="_blank">The Cell Assembly: Mark II</a>,” in 1957. The paper describes how, when a neuron in the brain fires, it excites similar connected neurons (especially those already aroused by sensory input) and randomly excites other cortical neurons. Cells may form assemblies and connect with other assemblies. But the neurons don’t seem to exhibit the same snowballing behavior of atoms that leads to an exponential explosion. How neurons might inhibit this effect were among his ideas that led to new insights at the workshop.</p><p>Milner’s work contributed to the early development of artificial neural networks, and it’s why he was included in the Dartmouth meeting. There was considerable interest among AI researchers in studying the brain and neurons in order to reproduce its functions and intelligence.</p><p>But as Strickland notes in her October 2021 <em>Spectrum</em> article, a division was already forming in AI research. One side focused on replicating the brain, while the other was more interested in what the mind might do to directly solve problems. Scientists interested in this latter approach were also represented at Dartmouth and later championed the rise of symbolic logic, using heuristic and algorithmic processes, which I’ll discuss in a bit.</p><h2>Where Was the Photo Taken?</h2><p>Rochester’s photo from 1956 shows the left-hand side of Dartmouth Hall in the background. In 2006 Dartmouth convened a conference, <a href="https://en.wikipedia.org/wiki/AI@50" target="_blank">AI@50</a>, to celebrate the 50th anniversary of the AI gathering and to discuss AI’s present and future. Trenchard More, the person most often misidentified as the “unknown person” in Nat’s photo, met with the organizers, <a href="https://en.wikipedia.org/wiki/James_H._Moor" rel="noopener noreferrer" target="_blank">James Moor</a> and Carey Heckman, as well as Wendy Conquest, who was working on a movie about AI for the conference. None of the AI@50 organizers knew exactly where the 1956 meeting had taken place.</p><p>More led them across the lawn and to the left-hand side door of Dartmouth Hall. He showed them the rooms that were used, which in turn triggered an old memory. During the 1956 meeting, as More recalled in a <a href="http://raysolomonoff.com/dartmouth/misc/Trenchardinterview2011.pdf" rel="noopener noreferrer" target="_blank">2011 interview</a>, “Selfridge, and Minsky, and McCarthy, and Ray Solomonoff, and I gathered around a dictionary on a stand to look up the word <em>heuristic</em>, because we thought that might be a useful word.” On that 2006 tour of Dartmouth Hall, he was delighted to find that the dictionary was still there.</p><p>The word <em>heuristic</em> was invoked all through the summer of 1956. Instead of trying to analyze the brain to develop machine intelligence, some participants focused on the operational steps needed to solve a given problem, making particular use of heuristic methods to quickly identify the steps.</p><p>Early in the summer, for instance, Herb Simon and Allen Newell gave a talk on a program they had written, the <a href="https://ieeexplore.ieee.org/document/1056797" target="_blank">logic theory machine</a>. The program relied on early ideas of symbolic logic, with algorithmic steps and heuristic guidance in list form. They later won the 1975 Turing Award for these ideas. Think of heuristics as intuitive guides. The logic theory machine used such guides to initiate the algorithmic steps—that is, the set of instructions to actually carry out the problem solving.<br></p><h2>Who Wasn’t in the Photo</h2><p>There was one person who was at the Dartmouth Workshop from time to time but was never included in any of the lists of attendees: Gloria Minsky, Marvin’s wife.</p><p>But Gloria was definitely a presence that summer. Marvin, Ray, and John McCarthy were the only three participants to stay for the entire eight-week workshop. Everyone else came and went as their schedules allowed. At the time, Gloria was a pediatrics fellow at Children’s Hospital in Boston, but whenever she could, she would drive up to Dartmouth, stay in Marvin’s apartment, and visit with whoever was at the workshop.</p><p>Several years earlier, in the spring of 1952, Gloria had been doing her residency in pathology at New York’s Bellevue Hospital, when she began dating Marvin. Marvin was a Ph.D. student at Princeton, as was McCarthy, and the two were invited to Bell Labs for the summer to work under Claude Shannon. In July, just four months after their first meeting, Gloria and Marvin got married. Although Marvin was working nonstop for Shannon, Shannon insisted he and Gloria take a honeymoon in New Mexico.</p><p data-rm-resized-container="25%"><img alt="A letter from John McCarthy to Ray Solomonoff on Dartmouth College stationery." data-rm-shortcode-id="85ad1d73a223bb5735e7b0fe2c2fc67d" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-letter-from-john-mccarthy-to-ray-solomonoff-on-dartmouth-college-stationery.jpg?id=33603735&amp;width=980" height="1001" id="4a2b5" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-letter-from-john-mccarthy-to-ray-solomonoff-on-dartmouth-college-stationery.jpg?id=33603735&amp;width=980" width="620"><small placeholder="Add Photo Caption...">In March 1956, John McCarthy, one of the Dartmouth AI workshop’s organizers, invited Ray Solomonoff to the summer workshop in Hanover, N.H.</small><small placeholder="Add Photo Credit...">Grace Solomonoff</small></p><p>Four years later, McCarthy, Shannon, and Minsky, along with Nat Rochester, organized the <a href="http://jmc.stanford.edu/articles/dartmouth/dartmouth.pdf" target="_blank">Dartmouth workshop</a>. Gloria remembered a conversation between her husband and Ray, in which Marvin expressed a thought that later became one of his hallmarks: “You need to see something in more than one way to understand it.” In Minsky’s 2007 book <a href="https://www.simonandschuster.com/books/The-Emotion-Machine/Marvin-Minsky/9780743276641" rel="noopener noreferrer" target="_blank"><em>The Emotion Machine</em></a>, he looked at how emotions, intuitions, and feelings create different descriptions and provide different ways of looking at things. He tended to favor symbolic logic and deductive methods in AI, which he called “good old-fashioned AI.”</p><p>Ray, meanwhile, was focused on probabilities—the likelihood of something happening and predictions of how it might evolve. He later developed algorithmic probability, an early version of algorithmic information theory, in which each different description of something leads with a probabilistic likelihood (some more likely, some less likely) of a given outcome in the future. Probabilistic methods eventually became the underpinnings of machine learning.</p><p>These days, as chatbots enter the limelight, and compression methods are used more in AI, the value of understanding things in many ways and using probabilistic predictions will only grow in importance. That is, logic and probability methods are uniting. These in turn are being aided by new work on neural nets as well as symbolic logic. And so the photo that Nat Rochester took not only captured a moment in time for AI. It also offered a glimpse into how AI would develop.</p><p><em>The author thanks Gloria Minsky, Margaret Minsky,</em><em>Nicholas Rochester, Julie Sussman, Gerald Jay Sussman, and Paul More for their help and patience.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Show HN: Firefox addon to quarantine a tab to use offline with private data (148 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37469321</link>
            <guid>37469321</guid>
            <pubDate>Mon, 11 Sep 2023 16:06:17 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37469321">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="37472816"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472816" href="https://news.ycombinator.com/vote?id=37472816&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>We have our first bug bounty!<p>Thank you "dz2742" for finding out [1] existing connections including websockets are not terminated and has won 100 USD! This is exactly the type of exploit I was hoping to catch.</p><p>Now I have to figure out how to fix that :) And also think about refilling the bug bounty pool without becoming very poor very soon.</p><p><a href="https://github.com/matusfaro/quarantab/issues/2">https://github.com/matusfaro/quarantab/issues/2</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37469787"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37469787" href="https://news.ycombinator.com/vote?id=37469787&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><br><div>
                  <p><span>Cool idea!  I don't really picture myself using this, but I think this add-on is a great example of how great a browser Firefox is.  I'd be the first to critique Mozilla, and there are definitely things about Firefox I don't like (ex. Pocket, telemetry on by default), but overall I think it's an amazing product in that it allows for multiple levels of isolation (profiles, containers, private mode) and a level of control over them that Chromium either doesn't do as cleanly or doesn't do at all.  As an aside, the only thing I think Chromium does better is the debugging experience; I don't truly understand why Firefox thinks it shouldn't support debugging Node.js like Chromium does.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37469951"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37469951" href="https://news.ycombinator.com/vote?id=37469951&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>&gt; it allows for multiple levels of isolation<p>Yes! Chrome has a visually similar functionality to Firefox Containers hidden away behind a feature flag [1] at the moment. BUT under the hood it's simply just tab grouping with no isolation. I presume isolation is against Google's interests so we will never see this kind of feature.</p><p>As for Firefox's API, the Contextual Identities API [2] that allows you to create/delete containers is amazing and easy to work with as a dev. And it works out-of-the-box, it doesn't need the companion addon Multi-Account Containers (MAC) [3] which really should've been part of Firefox in my opinion.</p><p>1. chrome://flags/#tab-groups-save</p><p>2. <a href="https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/API/contextualIdentities" rel="nofollow noreferrer">https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/Web...</a></p><p>3. <a href="https://addons.mozilla.org/en-US/firefox/addon/multi-account-containers/" rel="nofollow noreferrer">https://addons.mozilla.org/en-US/firefox/addon/multi-account...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471275"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37471275" href="https://news.ycombinator.com/vote?id=37471275&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>Firefox's containers are useless for privacy, given other enhancements of Firefox (e.g., Total Cookie Protection). And as far as "isolation", privacy or security are concerned, Chrome's profiles are actually superior due to ability to have different extensions and history per profile. Chrome's extensions in general still have superior security (e.g., activate on click or only for certain websites), so sometimes different profiles aren't even needed.<p>Chrome's Profiles are also remembered when you "install an app" (SSB/PWA), so you could have "apps" started in their own profiles.</p><p>Firefox's containers are only useful if you want multiple logins to the same service in the same browser window. But I never found that usecase to be very compelling.</p><p>Firefox's containers are an often lauded feature, and I don't understand why, given the integration issues or general awkwardness. It's probably a reminiscence of the "Facebook container" extension, which was a bandaid until better site isolation was implemented.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472905"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37472905" href="https://news.ycombinator.com/vote?id=37472905&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><br><div>
                  <p><span>A counter anecdote is that I have the exact opposite use case. I don't share my computer with other users, so I've never needed something like profiles. Firefox containers are great for keeping different sites, especially those notorious for tracking (e.g. Amazon, Google, LinkedIn) completely isolated from each other or from general browsing. Plus, the extension that allows for creating temporary containers is great for one-off visits to e-commerce sites without needing to switch to a new private/incognito window. I'm not sure I've ever wanted my extensions isolated by container/profile, that seems like it would hinder productivity. Same for history. It's great having all my history commingled, especially if I want to find something from 30 tabs ago.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37471780"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37471780" href="https://news.ycombinator.com/vote?id=37471780&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>&gt; Chrome's profiles are actually superior due to ability to have different extensions and history per profile<p>Interesting attack vector I haven't thought about which could leak information out of a network-locked Firefox Container. It would be under an assumption you have either:</p><p>1. A malicious extension installed (you have a much worse problem in this case)</p><p>2. A side-effect of an existing extension that leaks information to the outside world. (e.g. translate a part of a page, lookup a word in a dictionary, pre-fetch some images...)</p><p>&gt; Firefox's containers are only useful if you want multiple logins</p><p>I think there are valid use cases for both Containers and Profiles. You can go down the list to have more and more isolation as needed:</p><p>- Grouping tabs to stay organized, no isolation</p><p>- Firefox containers, same browser window, shared history &amp; extensions</p><p>- Chrome profiles, almost complete isolation within same browser (different processes)</p><p>- Separate browser instances</p><p>- Separate devices
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37471798"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37471798" href="https://news.ycombinator.com/vote?id=37471798&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><br><div>
                  <p><span>Firefox has profiles too.  Containers are for use within a profile.  You keep saying that containers aren't useful but you don't elucidate on <i>how</i> they are useless for privacy or <i>what</i> integration issues exist.  I don't know how to interpret 'general awkwardness.'  Can you fill in some details?</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37471987"><td></td></tr>
                <tr id="37472249"><td></td></tr>
                              <tr id="37473191"><td></td></tr>
                <tr id="37473550"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37473550" href="https://news.ycombinator.com/vote?id=37473550&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>This extension is still very cool.<p>+1 to Cyberchef, its awesome. If you really have qualms about the URL its trivial to re-host / serve it to yourself offline.</p><p>My favorite part is whole recipe feature (Cyberchef builds a URL with the configured processors you use to process data).</p><p>I find myself using that a ton to share XPath / JPAth expressions type work with sample data to others by sharing that URL.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37474291"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37474291" href="https://news.ycombinator.com/vote?id=37474291&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><br><div>
                  <p><span>I know this won't land well, and certainly it's a good option, but there's a terrific and hilarious irony in someone saying "I don't really trust the third parties with my non public data" And you're like, yeah use the one tool that's built and maintained by a literal spy agency.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37469420"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37469420" href="https://news.ycombinator.com/vote?id=37469420&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>While I applaud your effort and thinking of privacy issues, I will continue to do these in a terminal and Python REPL for all the reasons you bring up.<p>It would certainly be nice to get something ala F-droid for free software extensions like yours (which guarantees source code matches built package IIRC), as a response to your question 3.</p><p>I am sure one can create an alternative extensions store in FF and change some config in about:config to use it, though it's likely non-trivial.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37469649"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37469649" href="https://news.ycombinator.com/vote?id=37469649&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>Agreed, and to be honest, this extension is more for myself as I would be extremely skeptical if someone else made it especially with the permissions it requires.<p>It would probably be more successful as a feature added to an existing trusted extension such as Temporary Containers.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37470673"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37470673" href="https://news.ycombinator.com/vote?id=37470673&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>&gt; I will continue to do these in a terminal and Python REPL for all the reasons you bring up.<p>Do you have a way to prevent terminal utilities from accessing the network?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37471080"><td></td></tr>
                <tr id="37472108"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37472108" href="https://news.ycombinator.com/vote?id=37472108&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>Interesting options, wasn't aware of those.<p>The only minor counter-argument would be laziness as a security threat: the more difficult you make the process, the more likely the user will skip seemingly useless steps, thus compromising security.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="37470379"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470379" href="https://news.ycombinator.com/vote?id=37470379&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>&gt; Submit my name and birthdate to estimate my date of death<p>Totally off topic, but curious how this works? Nationality and life expectancy? Sex at birth? Assassins for hire?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37474316"><td></td></tr>
            <tr id="37470934"><td></td></tr>
                <tr id="37471496"><td></td></tr>
                <tr id="37471606"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37471606" href="https://news.ycombinator.com/vote?id=37471606&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>Oh my god, this is scary. I don't <i>want</i> to live until the 2060s, lol. It terrifies me to think about how the world will be then...<p>But anyway, thanks for the link!</p><p>For the curious, this is their methodology:</p><p>&gt; Remaining life expectancy at specific age (in days) was obtained by interpolating (spline) the 5 yearly/duration age-specific period life expectancies.</p><p>&gt; Population.io uses official demographic data produced by the United Nations and published in the World Population Prospects
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="37470504"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37470504" href="https://news.ycombinator.com/vote?id=37470504&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>Cool add on! Thanks for this. It's a use case I've often thought about, for the purposes you mention. I wish there was a built in permission to disable AJAX after page load. Bad for ads, I guess.<p>2. Exploit idea (not trying for the bounty, just thinking aloud). I wonder if a website could play background music (or a video) with stenographically encoded data, then another tab could listen to it with microphone permissions on and decode it that way. I'm thinking like a fake video conferencing site, or malicious telephony how-to doc that deals with API calls and such and links to a fake password hasher that then plays the audio for the first tab to hear. Convoluted, I know, just an idea.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472030"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37472030" href="https://news.ycombinator.com/vote?id=37472030&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>&gt; built in permission to disable AJAX after page load<p>Interesting, but consider this is a cat-and-mouse game. If you are the only one using this trick it may work for you, but I assume would be easy to overcome. (e.g. keep the page loading forever or until ads are loaded. Have the ads be J-free after page load, ...)</p><p>&gt; website could play background music ... another tab could listen</p><p>You would need mic access from the other tab, but yes. If you send it over high enough frequency you wouldn't even hear it. You would just have a visual feedback that the tab is playing music.</p><p>On a side-note, I recall there was some kind of hardware device pairing (maybe Chromecast?) that used data over voice to establish that you are physically near the other device.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472845"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37472845" href="https://news.ycombinator.com/vote?id=37472845&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>&gt; On a side-note, I recall there was some kind of hardware device pairing (maybe Chromecast?) that used data over voice to establish that you are physically near the other device.<p>Yeah, that's pretty common in home smart devices. Looks like Google patented one version and Sonos has their implementation too. In my experience it works better than Bluetooth, especially in (2.4 GHz) noisy environments
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472896"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37472896" href="https://news.ycombinator.com/vote?id=37472896&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>Funny that you say Sonos.<p>I also remember there was a data-over-voice library called "chirp.io" which now redirects to Sonos homepage. Now I know why they acquired them :)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37472964"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37472964" href="https://news.ycombinator.com/vote?id=37472964&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>I wonder if it's also part of the patent battle they got in with Google over smart speaker stuff.<p>Side rant: It's so sad, to this day Google Assistant works terribly on my Sonos system, and it's a major reason I'm reluctant to further buy into their ecosystem. And Sonos's own assistant doesn't even support Spotify, last I checked. Their whole UX is... not great. I really wanted to work there and maybe try to fix some of the issues I experience as a user, but they rejected me. Alas.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                                    <tr id="37472771"><td></td></tr>
                <tr id="37472870"><td></td></tr>
                  <tr id="37472086"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37472086" href="https://news.ycombinator.com/vote?id=37472086&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>that's a nice idea<p>the same way you can silence the sound output of a tab you should have as simple and reliable a tool to stop communication to either the network, os or both.</p><p>i'd love a tool to see which tabs are talking with each other also
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37473003"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37473003" href="https://news.ycombinator.com/vote?id=37473003&amp;how=up&amp;goto=item%3Fid%3D37469321"></a></center>    </td><td><p><span>&gt; love a tool to see which tabs are talking with each other also<p>Cool idea but probably not that useful and difficult to accomplish. There are many ways to communicate that could be grouped into:</p><p>1. tab -&gt; tab (same domain)</p><p>2. tab -&gt; tab (different domain)</p><p>3. tab -&gt; server -&gt; tab</p><p>For #1, there are so many ways to transfer information it would be hard to detect and differentiate whether it's communication or just happens to be using the same resource. (e.g. one sets a cookie or local storage and the other one reads it)</p><p>For #3, it would be impossible to detect. Especially if detection is an issue, both tabs could be communicating with unrelated servers which talk with each other.</p><p>For #2, it would be the only interesting one as there is limited options (e.g. Broadcast Channel), but at the same time I assume rarely used in practice. And if detection is an issue, they would switch to #3 to avoid it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37473044"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UK air traffic control meltdown (144 pts)]]></title>
            <link>https://jameshaydon.github.io/nats-fail/</link>
            <guid>37468600</guid>
            <pubDate>Mon, 11 Sep 2023 15:19:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jameshaydon.github.io/nats-fail/">https://jameshaydon.github.io/nats-fail/</a>, See on <a href="https://news.ycombinator.com/item?id=37468600">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
      <p>Comments on <a href="https://www.reddit.com/r/programming/comments/16fhmuq/a_deep_dive_into_the_bug_that_caused_the_uk_air/?utm_source=share&amp;utm_medium=web2x&amp;context=3">reddit</a></p>
<p>On 28 August 2023 <em>NATS</em>, the UK's air traffic control operator, suffered a
<strong>major</strong> technical incident. The BBC reports that more than <a href="https://www.bbc.com/news/uk-66685349">2000 flights were
cancelled</a> and the cost has been estimated
at over <em>£100 million</em> GBP. The incident probably affected hundreds of thousands
of people.</p>
<p>The press initially reported the cause was a faulty flight plan: <em>UK air traffic
control: inquiry into whether French error caused failure</em> (The Times) and in
typical Mail Online reporting style: <em>"Did blunder by French airline spark air
traffic control issues? Officials probe if a single badly filed travel plan
caused UK's entire flight-control system to collapse in worst outage for a
decade - with 1,000 flights cancelled and chaos set to last DAYS"</em>.</p>
<p>So what happened? These are notes on my reading of the incident report:</p>
<blockquote>
<p>NATS Major Incident Preliminary Report<br>
Flight Plan Reception Suite Automated (FPRSA-R) Sub-system Incident 28th August 2023 <br>
<a href="https://publicapps.caa.co.uk/docs/33/NERL%20Major%20Incident%20Investigation%20Preliminary%20Report.pdf">pdf</a>.</p>
</blockquote>
<p><em>NATS</em> is a "public-private" company in the UK that is responsible for all of
the UK's air traffic control:</p>
<blockquote>
<p>Air Traffic Control (ATC) is the provision and operation of a safe system for
controlling and monitoring aircraft.
[..] <br>
aircraft [..] are required to file a flight plan.
[..] <br>
ATC ensures that aircraft are safely separated laterally and vertically.</p>
</blockquote>
<h2 id="what-went-wrong">What went wrong</h2>
<blockquote>
<p>The start of the sequence of events leading to the incident can be tracked
back to the point at which a flight plan was entered into the flight planning
system.</p>
<p>[Airlines] submit the plan into Eurocontrol’s Integrated Initial Flight Plan
Processing System (IFPS).
[..]</p>
<p>If the submitted flight plan is accepted by IFPS, i.e. it is compliant with
IFPS defined parameters [...] this is sufficient for a flight to depart with
local ATC approval. The flight plan will be sent from IFPS to all relevant
ANSPs who need to manage the flight.
[..]</p>
<p>Within the NATS En-route operations at Swanwick Centre, the data is passed to
FPRSA-R. The FPRSA-R sub-system exists to convert the data received from IFPS
(in a format known as ATS Data Exchange Presentation, ADEXP) into a format
that is compatible with the UK National Airspace System (NAS). NAS is the
flight data processing system which contains all of the relevant airspace and
routings.
[..]</p>
<p>FPRSA-R has a primary and backup system monitored both by dedicated Control
and Monitoring (C&amp;M) systems and also an aggregated central C&amp;M system.
Further resilience is provided by NAS storing 4 hours of previously filed
flight data to allow the operation to continue in the event of the loss of
automatic processing of flight data.
[..]</p>
<p>In addition to the technical resilience provided by backup systems, and the 4
hours of stored flight data, there is operational contingency available to
allow safe service to continue. This is provided through the ability to input
flight data manually, directly into NAS using a manual input system.</p>
</blockquote>
<p>To summarise:</p>
<ul>
<li>Flight plans are first submitted to a European-wide authority <em>IFPS</em>.</li>
<li>If a plan is accepted, the flight is cleared for takeoff.</li>
<li><em>NATS</em> requires the flight plan be transferred to them at least 4 hours before
the aircraft is due to enter UK airspace. This is supposed to give NATS a
4-hour window to be able to fix any problems in processing flight plans.</li>
<li>It seems that there is also probably some process which <em>delays</em> flight plans
until <em>close</em> to the deadline (see below). This might be to avoid congesting
the system with flight plans too early, or lots of plans that may later
change. Still, this results in flight plans being received by NATS sometimes
<em>hours</em> after the flight has taken off.</li>
</ul>
<blockquote>
<p>The NATS ATC System was operating normally.
[..]</p>
<p>[On] 28 August the airline submitted an ICAO4444 compliant flight plan into
Eurocontrol’s flight planning distribution system, IFPS.</p>
</blockquote>
<p>ICAO stands for <a href="https://en.wikipedia.org/wiki/International_Civil_Aviation_Organization" title="wikipedia">International Civil Aviation
Organization</a>, a United Nations agency.
An ICAO4444 flight plan looks like this:</p>
<pre><code><span>(FPL-TTT123-IS
</span><span>-C550/L-SDE1E2GHIJ3J5RWZ/SB1D1
</span><span>-KPWM1225
</span><span>-N0440F310 SSOXS5 SSOXS DCT BUZRD
</span><span>DCT SEY DCT HTO J174 ORF J121
</span><span>CHS EESNT LUNNI1
</span><span>-KJAX0214 KMCO
</span><span>-PBN/A1L1B1C1D1O1T1 NAV/Z1 GBAS
</span><span>DAT/1FANS2PDC SUR/260B RSP180
</span><span>DOF/220501 REG/N123A SEL/BPAM
</span><span>CODE/A05ED7)
</span></code></pre>
<p>Such messages are in a format that is meant to be read by machines, but also by
humans if necessary. The format is spec'd over many many pages of PDF, but is
roughly:</p>
<pre><code><span>( FPL-ACID-Flt Rules Flight Type
</span><span>- AC Type/Wake Cat-
</span><span>Equip.&amp;Capability
</span><span>- Departure EOBT
</span><span>- Speed Altitude [sp] Route
</span><span>- Destination ETE [sp]
</span><span>Alternate(s)
</span><span>- Other Information )
</span></code></pre>
<p>The route part (in this example: <code>N0440F310 SSOXS5 SSOXS DCT BUZRD DCT SEY DCT HTO J174 ORF J121 CHS EESNT LUNNI1</code>) encodes an overall speed (here <code>N0440</code>
meaning <code>440 knots</code>), an overall altitude (here <code>F310</code> which means "Flight
Level 310" which means <code>310 × 100 ft</code> (can also be in <code>km</code>)), and a sequence of
waypoints (referenced by name) separated by a description of how to get from the
previous waypoint to the next one, usually by referencing a "known route" by
name.</p>
<blockquote>
<p>The flight plan was accepted by IFPS
[..] <br>
the aircraft was cleared to depart at 04:00.
[..]</p>
<p>At 08:32 the flight plan was received by NATS’ FPRSA-R sub-system from
Eurocontrol’s IFPS system. This is consistent with the 4 hour rule mentioned
above. The purpose of the FPRSA-R software is to extract the UK portion of the
flight plan [..]</p>
<p>The flight plans delivered to FPRSA-R by IFPS are converted from [..] ICAO4444
to [..] ADEXP. ADEXP is a European-wide flight plan specification that
includes, amongst other data, additional geographical waypoints within the
European region specific to the route of a flight. For flights transiting
through UK airspace, rather than landing in the UK, this will include
additional waypoints outside of UK airspace required for its onward journey.
Following this conversion the ADEXP version of a flight plan includes, amongst
other aspects, the original ICAO4444 flight plan plus an additional list of
waypoints and other data.</p>
</blockquote>
<p>ADEXP looks like this:</p>
<pre><code><span>-TITLE IFPL
</span><span>-BEGIN ADDR
</span><span>  -FAC LIIRZEZX
</span><span>  [...]
</span><span>  -FAC LYZZEBXX
</span><span>-END ADDR
</span><span>-ADEP EDDF
</span><span>-ADES LGTS
</span><span>-ARCID KIM1
</span><span>-ARCTYP B738
</span><span>-CEQPT SDGRWY
</span><span>-EOBD 170729
</span><span>-EOBT 0715
</span><span>-FILTIM 280832
</span><span>-IFPLID AT00441635
</span><span>-ORIGIN -NETWORKTYPE SITA -FAC FRAOXLH
</span><span>-SEQPT C
</span><span>-WKTRC M
</span><span>-PBN B2
</span><span>-REG DABHM
</span><span>-SEL KMGJ
</span><span>-SRC FPL
</span><span>-TTLEET 0210
</span><span>-RFL F330
</span><span>-SPEED N0417
</span><span>-FLTRUL I
</span><span>-FLTTYP S
</span><span>-ROUTE N0417F330 ANEKI8L ANEKI Y163 NATOR UN850 TRA UP131 RESIA Q333
</span><span>BABAG UN606 PEVAL DCT PETAK UL607 PINDO UM603 EDASI
</span><span>-ALTRNT1 LBSF
</span><span>-BEGIN RTEPTS
</span><span>  -PT -PTID EDDF -FL F004 -ETO 170729073000
</span><span>  -PT -PTID RID -FL F100 -ETO 170729073404
</span><span>  -PT -PTID ANEKI -FL F210 -ETO 170729073856
</span><span>  -PT -PTID NEKLO -FL F214 -ETO 170729073911
</span><span>  -PT -PTID BADLI -FL F248 -ETO 170729074118
</span><span>  -PT -PTID PABLA -FL F279 -ETO 170729074348
</span><span>  -PT -PTID HERBI -FL F308 -ETO 170729074624
</span><span>  -PT -PTID NATOR -FL F330 -ETO 170729074911
</span><span>  -PT -PTID TITIX -FL F330 -ETO 170729075154
</span><span>  -PT -PTID TRA -FL F330 -ETO 170729075323
</span><span>  -PT -PTID ARGAX -FL F330 -ETO 170729080055
</span><span>  -PT -PTID RESIA -FL F330 -ETO 170729080731
</span><span>  -PT -PTID UNTAD -FL F330 -ETO 170729081243
</span><span>  -PT -PTID DIKEM -FL F330 -ETO 170729081627
</span><span>  -PT -PTID ROKIB -FL F330 -ETO 170729081824
</span><span>  -PT -PTID BABAG -FL F330 -ETO 170729082816
</span><span>  -PT -PTID PEVAL -FL F330 -ETO 170729082916
</span><span>  -PT -PTID PETAK -FL F330 -ETO 170729091754
</span><span>  -PT -PTID PINDO -FL F330 -ETO 170729093322
</span><span>  -PT -PTID EDASI -FL F165 -ETO 170729094347
</span><span>  -PT -PTID LGTS -FL F000 -ETO 170729095713
</span><span>-END RTEPTS
</span><span>-SID ANEKI8L
</span><span>-ATSRT Y163 ANEKI NATOR
</span><span>-ATSRT UN850 NATOR TRA
</span><span>-ATSRT UP131 TRA RESIA
</span><span>-ATSRT Q333 RESIA BABAG
</span><span>-ATSRT UN606 BABAG PEVAL
</span><span>-DCT PEVAL PETAK
</span><span>-ATSRT UL607 PETAK PINDO
</span><span>n -ATSRT UM603 PINDO EDASI
</span></code></pre>
<p>You can read about ADEXP in the <a href="https://www.eurocontrol.int/sites/default/files/2023-06/eurocontrol-released-specification-adexp-3-4.pdf" title="pdf">official spec</a>.
Some notable fields (page 48):</p>
<table><thead><tr><th>Adexp Primary Field</th><th>Kind</th><th>Syntax</th><th>Semantic</th></tr></thead><tbody>
<tr><td>route</td><td>b</td><td><code>'-' "ROUTE" {LIM_CHAR}</code></td><td>Complete ICAO Field 15 information containing speed, RFL and route (conforming to the syntax given in Ref. [3]).</td></tr>
<tr><td>rtepts</td><td>c</td><td><code>'-' "BEGIN" "RTEPTS" { pt I ad / vec} '-' "END" "RTEPTS"</code></td><td>List of route points. May also contain an aerodrome identifier.</td></tr>
</tbody></table>
<p>In the example, we have the ICAO route:</p>
<pre><code><span>-ROUTE N0417F330 ANEKI8L ANEKI Y163 NATOR UN850 TRA UP131 RESIA Q333 BABAG UN606 PEVAL DCT PETAK UL607 PINDO UM603 EDASI
</span></code></pre>
<p>(9 waypoints, 11 if you add the start and end waypoints)</p>
<p>Visually, routes look like:
<img src="https://jameshaydon.github.io/nats-fail/flight_plan.png" alt="some route" title="A flight plan route"></p>
<p>(You can play around with flight plans at
<a href="https://flightplandatabase.com/">flightplandatabase.com</a>, a website for people
who like playing with flight simulators)</p>
<p>We can indent the "route" parts between the waypoints in the ICAO plan to make
things clearer:</p>
<pre><code><span>N0417F330
</span><span>  ANEKI8L 
</span><span>  ANEKI 
</span><span>    Y163
</span><span>  NATOR
</span><span>    UN850
</span><span>  TRA
</span><span>    UP131
</span><span>  RESIA
</span><span>    Q333
</span><span>  BABAG
</span><span>    UN606
</span><span>  PEVAL
</span><span>    DCT
</span><span>  PETAK
</span><span>    UL607
</span><span>  PINDO
</span><span>    UM603
</span><span>  EDASI
</span></code></pre>
<p>E.g. <code>ANEKI Y163 NATOR</code> means "go from waypoint <code>ANEKI</code> to waypoint <code>NATOR</code> via
the route <code>Y163</code>". <code>DCT</code> means "direct".</p>
<p>The <code>ADEXP</code> format has more waypoints, along with more precision about altitude and estimated time at each waypoint:</p>
<pre><code><span>-BEGIN RTEPTS
</span><span>-PT -PTID EDDF  -FL F004 -ETO 170729073000
</span><span>-PT -PTID RID   -FL F100 -ETO 170729073404
</span><span>-PT -PTID ANEKI -FL F210 -ETO 170729073856
</span><span>-PT -PTID NEKLO -FL F214 -ETO 170729073911
</span><span>-PT -PTID BADLI -FL F248 -ETO 170729074118
</span><span>-PT -PTID PABLA -FL F279 -ETO 170729074348
</span><span>-PT -PTID HERBI -FL F308 -ETO 170729074624
</span><span>-PT -PTID NATOR -FL F330 -ETO 170729074911
</span><span>-PT -PTID TITIX -FL F330 -ETO 170729075154
</span><span>-PT -PTID TRA   -FL F330 -ETO 170729075323
</span><span>-PT -PTID ARGAX -FL F330 -ETO 170729080055
</span><span>-PT -PTID RESIA -FL F330 -ETO 170729080731
</span><span>-PT -PTID UNTAD -FL F330 -ETO 170729081243
</span><span>-PT -PTID DIKEM -FL F330 -ETO 170729081627
</span><span>-PT -PTID ROKIB -FL F330 -ETO 170729081824
</span><span>-PT -PTID BABAG -FL F330 -ETO 170729082816
</span><span>-PT -PTID PEVAL -FL F330 -ETO 170729082916
</span><span>-PT -PTID PETAK -FL F330 -ETO 170729091754
</span><span>-PT -PTID PINDO -FL F330 -ETO 170729093322
</span><span>-PT -PTID EDASI -FL F165 -ETO 170729094347
</span><span>-PT -PTID LGTS  -FL F000 -ETO 170729095713
</span><span>-END RTEPTS
</span></code></pre>
<p>(21 waypoints)</p>
<p>We can mark which of the ADEXP waypoints have a corresponding waypoint in the ICAO plan (with a <code>+</code>) and which are implicit (with a <code>|</code>): </p>
<pre><code><span>EDDF   |
</span><span>RID    |
</span><span>ANEKI  + 
</span><span>NEKLO  |
</span><span>BADLI  |
</span><span>PABLA  |
</span><span>HERBI  |
</span><span>NATOR  +
</span><span>TITIX  |
</span><span>TRA    +
</span><span>ARGAX  |
</span><span>RESIA  +
</span><span>UNTAD  |
</span><span>DIKEM  |
</span><span>ROKIB  |
</span><span>BABAG  +
</span><span>PEVAL  |
</span><span>PETAK  +
</span><span>PINDO  +
</span><span>EDASI  +
</span><span>LGTS   |
</span></code></pre>
<p>Note that the ICAO waypoints do not contain the start and end, since in the
original ICAO format these are specified in other fields (so it would waste
space to list them again in this list).</p>
<blockquote>
<p>The ADEXP waypoints plan included two waypoints along its route that were
geographically distinct but which have the same designator.</p>
</blockquote>
<p>This means there were two lines like:</p>
<pre><code><span>-PT -PTID RESIA -FL F330 -ETO 170729080731
</span></code></pre>
<p>that had the same <code>PTID</code> string like <code>"RESIA"</code>.</p>
<blockquote>
<p>Although there has been work by ICAO and other bodies to eradicate non-unique
waypoint names there are duplicates around the world. In order to avoid
confusion latest standards state that such identical designators should be
geographically widely spaced. In this specific event, both of the waypoints
were located outside of the UK, one towards the beginning of the route and one
towards the end; approximately 4000 nautical miles apart.</p>
</blockquote>
<p>4000 nautical miles is 7408km. Here is an arc of that length on the globe:
<img src="https://jameshaydon.github.io/nats-fail/4000-nautical-miles.png" alt="4000 nautical miles on a globe"></p>
<blockquote>
<p>Once the ADEXP file had been received, the FPRSA-R software commenced
searching for the UK airspace entry point in the waypoint information per the
ADEXP flight plan, commencing at the first line of that waypoint data. FPRSA-R
was able to specifically identify the character string as it appeared in the
ADEXP flight plan text.</p>
</blockquote>
<p>The programming style is very imperative. Furthermore, the description sounds
like the procedure is working directly on the textual representation of the
flight plan, rather than a data structure parsed from the text file. This would
be quite worrying, but it might also just be how it is explained.</p>
<blockquote>
<p>Having correctly identified the entry point, the software moved on to search
for the exit point from UK airspace in the waypoint data.</p>
<p>Having completed those steps,</p>
</blockquote>
<p>This part of the code identified <code>entry</code> and <code>exit</code> waypoints to UK airspace in
the list of <code>ADEXP</code> waypoints.</p>
<blockquote>
<p>FPRSA-R then searches the ICAO4444 section of
the ADEXP file.</p>
</blockquote>
<p>It seems at this point, having identified the entry and exit points from the
list of ADEXP waypoints, it will try to extract the UK portion of the flight plan from the ICAO route.</p>
<blockquote>
<p>It initially searches from the beginning of that data, to find
the identified UK airspace entry point. This was successfully found. Next, it
searches backwards, from the end of that section, to find the UK airspace exit
point. This did not appear in that section of the flight plan so the search
was unsuccessful. As there is no requirement for a flight plan to contain an
exit waypoint from a Flight Information Region (FIR) or a country’s airspace,
the software is designed to cope with this scenario.</p>
<p>Therefore, where there is no UK exit point explicitly included, the software
logic utilises the waypoints as detailed in the ADEXP file to search for the
next nearest point beyond the UK exit point. This was also not present.</p>
<p>The software therefore moved on to the next waypoint.</p>
</blockquote>
<p>OK, so I think this is what is going on, the situation looked something like
this:</p>
<pre><code><span>           4       2        8         5              1           9
</span><span>ICAO:  F------Q--------T--------O-----------P---------------Y--------U
</span><span>
</span><span>ADEXP: F   S  Q    C   T   A    O  E  X     P   W   B   Q   Y        U
</span><span>                       UK  UK   UK UK UK    UK  UK
</span></code></pre>
<p>Here the ICAO route has waypoints (represented by capital letters) separated by
known routes (numbers). On the bottom we have the ADEXP waypoints. The ADEXP
waypoints that are located in the UK airspace are marked with <code>UK</code>.</p>
<ul>
<li>The software has identified:
<ul>
<li><code>entry</code>: waypoint <code>T</code></li>
<li><code>exit</code>: waypoint <code>W</code>
in the ADEXP waypoints.</li>
</ul>
</li>
<li>The software finds waypoint <code>T</code> in the ICAO flight plan.</li>
<li>The software <em>does not</em> find waypoint <code>W</code> in the ICAO flight plan.</li>
<li>The software therefore takes the next waypoint in the ADEXP list, so <code>B</code>, and
tries to find it too, and also does not find it.</li>
<li>So it does this again, taking waypoint <code>Q</code>, and it <em>does</em> find it, but at the
<em>start</em> of the ICAO flight plan, before the plane even enters the UK.</li>
</ul>
<blockquote>
<p>This search was successful as a duplicate identifier appeared in the flight
plan.</p>
</blockquote>
<p>What should the software have done? Well, <code>Q</code> is clearly <em>not</em> the waypoint we
are searching for, we are searching for waypoint <code>Y</code>, since <code>[T, O, P, Y]</code> is
the smallest segment of the ICAO plan that contains all the UK waypoints.</p>
<p>It's important to note here that the original algorithm is buggy; it is perfectly
possible to unambiguously extract the UK portion of this example flight plan;
see <a href="https://jameshaydon.github.io/nats-fail/#how-to-code-this-properly">below</a>. And this is likely the case for the
flight plan that caused the meltdown too.</p>
<blockquote>
<p>Having found an entry and exit point, with the latter being the duplicate and
therefore geographically incorrect, the software could not extract a valid UK
portion of flight plan between these two points. This is the root cause of the
incident. We can therefore rule out any cyber related contribution to this
incident.</p>
</blockquote>
<p>It sounds like the exception was raised in a later portion of the code, which
converts the plan to an internal format for <em>NAS</em>. This part failed because the
identified entry/exit waypoints didn't even specify a valid segment of the ICAO
route.</p>
<blockquote>
<p>Safety critical software systems are designed to always fail safely. This
means that in the event they cannot proceed in a demonstrably safe manner,
they will move into a state that requires manual intervention.</p>
</blockquote>
<p>We are left wondering if, had the misidentified waypoint been in a more
plausible geographic location, the code might not have thrown an exception and
passed along wrong data to ATCOs.</p>
<blockquote>
<p>In this case the software within the FPRSA-R subsystem was unable to establish
a reasonable course of action that would preserve safety and so raised a
critical exception. A critical exception is, broadly speaking, an exception of
last resort after exploring all other handling options. Critical exceptions
can be raised as a result of software logic or hardware faults, but
essentially mark the point at which the affected system cannot continue.</p>
</blockquote>
<p>It sounds like the software was written thinking this exception would never
occur.</p>
<blockquote>
<p>Clearly a better way to handle this specific logic error would be for FPRSA-R
to identify and remove the message and avoid a critical exception. However,
since flight data is safety critical information that is passed to ATCOs the
system must be sure it is correct and could not do so in this case. It
therefore stopped operating, avoiding any opportunity for incorrect data being
passed to a controller. The change to the software will now remove the need
for a critical exception to be raised in these specific circumstances.</p>
<p>Having raised a critical exception the FPRSA-R primary system wrote a log file
into the system log. It then correctly placed itself into maintenance mode and
the C&amp;M system identified that the primary system was no longer available. In
the event of a failure of a primary system the backup system is designed to
take over processing seamlessly. In this instance the backup system took over
processing flight plan messages. As is common in complex real-time systems the
backup system software is located on separate hardware with separate power and
data feeds.</p>
<p>Therefore, on taking over the duties of the primary server, the backup system
applied the same logic to the flight plan with the same result. It
subsequently raised its own critical exception, writing a log file into the
system log and placed itself into maintenance mode.</p>
<p>At this point with both the primary and backup FPRSA-R sub-systems having
failed safely the FPRSA-R was no longer able to automatically process flight
plans. It required restoration to normal service through manual intervention.
The entire process described above, from the point of receipt of the ADEXP
message to both the primary and backup sub-systems moving into maintenance
mode, took less than 20 seconds. 08:32 therefore marks the point at which the
automatic processing of flight plans ceased and the 4 hour buffer to manual
flight plan input commenced. The steps taken to restore the FPRSA-R sub-system
are described in section 5 of this report.</p>
</blockquote>
<p>Then support teams tried to fix things, but unfortunately it took longer than
the 4 hours they had:</p>
<blockquote>
<p>The 1st Line support team were alerted to the incident through the C&amp;M systems
that directly monitor operational systems as well as through direct feedback
from the Operational teams using the FPRSA-R sub-system at the time. The
initial response for the team followed standard recovery processes using the
centralised C&amp;M systems to restart the sub-system. Following multiple attempts
to restore the service, which were unsuccessful, the 2nd Line engineering team
was mobilised and supported the on-site engineers remotely via video link.</p>
</blockquote>
<p><img src="https://jameshaydon.github.io/nats-fail/off-and-on-again.jpg" alt="have you tried turning it off and on again?"></p>
<blockquote>
<p>The on-call teams working remotely with the on-site engineering teams followed
a staged analysis, involving increasingly detailed procedures to attempt to
resolve the issue, none of which were successful. As per standard escalation
procedures, 2nd Line engineers were engaged to provide further access to
advanced diagnostics and logging capabilities.</p>
</blockquote>
<p>It doesn't say how long it took, but the manufacturer of the <code>FPRSA-R</code> system was
eventually called:</p>
<blockquote>
<p>Additional support was then requested from the Technical Design team and
sub-system manufacturer as 1st and 2nd Line support had been unable to restore
the service or identify the precise root cause, which was unusual. The
manufacturer was able to offer further expertise including analysis of
lower-level software logs which led to identification of the likely flight
plan that had caused the software exception. Through understanding which
flight plan had caused the incident the manufacturer was able to provide the
precise sequence of actions necessary to recover the system in a controlled
and safe manner.</p>
</blockquote>
<p>The system was eventually restored, but unfortunately the knock-on effects by
that point were already disastrous.</p>
<p>The manufacturer is an Austrian company, <a href="https://en.wikipedia.org/wiki/Frequentis">Frequentis
AG</a>:</p>
<blockquote>
<p>An FPRSA sub-system has existed in NATS for many years and in 2018 the
previous FPRSA sub- system was replaced with new hardware and software
manufactured by Frequentis AG, one of the leading global ATC System providers.
The manufacturer’s ATC products are operating in approximately 150 countries
and they hold a world-leading position in aeronautical information management
(AIM) and message handling systems.</p>
</blockquote>
<p>The "Nobody ever gets fired for hiring Accenture" defence.</p>
<p>We can find a few job ads related to air traffic control systems at Frequentis
AG on their <a href="https://jobs.frequentis.com/careers/SearchJobs/air?listFilterMode=1">careers
page</a>
Programming languages used: <code>Ada</code>, <code>C++</code>, <code>Java</code>, <code>Python</code>, with <code>Java</code> being
the most common. The code above sounds like it could have been written in any of
these languages, but Ada would at least be safer than the others in other ways.</p>
<h2 id="thoughts">Thoughts</h2>
<p>Things that went wrong:</p>
<ol>
<li>The software that processes flight plans (<code>FPRSA-R</code>) was written in a buggy
way.</li>
<li>The software and system are not properly tested.</li>
<li>The <code>FPRSA-R</code> system has bad <a href="https://en.wikipedia.org/wiki/Failure_mode_and_effects_analysis" title="wikipedia">failure modes</a></li>
</ol>
<h3 id="the-software-was-buggy">The software was buggy</h3>
<p>The software was incapable of extracting the UK portion of the ICAO flight plan,
even though the flight plan was apparently valid (at least according to IFPS).</p>
<ul>
<li>
<p>The procedure was very fiddly and failed for a silly reason.</p>
</li>
<li>
<p>Waypoint markers are not globally unique, but this is a known issue, so NATS
should make sure their systems are robust enough to handle it. <em>All other air
traffic control authorities have to deal with this</em>. NATS says the following
about this in the report:</p>
<blockquote>
<p>Although there has been work by ICAO and other bodies to eradicate
non-unique waypoint names there are duplicates around the world. In order to
avoid confusion latest standards state that such identical designators
should be geographically widely spaced. In this specific event, both of the
waypoints were located outside of the UK, one towards the beginning of the
route and one towards the end; approximately 4000 nautical miles apart.</p>
</blockquote>
<p>When waypoints with the same name are widely spaced, this makes flight plans
unambiguous, because successive waypoints in a flight plan cannot be too far
apart. They also mention possible actions they will take:</p>
<blockquote>
<p>The feasibility of working through the UK state with ICAO to remove the
small number of duplicate waypoint names in the ICAO administered global
dataset that relate to this incident.</p>
</blockquote>
<p>Waypoint names are clearly chosen to be short and snappy. Here's a sequence
from some flight plan I found: <code>KOMAL</code>, <code>ATRAK</code>, <code>SORES</code>, <code>SAKTA</code>, <code>ALMIK</code>,
<code>IGORO</code>, <code>ATMED</code>, etc. It's clear that the system has been designed so these
names can be communicated quickly, e.g. over radio, and that pilots and
air traffic controllers can become familiar with those on the routes they
usually fly. Changing the name of a waypoint can be a scary operation.
Uniqueness is obviously desirable, but it has to be balanced against other
considerations. Including this suggestion in the initial report feels like
NATS is trying to shift the blame onto ICAO.</p>
<p>Furthermore, I don't see why a flight plan can't include the same <em>geographic</em>
waypoint several times; for example for leisure flights or military exercises.
Taking off and landing at the same airport is definitely a thing (called a
"round-robin flight plan"). It doesn't sound like the <code>FPRSA-R</code> algorithm
would be very robust to that.</p>
</li>
</ul>
<p>NATS officials are trying to spin this as:</p>
<blockquote>
<p>An air traffic meltdown in Britain was caused by a "one in 15 million" event,
the boss of traffic control provider NATS said, as initial findings showed how
a single flight plan with two identically labelled markers caused the chaos.</p>
<p>"This was a one in 15 million chance. We've processed 15 million flight plans
with this system up until this point and never seen this before," NATS CEO
Martin Rolfe told the BBC, as airlines stepped up calls for compensation for
the breakdown. <a href="https://www.reuters.com/world/uk/uk-aviation-regulator-review-air-traffic-control-failure-2023-09-06/">Reuters</a></p>
</blockquote>
<p>The system was put in place in 2018, so what Martin Rolfe is saying here is that
this sort of thing only had a chance of occurring "once every 5 years", which is
apparently an acceptable frequency for having a complete air traffic control
meltdown.</p>
<h3 id="the-system-was-poorly-tested">The system was poorly tested</h3>
<p><a href="https://en.wikipedia.org/wiki/Fuzzing">fuzzing</a>, for example, may have
prevented this. By bombarding such a system with randomly generated flight
plans, you can see if any of them cause bad failure modes: a crashed system
where one doesn't know immediately what went wrong. By inspecting which sorts of
flight plans cause problems, it would become apparent that those with duplicate
waypoint identifiers in the ADEXP portion cannot be processed properly.</p>
<h3 id="the-fprsa-r-system-has-bad-failure-modes">The <code>FPRSA-R</code> system has bad failure modes</h3>
<p>All systems can malfunction, so the important thing is that they malfunction <em>in
a good way</em> and that those responsible are <em>prepared</em> for malfunctions.</p>
<p>A single flight plan caused a problem, and the entire <code>FPRSA-R</code> system crashed,
which means no flight plans are being processed at all. If there is a problem
with a single flight plan, it should be moved to a separate slower queue, for
manual processing by humans. NATS acknowledges this in their "actions already
undertaken or in progress":</p>
<blockquote>
<p>The addition of specific message filters into the data flow between IFPS and
FPRSA-R to filter out any flight plans that fit the conditions that caused the
incident.</p>
</blockquote>
<p>When <code>FPRSA-R</code> it did crash, it did so in an obscure way. This is a system
which <em>processes flight plans</em>, yet the relevant flight plan was only found in
"lower-level software logs". If there is an error processing a flight plan,
which brings down the whole system, a notification (including the flight plan)
should immediately be sent to some monitoring team.</p>
<p>NATS was also not prepared for an <code>FPRSA-R</code> system failure. The 1st
and 2nd Line support engineers were not able to locate, or did not think to
check, the low-level log files. This has been fixed:</p>
<blockquote>
<p>An operating instruction has been put in place to allow prompt recovery of the
FPRSA-R sub-system if the same circumstances recur. Each of the technical
operators have been trained to implement the new process. With enhanced
monitoring in place, additional engineering expertise will also be present to
oversee the activity.</p>
</blockquote>
<h3 id="possible-lack-of-formal-verification">Possible lack of formal verification</h3>
<p>As reddit user <code>DontWannaMissAFling</code> <a href="https://www.reddit.com/r/programming/comments/16fhmuq/comment/k02o6n8/?utm_source=share&amp;utm_medium=web2x&amp;context=3">points out</a>:</p>
<blockquote>
<p>But what's wild to me is that something as safety critical as air traffic
control apparently isn't using proven techniques like formal verification,
model checking to eliminate these classes of bugs entirely.</p>
<p>Like as an industry we use TLA+ to stop AWS from having downtime or Xboxes
segfaulting, but not to keep planes in the air?</p>
</blockquote>
<p>I agree that it certainly doesn't sound like any formal verification was used in
this case (for this system), and the report doesn't mention anything. Using
formal verification would certainly have helped here, I might explore this in
subsequent posts.</p>
<p>But it's possible formal verification was used, but faulty code still made its
way into production: end-2-end formal verification for large systems is still in
its infancy. We'll have to wait for the result of the enquiry to know more.</p>
<h2 id="humans-were-kept-safe-at-all-times">Humans were kept safe at all times</h2>
<p>I'd like to note (as does NATS in the report) that despite all the problems
highlighted above, planes in the air over the UK were still safe at all times.
They were being monitored by experienced ATCOs, which monitor planes by their
known flight plan, radio, radar and vision. The consequence of all this was not
that any human lives were put in danger, it's simply that far fewer flights
could take off in the first place, or had to be diverted away from UK airspace.
NATS did the right thing (reducing the number of flights), and kept everybody
safe.</p>
<h2 id="how-to-code-this-properly">How to code this properly</h2>
<p>So, how can we avoid this bug?</p>
<p>Let's recap the problem. There are two sequences of waypoints:</p>
<ul>
<li><code>ADEXP</code>: the full list of waypoints.</li>
<li><code>ICAO</code>: a subsequence of the ADEXP waypoints.</li>
</ul>
<p>Because the ICAO plan doesn't need to include the waypoints at which it
enters/exits an air traffic control region, extracting the segment of the ICAO
flight plan corresponding to the UK portion of the flight is not entirely
trivial. Of course, if we take the entire ICAO flight plan, it already contains
the UK portion, but what we really want is the <em>smallest</em> such segment. It's
interesting to note here that a flight could possibly enter UK airspace, and
then exit it again, and enter it again. We'll ignore this, that is, we will just
find a single contiguous segment that contains all UK portions of the flight,
since this is what the original code seemed to do.</p>
<p>I'm unsure why this task attempts to do this only using the ADEXP data, rather
than consulting a database about how waypoints and flight segments intersect UK
airspace. It seems strange, but let's move on.</p>
<p>Note that it is impossible to achieve this task with the ICAO flight plan alone
(and no knowledge of routes), even if you know for each waypoint if it is in the
UK or not. Indeed you could even be in a situation where <em>none</em> of the waypoints
in the ICAO route are in the UK, for example when the flight plan clips a small
portion of the UK between two of the ICAO waypoints.</p>
<p>So this is why the ADEXP waypoint list is used, and the assumption here, I
assume, is that the ADEXP list contains <em>all</em> the waypoints, and that
furthermore, waypoint granularity is such that if <em>adjacent</em> waypoints both
don't intersect UK airspace, then the segment between them doesn't either.</p>
<p>The mistake of the faulty algorithm described above is to try to work on both
the ICAO data and the ADEXP data as they are, maintaining pointers into each of
them, updating them with vague and wrong invariants in the background of the
programmer's mind. This is a recipe for bugs. Instead, the first thing to do is
to reconcile the data and then carefully extract the UK portion from that.</p>
<p>So we create a data structure for a plan:</p>
<pre data-lang="haskell"><code data-lang="haskell"><span>-- A flight plan, with segments between points 'p' via routes 'r'.
</span><span>data </span><span>Plan</span><span> p r
</span><span>  = </span><span>End</span><span> p
</span><span>  | </span><span>Leg</span><span> p r (</span><span>Plan</span><span> p r)
</span></code></pre>
<p>(This is <a href="https://www.haskell.org/">Haskell</a> code, but the ideas apply to most languages.)</p>
<p>This says that a <code>Plan p r</code> has either arrived at its destination <code>End p</code>, or
consists of a segment starting from <code>p</code>, via <code>r</code>, and the <code>rest</code> of the plan:
<code>Leg p r rest</code>.</p>
<p>We can now define all the sorts of flight plan data we will deal with:</p>
<pre data-lang="haskell"><code data-lang="haskell"><span>type </span><span>ICAO</span><span>     p r = </span><span>Plan</span><span> p r         </span><span>-- points and routes, no intermediate waypoints
</span><span>type </span><span>ADEXP</span><span>    p   = </span><span>Plan</span><span> p [p]       </span><span>-- points and intermediate waypoints, no route data
</span><span>type </span><span>Combined</span><span> p r = </span><span>Plan</span><span> p (</span><span>Via</span><span> p r) </span><span>-- all the data combined
</span><span>
</span><span>data </span><span>Via</span><span> p r = </span><span>Via
</span><span>  { route   :: r,
</span><span>    </span><span>through </span><span>::</span><span> [</span><span>p</span><span>]
</span><span>  }
</span><span>  </span><span>deriving</span><span> stock (</span><span>Show</span><span>)
</span></code></pre>
<p>Here <code>Combined</code> is our reconciled flight plan, it combined all the information
form ICAO and ADEXP. We can project a <code>Plan</code> back down to <code>ICAO</code> or <code>ADEXP</code>:</p>
<pre data-lang="haskell"><code data-lang="haskell"><span>projectICAO </span><span>:: Combined </span><span>p r </span><span>-&gt; ICAO </span><span>p r
</span><span>projectICAO = mapRoutes (.route)
</span><span>
</span><span>projectADEXP </span><span>:: Combined </span><span>p r </span><span>-&gt; ADEXP </span><span>p
</span><span>projectADEXP = mapRoutes (.through)
</span><span>
</span><span>mapRoutes </span><span>::</span><span> (</span><span>r </span><span>-&gt; </span><span>r</span><span>') </span><span>-&gt; Plan </span><span>p r </span><span>-&gt; Plan </span><span>p r</span><span>'
</span><span>mapRoutes _ (</span><span>End</span><span> p) = </span><span>End</span><span> p
</span><span>mapRoutes f (</span><span>Leg</span><span> p r rest) = </span><span>Leg</span><span> p (f r) (mapRoutes f rest)
</span></code></pre>
<p>We'll assume we have already parsed the data into the data structures above.
This is just a matter of reading the spec carefully and turning it into code,
and hopefully something the <code>FPRSA-R</code> did correctly, though as noted previously
it might be working on the text version directly.</p>
<p>Now we write our reconciliation function. For ICAO and ADEXP to reconcile, the
start and end points must match. When reconciling a leg of a flight plan, a
certain amount of waypoints can be skipped in the ICAO plan, and the rest of
them must reconcile with the rest of the flight plan:</p>
<pre data-lang="haskell"><code data-lang="haskell"><span>reconcile </span><span>::</span><span> (</span><span>Eq </span><span>p</span><span>) </span><span>=&gt; ICAO </span><span>p r </span><span>-&gt;</span><span> [</span><span>p</span><span>] </span><span>-&gt;</span><span> [</span><span>Combined </span><span>p r</span><span>]
</span><span>reconcile (</span><span>End</span><span> p) [p']             | p == p' = pure (</span><span>End</span><span> p)
</span><span>reconcile (</span><span>Leg</span><span> p r rest) (p' : ps) | p == p' = </span><span>do
</span><span>  (through, restAdexp) &lt;- splits ps
</span><span>  recoRest &lt;- reconcile rest restAdexp
</span><span>  pure (</span><span>Leg</span><span> p </span><span>Via</span><span> {route = r, through} recoRest)
</span><span>reconcile _ _ = </span><span>[]
</span><span>
</span><span>-- | All the ways to snap a list in two.
</span><span>splits </span><span>::</span><span> [</span><span>a</span><span>] </span><span>-&gt;</span><span> [([</span><span>a</span><span>], [</span><span>a</span><span>])]
</span><span>splits </span><span>[] </span><span>= [(</span><span>[]</span><span>, </span><span>[]</span><span>)]
</span><span>splits xs@(x : rest) = (</span><span>[]</span><span>, xs) : (first (x :) &lt;$&gt; splits rest)
</span></code></pre>
<p>Note that the function produces <em>all</em> the possible reconciliations. This is
because reconciliations are not necessarily unique because waypoints can appear
more than once. By calculating all the possible reconciliations, we'll know if
the data is ambiguous, and flag those flight plans for manual processing.</p>
<p>Next, we extract the UK portion of the flight plan. This is done in 3 steps:</p>
<ol>
<li>Remove all legs at the start which don't cross into UK airspace.</li>
<li>Traverse the legs which are in UK airspace.</li>
<li>Once the rest of the flight plan is never again in the UK, cut it short.</li>
</ol>
<p>Each function calls the next step in sequence. Note that we return a
<code>NonUkPlan</code> error when the system reaches the end of the plan without having
found a UK part. By having a compiler which checks that pattern-matches are
covering, the possible failures arise naturally while coding.</p>
<pre data-lang="haskell"><code data-lang="haskell"><span>-- Extract the UK part of the flight.
</span><span>ukSegment </span><span>::</span><span> (</span><span>p </span><span>-&gt; Bool</span><span>) </span><span>-&gt; Combined </span><span>p r </span><span>-&gt; Either Err</span><span> (</span><span>Combined </span><span>p r</span><span>)
</span><span>ukSegment uk (</span><span>End</span><span> p)
</span><span>  | nonUkPlan uk (</span><span>End</span><span> p) = </span><span>Left NonUkPlan
</span><span>  | otherwise = pure (</span><span>End</span><span> p)
</span><span>ukSegment uk plan@(</span><span>Leg</span><span> _ _ rest) =
</span><span>  </span><span>if</span><span> nonUkLeg uk plan
</span><span>    </span><span>then</span><span> ukSegment uk rest
</span><span>    </span><span>else</span><span> pure (flyUK uk plan)
</span><span>
</span><span>-- Fly the UK part of the flight.
</span><span>flyUK </span><span>::</span><span> (</span><span>p </span><span>-&gt; Bool</span><span>) </span><span>-&gt; Combined </span><span>p r </span><span>-&gt; Combined </span><span>p r
</span><span>flyUK _ (</span><span>End</span><span> end) = </span><span>End</span><span> end
</span><span>flyUK uk (</span><span>Leg</span><span> p v rest)
</span><span>  | nonUkPlan uk rest = </span><span>Leg</span><span> p v (afterUK rest)
</span><span>  | otherwise = </span><span>Leg</span><span> p v (flyUK uk rest)
</span><span>
</span><span>-- Skip the rest of the flight.
</span><span>afterUK </span><span>:: Combined </span><span>p r </span><span>-&gt; Combined </span><span>p r
</span><span>afterUK plan = </span><span>End</span><span> (start plan)
</span></code></pre>
<p>These use some small functions:</p>
<pre data-lang="haskell"><code data-lang="haskell"><span>-- The next leg of the journey doesn't fly through the UK.
</span><span>nonUkLeg </span><span>::</span><span> (</span><span>p </span><span>-&gt; Bool</span><span>) </span><span>-&gt; Combined </span><span>p r </span><span>-&gt; Bool
</span><span>nonUkLeg uk (</span><span>End</span><span> p) = not (uk p)
</span><span>nonUkLeg uk (</span><span>Leg</span><span> p v _) = not (uk p) &amp;&amp; not (any uk v.through)
</span><span>
</span><span>-- The whole plan isn't in the UK.
</span><span>nonUkPlan </span><span>::</span><span> (</span><span>a </span><span>-&gt; Bool</span><span>) </span><span>-&gt; Combined </span><span>a r </span><span>-&gt; Bool
</span><span>nonUkPlan uk plan = all (nonUkLeg uk) (legs plan)
</span><span>
</span><span>legs </span><span>:: Plan </span><span>p r </span><span>-&gt;</span><span> [</span><span>Plan </span><span>p r</span><span>]
</span><span>legs (</span><span>End</span><span> p) = [</span><span>End</span><span> p]
</span><span>legs plan@(</span><span>Leg</span><span> _ _ rest) = plan : legs rest
</span><span>
</span><span>start </span><span>:: Plan </span><span>p r </span><span>-&gt; </span><span>p
</span><span>start (</span><span>End</span><span> p) = p
</span><span>start (</span><span>Leg</span><span> p _ _) = p
</span></code></pre>
<p>Putting it all together, we get:</p>
<pre data-lang="haskell"><code data-lang="haskell"><span>ukPartOfICAO </span><span>::</span><span> (</span><span>Eq </span><span>p</span><span>) </span><span>=&gt;</span><span> (</span><span>p </span><span>-&gt; Bool</span><span>) </span><span>-&gt; ICAO </span><span>p r </span><span>-&gt;</span><span> [</span><span>p</span><span>] </span><span>-&gt; Either Err</span><span> (</span><span>ICAO </span><span>p r</span><span>)
</span><span>ukPartOfICAO uk icao adexp = </span><span>case</span><span> reconcile icao adexp </span><span>of
</span><span>  [plan] -&gt; projectICAO &lt;$&gt; ukSegment uk plan
</span><span>  </span><span>[]     </span><span>-&gt; </span><span>Left CannotReconcileIcaoAdexp
</span><span>  _      -&gt; </span><span>Left AmbiguousReconciliationsOfIcaoAdexp
</span></code></pre>
<p>We collected the following errors while coding:</p>
<pre data-lang="haskell"><code data-lang="haskell"><span>data </span><span>Err
</span><span>  = </span><span>NonUkPlan
</span><span>  | </span><span>CannotReconcileIcaoAdexp
</span><span>  | </span><span>AmbiguousReconciliationsOfIcaoAdexp
</span></code></pre>
<p>Let's test it with our example:</p>
<pre><code><span>           4       2        8         5              1           9
</span><span>ICAO:  F------Q--------T--------O-----------P---------------Y--------U
</span><span>
</span><span>ADEXP: F   S  Q    C   T   A    O  E  X     P   W   B   Q   Y        U
</span><span>                       UK  UK   UK UK UK    UK  UK
</span></code></pre>
<pre data-lang="haskell"><code data-lang="haskell"><span>inUK = (</span><span>`</span><span>elem</span><span>`</span><span> ["</span><span>T</span><span>", "</span><span>A</span><span>", "</span><span>O</span><span>", "</span><span>E</span><span>", "</span><span>X</span><span>", "</span><span>P</span><span>", "</span><span>W</span><span>"])
</span><span>icao = ("</span><span>F</span><span>", </span><span>4</span><span>) ~&gt; ("</span><span>Q</span><span>", </span><span>2</span><span>) ~&gt; ("</span><span>T</span><span>", </span><span>8</span><span>) ~&gt; ("</span><span>O</span><span>", </span><span>5</span><span>) ~&gt; ("</span><span>P</span><span>", </span><span>1</span><span>) ~&gt; ("</span><span>Y</span><span>", </span><span>9</span><span>) ~&gt; </span><span>End </span><span>"</span><span>U</span><span>"
</span><span>adexp = ["</span><span>F</span><span>", "</span><span>S</span><span>", "</span><span>Q</span><span>", "</span><span>C</span><span>", "</span><span>T</span><span>", "</span><span>A</span><span>", "</span><span>O</span><span>", "</span><span>E</span><span>", "</span><span>X</span><span>", "</span><span>P</span><span>", "</span><span>W</span><span>", "</span><span>B</span><span>", "</span><span>Q</span><span>", "</span><span>Y</span><span>", "</span><span>U</span><span>"]
</span><span>
</span><span>infixr </span><span>6 </span><span>~&gt;
</span><span>(~&gt;) (p, r) = </span><span>Leg</span><span> p r
</span></code></pre>
<p>And try this at the REPL:</p>
<pre><code><span>λ&gt; ukPortionOfICAO inUK icao adexp
</span><span>Right (Leg "T" 8 (Leg "O" 5 (Leg "P" 1 (End "Y"))))
</span></code></pre>
<p>We can see that this is the correct result:</p>
<pre><code><span>                                 UK portion of ICAO
</span><span>                       ┣━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┫
</span><span>           4       2        8         5              1           9
</span><span>ICAO:  F------Q--------T--------O-----------P---------------Y--------U
</span><span>
</span><span>ADEXP: F   S  Q    C   T   A    O  E  X     P   W   B   Q   Y        U
</span><span>                       UK  UK   UK UK UK    UK  UK
</span></code></pre>
<p>The waypoint <code>Q</code> is a duplicate in the ADEXP list, but the system still returns
the correct portion of the ICAO flight path. Crisis averted! The fact that there
is a duplicate identifier in this case is immaterial, the ICAO and ADEXP data
still reconcile unambiguously, and the correct sub-route is well-defined.</p>
<p>How large can flight plans get? Well here is a flight plan from London to Sydney
that contains a total of 158 waypoints, and about a third of them appear in the
ICAO route:
<img src="https://jameshaydon.github.io/nats-fail/london-sydney.png" alt="London to Sydney flight plan"> 
<code>ukPortionOfICAO</code> returns practically instantly for such a flight plan.</p>
<p>Comments on <a href="https://www.reddit.com/r/programming/comments/16fhmuq/a_deep_dive_into_the_bug_that_caused_the_uk_air/?utm_source=share&amp;utm_medium=web2x&amp;context=3">reddit</a></p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Webb Discovers Methane, Carbon Dioxide in Atmosphere of K2-18B (224 pts)]]></title>
            <link>https://www.nasa.gov/goddard/2023/webb-discovers-methane-carbon-dioxide-in-atmosphere-of-k2-18b/</link>
            <guid>37468342</guid>
            <pubDate>Mon, 11 Sep 2023 15:04:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nasa.gov/goddard/2023/webb-discovers-methane-carbon-dioxide-in-atmosphere-of-k2-18b/">https://www.nasa.gov/goddard/2023/webb-discovers-methane-carbon-dioxide-in-atmosphere-of-k2-18b/</a>, See on <a href="https://news.ycombinator.com/item?id=37468342">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Develop with Cocoa for Apple devices without using Objective-C (127 pts)]]></title>
            <link>https://felixk15.github.io/posts/c_ocoa/</link>
            <guid>37468031</guid>
            <pubDate>Mon, 11 Sep 2023 14:44:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://felixk15.github.io/posts/c_ocoa/">https://felixk15.github.io/posts/c_ocoa/</a>, See on <a href="https://news.ycombinator.com/item?id=37468031">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h2 id="summary">
<span>Summary</span><a href="#summary"><i></i></a>
</h2>
<p>In this post I’ll go into detail about how, during my contracting work for <a href="https://www.shimmerindustries.com/" target="_blank" rel="noopener noreferrer">Shimmer Industries</a> (a company founded by <a href="https://twitter.com/EskilSteenberg" target="_blank" rel="noopener noreferrer">Eskil Steenberg</a> which is focused on developing a real-time lighting designing software), I worked on a piece of software which enabled us to use the MacOS and iOS APIs using a custom C API without having to use ObjC.</p>
<p>This technology (which we decided to call <strong>c-ocoa</strong>) was used to implement support for OSX &amp; iOS for Eskil’s platform abstraction library <a href="https://gamepipeline.org/betray.html" target="_blank" rel="noopener noreferrer">betray</a>. Betray is what’s driving the tools of Shimmer Industries. This allowed us to easily ship existing applications to mobile with only minimal code changes. <a href="https://felixk15.github.io/assets/img/posts/c_ocoa/betray_abstraction.png"><img data-src="/assets/img/posts/c_ocoa/betray_abstraction.png" alt="Betray abstraction layers" data-proofer-ignore="" src="https://felixk15.github.io/assets/img/posts/c_ocoa/betray_abstraction.png"></a> <em>Abstraction layers of betray using c_ocoa for calling native APIs</em></p>
<p>Eskil was generous enough to allow me to write about how this problem was solved. Additionally, we decided to make the <a href="#sourcecode">final project completely open-source!</a>.</p>
<p>The result of the endevaour is effectively a C-API which you can use to interact with the Cocoa API. Since there are C bindings for virtually all programming languages out there, you could even use the generated API with</p>
<ul>
<li><a href="https://www.lua.org/pil/26.html" target="_blank" rel="noopener noreferrer">Lua</a></li>
<li><a href="https://docs.python.org/3/extending/extending.html" target="_blank" rel="noopener noreferrer">Python</a></li>
<li><a href="https://blog.appsignal.com/2018/10/30/ruby-magic-building-a-ruby-c-extension-from-scratch.html" target="_blank" rel="noopener noreferrer">Ruby</a></li>
<li>or even <a href="https://medium.com/jspoint/a-simple-guide-to-load-c-c-code-into-node-js-javascript-applications-3fcccf54fd32" target="_blank" rel="noopener noreferrer">Javascript</a>
</li>
</ul>
<p>Yes, that’s right. Cocoa from the web, y’all!</p>
<p>In the end we have an application that shares 95% of it’s code between platforms and has an identical look and feel. <a href="https://felixk15.github.io/assets/img/posts/c_ocoa/zenith_on_devices.png"><img data-src="/assets/img/posts/c_ocoa/zenith_on_devices.png" alt="Zenith on different devices" data-proofer-ignore="" src="https://felixk15.github.io/assets/img/posts/c_ocoa/zenith_on_devices.png"></a> <em>Application running on Android, Windows 11 and iOS</em></p>
<p>Since this is a code generator, another nice bonus is that you can immediately re-generate the C-API when a new ObjC API becomes available. There’s no waiting for the maintainer of a language bindings library to update the code, just re-run the generator and voila, you have the latest API.</p>
<p>Watched the latest WWDC where a new library got presented and want to try it out in your C-only program? Just re-generate and start tinkering!</p>
<h2 id="why-would-you-do-this-just-use-objective-c11">
<span>“Why would you do this? Just use Objective-C!!!11”</span><a href="#why-would-you-do-this-just-use-objective-c11"><i></i></a>
</h2>
<p>Let me start this post by saying that yes, we could’ve used ObjC to be able to use the MacOS and iOS APIs from within the C codebase that was already in place. We decided against it though, because we wanted to have a pure C codebase without any of the weird ObjC code in there. We didn’t want maintainers (who are all mostly familiar with C) to first learn how to parse ObjC and also thought that this would be a cool thing that might also interesting other developers. It was certainly an interesting experience for me since outside some courses in University, I’ve never touched an OSX or iOS development and was also unfamiliar with ObjC and XCode as an IDE.</p>
<h2 id="introducing-the-objective-c-runtime">
<span>Introducing the Objective-C runtime</span><a href="#introducing-the-objective-c-runtime"><i></i></a>
</h2>
<p>Eskil actually gave me the first hint by pointing me towards the ObjC runtime.</p>
<p>Quoting the official documentation it states that <a href="https://developer.apple.com/documentation/objectivec/objective-c_runtime" target="_blank" rel="noopener noreferrer">“The Objective-C runtime is a runtime library that provides support for the dynamic properties of the Objective-C language”</a>. This sounds interesting, but slighty vague, so after taking a closer look at the documentation and the API itself, to see if there’s stuff in there that could be of use for solving this problem, I was delighted to see that there’s stuff like “give me a list of all classes and methods” and, most importantly “call the function with a given name on this object”. Bingo, this is exactly what I was looking for and seems to be a good foundation to build a C API on. Even better: The ObjC runtime itself is even a pure C API!</p>
<h2 id="the-plan">
<span>The plan</span><a href="#the-plan"><i></i></a>
</h2>
<p>The overall plan is to have a C API which, under the hood, uses the ObjC runtime to call functions that are normally only accessible when programming in ObjC.</p>
<p>In praxis this would mean that ObjC code like this:</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
</pre></td>
<td><pre><span>char</span><span>*</span> <span>getClipboardString</span><span>()</span>
<span>{</span>
  <span>UIPasteboard</span> <span>pasteBoard</span> <span>=</span> <span>[</span><span>UIPasteboard</span> <span>generalPasteboard</span><span>];</span>
  <span>NSString</span> <span>pasteBoardContent</span> <span>=</span> <span>[</span><span>pasteBoard</span> <span>string</span><span>];</span>

  <span>NSUInteger</span> <span>length</span> <span>=</span> <span>[</span><span>pasteBoardContent</span> <span>lengthOfBytesUsingEncoding</span><span>:</span><span>NSUTF8StringEncoding</span><span>];</span>
  <span>char</span><span>*</span> <span>pString</span> <span>=</span> <span>(</span><span>char</span><span>*</span><span>)</span><span>malloc</span><span>(</span><span>length</span><span>);</span>

  <span>[</span><span>pasteBoardContent</span> <span>getCString</span><span>:</span><span>pString</span> <span>maxLength</span><span>:</span><span>length</span> <span>encoding</span><span>:</span><span>NSUTF8StringEncoding</span><span>];</span>
  <span>return</span> <span>pString</span><span>;</span>
<span>}</span>
</pre></td>
</tr></tbody></table></code></p>
</div>
<p>could be written like this in C:</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
</pre></td>
<td><pre><span>char</span><span>*</span> <span>getClipboardString</span><span>()</span>
<span>{</span>
  <span>uipasteboard_t</span> <span>pasteBoard</span> <span>=</span> <span>uipasteboard_generalPasteboard</span><span>();</span>
  <span>nsstring_t</span> <span>pasteBoardContent</span> <span>=</span> <span>uipasteboard_string</span><span>(</span> <span>pasteBoard</span> <span>);</span>

  <span>unsigned</span> <span>long</span> <span>length</span> <span>=</span> <span>nsstring_lengthOfBytesUsingEncoding</span><span>(</span> <span>pasteBoardContent</span><span>,</span> <span>NSUTF8StringEncoding</span> <span>);</span>
  <span>char</span> <span>*</span><span>cStr</span> <span>=</span> <span>(</span><span>char</span> <span>*</span><span>)</span><span>malloc</span><span>(</span><span>sizeof</span><span>(</span><span>char</span><span>)</span> <span>*</span> <span>(</span><span>uint</span><span>)</span><span>length</span><span>);</span>
  <span>nsstring_getCString</span><span>(</span><span>pasteBoardContent</span><span>,</span> <span>cStr</span><span>,</span> <span>length</span><span>,</span> <span>NSUTF8StringEncoding</span><span>);</span>

  <span>return</span> <span>cStr</span><span>;</span>
<span>}</span>
</pre></td>
</tr></tbody></table></code></p>
</div>
<p>It was clear pretty early on that we’d need some kind of external code-generation tool that would use the ObjC runtime to parse the ObjC APIs and use the parsed information to generate the C API.</p>
<h2 id="the-code-generator">
<span>The code generator</span><a href="#the-code-generator"><i></i></a>
</h2>
<p>Let’s get right into it and talk about the code generator. From a birds-eye-view the code generator is a command-line tool (written in C) which uses the ObjC runtime API to query various data from ObjC APIs and uses this queried data to generate multiple .c/.h files which can be used in a project to interface with these ObjC APIs using normal C functions.</p>
<p>For each ObjC class that you want to create a C API for, the tool basically does these things in order: <a href="https://felixk15.github.io/assets/img/posts/c_ocoa/code_generator_workflow.png"><img data-src="/assets/img/posts/c_ocoa/code_generator_workflow.png" alt="Code Generator Workflow" data-proofer-ignore="" src="https://felixk15.github.io/assets/img/posts/c_ocoa/code_generator_workflow.png"></a> <em>Code Generator Workflow</em></p>
<h3 id="query-objc-class">
<span>Query ObjC Class</span><a href="#query-objc-class"><i></i></a>
</h3>
<p>The first step is to query metadata about the class that we’re interested in turning into a C API. The code generator works by providing it with one or more ObjC class name(s) (optionally including wildcards). For each class the generator generates a pair of .c/.h files.</p>
<p>This is done by first querying for <em>all</em> (see notes about what <em>all</em> actually means) available classes using <a href="https://developer.apple.com/documentation/objectivec/1418579-objc_getclasslist?language=objc" target="_blank" rel="noopener noreferrer"><code>objc_getClassList()</code></a>. The query results will be returned using an opaque type <code>Class</code> which can be used together with <code>class_</code> prefixed functions to query more metadata about a specific class. So the generator first caches all available classes (I’ll later talk about how classes are made visible for the ObjC runtime API) and then compares the name of each class (by using <a href="https://developer.apple.com/documentation/objectivec/1418635-class_getname?language=objc" target="_blank" rel="noopener noreferrer"><code>class_getName()</code></a>) against the input of the user.</p>
<p>Once one ore more matching classes have been found, we move to the next step.</p>
<blockquote><p><em>Note</em>: <em>All</em> available classes depends on what ObjC frameworks are linked at the time the code generator is run. This also means that you can’t generate code for iOS only classes when running the code generator on OSX since you can’t link the iOS frameworks to the OSX app. For generating code for iOS classes, you have to run the code generator on an iOS simulator.</p></blockquote>
<h3 id="flattening-objc-class-hierarchy">
<span>Flattening ObjC Class Hierarchy</span><a href="#flattening-objc-class-hierarchy"><i></i></a>
</h3>
<p>ObjC is an object oriented language, C is not. So we need some way to map ObjC’s “object orientness” to C functions.</p>
<p>For this problem I decided to flatten the class hierarchy of the class that you want to export to C. This is done by using the <a href="https://developer.apple.com/documentation/objectivec/1418498-class_getsuperclass?language=objc" target="_blank" rel="noopener noreferrer"><code>class_getSuperClass()</code></a> function from the ObjC Runtime API - this also returns an object of type <code>Class</code> - same as <code>objc_getClassList()</code> before.</p>
<p>For each class found during the “Query ObjC Class” step, the program needs to recursively move up the class hierarchy, This is an image illustrating the class hierarchy for the class <a href="https://developer.apple.com/documentation/foundation/nsmutablestring?language=objc" target="_blank" rel="noopener noreferrer"><code>NSMutableString</code></a>: <a href="https://felixk15.github.io/assets/img/posts/c_ocoa/class_hierarchy.png"><img data-src="/assets/img/posts/c_ocoa/class_hierarchy.png" alt="Class Hierarchy of NSMutableString" data-proofer-ignore="" src="https://felixk15.github.io/assets/img/posts/c_ocoa/class_hierarchy.png"></a> <em>Class Hierarchy of NSMutableString (The arrow point upwards the class hierarchy)</em></p>
<p>Unlike C++, ObjC does <em>not</em> support multiple inheritance (thank god!), so this process is rather straight-forward.</p>
<p>By using <code>class_getSuperClass()</code>, we can recursively go up the class hierarchy and query the methods of each individual class. Querying the methods of a class is done using the <a href="https://developer.apple.com/documentation/objectivec/1418490-class_copymethodlist?language=objc" target="_blank" rel="noopener noreferrer"><code>class_copyMethodList()</code></a> API - this returns an array of the opaque type <code>Method</code> which can be used to query metadata about a specific method.</p>
<p>The result of this step is an array of methods from the complete class hierarchy. Spoiler: Since we’ll later call the methods using just their name, we don’t have to worry about methods that have been overriden at this point.</p>
<blockquote><p><em>Note</em>: This works for only for <em>non-static</em> methods. To also get the <em>static</em> methods of a class you have to get what’s called the <code>meta-class</code>. This is done by using the <a href="https://developer.apple.com/documentation/objectivec/1418629-object_getclass?language=objc" target="_blank" rel="noopener noreferrer"><code>object_getClass()</code></a> function with a <code>Class</code> object as it’s argument. This returns another object of type <code>Class</code>. This object can be used together with <code>class_copyMethodList()</code> to get the list of static methods.</p></blockquote>
<p>The result of this step are 2 arrays:</p>
<ol>
<li>Array of all <em>static</em> methods</li>
<li>Array of all <em>non-static</em> methods</li>
</ol>
<h3 id="resolving-objc-runtime-types">
<span>Resolving ObjC Runtime Types</span><a href="#resolving-objc-runtime-types"><i></i></a>
</h3>
<p>Before talking about how the metadata of methods is queried, I have to talk about how types are encoded in the ObjC runtime.</p>
<p>When querying argument and/or return types using the ObjC runtime you “only” get back a string. Naively I though that this is just the name of the return type (eg: <code>int</code>, <code>float</code> or <code>NSString</code>) but things are a bit more complicated than that. Basically we have to differentiate between base types (<code>int</code>, <code>char</code>, <code>float</code> etc), user defined POD(plain-old-data) types (your typical structs), references (pointers) and ids (more on that later). For each of these types, the type name from the ObjC runtime has to be parsed differently.</p>
<h4 id="base-types">
<span>Base Types</span><a href="#base-types"><i></i></a>
</h4>
<p>Let’s start with the most simple - base types. Instead of fully qualified type names (like <code>int</code>), the ObjC runtime returns a string with 1 element. Fortunately, there’s a 1:1 mapping between these “ObjC base types” and “C base types”. The mapping listed below is from the <a href="https://opensource.apple.com/source/objc4/objc4-709/runtime/runtime.h.auto.html" target="_blank" rel="noopener noreferrer"><code>objc/runtime.h</code></a> file</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td>
<td><pre><span>#define _C_CHR      'c'
#define _C_UCHR     'C'
#define _C_SHT      's'
#define _C_USHT     'S'
#define _C_INT      'i'
#define _C_UINT     'I'
#define _C_LNG      'l'
#define _C_ULNG     'L'
#define _C_LNG_LNG  'q'
#define _C_ULNG_LNG 'Q'
#define _C_FLT      'f'
#define _C_DBL      'd'
#define _C_BFLD     'b'
#define _C_BOOL     'B'
#define _C_VOID     'v'
</span></pre></td>
</tr></tbody></table></code></p>
</div>
<h4 id="user-defined-structs">
<span>User defined structs</span><a href="#user-defined-structs"><i></i></a>
</h4>
<p>The next type, user defined POD types (struct XY) are a bit more complex. The ObjC runtime returns the type name and the complete layout of the custom type. To drive home what I mean by this, think of a struct like this:</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
</pre></td>
<td><pre><span>struct</span> <span>AwesomeType</span>
<span>{</span>
  <span>char</span> <span>a</span><span>;</span>
  <span>int</span> <span>b</span><span>;</span>
  <span>float</span> <span>c</span><span>;</span>
<span>};</span>
</pre></td>
</tr></tbody></table></code></p>
</div>
<p>This type would be encoded by the ObjC runtime like this: <code>{AwesomeType=cif}</code> With the info on how base types are encoded, the part after the equal sign can be parsed as a list of base types. So this is basically <code>AwesomeType = char int float</code>.</p>
<p>This extends to structs within structs as well. So this struct:</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
12
</pre></td>
<td><pre><span>struct</span> <span>Foo</span>
<span>{</span>
  <span>int</span> <span>a</span><span>;</span>
  <span>unsigned</span> <span>int</span> <span>b</span><span>;</span>
  <span>float</span> <span>c</span><span>;</span>
<span>};</span>

<span>struct</span> <span>Boo</span>
<span>{</span>
  <span>double</span> <span>a</span><span>;</span>
  <span>Foo</span> <span>foo</span><span>;</span>
<span>};</span>
</pre></td>
</tr></tbody></table></code></p>
</div>
<p>Would result in this type name for <code>struct Boo</code>: <code>{Boo=d{Foo=iIf}}</code>.</p>
<blockquote><p><em>Note</em>: The first version of the code generator actually re-created all structs that where used as return and argument types but as you might already have guessed, you loose the member name of each struct member. The current version assumes that you use the actual ObjC struct (which are the same as C structs).</p></blockquote>
<h4 id="references">
<span>References</span><a href="#references"><i></i></a>
</h4>
<p>Following this, we have references which are marked with a <code>^</code> preceeding the encoded type name. This is thankfully quite easy to incorporate into the existing type parsing. Care should be taken with opaque types however since they’ll be returned like this: <code>{OpaqueType=}</code> (For reference: Opaque types are types where the caller doesn’t know the type layout).</p>
<h4 id="id">
<span>ID</span><a href="#id"><i></i></a>
</h4>
<p>Finally, we have id - this is actually a <a href="https://developer.apple.com/documentation/objectivec/id" target="_blank" rel="noopener noreferrer">type from the objc runtime API</a>. This is a reference to an ObjC object, unfortunately for this we don’t get any more type information. Looking at this from C, we basically only know that this is a <code>void*</code> and not what type.</p>
<h3 id="collect-objc-method-metadata">
<span>Collect ObjC Method Metadata</span><a href="#collect-objc-method-metadata"><i></i></a>
</h3>
<p>Now that it is established how the types are encoded, we can continue with querying metadata about each individual method.</p>
<p>After the class hierarchy has been flattened, we have generated a list of all methods of the complete class hierarchy. In this step we use this list to query information about each individual method. Things that we’re interested in include the following:</p>
<ol>
<li>Method Name</li>
<li>Return Type</li>
<li>Number of Parameters</li>
<li>Type of Parameters</li>
</ol>
<p>Since we now operate on objects of type <code>Method</code>, we can use the ObjC runtime functions prefixed with <code>method_</code>. The first function that is being used is <a href="https://developer.apple.com/documentation/objectivec/1418758-method_getname?language=objc" target="_blank" rel="noopener noreferrer"><code>method_getName()</code></a> - this is similar to <code>class_getName()</code> and returns a string.</p>
<p>The next function that we can use is <a href="https://developer.apple.com/documentation/objectivec/1418591-method_getreturntype?language=objc" target="_blank" rel="noopener noreferrer"><code>method_getReturnType()</code></a> this returns an ObjC runtime encoded type string (that we know how to parse thanks to <a href="#resolving-objc-runtime-types">the previous chapter</a>). To get the parameter list, <a href="https://developer.apple.com/documentation/objectivec/1418968-method_getnumberofarguments?language=objc" target="_blank" rel="noopener noreferrer"><code>method_getNumberOfArguments()</code></a> together with <a href="https://developer.apple.com/documentation/objectivec/1418607-method_getargumenttype?language=objc" target="_blank" rel="noopener noreferrer"><code>method_getArgumentType()</code></a> can be used. The returned type is also encoded as an ObjC runtime type.</p>
<blockquote><p><em>Note</em>: Unfortunately the name of the arguments can not be queried - until this is solved, the arguments follow a <code>arg0</code>, <code>arg1</code>, <code>arg2</code>, etc naming-scheme.</p></blockquote>
<p>With these information available, we have a complete function signature and now only need to find out how we can do the actual function call.</p>
<h3 id="calling-objc-methods-using-the-objc-runtime">
<span>Calling ObjC Methods Using The ObjC Runtime</span><a href="#calling-objc-methods-using-the-objc-runtime"><i></i></a>
</h3>
<p>For calling ObjC methods various variations of <code>objc_msgSend</code> can be used. I say “various variations” because there are multiple version depending on what kind of return value you expect (this only applies when targeting i386 hosts).</p>
<ol>
<li>
<a href="https://developer.apple.com/documentation/objectivec/1456712-objc_msgsend" target="_blank" rel="noopener noreferrer"><code>objc_msgSend</code></a> for function that return types &lt;= 16 bytes.</li>
<li>
<a href="https://developer.apple.com/documentation/objectivec/1456697-objc_msgsend_fpret" target="_blank" rel="noopener noreferrer"><code>objc_msgSend_fpret</code></a> for float return types.</li>
<li>
<a href="https://developer.apple.com/documentation/objectivec/1456730-objc_msgsend_stret" target="_blank" rel="noopener noreferrer"><code>objc_msgSend_stret</code></a> for functions that return types &gt; 16 bytes.</li>
</ol>
<p>Calculating the size of the return type can be added as part of the pass where the return type is parsed. During code generation, the size and type of the return value can then be used to select the correct <code>objc_msgSend</code> function.</p>
<p>If you look at the definition of any of these function, you’ll see that the first 2 arguments are always <code>ID self</code> and <code>SEL selector</code>. The first argument <code>ID self</code> is a pointer to the object which we want to call this method on (or, in case of static methods, the meta-class). The <code>SEL selector</code> argument is the name of the method at runtime. This can be retrieved by calling <a href="https://developer.apple.com/documentation/objectivec/1418557-sel_registername?language=objc" target="_blank" rel="noopener noreferrer"><code>sel_registerName()</code></a> with the method name as argument (as given by <code>method_getName()</code>).</p>
<blockquote><p><em>Note</em>: The return value of <code>sel_registerName()</code> is cached internally.</p></blockquote>
<h3 id="generate-c-source-code">
<span>Generate C Source Code</span><a href="#generate-c-source-code"><i></i></a>
</h3>
<p>We’re now perfectly set-up to start generating the C source code, which will become the basis of our API.</p>
<p>If you remember from <a href="#flattening-objc-class-hierarchy">the previous chapter</a> we are left with 2 arrays which contain elements of type <code>Method</code> after the class hierarchy has been flattened. One array for static and one array for non-static methods. These arrays are now used to create matching C functions for each individual method.</p>
<p>We do this by using the method meta-data that we’ve collected <a href="#collect-objc-method-metadata">earlier</a>. The result of this step will be matching .h/.c files for each ObjC class that should be exported.</p>
<p>The first step when writing the C source files is always the file prefix. For .h files the prefix is a header guard and a single typedef for syntactic sugar (example for <code>NSString</code>):</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
</pre></td>
<td><pre><span>#ifndef C_OCOA_NSSTRING_HEADER
#define C_OCOA_NSSTRING_HEADER
</span><span>typedef</span> <span>nsstring_t</span> <span>void</span><span>*</span><span>;</span>
</pre></td>
</tr></tbody></table></code></p>
</div>
<p>(for .h files there’s also a postfix needed to add the <code>#endif</code> for the header guard).</p>
<p>For .c files this is a couple of defines that change what variation of <code>objc_msgSend</code> is being called based on the target ABI (since this could theoretically be x86 or ARM). To make the generated C code work on both architectures, these defines are added at the beginning of every .c file:</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
</pre></td>
<td><pre><span>#ifdef __arm64__
#define abi_objc_msgSend_stret objc_msgSend
#else
#define abi_objc_msgSend_stret objc_msgSend_stret
#endif
#ifdef __i386__
#define abi_objc_msgSend_fpret objc_msgSend_fpret
#else
#define abi_objc_msgSend_fpret objc_msgSend
#endif
</span></pre></td>
</tr></tbody></table></code></p>
</div>
<blockquote><p><em>Note</em>: this means that instead of <code>objc_msgSend_stret</code> and <code>objc_msgSend_fpret</code> we need to use <code>abi_objc_msgSend_stret</code> &amp; <code>abi_objc_msgSend_fpret</code> respectively.</p></blockquote>
<p>For each C function we first call <code>sel_registerName()</code> to get the correct selector for the method that we want to call. After that we have to make the call to <code>objc_msgSend()</code> to perform the actual function call.</p>
<blockquote><p><em>Note</em>: Since all the <code>objc_msgSend</code> function are typless, they need to be cast to the correct function-ptr type to be called with the correct arguments. This will be done using a “synctactic-sugar” helper-macro for better readability and debugability.</p></blockquote>
<h4 id="example">
<span>Example</span><a href="#example"><i></i></a>
</h4>
<p>To give a concrete example let’s focus on the source code generation of the <code>NSString</code> class with one method, <a href="https://developer.apple.com/documentation/foundation/nsstring/1415702-getcstring?language=objc" target="_blank" rel="noopener noreferrer"><code>getCString()</code></a></p>
<p>The function declaration in Objective-C for that function looks like this:</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
</pre></td>
<td><pre><span>@interface</span> <span>NSString</span> <span>:</span> <span>NSSObject</span>

<span>-</span> <span>(</span><span>BOOL</span><span>)</span><span>getCString</span><span>:(</span><span>char</span> <span>*</span><span>)</span><span>buffer</span> 
        <span>maxLength</span><span>:(</span><span>NSUInteger</span><span>)</span><span>maxBufferCount</span> 
        <span>encoding</span><span>:(</span><span>NSStringEncoding</span><span>)</span><span>encoding</span><span>;</span>

<span>@end</span>
</pre></td>
</tr></tbody></table></code></p>
</div>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
</pre></td>
<td><pre><span>// Usage-Code:</span>
<span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>**</span> <span>argv</span><span>)</span>
<span>{</span>
  <span>NSString</span><span>*</span> <span>string</span> <span>=</span> <span>[[</span><span>NSString</span> <span>alloc</span><span>]</span> <span>init</span><span>];</span>
  <span>//Fill string with data...</span>
  <span>//..</span>
  <span>char</span> <span>buffer</span><span>[</span><span>512</span><span>];</span>
  <span>[</span><span>string</span> <span>getCstring</span><span>:</span><span>buffer</span> <span>maxLength</span><span>:</span><span>512</span> <span>encoding</span><span>:</span><span>NSUTF8StringEncoding</span><span>]</span>
<span>}</span>

</pre></td>
</tr></tbody></table></code></p>
</div>
<p>Using the information provided earlier, the generated c_ocoa .h/.c file(s) would look like this (including usage-code):</p>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
</pre></td>
<td><pre><span>// nsstring.h</span>
<span>#ifndef C_OCOA_NSSTRING_HEADER
#define C_OCOA_NSSTRING_HEADER
</span><span>typedef</span> <span>nsstring_t</span> <span>void</span><span>*</span><span>;</span>

<span>bool</span> <span>nsstring_getCString</span><span>(</span> <span>nsstring_t</span> <span>object</span><span>,</span> <span>char</span><span>*</span> <span>arg0</span><span>,</span> <span>unsigned</span> <span>long</span> <span>long</span> <span>arg1</span><span>,</span> <span>unsigned</span> <span>long</span> <span>long</span> <span>arg2</span> <span>);</span>

<span>#endif
</span></pre></td>
</tr></tbody></table></code></p>
</div>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
10
11
</pre></td>
<td><pre><span>// nsstring.c</span>
<span>#include</span> <span>"nsstring.h"</span><span>
</span>
<span>bool</span> <span>nsstring_getCString</span><span>(</span> <span>nsstring_t</span> <span>object</span><span>,</span> <span>char</span><span>*</span> <span>arg0</span><span>,</span> <span>unsigned</span> <span>long</span> <span>long</span> <span>arg1</span><span>,</span> <span>unsigned</span> <span>long</span> <span>long</span> <span>arg2</span> <span>)</span>
<span>{</span>
	<span>SEL</span> <span>methodSelector</span> <span>=</span> <span>sel_registerName</span><span>(</span> <span>"getCString:maxLength:encoding:"</span> <span>);</span>

	<span>#define nsstring_getCString_call( obj, selector, arg0, arg1, arg2 ) ((bool (*)( id, SEL, char*, unsigned long long, unsigned long long ))objc_msgSend) ( obj, selector, arg0, arg1, arg2 )
</span>	<span>return</span> <span>nsstring_getCString_call</span><span>(</span> <span>(</span><span>id</span><span>)</span><span>object</span><span>,</span> <span>methodSelector</span><span>,</span> <span>arg0</span><span>,</span> <span>arg1</span><span>,</span> <span>arg2</span> <span>);</span>
	<span>#undef nsstring_getCString_call
</span><span>}</span>
</pre></td>
</tr></tbody></table></code></p>
</div>
<div>

<p><code><table><tbody><tr>
<td><pre>1
2
3
4
5
6
7
8
9
</pre></td>
<td><pre><span>// Usage-Code:</span>
<span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span><span>**</span> <span>argv</span><span>)</span>
<span>{</span>
  <span>nsstring_t</span> <span>string</span> <span>=</span> <span>nsstring_alloc</span><span>(</span> <span>nsstring_init</span><span>()</span> <span>);</span>
  <span>//Fill string with data...</span>
  <span>//..</span>
  <span>char</span> <span>buffer</span><span>[</span><span>512</span><span>];</span>
  <span>nsstring_getCString</span><span>(</span><span>string</span><span>,</span> <span>buffer</span><span>,</span> <span>512</span><span>,</span> <span>NSUTF8StringEncoding</span><span>);</span>
<span>}</span>
</pre></td>
</tr></tbody></table></code></p>
</div>
<h2 id="conclusion">
<span>Conclusion</span><a href="#conclusion"><i></i></a>
</h2>
<p>Generating a pure C API to interface with the OSX/iOS system APIs was a very interesting problem for me personally since it gave me an excuse to work with ObjC and get to know some internals of the language. This was definitely a big undertaking and, to be honest, I expected it to fail at any moment because of some problem(s) that we didn’t foresee. But fortunately everything worked out pretty nicely and it was definitely a scary feeling letting the generator work through the <em>entire</em> class hierarchy of big frameworks like <strong>AppKit</strong>, <strong>Foundation</strong> or <strong>GLKit</strong>. That being said we did hit a couple of problems that we’re actively working on, namely parameter naming and adding auto-generated documentation to the various functions. But all in all the result is pretty cool and we were able to ship an iOS application using the technology.</p>
<h2 id="sourcecode">
<span>Sourcecode</span><a href="#sourcecode"><i></i></a>
</h2>
<p>The entire source code of the code generator has been made open source and can be <a href="https://github.com/FelixK15/c_ocoa" target="_blank" rel="noopener noreferrer">accessed on github</a>. Follow the README in the repository to build the generator locally.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[X sues Calif. to avoid revealing how it makes “controversial” content decisions (176 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/09/x-sues-calif-to-avoid-revealing-how-it-makes-controversial-content-decisions/</link>
            <guid>37467607</guid>
            <pubDate>Mon, 11 Sep 2023 14:15:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/09/x-sues-calif-to-avoid-revealing-how-it-makes-controversial-content-decisions/">https://arstechnica.com/tech-policy/2023/09/x-sues-calif-to-avoid-revealing-how-it-makes-controversial-content-decisions/</a>, See on <a href="https://news.ycombinator.com/item?id=37467607">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      "Rebranding censorship"    —
</h4>
            
            <h2 itemprop="description">X decried law's "draconian financial penalties," up to $15K per violation per day.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/09/GettyImages-1563274899-800x532.jpg" alt="X sues Calif. to avoid revealing how it makes “controversial” content decisions">
      <figcaption></figcaption>  </figure>

  




<!-- cache hit 35:single/related:c4dbb0a9ea6a9aff2f1f0298bc009002 --><!-- empty -->
<p>Today, Elon Musk's X Corp. <a href="https://cdn.arstechnica.net/wp-content/uploads/2023/09/X-Corp-v-Bonta-9-8-2023-Complaint.pdf">sued</a> to block California's content moderation law, AB 587. In its complaint, filed in a US district court in California, X Corp. is seeking a preliminary and permanent injunction stopping California Attorney General Robert Bonta from enforcing the law.</p>
<p>AB 587 passed in September 2022, requiring social media platforms to submit a "terms of service report" semi-annually to California's attorney general, providing "a detailed description of content moderation practices used" and "information about whether, and if so how, the social media company defines and moderates" hate speech or racism, extremism or radicalization, disinformation or misinformation, harassment, and foreign political interference. Under the law, social media platforms must also provide information and statistics on any content moderation actions taken in those categories.</p>
<p>In X's complaint, the company accused California of trying to dictate X's terms of service and compel "controversial disclosures about how X Corp. moderates content on its platform."</p>
<p>The law stipulated that all platforms were required to start collecting data for their first terms of service report covering content moderation during the third quarter of 2023 and submit those reports to Bonta by January 1, 2024.</p>
<p>Platforms could be found violating the law for failing to post terms of service about content moderation, missing a deadline to submit a terms of service report, or materially omitting or misrepresenting information about content moderation. Any platform violating the law risks fines—which X described as "draconian financial penalties"—up to $15,000 per violation per day.</p>
<p>In its complaint, X Corp. argued that AB 587 violates the First Amendment by compelling "companies like X Corp. to engage in speech against their will" and "impermissibly" interfering "with the constitutionally protected editorial judgments of companies." X Corp. said that if the court did not block the law, California could pressure companies "to remove, demonetize, or deprioritize constitutionally protected speech that the state deems undesirable or harmful."</p>
<p>"The State of California touts AB 587 as a mere 'transparency measure' under which certain social media companies must make their content moderation policies and statistics publicly&nbsp;available," X's complaint said. But, X alleged, the state's "true intent" is "to pressure social media platforms to 'eliminate' certain constitutionally protected content viewed by the state as problematic."</p>                                            
                                                        
<p>X Corp. alleged that AB 587 violates other laws, including the Dormant Commerce Clause—"failing to restrict its extensive reporting requirements to information about Californians"—and Section 230 of the Communications Decency Act—which grants platforms immunity from liability for “any action voluntarily taken in good faith to restrict access to or availability of material that the provider or user considers to be obscene, lewd, lascivious, filthy, excessively violent, harassing, or otherwise objectionable, whether or not such material is constitutionally protected.”</p>
<p>"Because AB 587 imposes liability on such actions if they are taken without the required disclosures, AB 587 is preempted by the broad immunity afforded by Section 230," X's complaint said.</p>
<p>Ars could not immediately reach X for comment. Bonta's office said: “While we have not yet been served with the complaint, we will review it and respond in court.”</p>
<p>The author of AB 587, California assemblymember Jesse Gabriel, released a statement saying that the law "is a pure transparency measure that simply requires companies to be upfront about if and how they are moderating content. It in no way requires any specific content moderation policies—which is why it passed with strong, bipartisan support. If Twitter has nothing to hide, then they should have no objection to this bill.”</p>
<p>But tech groups and policy experts echoed X's concerns over AB 587.</p>
<p>Adam Kovacevich, the CEO of the tech industry policy coalition Chamber of Progress, said that "requiring companies to give their content moderation playbook to scammers and conspiracists is a bad idea."</p>
<p>“Even if you don't like anything about Elon Musk’s leadership of X, it’s clear that requiring tech platforms to publish a detailed blueprint of how to work around content moderators will have negative consequences for users online," Kovacevich said. "Letting platforms set their own editorial standards also leaves consumers with more choices about what kind of platforms they spend time on.”</p>
<p>Netchoice, a group representing tech companies and trade associations, has called AB 587 "the Golden State’s new online censorship law." In a statement about X Corp.'s lawsuit, Netchoice said that the law would force companies to submit "intrusive" and "often impossible to comply with" disclosures "about constitutionally protected editorial decisions." Netchoice's director of litigation, Chris Marchese, said that the court should enjoin AB 587 to protect free speech online.</p>
<p>“The First Amendment prohibits the government from regulating lawful speech—directly or indirectly," Marchese said. "States cannot avoid this prohibition by rebranding censorship as ‘transparency’ requirements."</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia’s AI supremacy is only temporary (163 pts)]]></title>
            <link>https://petewarden.com/2023/09/10/why-nvidias-ai-supremacy-is-only-temporary/</link>
            <guid>37467585</guid>
            <pubDate>Mon, 11 Sep 2023 14:13:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://petewarden.com/2023/09/10/why-nvidias-ai-supremacy-is-only-temporary/">https://petewarden.com/2023/09/10/why-nvidias-ai-supremacy-is-only-temporary/</a>, See on <a href="https://news.ycombinator.com/item?id=37467585">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						
<figure><a href="https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png"><img data-attachment-id="7833" data-permalink="https://petewarden.com/2023/09/10/why-nvidias-ai-supremacy-is-only-temporary/dallc2b7e-2023-09-09-18-43-21-computer-chips-running-in-a-foot-race/" data-orig-file="https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png" data-orig-size="1024,1024" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race" data-image-description="" data-image-caption="" data-medium-file="https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png?w=300" data-large-file="https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png?w=550" src="https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png?w=1024" alt="" srcset="https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png 1024w, https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png?w=150 150w, https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png?w=300 300w, https://petewarden.files.wordpress.com/2023/09/dallc2b7e-2023-09-09-18.43.21-computer-chips-running-in-a-foot-race.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></a></figure>



<p>Nvidia is an amazing company that has executed a contrarian vision for decades, and has rightly become one of the most valuable corporations on the planet thanks to its central role in the AI revolution. I want to explain why I believe it’s top spot in machine learning is far from secure over the next few years. To do that, I’m going to talk about some of the drivers behind Nvidia’s current dominance, and then how they will change in the future.</p>



<h2><span>Currently</span></h2>



<p>Here’s why I think Nvidia is winning so hard right now.</p>



<p><strong>#1 – Almost Nobody is Running Large ML Apps</strong></p>



<p>Outside of a few large tech companies, very few corporations have advanced to actually running large scale AI models in production. They’re still figuring out how to get started with these new capabilities, so the main costs are around dataset collection, hardware for training, and salaries for model authors. This means that machine learning is focused on training, not inference.</p>



<p><strong>#2 – All Nvidia Alternatives Suck</strong></p>



<p>If you’re a developer creating or using ML models, using an Nvidia GPU is a lot easier and less time consuming than an AMD OpenCL card, Google TPU, a Cerebras system, or any other hardware. The software stack is much more mature, there are many more examples, documentation, and other resources, finding engineers experienced with Nvidia is much easier, and integration with all of the major frameworks is better. There is no realistic way for a competitor to beat the platform effect Nvidia has built. It makes sense for the current market to be winner-takes-all, and they’re the winner, full stop.</p>



<p><strong>#3 – Researchers have the Purchasing Power</strong></p>



<p>It’s incredibly hard to hire ML researchers, anyone with experience has their pick of job offers right now. That means they need to be kept happy, and one of the things they demand is use of the Nvidia platform. It’s what they know, they’re productive with it, picking up an alternative would take time and not result in skills the job market values, whereas working on models with the tools they’re comfortable with does. Because researchers are so expensive to hire and retain, their preferences are given a very high priority when purchasing hardware.</p>



<p><strong>#4 – Training Latency Rules</strong></p>



<p>As a rule of thumb models need to be trainable from scratch in about a week. I’ve seen this hold true since the early days of AlexNet, because if the iteration cycle gets any longer it’s very hard to do the empirical testing and prototyping that’s still essential to reach your accuracy goals. As hardware gets faster, people build bigger models up until the point that the training once again takes roughly the same amount of time, and reap the benefits through higher-quality models rather than reduced total training time. This makes buying the latest Nvidia GPUs very attractive, since your existing code will mostly just work, but faster. In theory there’s an opportunity here for competitors to win with lower latency, but the inevitably poor state of their software stack (CUDA has had decades of investment) means it’s mostly an illusion.</p>



<h2><strong>What’s going to change?</strong></h2>



<p>So, hopefully I’ve made a convincing case that there are strong structural reasons behind Nvidia’s success. Here’s how I see those conditions changing over the next few years.</p>



<p><strong>#1 – Inference will Dominate, not Training</strong></p>



<p>Somebody years ago told me “Training costs scale with the number of researchers, inference costs scale with the number of users”. What I took away from this is that there’s some point in the future where the amount of compute any company is using for running models on user requests will exceed the cycles they’re spending on training. Even if the cost of a single training run is massive and running inference is cheap, there are so many potential users in the world with so many different applications that the accumulated total of those inferences will exceed the training total. There are only ever going to be so many researchers.</p>



<p>What this means for hardware is that priorities will shift towards reducing inference costs. A lot of ML researchers see inference as a subset of training, but this is wrong in some fundamental ways. It’s often very hard to assemble a sizable batch of inputs during inference, because that process trades off latency against throughput, and latency is almost always key in user-facing applications. Small or single-input batches change the workload dramatically, and call for very different optimization approaches. There are also a lot of things (like the weights) that remain constant during inference, and so can benefit from pre-processing techniques like weight compression or constant folding.</p>



<p><strong>#2 – CPUs are Competitive for Inference </strong></p>



<p>I didn’t even list CPUs in the Nvidia alternatives above because they’re still laughably slow for training. The main desktop CPUs (x86, Arm, and maybe RISC-V soon) have the benefit of many decades of toolchain investment. They have an even more mature set of development tools and community than Nvidia. They can also be much cheaper per arithmetic op than any GPU.</p>



<p>Old-timers will remember the early days of the internet when most of the cost of setting up a dot-com was millions of dollars for a bunch of high-end web server hardware from someone like Sun. This was because they were the only realistic platform that could serve web pages reliably and with low-latency. They had the fastest hardware money could buy, and that was important when entire sites needed to fit on a single machine. Sun’s market share was rapidly eaten by the introduction of software that could distribute the work across a large number of individually much less capable machines, commodity x86 boxes that were far cheaper.</p>



<p>Training is currently very hard to distribute in a similar way. The workloads make it possible to split work across a few GPUs that are tightly interconnected, but the pattern of continuous updates makes reducing latency by sharding across low-end CPUs unrealistic. This is not true for inference though. The model weights are fixed and can easily be duplicated across a lot of machines at initialization time, so no communication is needed. This makes an army of commodity PCs very appealing for applications relying on ML inference.</p>



<p><strong>#3 – Deployment Engineers gain Power</strong></p>



<p>As inference costs begin to dominate training, there will be a lot of pressure to reduce those costs. Researchers will no longer be the highest priority, so their preferences will carry less weight. They will be asked to do things that are less personally exciting in order to streamline production. There are also going to be a lot more people capable of training models coming into the workforce over the next few years, as the skills involved become more widely understood. This all means researchers’ corporate power will shrink and the needs of the deployment team will be given higher priority.</p>



<p><strong>#4 – Application Costs Rule</strong></p>



<p>When inference dominates the overall AI budget, the hardware and workload requirements are very different. Researchers value the ability to quickly experiment, so they need flexibility to prototype new ideas. Applications usually change their models comparatively infrequently, and may use the same fundamental architecture for years, once the researchers have come up with something that meets their needs. We may almost be heading towards a world where model authors use a specialized tool, like Matlab is for mathematical algorithms, and then hand over the results to deployment engineers who will manually convert the results into something more efficient for an application. This will make sense because any cost savings will be multiplied over a long period of time if the model architecture remains constant (even if the weights change).</p>



<h2>What does this Mean for the Future?</h2>



<p>If you believe my four predictions above, then it’s hard to escape the conclusion that Nvidia’s share of the overall AI market is going to drop. That market is going to grow massively so I wouldn’t be surprised if they continue to grow in absolute unit numbers, but I can’t see how their current margins will be sustainable.</p>



<p>I expect the winners of this shift will be traditional CPU platforms like x86 and Arm. Inference will need to be tightly integrated into traditional business logic to run end user applications, so it’s difficult to see how even hardware specialized for inference can live across a bus, with the latency involved. Instead I expect CPUs to gain much more tightly integrated machine learning support, first as co-processors and eventually as specialized instructions, like the evolution of floating point support.</p>



<p>On a personal level, these beliefs drive my own research and startup focus. The impact of improving inference is going to be so high over the next few years, and it still feels neglected compared to training. There are signs that this is changing though. Communities like <a href="https://www.reddit.com/r/LocalLLaMA/">r/LocalLlama</a> are mostly focused on improving inference, the success of <a href="https://github.com/ggerganov/ggml">GGML</a> shows how much of an appetite there is for inference-focused frameworks, and the spread of a few general-purpose models increases the payoff of inference optimizations. One reason I’m so obsessed with the edge is that it’s the closest environment to the army of commodity PCs that I think will run most cloud AI in the future. Even back in 2013 I originally wrote <a href="https://github.com/jetpacapp/DeepBeliefSDK">the Jetpac SDK</a> to accelerate computer vision on a cluster of 100 m1.small AWS servers, since that was cheaper and faster than a GPU instance for running inference across millions of images. It was only afterwards that I realized what a good fit it was for mobile devices.</p>



<p>I’d love to hear your thoughts on whether inference is going to be as important as I’m predicting! Let me know in the comments if you think I’m onto something, or if I should be stocking up on Nvidia stock.</p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[9/11 in Realtime (214 pts)]]></title>
            <link>https://911realtime.org:443/</link>
            <guid>37467077</guid>
            <pubDate>Mon, 11 Sep 2023 13:38:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://911realtime.org:443/">https://911realtime.org:443/</a>, See on <a href="https://news.ycombinator.com/item?id=37467077">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <li> <h2>9/11 Realtime</h2> <p><b>Thanks and Open Source Notices</b></p> <h2>Thank you to our backers:</h2> <ul> <li>Will Harris</li> <li>Chris Wooster</li> <li>Robinson Collado</li> <li>Richard Harms</li> <li>Matt MG Herron</li> <li>Adil Majid</li> <li>Alana Malone</li> <li>Kori Stephens</li> <li>Marina Harper</li> <li>James Wendel</li> <li>Jason Smith</li> <li>Adam Garst</li> <li>Andrew Poirier</li> <li>Ty Satrang</li> </ul> <h2>Special thanks to <a href="http://hivelocity.net/">Hivelocity</a></h2> <p><b>Based on Platinum by Robbie Byrd</b></p> <p>A UI framework using native CSS/JS replications of the Mac OS 8.1 interface components. The project is named after the interface theme that came with MacOS 8 and 9, Platinum.</p> <p><a href="https://github.com/robbiebyrd/platinum" target="_blank">Platinum on Github</a></p> <p><b>Based on memento.js by Vijith Assar</b></p> <p><a href="https://github.com/vijithassar/memento" target="_blank">memento.js on Github</a></p> <p> Based on <b><a href="https://github.com/npjg/classic.css" target="_blank">New Dawn</a></b> by <b><a href="https://github.com/npjg" target="_blank">Nathanael Gentry</a></b>. </p><p>Copyright (c) 2019 Nathanael Gentry</p>  <p> Based on <b><a href="https://github.com/ticky/classic-scrollbars" target="_blank">Scrollbars of the Classic Mac OS</a></b> by <b><a href="https://github.com/ticky" target="_blank">Jessica Stokes (@ticky)</a></b>. </p> <hr> <p><b>New Dawn</b> and <b>Platinum</b> License</p> <div><p> MIT License </p><p>  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: </p></div> <div><p> The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. </p><p>  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. </p></div> <p>A huge thanks to Apple, Inc., who maintains the copyright on the Apple Icon, background patterns, interface sounds and interface components.</p>  </li> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unix Domain Sockets vs Loopback TCP Sockets (2014) (139 pts)]]></title>
            <link>https://nicisdigital.wordpress.com/2014/03/03/unix-domain-sockets-vs-loopback-tcp-sockets/</link>
            <guid>37466475</guid>
            <pubDate>Mon, 11 Sep 2023 12:51:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nicisdigital.wordpress.com/2014/03/03/unix-domain-sockets-vs-loopback-tcp-sockets/">https://nicisdigital.wordpress.com/2014/03/03/unix-domain-sockets-vs-loopback-tcp-sockets/</a>, See on <a href="https://news.ycombinator.com/item?id=37466475">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
       <p>Two communicating processes on a single machine have a few options. They can use regular TCP sockets, UDP sockets, unix domain sockets, or shared memory. A recent project I was working on used Node.js with two communicating processes on the same machine. I wanted to know how to reduce the CPU utilization of the machine, so I ran a few experiments to compare the efficiency between unix domain sockets and TCP sockets using the loopback interface. This post covers my experiments and test results.</p>
<p>First off, is a disclaimer. This test is not exhaustive. Both client and server are written in Node.js and can only be as efficient as the Node.js runtime.</p>
<p>All code in this post is available at:&nbsp;<a href="http://github.com/nicmcd/uds_vs_tcp">github.com/nicmcd/uds_vs_tcp</a></p>
<h2>Server Application</h2>
<p>I created a simple Node.js server application that could be connected to via TCP socket or Unix domain socket. It simply echos all received messages. Here is the code:</p>
<pre title="">var assert = require('assert');
assert(process.argv.length == 4, 'node server.js &lt;tcp port&gt; &lt;domain socket path&gt;');

var net = require('net');

var tcpPort = parseInt(process.argv[2]);
assert(!isNaN(tcpPort), 'bad TCP port');
console.log('TCP port: ' + tcpPort);

var udsPath = process.argv[3];
console.log('UDS path: ' + udsPath);

function createServer(name, portPath) {
    var server = net.createServer(function(socket) {
        console.log(name + ' server connected');
        socket.on('end', function() {
            console.log(name + ' server disconnected');
        });
        socket.write('start sending now!');
        socket.pipe(socket);
    });
    server.listen(portPath, function() {
        console.log(name + ' server listening on ' + portPath);
    });
}

var tcpServer = createServer('TCP', tcpPort);
var udsServer = createServer('UDS', udsPath);
</pre>
<h2>Client Application</h2>
<p><span>The client application complements the server application. It connects to the server via TCP or Unix domain sockets. It sends a bunch of randomly generated packets and measures the time it takes to finish. When complete, it prints the time and exits. Here is the code:</span></p>
<pre title="">var assert = require('assert');
assert(process.argv.length == 5, 'node client.js &lt;port or path&gt; &lt;packet size&gt; &lt;packet count&gt;');

var net = require('net');
var crypto = require('crypto');

if (isNaN(parseInt(process.argv[2])) == false)
    var options = {port: parseInt(process.argv[2])};
else
    var options = {path: process.argv[2]};
console.log('options: ' + JSON.stringify(options));

var packetSize = parseInt(process.argv[3]);
assert(!isNaN(packetSize), 'bad packet size');
console.log('packet size: ' + packetSize);

var packetCount = parseInt(process.argv[4]);
assert(!isNaN(packetCount), 'bad packet count');
console.log('packet count: ' + packetCount);

var client = net.connect(options, function() {
    console.log('client connected');
});

var printedFirst = false;
var packet = crypto.randomBytes(packetSize).toString('base64').substring(0,packetSize);
var currPacketCount = 0;
var startTime;
var endTime;
var delta;
client.on('data', function(data) {
    if (printedFirst == false) {
        console.log('client received: ' + data);
        printedFirst = true;
    }
    else {
        currPacketCount += 1;
        if (data.length != packetSize)
            console.log('weird packet size: ' + data.length);
        //console.log('client received a packet: ' + currPacketCount);
    }

    if (currPacketCount &lt; packetCount) {
        if (currPacketCount == 0) {
            startTime = process.hrtime();
        }
        client.write(packet);
    } else {
        client.end();
        endTime = process.hrtime(startTime);
        delta = (endTime[0] * 1e9 + endTime[1]) / 1e6;
        console.log('millis: ' + delta);
    }
});
</pre>
<h2>Running a Single Test</h2>
<p>First start the server application with:</p>
<pre title="">node server.js 5555 /tmp/uds
</pre>
<p>This starts the server using TCP port 5555 and Unix domain socket /tmp/uds.</p>
<p>Now we can run the client application to get some statistics. Let’s first try the TCP socket. Run the client with:</p>
<pre title="">
node client.js 5555 1000 100000

</pre>
<p>This runs the client application using TCP port 5555 and sends 100,000 packets all sized 1000 bytes. This tooks 8006 milliseconds on my machine. We can now try running with the Unix domain socket with:</p>
<pre title="">
node client.js /tmp/uds 1000 100000

</pre>
<p>This runs the client the same as before except it uses the /tmp/uds Unix domain socket instead of the TCP socket. On my machine this took 3570 milliseconds to run. These two runs show that for 1k byte packets, Unix domain sockets are about 2-3x more efficient than TCP sockets.<br>
At this point you might be completely convinced that Unix domain sockets are better and you’ll use them whenever you can. That’s too easy. Let’s run the client application a whole bunch of times and graph the results.<br>
I recently posted about a <a title="taskrun – An easy-to-use python package for running tasks with dependencies and process&nbsp;management" href="https://nicisdigital.wordpress.com/2013/12/13/taskrun/">python package</a> I created for running many tasks and aggregating the data. I thought this socket comparison would make a good example.</p>
<h2>Running the Full Test</h2>
<p>As mentioned, running the full test uses the Taskrun Python package (available at <a title="Taskrun Python Package" href="http://github.com/nicmcd/taskrun">github.com/nicmcd/taskrun</a>). The script I quickly hacked together to run the client application and parse the results is as follows:</p>
<pre title="">
import taskrun
import os

POWER = 15
RUNS = 10
PACKETS_PER_RUN = 100000

manager = taskrun.Task.Manager(
    numProcs = 1,
    showCommands = True,
    runTasks = True,
    showProgress = True)

DIR = "sims"
mkdir = manager.task_new('dir', 'rm -rI ' + DIR + '; mkdir ' + DIR)

def makeName(stype, size, run):
    return stype + '_size' + str(size) + '_run' + str(run)

def makeCommand(port_or_path, size, name):
    return 'node client.js ' + port_or_path + ' ' + str(size) + ' ' + str(PACKETS_PER_RUN) + \
        ' | grep millis | awk \'{printf "%s, ", $2}\' &gt; ' + os.path.join(DIR, name)

barrier1 = manager.task_new('barrier1', 'sleep 0')
for exp in range(0, POWER):
    size = pow(2, exp)
    for run in range(0, RUNS):
        # Unix domain socket test
        name = makeName('uds', size, run)
        task = manager.task_new(name, makeCommand('/tmp/uds', size, name))
        task.dependency_is(mkdir)
        barrier1.dependency_is(task)

        # TCP socket test
        name = makeName('tcp', size, run)
        task = manager.task_new(name, makeCommand('5555', size, name))
        task.dependency_is(mkdir)
        barrier1.dependency_is(task)

# create CSV header
filename = os.path.join(DIR, 'uds_vs_tcp.csv')
header = 'NAME, '
for run in range(0, RUNS):
    header += 'RUN ' + str(run) + ', '
hdr_task = manager.task_new('CSV header', 'echo \'' + header + '\' &gt; ' + filename)
hdr_task.dependency_is(barrier1)

# UDS to CSV
cmd = ''
for exp in range(0,POWER):
    size = pow(2, exp)
    cmd += 'echo -n \'UDS Size ' + str(size) + ', \' &gt;&gt; ' + filename + '; '
    for run in range(0, RUNS):
        name = makeName('uds', size, run)
        cmd += 'cat ' + os.path.join(DIR, name) + ' &gt;&gt; ' + filename + '; '
    cmd += 'echo \'\' &gt;&gt; ' + filename + '; '
uds_task = manager.task_new('UDS to CSV', cmd)
uds_task.dependency_is(hdr_task)

# TCP to CSV
cmd = ''
for exp in range(0,POWER):
    size = pow(2, exp)
    cmd += 'echo -n \'TCP Size ' + str(size) + ', \' &gt;&gt; ' + filename + '; '
    for run in range(0, RUNS):
        name = makeName('tcp', size, run)
        cmd += 'cat ' + os.path.join(DIR, name) + ' &gt;&gt; ' + filename + '; '
    cmd += 'echo \'\' &gt;&gt; ' + filename + '; '
tcp_task = manager.task_new('TCP to CSV', cmd)
tcp_task.dependency_is(uds_task)

manager.run_request_is()

</pre>
<p>Admittedly, this isn’t the prettiest code to look at, but it gets the job done. For both Unix domain socket and TCP socket, it runs the client application for all packet sizes that are a power of 2 from 1 to 16384. Each setup is run 10 times. Each test result is written to its own file. After all the tests have been run, the taskrun script creates a CSV file using all the test results. The CSV file can then be imported into a spreadsheet application for analysis.</p>
<h2>Results</h2>
<p>I ran this on an&nbsp;<a href="http://ark.intel.com/products/75789/">Intel E5-2620 v2</a>&nbsp;processor with 16GB of RAM. I imported the CSV into Excel, averaged the 10 results of each setup, then graphed the results. This first graph shows the execution time compared to packet size on a logarithmic graph.</p>
<p><a href="https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png"><img data-attachment-id="604" data-permalink="https://nicisdigital.wordpress.com/2014/03/03/unix-domain-sockets-vs-loopback-tcp-sockets/exe-time-vs-pkt-size/" data-orig-file="https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png" data-orig-size="966,611" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="Execution Time vs. Packet Size" data-image-description="" data-image-caption="" data-medium-file="https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png?w=300" data-large-file="https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png?w=510" alt="Execution Time vs. Packet Size" src="https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png?w=510&amp;h=322" width="510" height="322" srcset="https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png?w=510 510w, https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png?w=150 150w, https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png?w=300 300w, https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png?w=768 768w, https://nicisdigital.files.wordpress.com/2014/03/exe-time-vs-pkt-size.png 966w" sizes="(max-width: 510px) 100vw, 510px"></a></p>
<p>The results shown here are fairly predicable. The Unix domain sockets are always more efficient and the efficiency benefit is in the 2-3x range. After noticing some weird ups and down in the graph, I decided to generate a graph with the execution times normalized to the TCP execution time.</p>
<p><a href="https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png"><img data-attachment-id="605" data-permalink="https://nicisdigital.wordpress.com/2014/03/03/unix-domain-sockets-vs-loopback-tcp-sockets/rel-exe-time-vs-pkt-size/" data-orig-file="https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png" data-orig-size="964,562" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="Relative Execution Time vs Packet Size" data-image-description="" data-image-caption="" data-medium-file="https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png?w=300" data-large-file="https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png?w=510" alt="Relative Execution Time vs Packet Size" src="https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png?w=510&amp;h=297" width="510" height="297" srcset="https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png?w=510 510w, https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png?w=150 150w, https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png?w=300 300w, https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png?w=768 768w, https://nicisdigital.files.wordpress.com/2014/03/rel-exe-time-vs-pkt-size.png 964w" sizes="(max-width: 510px) 100vw, 510px"></a></p>
<p>I’m not exactly sure why the efficiency of Unix domain sockets varies as it does compared to TCP sockets, but it is always better. This is simply because Unix domain sockets don’t traverse the operating system’s network stack. The kernel simply copies the data from the client’s application into the file buffer in the server’s application.</p>

	              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intuitively Understanding Harris Corner Detector (172 pts)]]></title>
            <link>https://comsci.blog/posts/intuitive-harris</link>
            <guid>37466302</guid>
            <pubDate>Mon, 11 Sep 2023 12:35:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://comsci.blog/posts/intuitive-harris">https://comsci.blog/posts/intuitive-harris</a>, See on <a href="https://news.ycombinator.com/item?id=37466302">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>If you ever tried to learn how the Harris corner detection algorithm works, you might have
noticed that the process is not intuitive at all. First, you start with an energy function, approximate it
using Taylor approximation, get a matrix from that, then find the eigenvalues of that matrix, etc.
But when you come to the final implementation, it is rather simple and seems easier.
If you are like me, this is not intuitive at all. But today I will present you a much easier way to understand
how the Harris corner detection algorithm works.</p>

<p>Let’s start with understanding what is a corner. We can simply think of it as a connection of edges. For two edges
to be able to connect, they sure need to be not parallel, so looking at a corner, we should see that
edges moving in different directions (they would be parallel if they moved in the same directions):</p>

<p><img src="https://comsci.blog/assets/intuitive-harris-0.png" alt="" width="400"></p>

<p><em>Figure source: <a href="https://www.cse.psu.edu/~rtc12/CSE486/lecture06.pdf" target="_blank">https://www.cse.psu.edu/~rtc12/CSE486/lecture06.pdf</a></em></p>

<p>So it is obvious that the gradients of the image I<sub>x</sub> and I<sub>y</sub> will both be active in the corner
region. We know that adding I<sub>x</sub><sup>2</sup> and I<sub>y</sub><sup>2</sup> shows the regions with change
in x <strong><em>or</em></strong> y directions (which is the basis of the all edge detection algorithms). So one thing that comes to mind is multiplying the I<sub>x</sub><sup>2</sup>
and I<sub>y</sub><sup>2</sup> so that we will only see regions on the image that have a change in both x <strong><em>and</em></strong> y
directions at the same time, just like corners!</p>

<p>Let’s start implementing this. First, let’s find a pretty basic image that will have lots of corners inside it.</p>

<div><pre><code><span>import</span> <span>cv2</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>

<span># wget https://logowik.com/content/uploads/images/bbc-america9038.jpg -O assets/bbc.jpg
</span>
<span>img</span> <span>=</span> <span>cv2</span><span>.</span><span>imread</span><span>(</span><span>"assets/bbc.jpg"</span><span>,</span> <span>cv2</span><span>.</span><span>IMREAD_GRAYSCALE</span><span>)</span>
<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>img</span><span>,</span> <span>cmap</span><span>=</span><span>"gray"</span><span>)</span>
</code></pre></div>

<p><img src="https://comsci.blog/assets/intuitive-harris_1_1.png" alt="png"></p>

<p>Now we can start finding the gradient of this image using the Sobel operator:</p>

<div><pre><code><span>Ix</span> <span>=</span> <span>cv2</span><span>.</span><span>Sobel</span><span>(</span><span>img</span><span>,</span> <span>ddepth</span><span>=</span><span>cv2</span><span>.</span><span>CV_32F</span><span>,</span> <span>dx</span><span>=</span><span>1</span><span>,</span> <span>dy</span><span>=</span><span>0</span><span>)</span>
<span>Iy</span> <span>=</span> <span>cv2</span><span>.</span><span>Sobel</span><span>(</span><span>img</span><span>,</span> <span>ddepth</span><span>=</span><span>cv2</span><span>.</span><span>CV_32F</span><span>,</span> <span>dx</span><span>=</span><span>0</span><span>,</span> <span>dy</span><span>=</span><span>1</span><span>)</span>
</code></pre></div>

<p>Okay, we are there now. Let’s plot the I<sub>x</sub><sup>2</sup>I<sub>y</sub><sup>2</sup>, we are expecting it to give us regions with both x and y directions:</p>

<div><pre><code><span>plt</span><span>.</span><span>imshow</span><span>(</span><span>Ix</span><span>**</span><span>2</span> <span>*</span> <span>Iy</span><span>**</span><span>2</span><span>,</span> <span>cmap</span><span>=</span><span>'gray'</span><span>)</span>
</code></pre></div>

<p><img src="https://comsci.blog/assets/intuitive-harris_5_1.png" alt="png"></p>

<p>As you can see, we are kind of not successful, because this shows us the both corners and edges that move along in
both x and y directions. But we need to get rid of the edges.</p>

<p>If you carefully look at this resulting image, you will notice that corners are either isolated like the top left
corner of the B logo, or they are at the end of these edges. Maybe we can’t get rid of the edges directly,
but if somehow we can remove the corners from I<sub>x</sub><sup>2</sup>I<sub>y</sub><sup>2</sup>, we can subtract it from the original I<sub>x</sub><sup>2</sup>I<sub>y</sub><sup>2</sup> and
get only the corners. Actually, we can get rid of the corners. Since the corners are isolated in this image,
applying a Gaussian blur will decrease the intensities of the corners a lot!</p>

<p>Let’s see this:</p>

<div><pre><code><span>corners_suppressed</span> <span>=</span> <span>cv2</span><span>.</span><span>GaussianBlur</span><span>(</span><span>Ix</span><span>**</span><span>2</span> <span>*</span> <span>Iy</span><span>**</span><span>2</span><span>,</span> <span>ksize</span><span>=</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>),</span> <span>sigmaX</span><span>=</span><span>1</span><span>)</span>
<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>corners_suppressed</span><span>,</span> <span>cmap</span><span>=</span><span>'gray'</span><span>)</span>
</code></pre></div>

<p><img src="https://comsci.blog/assets/intuitive-harris_7_1.png" alt="png"></p>

<p>We can even do a better job of removing the corners by applying the blur before squaring. Because square will increase
the intensity of isolated corners, making it less affected by the blur. So we can instead do:</p>

<div><pre><code><span>corners_suppressed</span> <span>=</span> <span>cv2</span><span>.</span><span>GaussianBlur</span><span>(</span><span>Ix</span><span>*</span> <span>Iy</span><span>,</span> <span>ksize</span><span>=</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>),</span> <span>sigmaX</span><span>=</span><span>1</span><span>)</span> <span>**</span> <span>2</span>
<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>corners_suppressed</span><span>,</span> <span>cmap</span><span>=</span><span>'gray'</span><span>)</span>
</code></pre></div>

<p><img src="https://comsci.blog/assets/intuitive-harris_9_1.png" alt="png"></p>

<p>Now that we have the corners mostly suppressed image, we can try subtracting this from the I<sub>x</sub><sup>2</sup>I<sub>y</sub><sup>2</sup> and get only the corners. Let’s try it:</p>

<div><pre><code><span>plt</span><span>.</span><span>imshow</span><span>(</span><span>Ix</span><span>**</span><span>2</span> <span>*</span> <span>Iy</span><span>**</span><span>2</span> <span>-</span> <span>corners_suppressed</span><span>,</span> <span>cmap</span><span>=</span><span>'gray'</span><span>)</span>
</code></pre></div>

<p><img src="https://comsci.blog/assets/intuitive-harris_11_1.png" alt="png"></p>

<p>That doesn’t seem to work, but the reason is clear. Edges of the I<sub>x</sub><sup>2</sup>I<sub>y</sub><sup>2</sup>
have different intensity than <code>corners_suppressed</code>, since <code>corners_suppressed</code> has been blurred.
We want them to have the same intensity in edges so that they cancel the edges when they are subtracted.</p>

<p>We can make the edges of I<sub>x</sub><sup>2</sup>I<sub>y</sub><sup>2</sup> similar intensity to edges of <code>corners_suppressed</code>
by applying Gaussian blur to I<sub>x</sub><sup>2</sup> and I<sub>y</sub><sup>2</sup> seperately before multiplying them.
We will apply the blur to squared gradients to make sure the corners are less affected by the blur.</p>

<div><pre><code><span>Ix_squared</span> <span>=</span> <span>cv2</span><span>.</span><span>GaussianBlur</span><span>(</span><span>Ix</span><span>**</span><span>2</span><span>,</span> <span>ksize</span><span>=</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>),</span> <span>sigmaX</span><span>=</span><span>1</span><span>)</span>
<span>Iy_squared</span> <span>=</span> <span>cv2</span><span>.</span><span>GaussianBlur</span><span>(</span><span>Iy</span><span>**</span><span>2</span><span>,</span> <span>ksize</span><span>=</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>),</span> <span>sigmaX</span><span>=</span><span>1</span><span>)</span>

<span>corners</span> <span>=</span> <span>Ix_squared</span> <span>*</span> <span>Iy_squared</span> <span>-</span> <span>corners_suppressed</span>
<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>corners</span><span>,</span> <span>cmap</span><span>=</span><span>'gray'</span><span>)</span>
</code></pre></div>
<p><img src="https://comsci.blog/assets/intuitive-harris_13_1.png" alt="png"></p>

<p>Yes! We successfully get the corners of the image. Now if we look at the data inside the <code>corners</code> matrix, you will notice that
corners have extremely large values and other parts have smaller values. Let’s threshold it:</p>

<div><pre><code><span>corners</span><span>[</span><span>corners</span> <span>&lt;</span> <span>corners</span><span>.</span><span>max</span><span>()</span> <span>/</span> <span>5</span><span>]</span> <span>=</span> <span>0</span>
<span>corners</span><span>[</span><span>corners</span> <span>!=</span> <span>0</span><span>]</span> <span>=</span> <span>255</span>

<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>corners</span><span>,</span> <span>cmap</span><span>=</span><span>"gray"</span><span>)</span>
</code></pre></div>

<p><img src="https://comsci.blog/assets/intuitive-harris_15_1.png" alt="png"></p>

<p>Okay, let’s plot these points as circles in our image:</p>

<div><pre><code><span>new_img</span> <span>=</span> <span>cv2</span><span>.</span><span>cvtColor</span><span>(</span><span>img</span><span>,</span> <span>cv2</span><span>.</span><span>COLOR_GRAY2RGB</span><span>)</span>

<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>img</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]):</span>
    <span>for</span> <span>j</span> <span>in</span> <span>range</span><span>(</span><span>img</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>]):</span>
        <span>if</span> <span>corners</span><span>[</span><span>i</span><span>][</span><span>j</span><span>]</span> <span>==</span> <span>255</span><span>:</span>
            <span>cv2</span><span>.</span><span>circle</span><span>(</span><span>new_img</span><span>,</span> <span>(</span><span>j</span><span>,</span> <span>i</span><span>),</span> <span>radius</span><span>=</span><span>2</span><span>,</span> <span>color</span><span>=</span><span>(</span><span>255</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>),</span> <span>thickness</span><span>=-</span><span>1</span><span>)</span>

<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>new_img</span><span>,</span> <span>cmap</span><span>=</span><span>"gray"</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</code></pre></div>

<p><img src="https://comsci.blog/assets/intuitive-harris_17_0.png" alt="png"></p>

<p>And with this, we have implemented the Harris corner detection algorithm and we haven’t talked about things like
fitting ellipses, Taylor series approximation, or any of that stuff. This implementation is equivalent to the
other implementations of this algorithm.</p>

<p>Here is the full code:</p>
<div><pre><code><span>import</span> <span>cv2</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>

<span># wget https://logowik.com/content/uploads/images/bbc-america9038.jpg -O assets/bbc.jpg
</span>
<span>img</span> <span>=</span> <span>cv2</span><span>.</span><span>imread</span><span>(</span><span>"assets/bbc.jpg"</span><span>,</span> <span>cv2</span><span>.</span><span>IMREAD_GRAYSCALE</span><span>)</span>

<span>Ix</span> <span>=</span> <span>cv2</span><span>.</span><span>Sobel</span><span>(</span><span>img</span><span>,</span> <span>ddepth</span><span>=</span><span>cv2</span><span>.</span><span>CV_32F</span><span>,</span> <span>dx</span><span>=</span><span>1</span><span>,</span> <span>dy</span><span>=</span><span>0</span><span>)</span>
<span>Iy</span> <span>=</span> <span>cv2</span><span>.</span><span>Sobel</span><span>(</span><span>img</span><span>,</span> <span>ddepth</span><span>=</span><span>cv2</span><span>.</span><span>CV_32F</span><span>,</span> <span>dx</span><span>=</span><span>0</span><span>,</span> <span>dy</span><span>=</span><span>1</span><span>)</span>

<span>corners_suppressed</span> <span>=</span> <span>cv2</span><span>.</span><span>GaussianBlur</span><span>(</span><span>Ix</span><span>*</span> <span>Iy</span><span>,</span> <span>ksize</span><span>=</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>),</span> <span>sigmaX</span><span>=</span><span>1</span><span>)</span> <span>**</span> <span>2</span>
<span>Ix_squared</span> <span>=</span> <span>cv2</span><span>.</span><span>GaussianBlur</span><span>(</span><span>Ix</span><span>**</span><span>2</span><span>,</span> <span>ksize</span><span>=</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>),</span> <span>sigmaX</span><span>=</span><span>1</span><span>)</span>
<span>Iy_squared</span> <span>=</span> <span>cv2</span><span>.</span><span>GaussianBlur</span><span>(</span><span>Iy</span><span>**</span><span>2</span><span>,</span> <span>ksize</span><span>=</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>),</span> <span>sigmaX</span><span>=</span><span>1</span><span>)</span>

<span>corners</span> <span>=</span> <span>Ix_squared</span> <span>*</span> <span>Iy_squared</span> <span>-</span> <span>corners_suppressed</span>
<span>corners</span><span>[</span><span>corners</span> <span>&lt;</span> <span>corners</span><span>.</span><span>max</span><span>()</span> <span>/</span> <span>5</span><span>]</span> <span>=</span> <span>0</span>
<span>corners</span><span>[</span><span>corners</span> <span>!=</span> <span>0</span><span>]</span> <span>=</span> <span>255</span>

<span>new_img</span> <span>=</span> <span>cv2</span><span>.</span><span>cvtColor</span><span>(</span><span>img</span><span>,</span> <span>cv2</span><span>.</span><span>COLOR_GRAY2RGB</span><span>)</span>

<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>img</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]):</span>
    <span>for</span> <span>j</span> <span>in</span> <span>range</span><span>(</span><span>img</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>]):</span>
        <span>if</span> <span>corners</span><span>[</span><span>i</span><span>][</span><span>j</span><span>]</span> <span>==</span> <span>255</span><span>:</span>
            <span>cv2</span><span>.</span><span>circle</span><span>(</span><span>new_img</span><span>,</span> <span>(</span><span>j</span><span>,</span> <span>i</span><span>),</span> <span>radius</span><span>=</span><span>2</span><span>,</span> <span>color</span><span>=</span><span>(</span><span>255</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>),</span> <span>thickness</span><span>=-</span><span>1</span><span>)</span>

<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>new_img</span><span>,</span> <span>cmap</span><span>=</span><span>"gray"</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</code></pre></div>
<p>Hopefully, you now understand how this algorithm works and enjoy the process.</p>

  </div>
</article>



      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Beyond OpenAPI (144 pts)]]></title>
            <link>https://antonz.org/interactive-api-tutorials/</link>
            <guid>37466207</guid>
            <pubDate>Mon, 11 Sep 2023 12:26:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://antonz.org/interactive-api-tutorials/">https://antonz.org/interactive-api-tutorials/</a>, See on <a href="https://news.ycombinator.com/item?id=37466207">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><header></header><p>Not all documentation is created equal. According to the popular classification, there are four document types: tutorials, how-to guides, technical references, and explanations.</p><div><p><img alt="Four types of documentation" src="https://antonz.org/interactive-api-tutorials/documentation.png"></p></div><p>OpenAPI, the de facto standard for documenting APIs, is a decent reference-style documentation (and client code generator, of course). But it can't serve as a good how-to or tutorial.</p><p>In this article, I will introduce a concise and readable way to write interactive tutorials and how-tos for any HTTP API (REST, RPC, or other style). And for that (surprise, surprise), we will rely on the HTTP protocol itself.</p><h2 id="a-crash-course-in-http-messages">A crash course in HTTP messages</h2><p>HTTP/1.x is a plain-text protocol that describes the communication between the client and the server. The client sends messages like this:</p><pre tabindex="0"><code>POST /anything/chat HTTP/1.1
host: httpbingo.org
content-type: application/json
user-agent: curl/7.87.0

{
    "message": "Hello!"
}
</code></pre><p>And receives messages like this in response:</p><pre tabindex="0"><code>HTTP/1.1 200 OK
date: Mon, 28 Aug 2023 07:51:49 GMT
content-type: application/json

{
    "message": "Hi!"
}
</code></pre><blockquote><p>HTTP/2, the successor to HTTP/1.1, is a binary protocol. However, all tools (such as the browser devtools or curl) display HTTP/2 messages in plain text (just like HTTP/1.1), so we can safely ignore this fact for our purposes.</p></blockquote><div><figure><img alt="HTTP request and response" src="https://antonz.org/interactive-api-tutorials/http-messages.png"><figcaption>It's easy to read HTTP requests and responses once you get used to it.</figcaption></figure></div><p><strong>HTTP request</strong> consists of three main sections:</p><ol><li>Request line:</li></ol><pre tabindex="0"><code>POST /anything/chat HTTP/1.1
</code></pre><ul><li>The <em>method</em> (<code>POST</code>) defines the operation the client wants to perform.</li><li>The <em>path</em> (<code>/anything/chat</code>) is the URL of the requested resource (without the protocol, domain and port).</li><li>The <em>version</em> (<code>HTTP/1.1</code>) indicates the version of the HTTP protocol.</li></ul><ol start="2"><li>Request headers:</li></ol><pre tabindex="0"><code>host: httpbingo.org
content-type: application/json
user-agent: curl/7.87.0
</code></pre><p>Each header is a key-value pair that tells the server some useful information about the request. In our case it's the hostname of the server (<code>httpbingo.org</code>), the type of the content (<code>application/json</code>) and the client's self-identification (<code>user-agent</code>).</p><ol start="3"><li>Request body:</li></ol><pre tabindex="0"><code>{
    "message": "Hello!"
}
</code></pre><p>The actual data that the client sends to the server.</p><p>The HTTP protocol is stateless, so any state must be contained within the request itself, either in the headers or in the body.</p><p><strong>HTTP response</strong> also consists of three main sections:</p><ol><li>Status line:</li></ol><pre tabindex="0"><code>HTTP/1.1 200 OK
</code></pre><ul><li>The <em>version</em> (<code>HTTP/1.1</code>) indicates the version of the HTTP protocol.</li><li>The <em>status code</em> (<code>200</code>) tells whether the request was successful or not, and why (there are many status codes for <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Status">different situations</a>).</li><li>The <em>status message</em> is a human-readable description of the status code. HTTP/2 does not have it.</li></ul><ol start="2"><li>Response headers:</li></ol><pre tabindex="0"><code>date: Mon, 28 Aug 2023 07:51:49 GMT
content-type: application/json
</code></pre><p>Similar to request headers, these provide useful information about the response to the client.</p><ol start="3"><li>Response body:</li></ol><pre tabindex="0"><code>{
    "message": "Hi!"
}
</code></pre><p>The actual data that the server sends to the client.</p><p>There is much more to the HTTP protocol, but this basic knowledge is enough to cover most of API use cases. So let's move on.</p><h2 id="using-http-to-document-api-usage">Using HTTP to document API usage</h2><p>We are going to take an HTTP request:</p><pre tabindex="0"><code>POST /anything/chat HTTP/1.1
host: httpbingo.org
content-type: application/json
user-agent: curl/7.87.0

{
    "message": "Hello!"
}
</code></pre><p>And modify it just a little bit:</p><ul><li>include the full URL in the request line instead of the path;</li><li>remove the protocol version.</li></ul><pre tabindex="0"><code>POST http://httpbingo.org/anything/chat
content-type: application/json

{
    "message": "Hello!"
}
</code></pre><p>This format is perfect for API usage examples. It's concise and readable, yet formal enough to be executed programmatically (directly from the documentation, as we'll see shortly).</p><h2 id="writing-an-interactive-api-guide">Writing an interactive API guide</h2><p>Instead of telling you how to write an interactive API tutorial, I'm going to show you one. We'll use <a href="https://docs.github.com/en/rest/gists/gists">Gists API</a> as an example. It's a compact and useful GitHub service for storing code snippets (called "gists").</p><div><figure><img alt="GitHub Gists" src="https://antonz.org/interactive-api-tutorials/gists.png"><figcaption>Gists are quite handy when a full-blown Git repository is too much.</figcaption></figure></div><p>Even if you are not a GitHub user, you still have access to the Gists API.</p><h3 id="reading-gists">Reading gists</h3><p>Let's take a look at the <strong>public gists</strong> of my pal Redowan (user <code>rednafi</code>). The response can be quite chatty, so we'll only select the 3 most recent (<code>per_page = 3</code>):</p><pre tabindex="0"><code>GET https://api.github.com/users/rednafi/gists?per_page=3
accept: application/json
</code></pre><codapi-snippet sandbox="fetch" editor="basic"></codapi-snippet><p>A family of non-standard <code>x-ratelimit</code> headers tell us how GitHub <strong>limits</strong> our requests:</p><ul><li>There is a total number of <code>x-ratelimit-limit</code> requests available per hour.</li><li>We've already used <code>x-ratelimit-used</code> requests.</li><li>So there are <code>x-ratelimit-remaining</code> requests left.</li></ul><p>We need to keep an eye on these to make sure we don't exceed the quota.</p><p>We can use a combination of <code>page</code> and <code>per_page</code> query parameters to select a <strong>slice of gists</strong>. For example, here are gists 10-15:</p><pre tabindex="0"><code>GET https://api.github.com/users/rednafi/gists?page=3&amp;per_page=5
accept: application/json
</code></pre><codapi-snippet sandbox="fetch" editor="basic"></codapi-snippet><p>Note that GitHub provides navigation links in the <code>link</code> header:</p><pre tabindex="0"><code>link:
    &lt;https://api.github.com/user/30027932/gists?page=2&amp;per_page=5&gt;; rel="prev",
    &lt;https://api.github.com/user/30027932/gists?page=4&amp;per_page=5&gt;; rel="next",
    &lt;https://api.github.com/user/30027932/gists?page=7&amp;per_page=5&gt;; rel="last",
    &lt;https://api.github.com/user/30027932/gists?page=1&amp;per_page=5&gt;; rel="first"
</code></pre><p>That's thoughtful of them!</p><p>Okay, now let's take a look at the <strong>specific gist</strong> with id <code>88242fd822603290255877e396664ba5</code> (this one is mine; let's not bother Redowan anymore):</p><pre tabindex="0"><code>GET https://api.github.com/gists/88242fd822603290255877e396664ba5
accept: application/json
</code></pre><codapi-snippet sandbox="fetch" editor="basic"></codapi-snippet><p>We can see that there is a <code>greet.py</code> file written in the Python <code>language</code> with a certain <code>content</code>:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>Greeter</span>:
</span></span><span><span>    <span>def</span> <span>__init__</span>(<span>self</span>, <span>greeting</span>):
</span></span><span><span>        <span>self</span><span>.</span><span>greeting</span> <span>=</span> <span>greeting</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>greet</span>(<span>self</span>, <span>who</span>):
</span></span><span><span>        <span>print</span>(<span>f</span><span>"</span><span>{</span><span>self</span><span>.</span><span>greeting</span><span>}</span><span>, </span><span>{</span><span>who</span><span>}</span><span>!"</span>)
</span></span><span><span>
</span></span><span><span><span>gr</span> <span>=</span> <span>Greeter</span>(<span>"Hello"</span>)
</span></span><span><span><span>gr</span><span>.</span><span>greet</span>(<span>"world"</span>)
</span></span></code></pre></div><codapi-snippet sandbox="python" editor="basic"></codapi-snippet><p><em>(yep, you can also create interactive Python examples!)</em></p><p>Interestingly, the gist has a <code>history</code>. It appears that every time you edit a gist, GitHub creates a new version, while also keeping previous versions.</p><p>Let's get the <strong>earliest revision</strong>, which has a <code>version</code> = <code>4c10d27cfb163d654745f1d72f2c7ce14225b83b</code> (a bit long, I know):</p><pre tabindex="0"><code>GET https://api.github.com/gists/88242fd822603290255877e396664ba5/4c10d27cfb163d654745f1d72f2c7ce14225b83b
accept: application/json
</code></pre><codapi-snippet sandbox="fetch" editor="basic"></codapi-snippet><p>The code in the gist was much simpler back then:</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>msg</span> <span>=</span> <span>"Hello, world!"</span>
</span></span><span><span><span>print</span>(<span>msg</span>)
</span></span></code></pre></div><codapi-snippet sandbox="python" editor="basic"></codapi-snippet><h3 id="modifying-gists">Modifying gists</h3><p>Okay, so we know how to list gists for a user, how to get a specific gist, and even how to get a specific revision. Now let's <strong>create a new gist</strong>!</p><pre tabindex="0"><code>POST https://api.github.com/gists
content-type: application/json
accept: application/json

{
    "description": "Greetings in Markdown",
    "public": true,
    "files":{
        "README.md":{
            "content":"Hello, world!"
        }
    }
}
</code></pre><codapi-snippet sandbox="fetch" editor="basic"></codapi-snippet><p>What's that? We have a <code>401 Unauthorized</code> error. The response body explains: "requires authentication" and even provides a link to the documentation (oh, I just love GitHub APIs).</p><p>Understandably, GitHub does not allow anonymous users to create new gists. We have to authenticate with an API token.</p><blockquote><p>If you want the following examples to work, enter your API token in the field below. You can create one with a 'gist' scope in the <a href="https://github.com/settings/tokens">GitHub settings</a>.</p><p>After you enter the token below, it will be stored locally in the browser and will not be sent anywhere (except to the GitHub API when you click the Run button).</p></blockquote><p>Let's try again, this time with an <code>authorization</code> header.</p><p>Note the <code>public</code> parameter. The service supports "secret" gists (<code>public = false</code>), but it's the "security by obscurity" type of secrecy. Secret gists do not show up in the "GET gists" API method, but they are still accessible by id, even by anonymous users.</p><pre tabindex="0"><code data-lang="authenticated">POST https://api.github.com/gists
content-type: application/json
accept: application/json
authorization: bearer {token}

{
    "description": "Greetings in Markdown",
    "public": true,
    "files":{
        "README.md":{
            "content":"Hello, world!"
        }
    }
}
</code></pre><codapi-snippet sandbox="fetch" editor="basic"></codapi-snippet><details><summary>I don't have a token, just show me the results</summary><pre><code>HTTP/1.1 201 
cache-control: private, max-age=60, s-maxage=60
content-length: 3758
content-type: application/json; charset=utf-8
etag: "819f6b4f728843abcb50ad63da200a4c110245585b3eb1c0f59a5ebe86c8ecf5"
location: https://api.github.com/gists/b17474320a629af38255c0a6efbc72b9
x-accepted-oauth-scopes: 
x-github-media-type: github.v3
x-github-request-id: E8B5:8EDA:511F73:51AC33:64EE0266
x-oauth-scopes: gist
x-ratelimit-limit: 5000
x-ratelimit-remaining: 4997
x-ratelimit-reset: 1693323114
x-ratelimit-resource: core
x-ratelimit-used: 3

{
  "url": "https://api.github.com/gists/b17474320a629af38255c0a6efbc72b9",
  "forks_url": "https://api.github.com/gists/b17474320a629af38255c0a6efbc72b9/forks",
  "commits_url": "https://api.github.com/gists/b17474320a629af38255c0a6efbc72b9/commits",
  "id": "b17474320a629af38255c0a6efbc72b9",
  "node_id": "G_kwDOACz0htoAIGIxNzQ3NDMyMGE2MjlhZjM4MjU1YzBhNmVmYmM3MmI5",
  "git_pull_url": "https://gist.github.com/b17474320a629af38255c0a6efbc72b9.git",
  "git_push_url": "https://gist.github.com/b17474320a629af38255c0a6efbc72b9.git",
  "html_url": "https://gist.github.com/nalgeon/b17474320a629af38255c0a6efbc72b9",
  "files": {
    "README.md": {
      "filename": "README.md",
      "type": "text/markdown",
      "language": "Markdown",
      "raw_url": "https://gist.githubusercontent.com/nalgeon/b17474320a629af38255c0a6efbc72b9/raw/5dd01c177f5d7d1be5346a5bc18a569a7410c2ef/README.md",
      "size": 13,
      "truncated": false,
      "content": "Hello, world!"
    }
  },
  ...
}</code></pre></details><p>HTTP status <code>201 Created</code> means that a new gist has been created as a result of our request.</p><p>Okay, now we can <strong>update a gist</strong> using its <code>id</code> (don't forget to replace the <code>{gist_id}</code> in the request line with the actual <code>id</code> value):</p><pre tabindex="0"><code data-lang="authenticated">PATCH https://api.github.com/gists/{gist_id}
content-type: application/json
accept: application/json
authorization: bearer {token}

{
    "description": "Greetings in Markdown",
    "public": true,
    "files":{
        "README.md":{
            "content":"¡Hola, mundo!"
        }
    }
}
</code></pre><codapi-snippet sandbox="fetch" editor="basic"></codapi-snippet><details><summary>I don't have a token, just show me the results</summary><pre><code>HTTP/1.1 200 
cache-control: private, max-age=60, s-maxage=60
content-type: application/json; charset=utf-8
etag: W/"989eaec7cdb50ba6441e77ea2defba257b98a535f26c2ba6062f152ceffb2d77"
x-accepted-oauth-scopes: 
x-github-media-type: github.v3
x-github-request-id: E8B5:8EDA:5188AA:52163F:64EE027F
x-oauth-scopes: gist
x-ratelimit-limit: 100
x-ratelimit-remaining: 98
x-ratelimit-reset: 1693323129
x-ratelimit-resource: gist_update
x-ratelimit-used: 2

{
  "url": "https://api.github.com/gists/b17474320a629af38255c0a6efbc72b9",
  "forks_url": "https://api.github.com/gists/b17474320a629af38255c0a6efbc72b9/forks",
  "commits_url": "https://api.github.com/gists/b17474320a629af38255c0a6efbc72b9/commits",
  "id": "b17474320a629af38255c0a6efbc72b9",
  "node_id": "G_kwDOACz0htoAIGIxNzQ3NDMyMGE2MjlhZjM4MjU1YzBhNmVmYmM3MmI5",
  "git_pull_url": "https://gist.github.com/b17474320a629af38255c0a6efbc72b9.git",
  "git_push_url": "https://gist.github.com/b17474320a629af38255c0a6efbc72b9.git",
  "html_url": "https://gist.github.com/nalgeon/b17474320a629af38255c0a6efbc72b9",
  "files": {
    "README.md": {
      "filename": "README.md",
      "type": "text/markdown",
      "language": "Markdown",
      "raw_url": "https://gist.githubusercontent.com/nalgeon/b17474320a629af38255c0a6efbc72b9/raw/95975f3d0bac707ce4355dfc4a7955310d212fac/README.md",
      "size": 14,
      "truncated": false,
      "content": "¡Hola, mundo!"
    }
  },
  ...
}</code></pre></details><p>It now greets us in Spanish 🇪🇸</p><p>Very good. Finally, let's <strong>delete a gist</strong>:</p><pre tabindex="0"><code data-lang="authenticated">DELETE https://api.github.com/gists/{gist_id}
accept: application/json
authorization: bearer {token}
</code></pre><codapi-snippet sandbox="fetch" editor="basic"></codapi-snippet><details><summary>I don't have a token, just show me the results</summary><pre><code>HTTP/1.1 204 
x-accepted-oauth-scopes: 
x-github-media-type: github.v3
x-github-request-id: E8B5:8EDA:51E584:5273CC:64EE027F
x-oauth-scopes: gist
x-ratelimit-limit: 5000
x-ratelimit-remaining: 4996
x-ratelimit-reset: 1693323114
x-ratelimit-resource: core
x-ratelimit-used: 4</code></pre></details><p>HTTP status <code>204 No Content</code> means we deleted the gist, so GitHub has nothing more to tell us about it. It's a little sad to see it go, but we can always make another one, right?</p><p>The Gists API has other useful features, but they are beyond the scope of this tutorial. Here are the functions we've covered:</p><ul><li>List user gists.</li><li>Get a specific gist, or a specific revision of a gist.</li><li>Create a new gist.</li><li>Update an existing gist.</li><li>Delete a gist.</li></ul><p>Now try managing your gists! You can always use this article as a playground.</p><h2 id="implementation">Implementation</h2><p>To run the API examples as we did in the previous section, you'll need a bit of JavaScript that does the following:</p><ol><li>Parses the HTTP request example.</li><li>Calls the API.</li><li>Displays the result.</li></ol><div><figure><img alt="Fetch API playground" src="https://antonz.org/interactive-api-tutorials/playground.png"><figcaption>It's always nice when a playground doesn't need a server.</figcaption></figure></div><p>Since we've limited ourselves to a small subset of HTTP request capabilities, parsing is fairly easy:</p><div><pre tabindex="0"><code data-lang="js"><span><span><span>// parse parses the request specification.
</span></span></span><span><span><span></span><span>function</span> <span>parse</span>(<span>text</span>) {
</span></span><span><span>    <span>const</span> <span>lines</span> <span>=</span> <span>text</span>.<span>split</span>(<span>"\n"</span>);
</span></span><span><span>    <span>let</span> <span>lineIdx</span> <span>=</span> <span>0</span>;
</span></span><span><span>
</span></span><span><span>    <span>// parse method and URL
</span></span></span><span><span><span></span>    <span>const</span> <span>methodUrl</span> <span>=</span> <span>lines</span>[<span>0</span>].<span>split</span>(<span>" "</span>).<span>filter</span>((<span>s</span>) =&gt; <span>s</span>);
</span></span><span><span>    <span>const</span> [<span>method</span>, <span>url</span>] <span>=</span>
</span></span><span><span>        <span>methodUrl</span>.<span>length</span> <span>&gt;=</span> <span>2</span> <span>?</span> <span>methodUrl</span> <span>:</span> [<span>"GET"</span>, <span>methodUrl</span>[<span>0</span>]];
</span></span><span><span>    <span>lineIdx</span> <span>+=</span> <span>1</span>;
</span></span><span><span>
</span></span><span><span>    <span>// parse headers
</span></span></span><span><span><span></span>    <span>const</span> <span>headers</span> <span>=</span> {};
</span></span><span><span>    <span>for</span> (<span>let</span> <span>i</span> <span>=</span> <span>lineIdx</span>; <span>i</span> <span>&lt;</span> <span>lines</span>.<span>length</span>; <span>i</span><span>++</span>) {
</span></span><span><span>        <span>const</span> <span>line</span> <span>=</span> <span>lines</span>[<span>i</span>].<span>trim</span>();
</span></span><span><span>        <span>if</span> (<span>line</span> <span>===</span> <span>""</span>) {
</span></span><span><span>            <span>break</span>;
</span></span><span><span>        }
</span></span><span><span>        <span>const</span> [<span>headerName</span>, <span>headerValue</span>] <span>=</span> <span>line</span>.<span>split</span>(<span>":"</span>);
</span></span><span><span>        <span>headers</span>[<span>headerName</span>.<span>trim</span>()] <span>=</span> <span>headerValue</span>.<span>trim</span>();
</span></span><span><span>        <span>lineIdx</span> <span>+=</span> <span>1</span>;
</span></span><span><span>    }
</span></span><span><span>
</span></span><span><span>    <span>// parse body
</span></span></span><span><span><span></span>    <span>const</span> <span>body</span> <span>=</span> <span>lines</span>.<span>slice</span>(<span>lineIdx</span> <span>+</span> <span>1</span>).<span>join</span>(<span>"\n"</span>);
</span></span><span><span>
</span></span><span><span>    <span>return</span> { <span>method</span>, <span>url</span>, <span>headers</span>, <span>body</span> };
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>const</span> <span>spec</span> <span>=</span> <span>parse</span>(<span>`GET https://httpbingo.org/uuid`</span>);
</span></span><span><span><span>console</span>.<span>log</span>(<span>JSON</span>.<span>stringify</span>(<span>spec</span>, <span>null</span>, <span>2</span>));
</span></span></code></pre></div><codapi-snippet sandbox="javascript" editor="basic"></codapi-snippet><p>Calling the API and displaying the results is trivial — just use the Fetch API and display the result as plain text:</p><div><pre tabindex="0"><code data-lang="js"><span><span><span>// execCode sends an HTTP request according to the spec
</span></span></span><span><span><span>// and returns the response as text with status, headers and body.
</span></span></span><span><span><span></span><span>async</span> <span>function</span> <span>execCode</span>(<span>spec</span>) {
</span></span><span><span>    <span>const</span> <span>resp</span> <span>=</span> <span>await</span> <span>sendRequest</span>(<span>spec</span>);
</span></span><span><span>    <span>const</span> <span>text</span> <span>=</span> <span>await</span> <span>responseText</span>(<span>resp</span>);
</span></span><span><span>    <span>return</span> <span>text</span>;
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// sendRequest sends an HTTP request according to the spec.
</span></span></span><span><span><span></span><span>async</span> <span>function</span> <span>sendRequest</span>(<span>spec</span>) {
</span></span><span><span>    <span>const</span> <span>options</span> <span>=</span> {
</span></span><span><span>        <span>method</span><span>:</span> <span>spec</span>.<span>method</span>,
</span></span><span><span>        <span>headers</span><span>:</span> <span>spec</span>.<span>headers</span>,
</span></span><span><span>        <span>body</span><span>:</span> <span>spec</span>.<span>body</span> <span>||</span> <span>undefined</span>,
</span></span><span><span>    };
</span></span><span><span>    <span>return</span> <span>await</span> <span>fetch</span>(<span>spec</span>.<span>url</span>, <span>options</span>);
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>// responseText returns the response as text
</span></span></span><span><span><span>// with status, headers and body.
</span></span></span><span><span><span></span><span>async</span> <span>function</span> <span>responseText</span>(<span>resp</span>) {
</span></span><span><span>    <span>const</span> <span>version</span> <span>=</span> <span>"HTTP/1.1"</span>;
</span></span><span><span>    <span>const</span> <span>text</span> <span>=</span> <span>await</span> <span>resp</span>.<span>text</span>();
</span></span><span><span>    <span>const</span> <span>messages</span> <span>=</span> [<span>`</span><span>${</span><span>version</span><span>}</span><span> </span><span>${</span><span>resp</span>.<span>status</span><span>}</span><span> </span><span>${</span><span>resp</span>.<span>statusText</span><span>}</span><span>`</span>];
</span></span><span><span>    <span>for</span> (<span>const</span> <span>hdr</span> <span>of</span> <span>resp</span>.<span>headers</span>.<span>entries</span>()) {
</span></span><span><span>        <span>messages</span>.<span>push</span>(<span>`</span><span>${</span><span>hdr</span>[<span>0</span>]<span>}</span><span>: </span><span>${</span><span>hdr</span>[<span>1</span>]<span>}</span><span>`</span>);
</span></span><span><span>    }
</span></span><span><span>    <span>if</span> (<span>text</span>) {
</span></span><span><span>        <span>messages</span>.<span>push</span>(<span>""</span>, <span>text</span>);
</span></span><span><span>    }
</span></span><span><span>    <span>return</span> <span>messages</span>.<span>join</span>(<span>"\n"</span>);
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>const</span> <span>spec</span> <span>=</span> {
</span></span><span><span>    <span>method</span><span>:</span> <span>"GET"</span>,
</span></span><span><span>    <span>url</span><span>:</span> <span>"https://httpbingo.org/uuid"</span>,
</span></span><span><span>};
</span></span><span><span>
</span></span><span><span><span>const</span> <span>text</span> <span>=</span> <span>await</span> <span>execCode</span>(<span>spec</span>);
</span></span><span><span><span>console</span>.<span>log</span>(<span>text</span>);
</span></span></code></pre></div><codapi-snippet sandbox="javascript" editor="basic"></codapi-snippet><p>Fetch API works in the browser, so there is no intermediate server involved. The only nuance is that the documentation must either be on the same domain as the API itself, or the API must allow cross-domain requests. But even if that's not the case, you can always proxy the requests — it's not too much work.</p><p>If you want an out-of-the-box solution, I've written a simple library that supports both JavaScript and Fetch API playgrounds:</p><p><a href="https://github.com/nalgeon/codapi-js"><strong><code>codapi-js</code></strong></a></p><p>Ideally, I'd like most documentation to be interactive. Not just API guides, but everything from algorithms (like <a href="https://samwho.dev/hashing/">hashing</a>) to programming languages (like <a href="https://antonz.org/go-1-21-builtins/">Go</a> or <a href="https://antonz.org/trying-odin/">Odin</a>) to databases (like <a href="https://antonz.org/sql-compare-neighbors/">SQLite</a>), frameworks and tools, and even individual packages.</p><p>And (shameless plug here!) I'm building a platform that allows just that — easily embeddable code playgrounds for documentation, online education, and fun. Check it out if you are interested:</p><p><a href="https://codapi.org/"><strong><code>codapi</code></strong></a></p><p>And please try to write an interactive guide the next time you develop an API!</p>

<p><em><a href="https://antonz.org/subscribe/"><i></i>&nbsp;<strong>Subscribe</strong></a>
to keep up with new posts.</em></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A LLM+OLAP Solution (103 pts)]]></title>
            <link>https://doris.apache.org/zh-CN/blog/Tencent-LLM/</link>
            <guid>37466182</guid>
            <pubDate>Mon, 11 Sep 2023 12:23:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://doris.apache.org/zh-CN/blog/Tencent-LLM/">https://doris.apache.org/zh-CN/blog/Tencent-LLM/</a>, See on <a href="https://news.ycombinator.com/item?id=37466182">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__blog-post-container" itemprop="articleBody"><p>Six months ago, I wrote about <a href="https://doris.apache.org/blog/Tencent%20Music/" target="_blank" rel="noopener noreferrer">why we replaced ClickHouse with Apache Doris as an OLAP engine</a> for our data management system. Back then, we were struggling with the auto-generation of SQL statements. As days pass, we have made progresses big enough to be references for you (I think), so here I am again. </p><p>We have adopted Large Language Models (LLM) to empower our Doris-based OLAP services.</p><h2 id="llm--olap">LLM + OLAP<a href="#llm--olap" aria-label="LLM + OLAP的直接链接" title="LLM + OLAP的直接链接">​</a></h2><p>Our incentive was to save our internal staff from the steep learning curve of SQL writing. Thus, we used LLM as an intermediate. It transforms natural language questions into SQL statements and sends the SQLs to the OLAP engine for execution.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/zh-CN/assets/images/Tencent_LLM_1-6672112c0d09d75171d8ed9a749ff196.png" width="1280" height="253"></p><p>Like every AI-related experience, we came across some friction:</p><ol><li>LLM does not understand data jargons, like "fields", "rows", "columns" and "tables". Instead, they can perfectly translate business terms like "corporate income" and "DAU", which are basically what the fields/rows/columns are about. That means it can work well only if the analysts use the exact right word to refer to the metric they need when typing their questions.</li><li>The LLM we are using is slow in inference. It takes over 10 seconds to respond. As it charges fees by token, cost-effectiveness becomes a problem.</li><li>Although the LLM is trained on a large collection of public datasets, it is under-informed of niche knowledge. In our case, the LLM is super unfamiliar with indie songs, so even if the songs are included in our database, the LLM will not able to identify them properly. </li><li>Sometimes our input questions require adequate and latest legal, political, financial, and regulatory information, which is hard to be included in a training dataset or knowledge base. We need to connect the LLM to wider info bases in order to perform more diversified tasks.</li></ol><p>We knock these problems down one by one.</p><h3 id="1-a-semantic-layer">1. A semantic layer<a href="#1-a-semantic-layer" aria-label="1. A semantic layer的直接链接" title="1. A semantic layer的直接链接">​</a></h3><p>For problem No.1, we introduce a semantic layer between the LLM and the OLAP engine. This layer translates business terms into the corresponding data fields. It can identify data filtering conditions from the various natural language wordings, relate them to the metrics involved, and then generate SQL statements. </p><p>Besides that, the semantic layer can optimize the computation logic. When analysts input a question that involves a complicated query, let's say, a multi-table join, the semantic layer can split that into multiple single-table queries to reduce semantic distortion.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/zh-CN/assets/images/Tencent_LLM_2-bb2fdaed64ef15214c0542204dd45832.png" width="1280" height="289"></p><h3 id="2-llm-parsing-rules">2. LLM parsing rules<a href="#2-llm-parsing-rules" aria-label="2. LLM parsing rules的直接链接" title="2. LLM parsing rules的直接链接">​</a></h3><p>To increase cost-effectiveness in using LLM, we evaluate the computation complexity of all scenarios, such as metric computation, detailed record retrieval, and user segmentation. Then, we create rules and dedicate the LLM-parsing step to only complicated tasks. That means for the simple computation tasks, it will skip the parsing. </p><p>For example, when an analyst inputs "tell me the earnings of the major musical platforms", the LLM identifies that this question only entails several metrics or dimensions, so it will not further parse it but send it straight for SQL generation and execution. This can largely shorten query response time and reduce API expenses. </p><p><img loading="lazy" src="https://cdnd.selectdb.com/zh-CN/assets/images/Tencent_LLM_3-3ab023081e1acb069d34a4ce24aef010.png" width="1280" height="406"></p><h3 id="3-schema-mapper-and-external-knowledge-base">3. Schema Mapper and external knowledge base<a href="#3-schema-mapper-and-external-knowledge-base" aria-label="3. Schema Mapper and external knowledge base的直接链接" title="3. Schema Mapper and external knowledge base的直接链接">​</a></h3><p>To empower the LLM with niche knowledge, we added a Schema Mapper upstream from the LLM. The Schema Mapper maps the input question to an external knowledge base, and then the LLM will do parsing.</p><p>We are constantly testing and optimizing the Schema Mapper. We categorize and rate content in the external knowledge base, and do various levels of mapping (full-text mapping and fuzzy mapping) to enable better semantic parsing.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/zh-CN/assets/images/Tencent_LLM_4-261ee680cf77335b25f32e41d7a4924b.png" width="2001" height="647"></p><h3 id="4-plugins">4. Plugins<a href="#4-plugins" aria-label="4. Plugins的直接链接" title="4. Plugins的直接链接">​</a></h3><p>We used plugins to connect the LLM to more fields of information, and we have different integration methods for different types of plugins:</p><ul><li><strong>Embedding local files</strong>: This is especially useful when we need to "teach" the LLM the latest regulatory policies, which are often text files. Firstly, the system vectorizes the local text file, executes semantic searches to find matching or similar terms in the local file, extracts the relevant contents and puts them into the LLM parsing window to generate output. </li><li><strong>Third-party plugins</strong>: The marketplace is full of third-party plugins that are designed for all kinds of sectors. With them, the LLM is able to deal with wide-ranging topics. Each plugin has its own prompts and calling function. Once the input question hits a prompt, the relevant plugin will be called.</li></ul><p><img loading="lazy" src="https://cdnd.selectdb.com/zh-CN/assets/images/Tencent_LLM_5-70a170e771dd9eadcc1488b94d892478.png" width="2001" height="645"></p><p>After we are done with above four optimizations, the SuperSonic framework comes into being.</p><h2 id="the-supersonic-framework">The SuperSonic framework<a href="#the-supersonic-framework" aria-label="The SuperSonic framework的直接链接" title="The SuperSonic framework的直接链接">​</a></h2><p>Now let me walk you through this <a href="https://github.com/tencentmusic/supersonic" target="_blank" rel="noopener noreferrer">framework</a>:</p><p><img loading="lazy" src="https://cdnd.selectdb.com/zh-CN/assets/images/Tencent_LLM_6-cbbbb25041c807376b2b9d14609e82c8.png" width="1280" height="1117"></p><ul><li>An analyst inputs a question.</li><li>The Schema Mapper maps the question to an external knowledge base.</li><li>If there are matching fields in the external knowledge base, the question will not be parsed by the LLM. Instead, a metric computation formula will trigger the OLAP engine to start querying. If there is no matching field, the question will enter the LLM.</li><li>Based on the pre-defined rules, the LLM rates the complexity level of the question. If it is a simple query, it will go directly to the OLAP engine; if it is a complicated query, it will be semantically parsed and converted to a DSL statement.</li><li>At the Semantic Layer, the DSL statement will be split based on its query scenario. For example, if it is a multi-table join query, this layer will generate multiple single-table query SQL statements.</li><li>If the question involves external knowledge, the LLM will call a third-party plugin.</li></ul><p><strong>Example</strong></p><p><img loading="lazy" src="https://cdnd.selectdb.com/zh-CN/assets/images/Tencent_LLM_7-c20b3cc2b0b00b32bc2825c1d62b1d5d.png" width="2001" height="1126"></p><p>To answer whether a certain song can be performed on variety shows, the system retrieves the OLAP data warehouse for details about the song, and presents it with results from the Commercial Use Query third-party plugin.</p><h2 id="olap-architecture">OLAP Architecture<a href="#olap-architecture" aria-label="OLAP Architecture的直接链接" title="OLAP Architecture的直接链接">​</a></h2><p>As for the OLAP part of this framework, after several rounds of architectural evolution, this is what our current OLAP pipeline looks like. </p><p>Raw data is sorted into tags and metrics, which are custom-defined by the analysts. The tags and metrics are under unified management in order to avoid inconsistent definitions. Then, they are combined into various tagsets and metricsets for various queries. </p><p><img loading="lazy" src="https://cdnd.selectdb.com/zh-CN/assets/images/Tencent_LLM_8-6d517a787c782510bf3869176730ce3a.png" width="1709" height="1119"></p><p>We have drawn two main takeaways for you from our architectural optimization experience.</p><p><strong>1. Streamline the links</strong></p><p>Before we adopted Apache Doris, we used to have ClickHouse to accelerate the computation of tags and metrics, and Elasticsearch to process dimensional data. That's two analytic engines and requires us to adapt the query statements to both of them. It was high-maintenance.</p><p>Thus, we replaced ClickHouse with Apache Doris, and utilized the <a href="https://doris.apache.org/docs/dev/lakehouse/multi-catalog/es" target="_blank" rel="noopener noreferrer">Elasticsearch Catalog</a> functionality to connect Elasticsearch data to Doris. In this way, we make Doris our unified query gateway. </p><p><strong>2. Split the flat tables</strong></p><p>In early versions of our OLAP architecture, we used to put data into flat tables, which made things tricky. For one thing, flat tables absorbed all the writing latency from upstreams, and that added up to considerable loss in data realtimeliness. For another, 50% of data in a flat table was dimensional data, which was rarely updated. With every new flat table came some bulky dimensional data that consumed lots of storage space. </p><p>Therefore, we split the flat tables into metric tables and dimension tables. As they are updated in different paces, we put them into different data models.</p><ul><li><strong>Metric tables</strong>: We arrange metric data in the Aggregate Key model of Apache Doris, which means new data will be merged with the old data by way of SUM, MAX, MIN, etc.</li><li><strong>Dimension tables</strong>: These tables are in the Unique Key model of Apache Doris, which means new data record will replace the old. This can greatly increase performance in our query scenarios.</li></ul><p>You might ask, does this cause trouble in queries, since most queries require data from both types of tables? Don't worry, we address that with the Rollup feature of Doris. On the basis of the base tables, we can select the dimensions we need to create Rollup views, which will automatically execute <code>GROUP BY</code>. This relieves us of the need to define tags for each Rollup view and largely speed up queries.</p><h2 id="other-tricks">Other Tricks<a href="#other-tricks" aria-label="Other Tricks的直接链接" title="Other Tricks的直接链接">​</a></h2><p>In our experience with Apache Doris, we also find some other functionalities handy, so I list them here for you, too:</p><p><strong>1. Materialized View</strong></p><p>A Materialized View is a pre-computed dataset. It is a way to accelerate queries when you frequently need to access data of certain dimensions. In these scenarios, we define derived tags and metrics based on the original ones. For example, we create a derived metric by combining Metric 1, Metric 2, and Metric 3: <code>sum(m1+m2+m3)</code>. Then, we can create a Materialized View for it. According to the Doris release schedule, version 2.1 will support multi-table Materialized Views, and we look forward to that.</p><p><strong>2. Flink-Doris-Connector</strong></p><p>This is for Exactly-Once guarantee in data ingestion. The Flink-Doris-Connector implements a checkpoint mechanism and two-stage commit, and allows for auto data synchronization from relational databases to Doris.</p><p><strong>3. Compaction</strong></p><p>When the number of aggregation tasks or data volume becomes overwhelming for Flink, there might be huge latency in data compaction. We solve that with Vertical Compaction and Segment Compaction. Vertical Compaction supports loading of only part of the columns, so it can reduce storage consumption when compacting flat tables. Segment Compaction can avoid generating too much segments during data writing, and allows for compaction while writing simultaneously.   </p><h2 id="whats-next">What's Next<a href="#whats-next" aria-label="What's Next的直接链接" title="What's Next的直接链接">​</a></h2><p>With an aim to reduce costs and increase service availability, we plan to test the newly released Storage-Compute Separation and Cross-Cluster Replication of Doris, and we embrace any ideas and inputs about the SuperSonic framework and the Apache Doris project.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Networking for introverts (141 pts)]]></title>
            <link>https://www.economist.com/business/2023/09/07/networking-for-introverts-a-how-to-guide</link>
            <guid>37466147</guid>
            <pubDate>Mon, 11 Sep 2023 12:20:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/business/2023/09/07/networking-for-introverts-a-how-to-guide">https://www.economist.com/business/2023/09/07/networking-for-introverts-a-how-to-guide</a>, See on <a href="https://news.ycombinator.com/item?id=37466147">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><h2>Making the business of meeting strangers marginally less awful</h2></section><div><div data-body-id="cp2"><div><figure><div><figcaption>Listen to this story.</figcaption> <p><span>Enjoy more audio and podcasts on<!-- --> <a id="audio-ios-cta" href="https://economist-app.onelink.me/d2eC/bed1b25" target="_blank" rel="noreferrer">iOS</a> <!-- -->or<!-- --> <a id="audio-android-cta" href="https://economist-app.onelink.me/d2eC/7f3c199" target="_blank" rel="noreferrer">Android</a>.</span></p></div><audio controls="" id="audio-player" preload="none" src="https://www.economist.com/media-assets/audio/060%20Business%20-%20Bartleby%20copy-452e13ecc2cba8a5207cbc25654ef43a.mp3" title="Networking for introverts: a how-to guide " controlslist="nodownload"><p>Your browser does not support the &lt;audio&gt; element.</p></audio></figure></div><p data-component="paragraph"><span data-caps="initial">C</span><small>orporate life</small> throws up some stressful moments. Bringing bad news to your boss; facing an interview panel; making a big presentation. But few things are worse than networking if you are an introvert.</p><p data-component="paragraph">You arrive at an event to find that everyone there apparently knows each other already. And then you look more closely and spot the fellow-sufferers. They are the people who are actually reading the conference blurb. They look at email on their phones with greater intensity than ever happens at the office. They endlessly circulate the room, like bits of plastic in the ocean waiting to be snagged on something. They take a seat in the main hall while the sound engineers are still testing the microphones.</p><p data-component="paragraph">Fortunately, there is advice out there on how to break the ice with strangers. Unfortunately, it’s abysmal. One sage counsels making contact in queues, because it is easier to talk to the person in front of you and behind you. You are meant to ambush people on the escalator, in the toilets and in the queue to get your name tag. In the line for coffee, open the door to jobs and sales by saying six incomprehensible words: “Juicing up for the big keynote?”</p><p data-component="paragraph">On it goes. Don’t be afraid to laugh, because nothing drains the tension from a room like someone who cannot stop chuckling. Bring personal information into the conversation, lest people think you are at a conference on treasury-management software only for commercial gain. Use the other person’s name twice, to appear truly engaged. And take notes on conversations afterwards so you can follow up with them.</p><p data-component="paragraph">Add these ingredients together, and you have the recipe for success:</p><p data-component="paragraph">“Juicing up for the big keynote?”</p><p data-component="paragraph">“What?”</p><p data-component="paragraph">“Juicing up for the big keynote?”</p><p data-component="paragraph">“I don’t know what that means.”</p><p data-component="paragraph">[Scan name badge] “Keith, is it?”</p><p data-component="paragraph">“Er, yes.”</p><p data-component="paragraph">[Laughing] “I’m having a baby, Keith.”</p><p data-component="paragraph">“Keith?”</p><p data-component="paragraph">[Take out notepad]</p><p data-component="paragraph">If this is how to network, no wonder people go to the main hall early.</p><p data-component="paragraph">Making contacts on a site like LinkedIn is a lot less stressful. There is no eye contact, after all, and the rules of the road are agreed. And all those connection requests do appear to help with careers. A paper published last year by Karthik Rajkumar of LinkedIn and co-authors from academia found empirical evidence for the insight that underpins all kinds of networking—that, because they bring you new information, more infrequent and distant relationships (or “weak ties”) are more useful than close contacts.</p><p data-component="paragraph">The researchers randomly changed the “People You May Know” recommendations algorithm that LinkedIn shows its users, so that the prevalence of weaker and stronger connections varied among people on the site. The experiment showed that weaker ties (where a pair of users had only one mutual friend, say) were more likely to lead to job applications and job moves than those where people had 25 mutual friends or more.</p><p data-component="paragraph">This sounds like nirvana for introverts: start spamming everyone with connection requests, close the office door and wait for job offers. But it is not that easy. Even weak ties need tending. Even online, interacting with people is easier if you find it energising; a survey-based study of LinkedIn, by Joanna Davis of Augustana College and her co-authors, found that extroversion was a predictor of networking ability.</p><p data-component="paragraph">There isn’t a genuinely painless way for introverts to network. Still, methods to do it exist that are wiser than standing in a queue and hoping the guy who doesn’t know how to get coffee out of the machine is your ticket to career success.</p><p data-component="paragraph">The real secret is to save your energy for the people who are most likely to be interesting to you. In the online realm, for instance, Dr Rajkumar’s study does not find that the weaker the tie, the better. The sweet spot in networking on LinkedIn is someone with moderately weak ties to you: connecting with a person with ten mutual friends markedly increases the probability of changing jobs compared with someone with just one shared friend.</p><p data-component="paragraph">In other words, networking pays off if you can identify people who can bring you new information but are close enough to your world that this information is useful. In the offline world, a tool like Chat<small>GPT</small> should make it easier to find useful prospects in a list of event attendees. But you still need to overcome all your instincts and approach them.<span>■</span></p><p data-component="paragraph"><b>Read more from Bartleby, our columnist on management and work:<br></b><i><a href="https://www.economist.com/business/2023/08/31/the-best-bosses-know-how-to-subtract-work">The best bosses know how to subtract work</a> (Aug 31st)<br></i><i><a href="https://www.economist.com/business/2023/08/24/the-benefits-of-a-good-workplace-mentoring-scheme-are-undeniable">How to get the most out of mentoring</a> (Aug 24th)<br></i><i><a href="https://www.economist.com/business/article66888-prod.ece" target="_blank">A retiring consultant’s advice on consultants</a> (Jul 17th)</i></p><p data-component="paragraph"><i>Also: How the Bartleby column <a href="https://www.economist.com/column-names">got its name</a></i></p></div><p>This article appeared in the Business section of the print edition under the headline "Stranger things"</p><div data-tracking-id="content-well-chapter-list"><h2><a href="https://www.economist.com/business/">Business</a> <span>September 9th 2023</span></h2><ul><li><a href="https://www.economist.com/business/2023/09/03/meet-ernie-chinas-answer-to-chatgpt"><span>Meet Ernie, China’s answer to ChatGPT</span></a></li><li><a href="https://www.economist.com/business/2023/09/07/german-builders-are-on-the-brink-of-collapse"><span>German builders are on the brink of collapse</span></a></li><li><a href="https://www.economist.com/business/2023/09/07/tiktok-is-wading-into-south-east-asias-e-commerce-wars"><span>TikTok is wading into South-East Asia’s e-commerce wars</span></a></li><li><a href="https://www.economist.com/business/2023/09/07/a-strike-at-chevron-shows-a-reinvigorated-union-movement"><span>A strike at Chevron shows a reinvigorated union movement</span></a></li><li><a href="https://www.economist.com/business/2023/09/07/meet-the-worlds-most-enduring-product"><span>Meet the world’s most enduring product</span></a></li><li><a href="https://www.economist.com/business/2023/09/07/networking-for-introverts-a-how-to-guide"><span>Networking for introverts: a how-to guide</span></a></li><li><a href="https://www.economist.com/business/2023/09/04/americas-bosses-just-wont-quit-that-could-spell-trouble"><span>America’s bosses just won’t quit. That could spell trouble</span></a></li></ul></div><div orientation="vertical" data-test-id="vertical"><div orientation="vertical"><figure><img alt="The new Middle East: The promise and the perils" loading="lazy" width="1280" height="1684" decoding="async" data-nimg="1" sizes="300px" srcset="https://www.economist.com/img/b/16/21/90/media-assets/image/20230909_DE_EU.jpg 16w, https://www.economist.com/img/b/32/42/90/media-assets/image/20230909_DE_EU.jpg 32w, https://www.economist.com/img/b/48/63/90/media-assets/image/20230909_DE_EU.jpg 48w, https://www.economist.com/img/b/64/84/90/media-assets/image/20230909_DE_EU.jpg 64w, https://www.economist.com/img/b/96/126/90/media-assets/image/20230909_DE_EU.jpg 96w, https://www.economist.com/img/b/128/168/90/media-assets/image/20230909_DE_EU.jpg 128w, https://www.economist.com/img/b/256/336/90/media-assets/image/20230909_DE_EU.jpg 256w, https://www.economist.com/img/b/360/473/90/media-assets/image/20230909_DE_EU.jpg 360w, https://www.economist.com/img/b/384/505/90/media-assets/image/20230909_DE_EU.jpg 384w, https://www.economist.com/img/b/480/631/90/media-assets/image/20230909_DE_EU.jpg 480w, https://www.economist.com/img/b/600/789/90/media-assets/image/20230909_DE_EU.jpg 600w, https://www.economist.com/img/b/834/1097/90/media-assets/image/20230909_DE_EU.jpg 834w, https://www.economist.com/img/b/960/1263/90/media-assets/image/20230909_DE_EU.jpg 960w, https://www.economist.com/img/b/1096/1441/90/media-assets/image/20230909_DE_EU.jpg 1096w, https://www.economist.com/img/b/1280/1684/90/media-assets/image/20230909_DE_EU.jpg 1280w, https://www.economist.com/img/b/1424/1873/90/media-assets/image/20230909_DE_EU.jpg 1424w" src="https://www.economist.com/img/b/1424/1873/90/media-assets/image/20230909_DE_EU.jpg"></figure></div><div orientation="vertical"><h3 orientation="vertical">From the September 9th 2023 edition</h3><p orientation="vertical">Discover stories from this section and more in the list of contents </p><a href="https://www.economist.com/printedition/2023-09-09" data-analytics="sidebar:weekly_edition"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1zm.142 4.5l-1.008 1.062c3.33 3.276 4.194 4.14 4.608 4.5-1.602-.018-3.168-.018-10.242-.018v1.584c7.074 0 8.73 0 10.242-.018-.432.36-1.314 1.206-4.608 4.536l1.008 1.044 6.354-6.354L12.142 5.5z" fill="#2E45B8" fill-rule="nonzero"></path></g></svg><span>Explore the edition</span></a></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Project Gutenberg Open Audiobook Collection (283 pts)]]></title>
            <link>https://marhamilresearch4.blob.core.windows.net/gutenberg-public/Website/index.html</link>
            <guid>37466027</guid>
            <pubDate>Mon, 11 Sep 2023 12:06:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://marhamilresearch4.blob.core.windows.net/gutenberg-public/Website/index.html">https://marhamilresearch4.blob.core.windows.net/gutenberg-public/Website/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=37466027">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="particles">
        
        <p><strong>Thousands of free and open audiobooks powered by Microsoft AI</strong></p>
      </div><div id="Paper">
    <h2>Paper</h2>
    <div>
      <p>
        <h2>For more technical information on the code used to generate these audiobooks please see our IEEE Big Data paper: <a href="https://arxiv.org/abs/2009.08044">Large Scale Intelligent Microservices</a>‍<br></h2>
        
      </p>
      <p>@article{hamilton2020large,<br> &nbsp;title={Large-Scale Intelligent Microservices},<br> &nbsp;author={Hamilton, Mark and Gonsalves, Nick and Lee, Christina <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and Raman, Anand and Walsh, Brendan and Prasad, Siddhartha<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and Banda, Dalitso and Zhang, Lucy and Zhang, Lei and Freeman, William T},<br> &nbsp;journal={arXiv preprint arXiv:2009.08044},<br> &nbsp;year={2020}}</p>
    </div>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The right to data ownership is the only way to take on Big Tech (221 pts)]]></title>
            <link>https://www.telegraph.co.uk/business/2023/09/11/right-data-ownership-big-tech-google-meta-competition/</link>
            <guid>37465972</guid>
            <pubDate>Mon, 11 Sep 2023 12:02:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.telegraph.co.uk/business/2023/09/11/right-data-ownership-big-tech-google-meta-competition/">https://www.telegraph.co.uk/business/2023/09/11/right-data-ownership-big-tech-google-meta-competition/</a>, See on <a href="https://news.ycombinator.com/item?id=37465972">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-test="article-body-text">
  
  

	

	
	

	
	



  
  
    <p>Today, giant technology companies are more powerful than any nation state. Their whims set the political agenda.&nbsp;</p><p>Ian Bremmer calls this “technopolarity”, describing how digital power defines politics and reshapes the world. For example, consider their enthusiasm for AI regulation: laws which could in theory be written and enforced by them.&nbsp;</p><p>This does not seem healthy for either the economy or our democracy – so can anything on Earth stop Big Tech?</p><p>One argument is that we should not worry. As sure as eggs are eggs, we are told, a period of market dominance will be followed by hubris, in a self-correcting cycle.&nbsp;</p><p>Just look at Microsoft, they say. Twenty-five years ago this month, the software company surpassed General Electric to become the most valuable on the planet.&nbsp;</p><p>The same month, a tiny new company with an odd name was formally incorporated. It called itself Google. By 2012, it had overtaken Microsoft’s market capitalisation, although Microsoft has subsequently caught up.&nbsp;</p><p>This week, <a href="https://www.telegraph.co.uk/business/2023/09/10/google-monopoly-lawsuit-antitrust-trial-doj-kent-walker/" target="_blank" rel="noopener noreferrer">the biggest antitrust trial</a> since the American government took on Microsoft begins – and this time it is Google who is in the dock. So don’t worry about monopolies: it seems the system takes care of itself. Everything is for the best in all possible worlds, as Voltaire’s Dr Pangloss assured us.</p>
  
</div><div data-test="article-body-text">
  
  

	

	
	

	
	



  
  
    <p>How neat this is – perhaps too neat. For a start, monopoly profits ought to see private capital flooding into startups and would-be rivals, to grab their market share.&nbsp;</p><p>Startups like Neeva, for example, the search engine I wrote about last year, founded by top ex-Google executives and engineers. Its search results were so good, compared to Google’s, <a href="https://www.telegraph.co.uk/technology/2022/05/30/seen-google-free-future-like-breathing-clean-air/" target="_blank" rel="noopener noreferrer">they were like a breath of fresh&nbsp;air</a>. More fool me, though.</p><p>Neeva announced it was closing down for good in May, unable to make a business out of its superior product. In fact, there has not been a major new platform to challenge the incumbents for well over a decade. Capital keeps finding other things to fund, even some very silly things, like lab grown meat.</p><p>But advocates of strong competition law also have a problem. Very often, <a href="https://www.telegraph.co.uk/news/2023/06/15/the-eu-might-just-break-the-internet/" target="_blank" rel="noopener noreferrer">the authorities are all bark</a>, and no bite – and the European Union is one of the worst offenders in this regard.&nbsp;</p><p>The top Silicon Valley lawyer behind the Microsoft antitrust case, Gary Reback conceded as much when he also advised us not to worry. The mere act of competition scrutiny benefits the market: “the trial is the remedy”, he has said.</p><p>But very often the competition watchdog’s intervention seems to have no impact at all, and has only made the dominant player stronger. Google has run rings around competition authorities by appearing to take it on the chin, then offering up a less onerous behavioural remedy.&nbsp;</p><p>This is accepted and life carries on much as before. Structural remedies do not necessarily improve things much either. When in 2000, a Microsoft breakup was privately shopped around its computer rivals, it seemed nobody wanted to take any of the chopped up pieces of the company.</p><p>So if doing nothing is not an option and doing something is ineffective, what else is left to do?&nbsp;</p>
  
</div><div data-test="article-body-text">
  
  

	

	
	

	
	



  
  
    <p>Something needs to change, for just as advocates of the theory of network effects predicted, online markets are winner-takes all markets. Big Tech only ever seems to get bigger.</p><p>But do not despair – the answer may be a very old one.</p><p>Today, Google is no more about web search or maps than those American candy shops on Oxford Street are in the business of selling sweets. That is what we see when we walk past, but it is not really what they do.&nbsp;</p><p>Google and Meta, Facebook’s parent company, are giant personal data processing companies. But because of a peculiar loophole, they get that data for free – it doesn’t “belong” to anyone.&nbsp;</p><p>If only we were allowed to assert ownership of our data, in the form of a strong property right, we could start to do some interesting things with it.&nbsp;</p><p>We could demand its destruction, because it would be ours, giving us much stronger privacy protection than we enjoy today. We would also be able to trade it and do the one thing we cannot currently – help determine its value.&nbsp;</p><p>Our decisions would help set the price for this data. This is not a popular idea with everyone. Academics and the digital NGOs, a familiar looking blob, hate the prospect, in part because it leaves them with a diminished political role, if any at all.</p><p>The computer scientist Jaron Lanier, the best-known advocate of the idea of stronger property rights, says “some people are horrified by the idea of capitalism online, but this would be a more honest capitalism. The familiar ‘free’ arrangement has been a disaster.” He calls it “Data Dignity”.</p><p>I am not suggesting property rights are a panacea, or a replacement for careful and enlightened competition enforcement by nation states. But in the spirit of experimentation, should we not try the one thing we have not actually tried online yet – capitalism?</p>
  
</div></div>]]></description>
        </item>
    </channel>
</rss>