<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 04 Jan 2024 16:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Power over fiber (198 pts)]]></title>
            <link>https://chaos.social/@f4grx/111697027153656114</link>
            <guid>38865518</guid>
            <pubDate>Thu, 04 Jan 2024 10:51:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chaos.social/@f4grx/111697027153656114">https://chaos.social/@f4grx/111697027153656114</a>, See on <a href="https://news.ycombinator.com/item?id=38865518">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA["Anna's Archive" Shadow Library Blocked Following Publishers' Complaint (140 pts)]]></title>
            <link>https://torrentfreak.com/silenzio-annas-archive-shadow-library-blocked-following-publishers-complaint-240104/</link>
            <guid>38865079</guid>
            <pubDate>Thu, 04 Jan 2024 09:43:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://torrentfreak.com/silenzio-annas-archive-shadow-library-blocked-following-publishers-complaint-240104/">https://torrentfreak.com/silenzio-annas-archive-shadow-library-blocked-following-publishers-complaint-240104/</a>, See on <a href="https://news.ycombinator.com/item?id=38865079">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>

<span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to TorrentFreak." href="https://torrentfreak.com/"><span property="name">Home</span></a><meta property="position" content="1"></span> &gt; <span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to the Anti-Piracy category archives." href="https://torrentfreak.com/category/anti-piracy/"><span property="name">Anti-Piracy</span></a><meta property="position" content="2"></span> &gt; <span property="itemListElement" typeof="ListItem"><a property="item" typeof="WebPage" title="Go to the Site Blocking category archives." href="https://torrentfreak.com/category/anti-piracy/site-blocking/"><span property="name">Site Blocking</span></a><meta property="position" content="3"></span> &gt; <span></span>
</p>
<p>
<span> </span>
Appearing in the wake of the Z-Library shutdown late 2022, shadow library 'Anna's Archive' now bills itself as the "largest truly open library in human history." A complaint filed in December 2023 by the Italian Publishers Association, which represents publishers of books, scientific journals, and digital content, paints a somewhat different picture. As a result, telecoms regulator AGCOM has issued immediate blocking instructions to ISPs.
</p>
</div><div>
<p><a href="https://pixabay.com/photos/woman-book-you-read-library-6784555/"><img decoding="async" src="https://torrentfreak.com/images/anna.jpg" alt="anna's archive" width="300" height="228" srcset="https://torrentfreak.com/images/anna.jpg 888w, https://torrentfreak.com/images/anna-16x12.jpg 16w" sizes="(max-width: 300px) 100vw, 300px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20228'%3E%3C/svg%3E" data-lazy-srcset="https://torrentfreak.com/images/anna.jpg 888w, https://torrentfreak.com/images/anna-16x12.jpg 16w" data-lazy-src="https://torrentfreak.com/images/anna.jpg"></a>Over the past decade, platforms including Sci-Hub, Libgen and Z-Library have broken through a sea of movie, TV show, music and similarly unlicensed platforms to take their own places on the piracy front lines.</p>
<p>In 2022, a platform called Pirate Library Mirror appeared on the scene, courting controversy right from the start after obtaining a full copy of Z-Library before the site’s legal troubles began.</p>
<p>“We deliberately violate the copyright law in most countries. This allows us to do something that legal entities cannot do: making sure books are mirrored far and wide,” the team behind ‘PiLiMi’ wrote.</p>
<p>In November 2022, PiLiMi team member ‘Anna Archivist’ founded ‘<a href="https://www.google.com/search?q=anna%27s+archive">Anna’s Archive</a>‘, a platform promising access to Z-Library and Libgen content from the same interface. Just over a year later, the site describes itself as the “largest truly open library in human history” mirroring Sci-Hub, Libgen, Z-Library, and other platforms, to offer 25.5 million books and 99.4 million papers for download.</p>
<h2>90% of Italian Publishing Market Behind Complaint</h2>
<p>Anna’s Archive is a relative newcomer to the world of online shadow libraries, but its impact has already ensured the inevitable. In common with its counterparts who are already blocked by ISPs in several countries, a year after its launch Anna’s Archive will receive the same treatment, starting in Italy.</p>
<p>On December 4, 2023, the Italian Publishers Association (AIE) filed a copyright complaint against Anna’s Archive. Founded in 1869, AIE represents publishers of books, scientific journals, and digital content; together, these companies control 90% of the local market. AIE’s complaint lists over 30 books, but the association stresses this represents just a sample of the content distributed by Anna’s Archive to which its members hold the rights.</p>
<center><em>A sample of books listed in the complaint</em><a href="https://torrentfreak.com/images/AIE_Annas_Archive_Complaint_Dec_2023.png"><img decoding="async" src="https://torrentfreak.com/images/AIE_Annas_Archive_Complaint_Dec_2023.png" alt="AIE_Annas_Archive_Complaint_Dec_2023" width="650" height="991" srcset="https://torrentfreak.com/images/AIE_Annas_Archive_Complaint_Dec_2023.png 710w, https://torrentfreak.com/images/AIE_Annas_Archive_Complaint_Dec_2023-300x458.png 300w" sizes="(max-width: 650px) 100vw, 650px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20650%20991'%3E%3C/svg%3E" data-lazy-srcset="https://torrentfreak.com/images/AIE_Annas_Archive_Complaint_Dec_2023.png 710w, https://torrentfreak.com/images/AIE_Annas_Archive_Complaint_Dec_2023-300x458.png 300w" data-lazy-src="https://torrentfreak.com/images/AIE_Annas_Archive_Complaint_Dec_2023.png"></a></center>
<p>“The site annas-archive.org calls itself a mirror of various ‘shadow libraries’ and claims to have over 25 million books and nearly 100 million scholarly articles, which it makes available by disseminating numerous links to each work. Unauthorized reproductions of works belonging to Italian publishers number several thousand,” the complaint reads.</p>
<h2>Investigation Led to Ukraine</h2>
<p>An investigation by Italy’s Digital Services Directorate verified that the content listed in the complaint was actually accessible from Anna’s Archive. In view of the facts, that led investigators to believe that this was probably a case of “serious and massive infringement.”</p>
<p>Official papers indicate that the operator of Anna’s Archive proved “unidentifiable” but with assistance from Cloudflare, Epinatura LLC – a hosting provider in Kiev, Ukraine – was identified as the likely host of at least some of the platform’s servers. Notifications were sent to various service providers warning that “spontaneous compliance” with a blocking request filed by the publishers was a potential outcome.</p>
<h2>Decision: Site Must Be Blocked</h2>
<p>With no counterclaims received from the contacted parties and having determined mass infringement on the site, an order to disable https://annas-archive.org through a DNS block was issued to Italian ISPs, to be completed in 48 hours. Visitors to the site are now greeted by the blocking page below in Italian. <em>(translation on the right)</em></p>
<center><a href="https://torrentfreak.com/images/AGCOM_block_page_annas_archive.png"><img decoding="async" src="https://torrentfreak.com/images/AGCOM_block_page_annas_archive.png" alt="AGCOM_block_page_annas_archive" width="650" height="352" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20650%20352'%3E%3C/svg%3E" data-lazy-src="https://torrentfreak.com/images/AGCOM_block_page_annas_archive.png"></a></center>
<p>While Anna’s Archive operates alternative domains that aren’t specifically mentioned in the order (annas-archive.gs, annas-archive.se), the site faces perpetual blocking measures against “all future domain names of the same site.”</p>
<p>If the shadow library wishes to challenge the decision, it has until the middle of February to file a response before the Lazio Regional Administrative Court. All things considered, that seems unlikely.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Science of Concurrent Programs [pdf] (136 pts)]]></title>
            <link>https://lamport.azurewebsites.net/tla/science.pdf</link>
            <guid>38863339</guid>
            <pubDate>Thu, 04 Jan 2024 05:20:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lamport.azurewebsites.net/tla/science.pdf">https://lamport.azurewebsites.net/tla/science.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=38863339">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Lilygo T-Deck: 2.8-inch IPS LCD display, mini keyboard, and ESP32 processor (188 pts)]]></title>
            <link>https://www.lilygo.cc/products/t-deck</link>
            <guid>38862848</guid>
            <pubDate>Thu, 04 Jan 2024 04:02:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lilygo.cc/products/t-deck">https://www.lilygo.cc/products/t-deck</a>, See on <a href="https://news.ycombinator.com/item?id=38862848">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="MainContent" role="main" tabindex="-1">
      <section id="shopify-section-template--15547176517813__main" data-section="template--15547176517813__main">
  
  
  
  
  
  
  
  
<div id="ProductInfo-template--15547176517813__main"><p>LILYGO®</p><p>Portable microcontroller programmer </p><div id="price-template--15547176517813__main" role="status">
  <div><p><span>Regular price</span>
      <span>
        $43.08 USD
      </span>
    </p>
    <p><span>Regular price</span>
        <span>
          <s>
            
              
            
          </s>
        </span><span>Sale price</span>
      <span>
        $43.08 USD
      </span>
    </p>
    <p><small>
      <span>Unit price</span>
      <span>
        <span></span>
        
        <span>&nbsp;per&nbsp;</span>
        <span>
        </span>
      </span>
    </small>
  </p></div><p><span>
      Sale
    </span>

    <span>
      Sold out
    </span></p></div><variant-selects data-section="template--15547176517813__main" data-url="/products/t-deck">
                </variant-selects>

            <p><span>⚠️If you need technical support, you can get a quicker response on our <a href="https://github.com/Xinyuan-LilyGo" target="_blank">GitHub</a> and <a href="https://community.lilygo.cc/" target="_blank">community</a> (<strong>the tech team will check it directly</strong>).</span></p>
<p><strong><span>Different support: </span></strong><span>For order: <a href="mailto:sales@lilygo.cc;">sales@lilygo.cc;</a> </span><span>For product: <a href="mailto:lily@lilygo.cc">lily@lilygo.cc</a></span></p>
<div>
              <details id="Details-collapsible-row-1-template--15547176517813__main">
                <summary>
                  <p>
                    
                    <h2>
                      Instructions Before Order
                    </h2>
                  </p>
                  

                </summary>
                <div id="ProductAccordion-collapsible-row-1-template--15547176517813__main"><p>If you order the wrong product or want to add another, please choose the correct version to re-order and notify us to cancel the wrong order</p><p>When matching orders, if your address is abnormal we will re-match you with the most suitable channel of the same level and replace it</p><p><strong>If your order is over $200, we strongly recommend that you choose DHL or FedEx.</strong></p><p>Generally, except for DHL or FedEx, we will <strong>automatically split orders over $150-200 into two packages </strong>due to customs clearance and loss prevention issues, etc. </p><p>For more please check <a href="https://www.lilygo.cc/pages/more-details-about-the-order" target="_blank" title="More details about the order"><strong>here</strong></a></p></div>
              </details>
            </div><div>
              <details id="Details-collapsible-row-0-template--15547176517813__main">
                <summary>
                  <p>
                    
                    <h2>
                      Shipping &amp; Delivery
                    </h2>
                  </p>
                  

                </summary>
                <div id="ProductAccordion-collapsible-row-0-template--15547176517813__main">
                  <p><strong>Methods &amp; Delivery Time</strong></p><ol><li>Standard Express: 25-30 Days</li><li>EUB: 25-30 Days</li><li>YunTu Express : 10-20&nbsp;Days<br>(<strong>Company name can't be used consignee</strong>)</li><li>DHL: 10-17&nbsp;Days</li><li>FedEx: 10-17&nbsp;Days</li><li>YanWen: 20-35 Days</li></ol><p><strong>Please note that all&nbsp;delivery times are estimated for reference only.</strong></p><p>During ship, maybe some methods of transport do not update the logistic track until the parcel reaches the destination country.</p><p>If you&nbsp;do not see one of the above shipping methods when checking out, it may be because your country does not support that shipping method, please select one of the available channels already listed or contact us for help.</p><p><strong>Please make sure your address is fully filled in</strong> (including province and continent, which will affect the shipping time of your order)</p><p>For more details please check our <a href="https://www.lilygo.cc/pages/copy-of-shipping-delivery-1" target="_blank" title="Shipping &amp; Delivery">shipping policy</a>.</p>
                  
                </div>
              </details>
            </div><div>
              <details id="Details-collapsible-row-2-template--15547176517813__main">
                <summary>
                  <p>
                    
                    <h2>
                      About Tax
                    </h2>
                  </p>
                  

                </summary>
                <div id="ProductAccordion-collapsible-row-2-template--15547176517813__main"><p>At present, the price of the platform does not include any tax. <br>European customers should pay attention to the tax problem after the package reaches the local country, which needs to be borne/handled by the customer, the local policy shall prevail</p><p> [orders <strong>over 150 euros</strong> may generate tax (<em><strong>Amount for reference only</strong></em>), for orders over this amount, we will not inform the customer separately, so please pay attention to it by yourself.]</p></div>
              </details>
            </div><div>
                <p>T-Deck is a pocket-sized gadget with a 2.8-inch, 320 x 240 pixel IPS LCD display, a mini keyboard, and an ESP32 dual-core processor. While it's not exactly a smartphone, you can use your programming knowledge to turn it into a standalone messaging device, or coding software.</p>
<p><img alt="" src="https://cdn.shopify.com/s/files/1/0617/7190/7253/files/LILYGO-T-DECK_2_600x600.jpg?v=1692325851"></p>
<h2>Specifications</h2>
<table>
<tbody>
<tr>
<td><span>MCU</span></td>
<td><span><strong>ESP32-S3FN16R8</strong> Dual-core LX7 microprocessor</span></td>
</tr>
<tr>
<td><span>Wireless Connectivity</span></td>
<td><span>2.4 GHz Wi-Fi &amp; Bluetooth 5 (LE)</span></td>
</tr>
<tr>
<td><span>Development</span></td>
<td><span>Arduino, PlatformlO-IDE, Micropython</span></td>
</tr>
<tr>
<td><span>Flash</span></td>
<td><span>16MB</span></td>
</tr>
<tr>
<td><span>PSRAM</span></td>
<td><span>8MB</span></td>
</tr>
<tr>
<td><span>Battery ADC PIN</span></td>
<td><span>IO04</span></td>
</tr>
<tr>
<td><span>Onboard functions</span></td>
<td>&nbsp;<span data-mce-fragment="1"><strong>Trackball</strong>, Microphone, Speaker</span>
</td>
</tr>
</tbody>
</table>
<div>
<p>&nbsp;<strong>2.8 inch ST7789 SPI Interface IPS LCD:</strong></p>
<p><span data-mce-fragment="1">Resolution: 320 x 240 Full viewing angle</span></p>
</div>
<p><strong>&nbsp;SX1262 LoRa Transceiver (Chip Optional)</strong></p>
<table>
<tbody>
<tr>
<td>Transmit power</td>
<td>+22dBm</td>
</tr>
<tr>
<td>Frequency Optional</td>
<td>&nbsp;433/868/915Mhz</td>
</tr>
</tbody>
</table>
<p><img src="https://cdn.shopify.com/s/files/1/0617/7190/7253/files/lilygo-t-deck_600x600.png?v=1685087351" alt="" data-mce-fragment="1" data-mce-src="https://cdn.shopify.com/s/files/1/0617/7190/7253/files/lilygo-t-deck_600x600.png?v=1685087351"></p><p><img alt="" src="https://cdn.shopify.com/s/files/1/0617/7190/7253/files/LILYGO-T-DECK_1_600x600.jpg?v=1685085907" data-mce-src="https://cdn.shopify.com/s/files/1/0617/7190/7253/files/LILYGO-T-DECK_1_600x600.jpg?v=1685085907"></p>
<h2><strong>Sample Code For Reference [<span data-mce-style="color: #ff2a00;">Github</span>]</strong></h2>
<p><span data-mce-fragment="1">If you need technical support please check the link to find more details.</span></p>
<h2><strong><a href="https://github.com/Xinyuan-LilyGO/T-Deck" data-mce-href="https://github.com/Xinyuan-LilyGO/T-Deck" target="_blank">T-Deck</a></strong></h2>
<p><img alt="" src="https://cdn.shopify.com/s/files/1/0617/7190/7253/files/lilygo_cf03514d-c652-4559-874d-2d4201e4bd20_600x600.jpg?v=1685086008" data-mce-src="https://cdn.shopify.com/s/files/1/0617/7190/7253/files/lilygo_cf03514d-c652-4559-874d-2d4201e4bd20_600x600.jpg?v=1685086008"></p>
<h3><strong>Size</strong></h3>
<p><img src="https://cdn.shopify.com/s/files/1/0617/7190/7253/files/LILYGO-T-DECK_12_600x600.jpg?v=1685086390" alt=""></p>
<h3><strong>Pin Diagram</strong></h3>
<p><img alt="" src="https://cdn.shopify.com/s/files/1/0617/7190/7253/files/LILYGO-T-DECK_6_600x600.jpg?v=1685086214" data-mce-src="https://cdn.shopify.com/s/files/1/0617/7190/7253/files/LILYGO-T-DECK_6_600x600.jpg?v=1685086214"></p>

<p><img src="https://cdn.shopify.com/s/files/1/0617/7190/7253/files/lilygo3_f4a19391-9bee-41ec-a41d-aee00dcaf2f3_600x600.jpg?v=1685086247" alt=""></p>
<p><strong>Without LoRa</strong></p>

<ul>
<li>1 X T-Deck</li>
<li>1 X Pin(6pin)</li>
<li>1 X Power cable</li>
</ul>
<p><img alt="" src="https://cdn.shopify.com/s/files/1/0617/7190/7253/files/H641-without-lora-lilygo_600x600.jpg?v=1691119821"></p>
<p><strong>With LoRa</strong></p>
<p><img alt="" src="https://cdn.shopify.com/s/files/1/0617/7190/7253/files/H642-T-DECK-LILYGO_600x600.jpg?v=1691119875"></p>
              </div><share-button id="Share-template--15547176517813__main">
              
              
            </share-button>
            <a href="https://www.lilygo.cc/products/t-deck">
          View full details
          

        </a>
      </div>

  <product-modal id="ProductModal-template--15547176517813__main">
    
  </product-modal>

  

    
</section><div data-updated-at="2024-01-01T16:44:29Z" data-average-rating="4.57" data-number-of-reviews="7" data-number-of-questions="0" id="shopify-section-template--15547176517813__1662621831b7b38af2" data-product-title="T-Deck" data-id="7452917563573">  <div> <h2>Customer Reviews</h2>         </div> <div><div data-verified-buyer="true" data-review-id="35eb3b71-5e7f-4c09-893c-c43286ac2ec6" data-product-title="T-Deck" data-product-url="/products/t-deck" data-thumb-up-count="0" data-thumb-down-count="0">  <p><b>exzellent device for MeshCom and MeshTastic</b></p><p>with a lot of potential</p>   </div><div data-verified-buyer="true" data-review-id="14928760-f829-409d-af6e-9e385a9db8b8" data-product-title="T-Deck" data-product-url="/products/t-deck" data-thumb-up-count="0" data-thumb-down-count="0">  <p><b>Out of the box</b></p><p>I just started using the T-Deck and so far I like the looks of it and all the features. I have not had a chance to download code but expect to work on the device next week. So far, so very good....</p>   </div><div data-verified-buyer="true" data-review-id="ac2013c3-4f8f-4b64-97a2-9f4df6505f9c" data-product-title="T-Deck" data-product-url="/products/t-deck" data-thumb-up-count="0" data-thumb-down-count="0">  <p><b>perfect LoRa standalone device</b></p><p>A perfect client device for MeshCom (https.//icssw.org/meshcom), only the shell from the github repository needed modifications to support a bigger battery (2000mAh) and to fit the ipex connector of the LoRa antenna.</p>   </div><div data-verified-buyer="true" data-review-id="7a329849-0245-420c-8e46-f5d06ecefeea" data-product-title="T-Deck" data-product-url="/products/t-deck" data-thumb-up-count="0" data-thumb-down-count="0">  <p><b>One small but important thing</b></p><p>I'd really like to be able to purchase a back for the device.</p>   </div><div data-verified-buyer="true" data-review-id="07dfe945-5929-4a92-99a8-6b981ea68e2b" data-product-title="T-Deck" data-product-url="/products/t-deck" data-thumb-up-count="0" data-thumb-down-count="0">  <p><b>T-DECK a very great LoRa allround-Tool</b></p><div><p>.) We use 3D-schema for Lilygo and modiefied to fit a larger BATT
<br>.) We use DEMO-Code and modiefied it to a usefuilly "APP" for our MeshCVom 4.0 project</p>

<p>https.//icssw.org/meshcom</p></div> <p><a target="_blank" rel="nofollow" href="https://judgeme.imgix.net/lilygo/1692117178__t-deck__original.jpg?auto=format" data-mfp-src="https://judgeme.imgix.net/lilygo/1692117178__t-deck__original.jpg?auto=format&amp;w=1024" aria-label="Link to user picture 1"> <img alt="User picture" data-src="https://judgeme.imgix.net/lilygo/1692117178__t-deck__original.jpg?auto=format&amp;w=160" src="https://judgeme.imgix.net/lilygo/1692117178__t-deck__original.jpg?auto=format&amp;w=160"> </a>  </p>  </div></div>  </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SpaceX Illegally Fired Workers Critical of Musk, Federal Agency Says (277 pts)]]></title>
            <link>https://www.nytimes.com/2024/01/03/business/spacex-elon-musk-nlrb-workers.html</link>
            <guid>38862106</guid>
            <pubDate>Thu, 04 Jan 2024 01:42:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/01/03/business/spacex-elon-musk-nlrb-workers.html">https://www.nytimes.com/2024/01/03/business/spacex-elon-musk-nlrb-workers.html</a>, See on <a href="https://news.ycombinator.com/item?id=38862106">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/01/03/business/spacex-elon-musk-nlrb-workers.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Think more about what to focus on (230 pts)]]></title>
            <link>https://www.henrikkarlsson.xyz/p/multi-armed-bandit</link>
            <guid>38861467</guid>
            <pubDate>Thu, 04 Jan 2024 00:02:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.henrikkarlsson.xyz/p/multi-armed-bandit">https://www.henrikkarlsson.xyz/p/multi-armed-bandit</a>, See on <a href="https://news.ycombinator.com/item?id=38861467">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3efc63a4-3759-4c62-8042-bf30abd7a873_1200x1200.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3efc63a4-3759-4c62-8042-bf30abd7a873_1200x1200.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3efc63a4-3759-4c62-8042-bf30abd7a873_1200x1200.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3efc63a4-3759-4c62-8042-bf30abd7a873_1200x1200.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3efc63a4-3759-4c62-8042-bf30abd7a873_1200x1200.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3efc63a4-3759-4c62-8042-bf30abd7a873_1200x1200.jpeg" width="1200" height="1200" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3efc63a4-3759-4c62-8042-bf30abd7a873_1200x1200.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:1200,&quot;width&quot;:1200,&quot;resizeWidth&quot;:1200,&quot;bytes&quot;:135602,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3efc63a4-3759-4c62-8042-bf30abd7a873_1200x1200.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3efc63a4-3759-4c62-8042-bf30abd7a873_1200x1200.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3efc63a4-3759-4c62-8042-bf30abd7a873_1200x1200.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3efc63a4-3759-4c62-8042-bf30abd7a873_1200x1200.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><blockquote><p><em>Almost everyone I’ve ever met would be well-served by spending more time thinking about what to focus on. </em><span>—Sam Altman</span></p></blockquote><p>In May 2020, we parked two moving trucks in the harbor and carried everything we owned from one to the other. Johanna, Maud, and I were leaving Sweden, and Covid restrictions meant we were forbidden from returning once we boarded the ferry. Hence the second truck, which we had gotten a stranger to ferry from the island to us: the Swedish truck had to stay in Sweden.</p><p>The motivation to leave was that we wanted to homeschool Maud, who was 3. In Sweden, this is illegal, so most Swedish homeschoolers end up on one of two islands in the Baltic Sea. On our island, we knew no one. We had no jobs awaiting. We were leaving something, more than going somewhere. The life we had grown piecemeal over 30 years disappeared overnight. We had to figure out what to replace it with. Should I start another software consultancy to support us? Could I write? How would we find a meaningful social context?</p><p>The moldy apartment we rented as we looked for a house has a view of the sea. Every day, deep into winter, I’d walk down to the water and dive from the cliffs. Swimming in the channels between the rocks, I realized I could model our situation using a concept from probability theory.</p><p><span>It was a </span><a href="https://en.wikipedia.org/wiki/Multi-armed_bandit" rel="">multi-armed bandit problem</a><span>. This problem, which, under a different name, had&nbsp;</span><a href="https://www.dropbox.com/s/yhn9prnr5bz0156/1933-thompson.pdf" rel="">first been studied</a><span>&nbsp;by the biologist&nbsp;</span><a href="https://en.wikipedia.org/wiki/William_R._Thompson" rel="">William R. Thompson</a><span>&nbsp;in 1933, centers on a rather surreal thought experiment. A gambler faces a slot machine (“a one-armed bandit”), except this machine doesn’t have one arm—following some twisted dream logic, it has&nbsp;</span><em>k&nbsp;</em><span>arms, arms sticking out in every direction. Some of these arms have a high probability of paying out the jackpot, others are worse. But the gambler does not know which is which.</span></p><p>The problem is pulling the arms in an order that maximizes the expected total gains. ("Gains" could be anything. Early on, the problem was used to design drug trials. There, the jackpot was defined as finding a successful treatment. If you are looking for a partner, talking to people is how you pull the multi-armed bandit and the resonance (or lack thereof) is the payoff.)</p><p><span>The gambler needs to learn new knowledge about the machines&nbsp;</span><em>and simultaneously</em><span>&nbsp;use what they have already learned to optimize their decisions. In the literature, these two activities are referred to as&nbsp;</span><em>exploring</em><span>&nbsp;and&nbsp;</span><em>exploiting.</em><span>&nbsp;You can’t do both things at the same time. When you explore, you are pulling new arms on the bandit trying to figure out their expected payout. When you exploit, you pull the best arm you’ve found. You need to find the right balance. If you spend too little time exploring, you get stuck playing a machine with a low expected payoff. But if you spend too much time exploring, you will earn less than you would if you played the best arm. This is the explore/exploit trade-off.</span></p><p><span>People tend to gravitate to different sides of the explore/exploit spectrum. If you are high on openness, like I am, exploring comes easy. But it is harder to make a commitment and exploit what you’ve learned about yourself and the world. Other people are more committed, but risk being too conventional in their choices. They miss better avenues for their effort. Most, however, tend to do less than optimal of&nbsp;</span><em>both—</em><span>not exploring, not exploiting; but doing things out of blind habit, and half-heartedly.</span></p><p>First, I’ll say a few words about exploration and exploitation in real life. Then I'll return to the question of how to navigate the tradeoff between them.</p><p>There are two kinds of people. Those who do not understand how complex the world is, and those who know that they do not understand how complex the world is.</p><p>To navigate life we create mental models of the world out there, and then we confuse the models for reality. Have you ever noticed this when interacting with someone who has a less accurate model than you: that it’s like they have a VR headset on and are fighting against monsters you know are not there? For instance, there was a Swedish couple who were working at the embassy in New Delhi in the 1990s. They had a housekeeper. On the housekeeper’s birthday, they made her a cake and invited her to eat at their table. She refused. She insisted she had to eat on the floor, or else she’d be reborn as a lower animal. It is tempting to say, “You can take off the headset, there is nothing there. Have some cake.” But we all have headsets on. There is no way of living in direct contact with reality.</p><p>The trick is to collide your mental model with the outside world as often as possible. This is what exploring does. You think you know the distribution of payoffs of the slot machines, but you try something new. You discover that you were wrong. You update your model.</p><p>Many of the mental models I have are things I’ve picked up from others. On closer inspection, it turns up that they too picked up from someone else, who picked it up from someone else—going back to someone who lived in the 1950s. This is not the 1950s. Have some cake.</p><p><span>For instance, as I wrote in “</span><a href="https://www.henrikkarlsson.xyz/p/search-query" rel="">A blog post is a very long and complex search query to find fascinating people and make them route interesting things to your inbox</a><span>,” I wasn’t able to get my blog to “work” until I unlearned the patterns of communication I had picked up from mass media. Blogs are not mass media. They are more powerful than that. I was stuck in the 50s.</span></p><p><span>Some mental models have more leverage than others. I try to focus on these. Realizing that I didn’t understand </span><a href="https://www.henrikkarlsson.xyz/p/being-patient-with-problems" rel="">process</a><span>, or priorities, or </span><a href="https://www.henrikkarlsson.xyz/p/looking-for-alice" rel="">relationships</a><span>, was highly rewarding. Understanding those areas better pays off every day.</span></p><p>If you can break inaccurate mental models, life becomes easier to navigate. But how do you do that? I know two ways.</p><ol><li><p>Find people who understand things better than you and read what they have to say. Read with the intention of answering your questions. If you can’t find the answers, email them.</p></li><li><p><span>Perform experiments. By this I don’t mean do random things. I mean,&nbsp;</span><em>STATE YOUR ASSUMPTIONS </em><span>and&nbsp;</span><em>FIND WAYS TO TEST IF THEY ARE FALSE</em><span>. Most of the time, the slot machine of an experiment yields nothing. But that’s ok. A few will rearrange the world around you.</span></p></li></ol><p>But, as I was saying before, there is a tradeoff. Time spent exploring to gather new information means less time acting on it. Besides, exploiting is often more valuable than it seems, since narrowing your focus to “slot machines” you know are promising can have nonlinear returns.</p><p>As a rule of thumb, you can only do 1 or 2 things well. Some people are exceptional: they can do 3. I’m not exceptional.</p><p><span>I learned this, as many do, when I had my first child. I had been a bit nervous about becoming a father. Having failed to achieve what I had expected I would, I thought strapping a child to my chest meant setting myself up for permanent failure. It did not. When Maud ate about half my time, I had to force myself to make priorities: I would care for her, I would write,&nbsp;</span><em>and I would say no to everything else.</em></p><p>Narrowing my life like this, at least doubled how much I could achieve. When I had more time, I had spread myself too thin to get stuff done.</p><p>This is something I now notice whenever I read biographies of people who have done exceptional work: they lived narrow lives. They allowed themselves to care about less than others do. To take two quotes at random, here is Jony Ives who designed the iPhone:</p><blockquote><p><span>One of the things Steve [Jobs] would say [to me] because he was worried I wasn’t focused — he would say, “How many things have you said no to?” I would tell him I said no to this. And I said no to that. But he knew I wasn’t interested in doing those things. There was no&nbsp;</span><em>sacrifice</em><span>&nbsp;in saying no [to those things]. What focus means is saying no to something that with every bone in your body you think is a phenomenal idea, you wake up thinking about it, but you say no to it because you are focusing on something else.”</span></p></blockquote><p>And here is Werner Herzog, the German filmmaker:</p><blockquote><p>Although for many years I lived hand to mouth—sometimes in semi-poverty—I have lived like a rich man ever since I started making films. Throughout my life I have been able to do what I truly love, which is more valuable than any cash you could throw at me. At a time when friends were establishing themselves by getting university degrees, going into business, building careers and buying houses, I was making films, investing everything back into my work.</p></blockquote><p>In the terminology of multi-armed bandits, they found a good arm. Then they exploited it to the exclusion of everything else.</p><p>Why do some people achieve so many of the things they want, and others not? Do people have a fixed budget of things they can achieve in a lifetime? It doesn’t seem so. Rather, it seems like our achievement budget is a function of the number of priorities we have. Interestingly, it seems to be a&nbsp;nonlinear&nbsp;function. Meaning that if you go from 4 priorities to 3, you can get, say, 10 percent more done; but if you go from 4 to 1, you get 400 percent more done. (I’m obviously making these numbers up.) If I look at Elon Musk, I have a hard time even grasping that he has the same number of hours in a day as I have. But he has, of course. We all do. It is just that his decisions have compounded because of a sharp focus.</p><p><span>Why would focus compound? Part of it is time. If you care about less, you spend more time doing what you care about most. Also, you are always nonconsciously processing the thing you focus on.</span><strong>&nbsp;</strong><span>So cutting priorities means you work even when it looks like you’re not working. These days,</span><strong>&nbsp;</strong><span>I’ll spend the afternoon playing with the kids, doing the dishes, repairing the houses—being busy in a mind-clearing way. Then, when I sit down to write the next morning, I can type 700 words without thinking. The ideas have been churning in my head, just below the surface of conscious thought, and come fully formed.</span></p><p>When I was younger, I was never this lucky. It is partly because I was less skilled. But it is also partly because I would interrupt the nonconscious processing back then. Unintentionally, I would tell my brain to focus on something else—a conflict in a TV series I was watching, for instance. I would watch an episode before bed, and the cliffhanger would open a loop in my head. That loop would be churning in my head as I slept; I woke to a blank page. I don’t have time for that anymore. I make sure to always have an open loop concerning my writing. And I close every other loop—by wrapping it up as fast as I can, or by writing it down on a list, or, preferably, by not opening the loop at all.</p><p>Allocating more time and mental processing power alone doesn’t explain the nonlinearity, though. More time is just a linear increase. I would guess the nonlinearity comes from:</p><ul><li><p><strong>Focus accelerates the accumulation of skills and accurate world models.</strong><span>&nbsp;In open-ended domains, such as writing, relationships, or business, there is nearly endless room for skill growth. When you spend more time, you get a better model of the situation which allows you to allocate your time better, which accelerates your rate of learning, and so on, in a nonlinear way.</span></p></li><li><p><strong>Focus attracts “resources.”&nbsp;</strong><span>This is obvious in business: if you have a ferocious focus investors will start following you around begging you to take their money. Then you can use the money to pull even faster on the arms of the bandit. This is true in writing, too: the more I write, the more interesting people my blog attracts. They start giving me feedback and advice which helps me write better, which, in a flywheel, attracts more interesting people (and some money). If you are curious and kind to people around you, you attract strong supportive networks. Networks have nonlinear properties.</span></p></li></ul><p>But for me, as a person for whom narrow focus is against my instincts, the most remarkable thing about it is how rich it feels. My life these days is small and boring. I bicycle across the same fields every day, I notice how the wind turbines turn to face the wind, I rarely travel, and I spend my spare time staring at a Word document. Annie Dillard calls the writer’s life colorless to the point of sensory deprivation. That fits. But, as she also knows, there is another kind of color that can only be discovered three years down a writing hole. It is a subtle, nightly color; your eyes need time to adjust to the dark before you can see them. You wouldn't believe their beauty if I told you.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8666c64-0109-4d9a-a9f0-fc73aa5a3e7b_3264x1958.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8666c64-0109-4d9a-a9f0-fc73aa5a3e7b_3264x1958.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8666c64-0109-4d9a-a9f0-fc73aa5a3e7b_3264x1958.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8666c64-0109-4d9a-a9f0-fc73aa5a3e7b_3264x1958.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8666c64-0109-4d9a-a9f0-fc73aa5a3e7b_3264x1958.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8666c64-0109-4d9a-a9f0-fc73aa5a3e7b_3264x1958.jpeg" width="1456" height="873" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b8666c64-0109-4d9a-a9f0-fc73aa5a3e7b_3264x1958.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:873,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1150745,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8666c64-0109-4d9a-a9f0-fc73aa5a3e7b_3264x1958.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8666c64-0109-4d9a-a9f0-fc73aa5a3e7b_3264x1958.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8666c64-0109-4d9a-a9f0-fc73aa5a3e7b_3264x1958.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb8666c64-0109-4d9a-a9f0-fc73aa5a3e7b_3264x1958.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>So, as I was saying, I was swimming by the cliffs. Every day, walking up and down the coast I found new places to dive in. I got interested in high diving—something I had been too much of a coward to do as a kid. I felt a childish excitement for water. The kids on the beach would look at me, whisper, and laugh. That is one of the pleasures of youth. The pleasure of being a dad is knowing I was having more fun than them.</p><p>But beyond being a dad, I didn’t know what to do with my life. Swimming and thinking about the things I’ve said in this essay, I made a decision. I would approach my situation algorithmically. I would apply rules to decide when to explore and when to exploit to counteract my natural tendencies (which would push me to do too little of both).</p><p>There are several algorithmic solutions to the multi-armed bandit problem, going back to Thompson sampling in the 1930s, all the way up to contemporary algorithms used in machine learning, such as EXP3 and Upper Confidence Bounds. What they all have in common is some version of: 1) prioritize exploration early on and 2) dial up exploitation as the situation becomes more clear. If you are new in a city, it makes sense to meet as many people as possible. If you find someone you love hanging with early, you will have years of happiness. But if you are about to leave, it makes more sense to hang with your best friends. Even if you found someone you liked more, you wouldn’t have any time to hang out. The amount of exploration that is optimal depends on the complexity of the problem and the time horizon.</p><p>What I decided to do was this. I would divide the next 30 months into three parts. For the first ten months, I would allow myself to explore freely. After that, I’d switch to exploring 2/3 of the time and using 1/3 to double down on the most interesting opportunity I had found, then I’d do 1/3 exploring and 2/3 exploiting, and so on.</p><p>We can think of the amount of time I spent on explorative open-ended search as my “temperature.” When you heat atoms, they bounce around faster; when you heat a life, it becomes more explorative. When you cool it, you narrow down and spend more time exploiting what you know. Here’s what it looks like when you search for the highest peak while gradually reducing the “temperature”:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44a6a537-8851-4039-aef6-c61c66bcc26f_500x161.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44a6a537-8851-4039-aef6-c61c66bcc26f_500x161.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44a6a537-8851-4039-aef6-c61c66bcc26f_500x161.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44a6a537-8851-4039-aef6-c61c66bcc26f_500x161.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44a6a537-8851-4039-aef6-c61c66bcc26f_500x161.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44a6a537-8851-4039-aef6-c61c66bcc26f_500x161.gif" width="500" height="161" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/44a6a537-8851-4039-aef6-c61c66bcc26f_500x161.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:161,&quot;width&quot;:500,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44a6a537-8851-4039-aef6-c61c66bcc26f_500x161.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44a6a537-8851-4039-aef6-c61c66bcc26f_500x161.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44a6a537-8851-4039-aef6-c61c66bcc26f_500x161.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44a6a537-8851-4039-aef6-c61c66bcc26f_500x161.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Notice that the search starts in the area to the right, which does not contain the highest peak. If it had been a focused search early on it would have gotten stuck at a local optimum. (This is one of the reasons why education is a nightmare for many, by the way. Schooling forces you to decide what you will pursue long before you get to try it. It makes your future, more knowledgeable self, the servant of the younger, ignorant self.)</p><p>It felt comforting to put strict rules on myself like this, especially in the early part of the search. I felt stressed that I would not find the right thing to spend my time on and support my family. This made me want to go into exploit mode too soon. Knowing that there would be a time for focus later made me more willing to try anything and fail. I learned the piano and played around with coding. I wrote a few chapters of a novel. I got involved with a kindergarten/goat farm and studied the history of the island. I took a job at an art gallery. </p><p>After 10 months, I reduced the amount of randomness for the first time. I decided to spend 1/3 of my spare time programming. But I kept exploring 2/3s of my days.</p><p>The next time I dialed down the temperature I had started this blog. Since writing it was more interesting than programming, I stopped coding and switched 2/3s of my spare time to the blog. In retrospect, this seems like an obvious choice. Writing had always been my main obsession. But it wasn’t obvious at the time. I had for years been frustrated that no one was interested in what I wrote. The magazines that had published me when I was in my early twenties wanted nothing to do with the stuff I was exploring now. Writing seemed like a dead end.</p><p><span>I didn’t tell anyone that I was writing a blog. Having my friends read it would have made it harder for me to experiment and do things that risked embarrassment or failure. I wanted to put myself in a social context where I was rewarded for </span><a href="https://www.henrikkarlsson.xyz/p/writing-as-communion" rel="">exploring my illegible potential</a><span>.</span></p><p>It worked. In January 2023, I dialed down the temperature to 0. I had landed on a peak I could not have imagined when we arrived on the island: being on track to supporting my family by emailing my thoughts to strangers. Even better, it is a small family writing business since Johanna and I work as a team these days (and we project that the essays will be our main income by the end of 2024).    </p><p>If I had decided on a path when I swam by the cliffs, before exploring, this possibility would not have occurred to me. Thoughtful emails did not seem like the type of thing that can support a family. Had I known it was, I would have thought I lacked what it takes to pull it off. This is an important thing to keep in mind: you don’t know until you try. </p><p><span>Warmly,</span><br><span>Henrik</span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Airtable acquires Airplane (161 pts)]]></title>
            <link>https://www.airplane.dev/blog/airtable</link>
            <guid>38861271</guid>
            <pubDate>Wed, 03 Jan 2024 23:36:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.airplane.dev/blog/airtable">https://www.airplane.dev/blog/airtable</a>, See on <a href="https://news.ycombinator.com/item?id=38861271">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I’m excited to announce that Airplane is being acquired by <a href="https://www.airtable.com/" rel="noreferrer"><u>Airtable</u></a>, an app building platform that connects teams through shared data.</p><p>We started Airplane to democratize access to software tooling, and we found Airtable to be a fantastic match to continue that mission. Over 450,000 organizations use Airtable and over 50% of the Fortune 500 are paying customers. Beyond just a highly aligned roadmap and vision, we found Airtable shared our culture of rapid iteration and a high bar for quality. The Airplane team will be joining Airtable and is excited to continue our mission at scale.</p><p>As part of this transition, we will unfortunately be sunsetting the Airplane platform on <strong>March 1</strong>. We’ve closed new signups, and existing users will be receiving an email with further details.</p><p>To all our users and customers: we are sincerely grateful for your interest, support, and feedback over the years. It’s been a pleasure and an honor to learn about, collaborate with, and assist your organizations.</p><p>I wish you all the best in the new year.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amber: Smalltalk for the Web (110 pts)]]></title>
            <link>https://amber-lang.net/</link>
            <guid>38861203</guid>
            <pubDate>Wed, 03 Jan 2024 23:26:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://amber-lang.net/">https://amber-lang.net/</a>, See on <a href="https://news.ycombinator.com/item?id=38861203">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <h2>A <span>new language</span> and <span>live environment</span> made for the web.
        </h2>

        <p>The Amber language is deeply inspired by Smalltalk. It is designed to make client-side development faster
                and easier.
                Amber includes a live development environment with a class browser, workspace, unit test runner,
                transcript, object inspector and debugger.</p>

        <p>Amber is written in itself, including the compiler, and compiles into efficient JavaScript, mapping
                one-to-one with the JS equivalent.</p>

        

        

    </div><div id="content">
        <blockquote>
    The app is in Amber Smalltalk and the html is done with Silk. Silk is very exciting and makes web programming fun again. I am completely enthusiastic about it 🙂 !
    <cite>‑‑&nbsp;@c_haider, on making <a href="https://covidcrt.uber.space/">covidcrt.uber.space</a> website.</cite>
</blockquote>

<div>
  
  

  <dl>
    <dt>So...What is it about again?</dt>
    <dd>
      <p>Amber is a language (derived from <a href="http://en.wikipedia.org/wiki/Smalltalk">Smalltalk</a>) and environment built for the web.</p>
With Amber, client-side web development finally gets the power and productivity that exists in other Smalltalk dialects.
    </dd>

    <dt>Why should I care?</dt>
    <dd>
      <p>Having a true live &amp; incremental development environment where you can build your application interactively in the browser is unbeatable.</p>
    </dd>

    <dt>Why a Smalltalk dialect?</dt>
    <dd>
      <p>Smalltalk stands head and shoulders above most other languages for clarity, conciseness, and human-friendliness.</p>
      <p>As a language, it is immensely clean and mature, both syntactically and semantically. It is a pure OO language, with objects all the way down.</p>
    </dd>

    <dt>But what about all the JS ecosystem?</dt>
    <dd>
      <p>Amber plays very well with the outer world. You can interact with JavaScript objects seamlessly, and even inspect them as any Amber object.</p>
      <p>Evaluating JavaScript object methods is transparent and makes using libraries a breeze.</p>
    </dd>

  </dl>

</div>

<blockquote>
  It's a 3D WebGL game engine that is very easy to get to work with Amber. Specifically though I am using its WebVR integration.  This is a pretty nice way to do VR development.  Being able to take the headset off, change a method, and then put the headset back on without having to restart is pretty nice.
  <cite>‑‑&nbsp;@ZenChess, on using babylon.js in Amber</cite>
</blockquote>

<div>
    <h2 id="get-started">Getting started</h2>
    <p>
        Follow the README in <a href="https://lolg.it/amber/amber">git</a>.
    </p>
</div>

<div id="getinvolved">
  <h2>Get involved!</h2>

  <h3>Meet the people behind Amber</h3>
  <ul>
    <li>Amber hackers can be found on the Rocket.chat instance here: <a href="https://chat.amber-lang.net/">https://chat.amber-lang.net</a>.</li>
    <li>Most of Amber discussion and help happens on the <a href="http://groups.google.com/group/amber-lang">Google Group</a>.</li>
  </ul>

  <h3>Contributing to the project</h3>
  <p>In a sharing mood? Contributions to Amber are very much welcome!</p>
  <ul>
    <li>The Amber source code is hosted on <a href="https://lolg.it/amber/amber">lolg.it</a>. You can fork the main repository and send pull requests.</li>
    <li>You can also submit issues on the <a href="https://lolg.it/amber/amber/issues">bug tracker</a>.</li>
  </ul>
</div>

<div>
  <h2 id="mentions">Mentions</h2>
    <p>
        Thanks for creating Amber:
        <a href="http://nicolas.petton.fr/">Nicolas Petton</a>.
    </p>
    <p>
        Thanks for supporting Amber:
        <a href="http://www.instantiations.com/"><img src="https://amber-lang.net/images/VA-Smalltalk-Logo-Hori-Trans.png"></a> –
        they reward open-source contributors a free license of VAST.
    </p>
    <p>
        Amber is developed mostly in
        <a href="https://brave.com/amb399"><img src="https://amber-lang.net/brave-assets/Logos/Brave/PNG/primary/logotype/dark/brave-logotype-full-color.png"></a>,
        a privacy-oriented browser from JavaScript creator Brendan Eich, that blocks ads and trackers.
    </p>
</div>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Curious Case of MD5 (194 pts)]]></title>
            <link>https://katelynsills.com/law/the-curious-case-of-md5/</link>
            <guid>38861034</guid>
            <pubDate>Wed, 03 Jan 2024 23:05:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://katelynsills.com/law/the-curious-case-of-md5/">https://katelynsills.com/law/the-curious-case-of-md5/</a>, See on <a href="https://news.ycombinator.com/item?id=38861034">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/Article"><header><small>December 31, 2023</small></header><section itemprop="articleBody">
<p><small>Average reading time: 11 minutes.</small></p>
<p>Recently I came across a puzzling fact: the International Criminal Court <a href="https://www.icc-cpi.int/sites/default/files/RelatedRecords/0902ebd18037cb09.pdf">hashes electronic evidence with MD5</a><sup id="fnref-1"><a href="#fn-1">1</a></sup>.</p>
<p>What’s wrong with that? Well, MD5 is badly broken. So broken that experts have been <a href="https://www.schneier.com/blog/archives/2008/12/forging_ssl_cer.html">saying for over a decade that “no one should be using MD5 anymore.”</a></p>
<p>Given the wide variety of better alternatives, using MD5 today makes no sense.</p>
<p>And more puzzling: it’s not just the International Criminal Court using MD5. Apparently, the entire American legal and forensics community uses MD5.</p>
<p>So, why <em>are</em> lawyers using broken, outdated technology?</p>
<p>In asking this, I should be clear: I’m not a lawyer and I’m not a cryptographer. I’m a software engineer<sup id="fnref-2"><a href="#fn-2">2</a></sup> and consultant in applied cryptography. And I suspect that I might be one of the only people interested in both cryptography and law, otherwise someone else would have written this article already.</p>
<p>There are a few reasons why lawyers are still using broken, outdated technology. Fundamentally, the legal community disputes that MD5 <em>is</em> broken, for them. Yes, they say, MD5 is broken for <em>encryption,</em> but since they’re not doing encryption, it’s fine for them to use it.</p>
<p>This post is my exploration of how this dispute came about, and whether the legal community is correct in believing they can safely use MD5.</p>
<h2>How law uses hashing</h2>
<p>Let’s start with how MD5 and other hash functions are used in law.</p>
<p>Let’s imagine that we have 200 documents that we must store as evidence for a case. It’s important that each of these 200 documents is 1) identified correctly, 2) copied correctly from the originals, and 3) unchanged since the time we collected it.</p>
<p>We can use something called a <strong>cryptographic hash function</strong> to accomplish these goals. A cryptographic hash function produces a unique identifier (called a “hash” or “hash value”) for a document, based on the contents of the document.</p>
<p>Importantly, if anything at all in the document changes, even just a single character added or subtracted, the hash value will be different. As Judge Paul W. Grimm points out in <em>Lorraine v. Markel American Ins. Co.</em>, hashing a document can help to easily distinguish between the “final” (and thus legally operative) version and earlier versions. While both documents might look similar to the human eye, the hash value of two very similar but different documents is completely different.</p>
<p>We can test this out. For instance, in less than a second of computation, the 783 page book <a href="https://www.gutenberg.org/cache/epub/4300/pg4300.txt">Ulysses</a> by James Joyce hashes to:</p>
<p><small>c6061f63b03774425a5b06083af4c9cb33f6f47cf0efd71b1258828f3332a604</small></p>
<p>And if we change a <em>single letter</em> three-quarters of the way through the book, we get a completely different hash:</p>
<p><small>2e605eca536c927d629fec4c8ab4759af59bd2ba15ef562989766c348b1a72a6</small></p>
<p>And thus, by hashing, we know–<em>without reading a single line of the book</em>–that the book has been altered and is no longer the original. And we can do the same check for millions of documents at a time. That’s pretty useful as a time-saving device!</p>
<p>So, in law, we want to be able to identify a document with a particular id, and then be able to know that 1) we’re all talking about the same document and aren’t confusing it with some other document or a different version, and that 2) our copy of the document is the same as the original, and 3) that the document hasn’t been changed or altered without our awareness. Cryptographic hash functions help us accomplish all of these goals.</p>
<h2>MD5 is obsolete</h2>
<p>There are many cryptographic hash functions, and only a few are recommended for current use. The others are obsolete. MD5 is extremely old, in tech years. It was introduced in 1992, problems were noticed in 1996 and 2005, and by 2008, it was deemed unusable. <a href="https://www.kb.cert.org/vuls/id/836068">Carnegie Mellon’s Software Engineering Institute</a> stated in 2008:</p>
<blockquote>
<p>“Do not use the MD5 algorithm. Software developers, Certification Authorities, website owners, and users should avoid using the MD5 algorithm in any capacity. As previous research has demonstrated, it should be considered cryptographically broken and unsuitable for further use.”</p>
</blockquote>
<p>Note that this was not controversial in any sense among technologists. But, we find ourselves in the year 2023, a full 15 years after being told not to use MD5, with the legal community using and even <em>recommending</em> MD5.</p>
<h2>What exactly is wrong with MD5, and does it matter for law?</h2>
<p>As far as I can tell as an outsider, there’s very little current discussion in the legal community about whether MD5 should be replaced. For instance, the <a href="https://thesedonaconference.org/sites/default/files/publications/2_ESI_Evidence_and_Admissibility_0.pdf">Sedona Conference 2021 commentary on ESI Evidence &amp; Admissibility</a> mentions MD5 as one of the “most commonly used algorithms” for hashing, and eDiscovery and forensics software such as <a href="https://blogs.opentext.com/what-counts-as-a-duplicate-in-your-ediscovery-software/">OpenText</a> and <a href="https://www.exterro.com/blog/md5-hashing-the-foundation-of-a-defensible-e-discovery-process">Exterro</a> are apparently proud to use MD5.</p>
<p>And, when the legal community does attempt to grapple with MD5 being broken, the problem is dismissed because it is “only broken for encryption”, whatever that means.</p>
<p>For example, the Sedona commentary references a <a href="https://web.archive.org/web/20081208024210/https://www.forensicmag.com/articles.asp?pid=238">2008 article by Don L. Lewis</a> (now taken down and only available through the Internet Archive) that has the following exchange:</p>
<blockquote>
<p>When I testified recently a defense attorney brought this subject up. The testimony went something like this.</p>
<p>Q. “Mr. Lewis, are you aware that the MD5 algorithm has been compromised?”</p>
<p>A. “Yes, I am.”</p>
<p>Q. “So, its use to authenticate evidence is no longer valid!”</p>
<p>A. “No, the use of the MD5 algorithm is still a valid function for authentication.”</p>
<p>Q. “Why is that?”</p>
<p>A. “There are multiple uses for hash algorithms. One is cryptography (encryption), another is identification, and another is authentication. In digital evidence forensics, we use hash algorithms for known file identification and evidence authentication, which differs from its use in encryption.”</p>
</blockquote>
<p>Here, Lewis is simply wrong. The primary use of hashing in cryptography <em>is</em> identification and authentication, same as in law. Of course, the context differs – outside of the law, the hashed files don’t end up in a courtroom. But, this idea that the tech world mainly uses cryptographic hash functions for something other than identification and authentication is wrong.</p>
<h2>How Tech Uses Hashing</h2>
<p>Cryptographic hash functions have a <a href="https://en.wikipedia.org/wiki/Cryptographic_hash_function#Applications">number of use cases in tech</a>.</p>
<p>First, hashes are often used to verify that data was not tampered with or altered in transit. For instance, a file download page might also include a signed hash so that users can check that the file they have on their computer is an identical copy of the original. This is very similar to the legal use case of checking that a forensic image is an identical copy of the original.</p>
<p>Second, a major use of hashes in tech is in digital signatures. A digital signature is like a handwritten signature in that it connotes that the signer has either witnessed or approved of whatever is being signed, but a digital signature is very different in form. A digital signature uses cryptography, and effectively proves to the general public that only the person or entity that knows a particular secret has signed the file or data, without revealing the secret to anyone else.</p>
<p>There’s a lot more to understand about digital signatures (see <a href="https://youtu.be/-HWhqxSiXoE?si=baa9QGmtZ_ULFpxz">a talk on digital signatures for economists that I gave at APEE</a>), but for our purposes, the important thing about digital signature schemes is that they use cryptographic hash functions to get a unique identifier for the file, and it’s the unique identifier that is signed, not the entire file.</p>
<p>Now, there are a few more ways in which cryptographic hash functions are used in tech – for instance, as a way of identifying files in peer-to-peer file sharing and content-addressed storage (so still identification, contrary to Lewis.) But let’s focus on the digital signature example.</p>
<p>Back in <a href="https://web.archive.org/web/20050615005726/http://www.cits.rub.de/MD5Collisions/">2005, a group of researchers showed how the problems with MD5 could be used in a real world attack</a>. Their <a href="https://www.sos.cs.ru.nl/applications/courses/security2015/md5collisions/index.html">example</a> is a bit convoluted, but essentially, they showed that an attacker could make the victim sign a harmful document. Importantly, it requires that the attacker can create two files, one innocuous and one harmful, both of which have the same MD5 hash. MD5 is broken in this particular way: given access to two files, it is <a href="https://natmchugh.blogspot.com/2014/10/how-i-created-two-images-with-same-md5.html">easy to change some data in both of them to result in the same MD5 hash</a>.</p>
<p>In their particular example, the innocuous file is a letter of recommendation, and the harmful file is a security clearance, both postscript files. (If your computer can’t read postscript files, here are screenshots of both so that you see what each says. The hashes of the screenshots are of course not the same; only the postscript files have the same hash.)</p>
<p><span>
      <a href="https://katelynsills.com/static/b3ea75f553b7a86c2ab0098ee143112a/c54b3/letter_of_rec_screenshot.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Letter of Recommendation" title="" src="https://katelynsills.com/static/b3ea75f553b7a86c2ab0098ee143112a/f058b/letter_of_rec_screenshot.png" srcset="https://katelynsills.com/static/b3ea75f553b7a86c2ab0098ee143112a/c26ae/letter_of_rec_screenshot.png 158w,
https://katelynsills.com/static/b3ea75f553b7a86c2ab0098ee143112a/6bdcf/letter_of_rec_screenshot.png 315w,
https://katelynsills.com/static/b3ea75f553b7a86c2ab0098ee143112a/f058b/letter_of_rec_screenshot.png 630w,
https://katelynsills.com/static/b3ea75f553b7a86c2ab0098ee143112a/c54b3/letter_of_rec_screenshot.png 727w" sizes="(max-width: 630px) 100vw, 630px" loading="lazy" decoding="async">
  </a>
    </span></p>
<figcaption>The Innocuous File: A Letter of Recommendation</figcaption>
<p><span>
      <a href="https://katelynsills.com/static/f08546445993515ebc143fdf7792af8a/37523/order_screenshot.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Security Clearance" title="" src="https://katelynsills.com/static/f08546445993515ebc143fdf7792af8a/f058b/order_screenshot.png" srcset="https://katelynsills.com/static/f08546445993515ebc143fdf7792af8a/c26ae/order_screenshot.png 158w,
https://katelynsills.com/static/f08546445993515ebc143fdf7792af8a/6bdcf/order_screenshot.png 315w,
https://katelynsills.com/static/f08546445993515ebc143fdf7792af8a/f058b/order_screenshot.png 630w,
https://katelynsills.com/static/f08546445993515ebc143fdf7792af8a/37523/order_screenshot.png 720w" sizes="(max-width: 630px) 100vw, 630px" loading="lazy" decoding="async">
  </a>
    </span></p>
<figcaption>The Harmful File: The Security Clearance Order</figcaption>
<p>In this case, the key element is that a third party believes that Julius Caesar has signed a security clearance for Alice when he has not, and Alice has set up both files for the deception. In other words, the deception must be preplanned.</p>
<p>We can easily imagine a similar example in the legal world, even without digital signatures. For example, suppose that a witness has identified a particular file provided by the opposition as the legally operative document. And that the particular file is identified by its MD5 hash. It would be easy for the opposing party to create two documents with the same MD5 hash, which say different things. Depending on which is advantageous to them at the time, they could say either document is the legally operative version, since the legally operative version was only identified by the broken MD5 hash. Of course, like the postscript files, on close and thorough manual inspection, the two files will be found to be distinct. But the whole point of cryptographic hashes is that we don’t need to use manual inspection, especially for large files that are hundreds of pages long.</p>
<p>Another example: suppose someone is arrested for possession of CSAM, but ahead of time, they have manipulated both the harmful image and an innocuous image so that they both have the same MD5 hash. If law enforcement only records the MD5 hash of the image, law enforcement cannot say for sure, based on the hash alone, whether the image was CSAM or not. The image must be manually inspected, which defeats the entire purpose of the cryptographic hash. A cryptographic hash should <em>uniquely identify</em> a file.</p>
<p>It might be possible that the adversarial setup of the American court system can help in this regard, to provide an opportunity for manual inspection. But this assumes that the attack is detected in the first place. What if the attack is undetected? What if it is subtle enough, or the files large enough that it is not caught? For instance, what if the harmful file has a slightly different excel function hidden in the background, that changes the calculations just a tad, but enough to make a difference?</p>
<p>It’s best not to gamble, especially since the fix is trivial.</p>
<h2>MD5 isn’t collision resistant</h2>
<p>In each of the above examples, MD5 is badly broken in a particular way: it isn’t collision resistant. Practically, for MD5, this means that given access to two files, it is easy, <a href="https://github.com/corkami/collisions">given the right software</a>, to change both of them such that they will produce the same MD5 hash. In other words, to produce a particular collision. In fact, producing <em>any</em> arbitrary collision would be enough to make MD5 unusable.</p>
<p>But, I want to be careful not to claim too much. So far, it is <em>not</em> possible, given a particular, non-manipulated file and the MD5 hash for that file, to find a second file that has that same MD5 hash. This is called a second-preimage attack. This is good news in the case in which we know an attacker or a malicious party doesn’t have access to files before we hash them. For instance, as Lewis points out, the <a href="https://www.nist.gov/itl/ssd/software-quality-group/national-software-reference-library-nsrl/about-nsrl/nsrl-introduction">“hash sets”</a> (databases of MD5 hashes for known operating system and application files) are unlikely to be affected by MD5’s lack of collision-resistance, given that the hashes are of presumably non-manipulated files.</p>
<p>However, as society becomes more tech-adept, and particularly when <a href="https://casetext.com/case/united-states-v-vayner-1">the defendants and their associates are familiar with forgery</a>, malicious manipulation of evidence should be prevented as much as possible, especially since doing so is trivial: just use an up-to-date, unbroken hash function instead of MD5.</p>
<h2>What to use instead of MD5</h2>
<p>NIST’s <a href="https://csrc.nist.gov/projects/hash-functions/nist-policy-on-hash-functions">current recommendation is the SHA3 family</a>. It does not have any of the flaws that MD5 does, and works just as well.</p>
<p>If speed is a particular concern, perhaps with very large files, <a href="https://github.com/BLAKE3-team/BLAKE3?tab=readme-ov-file">Blake3</a> is one of the fastest algorithms.</p>
<p>Do not use SHA1, which is also broken.</p>
<p>Shameless plug: If you’d like help integrating SHA3 or Blake3 into your current practice, feel free to contact me at <a href="mailto:katelynsills@gmail.com">katelynsills@gmail.com</a> for consulting.</p>
<h2>How did this happen? The difference between law and tech</h2>
<p>So, going back to our question: why <em>are</em> lawyers using broken, outdated technology?</p>
<p>In addition to lawyers mistakenly believing Lewis’ argument that MD5 is only broken for “encryption,” I think there are a few more reasons:</p>
<ol>
<li>
<p>The legal community started using MD5 at the same time that they started hashing, and it stuck.</p>
</li>
<li>
<p>The legal community doesn’t know that better alternatives to MD5 exist, because digital forensics is isolated from computer science.</p>
</li>
<li>
<p>The legal community hasn’t updated to the latest technology because legal culture isn’t accustomed to (and might even be hostile to) performing continuity-breaking updates at the pace that tech requires.</p>
</li>
</ol>
<p>For example, in the previously mentioned Sedona commentary, the mention of MD5 is from a 2007 text: <a href="https://www.uscourts.gov/sites/default/files/eldscpkt_1.pdf"><em>Managing Discovery of Electronic Information: A Pocket Guide for Judges</em>.</a> It’s admirable that a 2007 text included hashing, and it is somewhat excusable that MD5 is recommended, since the text was published before the major publicity of MD5’s flaws in 2008.</p>
<p>But making software recommendations based on what a 2007 text recommends is deeply negligent. There’s no reason to believe that what was true in 2007 is true now. In software years, 2007 is ancient history.</p>
<p>However, in legal culture, 2007 is yesterday, which is why this reference wasn’t seen as problematic.</p>
<p>It also appears that the legal community does not know that better alternatives to MD5 exist – algorithms that don’t have known weaknesses. For example, many forensics textbooks only reference MD5 and SHA1, both of which are <em>not</em> recommended for use by NIST. The textbook <em>Digital Evidence and Computer Crime</em> states:</p>
<blockquote>
<p>“One approach to addressing concerns about weaknesses in any given hash algorithm is to use two independent hash algorithms. For this reason, some digital forensic tools automatically calculate both the MD5 and SHA-1 hash value of acquired digital evidence, and other tools provide multiple hashing options for the user to select.”</p>
</blockquote>
<p>Or, you know, you could just use <em>a single cryptographic hash function that actually works.</em></p>
<p>Lastly, I think a big part of the problem is cultural. Legal culture is hostile to rapid updates, and for good reason: lawyers and law-makers are used to running “code” on human beings, who simply can’t handle rapid change.</p>
<h2>Law needs to learn to update</h2>
<p>In the Morality of Law, legal theorist Lon L. Fuller identified the ways in which we can “fail to make law.” One of those is “introducing such frequent changes in the rules that the subject cannot orient his action by them.”</p>
<p>This makes sense: people need to be able to know what the rules are and will be, so that they can plan their actions accordingly.</p>
<p>But software, unlike law, doesn’t run on people. Software runs on computers, which can handle rapid change. And software has to defend against motivated attackers, who will exploit any weaknesses in old code at a rapid pace themselves. So software has a very different culture than law. In software security, it is expected that if any weakness is found or even suspected, it is patched and everyone updates as quickly as possible to new code. At times, this can be jarring, but for the most part, it happens entirely outside of human perception.</p>
<p>When law adopts technology, law must also adopt tech culture: a culture of regular updates for that particular technology. Otherwise, as in the case of hashing, the legal community will be going through the motions but won’t be able to reap the full benefits of the technology, because what they are using is badly outdated.</p>
<p>Imagine being able to instantly uniquely identify a document, no matter how large it is, and know for certain that it is distinct from any other document, without any confusion. Imagine knowing that the document is 1) identified correctly, 2) copied correctly from the originals, and 3) unchanged since the time we collected it.</p>
<p>If law uses a proper cryptographic hash function rather than MD5, we can be certain of all three.</p>
<h3>Notes</h3>
</section><hr></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Understand how transformers work by demystifying the math behind them (430 pts)]]></title>
            <link>https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/</link>
            <guid>38859976</guid>
            <pubDate>Wed, 03 Jan 2024 21:10:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/">https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/</a>, See on <a href="https://news.ycombinator.com/item?id=38859976">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">



<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p>In this blog post, we’ll do an end-to-end example of the math within a transformer model. The goal is to get a good understanding of how the model works. To make this manageable, we’ll do lots of simplification. As we’ll be doing quite a bit of the math by hand, we’ll reduce the dimensions of the model. For example, rather than using embeddings of 512 values, we’ll use embeddings of 4 values. This will make the math easier to follow! We’ll use random vectors and matrices, but you can use your own values if you want to follow along.</p>
<p>As you’ll see, the math is not that complicated. The complexity comes from the number of steps and the number of parameters. I recommend you to read the <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> blog before reading this blog post (or reading in parallel). It’s a great blog post that explains the transformer model in a very intuitive (and illustrative!) way and I don’t intend to explain what it’s already explained there. My goal is to explain the “how” of the transformer model, not the “what”. If you want to dive even deeper, check out the famous original paper: <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>.</p>
<p><strong>Prerequisites</strong></p>
<p>A basic understanding of linear algebra is required - we’ll mostly do simple matrix multiplications, so no need to be an expert. Apart from that, basic understanding of Machine Learning and Deep Learning will be useful.</p>
<p><strong>What is covered here?</strong></p>
<ul>
<li>An end-to-end example of the math within a transformer model during inference</li>
<li>An explanation of attention mechanisms</li>
<li>An explanation of residual connections and layer normalization</li>
<li>Some code to scale it up!</li>
</ul>
<p>Without further ado, let’s get started! The original transformer model has two parts: encoder and decoder. Our goal will be to use this model as a translation tool! We’ll first focus on the encoder part.</p>
<section id="encoder">
<h2 data-anchor-id="encoder">Encoder</h2>
<p>The whole goal of the encoder is to generate a rich embedding representation of the input text. This embedding will capture semantic information about the input, and will then be passed to the decoder to generate the output text. The encoder is composed of a stack of N layers. Before we jump into the layers, we need to see how to pass the words (or tokens) into the model.</p>
<div>

<p>Embeddings are a somewhat overused term. We’ll first create an embedding that will be the input to the encoder. The encoder also outputs an embedding (also called hidden states sometimes). The decoder will also receive an embedding! 😅 The whole point of an embedding is to represent a token as a vector.</p>
</div>
<section id="embedding-the-text">
<h3 data-anchor-id="embedding-the-text">1. Embedding the text</h3>
<p>Let’s say that we want to translate “Hello World” from English to Spanish. The first step is to turn each input token into a vector using an embedding algorithm. This is a learned encoding. Usually we use a big vector size such as 512, but let’s do 4 for our example so we can keep the maths manageable. I’ll assign some random values to each token (as mentioned, this mapping is usually learned by the model).</p>
<p>Hello -&gt; [1,2,3,4]</p>
<p>World -&gt; [2,3,4,5]</p>
<p>We can represent our input as a single matrix</p>
<p><span>\[
E = \begin{bmatrix}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 3 &amp; 4 &amp; 5
\end{bmatrix}
\]</span></p>
<div>

<p>Although we could manage the two embeddings as separate vectors, it’s easier to manage them as a single matrix. This is because we’ll be doing matrix multiplications as we move forward!</p>
</div>
</section>
<section id="positional-encoding">
<h3 data-anchor-id="positional-encoding">2 Positional encoding</h3>
<p>The embedding above has no information about the position of the word in the sentence, so we need to feed some positional information. The way we do this is by adding a positional encoding to the embedding. There are different choices on how to obtain these - we could use a learned embedding or a fixed vector. The original paper uses a fixed vector as they see almost no difference between the two approaches (see section 3.5 of the original paper). We’ll use a fixed vector as well. Sine and cosine functions have a wave-like pattern, and they repeat over time. By using these functions, each position in the sentence gets a unique yet consistent pattern of numbers. These are the functions they use in the paper (section 3.5):</p>
<p><span>\[
PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\]</span></p>
<p><span>\[
PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\]</span></p>
<p>The idea is to interpolate between sine and cosine for each value in the embedding (even indices will use sine, odd indices will use cosine). Let’s calculate them for our example!</p>
<p>For “Hello”</p>
<ul>
<li>i = 0 (even): PE(0,0) = sin(0 / 10000^(0 / 4)) = sin(0) = 0</li>
<li>i = 1 (odd): PE(0,1) = cos(0 / 10000^(2*1 / 4)) = cos(0) = 1</li>
<li>i = 2 (even): PE(0,2) = sin(0 / 10000^(2*2 / 4)) = sin(0) = 0</li>
<li>i = 3 (odd): PE(0,3) = cos(0 / 10000^(2*3 / 4)) = cos(0) = 1</li>
</ul>
<p>For “World”</p>
<ul>
<li>i = 0 (even): PE(1,0) = sin(1 / 10000^(0 / 4)) = sin(1 / 10000^0) = sin(1) ≈ 0.84</li>
<li>i = 1 (odd): PE(1,1) = cos(1 / 10000^(2*1 / 4)) = cos(1 / 10000^0.5) ≈ cos(0.01) ≈ 0.99</li>
<li>i = 2 (even): PE(1,2) = sin(1 / 10000^(2*2 / 4)) = sin(1 / 10000^1) ≈ 0</li>
<li>i = 3 (odd): PE(1,3) = cos(1 / 10000^(2*3 / 4)) = cos(1 / 10000^1.5) ≈ 1</li>
</ul>
<p>So concluding</p>
<ul>
<li>“Hello” -&gt; [0, 1, 0, 1]</li>
<li>“World” -&gt; [0.84, 0.99, 0, 1]</li>
</ul>
<p>Note that these encodings have the same dimension as the original embedding.</p>
</section>
<section id="add-positional-encoding-and-embedding">
<h3 data-anchor-id="add-positional-encoding-and-embedding">3. Add positional encoding and embedding</h3>
<p>We now add the positional encoding to the embedding. This is done by adding the two vectors together.</p>
<p>“Hello” = [1,2,3,4] + [0, 1, 0, 1] = [1, 3, 3, 5] “World” = [2,3,4,5] + [0.84, 0.99, 0, 1] = [2.84, 3.99, 4, 6]</p>
<p>So our new matrix, which will be the input to the encoder, is:</p>
<p><span>\[
E = \begin{bmatrix}
1 &amp; 3 &amp; 3 &amp; 5 \\
2.84 &amp; 3.99 &amp; 4 &amp; 6
\end{bmatrix}
\]</span></p>
<p>If you look at the original paper’s image, what we just did is the bottom left part of the image (the embedding + positional encoding).</p>
<div>
<figure>
<p><a href="https://arxiv.org/abs/1706.03762"><img src="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/transformer.png"></a></p>
<figcaption>Transformer model from the original “attention is all you need” paper</figcaption>
</figure>
</div>
</section>
<section id="self-attention">
<h3 data-anchor-id="self-attention">4. Self-attention</h3>
<section id="matrices-definition">
<h4 data-anchor-id="matrices-definition">4.1 Matrices Definition</h4>
<p>We’ll now introduce the concept of multi-head attention. Attention is a mechanism that allows the model to focus on certain parts of the input. Multi-head attention is a way to allow the model to jointly attend to information from different representation subspaces. This is done by using multiple attention heads. Each attention head will have its own K, V, and Q matrices.</p>
<p>Let’s use 2 attention heads for our example. We’ll use random values for these matrices. Each matrix will be a 4x3 matrix. With this, each matrix will transform the 4-dimensional embeddings into 3-dimensional keys, values, and queries. This reduces the dimensionality for attention mechanism, which helps in managing the computational complexity. Note that using a too small attention size will hurt the performance of the model. Let’s use the following values (just random values):</p>
<p><strong>For the first head</strong></p>
<p><span>\[
\begin{align*}
WK1 &amp;= \begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{bmatrix}, \quad
WV1 &amp;= \begin{bmatrix}
0 &amp; 1 &amp; 1 \\
1  &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{bmatrix}, \quad
WQ1 &amp;= \begin{bmatrix}
0 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 0
\end{bmatrix}
\end{align*}
\]</span></p>
<p><strong>For the second head</strong></p>
<p><span>\[
\begin{align*}
WK2 &amp;= \begin{bmatrix}
0 &amp; 1 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{bmatrix}, \quad
WV2 &amp;= \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0
\end{bmatrix}, \quad
WQ2 &amp;= \begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 1
\end{bmatrix}
\end{align*}
\]</span></p>
</section>
<section id="keys-queries-and-values-calculation">
<h4 data-anchor-id="keys-queries-and-values-calculation">4.2 Keys, queries, and values calculation</h4>
<p>We now need to multiply our input embeddings with the weight matrices to obtain the keys, queries, and values.</p>
<p><strong>Key calculation</strong></p>
<p><span>\[
\begin{align*}
E \times WK1 &amp;= \begin{bmatrix}
1 &amp; 3 &amp; 3 &amp; 5 \\
2.84 &amp; 3.99 &amp; 4 &amp; 6
\end{bmatrix}
\begin{bmatrix}
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{bmatrix} \\
&amp;= \begin{bmatrix}
(1 \times 1) + (3 \times 0) + (3 \times 1) + (5 \times 0) &amp; (1 \times 0) + (3 \times 1) + (3 \times 0) + (5 \times 1) &amp; (1 \times 1) + (3 \times 0) + (3 \times 1) + (5 \times 0) \\
(2.84 \times 1) + (3.99 \times 0) + (4 \times 1) + (6 \times 0) &amp; (2.84 \times 0) + (4 \times 1) + (4 \times 0) + (6 \times 1) &amp; (2.84 \times 1) + (4 \times 0) + (4 \times 1) + (6 \times 0)
\end{bmatrix} \\
&amp;= \begin{bmatrix}
4 &amp; 8 &amp; 4 \\
6.84 &amp; 9.99 &amp; 6.84
\end{bmatrix}
\end{align*}
\]</span></p>
<p>Ok, I actually do not want to do the math by hand for all of these - it gets a bit repetitive plus it breaks the site. So let’s cheat and use NumPy to do the calculations for us.</p>
<p>We first define the matrices</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>import</span> numpy <span>as</span> np</span>
<span id="cb1-2"></span>
<span id="cb1-3">WK1 <span>=</span> np.array([[<span>1</span>, <span>0</span>, <span>1</span>], [<span>0</span>, <span>1</span>, <span>0</span>], [<span>1</span>, <span>0</span>, <span>1</span>], [<span>0</span>, <span>1</span>, <span>0</span>]])</span>
<span id="cb1-4">WV1 <span>=</span> np.array([[<span>0</span>, <span>1</span>, <span>1</span>], [<span>1</span>, <span>0</span>, <span>0</span>], [<span>1</span>, <span>0</span>, <span>1</span>], [<span>0</span>, <span>1</span>, <span>0</span>]])</span>
<span id="cb1-5">WQ1 <span>=</span> np.array([[<span>0</span>, <span>0</span>, <span>0</span>], [<span>1</span>, <span>1</span>, <span>0</span>], [<span>0</span>, <span>0</span>, <span>1</span>], [<span>1</span>, <span>0</span>, <span>0</span>]])</span>
<span id="cb1-6"></span>
<span id="cb1-7">WK2 <span>=</span> np.array([[<span>0</span>, <span>1</span>, <span>1</span>], [<span>1</span>, <span>0</span>, <span>1</span>], [<span>1</span>, <span>1</span>, <span>0</span>], [<span>0</span>, <span>1</span>, <span>0</span>]])</span>
<span id="cb1-8">WV2 <span>=</span> np.array([[<span>1</span>, <span>0</span>, <span>0</span>], [<span>0</span>, <span>1</span>, <span>1</span>], [<span>0</span>, <span>0</span>, <span>1</span>], [<span>1</span>, <span>0</span>, <span>0</span>]])</span>
<span id="cb1-9">WQ2 <span>=</span> np.array([[<span>1</span>, <span>0</span>, <span>1</span>], [<span>0</span>, <span>1</span>, <span>0</span>], [<span>1</span>, <span>0</span>, <span>0</span>], [<span>0</span>, <span>1</span>, <span>1</span>]])</span></code></pre></div>
<p>And let’s confirm that I didn’t make any mistakes in the calculations above.</p>
<div>
<div id="cb2"><pre><code><span id="cb2-1">embedding <span>=</span> np.array([[<span>1</span>, <span>3</span>, <span>3</span>, <span>5</span>], [<span>2.84</span>, <span>3.99</span>, <span>4</span>, <span>6</span>]])</span>
<span id="cb2-2">K1 <span>=</span> embedding <span>@</span> WK1</span>
<span id="cb2-3">K1</span></code></pre></div>
<div>
<pre><code>array([[4.  , 8.  , 4.  ],
       [6.84, 9.99, 6.84]])</code></pre>
</div>
</div>
<p>Phew! Let’s now get the values and queries</p>
<p><strong>Value calculations</strong></p>
<div>
<pre><code>array([[6.  , 6.  , 4.  ],
       [7.99, 8.84, 6.84]])</code></pre>
</div>
<p><strong>Query calculations</strong></p>
<div>
<pre><code>array([[8.  , 3.  , 3.  ],
       [9.99, 3.99, 4.  ]])</code></pre>
</div>
<p>Let’s skip the second head for now and focus on the first head final score. We’ll come back to the second head later.</p>
</section>
<section id="attention-calculation">
<h4 data-anchor-id="attention-calculation">4.3 Attention calculation</h4>
<p>Calculating the attention score requires a couple of steps:</p>
<ol type="1">
<li>Calculate the dot product of the query with each key</li>
<li>Divide the result by the square root of the dimension of the key vector</li>
<li>Apply a softmax function to obtain the attention weights</li>
<li>Multiply each value vector by the attention weights</li>
</ol>
<section id="dot-product-of-query-with-each-key">
<h5 data-anchor-id="dot-product-of-query-with-each-key">4.3.1 Dot product of query with each key</h5>
<p>The score for “Hello” requires calculating the dot product of q1 with each key vector (k1 and k2)</p>
<p><span>\[
\begin{align*}
q1 \cdot k1 &amp;= \begin{bmatrix} 8 &amp; 3 &amp; 3 \end{bmatrix} \cdot \begin{bmatrix} 4 \\ 8 \\ 4 \end{bmatrix} \\
&amp;= 8 \cdot 4 + 3 \cdot 8 + 3 \cdot 4 \\
&amp;= 68
\end{align*}
\]</span></p>
<p>In matrix world, that would be Q1 multiplied by the transpose of K1</p>
<p><span>\[\begin{align*}
Q1 \times K1^\top &amp;= \begin{bmatrix} 8 &amp; 3 &amp; 3 \\ 9.99 &amp; 3.99 &amp; 4 \end{bmatrix} \times \begin{bmatrix} 4 &amp; 6.84 \\ 8 &amp; 9.99 \\ 4 &amp; 6.84 \end{bmatrix} \\
&amp;= \begin{bmatrix}
    8 \cdot 4 + 3 \cdot 8 + 3 \cdot 4 &amp; 8 \cdot 6.84 + 3 \cdot 9.99 + 3 \cdot 6.84 \\
    9.99 \cdot 4 + 3.99 \cdot 8 + 4 \cdot 4 &amp; 9.99 \cdot 6.84 + 3.99 \cdot 9.99 + 4 \cdot 6.84
    \end{bmatrix} \\
&amp;= \begin{bmatrix}
    68 &amp; 105.21 \\
    87.88 &amp; 135.5517
    \end{bmatrix}
\end{align*}\]</span></p>
<p>I’m prone to do mistakes, so let’s confirm with Python once again</p>
<div>
<div id="cb8"><pre><code><span id="cb8-1">scores1 <span>=</span> Q1 <span>@</span> K1.T</span>
<span id="cb8-2">scores1</span></code></pre></div>
<div>
<pre><code>array([[ 68.    , 105.21  ],
       [ 87.88  , 135.5517]])</code></pre>
</div>
</div>
</section>
<section id="divide-by-square-root-of-dimension-of-key-vector">
<h5 data-anchor-id="divide-by-square-root-of-dimension-of-key-vector">4.3.2 Divide by square root of dimension of key vector</h5>
<p>We then divide the scores by the square root of the dimension (d) of the keys (3 in this case, but 64 in the original paper). Why? For large values of d, the dot product grows too large (we’re adding the multiplication of a bunch of numbers, after all, leading to high values). And large values are bad! We’ll discuss soon more about this.</p>
<div>
<div id="cb10"><pre><code><span id="cb10-1">scores1 <span>=</span> scores1 <span>/</span> np.sqrt(<span>3</span>)</span>
<span id="cb10-2">scores1</span></code></pre></div>
<div>
<pre><code>array([[39.2598183 , 60.74302182],
       [50.73754166, 78.26081048]])</code></pre>
</div>
</div>
</section>
<section id="apply-softmax-function">
<h5 data-anchor-id="apply-softmax-function">4.3.3 Apply softmax function</h5>
<p>We then softmax to normalize so they are all positive and add up to 1.</p>
<div title="What is softmax?">
<p>Softmax is a function that takes a vector of values and returns a vector of values between 0 and 1, where the sum of the values is 1. It’s a nice way of obtaining probabilities. It’s defined as follows:</p>
<p><span>\[
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}
\]</span></p>
<p>Don’t be intimidated by the formula - it’s actually quite simple. Let’s say we have the following vector:</p>
<p><span>\[
x = \begin{bmatrix} 1 &amp; 2 &amp; 3 \end{bmatrix}
\]</span></p>
<p>The softmax of this vector would be:</p>
<p><span>\[
\text{softmax}(x) = \begin{bmatrix} \frac{e^1}{e^1 + e^2 + e^3} &amp; \frac{e^2}{e^1 + e^2 + e^3} &amp; \frac{e^3}{e^1 + e^2 + e^3} \end{bmatrix} = \begin{bmatrix} 0.09 &amp; 0.24 &amp; 0.67 \end{bmatrix}
\]</span></p>
<p>As you can see, the values are all positive and add up to 1.</p>
</div>
<div>
<div id="cb12"><pre><code><span id="cb12-1"><span>def</span> softmax(x):</span>
<span id="cb12-2">    <span>return</span> np.exp(x) <span>/</span> np.<span>sum</span>(np.exp(x), axis<span>=</span><span>1</span>, keepdims<span>=</span><span>True</span>)</span>
<span id="cb12-3"></span>
<span id="cb12-4"></span>
<span id="cb12-5">scores1 <span>=</span> softmax(scores1)</span>
<span id="cb12-6">scores1</span></code></pre></div>
<div>
<pre><code>array([[4.67695573e-10, 1.00000000e+00],
       [1.11377182e-12, 1.00000000e+00]])</code></pre>
</div>
</div>
</section>
<section id="multiply-value-matrix-by-attention-weights">
<h5 data-anchor-id="multiply-value-matrix-by-attention-weights">4.3.4 Multiply value matrix by attention weights</h5>
<p>We then multiply times the value matrix</p>
<div>
<div id="cb14"><pre><code><span id="cb14-1">attention1 <span>=</span> scores1 <span>@</span> V1</span>
<span id="cb14-2">attention1</span></code></pre></div>
<div>
<pre><code>array([[7.99, 8.84, 6.84],
       [7.99, 8.84, 6.84]])</code></pre>
</div>
</div>
<p>Let’s combine 4.3.1, 4.3.2, 4.3.3, and 4.3.4 into a single formula using matrices (this is from section 3.2.1 of the original paper):</p>
<p><span>\[
Attention(Q,K,V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V
\]</span></p>
<p>Yes, that’s it! All the math we just did can easily be encapsulated in the attention formula above! Let’s now translate this to code!</p>
<div id="cb16"><pre><code><span id="cb16-1"><span>def</span> attention(x, WQ, WK, WV):</span>
<span id="cb16-2">    K <span>=</span> x <span>@</span> WK</span>
<span id="cb16-3">    V <span>=</span> x <span>@</span> WV</span>
<span id="cb16-4">    Q <span>=</span> x <span>@</span> WQ</span>
<span id="cb16-5"></span>
<span id="cb16-6">    scores <span>=</span> Q <span>@</span> K.T</span>
<span id="cb16-7">    scores <span>=</span> scores <span>/</span> np.sqrt(<span>3</span>)</span>
<span id="cb16-8">    scores <span>=</span> softmax(scores)</span>
<span id="cb16-9">    scores <span>=</span> scores <span>@</span> V</span>
<span id="cb16-10">    <span>return</span> scores</span></code></pre></div>
<div>
<div id="cb17"><pre><code><span id="cb17-1">attention(embedding, WQ1, WK1, WV1)</span></code></pre></div>
<div>
<pre><code>array([[7.99, 8.84, 6.84],
       [7.99, 8.84, 6.84]])</code></pre>
</div>
</div>
<p>We confirm we got same values as above. Let’s chear and use this to obtain the attention scores the second attention head:</p>
<div>
<div id="cb19"><pre><code><span id="cb19-1">attention2 <span>=</span> attention(embedding, WQ2, WK2, WV2)</span>
<span id="cb19-2">attention2</span></code></pre></div>
<div>
<pre><code>array([[8.84, 3.99, 7.99],
       [8.84, 3.99, 7.99]])</code></pre>
</div>
</div>
<p>If you’re wondering how come the attention is the same for the two embeddings, it’s because the softmax is taking our scores to 0 and 1. See this:</p>
<div>
<div id="cb21"><pre><code><span id="cb21-1">softmax(((embedding <span>@</span> WQ2) <span>@</span> (embedding <span>@</span> WK2).T) <span>/</span> np.sqrt(<span>3</span>))</span></code></pre></div>
<div>
<pre><code>array([[1.10613872e-14, 1.00000000e+00],
       [4.95934510e-20, 1.00000000e+00]])</code></pre>
</div>
</div>
<p>This is due to bad initialization of the matrices and small vector sizes. Large differences in the scores before applying softmax will just be amplified with softmax, leading to one value being close to 1 and others close to 0. In practice, our initial embedding matrices’ values were maybe too high, leading to high values for the keys, values, and queries, which just grew larger as we multiplied them.</p>
<p>Remember when we were dividing by the square root of the dimension of the keys? This is why we do that. If we don’t do that, the values of the dot product will be too large, leading to large values after the softmax. In this case, though, it seems it wasn’t enough given our small values! As a short-term hack, we can scale down the values by a larger amount than the square root of 3. Let’s redefine the attention function but scaling down by 30. This is not a good long-term solution, but it will help us get different values for the attention scores. We’ll get back to a better solution later.</p>
<div id="cb23"><pre><code><span id="cb23-1"><span>def</span> attention(x, WQ, WK, WV):</span>
<span id="cb23-2">    K <span>=</span> x <span>@</span> WK</span>
<span id="cb23-3">    V <span>=</span> x <span>@</span> WV</span>
<span id="cb23-4">    Q <span>=</span> x <span>@</span> WQ</span>
<span id="cb23-5"></span>
<span id="cb23-6">    scores <span>=</span> Q <span>@</span> K.T</span>
<span id="cb23-7">    scores <span>=</span> scores <span>/</span> <span>30</span>  <span># we just changed this</span></span>
<span id="cb23-8">    scores <span>=</span> softmax(scores)</span>
<span id="cb23-9">    scores <span>=</span> scores <span>@</span> V</span>
<span id="cb23-10">    <span>return</span> scores</span></code></pre></div>
<div>
<div id="cb24"><pre><code><span id="cb24-1">attention1 <span>=</span> attention(embedding, WQ1, WK1, WV1)</span>
<span id="cb24-2">attention1</span></code></pre></div>
<div>
<pre><code>array([[7.54348784, 8.20276657, 6.20276657],
       [7.65266185, 8.35857269, 6.35857269]])</code></pre>
</div>
</div>
<div>
<div id="cb26"><pre><code><span id="cb26-1">attention2 <span>=</span> attention(embedding, WQ2, WK2, WV2)</span>
<span id="cb26-2">attention2</span></code></pre></div>
<div>
<pre><code>array([[8.45589591, 3.85610456, 7.72085664],
       [8.63740591, 3.91937741, 7.84804146]])</code></pre>
</div>
</div>
</section>
<section id="heads-attention-output">
<h5 data-anchor-id="heads-attention-output">4.3.5 Heads’ attention output</h5>
<p>The next layer of the encoder will expect a single matrix, not two. The first step will be to concatenate the two heads’ outputs (section 3.2.2 of the original paper)</p>
<div>
<div id="cb28"><pre><code><span id="cb28-1">attentions <span>=</span> np.concatenate([attention1, attention2], axis<span>=</span><span>1</span>)</span>
<span id="cb28-2">attentions</span></code></pre></div>
<div>
<pre><code>array([[7.54348784, 8.20276657, 6.20276657, 8.45589591, 3.85610456,
        7.72085664],
       [7.65266185, 8.35857269, 6.35857269, 8.63740591, 3.91937741,
        7.84804146]])</code></pre>
</div>
</div>
<p>We finally multiply this concatenated matrix by a weight matrix to obtain the final output of the attention layer. This weight matrix is also learned! The dimension of the matrix ensures we go back to the same dimension as the embedding (4 in our case).</p>
<div>
<div id="cb30"><pre><code><span id="cb30-1"><span># Just some random values</span></span>
<span id="cb30-2">W <span>=</span> np.array(</span>
<span id="cb30-3">    [</span>
<span id="cb30-4">        [<span>0.79445237</span>, <span>0.1081456</span>, <span>0.27411536</span>, <span>0.78394531</span>],</span>
<span id="cb30-5">        [<span>0.29081936</span>, <span>-</span><span>0.36187258</span>, <span>-</span><span>0.32312791</span>, <span>-</span><span>0.48530339</span>],</span>
<span id="cb30-6">        [<span>-</span><span>0.36702934</span>, <span>-</span><span>0.76471963</span>, <span>-</span><span>0.88058366</span>, <span>-</span><span>1.73713022</span>],</span>
<span id="cb30-7">        [<span>-</span><span>0.02305587</span>, <span>-</span><span>0.64315981</span>, <span>-</span><span>0.68306653</span>, <span>-</span><span>1.25393866</span>],</span>
<span id="cb30-8">        [<span>0.29077448</span>, <span>-</span><span>0.04121674</span>, <span>0.01509932</span>, <span>0.13149906</span>],</span>
<span id="cb30-9">        [<span>0.57451867</span>, <span>-</span><span>0.08895355</span>, <span>0.02190485</span>, <span>0.24535932</span>],</span>
<span id="cb30-10">    ]</span>
<span id="cb30-11">)</span>
<span id="cb30-12">Z <span>=</span> attentions <span>@</span> W</span>
<span id="cb30-13">Z</span></code></pre></div>
<div>
<pre><code>array([[ 11.46394285, -13.18016471, -11.59340253, -17.04387829],
       [ 11.62608573, -13.47454936, -11.87126395, -17.4926367 ]])</code></pre>
</div>
</div>
<p>The image from <a href="https://jalammar.github.io/illustrated-transformer/">The Ilustrated Transformer</a> encapsulates all of this in a single image <img src="http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" alt="Attention"></p>
</section>
</section>
</section>
<section id="feed-forward-layer">
<h3 data-anchor-id="feed-forward-layer">5. Feed-forward layer</h3>
<section id="basic-feed-forward-layer">
<h4 data-anchor-id="basic-feed-forward-layer">5.1 Basic feed-forward layer</h4>
<p>After the self-attention layer, the encoder has a feed-forward neural network (FFN). This is a simple network with two linear transformations and a ReLU activation in between. The Illustrated Transformer blog post does not dive into it, so let me briefly explain a bit more. The goal of the FFN is to process and transformer the representation produced by the attention mechanism. The flow is usually as follows (see section 3.3 of the original paper):</p>
<ol type="1">
<li><strong>First linear layer:</strong> this usually expands the dimensionality of the input. For example, if the input dimension is 512, the output dimension might be 2048. This is done to allow the model to learn more complex functions. In our simple of example with dimension of 4, we’ll expand to 8.</li>
<li><strong>ReLU activation:</strong> This is a non-linear activation function. It’s a simple function that returns 0 if the input is negative, and the input if it’s positive. This allows the model to learn non-linear functions. The math is as follows:</li>
</ol>
<p><span>\[
\text{ReLU}(x) = \max(0, x)
\]</span></p>
<ol start="3" type="1">
<li><strong>Second linear layer:</strong> This is the opposite of the first linear layer. It reduces the dimensionality back to the original dimension. In our example, we’ll reduce from 8 to 4.</li>
</ol>
<p>We can represent all of this as follows</p>
<p><span>\[
\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2
\]</span></p>
<p>Just as a reminder, the input for this layer is the Z we calculated in the self-attention above. Here are the values as a reminder</p>
<p><span>\[
Z =
\begin{bmatrix}
11.46394281 &amp; -13.18016469 &amp; -11.59340253 &amp; -17.04387833 \\
11.62608569 &amp; -13.47454934 &amp; -11.87126395 &amp; -17.49263674
\end{bmatrix}
\]</span></p>
<p>Let’s now define some random values for the weight matrices and bias vectors. I’ll do it with code, but you can do it by hand if you feel patient!</p>
<div id="cb32"><pre><code><span id="cb32-1">W1 <span>=</span> np.random.randn(<span>4</span>, <span>8</span>)</span>
<span id="cb32-2">W2 <span>=</span> np.random.randn(<span>8</span>, <span>4</span>)</span>
<span id="cb32-3">b1 <span>=</span> np.random.randn(<span>8</span>)</span>
<span id="cb32-4">b2 <span>=</span> np.random.randn(<span>4</span>)</span></code></pre></div>
<p>And now let’s write the forward pass function</p>
<div id="cb33"><pre><code><span id="cb33-1"><span>def</span> relu(x):</span>
<span id="cb33-2">    <span>return</span> np.maximum(<span>0</span>, x)</span>
<span id="cb33-3"></span>
<span id="cb33-4"><span>def</span> feed_forward(Z, W1, b1, W2, b2):</span>
<span id="cb33-5">    <span>return</span> relu(Z.dot(W1) <span>+</span> b1).dot(W2) <span>+</span> b2</span></code></pre></div>
<div>
<div id="cb34"><pre><code><span id="cb34-1">output_encoder <span>=</span> feed_forward(Z, W1, b1, W2, b2)</span>
<span id="cb34-2">output_encoder</span></code></pre></div>
<div>
<pre><code>array([[ -3.24115016,  -9.7901049 , -29.42555675, -19.93135286],
       [ -3.40199463,  -9.87245924, -30.05715408, -20.05271018]])</code></pre>
</div>
</div>
</section>
<section id="encapsulating-everything-the-random-encoder">
<h4 data-anchor-id="encapsulating-everything-the-random-encoder">5.2 Encapsulating everything: The Random Encoder</h4>
<p>Let’s now write some code to have the multi-head attention and the feed-forward, all together in the encoder block.</p>
<div>

<p>The code optimizes for understanding and educational purposes, not for performance! Don’t judge too hard!</p>
</div>
<div id="cb36"><pre><code><span id="cb36-1">d_embedding <span>=</span> <span>4</span></span>
<span id="cb36-2">d_key <span>=</span> d_value <span>=</span> d_query <span>=</span> <span>3</span></span>
<span id="cb36-3">d_feed_forward <span>=</span> <span>8</span></span>
<span id="cb36-4">n_attention_heads <span>=</span> <span>2</span></span>
<span id="cb36-5"></span>
<span id="cb36-6"><span>def</span> attention(x, WQ, WK, WV):</span>
<span id="cb36-7">    K <span>=</span> x <span>@</span> WK</span>
<span id="cb36-8">    V <span>=</span> x <span>@</span> WV</span>
<span id="cb36-9">    Q <span>=</span> x <span>@</span> WQ</span>
<span id="cb36-10"></span>
<span id="cb36-11">    scores <span>=</span> Q <span>@</span> K.T</span>
<span id="cb36-12">    scores <span>=</span> scores <span>/</span> np.sqrt(d_key)</span>
<span id="cb36-13">    scores <span>=</span> softmax(scores)</span>
<span id="cb36-14">    scores <span>=</span> scores <span>@</span> V</span>
<span id="cb36-15">    <span>return</span> scores</span>
<span id="cb36-16"></span>
<span id="cb36-17"><span>def</span> multi_head_attention(x, WQs, WKs, WVs):</span>
<span id="cb36-18">    attentions <span>=</span> np.concatenate(</span>
<span id="cb36-19">        [attention(x, WQ, WK, WV) <span>for</span> WQ, WK, WV <span>in</span> <span>zip</span>(WQs, WKs, WVs)], axis<span>=</span><span>1</span></span>
<span id="cb36-20">    )</span>
<span id="cb36-21">    W <span>=</span> np.random.randn(n_attention_heads <span>*</span> d_value, d_embedding)</span>
<span id="cb36-22">    <span>return</span> attentions <span>@</span> W</span>
<span id="cb36-23"></span>
<span id="cb36-24"><span>def</span> feed_forward(Z, W1, b1, W2, b2):</span>
<span id="cb36-25">    <span>return</span> relu(Z.dot(W1) <span>+</span> b1).dot(W2) <span>+</span> b2</span>
<span id="cb36-26"></span>
<span id="cb36-27"><span>def</span> encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2):</span>
<span id="cb36-28">    Z <span>=</span> multi_head_attention(x, WQs, WKs, WVs)</span>
<span id="cb36-29">    Z <span>=</span> feed_forward(Z, W1, b1, W2, b2)</span>
<span id="cb36-30">    <span>return</span> Z</span>
<span id="cb36-31"></span>
<span id="cb36-32"><span>def</span> random_encoder_block(x):</span>
<span id="cb36-33">    WQs <span>=</span> [</span>
<span id="cb36-34">        np.random.randn(d_embedding, d_query) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb36-35">    ]</span>
<span id="cb36-36">    WKs <span>=</span> [</span>
<span id="cb36-37">        np.random.randn(d_embedding, d_key) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb36-38">    ]</span>
<span id="cb36-39">    WVs <span>=</span> [</span>
<span id="cb36-40">        np.random.randn(d_embedding, d_value) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb36-41">    ]</span>
<span id="cb36-42">    W1 <span>=</span> np.random.randn(d_embedding, d_feed_forward)</span>
<span id="cb36-43">    b1 <span>=</span> np.random.randn(d_feed_forward)</span>
<span id="cb36-44">    W2 <span>=</span> np.random.randn(d_feed_forward, d_embedding)</span>
<span id="cb36-45">    b2 <span>=</span> np.random.randn(d_embedding)</span>
<span id="cb36-46">    <span>return</span> encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2)</span></code></pre></div>
<p>Recall that our input is the matrix E which has the positional encoding and the embedding.</p>
<div>
<pre><code>array([[1.  , 3.  , 3.  , 5.  ],
       [2.84, 3.99, 4.  , 6.  ]])</code></pre>
</div>
<p>Let’s now pass this to our <code>random_encoder_block</code> function</p>
<div>
<div id="cb39"><pre><code><span id="cb39-1">random_encoder_block(embedding)</span></code></pre></div>
<div>
<pre><code>array([[ -71.76537515, -131.43316885,   13.2938131 ,   -4.26831998],
       [ -72.04253781, -131.84091347,   13.3385937 ,   -4.32872015]])</code></pre>
</div>
</div>
<p>Nice! This was just one encoder block. The original paper uses 6 encoders. The output of one encoder goes to the next, and so on:</p>
<div>
<div id="cb41"><pre><code><span id="cb41-1"><span>def</span> encoder(x, n<span>=</span><span>6</span>):</span>
<span id="cb41-2">    <span>for</span> _ <span>in</span> <span>range</span>(n):</span>
<span id="cb41-3">        x <span>=</span> random_encoder_block(x)</span>
<span id="cb41-4">    <span>return</span> x</span>
<span id="cb41-5"></span>
<span id="cb41-6"></span>
<span id="cb41-7">encoder(embedding)</span></code></pre></div>
<div>
<pre><code>/tmp/ipykernel_11906/1045810361.py:2: RuntimeWarning: overflow encountered in exp
  return np.exp(x)/np.sum(np.exp(x),axis=1, keepdims=True)
/tmp/ipykernel_11906/1045810361.py:2: RuntimeWarning: invalid value encountered in divide
  return np.exp(x)/np.sum(np.exp(x),axis=1, keepdims=True)</code></pre>
</div>
<div>
<pre><code>array([[nan, nan, nan, nan],
       [nan, nan, nan, nan]])</code></pre>
</div>
</div>
</section>
<section id="residual-and-layer-normalization">
<h4 data-anchor-id="residual-and-layer-normalization">5.3 Residual and Layer Normalization</h4>
<p>Uh oh! We’re getting NaNs! It seems our values are too high, and when being passed to the next encoder, they end up being too high and exploding! This is called <strong>gradient explosion</strong>. Without any kind of normalization, small changes in the input of early layers end up being amplified in later layers. This is a common problem in deep neural networks. There are two common techniques to mitigate this problem: residual connections and layer normalization (section 3.1 of the paper, barely mentioned).</p>
<ul>
<li><strong>Residual connections:</strong> Residual connections are simply adding the input of the layer to it output. For example, we add the initial embedding to the output of the attention. Residual connections mitigate the vanishing gradient problem. The intuition is that if the gradient is too small, we can just add the input to the output and the gradient will be larger. The math is very simple:</li>
</ul>
<p><span>\[
\text{Residual}(x) = x + \text{Layer}(x)
\]</span></p>
<p>That’s it! We’ll do this to the output of the attention and the output of the feed-forward layer.</p>
<ul>
<li><strong>Layer normalization</strong> Layer normalization is a technique to normalize the inputs of a layer. It normalizes across the embedding dimension. The intuition is that we want to normalize the inputs of a layer so that they have a mean of 0 and a standard deviation of 1. This helps with the gradient flow. The math does not look so simple at a first glance.</li>
</ul>
<p><span>\[
\text{LayerNorm}(x) = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \times \gamma + \beta
\]</span></p>
<p>Let’s explain each parameter:</p>
<ul>
<li><span>\(\mu\)</span> is the mean of the embedding</li>
<li><span>\(\sigma\)</span> is the standard deviation of the embedding</li>
<li><span>\(\epsilon\)</span> is a small number to avoid division by zero. In case the standard deviation is 0, this small epsilon saves the day!</li>
<li><span>\(\gamma\)</span> and <span>\(\beta\)</span> are learned parameters that control scaling and shifting steps.</li>
</ul>
<p>Unlike batch normalization (no worries if you don’t know what it is), layer normalization normalizes across the embedding dimension - that means that each embedding will not be affected by other samples in the batch. The intuition is that we want to normalize the inputs of a layer so that they have a mean of 0 and a standard deviation of 1.</p>
<p>Why do we add the learnable parameters <span>\(\gamma\)</span> and <span>\(\beta\)</span>? The reason is that we don’t want to lose the representational power of the layer. If we just normalize the inputs, we might lose some information. By adding the learnable parameters, we can learn to scale and shift the normalized values.</p>
<p>Combining the equations, the equation for the whole encoder could look like this</p>
<p><span>\[
\text{Z}(x) = \text{LayerNorm}(x + \text{Attention}(x))
\]</span></p>
<p><span>\[
\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2
\]</span></p>
<p><span>\[
\text{Encoder}(x) = \text{LayerNorm}(Z(x) + \text{FFN}(Z(x) + x))
\]</span></p>
<p>Let’s try with our example! Let’s go with E and Z values from before</p>
<p><span>\[
\begin{align*}
\text{E} + \text{Attention(E)} &amp;= \begin{bmatrix}
1.0 &amp; 3.0 &amp; 3.0 &amp; 5.0 \\
2.84 &amp; 3.99 &amp; 4.0 &amp; 6.0
\end{bmatrix} + \begin{bmatrix}
11.46394281 &amp; -13.18016469 &amp; -11.59340253 &amp; -17.04387833 \\
11.62608569 &amp; -13.47454934 &amp; -11.87126395 &amp; -17.49263674
\end{bmatrix} \\
&amp;= \begin{bmatrix}
12.46394281 &amp; -10.18016469 &amp; -8.59340253 &amp; -12.04387833 \\
14.46608569 &amp; -9.48454934 &amp; -7.87126395 &amp; -11.49263674
\end{bmatrix}
\end{align*}
\]</span></p>
<p>Let’s now calculate the layer normalization, we can divide it into three steps:</p>
<ol type="1">
<li>Compute mean and variance for each embedding.</li>
<li>Normalize by substracting the mean of its row and dividing by the square root of its row variance (plus a small number to avoid division by zero).</li>
<li>Scale and shift by multiplying by gamma and adding beta.</li>
</ol>
<section id="mean-and-variance">
<h5 data-anchor-id="mean-and-variance">5.3.1 Mean and variance</h5>
<p>For the first embedding</p>
<p><span>\[
\begin{align*}
\mu_1 &amp;= \frac{12.46394281-10.18016469-8.59340253-12.04387833}{4} = -4.58837568 \\
\sigma^2 &amp;= \frac{\sum (x_i - \mu)^2}{N} \\
&amp;= \frac{(12.46394281 - (-4.588375685))^2 + \ldots + (-12.04387833 - (-4.588375685))^2}{4} \\
&amp;= \frac{393.67443005013}{4} \\
&amp;= 98.418607512533 \\
\sigma &amp;= \sqrt{98.418607512533} \\
&amp;= 9.9206152789297
\end{align*}
\]</span></p>
<p>We can do the same for the second embedding. We’ll skip the calculations but you get the hang of it.</p>
<p><span>\[
\begin{align*}
\mu_2 &amp;= -3.59559109 \\
\sigma_2 &amp;= 10.50653018
\end{align*}
\]</span></p>
<p>Let’s confirm with Python</p>
<div>
<div id="cb44"><pre><code><span id="cb44-1">(embedding <span>+</span> Z).mean(axis<span>=-</span><span>1</span>, keepdims<span>=</span><span>True</span>)</span></code></pre></div>
<div>
<pre><code>array([[-4.58837567],
       [-3.59559107]])</code></pre>
</div>
</div>
<div>
<div id="cb46"><pre><code><span id="cb46-1">(embedding <span>+</span> Z).std(axis<span>=-</span><span>1</span>, keepdims<span>=</span><span>True</span>)</span></code></pre></div>
<div>
<pre><code>array([[ 9.92061529],
       [10.50653019]])</code></pre>
</div>
</div>
<p>Amazing! Let’s now normalize</p>
</section>
<section id="normalize">
<h5 data-anchor-id="normalize">5.3.2 Normalize</h5>
<p>For normalization, for each value in the embedding, we subsctract the mean and divide by the standard deviation. Epsilon is a very small value, such as 0.00001. We’ll assume <span>\(\gamma=1\)</span> and <span>\(\beta=0\)</span>, it simplifies things.</p>
<p><span>\[\begin{align*}
\text{normalized}_1 &amp;= \frac{12.46394281 - (-4.58837568)}{\sqrt{98.418607512533 + \epsilon}} \\
&amp;= \frac{17.05231849}{9.9206152789297} \\
&amp;= 1.718 \\
\text{normalized}_2 &amp;= \frac{-10.18016469 - (-4.58837568)}{\sqrt{98.418607512533 + \epsilon}} \\
&amp;= \frac{-5.59178901}{9.9206152789297} \\
&amp;= -0.564 \\
\text{normalized}_3 &amp;= \frac{-8.59340253 - (-4.58837568)}{\sqrt{98.418607512533 + \epsilon}} \\
&amp;= \frac{-4.00502685}{9.9206152789297} \\
&amp;= -0.404 \\
\text{normalized}_4 &amp;= \frac{-12.04387833 - (-4.58837568)}{\sqrt{98.418607512533 + \epsilon}} \\
&amp;= \frac{-7.45550265}{9.9206152789297} \\
&amp;= -0.752
\end{align*}\]</span></p>
<p>We’ll skip the calculations by hand for the second embedding. Let’s confirm with code! Let’s re-define our <code>encoder_block</code> function with this change</p>
<div id="cb48"><pre><code><span id="cb48-1"><span>def</span> layer_norm(x, epsilon<span>=</span><span>1e-6</span>):</span>
<span id="cb48-2">    mean <span>=</span> x.mean(axis<span>=-</span><span>1</span>, keepdims<span>=</span><span>True</span>)</span>
<span id="cb48-3">    std <span>=</span> x.std(axis<span>=-</span><span>1</span>, keepdims<span>=</span><span>True</span>)</span>
<span id="cb48-4">    <span>return</span> (x <span>-</span> mean) <span>/</span> (std <span>+</span> epsilon)</span>
<span id="cb48-5"></span>
<span id="cb48-6"><span>def</span> encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2):</span>
<span id="cb48-7">    Z <span>=</span> multi_head_attention(x, WQs, WKs, WVs)</span>
<span id="cb48-8">    Z <span>=</span> layer_norm(Z <span>+</span> x)</span>
<span id="cb48-9"></span>
<span id="cb48-10">    output <span>=</span> feed_forward(Z, W1, b1, W2, b2)</span>
<span id="cb48-11">    <span>return</span> layer_norm(output <span>+</span> Z)</span></code></pre></div>
<div>
<div id="cb49"><pre><code><span id="cb49-1">layer_norm(Z <span>+</span> embedding)</span></code></pre></div>
<div>
<pre><code>array([[ 1.71887693, -0.56365339, -0.40370747, -0.75151608],
       [ 1.71909039, -0.56050453, -0.40695381, -0.75163205]])</code></pre>
</div>
</div>
<p>It works! Let’s retry to pass the embedding through the six encoders.</p>
<div>
<div id="cb51"><pre><code><span id="cb51-1"><span>def</span> encoder(x, n<span>=</span><span>6</span>):</span>
<span id="cb51-2">    <span>for</span> _ <span>in</span> <span>range</span>(n):</span>
<span id="cb51-3">        x <span>=</span> random_encoder_block(x)</span>
<span id="cb51-4">    <span>return</span> x</span>
<span id="cb51-5"></span>
<span id="cb51-6"></span>
<span id="cb51-7">encoder(embedding)</span></code></pre></div>
<div>
<pre><code>array([[-0.335849  , -1.44504571,  1.21698183,  0.56391289],
       [-0.33583947, -1.44504861,  1.21698606,  0.56390202]])</code></pre>
</div>
</div>
<p>Amazing! These values make sense and we don’t get NaNs! The idea of the stack of encoders is that they output a continuous representation, z, that captures the meaning of the input sequence. This representation is then passed to the decoder, which will genrate an output sequence of symbols, one element at a time.</p>
<p>Before diving into the decoder, here’s an image from Jay’s amazing blog post:</p>
<div>
<figure>
<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png"></p>
<figcaption>Encoder and decoder</figcaption>
</figure>
</div>
<p>You should be able to explain each component at the left side! Quite impressive, right? Let’s now move to the decoder.</p>
</section>
</section>
</section>
</section>
<section id="decoder">
<h2 data-anchor-id="decoder">Decoder</h2>
<p>Most of the thing we learned for encoders will be used in the decoder as well! The decoder has two self-attention layers, one for the encoder and one for the decoder. The decoder also has a feed-forward layer. Let’s go through each of these.</p>
<p>The decoder block receives two inputs: the output of the encoder and the generated output sequence. The output of the encoder is the representation of the input sequence. During inference, the generated output sequence starts with a special start-of-sequence token (SOS). During training, the target output sequence is the actual output sequence, shifted by one position. This will be clearer soon!</p>
<p>Given the embedding generated by the encoder and the SOS token, the decoder will then generate the next token of the sequence, e.g.&nbsp;“hola”. The decoder is autoregressive, that means that the decoder will take the previously generated tokens and again generate the second token.</p>
<ul>
<li>Iteration 1: Input is SOS, output is “hola”</li>
<li>Iteration 2: Input is SOS + “hola”, output is “mundo”</li>
<li>Iteration 3: Input is SOS + “hola” + “mundo”, output is EOS</li>
</ul>
<p>Here, SOS is the start-of-sequence token and EOS is the end-of-sequence token. The decoder will stop when it generates the EOS token. It generates one token at a time. Note that all iterations use the embedding generated by the encoder.</p>
<div>

<p><strong>This autoregressive design makes decoder slow.</strong> The encoder is able to generate its embedding in a single forward pass while the decoder needs to do many forward passes. This is one of the reasons why architectures that only use the encoder (such as BERT or sentence similarity models) are much faster than decoder-only architectures (such as GPT-2 or BART).</p>
</div>
<p>Let’s dive into each step! Just as the encoder, the decoder is composed of a stack of decoder blocks. The decoder block is a bit more complex than the encoder block. The general structure is:</p>
<ol type="1">
<li>(Masked) Self-attention layer</li>
<li>Residual connection and layer normalization</li>
<li>Encoder-decoder attention layer</li>
<li>Residual connection and layer normalization</li>
<li>Feed-forward layer</li>
<li>Residual connection and layer normalization</li>
</ol>
<p>We’re already familiar with all the math from 1, 2, 3, 5 and 6. See the right side of the image below, you’ll see that all these blocks you already know (the right part):</p>
<div>
<figure>
<p><a href="https://arxiv.org/abs/1706.03762"><img src="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/transformer.png"></a></p>
<figcaption>Transformer model from the original “attention is all you need” paper</figcaption>
</figure>
</div>
<section id="embedding-the-text-1">
<h3 data-anchor-id="embedding-the-text-1">1. Embedding the text</h3>
<p>The first text of the decoder is to embed the input tokens. The input token is <code>SOS</code>, so we’ll embed it. We’ll use the same embedding dimension as the encoder. Let’s assume the embedding vector is the following:</p>
<p><span>\[
E = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0
\end{bmatrix}
\]</span></p>
</section>
<section id="positional-encoding-1">
<h3 data-anchor-id="positional-encoding-1">2. Positional encoding</h3>
<p>We’ll now add the positional encoding to the embedding, just as we did for the encoder. Given it’s the same position as “Hello”, we’ll have same positional encoding as we did before:</p>
<ul>
<li>i = 0 (even): PE(0,0) = sin(0 / 10000^(0 / 4)) = sin(0) = 0</li>
<li>i = 1 (odd): PE(0,1) = cos(0 / 10000^(2*1 / 4)) = cos(0) = 1</li>
<li>i = 2 (even): PE(0,2) = sin(0 / 10000^(2*2 / 4)) = sin(0) = 0</li>
<li>i = 3 (odd): PE(0,3) = cos(0 / 10000^(2*3 / 4)) = cos(0) = 1</li>
</ul>
</section>
<section id="add-positional-encoding-and-embedding-1">
<h3 data-anchor-id="add-positional-encoding-and-embedding-1">3. Add positional encoding and embedding</h3>
<p>Adding the positional encoding to the embedding is done by adding the two vectors together:</p>
<p><span>\[
E = \begin{bmatrix}
1 &amp; 1 &amp; 0 &amp; 1
\end{bmatrix}
\]</span></p>
</section>
<section id="self-attention-1">
<h3 data-anchor-id="self-attention-1">4. Self-attention</h3>
<p>The first step within the decoder block is the self-attention mechanism. Luckily, we have some code for this and can just use it!</p>
<div>
<div id="cb53"><pre><code><span id="cb53-1">d_embedding <span>=</span> <span>4</span></span>
<span id="cb53-2">n_attention_heads <span>=</span> <span>2</span></span>
<span id="cb53-3"></span>
<span id="cb53-4">E <span>=</span> np.array([[<span>1</span>, <span>1</span>, <span>0</span>, <span>1</span>]])</span>
<span id="cb53-5">WQs <span>=</span> [np.random.randn(d_embedding, d_query) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb53-6">WKs <span>=</span> [np.random.randn(d_embedding, d_key) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb53-7">WVs <span>=</span> [np.random.randn(d_embedding, d_value) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb53-8"></span>
<span id="cb53-9">Z_self_attention <span>=</span> multi_head_attention(E, WQs, WKs, WVs)</span>
<span id="cb53-10">Z_self_attention</span></code></pre></div>
<div>
<pre><code>array([[ 2.19334924, 10.61851198, -4.50089666, -2.76366551]])</code></pre>
</div>
</div>
<div>
<p>Things are quite simple for inference. For training, things are a bit tricky. During training, we use unlabeled data: just a bunch of text data, frequentyl scraped from the web. While the encoder’s goal is to capture all information of the input, the decoder’s goal is to predict the most likely next token. This means that the decoder can only use the tokens that have been generated so far (it cannot cheat and see the next tokens).</p>
<p>Because of this, we use masked self-attention: we mask the tokens that have not been generated yet. This is done by setting the attention scores to -inf. This is done in the original paper (section 3.2.3.1). We’ll skip this for now, but it’s important to keep in mind that the decoder is a bit more complex during training.</p>
</div>
</section>
<section id="residual-connection-and-layer-normalization">
<h3 data-anchor-id="residual-connection-and-layer-normalization">5. Residual connection and layer normalization</h3>
<p>Nothing magical here, we just add the input to the output of the self-attention and apply layer normalization. We’ll use the same code as before.</p>
<div>
<div id="cb55"><pre><code><span id="cb55-1">Z_self_attention <span>=</span> layer_norm(Z_self_attention <span>+</span> E)</span>
<span id="cb55-2">Z_self_attention</span></code></pre></div>
<div>
<pre><code>array([[ 0.17236212,  1.54684892, -1.0828824 , -0.63632864]])</code></pre>
</div>
</div>
</section>
<section id="encoder-decoder-attention">
<h3 data-anchor-id="encoder-decoder-attention">6. Encoder-decoder attention</h3>
<p><strong>This part is the new one!</strong> If you were wondering where do the encoder-generated embeddings come in, this is their moment to shine!</p>
<p>Let’s assume the output of the encoder is the following matrix</p>
<p><span>\[
\begin{bmatrix}
-1.5 &amp; 1.0 &amp; -0.8 &amp; 1.5 \\
1.0 &amp; -1.0 &amp; -0.5 &amp; 1.0
\end{bmatrix}
\]</span></p>
<p>In the self-attention mechanism, we calculate the queries, keys, and values from the input embedding.</p>
<p>In the encoder-decoder attention, we calculate the queries from the previous decoder layer and the keys and values from the encoder output! All the math is the same as before; the only difference is what embedding to use for the queries. Let’s look at some code</p>
<div id="cb57"><pre><code><span id="cb57-1"><span>def</span> encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV):</span>
<span id="cb57-2">    <span># The next three lines are the key difference!</span></span>
<span id="cb57-3">    K <span>=</span> encoder_output <span>@</span> WK    <span># Note that now we pass the previous encoder output!</span></span>
<span id="cb57-4">    V <span>=</span> encoder_output <span>@</span> WV    <span># Note that now we pass the previous encoder output!</span></span>
<span id="cb57-5">    Q <span>=</span> attention_input <span>@</span> WQ   <span># Same as self-attention</span></span>
<span id="cb57-6"></span>
<span id="cb57-7">    <span># This stays the same</span></span>
<span id="cb57-8">    scores <span>=</span> Q <span>@</span> K.T</span>
<span id="cb57-9">    scores <span>=</span> scores <span>/</span> np.sqrt(d_key)</span>
<span id="cb57-10">    scores <span>=</span> softmax(scores)</span>
<span id="cb57-11">    scores <span>=</span> scores <span>@</span> V</span>
<span id="cb57-12">    <span>return</span> scores</span>
<span id="cb57-13"></span>
<span id="cb57-14"></span>
<span id="cb57-15"><span>def</span> multi_head_encoder_decoder_attention(</span>
<span id="cb57-16">    encoder_output, attention_input, WQs, WKs, WVs</span>
<span id="cb57-17">):</span>
<span id="cb57-18">    <span># Note that now we pass the previous encoder output!</span></span>
<span id="cb57-19">    attentions <span>=</span> np.concatenate(</span>
<span id="cb57-20">        [</span>
<span id="cb57-21">            encoder_decoder_attention(</span>
<span id="cb57-22">                encoder_output, attention_input, WQ, WK, WV</span>
<span id="cb57-23">            )</span>
<span id="cb57-24">            <span>for</span> WQ, WK, WV <span>in</span> <span>zip</span>(WQs, WKs, WVs)</span>
<span id="cb57-25">        ],</span>
<span id="cb57-26">        axis<span>=</span><span>1</span>,</span>
<span id="cb57-27">    )</span>
<span id="cb57-28">    W <span>=</span> np.random.randn(n_attention_heads <span>*</span> d_value, d_embedding)</span>
<span id="cb57-29">    <span>return</span> attentions <span>@</span> W</span></code></pre></div>
<div>
<div id="cb58"><pre><code><span id="cb58-1">WQs <span>=</span> [np.random.randn(d_embedding, d_query) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb58-2">WKs <span>=</span> [np.random.randn(d_embedding, d_key) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb58-3">WVs <span>=</span> [np.random.randn(d_embedding, d_value) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)]</span>
<span id="cb58-4"></span>
<span id="cb58-5">encoder_output <span>=</span> np.array([[<span>-</span><span>1.5</span>, <span>1.0</span>, <span>-</span><span>0.8</span>, <span>1.5</span>], [<span>1.0</span>, <span>-</span><span>1.0</span>, <span>-</span><span>0.5</span>, <span>1.0</span>]])</span>
<span id="cb58-6"></span>
<span id="cb58-7">Z_encoder_decoder <span>=</span> multi_head_encoder_decoder_attention(</span>
<span id="cb58-8">    encoder_output, Z_self_attention, WQs, WKs, WVs</span>
<span id="cb58-9">)</span>
<span id="cb58-10">Z_encoder_decoder</span></code></pre></div>
<div>
<pre><code>array([[ 1.57651431,  4.92489307, -0.08644448, -0.46776051]])</code></pre>
</div>
</div>
<p>This worked! You might be asking “why do we do this?”. The reason is that we want the decoder to focus on the relevant parts of the input text (e.g., “hello world”). The encoder-decoder attention allows each position in the decoder to attend over all positions in the input sequence. This is very helpful for tasks such as translation, where the decoder needs to focus on the relevant parts of the input sequence. The decoder will learn to focus on the relevant parts of the input sequence by learning to generate the correct output tokens. This is a very powerful mechanism!</p>
</section>
<section id="residual-connection-and-layer-normalization-1">
<h3 data-anchor-id="residual-connection-and-layer-normalization-1">7. Residual connection and layer normalization</h3>
<p>Same as before!</p>
<div>
<div id="cb60"><pre><code><span id="cb60-1">Z_encoder_decoder <span>=</span> layer_norm(Z_encoder_decoder <span>+</span> Z)</span>
<span id="cb60-2">Z_encoder_decoder</span></code></pre></div>
<div>
<pre><code>array([[-0.44406723,  1.6552893 , -0.19984632, -1.01137575]])</code></pre>
</div>
</div>
</section>
<section id="feed-forward-layer-1">
<h3 data-anchor-id="feed-forward-layer-1">8. Feed-forward layer</h3>
<p>Once again, same as before! I’ll also do the residual connection and layer normalization after it.</p>
<div>
<div id="cb62"><pre><code><span id="cb62-1">W1 <span>=</span> np.random.randn(<span>4</span>, <span>8</span>)</span>
<span id="cb62-2">W2 <span>=</span> np.random.randn(<span>8</span>, <span>4</span>)</span>
<span id="cb62-3">b1 <span>=</span> np.random.randn(<span>8</span>)</span>
<span id="cb62-4">b2 <span>=</span> np.random.randn(<span>4</span>)</span>
<span id="cb62-5"></span>
<span id="cb62-6">output <span>=</span> feed_forward(Z_encoder_decoder, W1, b1, W2, b2) <span>+</span> Z_encoder_decoder</span>
<span id="cb62-7">output</span></code></pre></div>
<div>
<pre><code>array([[-0.97650182,  0.81470137, -2.79122044, -3.39192873]])</code></pre>
</div>
</div>
</section>
<section id="encapsulating-everything-the-random-decoder">
<h3 data-anchor-id="encapsulating-everything-the-random-decoder">9. Encapsulating everything: The Random Decoder</h3>
<p>Let’s write the code for a single decoder block. The main change is that we now have an additional attention mechanism.</p>
<div id="cb64"><pre><code><span id="cb64-1">d_embedding <span>=</span> <span>4</span></span>
<span id="cb64-2">d_key <span>=</span> d_value <span>=</span> d_query <span>=</span> <span>3</span></span>
<span id="cb64-3">d_feed_forward <span>=</span> <span>8</span></span>
<span id="cb64-4">n_attention_heads <span>=</span> <span>2</span></span>
<span id="cb64-5">encoder_output <span>=</span> np.array([[<span>-</span><span>1.5</span>, <span>1.0</span>, <span>-</span><span>0.8</span>, <span>1.5</span>], [<span>1.0</span>, <span>-</span><span>1.0</span>, <span>-</span><span>0.5</span>, <span>1.0</span>]])</span>
<span id="cb64-6"></span>
<span id="cb64-7"><span>def</span> decoder_block(</span>
<span id="cb64-8">    x,</span>
<span id="cb64-9">    encoder_output,</span>
<span id="cb64-10">    WQs_self_attention, WKs_self_attention, WVs_self_attention,</span>
<span id="cb64-11">    WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,</span>
<span id="cb64-12">    W1, b1, W2, b2,</span>
<span id="cb64-13">):</span>
<span id="cb64-14">    <span># Same as before</span></span>
<span id="cb64-15">    Z <span>=</span> multi_head_attention(</span>
<span id="cb64-16">        x, WQs_self_attention, WKs_self_attention, WVs_self_attention</span>
<span id="cb64-17">    )</span>
<span id="cb64-18">    Z <span>=</span> layer_norm(Z <span>+</span> x)</span>
<span id="cb64-19"></span>
<span id="cb64-20">    <span># The next three lines are the key difference!</span></span>
<span id="cb64-21">    Z_encoder_decoder <span>=</span> multi_head_encoder_decoder_attention(</span>
<span id="cb64-22">        encoder_output, Z, WQs_ed_attention, WKs_ed_attention, WVs_ed_attention</span>
<span id="cb64-23">    )</span>
<span id="cb64-24">    Z_encoder_decoder <span>=</span> layer_norm(Z_encoder_decoder <span>+</span> Z)</span>
<span id="cb64-25"></span>
<span id="cb64-26">    <span># Same as before</span></span>
<span id="cb64-27">    output <span>=</span> feed_forward(Z_encoder_decoder, W1, b1, W2, b2)</span>
<span id="cb64-28">    <span>return</span> layer_norm(output <span>+</span> Z_encoder_decoder)</span>
<span id="cb64-29"></span>
<span id="cb64-30"><span>def</span> random_decoder_block(x, encoder_output):</span>
<span id="cb64-31">    <span># Just a bunch of random initializations</span></span>
<span id="cb64-32">    WQs_self_attention <span>=</span> [</span>
<span id="cb64-33">        np.random.randn(d_embedding, d_query) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-34">    ]</span>
<span id="cb64-35">    WKs_self_attention <span>=</span> [</span>
<span id="cb64-36">        np.random.randn(d_embedding, d_key) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-37">    ]</span>
<span id="cb64-38">    WVs_self_attention <span>=</span> [</span>
<span id="cb64-39">        np.random.randn(d_embedding, d_value) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-40">    ]</span>
<span id="cb64-41"></span>
<span id="cb64-42">    WQs_ed_attention <span>=</span> [</span>
<span id="cb64-43">        np.random.randn(d_embedding, d_query) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-44">    ]</span>
<span id="cb64-45">    WKs_ed_attention <span>=</span> [</span>
<span id="cb64-46">        np.random.randn(d_embedding, d_key) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-47">    ]</span>
<span id="cb64-48">    WVs_ed_attention <span>=</span> [</span>
<span id="cb64-49">        np.random.randn(d_embedding, d_value) <span>for</span> _ <span>in</span> <span>range</span>(n_attention_heads)</span>
<span id="cb64-50">    ]</span>
<span id="cb64-51"></span>
<span id="cb64-52">    W1 <span>=</span> np.random.randn(d_embedding, d_feed_forward)</span>
<span id="cb64-53">    b1 <span>=</span> np.random.randn(d_feed_forward)</span>
<span id="cb64-54">    W2 <span>=</span> np.random.randn(d_feed_forward, d_embedding)</span>
<span id="cb64-55">    b2 <span>=</span> np.random.randn(d_embedding)</span>
<span id="cb64-56"></span>
<span id="cb64-57"></span>
<span id="cb64-58">    <span>return</span> decoder_block(</span>
<span id="cb64-59">        x, encoder_output,</span>
<span id="cb64-60">        WQs_self_attention, WKs_self_attention, WVs_self_attention,</span>
<span id="cb64-61">        WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,</span>
<span id="cb64-62">        W1, b1, W2, b2,</span>
<span id="cb64-63">    )</span></code></pre></div>
<div>
<div id="cb65"><pre><code><span id="cb65-1"><span>def</span> decoder(x, decoder_embedding, n<span>=</span><span>6</span>):</span>
<span id="cb65-2">    <span>for</span> _ <span>in</span> <span>range</span>(n):</span>
<span id="cb65-3">        x <span>=</span> random_decoder_block(x, decoder_embedding)</span>
<span id="cb65-4">    <span>return</span> x</span>
<span id="cb65-5"></span>
<span id="cb65-6">decoder(E, encoder_output)</span></code></pre></div>
<div>
<pre><code>array([[ 0.71866458, -1.72279956,  0.57735876,  0.42677623]])</code></pre>
</div>
</div>
</section>
</section>
<section id="generating-the-output-sequence">
<h2 data-anchor-id="generating-the-output-sequence">Generating the output sequence</h2>
<p>We have all the building blocks! Let’s now generate the output sequence.</p>
<ul>
<li>We have the <strong>encoder</strong>, which takes the input sequence and generates its rich representation. It’s composed of a stack of encoder blocks.</li>
<li>We have the <strong>decoder</strong>, which takes the encoder output and generated tokens, and generates the output sequence. It’s composed of a stack of decoder blocks.</li>
</ul>
<p>How do we go from the decoder’s output to a word? We need to add a final linear layer and a softmax layer on top of the decoder. The whole algorithm looks like this:</p>
<ol type="1">
<li>The encoder receives the input sequence and generates a representation of it.</li>
<li>The decoder begins with the SOS token and the encoder output. It generates the next token of the output sequence.</li>
<li>We then apply a linear layer to generate the logits.</li>
<li>We then apply a softmax layer to generate the probabilities.</li>
<li>The decoder uses the encoder output and the previously generated token to generate the next token of the output sequence.</li>
<li>We repeat steps 2-5 until we generate the EOS token.</li>
</ol>
<p>This is mentioned in the section 3.4 of the paper.</p>
<section id="linear-layer">
<h3 data-anchor-id="linear-layer">1. Linear layer</h3>
<p>The linear layer is a simple linear transformation. It takes the decoder’s output and transforms it into a vector of size <code>vocab_size</code>. This is the size of the vocabulary. For example, if we have a vocabulary of 10000 words, the linear layer will transform the decoder’s output into a vector of size 10000. This vector will contain the probability of each word being the next word in the sequence. For simplicity, let’s go with a vocabulary of 10 words and assume the first decoder output is a very simple vector: [1, 0, 1, 0]. We’ll use random weights and biases matrices of the size <code>vocab_size</code> x <code>decoder_output_size</code>.</p>
<div>
<div id="cb67"><pre><code><span id="cb67-1"><span>def</span> linear(x, W, b):</span>
<span id="cb67-2">    <span>return</span> np.dot(x, W) <span>+</span> b</span>
<span id="cb67-3"></span>
<span id="cb67-4">x <span>=</span> linear([[<span>1</span>, <span>0</span>, <span>1</span>, <span>0</span>]], np.random.randn(<span>4</span>, <span>10</span>), np.random.randn(<span>10</span>))</span>
<span id="cb67-5">x</span></code></pre></div>
<div>
<pre><code>array([[-0.39929948,  0.96345013,  2.77090264,  0.25651866, -0.84738762,
        -1.67834992, -0.29583529, -3.55515281,  2.97453801, -1.10682376]])</code></pre>
</div>
</div>
</section>
<section id="softmax">
<h3 data-anchor-id="softmax">2. Softmax</h3>
<p>These are called logits but they are not easily interpretable. We need to apply a softmax function to obtain the probabilities.</p>
<div>
<pre><code>array([[0.01602618, 0.06261303, 0.38162024, 0.03087794, 0.0102383 ,
        0.00446011, 0.01777314, 0.00068275, 0.46780959, 0.00789871]])</code></pre>
</div>
<p>This is giving us probabilities! Let’a assume the vocabulary is the following:</p>
<p><span>\[
\text{vocab} = \begin{bmatrix}
\text{hello} &amp; \text{mundo} &amp; \text{world} &amp; \text{how} &amp; \text{?} &amp; \text{EOS} &amp; \text{SOS} &amp; \text{a} &amp; \text{hola} &amp; \text{c}
\end{bmatrix}
\]</span></p>
<p>The above tells us that the probabilities are</p>
<ul>
<li>hello: 0.01602618</li>
<li>mundo: 0.06261303</li>
<li>world: 0.38162024</li>
<li>how: 0.03087794</li>
<li>?: 0.0102383</li>
<li>EOS: 0.00446011</li>
<li>SOS: 0.01777314</li>
<li>a: 0.00068275</li>
<li>hola: 0.46780959</li>
<li>c: 0.00789871</li>
</ul>
<p>From these, the most likely next token is “hola”. Picking always the most likely token is called greedy decoding. This is not always the best approach, as it might lead to suboptimal results, but we won’t dive into generation techniques at the moment. If you want to learn more about it, check out this amazing <a href="https://huggingface.co/blog/how-to-generate">blog post</a>.</p>
</section>
<section id="the-random-encoder-decoder-transformer">
<h3 data-anchor-id="the-random-encoder-decoder-transformer">3. The Random Encoder-Decoder Transformer</h3>
<p>Let’s write the whole code for this! Let’s define a dictionary that maps the words to their initial embeddings. Note that this is also learned during training, but we’ll use random values for now.</p>
<div>
<div id="cb71"><pre><code><span id="cb71-1">vocabulary <span>=</span> [</span>
<span id="cb71-2">    <span>"hello"</span>,</span>
<span id="cb71-3">    <span>"mundo"</span>,</span>
<span id="cb71-4">    <span>"world"</span>,</span>
<span id="cb71-5">    <span>"how"</span>,</span>
<span id="cb71-6">    <span>"?"</span>,</span>
<span id="cb71-7">    <span>"EOS"</span>,</span>
<span id="cb71-8">    <span>"SOS"</span>,</span>
<span id="cb71-9">    <span>"a"</span>,</span>
<span id="cb71-10">    <span>"hola"</span>,</span>
<span id="cb71-11">    <span>"c"</span>,</span>
<span id="cb71-12">]</span>
<span id="cb71-13">embedding_reps <span>=</span> np.random.randn(<span>10</span>, <span>1</span>, <span>4</span>)</span>
<span id="cb71-14">vocabulary_embeddings <span>=</span> {</span>
<span id="cb71-15">    word: embedding_reps[i] <span>for</span> i, word <span>in</span> <span>enumerate</span>(vocabulary)</span>
<span id="cb71-16">}</span>
<span id="cb71-17">vocabulary_embeddings</span></code></pre></div>
<div>
<pre><code>{'hello': array([[-1.19489531, -1.08007463,  1.41277762,  0.72054139]]),
 'mundo': array([[-0.70265064, -0.58361306, -1.7710761 ,  0.87478862]]),
 'world': array([[ 0.52480342,  2.03519246, -0.45100608, -1.92472193]]),
 'how': array([[-1.14693176, -1.55761929,  1.09607545, -0.21673596]]),
 '?': array([[-0.23689522, -1.12496841, -0.03733462, -0.23477603]]),
 'EOS': array([[ 0.5180958 , -0.39844119,  0.30004136,  0.03881324]]),
 'SOS': array([[ 2.00439161,  2.19477149, -0.84901634, -0.89269937]]),
 'a': array([[ 1.63558337, -1.2556952 ,  1.65365362,  0.87639945]]),
 'hola': array([[-0.5805717 , -0.93861149,  1.06847734, -0.34408367]]),
 'c': array([[-2.79741142,  0.70521986, -0.44929098, -1.66167776]])}</code></pre>
</div>
</div>
<p>And now let’s write our random <code>generate</code> method that generates tokens autorergressively.</p>
<div id="cb73"><pre><code><span id="cb73-1"><span>def</span> generate(input_sequence, max_iters<span>=</span><span>10</span>):</span>
<span id="cb73-2">    <span># We first encode the inputs into embeddings</span></span>
<span id="cb73-3">    <span># This skips the positional encoding step for simplicity</span></span>
<span id="cb73-4">    embedded_inputs <span>=</span> [</span>
<span id="cb73-5">        vocabulary_embeddings[token][<span>0</span>] <span>for</span> token <span>in</span> input_sequence</span>
<span id="cb73-6">    ]</span>
<span id="cb73-7">    <span>print</span>(<span>"Embedding representation (encoder input)"</span>, embedded_inputs)</span>
<span id="cb73-8"></span>
<span id="cb73-9">    <span># We then generate an embedding representation</span></span>
<span id="cb73-10">    encoder_output <span>=</span> encoder(embedded_inputs)</span>
<span id="cb73-11">    <span>print</span>(<span>"Embedding generated by encoder (encoder output)"</span>, encoder_output)</span>
<span id="cb73-12"></span>
<span id="cb73-13">    <span># We initialize the decoder output with the embedding of the start token</span></span>
<span id="cb73-14">    sequence <span>=</span> vocabulary_embeddings[<span>"SOS"</span>]</span>
<span id="cb73-15">    output <span>=</span> <span>"SOS"</span></span>
<span id="cb73-16"></span>
<span id="cb73-17">    <span># Random matrices for the linear layer</span></span>
<span id="cb73-18">    W_linear <span>=</span> np.random.randn(d_embedding, <span>len</span>(vocabulary))</span>
<span id="cb73-19">    b_linear <span>=</span> np.random.randn(<span>len</span>(vocabulary))</span>
<span id="cb73-20"></span>
<span id="cb73-21">    <span># We limit number of decoding steps to avoid too long sequences without EOS</span></span>
<span id="cb73-22">    <span>for</span> i <span>in</span> <span>range</span>(max_iters):</span>
<span id="cb73-23">        <span># Decoder step</span></span>
<span id="cb73-24">        decoder_output <span>=</span> decoder(sequence, encoder_output)</span>
<span id="cb73-25">        logits <span>=</span> linear(decoder_output, W_linear, b_linear)</span>
<span id="cb73-26">        probs <span>=</span> softmax(logits)</span>
<span id="cb73-27"></span>
<span id="cb73-28">        <span># We get the most likely next token</span></span>
<span id="cb73-29">        next_token <span>=</span> vocabulary[np.argmax(probs)]</span>
<span id="cb73-30"></span>
<span id="cb73-31">        sequence <span>=</span> vocabulary_embeddings[next_token]</span>
<span id="cb73-32">        output <span>+=</span> <span>" "</span> <span>+</span> next_token</span>
<span id="cb73-33"></span>
<span id="cb73-34">        <span>print</span>(</span>
<span id="cb73-35">            <span>"Iteration"</span>, i, </span>
<span id="cb73-36">            <span>"next token"</span>, next_token,</span>
<span id="cb73-37">            <span>"with probability of"</span>, np.<span>max</span>(probs),</span>
<span id="cb73-38">        )</span>
<span id="cb73-39"></span>
<span id="cb73-40">        <span># If the next token is the end token, we return the sequence</span></span>
<span id="cb73-41">        <span>if</span> next_token <span>==</span> <span>"EOS"</span>:</span>
<span id="cb73-42">            <span>return</span> output</span>
<span id="cb73-43"></span>
<span id="cb73-44">    <span>return</span> output</span></code></pre></div>
<p>Let’s run this now!</p>
<div>
<div id="cb74"><pre><code><span id="cb74-1">generate([<span>"hello"</span>, <span>"world"</span>])</span></code></pre></div>
<div>
<pre><code>Embedding representation (encoder input) [array([-1.19489531, -1.08007463,  1.41277762,  0.72054139]), array([ 0.52480342,  2.03519246, -0.45100608, -1.92472193])]
Embedding generated by encoder (encoder output) [[-0.15606365  0.90444064  0.82531037 -1.57368737]
 [-0.15606217  0.90443936  0.82531082 -1.57368802]]
Iteration 0 next token how with probability of 0.6265258176587956
Iteration 1 next token a with probability of 0.42708031743571
Iteration 2 next token c with probability of 0.44288777368698484</code></pre>
</div>

</div>
<p>Ok, so we got the tokens “how”, “a”, and “c”. This is not a good translation, but it’s expected! We only used random weights!</p>
<p>I suggest you to look again in detail at the whole encoder-decoder architecture from the original paper:</p>
<div>
<figure>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/transformer.png"></p>
<figcaption>Encoder and decoder</figcaption>
</figure>
</div>
</section>
</section>
<section id="conclusions">
<h2 data-anchor-id="conclusions">Conclusions</h2>
<p>I hope that was fun and informational! We covered a lot of ground. Wait…was that it? And the answer is, mostly, yes! New transformer architectures add lots of tricks, but the core of the transformer is what we just covered. Depending on what task you want to solve, you can also only the encoder or the decoder. For example, for understanding-heavy tasks such as classification, you can use the encoder stack with a linear layer on top. For generation-heavy tasks such as translation, you can use the encoder and decoder stacks. And finally, for free generation, as in ChatGPT or Mistral, you can use only the decoder stack.</p>
<p>Of course, we also did lots of simplifications. Let’s briefly check which were the numbers in the original transformer paper:</p>
<ul>
<li>Embedding dimension: 512 (4 in our example)</li>
<li>Number of encoders: 6 (6 in our example)</li>
<li>Number of decoders: 6 (6 in our example)</li>
<li>Feed-forward dimension: 2048 (8 in our example)</li>
<li>Number of attention heads: 8 (2 in our example)</li>
<li>Attention dimension: 64 (3 in our example)</li>
</ul>
<p>We just covered lots of topics, but it’s quite interesting we can achieve impressive results by scaling up this math and doing smart training. We didn’t cover training in this blog post as the goal was to understand the math when using an existing model, but I hope this provided strong foundations for jumping into the training part. I hope you enjoyed this blog post!</p>
</section>
<section id="exercises">
<h2 data-anchor-id="exercises">Exercises</h2>
<p>Here are some exercises to practice your understanding of the transformer.</p>
<ol type="1">
<li>What is the purpose of the positional encoding?</li>
<li>How does self-attention and encoder-decoder attention differ?</li>
<li>What would happen if our attention dimension was too small? What about if it was too large?</li>
<li>Briefly describe the structure of a feed-forward layer.</li>
<li>Why is the decoder slower than the encoder?</li>
<li>What is the purpose of the residual connections and layer normalization?</li>
<li>How do we go from the decoder output to probabilities?</li>
<li>Why is picking the most likely next token every single time problematic?</li>
</ol>
</section>
<section id="resources">
<h2 data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
<li><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></li>
<li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></li>
<li><a href="https://huggingface.co/learn/nlp-course/chapter1/1">Hugging Face free NLP course</a></li>
</ul>


</section>

</main> <!-- /main -->


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones (220 pts)]]></title>
            <link>https://github.com/DLYuanGod/TinyGPT-V</link>
            <guid>38859749</guid>
            <pubDate>Wed, 03 Jan 2024 20:53:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/DLYuanGod/TinyGPT-V">https://github.com/DLYuanGod/TinyGPT-V</a>, See on <a href="https://news.ycombinator.com/item?id=38859749">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">TinyGPT-V</h2>
<p dir="auto"><strong>TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones</strong></p>
<p dir="auto">Zhengqing Yuan❁, Zhaoxu Li❁, Lichao Sun❋</p>
<p dir="auto">❁Visiting Students at LAIR Lab, Lehigh University
❋Lehigh University</p>
<p dir="auto"> <a href="https://arxiv.org/abs/2312.16862" rel="nofollow"><img src="https://camo.githubusercontent.com/36622932abbbd4c66f324e4cc02e7046b72a8537858d95eaad23e7ea84f379b0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617065722d41727869762d726564" data-canonical-src="https://img.shields.io/badge/Paper-Arxiv-red"></a>  <a href="https://huggingface.co/Tyrannosaurus/TinyGPT-V" rel="nofollow"><img src="https://camo.githubusercontent.com/e5f7577427133b911a9e8a4b5ad96a8b23f11c9fe148cabbf13e5f3161910dfa/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d4d6f64656c2d626c7565" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue"></a> <a href="https://huggingface.co/spaces/llizhx/TinyGPT-V" rel="nofollow"><img src="https://camo.githubusercontent.com/5762a687b24495afb299c2c0bc68674a2a7dfca9bda6ee444b9da7617d4223a6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565" data-canonical-src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue"></a></p>

<h2 tabindex="-1" dir="auto">News</h2>
<p dir="auto">[Jan.03 2024] Welcome to Hugging Face online demo to try out our models (for Stage-3)!</p>
<p dir="auto">[Dec.28 2023] Breaking! We release the code of our TinyGPT-V.</p>
<h2 tabindex="-1" dir="auto">TinyGPT-V Traning Process</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/DLYuanGod/TinyGPT-V/blob/main/examples/Training_S.png"><img src="https://github.com/DLYuanGod/TinyGPT-V/raw/main/examples/Training_S.png" alt="Traning_Process"></a></p>
<h2 tabindex="-1" dir="auto">TinyGPT-V Model Structure</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/DLYuanGod/TinyGPT-V/blob/main/examples/TinyGPT-V-ST.png"><img src="https://github.com/DLYuanGod/TinyGPT-V/raw/main/examples/TinyGPT-V-ST.png" alt="Model"></a></p>
<h2 tabindex="-1" dir="auto">TinyGPT-V Results</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/DLYuanGod/TinyGPT-V/blob/main/examples/result.png"><img src="https://github.com/DLYuanGod/TinyGPT-V/raw/main/examples/result.png" alt="Results"></a></p>
<h2 tabindex="-1" dir="auto">Getting Started</h2>
<h3 tabindex="-1" dir="auto">Installation</h3>
<p dir="auto"><strong>1. Prepare the code and the environment</strong></p>
<p dir="auto">Git clone our repository, creating a python environment and activate it via the following command</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/DLYuanGod/TinyGPT-V.git
cd TinyGPT-V
conda env create -f environment.yml
conda activate tinygptv"><pre>git clone https://github.com/DLYuanGod/TinyGPT-V.git
<span>cd</span> TinyGPT-V
conda env create -f environment.yml
conda activate tinygptv</pre></div>
<p dir="auto"><strong>2. Prepare the pretrained LLM weights</strong></p>
<p dir="auto"><strong>TinyGPT-V</strong> is based on Phi-2.
Download the corresponding LLM weights from the following huggingface space via clone the repository using git-lfs.</p>
<p dir="auto">Phi-2 2.7B: <a href="https://huggingface.co/susnato/phi-2" rel="nofollow">Download</a></p>
<p dir="auto">Then, set the variable <em>phi_model</em> in the model config file to the LLM weight path.</p>
<ul dir="auto">
<li>For MiniGPT-v2, set the LLM path
<a href="https://github.com/DLYuanGod/TinyGPT-V/blob/main/minigpt4/configs/models/minigpt_v2.yaml#L14">here</a> at Line 14 and <a href="https://github.com/DLYuanGod/TinyGPT-V/blob/main/minigpt4/configs/models/minigpt4_vicuna0.yaml#L18">here</a> at Line 18.</li>
</ul>
<p dir="auto"><strong>3. Prepare the pretrained model checkpoints</strong></p>
<p dir="auto">Download the pretrained model checkpoints</p>
<table>
<thead>
<tr>
<th>After stage-1</th>
<th>After stage-2</th>
<th>After stage-3</th>
<th>After stage-4</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/Tyrannosaurus/TinyGPT-V/blob/main/TinyGPT-V_for_Stage1.pth" rel="nofollow">Download</a></td>
<td><a href="https://huggingface.co/Tyrannosaurus/TinyGPT-V/blob/main/TinyGPT-V_for_Stage2.pth" rel="nofollow">Download</a></td>
<td><a href="https://huggingface.co/Tyrannosaurus/TinyGPT-V/blob/main/TinyGPT-V_for_Stage3.pth" rel="nofollow">Download</a></td>
<td><a href="https://huggingface.co/Tyrannosaurus/TinyGPT-V/blob/main/TinyGPT-V_for_Stage4.pth" rel="nofollow">Download</a></td>
</tr>
</tbody>
</table>
<p dir="auto">For <strong>TinyGPT-V</strong>, set the path to the pretrained checkpoint in the evaluation config file
in <a href="https://github.com/DLYuanGod/TinyGPT-V/blob/main/eval_configs/tinygptv_stage1_2_3_eval.yaml#L8">tinygptv_stage1_2_3_eval.yaml</a> at Line 8 for Stage 1, 2 and 3 version or <a href="https://github.com/DLYuanGod/TinyGPT-V/blob/main/eval_configs/tinygptv_stage4_eval.yaml#L8">tinygptv_stage4_eval.yaml</a> for Stage 4 version.</p>
<p dir="auto"><strong>4. Update the Phi-2 Modeling for transformers lib.</strong></p>
<p dir="auto">Linux system:</p>
<div data-snippet-clipboard-copy-content="cp modeling_phi.py /root/miniconda3/envs/tinygptv/lib/python3.9/site-packages/transformers/models/phi/"><pre><code>cp modeling_phi.py /root/miniconda3/envs/tinygptv/lib/python3.9/site-packages/transformers/models/phi/
</code></pre></div>
<p dir="auto">Windows system</p>
<p dir="auto">Find your conda yourself: conda_sit/envs/tinygptv/lib/python3.9/site-packages/transformers/models/phi/ Replace modeling_phi.py in that directory with the one in TinyGPT-V/modeling_phi.py.</p>
<h3 tabindex="-1" dir="auto">Launching Demo Locally</h3>
<p dir="auto">For Stage 4, run</p>
<div data-snippet-clipboard-copy-content="python demo_v2.py --cfg-path eval_configs/tinygptv_stage4_eval.yaml  --gpu-id 0"><pre><code>python demo_v2.py --cfg-path eval_configs/tinygptv_stage4_eval.yaml  --gpu-id 0
</code></pre></div>
<p dir="auto">For Stage 1, 2 and 3, run</p>
<div data-snippet-clipboard-copy-content="python demo.py --cfg-path eval_configs/tinygptv_stage1_2_3_eval.yaml  --gpu-id 0"><pre><code>python demo.py --cfg-path eval_configs/tinygptv_stage1_2_3_eval.yaml  --gpu-id 0
</code></pre></div>
<p dir="auto">To perfer more powerful model, LLMs loads as 16 bit by default. This configuration requires about 8G GPU memory.
To more save GPU memory, you can run the model
in 8 bit below 8G device by setting <code>low_resource</code> to <code>True</code> in the relevant config file:</p>
<ul dir="auto">
<li>
<p dir="auto">Stage 4 <a href="https://github.com/DLYuanGod/TinyGPT-V/blob/main/eval_configs/tinygptv_stage4_eval.yaml#6">tinygptv_stage4_eval.yaml</a></p>
</li>
<li>
<p dir="auto">Stage 1, 2 and 3 <a href="https://github.com/DLYuanGod/TinyGPT-V/blob/main/eval_configs/tinygptv_stage1_2_3_eval.yaml#6">tinygptv_stage1_2_3_eval.yaml</a></p>
</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="-Note: Stage 4 is currently a test version as it utilizes partial data for traing. Please use Stage 3 for the demo."><pre><span><span>-</span>Note: Stage 4 is currently a test version as it utilizes partial data for traing. Please use Stage 3 for the demo.</span></pre></div>
<h3 tabindex="-1" dir="auto">Training</h3>
<p dir="auto">First you need to adjust all the updated weights in the LLM to be calculated with full precision:<a href="https://github.com/DLYuanGod/TinyGPT-V/blob/main/minigpt4%5Cmodels%5Cbase_model.py">Here</a>. Remove the comments from the following lines:</p>
<div data-snippet-clipboard-copy-content="                layer.self_attn.q_layernorm.weight.data = layer.self_attn.q_layernorm.weight.data.float()
                layer.self_attn.k_layernorm.weight.data = layer.self_attn.k_layernorm.weight.data.float()
                layer.post_layernorm.weight.data = layer.post_layernorm.weight.data.float()
                layer.input_layernorm.weight.data = layer.input_layernorm.weight.data.float()

                # Perform a similar operation for the bias item
                if layer.self_attn.q_layernorm.bias is not None:
                    layer.self_attn.q_layernorm.bias.data = layer.self_attn.q_layernorm.bias.data.float()
                if layer.self_attn.k_layernorm.bias is not None:
                    layer.self_attn.k_layernorm.bias.data = layer.self_attn.k_layernorm.bias.data.float()
                if layer.input_layernorm.bias is not None:
                    layer.input_layernorm.bias.data = layer.input_layernorm.bias.data.float()


            llama_model.model.model.final_layernorm.weight.requires_grad = True
            llama_model.model.model.final_layernorm.weight.data = llama_model.model.model.final_layernorm.weight.data.float()
            if llama_model.model.model.final_layernorm.bias is not None:
                llama_model.model.model.final_layernorm.bias.data = llama_model.model.model.final_layernorm.bias.float()"><pre><code>                layer.self_attn.q_layernorm.weight.data = layer.self_attn.q_layernorm.weight.data.float()
                layer.self_attn.k_layernorm.weight.data = layer.self_attn.k_layernorm.weight.data.float()
                layer.post_layernorm.weight.data = layer.post_layernorm.weight.data.float()
                layer.input_layernorm.weight.data = layer.input_layernorm.weight.data.float()

                # Perform a similar operation for the bias item
                if layer.self_attn.q_layernorm.bias is not None:
                    layer.self_attn.q_layernorm.bias.data = layer.self_attn.q_layernorm.bias.data.float()
                if layer.self_attn.k_layernorm.bias is not None:
                    layer.self_attn.k_layernorm.bias.data = layer.self_attn.k_layernorm.bias.data.float()
                if layer.input_layernorm.bias is not None:
                    layer.input_layernorm.bias.data = layer.input_layernorm.bias.data.float()


            llama_model.model.model.final_layernorm.weight.requires_grad = True
            llama_model.model.model.final_layernorm.weight.data = llama_model.model.model.final_layernorm.weight.data.float()
            if llama_model.model.model.final_layernorm.bias is not None:
                llama_model.model.model.final_layernorm.bias.data = llama_model.model.model.final_layernorm.bias.float()
</code></pre></div>
<p dir="auto"><strong>Stage 1 and 2:</strong></p>
<ul dir="auto">
<li>
<p dir="auto">Datasets: <a href="https://github.com/Vision-CAIR/MiniGPT-4/blob/main/dataset/README_1_STAGE.md">first stage dataset preparation instruction</a></p>
</li>
<li>
<p dir="auto">Then run:</p>
</li>
</ul>
<div data-snippet-clipboard-copy-content="torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage1.yaml"><pre><code>torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage1.yaml
</code></pre></div>
<p dir="auto">You need to execute the above code 17 times to complete the first stage of training.</p>
<ul dir="auto">
<li>Then run:</li>
</ul>
<div data-snippet-clipboard-copy-content="torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage2.yaml"><pre><code>torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage2.yaml
</code></pre></div>
<p dir="auto"><strong>Stage 3:</strong></p>
<ul dir="auto">
<li>
<p dir="auto">Datasets: <a href="https://github.com/Vision-CAIR/MiniGPT-4/blob/main/dataset/README_2_STAGE.md">stage 3 dataset preparation instruction</a></p>
</li>
<li>
<p dir="auto">Then run:</p>
</li>
</ul>
<div data-snippet-clipboard-copy-content="torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage3.yaml"><pre><code>torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage3.yaml
</code></pre></div>
<p dir="auto"><strong>Stage 4:</strong></p>
<ul dir="auto">
<li>
<p dir="auto">Datasets: <a href="https://github.com/Vision-CAIR/MiniGPT-4/blob/main/dataset/README_MINIGPTv2_FINETUNE.md">stage 4 dataset preparation instruction</a> Please prepare all datasets except COCO captions and OCR-VQA.</p>
</li>
<li>
<p dir="auto">Then run:</p>
</li>
</ul>
<div data-snippet-clipboard-copy-content="torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage4.yaml"><pre><code>torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/tinygptv_stage4.yaml
</code></pre></div>
<h3 tabindex="-1" dir="auto">Evaluation</h3>
<p dir="auto">For eval. details of TinyGPT-V, check <a href="https://github.com/DLYuanGod/TinyGPT-V/blob/main/eval_scripts/EVAL_README.md">here</a></p>
<h2 tabindex="-1" dir="auto">Star History</h2>
<p dir="auto"><a href="https://star-history.com/#DLYuanGod/TinyGPT-V&amp;Timeline" rel="nofollow"><img src="https://camo.githubusercontent.com/7193763c6fd2fce0bc6c571d185b3fe2a6263c3f658ab072b9367fceb545633e/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d444c5975616e476f642f54696e794750542d5626747970653d54696d656c696e65" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=DLYuanGod/TinyGPT-V&amp;type=Timeline"></a></p>
<h2 tabindex="-1" dir="auto">Acknowledgement</h2>
<ul dir="auto">
<li><a href="https://github.com/Vision-CAIR/MiniGPT-4">MiniGPT</a> A very versatile model of MLLMs.</li>
</ul>
<p dir="auto">If you're using TinyGPT-V in your research or applications, please cite using this BibTeX:</p>
<div dir="auto" data-snippet-clipboard-copy-content="
@misc{yuan2023tinygptv,
      title={TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones}, 
      author={Zhengqing Yuan and Zhaoxu Li and Lichao Sun},
      year={2023},
      eprint={2312.16862},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}"><pre><span>@misc</span>{<span>yuan2023tinygptv</span>,
      <span>title</span>=<span><span>{</span>TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Zhengqing Yuan and Zhaoxu Li and Lichao Sun<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2023<span>}</span></span>,
      <span>eprint</span>=<span><span>{</span>2312.16862<span>}</span></span>,
      <span>archivePrefix</span>=<span><span>{</span>arXiv<span>}</span></span>,
      <span>primaryClass</span>=<span><span>{</span>cs.CV<span>}</span></span>
}</pre></div>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">This repository is under <a href="https://github.com/DLYuanGod/TinyGPT-V/blob/main/LICENSE.md">BSD 3-Clause License</a>.
Many codes are based on <a href="https://github.com/salesforce/LAVIS">Lavis</a> with
BSD 3-Clause License <a href="https://github.com/DLYuanGod/TinyGPT-V/blob/main/LICENSE_Lavis.md">here</a>.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My 3-year experiment as a digital nomad (112 pts)]]></title>
            <link>https://www.kapwing.com/blog/digital-nomad/</link>
            <guid>38859427</guid>
            <pubDate>Wed, 03 Jan 2024 20:31:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.kapwing.com/blog/digital-nomad/">https://www.kapwing.com/blog/digital-nomad/</a>, See on <a href="https://news.ycombinator.com/item?id=38859427">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
<p>A few years ago, I got a knock on my door in the middle of the workday. It was Bill, one of the guys who worked on the ranch. “A cow we didn’t know was pregnant just gave birth,” he told me. “We’re naming it after you.”</p><p>Now, that might be a typical day in the life of a rancher. But I’m not a rancher.</p><p>I’m a full-time software engineer. I’m also a digital nomad and have been since 2020.</p><h3 id="why-i-became-a-digital-nomad">Why I became a digital nomad</h3><p>Working at Kapwing, which has a flexible remote work policy, I’m able to work from anywhere—so I do.</p><figure><img src="https://lh7-us.googleusercontent.com/WJnhl_x3nq3SXmdIr2xFPjrvLeNa1debRL3TBmXBLvZBYqWITRaN2y6MjMXk_62xf22dR_0AGzUawVrYkdxOabPEjcvaHxqc2G-RxpPNMX6tVExBFdWbQzHfonicxQEwj8rv1MSyRgKrNIsGvOPmTCw" alt="" loading="lazy" width="1920" height="1079"></figure><p>The way I see it, there’s so much natural beauty in the world and only so much time. Without being anchored to any one location, I can get out into nature more often, see the mountains and beautiful views, and do it without taking a ton of PTO. If I’m going to spend the workweek, well, working, I might as well do it somewhere pretty.</p><p>That said, the logistics of digital nomadism can be challenging. </p><p>Not just in terms of figuring out your set up and routine. But relationships become harder, too. When people in my life have big life events that I want to show up for, I’m often not near an airport and have to figure out how to get back to whichever part of the country they’re in. Wedding season in particular is difficult (IYKYK).</p><p>Truthfully, becoming a digital nomad required a lot of trial and error on my part. Along the way, I'd say I tried out three different “styles” of nomadic lifestyle:</p><ol><li>The Airbnb Nomad</li><li>The Vanlife</li><li>The Full-time Wanderer</li></ol><p>There are stark differences and pros and cons to each. So I’ll break down each approach, how things went for me, and share some lessons I’ve gathered over the last three and a half years.</p><h2 id="option-1-become-an-airbnb-nomad-%F0%9F%8F%A0">Option #1: Become an Airbnb Nomad 🏠</h2><p>This is how I got started.</p><p>I was living in Philadelphia in early 2020, working remotely in the music industry. A few months after the onset of COVID, I decided to ditch my lease and hit the road. I started by booking 4-week stays, the minimum length required to get the 30% monthly discount, at different Airbnb properties, traveling on the weekends and working during the week.</p><p>Eventually, I bought a Tesla Model Y to save on lodging costs during longer trips between locations. The Model Y has plenty of space for sleeping and can be outfitted with a custom-fit mattress pad, so it’s actually quite comfortable.<br></p><figure><img src="https://lh7-us.googleusercontent.com/fGU9Ieu1w-aTns6k6T0pE3xWPdRTrt5rUphJ2I4MyS1dNegmdmb8D3fdUQgHzlpHY8ylh_ubB9nVJHahNaV4RYjzcBwQJDgGqgKW8Z0NhfWtmsjDgh0CoUCBc2R9X9QeuMSTAz52HBirUiZXOPBvmBo" alt="" loading="lazy" width="1664" height="935"><figcaption>There’s enough space in the Model Y to haul a mattress and my desk setup.</figcaption></figure><h3 id="why-i-like-the-airbnb-nomad-lifestyle">Why I like the Airbnb Nomad lifestyle</h3><p>This is probably the easiest way to get started as a digital nomad. There aren’t any huge investments you need to make up front, you still have the comforts of home at each Airbnb, and you can try it out without too much risk. You don’t even need to break your lease if you just want to test-drive nomadic life for a month or two.</p><p>Another reason I really liked the Airbnb route is that you have a built-in local guide to hang out with, thanks to your host. I met a lot of cool people traveling around this way (and had at least one cow named after me). Plus, there are lots of great spots in rural places you might never typically visit that open up to you with this method of traveling.</p><figure><img src="https://www.kapwing.com/blog/content/images/2024/01/image.png" alt="" loading="lazy" width="2000" height="1500" srcset="https://www.kapwing.com/blog/content/images/size/w600/2024/01/image.png 600w, https://www.kapwing.com/blog/content/images/size/w1000/2024/01/image.png 1000w, https://www.kapwing.com/blog/content/images/size/w1600/2024/01/image.png 1600w, https://www.kapwing.com/blog/content/images/size/w2400/2024/01/image.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Charging up the car at my casita Airbnb in Tombstone, Arizona</figcaption></figure><h3 id="what-you-need-as-an-airbnb-nomad">What you need as an Airbnb Nomad</h3><p>Like I said, this method doesn’t require much in terms of upfront costs or purchases, however there are a few things that I highly recommend.</p><p><strong>1. An ergonomic desk set up. </strong>I quickly learned that I <em>needed</em> an ergonomic chair and a good desk in order to make this work. As a software engineer, I’m sitting at my computer all day. A hardback chair and dining table at the Airbnb just aren’t going to cut it. Fortunately, my Tesla has enough room to bring both my desk and chair with me from place to place. </p><p><strong>2. Supplementary WiFi. </strong>This might not be necessary at every Airbnb, but if you’re picking places out in the middle of nowhere, consider bringing a backup connection.</p><p><strong>3. A vehicle equipped for car camping.</strong> Having a vehicle large enough to sleep in really increased my range of travel when moving between Airbnbs while still keeping costs down. Plus, you can travel more on the weekends with your Airbnb as home base.</p><h2 id="option-2-become-a-vanlifer-%F0%9F%9A%90">Option #2: Become a Vanlifer 🚐</h2><p>After a while, I realized that sleeping in the Tesla was perfectly comfortable and gave me way more freedom. That’s when I moved from being an Airbnb Nomad to being a Vanlifer. Although I never had a van. Just my Tesla.</p><p><strong>If you haven't heard the term, </strong>car camping (or "vanlife") is when you rely on the public for amenities like showers, food, and a place to work. You can replace the bed in your Airbnb with your car camping setup, but you still need to find a place to shower, eat, and make a living. Luckily, it’s pretty common to find towns, even rural towns, that have a Planet Fitness, coworking space, and even an EV charger.</p><h3 id="why-i-like-the-vanlife">Why I like the Vanlife</h3><p>As a full-time employee, using coworking spaces is the most convenient way to be a digital nomad. Depending on what kind of setup you have, you might find vanlife even more appealing.</p><p>With just the Tesla, I was saving quite a bit of money on lodging, but spending more on food because I didn’t have a cooking setup, which was a definite drawback. Plus, you get tired of eating out after a while. If you have a full van build-out, though, you might have a kitchenette that suits your needs just fine.</p><p>Although I found it somewhat annoying needing to be near a city at all times during the work week, the increased mobility made up for it. I was able to move daily if I wanted to because everything I needed was self-contained within my car. I also didn’t need to reserve the weekends for traveling between locations, anymore, which meant I had more time for exploring. </p><figure><img src="https://lh7-us.googleusercontent.com/hEzyN20PHZ5I5jJXGOV-R1PHlZty03TN2RYs-UVIGVd0AdBmMsUFlbJH_WB_eQlJN38dzxri1oPNeyblUS_3bKHafP4JkMvMFNdOhkKpDA7n6mlv7_CYiTxeQKoh6rY1eTHqxt8i441JbvIL4YdvHcI" alt="" loading="lazy" width="1920" height="1079"><figcaption>Off-roading with the Tesla in Dotsero, CO.</figcaption></figure><p>Working from a coworking space or coffee shop, I’d get all of my human interaction during the week, then on the weekends I had the range and off-road capabilities to get out into the wilderness and just be alone in nature.</p><h3 id="what-you-need-as-a-vanlifer">What you need as a Vanlifer</h3><p>Here’s my recommended starter kit for becoming a vanlifer as a full-time employee.</p><p><strong>1. A suitable vehicle. </strong>I’m calling it vanlife, but you don’t actually have to buy a kitted out Sprinter van to make this happen. I did it with just my Tesla Model Y. At minimum, you’ll need a car, truck, or van that you can comfortably sleep in.</p><p><strong>2. Planet Fitness (or similar) membership. </strong>Unless you have an RV or Airstream, you need somewhere to shower. When I was all-in on vanlife, I had a membership to Planet Fitness, because those gyms are just about everywhere, which made for easy shower and bathroom access wherever I went.</p><p><strong>3. Access to a coworking space or place to work.</strong> This is more a logistical consideration. Does the city you’re headed to have a coworking space? Sometimes you can get a membership, although then you’re even further restricted to that specific coworking franchise’s locations. I recommend just sticking with the day rate. If there are no coworking spaces, you can always post up in the local library or coffee shop.</p><p><strong>4. Access to car charging or RV hookups. </strong>Depending on what kind of vehicle you have, you might need to take this into consideration, too. Driving the Model Y, I needed to make sure there was always an EV charging station in the cities I visited.</p><figure><img src="https://lh7-us.googleusercontent.com/xsj-MbSGhImQYafmypSaUj9Hjd1tWF1IXNyF9oQBCVZoKyvXDJZECk9-274cL23Nn8yXZMWuF5CoCEdMxBkDdCCZJD4z_JtK5xao42L1oh1kj6fhD9P4AbiyiD1nZtou3Xf71tBRTQvoh3vW0NkNIg0" alt="" loading="lazy" width="1920" height="1079"><figcaption>Casual glacier views in Hyder, AK.</figcaption></figure><h2 id="option-3-become-a-full-time-wanderer-%F0%9F%A5%BE">Option #3: Become a Full-time Wanderer 🥾</h2><p>The final evolution of my digital nomadism, at least so far, has been full-time wandering with a trailer setup. </p><figure><img src="https://www.kapwing.com/blog/content/images/2024/01/image-1.png" alt="" loading="lazy" width="2000" height="1500" srcset="https://www.kapwing.com/blog/content/images/size/w600/2024/01/image-1.png 600w, https://www.kapwing.com/blog/content/images/size/w1000/2024/01/image-1.png 1000w, https://www.kapwing.com/blog/content/images/size/w1600/2024/01/image-1.png 1600w, https://www.kapwing.com/blog/content/images/size/w2400/2024/01/image-1.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Setting up camp outside the Valley of Fire</figcaption></figure><p>While vanlife was convenient, I found that I missed being able to get out and away from the cities for extended periods. While I enjoyed my minimalist setup with just the Tesla, I decided towing a trailer would give me that extra self-sufficiency I was looking for. </p><h3 id="why-i-like-the-full-time-wanderer-lifestyle">Why I like the Full-Time Wanderer lifestyle</h3><p>Similar to when I was doing long-term Airbnb stays, it’s nice to have all my creature comforts with me wherever I go. My daily routine looks pretty much the same as most people’s now, because my trailer has all the typical home amenities.</p><figure><img src="https://lh7-us.googleusercontent.com/42ktOK7w53C5URUkEE7T9dkIJ2qLZFkr3kiNIvv13lZQDzphaX9lcXb9rQJNi7j3covetY-PY5GkQswSnVNX65pg5MPY_WiK7SIcsPLp7A2o8SMv4Y4ak5Sp78RSuLDckOHXlTMy7n6UCZMcWRz7wPg" alt="" loading="lazy" width="1246" height="700"><figcaption>My trailer setup with solar panels outside of Monticello, Utah</figcaption></figure><p>I have a kitchen, a full bathroom with a shower, two beds, and even a whole office space inside the trailer.</p><p>One thing that’s not super optimal, unfortunately, is that I’m towing my trailer with an electric vehicle, something it’s not really designed for.</p><p>That means my range while hauling the trailer is pretty limited. With the extra weight, a charge will only last about 60 miles, rather than the standard ~300 it normally gets. That means moving from location to location takes quite a bit longer, so I reserve that for weekends. Recently, I was hopping back and forth between Grand Junction, CO and Moab, UT—a distance of 113 miles. I had to go about halfway, stop, unhitch the trailer, then go back and charge the Tesla up again before finishing the trip. It makes travel days very slow.</p><p>I like lingering in one place longer anyway, so I don’t have to move the trailer all that often. Plus, with the Model Y I still have the option to go car camping with the Tesla. I just drop the trailer as a home base and can make multiple little trips around the area or even further afield.</p><figure><img src="https://lh7-us.googleusercontent.com/70T-GLovmzjr0B2B0sRYxKUmuS6WmwDEesPO9bhGfi0aH5OHWQY6rW2hdq_Jnbf-9iqyWzFNR709_9QGsyZw6-_hQL2Y-OS-pT7ER-kdm7WtYkqL3EfB6KO0VkhP87Oz3hfxwHbsoLuZdpEOnicrn9Y" alt="" loading="lazy" width="1730" height="972"><figcaption>Sometimes I just set up my desk outside, wherever I am and work in nature.</figcaption></figure><p>Recently, I’ve added some solar panels to my setup. I don’t generate enough power to charge the car, but it will run my trailer’s electrical plus my internet, which makes being self-sufficient much easier. I tend to follow the sun to optimize my solar panel usage and a few other logistical reasons, so it’s always pretty good weather wherever I’m going which is a nice bonus.</p><h3 id="what-you-need-as-a-full-time-wanderer">What you need as a Full-time Wanderer</h3><p>Your experience with the full-time trailer lifestyle will probably vary if you’re not trying to tow it with an electric car, but there are a few essentials you’ll need:</p><p><strong>1. Reliable internet connection. </strong>With Airbnbs or coworking spaces, you typically have access to fast, reliable internet. Taking the self-sufficient trailer route, though, you’ll need to supply your own. I use <a href="https://www.starlink.com/">Starlink</a>, which meets my needs most of the time. I’ve learned to have a few redundancies in place, though. Cell signal to power a hotspot is ideal, but not always available. Starlink is a little more temperamental but it’s faster and great for areas where you can’t get a cell signal.</p><figure><img src="https://lh7-us.googleusercontent.com/SRQY9lYB1oFqLi7ATVyB_kE8PDJ0e9tAV-FzmQN6yPwxL27734sl_hVgWxfHSYZShIV4MXRyYndX8W69ig-skOYXyxIvkqA6kKuMuIaxnfaxNfX9AEhh2p-eZCl4_YCOAHJz4NTfOswt4P8aiHOn-x8" alt="" loading="lazy" width="1920" height="1079"></figure><p><strong>2. Temperate weather.</strong> There are seasonal limitations to living out of a trailer. Specifically, you can’t let the pipes freeze. I spent the summer and autumn visiting the more extreme climates, but as soon as temperatures started to dip I took the trailer to a more temperate area. Currently, I’m just outside of Las Vegas.</p><p><strong>3. Access to a water fill station. </strong>Part of self-sufficiency is having access to running water inside your trailer, which means you need somewhere to fill up your water tank. I’ve found that I can fill my tank on the weekends and that will last me a whole week with enough water for showering, cooking, and drinking.</p><h2 id="digital-nomad-setups-i%E2%80%99ve-tried-that-didn%E2%80%99t-work">Digital nomad setups I’ve tried that didn’t work</h2><p>With nearly four years of trial and error, there are bound to be some things that didn’t work for me. Here are a couple of failed experiments from my time on the road:</p><h3 id="1-using-my-car-as-an-office">1. Using my car as an office.</h3><p>I thought maybe the Tesla display could work as an extra monitor, but the chair isn’t in the right position for hours of work. I’ve mentioned before that I literally hauled around my own desk and chair to different Airbnbs because an ergonomic setup is so important in my line of work. And working from my car just wasn’t ergonomic at all<em>.</em></p><figure><img src="https://lh7-us.googleusercontent.com/d-tf2AUffkyD83TgAWJyYSkrM8adR_BdvF5ESAaQn3WM-8h_e8pecsoaCU-kYq0cwJqMVjy9eveSQivO_RkAFvzNnV-wf17PY7jai6wFZPGiQSno3TrS0xsvA5L8PI0cTg4y4_hrnrPxDN7jbnjCOh4" alt="" loading="lazy" width="1728" height="1296"><figcaption>You can see how cramped the workspace setup was in the front seat of the Tesla.</figcaption></figure><p>So, yeah, skip the Tesla display when building your digital nomad battle station. Your back will thank you.</p><h3 id="2-building-out-a-floating-office">2. Building out a floating office.</h3><p>At one point, I tried building a floating office onto my tow hitch, my thought process being that it would free me up to travel farther with just the Tesla, since I would have a mobile work station. I got as far as a working prototype before realizing that it wasn’t going to work. At least, not without way more effort and customization than I was willing to put in.</p><figure><img src="https://lh7-us.googleusercontent.com/4XHMgySm19rrHK5M2dOxfrObodKBOGOTD4F4egrq5frQkOD0DSCEh5kR51a6dCRl-duPg683anCWI5NKZW82-5x2yquoK8_kwWj5C4-TBuVNbkWNAISLpI9_hGKITuxb7wUiPPR9K0yVS9WUqBSXv5g" alt="" loading="lazy" width="2048" height="1536"></figure><p>The main problem was that I couldn’t reduce glare, wind, and weather enough for functional work, despite the various umbrella configurations I attempted. It also wasn’t ergonomic—my feet would kind of hang in the air while I was sitting at the desk, so not a good long term solution. <br></p><figure><img src="https://lh7-us.googleusercontent.com/xV8zdLehqmwK5LNngAOqFAm8MCWg52V3CXjFtuLLTMsVJKhwCk5Qd1Pxa77E4Xp-SSKj9V18btg10HVOn3jCqFAPDU9UZQSy1zBa0_zvgCTsv--JVIDk1luaqBZqj6WWDQJ-ajTW1gUNOcWZsrnvm3E" alt="" loading="lazy" width="1920" height="1080"><figcaption>Believe it or not, I drove down the highway with this contraption affixed to the back of my car.</figcaption></figure><p>It was around this point that I started looking into just getting a trailer.</p><h2 id="what%E2%80%99s-next-for-me-as-a-digital-nomad">What’s next for me as a digital nomad?</h2><p>I’m currently waiting out the winter season with my trailer parked here in Nevada, planning to travel around a bit without the trailer. Next summer, I’m taking the trailer all the way up to Alaska.</p><p>With so many different ways to be a digital nomad available to me, I don’t currently have any plans to go back to a single, permanent home base. It's a somewhat unique lifestyle that takes work to pull off, but it's one I'm really enjoying.</p>
<a href="https://www.kapwing.com/signin" id="post-upsell" data-utm-campaign="inline cta">Create content faster with Kapwing's online video editor&nbsp;→ </a>
 </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Most states start school too early in the morning (112 pts)]]></title>
            <link>https://www.atlasobscura.com/articles/usa-school-start-times</link>
            <guid>38859147</guid>
            <pubDate>Wed, 03 Jan 2024 20:09:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.atlasobscura.com/articles/usa-school-start-times">https://www.atlasobscura.com/articles/usa-school-start-times</a>, See on <a href="https://news.ycombinator.com/item?id=38859147">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="article-body">
<p><span>Louisiana’s state bird is the </span>brown pelican. It should be the early bird. High school students in the Bayou State start school on average at 7:30 a.m. That is much earlier than anywhere else in the United States.</p>
<p>But it is doubtful those Louisiana students will catch the worm. Studies show that early school starts contribute to sleep debt in adolescents, which is detrimental to their health, both physical and mental.</p>
<h3><strong>No earlier than 8:30 a.m.</strong></h3>
<p>That is why the American Academy of Pediatrics (AAP)<a href="https://www.sleepreviewmag.com/sleep-health/demographics/age/aap-recommends-delaying-school-start-times-combat-teen-sleep-deprivation/"> recommended in 2014</a> that school for middle and high school students should start no earlier than 8:30 a.m. It is good advice that largely goes unheeded. The AAP found that fully 93 percent of American high school bells ring before that time. This map, compiled from<a href="https://nces.ed.gov/surveys/ntps/tables/ntps1718_table_05_s1s.asp"> data provided by the National Center for Education Statistics</a> for the years 2017 and 2018, supports that finding.</p>
<p>In only three places—Washington DC, Alaska, and South Carolina—did a student’s day start at or after the recommended earliest time of 8:30 (on average, aggregating the starting times from the various school districts). Here is an overview, enough to make all but the most hardcore morning-persons shudder:</p>
<ul>
<li aria-level="1">7:30 a.m. – Louisiana’s sleepy-eyed students shuffle into class.</li>
<li aria-level="1">7:36-7:45 a.m. – Lessons start at public high schools in Connecticut, Massachusetts, Nevada, and New Hampshire.</li>
<li aria-level="1">7:46-7:55 a.m. – It is not even 8:00 a.m. yet, but high school students are already taking classes in Colorado, Delaware, Florida, Maine, Michigan, Mississippi, Missouri, New Jersey, Ohio, Pennsylvania, Rhode Island, West Virginia, Wisconsin, and Wyoming.</li>
<li aria-level="1">7:56-8:05 a.m. – Now it is the turn of high schoolers in Alabama, Arizona, Arkansas, California, Indiana, Kansas, New Mexico, New York, Oklahoma, Tennessee, Utah, and Washington.</li>
<li aria-level="1">8:06-8:15 a.m. – Well past the hour, students file into class in Georgia, Hawaii, Idaho, Illinois, Kentucky, Montana, Nebraska, North Carolina, North Dakota, Oregon, South Dakota, Texas, Virginia, and Vermont.</li>
<li aria-level="1">8:16-8:25 a.m. – Just two states start school this late: Iowa and Minnesota. But they are not the latest.</li>
<li aria-level="1">8:26-8:35 a.m. – That is how late high school starts in Alaska and South Carolina. But wait…</li>
<li aria-level="1">Only in Washington DC, Alaska, and South Carolina does the school day start at or after the recommended earliest time of 8:30 a.m.</li>
</ul>
<figure><img src="https://img.atlasobscura.com/GjBymsxhRK1kLqAvOhWgcoJSyz8zqLkkRkT7iYC1ut4/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy85MDYzYWM1Yy1l/MWYzLTRhMmItYWMy/OC01Y2I3OWRjOGRl/NjUzNGRkNmRhMWFk/MzlhNzVkM2Zfc3Rh/cnR0aW1lcy1oaWdo/c2Nob29sX21hcC5w/bmc.png" alt="About 93% of high schools in the U.S. start school earlier than the time the American Academy of Pediatrics recommends. " width="auto" data-article-image-id="undefined" data-full-size-image="https://assets.atlasobscura.com/article_images/full/99153/image" data-kind="article-image" id="article-image-99153" data-src="https://img.atlasobscura.com/GjBymsxhRK1kLqAvOhWgcoJSyz8zqLkkRkT7iYC1ut4/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy85MDYzYWM1Yy1l/MWYzLTRhMmItYWMy/OC01Y2I3OWRjOGRl/NjUzNGRkNmRhMWFk/MzlhNzVkM2Zfc3Rh/cnR0aW1lcy1oaWdo/c2Nob29sX21hcC5w/bmc.png"><figcaption>About 93% of high schools in the U.S. start school earlier than the time the American Academy of Pediatrics recommends. <a target="_blank" href="https://www.reddit.com/r/dataisbeautiful/comments/p46u94/comment/h8whns0/">1ew/reddit/; Created with MapChart/CC BY-SA 4.0 Deed</a></figcaption></figure>
<p>Overall, about 40 percent of American high schools start before 8 a.m. and more than 20 percent start at 7:45 a.m. or earlier. Only 15 percent start at or after the recommended earliest starting time of 8:30 a.m.</p>
<h3><strong>The school bus problem</strong></h3>
<p>Why do American high schools generally start so early? One large part of the answer: school buses. A lot of school districts re-use the same buses to pick up students from different schools: first the high schoolers, then the middle schoolers, and finally the elementary schoolers. In South Carolina, the order is generally reversed, which is why it is among the “latest” states on this map.</p>
<p>Early school starts are not the only cause of teenage drowsiness, but they are a crucial factor—especially because natural sleep cycles make it difficult for post-puberty teenagers to fall asleep before 11 p.m.</p>
<p>A poll by the National Sleep Foundation found that 59 percent of 6th through 8th graders and 87 percent of high school students got less than the recommended amount of sleep (8.5 to 9.5 hours) on school nights. In the words of America’s leading soporific publication<a href="https://www.sleepreviewmag.com/sleep-health/demographics/age/aap-recommends-delaying-school-start-times-combat-teen-sleep-deprivation/"> <em>Sleep Review</em></a>, the average American adolescent is “chronically sleep-deprived and pathologically sleepy.”</p>
<p>Chronic sleep loss in adolescents has been linked to a host of negative consequences:</p>
<ul>
<li aria-level="1">Adolescents with sleep debt and/or disrupted sleep-wake cycles may suffer from poor judgment, lack of motivation, and overall reduced alertness, leading to poor academic performance.</li>
<li aria-level="1">There is a bidirectional relationship between sleep disturbances and mood disorders, especially depression.</li>
<li aria-level="1">Irregular and insufficient sleep in high school students has been found to predict certain types of risky behavior such as drunk driving, smoking, taking drugs, and delinquency.</li>
<li aria-level="1">Adolescents with insufficient sleep have an increased risk of suicidal ideation.</li>
<li aria-level="1">Several studies found links between sleep deprivation and obesity.<a href="https://pediatrics.aappublications.org/content/134/3/e921#ref-110"> One study</a> estimates that for each hour of sleep lost (over a long period of time), the odds of being obese increased by 80 percent.</li>
<li aria-level="1">Sleep deprivation leads to metabolic perturbations that increase the risk of type 2 diabetes.</li>
<li aria-level="1">Sleepiness increases the risk of traffic accidents. Young people are particularly affected. A 1995 study found that 55 percent of crashes due to drowsiness were caused by drivers 25 years or younger.</li>
</ul>
<p>Because of all those reasons, not just the AAP but<a href="https://www.cdc.gov/sleep/features/schools-start-too-early.html"> also the CDC</a> recommends later school start times and urges parents to advocate for them. Fortunately, this has met some success. In 2019, California Governor Gavin Newsom signed into law Bill 328, which requires middle schools to begin no earlier than 8:00 a.m. and high schools no earlier than 8:30 a.m. It went into effect in 2022.</p>
<p>Other states may consider similar moves. And there is some evidence that starting school later is beneficial. Around 400 school districts around the country have already moved their start time to 8:30 or later, often resulting in dramatically improved test scores, attendance rates, and graduation rates. (One Texas school district<a href="https://www.fatherly.com/love-money/senator-anthony-portantino-sleep-in-school-start-time/"> reported an 11 percent increase</a> in its graduation rate.)</p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Someone was breaking into Orange Spain RIPE account (and break their /12) (120 pts)]]></title>
            <link>https://benjojo.co.uk/u/benjojo/h/r1zj333N4L6cF7P1xv</link>
            <guid>38858500</guid>
            <pubDate>Wed, 03 Jan 2024 19:20:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://benjojo.co.uk/u/benjojo/h/r1zj333N4L6cF7P1xv">https://benjojo.co.uk/u/benjojo/h/r1zj333N4L6cF7P1xv</a>, See on <a href="https://news.ycombinator.com/item?id=38858500">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="honksonpage">






<article data-convoy="data:,electrichonkytonk-GG6V3W6X8mx34Y9WSp">





<header>

<a href="https://benjojo.co.uk/u/benjojo" rel="noreferrer">

<img alt="" src="https://benjojo.co.uk/a?a=https%3a%2f%2fbenjojo.co.uk%2fu%2fbenjojo">


</a><p><a href="https://benjojo.co.uk/u/benjojo" rel="noreferrer">

</a><a href="https://benjojo.co.uk/u/benjojo" rel="noreferrer">benjojo</a>

<span><a href="https://benjojo.co.uk/u/benjojo/h/r1zj333N4L6cF7P1xv" rel="noreferrer">posted</a> 03 Jan 2024 17:18 +0000</span>



<br>

</p></header>
<details open="">
<summary></summary>
<p>Ah. Orange Spain has had their /12 (and likely others) broken by (what appears to be) someone breaking into their RIPE account and making RPKI ROA's to somewhere else.</p><p>Current reachability of <a href="https://bgp.tools/prefix/90.160.0.0/12#connectivity" rel="noreferrer">impacted prefixes</a> is pretty poor</p>
<p>The current ROA is pointing to AS49581 ("Ferdinand Zink trading as Tube-Hosting")</p>
<p>Someone has already claimed responsibility for this: <a href="https://twitter.com/Ms_Snow_OwO/status/1742357282917109928" rel="noreferrer">https://twitter.com/Ms_Snow_OwO/status/1742357282917109928</a> </p>
<p>Shout out to <a href="https://bsd.network/users/tstrickx" rel="noreferrer">@tstrickx</a> for informing me of this</p>







<p><img src="https://benjojo.co.uk/d/7wk8HvXcGYJZdJSVp1.png" title="JSs668h6KPWs7x1KP7.png" alt="JSs668h6KPWs7x1KP7.png">




</p></details>



</article>


<article data-convoy="data:,electrichonkytonk-GG6V3W6X8mx34Y9WSp">





<header>

<a href="https://benjojo.co.uk/u/benjojo" rel="noreferrer">

<img alt="" src="https://benjojo.co.uk/a?a=https%3a%2f%2fbenjojo.co.uk%2fu%2fbenjojo">


</a><p><a href="https://benjojo.co.uk/u/benjojo" rel="noreferrer">

</a><a href="https://benjojo.co.uk/u/benjojo" rel="noreferrer">benjojo</a>

<span><a href="https://benjojo.co.uk/u/benjojo/h/69w66xSkFsw73KS8k3" rel="noreferrer">replied</a> 03 Jan 2024 17:29 +0000</span>


<br>
<span>
in reply to: <a href="https://benjojo.co.uk/u/benjojo/h/r1zj333N4L6cF7P1xv" rel="noreferrer">https://benjojo.co.uk/u/benjojo/h/r1zj333N4L6cF7P1xv</a>
</span>


<br>

</p></header>
<details open="">
<summary></summary>
<p>Here is a full list of impacted prefixes, that's a lot of broken traffic I suspect...






</p><p><img src="https://benjojo.co.uk/d/C13jT6FkYT2mTF35B3.png" title="A screenshot of bgp.tools showing many /16's worth of IP space signed to the wrong ASN" alt="A screenshot of bgp.tools showing many /16's worth of IP space signed to the wrong ASN">




</p></details>



</article>


<article data-convoy="data:,electrichonkytonk-GG6V3W6X8mx34Y9WSp">





<header>

<a href="https://benjojo.co.uk/u/benjojo" rel="noreferrer">

<img alt="" src="https://benjojo.co.uk/a?a=https%3a%2f%2fbenjojo.co.uk%2fu%2fbenjojo">


</a><p><a href="https://benjojo.co.uk/u/benjojo" rel="noreferrer">

</a><a href="https://benjojo.co.uk/u/benjojo" rel="noreferrer">benjojo</a>

<span><a href="https://benjojo.co.uk/u/benjojo/h/29dBSKgf66c2D1mvnD" rel="noreferrer">replied</a> 03 Jan 2024 17:31 +0000</span>


<br>
<span>
in reply to: <a href="https://benjojo.co.uk/u/benjojo/h/69w66xSkFsw73KS8k3" rel="noreferrer">https://benjojo.co.uk/u/benjojo/h/69w66xSkFsw73KS8k3</a>
</span>


<br>

</p></header>
<details open="">
<summary></summary>
<p>In case it disappears here is the screenshots of the tweet from the alleged person who did the mis-signing






</p><p><img src="https://benjojo.co.uk/d/NDKy9p351Z8DC1M6Ln.png" title="A screenshot of twitter with a censored user name, saying @orange_es &quot; Meow meow meow! I have fixed your RIPE admin account security. Message me to get the new credentials :^)&quot;" alt="A screenshot of twitter with a censored user name, saying @orange_es &quot; Meow meow meow! I have fixed your RIPE admin account security. Message me to get the new credentials :^)&quot;">




</p></details>



</article>


<article data-convoy="data:,electrichonkytonk-GG6V3W6X8mx34Y9WSp">





<header>

<a href="https://benjojo.co.uk/u/benjojo" rel="noreferrer">

<img alt="" src="https://benjojo.co.uk/a?a=https%3a%2f%2fbenjojo.co.uk%2fu%2fbenjojo">


</a><p><a href="https://benjojo.co.uk/u/benjojo" rel="noreferrer">

</a><a href="https://benjojo.co.uk/u/benjojo" rel="noreferrer">benjojo</a>

<span><a href="https://benjojo.co.uk/u/benjojo/h/tBF62DsPzCYzbWTZC9" rel="noreferrer">replied</a> 03 Jan 2024 18:01 +0000</span>


<br>
<span>
in reply to: <a href="https://benjojo.co.uk/u/benjojo/h/29dBSKgf66c2D1mvnD" rel="noreferrer">https://benjojo.co.uk/u/benjojo/h/29dBSKgf66c2D1mvnD</a>
</span>


<br>

</p></header>
<details open="">
<summary></summary>
<p>The bad ROAs are now being withdrawn, as far as I can see only these remain with bad ROAs:</p><p>IP address blocks:</p>
<p>145.1.240.0/20 maxlen: 20</p>
<p>149.74.0.0/16 maxlen: 16</p>
<p>1.178.232.0/21 maxlen: 21<br></p>
<hr><p>Using the RPKI CRL File we can see rough estimates to when things where changed/timeline</p>







<p><img src="https://benjojo.co.uk/d/VmRkw18cf2m62f16H5.png" title="A list of timestamps, with a flurry of activity around 13:59:48 and 09:38:58" alt="A list of timestamps, with a flurry of activity around 13:59:48 and 09:38:58">




</p></details>



</article>


<article data-convoy="data:,electrichonkytonk-GG6V3W6X8mx34Y9WSp">





<header>

<a href="https://chaos.social/users/flangey" rel="noreferrer">

<img alt="" src="https://benjojo.co.uk/a?a=https%3a%2f%2fchaos.social%2fusers%2fflangey">


</a><p><a href="https://chaos.social/users/flangey" rel="noreferrer">

</a><a href="https://chaos.social/users/flangey" rel="noreferrer">flangey@chaos.social</a>

<span><a href="https://chaos.social/@flangey/111693504447361302" rel="noreferrer">replied</a> 03 Jan 2024 18:47 +0000</span>


<br>
<span>
in reply to: <a href="https://benjojo.co.uk/u/benjojo/h/r1zj333N4L6cF7P1xv" rel="noreferrer">https://benjojo.co.uk/u/benjojo/h/r1zj333N4L6cF7P1xv</a>
</span>


<br>

</p></header>
<details open="">
<summary></summary>
<p><a href="https://benjojo.co.uk/u/benjojo" rel="noreferrer">@benjojo</a> <a href="https://bsd.network/@tstrickx" rel="noreferrer">@tstrickx</a> the RIPE NCC Access available MFA options and auditability are not really good enough.</p>




</details>



</article>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A Who is Hiring app with AI filters (122 pts)]]></title>
            <link>https://bernawil.github.io/hn-who-is-hiring/</link>
            <guid>38858369</guid>
            <pubDate>Wed, 03 Jan 2024 19:10:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bernawil.github.io/hn-who-is-hiring/">https://bernawil.github.io/hn-who-is-hiring/</a>, See on <a href="https://news.ycombinator.com/item?id=38858369">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Any felons successfully found IT work post-release? (323 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=38858075</link>
            <guid>38858075</guid>
            <pubDate>Wed, 03 Jan 2024 18:53:57 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=38858075">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="38858075">
      <td><span></span></td>      <td><center><a id="up_38858075" href="https://news.ycombinator.com/vote?id=38858075&amp;how=up&amp;goto=item%3Fid%3D38858075"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=38858075">Ask HN: Any felons successfully found IT work post-release?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_38858075">118 points</span> by <a href="https://news.ycombinator.com/user?id=publicprivacy">publicprivacy</a> <span title="2024-01-03T18:53:57"><a href="https://news.ycombinator.com/item?id=38858075">3 hours ago</a></span> <span id="unv_38858075"></span> | <a href="https://news.ycombinator.com/hide?id=38858075&amp;goto=item%3Fid%3D38858075">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Any%20felons%20successfully%20found%20IT%20work%20post-release%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=38858075&amp;auth=8c85f0198b4de32032e47f550ab28e8b47c3e4ca">favorite</a> | <a href="https://news.ycombinator.com/item?id=38858075">100&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>Hello HN,</p><p>Does anyone have experience getting back into tech/startups post-felony?</p><p>I have been looking for work since I was released  for an assault charge in November 2022.</p><p>Previously I worked in Information Security as a SecOps Eng, most recently at Tinder. Between lack of recent job experience, and my record, I have been through a series of offer reneges, recruiters ghosting me, or going into HR resume black holes.</p><p>I am eager to get back into tech and feel like my old self adding value to a great team/org.</p><p>Anyone have leads on companies that are open to taking chances on good candidates with less than sparkling backgrounds?</p><p>NOTE: My offense was not computer/finance/fraud/selling drugs/physical violence/based at all.</p><p>Here is my linkedin:</p><p>https://www.linkedin.com/in/saunderscaleb/</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Niklaus Wirth has died (1808 pts)]]></title>
            <link>https://twitter.com/Bertrand_Meyer/status/1742613897675178347</link>
            <guid>38858012</guid>
            <pubDate>Wed, 03 Jan 2024 18:50:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/Bertrand_Meyer/status/1742613897675178347">https://twitter.com/Bertrand_Meyer/status/1742613897675178347</a>, See on <a href="https://news.ycombinator.com/item?id=38858012">Hacker News</a></p>
Couldn't get https://twitter.com/Bertrand_Meyer/status/1742613897675178347: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[The drive stats of Backblaze storage pods (253 pts)]]></title>
            <link>https://www.backblaze.com/blog/the-drive-stats-of-backblaze-storage-pods/</link>
            <guid>38857396</guid>
            <pubDate>Wed, 03 Jan 2024 18:07:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.backblaze.com/blog/the-drive-stats-of-backblaze-storage-pods/">https://www.backblaze.com/blog/the-drive-stats-of-backblaze-storage-pods/</a>, See on <a href="https://news.ycombinator.com/item?id=38857396">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-label="The Drive Stats of Backblaze Storage Pods" itemscope="" itemtype="https://schema.org/CreativeWork"><div itemprop="text">
<figure><img fetchpriority="high" decoding="async" width="1024" height="583" src="https://www.backblaze.com/blog/wp-content/uploads/2024/01/bb-bh-Server-Stats-1024x583.png" alt="A decorative image showing the Backblaze logo on a cloud over a pattern representing a network. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2024/01/bb-bh-Server-Stats-1024x583.png 1024w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/bb-bh-Server-Stats-300x171.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/bb-bh-Server-Stats-768x437.png 768w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/bb-bh-Server-Stats-560x319.png 560w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>

<p>Since 2009, Backblaze has <a href="https://www.backblaze.com/blog/petabytes-on-a-budget-how-to-build-cheap-cloud-storage/" target="_blank" rel="noreferrer noopener">written</a> <a href="https://www.backblaze.com/blog/petabytes-on-a-budget-v2-0revealing-more-secrets/" target="_blank" rel="noreferrer noopener">extensively</a> <a href="https://www.backblaze.com/blog/petabytes-on-a-budget-10-years-and-counting/" target="_blank" rel="noreferrer noopener">about</a> <a href="https://www.backblaze.com/blog/origins-of-the-pod/" target="_blank" rel="noreferrer noopener">the</a> <a href="https://www.backblaze.com/blog/open-source-data-storage-server/" target="_blank" rel="noreferrer noopener">data</a> <a href="https://www.backblaze.com/blog/the-storage-pod-story-innovation-to-commodity/" target="_blank" rel="noreferrer noopener">storage</a> <a href="https://www.backblaze.com/blog/next-backblaze-storage-pod/" target="_blank" rel="noreferrer noopener">servers</a> we created and deployed which we call <a href="https://www.backblaze.com/blog/category/cloud-storage/storage-pod/" target="_blank" rel="noreferrer noopener">Backblaze Storage Pods</a>. We not only wrote about our Storage Pods, we open sourced the design, published a parts list, and even provided instructions on how to build one. Many people did. Of the six storage pod versions we produced, four of them are still in operation in our data centers today. Over the last few years, we began using storage servers from Dell and, more recently, Supermicro, as they have proven to be economically and operationally viable in our environment.&nbsp;</p>
<p>Since 2013, we have also written extensively about our <a href="https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2023/" target="_blank" rel="noreferrer noopener">Drive Stats</a>, sharing reports on the failure rates of the HDDs and SSDs in our legion of storage servers. We have examined the drive failure rates by manufacturer, size, age, and so on, but we have never analyzed the drive failure rates of the storage servers—until now. Let’s take a look at the Drive Stats for our fleet of storage servers and see what we can learn.</p>
<h2>Storage Pods, Storage Servers, and Backblaze Vaults</h2>
<p>Let’s start with a few definitions:</p>
<ul>
<li><strong>Storage Server: </strong>A storage server is our generic name for a server from any manufacturer which we use to store customer data. We use storage servers from Backblaze, Dell, and Supermicro.</li>
<li><strong>Storage Pod: </strong>A Storage Pod is the name we gave to the storage servers Backblaze designed and had built for our data centers. The first Backblaze Storage Pod version was announced in September 2009. Subsequent versions are 2.0, 3.0, 4.0, 4.5, 5.0, 6.0, and 6.1. All but 6.1 were announced publicly.&nbsp;</li>
<li><strong>Backblaze Vault: </strong>A Backblaze Vault is 20 storage servers grouped together for the purpose of data storage. Uploaded data arrives at a given storage server within a Backblaze Vault and is <a href="https://www.backblaze.com/blog/reed-solomon/" target="_blank" rel="noreferrer noopener">encoded</a> into 20 parts with a given part being either a data blob or parity. Each of the 20 parts (shards) is then stored on one of the 20 storage servers.&nbsp;</li>
</ul>
<p>As you review the charts and tables here are a few things to know about Backblaze Vaults.</p>
<ul>
<li>There are currently six cohorts of storage servers in operation today: Supermicro, Dell, Backblaze 3.0, Backblaze 5.0, Backblaze 6.0, and Backblaze 6.1.</li>
<li>A given Vault will always be made up from one of the six cohorts of storage servers noted above. For example, Vault 1016 is made up of 20 Backblaze 5.0 Storage Pods and Vault 1176 is made of the 20 Supermicro servers.&nbsp;</li>
<li>A given Vault is made up of storage servers that contain the same number of drives as follows:
<ul>
<li>Dell servers: 26 drives.</li>
<li>Backblaze 3.0 and Backblaze 5.0 servers: 45 drives.</li>
<li>Backblaze 6.0, Backblaze 6.1, and Supermicro servers: 60 drives.</li>
</ul>
</li>
<li>All of the hard drives in a Backblaze Vault will be logically the same size; so, 16TB drives for example.</li>
</ul>
<h3>Drive Stats by Backblaze Vault Cohort</h3>
<p>With the background out of the way, let’s get started. As of the end of Q3 2023, there were a total of 241 Backblaze Vaults divided into the six cohorts, as shown in the chart below. The chart includes the server cohort, the number of Vaults in the cohort, and the percentage that cohort is of the total number of Vaults.</p>
<div>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2024/01/1-Breakdown-of-Vaults-Pie.png" data-rel="lightbox-gallery-ynFylTCA" data-rl_title="1 – Breakdown of Vaults – Pie" data-rl_caption="1 – Breakdown of Vaults – Pie" title="1 – Breakdown of Vaults – Pie"><img decoding="async" width="600" height="500" src="https://www.backblaze.com/blog/wp-content/uploads/2024/01/1-Breakdown-of-Vaults-Pie.png" alt="A pie chart showing the types of Backblaze Vaults by percentage. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2024/01/1-Breakdown-of-Vaults-Pie.png 600w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/1-Breakdown-of-Vaults-Pie-300x250.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/1-Breakdown-of-Vaults-Pie-560x467.png 560w" sizes="(max-width: 600px) 100vw, 600px"></a></figure></div>

<p>Vaults consisting of Backblaze servers still comprise 68% of the vaults in use today (shaded from orange to red), although that number is dropping as older Vaults are being replaced with newer server models, typically the Supermicro systems.</p>
<p>The table below shows the Drive Stats for the different Vault cohorts identified above for Q3 2023.</p>
<div>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2024/01/2-Drive-Stats-by-vault.png" data-rel="lightbox-gallery-ynFylTCA" data-rl_title="2 – Drive Stats by vault" data-rl_caption="2 – Drive Stats by vault" title="2 – Drive Stats by vault"><img decoding="async" width="660" height="380" src="https://www.backblaze.com/blog/wp-content/uploads/2024/01/2-Drive-Stats-by-vault.png" alt="A chart showing the Drive Stats for Backblaze Vaults. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2024/01/2-Drive-Stats-by-vault.png 660w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/2-Drive-Stats-by-vault-300x173.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/2-Drive-Stats-by-vault-560x322.png 560w" sizes="(max-width: 660px) 100vw, 660px"></a></figure></div>

<p>The <strong>Avg Age (months)</strong> column is the average age of the drives, not the average age of the Vaults. The two may seem to be related, that’s not entirely the case. It is true the Backblaze 3.0 Vaults were deployed first followed in order by the 5.0 and 6.0 Vaults, but that’s where things get messy. There was some overlap between the Dell and Backblaze 6.1 deployments as the Dell systems were deployed in our central Europe data center, while the 6.1 Vaults continued to be deployed in the U.S. In addition, some migrations from the Backblaze 3.0 Vaults were initially done to 6.1 Vaults while we were also deploying new drives in the Supermicro Vaults.&nbsp;</p>
<p>The AFR for each of the server versions does not seem to follow any pattern or correlation to the average age of the drives. This was unexpected because, in general, <a href="https://www.backblaze.com/blog/drive-failure-over-time-the-bathtub-curve-is-leaking/" target="_blank" rel="noreferrer noopener">as drives pass about four years in age, they start to fail more often</a>. This should mean that Vaults with older drives, especially those with drives whose average age is over four years (48 months), should have a higher failure rate. But, as we can see, the Backblaze 5.0 Vaults defy that expectation.&nbsp;</p>
<p>To see if we can determine what’s going on, let’s expand on the previous table and dig into the different drive sizes that are in each Vault cohort, as shown in the table below.</p>
<div>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2024/01/3-Vaults-by-Server-and-Size.png" data-rel="lightbox-gallery-ynFylTCA" data-rl_title="3 – Vaults by Server and Size" data-rl_caption="3 – Vaults by Server and Size" title="3 – Vaults by Server and Size"><img loading="lazy" decoding="async" width="720" height="750" src="https://www.backblaze.com/blog/wp-content/uploads/2024/01/3-Vaults-by-Server-and-Size.png" alt="A table showing Drive Stats by server version and drive size. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2024/01/3-Vaults-by-Server-and-Size.png 720w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/3-Vaults-by-Server-and-Size-288x300.png 288w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/3-Vaults-by-Server-and-Size-560x583.png 560w" sizes="(max-width: 720px) 100vw, 720px"></a></figure></div>

<p><strong>Observations for Each Vault Cohort</strong></p>
<ul>
<li><strong>Backblaze 3.0: </strong>Obviously these Vaults have the oldest drives and, given their AFR is nearly twice the average for all of the drives (1.53%), it would make sense to migrate off of these servers. Of course the 6TB drives seem to be the exception, but at some point they will most likely “hit the wall” and start failing.</li>
<li><strong>Backblaze 5.0:</strong> There are two Backblaze 5.0 drive sizes (4TB and 8TB) and the AFR for each is well below the average AFR for all of the drives (1.53%). The average age of the two drive sizes is nearly seven years or more. When compared to the Backblaze 6.0 Vaults, it would seem that migrating the 5.0 Vaults could wait, but there is an operational consideration here. The Backblaze 5.0 Vaults each contain 45 drives, and from the perspective of data density per system, they should be migrated to 60 drive servers sooner rather than later to optimize data center rack space.</li>
<li><strong>Backblaze 6.0:</strong> These Vaults as a group don’t seem to make any of the five different drive sizes happy. Only the AFR of the 4TB drives (1.42%) is just barely below the average AFR for all of the drives. The rest of the drive groups are well above the average.</li>
<li><strong>Backblaze 6.1:</strong> The 6.1 servers are similar to the 6.0 servers, but with an upgraded CPU and faster NIC cards. Is that why their annualized failure rates are much lower than the 6.0 systems? Maybe, but the drives in the 6.1 systems are also much younger, about half the age of those in the 6.0 systems, so we don’t have the full picture yet.</li>
<li><strong>Dell:</strong> The 14TB drives in the Dell Vaults seem to be a problem at a 5.46% AFR. Much of that is driven by two particular Dell vaults which have a high AFR, over 8% for Q3. This appears to be related to their location in the data center. All 40 of the Dell servers which make up these two Vaults were relocated to the top of 52U racks, and it appears that initially they did not like their new location. Recent data indicates they are doing much better, and we’ll publish that data soon. We’ll need to see what happens over the next few quarters. That said, if you remove these two Vaults from the Dell tally, the AFR is a respectable 0.99% for the remaining Vaults.</li>
<li><strong>Supermicro: </strong>This server cohort is mostly 16TB drives which are doing very well with an AFR of 0.62%. The one 14TB Vault is worth our attention with an AFR of 1.95%, and the 22TB Vault is too new to do any analysis.</li>
</ul>
<h3>Drive Stats by Drive Size and Vault Cohort</h3>
<p>Another way to look at the data is to take the previous table and re-sort it by drive size. Before we do that let’s establish the AFR for the different drive sizes aggregated over all Vaults.</p>
<div>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2024/01/4-AFR-by-Vault-and-Drive-Size-Bars.png" data-rel="lightbox-gallery-ynFylTCA" data-rl_title="4 – AFR by Vault and Drive Size – Bars" data-rl_caption="4 – AFR by Vault and Drive Size – Bars" title="4 – AFR by Vault and Drive Size – Bars"><img loading="lazy" decoding="async" width="570" height="430" src="https://www.backblaze.com/blog/wp-content/uploads/2024/01/4-AFR-by-Vault-and-Drive-Size-Bars.png" alt="A bar chart showing annualized failure rates for Backblaze Vaults by drive size. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2024/01/4-AFR-by-Vault-and-Drive-Size-Bars.png 570w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/4-AFR-by-Vault-and-Drive-Size-Bars-300x226.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/4-AFR-by-Vault-and-Drive-Size-Bars-350x263.png 350w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/4-AFR-by-Vault-and-Drive-Size-Bars-260x195.png 260w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/4-AFR-by-Vault-and-Drive-Size-Bars-560x422.png 560w" sizes="(max-width: 570px) 100vw, 570px"></a></figure></div>

<p>As we can see in Q3 the 6TB and 22TB Vaults had zero failures (AFR = 0%). Also, the 10TB Vault is indeed only one Vault, so there are no other 10TB Vaults to compare it to. Given this, for readability, we will remove the 6TB, 10TB, and 22TB Vaults from the next table which compares how each drive size has fared in each of the six different Vault cohorts.</p>
<div>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2024/01/5-AFR-by-Drive-Size-and-Vaults.png" data-rel="lightbox-gallery-ynFylTCA" data-rl_title="5 – AFR by Drive Size and Vaults" data-rl_caption="5 – AFR by Drive Size and Vaults" title="5 – AFR by Drive Size and Vaults"><img loading="lazy" decoding="async" width="720" height="650" src="https://www.backblaze.com/blog/wp-content/uploads/2024/01/5-AFR-by-Drive-Size-and-Vaults.png" alt="A table showing the annualized failure rates of servers by drive size and server version, not displaying the 6TB, 10TB, and 22TB Vaults. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2024/01/5-AFR-by-Drive-Size-and-Vaults.png 720w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/5-AFR-by-Drive-Size-and-Vaults-300x271.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2024/01/5-AFR-by-Drive-Size-and-Vaults-560x506.png 560w" sizes="(max-width: 720px) 100vw, 720px"></a></figure></div>

<p>Currently we are migrating the 4TB drive Vaults to larger Vaults, replacing them with drives of 16TB and above. The migrations are done using an in-house system which we’ll expand upon in a future post. The specific order of migrations is based on failure rates and durability of the existing 4TB Vaults with an eye towards removing the Backblaze 3.0 systems first as they are nearly 10 years old in some cases, and many of the non-drive replacement parts are no longer available. Whether we give away, <a href="https://www.backblaze.com/blog/megabot-vs-backblaze/" target="_blank" rel="noreferrer noopener">destroy</a>, or recycle the retired Backblaze 3.0 Storage Pods (sans drives) is still being debated.</p>
<p>For the 8TB drive Vaults, the Backblaze 5.0 Vaults are up first for migration when the time comes. Yes, their AFR is lower then the Backblaze 6.0 Vaults, but remember: the 5.0 Vaults are 45 drive units which are not as efficient storage density-wise versus the 60 drive systems.&nbsp;</p>
<p>Speaking of systems with less than 60 drives, the Dell servers are 26 drives. Those 26 drives are in a 2U chassis versus a 4U chassis for all of the other servers. The Dell servers are not quite as dense as the 60 drive units, but their 2U form factor gives us some flexibility in filling racks, especially when you add utility servers (1U or 2U) and networking gear to the mix. That’s one of the reasons the two Dell Vaults we noted earlier were moved to the top of the 52U racks. FYI, those two Vaults hold 14TB drives and are two of the four 14TB Dell Vaults making up the 5.46% AFR. The AFR for the Dell Vaults with 12TB and 16TB drives is 0.76% and 0.92% respectively. As noted earlier, we expect the AFR for 14TB Dell Vaults to drop over the coming months.</p>
<h2>What Have We Learned?</h2>
<p>Our goal today was to see what we can learn about the drive failure rates of the storage servers we use in our data centers. All of our storage servers are grouped in operational systems we call Backblaze Vaults. There are six different cohorts of storage servers with each vault being composed of the same type of storage server, hence there are six types of vaults.&nbsp;</p>
<p>As we dug into data, we found that the different cohorts of Vaults had different annualized failure rates. What we didn’t find was a correlation between the age of the drives used in the servers and the annualized failure rates of the different Vault cohorts. For example, the Backblaze 5.0 Vaults have a much lower AFR of 0.99%&nbsp; versus the Backblaze 6.0 Vault AFR at 2.14%—even though the drives in the 5.0 Vaults are nearly twice as old on average than the drives in the 6.0 Vaults.</p>
<p>This suggests that while our initial foray into the annualized failure rates of the different Vault cohorts is a good first step, there is more to do here.</p>
<h2>Where Do We Go From Here?</h2>
<p>In general, all of the Vaults in a given cohort were manufactured to the same specifications, used the same parts, and were assembled using the same processes. One obvious difference is that different drive models are used in each Vault cohort. For example, the 16TB vaults are composed of seven different drive models. Do some drive models work better in one Vault cohort versus another? Over the next couple of quarters we’ll dig into the data and let you know what we find. Hopefully it will add to our understanding of the annualized failures rates of the different Vault cohorts. Stay tuned.</p>
</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Ambiphone, no-nonsense ambient music and white noise (307 pts)]]></title>
            <link>https://ambiph.one?hn</link>
            <guid>38856999</guid>
            <pubDate>Wed, 03 Jan 2024 17:41:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ambiph.one?hn">https://ambiph.one?hn</a>, See on <a href="https://news.ycombinator.com/item?id=38856999">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[C and C++ Hot-Reload/Live Coding (184 pts)]]></title>
            <link>https://liveplusplus.tech/</link>
            <guid>38856696</guid>
            <pubDate>Wed, 03 Jan 2024 17:20:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://liveplusplus.tech/">https://liveplusplus.tech/</a>, See on <a href="https://news.ycombinator.com/item?id=38856696">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content" role="main">
      <div>
          

          <div>
              <h2>
                <span>Live++<sup>®</sup> 2</span> - spend more time coding <em>and less time loading</em>.<br>
                  <span>Hot</span>-Reload. <span>Hot</span>-Restart. <span>Hot</span>-Fix. <span>Hot</span>-Deoptimize.
              </h2>

              <iframe src="https://www.youtube.com/embed/ewbkdxskl7I?playlist=ewbkdxskl7I&amp;loop=1" title="Live++ overview" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

              <h2>
                <p>
                  The next generation of the industry-standard tool for Windows &amp; Xbox Series X|S.
                </p>
                <p>
                  <img alt="C++ logo" src="https://liveplusplus.tech/images/assets/products/cpp_logo.svg" width="64" height="64">
                  <img alt="Windows logo" src="https://liveplusplus.tech/images/assets/products/windows_logo.svg" width="64" height="64">
                  <img alt="Xbox Series X|S logo" src="https://liveplusplus.tech/images/assets/products/xbox_series_logo.svg" width="64" height="64">
                </p>
              </h2>
              <p>
                <a href="http://eepurl.com/ikKAqf" target="_blank">
                  Want to stay up to date with the latest releases and upcoming closed betas?<br>Subscribe to our mailing list
                </a>
              </p>
            </div>
          <hr>
          <div>
            <ul>
              <li>
                <h3>
                  Native speed
                </h3>
                <p>
                  <em>Live++</em> enables hot-reload for C &amp; C++ applications, combining the power of rapid iteration with the speed of a compiled language.
                </p>
              </li>
              <li>
                <img src="https://liveplusplus.tech/images/assets/hot_reload.png">
              </li>
            </ul>
            <ul>
              <li>
                <h3>
                  Binary code patching
                </h3>
                <p>
                  <em>Live++</em> compiles your changes in the background, directly patching the machine code of the running executable. It works with any kind of C &amp; C++ code and requires neither plug-ins nor a debugger or IDE.
                </p>
              </li>
              <li>
                <img src="https://liveplusplus.tech/images/assets/binary_patching.png">
              </li>
            </ul>

            <ul>
              <li>
                <h3>
                  Stand-alone
                </h3>
                <p>
                  Start your application from anywhere, modify its source code using your favourite editor, and let <em>Live++</em> do the rest.
                </p>
              </li>
              <li>
                <img src="https://liveplusplus.tech/images/assets/editor_vs.png">
              </li>
              <li>
                <img src="https://liveplusplus.tech/images/assets/editor_10x.png">
              </li>
            </ul>
          </div>

          <div>
            <ul>
              <li>
                <h3>
                  Versatile
                </h3>
                <p>
                  Whether you're working on new features, trying out gameplay mechanics, optimizing code, or fixing bugs, <em>Live++</em> will massively cut down your iteration times.
                </p>
              </li>
              <li>
                <h3>
                  Reliable
                </h3>
                <p>
                  <em>Live++</em> reliably works with both debug and optimized builds, supports multi-process and networked editing, and does not care about your build setup.
                </p>
              </li>
              <li>
                <h3>
                  Technical background
                </h3>
                <p>
                  Interested in how <em>Live++</em> works?
                </p>
                <p>
                  Check out <u><a href="https://liveplusplus.tech/downloads/THQ_Nordic_Dev_Summit_2023_Live++_Behind_the_Scenes.pptx">this talk</a></u> from the <span>THQ Nordic Dev Summit 2023</span>.
                </p>
              </li>
            </ul>
          </div>

        </div>

      <div>
              <h2>
                <p>
                  Join thousands of other developers and grab your free 30-day trial today.<br>
                  Includes example projects and solutions for Visual Studio 2017, 2019, and 2022.<br>
                  Fully featured, no registration required, no strings attached.
                </p>
              </h2>
              <p>
                <a href="https://liveplusplus.tech/trial_download.html">
                  <span>DOWNLOAD 30-DAY TRIAL</span>
                </a>
              </p>
            </div>

      <!-- Testimonials -->
      <div>
          
          <p>
            In addition to supporting thousands of developers using Unreal Engine all around the world, <em>Live++</em> is used by more than 70 companies on their proprietary engines and frameworks.
          </p>
          <p>
            Still not convinced? Check out what some of our customers have to say about <em>Live++</em>:
          </p>

          <div>
            <ul>
              <li>
                <div>
                  <p>
                    Live++ is "Edit and Continue that actually works". Programmers at Asobo Studio have been using it throughout development of Asobo Studio's C++-based products (millions of lines of code). It has become a standard part of our workflow, increasing developer productivity (and happiness 😊).
                  </p>
                  
                </div>
              </li>
              <li>
                <div>
                  <p>
                    I came to rely on Live++ in an almost unhealthy way. There's no C++ code I am working on where it can't be profitably used. It's integrated in minutes and gets me reliably around seconds or even minutes long program re-starts. Live++ also became my investigation sandbox in which I can explore and refactor my running code. I hunt down bugs by querying and testing my code interactively and old-school debugging methods get a modern spin - debug prints and runtime variable modification on steroids.
                  </p>
                  
                </div>
              </li>
              <li>
                <div>
                  <p>
                    Seeing Live++ in action seems like magic, and it very well might be. Aside from our Unreal projects we used it on a game with a custom engine and a very long iteration cycle. Over the five years of development, Live++ likely saved us months by avoiding the dreaded "change line - compile - load - test - repeat" cycle and letting us experiment and implement the game logic while the game was running. Between integration taking just a few minutes, and the free trial being available, there really isn't an excuse to not give it a try!
                  </p>
                  
                </div>
              </li>
            </ul>
          </div>

          

          <div>
            <ul>
              <li>
                <div>
                  <p>
                    Live++ is the single biggest improvement in productivity for C++ codebases. Fast feedback loops are a superpower.
                  </p>
                  
                </div>
              </li>
              <li>
                <div>
                  <p>
                    We integrated Live++ into Godot 4.0 and it was surprising how little work it required compared to how much it improved the workflow.<br>The time invested already paid off in the same week.
                  </p>
                  
                </div>
              </li>
              <li>
                <div>
                  <p>
                    Live++ has enabled engineers across Frostbite, and the game teams using it, to iterate and debug so much faster. No longer needing to restart the game/editor to apply changes is fantastic, but being able to deoptimize the current file has been a godsend for productivity. Can't recommend it enough!
                  </p>
                  
                </div>
              </li>
            </ul>
          </div>

          

          <div>
            <ul>
              <li>
                <div>
                  <p>
                    Most of the companies our embedded engineers provide services for thankfully use Live++ to supercharge their workflows.<br>
                    But even if we can't convince a client of its merits: Integration takes less than fifteen minutes and is easily shared among engineers locally without ever reaching version control, making for an amazing covert productivity boost.<br>
                    It's hard not to feel like a secret agent with gadgetry this nifty!
                  </p>
                  
                </div>
              </li>
              <li>
                <div>
                  <p>
                    Live++ has been a game changer for me. The ability to make changes to code while the application is running supercharges iteration times and makes tweaking and tuning behaviours so much easier. It's also come in handy a number of times when debugging an issue. Being able to add extra logging/instrumentation without having to restart the application is incredibly useful.
                  </p>
                  
                </div>
              </li>
              <li>
                <div>
                  <p>
                    Live++ is crazy simple to integrate and has transformed my workflow. Returning to a codebase without live editing feels like stepping into the distant past. Iterating faster lets me stay focused, save time, save money, and honestly just be happier every day while working.
                  </p>
                  
                </div>
              </li>
            </ul>
          </div>

          

          <div>
            <ul>
              <li>
                <div>
                  <p>
                    Remember reloading the same level over and over just to test small code changes? Me neither.
                  </p>
                  
                </div>
              </li>
              <li>
                <div>
                  <p>
                    I love that I can disable optimizations on individual files <em>while I am debugging</em> without needing to restart and compile the program.
                  </p>
                  
                </div>
              </li>
              <li>
                <div>
                  <p>
                    Live++ massively reduced my iteration times. No more quit-compile-restart cycle. And the cherry on top: Writing IMGUI code with Live++ is just amazingly interactive. 10/10, would hot-reload again ;)
                  </p>
                  
                </div>
              </li>
            </ul>
          </div>

          

          <div>
            <ul>
              <li>
                <div>
                  <p>
                    Live++ is a core pillar of my tool suite for optimizing critical code in both hobby and professional projects.<br>
                    Hot-reloading code changes in mere milliseconds decreased iteration times tremendously. It lets me quickly try out several optimization approaches and compare their impact in a single performance capture session.<br>
                    Any project that doesn't support this now feels like it's missing a fundamental part of the modern development process.
                  </p>
                  
                </div>
              </li>
            </ul>
          </div>
        </div>

      <!-- Customers -->
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fewer people are buying electric cars (249 pts)]]></title>
            <link>https://www.businessinsider.com/electric-car-ev-sales-prices-problem-transportation-2024-1</link>
            <guid>38856586</guid>
            <pubDate>Wed, 03 Jan 2024 17:12:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/electric-car-ev-sales-prices-problem-transportation-2024-1">https://www.businessinsider.com/electric-car-ev-sales-prices-problem-transportation-2024-1</a>, See on <a href="https://news.ycombinator.com/item?id=38856586">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-type="content-lock" data-load-strategy="exclude">
                                  <p>Electric vehicles were supposed to be inevitable. Two years ago President Joe Biden climbed behind the wheel of a beefy white electric Hummer to tout his plan to make <a target="_blank" href="https://www.businessinsider.com/biden-executive-order-electric-cars-evs-2030-target-half-2021-8" data-analytics-product-module="body_link" rel=""><u>half of all new cars sold electric by 2030</u></a>. The following year Congress passed the Inflation Reduction Act, which created a bevy of incentives for drivers to buy electric and for automakers to invest in EVs. That set off a flurry of new projects: EV plants, battery-manufacturing facilities, and mining operations began popping up. By the end of 2022 the situation looked promising: More and more Americans were going electric, and soon everyone would be driving an EV, reducing emissions in the process.</p><p>The transition to an all-EV future seemed like a slam dunk. It would not only give the government a highly visible way to show it's fighting the climate crisis but boost the economy through new jobs and investment. But the electric-vehicle takeover has hit some serious roadblocks.</p><p>Sure, <a target="_blank" href="https://www.businessinsider.com/automakers-over-estimated-electric-vehicle-demand-early-adopters-2023-10" data-analytics-product-module="body_link" rel=""><u>sales of EVs keep going up</u></a> — a record 300,000 cars sold in the US in the third quarter of 2023 were electric — but the pace of adoption has markedly slowed, and analysts have suggested the country is no longer <a target="_blank" href="https://www.electrifying.com/blog/article/electric-car-sales-growth-strong-but-not-on-target-for-2024-goal" data-analytics-product-module="body_link" rel=" nofollow"><u>on track</u></a> to hit the government's sales targets. The trickle-down effects of this decreased demand are everywhere. EVs <a target="_blank" href="https://www.businessinsider.com/dealers-turning-away-evs-velectric-cars-demand-cools-inventory-2023-8" data-analytics-product-module="body_link" rel=""><u>accumulated at dealerships</u></a> this fall, even as automakers cut prices to try to entice customers. Automakers have backtracked on their promised investments: <a target="_blank" href="https://www.businessinsider.com/ford-cuts-production-of-f150-lightning-cybertruck-rival-ev-demand-2023-12" data-analytics-product-module="body_link" rel="">Ford</a> delayed $12 billion of its planned $15 billion investment in EV manufacturing capacity, while General Motors delayed production of key EV models and scrapped a $5 billion partnership with Honda to make cheaper EVs. Even <a target="_blank" href="https://www.businessinsider.com/tesla-master-plan-part-three-investor-day-elon-musk-2023-3" data-analytics-product-module="body_link" rel="">Tesla</a> — once the superstar of EVs — announced it would delay a planned factory in Mexico. Auto execs who were once trumpeting the potential of electric cars are even publicly acknowledging that <a target="_blank" href="https://www.businessinsider.com/auto-executives-coming-clean-evs-arent-working-2023-10" data-analytics-product-module="body_link" rel=""><u>EVs aren't working</u></a>.</p><!-- Excluded mobile ad on desktop --><p>Industry analysts have pointed to several reasons for the slowdown, including insufficient charging infrastructure and a lack of affordable EV options. But they're a symptom of the larger problem: America's EV plan was flawed from the start. Instead of seeing EVs as one piece of a plan for more sustainable transportation, America has focused on using EVs as a one-to-one replacement for gas guzzlers. But this one-size-fits-all solution fails to address our broader transportation problems, meaning emissions targets are likely to be missed and other transportation problems will continue to go unaddressed.</p><p>"The entire myth at the heart of this whole transition is that the battery car seamlessly fits right into the gas car's position," Edward Niedermeyer, the author of "Ludicrous: The Unvarnished Story of Tesla Motors," told me. "It doesn't, and that's the problem."</p><h2><strong>The EV myth</strong></h2><p>The mission to replace gas cars with EVs has led to a series of major miscalculations, one of which has to do with the sheer size of the new electric vehicles being put on the road.</p><p>Over the past few decades the American auto industry has <a target="_blank" href="https://www.businessinsider.com/road-rage-speeding-driving-traffic-large-expensive-cars-suv-2022-5" data-analytics-product-module="body_link" rel=""><u>become obsessed with huge vehicles</u></a>. The reasons for the size inflation range from profit margins to distorted <a target="_blank" href="https://www.thedrive.com/news/small-cars-are-getting-huge-are-fuel-economy-regulations-to-blame" data-analytics-product-module="body_link" rel=" nofollow"><u>government fuel standards</u></a>, but the proliferation of bigger vehicles created a doom loop of consumer preference: Drivers saw the vehicles around them getting bigger, so they wanted bigger cars to make themselves feel safer. Automakers argued that this was proof that people wanted only big cars, so <a target="_blank" href="https://www.businessinsider.com/us-small-car-market-is-a-disaster-2016-8" data-analytics-product-module="body_link" rel=""><u>they cut small models</u></a> and made existing vehicles bigger, which made people with smaller cars feel less safe — you get the picture. Meanwhile, road deaths and injuries soared, while the larger, less efficient vehicles wiped out <a target="_blank" href="https://www.npr.org/2023/07/28/1190799503/new-fuel-economy-standards-cars-trucks" data-analytics-product-module="body_link" rel=" nofollow"><u>environmental benefits</u></a> from higher emissions standards.</p><es-blockquote data-styles="pullquote-breakout" data-quote="The entire myth at the heart of this whole transition is that the battery car seamlessly fits right into the gas car's position." data-quote-updated="true" data-source="Edward Niedermeyer" data-source-updated="true">
                              <blockquote>
                                <q>The entire myth at the heart of this whole transition is that the battery car seamlessly fits right into the gas car's position.</q>
                                <cite>Edward Niedermeyer</cite>
                              </blockquote>
                            </es-blockquote><p>When automakers pivoted to EVs, they focused on the kinds of cars that were already popular — which meant a flood of big electrified SUVs and trucks. But massive-bodied EVs don't make much sense. Larger EVs require bigger batteries, which require more raw materials to manufacture, which requires producers to beef up their <a target="_blank" href="https://www.businessinsider.com/electric-cars-vehicles-solution-climate-change-crisis-dirty-minerals-mining-2022-7" data-analytics-product-module="body_link" rel=""><u>environmentally destructive mining operations</u></a>. While bigger batteries allow drivers to travel farther between charges, they also make the cars heavier, <a target="_blank" href="https://www.iihs.org/news/detail/as-heavy-evs-proliferate-their-weight-may-be-a-drag-on-safety" data-analytics-product-module="body_link" rel=" nofollow"><u>more dangerous</u></a>, more expensive, and worse for the planet.</p><p>The "range anxiety" that has resulted in massive batteries is another reason EVs don't work as a replacement for gas cars. Niedermeyer said that while an electric car can meet most people's driving needs, it struggles with edge cases like road trips because of the need to recharge. Since Americans have been promised a one-to-one substitute for their gas cars, this seems like a failure; an EV should be able to do everything a gas car can. This idea persists even though in 2023 the average US driver traveled only about <a target="_blank" href="https://www.nytimes.com/2022/08/27/opinion/electric-car-battery-range.html" data-analytics-product-module="body_link" rel=" nofollow"><u>40 miles a day</u></a>, and in 2022 about 93% of US trips were less than 30 miles. Still, in a survey conducted by Ipsos last fall, 73% of respondents indicated they had concerns about EV range.</p><p>The focus on increasing EVs' range is contributing to their relatively high prices. Unlike with gas cars, the more you pay for an EV, the more range you can expect to receive. And since Americans have been conditioned to want a lot of range, cars with big batteries and longer ranges have dominated the market, resulting in stubbornly high prices. In September, Cox Automotive pegged the average EV price at $50,683, down 22% from the same time last year. But an analysis from CarGurus found that EV prices were still 28% higher than gas-vehicle prices on average. With prices for everything else — rent, groceries, and other goods — increasing, the average person has less cash to splurge on an expensive electric vehicle.</p><p>All of this means there's a natural limit to the number of American households willing and able to make the shift to electric. They've largely been high-income households in places like California, where charging infrastructure is more plentiful. The polling firm Strategic Vision found that EV buyers have <a target="_blank" href="https://www.theverge.com/23934889/electric-vehicle-ev-transition-sales-delays-politics" data-analytics-product-module="body_link" rel=" nofollow"><u>a median household income of $186,000</u></a>. Cox estimated that 8% to 9% of new-vehicle sales in the United States in 2023 would be electric, but getting above that threshold is proving to be more difficult than expected.</p><!-- Excluded mobile ad on desktop --><h2><strong>The Norway model</strong></h2><p>If there's any direct inspiration for the United States' EV policy, it's Norway. As the story goes, Norway introduced some compelling subsidies for EVs, sales took off, and now the vehicles virtually dominate the roads. But the reality isn't so simple. And Norway's challenges bode poorly for America's EV push.</p><es-blockquote data-styles="pullquote-right" data-quote="Norway shows that if US policymakers stick with the current model of EV transition, it's going to be a difficult road." data-quote-updated="true" data-source="Add source (optional)" data-source-updated="false">
                              <blockquote>
                                <q>Norway shows that if US policymakers stick with the current model of EV transition, it's going to be a difficult road.</q>
                                
                              </blockquote>
                            </es-blockquote><p>Norway introduced EV incentives <a target="_blank" href="https://www.vox.com/future-perfect/23939076/norway-electric-vehicle-cars-evs-tesla-oslo" data-analytics-product-module="body_link" rel=" nofollow"><u>in the 1990s</u></a>, then added more when EV technology really took off in the 2010s. EV drivers could get perks like free parking, permission to drive in bus lanes, and, most importantly, exemptions from taxes and fees that could ultimately save them a lot of money. In September, <a target="_blank" href="https://cleantechnica.com/2023/10/04/norways-evs-at-a-record-93-share/" data-analytics-product-module="body_link" rel=" nofollow"><u>87% of new-vehicle sales</u></a> were fully electric vehicles. The problem, Ketan Joshi, a climate-analysis expert in Oslo, told me, is that that stat "doesn't really give you a good picture of the rate of change." Though the new-vehicle sales figure is high, data <a target="_blank" href="https://www.ssb.no/en/transport-og-reiseliv/landtransport/statistikk/bilparken" data-analytics-product-module="body_link" rel=" nofollow"><u>from Statistics Norway</u></a> indicates the total share of EVs on Norwegian roads in 2022 was only about 20% — there's still a long way to go until everyone's driving electric.</p><p>Even with this shift, Norway isn't on track to meet its <a target="_blank" href="https://climateactiontracker.org/countries/norway/" data-analytics-product-module="body_link" rel=" nofollow"><u>2030 emissions-reduction targets</u></a>. While emissions from passenger vehicles have fallen slightly, Joshi said, those reductions are being canceled out by an increase in emissions from trucks. People in Norway own more cars than they have in the past, in part because EV incentives encourage people to buy <em>more</em> cars, and the government has <a target="_blank" href="https://www.vox.com/future-perfect/23939076/norway-electric-vehicle-cars-evs-tesla-oslo" data-analytics-product-module="body_link" rel=" nofollow"><u>no plans</u></a> to reduce how much people are driving. The researcher Benjamin Sovacool and his colleagues have <a target="_blank" href="https://www.sciencedirect.com/science/article/abs/pii/S0921800918307602" data-analytics-product-module="body_link" rel=" nofollow"><u>pointed out</u></a> that, just like in the US, EV buyers in Norway "tend to be in higher income brackets, often using their EV as a second car."</p><p>The Norwegian approach has also had a ton of unintended consequences. Joshi told me that the decline in gas tax revenue due to EV adoption had triggered a contentious political debate about increasing road tolls to make up the difference. (A political party was even formed on the platform of <a target="_blank" href="https://www.theguardian.com/world/2019/sep/08/road-rage-norway-goes-to-polls-split-over-environmental-policies" data-analytics-product-module="body_link" rel=" nofollow"><u>stopping the tolls</u></a>.) Plus, <a target="_blank" href="https://robbieandrew.github.io/EV/img/weight_by_drivetrain_smoothed.svg" data-analytics-product-module="body_link" rel=" nofollow"><u>heavier electric vehicles</u></a> are harder on roads, produce more air pollution, and pose a greater safety risk for pedestrians.</p><p>Norway has made great headway in getting buyers to go for EVs, but it's not a silver bullet, especially on a short timeline. "It shows you the extreme slowness of transition that is basically guided by the rate at which people buy a new car," Joshi said of Norway's approach. Reducing transportation emissions by incentivizing people to replace their car with an EV is incredibly slow. And for countries like the US that got started late, it's a warning sign.</p><p>Norway shows that if US policymakers stick with the current model of EV transition, it's going to be a difficult road. Even if adoption keeps ticking up, it will take a long time to get existing internal-combustion-engine vehicles off the road and see a notable decline in transport emissions. Plus, there will be other issues to deal with like increased road maintenance and pedestrian safety.</p><p>"Narrow success in one area is not something you necessarily want to emulate," Joshi said.</p><h2><strong>Time for a rethink</strong></h2><p>The shift from gas-powered cars to electric vehicles is an opportunity to rethink how Americans get from place to place. But so far the US government, carmakers, and consumers have been pursuing a small-minded swap that lacks the necessary ambition.</p><!-- Excluded mobile ad on desktop --><p>Getting Americans to <a target="_blank" href="https://www.businessinsider.com/electric-vehicles-wont-save-us-get-rid-of-cars-2021-11" data-analytics-product-module="body_link" rel=""><u>ditch driving altogether</u></a> would be the most effective way to reduce emissions, but it would require a massive rethink of our transport system. Something like that doesn't happen overnight, and given the decadeslong lack of investment in anything other than car infrastructure, there are plenty of other opportunities for a better future. If the government and automakers are serious about making transportation more sustainable, they should be incentivizing smaller vehicles, hybrid cars, and public transportation like trains and buses.</p><p>EVs can be an important part of the fight against the climate crisis, but America's EV plan needs to lean into what these cars do well: short daily trips that can be taken in <a target="_blank" href="https://www.businessinsider.com/cheap-chinese-electric-cars-will-upend-the-us-vehicle-market-2023-5" data-analytics-product-module="body_link" rel=""><u>small, affordable cars</u></a>. People who frequently take long trips can take advantage of hybrid cars. And better public transit and faster intercity trains could make a huge difference for people and the planet.</p><p>While it may be a sexy and industry-friendly approach to the climate crisis, an EV-first plan isn't the most effective way to tackle the enormous challenge we face.</p><hr><p><a target="_blank" rel="noreferrer nofollow noopener" href="https://www.businessinsider.com/author/paris-marx" data-analytics-product-module="body_link"><em>Paris Marx</em></a><em>&nbsp;is a tech writer and host of the Tech Won't Save Us podcast. He also writes the Disconnect newsletter and is the author of&nbsp;</em><a target="_blank" rel="noreferrer nofollow noopener" href="https://www.versobooks.com/books/3995-road-to-nowhere" data-analytics-product-module="body_link"><em>Road to Nowhere: What Silicon Valley Gets Wrong about the Future of Transportation</em></a><em>.</em></p>
                      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Container2wasm: Convert Containers to WASM Blobs (227 pts)]]></title>
            <link>https://github.com/ktock/container2wasm</link>
            <guid>38856559</guid>
            <pubDate>Wed, 03 Jan 2024 17:10:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ktock/container2wasm">https://github.com/ktock/container2wasm</a>, See on <a href="https://news.ycombinator.com/item?id=38856559">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a href="https://github.com/ktock/container2wasm/releases">[:arrow_down: <strong>Download</strong>]</a>
<a href="#command-reference">[:book: <strong>Command reference</strong>]</a>
<a href="#additional-resources">[:books: <strong>Additional Documents</strong>]</a>
<a href="https://ktock.github.io/container2wasm-demo/" rel="nofollow">[:arrow_forward: <strong>Demo</strong>]</a></p>
<h2 tabindex="-1" dir="auto">container2wasm: Container to WASM converter</h2>
<p dir="auto">container2wasm is a container-to-wasm image converter that enables to run the container on WASM.</p>
<ul dir="auto">
<li>Converts a container to WASM with emulation by Bochs (for x86_64 containers) and TinyEMU (for riscv64 containers).</li>
<li>Runs on WASI runtimes (e.g. wasmtime, wamr, wasmer, wasmedge, wazero)</li>
<li>Runs on browser</li>
<li>x86_64 or riscv64 containers are recommended. Other platforms (e.g. arm64) also work (but slow).</li>
</ul>
<p dir="auto">This is an experimental software.</p>
<p dir="auto">Demo page of containers on browser (debian,python,node,vim): <a href="https://ktock.github.io/container2wasm-demo/" rel="nofollow">https://ktock.github.io/container2wasm-demo/</a></p>
<h2 tabindex="-1" dir="auto">Examples</h2>
<h3 tabindex="-1" dir="auto">Container Image to WASM (WASI)</h3>
<div dir="auto" data-snippet-clipboard-copy-content="$ c2w ubuntu:22.04 out.wasm"><pre>$ <span>c2w ubuntu:22.04 out.wasm</span></pre></div>
<p dir="auto">The above command converts <code>ubuntu:22.04</code> container image to WASI image (<code>out.wasm</code>).</p>
<blockquote>
<p dir="auto">NOTE1: For selecting the container image's architecture other than <code>amd64</code>, use <code>--target-arch</code> flag of c2w (e.g. <code>c2w --target-arch=riscv64 riscv64/ubuntu:22.04 out.wasm</code>).</p>
</blockquote>
<blockquote>
<p dir="auto">NOTE2: x86_64 or riscv64 container is recommended. Other platform's containers should work but slow because of additional emulation.</p>
</blockquote>
<p dir="auto">The generated image runs on WASI runtimes:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ wasmtime out.wasm uname -a
Linux localhost 6.1.0 #1 PREEMPT_DYNAMIC Mon Jun  5 11:57:09 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
$ wasmtime out.wasm ls /
bin   dev  home  lib32	libx32	mnt  proc  run	 srv  tmp  var
boot  etc  lib	 lib64	media	opt  root  sbin  sys  usr"><pre>$ <span>wasmtime out.wasm uname -a</span>
<span>Linux localhost 6.1.0 #1 PREEMPT_DYNAMIC Mon Jun  5 11:57:09 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux</span>
$ <span>wasmtime out.wasm ls /</span>
<span>bin   dev  home  lib32	libx32	mnt  proc  run	 srv  tmp  var</span>
<span>boot  etc  lib	 lib64	media	opt  root  sbin  sys  usr</span></pre></div>
<p dir="auto">Directory mapped from the host is accessible on the container.</p>
<div data-snippet-clipboard-copy-content="$ mkdir -p /tmp/share/ &amp;&amp; echo hi > /tmp/share/from-host
$ wasmtime --mapdir /mnt/share::/tmp/share out.wasm cat /mnt/share/from-host
hi"><pre><code>$ mkdir -p /tmp/share/ &amp;&amp; echo hi &gt; /tmp/share/from-host
$ wasmtime --mapdir /mnt/share::/tmp/share out.wasm cat /mnt/share/from-host
hi
</code></pre></div>
<blockquote>
<p dir="auto">Please refer to <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking/wasi"><code>./examples/networking/wasi/</code></a> for enabling networking</p>
</blockquote>
<h3 tabindex="-1" dir="auto">Container on Browser</h3>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ktock/container2wasm/blob/main/docs/images/ubuntu-wasi-on-browser.png"><img src="https://github.com/ktock/container2wasm/raw/main/docs/images/ubuntu-wasi-on-browser.png" alt="Container on browser"></a></p>
<p dir="auto">You can run the container on browser as well.
There are two methods for running the container on browser.</p>
<blockquote>
<p dir="auto">Please also refer to <a href="https://github.com/ktock/container2wasm/blob/main/examples/wasi-browser"><code>./examples/wasi-browser</code></a> (WASI-on-browser example) and <a href="https://github.com/ktock/container2wasm/blob/main/examples/emscripten"><code>./examples/emscripten</code></a> (emscripten example).</p>
</blockquote>
<blockquote>
<p dir="auto">Please refer to <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking"><code>./examples/networking/</code></a> for details about enabling networking.</p>
</blockquote>
<h4 tabindex="-1" dir="auto">WASI on browser</h4>
<p dir="auto">This example converts the container to WASI and runs it on browser.</p>
<p dir="auto">The following command generates a WASI image.</p>
<div data-snippet-clipboard-copy-content="$ c2w ubuntu:22.04 /tmp/out-js2/htdocs/out.wasm"><pre><code>$ c2w ubuntu:22.04 /tmp/out-js2/htdocs/out.wasm
</code></pre></div>
<p dir="auto">The following is an example of running the image on browser relying on <a href="https://github.com/mame/xterm-pty">xterm-pty</a> and <a href="https://github.com/bjorn3/browser_wasi_shim">browser_wasi_shim</a>.
This example serves the image on <code>localhost:8080</code> using apache http server.</p>
<div data-snippet-clipboard-copy-content="$ cp -R ./examples/wasi-browser/* /tmp/out-js2/ &amp;&amp; chmod 755 /tmp/out-js2/htdocs
$ docker run --rm -p 8080:80 \
         -v &quot;/tmp/out-js2/htdocs:/usr/local/apache2/htdocs/:ro&quot; \
         -v &quot;/tmp/out-js2/xterm-pty.conf:/usr/local/apache2/conf/extra/xterm-pty.conf:ro&quot; \
         --entrypoint=/bin/sh httpd -c 'echo &quot;Include conf/extra/xterm-pty.conf&quot; >> /usr/local/apache2/conf/httpd.conf &amp;&amp; httpd-foreground'"><pre><code>$ cp -R ./examples/wasi-browser/* /tmp/out-js2/ &amp;&amp; chmod 755 /tmp/out-js2/htdocs
$ docker run --rm -p 8080:80 \
         -v "/tmp/out-js2/htdocs:/usr/local/apache2/htdocs/:ro" \
         -v "/tmp/out-js2/xterm-pty.conf:/usr/local/apache2/conf/extra/xterm-pty.conf:ro" \
         --entrypoint=/bin/sh httpd -c 'echo "Include conf/extra/xterm-pty.conf" &gt;&gt; /usr/local/apache2/conf/httpd.conf &amp;&amp; httpd-foreground'
</code></pre></div>
<p dir="auto">You can run the container on browser via <code>localhost:8080</code>.</p>
<h5 tabindex="-1" dir="auto">WASI on browser with networking</h5>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/ktock/container2wasm/blob/main/docs/images/debian-curl-wasi-on-browser-frontend-networking.png"><img src="https://github.com/ktock/container2wasm/raw/main/docs/images/debian-curl-wasi-on-browser-frontend-networking.png" alt="Debian container on browser with browser networking"></a></p>
<p dir="auto">Container can also perform networking.
This section is the demo of using curl command in the container.</p>
<blockquote>
<p dir="auto">Tested only on Chrome. The example might not work on other browsers.</p>
</blockquote>
<div data-snippet-clipboard-copy-content="$ cat <<EOF | docker build -t debian-curl -
FROM debian:sid-slim
RUN apt-get update &amp;&amp; apt-get install -y curl
EOF
$ c2w debian-curl /tmp/out-js2/htdocs/out.wasm"><pre><code>$ cat &lt;&lt;EOF | docker build -t debian-curl -
FROM debian:sid-slim
RUN apt-get update &amp;&amp; apt-get install -y curl
EOF
$ c2w debian-curl /tmp/out-js2/htdocs/out.wasm
</code></pre></div>
<p dir="auto">This example serves the image on <code>localhost:8080</code> using apache http server.
The following also puts the <a href="https://github.com/ktock/container2wasm/blob/main/extras/c2w-net-proxy">network stack runnable on browser</a> to the document root.</p>
<div data-snippet-clipboard-copy-content="$ cp -R ./examples/wasi-browser/* /tmp/out-js2/ &amp;&amp; chmod 755 /tmp/out-js2/htdocs
$ wget -O /tmp/out-js2/htdocs/c2w-net-proxy.wasm https://github.com/ktock/container2wasm/releases/download/v0.5.0/c2w-net-proxy.wasm
$ docker run --rm -p 8080:80 \
         -v &quot;/tmp/out-js2/htdocs:/usr/local/apache2/htdocs/:ro&quot; \
         -v &quot;/tmp/out-js2/xterm-pty.conf:/usr/local/apache2/conf/extra/xterm-pty.conf:ro&quot; \
         --entrypoint=/bin/sh httpd -c 'echo &quot;Include conf/extra/xterm-pty.conf&quot; >> /usr/local/apache2/conf/httpd.conf &amp;&amp; httpd-foreground'"><pre><code>$ cp -R ./examples/wasi-browser/* /tmp/out-js2/ &amp;&amp; chmod 755 /tmp/out-js2/htdocs
$ wget -O /tmp/out-js2/htdocs/c2w-net-proxy.wasm https://github.com/ktock/container2wasm/releases/download/v0.5.0/c2w-net-proxy.wasm
$ docker run --rm -p 8080:80 \
         -v "/tmp/out-js2/htdocs:/usr/local/apache2/htdocs/:ro" \
         -v "/tmp/out-js2/xterm-pty.conf:/usr/local/apache2/conf/extra/xterm-pty.conf:ro" \
         --entrypoint=/bin/sh httpd -c 'echo "Include conf/extra/xterm-pty.conf" &gt;&gt; /usr/local/apache2/conf/httpd.conf &amp;&amp; httpd-foreground'
</code></pre></div>
<p dir="auto">You can run the container on browser with several types of configurations:</p>
<ul dir="auto">
<li><code>localhost:8080/?net=browser</code>: Container with networking. <a href="https://github.com/ktock/container2wasm/blob/main/extras/c2w-net-proxy">Network stack <code>c2w-net-proxy</code></a> implemented based on <a href="https://github.com/containers/gvisor-tap-vsock"><code>gvisor-tap-vsock</code></a> runs on browser and forwards HTTP/HTTPS packets using the browser's Fetch API. The set of accesible sites is restricted by the browser configuration (e.g. CORS restriction). See also <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking/fetch"><code>./examples/networking/fetch</code></a> for detalis.</li>
<li><code>localhost:8080/?net=delegate=ws://localhost:8888</code>: Container with networking. You need to run <a href="https://github.com/ktock/container2wasm/blob/main/cmd/c2w-net">user-space network stack <code>c2w-net</code></a> implemented based on <a href="https://github.com/containers/gvisor-tap-vsock"><code>gvisor-tap-vsock</code></a> on the host (outside of browser). It forwards all packets received from the browser over WebSocket. See also <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking/websocket"><code>./examples/networking/websocket</code></a> for detalis and configuration. (tested only on Linux)</li>
<li><code>localhost:8080</code>: Container without networking.</li>
</ul>
<h4 tabindex="-1" dir="auto">emscripten on browser</h4>
<p dir="auto">This example uses emscripten for converting the container to WASM.</p>
<ul dir="auto">
<li>pros: WASM image size can be smaller than WASI.</li>
<li>cons: WASI-specific optimization like <a href="https://github.com/bytecodealliance/wizer/">Wizer</a> pre-initialization isn't available for this mode. So the startup of the container can be slow (For x86_64 containers it might take &gt;= 30s. For riscv64 containers it might take &gt;= 10s).</li>
</ul>
<p dir="auto">The following command generates a WASM image and a JS file runnable on browser.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ c2w --to-js ubuntu:22.04 /tmp/out-js/htdocs/"><pre>$ <span>c2w --to-js ubuntu:22.04 /tmp/out-js/htdocs/</span></pre></div>
<p dir="auto">The following is an example of running the image on browser relying on <a href="https://github.com/mame/xterm-pty">xterm-pty</a>.
This example serves the image on <code>localhost:8080</code> using apache http server.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ cp -R ./examples/emscripten/* /tmp/out-js/ &amp;&amp; chmod 755 /tmp/out-js/htdocs
$ docker run --rm -p 8080:80 \
         -v &quot;/tmp/out-js/htdocs:/usr/local/apache2/htdocs/:ro&quot; \
         -v &quot;/tmp/out-js/xterm-pty.conf:/usr/local/apache2/conf/extra/xterm-pty.conf:ro&quot; \
         --entrypoint=/bin/sh httpd -c 'echo &quot;Include conf/extra/xterm-pty.conf&quot; >> /usr/local/apache2/conf/httpd.conf &amp;&amp; httpd-foreground'"><pre>$ <span>cp -R ./examples/emscripten/<span>*</span> /tmp/out-js/ <span>&amp;&amp;</span> chmod 755 /tmp/out-js/htdocs</span>
$ <span>docker run --rm -p 8080:80 \</span>
<span>         -v "/tmp/out-js/htdocs:/usr/local/apache2/htdocs/:ro" \</span>
<span>         -v "/tmp/out-js/xterm-pty.conf:/usr/local/apache2/conf/extra/xterm-pty.conf:ro" \</span>
<span>         --entrypoint=/bin/sh httpd -c 'echo "Include conf/extra/xterm-pty.conf" &gt;&gt; /usr/local/apache2/conf/httpd.conf &amp;&amp; httpd-foreground'</span></pre></div>
<p dir="auto">You can run the container on browser via <code>localhost:8080</code>.</p>
<blockquote>
<p dir="auto">NOTE: It can take some time to load and start the container.</p>
</blockquote>
<p dir="auto">Networking can also be enabled using the <a href="https://github.com/ktock/container2wasm/blob/main/cmd/c2w-net">user-space network stack <code>c2w-net</code></a> implemented based on <a href="https://github.com/containers/gvisor-tap-vsock"><code>gvisor-tap-vsock</code></a> serving over WebSocket on the host (outside of browser).
See also <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking/websocket"><code>./examples/networking/websocket</code></a> for detalis.</p>
<h2 tabindex="-1" dir="auto">Getting Started</h2>
<ul dir="auto">
<li>requirements
<ul dir="auto">
<li>Docker 18.09+ (w/ <code>DOCKER_BUILDKIT=1</code>)</li>
<li><a href="https://docs.docker.com/build/install-buildx/" rel="nofollow">Docker Buildx</a> v0.8+ (recommended) or <code>docker build</code> (w/ <code>DOCKER_BUILDKIT=1</code>)</li>
</ul>
</li>
</ul>
<p dir="auto">You can install the converter command <code>c2w</code> using one of the following methods.</p>
<blockquote>
<p dir="auto">NOTE: The output binary also contains <a href="https://github.com/ktock/container2wasm/blob/main/cmd/c2w-net"><code>c2w-net</code></a> which a command usable for controlling networking feature (please see also <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking">./examples/networking</a> for details).</p>
</blockquote>
<h3 tabindex="-1" dir="auto">Release binaries</h3>
<p dir="auto">Binaries are available from <a href="https://github.com/ktock/container2wasm/releases">https://github.com/ktock/container2wasm/releases</a>
Extract the tarball and put the binary somewhere under <code>$PATH</code>.</p>
<h3 tabindex="-1" dir="auto">Building binaries using make</h3>
<p dir="auto">Go 1.19+ is needed.</p>

<h2 tabindex="-1" dir="auto">Command reference</h2>
<h3 tabindex="-1" dir="auto">c2w</h3>
<p dir="auto">Converts a container image into a WASM image and writes it to the specified path (default: <code>out.wasm</code> at the current directory).</p>
<p dir="auto">Usage: <code>c2w [options] image-name [output file]</code></p>
<ul dir="auto">
<li><code>image-name</code>: container image name (will be pulled from the registry if it doesn't exist in Docker)</li>
<li><code>[output file]</code>: path to the result WASM file.</li>
</ul>
<p dir="auto">Sub commands</p>
<ul dir="auto">
<li><code>help, h</code>: Shows a list of commands or help for one command</li>
</ul>
<p dir="auto">Options</p>
<ul dir="auto">
<li><code>--assets value</code>: Custom location of build assets.</li>
<li><code>--dockerfile value</code>: Custom location of Dockerfile (default: embedded to this command)</li>
<li><code>--builder value</code>: Bulider command to use (default: "docker")</li>
<li><code>--target-arch value</code>: target architecture of the source image to use (default: "amd64")</li>
<li><code>--build-arg value</code>: Additional build arguments (please see Dockerfile for available build args)</li>
<li><code>--to-js</code>: convert the container to WASM using emscripten</li>
<li><code>--debug-image</code>: Enable debug print in the output image</li>
<li><code>--show-dockerfile</code>: Show default Dockerfile</li>
<li><code>--legacy</code>: Use "docker build" instead of buildx (no support for assets flag) (default:false)</li>
<li><code>--help, -h</code>: show help</li>
<li><code>--version, -v: </code>print the version</li>
</ul>
<h3 tabindex="-1" dir="auto">Run-time flags for WASM image</h3>
<p dir="auto">You can specify run-time flags to the generated wasm image for configuring the execution (e.g. for changing command to run in the container).</p>
<p dir="auto">Usage: <code>out.wasm [options] [COMMAND] [ARG...]</code></p>
<ul dir="auto">
<li><code>[COMMAND] [ARG...]</code>: command to run in the container. (default: commands specified in the image config)</li>
</ul>
<p dir="auto">Options</p>
<ul dir="auto">
<li><code>-entrypoint &lt;command&gt;</code> : entrypoint command. (default: entrypoint specified in the image config)</li>
<li><code>-no-stdin</code> : disable stdin. (default: false)</li>
</ul>
<p dir="auto">Example:</p>
<p dir="auto">The following changes the container's entrypoint to <code>echo</code> and pass <code>hello</code> to the arguments.</p>
<div data-snippet-clipboard-copy-content="wasmtime -- /app/out.wasm --entrypoint=echo hello"><pre><code>wasmtime -- /app/out.wasm --entrypoint=echo hello
</code></pre></div>
<h3 tabindex="-1" dir="auto">Directory mapping</h3>
<p dir="auto">Directory mapped from the host is accessible on the container.</p>
<div data-snippet-clipboard-copy-content="$ mkdir -p /tmp/share/ &amp;&amp; echo hi > /tmp/share/hi
$ wasmtime --mapdir /test/dir/share::/tmp/share /app/out.wasm ls /test/dir/share/
hi"><pre><code>$ mkdir -p /tmp/share/ &amp;&amp; echo hi &gt; /tmp/share/hi
$ wasmtime --mapdir /test/dir/share::/tmp/share /app/out.wasm ls /test/dir/share/
hi
</code></pre></div>
<h2 tabindex="-1" dir="auto">Motivation</h2>
<p dir="auto">Though more and more programming languages start to support WASM, it's not easy to run the existing programs on WASM.
This sometimes requires re-implementing and re-compiling them and costs extra time for development.
This is a PoC converter tries to solve it by enabling running unmodified containers on WASM.</p>
<h2 tabindex="-1" dir="auto">How does it work</h2>
<p dir="auto">contaienr2wasm creates a WASM image that runs the container and the Linux kernel on the emulated CPU.</p>
<p dir="auto">The following shows the techniqual details:</p>
<ul dir="auto">
<li>Builder: <a href="https://github.com/moby/buildkit">BuildKit</a> runs the conversion steps written in Dockerfile.</li>
<li>Emulator: <a href="https://bochs.sourceforge.io/" rel="nofollow">Bochs</a> emulates x86_64 CPU on WASM. <a href="https://bellard.org/tinyemu/" rel="nofollow">TinyEMU</a> emulates RISC-V CPU on WASM. They're compiled to WASM using <a href="https://github.com/WebAssembly/wasi-sdk">wasi-sdk</a> (for WASI and on-browser) and <a href="https://github.com/emscripten-core/emscripten">emscripten</a> (for on-browser).</li>
<li>Guest OS: Linux runs on the emulated CPU. <a href="https://github.com/opencontainers/runc">runc</a> starts the container. Non-x86 and non-RISC-V containers runs with additional emulation by QEMU installed via <a href="https://github.com/tonistiigi/binfmt"><code>tonistiigi/binfmt</code></a>.</li>
<li>Directory Mapping: WASI filesystem API makes host directories visible to the emulator. Emulators mount them to the guest linux via virtio-9p.</li>
<li>Packaging: <a href="https://github.com/kateinoigakukun/wasi-vfs">wasi-vfs</a> (for WASI and on-browser) and emscripten (for on-browser) are used for packaging the dependencies. The kernel is pre-booted during the build using <a href="https://github.com/bytecodealliance/wizer/">wizer</a> to minimize the startup latency (for WASI only as of now).</li>
<li>Networking: Browser's Fetch API or WebSocket is used for on-browser image. <code>sock_*</code> API is used for WASI. <a href="https://github.com/containers/gvisor-tap-vsock"><code>gvisor-tap-vsock</code></a> can be used as the networking stack. (docs: <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking"><code>./examples/networking/</code></a>)</li>
<li>Security: The converted container runs in the sandboxed WASM (WASI) VM with the limited access to the host system.</li>
</ul>
<h2 tabindex="-1" dir="auto">WASI Runtimes Integration Status</h2>
<ul dir="auto">
<li>
<p dir="auto">✔️ : supported</p>
</li>
<li>
<p dir="auto">🚧 : WIP</p>
</li>
<li>
<p dir="auto"><strong>NOTE</strong>: WASI features not listed here are untested (future version will support more features)</p>
</li>
</ul>
<h3 tabindex="-1" dir="auto">x86_64 containers</h3>
<table>
<thead>
<tr>
<th>runtime</th>
<th>stdio</th>
<th>mapdir</th>
<th>networking</th>
<th>note</th>
</tr>
</thead>
<tbody>
<tr>
<td>wasmtime</td>
<td>✔️</td>
<td>✔️</td>
<td>✔️ (w/ <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking/wasi">host-side network stack</a>)</td>
<td></td>
</tr>
<tr>
<td>wamr(wasm-micro-runtime)</td>
<td>✔️</td>
<td>✔️</td>
<td>🚧</td>
<td></td>
</tr>
<tr>
<td>wazero</td>
<td>✔️</td>
<td>✔️</td>
<td>✔️ (w/ <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking/wasi">host-side network stack</a></td>
<td></td>
</tr>
<tr>
<td>wasmer</td>
<td>🚧 (stdin unsupported)</td>
<td>✔️</td>
<td>🚧</td>
<td>non-blocking stdin doesn't seem to work</td>
</tr>
<tr>
<td>wasmedge</td>
<td>🚧 (stdin unsupported)</td>
<td>✔️</td>
<td>🚧</td>
<td>non-blocking stdin doesn't seem to work</td>
</tr>
</tbody>
</table>
<h3 tabindex="-1" dir="auto">risc-v and other architecutre's containers</h3>
<table>
<thead>
<tr>
<th>runtime</th>
<th>stdio</th>
<th>mapdir</th>
<th>networking</th>
<th>note</th>
</tr>
</thead>
<tbody>
<tr>
<td>wasmtime</td>
<td>✔️</td>
<td>✔️</td>
<td>✔️ (w/ <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking/wasi">host-side network stack</a>)</td>
<td></td>
</tr>
<tr>
<td>wamr(wasm-micro-runtime)</td>
<td>✔️</td>
<td>✔️</td>
<td>🚧</td>
<td></td>
</tr>
<tr>
<td>wazero</td>
<td>✔️</td>
<td>✔️</td>
<td>✔️ (w/ <a href="https://github.com/ktock/container2wasm/blob/main/examples/networking/wasi">host-side network stack</a>)</td>
<td></td>
</tr>
<tr>
<td>wasmer</td>
<td>🚧 (stdin unsupported)</td>
<td>✔️</td>
<td>🚧</td>
<td>non-blocking stdin doesn't seem to work</td>
</tr>
<tr>
<td>wasmedge</td>
<td>🚧 (stdin unsupported)</td>
<td>✔️</td>
<td>🚧</td>
<td>non-blocking stdin doesn't seem to work</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" dir="auto">Similar projects</h2>
<p dir="auto">There are several container runtimes support running WASM applications, but they don't run containers on WASM.</p>
<ul dir="auto">
<li>WASM on container runtimes
<ul dir="auto">
<li>Docker+Wasm integration: <a href="https://docs.docker.com/desktop/wasm/" rel="nofollow">https://docs.docker.com/desktop/wasm/</a></li>
<li>runwasi: <a href="https://github.com/containerd/runwasi">https://github.com/containerd/runwasi</a></li>
<li>youki: <a href="https://github.com/containers/youki">https://github.com/containers/youki</a></li>
<li>crun: <a href="https://github.com/containers/crun">https://github.com/containers/crun</a></li>
<li>krustlet: <a href="https://github.com/krustlet/krustlet">https://github.com/krustlet/krustlet</a></li>
</ul>
</li>
</ul>
<p dir="auto">There are emulators that support running linux on WASM, but they don't support WASI.</p>
<ul dir="auto">
<li>
<p dir="auto">x86 on WASM</p>
<ul dir="auto">
<li>v86: <a href="https://github.com/copy/v86">https://github.com/copy/v86</a></li>
</ul>
</li>
<li>
<p dir="auto">RISCV on WASM</p>
<ul dir="auto">
<li>TinyEMU: <a href="https://bellard.org/tinyemu/" rel="nofollow">https://bellard.org/tinyemu/</a></li>
</ul>
</li>
</ul>
<p dir="auto">Some WASM API specs provides applications access to the host system.
Re-compilation (and possibe re-implementation) of the application is needed.</p>
<ul dir="auto">
<li>WASI: <a href="https://github.com/WebAssembly/WASI">https://github.com/WebAssembly/WASI</a></li>
<li>WASIX(WASI + additional syscall extensions): <a href="https://github.com/wasix-org">https://github.com/wasix-org</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Additional Resources</h2>
<ul dir="auto">
<li><a href="https://github.com/ktock/container2wasm/blob/main/examples"><code>./examples/</code></a>: Examples (python, php, on-browser, networking, etc.)</li>
<li><code>vscode-container-wasm</code>: VSCode extension for running containers on VSCode on browser (e.g. <code>github.dev</code>), leveraging container2wasm: <a href="https://github.com/ktock/vscode-container-wasm">https://github.com/ktock/vscode-container-wasm</a></li>
</ul>
<h2 tabindex="-1" dir="auto">Acknowledgement</h2>
<ul dir="auto">
<li>
<p dir="auto">container2wasi itself is licensed under Apache 2.0 but the generated WASM image will include third-pirty softwares:</p>
<ul dir="auto">
<li>Bochs (<a href="https://github.com/bochs-emu/Bochs/blob/master/LICENSE">GNU Lesser General Public License v2.1</a>) <a href="https://bochs.sourceforge.io/" rel="nofollow">https://bochs.sourceforge.io/</a>
<ul dir="auto">
<li>Source code is contained in (<a href="https://github.com/ktock/container2wasm/blob/main/patches/bochs"><code>./patches/bochs</code></a>). Bochs is modified by our project for making it work with containers</li>
</ul>
</li>
<li>TinyEMU (<a href="https://opensource.org/license/mit/" rel="nofollow">MIT License</a>) <a href="https://bellard.org/tinyemu/" rel="nofollow">https://bellard.org/tinyemu/</a>
<ul dir="auto">
<li>Source code is contained in (<a href="https://github.com/ktock/container2wasm/blob/main/patches/tinyemu"><code>./patches/tinyemu</code></a>). TinyEMU is modified by our project for making it work with containers</li>
</ul>
</li>
<li>GRUB (<a href="https://www.gnu.org/licenses/gpl.html" rel="nofollow">GNU General Public License Version 3</a>): <a href="https://www.gnu.org/software/grub/" rel="nofollow">https://www.gnu.org/software/grub/</a></li>
<li>BBL(riscv-pk) (<a href="https://github.com/riscv-software-src/riscv-pk/blob/master/LICENSE">license</a>): <a href="https://github.com/riscv-software-src/riscv-pk">https://github.com/riscv-software-src/riscv-pk</a></li>
<li>Linux (<a href="https://github.com/torvalds/linux/blob/master/COPYING">GNU General Public License version 2</a>): <a href="https://github.com/torvalds/linux/">https://github.com/torvalds/linux/</a></li>
<li>tini (<a href="https://github.com/krallin/tini/blob/master/LICENSE">MIT License</a>): <a href="https://github.com/krallin/tini">https://github.com/krallin/tini</a></li>
<li>runc (<a href="https://github.com/opencontainers/runc/blob/main/LICENSE">Apache License 2.0</a>): <a href="https://github.com/opencontainers/runc">https://github.com/opencontainers/runc</a></li>
<li>Binfmt (<a href="https://github.com/tonistiigi/binfmt/blob/master/LICENSE">MIT License</a>): <a href="https://github.com/tonistiigi/binfmt">https://github.com/tonistiigi/binfmt</a></li>
<li>QEMU (<a href="https://github.com/qemu/qemu/blob/master/LICENSE">license</a>): <a href="https://github.com/qemu/qemu">https://github.com/qemu/qemu</a></li>
<li>vmtouch (<a href="https://github.com/hoytech/vmtouch/blob/master/LICENSE">license</a>): <a href="https://github.com/hoytech/vmtouch">https://github.com/hoytech/vmtouch</a></li>
<li>BusyBox (<a href="https://www.busybox.net/license.html" rel="nofollow">GNU General Public License version 2</a>): <a href="https://git.busybox.net/busybox" rel="nofollow">https://git.busybox.net/busybox</a></li>
</ul>
</li>
<li>
<p dir="auto">On-browser example relies on the following softwares.</p>
<ul dir="auto">
<li>xterm-pty (<a href="https://github.com/mame/xterm-pty/blob/main/LICENSE.txt">MIT License</a>): <a href="https://github.com/mame/xterm-pty">https://github.com/mame/xterm-pty</a></li>
<li><code>browser_wasi_shim</code> (either of <a href="https://github.com/bjorn3/browser_wasi_shim/blob/main/LICENSE-MIT">MIT License</a> and <a href="https://github.com/bjorn3/browser_wasi_shim/blob/main/LICENSE-APACHE">Apache License 2.0</a>): <a href="https://github.com/bjorn3/browser_wasi_shim">https://github.com/bjorn3/browser_wasi_shim</a></li>
<li><code>gvisor-tap-vsock</code> (<a href="https://github.com/containers/gvisor-tap-vsock/blob/main/LICENSE">Apache License 2.0</a>): <a href="https://github.com/containers/gvisor-tap-vsock">https://github.com/containers/gvisor-tap-vsock</a></li>
</ul>
</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Block cookie banners on Firefox (151 pts)]]></title>
            <link>https://support.mozilla.org/en-US/kb/cookie-banner-reduction</link>
            <guid>38856515</guid>
            <pubDate>Wed, 03 Jan 2024 17:07:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://support.mozilla.org/en-US/kb/cookie-banner-reduction">https://support.mozilla.org/en-US/kb/cookie-banner-reduction</a>, See on <a href="https://news.ycombinator.com/item?id=38856515">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      
      
      <main role="main">
      
      
  <section id="document-list">
    <header>
      <div>
          
          <p><a href="https://support.mozilla.org/en-US/products/firefox" title="Firefox">
            <img src="https://assets-prod.sumo.prod.webservices.mozgcp.net/media/uploads/products/2020-04-14-08-36-13-8dda6f.png" height="48" width="48" alt="">
          </a></p>

          
        </div>
    </header>

    <article>
      
  
  
      <section id="doc-content">
    
      <div id="toc"><h2>Table of Contents</h2><ul><li><a href="#w_why-are-cookie-banners-a-problem"><span>1</span> <span>Why are cookie banners a problem?</span></a></li><li><a href="#w_how-it-works"><span>2</span> <span>How it works</span></a></li><li><a href="#w_which-banners-are-blocked"><span>3</span> <span>Which banners are blocked?</span></a></li><li><a href="#w_get-started"><span>4</span> <span>Get started</span></a></li><li><a href="#w_customize-your-experience"><span>5</span> <span>Customize Your Experience</span></a></li><li><a href="#w_integrated-cookie-protections"><span>6</span> <span>Integrated cookie protections</span></a></li><li><a href="#w_why-germany-and-private-browsing-mode"><span>7</span> <span>Why Germany and private browsing mode?</span></a></li></ul></div>
<p>Since the development of stricter consumer privacy laws that specifically target the usage and retention of user data, such as <a href="https://wikipedia.org/wiki/General_Data_Protection_Regulation">GDPR</a>, websites have adopted legal notices to inform users that they are using technologies such as cookies to store data on the user's device. To address the increasing frustration caused by these cookie banners on websites, <a href="https://support.mozilla.org/en-US/kb/find-what-version-firefox-you-are-using">Firefox version</a> 120 introduces the cookie banner blocker. This feature is designed to make your browsing experience smoother by taking care of these annoying banners automatically. In this article, we'll show you how it works now, and how we intend to expand on it in the future.
</p>

<p>Before we dive into the solution, let's understand why cookie banners can be a hassle:
</p>
<ul><li><strong>Annoying prompts:</strong> Cookie banners, as you've likely experienced, can be quite annoying. They interrupt your browsing to ask a question that can often be confusing and irrelevant to your browsing, before allowing you to refocus on why you navigated to that site in the first place. Their goal being to create a sense of urgency, making it feel like you must choose between trading your data for access to the site, or navigating away in hopes of protecting your privacy.
</li><li><strong>Deceptive designs:</strong> Many websites employ tricky tactics when it comes to their cookie banners. They may design them in a way that makes it difficult for you to customize your settings. For example, they might make the <span>Accept All Cookies</span> button prominent, colorful and appealing, while burying the <strong>Customize Settings</strong> option in small text or a less noticeable location. This design choice can be misleading, effectively nudging users towards accepting all cookies without considering the consequences.
</li><li><strong>Prompt fatigue:</strong> The sheer volume of cookie banners that users encounter daily can lead to what we call “prompt fatigue”. This phenomenon occurs when users are bombarded with these pop-ups so frequently that they ignore or mindlessly accept them without actually reading the contents or understanding the implications (assuming they are written in a manner that is easy to understand in the first place). Sites effectively betting that you would trade your data to save you the several seconds of your life it takes to opt out over and over again. In this environment, the websites that employ the most deceptive patterns gain an unfair advantage, as they effectively exploit this fatigue to get you to hand over your data, even when it is not needed.
</li></ul>
<h2 id="w_how-it-works">How it works</h2>
<p>When it comes to handling those cookie banners, the cookie banner blocker employs two clever methods:
</p>
<ul><li><strong>Cookie Injection:</strong> This method involves setting an “opt-out” cookie whenever possible. Think of it like a digital note that tells the website, “I don't want to see this banner again.” It's a preemptive way of getting rid of those banners before they even have a chance to appear.
</li><li><strong>Auto Click CSS Selector:</strong> In cases where setting an opt-out cookie isn't an option, the Cookie Banner Blocker uses another technique. It effectively clicks on the banner's “decline” or “reject” option in a way that mimics what you might do manually. This action dismisses the banner as if you made a choice yourself.
</li></ul>
<p>So, in a nutshell, the cookie banner blocker is like your automated assistant that takes care of annoying cookie banners by either preemptively telling the website you opt out or by clicking on your behalf to make them go away. This way, you can spend less time opting out and more time browsing.
</p>

<p>The cookie banner blocker works by using a careful selection of websites that we've put together. We've compiled this list by considering the most frequently visited sites in our key regions to have the broadest impact. We're actively working to expand this coverage by incorporating support for consent management platforms and other top sites we haven’t gotten to yet.
</p>
<h2 id="w_get-started">Get started</h2>
<p>The cookie banner blocker is available starting from Firefox version 120, and it's automatically enabled for users in Germany browsing in Private Browsing Mode. Here's how you can make the most of it:
</p>
<ol><li><strong>Visit supported websites:</strong> The cookie banner blocker works on websites that have a decline option in their cookie banners.
</li><li><strong>Automatic decline:</strong> When you visit a supported website, if the banner has a decline option, Firefox will automatically decline it for you.
</li><li><strong>Enjoy the benefits:</strong> You can now browse without the interruption of annoying cookie banners.
</li></ol>
<h2 id="w_customize-your-experience">Customize Your Experience</h2>
<p>If you want to enable or disable the cookie banner blocker for specific websites or altogether, here's how:
</p>
<ul><li><strong>Enable:</strong> By default, it's on in private windows for users in Germany.
</li><li><strong>Disable:</strong> You can turn it off through preferences if you wish.
</li></ul>
<h2 id="w_integrated-cookie-protections">Integrated cookie protections</h2>
<p>Firefox has additional cookie protections that work seamlessly with the cookie banner blocker:
</p>
<ul><li><strong><a href="https://support.mozilla.org/en-US/kb/introducing-total-cookie-protection-standard-mode">Total Cookie Protection (TCP)</a>:</strong> This limits the impact of tracking cookies, regardless of banner interactions.
</li><li><strong><a href="https://support.mozilla.org/en-US/kb/enhanced-tracking-protection-firefox-desktop">Enhanced Tracking Protection (ETP)</a> Strict Mode:</strong> Automatically blocks third-party cookies.
</li></ul>
<h2 id="w_why-germany-and-private-browsing-mode">Why Germany and private browsing mode?</h2>
<p>Our initial launch in Germany and private browsing mode has specific reasons:
</p>
<ul><li>Private browsing mode displays cookie banners repeatedly, making this feature especially useful. Germany, as a part of the European Union, is a prominent market where cookie banners are noticeable due to GDPR.
</li><li>We plan to gather insights from this launch before potentially expanding the feature to a broader audience.
</li></ul>
<p>By following these instructions, you can take full advantage of the cookie banner blocker in Firefox, simplifying your online experience while ensuring your privacy. Happy browsing!
</p>
    
  </section>

      
      
      
      
        
      
    </article>

    

    <section id="doc-contributors">
    <p>These fine people helped write this article:</p> 
  </section>

    <div>
    <p><img src="https://assets-prod.sumo.prod.webservices.mozgcp.net/static/volunteer.a3be8d331849774b.png" alt="Illustration of hands"></p><div>
      <h3>Volunteer</h3>
      <p>Grow and share your expertise with others. Answer questions and improve our knowledge base.</p>
      <p><strong><a href="https://support.mozilla.org/en-US/contribute">Learn More</a></strong></p>
    </div>
  </div>

    
      
  

    

  </section>

      </main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[23andMe tells victims it's their fault that their data was breached (230 pts)]]></title>
            <link>https://techcrunch.com/2024/01/03/23andme-tells-victims-its-their-fault-that-their-data-was-breached/</link>
            <guid>38856412</guid>
            <pubDate>Wed, 03 Jan 2024 16:59:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/01/03/23andme-tells-victims-its-their-fault-that-their-data-was-breached/">https://techcrunch.com/2024/01/03/23andme-tells-victims-its-their-fault-that-their-data-was-breached/</a>, See on <a href="https://news.ycombinator.com/item?id=38856412">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Facing <a href="https://www.law.com/radar/card/with-data-breach-lawsuits-mounting-23andme-moves-for-california-mdl-403-96744/">more than 30 lawsuits</a> from victims of its massive data breach, 23andMe is now deflecting the blame to the victims themselves in an attempt to absolve itself from any responsibility, <a href="https://www.documentcloud.org/documents/24252535-response-letter-to-tycko-zavareei-llp">according to a letter sent to a group of victims seen by TechCrunch</a>.</p>
<p>“Rather than acknowledge its role in this data security disaster, 23andMe has apparently decided to leave its customers out to dry while downplaying the seriousness of these events,” Hassan Zavareei, one of the lawyers representing the victims who received the letter from 23andMe, told TechCrunch in an email.</p>
<p>In December, 23andMe admitted that <a href="https://techcrunch.com/2023/12/04/23andme-confirms-hackers-stole-ancestry-data-on-6-9-million-users/">hackers had stolen the genetic and ancestry data of 6.9 million users</a>, nearly half of all its customers.</p>
<p>The data breach started with hackers accessing only around 14,000 user accounts. The hackers broke into this first set of victims by <a href="https://techcrunch.com/2023/10/10/23andme-resets-user-passwords-after-genetic-data-posted-online/">brute-forcing accounts with passwords that were known to be associated with the targeted customers</a>, a technique known as credential stuffing.</p>
<p>From these 14,000 initial victims, however, the hackers were able to then access the personal data of the other 6.9 million million victims because they had opted-in to 23andMe’s <a href="https://customercare.23andme.com/hc/en-us/articles/212170838">DNA Relatives</a> feature. This optional feature allows customers to automatically share some of their data with people who are considered their relatives on the platform.</p>
<p>In other words, by hacking into only 14,000 customers’ accounts, the hackers subsequently scraped personal data of another 6.9 million customers whose accounts were not directly hacked.</p>
<p>But in a letter sent to a group of hundreds of 23andMe users who are now suing the company, 23andMe said that “users negligently recycled and failed to update their passwords following these past security incidents, which are unrelated to 23andMe.”</p>
<p>“Therefore, the incident was not a result of 23andMe’s alleged failure to maintain reasonable security measures,” the letter reads.</p>
<p>Zavareei said that 23andMe is “shamelessly” blaming the victims of the data breach.</p>
<p>“This finger pointing is nonsensical. 23andMe knew or should have known that many consumers use recycled passwords and thus that 23andMe should have implemented some of the many safeguards available to protect against credential stuffing — especially considering that 23andMe stores personal identifying information, health information, and genetic information on its platform,” Zavareei said in an email.</p>
<p>“The breach impacted millions of consumers whose data was exposed through the DNA Relatives feature on 23andMe’s platform, not because they used recycled passwords. Of those millions, only a few thousand accounts were compromised due to credential stuffing. 23andMe’s attempt to shirk responsibility by blaming its customers does nothing for these millions of consumers whose data was compromised through no fault of their own whatsoever,” said Zavareei.</p>
<div>
		<h4>Contact Us</h4><p>
		Do you have more information about the 23andMe incident? We’d love to hear from you. You can contact Lorenzo Franceschi-Bicchierai securely on Signal at +1 917 257 1382, or via Telegram, Keybase and Wire @lorenzofb, or email lorenzo@techcrunch.com. You also can contact TechCrunch via SecureDrop.	</p></div>
	
<p>In response to 23andMe’s letter, Dante Termohs, a 23andMe customer who was impacted by the data breach, told TechCrunch that he found “it appalling that 23andMe is attempting to hide from consequences instead of helping its customers.”</p>
<p>23andMe’s lawyers argued that the stolen data cannot be used to inflict monetary damage against the victims.</p>
<p>“The information that was potentially accessed cannot be used for any harm. As explained in the October 6, 2023 blog post, the profile information that may have been accessed related to the DNA Relatives feature, which a customer creates and chooses to share with other users on 23andMe’s platform. Such information would only be available if plaintiffs affirmatively elected to share this information with other users via the DNA Relatives feature. Additionally, the information that the unauthorized actor potentially obtained about plaintiffs could not have been used to cause pecuniary harm (it did not include their social security number, driver’s license number, or any payment or financial information),” the letter read.</p>
<p>23andMe and one of its lawyers did not respond to TechCrunch’s request for comment.</p>
<p>After disclosing the breach, 23andMe reset all customer passwords, and then <a href="https://techcrunch.com/2023/11/07/23andme-ancestry-myheritage-two-factor-by-default/">required all customers to use multi-factor authentication</a>, which was only optional before the breach.</p>
<p>In an attempt to pre-empt the inevitable class action lawsuits and mass arbitration claims, <a href="https://techcrunch.com/2023/12/11/23andme-changes-to-terms-of-service-are-cynical-and-self-serving-lawyers-say/">23andMe changed its terms of service to make it more difficult for victims to band together</a> when filing a legal claim against the company. Lawyers with experience representing data breach victims told TechCrunch that the changes were “cynical,” “self-serving,” and “a desperate attempt” to protect itself and deter customers from going after the company.</p>
<p>Clearly, the changes didn’t stop what is now a flurry of <a href="https://topclassactions.com/lawsuit-settlements/privacy/data-breach/23andme-hit-with-another-class-action-lawsuit-over-data-breach/">class action lawsuits</a>.</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Xerox to cut 15% of its workforce (114 pts)]]></title>
            <link>https://www.cnbc.com/2024/01/03/xerox-layoffs-company-to-cut-15percent-of-workforce.html</link>
            <guid>38855621</guid>
            <pubDate>Wed, 03 Jan 2024 16:05:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/01/03/xerox-layoffs-company-to-cut-15percent-of-workforce.html">https://www.cnbc.com/2024/01/03/xerox-layoffs-company-to-cut-15percent-of-workforce.html</a>, See on <a href="https://news.ycombinator.com/item?id=38855621">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-6" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-6-2"><div id="ArticleBody-InlineImage-106258696" data-test="InlineImage"><p>Signage is displayed outside the Xerox headquarters in Norwalk, Connecticut.</p><p>Michael Nagle | Bloomberg | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/XRX/">Xerox</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> on Wednesday announced it will cut 15% of its workforce as part of a plan to implement a new organizational structure and operating model.</p><p>Xerox, which offers digital printing and document management technologies, had about 20,500 employees as of Dec. 31, 2022, according to a filing with the U.S. Securities and Exchange Commission. Based on this figure, Wednesday's layoffs will affect about 3,075 employees.</p><p>Shares of Xerox fell more than 9% following the announcement Wednesday.</p><p>The company's restructuring plan involves simplifying its products within its core print business, increasing efficiency across its global business services and boosting focus on IT and other digital services, according to <a href="https://www.news.xerox.com/news/xerox-reinvention-and-operating-model-evolution" target="_blank">a release</a>. Xerox said it also redesigned its executive team to help carry out the new model.</p><p><em>"</em>The shift to a business unit operating model is a continuation of our client-focused, balanced execution priorities and is designed to accelerate product and services, go-to-market, and corporate functions' operating efficiencies across all geographies we serve," Xerox CEO Steven Bandrowczak said in the release.</p><p>Xerox will carry out the cuts this quarter, according to the release. A representative for Xerox did not comment beyond the release.<br><em><strong><br>Don't miss these stories from CNBC PRO:</strong></em></p><ul><li><a href="https://www.cnbc.com/2024/01/01/these-stocks-will-be-the-biggest-sp-500-winners-of-2024-according-to-analysts.html"><em>These stocks will be the biggest S&amp;P 500 winners of 2024, according to analysts</em></a></li><li><a href="https://www.cnbc.com/2024/01/01/heres-where-to-invest-50000-in-the-new-year-according-to-the-pros.html"><em>Here's where to invest $50,000 in the new year, according to the pros</em></a></li><li><a href="https://www.cnbc.com/2023/12/28/could-a-bitcoin-etf-approval-be-a-sell-the-news-event-what-investors-are-expecting-on-the-big-day.html"><em>Could a bitcoin ETF approval be a sell-the-news event? Here's what to expect if it happens</em></a></li><li><a href="https://www.cnbc.com/2023/12/29/these-stocks-will-be-the-biggest-dow-winners-of-2024-according-to-analysts.html"><em>These stocks will be the biggest Dow winners of 2024, according to analysts</em></a></li><li><a href="https://www.cnbc.com/2023/12/25/oprahs-flip-on-weight-loss-drugs-is-a-sign-of-whats-to-come-for-the-ozempic-trade-in-2024.html"><em>Oprah's flip on weight loss drugs is a sign of what's to come for the 'Ozempic trade' in 2024</em></a></li></ul></div></div></div>]]></description>
        </item>
    </channel>
</rss>