<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 17 Jan 2024 09:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: I made a website to find best bus seat to avoid the sun while traveling (227 pts)]]></title>
            <link>https://sitinshade.com</link>
            <guid>39022693</guid>
            <pubDate>Wed, 17 Jan 2024 02:59:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sitinshade.com">https://sitinshade.com</a>, See on <a href="https://news.ycombinator.com/item?id=39022693">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
      <p> Find Best Bus Seat to Minimize Sun Exposure While Traveling</p>
      
      
      
        <div>
          <div>
            <p><label>Date</label></p>
          </div>
          
          <div>
            <p><label>Time</label></p>
          </div>
        </div>
        
        
        
        <div>
            <p><label>Timezone</label></p><div>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
              <path d="M233.4 406.6c12.5 12.5 32.8 12.5 45.3 0l192-192c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L256 338.7 86.6 169.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l192 192z"></path>
            </svg>
          </div>
        </div>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI drops ban on military tools to partner with The Pentagon (263 pts)]]></title>
            <link>https://www.semafor.com/article/01/16/2024/openai-is-working-with-the-pentagon-on-cybersecurity-projects</link>
            <guid>39020778</guid>
            <pubDate>Tue, 16 Jan 2024 23:37:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.semafor.com/article/01/16/2024/openai-is-working-with-the-pentagon-on-cybersecurity-projects">https://www.semafor.com/article/01/16/2024/openai-is-working-with-the-pentagon-on-cybersecurity-projects</a>, See on <a href="https://news.ycombinator.com/item?id=39020778">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div><p><img alt="icon" loading="lazy" width="26" height="18" decoding="async" data-nimg="1" src="https://www.semafor.com/_next/static/media/2circles.337bd7ed.svg"></p><h3>Semafor Signals</h3></div><div><p>Insights from Wired, The Wall Street Journal, and The Information</p><p><img alt="Arrow Down" loading="lazy" width="49" height="49" decoding="async" data-nimg="1" src="https://www.semafor.com/_next/static/media/blue-arrow-down.388f0335.svg"></p></div></div><div><div><p><img src="https://www.semafor.com/_next/static/media/thenews@2x.f322bda2.png" alt="Title icon" width="20" height="16"></p><h3>The News</h3></div><div><p>OpenAI is working with the Pentagon on software projects, including ones related to cybersecurity, the company said Tuesday, in a dramatic change from its previous ban on providing its artificial intelligence technology to militaries.</p><p>The ChatGPT creator is also in discussions with the U.S. government about developing tools to reduce veteran suicides, Anna Makanju, the company’s vice president of global affairs, said at the World Economic Forum — but added that it will retain its ban on developing weapons.</p><p>Last week, OpenAI removed language in its usage policy that would ban its AI from being used in “military and warfare” applications, sparking <a href="https://theintercept.com/2024/01/12/open-ai-military-ban-chatgpt/?utm_source=substack&amp;utm_medium=email" rel="no-referrer">alarm among AI safety advocates.</a></p></div></div><div><div><p><img alt="icon" loading="lazy" width="20" height="16" decoding="async" data-nimg="1" src="https://www.semafor.com/_next/static/media/2circles-black.0f46bdfb.svg"></p><h3>SIGNALS</h3></div><p><strong>Semafor Signals:</strong> Global insights on today's biggest stories.</p><div><h3>Silicon Valley has changed its mind about collaborating with the Pentagon</h3><div><p>Wired</p><!-- --><p>, </p><!-- --><p>The Wall Street Journal</p><!-- --><p>, </p><!-- --><p>Semafor’s Technology Editor Reed Albergotti</p></div><p>Silicon Valley has softened its stance on collaborating with the U.S. military in recent years. In 2018, thousands of Google employees protested a Pentagon project, fearing technology they developed could be used for lethal purposes. That proved to be the <a href="https://www.wired.com/story/3-years-maven-uproar-google-warms-pentagon/" rel="noopener" target="_blank">high water mark of Silicon Valley opposition</a> to the Department of Defense, with Google since earning hundreds of millions from its defense contracts. The Pentagon has made <a href="https://www.wsj.com/amp/articles/pentagon-woos-silicon-valley-to-join-ranks-of-arms-makers-38b1d4c0" rel="noopener" target="_blank">a concerted effort</a> in recent years to win over Silicon Valley startups in order to develop new weapons technology and integrate advanced tools into the department’s operations. U.S.-China tensions and Russia’s war in Ukraine have also served to dispel many of the qualms entrepreneurs once had about military collaboration. “What’s emerged lately is a kind of <a href="https://www.semafor.com/article/01/03/2024/defense-tech-is-having-its-moment-in-silicon-valley" rel="noopener" target="_blank">techno-patriotism in Silicon Valley</a>,” wrote Semafor’s technology editor Reed Albergotti.</p></div><div><h3>AI may remake the military, but could come with profound risks</h3><div><p>Wired</p><!-- --><p>, </p><!-- --><p>Vox</p><!-- --><p>, </p><!-- --><p>Foreign Policy</p></div><p>Defense experts have been bullish about the impact AI will have on the military. Former Google CEO Eric Schmidt, now a prominent defense industry figure, has compared the arrival of AI to the <a href="https://www.wired.com/story/eric-schmidt-is-building-the-perfect-ai-war-fighting-machine/#:~:text=%E2%80%9CEinstein%20wrote%20a%20letter%20to,distributed%20systems%20are%20that%20powerful.%E2%80%9D" rel="noopener" target="_blank">advent of nuclear weapons</a>, Wired reported. “Einstein wrote a letter to Roosevelt in the 1930s saying that there is this new technology — nuclear weapons — that could change war, which it clearly did. I would argue that [AI-powered] autonomy and decentralized, distributed systems are that powerful,” Schmidt said. But advocacy groups have warned that integrating AI into warfare could come with profound risks given AI’s tendency to “hallucinate” — make up fake information and pass it off as real — which could have far higher stakes if AI-powered systema wwew integrated into <a href="https://www.vox.com/future-perfect/2023/11/28/23972547/the-militarized-ai-risk-thats-bigger-than-killer-robots" rel="noopener" target="_blank">command and control systems</a>. The Arms Control Association has warned that the rush to “​​exploit emerging technologies for military use has accelerated at a much faster pace than efforts to assess <a href="https://foreignpolicy.com/2023/04/11/ai-arms-race-artificial-intelligence-chatgpt-military-technology/" rel="noopener" target="_blank">the dangers they pose</a>.”</p></div><div><h3>OpenAI rules are unclear about scope of possible military deals</h3><div><p>The Information</p><!-- --><p>, </p><!-- --><p>Euromaidan Press</p><!-- --><p>, </p><!-- --><p>The Economist</p></div><p>Although OpenAI has ruled out developing weapons, its new policy would likely allow it to provide <a href="https://www.theinformation.com/articles/chatgpt-coming-to-an-army-near-you" rel="noopener" target="_blank">AI software to the Department of Defense</a> for uses such as helping analysts interpret data or write code, The Information reported. But as the war in Ukraine has shown, the divide between data crunching and warfare may not be as clear-cut as OpenAI would like. Ukraine has developed and imported software <a href="https://euromaidanpress.com/2023/08/09/ground-and-marine-drones-ai-and-javelin-training-simulator-how-ukrainian-military-technology-advancements-bring-country-closer-to-victory/" rel="noopener" target="_blank">to analyze large data</a>, which has allowed its artillery operators to be rapidly notified of Russian targets in the area and dramatically speed up <a href="https://www.economist.com/special-report/2023/07/03/the-war-in-ukraine-shows-how-technology-is-changing-the-battlefield" rel="noopener" target="_blank">the pace at which they can fire</a>. Meanwhile, The Information warned that the change in policy could be enough to reignite the debate over AI safety at OpenAI that contributed to Sam Altman’s brief firing as CEO.</p></div></div><div><p><img src="https://www.semafor.com/_next/static/media/semafor-logo-small.cc0a7c9c.svg" width="19" height="19" alt="Semafor Logo"></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["Amazonian dark earth" was the work of ancient humans (114 pts)]]></title>
            <link>https://www.bbc.com/future/article/20240116-the-dark-earth-revealing-the-amazons-secrets</link>
            <guid>39020600</guid>
            <pubDate>Tue, 16 Jan 2024 23:21:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/future/article/20240116-the-dark-earth-revealing-the-amazons-secrets">https://www.bbc.com/future/article/20240116-the-dark-earth-revealing-the-amazons-secrets</a>, See on <a href="https://news.ycombinator.com/item?id=39020600">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="futurearticle20240116-the-dark-earth-revealing-the-amazons-secrets"><div id="headline-futurearticle20240116-the-dark-earth-revealing-the-amazons-secrets"><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bx6q.webp" type="image/webp"><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bx6q.jpg" type="image/jpeg"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bx6q.webp" type="image/webp"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bx6q.jpg" type="image/jpeg"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bx6q.webp" type="image/webp"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bx6q.jpg" type="image/jpeg"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bx6q.webp" type="image/webp"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bx6q.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="The roots of a strangler fig in the Amazon (Credit: Alamy)" src="https://ychef.files.bbci.co.uk/976x549/p0h5bx6q.jpg" alt="The roots of a strangler fig in the Amazon (Credit: Alamy)" id=""></picture></div><div><article><div><p>Amid the discovery of a lost city in the Amazon rainforest, scientists are uncovering a different kind of relic underground – one that's still being used today.</p><div><p>D</p><div><p>Deep within the Amazon, Mark Robinson was up to his knees in buried treasure.</p>
<p>Together with an international team of scientists, Robinson was on an expedition to a remote patch of forest in Iténez, northwest Bolivia, close to the border with Brazil. Getting there had not been easy. To avoid a 10-hour boat ride, they took a hair-raising flight to the nearest village, Versalles, where the plane had to circle back over a grass runway to avoid landing on a herd of grazing animals. Then came a long trek through thick rainforest, navigating over gnarled roots and past marauding armies of ants. "It's hot, it's humid, you're getting bitten constantly," says Robinson, a senior lecturer in archaeology at the University of Exeter.</p>
<p>The journey, however, was worth it. The researchers had an important mission: they were searching for "Amazonian dark earth" (ADE), sometimes known as "black gold" or terra preta.</p>
<p>This layer of charcoal-black soil, which can be up to <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/gea.21839">3.8m (12.5ft) thick</a>, is <a href="https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecs2.2035">found in patches across the Amazon basin</a>. It is intensely fertile – rich in decaying organic matter and nutrients essential for growing crops, such as <a href="https://link.springer.com/article/10.1007/s001140000193">nitrogen, potassium and phosphorus</a>. But unlike the thin, sandy soils typical of the rainforest, this layer was not deposited naturally – it was the work of ancient humans.</p>
<p>This rich soil is a relic from a very different time – an era when indigenous groups formed a thriving network of settlements across this rainforest world.&nbsp;</p>
<p>In January 2024, scientists announced the rediscovery of a long-vanished "garden" city. Hidden beneath the foliage of the rainforest in Ecuador's Upano valley was <a href="https://www.science.org/doi/10.1126/science.adi6317">a 2,000 year-old urban centre</a>, complete with plazas, streets and ceremonial platforms. (<em>Read more from BBC News about </em><a href="https://www.bbc.com/news/science-environment-67940671"><em>the lost city found in the Amazon</em></a><em>.</em>) The discovery has raised questions about whether there may be other ancient settlements concealed in the Amazon. And this is where ADE comes in.&nbsp;</p>
<p>It's thought that the garden city could only support so many people because of the region's fertile volcanic soil. But elsewhere in the Amazon, indigenous communities relied on ADE to improve the productivity of their land. Now there's growing interest in the lessons their methods may hold for societies today, from improving crop yields to beating climate change.</p></div></div><div id="future/article/20240116-the-dark-earth-revealing-the-amazons-secrets-p0h5bsnz"><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bsnz.webp" type="image/webp"><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bsnz.jpg" type="image/jpeg"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bsnz.webp" type="image/webp"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bsnz.jpg" type="image/jpeg"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bsnz.webp" type="image/webp"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bsnz.jpg" type="image/jpeg"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bsnz.webp" type="image/webp"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bsnz.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="Amazonian dark earth is packed with ancient artefacts, such as pottery and fossilised seeds (Credit: Mark Robinson)" src="https://ychef.files.bbci.co.uk/976x549/p0h5bsnz.jpg" alt="Amazonian dark earth is packed with ancient artefacts, such as pottery and fossilised seeds (Credit: Mark Robinson)" id=""></picture><div><p>Amazonian dark earth is packed with ancient artefacts, such as pottery and fossilised seeds (Credit: Mark Robinson)</p></div></div><div><p><strong>A hidden influence</strong></p>
<p>Surrounded by the smells and sounds of the rainforest at Versalles, in the remoteness of the Amazon, Robinson says it would be tempting to think that you're in a pristine wilderness. But this is not the case.</p>
<p>"The more we find out, [it becomes clear that] it's not necessarily primary forest," says Robinson. "Everywhere we look, although it seems like a really arduous trip to us, and that we're in the most remote place, we just find evidence of past communities everywhere."</p>
<p>In 2017, research revealed that <a href="https://www.science.org/doi/10.1126/science.aal0157#:~:text=Plants%20domesticated%20by%20pre%2DColumbian,and%20richness%20of%20domesticated%20species.">domesticated trees</a> are five times more likely to be dominant in the Amazon than non-domesticated ones – with more appearing the closer you get to ancient settlements. Though today many of the Amazon’s indigenous communities have vanished, wiped out by Western colonists and the diseases they carried, their farming practices continue to shape the rainforest.</p>
<p>Another crucial element of this hidden influence is ADE, which is widespread. "This is the fascinating thing – it really is pan-Amazonian, we are finding it everywhere," says Robinson.</p>
<p>This <a href="https://eos.org/features/the-nutrient-rich-legacy-in-the-amazons-dark-earths">precious layer</a> contains a potent <a href="https://acsess.onlinelibrary.wiley.com/doi/abs/10.2136/sssaspecpub63.2014.0035.5">blend of inorganic material</a>, including ash, pottery, bone and shells, together with organic matter such as food scraps, manure, and urine. It's simultaneously a treasure trove of ancient rubbish that's extremely exciting to archaeologists like Robinson, and a functional part of the Amazonian soil – one that continues to enrich the rainforest and allow indigenous communities to farm there today.</p></div><div id="future/article/20240116-the-dark-earth-revealing-the-amazons-secrets-p0h5bq99"><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bq99.webp" type="image/webp"><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bq99.jpg" type="image/jpeg"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bq99.webp" type="image/webp"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bq99.jpg" type="image/jpeg"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bq99.webp" type="image/webp"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bq99.jpg" type="image/jpeg"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bq99.webp" type="image/webp"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bq99.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="Amazonian dark earth has been estimated to cover 0.1%-10% of the Amazon basin (Credit: Getty Images)" src="https://ychef.files.bbci.co.uk/976x549/p0h5bq99.jpg" alt="Amazonian dark earth has been estimated to cover 0.1%-10% of the Amazon basin (Credit: Getty Images)" id=""></picture><div><p>Amazonian dark earth has been estimated to cover 0.1%-10% of the Amazon basin (Credit: Getty Images)</p></div></div><div><p>"They really are a goldmine," says Robinson. Along with fossilised seeds and ceramic artefacts dating back thousands of years, there are microscopic clues to what the rainforest may have been like thousands of years ago. One example is faecal spherulites: tiny <a href="https://www.sciencedirect.com/science/article/pii/S0305440317301681">crystals found in animal dung</a> that hint at the kinds of animals that once roamed across the landscape – and defecated in it.&nbsp;</p>
<p><strong>A living history</strong></p>
<p>ADE first piqued the interest of Westerners in the 1870s, when several scientists independently noticed black layers of soil that contrasted with the <a href="https://cdn.hackaday.io/files/20931895511904/Black%20Soil%20Green%20Rice.pdf">pale or reddish kind</a> that surrounded them. One early explorer <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=xGotEAAAQBAJ&amp;oi=fnd&amp;pg=PA129&amp;dq">described it as a</a> "fine, black loam", and noted that "strewn over it everywhere we find fragments of Indian pottery, so abundant in some places that they almost cover the ground".</p>
<p>However, how ADEs were created has been something of a mystery. Scientists have questioned whether these soils were produced by accident – the product of generations of indigenous people discarding rubbish – or via an intentional process to enrich the rainforest and make its ground more suitable for farming.</p>
<p>In 2023, an international team of scientists weighed in. By combining an analysis of the structure and composition of ADEs with observations of, and interviews with, the indigenous community at Kuikuro – in the southeastern Amazon, in central Brazil – the researchers concluded that these layers of soil were indeed <a href="https://www.science.org/doi/10.1126/sciadv.adh8499">made on purpose</a>.</p></div><div id="future/article/20240116-the-dark-earth-revealing-the-amazons-secrets-p0h5bttv"><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bttv.webp" type="image/webp"><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bttv.jpg" type="image/jpeg"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bttv.webp" type="image/webp"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bttv.jpg" type="image/jpeg"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bttv.webp" type="image/webp"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bttv.jpg" type="image/jpeg"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bttv.webp" type="image/webp"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bttv.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="While the Iténez rainforest is still inhabited today, much of the Amazon basin was depopulated after the arrival of Western colonists (Credit: Mark Robinson)" src="https://ychef.files.bbci.co.uk/976x549/p0h5bttv.jpg" alt="While the Iténez rainforest is still inhabited today, much of the Amazon basin was depopulated after the arrival of Western colonists (Credit: Mark Robinson)" id=""></picture><div><p>While the Iténez rainforest is still inhabited today, much of the Amazon basin was depopulated after the arrival of Western colonists (Credit: Mark Robinson)</p></div></div><div><p>The age and distribution of these soil deposits tell the story of the rise and fall of ancient indigenous civilisations across the Amazon. While the oldest layers of these black soils are around <a href="https://www.pnas.org/doi/10.1073/pnas.2022213118">5,000 years old</a>, "we see a lot more [evidence of ADEs being produced] about 4,000 years ago", says Robinson. "There's a lot more activity, a lot of cultural changes."</p>
<p>It's not until around 2,000 years ago, however, that they reach their peak, says Robinson. That's the average age of the black deposits that are found over a wide area across the Amazon basin. At this point, communities were larger and formed vast networks. However, the settlements where people produced ADE were typically not on the same scale as the recently rediscovered city in Ecuador.</p>
<p>One reason for this could be the power of ADE itself. Within the setting of an abundant jungle habitat, enriched by indigenous people with everything they need – fruiting trees and rich soil for growing crops – Robinson believes that there may have been no need for people to turn to larger-scale agriculture. "So [it's possible that] you don't really need the extra hierarchical level [that tends to develop in mass settlements]," says Robinson.</p>
<p>But by around 500 years ago, something is clearly very wrong. "That's when we really see it [ADE production] drop off," says Robinson.</p>
<p>This is thought to reflect the aftermath of Christopher Columbus' arrival on South American soil on 1 August 1498. When he plunged the red and gold flag of Spain into the ground on the Paria Peninsula in Venezuela, it marked the beginning of a "great dying". It's been estimated that 56 million indigenous people were killed across the Americas by 1600 – so many, it <a href="https://www.sciencedirect.com/science/article/pii/S0277379118307261">cooled the Earth</a>'s climate.</p></div><div id="future/article/20240116-the-dark-earth-revealing-the-amazons-secrets-p0h5byhz"><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5byhz.webp" type="image/webp"><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5byhz.jpg" type="image/jpeg"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5byhz.webp" type="image/webp"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5byhz.jpg" type="image/jpeg"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5byhz.webp" type="image/webp"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5byhz.jpg" type="image/jpeg"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5byhz.webp" type="image/webp"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5byhz.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="Taking inspiration from ancient methods such as Amazonian dark earth, today many companies are producing biochar as a way to lock away carbon and improve soil (Credit: Alamy)" src="https://ychef.files.bbci.co.uk/976x549/p0h5byhz.jpg" alt="Taking inspiration from ancient methods such as Amazonian dark earth, today many companies are producing biochar as a way to lock away carbon and improve soil (Credit: Alamy)" id=""></picture><div><p>Taking inspiration from ancient methods such as Amazonian dark earth, today many companies are producing biochar as a way to lock away carbon and improve soil (Credit: Alamy)</p></div></div><div><p><strong>A carbon sink</strong></p>
<p>Though many of the ancient inhabitants of the Amazon have long since vanished, their legacy remains. Intriguingly, not all of the ADEs they left behind have the same make-up – in fact, they vary widely, depending on the specific ingredients used in different locations.</p>
<p>"But the basic mechanism for creating the soils and enriching them seems to be similar," says Robinson. "They [indigenous people] are directly investing into the soils, starting with their own waste products," he says. The base is mostly composed of food scraps, with added faeces and charcoal. And it is the latter that is attracting increasing attention.</p>
<p>It turns out that not only are ADEs extraordinarily rich in nutrients, but they are powerful carbon sinks – with up to 7.5 times more carbon within <a href="https://www.css.cornell.edu/faculty/lehmann/research/terra%20preta/terrapretamain.html">compared to the surrounding soils</a>. As ADEs accumulate, the carbon becomes trapped underground, where it remains stable for hundreds of years – locking it away and delaying its entry into the atmosphere.</p>
<p>It's not clear why the carbon within ADEs behaves this way, but scientists suspect that it has something to do with "<a href="https://www.css.cornell.edu/faculty/lehmann/research/terra%20preta/terrapretamain.html">black carbon</a>", also known as "biochar". This key ingredient is made from organic material that has been turned to almost pure carbon at high temperatures, in the presence of little oxygen. The process doesn't emit as much carbon dioxide as charcoal production, but leads to a fine, crumbly black product that has been found in ADEs across the Amazon. (<a href="https://www.bbc.com/future/article/20200206-can-charcoal-cut-cows-methane-to-fight-climate-change"><em>Read more about biochar from the BBC</em></a>).</p></div><div id="future/article/20240116-the-dark-earth-revealing-the-amazons-secrets-p0h5bs2q"><picture><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bs2q.webp" type="image/webp"><source media="(min-width:1200px)" srcset="https://ychef.files.bbci.co.uk/1600x900/p0h5bs2q.jpg" type="image/jpeg"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bs2q.webp" type="image/webp"><source media="(min-width:880px)" srcset="https://ychef.files.bbci.co.uk/1280x720/p0h5bs2q.jpg" type="image/jpeg"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bs2q.webp" type="image/webp"><source media="(min-width:576px)" srcset="https://ychef.files.bbci.co.uk/976x549/p0h5bs2q.jpg" type="image/jpeg"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bs2q.webp" type="image/webp"><source media="(min-width:224px)" srcset="https://ychef.files.bbci.co.uk/624x351/p0h5bs2q.jpg" type="image/jpeg"><img loading="lazy" draggable="false" title="In the village of Versalles, the indigenous Itonama community still use the fertile Amazonian dark soil to grow crops (Credit: Mark Robinson)" src="https://ychef.files.bbci.co.uk/976x549/p0h5bs2q.jpg" alt="In the village of Versalles, the indigenous Itonama community still use the fertile Amazonian dark soil to grow crops (Credit: Mark Robinson)" id=""></picture><div><p>In the village of Versalles, the indigenous Itonama community still use the fertile Amazonian dark soil to grow crops (Credit: Mark Robinson)</p></div></div><div><p>Now businesses are attempting to capitalise on this ancient method, in a quest to help farmers to improve their soil and combat climate change at the same time. Take Carbon Gold, a company that produces biochar for use as an organic, peat-free planting aid. Founded in 2007 by the creator of a chocolate brand, the company based its methods on Mayan cacao farmers from Belize – who have also been using biochar for millennia.</p>
<p>In addition to locking away carbon, "biochar improves <a href="https://chembioagro.springeropen.com/articles/10.1186/s40538-020-00204-5">structure</a>, aeration, <a href="https://www.mdpi.com/2073-4395/12/2/311">water-holding capacity</a>, and <a href="https://www.nature.com/articles/s41467-017-01123-0">nutrient retention</a>" that can support healthy plant growth, says Sue Rawlings, the managing director at Carbon Gold. Today the company's clients include organic growers, gardeners, sports stadiums, premier league football clubs, major race and golf courses and Royal Parks and gardens in the UK, she says.</p>
<p>For his part, Robinson thinks copying the methods of ancient indigenous people in the Amazon is going to be essential for future generations. He points to the predictions that by 2050, <a href="https://www.jcu.edu.au/state-of-the-tropics/why-do-the-tropics-matter">around half of the world's population</a> will live in the tropics – with large amounts of migration into tropical forests.</p>
<p>"Finding ways for communities to be more sustainable in them, I think it's essential," he says. "And there are things we can learn from the past about this. I think we're just on the cusp of understanding this."</p>
<p>--</p>
<p><em>If you liked this story,&nbsp;</em><a href="https://cloud.email.bbc.com/SignUp10_08?&amp;at_bbc_team=studios&amp;at_medium=Onsite&amp;at_objective=acquisition&amp;at_ptr_name=bbc.com&amp;at_link_origin=featuresarticle&amp;at_campaign=essentiallist&amp;at_campaign_type=owned"><em>sign up for The Essential List newsletter</em></a><em>&nbsp;– a handpicked selection of features, videos and can't-miss news delivered to your inbox every Friday.</em></p>
<p><em>Join one million Future fans by liking us on </em><a href="https://www.facebook.com/BBCFuture/"><em>Facebook</em></a><em>, or follow us on </em><a href="https://twitter.com/BBC_Future"><em>Twitter</em></a><em> or </em><a href="https://www.instagram.com/bbcfuture_official/"><em>Instagram</em></a><em>.</em></p></div></div></article></div>;</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US developers can offer non-app store purchasing, Apple still collect commission (494 pts)]]></title>
            <link>https://www.macrumors.com/2024/01/16/us-app-store-alternative-purchase-option/</link>
            <guid>39020365</guid>
            <pubDate>Tue, 16 Jan 2024 22:58:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2024/01/16/us-app-store-alternative-purchase-option/">https://www.macrumors.com/2024/01/16/us-app-store-alternative-purchase-option/</a>, See on <a href="https://news.ycombinator.com/item?id=39020365">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2024/01/16/us-app-store-alternative-purchase-option/"><p>Apple is making major changes to its U.S. iOS <a href="https://www.macrumors.com/guide/app-store/">App Store</a> policies, and developers are now able to direct customers to a non-App Store purchasing option for digital goods. Apple is allowing apps to feature a single link to a developer website that leads to an in-app purchase alternative, but Apple plans to continue to collect a 12 to 27 percent commission on content bought this way.</p>
<p><img src="https://images.macrumors.com/t/MuYr2cyAEYdr0CMrrvuycUPK5vs=/400x0/article-new/2022/01/iOS-App-Store-General-Feature-JoeBlue.jpg?lossy" srcset="https://images.macrumors.com/t/MuYr2cyAEYdr0CMrrvuycUPK5vs=/400x0/article-new/2022/01/iOS-App-Store-General-Feature-JoeBlue.jpg?lossy 400w,https://images.macrumors.com/t/j81xjhvPhb1xAaD6jc-kW3SoaHc=/800x0/article-new/2022/01/iOS-App-Store-General-Feature-JoeBlue.jpg?lossy 800w,https://images.macrumors.com/t/6S1CCkPCfv7Bu5OKPv07871bKhY=/1600x0/article-new/2022/01/iOS-App-Store-General-Feature-JoeBlue.jpg 1600w,https://images.macrumors.com/t/2D83fOzXH1a-mo51oJaSX0SjmzQ=/2500x0/filters:no_upscale()/article-new/2022/01/iOS-App-Store-General-Feature-JoeBlue.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="iOS App Store General Feature JoeBlue" width="2250" height="1266"><br>Apple's update and the backstory that led to it are a bit complicated, but what <a href="https://www.macrumors.com/guide/iphone/">iPhone</a> and <a href="https://www.macrumors.com/roundup/ipad/">iPad</a> users need to know is that some apps in the U.S. storefront will soon feature a link to their website where subscriptions and other content can be purchased outside of the ‌App Store‌ in-app purchase system, likely with a discounted price.</p>
<p>Developers who want to offer this option will need to apply for a StoreKit External Purchase Link Entitlement, as Apple has outlined in both updated ‌App Store‌ Review Guidelines and the statement of compliance submitted to the Northern California U.S. District Court. With a Link Entitlement, a developer is able to direct a user to an out-of-app purchasing mechanism using an external purchase link. From Apple's modified ‌App Store‌ rules:<br>
</p>
<blockquote><p>Developers may apply for an entitlement to provide a link in their app to a website the developer owns or maintains responsibility for in order to purchase such items. Learn more about the entitlement. In accordance with the entitlement agreement, the link may inform users about where and how to purchase those in-app purchase items, and the fact that such items may be available for a comparatively lower price. The entitlement is limited to use only in the iOS or iPadOS App Store on the United States storefront. In all other storefronts, apps and their metadata may not include buttons, external links, or other calls to action that direct customers to purchasing mechanisms other than in-app purchase.</p>
<p>If your app engages in misleading marketing practices, scams, or fraud in relation to the entitlement, your app will be removed from the App Store and you may be removed from the Apple Developer Program.</p></blockquote>
<p>There are several requirements that developers need to adhere to maintain the privacy and security of the ‌App Store‌ ecosystem, and notably, Apple will collect a commission on purchases made using these Entitlement Links. Rather than 30 percent, Apple will collect a 27 percent fee on user purchases or year-one subscriptions made through the link. On the second year of a subscription, the commission fee drops to 12 percent, which is three percentage points lower than the 15 percent fee that Apple collects from second-year or longer subscriptions made through the in-app purchase system. Apps that participate in the ‌App Store‌ Small Business Program will be charged a 12 percent commission rate.</p>
<p>The commission will apply to transactions for digital goods and services that take place on a developers website within seven days after a user taps through an External Purchase Link to an external website. </p>
<p>Several key points about Entitlement Links are listed below.</p>
<ul>
<li>All links to outside purchasing methods must use the Entitlement Link system, and developers must apply and get Apple's approval.</li>
<li>Developers are permitted to have a single plain link on <strong>one</strong> screen of an app. The link can be at a sign-in screen, in user settings, or elsewhere, but it can only be in one place. The single location may not be an interstitial, modal, or pop-up.</li>
<li>The link can mention the specific price of content on a website, or that content is discounted on the website from the ‌App Store‌ price. Comparisons are allowed.</li>
<li>Links cannot be placed directly on an in-app purchase screen or in the in-app purchase flow.</li>
<li>Developers need to certify that the third-party payment service provider they are using for out-of-app purchasing meets industry standards for payment processors, and that they will offer users processes for managing subscriptions, requesting refunds, and disputing unauthorized transactions.</li>
<li>Apps that participate in Apple's Video Partner Program or News Partner Program are not eligible for Link Entitlement.</li>
<li>Apps that use the StoreKit External Purchase Link must continue to offer in-app purchases as an option.</li>
<li>‌App Store‌ pages are not able to include information about purchasing on a website or a link to a website.</li>
<li>Digital purchases that are sold on an app's website through the Entitlement Link must be available for use in that app.</li>
<li>The StoreKit External Purchase Link cannot discourage users from making in-app purchases or mimic an in-app purchase.</li>
<li>Links must open a new window in the default browser of the device, and are not able to open a web view.</li>
<li>No redirecting, intermediate links, or URL tracking parameters are allowed.</li>
<li>Developers are required to provide a periodic accounting of qualifying out-of-app purchases, and Apple has a right to audit developers' accounting to ensure compliance with their commission obligations and to charge interest and offset payments.</li>
</ul>
<p>The Link Entitlement process and the ‌App Store‌ changes are applicable only in the U.S. ‌App Store‌. Apps for all other storefronts are not able to include buttons, external links, or calls to action that direct customers to alternative purchasing options.</p>
<p><img src="https://images.macrumors.com/t/fPe_AZQfPf2CAhnY60W6NBJ1tBA=/400x0/article-new/2024/01/apple-external-link-examples.jpg?lossy" srcset="https://images.macrumors.com/t/fPe_AZQfPf2CAhnY60W6NBJ1tBA=/400x0/article-new/2024/01/apple-external-link-examples.jpg?lossy 400w,https://images.macrumors.com/t/Zhg-_b1Z5Havuv-49n69jJbaTXk=/800x0/article-new/2024/01/apple-external-link-examples.jpg?lossy 800w,https://images.macrumors.com/t/xJdNK9YgbXaKeoLrQArGAG_xxD0=/1600x0/article-new/2024/01/apple-external-link-examples.jpg 1600w,https://images.macrumors.com/t/0BqXjjWqbh3lgY14hXDIAoMftOg=/2500x0/filters:no_upscale()/article-new/2024/01/apple-external-link-examples.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="apple external link examples" width="1635" height="920" data-old-src="https://images.macrumors.com/images-new/1x1.trans.gif" data-src="https://images.macrumors.com/t/fPe_AZQfPf2CAhnY60W6NBJ1tBA=/400x0/article-new/2024/01/apple-external-link-examples.jpg?lossy" data-srcset="https://images.macrumors.com/t/fPe_AZQfPf2CAhnY60W6NBJ1tBA=/400x0/article-new/2024/01/apple-external-link-examples.jpg?lossy 400w,https://images.macrumors.com/t/Zhg-_b1Z5Havuv-49n69jJbaTXk=/800x0/article-new/2024/01/apple-external-link-examples.jpg?lossy 800w,https://images.macrumors.com/t/xJdNK9YgbXaKeoLrQArGAG_xxD0=/1600x0/article-new/2024/01/apple-external-link-examples.jpg 1600w,https://images.macrumors.com/t/0BqXjjWqbh3lgY14hXDIAoMftOg=/2500x0/filters:no_upscale()/article-new/2024/01/apple-external-link-examples.jpg 2500w"><em></em></p><p><em>Examples of how Entitlement Links can be used in apps</em></p><p>Apple will provide an in-app warning to customers to let them know that they are leaving the ‌App Store‌ ecosystem to make a purchase on an external website and that ‌App Store‌ protections will not be available.</p>
<p><img src="https://images.macrumors.com/t/luUzlhnSjO9bR4tK8OwC1qc0z6c=/400x0/article-new/2024/01/apple-external-app-store-warning.jpg?lossy" srcset="https://images.macrumors.com/t/luUzlhnSjO9bR4tK8OwC1qc0z6c=/400x0/article-new/2024/01/apple-external-app-store-warning.jpg?lossy 400w,https://images.macrumors.com/t/VUmHxkAoOP8GY3Aobo_o6Q9PILc=/800x0/article-new/2024/01/apple-external-app-store-warning.jpg?lossy 800w,https://images.macrumors.com/t/gb-PABu893tLV5VX96yj86mCD0M=/1600x0/article-new/2024/01/apple-external-app-store-warning.jpg 1600w,https://images.macrumors.com/t/PHds6_lxLxrM2XJyaUe5x7Mux88=/2500x0/filters:no_upscale()/article-new/2024/01/apple-external-app-store-warning.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="apple external app store warning" width="2030" height="1142" data-old-src="https://images.macrumors.com/images-new/1x1.trans.gif" data-src="https://images.macrumors.com/t/luUzlhnSjO9bR4tK8OwC1qc0z6c=/400x0/article-new/2024/01/apple-external-app-store-warning.jpg?lossy" data-srcset="https://images.macrumors.com/t/luUzlhnSjO9bR4tK8OwC1qc0z6c=/400x0/article-new/2024/01/apple-external-app-store-warning.jpg?lossy 400w,https://images.macrumors.com/t/VUmHxkAoOP8GY3Aobo_o6Q9PILc=/800x0/article-new/2024/01/apple-external-app-store-warning.jpg?lossy 800w,https://images.macrumors.com/t/gb-PABu893tLV5VX96yj86mCD0M=/1600x0/article-new/2024/01/apple-external-app-store-warning.jpg 1600w,https://images.macrumors.com/t/PHds6_lxLxrM2XJyaUe5x7Mux88=/2500x0/filters:no_upscale()/article-new/2024/01/apple-external-app-store-warning.jpg 2500w"><br>According to Apple's statement filed with the court, the requirements surrounding links are aimed at minimizing "fraud, scams, and confusion," while also providing developers with an opportunity to "entice users to other platforms" and give customers a choice between non-App Store purchasing and in-app purchases.</p>
<p>The changes today stem from Apple's 2021 legal battle with <a href="https://www.macrumors.com/guide/epic-games/">Epic Games</a>. Apple won the dispute and the court did not find that Apple had violated U.S. antitrust law, but Apple was at the time ordered to remove "anti-steering" rules preventing developers from informing customers about alternatives to in-app purchases. That order has been on hold <a href="https://www.macrumors.com/2023/07/17/app-store-rule-change-epic-games-delayed/">during the appeals process</a>, but the appeals process ended today.</p>
<p>Both Apple and ‌Epic Games‌ <a href="https://www.macrumors.com/2023/09/28/apple-epic-appeal-supreme-court/">had appealed</a> to the United States Supreme Court, but the Supreme Court <a href="https://www.macrumors.com/2024/01/16/supreme-court-declines-to-hear-apple-vs-epic-case/">declined to hear the case</a>. That means the initial ruling and the appeals court ruling that agreed with it are permanent, and Apple now has to comply with the part of that order that required it to change the ‌App Store‌ rules.</p>
<p>The anti-steering rule was two-pronged, requiring Apple to allow for links to in-app purchase alternatives and to allow developers to communicate with customers outside of the ‌App Store‌ through email and other contact information collected in the app. The outside communication part of the order was already satisfied with a change that Apple made to the ‌App Store‌ rules in 2021 to <a href="https://www.macrumors.com/2021/08/26/app-store-changes-developer-lawsuit-settlement/">settle a class-action developer lawsuit</a>.</p>
<p>Apple has already been allowing developers to use communication methods like email to inform customers about payment methods available outside of iOS apps, and Apple makes it clear in its messaging today that there are no limits on developers' out-of-app communications with users. The full statements that Apple provided to the court have been obtained by <em>MacRumors</em> and can be read below. </p>


<p><b>Update:</b> ‌Epic Games‌ CEO Tim Sweeney criticized Apple's ‌App Store‌ changes and said that Epic plans to contest Apple's "bad-faith compliance plan" in District Court. </p>
<div>
<blockquote data-lang="en" data-script="//platform.twitter.com/widgets.js">
<p lang="en" dir="ltr">A quick summary of glaring problems we've found so far:
1) Apple has introduced an anticompetitive new 27% tax on web purchases. Apple has never done this before, and it kills price competition. Developers can't offer digital items more cheaply on the web after paying a… <a href="https://t.co/YkHuapG7xa">pic.twitter.com/YkHuapG7xa</a>
— Tim Sweeney (@TimSweeneyEpic) <a href="https://twitter.com/TimSweeneyEpic/status/1747408148799881390?ref_src=twsrc%5Etfw">January 16, 2024</a></p></blockquote>
</div><br>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2024/01/13/new-ios-features-coming-2024/">Apple Plans to Release These 8 New iOS Features This Year</a></h3><p>The calendar has turned to 2024, and there are many new iOS 17 and iOS 18 features that are expected to launch throughout the year. Below, we have recapped eight new iOS features expected in 2024, including Stolen Device Protection, collaborative Apple Music playlists, AirPlay on hotel room TVs, app sideloading in the EU, next-generation CarPlay, roadside assistance via satellite outside of...</p></div><div><h3><a href="https://www.macrumors.com/2024/01/15/apple-vision-pro-virtual-keyboard-criticism/">Apple Vision Pro Virtual Keyboard Blasted As 'Complete Write-Off'</a></h3><p>Monday January 15, 2024 3:25 am PST by <a href="https://www.macrumors.com/author/tim-hardwick/" rel="author">Tim Hardwick</a></p><p>Anyone paying attention to Apple's Vision Pro headset unveiling at WWDC 2023 will have seen its virtual keyboard demo. The keyboard floats in mid-air, allowing you to input text in your spatial computing environment while wearing the device. However, anyone planning to ditch their physical keyboard may want to hold onto it for a little while longer: According to Bloomberg's Mark Gurman, the ...</p></div><div><h3><a href="https://www.macrumors.com/2024/01/15/app-store-to-be-split-in-two/">App Store to Be 'Split in Two' Ahead of EU iPhone Sideloading Deadline</a></h3><p>Apple is preparing to split the App Store "in two" in the coming weeks ahead of European Union requirements that will force Apple to enable app sideloading in the region, Bloomberg's Mark Gurman reports. In the latest edition of his "Power On" newsletter, Gurman explained that Apple is gearing up to make changes to the App Store in the EU to comply with the region's impending Digital Markets ...</p></div><div><h3><a href="https://www.macrumors.com/2024/01/10/ios-17-3-coming-soon/">iOS 17.3 Coming Soon With These Two New Features for Your iPhone</a></h3><p>iOS 17.3 has been in beta testing since mid-December, and the upcoming software update includes two new features for the iPhone so far. Apple seeded the third beta of iOS 17.3 this week. The update should be coming soon, with a release likely later this month. Below, we provide additional details about the new features in iOS 17.3 so far. Stolen Device Protection Earlier this year, T...</p></div><div><h3><a href="https://www.macrumors.com/2024/01/12/iphone-16-capture-button-leak/">iPhone 16 Leak Reveals This All-New Button</a></h3><p>Following the addition of the Action button on the iPhone 15 Pro models, all iPhone 16 models may feature yet another all-new button. Apple plans to add a so-called "Capture" button to all iPhone 16 models, according to pre-production information obtained by MacRumors. The button would be located below the power button on the right side of the device, where the mmWave antenna window is...</p></div><div><h3><a href="https://www.macrumors.com/2024/01/16/here-are-all-the-new-m3-apple-macs-expected-in-2024/">Here Are All the New M3 Apple Macs Expected This Year</a></h3><p>Tuesday January 16, 2024 2:26 am PST by <a href="https://www.macrumors.com/author/tim-hardwick/" rel="author">Tim Hardwick</a></p><p>Apple in 2023 launched an M3-powered 24-inch iMac, as well as new 14-inch and 16-inch MacBook Pro models with M3 series chips. But the rest of Apple's Mac lineup is still to be updated to the latest M3 processors. Now that 2023 is over, attention naturally turns to the other Macs in the company's lineup and where they fit into Apple's M3 roadmap for the year ahead. Here's what the latest...</p></div><div><h3><a href="https://www.macrumors.com/2024/01/14/iphone-16-and-16-plus-8gb-ram-rumor/">iPhone 16 and iPhone 16 Plus Rumored to Feature Increased 8GB RAM and Wi-Fi 6E Support</a></h3><p>Apple's next-generation iPhone 16 and iPhone 16 Plus models will both feature 8GB of RAM, an increase over the 6GB of RAM in the iPhone 15 and iPhone 15 Plus, according to information shared today by technology analyst Jeff Pu. In a research note with investment firm Haitong International Securities, Pu reiterated his belief that all iPhone 16 models will be equipped with 8GB of RAM....</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fluorite lenses: Corrective capabilities beyond ordinary optical glass (257 pts)]]></title>
            <link>https://global.canon/en/c-museum/special/exhibition2.html</link>
            <guid>39020258</guid>
            <pubDate>Tue, 16 Jan 2024 22:50:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://global.canon/en/c-museum/special/exhibition2.html">https://global.canon/en/c-museum/special/exhibition2.html</a>, See on <a href="https://news.ycombinator.com/item?id=39020258">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

          <h3>Fluorite lenses: Corrective capabilities beyond the limits of ordinary optical glass</h3>

          <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-1.jpg" alt="Fluorite lenses: Corrective capabilities beyond the limits of ordinary optical glass"></p><p>One crucial material that supports the high image quality characteristic of Canon lenses is fluorite, which is a crystallized form of calcium fluoride. It has long been known that using a fluorite lens in conjunction with a
            glass lens can reduce chromatic aberration to extremely low levels. However, natural fluorite occurs in small sizes that are suitable only for use in small optical equipment such as the object lenses of microscopes. Canon, in its pursuit
            of progress in imaging capabilities, was keen on utilizing fluorite in its photography lenses, set out to develop its own technology for forming large, high-purity artificial fluorite crystals using fluorite ore as a raw material. In May
            1969, the <a href="https://global.canon/en/c-museum/product/fl117.html">FL-F300mm f/5.6</a>—the world’s first consumer telephoto lens to employ fluorite lens elements—was released.</p>

          
          <h4>How fluorite lens elements correct chromatic aberration</h4>
          <p>You might have noticed the outlines of your subjects tinted in a way that resembles a color fringe. This is chromatic aberration, and it can also take the form of a haziness throughout the entire image. As this prevents the subject
              from being rendered accurately, it needs to be corrected so that the image quality is sharp, clear, and faithful to the scene.</p>
          <div>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-2.jpg" alt="The purple fringing at the edges if these branches is a form of chromatic aberration."></p><p>The purple fringing at the edges if these branches is a form of chromatic aberration.</p>
          </div>
          
          <p>Chromatic aberration happens because when light passes through a glass surface, the different-colored waves within it (red, green, blue, etc.) are refracted at different angles due to their different lengths, with each
            color converging at a different focal point. Such aberration is usually corrected by using a combination of concave and convex lenses, which refract the light in opposite directions to each other.</p>
          <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-3_en.jpg" alt="Chromatic aberration correction using concave and convex glass lenses">
          </p>
          <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-4_en.jpg" alt="Chromatic aberration correction using concave and convex glass lenses">
          </p>
          
          <p><strong>Chromatic aberration correction using concave and convex glass lenses</strong></p>
          <p>However, it is not possible to correct the chromatic aberration on all wavelengths by using ordinary glass. The chromatic aberration, which occurs on certain wavelengths such as red, is called residual chromatic
            aberration. As the refractive index of the wavelengths differ depending on the type of optical glass, there are limitations to how much residual chromatic aberration can be mitigated depending on the combination and properties of the
            optical glass. </p>
          <p>This is where fluorite comes in handy. As fluorite is a fundamentally different material than conventional optical glass, it can be used in combination with glass to correct chromatic aberration more effectively. It is
            particularly effective on telephoto lenses, where the long focal length exacerbates chromatic aberration.</p>

          <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-5_en.jpg" alt="Chromatic aberration correction using a convex fluorite lens and a concave glass lens">
          </p>
          <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-6_en.jpg" alt="Chromatic aberration correction using a convex fluorite lens and a concave glass lens">
          </p>
          
          <p><strong>Chromatic aberration correction using a convex fluorite lens and a concave glass lens</strong></p>
          <p>Canon’s fluorite lens elements incorporate natural fluorite as a raw material, endowing the lenses with low-refractive, low-dispersion properties not possible with glass lenses. Fluorite lenses are also unique in their
            extraordinary partial dispersion tendencies: the red to green wavelengths are dispersed with the same tendencies as glass, but the green to blue wavelengths are dispersed more than glass. Using a convex fluorite lens element alongside a
            high-dispersion glass concave lens element therefore eliminates residual chromatic aberration, making possible a lens that produces clear, sharp, and high-quality images.</p>
          <div>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-7_en.png" alt="Refraction and dispersion">
            </p>
            <div>
              <h5><strong>Refraction and dispersion</strong></h5>
              <p>‘Refraction’ is the phenomenon in which light changes direction when it passes through the surface of a material such as glass. The degree of the directional change is called the ‘refractive index’. As the refractive
                index varies depending on the color constituents (wavelengths) of the light, each color bends in a different direction. This is known as color dispersion. On optical glass, dispersion occurs at a fixed proportion regardless of the
                wavelength, whereas on fluorite, dispersion occurs at different proportions for different wavelengths and is known as ‘extraordinary partial dispersion’. </p>
            </div>
            
          </div>
          
          <h4>The emergence of fluorite lenses and how they improve the image quality of telephoto lenses</h4>
          <p>Fluorite lenses transcend traditional limitations to reduce chromatic aberration to an extremely low level. These lenses have their origins the Canon F Project, which started in August 1966. Canon’s lens developers strongly
            believed that to create a lens that was performed better than existing lenses, it was first necessary to create a new material, and it was this conviction that drive them to establish the technology for producing artificial fluorite
            crystals to use in camera lenses.</p>
          <p>The challenge in producing artificial fluorite crystals lay in the crystallization. The term “glass” was originally used to describe the state of a material. Being non-crystalline, it consists of atoms fixed in random
            positions--simply melting glass allows it to be processed into different shapes. On the other hand, fluorite is a crystalline substance, and its constituent atoms must be arranged in a specific configuration for it to crystalize. </p>
          <p>To pulverize natural fluorite, purify it, and then restore the exact atomic structure to recrystallize it into a size large enough for application in a camera lens is a near-impossible feat. Therefore, the developers had to
            ensure a precisely controlled vacuum environment where the temperature is kept at least 1,000℃, so they designed an apparatus to artificially produce large crystals with high purity In 1968, two years after the F project started, Canon
            finally managed to overcome these obstacles to successfully form artificial fluorite crystals large enough to be used in camera lenses. </p>
          <div>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-8.jpg" alt="natural fluorite crystals">
            </p>
            <p>The stones on the left are natural fluorite crystals. They are green and purple due to impurities within the crystals. At the middle is an artificial fluorite crystal ingot produced by Canon.<br>When heated, natural
                fluorite glows with a dreamy light that resembles that of fireflies, which is said to be the reason behind its Japanese name, <i>hotaru ishi</i>, literally “firefly stone”. Just like how fireflies require clean water to live, only
                pure fluorite can “shine” as photography lenses.</p>
            
          </div>
          
          <p>Another challenge in fluorite lens production is its polishing. As fluorite is softer and more delicate than glass, the same methods used for polishing glass are not suitable. Therefore, Canon developed a special technology
            for polishing fluorite lenses that requires up to four times longer than the time needed to polish ordinary optical glass. This technology was successfully commercialized the following year, in May 1969.
          </p>
          <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-9.jpg" alt="The first lens to employ fluorite lens elements was the FL-F300mm f/5.6 (released in 1969). The bright green line, meant to evoke the image of the glow of fluorite, indicates that the lens features a fluorite lens element."></p><p>The first lens to employ fluorite lens elements was the FL-F300mm f/5.6 (released in 1969). The bright green line, meant to evoke the image of the glow of fluorite, indicates that the lens features a fluorite
            lens element.</p>
          <p>The FL-F300mm f/5.6, whose successful commercialization was achieved by transcending many challenges, was highly acclaimed for is vivid, high-contrast rendering, and became widely used in such professions as
            photojournalism. </p>
          <p>Canon subsequently improved on is high temperature vacuum, temperature control, and polishing technologies, enabling the use of fluorite lens elements in many more lenses. Canon remains committed to pursuing the highest
            image quality for its telephoto lenses. </p>

          <h4>The fluorite lens production process</h4>
          <p>While the grinding and polishing processes may seem identical for all kinds of optical glass, each stage of the fluorite lens production process requires slow, meticulous attention to detail.</p>
          <div>

            <p><b>1. Raw materials</b><br>The raw material for fluorite lenses is naturally occurring fluorite ore.</p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-10.jpg" alt="1. Raw materials">
            </p>
            
            <p><b>2. Pulverization and refinement</b><br>The raw fluorite is pulverized and refined to remove impurities before being poured into a graphite crucible that does not melt easily.</p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-11.jpg" alt="2. Pulverization and refinement">
            </p>
            
            <p><b>3. Crystallization </b><br>The crucible is placed in a crystal-growing apparatus with a heater on top and heated to 1,400℃. After the raw fluorite has melted, the crucible is gradually lowered, allowing crystallization to occur
                starting from the bottom of the crucible.</p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-12_en.png" alt="3. Crystallization ">
            </p>
            
            <p><b>4. Annealing</b><br>The annealing process removes strains that occur inside the crystals formed. Strain that lead to cracks are removed by heating the crystals to a high temperature insufficient to melt them, and then slowly
                cooling them to room temperature over a long period of several weeks.</p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-13.jpg" alt="4. Annealing">
            </p>
            
            <p><b>5. Trimming and rough processing</b><br>The unnecessary parts of the crystal surface are trimmed off, and the crystal is rough-processed to the required size. The interior of the crystal is inspected for anomalies. </p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-14.jpg" alt="5. Trimming and rough processing">
            </p>
            
            <p><b>6. Grinding</b><br>The top and bottom surfaces of the crystal are ground into a spherical shape with a surface that resembles frosted glass.</p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-15.jpg" alt="6. Grinding">
            </p>
            
            <p><b>7. Polishing</b><br>The surfaces of the crystal are polished with a pellet made from coagulated polish until they are semi-transparent and meet the specified dimensions. Finally, a special polish is used to remove fine scratches.
              </p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-16.jpg" alt="7. Polishing">
            </p>
            
            <p><b>8. Vapor deposition</b><br>Coating material is heat-evaporated under high vacuum conditions up to one-millionth to one-hundred millionth of one unit of atmospheric pressure. This forms a thin film over the polished lens. </p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-17_en.png" alt="8. Vapor deposition">
            </p>
            
            <p><b>9. Completion</b><br>An experienced technician inspects purity using an interferometer. Only lens elements that pass the inspection are sent to be assembled in lenses. </p>
            <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-18.jpg" alt="9. Completion">
            </p>
            
          </div>
          <h4>Lenses that employ fluorite lens elements <span>(as at May 2021)</span></h4>
          <div>
            <p>Since the FL-F300mm, Canon has produced 39 more lenses that employ fluorite lens elements. As fluorite lens elements not only correct chromatic aberration but also contribute to reducing the size and weight of products,
                they are proactively used in large telephoto lenses.<br>These lenses are much beloved by many photographers who demand high image quality at super telephoto focal lengths, including not only professional sports photographers and
                photojournalists, but also enthusiasts who photograph subjects such as wild birds, trains and aircraft.</p>
            <div>
              <p><img src="https://global.canon/ja/c-museum/common/img/special/exhibition2-19.jpg" alt="RF600mm F4 L IS USM (released in 2021)"></p><p>RF600mm F4 L IS USM (released in 2021)</p>
            </div>

            
          </div>
          <h4>RF lenses that employ fluorite lens elements <span>(as at May 2021)</span></h4>
          


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stable Code 3B: Coding on the Edge (248 pts)]]></title>
            <link>https://stability.ai/news/stable-code-2024-llm-code-completion-release</link>
            <guid>39019532</guid>
            <pubDate>Tue, 16 Jan 2024 21:40:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stability.ai/news/stable-code-2024-llm-code-completion-release">https://stability.ai/news/stable-code-2024-llm-code-completion-release</a>, See on <a href="https://news.ycombinator.com/item?id=39019532">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1705434376921_5483">
  <p>Today, we announce our first Large Language Model release of 2024: <a href="https://huggingface.co/stabilityai/stable-code-3b" target="_blank"><span>Stable Code 3B</span></a>. This new LLM is a follow-up to our previously released <a href="https://stability.ai/news/stablecode-llm-generative-ai-coding"><span>Stable Code Alpha 3B</span></a> and the first major Stable Code release, offering a new state-of-the-art model designed for code completion with multiple additional capabilities.&nbsp;</p><p>Compared to CodeLLaMA 7b, Stable Code 3B is 60% smaller while featuring a similar high-level performance across programming languages. Based on our pre-existing <a href="https://huggingface.co/stabilityai/stablelm-3b-4e1t" target="_blank"><span>Stable LM 3B</span></a> foundational model trained on 4 trillion tokens of natural language data, Stable Code was further trained on software engineering-specific data, including code. The model's compact size allows it to be run privately on the edge in real-time on modern laptops, even those without a dedicated GPU.</p><p>Stable Code 3B offers more features and significantly better performance across multiple languages with additional benefits such as support for Fill in the Middle capabilities (FIM) and expanded context size. Stable Code as a base is trained on sequences of up to 16,384 tokens but follows a similar approach to CodeLlama with the implementation of Rotary Embeddings, optionally allowing modification of the rotary base up to 1,000,000, further expanding the model’s context length up to 100k tokens.</p><p>Stable Code is trained on 18 programming languages (selected based on the <a href="https://survey.stackoverflow.co/2023/#section-most-popular-technologies-programming-scripting-and-markup-languages" target="_blank"><span>2023 StackOverflow Developer Survey</span></a>) and demonstrates state-of-the-art performance (compared to models of similar size) on the MultiPL-E metrics across multiple programming languages tested.</p><p><strong>Performance Comparison</strong></p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1705430802082_7238">
  <p><strong>Training Insights</strong></p><p>Our training pipeline consists of a multi-stage process similar to Codellama. We start with an LM pre-trained on natural language data, in this case, <a href="https://huggingface.co/stabilityai/stablelm-3b-4e1t" target="_blank"><span>StableLM-3B-4e1t</span></a>, followed up with unsupervised fine-tuning on multiple code and code-related datasets, including CommitPack, GitHub Issues, StarCoder &amp; other Math datasets. In the second step, we further fine-tune the model with longer sequences of 16,384 tokens with the base modification suggested in CodeLLama. The new stable-code model also supports Flash Attention 2 and is available for use.</p><p>Further references to the data and model can be found in our <a href="https://huggingface.co/stabilityai/stable-code-3b" target="_blank"><span>model card</span></a>. We will release a full technical report with additional details and ablations to be more transparent and open to the community.</p><p><strong>Commercial Applications</strong></p><p>Stay updated on our progress by signing up for our newsletter, and learn more about commercial applications by contacting us here.&nbsp;</p><p>Follow us on <a href="https://twitter.com/stabilityai" target="_blank"><span>Twitter</span></a>, <a href="https://www.instagram.com/stability.ai/" target="_blank"><span>Instagram</span></a>, <a href="https://www.linkedin.com/company/66318622/"><span>LinkedIn</span></a>, and join our <a href="https://discord.gg/stablediffusion"><span>Discord Community</span></a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Post-mortem for last week's incident at Kagi (241 pts)]]></title>
            <link>https://status.kagi.com/issues/2024-01-12-kagi-down-on-some-regions/</link>
            <guid>39019119</guid>
            <pubDate>Tue, 16 Jan 2024 21:04:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://status.kagi.com/issues/2024-01-12-kagi-down-on-some-regions/">https://status.kagi.com/issues/2024-01-12-kagi-down-on-some-regions/</a>, See on <a href="https://news.ycombinator.com/item?id=39019119">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><small>January 12, 2024 at 4:30 PM UTC</small></p><p><small><a href="https://status.kagi.com/affected/us-east4/">us-east4</a>
<a href="https://status.kagi.com/affected/us-west2/">us-west2</a>
<a href="https://status.kagi.com/affected/europe-west2/">europe-west2</a>
<a href="https://status.kagi.com/affected/asia-east2/">asia-east2</a>
<a href="https://status.kagi.com/affected/us-central1/">us-central1</a>
<a href="https://status.kagi.com/affected/europe-west4/">europe-west4</a>
<a href="https://status.kagi.com/affected/asia-southeast1/">asia-southeast1</a>
<a href="https://status.kagi.com/affected/australia-southeast1/">australia-southeast1</a>
<a href="https://status.kagi.com/affected/southamerica-east1/">southamerica-east1</a></small></p><p><strong>Resolved after
6h
50m of downtime.</strong>
<span>January 12, 2024 at 11:20 PM UTC</span></p><hr><p><em>Investigating</em> - We are experiencing issues following a deploy. Team is working on resolving this..
<span>(16:45 UTC — Jan 12)</span></p><p><em>Monitoring</em> - We have are reverting a configuration change that we believe to be the culprit, and are continuing to monitor as service is coming back to full health.
<span>(18:30 UTC — Jan 12)</span></p><p><em>Update</em> - In order to fully restore stability we will need to pause traffic momentarily. We will be redirecting users to this page while we restore load to the service in a controlled manner. We will follow up with further details as the situation progresses.
<span>(20:26 UTC — Jan 12)</span></p><p><em>Monitoring</em> - Traffic has been restored and we are continuing to monitor the service as it comes back to full health.
<span>(21:14 UTC — Jan 12)</span></p><p><em>Resolved</em> - All services are now operating as normal. Thank you for your patience while we resolved this issue.</p><h2 id="post-mortem">Post-Mortem</h2><p>Hi all,</p><p>This is Zac, the Tech Lead at Kagi.
I’m going to be sharing below a more in-depth post-mortem of our service interruption last week.
Assisting me in responding to this incident was Seth, one of our senior engineers, and Luan, our DevOps engineer.</p><p>This will be fairly technical, you can skip to “Next Steps” for takeaways.
The summary is that we were the target of some actors misusing the service, exploting a pathological case in our infrastructure, and we immediately released mitigations and are working on improvements in several areas of our code and communications.</p><h2 id="timeline">Timeline</h2><p>On January 12th, approx. 5:30PM UTC, the team became aware of an infrastructure issue occurring by way of our internal monitoring and user reports of issues.
The nature of the issue was causing slow loading or complete page timeouts for users in various regions.</p><p>This incident unfortunately took us quite some time to resolve - we deeply thank our users for their patience, and the opportunity to give some background as to what was going on, and how we plan to move forward.</p><p>At first, by what turned out to be a complete coincidence, the incident occurred at precisely the same time that we were performing an infrastructure upgrade to our VMs with additional RAM resources.
Our monitoring was reporting both high latency and issues with our application’s database connection pool.
While no code changes were part of this upgrade, and “old” instances were reporting some issues as well, we decided the first course of action was to revert this change.</p><p>This revert completed at around 6:50 PM, but as we continued to monitor the issue persisted.
Meanwhile, we had been inspecting the behavior of our application’s database connection pools, which were saturated with connections to our primary DB instance.
It was unclear what the exact cause of this was yet, but what was clear is that the total number of connections being established globally to our primary exceeded its maximum configured connection limit.</p><p>Our next move was to evaluate if we had somehow caught ourselves in a “spiral” of exhausting our maximum connections, wherein any instance that we would replace would simply get its connection pool exhausted again, queuing for access to the primary.
In several steps we tried replacing a few instances to see what effect reducing the congestion would have.
We also were making progress on evaluating various parts of the databases internal health and query performance.</p><p>With mild signs that cycling some instances was helping, at 9:30PM UTC we decided to pause all user traffic by redirecting users to our status page to give the database a break and completely reset all connection pools in one shot.
We installed the redirect and issued a restart of all nodes.
Once all appeared stable, we started letting traffic back in again.
Unfortunately, the issue persisted.</p><p>While looking at the database state, it became clear to our engineers that the root cause was in fact high contention on rows in the users table.
This contention caused a steep increase in write latency, which in turn put backpressure on our application’s connection pool, causing it to eventually exhaust all available connections as writes were taking too long to complete.
The writes were all stemming from one instance that would eventually starve the rest of our global instances of access to our primary, thus causing disruption in all other regions.</p><p>This didn’t exactly come as a surprise to us, as for the entirety of Kagi’s life so far we have actually used the cheapest, single-core database available to us on GCP!
To this day we’ve yet to exceed 50% load capacity on it, which we’ve worked hard to achieve.
This has always carried the risk of the database being relatively easy to knock over, but we have so far kept load and latency under control with redundancy, distributed read-replicas, and high scrutiny over the SQL we write.</p><p>We then took steps to identify the bad actors, where we found accounts created within 24hrs and over 60,000 searches performed in short time period from a single user facing account.
While we do offer unlimited searches to our users, such volume was a clear abuse of our platform, against our terms of use.
As such, we removed searching capability from the offending accounts.</p><p>At the same time, we issued a hotfix that disabled the particular writes that were causing high contention, in addition to some upgrades of our database driver, which included several relevant bug fixes to connection pool health.
This would help ensure that immediately the same pathological case could not be exploited again.</p><p>By midnight, the issue was fully resolved.
The team continued to closely monitor for any signals that the actors were returning, but they did not.</p><p>We were later in contact with an account that we blocked who claimed they were using their account to perform automated scraping of our results, which is not something our terms allow for.</p><h2 id="next-steps">Next Steps</h2><p>There’s a lot we took away from this incident, and we have immediate plans already in motion to make our system more robust to this type of abuse, as well as our communication processes around incidents.</p><p>First, regretfully we were not very prompt in updating our status page.
We owe it to our customers to be quick and transparent about any issues going on that might affect access to the product that they pay for.
To address this, we are moving to a status page platform to one that will more easily allow us expose some of our automated internal monitoring to users.
This way users have an idea of the platform’s health in real-time, even if our small team of engineers has their hands full to immediately post an update (which was not a very fast process to begin with, even if we were on top of it).
We should have this available by next week.</p><p>Secondly, we have directly mitigated the queries causing issues under load.
With this issue in mind, we are also running load tests to learn about any other similar deficiencies that may still exist, and what to avoid in the future.
We are also installing some additional monitoring to more quickly point us to the right place in our infra, and hopefully not waste as much time chasing a false-flag as we did this time.</p><p>Lastly, we are performing a re-upping of our systems set up to detect this kind of abuse of our terms, which were clearly too lax.
Besides any potential performance impact, this vector also directly costs us money as we pay for each search.
To protect our finances, and all of our subscribers continued access to unlimited (organic) searches, we need to set some automated limits to help us enforce this.
From analyzing our user’s usage, we have picked some limits that no good-faith user of Kagi should reasonably hit.</p><p>These new limits should already be in place by the time of this post, and we will monitor their impact and continue to tune them as needed.
If you believe you find yourself wrongly unable to access Kagi, please reach out to <a href="mailto:support@kagi.com">support@kagi.com</a>.</p><p>Thank you so much for bearing with us through this incident!
Please look forward to a more robust service as we implement these things, and as usual, more features &amp; improvements are on their way.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[6174 (405 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/6174</link>
            <guid>39018769</guid>
            <pubDate>Tue, 16 Jan 2024 20:35:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/6174">https://en.wikipedia.org/wiki/6174</a>, See on <a href="https://news.ycombinator.com/item?id=39018769">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div lang="en" dir="ltr" id="mw-content-text"><p>The number <b>6174</b> is known as <b>Kaprekar's constant</b><sup id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup><sup id="cite_ref-Kaprekar1955_2-0"><a href="#cite_note-Kaprekar1955-2">[2]</a></sup><sup id="cite_ref-Kaprekar1980_3-0"><a href="#cite_note-Kaprekar1980-3">[3]</a></sup> after the <a href="https://en.wikipedia.org/wiki/India" title="India">Indian</a> <a href="https://en.wikipedia.org/wiki/Mathematician" title="Mathematician">mathematician</a> <a href="https://en.wikipedia.org/wiki/D._R._Kaprekar" title="D. R. Kaprekar">D. R. Kaprekar</a>. This number is renowned for the following rule:
</p>
<ol><li>Take any four-digit number, using at least two different digits (leading zeros are allowed).</li>
<li>Arrange the digits in descending and then in ascending order to get two four-digit numbers, adding leading zeros if necessary.</li>
<li>Subtract the smaller number from the bigger number.</li>
<li>Go back to step 2 and repeat.</li></ol>
<p>The above process, known as <a href="https://en.wikipedia.org/wiki/Kaprekar%27s_routine" title="Kaprekar's routine">Kaprekar's routine</a>, will always reach its <a href="https://en.wikipedia.org/wiki/Fixed_point_(mathematics)" title="Fixed point (mathematics)">fixed point</a>, 6174, in at most 7 iterations.<sup id="cite_ref-mathworld_4-0"><a href="#cite_note-mathworld-4">[4]</a></sup> Once 6174 is reached, the process will continue yielding 7641 – 1467 = 6174. For example, choose 1459:
</p>
<div>
<ul><li>9541 – 1459 = 8082</li>
<li>8820 – 0288 = 8532</li>
<li>8532 – 2358 = 6174</li>
<li>7641 – 1467 = <b>6174</b></li></ul>
</div><p>
The only four-digit numbers for which Kaprekar's routine does not reach 6174 are <a href="https://en.wikipedia.org/wiki/Repdigit" title="Repdigit">repdigits</a> such as 1111, which give the result <a href="https://en.wikipedia.org/wiki/0_(number)" title="0 (number)">0000</a> after a single iteration. All other four-digit numbers eventually reach 6174 if leading zeros are used to keep the number of digits at 4. For numbers with three identical numbers and a fourth number that is one number higher or lower (such as 2111), it is essential to treat 3-digit numbers with a leading zero; for example: 2111 – 1112 = 0999; 9990 – 999 = 8991; 9981 – 1899 = 8082; 8820 – 288 = 8532; 8532 – 2358 = 6174.<sup id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup></p><table><tbody><tr><th colspan="2"><table><tbody><tr>
<td>← 6173</td>
<td>6174</td>
<td>6175 →</td>
</tr></tbody></table></th></tr><tr><td colspan="2"><div><ul><li><a href="https://en.wikipedia.org/wiki/List_of_numbers" title="List of numbers">List of numbers</a></li><li><a href="https://en.wikipedia.org/wiki/Integer" title="Integer">Integers</a></li></ul></div><div><p><a href="https://en.wikipedia.org/wiki/Negative_number" title="Negative number">←</a> <a href="https://en.wikipedia.org/wiki/0" title="0">0</a> <a href="https://en.wikipedia.org/wiki/1000_(number)" title="1000 (number)">1k</a> <a href="https://en.wikipedia.org/wiki/2000_(number)" title="2000 (number)">2k</a> <a href="https://en.wikipedia.org/wiki/3000_(number)" title="3000 (number)">3k</a> <a href="https://en.wikipedia.org/wiki/4000_(number)" title="4000 (number)">4k</a> <a href="https://en.wikipedia.org/wiki/5000_(number)" title="5000 (number)">5k</a> <a href="https://en.wikipedia.org/wiki/6000_(number)" title="6000 (number)">6k</a> <a href="https://en.wikipedia.org/wiki/7000_(number)" title="7000 (number)">7k</a> <a href="https://en.wikipedia.org/wiki/8000_(number)" title="8000 (number)">8k</a> <a href="https://en.wikipedia.org/wiki/9000_(number)" title="9000 (number)">9k</a> <a href="https://en.wikipedia.org/wiki/10000_(number)" title="10000 (number)">→</a></p></div></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Cardinal_numeral" title="Cardinal numeral">Cardinal</a></th><td>six thousand one hundred seventy-four</td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Ordinal_numeral" title="Ordinal numeral">Ordinal</a></th><td>6174th<br>(six thousand one hundred seventy-fourth)</td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Factorization" title="Factorization">Factorization</a></th><td>2 × 3<sup>2</sup> × 7<sup>3</sup></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Divisor" title="Divisor">Divisors</a></th><td>1, 2, 3, 6, 7, 9, 14, 18, 21, 42, 49, 63, 98, 126, 147, 294, 343, 441, 686, 882, 1029, 2058, 3087, 6174</td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Greek_numerals" title="Greek numerals">Greek numeral</a></th><td>,ϚΡΟΔ´</td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Roman_numerals" title="Roman numerals">Roman numeral</a></th><td><span>V</span>MCLXXIV, or <span>VI</span>CLXXIV</td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Binary_number" title="Binary number">Binary</a></th><td>1100000011110<sub>2</sub></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Ternary_numeral_system" title="Ternary numeral system">Ternary</a></th><td>22110200<sub>3</sub></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Senary" title="Senary">Senary</a></th><td>44330<sub>6</sub></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Octal" title="Octal">Octal</a></th><td>14036<sub>8</sub></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Duodecimal" title="Duodecimal">Duodecimal</a></th><td>36A6<sub>12</sub></td></tr><tr><th scope="row"><a href="https://en.wikipedia.org/wiki/Hexadecimal" title="Hexadecimal">Hexadecimal</a></th><td>181E<sub>16</sub></td></tr></tbody></table>
<meta property="mw:PageProp/toc">
<h2><span id="Other_.22Kaprekar.27s_constants.22"></span><span id="Other_&quot;Kaprekar's_constants&quot;">Other "Kaprekar's constants"</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=6174&amp;action=edit&amp;section=1" title="Edit section: Other &quot;Kaprekar's constants&quot;"><span>edit</span></a><span>]</span></span></h2>

<p>There can be analogous fixed points for digit lengths other than four; for instance, if we use 3-digit numbers, then most sequences (i.e., other than repdigits such as 111) will terminate in the value <a href="https://en.wikipedia.org/wiki/495_(number)" title="495 (number)">495</a> in at most 6 iterations. Sometimes these numbers (495, 6174, and their counterparts in other digit lengths or in bases other than 10) are called "Kaprekar constants".
</p>
<h2><span id="Other_properties">Other properties</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=6174&amp;action=edit&amp;section=2" title="Edit section: Other properties"><span>edit</span></a><span>]</span></span></h2>
<ul><li>6174 is a 7-<a href="https://en.wikipedia.org/wiki/Smooth_number" title="Smooth number">smooth number</a>, i.e. none of its prime factors are greater than 7.</li>
<li>6174 can be written as the sum of the first three powers of 18:
<ul><li>18<sup>3</sup> + 18<sup>2</sup> + 18<sup>1</sup> = 5832 + 324 + 18 = 6174, and coincidentally, 6 + 1 + 7 + 4 = 18.</li></ul></li>
<li>The sum of squares of the prime factors of 6174 is a square:
<ul><li>2<sup>2</sup> + 3<sup>2</sup> + 3<sup>2</sup> + 7<sup>2</sup> + 7<sup>2</sup> + 7<sup>2</sup> = 4 + 9 + 9 + 49 + 49 + 49 = 169 = 13<sup>2</sup></li></ul></li></ul>
<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=6174&amp;action=edit&amp;section=3" title="Edit section: References"><span>edit</span></a><span>]</span></span></h2>
<div><ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span><cite id="CITEREFNishiyama2006"><a href="https://en.wikipedia.org/wiki/Yutaka_Nishiyama" title="Yutaka Nishiyama">Nishiyama, Yutaka</a> (March 2006). <a rel="nofollow" href="http://plus.maths.org/issue38/features/nishiyama/index.html">"Mysterious number 6174"</a>. <i><a href="https://en.wikipedia.org/wiki/Plus_Magazine" title="Plus Magazine">Plus Magazine</a></i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Plus+Magazine&amp;rft.atitle=Mysterious+number+6174&amp;rft.date=2006-03&amp;rft.aulast=Nishiyama&amp;rft.aufirst=Yutaka&amp;rft_id=http%3A%2F%2Fplus.maths.org%2Fissue38%2Ffeatures%2Fnishiyama%2Findex.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A6174"></span></span>
</li>
<li id="cite_note-Kaprekar1955-2"><span><b><a href="#cite_ref-Kaprekar1955_2-0">^</a></b></span> <span><cite id="CITEREFKaprekar_DR1955">Kaprekar DR (1955). "An Interesting Property of the Number 6174". <i><a href="https://en.wikipedia.org/wiki/Scripta_Mathematica" title="Scripta Mathematica">Scripta Mathematica</a></i>. <b>15</b>: 244–245.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Scripta+Mathematica&amp;rft.atitle=An+Interesting+Property+of+the+Number+6174&amp;rft.volume=15&amp;rft.pages=244-245&amp;rft.date=1955&amp;rft.au=Kaprekar+DR&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A6174"></span></span>
</li>
<li id="cite_note-Kaprekar1980-3"><span><b><a href="#cite_ref-Kaprekar1980_3-0">^</a></b></span> <span><cite id="CITEREFKaprekar_DR1980">Kaprekar DR (1980). "On Kaprekar Numbers". <i>Journal of Recreational Mathematics</i>. <b>13</b> (2): 81–82.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Recreational+Mathematics&amp;rft.atitle=On+Kaprekar+Numbers&amp;rft.volume=13&amp;rft.issue=2&amp;rft.pages=81-82&amp;rft.date=1980&amp;rft.au=Kaprekar+DR&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A6174"></span></span>
</li>
<li id="cite_note-mathworld-4"><span><b><a href="#cite_ref-mathworld_4-0">^</a></b></span> <span><a href="https://en.wikipedia.org/wiki/Kaprekar%27s_routine#CITEREFHanover2017" title="Kaprekar's routine">Hanover 2017</a>, p. 1, Overview.</span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span><cite><a rel="nofollow" href="https://www.cut-the-knot.org/Curriculum/Arithmetic/Kaprekar.shtml">"Kaprekar's Iterations and Numbers"</a>. <i>www.cut-the-knot.org</i><span>. Retrieved <span>2022-09-21</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.cut-the-knot.org&amp;rft.atitle=Kaprekar%27s+Iterations+and+Numbers&amp;rft_id=https%3A%2F%2Fwww.cut-the-knot.org%2FCurriculum%2FArithmetic%2FKaprekar.shtml&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A6174"></span></span>
</li>
</ol></div>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=6174&amp;action=edit&amp;section=4" title="Edit section: External links"><span>edit</span></a><span>]</span></span></h2>

<ul><li><cite id="CITEREFBowley">Bowley, Roger. <a rel="nofollow" href="https://www.youtube.com/watch?v=d8TRcZklX_Q">"6174 is Kaprekar's Constant"</a>. <i>Numberphile</i>. <a href="https://en.wikipedia.org/wiki/University_of_Nottingham" title="University of Nottingham">University of Nottingham</a>: <a href="https://en.wikipedia.org/wiki/Brady_Haran" title="Brady Haran">Brady Haran</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Numberphile&amp;rft.atitle=6174+is+Kaprekar%27s+Constant&amp;rft.aulast=Bowley&amp;rft.aufirst=Roger&amp;rft_id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dd8TRcZklX_Q&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3A6174"></span></li>
<li><a rel="nofollow" href="http://www.panix.com/~baldwin/kaprekar.pl">Sample (Perl) code to walk any four-digit number to Kaprekar's Constant</a></li>
<li><a rel="nofollow" href="http://www.panix.com/~baldwin/kaprekar.py">Sample (Python) code to walk any four-digit number to Kaprekar's Constant</a></li>
<li><a rel="nofollow" href="https://gist.github.com/dakonr/10865d113dd2ca6a7dd3d3fcdb169c13#file-kaprekar_constant-c">Sample (C) code to walk the first 10000 numbers and their steps to Kaprekar's Constant</a></li></ul>
<!-- 
NewPP limit report
Parsed by mw1364
Cached time: 20240108183432
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.261 seconds
Real time usage: 0.360 seconds
Preprocessor visited node count: 1434/1000000
Post‐expand include size: 26512/2097152 bytes
Template argument size: 1448/2097152 bytes
Highest expansion depth: 14/100
Expensive parser function count: 3/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 18080/5000000 bytes
Lua time usage: 0.156/10.000 seconds
Lua memory usage: 4900527/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  334.287      1 -total
 47.53%  158.872      1 Template:Infobox_number
 30.39%  101.591      1 Template:Infobox_number/box
 29.60%   98.944      1 Template:Infobox
 28.58%   95.525      1 Template:Reflist
 22.44%   75.016      3 Template:Cite_web
 16.07%   53.705      1 Template:Short_description
 14.14%   47.272      1 Template:Commons_category
 13.68%   45.723      1 Template:Infobox_number/range
 13.49%   45.097      1 Template:Sister_project
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:52724119-0!canonical and timestamp 20240108183431 and revision id 1186034556. Rendering was triggered because: page-view
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Web AI Model Testing: WebGPU, WebGL, and Headless Chrome (175 pts)]]></title>
            <link>https://developer.chrome.com/blog/supercharge-web-ai-testing</link>
            <guid>39017607</guid>
            <pubDate>Tue, 16 Jan 2024 19:16:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.chrome.com/blog/supercharge-web-ai-testing">https://developer.chrome.com/blog/supercharge-web-ai-testing</a>, See on <a href="https://news.ycombinator.com/item?id=39017607">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  
    




<div>
        
          <p><img alt="François Beaufort" src="https://web.dev/images/authors/beaufortfrancois.jpg" decoding="async" height="64" loading="lazy" width="64"></p>
      </div>

<p>Great news! You've built a cool <a href="https://goo.gle/made-with-tfjs">Web AI application</a> 
that runs machine learning models directly on a user's device. It runs entirely
on the client-side web browser, without relying on the cloud. This on-device
design enhances user privacy, boosts performance, and reduces costs
significantly.</p>

<p>However, there's a hurdle. Your
<a href="https://www.tensorflow.org/js">TensorFlow.js</a> model can operate on
both CPUs (WebAssembly) and more powerful GPUs (through
<a href="https://developer.mozilla.org/docs/Web/API/WebGL_API">WebGL</a> and
<a href="https://developer.chrome.com/blog/webgpu-release">WebGPU</a>). The question is:
<strong>how can you consistently automate browser testing with the selected hardware?</strong></p>

<p>Maintaining consistency is crucial for comparing machine learning model
performance over time as you iterate and improve them, prior to deployment for
real-world users to use on their device.</p>

<p>Setting up a consistent testing environment with GPUs can be harder than
expected. In this blog post, we'll share the problems we faced and how we solved
them, so you can improve your application's performance.</p>

<p>This isn't just for Web AI developers! If you're working on web gaming or
graphics, this post is valuable for you, too.</p>

<h2 id="what's-automation" data-text="What's in our automation toolbox" tabindex="-1">What's in our automation toolbox</h2>

<p>Here is what we are using:</p>

<ul>
<li><strong>Environment</strong>: A Linux-based Google Colab
<a href="https://colab.google/notebooks/">notebook</a> connected to an NVIDIA
T4 or V100 GPU. You can use other cloud platforms, such as Google Cloud
(GCP), if preferred.</li>
<li><strong>Browser</strong>: Chrome supports <a href="https://developer.chrome.com/docs/web-platform/webgpu">WebGPU</a>,
a powerful <a href="https://developer.chrome.com/blog/from-webgl-to-webgpu">successor to WebGL</a>, that
brings the advancements of modern GPU APIs to the web.</li>
<li><strong>Automation</strong>: <a href="https://developer.chrome.com/docs/puppeteer">Puppeteer</a> is a Node.js library that lets
you control browsers programmatically with JavaScript. With Puppeteer, we can
automate Chrome in headless mode, which means the browser runs without a
visible interface, on a server. We are using the improved
<a href="https://developer.chrome.com/docs/chromium/new-headless">new headless mode</a>, not the
<a href="https://developer.chrome.com/blog/headless-chrome">legacy</a> form.</li>
</ul>



<h2 id="verify-environment" data-text="Verify the environment" tabindex="-1">Verify the environment</h2>

<p>The best way to check whether hardware acceleration is turned on in Chrome is to
type <code translate="no" dir="ltr">chrome://gpu</code> into the address bar. You can
programmatically <a href="https://github.com/jasonmayes/headless-chrome-nvidia-t4-gpu-support/blob/main/examples/puppeteer/jPuppet.js">perform the equivalent with Puppeteer</a> 
with <code translate="no" dir="ltr">console.log</code> or save the full report as PDF to check manually:</p>
<pre translate="no" dir="ltr"><code translate="no" dir="ltr">/* Incomplete example.js */
import puppeteer from 'puppeteer';

// Configure launch parameters: Expands later
const browser = await puppeteer.launch({
  headless: 'new',
  args:  ['--no-sandbox']
});

const page = await browser.newPage();
await page.goto('chrome://gpu');

// Verify: log the WebGPU status or save the GPU report as PDF
const txt = await page.waitForSelector('text/WebGPU');
const status = await txt.evaluate(g =&gt; g.parentElement.textContent);
console.log(status);
await page.pdf({ path: './gpu.pdf' });

await browser.close();
</code></pre>
<p>Open <code translate="no" dir="ltr">chrome://gpu</code> and you should have the following results:</p>

<table>
  <tbody><tr>
    <th colspan="2">Graphics feature status</th>
  </tr>
  <tr>
    <td>OpenGL:</td>
    <td><span>Disabled</span></td>
  </tr>
  <tr>
    <td>Vulkan:</td>
    <td><span>Disabled</span></td>
  </tr>
  <tr>
    <td>WebGL:</td>
    <td><span>Software only, hardware acceleration unavailable.</span></td>
  </tr>
  <tr>
    <td>WebGL2:</td>
    <td><span>Software only, hardware acceleration unavailable.</span></td>
  </tr>
  <tr>
    <td>WebGPU:</td>
    <td><span>Disabled</span></td>
  </tr>
  <tr>
    <td colspan="2">
      <p><b>Problems detected.</b><br>WebGPU has been disabled via blocklist or the command line.</p>
    </td>
  </tr>
</tbody></table>

<p>Not a great start. It's fairly clear that hardware detection was failing.
<strong>WebGL, WebGL2, and WebGPU are essentially disabled or software only</strong>. We
aren't alone in this problem - there are numerous discussions online of people
in a similar situation, including on the official Chrome support channels
(<a href="https://bugs.chromium.org/p/chromium/issues/detail?id=1459930">1</a>),
(<a href="https://bugs.chromium.org/p/chromium/issues/detail?id=765284">2</a>).</p>

<h2 id="enable-webgpu" data-text="Enable WebGPU and WebGL support" tabindex="-1">Enable WebGPU and WebGL support</h2>

<p>By default, Headless Chrome
<a href="https://bugs.chromium.org/p/chromium/issues/detail?id=1416283">disables GPU</a>.
To enable it on Linux, apply all of the following flags when launching Headless
Chrome:</p>

<ul>
<li><strong><code translate="no" dir="ltr">--no-sandbox</code></strong> flag disables <a href="https://developer.chrome.com/docs/puppeteer/troubleshooting#set_up_a_chrome_linux_sandbox">Chrome's security sandbox</a>, which isolates the
browser process from the rest of the system. Running Chrome as root without
this sandbox is not supported.</li>
<li><strong><code translate="no" dir="ltr">--headless=new</code></strong> flag runs Chrome with the new and improved
<a href="https://developer.chrome.com/docs/chromium/new-headless">headless mode</a>, without any visible UI.</li>
<li><strong><code translate="no" dir="ltr">--use-angle=vulkan</code></strong> flag tells Chrome to use the
<a href="https://chromium.googlesource.com/angle/angle/+/HEAD/src/libANGLE/renderer/vulkan/README.md">Vulkan backend</a> 
for <a href="https://chromium.googlesource.com/angle/angle/">ANGLE</a>, which
translates OpenGL ES 2/3 calls to Vulkan API calls.</li>
<li><strong><code translate="no" dir="ltr">--enable-features=Vulkan</code></strong> flag enables Vulkan graphics backend for
compositing and rasterization in Chrome.</li>
<li><strong><code translate="no" dir="ltr">--disable-vulkan-surface</code></strong> flag disables the <code translate="no" dir="ltr">VK_KHR_surface</code> vulkan
instance extension. Instead of using a swapchain,
<a href="https://en.wikipedia.org/wiki/Bit_blit">Bit blit</a> is used for the
present render result on screen.</li>
<li><strong><code translate="no" dir="ltr">--enable-unsafe-webgpu</code></strong> flag enables the experimental WebGPU API in
Chrome on Linux and disables the adapters blocklist.</li>
</ul>

<p>Now we combine all the changes we have made so far. Here is the complete script.</p>
<pre translate="no" dir="ltr"><code translate="no" dir="ltr">/* Complete example.js */
import puppeteer from 'puppeteer';

// Configure launch parameters
const browser = await puppeteer.launch({
  headless: 'new',
  args: [
    '--no-sandbox',
    '--headless=new',
    '--use-angle=vulkan',
    '--enable-features=Vulkan',
    '--disable-vulkan-surface',
    '--enable-unsafe-webgpu',
  ]
});

const page = await browser.newPage();
await page.goto('chrome://gpu');

// Verify: log the WebGPU status or save the GPU report as PDF
const txt = await page.waitForSelector('text/WebGPU');
const status = await txt.evaluate(g =&gt; g.parentElement.textContent);
console.log(status);
await page.pdf({path: './gpu.pdf'});

await browser.close();
</code></pre>
<p>Run the script again. No WebGPU problems are detected and the value changes from
disabled to software only.</p>

<table>
  <tbody><tr>
    <th colspan="2">Graphics feature status</th>
  </tr>
  <tr>
    <td>OpenGL:</td>
    <td><span>Disabled</span></td>
  </tr>
  <tr>
    <td>Vulkan:</td>
    <td><span>Disabled</span></td>
  </tr>
  <tr>
    <td>WebGL:</td>
    <td><span>Software only, hardware acceleration unavailable.</span></td>
  </tr>
  <tr>
    <td>WebGL2:</td>
    <td><span>Software only, hardware acceleration unavailable.</span></td>
  </tr>
  <tr>
    <td>WebGPU:</td>
    <td><span>Software only, hardware acceleration unavailable.</span></td>
  </tr>
</tbody></table>

<p>However, hardware acceleration is still unavailable, the NVIDIA T4 GPU isn't
detected.</p>

<h2 id="install-drivers" data-text="Install the correct GPU drivers" tabindex="-1">Install the correct GPU drivers</h2>

<p>We investigated more closely the output of <code translate="no" dir="ltr">chrome://gpu</code>, with some GPU experts
on the Chrome team. We found issues with the <a href="https://github.com/googlecolab/colabtools/issues/3556#issuecomment-1499397023">default drivers installed on the
Linux Colab</a> 
instance, causing issues with Vulkan, leading to Chrome unable to detect the
NVIDIA T4 GPU at the <code translate="no" dir="ltr">GL_RENDERER</code> level as shown in the following output. This
causes problems with Headless Chrome.</p>

<table>
  <caption>The default output doesn't detect NVIDIA T4 GPU.</caption>
  <tbody><tr>
    <th colspan="2">Driver information</th>
  </tr>
  <tr>
    <td>GL_RENDERER</td>
    <td>ANGLE (Google, Vulkan 1.3.0 (SwiftShader Device (Subzero) (0x0000C0DE)), SwiftShader driver-5.0.0)</td>
  </tr>
</tbody></table>

<p>Installing the correct drivers that were compatible therefore fixes the issue.</p>

<table>
  <caption>Updated output after drivers are installed.</caption>
  <tbody><tr>
    <th colspan="2">Driver information</th>
  </tr>
  <tr>
    <td>GL_RENDERER</td>
    <td>ANGLE (NVIDIA Corporation, Tesla T4/PCIe/SSE2, OpenGL ES 3.2 NVIDIA 525.105.17)</td>
  </tr>
</tbody></table>

<p>To install the correct drivers, run the following commands during setup. The
last two lines help you to log the outputs of what NVIDIA drivers detects along
with <code translate="no" dir="ltr">vulkaninfo</code>.</p>

<pre translate="no" dir="ltr"><code translate="no" dir="ltr">apt-get install -y vulkan-tools libnvidia-gl-525
</code>
<code translate="no" dir="ltr">// Verify the NVIDIA drivers detects along with vulkaninfo</code>
<code translate="no" dir="ltr">nvidia-smi</code>
<code translate="no" dir="ltr">vulkaninfo --summary</code>
</pre>

<p>Now run the script again and we get the following result. 🎉</p>

<table>
  <tbody><tr>
    <th colspan="2">Graphics feature status</th>
  </tr>
  <tr>
    <td>OpenGL:</td>
    <td><span>Enabled</span></td>
  </tr>
  <tr>
    <td>Vulkan:</td>
    <td><span>Enabled</span></td>
  </tr>
  <tr>
    <td>WebGL:</td>
    <td><span>Hardware accelerated but at reduced performance.</span></td>
  </tr>
  <tr>
    <td>WebGL2:</td>
    <td><span>Hardware accelerated but at reduced performance.</span></td>
  </tr>
  <tr>
    <td>WebGPU:</td>
    <td><span>Hardware accelerated but at reduced performance.</span></td>
  </tr>
</tbody></table>

<p>By using the correct drivers and flags when running Chrome, we now have WebGPU
and WebGL support using the shiny, new headless mode.</p>

<h2 id="investigation" data-text="Behind the scenes: Our team's investigation" tabindex="-1">Behind the scenes: Our team's investigation</h2>

<p>After much research, we didn't find working methods for the environment we
needed to execute in Google Colab, although there were some
<a href="https://mirzabilal.com/how-to-enable-hardware-acceleration-on-chrome-chromium-puppeteer-on-aws-in-headless-mode">hopeful posts</a> 
that worked in other environments, which was promising. Ultimately, we weren't
able to replicate their success in the Colab NVIDIA T4 environment, as we had 2
key issues:</p>

<ol>
<li>Some combinations of flags allow detection of the GPU, but don't allow you to
actually use the GPU.</li>
<li>Examples of working solutions by third parties used the old Chrome headless
version, which at some point will be deprecated in favor of the
<a href="https://developer.chrome.com/docs/chromium/new-headless">new version</a>. We needed a solution
that worked with the new Headless Chrome to be better future proofed.</li>
</ol>

<p>We confirmed the under utilization of the GPU by running an
<a href="https://tensorflowjs-fashion-mnist-classifier.glitch.me/">example TensorFlow.js web page for image recognition</a>,
whereby we trained a model to recognize clothing samples (sort of like a "hello
world" of machine learning).</p>

<p>On a regular machine, 50 training cycles (known as epochs) should run in less
than 1 second each. Calling Headless Chrome in its default state, we could log
the JavaScript console output to the Node.js server-side command line to see how
fast these training cycles were actually taking.</p>

<p>As expected, each training epoch took much longer than expected (several
seconds), which suggests Chrome has fallen back to plain old JS CPU execution
instead of utilizing the GPU:</p>

<figure>
  <img src="https://developer.chrome.com/blog/supercharge-web-ai-testing/image/initial_epoch_timing.gif" alt="The training epochs move at a slower cadence.">
  <figcaption><b>Figure 1</b>: Real-time capture showing how long each training epoch took to execute (seconds).</figcaption>
</figure>

<p>After fixing the drivers and using the right combination of flags for Headless
Chrome, rerunning the TensorFlow.js training example results in much faster
training epochs.</p>

<figure>
  <img src="https://developer.chrome.com/blog/supercharge-web-ai-testing/image/epoch_timing_with_headless.gif" alt="There's an increase in speed for epochs..">
  <figcaption><b>Figure 2</b>: Real-time capture showing the speed up of epochs.</figcaption>
</figure>

<h2 id="summary" data-text="Summary" tabindex="-1">Summary</h2>

<p><a href="https://www.youtube.com/watch?v=r7hOoCY6uGo">Web AI has grown exponentially</a> 
since its creation in 2017. With browser technologies such as WebGPU, WebGL, and
<a href="https://webassembly.org/">WebAssembly</a>, a machine learning model's
mathematical operations can be further accelerated on the client side.</p>

<p>As of 2023 TensorFlow.js and MediaPipe Web crossed over 1 billion downloads of
models and libraries—a historic milestone and a sign of how web
developers and engineers are shifting to embrace <a href="https://goo.gle/made-with-tfjs">AI in their next generation
web apps to make some truly incredible solutions</a>.</p>

<p>With great success in usage comes great responsibility. At this level of usage
in production systems, the need arises for testing client-side, browser-based AI
models in a true browser environment, while also being scalable, automatable,
and within a known standardized hardware setup.</p>



<p>By harnessing the combined power of the new Headless Chrome and Puppeteer, you
can confidently test such workloads in a standardized and replicable
environment, ensuring consistent and reliable results.</p>

<h2 id="wrap" data-text="Wrap up" tabindex="-1">Wrap up</h2>

<p>A <a href="https://developer.chrome.com/docs/web-platform/webgpu/colab-headless">step-by-step guide</a> is available in
our documentation, so you can try out the complete setup yourself.</p>

<p>If you found this useful, drop a shout out over on
<a href="https://www.linkedin.com/in/WebAI">LinkedIn</a>,
<a href="https://twitter.com/jason_mayes">X (formerly Twitter)</a>, or whatever
social network you use using hashtag <strong>#WebAI</strong>. It would be great to hear any
feedback you have so we know to write more stuff like this in the future.</p>

<p><a href="https://github.com/jasonmayes/headless-chrome-nvidia-t4-gpu-support">Add a star on the Github repo</a> 
to receive any future updates.</p>

<h2 id="acknowledgements" data-text="Acknowledgements" tabindex="-1">Acknowledgements</h2>

<p>A huge thank you to everyone on the Chrome team who helped debug the driver and
WebGPU issues we faced in this solution, with a special thanks to
<a href="https://jec.fish/">Jecelyn Yeen</a> and
<a href="https://heyawhite.com/">Alexandra White</a> for helping to wordsmith
this blog post. Thanks to Yuly Novikov, Andrey Kosyakov, and
<a href="https://mastodon.online/@orkon">Alex Rudenko</a> who were instrumental
in creating the final, working solution.</p>

  

  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Infowars and Goop sell the same exact pseudoscientific "wellness" products (162 pts)]]></title>
            <link>https://qz.com/1010684/all-the-wellness-products-american-love-to-buy-are-sold-on-both-infowars-and-goop</link>
            <guid>39017350</guid>
            <pubDate>Tue, 16 Jan 2024 19:01:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qz.com/1010684/all-the-wellness-products-american-love-to-buy-are-sold-on-both-infowars-and-goop">https://qz.com/1010684/all-the-wellness-products-american-love-to-buy-are-sold-on-both-infowars-and-goop</a>, See on <a href="https://news.ycombinator.com/item?id=39017350">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>There are two Americas, we’ve been told.</p><p><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.nytimes.com/interactive/2016/12/26/upshot/duck-dynasty-vs-modern-family-television-maps.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.nytimes.com/interactive/2016/12/26/upshot/duck-dynasty-vs-modern-family-television-maps.html" target="_blank" rel="noopener noreferrer">There’s</a></span> <em>Duck Dynasty</em> America and <em>Modern Family</em> America.<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://qz.com/836813&quot;,{&quot;metric25&quot;:1}]]" href="https://qz.com/836813" target="_blank" rel="noopener noreferrer"> There’s</a></span> “gosh” America and “dope” America. Sometimes, though, Americans unite around a common idea. Like the healing powers of eleuthero root, cordyceps mushrooms, and “nascent iodine.”</p><p>Near the end of <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.nytimes.com/2017/05/25/magazine/how-amanda-chantal-bacon-perfected-the-celebrity-wellness-business.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.nytimes.com/2017/05/25/magazine/how-amanda-chantal-bacon-perfected-the-celebrity-wellness-business.html" target="_blank" rel="noopener noreferrer">a profile</a></span> of Amanda Chantal Bacon, founder of the “wellness” brand Moon Juice, the <em>New York Times Magazine</em> noted that many of the alternative-medicine ingredients in her products are sold—with very different branding—on the Infowars store. That’s the site run by Alex Jones, the radio show host and conspiracy theorist who<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.washingtonpost.com/lifestyle/style/how-alex-jones-conspiracy-theorist-extraordinaire-got-donald-trumps-ear/2016/11/17/583dc190-ab3e-11e6-8b45-f8e493f06fcd_story.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.washingtonpost.com/lifestyle/style/how-alex-jones-conspiracy-theorist-extraordinaire-got-donald-trumps-ear/2016/11/17/583dc190-ab3e-11e6-8b45-f8e493f06fcd_story.html" target="_blank" rel="noopener noreferrer"> has said</a></span> that both the shooting at Sandy Hook Elementary School and the Boston Marathon bombing were staged. Moon Juice is<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://shop.goop.com/shop/collection/brands/moon-juice&quot;,{&quot;metric25&quot;:1}]]" href="https://shop.goop.com/shop/collection/brands/moon-juice" target="_blank" rel="noopener noreferrer"> frequently recommended</a></span> by Gwyneth Paltrow’s wellness blog, Goop; &nbsp;it’s a favorite of Hollywood celebrities and others who can afford things like $25 “<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.moonjuiceshop.com/products/cashews&quot;,{&quot;metric25&quot;:1}]]" href="https://www.moonjuiceshop.com/products/cashews" target="_blank" rel="noopener noreferrer">activated cashews</a></span>.” Infowars, on the other hand, is a dark corner of the American right, heavy on guns, light on government intervention, and still very mad at Obama.</p><p>We at Quartz have created a compendium, from Ashwagandha to zizyphus, of the magical healing ingredients both sides of the political spectrum are buying, and how they are presented to each. We looked at the ingredients used in products sold on the<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/" target="_blank" rel="noopener noreferrer"> Infowars store</a></span>, and compared them to products on the wellness shops<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.moonjuiceshop.com/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.moonjuiceshop.com/" target="_blank" rel="noopener noreferrer"> Moon Juice</a></span> and&nbsp;<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://shop.goop.com/shop&quot;,{&quot;metric25&quot;:1}]]" href="https://shop.goop.com/shop" target="_blank" rel="noopener noreferrer">Goop</a></span>. &nbsp;All make similar claims about the health benefits of these ingredients, but what gets called “Super Male Vitality” by Infowars is branded as “Sex Dust” by Moon Juice.</p><h2 id="h24646"><a id=""></a>Ashwagandha</h2><p>Ashwagandha is an herb commonly used in Indian Ayurvedic medicine. In the wellness world it is purported to have all kinds of benefits—<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://draxe.com/ashwagandha-proven-to-heal-thyroid-and-adrenals/&quot;,{&quot;metric25&quot;:1}]]" href="https://draxe.com/ashwagandha-proven-to-heal-thyroid-and-adrenals/" target="_blank" rel="noopener noreferrer">everything from</a></span> reducing stress to preventing cancer. According to Goop, it “tonifies the immune system,” whatever that means. (The Oxford Dictionary <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://en.oxforddictionaries.com/definition/tonify&quot;,{&quot;metric25&quot;:1}]]" href="https://en.oxforddictionaries.com/definition/tonify" target="_blank" rel="noopener noreferrer">says</a></span> that means “increase the available energy of (a bodily part or system).”) Infowars says it is “rejuvenative.” Animal studies in the lab <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.mskcc.org/cancer-care/integrative-medicine/herbs/ashwagandha&quot;,{&quot;metric25&quot;:1}]]" href="https://www.mskcc.org/cancer-care/integrative-medicine/herbs/ashwagandha" target="_blank" rel="noopener noreferrer">suggest</a></span>&nbsp;Ashwagandha may be effective for treating cancer, diabetes, and somehow, both reducing fatigue and as a sedative, but these effects have not been thoroughly tested on humans.</p><p><strong>Goop: Recommended in </strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://goop.com/magic-potions-for-clarity-beauty-and-energy/&quot;,{&quot;metric25&quot;:1}]]" href="http://goop.com/magic-potions-for-clarity-beauty-and-energy/" target="_blank" rel="noopener noreferrer"><strong>Magic Potions for Clarity, Beauty, and Energy</strong></a></span></p><blockquote data-type="BlockQuote"><p>Tonifies the immune system; inspires vigor and strength; relieves mental, emotional, and physical stress; and harmonizes mind, body, and spirit.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/heirloom-organics-professional-medicine-pack.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/heirloom-organics-professional-medicine-pack.html" target="_blank" rel="noopener noreferrer"><strong>Heirloom Organics Professional Medicine Pack</strong></a></span></p><blockquote data-type="BlockQuote"><p>Ashwagandha is regarded as one of the great rejuvenative [sic] herbs of India. According to Ayurveda, the traditional healing system of India, the root of this low-growing shrub is said to be effective for a host of debilitated [sic] conditions, including general weakness, impotence, infertility, and others. Ashwaganda is sometimes described as Indian Ginseng for the significance of this botanical in Indian pharmacopoeia.</p></blockquote><h2 id="h24647"><a id=""></a>Bacopa</h2><p>Also an Ayurvedic herb, said to reduce stress, improve memory, and treat epilepsy, among other purported benefits. Goop uses bacopa in a supplement pack called “Why am I so Effing Tired;” Infowars sticks it in its “Brain Force Plus.” The science, based on animal studies, <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.uofmhealth.org/health-library/hn-4019005&quot;,{&quot;metric25&quot;:1}]]" href="https://www.uofmhealth.org/health-library/hn-4019005" target="_blank" rel="noopener noreferrer">shows</a></span>&nbsp;some preliminary—but contradictory—evidence of improvements to memory and brain function. There is minimal support for the claims about epilepsy and anxiety.</p><figure data-id="e7e1777b17caddcb17d1d8480c011a7f" data-recommend-id="image://e7e1777b17caddcb17d1d8480c011a7f" data-format="jpg" data-width="1600" data-height="900" data-lightbox="false" data-recommended="false" data-hide="false" contenteditable="false" draggable="false"><div contenteditable="false" data-link-reference="" data-link-target="" data-syndicationrights="true" data-imagerights="other-license" data-notes="" data-hide="false" data-hidecredit="false"><div><picture><source media="(max-width: 37.31em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/e7e1777b17caddcb17d1d8480c011a7f.jpg"><source media="(min-width: 37.37em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/e7e1777b17caddcb17d1d8480c011a7f.jpg"><img alt="Image for article titled All the “wellness” products Americans love to buy are sold on both Infowars and Goop" data-chomp-id="e7e1777b17caddcb17d1d8480c011a7f" data-format="jpg" data-alt="Image for article titled All the “wellness” products Americans love to buy are sold on both Infowars and Goop" data-anim-src="" data-src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/e7e1777b17caddcb17d1d8480c011a7f.jpg" src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/e7e1777b17caddcb17d1d8480c011a7f.jpg"></picture></div><p><figcaption>Image<!-- -->: <!-- -->Goop/Infowars</figcaption></p></div><span data-id="e7e1777b17caddcb17d1d8480c011a7f" data-recommend-id="image://e7e1777b17caddcb17d1d8480c011a7f" data-format="jpg" data-width="1600" data-height="900" data-lightbox="false" data-recommended="false" data-hide="false"></span></figure><p><strong>Goop:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://shop.goop.com/shop/products/why-am-i-so-effing-tired&quot;,{&quot;metric25&quot;:1}]]" href="https://shop.goop.com/shop/products/why-am-i-so-effing-tired" target="_blank" rel="noopener noreferrer"><strong>Why am I so effing tired</strong></a></span></p><blockquote data-type="BlockQuote"><p>Formulated with a variety of vitamins (including a high dose of the B’s) and supplements—many sourced from ancient Ayurveda—this helps re-balance an overtaxed system. Replenishing the nutrients you may be lacking may improve energy levels and diminish stress.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/brain-force.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/brain-force.html" target="_blank" rel="noopener noreferrer"><strong>Brain Force Plus</strong></a></span></p><blockquote data-type="BlockQuote"><p>Top scientists and researchers agree: we are being hit by toxic weapons in the food and water supply that are making us fat, sick, and stupid. It’s time to fight back with Brain Force Plus, the next generation of advanced neural activation.</p></blockquote><h2 id="h24648"><a id=""></a>Chaga mushroom</h2><p>Most of these wellness sites provide a long list of potential benefits from ingredients like the chaga mushroom. <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.globalhealingcenter.com/natural-health/chaga-mushroom-the-immune-boosting-superfood/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.globalhealingcenter.com/natural-health/chaga-mushroom-the-immune-boosting-superfood/" target="_blank" rel="noopener noreferrer">One site</a></span> promises that&nbsp;<em>Inonotus obliquus </em>offers&nbsp;immune system support, “soothing properties,” blood pressure normalization, “DNA damage protection,” and a few more unbelievable health benefits. Moon Juice calls the mushroom a “joy promoter.” Studies on animals <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.mskcc.org/cancer-care/integrative-medicine/herbs/chaga-mushroom&quot;,{&quot;metric25&quot;:1}]]" href="https://www.mskcc.org/cancer-care/integrative-medicine/herbs/chaga-mushroom" target="_blank" rel="noopener noreferrer">have shown</a></span>&nbsp;that chaga can “inhibit cancer progression” and “activate some types of immune cells,” but the consensus is that studies in humans are needed.</p><p><strong>Moon Juice:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.moonjuiceshop.com/products/chaga&quot;,{&quot;metric25&quot;:1}]]" href="https://www.moonjuiceshop.com/products/chaga" target="_blank" rel="noopener noreferrer"><strong>Chaga</strong></a></span></p><blockquote data-type="BlockQuote"><p>Our Chaga mushrooms contain bio-active beta glucans [these have been shown to live up to <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.uofmhealth.org/health-library/hn-2805007#hn-2805007-uses&quot;,{&quot;metric25&quot;:1}]]" href="https://www.uofmhealth.org/health-library/hn-2805007#hn-2805007-uses" target="_blank" rel="noopener noreferrer">some</a></span> of their promises]<strong>&nbsp;</strong>to support the body’s innate defense systems.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/health-and-wellness/infowars-life/caveman-paleo-formula.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/health-and-wellness/infowars-life/caveman-paleo-formula.html" target="_blank" rel="noopener noreferrer"><strong>Caveman True Paleo Formula</strong></a></span></p><blockquote data-type="BlockQuote"><p>The Ultimate In True Paleo Nutrition with Bone Broth, Turmeric Root, Chaga Mushroom, Bee Pollen, and other Ancient Supernutrients [sic, entire sentence].</p></blockquote><h2 id="h24649"><a id=""></a>Colloidal Silver</h2><p>Apparently colloidal silver is “a suspension of tiny silver particles in a liquid.” On Goop, Gwyneth Paltrow <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://goop.com/fly-better-tricks-for-better-plane-trips/&quot;,{&quot;metric25&quot;:1}]]" href="http://goop.com/fly-better-tricks-for-better-plane-trips/" target="_blank" rel="noopener noreferrer">writes</a></span>, “They say that active silver keeps germs at bay so I spray this in the air around me when I sit down.” The US National Institutes of Health, on the other hand, <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://nccih.nih.gov/health/silver&quot;,{&quot;metric25&quot;:1}]]" href="https://nccih.nih.gov/health/silver" target="_blank" rel="noopener noreferrer">says directly</a></span> that “claims made about the health benefits of taking colloidal silver aren’t backed up by studies,” adding, “colloidal silver can cause serious side effects.”</p><p><strong>Higher Nature:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.highernature.co.uk/Products/Colloidal-Silver-Spray&quot;,{&quot;metric25&quot;:1}]]" href="https://www.highernature.co.uk/Products/Colloidal-Silver-Spray" target="_blank" rel="noopener noreferrer"><strong>Colloidal Silver</strong></a></span><strong>&nbsp;(recommended by Goop)</strong></p><blockquote data-type="BlockQuote"><p>Our Colloidal silver contains pure, medical grade silver, which has been used for many centuries. It is produced using electro-controlled technology and pure water from a 9-stage water purification process, to achieve a very small particle size (0.0006 to 0.005 microns). This small particle size is important because it provides a much greater surface area and therefore more of an effective liquid.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/silver-bullet-40-off.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/silver-bullet-40-off.html" target="_blank" rel="noopener noreferrer"><strong>Silver Bullet</strong></a></span></p><blockquote data-type="BlockQuote"><p>The Infowars Life Silver Bullet Colloidal Silver is finally here following Alex’s extensive search for a powerful colloidal silver product that is both free of artificial additives and utilizes high quality processes to ensure for [sic] a truly unique product that has applications for both preparedness and regular use.</p></blockquote><h2 id="h24650"><a id=""></a>Cordyceps mushroom</h2><p>Another obscure fungus, this one used in traditional Chinese medicine. It <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://draxe.com/cordyceps/&quot;,{&quot;metric25&quot;:1}]]" href="https://draxe.com/cordyceps/" target="_blank" rel="noopener noreferrer">is purported</a></span> to “increase immune function,” act as a natural aphrodisiac, and improve stamina.&nbsp;According to Goop, it’s “an important Yang tonic,” which means it <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://goop.com/the-grandmothers-on-why-the-universe-is-out-of-whack/&quot;,{&quot;metric25&quot;:1}]]" href="http://goop.com/the-grandmothers-on-why-the-universe-is-out-of-whack/" target="_blank" rel="noopener noreferrer">provides</a></span> “masculine energy.” There is some <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.uofmhealth.org/health-library/hn-3936006#hn-3936006-uses&quot;,{&quot;metric25&quot;:1}]]" href="https://www.uofmhealth.org/health-library/hn-3936006#hn-3936006-uses" target="_blank" rel="noopener noreferrer">preliminary evidence</a></span> for the immune system thing, but other claims <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.mskcc.org/cancer-care/integrative-medicine/herbs/cordyceps&quot;,{&quot;metric25&quot;:1}]]" href="https://www.mskcc.org/cancer-care/integrative-medicine/herbs/cordyceps" target="_blank" rel="noopener noreferrer">are unproven</a></span>. Goop sells cordyceps as a dietary supplement; Infowars infuses them into its “Wake Up America” coffee.</p><figure data-id="c9abee5e13d318d785b7c08c8661985e" data-recommend-id="image://c9abee5e13d318d785b7c08c8661985e" data-format="jpg" data-width="1600" data-height="900" data-lightbox="false" data-recommended="false" data-hide="false" contenteditable="false" draggable="false"><div contenteditable="false" data-link-reference="" data-link-target="" data-syndicationrights="true" data-imagerights="other-license" data-notes="" data-hide="false" data-hidecredit="false"><div><picture><source media="(max-width: 37.31em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/c9abee5e13d318d785b7c08c8661985e.jpg"><source media="(min-width: 37.37em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/c9abee5e13d318d785b7c08c8661985e.jpg"><img alt="Image for article titled All the “wellness” products Americans love to buy are sold on both Infowars and Goop" data-chomp-id="c9abee5e13d318d785b7c08c8661985e" data-format="jpg" data-alt="Image for article titled All the “wellness” products Americans love to buy are sold on both Infowars and Goop" data-anim-src="" data-src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/c9abee5e13d318d785b7c08c8661985e.jpg" src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/c9abee5e13d318d785b7c08c8661985e.jpg"></picture></div><p><figcaption>Image<!-- -->: <!-- -->Goop/Infowars</figcaption></p></div><span data-id="c9abee5e13d318d785b7c08c8661985e" data-recommend-id="image://c9abee5e13d318d785b7c08c8661985e" data-format="jpg" data-width="1600" data-height="900" data-lightbox="false" data-recommended="false" data-hide="false"></span></figure><p><strong>Goop:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://shop.goop.com/shop/products/cordyceps?taxon_id=751&quot;,{&quot;metric25&quot;:1}]]" href="https://shop.goop.com/shop/products/cordyceps?taxon_id=751" target="_blank" rel="noopener noreferrer"><strong>Sun Potion</strong></a></span></p><blockquote data-type="BlockQuote"><p>Organic, USA-grown cordyceps mushroom and is [sic] an important Yang tonic. May support the oxygenation of the whole body, mental power, muscle tone, sexual energy, and immune function. Mix 1/2 teaspoon (2 grams) in warm water or tea 1-2 times daily. Great added to soups, smoothies, raw chocolate, and anytime you are looking to activate fortitude, sensuality, and endurance.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://www.infowarsshop.com/Immune-Support-Blend-100-Organic-Coffee_p_1174.html&quot;,{&quot;metric25&quot;:1}]]" href="http://www.infowarsshop.com/Immune-Support-Blend-100-Organic-Coffee_p_1174.html" target="_blank" rel="noopener noreferrer"><strong>Wake Up America&nbsp;Immune Support Blend 100% Organic Coffee</strong></a></span></p><blockquote data-type="BlockQuote"><p>Certain strands of mushroom such as Cordyceps and Reishi have a history of medicinal use spanning millennia in countries such as China, Tibet, and Japan. Throughout history these are [sic] some of the most expensive herbal raw materials in the world. Only recently has western medicine begun to research all the potential medical benefits of medicinal mushrooms. The cutting-edge Wake Up America! Immune Support Blend brings ancient Asian wisdom together with modern technology.</p></blockquote><h2 id="h24651"><a id=""></a>Eleuthero root</h2><p>There is some preliminary evidence that eleuthero, another ingredient in traditional Chinese medicine, <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.uofmhealth.org/health-library/hn-2084007#hn-2084007-uses&quot;,{&quot;metric25&quot;:1}]]" href="https://www.uofmhealth.org/health-library/hn-2084007#hn-2084007-uses" target="_blank" rel="noopener noreferrer">has various benefits</a></span>, such as reducing fatigue and stress, and improving immune functions. Both Moon Juice and Infowars sell it blended with a bunch of other herbal medicines, though, so it would be difficult to isolate eleuthero’s possible positive effects. Moon Juice says it can help “fuel your physical and entrepreneurial feats.” Also, this is the one case Quartz found where the Moon Juice product sounds more hardcore than the Infowars version.</p><p><strong>Moon Juice:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.moonjuiceshop.com/products/power-dust&quot;,{&quot;metric25&quot;:1}]]" href="https://www.moonjuiceshop.com/products/power-dust" target="_blank" rel="noopener noreferrer"><strong>Power Dust</strong></a></span></p><blockquote data-type="BlockQuote"><p>Power Dust® is an elite blend of adaptogenic superherbs and supermushrooms that help combat [sic] the effects of stress to fuel your physical and entrepreneurial feats.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://www.infowarsshop.com/Relax-De-Stress-Herbal-Extract_p_1094.html&quot;,{&quot;metric25&quot;:1}]]" href="http://www.infowarsshop.com/Relax-De-Stress-Herbal-Extract_p_1094.html" target="_blank" rel="noopener noreferrer"><strong>Relax &amp; De-Stress Herbal Extract</strong></a></span></p><blockquote data-type="BlockQuote"><p>Relax &amp; De-Stress Herbal Extract is a [sic] herbal tincture great for relaxing and supporting the nervous system while aiding in maintaining [sic] a healthy heart and adrenals [sic] gland function.</p></blockquote><h2 id="h24652"><a id=""></a>Eyebright herb</h2><p>The two sides of our herbal medicine spectrum seem to have come to different conclusions about what “eyebright” does for the eyes. Infowars sells it in a supplement called “Occu Power,” which makes your eyes “healthy.” Goop sells it as an ingredient in eye makeup. There is <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.uofmhealth.org/health-library/hn-2087000#hn-2087000-uses&quot;,{&quot;metric25&quot;:1}]]" href="https://www.uofmhealth.org/health-library/hn-2087000#hn-2087000-uses" target="_blank" rel="noopener noreferrer">no scientific evidence</a></span> for its purported eye health benefits.</p><figure data-id="3630675af9e35b2803a4530cc9ff7804" data-recommend-id="image://3630675af9e35b2803a4530cc9ff7804" data-format="jpg" data-width="1600" data-height="900" data-lightbox="false" data-recommended="false" data-hide="false" contenteditable="false" draggable="false"><div contenteditable="false" data-link-reference="" data-link-target="" data-syndicationrights="true" data-imagerights="other-license" data-notes="" data-hide="false" data-hidecredit="false"><div><picture><source media="(max-width: 37.31em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/3630675af9e35b2803a4530cc9ff7804.jpg"><source media="(min-width: 37.37em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/3630675af9e35b2803a4530cc9ff7804.jpg"><img alt="Image for article titled All the “wellness” products Americans love to buy are sold on both Infowars and Goop" data-chomp-id="3630675af9e35b2803a4530cc9ff7804" data-format="jpg" data-alt="Image for article titled All the “wellness” products Americans love to buy are sold on both Infowars and Goop" data-anim-src="" data-src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/3630675af9e35b2803a4530cc9ff7804.jpg" src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/3630675af9e35b2803a4530cc9ff7804.jpg"></picture></div><p><figcaption>Image<!-- -->: <!-- -->Goop/Infowars</figcaption></p></div><span data-id="3630675af9e35b2803a4530cc9ff7804" data-recommend-id="image://3630675af9e35b2803a4530cc9ff7804" data-format="jpg" data-width="1600" data-height="900" data-lightbox="false" data-recommended="false" data-hide="false"></span></figure><p><strong>Goop:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://shop.goop.com/shop/products/mesmerize-eye-shimmer&quot;,{&quot;metric25&quot;:1}]]" href="https://shop.goop.com/shop/products/mesmerize-eye-shimmer" target="_blank" rel="noopener noreferrer"><strong>Vapour Beauty’s Mesmerize Eye Shimmer</strong></a></span></p><blockquote data-type="BlockQuote"><p>This is a sheer, modern wash of gleamy color that’s as brilliant all over the lid as it is when used as a translucent, smoky touch of liner. Made with organic chrysanthemum, eyebright, and horsetail herb—the blend is Vapour’s famous Herbal Eyebright complex—the creamy stick is hydrating and packed with antioxidants to treats [sic] the delicate eye area, soothing inflammation and stimulating circulation.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/health-and-wellness/wellness/occu-power.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/health-and-wellness/wellness/occu-power.html" target="_blank" rel="noopener noreferrer"><strong>Occu Power</strong></a></span></p><blockquote data-type="BlockQuote"><p>Occu-Power by Infowars Life is a new formulation specifically designed to nutritionally assist the natural function of healthy eyes. Arguably the most important sense, sight is the primary input to the brain. Combining key ingredients like astaxanthin, lutein, and Eyebright herb extract, Occu-Power is a long awaited ‘super formula’ now available exclusively through the Infowars Life line.</p></blockquote><h2 id="h24653"><a id=""></a>Maca</h2><p>Maca is supposed to increase sex drive and male fertility. The Sloan Kettering Cancer Center <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.mskcc.org/cancer-care/integrative-medicine/herbs/maca&quot;,{&quot;metric25&quot;:1}]]" href="https://www.mskcc.org/cancer-care/integrative-medicine/herbs/maca" target="_blank" rel="noopener noreferrer">says</a></span>&nbsp;”the evidence to support the use of maca in improving sexual function is limited,” and that “more studies are warranted.” Both Moon Juice and Infowars sell it as an ingredient in products targeting an increased libido. (Although, at Moon Juice, it is an ingredient in both “Sex Dust” and “Brain Dust.”)</p><p><strong>Moon Juice:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://shop.goop.com/shop/products/sex-dust&quot;,{&quot;metric25&quot;:1}]]" href="https://shop.goop.com/shop/products/sex-dust" target="_blank" rel="noopener noreferrer"><strong>Sex Dust</strong></a></span></p><blockquote data-type="BlockQuote"><p>Sex Dust™ is a lusty edible formula alchemized to ignite and excite sexy energy in and out of the bedroom.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://www.infowarsshop.com/Super-Male-Vitality-_p_1227.html&quot;,{&quot;metric25&quot;:1}]]" href="http://www.infowarsshop.com/Super-Male-Vitality-_p_1227.html" target="_blank" rel="noopener noreferrer"><strong>Super Male Vitality</strong></a></span></p><blockquote data-type="BlockQuote"><p>As men age, they may often experience a slow-down in vitality, energy, and overall wellness. Super Male Vitality is specifically designed to assist the body in regulating proper balance to create superior vitality in males, and has been used by Alex Jones in order to maximize vitality when working up to 12 hours a day or more in the fight for freedom.</p></blockquote><h2 id="h24654"><a id=""></a>Nascent iodine</h2><p>We couldn’t find any reliable scientific information on “nascent iodine.” Normal, run-of-the-mill iodine is an important thing to have in your body, but most people <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://ods.od.nih.gov/factsheets/Iodine-Consumer/&quot;,{&quot;metric25&quot;:1}]]" href="https://ods.od.nih.gov/factsheets/Iodine-Consumer/" target="_blank" rel="noopener noreferrer">get enough</a></span> from foods that are rich in the mineral. The “nascent” stuff is <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.globalhealingcenter.com/natural-health/what-is-nascent-iodine/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.globalhealingcenter.com/natural-health/what-is-nascent-iodine/" target="_blank" rel="noopener noreferrer">supposed to</a></span> have an “electromagnetic charge” that makes it easier to digest.</p><p>Putting the question of the veracity of those claims to the side, ConsumerLab, an independent group that tests and vets dietary supplements, says that iodine in this “charged” form <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://www.consumerlab.com/answers/I+have+heard+that+nascent+iodine+may+be+better+than+regular+iodine+or+potassium+iodide+for+thyroid+support.+Is+this+true,+and+what+exactly+is+nascent+iodine%3F/nascent_iodine/&quot;,{&quot;metric25&quot;:1}]]" href="http://www.consumerlab.com/answers/I+have+heard+that+nascent+iodine+may+be+better+than+regular+iodine+or+potassium+iodide+for+thyroid+support.+Is+this+true,+and+what+exactly+is+nascent+iodine%3F/nascent_iodine/" target="_blank" rel="noopener noreferrer">cannot even exist</a></span> in liquid form, which is how Alex Jones and company distribute it.&nbsp;What’s more, the amount of nascent iodine wellness websites suggest you take—in order to supposedly maintain a healthy thyroid, usually—is way beyond normal levels for the normal iodine we usually get from foods.&nbsp;The US National Institutes of Health says that “adults should avoid prolonged use of doses higher than 1,100 micrograms per day.” Infowars and the “<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.globalhealingcenter.com/nascent-iodine-detoxadine.html?icn_ghc=ddt1_1_052412_winix&amp;ici_ghc=ddnha#description&quot;,{&quot;metric25&quot;:1}]]" href="https://www.globalhealingcenter.com/nascent-iodine-detoxadine.html?icn_ghc=ddt1_1_052412_winix&amp;ici_ghc=ddnha#description" target="_blank" rel="noopener noreferrer">Global Healing Center</a></span>” both recommend 1,950 micrograms a day.</p><p><strong>Goop: No particular product, but recommends nascent iodine in </strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://goop.com/why-we-shouldnt-dismiss-iodine/&quot;,{&quot;metric25&quot;:1}]]" href="http://goop.com/why-we-shouldnt-dismiss-iodine/" target="_blank" rel="noopener noreferrer"><strong>a Q&amp;A published on the site</strong></a></span></p><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/survival-shield-x-2-nascent-iodine.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/survival-shield-x-2-nascent-iodine.html" target="_blank" rel="noopener noreferrer"><strong>Survival Shield X-2</strong></a></span></p><blockquote data-type="BlockQuote"><p>Experience the benefits of next level proprietary nascent iodine, developed using our Thermodynamic Pressure Sensitive High Energy Sound Pulse Nano-Emulsion Technology [sic] that allows for a highly unique nascent iodine that is both concentrated and free of unwanted additives and genetically modified ingredients.</p></blockquote><h2 id="h24655"><a id=""></a>Reishi mushrooms</h2><p>More fungi with a load of <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://draxe.com/reishi-mushroom/&quot;,{&quot;metric25&quot;:1}]]" href="https://draxe.com/reishi-mushroom/" target="_blank" rel="noopener noreferrer">supposed</a></span> health benefits. Current research shows there’s <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.mskcc.org/cancer-care/integrative-medicine/herbs/reishi-mushroom&quot;,{&quot;metric25&quot;:1}]]" href="https://www.mskcc.org/cancer-care/integrative-medicine/herbs/reishi-mushroom" target="_blank" rel="noopener noreferrer">no scientific evidence to support the claims that</a></span>&nbsp;reishi mushrooms can treat fatigue or increase stamina. There is a small amount of inconclusive scientific support for claims that the fungi improve immune system function, reduce inflammation, and lower cholesterol. Reishi mushrooms are another ingredient in Infowars’ Wake Up America coffee.</p><p><strong>Moon Juice:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.moonjuiceshop.com/products/spirit-dust&quot;,{&quot;metric25&quot;:1}]]" href="https://www.moonjuiceshop.com/products/spirit-dust" target="_blank" rel="noopener noreferrer"><strong>Spirit Dust</strong></a></span></p><blockquote data-type="BlockQuote"><p>Spirit Dust® is an uplifting blend of adaptogenic superherbs and supermushrooms that help [sic] combat the effects of stress to expand peaceful awareness and align you with bliss.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://www.infowarsshop.com/Immune-Support-Blend-100-Organic-Coffee_p_1174.html&quot;,{&quot;metric25&quot;:1}]]" href="http://www.infowarsshop.com/Immune-Support-Blend-100-Organic-Coffee_p_1174.html" target="_blank" rel="noopener noreferrer"><strong>Wake Up America&nbsp;Immune Support Blend 100% Organic Coffee</strong></a></span></p><blockquote data-type="BlockQuote"><p>Prized for thousands of years for their culinary and medicinal properties, mushrooms are more than just a low kilojoule, low sodium and high-fiber ingredient for pasta and pizzas. One of the many conditions that certain species of mushrooms have been found to heal is human papilloma virus infections, which are feared due to their association with certain cancers.</p></blockquote><h2 id="h24656"><a id=""></a>Selenium</h2><p>Selenium is an element found in trace amounts in all animals, including humans, and is required&nbsp;for cellular function. Infowars says the element “supports a healthy thyroid gland, supports the immune system, is essential for metabolic pathways, and much more.” Some of these claims are&nbsp;<span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.uofmhealth.org/health-library/hn-2908005#hn-2908005-uses&quot;,{&quot;metric25&quot;:1}]]" href="https://www.uofmhealth.org/health-library/hn-2908005#hn-2908005-uses" target="_blank" rel="noopener noreferrer">backed by evidence</a></span>, though many are not. The <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://draxe.com/selenium-benefits/&quot;,{&quot;metric25&quot;:1}]]" href="https://draxe.com/selenium-benefits/" target="_blank" rel="noopener noreferrer">claim</a></span> that selenium defends against cancer is also dangerously inconclusive. “Clinical trials show that selenium may not help prevent cancer; it may actually increase the risk of aggressive and secondary cancers,” <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.mskcc.org/cancer-care/integrative-medicine/herbs/selenium&quot;,{&quot;metric25&quot;:1}]]" href="https://www.mskcc.org/cancer-care/integrative-medicine/herbs/selenium" target="_blank" rel="noopener noreferrer">writes</a></span> Sloan Kettering Cancer Center.</p><figure data-id="d4a74e667448ad7aeec425ed3c811fba" data-recommend-id="image://d4a74e667448ad7aeec425ed3c811fba" data-format="jpg" data-width="1600" data-height="900" data-lightbox="false" data-recommended="false" data-hide="false" contenteditable="false" draggable="false"><div contenteditable="false" data-link-reference="" data-link-target="" data-syndicationrights="true" data-imagerights="other-license" data-notes="" data-hide="false" data-hidecredit="false"><div><picture><source media="(max-width: 37.31em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/d4a74e667448ad7aeec425ed3c811fba.jpg"><source media="(min-width: 37.37em)" type="image/jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/d4a74e667448ad7aeec425ed3c811fba.jpg"><img alt="Image for article titled All the “wellness” products Americans love to buy are sold on both Infowars and Goop" data-chomp-id="d4a74e667448ad7aeec425ed3c811fba" data-format="jpg" data-alt="Image for article titled All the “wellness” products Americans love to buy are sold on both Infowars and Goop" data-anim-src="" data-src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/d4a74e667448ad7aeec425ed3c811fba.jpg" src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/d4a74e667448ad7aeec425ed3c811fba.jpg"></picture></div><p><figcaption>Image<!-- -->: <!-- -->Goop/Infowars</figcaption></p></div><span data-id="d4a74e667448ad7aeec425ed3c811fba" data-recommend-id="image://d4a74e667448ad7aeec425ed3c811fba" data-format="jpg" data-width="1600" data-height="900" data-lightbox="false" data-recommended="false" data-hide="false"></span></figure><p><strong>Goop:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://shop.goop.com/shop/products/balls-in-the-air&quot;,{&quot;metric25&quot;:1}]]" href="https://shop.goop.com/shop/products/balls-in-the-air" target="_blank" rel="noopener noreferrer"><strong>Balls in the Air</strong></a></span></p><blockquote data-type="BlockQuote"><p>This antioxidant-rich (beta-carotene, vitamin C, and vitamin E) regimen plays defense so you can play offense, helping to unburden inflammation in the body, ensuring that all systems operate at full capacity. Formulated with a blend of building blocks that boosts the body’s production of glutathione—the master detoxifier—this regimen is designed for women who function at an intense pace, and want to keep it that way.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/bio-true-selenium.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/bio-true-selenium.html" target="_blank" rel="noopener noreferrer"><strong>Bio-True Selenium</strong></a></span></p><blockquote data-type="BlockQuote"><p>Selenium is an essential trace mineral that supports a healthy thyroid gland, supports the immune system, is essential for metabolic pathways, and much more. Selenium even plays a role in the natural function of reproductive health, DNA production, and eyesight. As a powerful antioxidant, selenium helps fight free radicals and may even be considered a super antioxidant’ [sic] because of the way in which it may support other antioxidants.</p></blockquote><h2 id="h24657"><a id=""></a>Shilajit</h2><p>The science on this tar-like substance found in the Himalayas is scant. <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.ncbi.nlm.nih.gov/pubmed/26395129&quot;,{&quot;metric25&quot;:1}]]" href="https://www.ncbi.nlm.nih.gov/pubmed/26395129" target="_blank" rel="noopener noreferrer">One study</a></span> found it increased testosterone levels. Another said it might have some benefit in helping control Alzheimer’s, but <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3296184/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3296184/" target="_blank" rel="noopener noreferrer">adds that</a></span> “more investigations at the basic biological level as well as clinical trials are necessary.” Alex Jones sells it in his Z-Shield drops, with “proprietary science” behind them, saying it defends us from the “toxic substances” that “bombard” us. Over at Moon Juice, it is yet another ingredient in Brain Dust.</p><p><strong>Moon Juice:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.moonjuiceshop.com/products/brain-dust&quot;,{&quot;metric25&quot;:1}]]" href="https://www.moonjuiceshop.com/products/brain-dust" target="_blank" rel="noopener noreferrer"><strong>Brain Dust</strong></a></span></p><blockquote data-type="BlockQuote"><p>Brain Dust® is an enlightening blend of adaptogenic superherbs and supermushrooms that help combat [sic] the effects of stress to align you with the cosmic flow for great achievement.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://www.infowarsshop.com/Z-Shield_p_2053.html&quot;,{&quot;metric25&quot;:1}]]" href="http://www.infowarsshop.com/Z-Shield_p_2053.html" target="_blank" rel="noopener noreferrer"><strong>Z-Shield</strong></a></span></p><blockquote data-type="BlockQuote"><p>More than four years ago, our team of doctors, chemists, master herbalists and nutraceutical experts set out to develop a toxic metal and chemical defense support formula that didn’t cut any corners. Now, after years of deep research and the development of new proprietary processing technology, our team is proud to announce the launch of Z-Shield: The next big game-changer in the Infowars Life line of super high-quality formulations. Z-Shield is designed to help you fight back with natural ingredients that don’t hold back.</p></blockquote><h2 id="h24658"><a id=""></a>Zizyphus (sometimes referred to as “ziziphus”)</h2><p>Infowars calls this a “nourishing tonifier” and recommends using it to calm your kids down. Both Infowars and Moon Juice market it as a sedative, though there are “no human studies on the sedative or anxiety-reducing effects of Jujube” (though some studies on rats have shown promise), <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://examine.com/supplements/ziziphus-jujuba/&quot;,{&quot;metric25&quot;:1}]]" href="https://examine.com/supplements/ziziphus-jujuba/" target="_blank" rel="noopener noreferrer">according to</a></span> Examine.com, an online encyclopedia that analyzes evidence on supplements. Getting a good night’s sleep may remain a zizyphean task.</p><p><strong>Moon Juice:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.moonjuiceshop.com/products/dream-dust&quot;,{&quot;metric25&quot;:1}]]" href="https://www.moonjuiceshop.com/products/dream-dust" target="_blank" rel="noopener noreferrer"><strong>Dream Dust</strong></a></span></p><blockquote data-type="BlockQuote"><p>Dream Dust® is a tranquil blend of adaptogenic superherbs that help combat the effects of stress to soothe your tension for deep, nocturnal rest.</p></blockquote><p><strong>Infowars:&nbsp;</strong><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.infowarsstore.com/health-and-wellness/wellness/child-ease.html&quot;,{&quot;metric25&quot;:1}]]" href="https://www.infowarsstore.com/health-and-wellness/wellness/child-ease.html" target="_blank" rel="noopener noreferrer"><strong>Child Ease</strong></a></span></p><blockquote data-type="BlockQuote"><p>Children today live in a stressful world. Over-stimulation can affect their behavior and concentration.&nbsp;Child Ease™&nbsp;by Infowars Life™&nbsp;is a special blend of herbs that has been specifically designed to soothe the mind and bodies of children. Our new formula uses soothing botanicals like chamomile, lemon balm, and catnip, with the nourishing tonifiers hawthorn, zizyphus, gotu kola extract, and amla. We have even added additional herbs and key nutrients that have been traditionally used by cultures around the world.</p></blockquote><p>____________________________________________________________________________</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LeftoverLocals: Listening to LLM responses through leaked GPU local memory (112 pts)]]></title>
            <link>https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/</link>
            <guid>39016405</guid>
            <pubDate>Tue, 16 Jan 2024 17:58:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/">https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/</a>, See on <a href="https://news.ycombinator.com/item?id=39016405">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page" role="main">

			
				
<article id="post-105881">
	<!-- .entry-header -->

	<div>
		<p><em>By Tyler Sorensen and Heidy Khlaaf</em></p>
<p>We are disclosing LeftoverLocals: a vulnerability that allows recovery of data from GPU local memory created by another process on Apple, Qualcomm, AMD, and Imagination GPUs. LeftoverLocals impacts the security posture of GPU applications as a whole, with particular significance to LLMs and ML models <a href="https://twitter.com/AMD/status/1744831880241750112">run on impacted GPU platforms</a>. By recovering local memory—an optimized GPU memory region—we were able to build a PoC where an attacker can listen into another user’s interactive LLM session (e.g., llama.cpp) across process or container boundaries, as shown below:</p>

<p><strong>Figure 1:</strong> An illustration of how LeftoverLocals can be used to implement an attack on an interactive LLM chat session. The LLM user (left) queries the LLM, while a co-resident attacker (right) can listen to the LLM response.</p>
<p>LeftoverLocals can leak ~5.5 MB per GPU invocation on an AMD Radeon RX 7900 XT which, when running a 7B model on llama.cpp, adds up to ~181 MB for each LLM query. This is enough information to reconstruct the LLM response with high precision. The vulnerability highlights that many parts of the ML development stack have unknown security risks and have not been rigorously reviewed by security experts.</p>
<div id="attachment_105917"><p><img fetchpriority="high" decoding="async" aria-describedby="caption-attachment-105917" data-attachment-id="105917" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig1/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?fit=1090%2C772&amp;ssl=1" data-orig-size="1090,772" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="figfig1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?fit=300%2C212&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?fit=690%2C489&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?resize=543%2C385&amp;ssl=1" alt="" width="543" height="385" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?resize=1024%2C725&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?resize=300%2C212&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?resize=768%2C544&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig1.png?w=1090&amp;ssl=1 1090w" sizes="(max-width: 543px) 100vw, 543px" data-recalc-dims="1"></p><p id="caption-attachment-105917"><strong>Figure 2:</strong> LeftoverLocals logo: what leftover data is your ML model leaving for another user to steal?</p></div>
<p>This vulnerability is tracked by <a href="https://kb.cert.org/vuls/id/446598">CVE-2023-4969</a>. It was discovered by Tyler Sorensen as part of his work within the ML/AI Assurance team. Tyler Sorensen is also an assistant professor at UCSC. Since September 2023, we have been working with CERT Coordination Center on a large coordinated disclosure effort involving all major GPU vendors, including: NVIDIA, Apple, AMD, Arm, Intel, Qualcomm, and Imagination.</p>
<p>As of writing, the status of the impacted vendors, Apple, AMD, and Qualcomm are as follows:</p>
<ul>
<li><strong>Apple</strong>: Despite multiple efforts to establish contact through CERT/CC, we only received a response from Apple on January 13, 2024. We re-tested the vulnerability on January 10 where it appears that some devices have been patched, i.e., Apple iPad Air 3rd G (A12). However, the issue still appears to be present on the Apple MacBook Air (M2). Furthermore, the recently released Apple iPhone 15 does not appear to be impacted as previous versions have been. <span>Apple has confirmed that the A17 and M3 series </span><span>processors</span><span> contain fixes</span>, but we have not been notified of the specific patches deployed across their devices.</li>
<li><strong>AMD</strong>: We have confirmed with AMD that their devices remain impacted, although they continue to investigate potential mitigation plans. Their statement on the issue can be read <a href="https://www.amd.com/en/resources/product-security/bulletin/amd-sb-6010.html">here</a>.</li>
<li><strong>Qualcomm</strong>: We received notice that there is a patch to Qualcomm firmware <a href="https://lore.kernel.org/linux-firmware/20240111114032.126035-1-quic_akhilpo@quicinc.com/T/#u">v2.07</a> that addresses LeftoverLocals for some devices. However, there may still be other devices impacted at this time. A Qualcomm representative has provided the following comment: <em>“Developing technologies that endeavor to support robust security and privacy is a priority for Qualcomm Technologies. We commend Dr. Tyler Sorensen and Dr. Heidy Khlaaf from the AI/ML Assurance group at Trail of Bits for using coordinated disclosure practices and are in the process of providing security updates to our customers. We encourage end users to apply security updates as they become available from their device makers.”</em></li>
<li><strong>Imagination: <span>Despite not observing LeftoverLocals ourselves across the Imagination GPUs that we tested, Google has confirmed that some Imagination GPUs are indeed impacted. Imagination <a href="https://www.imaginationtech.com/gpu-driver-vulnerabilities/">released a fix</a> in their latest DDK release, 23.3, made available to customers in December 2023.</span></strong></li>
</ul>
<p>Further details are discussed in “Coordinated disclosure,” and a list of tested and impacted devices can be found in “Testing GPU platforms for LeftoverLocals.” Other vendors have provided us the following details:</p>
<ul>
<li><strong>NVIDIA</strong>: confirmed that their devices are not currently impacted. One reason for this could be that researchers have explored various memory leaks on NVIDIA GPUs <a href="https://arxiv.org/abs/1305.7383">previously</a>, and thus, they are aware of these types of issues.</li>
<li><strong>ARM</strong>: also confirmed that their devices are not currently impacted.</li>
</ul>
<p>While we did not hear a response from these vendors, we tested at least one GPU from them and did not observe that they were impacted: <strong>Intel</strong>.</p>
<h3>Exploit brief</h3>
<p>GPUs were initially developed to accelerate graphics computations. In this domain, performance is critical, and previously uncovered security issues have generally not had any significant consequences on applications. Historically, this entailed that GPU hardware and software stacks iterated rapidly, with frequent major architecture and programming model changes. This has led to complex system stacks and vague specifications. For example, while CPU ISAs have volumes of documentation, NVIDIA simply provides a few short <a href="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#instruction-set-reference">tables</a>. This type of vague specification has led to alarming issues, both <a href="https://users.soe.ucsc.edu/~tsorensen/files/asplos2015.pdf">previously</a> and currently, as LeftoverLocals exemplifies.</p>
<h4>Exploitation requirements</h4>
<p>This is a co-resident exploit, meaning that a threat actor’s avenue of attack could be implemented as another application, app, or user on a shared machine. The attacker only requires the ability to run GPU compute applications, e.g., through OpenCL, Vulkan, or Metal. These frameworks are well-supported and typically do not require escalated privileges. Using these, the attacker can read data that the victim has left in the GPU local memory simply by writing a GPU kernel that dumps uninitialized local memory. These attack programs, as our code demonstrates, can be less than 10 lines of code. Implementing these attacks is thus not difficult and is accessible to amateur programmers (at least in obtaining stolen data). We note that it appears that browser GPU frameworks (e.g., WebGPU) are not currently impacted, as they insert dynamic memory checks into GPU kernels.</p>
<p>Unless the user inspects the application’s low-level GPU source-code, it is not possible for them to uncover if their application is utilizing GPU local memory; this matter is further complicated as the GPU code is often hidden deep in library calls, at low levels of deep software stacks (e.g., for ML). Overall, there are very limited ways to observe that an attacker is currently stealing data, or has stolen data. This attack hinges on the attacker reading uninitialized memory on the GPU, and while this is technically undefined behavior, it is not currently checked dynamically, or logged. Any additional defenses would be quite invasive, e.g., performing code analysis on GPU kernels to check for undefined behavior.</p>
<p>We have <a href="https://github.com/trailofbits/LeftoverLocalsRelease">released a PoC</a> that exploits this vulnerability, and the sections below describe how it works.</p>
<h4>User mitigations</h4>
<p>Given the lack of comprehensive patches across impacted GPU vendors, LeftoverLocals can be defended by modifying the source code of all GPU kernels that use local memory. Before the kernel ends, the GPU threads should clear memory (e.g., store 0s) to any local memory memory locations that were used in the kernel. Additionally, the users should ensure the compiler doesn’t remove these memory-clearing instructions away (e.g., by annotating their local memory as volatile), as the compiler may detect that the cleared memory is not used later in the kernel. This is difficult to verify because GPU binaries are typically not stored explicitly, and there are very few GPU binary analysis tools. Because of reasons like this, we note that this mitigation may be difficult for many users, and we discuss this further in “Mitigations” below.</p>
<h2>The vulnerability: LeftoverLocals</h2>
<p>In this section we describe the vulnerability, named LeftoverLocals, and the corresponding exploit in more detail. We then detail our testing campaign across a wide variety of GPU devices, which found that GPUs from AMD, Apple, and Qualcomm are vulnerable to LeftoverLocals. For those unfamiliar with GPU architecture and terminology, we provide a more in-depth level-setter in “Background: How GPUs work.” We also note that while GPU memory leaks are not <a href="https://arxiv.org/abs/1305.7383">new</a> (a further discussion follows below), LeftoverLocals has demonstrated both deeper impact and wider breadth than previously discovered vulnerabilities.</p>
<p>At a high level, we found that several GPU frameworks do not sufficiently isolate memory in the same way that it is traditionally expected in CPU-based frameworks. We have observed that on impacted GPUs, it is possible for one kernel—potentially from another user that is co-resident on the same machine—to observe values in local memory that were written by another kernel. Thus, an attacker who has access to a shared GPU through its programmable interface (e.g., OpenCL) can steal memory from other users and processes, violating traditional process isolation properties. This data leaking can have severe security consequences, especially given the rise of ML systems, where local memory is used to store model inputs, outputs, and weights.</p>
<p>Previous <a href="https://arxiv.org/abs/1305.7383">academic work</a> showed that NVIDIA GPUs leaked memory across processes through a variety of memory regions, including local memory. However, they examined only GPUs from NVIDIA (and the results from this paper may be part of the reason why we didn’t observe LocalLeftovers on NVIDIA GPUs). They also did not discuss the impact on widely deployed use-cases, such as ML. Other works have shown how GPUs leak graphics data, and that a co-resident attacker can reconstruct partial visual information from another process (see some examples documented <a href="https://ieeexplore.ieee.org/document/6956554">here</a>, <a href="https://arxiv.org/pdf/1605.06610.pdf">here</a>, and <a href="https://www.hertzbleed.com/gpu.zip/">here</a>). Despite these prior works, LeftoverLocals shows that many GPUs remain vulnerable to local memory leaks and that this vulnerability can be exploited in co-resident attacks on important ML applications.</p>
<p>Overall, this vulnerability can be illustrated using two simple programs: a Listener and a Writer, where the writer stores canary values in local memory, while a listener reads uninitialized local memory to check for the canary values. The Listener repeatedly launches a GPU kernel that reads from uninitialized local memory. The Writer repeatedly launches a GPU kernel that writes canary values to local memory. Below, we demonstrate how each of these operations is carried out.</p>
<p><strong><em>The Listener</em></strong>: The Listener launches a GPU kernel that reads from uninitialized local memory and stores the result in a persistent main memory region (i.e., global memory). This can be accomplished with the OpenCL kernel below:</p>
<pre><span>__kernel</span> void listener(__global <span>volatile</span> int *<span>dump</span>) {
  local <span>volatile</span> int lm[LM_SIZE];
  for (int i = <span>get_local_id(0)</span>; i &lt; LM_SIZE; i+= <span>get_local_size(0)</span>) {
    <span>dump</span>[((LM_SIZE * <span>get_group_id(0)</span>) + i)] = lm[i];
  }
}</pre>
<p>The keyword <span><code>__kernel</code></span> denotes that this is the GPU kernel function. We pass a global memory array <span><code>dump</code></span> to the function. Whatever the kernel writes to this array can be read later by the CPU. We statically declare a local memory array lm with a predefined size LM_SIZE (which we set to be the max size of local memory for each GPU we test). This program technically contains undefined behavior, as it reads from uninitialized local memory. Because of this, we use the <span><code>volatile</code></span> qualifier to suppress aggressive compiler optimizations that might optimize away the memory accesses. In fact, our code contains a few more code patterns included to further stop the compiler from optimizing away our memory dump. This process is more of a trial-and-error process than a science.</p>
<p>For each loop iteration, the invocation (thread) is read from a location in local memory, and that location is dumped to a unique location in the <span><code>dump</code></span> array. The only tricky part of this code is the indexing, because local memory is disjointed across workgroups, so workgroup local IDs need to be mapped to a unique global ID in <span><code>dump</code></span>. The process utilizes <span>built-in identifiers</span> to achieve this, which are documented <a href="https://registry.khronos.org/OpenCL/specs/3.0-unified/html/OpenCL_C.html#work-item-functions">here</a>. At the end of the kernel, dump contains every value that was stored in local memory when the listener kernel started executing. Because dump is in the global memory region, it can be examined by the CPU host code to check for canary values.</p>
<p><strong><em>The Writer</em></strong>: On the other hand, the Writer launches a kernel that writes a canary value to local memory (for example, this work uses the value 123). We show an example of the OpenCL kernel code below:</p>
<pre>__kernel void writer(__global volatile int *<span>canary</span>) {
  local volatile int lm[LM_SIZE];
  for (uint i = get_local_id(0); i &lt; LM_SIZE; i+=get_local_size(0)) {
    lm[i] = <span>canary</span>[i];
  }
}
</pre>
<p>This code is very similar to the Listener, except that rather than dumping local memory, we are writing a value. In this case, we are writing a value from an array <span><code>canary</code></span>. We use an extra array so that the compiler does not optimize away the memory write (as it is prone to do with constant values). At the end of the kernel, the writer has filled all available local memory with the <span>canary</span> values.</p>
<p>The CPU programs for both the Listener and the Writer launch their respective kernels repeatedly. In the case of the listener, at each iteration, the CPU analyzes the values observed in the local memory and checks for the canary value. On a server, these two programs can be run by different users or in different Docker containers. On a mobile device, these routines can be run in different apps. The apps can be swapped in and out of focus to alternate reading and writing. <em>If the Listener can reliably read the canary values, then we say that the platform is vulnerable to LeftoverLocals.</em></p>
<p>The following animation shows how the listener and writer interact, and how the listener may observe values from the writer if local memory is not cleared.</p>

<p><strong>Figure 3:</strong> A Listener and a Writer processes, where the writer stores canary values in local memory, while a listener reads uninitialized local memory to check for the canary values</p>
<h3>Listening to LLM responses</h3>
<p>In this section, we provide an overview of how LeftoverLocals can be exploited by a malicious actor (an attacker) to listen to another user’s (the victim) LLM responses on a multi-tenant GPU machine, followed by a detailed description of the PoC.</p>
<p>At a high level, both actors are executed as co-resident processes. The attack process implements the listener described above, with the additional steps of comparing the stolen values to various fingerprints. The victim process is unknowingly the writer, where instead of canary values, the values being written are sensitive components of an interactive LLM chat session. The attack ultimately follows two steps:</p>
<ul>
<li>The attack process fingerprints the model that the victim process is using by repeatedly dumping (i.e., listening) to the leftover local memory, which, in this scenario, consists of sensitive components of linear algebra operations used by the victim in the LLM model architecture.</li>
<li>The attacker then repeatedly listens to the victim’s process again, specifically seeking for an LLM to execute the output layer, which can be identified using weights or memory layout patterns from the earlier fingerprinting.</li>
</ul>
<p>Note that the output layer is a matrix-vector multiplication with two inputs: the model weights, and the layer input—in other words, the values derived from the user input that propagated through the earlier levels of the deep neural network (DNN). Given that the model weights of the output layer are too large to comprehensively steal, an attacker can inspect available open-source models to fully obtain the weights through the exposed model fingerprint. We found that the second input to the last layer (i.e., the layer input) is subsequently small enough to fit into local memory. Thus, the entire layer input can be stolen, and the attacker can reproduce the final layer computation to uncover the final result of the DNN.</p>
<div id="attachment_105919"><p><img decoding="async" aria-describedby="caption-attachment-105919" data-attachment-id="105919" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig2/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?fit=1999%2C909&amp;ssl=1" data-orig-size="1999,909" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="figfig2" data-image-description="" data-image-caption="<p>Figure 4: Steps of the PoC exploit whereby an attacker process can uncover data to listen to another user’s interactive LLM session with high fidelity</p>
" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?fit=300%2C136&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?fit=690%2C314&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=690%2C314&amp;ssl=1" alt="" width="690" height="314" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=1024%2C466&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=300%2C136&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=768%2C349&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=1536%2C698&amp;ssl=1 1536w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?resize=1200%2C546&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?w=1999&amp;ssl=1 1999w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig2.png?w=1380&amp;ssl=1 1380w" sizes="(max-width: 690px) 100vw, 690px" data-recalc-dims="1"></p><p id="caption-attachment-105919"><strong>Figure 4:</strong> Steps of the PoC exploit whereby an attacker process can uncover data to listen to another user’s interactive LLM session with high fidelity</p></div>
<p>We note that this is a fairly straightforward attack, and with further creativity and ingenuity, a threat actor may be able to construct further complex and sophisticated malicious scenarios that may compromise ML applications in more severe ways. Below we provide a detailed description of the PoC, and the configuration and testing carried out on various GPU platforms to uncover their susceptibility to LeftoverLocals.</p>
<p><strong><em>Our configuration</em></strong>: We outline our configuration in the table below. Our attack builds on the llama.cpp LLM due to its simplicity and variety of support for GPU acceleration. In our example we use a large discrete GPU that we found to be susceptible to LeftoverLocals: the AMD Radeon RX 7900 XT. We configure llama.cpp to use OpenCL for GPU acceleration, which uses the CLBLAST linear algebra library. We use the wizardLM-7B.ggmlv3.q5_0.bin model, which <a href="https://huggingface.co/TheBloke/wizardLM-7B-GGML/tree/main">can be obtained</a> from Hugging Face. This model was selected due to its reasonable size, which enabled rapid prototyping and analysis; however, this attack is transferable to many different models. In our threat model, we assume that the victim is using the LLM in an interactive chat session.</p>
<p><img decoding="async" data-attachment-id="105921" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig3/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?fit=1258%2C504&amp;ssl=1" data-orig-size="1258,504" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="figfig3" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?fit=300%2C120&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?fit=690%2C276&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?resize=600%2C241&amp;ssl=1" alt="" width="600" height="241" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?resize=1024%2C410&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?resize=300%2C120&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?resize=768%2C308&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?resize=1200%2C481&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig3.png?w=1258&amp;ssl=1 1258w" sizes="(max-width: 600px) 100vw, 600px" data-recalc-dims="1"></p>
<p><strong><em>Modification</em></strong>: The attack requires an optimized GPU implementation of matrix-vector multiplication. We found that the current matrix-vector multiplication in llama.cpp (which does not call into CLBLAST) is not implemented in an optimized idiomatic way. It stores partial dot product results in local memory and then combines them at the end. While there is a more complex approach using linear algebra to achieve our same results, for the simplicity of our PoC and demonstration, we replace the llama.cpp matrix-vector multiplication with our own that is more idiomatic (following best GPU programming programming practices).</p>
<p><strong><em>Step 1—Fingerprinting the model</em></strong>: An attacker can fingerprint a model if it can listen to several inference queries from the victim. In our configuration, the GPU contains roughly 5MB of local memory. The model has roughly 33 layers, each of them consisting of a matrix multiplication operation. Matrix multiplication is often optimized on GPUs by using tiling: an approach that subdivides the matrices into small matrices, performs the multiplication, and then combines the results (as <a href="https://cnugteren.github.io/tutorial/pages/page4.html">detailed here</a>). In many optimized libraries, including CLBLAST, local memory is used to cache the smaller matrices. Thus, for every layer, the attacker can steal ~2.5MB of weights, and ~2.5MB of the inputs. While this is a significant amount of data, we note that it is not enough to reconstruct the entire computation. Many of these layers have weights and inputs that are 100s of MB large.</p>
<p>However, for a whole inference computation (33 layers), the attacker can steal around 80MB of the weights, which is sufficient to fingerprint the model (assuming the user is using an open-source model, such as one that can be found on Hugging Face). Given this, we assume that it is a straightforward task to fingerprint the model, and thus for the attacker to obtain the full model being used by the victim.</p>
<p><strong><em>Step 2—Listening to the LLM output</em></strong>: The attacker can then turn their attention to the output layer of the DNN. In our configuration, we found that the output layer is a matrix-vector multiplication, rather than a matrix-matrix multiplication. The weights matrix is large (~128MB), but the input vector is quite small (~4KB). However, given that the attacker has fingerprinted the model in step 1, the attacker does not need to comprehensively steal the weights as they are available from the fingerprinted model.</p>
<p>Matrix-vector multiplication has a different GPU implementation than matrix-matrix multiplication. In the case where the input vector fits in local memory, the most performant implementation is often to cache the input vector in local memory, as it is used repeatedly (i.e., for repeated dot products). Because the input vector is stored entirely in local memory, the attacker can steal this entire vector. In determining whether the attacker has found local memory from the output layer, we discovered that the attacker could simply look for 4KB of floating point values with zeros on either side. In our testing, this unique fingerprint was associated with the output layer nearly every single time. For different models and different GPUs, this fingerprint will likely have to be recalibrated.</p>
<p><strong><em>Putting it together</em></strong>: With an attacker in possession of both the weights and the input vector, they can perform the final computation and obtain the result of the inference. This allows the attacker to reproduce the output of the victim’s LLM chat session with high fidelity, as demonstrated in the introduction. In practice, we tuned the attacker to dump the local memory very efficiently (that is, by using only a small number of threads and requiring a small amount of memory). This allows the attacker to listen to long chat queries with only a small number of noticeable artifacts. Some of the artifacts observed include:</p>
<ul>
<li><em>Duplicate tokens</em>: This occurs when the attacker steals the same output layer twice due to circumstances such as the attacker process being scheduled twice in a row, thus the LLM was not scheduled to compute its next token.</li>
<li><em>Missing tokens</em>: This occurs when the attacker kernel isn’t scheduled at the right time, i.e., immediately after the output layer computation kernel.</li>
<li><em>Incorrect tokens</em> outputted occurring due to:</li>
<li>the attacker mis-identifying a stolen set of data to be the last layer. In this case, it will print a junk token.</li>
<li>Production of a token that is “close” to the original output, even if it is not exact. That is, the attacker may be unable to steal the exact token embedding at the target layer. This results in a corrupted token embedding which, when decoded, is semantically similar (in the word2vec sense) to the original token. As an example, in the GIF provided at the beginning, the attacker extracts the incorrect word “Facebook”, which is semantically similar to other Named Entities tokens (like “Google”, and “Amazon”) in the generated text.</li>
</ul>
<p>Despite these discrepant artifacts, the stolen text is more than sufficient to uncover the LLM response. Additionally, the attacker can be further tuned by, for example, having multiple threads launch the listener kernel or by having a more precise fingerprint of the last layer.</p>
<h3>Testing GPU platforms for LeftoverLocals</h3>
<p>Given the diversity of the devices we tested, there exists several applications that can test for LeftoverLocals written in a variety of frameworks:</p>
<ul>
<li><strong>Vulkan Command Line</strong>: A command line application using Vulkan. The kernel is written in OpenCL and compiled to SPIR-V using <a href="https://github.com/google/clspv">clspv</a>. It uses a simple Vulkan wrapper called <a href="https://github.com/ucsc-chpl/easyvk">EasyVK</a>.</li>
<li><strong>OpenCL Command Line</strong>: A command line application that uses the OpenCL framework.</li>
<li><strong>Apple App</strong>: An Apple app that can be deployed on iOS or Mac OS. It targets the GPU using Apple’s Metal framework.</li>
<li><strong>Android App</strong>: An Android app that uses Vulkan to target mobile GPUs. The code uses Vulkan’s C API (through EasyVK again) using JNI. The kernels are the same as in the Vulkan command line app: they are written in OpenCL and compiled to SPIR-V using clspv.</li>
</ul>
<p>Using the above programs, we tested 11 devices spanning seven GPU vendors (and multiple GPU frameworks in some cases). We observed LeftoverLocals on devices from three of the vendors (Apple, Qualcomm, and AMD). The amount of memory leaked depends on the size of the GPU. Larger GPUs contain more physical memory, and thus, leak more data. For the larger GPUs (e.g., an AMD Radeon RX 7900 XT), we found that we can leak over ~5MB per kernel. The following tables outlines the system info for the GPUs we were able to observe LeftoverLocals (QC refers to Qualcomm):</p>
<p><img loading="lazy" decoding="async" data-attachment-id="105923" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig4/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?fit=1576%2C404&amp;ssl=1" data-orig-size="1576,404" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="figfig4" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?fit=300%2C77&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?fit=690%2C177&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=690%2C177&amp;ssl=1" alt="" width="690" height="177" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=1024%2C262&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=300%2C77&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=768%2C197&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=1536%2C394&amp;ssl=1 1536w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?resize=1200%2C308&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?w=1576&amp;ssl=1 1576w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig4.png?w=1380&amp;ssl=1 1380w" sizes="(max-width: 690px) 100vw, 690px" data-recalc-dims="1"></p>
<p>For some devices, specifically those from Arm, we were not able to observe the canary value from the Writer in the Listener, but we did observe non-zero data. Representatives from Arm reviewed our observations and concluded that although these values are not zero, they are not from a memory leak.</p>
<p><img loading="lazy" decoding="async" data-attachment-id="105924" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig5/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?fit=1508%2C216&amp;ssl=1" data-orig-size="1508,216" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="figfig5" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?fit=300%2C43&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?fit=690%2C99&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?resize=690%2C99&amp;ssl=1" alt="" width="690" height="99" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?resize=1024%2C147&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?resize=300%2C43&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?resize=768%2C110&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?resize=1200%2C172&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?w=1508&amp;ssl=1 1508w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig5.png?w=1380&amp;ssl=1 1380w" sizes="(max-width: 690px) 100vw, 690px" data-recalc-dims="1"></p>
<p>Additionally, we tested some GPUs from NVIDIA, Intel, and Imagination. For these devices, we observed only zeros in local memory, and thus did not observe LeftoverLocals. <span>It is unclear if all their devices are not impacted. For example, although we did not observe the issue on our </span><span>Imagination device, Google notified us that they were able to observe it on other Imagination devices.</span></p>
<p><img loading="lazy" decoding="async" data-attachment-id="105925" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig6/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?fit=1596%2C272&amp;ssl=1" data-orig-size="1596,272" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="figfig6" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?fit=300%2C51&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?fit=690%2C118&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=690%2C118&amp;ssl=1" alt="" width="690" height="118" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=1024%2C175&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=300%2C51&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=768%2C131&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=1536%2C262&amp;ssl=1 1536w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?resize=1200%2C205&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?w=1596&amp;ssl=1 1596w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig6.png?w=1380&amp;ssl=1 1380w" sizes="(max-width: 690px) 100vw, 690px" data-recalc-dims="1"></p>
<p>The following YouTube video demonstrates the different interfaces and examples of LocalLeftovers—namely the LLM PoC attack, covert communication channels, and searching for canary values—on a few different platforms using a few different applications.</p>
<p><iframe loading="lazy" title="YouTube video player" src="https://www.youtube.com/embed/g2A7GvbnItg?si=w0tvRgk2Kn1YdcX7" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p><strong><em>Vulnerable environments</em></strong>: An attack program must be co-resident on the same machine and must be “listening” at the same time that the victim is running a sensitive application on the GPU. This could occur in many scenarios: for example, if the attack program is co-resident with the victim on a shared cloud computer with a GPU. On a mobile device, the attack could be implemented in an app or a library. Listening can be implemented efficiently, and thus can be done repeatedly and constantly with almost no obvious performance degradation.</p>
<p>Next, we briefly discuss other environments where GPUs are either deployed or where an attacker might have access to sensitive information. Although it appears that some current systems (e.g., WebGPU) are not currently impacted, the ever-growing prevalence of ML and the diversity of modern GPUs mean that the next iteration of these systems (or other near-future systems) may be severely compromised by these types of vulnerabilities.</p>
<ul>
<li><strong>Cloud providers</strong>: Cloud providers (e.g., AWS and Azure) are unlikely to provide shared GPU instances, especially if users have dedicated access to the GPU machine. In other cases, GPUs could be shared using very conservative GPU VM technology (such as NVIDIA’s vGPU or MxGPU), which physically partitions the GPU and therefore prevents users from sharing GPU resources (e.g., local memory). Given this, many current cloud GPU systems may not currently be vulnerable to LeftoverLocals; however, we do not have conclusive evidence to determine this given the general lack of visibility into the specification and implementation of these systems. We note that we have observed LeftoverLocals on multi-user Linux servers, as well as on desktop (Windows and Mac) systems through traditional multi-processing. This includes Docker containers on these systems.</li>
<li><strong>Mobile applications</strong>: In our experiments and explorations in the mobile domain, we were able to run concurrent GPU processes (from different apps on iOS or Android) only in very specific instances. That is, we were not able to run a GPU process (e.g., from a malicious listener app) in the background while other apps (e.g., the victim) were run in the foreground. As with our analysis of cloud providers, we were unable to find clear documentation that explicitly detailed these constraints, and so we cannot definitively claim whether they are vulnerable. However, as seen in the video above, LeftoverLocals can be exploited either when a malicious listener app is run side-by-side with a victim app, or if the malicious listener app is quickly swapped from the background into the foreground from a victim app.</li>
<li><strong>Remote attacks</strong>: We preliminarily investigated the possibility of attacks originating from websites (e.g., those hosted by a remote attacker). To our knowledge, web applications do not have the low-level features required to listen to local memory using GPU graphics frameworks, such as WebGL. We note that the new WebGPU framework does provide low-level capabilities that allow a webpage to access local memory. Conservatively, WebGPU initializes and performs dynamic array bounds checking on local memory (and global memory), which mitigates this vulnerability. However, these checks cause significant overhead, as documented in discussions like <a href="https://github.com/gpuweb/gpuweb/issues/1202">this one</a>. To test this further, our code repo contains a simple listener in WebGPU. As expected, we have only observed zeros in local memory, even on devices that are vulnerable to LeftoverLocals through other frameworks. However, GPU compilers are <a href="https://medium.com/@afd_icl/crashes-hangs-and-crazy-images-by-adding-zero-689d15ce922b">known to be fragile</a>, and it is not difficult to imagine finding a compiler bug that could somehow bypass these checks (especially using fuzzing techniques). Our position is that LocalLeftovers should be addressed at a lower level (e.g., the driver).</li>
</ul>
<p><strong>How GPU vendors can resolve this vulnerability</strong>: To defend against LocalLeftovers, GPUs should clear their local memory between kernel calls. While this could cause some performance overhead, our experiments show that many GPU vendors (e.g., NVIDIA, Intel) currently appear to provide this functionality. It even appears that some of this functionality is provided for impacted GPUs. For example, Mesa drivers <a href="https://github.com/Mesa3D/mesa/blob/957009978ef6d7121fc0d710d03bc20097d4d46b/src/amd/vulkan/radv_shader.c#L709">for AMD GPUs</a> clears local memory after a compute kernel launch. However, this approach has a fundamental flaw that makes it vulnerable to LeftoverLocals: this memory wipe is done with a separate kernel, thus, the GPU kernel queue may contain a malicious listener between the computation kernel and the local memory wipe, allowing the listener to steal memory. Instead, the computation kernel and the local memory wipe need to occur atomically, i.e., without allowing any other kernel to be interleaved between them. Otherwise, a user may attempt to preemptively defend themselves against LeftoverLocals as described in the next section.</p>
<p><strong>Mitigations</strong>: In light of a lack of comprehensive patches across impacted GPU vendors, LeftoverLocals can be defended by modifying the source code of all GPU kernels that use local memory. As we’ve previously noted, before the kernel ends, the GPU threads should store 0 to any local memory locations that were used in the kernel. Given that GPU tasks are typically interleaved at the kernel boundary, this will prevent another user from being able to read leftover values. We note that this mitigation may be difficult for many users, especially because GPU code is often buried deep in complex software stacks (e.g., for ML). Furthermore, the GPU code may be part of a highly optimized library (e.g., ML linear algebra routines). In these cases, it is very difficult to identify how local memory is used, and even more difficult to modify the kernel to zero it out. It may be possible to augment a compiler to add this functionality, similar to how WebGPU handles GPU memory accesses (described above). These mitigations do have a performance overhead that should be taken into account. Another blunt mitigation involves simply avoiding multi-tenant GPU environments.</p>
<h2>Impact on LLMs and GPU platforms</h2>
<h3>LLM security</h3>
<p>Our PoC attack examines only one application: an interactive open-source LLM session. However, with a little creativity, attackers could likely target many GPU applications, including those used within privacy-sensitive domains. Our motivation stems from the recent increased use and support of open-source models, often accompanied by claims that their “openness” inherently entails safety and security through transparency. A recent article in <a href="https://www.nature.com/articles/d41586-023-03803-y">Nature</a> even alleges that only open-source generative AI models can “safely” revolutionize health care, a safety-critical domain. Yet, even if open-source models provide the opportunity to be rigorously audited and assessed (<a href="https://blog.trailofbits.com/2023/11/15/assessing-the-security-posture-of-a-widely-used-vision-model-yolov7/">which they have yet to be</a>), their deployment still hinges on a closed-source stack (i.e., GPUs). And as demonstrated by LeftoverLocals, open-source LLMs are particularly susceptible to our vulnerability given our ability to fingerprint these models to obtain remaining weights as needed. Indeed, we have already observed announcements regarding the deployment of open-source models in collaboration with impacted GPU vendors, including <a href="https://twitter.com/AMD/status/1744831880241750112">Hugging Face’s collaboration with AMD</a>, <a href="https://www.lamini.ai/blog/lamini-amd-paving-the-road-to-gpu-rich-enterprise-llms">Lamini’s deployment on AMD GPUs</a>, and the <a href="https://www.qualcomm.com/news/releases/2023/07/qualcomm-works-with-meta-to-enable-on-device-ai-applications-usi">Qualcomm and Meta partnership</a> for edge devices.</p>
<p>Generally, the introduction of ML poses new attack surfaces that traditional threat models do not account for, and that can lead to implicit and explicit access to data, model parameters, or resulting outputs, increasing the overall attack surface of the system. It is crucial to identify and taxonomize novel classes of failure modes that directly impact ML models, in addition to novel threats that can compromise the ML Ops pipeline, as we have demonstrated with LeftoverLocals. We discuss GPU-specific threat implications in the following section.</p>
<h3>GPU providers, applications, and vendors</h3>
<p>While many platforms are not currently impacted (see Vulnerable environments), we emphasize that the GPU compute landscape is evolving rapidly. As some examples: <a href="https://cloud-gpus.com/">a growing number of GPU cloud providers</a> have various policies and available configurations; and GPU programming frameworks, such as Vulkan and Metal, are well-supported on mainstream platforms, and can be used in apps without requiring extra privileges. While these developments are exciting, they increase the threat potential of GPU vulnerabilities, as LeftoverLocals illustrates. As far as we are aware, there is no unified security specification for how GPUs are required to handle sensitive data, and no portable test suite to check if systems are vulnerable to simple memory leaks, like LeftoverLocals. Thus, GPU compute environments should be rigorously scrutinized when used for processing any type of sensitive data.</p>
<p>As mentioned above, while we focus on LLM applications, GPU local memory is one of the first tools that a GPU developer uses when optimizing an application. Although other attacks would likely require analyzing the victim’s GPU kernel code to identify local memory usage, other attacks are likely possible in GPU compute domains, such as image processing and scientific computing. It will likely be increasingly difficult for users to detect and defend against these attacks since it’s unlikely they will know if their application is vulnerable to LeftoverLocals; this would require knowing the details of the exact GPU kernel code, which are often hidden away in highly optimized linear algebra libraries (e.g., <a href="https://github.com/CNugteren/CLBlast">CLBLAST</a>). Additionally, an overall lack of specification in up-and-coming GPU platforms makes it difficult to determine whether the compiler or runtime will use impacted memory regions without the user knowing. For example, Apple GPUs have a new caching mechanism, called <a href="https://www.digitaltrends.com/computing/apple-dynamic-caching-explained/">dynamic caching</a>, that does not have a clear specification regarding if local memory regions are being used for other purposes.</p>
<h2>Coordinated disclosure</h2>
<p>Since September 2023, we have been working CERT/CC on a large coordinated disclosure involving all major GPU vendors, including NVIDIA, Apple, AMD, Arm, Intel, Qualcomm, and Imagination. Trail of Bits provided vendors a total of 125 days to test their products and provide remediations. The coordination gradually grew to include software stakeholders, including Google, Microsoft, and others, which allowed us to understand how LocalLeftovers impacts privacy requirements and impact at different stages in the ML supply chain. Apple did not respond or engage with us regarding the disclosure.</p>
<p>A high-level timeline of the disclosure is provided below:</p>
<ul>
<li>September 8, 2023: Trail of Bits submitted report to the CERT/CC</li>
<li>September 11, 2023: CERT/CC acknowledged the submission of LeftoverLocals and began the process of vendor outreach and CVE assignment with a preliminary disclosure date of December 11, 2023</li>
<li>September 14, 2023: AMD acknowledged the CERT disclosure</li>
<li>September 15, 2023: Qualcomm acknowledged the CERT disclosure</li>
<li>September 22, 2023: The case report was shared with Khronos and OpenCL working group</li>
<li>September 29, 2023: NVIDIA acknowledged disclosure and confirmed they were not affected by the vulnerability</li>
<li>November 22, 2023: ToB extended release of embargo to January 16, 2024 to accommodate for vendor requests for further time</li>
<li>January 11, 2024: We received a notice that Qualcomm provided a patch to their firmware that addresses this issue only for some of their devices. Additionally, Google noted that ChromeOS Stable 120 and LTS 114 will be released on January 16 to include AMD and Qualcomm mitigations.</li>
<li><span>January 13, 2024: Apple confirmed that the A17 and M3 series processors contain fixes to the vulnerability.</span></li>
<li><span>January 14, 2024: Google notified us that they observed that that some Imagination GPUs are impacted.</span></li>
<li>January 16, 2024: Embargo lift and public disclosure of LeftoverLocals</li>
</ul>
<h2>Moving forward</h2>
<p>Now that GPUs are being used in a wide range of applications, including privacy sensitive applications, we believe that the wider GPU systems community (vendors, researchers, developers) must work towards hardening the GPU system stack and corresponding specifications. This should be accomplished through robust, holistic specifications that describe both GPU programs’ behavior and how GPU devices integrate with the rest of the system stack (e.g., the OS or hypervisor). Furthermore, these specifications should be rigorously tested to account for the diversity of GPU systems and safety requirements of diverse application domains. Looking forward, a wide variety of <a href="https://www.theinformation.com/articles/the-twelve-startups-battling-for-a-slice-of-nvidias-pie?utm_source=ti_app">new AI chips</a> are being developed and will require rigorous security analysis.</p>
<p>There are positive developments in this direction. For example, AMD’s <a href="https://www.amd.com/en/products/software/rocm.html">ROCm</a> stack is open, and thus available for independent rigorous evaluation, and the Khronos Group has <a href="https://www.khronos.org/syclsc">safety critical specification groups</a>. Additionally, cross-vendor programming frameworks, such as Vulkan, have been incredibly useful for writing portable test suites, as opposed to single-vendor programming frameworks.</p>
<p>While GPU security and privacy guarantees are scattered and scarce, the Vulkan <a href="https://registry.khronos.org/vulkan/specs/1.3-extensions/html/vkspec.html#fundamentals-validusage">specification</a> outlines a reasonable definition of security for GPU platforms to adhere to—a definition that several platforms clearly violate, as our results show:</p>
<blockquote><p><em>… implementations must ensure that […] an application does not affect the integrity of the operating system[…]. In particular, any guarantees made by an operating system about whether memory from one process can be visible to another process or not must not be violated by a Vulkan implementation for any memory allocation.</em></p></blockquote>
<p><span>Given the role of Khronos specifications in this result, we included the Khronos Group in the coordinated disclosure. They connected us with representatives of various impacted vendors, and engaged in fruitful discussions about security specifications and testing. Prior to the release, Khronos released this statement in support of this work:</span></p>
<blockquote><p><i><span>Khronos welcomes the work by Tyler Sorensen and Trail of Bits to increase security around the usage of Khronos APIs and have been working closely with them for several months to ensure that API implementers are aware and able to act on any issues. Khronos is also diligently exploring additional actions relating to API specifications, conformance testing, and platform vendor cooperation to continually strengthen safety and security when using Khronos compute and rendering APIs. – Neil Trevett, Khronos President</span></i></p></blockquote>
<p>With the dust settling, our position is the following: given the wide diversity of GPUs and their critical importance in enabling machine learning applications, these devices, and their ecosystems, are in need of (1) a detailed threat model that considers the various types of data processed on GPUs and how this data might be compromised; (2) an exploration of the GPU execution stack to determine where and how GPU security properties should be specified and implemented; and (3) significant testing and auditing to fortify GPU ecosystem, which is the computational foundation of machine learning.</p>
<p><em><span>For full transparency, we note that Tyler Sorensen has been an invited member of the Khronos group (sponsored by Google) since 2019, and participates in the memory model technical specification group.</span></em></p>
<p><strong>Acknowledgements</strong>: We thank Max Ammann, Dominik Czarnota, Kelly Kaoudis, Jay Little, and Adelin Travers for their insightful comments and feedback on the vulnerability, PoC, and throughout the disclosure process. We also thank the Khronos Group for discussing technical specification details with us, and providing an avenue for us to engage with many vendors. We thank CERT/CC, specifically Vijay Sarvepalli and Ben Koo, for organizing the coordinated disclosure, especially considering the potential breadth of the vulnerability. Thanks to Adam Sorensen and Trent Brunson for helping create the vulnerability logo. <span>Finally, t</span><span>hank you to everyone who engaged with us on this issue. T</span><span>his was a large project and we had </span><span>discussions</span><span> with many people who provided valuable insights and perspectives.</span></p>
<h2>Background: How GPUs work</h2>
<p>GPUs are massively parallel, throughput-oriented co-processors. While originally designed to accelerate graphics workloads, their design, which balances flexible programming and high computational throughput, has been highly effective in a variety of applications. Perhaps the most impactful current application domain is machine learning, where GPUs are the computational workhorse and achieve nearly all major results in this area.</p>
<p>GPUs are not only in large servers; they are in our phones, our tablets, and our laptops. These GPUs come from a variety of vendors, with almost all major hardware vendors (Apple, AMD, Arm, Qualcomm, Intel, and Imagination) producing their own GPU architecture. These GPUs are increasingly used for ML tasks, especially because doing ML locally can preserve users’ privacy, achieve lower latency, and reduce computational burdens on service providers.</p>
<p><strong>GPU architecture</strong>: GPU architecture has a parallel, hierarchical structure. At the top level, a GPU is made up of Compute Units (sometimes called Streaming Multiprocessors in NVIDIA literature). Large, discrete GPUs contain many compute units, and smaller, mobile GPUs have fewer. For example, the large AMD Radeon RX 7900 XT discrete GPU has 84 compute units, while the mobile Qualcomm Adreno 740 GPU has 8. All compute units have access to global memory. On discrete GPUs, global memory is implemented using VRAM; on integrated GPUs, global memory simply uses the CPU’s main memory.</p>
<p>Compute units encapsulate both compute and memory components. Compute units contain an array of processing elements; these simple cores are the fundamental units of computation and execute a stream of GPU instructions. In terms of memory, compute units often contain a cache for global memory, but they also contain a special region of memory called local memory. This is an optimized memory region that is shared only across processing elements in the same compute unit. This memory can be accessed with significantly less latency than global memory, but also has much smaller capacity. Different GPUs have varying amounts of local memory, typically ranging from 16KB to 64KB. For example, the AMD Radeon RX 7900 XT GPU has 84 compute units and a local memory size of 64KB; thus, the total amount of local memory on the GPU is ~5MB. Local memory is a software-managed cache: the program executing on the processing elements is responsible for loading values into local memory (e.g., values that will be repeatedly used from global memory).</p>
<p><img loading="lazy" decoding="async" data-attachment-id="105927" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig7/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?fit=1384%2C786&amp;ssl=1" data-orig-size="1384,786" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="figfig7" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?fit=300%2C170&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?fit=690%2C392&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?resize=604%2C343&amp;ssl=1" alt="" width="604" height="343" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?resize=1024%2C582&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?resize=300%2C170&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?resize=768%2C436&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?resize=1200%2C682&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig7.png?w=1384&amp;ssl=1 1384w" sizes="(max-width: 604px) 100vw, 604px" data-recalc-dims="1"></p>
<p><strong>GPU execution model</strong>: A GPU program, called a (GPU) kernel, is written in a shader language. Common examples are SPIR-V (Vulkan), OpenCL C, (OpenCL), and Metal Shading Language (Metal). These kernels specify a single entry point function, called the kernel function, which is executed by many invocations (i.e., GPU threads). Invocations have unique built-in identifiers (such as a global ID), which can be used to index a unique data element in a data-parallel program. Invocations are further partitioned into workgroups. Each workgroup is mapped to a compute unit (although many workgroups may execute on the same compute unit, depending on resource requirements). All invocations have access to the same global memory, but only invocations in the same workgroup will share the same local memory.</p>
<p>Applications that use the GPU often launch many short-running kernels. These kernels often correspond to basic operations, such as matrix multiplication or convolution. Kernels can then be executed in sequence; for example, each layer in a deep neural network will be a kernel execution. Local memory is statically allocated at each kernel launch and is not specified to persist across kernel calls.</p>
<p>Platforms generally do not time-multiplex different GPU kernels. That is, if multiple kernels are launched simultaneously (e.g., by different users), the GPU will execute one kernel to competition before the next kernel starts. Because GPU kernels are typically short running, sharing GPU resources at kernel boundaries saves expensive preemption overhead while also maintaining acceptable latency in practice.</p>
<p><strong>Terminology</strong>: Because this blog post focuses on portable GPU computing, it uses OpenCL GPU terminology. For readers more familiar with GPU terminology from a different framework (e.g., CUDA or Metal), we provide the following translation table:</p>
<p><img loading="lazy" decoding="async" data-attachment-id="105930" data-permalink="https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/figfig8/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?fit=1256%2C288&amp;ssl=1" data-orig-size="1256,288" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="figfig8" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?fit=300%2C69&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?fit=690%2C158&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?resize=611%2C140&amp;ssl=1" alt="" width="611" height="140" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?resize=1024%2C235&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?resize=300%2C69&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?resize=768%2C176&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?resize=1200%2C275&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/figfig8.png?w=1256&amp;ssl=1 1256w" sizes="(max-width: 611px) 100vw, 611px" data-recalc-dims="1"></p>

			</div><!-- .entry-content -->

	
</article><!-- #post-105881 -->
						<!-- #nav-below -->
		
					<!-- #comments .comments-area -->

			
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[On being listed as an artist whose work was used to train Midjourney (648 pts)]]></title>
            <link>https://catandgirl.com/4000-of-my-closest-friends/</link>
            <guid>39016395</guid>
            <pubDate>Tue, 16 Jan 2024 17:58:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://catandgirl.com/4000-of-my-closest-friends/">https://catandgirl.com/4000-of-my-closest-friends/</a>, See on <a href="https://news.ycombinator.com/item?id=39016395">Hacker News</a></p>
<div id="readability-page-1" class="page"><div href="" rel="bookmark" title="Permanent Link to Generation Gaps">
          <h2>4,000 of My Closest Friends</h2>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[5 minutes of coding yields a 6%+ boost to Linux I/O performance (112 pts)]]></title>
            <link>https://www.phoronix.com/news/Linux-Caching-Time-Block-IO</link>
            <guid>39016337</guid>
            <pubDate>Tue, 16 Jan 2024 17:53:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phoronix.com/news/Linux-Caching-Time-Block-IO">https://www.phoronix.com/news/Linux-Caching-Time-Block-IO</a>, See on <a href="https://news.ycombinator.com/item?id=39016337">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="LINUX STORAGE" src="https://www.phoronix.com/assets/categories/linuxstorage.webp" width="100" height="100"></p><p>
IO_uring creator and Linux block subsystem maintainer Jens Axboe spent about five minutes working on two patches to implement caching for issue-side time querying in the block layer and can yield 6% or more better I/O performance.
</p><p>
Axboe <a href="https://twitter.com/axboe/status/1747016366891442220">shared</a> about his latest interesting Linux I/O performance optimization, "<em>Something I've had in the back of my mind for years, and finally did it today. Which is kind of sad, since it was literally a 5 min job, yielding a more than 6% improvement. Would likely be even larger on a full scale distro style kernel config.</em>"
</p><p>
Axboe explained he typically disables iostats when testing due to the performance overhead of the time querying by default. But when providing some basic caching for the issue-side time querying, he's seeing around a 6% boost to IOPS and for a more bloated Linux distribution vendor kernel the gains are likely more significant.
</p><p><img src="https://www.phoronix.net/image.php?id=optane-linux-raid&amp;image=optane_raid_2_med" alt="Intel Optane storage"></p>
<p>He detailed in the <a href="https://lore.kernel.org/linux-block/20240115215840.54432-1-axboe@kernel.dk/">RFC patch series</a>:
</p><blockquote>"Querying the current time is the most costly thing we do in the block layer per IO, and depending on kernel config settings, we may do it many times per IO.
<p>
None of the callers actually need nsec granularity. Take advantage of that by caching the current time in the plug, with the assumption here being that any time checking will be temporally close enough that the slight loss of precision doesn't matter.
</p><p>
If the block plug gets flushed, eg on preempt or schedule out, then we invalidate the cached clock.
<br>...
<br>which is more than a 6% improvement in performance. Looking at perf diff, we can see a huge reduction in time overhead:
</p><p>
    10.55%     -9.88%  [kernel.vmlinux]  [k] read_tsc
<br>     1.31%     -1.22%  [kernel.vmlinux]  [k] ktime_get
</p><p>
Note that since this relies on blk_plug for the caching, it's only applicable to the issue side. But this is where most of the time calls happen anyway. It's also worth nothing that the above testing doesn't enable any of the higher cost CPU items on the block layer side, like wbt, cgroups, iocost, etc, which all would add additional time querying. IOW, results would likely look even better in comparison with those enabled, as distros would do."</p></blockquote>
<p>A nice win and hopefully this continues to pan out and prove useful for upstreaming with the Linux v6.9 cycle in a few months,</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bees have an internal sense of time (2022) (111 pts)]]></title>
            <link>https://greenrosechemistry.com/how-scientists-proved-that-bees-can-perceive-time/</link>
            <guid>39016040</guid>
            <pubDate>Tue, 16 Jan 2024 17:31:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://greenrosechemistry.com/how-scientists-proved-that-bees-can-perceive-time/">https://greenrosechemistry.com/how-scientists-proved-that-bees-can-perceive-time/</a>, See on <a href="https://news.ycombinator.com/item?id=39016040">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                        <p>Did you know that bees can perceive time? We don’t mean simple circadian rhythm stuff, like knowing when the sun is up. They can actually internally track the hours going by, which is presumably helpful when gathering pollen.</p>
<p>But how could we possibly know this? Well, like any science experiment, it started with a weird observation, and then scientists got curious and started guessing and testing how it works. A <a href="https://www.youtube.com/watch?v=xlGuBT5GT10">video on Tom Lun’s YouTube channel</a> brilliantly tells the story of in short format, but if you don’t feel like watching a video, read on!</p>
<h3>How to train a bee</h3>
<p>The investigation started back in 1929, when Ingeborg Beling, a German chronobiologist (no, really, it’s an <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5785655/">actual field</a>!) trained honeybees to come out of their hive at a specific time of day by putting sugar water out at that time. Unfortunately her 1929 paper is <a href="https://link.springer.com/article/10.1007/BF00340159">behind a paywall</a>, and also in German, but Beling’s groundbreaking work found that the bees’ memory of time (<em>Zeitgedächtnis</em>, if you want to really impress your friends) was quite precise and dependable, but only on a 24-hour cycle. Trying to train them to come out every 19 hours was no good, and every other day didn’t work either.</p>
<p>“Ah,” you may be thinking, “but that’s easy, there’s so many cues like sunshine and warmth to tell you the time!” So naturally the next step was to repeat the experiment in a dark, temperature-controlled room. Other factors eliminated were humidity, which is fairly easy to control, and air ionisation, which required using radioactive substances. Finally, O. Wahl decided to rule out cosmic radiation by <a href="https://psycnet.apa.org/record/1934-00291-001">repeating the experiment</a> 180 metres below ground, in a salt mine. Even in a salt mine, the bees were dependable timekeepers.</p>
<h3>Research intensifies</h3>
<p>That might seem like enough proof for most people, but not for chronobiologists. They carried out <a href="http://symposium.cshlp.org/content/25/361">more experiments</a>, raising bees in a special incubator to try and disrupt their 24-hour rhythm, speeding up or slowing down their metabolism with chemicals, and even cooling them down to near-freezing temperatures. This last one did actually have an effect–cooling a bee down for about five hours will make them 3-6 hours late for their sugar water appointment. So I guess now we know how to embarrass a bee.</p>
<h3>Bees get jet lag</h3>
<p>The final piece of proof was the most convincing. Researchers reasoned that if bees were relying on external factors for their 24-hour timekeeping, those factors must be linked to the earth’s 24-hour rotational period. Changing the bees’ longitude (or time zone, as most people call it) would prove whether they were relying on external factors or their internal sense of time. Max Renner, yet another German chronobiologist, <a href="http://symposium.cshlp.org/content/25/361">settled the matter</a> once and for all by training some bees in Paris, then flying them on a red-eye to New York. He found that the bees were jet-lagged, searching for food at their normal Paris time. He followed it up by showing that if the experiments were done outside, the bees would gradually recover from jet lag, just like humans, using the sun to reset their internal clocks!</p>
<p>So, in short, we now know that bees have an internal sense of time, and also that chronobiology conferences are probably pretty wild.</p>
<p><img fetchpriority="high" decoding="async" src="https://greenrosechemistry.com/wp-content/uploads/2022/03/bees-4812811_1920-1024x254.jpg" alt="bees" width="1024" height="254" srcset="https://greenrosechemistry.com/wp-content/uploads/2022/03/bees-4812811_1920-1024x254.jpg 1024w, https://greenrosechemistry.com/wp-content/uploads/2022/03/bees-4812811_1920-300x75.jpg 300w, https://greenrosechemistry.com/wp-content/uploads/2022/03/bees-4812811_1920-768x191.jpg 768w, https://greenrosechemistry.com/wp-content/uploads/2022/03/bees-4812811_1920-1536x382.jpg 1536w, https://greenrosechemistry.com/wp-content/uploads/2022/03/bees-4812811_1920.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></p>
<h3>Why do we care?</h3>
<p>These studies show that bees (and insects as a whole) have better cognitive abilities than we previously thought. Bees have tiny brains – only <a href="https://www.imperial.ac.uk/news/171050/bee-brains-have-never-seen-them/">2 cubic millimetres</a>, which is about 0.0002% the size of a human brain. We’re now learning that they can do more with that than we expected. Studies have suggested that time measuring could be a marker for more complex cognitive functions in the species, so now we’re interested in what else we can discover about bees.</p>
<p>In terms of timekeeping, we’ve learned that <a href="https://www.cell.com/current-biology/comments/S0960-9822(06)01843-4">bees can measure short intervals</a> of 6-36 seconds, and may even keep track of multiple different timings at once. That’s more than most humans can do!</p>
<h3>How does it work?</h3>
<p>The short answer is, we don’t know yet, and the bees aren’t telling. A number of potential timekeeping methods have been suggested: counting heartbeats, <a href="https://news.vanderbilt.edu/2021/05/20/research-snapshot-bees-can-tell-time-by-temperature-vanderbilt-research-finds/">temperature cycles in the hive</a>, or even simply an innate sense of time passing. It may be that the short and long timing functions depend on two different mechanisms. This area doesn’t get a huge amount of research funding, so it may be some time before scientists figure it out.</p>
<h3>Further reading</h3>
<p>‘<a href="https://www.pnas.org/doi/10.1073/pnas.1408039111" target="_blank" rel="noopener"><em>Way-finding in displaced clock-shifted bees prove bees use a cognitive map</em></a>’ by J.F. Cheeseman <em>et al</em>., <em>P.N.A.S.</em>, 2014</p>

                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Understanding x86_64 Paging (143 pts)]]></title>
            <link>https://zolutal.github.io/understanding-paging/</link>
            <guid>39015377</guid>
            <pubDate>Tue, 16 Jan 2024 16:44:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zolutal.github.io/understanding-paging/">https://zolutal.github.io/understanding-paging/</a>, See on <a href="https://news.ycombinator.com/item?id=39015377">Hacker News</a></p>
<div id="readability-page-1" class="page"><section itemprop="text">
        
        <p>I’ve spent quite a lot of time messing with x86_64 page tables, understanding address translation is not easy and when I started learning about it I felt like a lot of the material out there on how it works was hard for me to wrap my head around. So in this blog post I am going to attempt to provide a kind of “what I wish I had when learning about paging”.</p>

<p>Quick note, I’ll only be discussing paging in the context of PML4 (Page Map Level 4) since it’s currently the dominant x86_64 paging scheme and probably will be for a while still.</p>

<h2 id="environment">environment</h2>

<p>Its not necessary, but I recommend that you have a Linux kernel debugging setup with QEMU + gdb prepared to follow along with. If you’ve never done this, maybe give this repo a shot: <a href="https://github.com/deepseagirl/easylkb">easylkb</a> (I’ve never used it, but I’ve heard good things) or if you want to avoid having to setup the environment yourself, the practice mode on any of the Kernel Security challenges on <a href="https://pwn.college/">pwn.college</a> would also work (<code>vm connect</code> and <code>vm debug</code> are the commands to know).</p>

<p>I suggest this because I think running the same commands I am on your own and being able to perform a page walk based on what you can see in gdb is a good test of understanding.</p>

<h2 id="wtf-is-a-page">wtf is a page</h2>

<p>On x86_64 a page is a 0x1000 byte slice of memory which is 0x1000 byte aligned.</p>

<p>This is the reason why if you ever look at /proc/&lt;pid&gt;/maps you see that all the address ranges will start and end with an address ending with 0x000 because the minimum size of a memory mapping on x86_64 is page size (0x1000 bytes) and pages are required to be ‘page aligned’ (the last 12 bits must be zero).</p>

<p>A ‘Virtual Page’ will be resolved to a single ‘Physical Page’ (aka ‘Page Frame’) by your MMU though many Virtual Pages may refer to the same Physical Page.</p>

<h2 id="what-is-in-a-virtual-address">what is in a virtual address</h2>

<p>PML4, as one might guess, has four level of paging structures, these paging structures are called ‘Page Tables’. A page table is a page-sized memory region which contains 512 8-byte page table entries. Each entry of a page table will refer to either the next level page table or to the final physical address a virtual address resolves to.</p>

<p>The entry from a page table that is used for address translation is based on the virtual address of the memory access. With 512 entries per level, that means 9-bits of the virtual address are used at every level to index into the corresponding page table.</p>

<p>Say we have an address like this:</p>

<p><code>0x7ffe1c9c9000</code></p>

<p>The last 12 bits of this address represent the offset within the physical page:</p>

<p><code>0x7ffe1c9c9000 &amp; 0xfff = 0x0</code></p>

<p>This means that once we determine the physical address of the page this virtual address resolves to, we will add zero to the result to get the final physical address.</p>

<p>After the last 12 bits, which is again just the offset within the final page, a virtual address is comprised of indicies into the page tables. As mentioned each level of paging uses 9 bits of the virtual address, so the lowest level of the paging structures, a Page Table, is indexed by the next 9 bits of the address (by bit masking with <code>&amp; 0x1ff</code> on the shifted value). For the following levels we just need to shift right by another nine bits each time and again mask off the lower nine bits as our index. Doing this for the address above gives us these indicies:</p>

<div><pre><code>Level 1, Page Table (PT):
Index = (0x7ffe1c9c9000 &gt;&gt; 12) &amp; 0x1ff = 0x1c9

Level 2, Page Middle Directory (PMD):
Index = (0x7ffe1c9c9000 &gt;&gt; 21) &amp; 0x1ff = 0x0e4

Level 3, Page Upper Directory (PUD):
Index = (0x7ffe1c9c9000 &gt;&gt; 30) &amp; 0x1ff = 0x1f8

Level 4, Page Global Directory (PGD):
Index = (0x7ffe1c9c9000 &gt;&gt; 39) &amp; 0x1ff = 0x0ff
</code></pre></div>

<h2 id="all-your-base">all your base</h2>

<p>Now that we know how to index into page tables and vaguely what they contain, where actually are they???</p>

<p>Well each thread of your CPU has a page table base register called <code>cr3</code>.</p>

<p><code>cr3</code> holds the physical address of the highest level of the paging structure, aka the Page Global Directory (PGD).</p>

<p>From gdb, when debugging the kernel, you can read the contents of <code>cr3</code> like this:</p>

<div><pre><code>gef➤  p/x $cr3
$1 = 0x10d664000
</code></pre></div>

<p>The <code>cr3</code> register can hold some additional information besides just the PGD address depending on what processor features are in use, so a more general way of getting the physical address of the PGD from the <code>cr3</code> register is to mask off the lower 12 bits of its contents like so:</p>

<div><pre><code>gef➤  p/x $cr3 &amp; ~0xfff
$2 = 0x10d664000
</code></pre></div>

<h2 id="page-table-entries">page table entries</h2>

<p>Lets look at what is at that physical address we got from <code>cr3</code> in gdb. The <code>monitor xp/...</code> command that is exposed to gdb by the QEMU Monitor lets us print out the physical memory of the vm and doing <code>monitor xp/512gx ...</code> will print the entire contents, all 512 entries, of the PGD referred to by <code>cr3</code>:</p>

<div><pre><code>gef➤  monitor xp/512gx 0x10d664000
...
000000010d664f50: 0x0000000123fca067 0x0000000123fc9067
000000010d664f60: 0x0000000123fc8067 0x0000000123fc7067
000000010d664f70: 0x0000000123fc6067 0x0000000123fc5067
000000010d664f80: 0x0000000123fc4067 0x0000000123fc3067
000000010d664f90: 0x0000000123fc2067 0x000000000b550067
000000010d664fa0: 0x000000000b550067 0x000000000b550067
000000010d664fb0: 0x000000000b550067 0x0000000123fc1067
000000010d664fc0: 0x0000000000000000 0x0000000000000000
000000010d664fd0: 0x0000000000000000 0x0000000000000000
000000010d664fe0: 0x0000000123eab067 0x0000000000000000
000000010d664ff0: 0x000000000b54c067 0x0000000008c33067
</code></pre></div>

<p>This produces a lot of output and most of it is zero, so I’m only including the tail of the output here.</p>

<p>This output probably doesn’t mean much to you yet, but we can observe some patterns in the data, lots of the 8-byte entries end in <code>0x67</code>, for example.</p>

<h2 id="decoding-a-pgd-entry">decoding a PGD entry</h2>

<p>From the PGD output above, lets take the PGD entry at <code>0x000000010d664f50</code> with value <code>0x0000000123fca067</code> as an example to see how to decode an entry.</p>

<p>and lets do this with the binary representation of that entry’s value:</p>

<div><pre><code>gef➤  p/t 0x0000000123fca067
$6 = 100100011111111001010000001100111
</code></pre></div>

<p>Here is a little diagram to show what each bit in the entry represents:</p>

<div><pre><code>~ PGD Entry ~                                                   Present ──────┐
                                                            Read/Write ──────┐|
                                                      User/Supervisor ──────┐||
                                                  Page Write Through ──────┐|||
                                               Page Cache Disabled ──────┐ ||||
                                                         Accessed ──────┐| ||||
                                                         Ignored ──────┐|| ||||
                                                       Reserved ──────┐||| ||||
┌─ NX          ┌─ Reserved                             Ignored ──┬──┐ |||| ||||
|┌───────────┐ |┌──────────────────────────────────────────────┐ |  | |||| ||||
||  Ignored  | ||               PUD Physical Address           | |  | |||| ||||
||           | ||                                              | |  | |||| ||||
0000 0000 0000 0000 0000 0000 0000 0001 0010 0011 1111 1100 1010 0000 0110 0111
       56        48        40        32        24        16         8         0
</code></pre></div>

<p>and here’s a key for what each of those label mean:</p>

<ul>
  <li>NX (Not Executable) – if this bit is set, no memory mapping that is a descendant of this PGD entry will be executable.</li>
  <li>Reserved – these values must be zero.</li>
  <li>PUD Physical Address – the physical address of the PUD associated with this PGD entry.</li>
  <li>Accessed –  If any pages referred to by this entry or its descendants, this bit will be set by the MMU, and can be cleared by the OS.</li>
  <li>Page Cache Disabled (PCD) – pages descendant of this PGD entry should not enter the CPU’s cache hierarchy, sometimes also called the ‘Uncacheable’ (UC) bit.</li>
  <li>Page Write Through (WT) – writes to pages descendant of this PGD entry should immediately write to RAM rather than buffering writes to CPU cache before eventually updating RAM.</li>
  <li>User/Supervisor – if this bit is unset, pages descendant of this PGD cannot be accessed unless in supervisor mode.</li>
  <li>Read/Write – if this bit is unset, pages descendant of this PGD cannot be written to.</li>
  <li>Present – if this bit is unset then the processor will not use this entry for address translation and none of the other bits will apply.</li>
</ul>

<p>The bits that we really care about here are the the Present bit, the ones representing the physical address of the next level of the paging structures, the PUD Physical Address bits, and the permission bits: NX, User/Supervisor, and Read/Write.</p>

<ul>
  <li>The Present bit is super important because without it set the rest of the entry is ignored.</li>
  <li>The PUD Physical Address lets us continue page walking by telling us where the physical address of the next level of the paging structures is at.</li>
  <li>The Permission bits all apply to pages which are descendants of the PGD entry and determine how those pages are able to be accesssed.</li>
</ul>

<p>The remaining bits are not as important for our purposes:</p>
<ul>
  <li>The Accessed bit is set if the entry was used in translating a memory access, its not important for page walking.</li>
  <li>Page Cache Disabled and Page Write Through are not used for normal memory mappings and do not affect page translation or permissions so lets ignore them.</li>
</ul>

<p>So decoding this entry, we learn:</p>

<p>The PUD is Present:</p>
<div><pre><code>gef➤  p/x 0x0000000123fca067 &amp; 0b0001
$18 = 0x1
</code></pre></div>
<p>The mappings in the PUD and below may be able to be Writable:</p>
<div><pre><code>gef➤  p/x 0x0000000123fca067 &amp; 0b0010
$19 = 0x2
</code></pre></div>
<p>The mappings in the PUD and below may be able to be User accessible:</p>
<div><pre><code>gef➤  p/x 0x0000000123fca067 &amp; 0b0100
$20 = 0x4
</code></pre></div>
<p>The PUD’s physical address ( bits (51:12] ) is <code>0x123fca000</code>:</p>
<div><pre><code>gef➤  p/x 0x0000000123fca067 &amp; ~((1ull&lt;&lt;12)-1) &amp; ((1ull&lt;&lt;51)-1)
$21 = 0x123fca000
</code></pre></div>
<p>The mappings in the PUD and below may be able to be Executable:</p>
<div><pre><code>gef➤  p/x 0x0000000123fca067 &amp; (1ull&lt;&lt;63)
$22 = 0x0
</code></pre></div>

<h2 id="decoding-entries-for-all-levels">decoding entries for all levels</h2>

<p>Now that we’ve seen how to decode a PGD entry, decoding the rest of the levels aren’t so much different, at least in the common case.</p>

<p>For all of these diagrams ‘X’ means the bit can be either zero or one, otherwise, if a bit is set to a specific value then that value is either required by the architecture or by the specific encoding shown by the diagram.</p>

<h3 id="pgd">PGD</h3>

<div><pre><code>~ PGD Entry ~                                                   Present ──────┐
                                                            Read/Write ──────┐|
                                                      User/Supervisor ──────┐||
                                                  Page Write Through ──────┐|||
                                               Page Cache Disabled ──────┐ ||||
                                                         Accessed ──────┐| ||||
                                                         Ignored ──────┐|| ||||
                                                       Reserved ──────┐||| ||||
┌─ NX          ┌─ Reserved                             Ignored ──┬──┐ |||| ||||
|┌───────────┐ |┌──────────────────────────────────────────────┐ |  | |||| ||||
||  Ignored  | ||               PUD Physical Address           | |  | |||| ||||
||           | ||                                              | |  | |||| ||||
XXXX XXXX XXXX 0XXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX 0XXX XXXX
       56        48        40        32        24        16         8         0
</code></pre></div>

<p>This one we’ve already seen, I described it in detail in the previous section, but here it is without that specific PGD entry filled in.</p>

<h3 id="pud">PUD</h3>

<div><pre><code>~ PUD Entry, Page Size unset ~                                  Present ──────┐
                                                            Read/Write ──────┐|
                                                      User/Supervisor ──────┐||
                                                  Page Write Through ──────┐|||
                                               Page Cache Disabled ──────┐ ||||
                                                         Accessed ──────┐| ||||
                                                         Ignored ──────┐|| ||||
                                                      Page Size ──────┐||| ||||
┌─ NX          ┌─ Reserved                             Ignored ──┬──┐ |||| ||||
|┌───────────┐ |┌──────────────────────────────────────────────┐ |  | |||| ||||
||  Ignored  | ||               PMD Physical Address           | |  | |||| ||||
||           | ||                                              | |  | |||| ||||
XXXX XXXX XXXX 0XXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX 0XXX XXXX
       56        48        40        32        24        16         8         0
</code></pre></div>

<p>As you can see the diagram above for the PUD is very similar to the one for the PGD, the only difference is the introduction of the ‘Page Size’ bit. The Page Size bit being set changes how we need to interpret a PUD entry quite a lot. For this diagram we are assuming it is unset, which is the most common case.</p>

<h3 id="pmd">PMD</h3>

<div><pre><code>~ PMD Entry, Page Size unset ~                                  Present ──────┐
                                                            Read/Write ──────┐|
                                                      User/Supervisor ──────┐||
                                                  Page Write Through ──────┐|||
                                               Page Cache Disabled ──────┐ ||||
                                                         Accessed ──────┐| ||||
                                                         Ignored ──────┐|| ||||
                                                      Page Size ──────┐||| ||||
┌─ NX          ┌─ Reserved                             Ignored ──┬──┐ |||| ||||
|┌───────────┐ |┌──────────────────────────────────────────────┐ |  | |||| ||||
||  Ignored  | ||                PT Physical Address           | |  | |||| ||||
||           | ||                                              | |  | |||| ||||
XXXX XXXX XXXX 0XXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX 0XXX XXXX
       56        48        40        32        24        16         8         0
</code></pre></div>

<p>Again, the PMD diagram is very similar to the previous diagram, and like with the PUD entry, we are ignoring the Page Size bit for now.</p>

<h3 id="pt">PT</h3>

<div><pre><code>~ PT Entry ~                                                    Present ──────┐
                                                            Read/Write ──────┐|
                                                      User/Supervisor ──────┐||
                                                  Page Write Through ──────┐|||
                                               Page Cache Disabled ──────┐ ||||
                                                         Accessed ──────┐| ||||
┌─── NX                                                    Dirty ──────┐|| ||||
|┌───┬─ Memory Protection Key              Page Attribute Table ──────┐||| ||||
||   |┌──────┬─── Ignored                               Global ─────┐ |||| ||||
||   ||      | ┌─── Reserved                          Ignored ───┬─┐| |||| ||||
||   ||      | |┌──────────────────────────────────────────────┐ | || |||| ||||
||   ||      | ||            4KB Page Physical Address         | | || |||| ||||
||   ||      | ||                                              | | || |||| ||||
XXXX XXXX XXXX 0XXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX
       56        48        40        32        24        16         8         0
</code></pre></div>

<p>At the Page Table entry things get more interesting, there are some new fields/attributes that weren’t there in the previous levels.</p>

<p>Those new fields/attributes are:</p>

<ul>
  <li>Memory Protection Key (MPK or PK): This is an x86_64 extension that allows assigning a 4-bit keys to pages which can be used to configure memory permissions for all pages with that key.</li>
  <li>Global: This has to do with how the TLB (Translation Lookaside Buffer, the MMU’s cache for virtual to physical address translations) caches the translation for th page, this bit being set means the page will not be flushed from the TLB on context switch, this is commonly enabled on Kernel pages to reduce TLB misses.</li>
  <li>Page Attribute Table (PAT): If set, the MMU should consult the Page Attribute Table MSR when determining whether the ‘Memory Type’ of the page, e.g. whether this page is ‘Uncacheable’, ‘Write Through’, or one of a few other memory types.</li>
  <li>Dirty: This bit is similar to the accessed bit, it gets set by the MMU if this page was written to and must be reset by the OS.</li>
</ul>

<p>None of these actually affect the address translation itself, but the configuration of the Memory Protection Key can mean that the expected memory access permissions for the page referred to by this entry may be stricter than what is encoded by the entry itself.</p>

<p>Unlike the previous levels, since this is the last level, the entry holds the final physical address of the page associated with the virtual address we are translating. Once you apply a bit-mask to get the physical address bytes and add the last 12 bits of the original virtual address (the offset within the page), you have your physical address!</p>

<p>Hopefully, this doesn’t seem so bad, the general case of page walking is just a few steps:</p>
<ul>
  <li>Convert the virtual address to indicies and a page offset by shifting the address and applying bitmasks</li>
  <li>Read <code>cr3</code> to get the physical address of the PGD</li>
  <li>For each level until the last:
    <ul>
      <li>Use the indicies calculated from the virtual address to know what entry from the page table to use</li>
      <li>Apply a bitmask to the entry to get the physical address of the next level</li>
    </ul>
  </li>
  <li>On the final level, again find the entry corresponding with the index from the virtual address</li>
  <li>Apply a bitmask to get the physical address of the page associated with the virtual address</li>
  <li>Add offset within the page from the virtual address to the page’s physical address</li>
  <li>Done!</li>
</ul>

<h2 id="hugeify">hugeify</h2>

<p>As mentioned, the previous diagrams for the PUD and PMD are for the common case, when the Page Size bit is not set.</p>

<p>So, what about when it is set?</p>

<p>When it is set that is effectively telling the MMU, pack it up, we’re done here, don’t keep page walking, the current entry holds the physical address of the page we are looking for.</p>

<p>But there is a bit more to it than that, the physical address of the page in entries where the Page Size bit is set isn’t for a normal 4KB (0x1000 byte) page, it is a ‘Huge Page’ which comes in two variants: 1GB Huge Pages and 2MB Huge Pages.</p>

<p>When a PUD entry has the Page Size bit set then it refers to a 1GB Huge Page, and when a PMD has the Page Size bit set it refers to a 2MB Huge Page.</p>

<p>But where do the 1GB and 2MB numbers come from?</p>

<p>Each page table level holds up to 512 entries, that means a PT can refer to at most 512 pages and <code>512 * 4KB = 2MB</code>. So a Huge Page at the PMD level effectively means that the entry refers to a page that is the same size as a full PT.</p>

<p>Extending this to the PUD level, we just multiply by 512 again to get the size of a full PMD that has full PTs: <code>512 * 512 * 4KB = 1GB</code>.</p>

<h3 id="huge-page-pud">Huge Page PUD</h3>

<div><pre><code>~ PUD Entry, Page Size set ~                                     Present ─────┐
                                                             Read/Write ─────┐|
                                                       User/Supervisor ─────┐||
                                                   Page Write Through ─────┐|||
                                                Page Cache Disabled ─────┐ ||||
                                                          Accessed ─────┐| ||||
                                                            Dirty ─────┐|| ||||
┌─── NX                                                Page Size ─────┐||| ||||
|┌───┬─── Memory Protection Key                         Global ─────┐ |||| ||||
||   |┌──────┬─── Ignored                             Ignored ───┬─┐| |||| ||||
||   ||      | ┌─── Reserved           Page Attribute Table ───┐ | || |||| ||||
||   ||      | |┌────────────────────────┐┌───────────────────┐| | || |||| ||||
||   ||      | || 1GB Page Physical Addr ||      Reserved     || | || |||| ||||
||   ||      | ||                        ||                   || | || |||| ||||
XXXX XXXX XXXX 0XXX XXXX XXXX XXXX XXXX XX00 0000 0000 0000 000X XXXX 1XXX XXXX
       56        48        40        32        24        16         8         0
</code></pre></div>

<p>When the page size bit is set notice that the PUD entry looks more like a PT entry than a normal PUD entry, which makes sense because it is also referring to a page rather than a page table.</p>

<p>There are some distinctions from a PT entry though:</p>
<ol>
  <li>The Page Size bit is where the Page Attribute Table (PAT) bit is at on a PT, so the PAT bit is relocated to bit 12.</li>
  <li>The physical address of a 1GB Huge Page is required to have 1GB alignment in physical memory, this is why the new reserved bits exist and why bit 12 is able to be repurposed as the PAT bit.</li>
</ol>

<p>Overall, not too much new here, the only other differences when dealing with huge pages really is that a different bitmask needs to be applied to the address to get the bits for the physical address of the page, also the 1GB alignment means when calculating the physical address of a virtual address within the page we need to use a mask based on 1GB alignment instead of 4KB alignment.</p>

<h3 id="huge-page-pmd">Huge Page PMD</h3>

<div><pre><code>~ PMD Entry, Page Size set ~                                     Present ─────┐
                                                             Read/Write ─────┐|
                                                       User/Supervisor ─────┐||
                                                   Page Write Through ─────┐|||
                                                Page Cache Disabled ─────┐ ||||
                                                          Accessed ─────┐| ||||
                                                            Dirty ─────┐|| ||||
┌─── NX                                                Page Size ─────┐||| ||||
|┌───┬─── Memory Protection Key                         Global ─────┐ |||| ||||
||   |┌──────┬─── Ignored                             Ignored ───┬─┐| |||| ||||
||   ||      | ┌─── Reserved         Page Attribute Table ─────┐ | || |||| ||||
||   ||      | |┌───────────────────────────────────┐┌────────┐| | || |||| ||||
||   ||      | ||     2MB Page Physical Address     ||Reserved|| | || |||| ||||
||   ||      | ||                                   ||        || | || |||| ||||
XXXX XXXX XXXX 0XXX XXXX XXXX XXXX XXXX XXXX XXXX XXX0 0000 000X XXXX 1XXX XXXX
       56        48        40        32        24        16         8         0
</code></pre></div>

<p>This is very similar to the PUD entry with the Page Size bit set, the only thing that has changed is that since the alignment is smaller for the 2MB pages at this level, there are less reserved bits set.</p>

<p>The 2MB alignment means the offset within the huge page should be calculated using a mask based on 2MB alignment.</p>

<h2 id="going-for-a-walk">going for a walk</h2>

<p>So the last section was a lot of diagrams, in this section lets look at how to actually do a page walk manually in gdb.</p>

<h3 id="preparation">preparation</h3>

<p>With a booted up vm and gdb attached I first will pick an address to do a page walk on, as an example I’ll use the current stack pointer while running in the kernel:</p>

<div><pre><code>gef➤  p/x $rsp
$42 = 0xffffffff88c07da8
</code></pre></div>

<p>Now we have the address we are going to walk, lets also get the physical address of the PGD from <code>cr3</code>:</p>

<div><pre><code>gef➤  p/x $cr3 &amp; ~0xfff
$43 = 0x10d664000
</code></pre></div>

<p>I’ll use this little python function to extract the page table offsets from the virtual address:</p>

<div><pre><code><span>def</span> <span>get_virt_indicies</span><span>(</span><span>addr</span><span>):</span>
    <span>pageshift</span> <span>=</span> <span>12</span>
    <span>addr</span> <span>=</span> <span>addr</span> <span>&gt;&gt;</span> <span>pageshift</span>
    <span>pt</span><span>,</span> <span>pmd</span><span>,</span> <span>pud</span><span>,</span> <span>pgd</span> <span>=</span> <span>(((</span><span>addr</span> <span>&gt;&gt;</span> <span>(</span><span>i</span><span>*</span><span>9</span><span>))</span> <span>&amp;</span> <span>0x1ff</span><span>)</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>4</span><span>))</span>
    <span>return</span> <span>pgd</span><span>,</span> <span>pud</span><span>,</span> <span>pmd</span><span>,</span> <span>pt</span>
</code></pre></div>

<p>which outputs this:</p>
<div><pre><code><span>In</span> <span>[</span><span>2</span><span>]:</span> <span>get_virt_indicies</span><span>(</span><span>0xffffffff88c07da8</span><span>)</span>
<span>Out</span><span>[</span><span>2</span><span>]:</span> <span>(</span><span>511</span><span>,</span> <span>510</span><span>,</span> <span>70</span><span>,</span> <span>7</span><span>)</span>
</code></pre></div>

<h3 id="pgd-1">PGD</h3>

<p>The index we got for the PGD based on the virtual address was 511, multiplying 511 by 8 will let us get the byte offset into the PGD that the PGD entry for our virtual address starts at:</p>

<div><pre><code>gef➤  p/x 511*8
$44 = 0xff8
</code></pre></div>

<p>adding that offset to the PGD’s physical address gets us the physical address of the PGD entry:</p>

<div><pre><code>gef➤  p/x 0x10d664000+0xff8
$45 = 0x10d664ff8
</code></pre></div>

<p>and reading the physical memory at that address gets us the PGD entry itself:</p>
<div><pre><code>gef➤  monitor xp/gx 0x10d664ff8
000000010d664ff8: 0x0000000008c33067
</code></pre></div>

<p>Looks like the entry has the last three bits (present, user, and writeable) set, and the top bit (NX) is unset, meaning there aren’t any restrictions so far on the permissions of the pages associated with this virtual address.</p>

<p>Masking the bits [12, 51) gives us the physical address of the PUD:</p>

<div><pre><code>gef➤  p/x 0x0000000008c33067 &amp; ~((1&lt;&lt;12)-1) &amp; ((1&lt;&lt;51) - 1)
$46 = 0x8c33000
</code></pre></div>

<h3 id="pud-1">PUD</h3>

<p>The index we got for the PUD based on the virtual address was 510, multiplying 510 by 8 will let us get the byte offset into the PUD that the PUD entry for our virtual address starts at:</p>

<div><pre><code>gef➤  p/x 510*8
$47 = 0xff0
</code></pre></div>

<p>adding that offset to the PUD’s physical address gets us the physical address of the PUD entry:</p>

<div><pre><code>gef➤  p/x 0x8c33000+0xff0
$48 = 0x8c33ff0
</code></pre></div>

<p>and reading the physical memory at that address gets us the PUD entry itself:</p>
<div><pre><code>gef➤  monitor xp/gx 0x8c33ff0
0000000008c33ff0: 0x0000000008c34063
</code></pre></div>

<p>At this level we need to start paying attention to the Size Bit (bit 7), because if it is a 1GB page we would stop our page walk here.</p>

<div><pre><code>gef➤  p/x 0x0000000008c34063 &amp; (1&lt;&lt;7)
$49 = 0x0
</code></pre></div>

<p>Seems it is unset on this entry so we will continue page walking.</p>

<p>Notice also that the PUD entry ends in 0x3 and not 0x7 like the previous level, the bottom two bits (present, writeable) are still set but the third bit, the user bit is now unset. That means that usermode accesses to pages belonging to this PUD entry will result in a page fault due to the failed permission check on the access.</p>

<p>The NX bit is still unset, so pages belonging to this PUD can still be executable.</p>

<p>Masking the bits [12, 51) gives us the physical address of the PMD:</p>

<div><pre><code>gef➤  p/x 0x0000000008c34063 &amp; ~((1ull&lt;&lt;12)-1) &amp; ((1ull&lt;&lt;51)-1)
$50 = 0x8c34000
</code></pre></div>

<h3 id="pmd-1">PMD</h3>

<p>The index we got for the PMD based on the virtual address was 70, multiplying 70 by 8 will let us get the byte offset into the PMD that the PMD entry for our virtual address starts at:</p>

<div><pre><code>gef➤  p/x 70*8
$51 = 0x230
</code></pre></div>

<p>adding that offset to the PMD’s physical address gets us the physical address of the PMD entry:</p>

<div><pre><code>gef➤  p/x 0x8c34000+0x230
$52 = 0x8c34230
</code></pre></div>

<p>and reading the physical memory at that address gets us the PMD entry itself:</p>
<div><pre><code>gef➤  monitor xp/gx 0x8c34230
0000000008c34230: 0x8000000008c001e3
</code></pre></div>

<p>Again, at this level we need paying attention to the Size Bit, because if it is a 2MB page we will stop our page walk here.</p>

<div><pre><code>gef➤  p/x 0x8000000008c001e3 &amp; (1&lt;&lt;7)
$53 = 0x80
</code></pre></div>

<p>Looks like our virtual address refers to a 2MB Huge Page! so the physical address in this PMD entry is the physical address of that Huge Page.</p>

<p>Also, looking at the permission bits, looks like the page is still present and writeable and the user bit is still unset, so this page is only accessible from supervisor mode (ring-0).</p>

<p>Unlike the previous levels, the top bit, the NX bit, is set:</p>

<div><pre><code>gef➤  p/x 0x8000000008c001e3 &amp; (1ull&lt;&lt;63)
$54 = 0x8000000000000000
</code></pre></div>

<p>So this Huge Page is not executable memory.</p>

<p>Applying a bitmask on bits [21:51) gets us the physical address of the huge page:</p>

<div><pre><code>gef➤  p/x 0x8000000008c001e3 &amp; ~((1ull&lt;&lt;21)-1) &amp; ((1ull&lt;&lt;51)-1)
$56 = 0x8c00000
</code></pre></div>

<p>Now we need to apply a mask to the virtual address based on 2MB page alignment to get the offset into the Huge Page.</p>

<p>2MB is equivalent to <code>1&lt;&lt;21</code> so applying a bitmask of <code>(1ull&lt;&lt;21)-1</code> will get us the offset:</p>

<div><pre><code>gef➤  p/x 0xffffffff88c07da8 &amp; ((1ull&lt;&lt;21)-1)
$57 = 0x7da8
</code></pre></div>

<p>Now adding this offset to the base address of the 2MB Huge Page will get us the physical address associated with the virtual address we started with:</p>

<div><pre><code>gef➤  p/x 0x8c00000 + 0x7da8
$58 = 0x8c07da8
</code></pre></div>

<p>Looks like the Virtual Address: <code>0xffffffff88c07da8</code> has a Physical Address of: <code>0x8c07da8</code>!</p>

<h3 id="checking-ourselves">checking ourselves</h3>

<p>There are a few ways to test that we page walked correctly, an easy check is to just dump the memory at the virtual and physical address and compare them, if they look the same we were probably right:</p>

<p>Physical:</p>
<div><pre><code>gef➤  monitor xp/10gx 0x8c07da8
0000000008c07da8: 0xffffffff810effb6 0xffffffff88c07dc0
0000000008c07db8: 0xffffffff810f3685 0xffffffff88c07de0
0000000008c07dc8: 0xffffffff8737dce3 0xffffffff88c3ea80
0000000008c07dd8: 0xdffffc0000000000 0xffffffff88c07e98
0000000008c07de8: 0xffffffff8138ab1e 0x0000000000000000
</code></pre></div>

<p>Virtual:</p>
<div><pre><code>gef➤  x/10gx 0xffffffff88c07da8
0xffffffff88c07da8:	0xffffffff810effb6	0xffffffff88c07dc0
0xffffffff88c07db8:	0xffffffff810f3685	0xffffffff88c07de0
0xffffffff88c07dc8:	0xffffffff8737dce3	0xffffffff88c3ea80
0xffffffff88c07dd8:	0xdffffc0000000000	0xffffffff88c07e98
0xffffffff88c07de8:	0xffffffff8138ab1e	0x0000000000000000
</code></pre></div>

<p>Looks good to me!</p>

<p>Another way to check is using the <code>monitor gva2gpa</code> (guest virtual address to guest physical address) command exposed to gdb by the QEMU Monitor:</p>

<div><pre><code>gef➤  monitor gva2gpa 0xffffffff88c07da8
gpa: 0x8c07da8
</code></pre></div>

<p>Assuming QEMU is doing address translation correctly (probably a fair assumption), then looks like we have double confirmation that our page walk was successful!</p>

<h2 id="wrapping-up">wrapping up</h2>

<p>Hopefully by the end of this you have a pretty solid understanding of how paging works on x86_64 systems. I wanted to pack a lot of information into the post so it took some thought to figure out how to organize all of it and I’m still not sure if this was a great way to go about it.</p>

<p>Anyways, I think paging is pretty neat and I think its one of those things where once you get it you’ve got it, but getting to that point can take some time and some screwing around in gdb.</p>

<p>I’d also like to mention that the inspiration for the diagrams of the various page table entries I made for this post came from the documentation of the <a href="https://github.com/jart/blink/">blink</a> project: <a href="https://github.com/jart/blink/blob/46d82a0ced97c0df1fc645c5d81a88f0d142fbfd/blink/machine.h#L61">blink/machine.h</a>.</p>

<p>Thanks for reading!</p>

        
      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A NetBSD/amd64 guest can now boot in 18ms (104 pts)]]></title>
            <link>https://old.reddit.com/r/BSD/comments/197vfmq/a_netbsdamd64_guest_can_now_boot_in_40ms_details/</link>
            <guid>39015036</guid>
            <pubDate>Tue, 16 Jan 2024 16:15:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/BSD/comments/197vfmq/a_netbsdamd64_guest_can_now_boot_in_40ms_details/">https://old.reddit.com/r/BSD/comments/197vfmq/a_netbsdamd64_guest_can_now_boot_in_40ms_details/</a>, See on <a href="https://news.ycombinator.com/item?id=39015036">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I've been working the past 3 months into catching up with Colin Percival's work on FreeBSD with the Firecracker hypervisor.<br>
The result is that NetBSD/amd64 can now boot in PVH mode, i.e. directly into the kernel using qemu's -kernel flag or the PVH-enabled version of AWS's Firecracker.<br>
I then added support for MMIO kernel command line parameters, so NetBSD now supports memory mapped backed devices like ld(4) or vioif(4).
Last but not least, I've been tracking boot time until reaching 40ms from the assembly entry point to handling over to userland.</p>

<p>The latest branch with performances is here: <a href="https://github.com/NetBSDfr/NetBSD-src/tree/perf">https://github.com/NetBSDfr/NetBSD-src/tree/perf</a>
The experimental branch with "only" PVH and MMIO support is here: <a href="https://github.com/NetBSDfr/NetBSD-src/tree/mmio_cmdline">https://github.com/NetBSDfr/NetBSD-src/tree/mmio_cmdline</a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Benchmarks and comparison of LLM AI models and API hosting providers (133 pts)]]></title>
            <link>https://artificialanalysis.ai</link>
            <guid>39014985</guid>
            <pubDate>Tue, 16 Jan 2024 16:11:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://artificialanalysis.ai">https://artificialanalysis.ai</a>, See on <a href="https://news.ycombinator.com/item?id=39014985">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h3>Subscribe to our newsletter</h3><form><label for="email">Email address</label></form></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TinyML: Ultra-low power Machine Learning (336 pts)]]></title>
            <link>https://www.ikkaro.net/what-tinyml-is/</link>
            <guid>39014866</guid>
            <pubDate>Tue, 16 Jan 2024 16:03:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ikkaro.net/what-tinyml-is/">https://www.ikkaro.net/what-tinyml-is/</a>, See on <a href="https://news.ycombinator.com/item?id=39014866">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
			

<figure><img fetchpriority="high" decoding="async" width="1024" height="320" src="https://www.ikkaro.net/wp-content/uploads/2024/01/what-is-tinyml-1024x320.jpg" alt="What TinyML is" srcset="https://www.ikkaro.net/wp-content/uploads/2024/01/what-is-tinyml-1024x320.jpg 1024w, https://www.ikkaro.net/wp-content/uploads/2024/01/what-is-tinyml-300x94.jpg 300w, https://www.ikkaro.net/wp-content/uploads/2024/01/what-is-tinyml-768x240.jpg 768w, https://www.ikkaro.net/wp-content/uploads/2024/01/what-is-tinyml-1536x480.jpg 1536w, https://www.ikkaro.net/wp-content/uploads/2024/01/what-is-tinyml.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>TinyML or Tiny Machine Learning refers to the <strong>use of Machine Learning in microcontrollers</strong>. In systems that unlike those used in traditional ML have few resources, are systems that have little CPU, little RAM and extremely low power consumption in the order of magnitude of milliwatts or microwatts.</p>



<p>Its official website is the <a href="https://www.tinyml.org/" target="_blank" rel="noreferrer noopener">TinyML Foundation</a>.</p>



<p>What is done is to reduce large models for use with equipment with very few resources and microcontrollers. The preferred field of the Makers.</p>




<p>I have started a series of 3 courses offered by Harvard for free</p>



<ol>
<li><em><a href="https://www.edx.org/learn/machine-learning/harvard-university-fundamentals-of-tinyml" target="_blank" rel="noreferrer noopener">Fundamentals of TinyML</a></em> (What do I build, what for and what are the problems)</li>



<li><em>Applications of TinyML</em> (data-driven, bias, etc) </li>



<li><em>Deploying TinyML</em> (where do we put our models, security and privacy)</li>
</ol>



<p>The following notes are from the first Fundamentals of TinyML where they explain what it is, when it is applied, the different techniques that are used, etc, etc.</p>



<p>Embedded systems using microcontrollers cannot work with the large models, as they have memories up to 256kB. Here are some examples of operating systems that can be used with microcontrollers</p>



<ul>
<li><a href="https://www.freertos.org/index.html" target="_blank" rel="noreferrer noopener">FreeRTOS</a></li>



<li><a href="https://os.mbed.com/mbed-os/" target="_blank" rel="noreferrer noopener">Mbed OS</a></li>
</ul>



<p><a href="https://www.ikkaro.net/machine-learning/" target="_blank" rel="noreferrer noopener">Machine Learning</a> consists of algorithms that search for patterns in data.</p>



<p>With TinyML, techniques are used to compress these algorithms so that they remain effective in finding patterns in data.</p>



<p>There are 5 quintillion bytes of data produced daily by IoT and only less than 1% is analyzed.</p>



<h2><span id="Algorithm_compression_techniques">Algorithm compression techniques</span></h2>



<p>Some algorithm compression techniques are:</p>



<h3><span id="Pruning">Pruning</span></h3>



<p><strong>Pruning Synapsis:</strong> We remove network connections from the model. Sometimes it can decrease the accuracy.</p>



<p><strong>Pruning Neurons:</strong> We can also eliminate entire neurons from our model which reduces the computational demand of the network.</p>



<h3><span id="Quantization">Quantization</span></h3>



<p>It consists of discretizing the values within a small range. For example if we discretize a float within the range -128 to 127 we only have to traverse 256 values. Going from a float point value that is stored in 4 bytes to an integer value that is stored in 1 byte implies a x4 reduction in size.</p>



<p>Quantization is going to be critical in TinyML due to the limited resources available.</p>



<h3><span id="Knowledge_distillation">Knowledge distillation</span></h3>



<p>Apply our knowledge and know how to make the model small.</p>



<h2><span id="Tools">Tools</span></h2>



<p>We use Tensor Flow Lite. While tensorFlow is focused on ML Researcher, Tensor Flow Lite is for Application Developer.</p>



<h2><span id="Uses_of_TinyML">Uses of TinyML</span></h2>



<p>Although they are not cited, of course being on this website we can find <strong>uses of TinyML dedicated to the DIY, Maker and Hacker world</strong>.</p>



<h3><span id="Uses_of_TinyML_in_Industry">Uses of TinyML in Industry</span></h3>



<p>In Industry, in maintenance, to warn us when there are vibrations that indicate that there will be breakage, etc, etc. increases efficiency and reduces costs. The negative points are the accuracy that can give us false alarms. In case of false alarm whose responsibility is the operator or the system.</p>



<h3><span id="TinyML_in_the_environment">TinyML in the environment</span></h3>



<p>Instead of collecting data that then has to be processed, with TinyML we have real-time answers about changes in the environment, for example in the life of wild animals.</p>



<h3><span id="TinyML_for_humans">TinyML for humans</span></h3>



<p>Helps people with disabilities to perform more tasks without having to use their hands. Improving the UI and UX of applications to make them easier to use.</p>



<p>We build technology to improve our experience as humans. Technology has to help people</p>



<h2><span id="Risks_and_downsides">Risks and downsides</span></h2>



<ul>
<li>Will it work well across all population groups? </li>



<li>Is the privacy of our data assured? </li>



<li>Can we protect this data?</li>
</ul>



<p>We have to create technology based on human-centered AI. Design, development and deployment</p>
<!-- AI CONTENT END 1 -->
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Theory of Grift (101 pts)]]></title>
            <link>https://www.thediff.co/archive/a-theory-of-grift/</link>
            <guid>39014737</guid>
            <pubDate>Tue, 16 Jan 2024 15:54:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thediff.co/archive/a-theory-of-grift/">https://www.thediff.co/archive/a-theory-of-grift/</a>, See on <a href="https://news.ycombinator.com/item?id=39014737">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>In this issue:</p><ul><li>A Theory of Grift—Why does the world increasingly seem to be awash in grift? There's a supply story and a demand story: many of the traits that make people excellent grifters used to have more legitimate applications, but don't any longer. Meanwhile, there are simply bigger opportunities to take advantage of people than there used to be. But this is not a stable situation.</li><li>Buy The Change You Wish to See in the World—BlackRock has pushed companies to change their behavior in ways that create demand for new kinds of infrastructure, and is stepping in to offer that infrastructure as an investment.</li><li>Schmuck Insurance—When an industry's long-term economics are uncertain, it makes sense to vertically integrate.</li><li>Uncertainty—Analysts' estimates have higher dispersion than they did pre-Covid; the New Normal is uncertainty about what the New Normal will turn out to be.</li><li>Block Trading—Block trades create a massive temptation to leak information, and once this becomes a habit, it's a very bad look.</li><li>Ads—The long upward march of ads as a share of revenue at companies that traditionally didn't have an ad business continues. The latest: ads as a way for gyms to hedge against the risk that customers actually show up.</li></ul>
<p>Google Ngram Viewer, the world's canonical source for all slow-moving zeitgeist shifts, indicates that there's been <a href="https://books.google.com/ngrams/graph?content=grifter&amp;year_start=1800&amp;year_end=2019&amp;corpus=en-2019&amp;smoothing=3&amp;ref=thediff.co">a bull market in calling people "grifters" since the mid-1980s</a>. Google Trends <a href="https://trends.google.com/trends/explore?date=all&amp;geo=US&amp;q=grifter&amp;hl=en&amp;ref=thediff.co">shows a completely different year-to-year trend trend</a>, but also indicates a rise in the number of people searching for the term, perhaps originally catalyzed by <a href="https://observer.com/2009/04/the-hipster-grifter/?ref=thediff.co">the memorable tale of the Hipster Grifter</a> (this coincides perfectly with the April 2009 spike), and further ebbs and flows as either specific grifters become famous of specific famous people get accused of some kind of grift.</p>
<p>What do we even mean by grift? Why is it so abundant? What, if anything, should we do about it?</p>
<p>The heart of a grift is that it's a promise that could be true: the fitness influencer who says you'll look like a model in four weeks is obviously lying, but the one who offers exercise guides and meal plans that put you on pace for steady weight loss, and who has the before-and-after pictures to prove it? They could easily be telling the truth. Products that fit that description exist; some of them work! But often in this category, they're simply not very good, or not much of an improvement on some diligent Googling and prompt engineering.<sup><a href="#fn1" id="fnref1">[1]</a></sup></p>
<p>So the heart of a grift is that you get something that, in a technical sense, is what you paid for, but that is also not worth what you paid for it.</p>
<p>In politics, an increasingly grifty arena, the pitch is once again plausible, just not going to happen. One notorious example of this is <a href="https://www.opensecrets.org/news/2019/08/political-contributions-campaigns-say-theyll-match/?ref=thediff.co">fundraising appeals that say that donations will be matched, doubled, quadrupled, etc.</a> despite campaign finance limitations on actually doing this. (Is there really a cohort of four people who are willing to donate the maximum, but <em>only</em> if the latest fundraising email pulls in at least one pay-the-max donor? That's the implicit claim behind matching donations!) There are grifty policy ideas, too, usually the ones that are appealing to a party base but unworkable nationally.<sup><a href="#fn2" id="fnref2">[2]</a></sup></p>
<p>Finance has its own ecosystem of grifts. Leaving aside things like ponzi schemes, which are outright scams, the broad categories of finance grift are:</p>
<ol>
<li>A fancy-sounding high-fee wrapper on something that is trivial to implement manually, and</li>
<li>A strategy that looks good in backtests because it has some kind of nonlinear blow-up risk.</li>
</ol>
<p>There's a lot of both. In the aggregate, one of the biggest historical finance grifts was running a strategy that implicitly replicated the S&amp;P 500, but charged much higher fees than an index fund. As index fund fees have declined—to <a href="https://fundresearch.fidelity.com/mutual-funds/summary/315911628?ref=thediff.co">as low as zero for some categories</a>!—it's gotten harder for firms to get away with this, at least for simple products. But for more complex ones, there's still room to take a straightforward options strategy and overcharge for it.</p>
<p>The options tie into the second kind of grift, pitching a product with good historical returns while declining to highlight its risks. One notorious instance of this was inverse volatility exchange-traded notes, which would systematically bet that volatility would decline. This is a positive-carry trade, i.e. most of the time, you can bet against volatility at greater than fair value, and make money even if nothing changes. If volatility declines, returns are great—you sold something at 17 when its fair value was 14, and then fair value went down to 13, so you made some extra money. But there's a good reason it pays to sell volatility on average: sometimes, it really, really doesn't: in early February 2018, <a href="https://www.bloomberg.com/news/articles/2019-02-06/the-day-the-vix-doubled-tales-of-volmageddon?ref=thediff.co">volatility more than tripled in a few days</a>, partly because of a feedback loop with the products themselves, and the inverse-volatility trade blew up.</p>
<p>One reason finance can be prone to grifts is that it's entirely possible to drift into one. Plenty of reasonable professionals treated betting against volatility as part of their toolkit, and an exchange-traded product made this easier. But retail investors who looked at the chart of these products wouldn't necessarily know that 1) they were selling insurance over time, 2) the size of the inverse-volatility funds was actually affecting the price of volatility, adding to their returns in the short term but lowering the insurance premium, and 3) that a sufficiently large move could wipe investors out.</p>
<p>What are the mechanics of a grift? One way to think of it is that it's a particularly pathological example of a company with high churn. Of course, churn rates vary by industry; people stick with checking accounts for a long time—far longer than they do for app-based dating sites. But within the same industry, some companies optimize for keeping customers forever, while others focus on acquiring them quickly, running through them profitably, and repeating that cycle.</p>
<p>So grifts tend to target the middle of whatever the relevant bell curve is. There are a lot more average people than non-average people, so the market is bigger. And their averageness makes it easier to reason about their motivations.<sup><a href="#fn3" id="fnref3">[3]</a></sup> Targeting the average also enables the plausibly-deniable part of the grift: the fitness influencer really did sell you a product that meets the specifications, the inverse-vol note did disclose in its prospectus that it could blow up.</p>
<p>One possible reason grifts seem to have proliferated is elite overproduction, specifically elite overproduction of extroverts. There's an ongoing debate over whether or not people skills are undervalued, and perhaps for many people they are, but it's hard to deny that there are many, many more ways for someone who doesn't like social interaction much to get rich. If ads and sales are on the same continuum, then the world's best salespeople are engineers, data scientists, and product managers.</p>
<p>Meanwhile, the growth of software corresponds to a drop in the number of required human touchpoints for a given transaction. When travel was booked by phone or in person, there was a direct financial reward to being a good conversationalist in the travel business; rhapsodizing about the beauty of a beach was a good way to upsell customers on a nicer hotel a bit closer to it, and sussing out whether someone was more interested in beaches, landmarks, or bars meant figuring out exactly what to pitch them. All this work is now silently and efficiently happening on the backend of the big online travel agencies, with no human interaction required.</p>
<p>The pre-Internet white collar economy was basically a universal job guarantee for personable people. That's increasingly going away, to be replaced with a more neoliberal attention economy with a more extreme distribution of outcomes. And one result of <em>that</em> is that it's possible to craft an online persona as the top of funnel for some product.</p>
<p>This is common: individual creators have converged with media brands (some of them are <a href="https://twitter.com/MrBeast/status/1699459457002918138?ref=thediff.co">literally A/B testing facial expressions</a>). But the difference comes back to extroversion (as well as moral flexibility): a grift achieves a healthy ratio of customer lifetime value to customer acquisition cost by making sure customers pay as much as possible upfront and are cheap to replace. So an extrovert who thrives on having a new audience all the time will also prosper from a business model where everyone who is burned out by working with them is quickly replaced by someone new. This shows up in the high-fee finance grift in a very specific way: a high cost structure for a fund with a commoditized offering can fund a sales force that persuasively argues it's not commoditized after all.<sup><a href="#fn4" id="fnref4">[4]</a></sup></p>
<p>In this model, grift will only get worse over time, as it's a function of media distribution that increasingly allows people to build a following—80th percentile knowledge in some domain rounds up to world's leading expert if you're far enough below that percentile—and a lack of more legitimate opportunities for charismatic people. But two forces push back against this:</p>
<ol>
<li>Automation is coming, even for the grifters. Real-world influencers are more charismatic than digital ones for now, but that won't stay true for very long. And digital influencers can be endlessly A/B tested both based on their core message and the variants for specific audiences. AI-generated video will, among other things, lead to a golden age of affinity fraud.</li>
<li>Some software products replace human labor, some complement it, and some create entirely new categories. There might be some limited sense in which the <em>Call of Duty</em> franchise competes with bowling, but it's mostly a brand new thing. As the number of products proliferates, and as AI makes it faster to produce a customized one for different domains, the labor market's demand for extroverts will rise. Cheaper-to-produce software creates more demand for proof-of-stake in the form of a human being who pitches the product and offers support. Some of this can happen digitally, and can be replaced by avatars, but business travel is still recovering, and on their earnings call last week Delta specifically called out tech companies as former travel laggards who had finally started spending again.</li>
</ol>
<p>So, for the moment, we're probably stuck with some baseline level of grift. No one is immune, because everyone is part of the mass middle of the bell curve for something. So for now we'll still see weird stuff like <a href="https://qz.com/1010684/all-the-wellness-products-american-love-to-buy-are-sold-on-both-infowars-and-goop?ref=thediff.co">Goop and Infowars selling identical supplements with different labels</a>. And, of course, once grifters learn to specialize, they'll stick around for a while: <a href="https://nypost.com/2023/06/22/hipster-grifter-sued-for-unpaid-rent-in-bushwick/?ref=thediff.co">last June, the Hipster Grifter was sued for six months of unpaid rent</a>.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>One useful feature for ChatGPT is giving serviceable workout recommendations subject to constraints. So something like "what's a good routine for arms and chest while I recover from a bench press injury?" you will get decent answers. This is especially helpful because there's little middle ground in workout advice from "what advice would you give a sedentary beginner if you were mostly worried about getting sued?" and "what's a good way for an experienced athlete to eke out a 0.5% performance improvement?" <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>One meta problem here is that in theory, someone who gets elected because they mobilize the party base to vote for them, dominating the primaries and eking out a national victory because of turnout, <em>could</em> enact such policies. But making a big unpopular change involves the other kind of politics, where you make lots of compromises and cut lots of deals. And that sort is the comparative advantage of party insiders who've been in office for a long time, i.e. exactly the sorts of people who don't win on some kind of insurgent platform. <a href="#fnref2">↩︎</a></p>
</li>
<li id="fn3"><p>Outright scams tend to target the tails: at one end, they're going after small customers who aren't sophisticated enough to know that 30% a month is not a realistic rate of return, but at the other end, they're targeting buyers who don't want to feel embarrassed asking the obvious questions. Bernie Madoff was always a few diligence questions away from seeing his whole scheme unravel, but by far the most awkward question you can ask during a fundraising pitch is: "By the way, is everything you just told me a complete lie?" <a href="#fnref3">↩︎</a></p>
</li>
<li id="fn4"><p>So asset management, like credit cards, has two fee equilibria, one in which companies compete on cost, and another in which they compete on distribution funded by a much higher cut of the transaction. <a href="#fnref4">↩︎</a></p>
</li>
</ol>
</section>
<h2 id="diff-jobs"><strong>Diff Jobs</strong></h2><p>Companies in the <em>Diff</em> network are actively looking for talent. See a sampling of current open roles below:</p><ul><li>A company building the new pension of the 21st century and building universal basic capital is looking for a frontend engineer. (NYC)</li><li>A diversified prop trading firm with a uniquely collaborative team structure is looking for experienced software engineers. (Singapore or Austin, TX preferred)</li><li>A startup building a new financial market within a multi-trillion dollar asset class is looking for generalists with banking and legal experience. (US, Remote)</li><li>A private credit fund denominated in Bitcoin needs a credit analyst that can negotiate derivatives pricing. Experience with low-risk crypto lending preferred (i.e. to large miners, prop-trading firms in safe jurisdictions). (Remote)</li><li>A concentrated crossover fund is looking for an experienced full stack software engineer to help develop and maintain internal applications to improve investment decision-making and external applications to enable portfolio companies. (SF)</li></ul><p>Even if you don't see an exact match for your skills and interests right now, we're happy to talk early so we can let you know if a good opportunity comes up.</p><p>If you’re at a company that's looking for talent, we should talk! Diff Jobs works with companies across fintech, hard tech, consumer software, enterprise software, and other areas—any company where finding unusually effective people is a top priority.</p><h2 id="elsewhere">Elsewhere</h2>
<h3 id="buy-the-change-you-wish-to-see-in-the-world">Buy The Change You Wish to See in the World</h3>
<p>On Friday morning, along with its earnings release, BlackRock <a href="https://www.businesswire.com/news/home/20240112400876/en/BlackRock-Agrees-to-Acquire-Global-Infrastructure-Partners-%E2%80%9CGIP%E2%80%9D-Creating-a-World-Leading-Infrastructure-Private-Markets-Investment-Platform?ref=thediff.co">agreed to buy infrastructure private equity firm Global Infrastructure Partners for $12.5bn</a>. BlackRock has had some solid acquisitions in the past, but their previous big acquisition, Barclays Global Investors, was partly a way to achieve scale in the intrinsically scale-driven game of managing indexed assets. GIP is a very different model, funding the acquisition or construction of ports, airports, and energy assets.</p>
<p>What's notable about this deal is that the big source of assets and potential upside comes from trends that BlackRock itself has pushed in other areas. From their earnings call:</p>
<blockquote>
<p>If we are going to decarbonize the world, the amount of capital and infrastructure is going to be very necessary. If we are going to be more and more reliant on interconnectivity worldwide, the need for the upgrading of ports is vital. As more and more human beings grow into a middle-class lifestyle, the demand for air travel grows dramatically, the need for high-quality airports grows dramatically.</p>
</blockquote>
<p>Passive investing is implicitly a bet on globalization, which leads to economic convergence. And BlackRock has been pressuring portfolio companies to adopt ESG standards for years. So this fits in with their broader strategy: reduce the world's aggregate capital expenditures for carbon-intensive assets to create more demand for carbon-mitigating assets, most of which have a longer duration than dirtier energy sources; create global portfoliosto slowly apply pressure on the rest of the world to conform to American standards for corporate governance and property rights. BlackRock is not the world's most influential organization, but at their scale it's impossible for them not to have an impact—and that impact is big enough that they can do deals to take advantage of it.</p>
<h3 id="schmuck-insurance">Schmuck Insurance</h3>
<p>Sports broadcasting is obviously worth a lot, and at present it's very non-obvious where that value accrues: dumb pipes, legacy cable networks, direct-to-consumer sports apps, or league- or team-specific subscriptions. The uncertainty means people are looking for deals, but it also means they're in the market for <a href="https://capitalgains.thediff.co/p/schmuck-insurance?ref=thediff.co">schmuck insurance</a>. So, <a href="https://nypost.com/2024/01/12/sports/espn-nfl-in-advanced-talks-on-agreement-that-could-give-league-equity-in-tv-giant/?ref=thediff.co">the NFL is considering a deal in which it will sell some of its online media assets to ESPN, in exchange for ESPN equity</a>. When company growth within an industry is a function of pricing power instead of rising unit volume, reaching the limit of that pricing power creates immense uncertainty—sometimes, one part of the value chain will grow and eat most of the margin, while in other cases aggregate profits will shrink fast as everyone fights for share. A merger like this mitigates some of that uncertainty without eliminating it, but in the end everyone involved is in the market for a bit less uncertainty.</p>
<h3 id="uncertainty">Uncertainty</h3>
<p>Snippet Finance has a <a href="https://snippet.finance/dispersion-of-estimates/?ref=thediff.co">nice chart of analysts' uncertainty about company earnings, as captured by the dispersion in estimates</a>. The short-term spikes in the chart have a simple driver: when there's an industry downturn, some analysts cut estimates fast and others take their time, so average earnings estimates going from $1 to $0.50 can represent half of the analysts cutting their estimates to $0.00 while the other half haven't gotten around to it yet. (That's one reason for the big increase in early 2015: everyone knew oil companies would take a giant earnings hit from the collapse in oil prices, but not everyone published this promptly.) What the chart highlights is that uncertainty has gone up post-Covid, and mostly stayed there. There's been a lively debate since about which companies' economics permanently reset higher (like sporting goods retailers seeing their net margins rise by ~50%) while others seem to have reset lower (Delta airlines' headcount is 10% higher than it was pre-pandemic, at a similar business size). Every quarter is either evidence that things are returning to normal or evidence that we're in a new normal.</p>
<h3 id="block-trading">Block Trading</h3>
<p>Morgan Stanley has <a href="https://www.sec.gov/files/litigation/admin/2024/34-99337.pdf?ref=thediff.co">settled a complaint with the SEC over block trading</a>. The way block trades are supposed to work is that a seller wants to move a large amount of stock with minimal market impact, and instead of dumping it on the market, they sell it to a counterparty (often Morgan Stanley) who then promptly sells to their customers. The bank's value-add is that they know more about what market depth looks like conditional on a large sale, and is willing to take some risk that their estimate is wrong. Suppose there's a stock trading at $42, and a seller wants to get rid of 10m shares. Perhaps they estimate that the price impact of selling would knock the stock down to $40, but Morgan Stanley figures they could sell carefully enough to only push the stock to $41. Morgan Stanley offers $40.50, makes a $0.50/share profit, and saves their customer $0.50/share, too.</p>
<p>One trader at Morgan Stanley discovered a fun variant on this trade: leaking information about it to potential buyers so they can profit from that initial drop (and, presumably, this customer will owe Morgan Stanley a favor and buy into future deals that are harder to sell). This is a straightforward wealth transfer from one client (the seller) to another client (the prospective buyer), and increases the market impact of the trade—once the hedge fund knows an offer is coming, their incentive is to short the stock aggressively and then cover by buying into the offering. Once the offering is committed, it's hard to back out even if there are fluctuations, and Morgan Stanley's customers probably don't audit every block trade Morgan Stanley ever did to see if there's some kind of pre-trade price impact indicating a leak.</p>
<p>There is an obvious temptation here, because the block trader has basically been told "there's a specific time at which this stock will move by a predictable amount, and while you yourself can't trade on it, somebody else would clearly owe you a favor if they did." The people doing this trade were pretty aggressive about it: one part of the SEC document references a case where one fund was 89% of the trading volume in a stock the afternoon before an offer. And Morgan Stanley even pitched customers on giving them exclusive rights to do the deal—by citing the price impact of a leak they themselves were responsible for! (It was, in fact, the same transaction where one fund tipped off by Morgan Stanley did almost all the trading volume in the stock.) The real trouble with this, from a white collar crime optimization standpoint, is that once someone goes looking, it's very easy to assemble evidence of exactly what happened: a straightforward illicit trading strategy is straightforwardly easy to spot after the fact.</p>
<h3 id="ads">Ads</h3>
<p><em>The Diff</em> <a href="https://www.thediff.co/archive/why-the-answer-is-so-often-ads/">wrote last year about how common it is for companies to either have an ad-based revenue model or to include ads in an existing model</a>. The trend continues: <a href="https://www.adexchanger.com/commerce/how-a-gym-chain-became-an-ad-data-seller/?ref=thediff.co">here's an interview with the Chief Digital Officer of Planet Fitness about their new ad network</a>. (Even someone writing in an ad trade magazine is surprised that PF is getting so much into ads.) Every in-person business is, from an advertiser's perspective, an audience that has been pre-selected based on some traits that predict other kinds of spending, and that is somewhat captive because the venue can control the local media environment. Naturally, this model works even better with a membership/loyalty program that makes it possible to track who saw which ad, and to retarget them later through an app. Planet Fitness is not one of the companies that will end up switching to an entirely ad-supported business, of course. But the gym business is already built on paying fixed costs in order to achieve high-incremental margin revenue. In fact, this is a sort of usage-based hedge: the customer who signs up because of their New Years Resolution in January and stops showing up in March is pure margin until they inevitably cancel; now, the customer who insists on continuing to work out throughout the year can contribute revenue, too.</p>

            
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OutRun – Open-source, privacy oriented, outdoor fitness tracker (185 pts)]]></title>
            <link>https://github.com/timfraedrich/OutRun</link>
            <guid>39014652</guid>
            <pubDate>Tue, 16 Jan 2024 15:48:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/timfraedrich/OutRun">https://github.com/timfraedrich/OutRun</a>, See on <a href="https://news.ycombinator.com/item?id=39014652">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:timfraedrich/OutRun" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="BrzVpYFtQOVkePhdF2sTSjHQdB5cDsGa7RppcoIBOMyMhdGk-AFmEvJbE_NxSG-vrH5S7m6NzRQQuZLhtMJYVg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="timfraedrich/OutRun" data-current-org="" data-current-owner="timfraedrich" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=nvAdt6gfyRL9vW9tkgCHUJh2s4sYsqTEFlQo97zKH6NeFMwNik5tFoMQyLDw3WRLKvTU7HvysX8oUqXSqA%2FeX%2BoW2X26rI05ICzL10%2FJj3xx7klw63fFSrxutqHgxN%2FQTy8UTaGucFqMP5TwgkSjcsVLvGqGmh6ZjkN0XDrnKLe9R2eyAxrbvqZagjnLQaUsXU6B0bjm%2BbCdaqwC5B2WX1gdjiDA6XOXBS80MBJ%2BcDkWYIXLs0Ddm0YJl0RyqCrUHOi5ob1UG8EkkPup9MyJwGYYoJmhMMRRxjPEruL4RleLgyyAfeDAcSNp64zT%2FvL6IAgJ%2BehdDVT9Xfn5T0DxoaqrZ0B%2BHI2iSLwCa4weRG8NV2%2B2QauVXdiMWwsmtXiBn4tsx1wMmtB9yt9bvtOeii9aXB%2B68ufVeHP2RI27eVkOu5XsogDyDqOCGNvMmjVeT36s0ZB8xCLlgjFjVXw1mcNi9HMwy8zjJWFsR9FUgSGbneEM0FMQlgNn9xw5tN8qAp8neqKTOPgmJSbMB5fVx5Rr--oAhxTr2Iz7E4rrU4--goxlxFh7X%2FWv5JDp5OjHfA%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=timfraedrich%2FOutRun" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/timfraedrich/OutRun&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="299d5153f5e18b4429dece7923a0c99cf0a6b74c4b290964e547d00f94d1c6ca" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Supreme Court declines to hear appeals in Apple-Epic Games legal battle (175 pts)]]></title>
            <link>https://www.reuters.com/legal/us-supreme-court-snubs-epic-games-legal-battle-with-apple-2024-01-16/</link>
            <guid>39014642</guid>
            <pubDate>Tue, 16 Jan 2024 15:48:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/legal/us-supreme-court-snubs-epic-games-legal-battle-with-apple-2024-01-16/">https://www.reuters.com/legal/us-supreme-court-snubs-epic-games-legal-battle-with-apple-2024-01-16/</a>, See on <a href="https://news.ycombinator.com/item?id=39014642">Hacker News</a></p>
Couldn't get https://www.reuters.com/legal/us-supreme-court-snubs-epic-games-legal-battle-with-apple-2024-01-16/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Field experimental evidence of AI on knowledge worker productivity and quality (135 pts)]]></title>
            <link>https://www.oneusefulthing.org/p/centaurs-and-cyborgs-on-the-jagged</link>
            <guid>39014521</guid>
            <pubDate>Tue, 16 Jan 2024 15:41:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.oneusefulthing.org/p/centaurs-and-cyborgs-on-the-jagged">https://www.oneusefulthing.org/p/centaurs-and-cyborgs-on-the-jagged</a>, See on <a href="https://news.ycombinator.com/item?id=39014521">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>A lot of people have been asking if AI is really a big deal for the future of work. We have a new paper that strongly suggests the answer is YES.</p><p><span>For the last several months, I been part of a team of social scientists working with Boston Consulting Group, turning their offices into the largest pre-registered experiment on the future of professional work in our AI-haunted age. </span><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321" rel="">Our first working paper is out today</a><span>. There is a ton of important and useful nuance in the paper but let me tell you the headline first: for 18 different tasks selected to be realistic samples of the kinds of work done at an elite consulting company, consultants using ChatGPT-4 outperformed those who did not, by a lot. On every dimension. Every way we measured performance.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d0dcea1-a78d-49f6-8410-c356f71aa535_1756x1120.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d0dcea1-a78d-49f6-8410-c356f71aa535_1756x1120.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d0dcea1-a78d-49f6-8410-c356f71aa535_1756x1120.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d0dcea1-a78d-49f6-8410-c356f71aa535_1756x1120.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d0dcea1-a78d-49f6-8410-c356f71aa535_1756x1120.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d0dcea1-a78d-49f6-8410-c356f71aa535_1756x1120.png" width="1456" height="929" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1d0dcea1-a78d-49f6-8410-c356f71aa535_1756x1120.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:929,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:359728,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d0dcea1-a78d-49f6-8410-c356f71aa535_1756x1120.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d0dcea1-a78d-49f6-8410-c356f71aa535_1756x1120.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d0dcea1-a78d-49f6-8410-c356f71aa535_1756x1120.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1d0dcea1-a78d-49f6-8410-c356f71aa535_1756x1120.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Distribution of output quality across all the tasks. The blue group did not use AI, the green and red groups used AI, the red group got some additional training on how to use AI.</figcaption></figure></div><p><span>Consultants using AI </span><strong>finished</strong><span> </span><strong>12.2% more tasks</strong><span> on average, </span><strong>completed tasks 25.1% more quickly</strong><span>, and produced </span><strong>40% higher quality results</strong><span> than those without. Those are some very big impacts. Now, let’s add in the nuance.</span></p><p><span>First, it is important to know that this effort was multidisciplinary, involving multiple types of experiments and hundreds of interviews, conducted by a great team, including the Harvard social scientists </span><a href="https://www.fabriziodellacqua.com/" rel="">Fabrizio Dell’Acqua</a><span>, </span><a href="https://www.hbs.edu/faculty/Pages/profile.aspx?facId=772797" rel="">Edward McFowland III</a><span>, and </span><a href="https://www.hbs.edu/faculty/Pages/profile.aspx?facId=240491" rel="">Karim Lakhani</a><span>; </span><a href="https://www.hilalifshitz.com/" rel="">Hila Lifshitz-Assaf</a><span> from Warwick Business School and</span><a href="https://mitsloan.mit.edu/faculty/directory/kate-kellogg" rel=""> Katherine Kellogg </a><span>of MIT (plus myself). </span><a href="https://bcghendersoninstitute.com/contributors/saran-rajendran/" rel="">Saran Rajendran</a><span>, </span><a href="https://bcghendersoninstitute.com/contributors/lisa-krayer/" rel="">Lisa Krayer</a><span>, and </span><a href="https://www.bcg.com/about/people/experts/francois-candelon" rel="">François Candelon</a><span> ran the experiment on the BCG side, using a full 7% of its consulting force (758 consultants). They all did a lot of very careful work that goes far, far beyond the post. So, please </span><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321" rel="">look at the paper to make sure you get all the details</a><span> - especially if you have questions about numbers or methods. I need to simplify a lot to fit 58 pages of findings into a post, and any mistakes are mine, not my co-authors. Also, while we pre-registered these experiments, this is still a new working paper, so there might be errors or mistakes, and the paper is not yet peer-reviewed. With that in mind, let’s get to the details…</span></p><p>AI is weird. No one actually knows the full range of capabilities of the most advanced Large Language Models, like GPT-4. No one really knows the best ways to use them, or the conditions under which they fail. There is no instruction manual. On some tasks AI is immensely powerful, and on others it fails completely or subtly. And, unless you use AI a lot, you won’t know which is which.</p><p><span>The result is what we call the “Jagged Frontier” of AI. Imagine a fortress wall, with some towers and battlements jutting out into the countryside, while others fold back towards the center of the castle. That wall is the capability of AI, and the further from the center, the harder the task. Everything inside the wall can be done by the AI, everything outside is hard for the AI to do. The problem is that the wall is invisible, so some tasks that might logically seem to be the same distance away from the center, and therefore equally difficult – say, writing a sonnet and an exactly 50 word poem – are actually on different sides of the wall. The AI is great at the sonnet, but, because of how it conceptualizes the world in tokens, rather than words, it consistently produces poems of more or less than 50 words.&nbsp; Similarly, some unexpected tasks (</span><a href="https://www.oneusefulthing.org/p/automating-creativity" rel="">like idea generation</a><span>) are easy for AIs while other tasks that seem to be easy for machines to do (like basic math) are challenges for LLMs.</span></p><p>I asked the ChatGPT with Code Interpreter to visualize this for you:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F237aac17-b0ac-4174-9c0b-6b3e5e9ba0be_1551x1580.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F237aac17-b0ac-4174-9c0b-6b3e5e9ba0be_1551x1580.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F237aac17-b0ac-4174-9c0b-6b3e5e9ba0be_1551x1580.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F237aac17-b0ac-4174-9c0b-6b3e5e9ba0be_1551x1580.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F237aac17-b0ac-4174-9c0b-6b3e5e9ba0be_1551x1580.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F237aac17-b0ac-4174-9c0b-6b3e5e9ba0be_1551x1580.png" width="330" height="336.1195054945055" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/237aac17-b0ac-4174-9c0b-6b3e5e9ba0be_1551x1580.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1483,&quot;width&quot;:1456,&quot;resizeWidth&quot;:330,&quot;bytes&quot;:93754,&quot;alt&quot;:&quot;Hey GPT, I want you to create an image to illustrate our new paper on the effects of AI and work. And the key element of the paper is the idea of a jagged frontier. That AI capabilities are uneven and so tasks that seem to be of equal difficulty, some of those tasks will be just outside the frontier, some of the tasks will be inside. So I want you to create an image with whatever technique you want that shows a jagged frontier extending from a center point where the distance from the center indicates the difficulty of a task. And to show tasks that can be represented by points, for example, one just inside the wall of the frontier, one just outside the wall of the frontier, and one task labeled task outside the frontier, the other labeled task inside the frontier, and there will be a line, a circular line, showing that they are the same distance from the center, therefore the same difficulty level.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="Hey GPT, I want you to create an image to illustrate our new paper on the effects of AI and work. And the key element of the paper is the idea of a jagged frontier. That AI capabilities are uneven and so tasks that seem to be of equal difficulty, some of those tasks will be just outside the frontier, some of the tasks will be inside. So I want you to create an image with whatever technique you want that shows a jagged frontier extending from a center point where the distance from the center indicates the difficulty of a task. And to show tasks that can be represented by points, for example, one just inside the wall of the frontier, one just outside the wall of the frontier, and one task labeled task outside the frontier, the other labeled task inside the frontier, and there will be a line, a circular line, showing that they are the same distance from the center, therefore the same difficulty level." title="Hey GPT, I want you to create an image to illustrate our new paper on the effects of AI and work. And the key element of the paper is the idea of a jagged frontier. That AI capabilities are uneven and so tasks that seem to be of equal difficulty, some of those tasks will be just outside the frontier, some of the tasks will be inside. So I want you to create an image with whatever technique you want that shows a jagged frontier extending from a center point where the distance from the center indicates the difficulty of a task. And to show tasks that can be represented by points, for example, one just inside the wall of the frontier, one just outside the wall of the frontier, and one task labeled task outside the frontier, the other labeled task inside the frontier, and there will be a line, a circular line, showing that they are the same distance from the center, therefore the same difficulty level." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F237aac17-b0ac-4174-9c0b-6b3e5e9ba0be_1551x1580.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F237aac17-b0ac-4174-9c0b-6b3e5e9ba0be_1551x1580.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F237aac17-b0ac-4174-9c0b-6b3e5e9ba0be_1551x1580.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F237aac17-b0ac-4174-9c0b-6b3e5e9ba0be_1551x1580.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption></figcaption></figure></div><p>To test the true impact of AI on knowledge work, we took hundreds of consultants and randomized whether they were allowed to use AI. We gave those who were allowed to use AI access to GPT-4, the same model everyone in 169 countries can access for free with Bing, or by paying $20 a month to OpenAI. No special fine-tuning or prompting, just GPT-4 through the API.</p><p>We then did a lot of pre-testing and surveying to establish baselines, and asked consultants to do a wide variety of work for a fictional shoe company, work that the BCG team had selected to accurately represent what consultants do. There were creative tasks (“Propose at least 10 ideas for a new shoe targeting an underserved market or sport.”), analytical tasks (“Segment the footwear industry market based on users.”), writing and marketing tasks (“Draft a press release marketing copy for your product.”), and persuasiveness tasks (“Pen an inspirational memo to employees detailing why your product would outshine competitors.”). We even checked with a shoe company executive to ensure that this work was realistic - they were. And, knowing AI, these are tasks that we might expect to be inside the frontier.</p><p>In line with our theories, and as we have discussed, we found that the consultants with AI access did significantly better, whether we briefly introduced them to AI first (the “overview” group in the diagram) or did not. This was true for every measurement, whether the time it took to complete tasks, the number of tasks completed overall (we gave them an overall time limit) or the quality of the outputs. We rated that quality using both human and AI graders, who agreed with each other (itself an interesting finding).</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ffa6d74-dc5d-45fc-9fe0-b1511556716a_1028x814.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ffa6d74-dc5d-45fc-9fe0-b1511556716a_1028x814.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ffa6d74-dc5d-45fc-9fe0-b1511556716a_1028x814.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ffa6d74-dc5d-45fc-9fe0-b1511556716a_1028x814.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ffa6d74-dc5d-45fc-9fe0-b1511556716a_1028x814.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ffa6d74-dc5d-45fc-9fe0-b1511556716a_1028x814.png" width="582" height="460.84435797665367" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0ffa6d74-dc5d-45fc-9fe0-b1511556716a_1028x814.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:814,&quot;width&quot;:1028,&quot;resizeWidth&quot;:582,&quot;bytes&quot;:436765,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ffa6d74-dc5d-45fc-9fe0-b1511556716a_1028x814.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ffa6d74-dc5d-45fc-9fe0-b1511556716a_1028x814.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ffa6d74-dc5d-45fc-9fe0-b1511556716a_1028x814.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ffa6d74-dc5d-45fc-9fe0-b1511556716a_1028x814.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>We also found something else interesting, an effect that is increasingly apparent in other studies of AI: it works as a skill leveler. The consultants who scored the worst when we assessed them at the start of the experiment had the biggest jump in their performance, 43%, when they got to use AI. The top consultants still got a boost, but less of one. Looking at these results, I do not think enough people are considering what it means when a technology raises all workers to the top tiers of performance. It may be like how it used to matter whether miners were good or bad at digging through rock… until the steam shovel was invented and now differences in digging ability do not matter anymore. AI is not quite at that level of change, but skill levelling is going to have a big impact.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F970b3354-1bac-4146-a92f-57f65440f872_1228x610.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F970b3354-1bac-4146-a92f-57f65440f872_1228x610.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F970b3354-1bac-4146-a92f-57f65440f872_1228x610.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F970b3354-1bac-4146-a92f-57f65440f872_1228x610.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F970b3354-1bac-4146-a92f-57f65440f872_1228x610.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F970b3354-1bac-4146-a92f-57f65440f872_1228x610.png" width="1228" height="610" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/970b3354-1bac-4146-a92f-57f65440f872_1228x610.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:610,&quot;width&quot;:1228,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:97820,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F970b3354-1bac-4146-a92f-57f65440f872_1228x610.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F970b3354-1bac-4146-a92f-57f65440f872_1228x610.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F970b3354-1bac-4146-a92f-57f65440f872_1228x610.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F970b3354-1bac-4146-a92f-57f65440f872_1228x610.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>But there is more to the story. BCG designed one more task, this one carefully selected to ensure that the AI couldn’t come to a correct answer. This wasn’t easy. As we say in the paper “since AI proved surprisingly capable, it was difficult to design a task in this experiment outside the AI’s frontier where humans with high human capital doing their job would consistently outperform AI.” But we identified a task that used the blind spots of AI to ensure it would give a wrong, but convincing, answer to a problem that humans would be able to solve. Indeed, human consultants got the problem right 84% of the time without AI help, but when consultants used the AI, they did worse – only getting it right 60-70% of the time. What happened?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e622c18-0d75-4fe9-a7b4-0bc0431e5cc3_1046x510.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e622c18-0d75-4fe9-a7b4-0bc0431e5cc3_1046x510.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e622c18-0d75-4fe9-a7b4-0bc0431e5cc3_1046x510.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e622c18-0d75-4fe9-a7b4-0bc0431e5cc3_1046x510.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e622c18-0d75-4fe9-a7b4-0bc0431e5cc3_1046x510.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e622c18-0d75-4fe9-a7b4-0bc0431e5cc3_1046x510.png" width="1046" height="510" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1e622c18-0d75-4fe9-a7b4-0bc0431e5cc3_1046x510.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:510,&quot;width&quot;:1046,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:74167,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e622c18-0d75-4fe9-a7b4-0bc0431e5cc3_1046x510.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e622c18-0d75-4fe9-a7b4-0bc0431e5cc3_1046x510.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e622c18-0d75-4fe9-a7b4-0bc0431e5cc3_1046x510.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1e622c18-0d75-4fe9-a7b4-0bc0431e5cc3_1046x510.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><a href="https://static1.squarespace.com/static/604b23e38c22a96e9c78879e/t/62d5d9448d061f7327e8a7e7/1658181956291/Falling+Asleep+at+the+Wheel+-+Fabrizio+DellAcqua.pdf" rel="">In a different paper than the one we worked on together,</a><span> Fabrizio Dell’Acqua shows why relying too much on AI can backfire. In an experiment, he found that recruiters who used high-quality AI became lazy, careless, and less skilled in their own judgment. They missed out on some brilliant applicants and made worse decisions than recruiters who used low-quality AI or no AI at all. When the AI is very good, humans have no reason to work hard and pay attention. They let the AI take over, instead of using it as a tool. He called this “falling asleep at the wheel”, and it can hurt human learning, skill development, and productivity.</span></p><p>In our experiment, we also found that the consultants fell asleep at the wheel – those using AI actually had less accurate answers than those who were not allowed to use AI (but they still did a better job writing up the results than consultants who did not use AI). The authoritativeness of AI can be deceptive if you don’t know where the frontier lies.</p><p>But a lot of consultants did get both inside and outside the frontier tasks right, gaining the benefits of AI without the disadvantages. The key seemed to be following one of two approaches: becoming a Centaur or becoming a Cyborg. Fortunately, this does not involve any actual grafting of electronic gizmos to your body or getting cursed to turn into the half-human/half-horse of Greek myth. They are rather two approaches to navigating the jagged frontier of AI that integrates the work of person and machine.</p><p>Centaur work has a clear line between person and machine, like the clear line between the human torso and horse body of the mythical centaur. &nbsp;Centaurs have a strategic division of labor, switching between AI and human tasks, allocating responsibilities based on the strengths and capabilities of each entity. When I am doing an analysis with the help of AI, I often approach it as a Centaur. I will decide on what statistical techniques to do, but then let the AI handle producing graphs. In our study at BCG, centaurs would do the work they were strongest at themselves, and then hand off tasks inside the jagged frontier to the AI.</p><p><span>On the other hand, Cyborgs blend machine and person, integrating the two deeply. Cyborgs don't just delegate tasks; they intertwine their efforts with AI, moving back and forth over the jagged frontier. Bits of tasks get handed to the AI, such as initiating a sentence for the AI to complete, so that Cyborgs find themselves working in tandem with the AI. </span><a href="https://www.oneusefulthing.org/p/embracing-weirdness-what-it-means" rel="">This is how I suggest approaching using AI for writing</a><span>, for example. It is also how I generated two of the illustrations in the paper (the Jagged Frontier image and the 54 line graph, both of which were built by ChatGPT, with my initial direction and guidance)</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd38f8137-74ed-4772-b1e3-580a1c18f170_1955x768.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd38f8137-74ed-4772-b1e3-580a1c18f170_1955x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd38f8137-74ed-4772-b1e3-580a1c18f170_1955x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd38f8137-74ed-4772-b1e3-580a1c18f170_1955x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd38f8137-74ed-4772-b1e3-580a1c18f170_1955x768.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd38f8137-74ed-4772-b1e3-580a1c18f170_1955x768.png" width="1456" height="572" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d38f8137-74ed-4772-b1e3-580a1c18f170_1955x768.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:572,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:342709,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd38f8137-74ed-4772-b1e3-580a1c18f170_1955x768.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd38f8137-74ed-4772-b1e3-580a1c18f170_1955x768.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd38f8137-74ed-4772-b1e3-580a1c18f170_1955x768.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd38f8137-74ed-4772-b1e3-580a1c18f170_1955x768.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Our paper, along with a stream of excellent </span><a href="https://www.oneusefulthing.org/p/secret-cyborgs-the-present-disruption" rel="">work </a><span>by other scholars, suggests that, regardless of the philosophic and technical debates over the nature and future of AI, it is already a powerful disrupter to how we actually work. And this is not a hyped new technology that will change the world in five years, or that requires a lot of investment and the resources of huge companies - it is here, NOW. The tools the elite consultants used to supercharge their work are the exact same as the ones available to everyone reading this post. And the tools the consultants used will soon be much worse than what is available to you. Because the technological frontier is not just jagged, it is expanding. I am very confident that in the next year, at least two companies will release models more powerful than GPT-4. The Jagged Frontier advances, and we have to be ready for that.</span></p><p>Even aside from any anxiety that statement might cause, it is also worth noting the other downsides of AI. People really can go on autopilot when using AI, falling asleep at the wheel and failing to notice AI mistakes. And, like other research, we also found that AI outputs, while of higher quality than that of humans, were also a bit homogenous and same-y in aggregate. Which is why Cyborgs and Centaurs are important - they allow humans to work with AI to produce more varied, more correct, and better results than either humans or AI can do alone. And becoming one is not hard. Just use AI enough for work tasks and you will start to see the shape of the jagged frontier, and start to understand where AI is scarily good... and where it falls short.</p><p>In my mind, the question is no longer about whether AI is going to reshape work, but what we want that to mean. We get to make choices about how we want to use AI help to make work more productive, interesting, and meaningful. But we have to make those choices soon, so that we can begin to actively use AI in ethical and valuable ways, as Cyborgs and Centaurs, rather than merely reacting to technological change. Meanwhile, the Jagged Frontier advances.</p><div><figure><a target="_blank" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F855e050e-a17e-41d4-9121-27cf464ab670_1042x1238.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F855e050e-a17e-41d4-9121-27cf464ab670_1042x1238.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F855e050e-a17e-41d4-9121-27cf464ab670_1042x1238.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F855e050e-a17e-41d4-9121-27cf464ab670_1042x1238.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F855e050e-a17e-41d4-9121-27cf464ab670_1042x1238.png" width="328" height="389.6967370441459" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/855e050e-a17e-41d4-9121-27cf464ab670_1042x1238.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1238,&quot;width&quot;:1042,&quot;resizeWidth&quot;:328,&quot;bytes&quot;:462487,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:&quot;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4573321&quot;,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F855e050e-a17e-41d4-9121-27cf464ab670_1042x1238.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F855e050e-a17e-41d4-9121-27cf464ab670_1042x1238.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F855e050e-a17e-41d4-9121-27cf464ab670_1042x1238.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F855e050e-a17e-41d4-9121-27cf464ab670_1042x1238.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p data-attrs="{&quot;url&quot;:&quot;https://www.oneusefulthing.org/p/centaurs-and-cyborgs-on-the-jagged?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.oneusefulthing.org/p/centaurs-and-cyborgs-on-the-jagged?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mad Scientists' Club: The Books (102 pts)]]></title>
            <link>http://www.madscientistsclub.com/books.html</link>
            <guid>39014426</guid>
            <pubDate>Tue, 16 Jan 2024 15:35:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://www.madscientistsclub.com/books.html">http://www.madscientistsclub.com/books.html</a>, See on <a href="https://news.ycombinator.com/item?id=39014426">Hacker News</a></p>
<div id="readability-page-1" class="page"><div width="700">
		  <tbody><tr> 
			<td><span face="Arial, Helvetica, sans-serif" size="2" color="#000000"><img src="http://www.madscientistsclub.com/gasbag.jpg" width="411" height="283"></span><span face="Times New Roman, Times, serif" size="5" color="#000000"><b>M</b></span><span face="Arial, Helvetica, sans-serif" size="2" color="#000000">y 
              father wrote twelve stories and two short novels describing the 
              adventures of the Mad Scientists' Club. The order in which these 
              works were written, starting in 1960, is as follows:<p>
              
              <i>The Strange Sea Monster of Strawberry Lake</i> (1960)<br>
              <i>Night Rescue</i> (1961)<br>
              <i>The Unidentified Flying Man of Mammoth Falls </i>(1962)<br>
              <i>The Secret of the Old Cannon</i> (1963)<br>
              <i>The Great Gas Bag Race</i> (1964)<br>
              <i>The Big Egg </i>(1964)<br>
              <i>The Voice in the Chimney</i> (1964)<br>
              <i>Big Chief Rainmaker</i> (1965)<br>
              <i>The Telltale Transmitter</i> (1966)<br>
              <i>The Cool Cavern</i> (1966)<i><br>
              The Flying Sorcerer</i> (1968)<br>
              <i>The Great Confrontation</i> (1968)</p><p>
              
              The first seven tales were collected in <i>The Mad Scientists' Club</i> 
              book published in 1965. The first three stories originally<br>
              appeared in Boys' Life--<i>The Strange Sea Monster of Strawberry 
              Lake</i>--in the September 1961 issue, followed by <i>The Unidentified 
              Flying Man of Mammoth Falls</i> in November 1962 and <i>Night Rescue</i> 
              in February 1964. <i>The Secret of the Old Cannon</i> and <i>The 
              Great Gas Bag Race</i> appeared in Boys' Life in 1966. MacRae Smith 
              wanted to include eight stories in the book; however, in early 1965, 
              the editor rejected the eighth story, <i>Big Chief Rainmaker</i>, 
              so the book was published with seven tales. </p><p>
              
              <img src="http://www.madscientistsclub.com/egg.jpg" width="397" height="199"> In one 
              of those amusing twists, the publisher wrote my father some months 
              later, complimented him on the <i>Rainmaker</i> story, and urged 
              that it be included in the subsequent book. It was. The last four 
              stories were written for <i>The New Adventures of the Mad Scientists' 
              Club</i>, published in 1968. The last story in that volume, <i>The 
              Great Confrontation</i>, completed in 1968, was the last Mad Scientists' 
              Club short story. My father was already at work on <i>The Sunken 
              Village</i>, which became entitled <i>The Big Kerplop!<br>
              </i></p><p>
              
              The Big Kerplop! was published in 1974. It is the first novel-length 
              story of the Mad Scientists of Mammoth Falls (the name they liked). 
              Although it relates how the Club was formed, it is a more complex 
              story about reputations and the propensity of public officials to 
              cover up unpleasant facts. It was written from 1968 through 1973. 
              As the publisher was encountering financial difficulties at the 
              time, the book was not published until 1974, and only 1000 or so 
              copies were distributed. That is why original copies of the book 
              are so rare. One minor item is that the title my father gave the 
              story is as you see it here with the exclamation mark. The publisher 
              dropped the mark in the final title. We restored the title when 
              we re-issued the book. More details on how the book was written 
              and previews of the chapters are found on the <a href="http://www.madscientistsclub.com/tbk.html">TBK! 
              page</a>. </p><p>
              
              The Big Chunk of Ice is the last tale of the Mad Scientists' Club. 
              Completed in 1974, it is a novel-length international adventure 
              of the Club, taking them to Austria. It was still in manuscript 
              form when Purple House Press published it for the first time in 
              2005. </p></span></td>
		  </tr>
		</tbody></div><p><span face="Arial, Helvetica, sans-serif" size="2" color="#000000"> <span size="1"> 
		<br>
		</span><span face="Arial, Helvetica, sans-serif" size="2" color="#000000"><img name="Image30" src="http://www.madscientistsclub.com/menu_books-on.jpg" width="46" height="21"> 
		<img src="http://www.madscientistsclub.com/menu_dot.jpg" width="14" height="16"><a href="http://www.madscientistsclub.com/club.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage('Image291','','menu_club-on.jpg',1)"><img name="Image291" src="http://www.madscientistsclub.com/menu_club.jpg" width="35" height="21"></a> 
		<img src="http://www.madscientistsclub.com/menu_dot.jpg" width="14" height="16"><a href="http://www.madscientistsclub.com/author.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage('Image261','','menu_author-on.jpg',1)"><img name="Image261" src="http://www.madscientistsclub.com/menu_author.jpg" width="51" height="21"></a><img src="http://www.madscientistsclub.com/menu_dot.jpg" width="14" height="16"> 
		<a href="http://www.madscientistsclub.com/news.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage('Image281','','menu_news-on.jpg',1)"><img name="Image281" src="http://www.madscientistsclub.com/menu_news.jpg" width="41" height="21"></a><img src="http://www.madscientistsclub.com/menu_dot.jpg" width="14" height="16"><a href="http://www.madscientistsclub.com/links.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage('Image271','','menu_links-on.jpg',1)"><img name="Image271" src="http://www.madscientistsclub.com/menu_links.jpg" width="41" height="21"></a><br>
		<img src="http://www.madscientistsclub.com/menu_line.jpg" width="700" height="2"><br>
		</span><span size="1"> </span><span face="Arial, Helvetica, sans-serif" size="2" color="#000000"><span face="Arial, Helvetica, sans-serif" size="2" color="#000000"><span face="Arial, Helvetica, sans-serif" size="2" color="#000000"><span size="1"><br>
		Illustrations used by permission of Bertrand R. Brinley, L.L.C.</span></span><span size="1"></span></span><span size="1"><br>
		Copyright © 2010 Sheridan Brinley</span></span><span size="1"><br>
		</span></span> </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is Google Getting Worse? A Longitudinal Investigation of SEO Spam in Search [pdf] (223 pts)]]></title>
            <link>https://downloads.webis.de/publications/papers/bevendorff_2024a.pdf</link>
            <guid>39013497</guid>
            <pubDate>Tue, 16 Jan 2024 14:21:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://downloads.webis.de/publications/papers/bevendorff_2024a.pdf">https://downloads.webis.de/publications/papers/bevendorff_2024a.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=39013497">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Filecoin Foundation Successfully Deploys IPFS in Space (160 pts)]]></title>
            <link>https://fil.org/blog/filecoin-foundation-successfully-deploys-interplanetary-file-system-ipfs-in-space/</link>
            <guid>39013412</guid>
            <pubDate>Tue, 16 Jan 2024 14:10:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fil.org/blog/filecoin-foundation-successfully-deploys-interplanetary-file-system-ipfs-in-space/">https://fil.org/blog/filecoin-foundation-successfully-deploys-interplanetary-file-system-ipfs-in-space/</a>, See on <a href="https://news.ycombinator.com/item?id=39013412">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><em>Mission Sending Files from Earth to a Satellite to Demonstrate How IPFS Can Improve Communications Across Long Distances</em></p><p>Filecoin Foundation (FF) successfully completed a first-of-its-kind mission deploying the InterPlanetary File System (IPFS) in space. The recent demonstration involved sending files from Earth to orbit and back using an implementation of the IPFS protocol designed for space communications.</p><p>This mission, conducted with Lockheed Martin-developed software, demonstrated how IPFS – a decentralized content distribution system – can bring the benefits of decentralized technologies to space to enable better communications across long distances and resilience in challenging environments.</p><p>As part of the demonstration, <a href="https://github.com/ipfs/papers/raw/master/ipfs-cap2pfs/ipfs-p2p-file-system.pdf">the IPFS white paper</a> and an image of FF’s mascot (Biscuit, the “FileCorgi”) were transmitted to an orbiting LM LINUSS ™CubeSat and back using a purpose-built IPFS implementation that ran atop <a href="https://www.lockheedmartin.com/content/lockheed-martin/en-us/news/features/2024/smartsat-equipped-satellite-uploads-new-mission-on-orbit.html">Lockheed Martin’s SmartSat technology</a>, a software platform that makes it easier to dynamically add and quickly change missions in orbit through simple app uploads.</p><p>This successful demonstration follows nearly three years of collaboration. FF and Lockheed Martin Space initially <a href="https://fil.org/blog/filecoin-foundation-and-lockheed-martin-bring-decentralized-storage-to-space/">announced the collaboration</a> in Davos in May 2022, and <a href="https://fil.org/blog/ff-x-lockheed-martin-mission-announcement/">announced details of the mission</a> in Davos in January 2023.</p><p>“IPFS is called the ‘interplanetary’ file system because, from the beginning, IPFS was envisioned as a technology that could enable networking in space,” said Marta Belcher, president and chair of FF. “We could not be more thrilled to make that vision a reality, thanks to the amazing team at Lockheed Martin Space.”</p><p>This mission demonstrates several key benefits of using IPFS for communications and networking in space:</p><p><em>Faster Communications</em> – Today’s centralized Internet model doesn’t work in space. In a centralized Internet model, data is retrieved from a particular server in a particular place. On Earth, the delay of retrieving that data may not be noticeable. But if you’re on the moon, there will be a multi-second delay each time you retrieve data from Earth. With IPFS, data doesn’t need to go back and forth from Earth with every click. That’s because, with IPFS, data is identified by what it is rather than where it is. Each piece of content has a unique “content ID.” When you look for a piece of content, that content is retrieved from wherever is closest, rather than being retrieved from a particular server in a particular place. That means if someone nearby already has that data, it only has to travel a short distance and can get to you quickly instead of traveling back and forth from Earth with every click.</p><p><em>Data Verification</em> – With IPFS, each piece of content has a unique identifier called a “content ID.” If a piece of content is altered, its content ID will be different as a result. That means that, by using IPFS, you can cryptographically verify that data has not been modified. This is also useful for authenticating data from space. For example, if a satellite takes photographs and then transmits them to the ground using IPFS, it is possible to cryptographically prove that those images were not tampered with, and are, in fact, the original images taken by the satellite.</p><p><em>Data Resilience</em> – One challenge with storing data in space is that the data can easily be corrupted by radiation, or the storage hardware can be damaged by debris. In a centralized Internet model, data is stored in a particular location on a particular piece of hardware, and when you are trying to retrieve that data, there is only one place you can retrieve it from. To put it another way: imagine that you recommend a book to a friend, but you don’t tell your friend the name of the book – instead, you tell your friend that the book is in the New York Public Library, on the third floor, on the second shelf from the left, five books over. That’s how today’s centralized Internet model works – you’re looking for content in a particular location on a particular server. But it makes much more sense to tell your friend the name of the book, and let them find it wherever is closest and most convenient. That’s how IPFS works. With IPFS, you can store many copies of the data in many different locations. When you retrieve data using IPFS, you are looking for a particular content ID rather than looking for data at a particular location. That content will be retrieved from wherever is closest – so if there are many copies of the data, it doesn’t matter if some of those copies have been lost or corrupted (as is often the case when data is being stored in space).</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Passing nothing is surprisingly difficult (110 pts)]]></title>
            <link>https://davidben.net/2024/01/15/empty-slices.html</link>
            <guid>39013194</guid>
            <pubDate>Tue, 16 Jan 2024 13:45:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://davidben.net/2024/01/15/empty-slices.html">https://davidben.net/2024/01/15/empty-slices.html</a>, See on <a href="https://news.ycombinator.com/item?id=39013194">Hacker News</a></p>
<div id="readability-page-1" class="page">





<p>
My day job is in <a href="https://www.chromium.org/">browsers</a> and <a href="https://boringssl.googlesource.com/boringssl/">cryptography</a>, not compilers, yet I often find that I need to spend more of my time working through the semantics of programming languages than using them. This post discusses a thorny cross-language issue between C, C++, and Rust. In short:
</p>
<ul>

<li>C’s rules around pointers and <code>memcpy</code> leave no good ways to represent an empty slice of memory.

</li><li>C++’s pointer rules are fine, but <code>memcpy</code> in C++ inherits the C behavior.

</li><li>Rust FFI is not <a href="https://blog.rust-lang.org/2015/04/24/Rust-Once-Run-Everywhere.html">zero-cost</a>. Rust picked a C/C++-incompatible slice representation, requiring a conversion in each direction. Forgetting the conversion is an easy mistake and unsound.

</li><li>Rust slices appear to <em>also</em> be incompatible with Rust pointer arithmetic, to the point that the standard library’s slice iterator is unsound. <span>(Update 2024-01-16: It sounds like this is <a href="https://github.com/rust-lang/rust/issues/117945">in the process of being fixed</a>!)</span>
</li>
</ul>

<p>As FFI challenges inherently span multiple languages, I wrote this mostly to have one common reference that describes the mismatch.</p>

<h2 id="slices">Slices</h2>

<p>
All three languages allow working with <em>slices</em>, or contiguous sequences of objects in memory. (Also called <a href="https://en.cppreference.com/w/cpp/container/span">spans</a>, but we’ll use “slices” here.) A slice is typically a pointer and a length, <code>(start, count)</code>, giving <code>count</code> objects from <code>start</code>, of some type <code>T</code>.
</p>
<p>
A slice can also be specified by two pointers, <code>(start, end)</code>, giving the objects from <code>start</code> (inclusive) to <code>end</code> (exclusive). This is better for iteration because only one value needs to be adjusted to advance, but the length is less available. C++ iterator pairs are a generalization of this form, and Rust slice iterators use this internally. The two forms can be converted with <code>end = start + count</code> and <code>count = end - start</code>, using C-style pointer arithmetic where everything is scaled by the object size. We’ll primarily discuss <code>(start, count)</code>, but this duality means slices are closely related to pointer arithmetic.
</p>
<p>
In C and C++, slices are, at best, library types built out of pointers and lengths. Sometimes functions just take or return pointer and length separately, but still use it to represent a slice of memory. In Rust, slices are language primitives, but the underlying components are exposed for unsafe code and FFIs. To work with each of these, we must understand what combinations of pointers and lengths are valid.
</p>
<p>
This is straightforward for a positive-length slice: <code>start</code> must point within an allocation where there are at least <code>count</code> objects of type <code>T</code>. But suppose we want an empty (length zero) slice. <strong>What are the valid representations of an empty slice?</strong>
</p>
<p>
Certainly we can point <code>start</code> within (or just past) some array of <code>T</code>s and set <code>count</code> to zero. But we may want to make an empty slice without an existing array. For example, a default-constructed <code>std::span&lt;T&gt;()</code> in C++ or <code>&amp;[]</code> in Rust. What are our options then? In particular:
</p>
<ol>

<li>Can an empty slice be <code>(nullptr, 0)</code>?

</li><li>Can an empty slice be <code>(alignof(T), 0)</code>, or some other aligned address that doesn’t correspond to an allocation?
</li>
</ol>
<p>
The second question may seem odd to C and C++ folks, but Rust folks may recognize it as <code><a href="https://doc.rust-lang.org/std/ptr/struct.NonNull.html#method.dangling">std::ptr::NonNull::dangling</a></code>.

</p><h2 id="c++-slices">C++</h2>

<p>
C++ is the easiest to discuss, as it has a formal specification and is (almost) self-consistent.
</p>
<p>
First, <code>(nullptr, 0)</code> is a valid empty slice in C++. The STL’s types <a href="https://eel.is/c++draft/span.cons#2">routinely return it</a>, and the language is compatible with it:
</p>
<ul>

<li><code>(T*)nullptr + 0</code> is <a href="https://eel.is/c++draft/expr.add#4.1">defined</a> to be <code>(T*)nullptr</code>

</li><li><code>(T*)nullptr - (T*)nullptr</code> is <a href="https://eel.is/c++draft/expr.add#5.1">defined</a> to be <code>0</code>
</li>
</ul>
<p>
C++ defines APIs like <code>std::span</code> in terms of <a href="https://eel.is/c++draft/span.cons#4.1">pointer addition</a> for the <code>(start, count)</code> form, and iterator pairs in terms of <a href="https://eel.is/c++draft/iterator.operations#5">pointer subtraction</a> for the <code>(start, end)</code> form, so this is both necessary and sufficient.
</p>
<p>
Moreover, it would be impractical for C++ to forbid <code>(nullptr, 0)</code>. C++ code routinely needs to interact with APIs that specify slices as individual components. Given such an API, <em>no one</em> writes code like this:
</p>

<pre>void takes_a_slice(const uint8_t *in, size_t len);

uint8_t placeholder;
takes_a_slice(&amp;placeholder, 0);
</pre>

<p>
It is much more natural to use <code>nullptr</code>:
</p>

<pre>void takes_a_slice(const uint8_t *in, size_t len);

takes_a_slice(nullptr, 0);
</pre>

<p>
This means, to be practical, functions like <code>takes_a_slice</code> must accept <code>(nullptr, 0)</code>. For implementing such functions to be practical, the underlying language primitives must then also accept <code>(nullptr, 0)</code>.
</p>

<p>
As for the <code>(alignof(T), 0)</code> question, pointer <a href="https://eel.is/c++draft/expr.add#4.2">addition</a> and <a href="https://eel.is/c++draft/expr.add#5.2">subtraction</a> require the pointers point to some object and that the operation stays within the bounds of that object (or one past the end). C++ does not define there to be an object at <code>alignof(T)</code>, so this is not allowed, instead producing Undefined Behavior. This has no immediate usability concern (no one is going to write <code>reinterpret_cast&lt;uint8_t*&gt;(1)</code> to call <code>takes_a_slice</code>), but we’ll see later that it has some consequences for Rust FFIs.
</p>

<p>(Update 2024-01-16: Added the following paragraph, as this shorthand seems to have been unclear.)</p>

<p>
Of course, <em>in principle</em>, nothing stops <code>takes_a_slice</code> from defining its own unique rules for these corner cases. Beyond what the type system naturally provides, user code will rarely formally define semantics, and we must instead look to fuzzier conventions and expectations. Sadly, in C and C++, these fuzzier aspects often include well-definedness. When all the slice-adjacent language primitives consistently use the same preconditions for slice components, a naively written function will match. This is then a reasonable default interpretation for <code>takes_a_slice</code>’s preconditions. When this post discusses “the” rules for slices for C++ or C, it is in part a shorthand for this emergent convention.
</p>

<p>
However, C++ is merely <em>almost</em> self-consistent. C++ picks up <code>memcpy</code> and other functions from C’s standard library, complete with C’s semantics…
</p>

<h2 id="c-slices">C</h2>

<p>
C is messier. As in C++, it is impractical to reject <code>(nullptr, 0)</code>. However, C does not have C++’s special cases for <code>(T*)nullptr + 0</code> and <code>(T*)nullptr - (T*)nullptr</code>. See clauses 8 and 9 of section 6.5.6 of <a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2310.pdf">N2310</a>. <code>memcpy</code> and the rest of the C standard library similarly <a href="https://www.imperialviolet.org/2016/06/26/nonnull.html">forbid</a> <code>(nullptr, 0)</code>.
</p>
<p>
I think <strong>this should be considered a bug in the C specification</strong>, and compilers should not <a href="https://gcc.gnu.org/gcc-4.9/porting_to.html">optimize based on it</a>. In <a href="https://boringssl.googlesource.com/boringssl/">BoringSSL</a>, we found the C rules so unusable that we resorted to <a href="https://boringssl.googlesource.com/boringssl/+/17cf2cb1d226b0ba2401304242df7ddd3b6f1ff2%5E%21/">wrapping the standard library</a> with <code>n != 0</code> checks. The pointer arithmetic rules are similarly a <a href="https://boringssl.googlesource.com/boringssl/+/6be491b7bb57c3950d4fbb97fdd4a141e3fa4d63%5E%21/">tax</a> <a href="https://boringssl.googlesource.com/boringssl/+/4984e4a6325e9c6302f846c7bf2b75e8ea3fd9dd%5E%21/">on</a> <a href="https://boringssl.googlesource.com/boringssl/+/3c6085b6ae982a80633bf5369c274036702c6848%5E%21/">development</a>. Moreover, C++ inherits C’s standard library (but not its pointer rules), including this behavior. In Chromium’s C++ code, <code>memcpy</code> has been the single biggest blocker to <a href="https://bugs.chromium.org/p/chromium/issues/detail?id=1394755">enabling UBSan</a>.
</p>
<p>
Fortunately, there is hope. <a href="https://www.npopov.com/2024/01/01/This-year-in-LLVM-2023.html#zero-length-operations-on-null">Nikita Popov</a> and Aaron Ballman have written a <a href="https://docs.google.com/document/d/1guH_HgibKrX7t9JfKGfWX2UCPyZOTLsnRfR6UleD1F8/edit">proposal</a> to fix this in C. (Thank you!) While it won’t make C and C++ safe by any stretch of imagination, this is an easy step to fix an unforced error.
</p>
<p>
Note that, apart from contrived examples with deleted null checks, the current rules do not actually help the compiler meaningfully optimize code. A <code>memcpy</code> implementation cannot rely on pointer validity to speculatively read because, even though <code>memcpy(NULL, NULL, 0)</code> is undefined, slices at the end of a buffer are fine:
</p>

<pre>char buf[16];
memcpy(dst, buf + 16, 0);
</pre>

<p>
If <code>buf</code> were at the end of a page with nothing allocated afterwards, a speculative read from <code>memcpy</code> would break.
</p>

<h2 id="rust-slices">Rust</h2>

<p>
Rust does <em>not</em> allow <code>(nullptr, 0)</code>. Functions like <code>std::slice_from_raw_parts</code> <a href="https://doc.rust-lang.org/std/slice/fn.from_raw_parts.html">require the pointer to be non-null</a>. This comes from Rust treating types like <code>&amp;[T]</code> and <code>*[T]</code> as analogous to <code>&amp;T</code> and <code>*T</code>. They are “references” and “pointers” that are represented as <code>(start, count)</code> pairs. Rust requires every pointer type to have a “null” value outside its reference type. This is used in <code>enum</code> layout optimizations. For example, <code>Option::&lt;&amp;[T]&gt;</code> has the same size as <code>&amp;[T]</code> because <code>None</code> uses this null value.
</p>
<p>
Unfortunately, Rust chose <code>(nullptr, 0)</code> for the null slice pointer, which means the empty slice, <code>&amp;[]</code>, cannot use it. That left Rust having to invent an unusual convention: some non-null, aligned, but otherwise dangling pointer, usually <code>(alignof(T), 0)</code>.
</p>
<p>
Is pointer arithmetic defined for this slice? From what I can tell, the answer appears to be no! <span>(Update 2024-01-16: It sounds like this is <a href="https://github.com/rust-lang/rust/issues/117945">in the process of being defined</a>!)</span>
</p>
<p>
Pointer arithmetic in Rust is spelled with the methods <code><a href="https://doc.rust-lang.org/std/primitive.pointer.html#method.add">add</a></code>, <code><a href="https://doc.rust-lang.org/std/primitive.pointer.html#method.sub_ptr">sub_ptr</a></code>, and <code><a href="https://doc.rust-lang.org/std/primitive.pointer.html#method.offset_from">offset_from</a></code>, which the standard library defines in terms of <a href="https://doc.rust-lang.org/std/ptr/index.html#allocated-object">allocated objects</a>. That means, for pointer arithmetic to work with <code>alignof(T)</code>, there must be zero-size slices allocated at every non-zero address. Moreover, <code>offset_from</code> requires the two dangling pointers derived from the same slice to point to the “same” of these objects. While the third bullet <a href="https://doc.rust-lang.org/std/ptr/index.html#safety">here</a>, second sentence, says casting literals gives a pointer that is “valid for zero-sized accesses”, it says nothing about allocated objects or pointer arithmetic.

</p><p>
Ultimately, these semantics come from LLVM. The Rustonomicon has <a href="https://doc.rust-lang.org/nomicon/vec/vec-alloc.html#:~:text=The%20other%20corner%2Dcase%20we%20need%20to%20worry%20about%20is%20empty%20allocations">more to say on this</a> (beginning “The other corner-case…”). It concludes that, while there are infinitely many <em>zero-size</em> types at <code>0x01</code>, Rust conservatively assumes alias analysis does <em>not</em> allow offsetting <code>alignof(T)</code> with zero for <em>positive-sized</em> types. This means <strong>Rust pointer arithmetic rules are incompatible with Rust empty slices.</strong> But recall that slice iteration and pointer arithmetic are deeply related. The Rustonomicon’s <a href="https://doc.rust-lang.org/nomicon/vec/vec-into-iter.html">sample iterator</a> uses pointer arithmetic, but needs to guard addition with <code>cap == 0</code> in <code>into_iter</code> and cast to <code>usize</code> in <code>size_hint</code>.
</p>
<p>
This is too easy for programmers to forget. Indeed the real Rust slice iterator does pointer arithmetic unconditionally (<a href="https://github.com/rust-lang/rust/blob/76101eecbe9aa80753664bbe637ad06d1925f315/library/core/src/slice/iter.rs#L94">pointer addition</a>, <a href="https://github.com/rust-lang/rust/blob/76101eecbe9aa80753664bbe637ad06d1925f315/library/core/src/slice/iter/macros.rs#L57">pointer subtraction</a>, behind <a href="https://github.com/rust-lang/rust/blob/76101eecbe9aa80753664bbe637ad06d1925f315/library/core/src/slice/iter/macros.rs#L141">some</a> <a href="https://github.com/rust-lang/rust/blob/76101eecbe9aa80753664bbe637ad06d1925f315/library/core/src/slice/iter.rs#L132">macros</a>). This suggests <strong>Rust slice iterators are unsound.</strong>
</p>

<h2 id="ffis">FFIs</h2>

<p>
Beyond self-consistency concerns, all this means Rust and C++ slices are incompatible. Not all Rust <code>(start, count)</code> pairs can be passed into C++ and vice versa. C’s issues make its situation less clear, but the natural fix is to bring it in line with C++.
</p>
<p>
This means Rust FFI is not <a href="https://blog.rust-lang.org/2015/04/24/Rust-Once-Run-Everywhere.html">“zero-cost”</a>. <strong>Passing slices between C/C++ and Rust requires conversions in both directions to avoid Undefined Behavior.</strong>
</p>
<p>
More important (to me) than performance, this is a safety and ergonomics problem. Programmers cannot be expected to memorize language specifications. If given a <code>&amp;[T]</code> and trying to call a C API, the natural option is to use <code><a href="https://doc.rust-lang.org/std/primitive.slice.html#method.as_ptr">as_ptr</a></code>, but that will return a C/C++-incompatible output. Most Rust crates I’ve seen which wrap C/C++ APIs do not convert and are unsound as a result.

</p><p>
This is particularly an issue because C and C++’s (more serious) safety problems cause <a href="https://security.googleblog.com/2021/09/an-update-on-memory-safety-in-chrome.html">real user harm</a> and need addressing. But there is half a century of existing C and C++ code. We cannot realistically address this with a new language without good FFI. What makes for good FFI? At a bare minimum, I think <strong>calling a C or C++ function from Rust should not be dramatically less safe than calling that function from C or C++.</strong>
</p>

<h2 id="wishlist">Wishlist</h2>

<p>
Empty lists should not be so complicated. We could change C, C++, and Rust in a few ways to improve things: 
</p>

<h3 id="make-c-accept-nullptr">Make C accept <code>nullptr</code></h3>

<p>
See Nikita Popov and Aaron Ballman’s <a href="https://docs.google.com/document/d/1guH_HgibKrX7t9JfKGfWX2UCPyZOTLsnRfR6UleD1F8/edit">proposal</a>.
</p>

<h3 id="fix-rust-slices">Fix Rust’s slice representation</h3>

<p>
All the <code>alignof(T)</code> problems ultimately come from Rust’s unusual empty slice representation. This falls out of Rust’s need for a “null” <code>*[T]</code> value that is not a <code>&amp;[T]</code> value. Rust could have chosen any of a number of unambiguously unused representations, such as <code>(nullptr, 1)</code>, <code>(nullptr, -1)</code>, or <code>(-1, -1)</code>.
</p>
<p>
While this would be a significant change now, with compatibility implications to work through, I think it is worth seriously considering. It would address the root cause of this mess, fixing a soundness hazard in not just Rust FFI, but Rust on its own. This hazard is real enough that Rust’s standard library hits it.
</p>
<p>
This is also the only option I see that fully meets Rust’s “zero-cost” FFI <a href="https://blog.rust-lang.org/2015/04/24/Rust-Once-Run-Everywhere.html">goals</a>. Even if we make C and C++ accept <code>(alignof(T), 0)</code> from Rust (see below), any slices passed from C/C++ to Rust may still be <code>(nullptr, 0)</code>.
</p>

<h3 id="define-invalid-pointers">Define pointer arithmetic for invalid pointers</h3>

<p>
If Rust leaves its slice representation as is, we instead should define pointer arithmetic for <code>NonNull::dangling()</code>. Expecting low-level Rust code to guard all pointer offsets is impractical.
</p>

<p>Update 2024-01-16: Happily, it sounds like this is already <a href="https://github.com/rust-lang/rust/issues/117945">in the process of being defined</a>!</p>

<p>
Where <code>nullptr</code> is a single value which could just be special-cased, there are many <code>alignof(T)</code> values. It seems one would need to define it in terms of the actual allocated objects. This is well beyond my expertise, so I’ve likely gotten all the details and terminology wrong, but one possibility is, in the vein of <a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2364.pdf">PNVI-ae-udi</a>:
</p>
<ul>

<li><code>cast_ival_to_ptrval</code> returns a special <code>@empty</code> provenance when casting garbage values (unchanged from PNVI-ae-udi)

</li><li>Adding zero to a pointer with the <code>@empty</code> provenance is valid and gives back the original pointer

</li><li>Two pointers with <code>@empty</code> provenance can be subtracted to give zero if they have the same address
</li>
</ul>
<p>
One subtlety, however, is that <code>cast_ival_to_ptrval</code> might not give back <code>@empty</code> if there is an object at that address. Giving back a concrete provenance means picking up <a href="https://www.open-std.org/jtc1/sc22/wg14/www/docs/n2443.pdf">pointer-zapping</a> semantics, which would be undesirable here. For <code>alignof(T)</code>, that shouldn’t happen if the maximum alignment is under a page and the bottom page is never allocated. But Rust allows not just <code>alignof(T)</code> but <a href="https://doc.rust-lang.org/std/ptr/index.html#safety">any non-zero integer literal</a>, <em>even if some allocation happens to exist at that address</em>. (Perhaps we could use the “user-disambiguation” aspect and say all integer-to-pointer casts may additionally disambiguate to <code>@empty</code>? Would that impact the compiler’s aliasing analysis?)
</p>
<p>
I think this complexity demonstrates why <code>nullptr</code> is a much better choice for an empty slice than a dangling pointer. Pointer arithmetic with <code>nullptr</code> is easy to define, and <code>nullptr</code> cannot alias a real allocation.
</p>
<p>
If Rust (and LLVM) accepted invalid pointers, it would fix the soundness issues within Rust, but not with FFIs. If the C and C++ standards <em>also</em> picked this up, it would <em>partially</em> fix FFIs. We could then directly pass Rust slices into C and C++, but not in the other direction. Directly passing C and C++ slices into Rust can only be fixed by changing Rust to accept <code>(nullptr, 0)</code> form.</p>

<p><s>(Outside of Rust FFI, there’s no reason to use <code>alignof(T)</code> as a pointer in C/C++, so I do not know how plausible it would be for C/C++ to accept it.)</s> <span>Update 2024-01-16: Nelson Elhage reminded me that non-null sentinel pointers are sometimes used to <a href="https://github.com/torvalds/linux/blob/ffc253263a1375a65fa6c9f62a893e9767fbebfa/include/linux/slab.h#L167-L178">allocate zero bytes</a>. While C forbids <code>malloc</code> from doing this (<code>malloc(0)</code> must return either <code>nullptr</code> or a <em>unique</em> non-null pointer), other allocators might reasonably pick this option. It makes error checks more uniform without actually reserving address space. So there is a non-Rust reason to allow these pointers in C and C++.
</span></p>

<h3 id="ffi-helpers">FFI helpers in Rust standard library</h3>

<p>
If the languages’ slice representations cannot be made compatible, we’re still left with safety hazards in Rust FFI. In that case, Rust’s standard library should do more to help programmers pick the right operations: Add analogs of <code>slice::from_raw_parts</code>, <code>slice::as_ptr</code>, etc., that use the C and C++ representation, converting internally as needed. Document existing functions very clear warnings that they cannot be used for FFI. Finally, audit all existing calls in crates.io, as the majority of existing calls are likely for FFI.
</p>
<p>
For <code>slice::from_raw_parts</code>, we could go further and fix the existing function itself. This would be backwards-compatible, but adds unnecessary conversions to non-FFI uses. That said, if the crates.io audit reveals mostly FFI uses, that conversion may be warranted. For non-FFI uses, a type signature incorporating <code><a href="https://doc.rust-lang.org/std/ptr/struct.NonNull.html">std::ptr::NonNull</a></code> would have been more appropriate anyway.

</p><p>
This would improve things, but it’s an imperfect solution. We’d still sacrifice zero-cost FFI, and we’d still rely on programmers to read the warnings and realize the natural options are incorrect.
</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>
Thanks to Alex Chernyakhovsky, Alex Gaynor, Dana Jansens, Adam Langley, and Miguel Young de la Sota for reviewing early iterations of this post. Any mistakes in here are my own.
</p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Passwordless: a different kind of hell? (190 pts)]]></title>
            <link>https://jcarlosroldan.com/post/315/passwordless-a-different-kind-of-hell</link>
            <guid>39013036</guid>
            <pubDate>Tue, 16 Jan 2024 13:29:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jcarlosroldan.com/post/315/passwordless-a-different-kind-of-hell">https://jcarlosroldan.com/post/315/passwordless-a-different-kind-of-hell</a>, See on <a href="https://news.ycombinator.com/item?id=39013036">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
<article><header><time datetime="2024-01-16 12:27:07">
	16 ene 2024 / <a href="https://jcarlosroldan.com/code/">code</a>, <a href="https://jcarlosroldan.com/think/">think</a></time></header><p>It's no secret that authenticating into services is an unresolved topic. With time, we have managed to make them more secure, but that was at the expense of user experience. The new generation of mail codes and authenticator apps has moved us from the ease of one-click browser autocomplete to complex ordeals involving multiple steps and sometimes multiple devices.</p>
<p>Last month, I was logging into Notion after it automatically logged me out, and I couldn't help but think "It feels like I'm logging in here every second week; maybe I'm doing something wrong." After a long examination of the settings, I decided to open a ticket asking if the session length was indeed that short. The response from Notion's team was prompt and specific, a great example of customer service. However, the content of the answer was less pleasing.</p>
<p><img src="https://jcarlosroldan.com/static/172.png" alt="Notion response"></p>
<p>Notion is not alone in this; many other services enforce similarly short sessions and uncomfortable methods. This has me pondering the evolution of our authentication methods, from their ancient beginnings to modern complexities. Let's take a look at the history of authentication methods and rate them on two scales: user experience and security.</p>
<p>The first recorded password in western history is the book of Judges. Within the text, Gileadite soldiers used the word "shibboleth" to detect their enemies, the Ephraimites. The Ephraimites spoke in a different dialect so that they would say "sibboleth" instead. <strong>Experience ★★★★★:</strong> you just had to say a word. <strong>Security ☆☆☆☆☆:</strong> there's a single word to authenticate multiple users and it can be cracked by learning how to spell it.</p>
<p>Ancient Romans also relied on passwords in a similar manner called them "watchwords". Every night, roman military guards would pass around a wooden tablet with the watchword inscribed and every military man would pass the tablet around until every encampment marked their initials. During night patrols, soldiers would whisper the watchword to identify allies. <strong>Experience ★★★☆☆:</strong> you just had to say a word but you have to memorize it every day. <strong>Security ★☆☆☆☆:</strong> it changes every day, but it's still a single word, and without a "forgot password" button, a wrong answer would mean a spear in the gut.</p>
<p>Fast forward to the '20s, alcohol became illegal in the US, and speakeasies (illegal drinking establishments) were born. To enter the speakeasy, people had to quietly whisper a code word to keep law enforcement from finding out. Code words were ridiculous, to say the least: coffin varnish, monkey rum, panther sweat, and tarantula juice, to name a few. <strong>Experience ★★★★☆:</strong> you just had to say a word, and they were made to be memorable. <strong>Security ★☆☆☆☆:</strong> it's a single word, and it's not even a secret, but at least you don't get stabbed for getting it wrong.</p>
<p>The first recorded usage of a password in the digital age is attributed to <a href="https://en.wikipedia.org/wiki/Fernando_J._Corbat%C3%B3">Dr. Fernando Corbató</a>. In the 60's, monolithic machines could only work on one problem at a time, which meant that the queue of jobs waiting to be processed was huge and a lot of processing time was lost. He developed an operating system called the Compatible Time-Sharing System (CTSS) that broke large processing tasks into smaller components and gave small slices of time to each task. Since multiple users were sharing one computer, files had to be assigned to individual researchers and available only to them, so he gave every user a unique name and password to access their files stored in the database. However, these passwords were stored in a plaintext file in the computer and there were a few cases of accidental and intentional password leaks. <strong>Experience ★★★☆☆:</strong> you have to remember a user and password. <strong>Security ★★☆☆☆:</strong> it's one per user, but they're stored in plaintext.</p>
<p>To prevent the problem of plaintext passwords, <a href="https://jcarlosroldan.com/files/MorrisThompson79-password-hashing.pdf">Robert Morris and Ken Thompson</a> developed a simulation of a World War 2 crypto machine that scrambled the password before storing it into the system. This way, the system could ask for the password, scramble it, and compare it to the scrambled password stored in the system, a process called one-way hashing. This simulation was included in 6th Edition Unix in 1974, and got several improvements up to our days, but the basic idea remains the same. <strong>Experience ★★★☆☆:</strong> you have to remember a user and password. <strong>Security ★★★☆☆:</strong> it's no longer plaintext, but stealing it would still give you access to the system.</p>
<p><img src="https://jcarlosroldan.com/static/173.png" alt="A Hagelin rotor crypto machine"></p>
<p>Over time, many different problems arised from the fact that people use the same password for multiple services, so the industry started to push for unique passwords for each service. This was a problem for users, since they had to remember a lot of passwords, and password managers were borned. The first password manager was developed by <a href="https://www.schneier.com/">Bruce Schneier</a> in 1997, and currently every major browser comes with a built-in one, often with an option to generate strong passwords and store them for you. <strong>Experience ★★★★☆:</strong> you have to remember a master password, but the browser remembers the rest. <strong>Security ★★★★☆:</strong> it's no longer plaintext, but the master password is the weakest link in the chain.</p>
<p>Phishing attacks and data breaches have made passwords a liability, so the industry has been pushing for multiple-factor authentication (MFA) for a while now. 2FA is a method of authentication that requires two different factors to verify your identity. The first factor is usually something you know, like a password, and the second factor is something you have, like a phone. This way, even if someone steals your password, they still need your phone to log in. There is a myriad of ways to implement 2FA, but the most common ones are SMS codes, authenticator apps, and mail codes. It is often used in conjunction with very short session lengths. <strong>Experience ☆☆☆☆☆:</strong> you have to remember something, have a phone or mail app, and it requires multiple steps. <strong>Security ★★★★☆:</strong> it's no longer a single factor, but it's still vulnerable to phishing attacks.</p>
<p>I, like most people, hate passwords and all means of authentication bureaucracy. And it looks like we're now at the lowest point in history in terms of UX. There is still hope with the rise of Single Sign-On (SSO) and biometrics. And certainly <a href="https://safety.google/authentication/passkey/">passkeys</a>, which are getting a lot of traction lately, are a step in the right direction. But only time will tell if their adoption will be widespread enough to make a difference or if we'll be stuck in this dark age of authentication experience for a while.</p>
<p><br>Related post: <a href="https://jcarlosroldan.com/post/80/the-xor-cipher">The XOR reversible cipher</a></p><p><a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Fjcarlosroldan.com%2Fpost%2F315%2F&amp;title=Passwordless:%20a%20different%20kind%20of%20hell?&amp;summary=Passwordless:%20a%20different%20kind%20of%20hell?&amp;source=https%3A%2F%2Fjcarlosroldan.com%2Fpost%2F315%2F"><img src="https://jcarlosroldan.com/img/sc/lk.svg"></a>
		<a target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fjcarlosroldan.com%2Fpost%2F315%2F"><img src="https://jcarlosroldan.com/img/sc/fb.svg"></a>
		<a target="_blank" href="https://twitter.com/intent/tweet/?text=Passwordless:%20a%20different%20kind%20of%20hell?&amp;url=https%3A%2F%2Fjcarlosroldan.com%2Fpost%2F315%2F"><img src="https://jcarlosroldan.com/img/sc/tw.svg"></a>
		<a target="_blank" href="https://telegram.me/share/url?text=Passwordless:%20a%20different%20kind%20of%20hell?&amp;url=https%3A%2F%2Fjcarlosroldan.com%2Fpost%2F315%2F"><img src="https://jcarlosroldan.com/img/sc/tg.svg"></a>
		<a target="_blank" href="whatsapp://send?text=Passwordless:%20a%20different%20kind%20of%20hell?%20https%3A%2F%2Fjcarlosroldan.com%2Fpost%2F315%2F"><img src="https://jcarlosroldan.com/img/sc/ws.svg"></a>
		<a target="_blank" href="https://jcarlosroldan.com/rss"><img src="https://jcarlosroldan.com/img/sc/rs.svg"></a>
		<a target="_blank" href="https://reddit.com/submit/?url=https%3A%2F%2Fjcarlosroldan.com%2Fpost%2F315%2F&amp;resubmit=true&amp;title=Passwordless:%20a%20different%20kind%20of%20hell?"><img src="https://jcarlosroldan.com/img/sc/rd.svg"></a>
		<a target="_blank" href="https://pinterest.com/pin/create/button/?url=https%3A%2F%2Fjcarlosroldan.com%2Fpost%2F315%2F&amp;media=https%3A%2F%2Fjcarlosroldan.com%2Fpost%2F315%2F&amp;description=Passwordless:%20a%20different%20kind%20of%20hell?"><img src="https://jcarlosroldan.com/img/sc/pt.svg"></a>
		</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Speedbump – a TCP proxy to simulate variable network latency (276 pts)]]></title>
            <link>https://github.com/kffl/speedbump</link>
            <guid>39012697</guid>
            <pubDate>Tue, 16 Jan 2024 12:47:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/kffl/speedbump">https://github.com/kffl/speedbump</a>, See on <a href="https://news.ycombinator.com/item?id=39012697">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:kffl/speedbump" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="0sM7nZRRiW1LupU2NCsmoyZk7EfD5mqwkwnn4sfbbamE7keQVUvkqms-Mgv9_aejnsoqfJLjiTnAmBrLfQ4pAg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="kffl/speedbump" data-current-org="" data-current-owner="kffl" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=3kufkrYHihzjeC8u%2BS87GTM7e3UMzYCYn1Cq2i9c%2BJVU%2BNSruncq8XQd7ss88Xf0tAxFCLVY6Ul%2BCpKVcqUzs3Cb9iC7hwNeo9CmYl1SKY7pNykOiIsWelGPZC3bHW7e3KU9ivS%2F%2Bk6tH9i%2B23nGxbMHmvLOLqidOISISnmuXv699hcX4Y6E7KjQgz%2FWs0oaCF0tHrPvNuIjyYfVVXFN5pd20y4in4djww%2F3q4oJm12VCuTy1S6JIXkI96xNa%2BgBFhPNC4jTFxOsGqHKYIo65ZFK754Ak4F3cBxtJ8eZ61io2ErJR4TwWCjDPJhTLJgeK1YH5tPSb9YYqnGF2hB%2BLJEqVafSFhXqQap5F5Pw38sZSmIxikFcALln0CmMcIDzPBIQsvYXbNnd1V5GZSe2RxjAP4iFLGdd%2F2sQB6WWvtBl5IMtM1v83kvLwOBbqYDpsGIxg982%2BlJ9HidX6x%2BYTIncjgmQC4kTnLA%2FL6ALiLQIB%2FKEDMzRaUD8g2daKYCsFRHqBim%2FDnTcFA%3D%3D--j4ttA4Hteg6R%2FI94--8AUIcMwuxTtMQhIDvwZ1eQ%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=kffl%2Fspeedbump" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/kffl/speedbump&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="c7923f168dce214796196a0b8da925aa646051fc8caf3ff4070290e5bb76c3f0" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bug Thread (148 pts)]]></title>
            <link>https://xkcd.com/2881/</link>
            <guid>39012656</guid>
            <pubDate>Tue, 16 Jan 2024 12:41:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xkcd.com/2881/">https://xkcd.com/2881/</a>, See on <a href="https://news.ycombinator.com/item?id=39012656">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="bottom">
<p><img src="https://imgs.xkcd.com/s/a899e84.jpg" width="520" height="100" alt="Selected Comics" usemap="#comicmap"></p><map id="comicmap" name="comicmap">
<area shape="rect" coords="0,0,100,100" href="/150/" alt="Grownups">
<area shape="rect" coords="104,0,204,100" href="/730/" alt="Circuit Diagram">
<area shape="rect" coords="208,0,308,100" href="/162/" alt="Angular Momentum">
<area shape="rect" coords="312,0,412,100" href="/688/" alt="Self-Description">
<area shape="rect" coords="416,0,520,100" href="/556/" alt="Alternative Energy Revolution">
</map>

<p><a href="https://xkcd.com/1732/"><img src="https://imgs.xkcd.com/s/temperature.png" width="520" height="100" alt="Earth temperature timeline"></a></p>
<br>
<div id="comicLinks"><p>
Comics I enjoy:<br>
        <a href="http://threewordphrase.com/">Three Word Phrase</a>,
        <a href="https://www.smbc-comics.com/">SMBC</a>,
        <a href="https://www.qwantz.com/">Dinosaur Comics</a>,
        <a href="https://oglaf.com/">Oglaf</a> (nsfw),
        <a href="https://www.asofterworld.com/">A Softer World</a>,
        <a href="https://buttersafe.com/">Buttersafe</a>,
        <a href="https://pbfcomics.com/">Perry Bible Fellowship</a>,
        <a href="https://questionablecontent.net/">Questionable Content</a>,
        <a href="http://www.buttercupfestival.com/">Buttercup Festival</a>,
        <a href="https://www.homestuck.com/">Homestuck</a>,
	<a href="https://www.jspowerhour.com/">Junior Scientist Power Hour</a>
</p></div>
<br>

<br>
<center>
<p>xkcd.com is best viewed with Netscape Navigator 4.0 or below on a Pentium 3±1 emulated in Javascript on an Apple IIGS<br>at a screen resolution of 1024x1. Please enable your ad blockers, disable high-heat drying, and remove your device<br>from Airplane Mode and set it to Boat Mode. For security reasons, please leave caps lock on while browsing.</p>
</center>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Where can I find good legal documents? (269 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39012544</link>
            <guid>39012544</guid>
            <pubDate>Tue, 16 Jan 2024 12:27:01 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39012544">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="39012544">
      <td><span></span></td>      <td><center><a id="up_39012544" href="https://news.ycombinator.com/vote?id=39012544&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=39012544">Ask HN: Where can I find good legal documents?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_39012544">164 points</span> by <a href="https://news.ycombinator.com/user?id=yonom">yonom</a> <span title="2024-01-16T12:27:01"><a href="https://news.ycombinator.com/item?id=39012544">3 hours ago</a></span> <span id="unv_39012544"></span> | <a href="https://news.ycombinator.com/hide?id=39012544&amp;goto=item%3Fid%3D39012544">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Where%20can%20I%20find%20good%20legal%20documents%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=39012544&amp;auth=0a327e5b013037f81479976486799cdecb6f4cda">favorite</a> | <a href="https://news.ycombinator.com/item?id=39012544">45&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>Whenever I start a new (side) project, getting the website set up with T&amp;C, Privacy Policy, etc. is a pain point.</p><p>Here are a couple sources I've found:</p><p>- Common Paper (NDA, TOS, SLA, DPA, CSA, ...)</p><p>- YC Safe (Fundraising)</p><p>- Clerky (Fundraising, Employment, ...)</p><p>Looking for more resources like these.</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table><table>
            <tbody><tr id="39013332"><td></td></tr>
                <tr id="39013404"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39013404" href="https://news.ycombinator.com/vote?id=39013404&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>Thanks for the mention and kind words! I’m one of the cofounders of Common Paper, happy to answer any questions.<p>Our docs are free, released under creative comments, have been downloaded more than 17,000 times and used to close millions of dollars worth of deals.</p><p>If you’re not sure what kind of contract you need, this blog post might help:</p><p><a href="https://commonpaper.com/blog/saas-contracts/">https://commonpaper.com/blog/saas-contracts/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39014241"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39014241" href="https://news.ycombinator.com/vote?id=39014241&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>Do you know of any similar resources in the Family Law area? (not sure how to specifically word my question, but helping someone with some custody related issues - and wondering if there is some resource I could be aware of to help?)<p>Else, was going to turn to the GPTs and see what they may muster, but any even general direction pointers would be appreciated?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39014519"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39014519" href="https://news.ycombinator.com/vote?id=39014519&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>IANAL, but there are some things where you should really get expert advice. Child custody is one of them.<p>I think with legal docs generally, you have to decide what the stakes are and act accordingly. In general, keep in mind that most lawyers won't take a case unless there's someone with deep pockets to sue. So for that $20k loan you give to a friend, a boilerplate template is fine; if they don't want to pay you back, a lawsuit is gonna cost you more than the loan anyway. You've got a new startup for website monitoring with 20 customers? Worry about growing your userbase, not the remote chance that you get sued and something in the boilerplate docs you used wasn't worded properly (of course, once you raise significant money or have significant revenue, those legal docs become much more important, and also this doesn't apply if you are working on something with significant risk, such as a medical device).</p><p>But child custody isn't one of those things. It is high stakes, the chances that your counterparty will sue you are very high, and a bad outcome might be one of the worst things that can happen to you. Personally, the possibility of losing custody of my children would be much more worrying to me than any financial lawsuit.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39014805"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39014805" href="https://news.ycombinator.com/vote?id=39014805&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>I have a friend in a messy divorce.  With custody, all rational thought has gone out the window.  Cost of lawyers?  Doesn’t matter, sue!  Need to comply or get fined?  Don’t care.  Court ordered therapy? Don’t feel like it.<p>The pockets that fund this behavior will be empty at some point.  But until then, primal irrational impulses are running the show.</p><p>In a case like that, the difference between the right legal docs and the mostly right docs would be huge.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39014578"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39014578" href="https://news.ycombinator.com/vote?id=39014578&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>I was more looking for properly formatting a document for filings was all?<p>Maybe thats a poor example - but forms help thats not spammy, but also not "starting a business related"</p><p>Legal resources online all seem so "Better Call Saul" quality. Like going to a used car lot.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39014579"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39014579" href="https://news.ycombinator.com/vote?id=39014579&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>&gt; was going to turn to the GPTs<p>this seems ripe for disaster. hopefully, you weren't serious. as with all things, I'd really hope anything in the realm of legal documents from GPT would be then consulted with an actual lawyer
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39014662"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39014662" href="https://news.ycombinator.com/vote?id=39014662&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>Sorry - I am looking for properly formatting, the fact that their case is custody was useless info I guess...<p>I just want to ensure that documents are formatted properly... that was all.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39014771"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_39014771" href="https://news.ycombinator.com/vote?id=39014771&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><br><div>
                  <p><span>just to pile on, but forgoing a lawyer in a custody case is the definition of not smart. just don't do it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                                    <tr id="39014554"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39014554" href="https://news.ycombinator.com/vote?id=39014554&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><br><div>
                  <p><span>I've saved so much time using CommonPaper at my company. Many agreements are still negotiated, but even the cover-page concept makes it that much easier to understand what is being argued.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39014776"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39014776" href="https://news.ycombinator.com/vote?id=39014776&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><br><div>
                  <p><span>Great question! I updated my privacy policy last year and wanted to leverage more "open" policies, but found that world to be lack and instead pooled a bunch I appreciated and asked my laywer to emulate. Wasnt the cheapest approach, but Im happy-</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39014601"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39014601" href="https://news.ycombinator.com/vote?id=39014601&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>I work for iubenda (<a href="https://www.iubenda.com/" rel="nofollow">https://www.iubenda.com/</a>) and it's a precious tool for website compliance. I was a user myself before joining the team.<p>You can get: Privacy Policy/T&amp;C/Cookie and Consent Banner as well as a Consent Database tool.</p><p>The onboarding starts with a scan of your website, and you are suggested to use specific configurations based on the legislation that will apply to your website. Moreover, iubenda scans regularly your website and checks for non-compliance clues (e.g. a missing service in your privacy policy).</p><p>Pricing: there's a free plan for you to start with a basic configuration + pay as you grow.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39014148"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39014148" href="https://news.ycombinator.com/vote?id=39014148&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>I agree with what lots have written here. The biglaw firms that have notable tech practices are good and have resources for brand new startups. For example, CooleyGo or Latham Drive or Wilson Sonsini's term sheet generator. For PPs and Terms, I tend to start with competitor services and see how theirs are written/compare clauses. The more established the company, the more likely it is that you can rely on them to have had their own docs vetted by decent attorneys, though of course quality isn't guaranteed. I have used TermsFeed as a starting point before.<p>For employment matters, SHRM's "Tools and Samples" resources are good.</p><p>Thompson Reuters has a free 7 day trial of their "Practical Law" product, though I haven't explored it personally.</p><p>Techcontracts.com is a good resource.</p><p>ETA: these are all starting points - the docs always have to be reviewed and modified for your particular circumstances. But they’re reasonable for the first draft.</p><p>(I do outside general counsel work for small startups)</p><p>Good luck!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39012918"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39012918" href="https://news.ycombinator.com/vote?id=39012918&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>Here are some early Startup related ideas, links, tools that I gather to not repeat my answers to founders asking for them. For Legals, please go to <a href="https://docs.inboxstartup.com/operate/legal" rel="nofollow">https://docs.inboxstartup.com/operate/legal</a><p>Quite a lot of the founders from the mentioned links/startup/companies are friends or part of a cohort. This is a like an Inbox and I might need to keep cleaning them up.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39014694"><td></td></tr>
            <tr id="39014085"><td></td></tr>
            <tr id="39012967"><td></td></tr>
            <tr id="39014107"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39014107" href="https://news.ycombinator.com/vote?id=39014107&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>&gt; Where can I find good legal documents?<p>A lawyer. A lawyer. A lawyer. A lawyer. END OF STORY.</p><p>And I'm saying that from a perspective of someone who used to use free/cheap template docs a long time ago.</p><p>The hard reality is that free/cheap ready-made docs are highly unlikely to be suitable for your business context for one or more of the following reasons:</p><pre><code>          - Jurisdiction of you or your clients
          - Insurance requirements from your insurer or your clients insurer
          - Clauses not there that should be there
          - Clauses there that are not good enough
          - Clauses there that should not be there
</code></pre>
Free/cheap docs are all fun and games until the shit hits the fan and you need to rely on them.  Its at that point you'll find yourself wishing you ponied up for a lawyer. Trust me, been there, done that, got the postcard, never again.<p>Paying a lawyer to help you with legal documents is a necessary business expense.  Just like paying taxes, either you pay upfront or you pay the penalty later.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39014602"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39014602" href="https://news.ycombinator.com/vote?id=39014602&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>This isn't necessarily true.<p>I've spent six figures on legal fees easily, and I also use templates and off the shelf stuff all the time. Clerky is a good resource and is fine for most core stuff.</p><p>You just can't pay lawyers every time you do everything, it's a waste of resources for small simple businesses that may never go anywhere. And the other issues is EVEN IF YOU DO that doesn't guarantee anything, most lawyers are just using THEIR templates anyways and charging more. If you don't know what to ask for you and don't yet understand the business dynamics you really get almost no value add from having an actual lawyer.</p><p>I'm currently paying a law firm about $20k to rewrite a bunch of docs that I used templates for about 5 years ago. I consider that a success, the business now has millions in revenue and can afford it and it's fine. That's a pretty normal sequence of events in business.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39014234"><td></td></tr>
                <tr id="39014546"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39014546" href="https://news.ycombinator.com/vote?id=39014546&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>Extending this thread because it costs maybe max 2 hours of consult time with a lawyer to put one together.  $500 today can save you from a $5M lawsuit tomorrow.<p>And even then you should still read it and become intimately knowledgeable with each provision
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39013964"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39013964" href="https://news.ycombinator.com/vote?id=39013964&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><br><div>
                  <p><span>Public companies have to file a lot of documents with the SEC, and often contracts get disclosed. Paid services such as Bloomberg Law are essentially glorified search engines on this free public dataset.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39014682"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39014682" href="https://news.ycombinator.com/vote?id=39014682&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>Surprised I had to scroll this far down to find this.  Options? Employment contracts? The SEC database is golden.<p>If you're not doing well enough to pay the lawyer for custom advice, use the example of people who paid to get it right for them.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39013359"><td></td></tr>
            <tr id="39013299"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39013299" href="https://news.ycombinator.com/vote?id=39013299&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><br><div>
                  <p><span>I had a similar problem and found that the ones I made from free sites or using different terminology are often wrong or say the wrong things that don't fit our service. In the end, we outsourced it to a professional legal service. (The ones that make them for free or for a fraction of the cost are often templated and fill-in-the-blank, which is attractive, but has obvious limitations).</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39014047"><td></td></tr>
            <tr id="39012983"><td></td></tr>
            <tr id="39013791"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39013791" href="https://news.ycombinator.com/vote?id=39013791&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><br><div>
                  <p><span>Do any of these resources target multiple different countries? The requirements for these kinds of documents tend to vary wildly, even between countries with similar legal traditions in the Anglosphere.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39013890"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39013890" href="https://news.ycombinator.com/vote?id=39013890&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>I had edited/signed a lot of documents for my service company, other companies I consulted with, and quite a few of the Startups I was part of. The idea is to keep the generic ones as possible and then look for the part particular to the country’s legalities.<p>For instance, for NDA, I will see that “in the case of dispute, the legal court will be this city/country.” I just found a template that can be adapted - <a href="https://www.onenda.org/" rel="nofollow">https://www.onenda.org</a></p><p>I have done this for MSA (Master Service Agreement) and a lot of Statment of Work (SOW) for projects. However, for employment and contracts, I let the lawyers handle it.</p><p>Once you are big, growing, and important enough, you are not asking her on HackerNews; you talk to your lawyers. Before that, most agreements are good to stay afloat till the next stage.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39013248"><td></td></tr>
            <tr id="39013397"><td></td></tr>
            <tr id="39014263"><td></td></tr>
            <tr id="39013297"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39013297" href="https://news.ycombinator.com/vote?id=39013297&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><br><div>
                  <p><span>Wilmerhale has a document generator for a lot of what you need.  No need to log in, create an account etc.  I use these.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39014090"><td></td></tr>
                  <tr id="39014269"><td></td></tr>
            <tr id="39014074"><td></td></tr>
            <tr id="39013849"><td></td></tr>
            <tr id="39013035"><td></td></tr>
                <tr id="39013157"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39013157" href="https://news.ycombinator.com/vote?id=39013157&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><br><div>
                  <p><span>This actually depends. If you need something specific, tailored to your needs and operational niche, then obviously you can not forego a visit to a lawyer. However, for some documents, a reputable template is more than enough. (As even lawyers rarely draft "bespoke" documents for every client and happen to use a templated text more often than not).</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39013743"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39013743" href="https://news.ycombinator.com/vote?id=39013743&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><br><div>
                  <p><span>Even when I've ended up needing a lawyer (albeit for private legal transactions like estate planning) going through the process of finding a template, customizing it to what I wanted, and thinking through as much as I could was invaluable.  The subject matter can be complex and lawyers are expensive, paid by time, and (in my experience) have a habit of bulldozing through explanations to clients; the more prep you've done ahead of time the more value you can get out of your lawyer time (e.g. by knowing what questions to ask and having a clear picture of the issues at hand).</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39013537"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39013537" href="https://news.ycombinator.com/vote?id=39013537&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>It is definitely possible.  But it is not always trivial to know which solution you need without the advice of a lawyer.  Lawyers use templates a lot, but they also know which templates work in which scenarios and in which jurisdictions.<p>Personally I'd be more comfortable using templates on my own for generic business documents, and less comfortable using them for areas of the law that vary greatly by state, like landlord/tenant law, or employment law.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39013603"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39013603" href="https://news.ycombinator.com/vote?id=39013603&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><br><div>
                  <p><span>You have a point, but it's important to note that the template needs to be <i>local</i> to you. Laws aren't global, what is an appropriate T&amp;C for a service hosted in USA won't be okay for a service hosted in UK or Singapore, and even within USA differences in state laws sometimes are critical, so you need to ensure that you're not getting a template aimed at somewhere else.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39014623"><td></td></tr>
                              <tr id="39013907"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39013907" href="https://news.ycombinator.com/vote?id=39013907&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><p><span>A lawyer. I would try contacting a local startup incubator to see if they have any recommendations before you start googling or looking up Avvo.<p>I also found Termly helpful for a first Privacy Policy especially through their wizard, which clears up all your GDPR/CCPA matters, but you want a professional to look this over at some point.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39014059"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39014059" href="https://news.ycombinator.com/vote?id=39014059&amp;how=up&amp;goto=item%3Fid%3D39012544"></a></center>    </td><td><br><div>
                  <p><span>It’s critical to only hire a lawyer who has exact and recent experience with what you need in tech/startups, or you’re paying to educate them and things might not be as good as they could be because they are new to it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ruby (3.3) on Rails (1.0) (210 pts)]]></title>
            <link>https://nashby.github.io/2024/01/15/ruby-3-on-rails-1/</link>
            <guid>39012235</guid>
            <pubDate>Tue, 16 Jan 2024 11:48:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nashby.github.io/2024/01/15/ruby-3-on-rails-1/">https://nashby.github.io/2024/01/15/ruby-3-on-rails-1/</a>, See on <a href="https://news.ycombinator.com/item?id=39012235">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
  <p><span>15 Jan 2024</span></p><p><code>Rails 8.0</code> has recently branched out on Github, and I found myself curious about the feasibility of running <code>Rails 1.0</code> on the latest Ruby version. While I was pretty sure it wouldn’t work right off the bat, I wondered: how many modifications would be necessary to at least reach the iconic “Welcome aboard! You’re riding the Rails!” screen?</p>

<p>So, let’s dive in. My starting point was this Gemfile:</p>

<div><pre><code><span>source</span> <span>"https://rubygems.org"</span>
<span>gem</span> <span>"rails"</span><span>,</span> <span>"1.0.0"</span>
</code></pre></div>

<p>Since I knew that it would be required to make some changes to Rails gems I install it with <code>bundle install --local</code>. This would allow for easier modifications later on. My first attempt was running <code>bundle exec rails --help</code>:</p>

<div><pre><code>/activesupport-1.2.5/lib/active_support/inflector.rb:157: syntax error, unexpected ':', expecting `then' or ',' or ';' or '\n' (SyntaxError)
        when 1: "#{number}st"
              ^
</code></pre></div>

<p>Indeed, older Ruby versions allowed the use of a colon in place of <code>then</code> in case/when expressions. This syntax is no longer supported, so I updated it across the codebase to match the current Ruby syntax.</p>

<p>Ok let’s try again with <code>bundle exec rails --help</code>:</p>

<div><pre><code>cannot load such file -- parsedate (LoadError)
</code></pre></div>

<p>Oh yeah, <code>parsedate</code> lib that was shipped with <code>Ruby 1.8</code> is not longer there. It was used to parse date strings, like so:</p>

<div><pre><code><span>ParseDate</span><span>.</span><span>parsedate</span> <span>"Tuesday, July 5th, 2007, 18:35:20 UTC"</span>
<span># =&gt; [2007, 7, 5, 18, 35, 20, "UTC", 2]</span>
</code></pre></div>

<p>Not sure why it was returning an array but ok. Now I can replace it with <code>DateTime.parse</code> that returns <code>DateTime</code> object. So I’ve fixed that and tried to run it again. Ugh, another error:</p>

<div><pre><code>rails-1.0.0/lib/rails_generator/options.rb:124: syntax error, unexpected '[', expecting '|' (SyntaxError)
...make any changes.') { |options[:pretend]| }
</code></pre></div>

<p>That’s some weird syntax. Turns out you could assign something to a hash right inside the block variable thing:</p>

<div><pre><code><span>opt</span><span>.</span><span>on</span><span>(</span><span>'-p'</span><span>,</span> <span>'--pretend'</span><span>,</span> <span>'Run but do not make any changes.'</span><span>)</span> <span>{</span> <span>|</span><span>options</span><span>[</span><span>:pretend</span><span>]</span><span>|</span> <span>}</span>
</code></pre></div>

<p>meaning that whatever you pass as <code>-p</code> option will end up being assigned to <code>options[:pretend]</code>. Basically it equals to</p>

<div><pre><code><span>opt</span><span>.</span><span>on</span><span>(</span><span>'-p'</span><span>,</span> <span>'--pretend'</span><span>,</span> <span>'Run but do not make any changes.'</span><span>)</span> <span>{</span> <span>|</span><span>o</span><span>|</span> <span>options</span><span>[</span><span>:pretend</span><span>]</span> <span>=</span> <span>o</span> <span>}</span>
</code></pre></div>

<p>Alrighty then. Rerun <code>bundle exec rails --help</code>:</p>

<div><pre><code>no implicit conversion of Enumerator into Array
</code></pre></div>

<p>And it’s without any stacktrace. Great. Looks like something catches all the errors and just prints them. After
some investigation I’ve found this code:</p>

<div><pre><code><span>def</span> <span>cache</span>
  <span>@cache</span> <span>||=</span> <span>sources</span><span>.</span><span>inject</span><span>([])</span> <span>{</span> <span>|</span><span>cache</span><span>,</span> <span>source</span><span>|</span> <span>cache</span> <span>+</span> <span>source</span><span>.</span><span>map</span> <span>}</span>
<span>end</span>
</code></pre></div>

<p>In Ruby 1.8 <code>[].map</code> would return an array but now it returns <code>Enumerator</code> object and you can concat an <code>Array</code> with
<code>Enumerator</code> hence the error:</p>

<div><pre><code>irb(main):001&gt; [] + [].map
(irb):1:in `+': no implicit conversion of Enumerator into Array (TypeError)
</code></pre></div>

<p>It’s an easy fix though. Let’s just call <code>.to_a</code> on the <code>source</code>:</p>

<div><pre><code>def cache
  @cache ||= sources.inject([]) { |cache, source| cache + source.to_a }
end
</code></pre></div>

<p>Are we getting there?</p>

<div><pre><code>bundle exec rails --help

`load': cannot load such file -- config.rb (LoadError)
</code></pre></div>

<p>The code in question is</p>

<div><pre><code><span>require</span> <span>'rbconfig'</span>

<span>DEFAULT_SHEBANG</span> <span>=</span> <span>File</span><span>.</span><span>join</span><span>(</span><span>Config</span><span>::</span><span>CONFIG</span><span>[</span><span>'bindir'</span><span>],</span>
                            <span>Config</span><span>::</span><span>CONFIG</span><span>[</span><span>'ruby_install_name'</span><span>])</span>
</code></pre></div>

<p>Makes sense, in old Ruby <code>RbConfig</code> could be referenced with <code>Config</code> constant and now it’s only <code>RbConfig</code>. Fixed. Does it work now?</p>

<div><pre><code>bundle exec rails --help

Usage: /vendor/bundle/ruby/3.3.0/bin/rails /path/to/your/app [options]
</code></pre></div>

<p>Great Scott! It works! Let’s try to generate a new app:</p>

<div><pre><code>bundle exec rails blog

undefined method `exists?' for class File
</code></pre></div>

<p>Dammit, <code>File.exists?</code>/<code>FileTest.exists?</code> were removed in Ruby 1.9. Let’s replace it with <code>File.exist?</code>/<code>FileTest.exist?</code> and try again:</p>

<div><pre><code>bundle exec rails blog
      create
      create  app/controllers
      create  app/helpers
      create  app/models
      create  app/views/layouts
      create  config/environments
      create  components
      create  db
      create  doc
      create  lib
      create  lib/tasks
      create  log
      create  public/images
      create  public/javascripts
      create  public/stylesheets
      create  script/performance
      create  script/process
      create  test/fixtures
      create  test/functional
      create  test/mocks/development
      create  test/mocks/test
      create  test/unit
      create  vendor
      create  vendor/plugins
      create  Rakefile
      create  README
      create  app/controllers/application.rb
Cannot create Binding object for non-Ruby caller
</code></pre></div>

<p>Success! Is it though? It has generated an app but all the files are empty. And if you have a sharp eye you’ll have noticed this error:</p>

<div><pre><code>Cannot create Binding object for non-Ruby caller
</code></pre></div>

<p>Again, no stacktracks, just some plain error. It took me some time to locate that line of code that was failing with such error but here it is:</p>

<div><pre><code><span>file</span><span>(</span><span>relative_source</span><span>,</span> <span>relative_destination</span><span>,</span> <span>template_options</span><span>)</span> <span>do</span> <span>|</span><span>file</span><span>|</span>
  <span># Evaluate any assignments in a temporary, throwaway binding.</span>
  <span>vars</span> <span>=</span> <span>template_options</span><span>[</span><span>:assigns</span><span>]</span> <span>||</span> <span>{}</span>
  <span>b</span> <span>=</span> <span>binding</span>
  <span>vars</span><span>.</span><span>each</span> <span>{</span> <span>|</span><span>k</span><span>,</span><span>v</span><span>|</span> <span>eval</span> <span>"</span><span>#{</span><span>k</span><span>}</span><span> = vars[:</span><span>#{</span><span>k</span><span>}</span><span>] || vars['</span><span>#{</span><span>k</span><span>}</span><span>']"</span><span>,</span> <span>b</span> <span>}</span>

  <span>...</span>
<span>end</span>
</code></pre></div>

<p>Believe me, I really tried to figure you what this error was about given that it’s obviously a Ruby caller but luck wasn’t there for me. Then I tried to replace <code>binding</code> with <code>Kernel.binding</code>
and it worked… If you know what’s going on here please let me know! Maybe Rails were redefining <code>binding</code> somewhere?</p>

<p>Alright, let’s proceed:</p>

<div><pre><code>bundle exec rails blog
      create
      create  app/controllers
      create  app/helpers
      create  app/models
      ...
</code></pre></div>

<p>Finally! The app is generated, files are not empty. We’re close, I can smell it! Let’s try to start it:</p>

<div><pre><code> bundle exec ruby script/server -p 3001

`require': cannot load such file -- script/../config/boot (LoadError)
</code></pre></div>

<p>Sure, just some random load error. Turns out in Ruby 1.8 you could require a file with relative to current file path and now you can’t:</p>

<div><pre><code><span># In Ruby 1.8</span>
<span>require</span> <span>File</span><span>.</span><span>dirname</span><span>(</span><span>__FILE__</span><span>)</span> <span>+</span> <span>'/../config/boot'</span>

<span># In Ruby 1.9+</span>
<span>require_relative</span> <span>'../config/boot'</span>
</code></pre></div>

<p>With this one fixed we can try it one more time:</p>

<div><pre><code>bundle exec ruby script/server -p 3001

=&gt; Booting WEBrick...

activerecord-1.13.2/lib/active_record/base.rb:708: circular argument reference - table_name (SyntaxError)
</code></pre></div>

<p>Ok this one should be trivial. The code in question:</p>

<div><pre><code><span>def</span> <span>class_name</span><span>(</span><span>table_name</span> <span>=</span> <span>table_name</span><span>)</span>
  <span>...</span>
<span>end</span>
</code></pre></div>

<p>I’m a bit surprised that this was working in Ruby 1.8. The error is pretty self-explanatory so I just renamed default argument value and continued with my life:</p>

<div><pre><code>bundle exec ruby script/server -p 3001

=&gt; Booting WEBrick...

`load': cannot load such file -- big_decimal.rb (LoadError)
Did you mean?  bigdecimal
</code></pre></div>

<p>Right, <code>bigdecimal</code> is not required by default now. I’ll spare you some time and say that there was the same issue
with <code>rexml</code> and <code>net-smtp</code> gems (<code>net-smtp</code> is not even part of Ruby anymore and I had to add it to the <code>Gemfile</code>). So I fixed it and tried again:</p>

<div><pre><code>bundle exec ruby script/server -p 3001

=&gt; Booting WEBrick...
actionmailer-1.1.5/lib/action_mailer/quoting.rb:22: invalid multibyte escape: /[\000-\011\013\014\016-\037\177-\377]/ (SyntaxError)
</code></pre></div>

<p>Oh yeah, Ruby 1.9 did a lot of changes to string encoding (you can read more on this <a href="http://graysoftinc.com/character-encodings/ruby-19s-string">here</a>) and now using raw bytes doesn’t work anymore. So I believe we can convert it to <code>/[\x00-\x11\x13\x14\x16-\x1F\x7F-\xFF]/n</code> and it’s going to work? Well, at least the issue was fixed (yeah, yeah who cares if we’ve just introduced some vulnerability? I don’t):</p>

<div><pre><code>bundle exec ruby script/server -p 3001

=&gt; Booting WEBrick...
`require': cannot load such file -- soap/rpc/driver (LoadError)
</code></pre></div>

<p>Oh ffs. It comes from <code>action_web_service</code> (god knows what was that back in the days) and lucky us we can remove this Rails component
from our stack with this config:</p>

<div><pre><code># Skip frameworks you're not going to use
config.frameworks -= [ :action_web_service ]
</code></pre></div>

<p><img src="https://github.com/nashby/nashby.github.io/assets/200500/1b81569a-5265-4e37-8d33-596c1c130f4b" alt="image"></p>

<div><pre><code>bundle exec ruby script/server -p 3001

=&gt; Booting WEBrick...
`require': cannot load such file -- soap/rpc/driver (LoadError)

rails-1.0.0/lib/rails_info.rb:8: syntax error, unexpected ')' (SyntaxError)
        map {|(name, )| name}
</code></pre></div>

<p>Cool cool, you could do <code>.map { |(param1, )| param1 }</code> in Ruby 1.8 to ommit the second block param. You can actually do it in Ruby 3.3 but you don’t need this extra comma:</p>

<div><pre><code><span>{</span><span>a: </span><span>1</span><span>,</span> <span>b: </span><span>2</span><span>,</span> <span>c: </span><span>3</span><span>}.</span><span>map</span> <span>{</span> <span>|</span><span>a</span><span>,</span> <span>|</span> <span>a</span> <span>}</span> <span># =&gt; [:a, :b, :c]</span>
<span># or</span>
<span>{</span><span>a: </span><span>1</span><span>,</span> <span>b: </span><span>2</span><span>,</span> <span>c: </span><span>3</span><span>}.</span><span>map</span> <span>{</span> <span>|</span><span>(</span><span>a</span><span>)</span><span>|</span> <span>a</span> <span>}</span> <span># =&gt; [:a, :b, :c]</span>
<span># without</span>
<span>{</span><span>a: </span><span>1</span><span>,</span> <span>b: </span><span>2</span><span>,</span> <span>c: </span><span>3</span><span>}.</span><span>map</span> <span>{</span> <span>|</span><span>a</span><span>|</span> <span>a</span> <span>}</span> <span># =&gt;[[:a, 1], [:b, 2], [:c, 3]]</span>
</code></pre></div>

<p>And one more time…</p>

<div><pre><code>bundle exec ruby script/server -p 3001

=&gt; Booting WEBrick...
[2024-01-15 21:23:25] INFO  WEBrick 1.8.1
[2024-01-15 21:23:25] INFO  ruby 3.3.0 (2023-12-25) [arm64-darwin23]
[2024-01-15 21:23:25] INFO  WEBrick::HTTPServer#start: pid=98161 port=3001
</code></pre></div>

<p>Oh my God, we did it!</p>

<p><img src="https://github.com/nashby/nashby.github.io/assets/200500/3710f177-d1ad-47df-9d0c-b80670bf427e" alt="Screenshot 2024-01-15 at 21 24 54"></p>

<p>If for some reason you want to check the code here it is: https://github.com/nashby/rails-from-the-past</p>

<p>I’m pretty sure there are way more issues to fix to make it work properly but I’m not going to do it.
I’m just happy enough that I’ve got to the point where I can see this greeting screen! Fin.</p>

<p>And may Ruby be with you!</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Outline: Self hostable, realtime, Markdown compatible knowledge base (104 pts)]]></title>
            <link>https://github.com/outline/outline</link>
            <guid>39012054</guid>
            <pubDate>Tue, 16 Jan 2024 11:19:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/outline/outline">https://github.com/outline/outline</a>, See on <a href="https://news.ycombinator.com/item?id=39012054">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:outline/outline" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="12vajfGG0To9JTjio7EoeRlpAQ7BZLSxKK77hMEluc9NpM2dCRyp9lJeztd306kHEEf0gbDoRYsPyXao1tdbRQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="outline/outline" data-current-org="outline" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=YwqYHDtuGImxZyPJy4NCevG%2FQy3BGgRq9Q6tfNnGfBcdGO1Y1AYBnfpMJ0d6Kvs8Ww4b9OzrJWhxrKpJOpqg1HvtgaY7Pvx35to73EXEZboguapMyJx9YCrUncgzS1iH7V%2FlGajA0XmKalLaBxhGaatjHYzZN2BchDXCP9fmA3DCtv0Ls0ttru5VScZcfMw64YHVwALpGjmdCQx9nfiB4JeAJueuNjQBlFuGaKwBNzt1JS3uMBbCQCE4pMHq%2FsCre7otRXTIInGs%2BEoQdvB1kUljHO6tsMKi4NkwGKSk6GVBnZBP0vaI1FXatkMNrc0NxT9XUkzSFoIn4n57wORuoeL%2FB2F9KjRDUzfvTSn1ae2awVlYnnf4anA8z5UYoDqQLKc0DbZS5VOkgTRps0PewAVpFJiKmyBUuWM4JlX16x5xjaH6g%2BkVJRvFPHIhdfdrxSNCt9E27DZP3O9GZ2nKpu7IzC0lEhVrF4paD%2ByOVuJ9FQip4S9IL2NVGEOCrx1MRnvV3PyVBe%2F8TQ%3D%3D--D1X1Gx2Ag6P7pPXB--ISzI%2FGkHxEQatl6sdezkVw%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=outline%2Foutline" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/outline/outline&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="2e9f2dd4a691bcf5125d47879d19b600d3b3f4c3364af8de265dcfb964ea6b90" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Std: Clamp generates less efficient assembly than std:min(max,std:max(min,v)) (154 pts)]]></title>
            <link>https://1f6042.blogspot.com/2024/01/stdclamp-still-generates-less-efficient.html</link>
            <guid>39011850</guid>
            <pubDate>Tue, 16 Jan 2024 10:53:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://1f6042.blogspot.com/2024/01/stdclamp-still-generates-less-efficient.html">https://1f6042.blogspot.com/2024/01/stdclamp-still-generates-less-efficient.html</a>, See on <a href="https://news.ycombinator.com/item?id=39011850">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-2600092658134636588" itemprop="description articleBody">
<p>I originally wrote this blog post in 2019 (or maybe 2018 - my timestamps say that it was written before 1 May 2019). Recently I decided to revisit my old blog and found that <code>std::clamp</code> still generates less efficient assembly than <code>std::min(max, std::max(min, v))</code> even on the latest versions of GCC (13.2) and Clang (17.0.1).</p>
<p>Here’s my old blog post:</p>
<p>Contents:</p>
<ul>
<li><a href="#ternary">Ternary</a></li>
<li><a href="#using-intermediate-values">Using intermediate values</a></li>
<li><a href="#using-stdmin-and-stdmax">using std::min and std::max</a></li>
<li><a href="#using-stdclamp">Using std::clamp</a></li>
</ul>
<p>Let’s say you want to clamp a value v between 2 values, min and max. If v is greater than max, return max. If v is smaller than min, return min. Otherwise return v.</p>
<h2 id="ternary">Ternary</h2>
<p>Implementing it directly as per the description:</p>
<pre><code><span>double</span> <span>clamp</span><span>(</span><span>double</span> v<span>,</span> <span>double</span> min<span>,</span> <span>double</span> max<span>)</span><span>{</span>
    <span>return</span> v <span>&lt;</span> min<span>?</span> min <span>:</span> v <span>&gt;</span> max<span>?</span> max <span>:</span> v<span>;</span>
<span>}</span>
</code></pre>
<p>gcc 8.2:</p>
<pre><code>clamp(double, double, double):
        comisd  xmm1, xmm0
        ja      .L2
        minsd   xmm2, xmm0
        movapd  xmm1, xmm2
.L2:
        movapd  xmm0, xmm1
        ret
</code></pre>
<p>One branch instruction.</p>
<p>clang 7.0:</p>
<pre><code>clamp(double, double, double):                            # @clamp(double, double, double)
        minsd   xmm2, xmm0
        cmpltsd xmm0, xmm1
        movapd  xmm3, xmm0
        andnpd  xmm3, xmm2
        andpd   xmm0, xmm1
        orpd    xmm0, xmm3
        ret
</code></pre>
<p>Branchless.</p>

<p>From this stackoverflow answer: <a href="https://stackoverflow.com/questions/427477/fastest-way-to-clamp-a-real-fixed-floating-point-value">https://stackoverflow.com/questions/427477/fastest-way-to-clamp-a-real-fixed-floating-point-value</a></p>
<pre><code><span>double</span> <span>clamp</span><span>(</span><span>double</span> v<span>,</span> <span>double</span> min<span>,</span> <span>double</span> max<span>)</span><span>{</span>
    <span>double</span> out <span>=</span> v <span>&gt;</span> max <span>?</span> max <span>:</span> v<span>;</span>
    <span>return</span> out <span>&lt;</span> min <span>?</span> min <span>:</span> out<span>;</span>
<span>}</span>
</code></pre>
<p>gcc 8.2:</p>
<pre><code>clamp(double, double, double):
        minsd   xmm2, xmm0
        maxsd   xmm1, xmm2
        movapd  xmm0, xmm1
        ret
</code></pre>
<p>clang 7.0:</p>
<pre><code>clamp(double, double, double):                            # @clamp(double, double, double)
        minsd   xmm2, xmm0
        maxsd   xmm1, xmm2
        movapd  xmm0, xmm1
        ret
</code></pre>
<p>Identical output. Much better than before. Can we do better?</p>

<p>This is the idiomatic way to do clamp in C++ (and most other languages):</p>
<pre><code><span>#<span>include</span> <span>&lt;algorithm&gt;</span></span>
<span>double</span> <span>clamp</span><span>(</span><span>double</span> v<span>,</span> <span>double</span> min<span>,</span> <span>double</span> max<span>)</span><span>{</span>
    <span>return</span> std<span>::</span><span>min</span><span>(</span>max<span>,</span> std<span>::</span><span>max</span><span>(</span>min<span>,</span> v<span>)</span><span>)</span><span>;</span>
<span>}</span>
</code></pre>
<p>gcc 8.2:</p>
<pre><code>clamp(double, double, double):
        maxsd   xmm0, xmm1
        minsd   xmm0, xmm2
        ret
</code></pre>
<p>clang 7.0:</p>
<pre><code>clamp(double, double, double):                            # @clamp(double, double, double)
        maxsd   xmm0, xmm1
        minsd   xmm0, xmm2
        ret
</code></pre>
<p>Also seems to generate the best code.</p>
<h2 id="using-stdclamp">Using std::clamp</h2>
<pre><code><span>#<span>include</span> <span>&lt;algorithm&gt;</span></span>
<span>double</span> <span>clamp</span><span>(</span><span>double</span> v<span>,</span> <span>double</span> min<span>,</span> <span>double</span> max<span>)</span><span>{</span>
    <span>return</span> std<span>::</span><span>clamp</span><span>(</span>v<span>,</span> min<span>,</span> max<span>)</span><span>;</span>
<span>}</span>
</code></pre>
<p>gcc 8.2:</p>
<pre><code>clamp(double, double, double):
        comisd  xmm1, xmm0
        ja      .L2
        minsd   xmm2, xmm0
        movapd  xmm1, xmm2
.L2:
        movapd  xmm0, xmm1
        ret
</code></pre>
<p>clang 7.0:</p>
<pre><code>clamp(double, double, double):                            # @clamp(double, double, double)
        minsd   xmm2, xmm0
        cmpltsd xmm0, xmm1
        movapd  xmm3, xmm0
        andnpd  xmm3, xmm2
        andpd   xmm0, xmm1
        orpd    xmm0, xmm3
        ret
</code></pre>
<p>Not very efficient.</p>
<p>EDIT: It’s been almost 5 years since I originally wrote this article, so I decided to try again using the latest versions of GCC and Clang:</p>
<p>gcc 13.2:</p>
<pre><code>clamp(double, double, double):
        maxsd   xmm1, xmm0
        minsd   xmm2, xmm1
        movapd  xmm0, xmm2
        ret
</code></pre>
<p>clang 17.0.1:</p>
<pre><code>clamp(double, double, double):                            # @clamp(double, double, double)
        maxsd   xmm1, xmm0
        minsd   xmm2, xmm1
        movapd  xmm0, xmm2
        ret
</code></pre>
<p>Still not the most efficient - it uses one more instruction than the <code>std::min(max, std::max(min, v))</code> implementation.</p>
<p>But how does the fastest implementation work you ask? Going through the code line by line:</p>
<pre><code>clamp(double, double, double):
        maxsd   xmm0, xmm1
        minsd   xmm0, xmm2
        ret
</code></pre>
<p>The <code>maxsd xmm0, xmm1</code> puts the max value of xmm0 and xmm1 into xmm0.</p>
<p>The <code>minsd xmm0, xmm2</code> puts the min value of xmm0 and xmm2 into xmm0.</p>
<p>Thus, after the first line, xmm0 contains the max of the lower bound and the value.</p>
<p>And after the second line, xmm0 contains the min of the upper bound and the previous result.</p>
<p>But let’s step through with gdb to confirm.</p>
<p>Source code:</p>
<pre><code><span>#<span>include</span> <span>&lt;algorithm&gt;</span></span>
<span>double</span> <span>__attribute__</span> <span>(</span><span>(</span>noinline<span>)</span><span>)</span> <span>clamp</span><span>(</span><span>double</span> v<span>,</span> <span>double</span> min<span>,</span> <span>double</span> max<span>)</span><span>{</span>
    <span>return</span> std<span>::</span><span>min</span><span>(</span>max<span>,</span> std<span>::</span><span>max</span><span>(</span>min<span>,</span> v<span>)</span><span>)</span><span>;</span>
<span>}</span>

<span>int</span> <span>main</span><span>(</span><span>)</span><span>{</span>
    <span>volatile</span> <span>double</span> x<span>,</span> min<span>,</span> max<span>;</span>
    x <span>=</span> <span>1653</span><span>;</span>
    min <span>=</span> <span>1776</span><span>;</span>
    max <span>=</span> <span>1729</span><span>;</span>
    <span>return</span> <span>clamp</span><span>(</span>x<span>,</span> min<span>,</span> max<span>)</span><span>;</span>
<span>}</span>
</code></pre>
<p>gdb logs:</p>
<p>Before running maxsd:</p>
<pre><code>│  &gt; 0x555555555180 &lt;_Z5clampddd&gt;                    maxsd  %xmm1,%xmm0                                                                                       │
│    0x555555555184 &lt;_Z5clampddd+4&gt;                  minsd  %xmm2,%xmm0                                                                                       │
│    0x555555555188 &lt;_Z5clampddd+8&gt;                  ret                                                                                                      │
└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
multi-thre Thread 0x7ffff7dca7 In: clamp                                                                                              L254  PC: 0x555555555180
xmm0           {v8_bfloat16 = {0x0, 0x0, 0xd400, 0x4099, 0x0, 0x0, 0x0, 0x0}, v8_half = {0x0, 0x0, 0xd400, 0x4099, 0x0, 0x0, 0x0, 0x0}, v4_float = {0x0, 0x4099
d400, 0x0, 0x0}, v2_double = {0x4099d40000000000, 0x0}, v16_int8 = {0x0, 0x0, 0x0, 0x0, 0x0, 0xd4, 0x99, 0x40, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, v8_int1
6 = {0x0, 0x0, 0xd400, 0x4099, 0x0, 0x0, 0x0, 0x0}, v4_int32 = {0x0, 0x4099d400, 0x0, 0x0}, v2_int64 = {0x4099d40000000000, 0x0}, uint128 = 0x4099d40000000000}

xmm1           {v8_bfloat16 = {0x0, 0x0, 0xc000, 0x409b, 0x0, 0x0, 0x0, 0x0}, v8_half = {0x0, 0x0, 0xc000, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_float = {0x0, 0x409b
c000, 0x0, 0x0}, v2_double = {0x409bc00000000000, 0x0}, v16_int8 = {0x0, 0x0, 0x0, 0x0, 0x0, 0xc0, 0x9b, 0x40, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, v8_int1
6 = {0x0, 0x0, 0xc000, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_int32 = {0x0, 0x409bc000, 0x0, 0x0}, v2_int64 = {0x409bc00000000000, 0x0}, uint128 = 0x409bc00000000000}

xmm2           {v8_bfloat16 = {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v8_half = {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_float = {0x0, 0x409b04
00, 0x0, 0x0}, v2_double = {0x409b040000000000, 0x0}, v16_int8 = {0x0, 0x0, 0x0, 0x0, 0x0, 0x4, 0x9b, 0x40, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, v8_int16 =
 {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_int32 = {0x0, 0x409b0400, 0x0, 0x0}, v2_int64 = {0x409b040000000000, 0x0}, uint128 = 0x409b040000000000}
</code></pre>
<p>After running maxsd but before running minsd:</p>
<pre><code>│    0x555555555180 &lt;_Z5clampddd&gt;                    maxsd  %xmm1,%xmm0                                                                                       │
│  &gt; 0x555555555184 &lt;_Z5clampddd+4&gt;                  minsd  %xmm2,%xmm0                                                                                       │
│    0x555555555188 &lt;_Z5clampddd+8&gt;                  ret                                                                                                      │
└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
multi-thre Thread 0x7ffff7dca7 In: clamp                                                                                              L4    PC: 0x555555555184
xmm0           {v8_bfloat16 = {0x0, 0x0, 0xc000, 0x409b, 0x0, 0x0, 0x0, 0x0}, v8_half = {0x0, 0x0, 0xc000, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_float = {0x0, 0x409b
c000, 0x0, 0x0}, v2_double = {0x409bc00000000000, 0x0}, v16_int8 = {0x0, 0x0, 0x0, 0x0, 0x0, 0xc0, 0x9b, 0x40, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, v8_int1
6 = {0x0, 0x0, 0xc000, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_int32 = {0x0, 0x409bc000, 0x0, 0x0}, v2_int64 = {0x409bc00000000000, 0x0}, uint128 = 0x409bc00000000000}

xmm1           {v8_bfloat16 = {0x0, 0x0, 0xc000, 0x409b, 0x0, 0x0, 0x0, 0x0}, v8_half = {0x0, 0x0, 0xc000, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_float = {0x0, 0x409b
c000, 0x0, 0x0}, v2_double = {0x409bc00000000000, 0x0}, v16_int8 = {0x0, 0x0, 0x0, 0x0, 0x0, 0xc0, 0x9b, 0x40, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, v8_int1
6 = {0x0, 0x0, 0xc000, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_int32 = {0x0, 0x409bc000, 0x0, 0x0}, v2_int64 = {0x409bc00000000000, 0x0}, uint128 = 0x409bc00000000000}

xmm2           {v8_bfloat16 = {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v8_half = {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_float = {0x0, 0x409b04
00, 0x0, 0x0}, v2_double = {0x409b040000000000, 0x0}, v16_int8 = {0x0, 0x0, 0x0, 0x0, 0x0, 0x4, 0x9b, 0x40, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, v8_int16 =
 {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_int32 = {0x0, 0x409b0400, 0x0, 0x0}, v2_int64 = {0x409b040000000000, 0x0}, uint128 = 0x409b040000000000}
</code></pre>
<p>After running minsd:</p>
<pre><code>│    0x555555555180 &lt;_Z5clampddd&gt;                    maxsd  %xmm1,%xmm0                                                                                       │
│    0x555555555184 &lt;_Z5clampddd+4&gt;                  minsd  %xmm2,%xmm0                                                                                       │
│B+&gt; 0x555555555188 &lt;_Z5clampddd+8&gt;                  ret                                                                                                      │
└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
multi-thre Thread 0x7ffff7dca7 In: clamp                                                                                              L5    PC: 0x555555555188
xmm0           {v8_bfloat16 = {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v8_half = {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_float = {0x0, 0x409b04
00, 0x0, 0x0}, v2_double = {0x409b040000000000, 0x0}, v16_int8 = {0x0, 0x0, 0x0, 0x0, 0x0, 0x4, 0x9b, 0x40, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, v8_int16 =
 {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_int32 = {0x0, 0x409b0400, 0x0, 0x0}, v2_int64 = {0x409b040000000000, 0x0}, uint128 = 0x409b040000000000}

xmm1           {v8_bfloat16 = {0x0, 0x0, 0xc000, 0x409b, 0x0, 0x0, 0x0, 0x0}, v8_half = {0x0, 0x0, 0xc000, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_float = {0x0, 0x409b
c000, 0x0, 0x0}, v2_double = {0x409bc00000000000, 0x0}, v16_int8 = {0x0, 0x0, 0x0, 0x0, 0x0, 0xc0, 0x9b, 0x40, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, v8_int1
6 = {0x0, 0x0, 0xc000, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_int32 = {0x0, 0x409bc000, 0x0, 0x0}, v2_int64 = {0x409bc00000000000, 0x0}, uint128 = 0x409bc00000000000}

xmm2           {v8_bfloat16 = {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v8_half = {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_float = {0x0, 0x409b04
00, 0x0, 0x0}, v2_double = {0x409b040000000000, 0x0}, v16_int8 = {0x0, 0x0, 0x0, 0x0, 0x0, 0x4, 0x9b, 0x40, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, v8_int16 =
 {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_int32 = {0x0, 0x409b0400, 0x0, 0x0}, v2_int64 = {0x409b040000000000, 0x0}, uint128 = 0x409b040000000000}
</code></pre>
<p>And how does the value from xmm0 get placed into eax? The answer is the cvttsd2si instruction:</p>
<pre><code>│    0x555555555080 &lt;main()+64&gt;      call   0x555555555180 &lt;_Z5clampddd&gt;                                                                                      │
│    0x555555555085 &lt;main()+69&gt;      add    $0x20,%rsp                                                                                                        │
│  &gt; 0x555555555089 &lt;main()+73&gt;      cvttsd2si %xmm0,%eax                                                                                                     │
│    0x55555555508d &lt;main()+77&gt;      ret                                                                                                                      │
└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
multi-thre Thread 0x7ffff7dca7 In: main                                                                                               L12   PC: 0x555555555089
(gdb) i r eax xmm0
eax            0x55555040          1431654464
xmm0           {v8_bfloat16 = {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v8_half = {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_float = {0x0, 0x409b04
00, 0x0, 0x0}, v2_double = {0x409b040000000000, 0x0}, v16_int8 = {0x0, 0x0, 0x0, 0x0, 0x0, 0x4, 0x9b, 0x40, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, v8_int16 =
 {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_int32 = {0x0, 0x409b0400, 0x0, 0x0}, v2_int64 = {0x409b040000000000, 0x0}, uint128 = 0x409b040000000000}



│    0x555555555080 &lt;main()+64&gt;      call   0x555555555180 &lt;_Z5clampddd&gt;                                                                                      │
│    0x555555555085 &lt;main()+69&gt;      add    $0x20,%rsp                                                                                                        │
│    0x555555555089 &lt;main()+73&gt;      cvttsd2si %xmm0,%eax                                                                                                     │
│  &gt; 0x55555555508d &lt;main()+77&gt;      ret                                                                                                                      │
└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
multi-thre Thread 0x7ffff7dca7 In: main                                                                                               L13   PC: 0x55555555508d
(gdb) i r eax xmm0
eax            0x6c1               1729
xmm0           {v8_bfloat16 = {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v8_half = {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_float = {0x0, 0x409b04
00, 0x0, 0x0}, v2_double = {0x409b040000000000, 0x0}, v16_int8 = {0x0, 0x0, 0x0, 0x0, 0x0, 0x4, 0x9b, 0x40, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, v8_int16 =
 {0x0, 0x0, 0x400, 0x409b, 0x0, 0x0, 0x0, 0x0}, v4_int32 = {0x0, 0x409b0400, 0x0, 0x0}, v2_int64 = {0x409b040000000000, 0x0}, uint128 = 0x409b040000000000}
</code></pre>
<p>Pretty cool.</p>
<p>Anyway, I found it surprising that <code>std::clamp</code> still generates less efficient assembly than <code>std::min(max, std::max(min, v))</code> even on the latest versions of GCC (13.2) and Clang (17.0.1).</p>


</div></div>]]></description>
        </item>
    </channel>
</rss>